<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003819">
<title confidence="0.955463">
Iterative Transformation of Annotation Guidelines for
Constituency Parsing
</title>
<author confidence="0.996665">
Xiang Li 1, 2 Wenbin Jiang 1 Yajuan L¨u 1 Qun Liu 1, 3
</author>
<affiliation confidence="0.998211">
1Key Laboratory of Intelligent Information Processing
Institute of Computing Technology, Chinese Academy of Sciences
</affiliation>
<email confidence="0.789843">
{lixiang, jiangwenbin, lvyajuan}@ict.ac.cn
</email>
<affiliation confidence="0.996908333333333">
2University of Chinese Academy of Sciences
3Centre for Next Generation Localisation
Faculty of Engineering and Computing, Dublin City University
</affiliation>
<email confidence="0.987458">
qliu@computing.dcu.ie
</email>
<sectionHeader confidence="0.998548" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999938538461539">
This paper presents an effective algorith-
m of annotation adaptation for constituen-
cy treebanks, which transforms a treebank
from one annotation guideline to anoth-
er with an iterative optimization proce-
dure, thus to build a much larger treebank
to train an enhanced parser without in-
creasing model complexity. Experiments
show that the transformed Tsinghua Chi-
nese Treebank as additional training da-
ta brings significant improvement over the
baseline trained on Penn Chinese Tree-
bank only.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999894310344828">
Annotated data have become an indispensable
resource for many natural language processing
(NLP) applications. On one hand, the amount of
existing labeled data is not sufficient; on the other
hand, however there exists multiple annotated da-
ta with incompatible annotation guidelines for the
same NLP task. For example, the People’s Daily
corpus (Yu et al., 2001) and Chinese Penn Tree-
bank (CTB) (Xue et al., 2005) are publicly avail-
able for Chinese segmentation.
An available treebank is a major resource for
syntactic parsing. However, it is often a key bottle-
neck to acquire credible treebanks. Various tree-
banks have been constructed based on differen-
t annotation guidelines. In addition to the most
popular CTB, Tsinghua Chinese Treebank (TC-
T) (Zhou, 2004) is another real large-scale tree-
bank for Chinese constituent parsing. Figure 1 il-
lustrates some differences between CTB and TCT
in grammar category and syntactic structure. Un-
fortunately, these heterogeneous treebanks can not
be directly merged together for training a parsing
model. Such divergences cause a great waste of
human effort. Therefore, it is highly desirable to
transform a treebank into another compatible with
another annotation guideline.
In this paper, we focus on harmonizing het-
erogeneous treebanks to improve parsing perfor-
mance. We first propose an effective approach to
automatic treebank transformation from one an-
notation guideline to another. For convenience
of reference, a treebank with our desired anno-
tation guideline is named as target treebank, and
a treebank with a differtn annotation guideline is
named as source treebank. Our approach proceeds
in three steps. A parser is firstly trained on source
treebank. It is used to relabel the raw sentences
of target treebank, to acquire parallel training da-
ta with two heterogeneous annotation guidelines.
Then, an annotation transformer is trained on the
parallel training data to model the annotation in-
consistencies. In the last step, a parser trained on
target treebank is used to generate k-best parse
trees with target annotation for source sentences.
Then the optimal parse trees are selected by the an-
notation transformer. In this way, the source tree-
bank is transformed to another with our desired
annotation guideline. Then we propose an op-
timization strategy of iterative training to further
improve the transformation performance. At each
iteration, the annotation transformation of source-
to-target and target-to-source are both performed.
The transformed treebank is used to provide better
annotation guideline for the parallel training da-
ta of next iteration. As a result, the better paral-
lel training data will bring an improved annotation
transformer at next iteration.
We perform treebank transformation from TC-
</bodyText>
<page confidence="0.974534">
591
</page>
<note confidence="0.452346">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 591–596,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.990001090909091">
���� �zj�����
�
IP
dj , dj NP VP
�� ��� � � ����
��
n
v
PU
VV
NN
NN
np
,
IP
� � ��
vp
X\\&apos;
n
n
NP
v
d
fkA
z,3�
z,3�
&apos;rflrt
��
,
VP
Z�
&apos;rflrt �� 4 4k* NN AD VV
fkA 4 4k*
</figure>
<figureCaption confidence="0.999976">
Figure 1: Example heterogeneous trees with TCT (left) and CTB (rigth) annotation guidelines.
</figureCaption>
<bodyText confidence="0.999774">
T to CTB, in order to obtain additional tree-
bank to improve a parser. Experiments on Chi-
nese constituent parsing show that, the iterative
training strategy outperforms the basic annotation
transformation baseline. With addidional trans-
formed treebank, the improved parser achieves an
F-measure of 0.95% absolute improvement over
the baseline parser trained on CTB only.
</bodyText>
<sectionHeader confidence="0.983771" genericHeader="introduction">
2 Automatic Annotation Transformation
</sectionHeader>
<bodyText confidence="0.99986725">
In this section, we present an effective approach
that transforms the source treebank to another
compatible with the target annotation guideline,
then describe an optimization strategy of itera-
tive training that conducts several rounds of bidi-
rectional annotation transformation and improves
the transformation performance gradually from a
global view.
</bodyText>
<subsectionHeader confidence="0.898661">
2.1 Principle for Annotation Transformation
</subsectionHeader>
<bodyText confidence="0.99977121875">
In training procedure, the source parser is used to
parse the sentences in the target treebank so that
there are k-best parse trees with the source anno-
tation guideline and one gold tree with the target
annotation guideline for each sentence in the tar-
get treebank. This parallel data is used to train
a source-to-target tree transformer. In transfor-
mation procedure, the source k-best parse trees
are first generated by a parser trained on the tar-
get treebank. Then the optimal source parse trees
with target annotation are selected by the annota-
tion transformer with the help of gold source parse
trees. By combining the target treebank with the
transformed source treebank, it can improve pars-
ing accuracy using a parser trained on the enlarged
treebank.
Algorithm 1 shows the training procedure of
treebank annotation transformation. treebanks
and treebankt denote the source and target tree-
bank respectively. parsers denotes the source
parser. transformers_+t denotes the annota-
tion transformer. treebanknm denotes m treebank
re-labeled with n annotation guideline. Func-
tion TRAIN invokes the Berkeley parser (Petro-
v et al., 2006; Petrov and Klein, 2007) to
train the constituent parsing models. Function
PARSE generates k-best parse trees. Function
TRANSFORMTRAIN invokes the perceptron algo-
rithm (Collins, 2002) to train a discriminative an-
notation transformer. Function TRANSFORM se-
lects the optimal transformed parse trees with the
target annotation.
</bodyText>
<subsectionHeader confidence="0.999742">
2.2 Learning the Annotation Transformer
</subsectionHeader>
<bodyText confidence="0.999898647058824">
To capture the transformation information from
the source treebank to the target treebank, we use
the discriminative reranking technique (Charniak
and Johnson, 2005; Collins and Koo, 2005) to
train the annotation transformer and to score k-
best parse trees with some heterogeneous features.
In this paper, the averaged perceptron algorithm
is used to train the treebank transformation model.
It is an online training algorithm and has been suc-
cessfully used in many NLP tasks, such as pars-
ing (Collins and Roark, 2004) and word segmen-
tation (Zhang and Clark, 2007; Zhang and Clark,
2010).
In addition to the target features which closely
follow Sun et al. (2010). We design the following
quasi-synchronous features to model the annota-
tion inconsistencies.
</bodyText>
<listItem confidence="0.9794888">
• Bigram constituent relation For two con-
secutive fundamental constituents si and sj
in the target parse tree, we find the minimum
categories Ni and Nj of the spans of si and
sj in the source parse tree respectively. Here
</listItem>
<page confidence="0.96314">
592
</page>
<bodyText confidence="0.458454">
Algorithm 1 Basic treebank annotation transformation.
</bodyText>
<listItem confidence="0.7213578">
1: function TRANSFORM-TRAIN(treebanks, treebankt)
2: parsers ← TRAIN(treebanks)
3: treebankst ← PARSE(parsers, treebankt)
4: transformers→t ← TRANSFORMTRAIN(treebankt, treebankst)
5: treebankt s ← TRANSFORM(transformers→t, treebanks)
6: return treebankts ∪ treebankt
Algorithm 2 Iterative treebank annotation transformation.
1: function TRANSFORM-ITERTRAIN(treebanks, treebankt)
2: parsers ← TRAIN(treebanks)
3: parsert ← TRAIN(treebankt)
4: treebankst ← PARSE(parsers, treebankt)
5: treebankts ← PARSE(parsert, treebanks)
6: repeat
7: transformers→t ← TRANSFORMTRAIN(treebankt,treebankst )
8: transformert→s ← TRANSFORMTRAIN(treebanks,treebankts)
9: treebankts ← TRANSFORM(transformers→t, treebanks)
10: treebankst ← TRANSFORM(transformert→s, treebankt)
11: parsert ← TRAIN(treebankts ∪ treebankt)
12: until EVAL(parsert) converges
13: return treebankt s ∪ treebankt
</listItem>
<bodyText confidence="0.95296">
a fundamental constituent is defined to be a
pair of word and its POS tag. If Ni is a sibling
of Nj or each other is identical, we regard the
relation between si and sj as a positive fea-
ture.
</bodyText>
<listItem confidence="0.985653818181818">
• Consistent relation If the span of a target
constituent can be also parsed as a constituent
by the source parser, the combination of tar-
get rule and source category is used.
• Inconsistent relation If the span of a tar-
get constituent cannot be analysed as a con-
stituent by the source parser, the combination
of target rule and corresponding treelet in the
source parse tree is used.
• POS tag The combination of POS tags of
same words in the parallel data is used.
</listItem>
<subsectionHeader confidence="0.900125">
2.3 Iterative Training for Annotation
Transformation
</subsectionHeader>
<bodyText confidence="0.999925764705883">
Treebank annotation transformation relies on the
parallel training data. Consequently, the accuracy
of source parser decides the accuracy of annota-
tion transformer. We propose an iterative training
method to improve the transformation accuracy by
iteratively optimizing the parallel parse trees. At
each iteration of training, the treebank transfor-
mation of source-to-target and target-to-source are
both performed, and the transformed treebank pro-
vides more appropriate annotation for subsequent
iteration. In turn, the annotation transformer can
be improved gradually along with optimization of
the parallel parse trees until convergence.
Algorithm 2 shows the overall procedure of it-
erative training, which terminates when the per-
formance of a parser trained on the target treebank
and the transformed treebank converges.
</bodyText>
<sectionHeader confidence="0.999945" genericHeader="background">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.998318">
3.1 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999907681818182">
We conduct the experiments of treebank transfor-
mation from TCT to CTB. CTB 5.1 is used as
the target treebank. We follow the convention-
al corpus splitting of CTB 5.1: articles 001-270
and 400-1151 are used for training, articles 271-
300 are used as test data and articles 301-325 are
used as developing data. We use slightly modi-
fied version of CTB 5.1 by deleting all the function
tags and empty categories, e.g., *OP*, using Tsur-
geon (Levy and Andrew, 2006). The whole TCT
1.0 is taken as the source treebank for training the
annotation transformer.
The Berkeley parsing model is trained with 5
split-merge iterations. And we run the Berkeley
parser in 100-best mode and construct the 20-fold
cross validation training as described in Charni-
ak and Johnson (2005). In this way, we acquire
the parallel parse trees for training the annotation
transformer.
In this paper, we use bracketing F1 as the Par-
seVal metric provided by EVALB 1 for all experi-
ments.
</bodyText>
<footnote confidence="0.302221">
lhttp://nlp.cs.nyu.edu/evalb/
</footnote>
<page confidence="0.992292">
593
</page>
<figure confidence="0.948797466666667">
Model
Self-training
Base Annotation Transformation
Iterative Annotation Transformation
Baseline
F-Measure (&lt; 40 words)
86.11
86.56
86.75
85.71
F-Measure (all)
83.81
84.23
84.37
83.42
</figure>
<tableCaption confidence="0.984681">
Table 1: The performance of treebank annotation transformation using iterative training.
</tableCaption>
<figure confidence="0.997959736842105">
F score
86.4
86.2
85.8
85.6
85.4
86
F score
78
76
74
84
82
80
Directly parsing
Self-training
Annotation transformation
0.2 0.4 0.6 0.8 1×18,104
Size of CTB training data
</figure>
<figureCaption confidence="0.994237">
Figure 2: Parsing accuracy with different amounts
of CTB training data.
</figureCaption>
<figure confidence="0.992403">
0 1 2 3 4 5 6 7 8 9 10
Training iterations
</figure>
<figureCaption confidence="0.9979515">
Figure 3: Learning curve of iterative transforma-
tion training.
</figureCaption>
<subsectionHeader confidence="0.999473">
3.2 Basic Transformation
</subsectionHeader>
<bodyText confidence="0.999967">
We conduct experiments to evaluate the effect of
the amount of target training data on transforma-
tion accuracy, and how much constituent parser-
s can benefit from our approach. An enhanced
parser is trained on the CTB training data with
the addition of transformed TCT by our anno-
tation transformer. As comparison, we build a
baseline system (direct parsing) using the Berke-
ley parser only trained on the CTB training data.
In this experiment, the self-training method (M-
cClosky et al., 2006a; McClosky et al., 2006b)
is also used to build another strong baseline sys-
tem, which uses unlabelled TCT as additional da-
ta. Figure 2 shows that our approach outperform-
s the two strong baseline systems. It achieves a
0.69% absolute improvement on the CTB test da-
ta over the direct parsing baseline when the whole
CTB training data is used for training. We also can
find that our approach further extends the advan-
tage over the two baseline systems as the amount
of CTB training data decreases in Figure 2. The
figure confirms our approach is effective for im-
proving parser performance, specially for the sce-
nario where the target treebank is scarce.
</bodyText>
<subsectionHeader confidence="0.998603">
3.3 Iterative Transformation
</subsectionHeader>
<bodyText confidence="0.999947625">
We use the iterative training method for annota-
tion transformation. The CTB developing set is
used to determine the optimal training iteration.
After each iteration, we test the performance of
a parser trained on the combined treebank. Fig-
ure 3 shows the performance curve with iteration
ranging from 1 to 10. The performance of basic
annotation transformation is also included in the
curve when iteration is 1. The curve shows that
the maximum performance is achieved at iteration
5. Compared to the basic annotation transforma-
tion, the iterative training strategy leads to a bet-
ter parser with higher accuracy. Table 1 reports
that the final optimized parsing results on the CTB
test set contributes a 0.95% absolute improvement
over the directly parsing baseline.
</bodyText>
<sectionHeader confidence="0.999946" genericHeader="related work">
4 Related Work
</sectionHeader>
<bodyText confidence="0.999916157894737">
Treebank transformation is an effective strategy to
reuse existing annotated data. Wang et al. (1994)
proposed an approach to transform a treebank in-
to another with a different grammar using their
matching metric based on the bracket information
of original treebank. Jiang et al. (2009) proposed
annotation adaptation in Chinese word segmenta-
tion, then, some work were done in parsing (Sun
et al., 2010; Zhu et al., 2011; Sun and Wan, 2012).
Recently, Jiang et al. (2012) proposed an advanced
annotation transformation in Chinese word seg-
mentation, and we extended it to the more com-
plicated treebank annotation transformation used
for Chinese constituent parsing.
Other related work has been focused on semi-
supervised parsing methods which utilize labeled
data to annotate unlabeled data, then use the ad-
ditional annotated data to improve the original
model (McClosky et al., 2006a; McClosky et
</bodyText>
<page confidence="0.99625">
594
</page>
<bodyText confidence="0.99878216">
al., 2006b; Huang and Harper, 2009). The self-
training methodology enlightens us on getting an-
notated treebank compatibal with another annota-
tion guideline. Our approach places extra empha-
sis on improving the transformation performance
with the help of source annotation knowledge.
Apart from constituency-to-constituency tree-
bank transformation, there also exists some re-
search on dependency-to-constituency treebank
transformation. Collins et al. (1999) used trans-
formed constituency treebank from Prague De-
pendency Treebank for constituent parsing on
Czech. Xia and Palmer (2001) explored different
algorithms that transform dependency structure to
phrase structure. Niu et al. (2009) proposed to con-
vert a dependency treebank to a constituency one
by using a parser trained on a constituency tree-
bank to generate k-best lists for sentences in the
dependency treebank. Optimal conversion results
are selected from the k-best lists. Smith and Eisner
(2009) and Li et al. (2012) generated rich quasi-
synchronous grammar features to improve parsing
performance. Some work has been done from the
other direction (Daum et al., 2004; Nivre, 2006;
Johansson and Nugues, 2007).
</bodyText>
<sectionHeader confidence="0.999347" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999952222222222">
This paper propose an effective approach to trans-
form one treebank into another with a different
annotation guideline. Experiments show that our
approach can effectively utilize the heterogeneous
treebanks and significantly improve the state-of-
the-art Chinese constituency parsing performance.
How to exploit more heterogeneous knowledge to
improve the transformation performance is an in-
teresting future issue.
</bodyText>
<sectionHeader confidence="0.998137" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999658833333333">
The authors were supported by National Nat-
ural Science Foundation of China (Contracts
61202216), National Key Technology R&amp;D Pro-
gram (No. 2012BAH39B03), and Key Project
of Knowledge Innovation Program of Chinese A-
cademy of Sciences (No. KGZD-EW-501). Qun
Liu’s work was partially supported by Science
Foundation Ireland (Grant No.07/CE/I1142) as
part of the CNGL at Dublin City University. Sin-
cere thanks to the three anonymous reviewers for
their thorough reviewing and valuable suggestion-
s!
</bodyText>
<sectionHeader confidence="0.995791" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999371519230769">
E. Charniak and M. Johnson. 2005. Coarse-to-fine
n-best parsing and maxent discriminative reranking.
In Proceedings of ACL, pages 173–180.
M. Collins and T. Koo. 2005. Discriminative reranking
for natural language parsing. Computational Lin-
guistics, 31(1):25–70.
M. Collins and B. Roark. 2004. Incremental parsing
with the perceptron algorithm. In Proceedings of
ACL, volume 2004.
M. Collins, L. Ramshaw, J. Hajiˇc, and C. Tillmann.
1999. A statistical parser for czech. In Proceedings
ofACL, pages 505–512.
M. Collins. 2002. Discriminative training method-
s for hidden markov models: Theory and experi-
ments with perceptron algorithms. In Proceedings
of EMNLP, pages 1–8.
M. Daum, K. Foth, and W. Menzel. 2004. Automat-
ic transformation of phrase treebanks to dependency
trees. In Proceedings of LREC.
Z. Huang and M. Harper. 2009. Self-training pcfg
grammars with latent annotations across languages.
In Proceedings of EMNLP, pages 832–841.
W. Jiang, L. Huang, and Q. Liu. 2009. Automatic
adaptation of annotation standards: Chinese word
segmentation and pos tagging: a case study. In Pro-
ceedings of ACL, pages 522–530.
Wenbin Jiang, Fandong Meng, Qun Liu, and Yajuan
L¨u. 2012. Iterative annotation transformation with
predict-self reestimation for chinese word segmen-
tation. In Proceedings of EMNLP, pages 412–420.
R. Johansson and P. Nugues. 2007. Extended
constituent-to-dependency conversion for english.
In Proc. of the 16th Nordic Conference on Compu-
tational Linguistics.
R. Levy and G. Andrew. 2006. Tregex and tsur-
geon: tools for querying and manipulating tree data
structures. In Proceedings of the fifth international
conference on Language Resources and Evaluation,
pages 2231–2234.
Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Ex-
ploiting multiple treebanks for parsing with quasi-
synchronous grammars. In Proceedings of ACL,
pages 675–684.
D. McClosky, E. Charniak, and M. Johnson. 2006a.
Effective self-training for parsing. In Proceedings
of NAACL, pages 152–159.
D. McClosky, E. Charniak, and M. Johnson. 2006b.
Reranking and self-training for parser adaptation. In
Proceedings ofACL, pages 337–344.
Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009.
Exploiting heterogeneous treebanks for parsing. In
Proceedings ofACL, pages 46–54.
</reference>
<page confidence="0.984053">
595
</page>
<reference confidence="0.999796787234042">
J. Nivre. 2006. Inductive dependency parsing.
Springer Verlag.
S. Petrov and D. Klein. 2007. Improved inference for
unlexicalized parsing. In Proceedings of NAACL,
pages 404–411.
S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006.
Learning accurate, compact, and interpretable tree
annotation. In Proceedings of ACL, pages 433–440.
David A Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In Proceedings of EMNLP, pages
822–831.
W. Sun and X. Wan. 2012. Reducing approximation
and estimation errors for chinese lexical processing
with heterogeneous annotations. In Proceedings of
ACL.
W. Sun, R. Wang, and Y. Zhang. 2010. Discriminative
parse reranking for chinese with homogeneous and
heterogeneous annotations. In Proceedings of CIPS-
SIGHAN.
J.N. Wang, J.S. Chang, and K.Y. Su. 1994. An au-
tomatic treebank conversion algorithm for corpus
sharing. In Proceedings of ACL, pages 248–254.
F. Xia and M. Palmer. 2001. Converting dependen-
cy structures to phrase structures. In Proceedings
of the first international conference on Human lan-
guage technology research, pages 1–5.
N. Xue, F. Xia, F.D. Chiou, and M. Palmer. 2005. The
penn chinese treebank: Phrase structure annotation
of a large corpus. Natural Language Engineering,
11(02):207–238.
S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,
H. Wang, Q. Zhao, and W. Zhan. 2001. Processing
norms of modern chinese corpus. Technical Report.
Y. Zhang and S. Clark. 2007. Chinese segmentation
with a word-based perceptron algorithm. In Pro-
ceedings of ACL, pages 840–847.
Y. Zhang and S. Clark. 2010. A fast decoder for joint
word segmentation and pos-tagging using a single
discriminative model. In Proceedings of EMNLP,
pages 843–852.
Q. Zhou. 2004. Annotation scheme for chinese tree-
bank. Journal of Chinese Information Processing,
18(4).
M. Zhu, J. Zhu, and M. Hu. 2011. Better automatic
treebank conversion using a feature-based approach.
In Proceedings of ACL, pages 715–719.
</reference>
<page confidence="0.998661">
596
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.188435">
<title confidence="0.9962985">Iterative Transformation of Annotation Guidelines Constituency Parsing</title>
<author confidence="0.996331">Li Jiang Yajuan L¨u Qun Liu</author>
<affiliation confidence="0.9837545">Laboratory of Intelligent Information Institute of Computing Technology, Chinese Academy of</affiliation>
<email confidence="0.528799">jiangwenbin,</email>
<title confidence="0.818923">of Chinese Academy of for Next Generation</title>
<author confidence="0.479234">Faculty of Engineering</author>
<author confidence="0.479234">Dublin City Computing</author>
<email confidence="0.848664">qliu@computing.dcu.ie</email>
<abstract confidence="0.984851071428572">This paper presents an effective algorithm of annotation adaptation for constituency treebanks, which transforms a treebank from one annotation guideline to another with an iterative optimization procedure, thus to build a much larger treebank to train an enhanced parser without increasing model complexity. Experiments show that the transformed Tsinghua Chinese Treebank as additional training data brings significant improvement over the baseline trained on Penn Chinese Treebank only.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>173--180</pages>
<contexts>
<context position="6657" citStr="Charniak and Johnson, 2005" startWordPosition="1015" endWordPosition="1018">led with n annotation guideline. Function TRAIN invokes the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to train the constituent parsing models. Function PARSE generates k-best parse trees. Function TRANSFORMTRAIN invokes the perceptron algorithm (Collins, 2002) to train a discriminative annotation transformer. Function TRANSFORM selects the optimal transformed parse trees with the target annotation. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram </context>
<context position="10735" citStr="Charniak and Johnson (2005)" startWordPosition="1634" endWordPosition="1638">corpus splitting of CTB 5.1: articles 001-270 and 400-1151 are used for training, articles 271- 300 are used as test data and articles 301-325 are used as developing data. We use slightly modified version of CTB 5.1 by deleting all the function tags and empty categories, e.g., *OP*, using Tsurgeon (Levy and Andrew, 2006). The whole TCT 1.0 is taken as the source treebank for training the annotation transformer. The Berkeley parsing model is trained with 5 split-merge iterations. And we run the Berkeley parser in 100-best mode and construct the 20-fold cross validation training as described in Charniak and Johnson (2005). In this way, we acquire the parallel parse trees for training the annotation transformer. In this paper, we use bracketing F1 as the ParseVal metric provided by EVALB 1 for all experiments. lhttp://nlp.cs.nyu.edu/evalb/ 593 Model Self-training Base Annotation Transformation Iterative Annotation Transformation Baseline F-Measure (&lt; 40 words) 86.11 86.56 86.75 85.71 F-Measure (all) 83.81 84.23 84.37 83.42 Table 1: The performance of treebank annotation transformation using iterative training. F score 86.4 86.2 85.8 85.6 85.4 86 F score 78 76 74 84 82 80 Directly parsing Self-training Annotatio</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>E. Charniak and M. Johnson. 2005. Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proceedings of ACL, pages 173–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>T Koo</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="6681" citStr="Collins and Koo, 2005" startWordPosition="1019" endWordPosition="1022">ine. Function TRAIN invokes the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to train the constituent parsing models. Function PARSE generates k-best parse trees. Function TRANSFORMTRAIN invokes the perceptron algorithm (Collins, 2002) to train a discriminative annotation transformer. Function TRANSFORM selects the optimal transformed parse trees with the target annotation. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For</context>
</contexts>
<marker>Collins, Koo, 2005</marker>
<rawString>M. Collins and T. Koo. 2005. Discriminative reranking for natural language parsing. Computational Linguistics, 31(1):25–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>B Roark</author>
</authors>
<title>Incremental parsing with the perceptron algorithm.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL,</booktitle>
<volume>volume</volume>
<contexts>
<context position="7010" citStr="Collins and Roark, 2004" startWordPosition="1074" endWordPosition="1077">ts the optimal transformed parse trees with the target annotation. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 592 Algorithm 1 Basic treebank annotation transformation. 1: function TRANSFORM-TRAIN(treebanks, treebankt) 2: parsers ← TRAIN(treebanks) 3:</context>
</contexts>
<marker>Collins, Roark, 2004</marker>
<rawString>M. Collins and B. Roark. 2004. Incremental parsing with the perceptron algorithm. In Proceedings of ACL, volume 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>L Ramshaw</author>
<author>J Hajiˇc</author>
<author>C Tillmann</author>
</authors>
<title>A statistical parser for czech.</title>
<date>1999</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>505--512</pages>
<marker>Collins, Ramshaw, Hajiˇc, Tillmann, 1999</marker>
<rawString>M. Collins, L. Ramshaw, J. Hajiˇc, and C. Tillmann. 1999. A statistical parser for czech. In Proceedings ofACL, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms.</title>
<date>2002</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="6311" citStr="Collins, 2002" startWordPosition="969" endWordPosition="970">racy using a parser trained on the enlarged treebank. Algorithm 1 shows the training procedure of treebank annotation transformation. treebanks and treebankt denote the source and target treebank respectively. parsers denotes the source parser. transformers_+t denotes the annotation transformer. treebanknm denotes m treebank re-labeled with n annotation guideline. Function TRAIN invokes the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to train the constituent parsing models. Function PARSE generates k-best parse trees. Function TRANSFORMTRAIN invokes the perceptron algorithm (Collins, 2002) to train a discriminative annotation transformer. Function TRANSFORM selects the optimal transformed parse trees with the target annotation. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training al</context>
</contexts>
<marker>Collins, 2002</marker>
<rawString>M. Collins. 2002. Discriminative training methods for hidden markov models: Theory and experiments with perceptron algorithms. In Proceedings of EMNLP, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Daum</author>
<author>K Foth</author>
<author>W Menzel</author>
</authors>
<title>Automatic transformation of phrase treebanks to dependency trees.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC.</booktitle>
<contexts>
<context position="15587" citStr="Daum et al., 2004" startWordPosition="2404" endWordPosition="2407">ndency Treebank for constituent parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependency structure to phrase structure. Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate k-best lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. Smith and Eisner (2009) and Li et al. (2012) generated rich quasisynchronous grammar features to improve parsing performance. Some work has been done from the other direction (Daum et al., 2004; Nivre, 2006; Johansson and Nugues, 2007). 5 Conclusion This paper propose an effective approach to transform one treebank into another with a different annotation guideline. Experiments show that our approach can effectively utilize the heterogeneous treebanks and significantly improve the state-ofthe-art Chinese constituency parsing performance. How to exploit more heterogeneous knowledge to improve the transformation performance is an interesting future issue. Acknowledgments The authors were supported by National Natural Science Foundation of China (Contracts 61202216), National Key Techn</context>
</contexts>
<marker>Daum, Foth, Menzel, 2004</marker>
<rawString>M. Daum, K. Foth, and W. Menzel. 2004. Automatic transformation of phrase treebanks to dependency trees. In Proceedings of LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Huang</author>
<author>M Harper</author>
</authors>
<title>Self-training pcfg grammars with latent annotations across languages.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>832--841</pages>
<contexts>
<context position="14495" citStr="Huang and Harper, 2009" startWordPosition="2244" endWordPosition="2247">ation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 594 al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituency-to-constituency treebank transformation, there also exists some research on dependency-to-constituency treebank transformation. Collins et al. (1999) used transformed constituency treebank from Prague Dependency Treebank for constituent parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependen</context>
</contexts>
<marker>Huang, Harper, 2009</marker>
<rawString>Z. Huang and M. Harper. 2009. Self-training pcfg grammars with latent annotations across languages. In Proceedings of EMNLP, pages 832–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Jiang</author>
<author>L Huang</author>
<author>Q Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging: a case study.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>522--530</pages>
<contexts>
<context position="13846" citStr="Jiang et al. (2009)" startWordPosition="2141" endWordPosition="2144">rmance is achieved at iteration 5. Compared to the basic annotation transformation, the iterative training strategy leads to a better parser with higher accuracy. Table 1 reports that the final optimized parsing results on the CTB test set contributes a 0.95% absolute improvement over the directly parsing baseline. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; Mc</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>W. Jiang, L. Huang, and Q. Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging: a case study. In Proceedings of ACL, pages 522–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Fandong Meng</author>
<author>Qun Liu</author>
<author>Yajuan L¨u</author>
</authors>
<title>Iterative annotation transformation with predict-self reestimation for chinese word segmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>412--420</pages>
<marker>Jiang, Meng, Liu, L¨u, 2012</marker>
<rawString>Wenbin Jiang, Fandong Meng, Qun Liu, and Yajuan L¨u. 2012. Iterative annotation transformation with predict-self reestimation for chinese word segmentation. In Proceedings of EMNLP, pages 412–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Johansson</author>
<author>P Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In Proc. of the 16th Nordic Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="15629" citStr="Johansson and Nugues, 2007" startWordPosition="2410" endWordPosition="2413">parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependency structure to phrase structure. Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate k-best lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. Smith and Eisner (2009) and Li et al. (2012) generated rich quasisynchronous grammar features to improve parsing performance. Some work has been done from the other direction (Daum et al., 2004; Nivre, 2006; Johansson and Nugues, 2007). 5 Conclusion This paper propose an effective approach to transform one treebank into another with a different annotation guideline. Experiments show that our approach can effectively utilize the heterogeneous treebanks and significantly improve the state-ofthe-art Chinese constituency parsing performance. How to exploit more heterogeneous knowledge to improve the transformation performance is an interesting future issue. Acknowledgments The authors were supported by National Natural Science Foundation of China (Contracts 61202216), National Key Technology R&amp;D Program (No. 2012BAH39B03), and </context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>R. Johansson and P. Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proc. of the 16th Nordic Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>G Andrew</author>
</authors>
<title>Tregex and tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth international conference on Language Resources and Evaluation,</booktitle>
<pages>2231--2234</pages>
<contexts>
<context position="10430" citStr="Levy and Andrew, 2006" startWordPosition="1586" endWordPosition="1589">ng, which terminates when the performance of a parser trained on the target treebank and the transformed treebank converges. 3 Experiments 3.1 Experimental Setup We conduct the experiments of treebank transformation from TCT to CTB. CTB 5.1 is used as the target treebank. We follow the conventional corpus splitting of CTB 5.1: articles 001-270 and 400-1151 are used for training, articles 271- 300 are used as test data and articles 301-325 are used as developing data. We use slightly modified version of CTB 5.1 by deleting all the function tags and empty categories, e.g., *OP*, using Tsurgeon (Levy and Andrew, 2006). The whole TCT 1.0 is taken as the source treebank for training the annotation transformer. The Berkeley parsing model is trained with 5 split-merge iterations. And we run the Berkeley parser in 100-best mode and construct the 20-fold cross validation training as described in Charniak and Johnson (2005). In this way, we acquire the parallel parse trees for training the annotation transformer. In this paper, we use bracketing F1 as the ParseVal metric provided by EVALB 1 for all experiments. lhttp://nlp.cs.nyu.edu/evalb/ 593 Model Self-training Base Annotation Transformation Iterative Annotati</context>
</contexts>
<marker>Levy, Andrew, 2006</marker>
<rawString>R. Levy and G. Andrew. 2006. Tregex and tsurgeon: tools for querying and manipulating tree data structures. In Proceedings of the fifth international conference on Language Resources and Evaluation, pages 2231–2234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Ting Liu</author>
<author>Wanxiang Che</author>
</authors>
<title>Exploiting multiple treebanks for parsing with quasisynchronous grammars.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>675--684</pages>
<contexts>
<context position="15438" citStr="Li et al. (2012)" startWordPosition="2381" endWordPosition="2384">s some research on dependency-to-constituency treebank transformation. Collins et al. (1999) used transformed constituency treebank from Prague Dependency Treebank for constituent parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependency structure to phrase structure. Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate k-best lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. Smith and Eisner (2009) and Li et al. (2012) generated rich quasisynchronous grammar features to improve parsing performance. Some work has been done from the other direction (Daum et al., 2004; Nivre, 2006; Johansson and Nugues, 2007). 5 Conclusion This paper propose an effective approach to transform one treebank into another with a different annotation guideline. Experiments show that our approach can effectively utilize the heterogeneous treebanks and significantly improve the state-ofthe-art Chinese constituency parsing performance. How to exploit more heterogeneous knowledge to improve the transformation performance is an interest</context>
</contexts>
<marker>Li, Liu, Che, 2012</marker>
<rawString>Zhenghua Li, Ting Liu, and Wanxiang Che. 2012. Exploiting multiple treebanks for parsing with quasisynchronous grammars. In Proceedings of ACL, pages 675–684.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="12096" citStr="McClosky et al., 2006" startWordPosition="1853" endWordPosition="1857">0 1 2 3 4 5 6 7 8 9 10 Training iterations Figure 3: Learning curve of iterative transformation training. 3.2 Basic Transformation We conduct experiments to evaluate the effect of the amount of target training data on transformation accuracy, and how much constituent parsers can benefit from our approach. An enhanced parser is trained on the CTB training data with the addition of transformed TCT by our annotation transformer. As comparison, we build a baseline system (direct parsing) using the Berkeley parser only trained on the CTB training data. In this experiment, the self-training method (McClosky et al., 2006a; McClosky et al., 2006b) is also used to build another strong baseline system, which uses unlabelled TCT as additional data. Figure 2 shows that our approach outperforms the two strong baseline systems. It achieves a 0.69% absolute improvement on the CTB test data over the direct parsing baseline when the whole CTB training data is used for training. We also can find that our approach further extends the advantage over the two baseline systems as the amount of CTB training data decreases in Figure 2. The figure confirms our approach is effective for improving parser performance, specially fo</context>
<context position="14441" citStr="McClosky et al., 2006" startWordPosition="2235" endWordPosition="2238">ebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 594 al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituency-to-constituency treebank transformation, there also exists some research on dependency-to-constituency treebank transformation. Collins et al. (1999) used transformed constituency treebank from Prague Dependency Treebank for constituent parsing on Czech. Xia and Palmer (2001)</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006a. Effective self-training for parsing. In Proceedings of NAACL, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McClosky</author>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Reranking and self-training for parser adaptation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>337--344</pages>
<contexts>
<context position="12096" citStr="McClosky et al., 2006" startWordPosition="1853" endWordPosition="1857">0 1 2 3 4 5 6 7 8 9 10 Training iterations Figure 3: Learning curve of iterative transformation training. 3.2 Basic Transformation We conduct experiments to evaluate the effect of the amount of target training data on transformation accuracy, and how much constituent parsers can benefit from our approach. An enhanced parser is trained on the CTB training data with the addition of transformed TCT by our annotation transformer. As comparison, we build a baseline system (direct parsing) using the Berkeley parser only trained on the CTB training data. In this experiment, the self-training method (McClosky et al., 2006a; McClosky et al., 2006b) is also used to build another strong baseline system, which uses unlabelled TCT as additional data. Figure 2 shows that our approach outperforms the two strong baseline systems. It achieves a 0.69% absolute improvement on the CTB test data over the direct parsing baseline when the whole CTB training data is used for training. We also can find that our approach further extends the advantage over the two baseline systems as the amount of CTB training data decreases in Figure 2. The figure confirms our approach is effective for improving parser performance, specially fo</context>
<context position="14441" citStr="McClosky et al., 2006" startWordPosition="2235" endWordPosition="2238">ebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 594 al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituency-to-constituency treebank transformation, there also exists some research on dependency-to-constituency treebank transformation. Collins et al. (1999) used transformed constituency treebank from Prague Dependency Treebank for constituent parsing on Czech. Xia and Palmer (2001)</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>D. McClosky, E. Charniak, and M. Johnson. 2006b. Reranking and self-training for parser adaptation. In Proceedings ofACL, pages 337–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zheng-Yu Niu</author>
<author>Haifeng Wang</author>
<author>Hua Wu</author>
</authors>
<title>Exploiting heterogeneous treebanks for parsing.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>46--54</pages>
<contexts>
<context position="15146" citStr="Niu et al. (2009)" startWordPosition="2332" endWordPosition="2335">ightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituency-to-constituency treebank transformation, there also exists some research on dependency-to-constituency treebank transformation. Collins et al. (1999) used transformed constituency treebank from Prague Dependency Treebank for constituent parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependency structure to phrase structure. Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate k-best lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. Smith and Eisner (2009) and Li et al. (2012) generated rich quasisynchronous grammar features to improve parsing performance. Some work has been done from the other direction (Daum et al., 2004; Nivre, 2006; Johansson and Nugues, 2007). 5 Conclusion This paper propose an effective approach to transform one treebank into another with a different annot</context>
</contexts>
<marker>Niu, Wang, Wu, 2009</marker>
<rawString>Zheng-Yu Niu, Haifeng Wang, and Hua Wu. 2009. Exploiting heterogeneous treebanks for parsing. In Proceedings ofACL, pages 46–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Inductive dependency parsing.</title>
<date>2006</date>
<publisher>Springer Verlag.</publisher>
<contexts>
<context position="15600" citStr="Nivre, 2006" startWordPosition="2408" endWordPosition="2409"> constituent parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependency structure to phrase structure. Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate k-best lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. Smith and Eisner (2009) and Li et al. (2012) generated rich quasisynchronous grammar features to improve parsing performance. Some work has been done from the other direction (Daum et al., 2004; Nivre, 2006; Johansson and Nugues, 2007). 5 Conclusion This paper propose an effective approach to transform one treebank into another with a different annotation guideline. Experiments show that our approach can effectively utilize the heterogeneous treebanks and significantly improve the state-ofthe-art Chinese constituency parsing performance. How to exploit more heterogeneous knowledge to improve the transformation performance is an interesting future issue. Acknowledgments The authors were supported by National Natural Science Foundation of China (Contracts 61202216), National Key Technology R&amp;D Pro</context>
</contexts>
<marker>Nivre, 2006</marker>
<rawString>J. Nivre. 2006. Inductive dependency parsing. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Improved inference for unlexicalized parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of NAACL,</booktitle>
<pages>404--411</pages>
<contexts>
<context position="6152" citStr="Petrov and Klein, 2007" startWordPosition="946" endWordPosition="949"> the annotation transformer with the help of gold source parse trees. By combining the target treebank with the transformed source treebank, it can improve parsing accuracy using a parser trained on the enlarged treebank. Algorithm 1 shows the training procedure of treebank annotation transformation. treebanks and treebankt denote the source and target treebank respectively. parsers denotes the source parser. transformers_+t denotes the annotation transformer. treebanknm denotes m treebank re-labeled with n annotation guideline. Function TRAIN invokes the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to train the constituent parsing models. Function PARSE generates k-best parse trees. Function TRANSFORMTRAIN invokes the perceptron algorithm (Collins, 2002) to train a discriminative annotation transformer. Function TRANSFORM selects the optimal transformed parse trees with the target annotation. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees wit</context>
</contexts>
<marker>Petrov, Klein, 2007</marker>
<rawString>S. Petrov and D. Klein. 2007. Improved inference for unlexicalized parsing. In Proceedings of NAACL, pages 404–411.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>L Barrett</author>
<author>R Thibaux</author>
<author>D Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>433--440</pages>
<contexts>
<context position="6127" citStr="Petrov et al., 2006" startWordPosition="941" endWordPosition="945">ation are selected by the annotation transformer with the help of gold source parse trees. By combining the target treebank with the transformed source treebank, it can improve parsing accuracy using a parser trained on the enlarged treebank. Algorithm 1 shows the training procedure of treebank annotation transformation. treebanks and treebankt denote the source and target treebank respectively. parsers denotes the source parser. transformers_+t denotes the annotation transformer. treebanknm denotes m treebank re-labeled with n annotation guideline. Function TRAIN invokes the Berkeley parser (Petrov et al., 2006; Petrov and Klein, 2007) to train the constituent parsing models. Function PARSE generates k-best parse trees. Function TRANSFORMTRAIN invokes the perceptron algorithm (Collins, 2002) to train a discriminative annotation transformer. Function TRANSFORM selects the optimal transformed parse trees with the target annotation. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to sc</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>S. Petrov, L. Barrett, R. Thibaux, and D. Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proceedings of ACL, pages 433–440.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>822--831</pages>
<contexts>
<context position="15417" citStr="Smith and Eisner (2009)" startWordPosition="2376" endWordPosition="2379">sformation, there also exists some research on dependency-to-constituency treebank transformation. Collins et al. (1999) used transformed constituency treebank from Prague Dependency Treebank for constituent parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependency structure to phrase structure. Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate k-best lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. Smith and Eisner (2009) and Li et al. (2012) generated rich quasisynchronous grammar features to improve parsing performance. Some work has been done from the other direction (Daum et al., 2004; Nivre, 2006; Johansson and Nugues, 2007). 5 Conclusion This paper propose an effective approach to transform one treebank into another with a different annotation guideline. Experiments show that our approach can effectively utilize the heterogeneous treebanks and significantly improve the state-ofthe-art Chinese constituency parsing performance. How to exploit more heterogeneous knowledge to improve the transformation perfo</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David A Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In Proceedings of EMNLP, pages 822–831.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sun</author>
<author>X Wan</author>
</authors>
<title>Reducing approximation and estimation errors for chinese lexical processing with heterogeneous annotations.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="14000" citStr="Sun and Wan, 2012" startWordPosition="2168" endWordPosition="2171">curacy. Table 1 reports that the final optimized parsing results on the CTB test set contributes a 0.95% absolute improvement over the directly parsing baseline. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 594 al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annota</context>
</contexts>
<marker>Sun, Wan, 2012</marker>
<rawString>W. Sun and X. Wan. 2012. Reducing approximation and estimation errors for chinese lexical processing with heterogeneous annotations. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Sun</author>
<author>R Wang</author>
<author>Y Zhang</author>
</authors>
<title>Discriminative parse reranking for chinese with homogeneous and heterogeneous annotations.</title>
<date>2010</date>
<booktitle>In Proceedings of CIPSSIGHAN.</booktitle>
<contexts>
<context position="7154" citStr="Sun et al. (2010)" startWordPosition="1099" endWordPosition="1102">om the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 592 Algorithm 1 Basic treebank annotation transformation. 1: function TRANSFORM-TRAIN(treebanks, treebankt) 2: parsers ← TRAIN(treebanks) 3: treebankst ← PARSE(parsers, treebankt) 4: transformers→t ← TRANSFORMTRAIN(treebankt, treebankst) 5: treebankt s ← TRANSFORM(transformers→t, tre</context>
<context position="13962" citStr="Sun et al., 2010" startWordPosition="2160" endWordPosition="2163">ds to a better parser with higher accuracy. Table 1 reports that the final optimized parsing results on the CTB test set contributes a 0.95% absolute improvement over the directly parsing baseline. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 594 al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated t</context>
</contexts>
<marker>Sun, Wang, Zhang, 2010</marker>
<rawString>W. Sun, R. Wang, and Y. Zhang. 2010. Discriminative parse reranking for chinese with homogeneous and heterogeneous annotations. In Proceedings of CIPSSIGHAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Wang</author>
<author>J S Chang</author>
<author>K Y Su</author>
</authors>
<title>An automatic treebank conversion algorithm for corpus sharing.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>248--254</pages>
<contexts>
<context position="13660" citStr="Wang et al. (1994)" startWordPosition="2111" endWordPosition="2114">e curve with iteration ranging from 1 to 10. The performance of basic annotation transformation is also included in the curve when iteration is 1. The curve shows that the maximum performance is achieved at iteration 5. Compared to the basic annotation transformation, the iterative training strategy leads to a better parser with higher accuracy. Table 1 reports that the final optimized parsing results on the CTB test set contributes a 0.95% absolute improvement over the directly parsing baseline. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused</context>
</contexts>
<marker>Wang, Chang, Su, 1994</marker>
<rawString>J.N. Wang, J.S. Chang, and K.Y. Su. 1994. An automatic treebank conversion algorithm for corpus sharing. In Proceedings of ACL, pages 248–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Xia</author>
<author>M Palmer</author>
</authors>
<title>Converting dependency structures to phrase structures.</title>
<date>2001</date>
<booktitle>In Proceedings of the first international conference on Human language technology research,</booktitle>
<pages>1--5</pages>
<contexts>
<context position="15041" citStr="Xia and Palmer (2001)" startWordPosition="2318" endWordPosition="2321">McClosky et al., 2006a; McClosky et 594 al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal with another annotation guideline. Our approach places extra emphasis on improving the transformation performance with the help of source annotation knowledge. Apart from constituency-to-constituency treebank transformation, there also exists some research on dependency-to-constituency treebank transformation. Collins et al. (1999) used transformed constituency treebank from Prague Dependency Treebank for constituent parsing on Czech. Xia and Palmer (2001) explored different algorithms that transform dependency structure to phrase structure. Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate k-best lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. Smith and Eisner (2009) and Li et al. (2012) generated rich quasisynchronous grammar features to improve parsing performance. Some work has been done from the other direction (Daum et al., 2004; Nivre, 2006; Johansson and Nugues, 2007). 5 Conclusi</context>
</contexts>
<marker>Xia, Palmer, 2001</marker>
<rawString>F. Xia and M. Palmer. 2001. Converting dependency structures to phrase structures. In Proceedings of the first international conference on Human language technology research, pages 1–5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
<author>F Xia</author>
<author>F D Chiou</author>
<author>M Palmer</author>
</authors>
<title>The penn chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Natural Language Engineering,</journal>
<volume>11</volume>
<issue>02</issue>
<contexts>
<context position="1382" citStr="Xue et al., 2005" startWordPosition="199" endWordPosition="202">mplexity. Experiments show that the transformed Tsinghua Chinese Treebank as additional training data brings significant improvement over the baseline trained on Penn Chinese Treebank only. 1 Introduction Annotated data have become an indispensable resource for many natural language processing (NLP) applications. On one hand, the amount of existing labeled data is not sufficient; on the other hand, however there exists multiple annotated data with incompatible annotation guidelines for the same NLP task. For example, the People’s Daily corpus (Yu et al., 2001) and Chinese Penn Treebank (CTB) (Xue et al., 2005) are publicly available for Chinese segmentation. An available treebank is a major resource for syntactic parsing. However, it is often a key bottleneck to acquire credible treebanks. Various treebanks have been constructed based on different annotation guidelines. In addition to the most popular CTB, Tsinghua Chinese Treebank (TCT) (Zhou, 2004) is another real large-scale treebank for Chinese constituent parsing. Figure 1 illustrates some differences between CTB and TCT in grammar category and syntactic structure. Unfortunately, these heterogeneous treebanks can not be directly merged togethe</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>N. Xue, F. Xia, F.D. Chiou, and M. Palmer. 2005. The penn chinese treebank: Phrase structure annotation of a large corpus. Natural Language Engineering, 11(02):207–238.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Yu</author>
<author>J Lu</author>
<author>X Zhu</author>
<author>H Duan</author>
<author>S Kang</author>
<author>H Sun</author>
<author>H Wang</author>
<author>Q Zhao</author>
<author>W Zhan</author>
</authors>
<title>Processing norms of modern chinese corpus.</title>
<date>2001</date>
<tech>Technical Report.</tech>
<contexts>
<context position="1331" citStr="Yu et al., 2001" startWordPosition="189" endWordPosition="192">ain an enhanced parser without increasing model complexity. Experiments show that the transformed Tsinghua Chinese Treebank as additional training data brings significant improvement over the baseline trained on Penn Chinese Treebank only. 1 Introduction Annotated data have become an indispensable resource for many natural language processing (NLP) applications. On one hand, the amount of existing labeled data is not sufficient; on the other hand, however there exists multiple annotated data with incompatible annotation guidelines for the same NLP task. For example, the People’s Daily corpus (Yu et al., 2001) and Chinese Penn Treebank (CTB) (Xue et al., 2005) are publicly available for Chinese segmentation. An available treebank is a major resource for syntactic parsing. However, it is often a key bottleneck to acquire credible treebanks. Various treebanks have been constructed based on different annotation guidelines. In addition to the most popular CTB, Tsinghua Chinese Treebank (TCT) (Zhou, 2004) is another real large-scale treebank for Chinese constituent parsing. Figure 1 illustrates some differences between CTB and TCT in grammar category and syntactic structure. Unfortunately, these heterog</context>
</contexts>
<marker>Yu, Lu, Zhu, Duan, Kang, Sun, Wang, Zhao, Zhan, 2001</marker>
<rawString>S. Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun, H. Wang, Q. Zhao, and W. Zhan. 2001. Processing norms of modern chinese corpus. Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>840--847</pages>
<contexts>
<context position="7055" citStr="Zhang and Clark, 2007" startWordPosition="1082" endWordPosition="1085">target annotation. 2.2 Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 592 Algorithm 1 Basic treebank annotation transformation. 1: function TRANSFORM-TRAIN(treebanks, treebankt) 2: parsers ← TRAIN(treebanks) 3: treebankst ← PARSE(parsers, treebankt) 4: tr</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Y. Zhang and S. Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In Proceedings of ACL, pages 840–847.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Clark</author>
</authors>
<title>A fast decoder for joint word segmentation and pos-tagging using a single discriminative model.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>843--852</pages>
<contexts>
<context position="7079" citStr="Zhang and Clark, 2010" startWordPosition="1086" endWordPosition="1089">Learning the Annotation Transformer To capture the transformation information from the source treebank to the target treebank, we use the discriminative reranking technique (Charniak and Johnson, 2005; Collins and Koo, 2005) to train the annotation transformer and to score kbest parse trees with some heterogeneous features. In this paper, the averaged perceptron algorithm is used to train the treebank transformation model. It is an online training algorithm and has been successfully used in many NLP tasks, such as parsing (Collins and Roark, 2004) and word segmentation (Zhang and Clark, 2007; Zhang and Clark, 2010). In addition to the target features which closely follow Sun et al. (2010). We design the following quasi-synchronous features to model the annotation inconsistencies. • Bigram constituent relation For two consecutive fundamental constituents si and sj in the target parse tree, we find the minimum categories Ni and Nj of the spans of si and sj in the source parse tree respectively. Here 592 Algorithm 1 Basic treebank annotation transformation. 1: function TRANSFORM-TRAIN(treebanks, treebankt) 2: parsers ← TRAIN(treebanks) 3: treebankst ← PARSE(parsers, treebankt) 4: transformers→t ← TRANSFORM</context>
</contexts>
<marker>Zhang, Clark, 2010</marker>
<rawString>Y. Zhang and S. Clark. 2010. A fast decoder for joint word segmentation and pos-tagging using a single discriminative model. In Proceedings of EMNLP, pages 843–852.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Q Zhou</author>
</authors>
<title>Annotation scheme for chinese treebank.</title>
<date>2004</date>
<journal>Journal of Chinese Information Processing,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="1729" citStr="Zhou, 2004" startWordPosition="256" endWordPosition="257">beled data is not sufficient; on the other hand, however there exists multiple annotated data with incompatible annotation guidelines for the same NLP task. For example, the People’s Daily corpus (Yu et al., 2001) and Chinese Penn Treebank (CTB) (Xue et al., 2005) are publicly available for Chinese segmentation. An available treebank is a major resource for syntactic parsing. However, it is often a key bottleneck to acquire credible treebanks. Various treebanks have been constructed based on different annotation guidelines. In addition to the most popular CTB, Tsinghua Chinese Treebank (TCT) (Zhou, 2004) is another real large-scale treebank for Chinese constituent parsing. Figure 1 illustrates some differences between CTB and TCT in grammar category and syntactic structure. Unfortunately, these heterogeneous treebanks can not be directly merged together for training a parsing model. Such divergences cause a great waste of human effort. Therefore, it is highly desirable to transform a treebank into another compatible with another annotation guideline. In this paper, we focus on harmonizing heterogeneous treebanks to improve parsing performance. We first propose an effective approach to automat</context>
</contexts>
<marker>Zhou, 2004</marker>
<rawString>Q. Zhou. 2004. Annotation scheme for chinese treebank. Journal of Chinese Information Processing, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhu</author>
<author>J Zhu</author>
<author>M Hu</author>
</authors>
<title>Better automatic treebank conversion using a feature-based approach.</title>
<date>2011</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>715--719</pages>
<contexts>
<context position="13980" citStr="Zhu et al., 2011" startWordPosition="2164" endWordPosition="2167">ser with higher accuracy. Table 1 reports that the final optimized parsing results on the CTB test set contributes a 0.95% absolute improvement over the directly parsing baseline. 4 Related Work Treebank transformation is an effective strategy to reuse existing annotated data. Wang et al. (1994) proposed an approach to transform a treebank into another with a different grammar using their matching metric based on the bracket information of original treebank. Jiang et al. (2009) proposed annotation adaptation in Chinese word segmentation, then, some work were done in parsing (Sun et al., 2010; Zhu et al., 2011; Sun and Wan, 2012). Recently, Jiang et al. (2012) proposed an advanced annotation transformation in Chinese word segmentation, and we extended it to the more complicated treebank annotation transformation used for Chinese constituent parsing. Other related work has been focused on semisupervised parsing methods which utilize labeled data to annotate unlabeled data, then use the additional annotated data to improve the original model (McClosky et al., 2006a; McClosky et 594 al., 2006b; Huang and Harper, 2009). The selftraining methodology enlightens us on getting annotated treebank compatibal</context>
</contexts>
<marker>Zhu, Zhu, Hu, 2011</marker>
<rawString>M. Zhu, J. Zhu, and M. Hu. 2011. Better automatic treebank conversion using a feature-based approach. In Proceedings of ACL, pages 715–719.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>