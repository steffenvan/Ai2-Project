<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002437">
<title confidence="0.985357">
Lexical Inference over Multi-Word Predicates: A Distributional Approach
</title>
<author confidence="0.996474">
Omri Abend Shay B. Cohen Mark Steedman
</author>
<affiliation confidence="0.99986">
School of Informatics, University of Edinburgh,
</affiliation>
<address confidence="0.878624">
Edinburgh EH8 9AB, United Kingdom
</address>
<email confidence="0.998789">
{oabend,scohen,steedman}@inf.ed.ac.uk
</email>
<sectionHeader confidence="0.993887" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99979956">
Representing predicates in terms of their
argument distribution is common practice
in NLP. Multi-word predicates (MWPs) in
this context are often either disregarded or
considered as fixed expressions. The lat-
ter treatment is unsatisfactory in two ways:
(1) identifying MWPs is notoriously diffi-
cult, (2) MWPs show varying degrees of
compositionality and could benefit from
taking into account the identity of their
component parts. We propose a novel
approach that integrates the distributional
representation of multiple sub-sets of the
MWP’s words. We assume a latent distri-
bution over sub-sets of the MWP, and esti-
mate it relative to a downstream prediction
task. Focusing on the supervised identi-
fication of lexical inference relations, we
compare against state-of-the-art baselines
that consider a single sub-set of an MWP,
obtaining substantial improvements. To
our knowledge, this is the first work to
address lexical relations between MWPs
of varying degrees of compositionality
within distributional semantics.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999790132075472">
Multi-word expressions (MWEs) constitute a
large part of the lexicon and account for much
of its growth (Jackendoff, 2002; Seaton and
Macaulay, 2002). However, despite their impor-
tance, MWEs remain difficult to define and model,
and consequently pose serious difficulties for NLP
applications (Sag et al., 2001). Multi-word Predi-
cates (MWPs; sometimes termed Complex Predi-
cates) form an important and much addressed sub-
class of MWEs and are the focus of this paper.
MWPs are informally defined as multiple words
that constitute a single predicate (Alsina et al.,
1997). MWPs encompass a wide range of phe-
nomena, including causatives, light verbs, phrasal
verbs, serial verb constructions and many others,
and pose considerable challenges to both linguistic
theory and NLP applications (see Section 2). Part
of the difficulty in treating them stems from their
position on the borderline between syntax and the
lexicon. It is therefore often unclear whether they
should be treated as fixed expressions, as compo-
sitional phrases that reflect the properties of their
component parts or as both.
This work addresses the modelling of MWPs
within the context of distributional semantics (Tur-
ney and Pantel, 2010), in which predicates are
represented through the distribution of arguments
they may take. In order to collect meaningful
statistics, the predicate’s lexical unit should be suf-
ficiently frequent and semantically unambiguous.
MWPs pose a challenge to such models, as
naively collecting statistics over all instances of
highly ambiguous verbs is likely to result in noisy
representations. For instance, the verb “take” may
appear in MWPs as varied as “take time”, “take
effect” and “take to the hills”. This heterogene-
ity of “take” is likely to have a negative effect on
downstream systems that use its distributional rep-
resentation. For instance, while “take” and “ac-
cept” are often considered lexically similar, the
high frequency in which “take” participates in
non-compositional MWPs is likely to push the two
verbs’ distributional representations apart.
A straightforward approach to this problem is
to represent the predicate as a conjunction of mul-
tiple words, thereby trading ambiguity for spar-
sity. For instance, the verb “take” could be con-
joined with its object (e.g., “take care”, “take a
bus”). This approach, however, raises the chal-
lenge of identifying the sub-set of the predicate’s
words that should be taken to represent it (hence-
forth, its lexical components or LCs).
We propose a novel approach that addresses this
</bodyText>
<page confidence="0.981012">
644
</page>
<note confidence="0.831161">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999913208333333">
challenge in the context of identifying lexical in-
ference relations between predicates (Lin and Pan-
tel, 2001; Schoenmackers et al., 2010; Melamud et
al., 2013a, inter alia). A (lexical) inference rela-
tion PL → PR is said to hold if the relation denoted
by PR generally holds between a set of arguments
whenever the relation PL does. For instance, an in-
ference relation holds between “annex” and “con-
trol” since if a country annexes another, it gener-
ally controls it. Most works to this task use dis-
tributional similarity, either as their main compo-
nent (Szpektor and Dagan, 2008; Melamud et al.,
2013b), or as part of a more comprehensive system
(Berant et al., 2011; Lewis and Steedman, 2013).
For example, consider the verb “take”. While
the inference relation “have → take” does not gen-
erally hold, it does hold in the case of some light
verbs, such as “have a look → take a look”, under-
scoring the importance of taking more inclusive
LCs into account. On the other hand, the pred-
icate “likely to give a green light” is unlikely to
appear often even within a very large corpus, and
could benefit from taking its lexical sub-units (e.g.,
“likely” or “give a green light”) into account.
We present a novel approach to the task that
models the selection and relative weighting of the
predicate’s LCs using latent variables. This ap-
proach allows the classifier that uses the distri-
butional representations to take into account the
most relevant LCs in order to make the predic-
tion. By doing so, we avoid the notoriously dif-
ficult problem of defining and identifying MWPs
and account for predicates of various sizes and de-
grees of compositionality. To our knowledge, this
is the first work to address lexical relations be-
tween MWPs of varying degrees of composition-
ality within distributional semantics.
We conduct experiments on the dataset of Ze-
ichner et al. (2012) and compare our methods with
analogous ones that select a fixed LC, using state-
of-the-art feature sets. Our method obtains sub-
stantial performance gains across all scenarios.
Finally, we note that our approach is cognitively
appealing. Significant cognitive findings support
the claim that a speaker’s lexicon consists of par-
tially overlapping lexical units of various sizes, of
which several can be evoked in the interpretation
of an utterance (Jackendoff, 2002; Wray, 2008).
</bodyText>
<sectionHeader confidence="0.818897" genericHeader="introduction">
2 Background and Related Work
</sectionHeader>
<bodyText confidence="0.999662169811321">
Inference Relations. The detection of inference
relations between predicates has become a central
task over the past few years (Sekine, 2005; Zan-
zotto et al., 2006; Schoenmackers et al., 2010;
Berant et al., 2011; Melamud et al., 2013a, in-
ter alia). Inference rules are used in a wide va-
riety of applications including Question Answer-
ing (Ravichandran and Hovy, 2002), Information
Extraction (Shinyama and Sekine, 2006), and as
a main component in Textual Entailment systems
(Dinu and Wang, 2009; Dagan et al., 2013).
Most approaches to the task used distributional
similarity as a major component within their sys-
tem. Lin and Pantel (2001) introduced DIRT, an
unsupervised distributional system for detecting
inference relations. The system is still considered
a state-of-the-art baseline (Melamud et al., 2013a),
and is often used as a component within larger sys-
tems. Schoenmackers et al. (2010) presented an
unsupervised system for learning inference rules
directly from open-domain web data. Melamud
et al. (2013a) used topic models to combine type-
level predicate inference rules with token-level in-
formation from their arguments in a specific con-
text. Melamud et al. (2013b) used lexical expan-
sion to improve the representation of infrequent
predicates. Lewis and Steedman (2013) combined
distributional and symbolic representations, eval-
uating on a Question Answering task, as well as
on a quantification-focused entailment dataset.
Several studies tackled the task using super-
vised systems. Weisman et al. (2012) used a set
of linguistically motivated features, but evaluated
their system on a corpus that consists almost en-
tirely of single-word predicates. Mirkin et al.
(2006) presented a system for learning inference
rules between nouns, using distributional similar-
ity and pattern-based features. Hagiwara et al.
(2009) identified synonyms using a supervised ap-
proach relying on distributional and syntactic fea-
tures. Berant et al. (2011) used distributional simi-
larity between predicates to weight the edges of an
entailment graph. By imposing global constraints
on the structure of the graph, they obtained a more
accurate set of inference rules.
Previous work used simple methods to select
the predicate’s LC. Some filtered out frequent
highly ambiguous verbs (Lewis and Steedman,
2013), others selected a single representative word
(Melamud et al., 2013a), while yet others used
multi-word LCs but treated them as fixed expres-
sions (Lin and Pantel, 2001; Berant et al., 2011).
The goals of the above studies are largely com-
</bodyText>
<page confidence="0.998592">
645
</page>
<bodyText confidence="0.993138708333333">
plementary to ours. While previous work focused
either on improving the quality of the distribu-
tional representations themselves or on their incor-
poration into more elaborate systems, we focus on
the integration of the distributional representation
of multiple LCs to improve the identification of
inference relations between MWPs.
MWP Extraction and Identification. MWPs
have received considerable attention over the years
in both theoretical and applicative contexts. Their
position on the crossroads of syntax and the lexi-
con, their varying degrees of compositionality, as
well as the wealth of linguistic phenomena they
exhibit, made them the object of ongoing linguis-
tic discussion (Alsina et al., 1997; Butt, 2010).
In NLP, the discovery and identification of
MWEs in general and MWPs in particular has
been the focus of much work over the years
(Lin, 1999; Baldwin et al., 2003; Biemann and
Giesbrecht, 2011). Despite wide interest, the
field has yet to converge to a general and widely
agreed-upon method for identifying MWPs. See
(Ramisch et al., 2013) for an overview.
Most work on MWEs emphasized idiosyncratic
or non-compositional expressions. Other lines of
work focused on specific MWP classes such as
light verbs (Tu and Roth, 2011; Vincze et al.,
2013) and phrasal verbs (McCarthy et al., 2003;
Pichotta and DeNero, 2013). Our work proposes a
uniform treatment to MWPs of varying degrees of
compositionality, and avoids defining MWPs ex-
plicitly by modelling their LCs as latent variables.
Compositional Distributional Semantics.
Much work in recent years has concentrated on
the relation between the distributional representa-
tions of composite phrases and the representations
of their component sub-parts (Widdows, 2008;
Mitchell and Lapata, 2010; Baroni and Zampar-
elli, 2010; Coecke et al., 2010). Several works
have used compositional distributional semantics
(CDS) representations to assess the composition-
ality of MWEs, such as noun compounds (Reddy
et al., 2011) or verb-noun combinations (Kiela
and Clark, 2013). Despite significant advances,
previous work has mostly been concerned with
highly compositional cases and does not address
the distributional representation of predicates of
varying degrees of compositionality.
</bodyText>
<sectionHeader confidence="0.989621" genericHeader="method">
3 Our Proposal: A Latent LC Approach
</sectionHeader>
<bodyText confidence="0.9999356">
This section details our approach for distribu-
tionally representing MWPs by leveraging their
component LCs. Section 3.1 describes our gen-
eral approach, Section 3.2 presents our model and
Section 3.3 details the feature set.
</bodyText>
<subsectionHeader confidence="0.99989">
3.1 General Approach and Notation
</subsectionHeader>
<bodyText confidence="0.999672977272727">
We propose a method for addressing MWPs of
varying degrees of compositionality through the
integration of the distributional representation of
multiple sub-sets of the predicate’s words (LCs).
We use it to tackle a supervised prediction task that
represents predicates distributionally. Our model
assumes a latent distribution over the LCs, and es-
timates its parameters so to best conform to the
goals of the target prediction task.
Formally, given a predicate p, we denote the set
of words comprising it as W(p). The set of al-
lowable LCs for p is denoted with Hp C 2W (p).
Hp contains all sub-sets of p that we consider as
apriori possible to represent p. For instance, if p is
“likely to give a green light”, Hp may include LCs
such as “likely” or “give light”. As our method is
aimed at discovering the most relevant LCs, we do
not attempt to analyze the MWPs in advance, but
rather take an inclusive Hp, allowing the model to
estimate the relative weights of the LCs.
The task we use as a testbed for our approach
is the lexical inference identification task between
predicates. Given a pair of predicates p =
(pL, pR), the task is to predict whether an infer-
ence relation holds between them. For instance, if
pL is “devour” and pR is “eat greedily”, the clas-
sifier should use the similarity between “devour”
and “eat” in order to correctly predict an infer-
ence relation in this case. Selecting the wider LC
“eat greedily” might result in sparser statistics. In
other examples, however, taking a wider LC is po-
tentially beneficial. For instance, the dissimilar-
ity between “take” and “make” should not prevent
the classifier from identifying the inference rela-
tion between “take a step” and “make a step”.
Our statistical model aims at predicting the cor-
rect label by making use of partially overlapping
LCs of various sizes, both for the premise left-
hand side (LHS) predicate pL and the hypothesis
right-hand side (RHS) predicate pR. More for-
mally, we take the space of values for our latent
LC variables to be HpL,pR = HpL x HpR.
Our evaluation dataset consists of pairs p(i) =
(p(i)
</bodyText>
<equation confidence="0.567954">
L , p(i)
</equation>
<bodyText confidence="0.9187216">
R ) for i E 11, ... , M}, where M is the
number of examples available, coupled with their
gold-standard labels y(i) E 11, −11. For brevity,
we denote H(i) = Hp(i) = H (i) (i) . We also as-
pL ,pR
</bodyText>
<page confidence="0.992653">
646
</page>
<bodyText confidence="0.99945225">
sume the existence of a feature function b(p, y, h)
which maps a triplet of a predicate pair p, an infer-
ence label y, and a latent state h E Hp to Rd for
some integer d. We denote the training set by D.
</bodyText>
<subsectionHeader confidence="0.995664">
3.2 The Model
</subsectionHeader>
<bodyText confidence="0.9999945">
We address the task with a latent variable log-
linear model, representing the LCs of the predi-
cates. We choose this model for its generality, con-
ceptual simplicity, and because it allows to easily
incorporate various feature sets and sets of latent
variables. We introduce L2 regularization to avoid
over-fitting. We use maximum likelihood estima-
tion, and arrive at the following objective function:
</bodyText>
<equation confidence="0.9971468">
L(wJD) = 1 log P(y(i)lp(i), w) − A2 l1wl12 =
ME
i=1
Z(w, i) = E E exp(wTΦ(pi, y, h)).
yET−1,11 hEHi
</equation>
<bodyText confidence="0.998404666666667">
We maximize L using the BFGS algorithm (No-
cedal and Wright, 1999). The gradient (with re-
spect to w) is the following:
</bodyText>
<equation confidence="0.665742">
OL = Eh[Φ(pi, yi, h)] − Eh,y[Φ(pi, y, h)] − A · w
</equation>
<bodyText confidence="0.976391130434783">
Hp can be defined to be any sub-set of 2W(p)
given that taking an expectation over H can be
done efficiently. It is therefore possible to use prior
linguistic knowledge to consider only sub-sets of p
that are likely to be non-compositional (e.g., verb-
preposition or verb-noun pairs).
In our experiments we attempt to keep the ap-
proach maximally general, and define Hp to be the
set of all subsets of size 1 or 2 of content words in
Wp1. We bound the size of h E Hp in order to re-
tain computational efficiency and a sufficient fre-
quency of the LCs in Hp. MWPs of length greater
than 2 are effectively approximated by their set of
subsets of sizes 1 and 2.
Each h can therefore be written as a 4-tuple
(hAL, hBL ,hAR, hBR), where hAL (hA R) denotes the first
word of the LHS (RHS) predicate’s LC. hB L (hBR)
denotes the (possibly empty) second word of the
predicate. Inference is carried out by maximizing
P(y|p(i)) over y. As |Hp |= O(k4), where k is the
1We use a POS tagger to identify content words. Preposi-
tions are considered content words under this definition.
number of content words in p, and as the number
of content words is usually small2, inference can
be carried out by directly summing over H(i).
Initialization. The introduction of latent vari-
ables into the log-linear model leads to a non-
convex objective function. Consequently, BFGS
is not guaranteed to converge to the global opti-
mum, but rather to a stationary point. The result
may therefore depend on the parameter initializa-
tion. Indeed, preliminary experiments showed that
both initializing w to be zero and using a random
initializer results in lower performance.
Instead, we initialize our model with a simpli-
fied convex model that fixes the LCs to be the
pair of left-most content words comprising each
of the predicates. This is a common method for
selecting the predicate’s LC (e.g., Melamud et al.,
2013a). Once h has been fixed, the model col-
lapses to a convex log-linear model. The optimal
w is then taken as an initialization point for the la-
tent variable model. While this method may still
not converge to the global maximum, our experi-
ments show that this initialization technique yields
high quality values for w (see Section 6).
</bodyText>
<subsectionHeader confidence="0.999477">
3.3 Feature Set
</subsectionHeader>
<bodyText confidence="0.999990958333333">
This section lists the features used for our exper-
iments. We intentionally select a feature set that
relies on either completely unsupervised or shal-
low processing tools that are available for a wide
variety of languages and domains.
Given a predicate pair p(i), a label y E 11, −1}
and a latent state h E H(i), we define their feature
vector as b(p(i), y, h) = y · b(p(i), h). The com-
putation of b(p(i), h) requires a reference corpus
R that contains triplets of the type (p, x, y) where
p is a binary predicate and x and y are its argu-
ments. We use the Reverb corpus as R in our ex-
periments (Fader et al., 2011; see Section 4). We
refrain from encoding features that directly reflect
the vocabulary of the training set. Such features
are not applicable beyond that set’s vocabulary,
and as available datasets contain no more than a
few thousand examples, these features are unlikely
to generalize well.
Table 1 presents the set of features we use in our
experiments. The features can be divided into two
main categories: similarity features between the
LHS and the RHS predicates (table’s top), and fea-
tures that reflect the individual properties of each
</bodyText>
<footnote confidence="0.8358215">
2AHpI is about 15 on average in our dataset, where less
than 5% of the H(i) are of size greater than 50.
</footnote>
<equation confidence="0.9968025">
1
n
=
En
i=1
where:
0 E ( )
�log exp w�Φ(p(i), y(i), h)
hEH(i)
− log Z(w, i)/ − 2 l1Iwl12
</equation>
<page confidence="0.99592">
647
</page>
<table confidence="0.999902260869565">
Category Name Description
Similarity COSINE DIRT cosine similarity between the vectors of hL and hR
COSINEA DIRT cosine similarity between the vectors of hAL and hAR
BInc DIRT BInc similarity between the vectors of hL and hR
BIncA DIRT BInc similarity between the vectors of hAL and hAR
Word A LHS POSA The most frequent POS tag for the lemma of hA
L L
POS2A The second most frequent POS tag for the word lemma of hA
L L
FREQA The number of occurrences of hA L in the reference corpus
L A binary feature indicating whether hAL appears in both predicates
COMMONAL The ordinal number of hA L among the content words of the LHS predicate
ORDINALA
L
Pair LHS POSAB The conjunction of POSA L and POSB
L L
FREQAB The frequency of hA L and hB L in the reference corpus
L P(hAL|hAL) as estimated from the reference corpus
PREFABL P(hBL |hAL) as estimated from the reference corpus
PREFBAL The point-wise mutual information of hAL and hBL
PMIABL
LDA TOPICSL P(topic|hL) for each of the induced topics.
TOPICENTL The entropy of the topic distribution P(topic|hL)
</table>
<tableCaption confidence="0.7659315">
Table 1: The feature set used in our experiments. The top part presents the similarity measures based on the DIRT approach.
The rest of the listed features apply to the LHS predicate (hL), and to the first word in it (hAL). Analogous features are
introduced for the second word, hL, and for the RHS predicate. The upper-middle part presents the word features for hAL. The
lower-middle part presents features that apply where hL is of size 2. The bottom part lists the LDA-based features.
</tableCaption>
<bodyText confidence="0.99671555">
of them. Within the LHS feature set, we distin-
guish between two sub-types of features: word
features that encode the individual properties of
hAL and hBL (table’s upper middle part), and pair
features that only apply to LCs of size 2 and re-
flect the relation between hAL and hBL (table’s lower
middle part). We further incorporate LDA-based
features that reflect the selectional preferences of
the predicates (table’s bottom).
Distributional Similarity Features. The distri-
butional similarity features are based on the DIRT
system (Lin and Pantel, 2001). The score defines
for each predicate p and for each argument slot
s E {L, R} (corresponding to the arguments to the
right and left of that predicate) a vector vps which
represents the distribution of arguments appearing
in that slot. We take vps(x) to be the number of
times that the argument x appeared in the slot s of
the predicate p. Given these vectors, the similarity
between the predicates p1 and p2 is defined as:
</bodyText>
<equation confidence="0.887844">
Vscore(p1, p2) = sim(vp1L , vp2L ) · sim(vp1R , vp2R )
</equation>
<bodyText confidence="0.993141813953488">
where situ is some vector similarity measure.
We use two common similarity measures: the
vector cosine metric, and the BInc (Szpektor and
Dagan, 2008) similarity measure. These measures
give complementary perspectives on the similar-
ity between the predicates, as the cosine similar-
ity is symmetric between the LHS and RHS predi-
cates, while BInc takes into account the direction-
ality of the inference relation. Preliminary exper-
iments with other measures, such as those of Lin
(1998) and Weeds and Weir (2003) did not yield
additional improvements.
We encode the similarity of all measures for the
pair hL and hR as well as the pair hAL and hAR. The
latter feature is an approximation to the similar-
ity between the heads of the predicates, as heads
in English tend to be to the left of the predicates.
These two features coincide for h values of size 1.
Word and Pair Features. These features en-
code the basic properties of the LC. The motiva-
tion behind them is to allow a more accurate lever-
aging of the similarity features, as well as to better
determine the relative weights of h E H(i).
The feature set is composed of four analogous
sets corresponding to hAL,hBL,hAR and hBR, as well
as two sets of features that capture relations be-
tween hAL, hBLand hAR, hBR (in cases h is of size 2).
The features include the ordinal index of the word
within the predicate, the lemma’s frequency ac-
cording to R, and a feature that indicates whether
that word’s lemma also appears in both predicates
of the pair. For instance, when considering the
predicates “likely to come” and “likely to leave”,
“likely” appears in both predicates, while “come”
and “leave” appear only in one of them.
In addition, we use POS-based features that
encode the most frequent POS tag for the word
lemma and the second most frequent POS tag (ac-
cording to R). Information about the second most
frequent POS tag can be important in identifying
light verb constructions, such as “take a swim” or
“give a smile”, where the object is derived from a
verb. It can thus be interpreted as a generalization
</bodyText>
<page confidence="0.994244">
648
</page>
<bodyText confidence="0.999589711864407">
of the feature that indicates whether the object is
a deverbal noun, which is used by some light verb
identification algorithms (Tu and Roth, 2011).
In cases where hL is of size 2, we additionally
encode features that apply to the conjunction of
hAL and hBL. We encode the conjunction of their
POS and the number of times the two lemmas oc-
curred together in R. We also introduce features
that capture the statistical correlation between the
words of hL. To do so, we use point-wise mu-
tual information, and the conditional probabili-
ties P(hAL|hBL) and P(hBL|hAL). Similar measures
have often been used for the unsupervised detec-
tion of MWEs (Villavicencio et al., 2007; Fazly
and Stevenson, 2006). We also include the analo-
gous set of features for hR.
LDA-based Features. We further incorporate
features based on a Latent Dirichlet Allocation
(LDA) topic model (Blei et al., 2003). Several
recent works have underscored the usefulness of
using topic models to model a predicate’s selec-
tional preferences (Ritter et al., 2010; Dinu and
Lapata, 2010; S´eaghdha, 2010; Lewis and Steed-
man, 2013; Melamud et al., 2013a). We adopt the
approach of Lewis and Steedman (2013), and de-
fine a pseudo-document for each LC in the evalu-
ation corpus. We populate the pseudo-documents
of an LC with its arguments according to R. We
then train an LDA model with 25 topics over these
documents. This yields a probability distribution
P(topic|h) for each LC h, reflecting the types of
arguments h may take.
We further include a feature for the entropy of
the topic distribution of the predicate, which re-
flects its heterogeneity. This feature is motivated
by the assumption that a heterogeneous predicate
is more likely to benefit from selecting a more in-
clusive LC than a homogeneous one.
Technical Issues. All features used, except the
similarity ones and the topic distribution features
are binary. Frequency features are binned into 4
bins of equal frequency. We conjoin some of the
feature sets by multiplying their values. Specifi-
cally, we add the cross product of the features of
the category “Similarity” (see Table 1) with the
rest of the features. In addition, we conjoin all
LHS (RHS) features with an indicator feature that
indicates whether hL (hR) is of size two. This re-
sults in 1605 non-constant features.
We further note that some LCs that appear in the
evaluation corpus do not appear at all in R. In our
experiments they amounted to 0.2% of the LCs in
our evaluation dataset. While previous work of-
ten discarded predicates below a certain frequency
from the evaluation, we include them in order to
facilitate comparison to future work. We assign
the similarity features of such examples a 0 value,
and assign their other numerical features the mean
value of those features.
</bodyText>
<sectionHeader confidence="0.998601" genericHeader="method">
4 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999964829268293">
Corpora and Preprocessing. As a reference
corpus R, we use Reverb (Fader et al., 2011), a
web-based corpus consisting of 15M web extrac-
tions of binary relations. Each relation is a triplet
of a predicate and two arguments, one preceding it
and one following it. Relations were extracted us-
ing regular expressions over the output of a POS
tagger and an NP chunker. Each predicate may
consist of a single verb, a verb and a preposi-
tion or a sequence of words starting in a verb and
ending in a preposition, between which there may
nouns, adjectives, adverbs, pronouns, determiners
and verbs. The verb may also be a copula. Exam-
ples of predicates are “make the most of”, “could
be exchanged for” and “is happy with”.
Reverb is an appealing reference corpus for this
task for several reasons. First, it uses fairly shal-
low preprocessing technology which is available
for many domains and languages. Second, Reverb
applies considerable noise filtering, which results
in extractions of fair quality. Third, our evaluation
dataset is based on Reverb extractions.
We evaluate our algorithm on the dataset of
Zeichner et al. (2012). This publicly available
corpus3 provides pairs of Reverb binary relations
and an indication of whether an inference rela-
tion holds between them within the context of
a specific pair of argument fillers. The corpus
was compiled using distributional methods to de-
tect pairs of relations in Reverb that are likely
to have an inference relation between. Annota-
tors, employed through Amazon Mechanical Turk,
were then asked to determine whether each pair
is meaningful, and if so, to determine whether an
inference relation holds. Further measures were
taken to monitor the accuracy of the annotation.
For example, the pair of predicates “make the
most of” and “take advantage of” appears in the
corpus as a pair between which an inference rela-
tion holds. The arguments in this case are “stu-
dents” and “their university experience”. An ex-
</bodyText>
<footnote confidence="0.948236">
3http://tinyurl.com/krx2acd
</footnote>
<page confidence="0.99887">
649
</page>
<bodyText confidence="0.9995799375">
ample of a pair between which an inference rela-
tion does not hold is “tend to neglect” and “under-
estimate the importance of”, where the arguments
are “Robert” and “his family”.
The dataset contains 6,565 instances in total.
We use 5,411 pairs of them, discarding instances
that were deemed as meaningless by the annota-
tors. We also discard cases where the set of ar-
guments is reversed between the LHS and RHS
predicates. In these examples, pR(x, y) is infer-
able from pL(y, x), rather than from pL(x, y). As
there are less than 150 reversed instances in the
corpus, experimenting on this sub-set is unlikely
to be informative.
The average length of a predicate in the cor-
pus is 2.7 words (including function words). In
87.3% of the predicate pairs, there was more than
one LC (i.e., |Hp |&gt; 1), underscoring the im-
portance of correctly leveraging the different LCs.
We randomly partition the corpus into a training
set which contains 4,343 instances (∼80%), and a
test set that contains 1,068 instances, maintaining
the same positive to negative label ratio in both
datasets4. Development was carried out using
cross-validation on the training data (see below).
We use a Maximum Entropy POS Tagger,
trained on the Penn Treebank, and the WordNet
lemmatizer, both implemented within the NLTK
package (Loper and Bird, 2002). To obtain a
coarse-grained set of POS tags, we collapse the
tag set to 7 categories: nouns, verbs, adjectives,
adverbs, prepositions, the word “to” and a cate-
gory that includes all other words. A Reverb argu-
ment is represented as the conjunction of its con-
tent words that appear more than 10 times in the
corpus. Function words are defined according to
their POS tags and include determiners, possessive
pronouns, existential “there”, numbers and coordi-
nating conjunctions. Auxiliary verbs and copulas
are also considered function words.
To compute the LDA features, we use the on-
line variational Bayes algorithm of (Hoffman et
al., 2010) as implemented in the Gensim software
package (Rehurek and Sojka, 2010).
Evaluated Algorithms. The only two previous
works on this dataset (Melamud et al., 2013a;
Melamud et al., 2013b) are not directly compara-
ble, as they used unsupervised systems and evalu-
</bodyText>
<footnote confidence="0.946122">
4A script that replicates our train-test partition of the cor-
pus can be found here: http://homepages.inf.ed.
ac.uk/oabend/mwpreds.html
</footnote>
<bodyText confidence="0.999815235294118">
ated on sub-sets of the evaluation dataset. Instead,
we use several baselines to demonstrate the use-
fulness of integrating multiple LCs, as well as the
relative usefulness of our feature sets.
The simplest baseline is ALLNEG, which pre-
dicts the most frequent label in the dataset (in our
case: “no inference”). The other evaluated sys-
tems are formed by taking various subsets of our
feature set. We experiment with 4 feature sets. The
smallest set, SIM, includes only the similarity fea-
tures. This feature set is related to the composi-
tional distributional model of Mitchell and Lap-
ata (2010) (see Section 6). We note that despite
recent advances in identifying predicate inference
relations, the DIRT system (Lin and Pantel, 2001)
remains a strong baseline, and is often used as a
component in state-of-the-art systems (Berant et
al., 2011), and specifically in the two aforemen-
tioned works that used the same evaluation corpus.
The next feature set BASIC includes the features
found to be most useful during the development
of the model: the most frequent POS tag, the fre-
quency features and the feature Common. More
inclusive is the feature set NO-LDA, which in-
cludes all features except the LDA features. Ex-
periments with this set were performed in order
to isolate the effect of the LDA features. Finally,
ALL includes our complete set of features.
The more direct comparison is against partial
implementations of our system where the LC h is
deterministically selected. Determining h for each
predicate yields a regular log-linear binary classi-
fication model. We use two variants of this base-
line. The first, LEFTMOST, selects the left-most
content word for each predicate. Similar selec-
tion strategy was carried out by Melamud et al.
(2013a). The second, VPREP, selects h to be the
verb along with its following preposition. In cases
the predicate contains multiple verbs, the one pre-
ceding the preposition is selected, and where the
predicate does not contain any non-copula verbs,
it regresses to LEFTMOST. This LC selection
method approximates a baseline that includes sub-
categorized prepositions. Such cases are highly
frequent and account for a large portion of the
MWPs in English. Including a verb’s preposition
in its LC was commonly done in previous work
(e.g., Lewis and Steedman, 2013).
We also attempted to identify verb-preposition
constructions using a dependency parser. Unfor-
tunately, our evaluation dataset is only available in
</bodyText>
<page confidence="0.995081">
650
</page>
<bodyText confidence="0.999696166666667">
a lemmatized version, which posed a difficulty for
the parser. Due to the low quality of the resulting
parses, we implemented VPREP using POS-based
regular expressions as defined above.
The full model is denoted with LATENTLC. For
each system and feature set, we report results us-
ing 10-fold cross-validation on the training set, as
well as results on the test set. Both cases use
the same set of parameters determined by cross-
validation on the training set. As the task at hand
is a binary classification problem, we use accuracy
scores to rate the performance of our systems.
</bodyText>
<sectionHeader confidence="0.999804" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999971033898305">
Table 2 presents the results of our experi-
ments. Rows correspond to the evaluated algo-
rithms, while columns correspond to the feature
sets used and the evaluation scenarios (i.e., train-
ing set cross-validation or test set evaluation). Our
experiments make first use of this dataset in its
fullest form for the problem of supervised learning
of inference relations, and may serve as a starting
point for further exploration of this dataset.
For all feature sets and settings, LATENTLC
scored highest, often with a considerable margin
of up to 3.0% in the cross-validation and up to
4.6% on the test set relative to the LEFTMOST
baseline, and 5.1% (cross-validation) and 6.8%
(test) margins relative to VPREP.
The best scoring result of our LATENTLC
model in the cross-validation scenario is 65.72%,
obtained by the feature set All. The best scoring
result by any of the baseline models in this sce-
nario is 62.7%, obtained by the same feature set.
For the test set scenario, LATENTLC obtained its
highest accuracy, 65.73%, when using the feature
set Basic. This is a substantial improvement over
the highest scoring baseline model in this scenario
that obtained 61.6% accuracy, using the feature set
All. This performance gap is substantial when tak-
ing into consideration that the improvements ob-
tained by the highly competitive DIRT similarity
features using the stronger LEFTMOST baseline,
result in an improvement of 3.1% and 5.3% over
the trivial ALLNEG baseline in the test set and
cross-validation scenarios respectively.
Comparing the different feature sets on our pro-
posed model, we find that the Basic feature set
gives a consistent and substantial increase over the
Sim feature set. Improvements are of 2.8% (test)
and 2.2% (cross-validation). Introducing more
elaborate features (i.e., the feature sets NoLDA
and All) yields some improvements in the cross-
validation, but these improvements are not repli-
cated on the test set. This may be due to idiosyn-
crasies in the test set that are averaged out in the
cross-validation scenario.
For a qualitative analysis, we took the best per-
forming model of the data set (i.e., with the Basic
feature set), and extracted the set of instances
where it made a correct prediction while both
baselines made an error. This set contains many
verb-preposition pairs, such as “list as → report
as” or “submit via → deliver by”, underscoring the
utility of leveraging multiple LCs rather than con-
sidering only a head word (as with LEFTMOST)
or the entire phrase (as with VPREP). Other ex-
amples in this set contain more complex patterns.
These include the positive pairs “talk much about
→ have much to say about” and “increase with
→ go up with”, and the negative “make predic-
tion about → meet the challenge of” and “enjoy
watching → love to play”.
</bodyText>
<sectionHeader confidence="0.998527" genericHeader="evaluation">
6 Discussion
</sectionHeader>
<bodyText confidence="0.9553975">
Relation to CDS. Much recent work subsumed
under the title Compositional Distributional Se-
mantics addressed the distributional representa-
tion of multi-word phrases (see Section 2). This
line of work focuses on compositional predicates,
such as “kick the ball” and not on idiosyncratic
predicates such as “kick the bucket”.
A variant of the CDS approach can be framed
within ours. Assume we wish to compute the
similarity of the predicates pL = (w1, ..., wn)
and pR = (w01, ..., w0m). Let us denote the vec-
tor space representations of the individual words
as v1, ..., vn and v01, ..., v0m respectively. A stan-
dard approach in CDS is to compose distributional
representations by taking their vector sum vL =
v1 + v2... + vn and vR = v01 + ... + v0 m (Mitchell
and Lapata, 2010). One of the most effective sim-
ilarity measures is the cosine similarity, which is a
normalized dot product. The distributional sim-
ilarity between pL and pR under this model is
sim(pL, pR) = �n �m j=1 sim(wi,w0 j), where
i=1
sim(wi, w0j) is the dot product between vi and v0j.
This similarity score is similar in spirit to a
simplified version of our statistical model that
restricts the set of allowable LCs Hp to be
{({wi}, {w0j})|i ≤ n, j ≤ m}, i.e., only LCs of
size 1. Indeed, taking Hp as above, and cosine
similarity as the only feature (i.e., w E ][R), yields
the distribution
</bodyText>
<page confidence="0.996357">
651
</page>
<table confidence="0.998296333333333">
Algorithm Test Set Cross Validation
Sim Basic NoLDA All Sim Basic NoLDA All
LATENTLC 62.9 65.7 64.4 64.6 62.7 ± 1.9 64.9 ± 1.9 65.0 ± 1.7 65.7 ±1.9
LEFTMOST 59.0 61.1 60.0 60.4 61.2 ± 2.1 62.5 ± 2.4 62.4 ±2.2 62.7 ± 2.0
VPREP 56.1 60.9 60.7 61.6* 58.1 ± 1.7 60.8 ± 2.2 60.4 ± 2.6 60.6 ± 2.2
ALLNEG 55.9 55.9
</table>
<tableCaption confidence="0.970161">
Table 2: Results for the various evaluated systems. Accuracy results are presented in percents, followed in the cross vali-
</tableCaption>
<bodyText confidence="0.988951428571428">
dation scenario by the standard deviation over the folds. The rows correspond to the various systems as defined in Section 4.
LATENTLC is our proposed model. The columns correspond to the various feature sets, from the least to the most inclusive.
SIM includes only similarity features. BASIC additionally includes POS-based and frequency features. NOLDA includes all
features except LDA-based features. ALL is the full feature set. ALLNEG is the classifier that invariably predicts the label “no
inference”. Bold marks best overall accuracy per column, and * marks figures that are not significantly worse (McNemar’s test,
p &lt; 0.05). The same positive to negative label ratio was maintained in both the cross validation and test set scenarios. In all
cases, LATENTLC obtains substantial improvements over the baseline systems.
</bodyText>
<equation confidence="0.9929715">
P(y|p) oc X exp `w · y · sim(wi, wD´ .
(wi,w�
</equation>
<bodyText confidence="0.994227260869565">
This derivation highlights the relation of a sim-
plified version of our approach to the additive
CDS model, as both approaches effectively aver-
age over the similarities of all pairs of words in pL
and pR. The derivation also highlights a few ad-
vantages of our approach. First, our approach al-
lows to straightforwardly introduce additional fea-
tures and to weight them in a way most consistent
with the task at hand. Second, it allows much more
flexibility in defining the set of allowable LCs, Hp.
Specifically, Hp may contain LCs of sizes greater
than 1. Third, our approach uses standard proba-
bilistic modelling, and therefore has a natural sta-
tistical interpretation.
In order to appreciate the effect of these advan-
tages, we perform an experiment that takes H to
be the set of all LCs of size 1, and uses a sin-
gle similarity measure. We run a 10-fold cross-
validation on our training data, obtaining 61.3%
accuracy using COSINE and 62.2% accuracy us-
ing BInc. The performance gap between these re-
sults and the accuracy obtained by our full model
(65.7%) underscores the latter’s effectiveness in
integrating multiple features and LCs.
Effectiveness of Optimization Method. Our
maximization of the log-likelihood function is
not guaranteed to converge to a global optimum.
Therefore, the quality of the learned parameters
may be sensitive to the initialization point. We
hereby describe an experiment that tests the sen-
sitivity of our approach to such variance.
Selecting the highest scoring feature set on our
test set (i.e., BASIC), we ran the model with mul-
tiple initializers, by randomly perturbing our stan-
dard convex initializer (see Section 3). Concretely,
given a convex initializer w, we select the starting
point to be w + q, where qi ∼ N(0,α|wi|). We
ran this experiment 400 times with α = 0.8.
To combine the resulting weight vectors into a
single classifier, we apply two types of standard
approaches: a Product of Experts (Hinton, 2002),
as well as a voting approach that selects the most
frequently predicted label. Neither of these exper-
iments yielded any significant performance gain.
This demonstrates the robustness of our optimiza-
tion method to the initialization point.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.976670444444444">
We have presented a novel approach to the
distributional representation of multi-word pred-
icates. Since MWPs demonstrate varying levels
of compositionality, a uniform treatment of MWPs
either as fixed expressions or through head words
is lacking. Instead, our approach integrates mul-
tiple lexical units contained in the predicate. The
approach takes into account both multi-word LCs
that address low compositionality cases, as well as
single-word LCs that address compositional cases
and are more frequent. It assumes a latent distribu-
tion over the LCs of the predicates, and estimates
it relative to a target application task.
We addressed the supervised inference identi-
fication task, obtaining substantial improvement
over state-of-the-art baseline systems. In future
work we intend to assess the benefit of this ap-
proach in MWP classes that are well-known from
the literature. We believe that a permissive ap-
proach that integrates multiple analyses would
perform better than standard single-analysis meth-
ods in a wide range of applications.
Acknowledgements. We would like to thank
Mike Lewis, Reshef Meir, Oren Melamud,
Michael Roth and Nathan Schneider for their help-
ful comments. This work was supported by ERC
Advanced Fellowship 249520 GRAMPLUS.
</bodyText>
<page confidence="0.997099">
652
</page>
<sectionHeader confidence="0.990065" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999089456310679">
Alex Alsina, Joan Wanda Bresnan, and Peter Sells.
1997. Complex predicates. Center for the Study of
Language and Information.
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model
of multiword expression decomposability. In Pro-
ceedings of the ACL 2003 workshop on Multiword
expressions: analysis, acquisition and treatment-
Volume 18, pages 89–96.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
EMNLP, pages 1183–1193.
Jonathan Berant, Jacob Goldberger, and Ido Dagan.
2011. Global learning of typed entailment rules. In
ACL, pages 610–619.
Chris Biemann and Eugenie Giesbrecht. 2011. Dis-
tributional semantics and compositionality 2011:
Shared task description and results. In Workshop
on Distributional Semantics and Compositionality,
pages 21–28.
David M Blei, Andrew Y Ng, and Michael I Jordan.
2003. Latent Dirichlet allocation. the Journal of
machine Learning research, 3:993–1022.
Miriam Butt. 2010. The light verb jungle: still hack-
ing away. In Complex predicates: cross-linguistic
perspectives on event structure, pages 48–78. Cam-
bridge University Press.
Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen
Clark. 2010. Mathematical foundations for a com-
positional distributional model of meaning. In J. van
Bentham, M. Moortgat, and W. Buszkowski, editors,
Linguistic Analysis, volume 36, pages 435–384.
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas-
simo Zanzotto. 2013. Recognizing textual entail-
ment: Models and applications. Synthesis Lectures
on Human Language Technologies, 6(4):1–220.
Georgiana Dinu and Mirella Lapata. 2010. Topic mod-
els for meaning similarity in context. In COLING:
Posters, pages 250–258.
Georgiana Dinu and Rui Wang. 2009. Inference rules
and their application to recognizing textual entail-
ment. In EACL, pages 211–219.
Anthony Fader, Stephen Soderland, and Oren Etzioni.
2011. Identifying relations for open information ex-
traction. In EMNLP, pages 1535–1545.
Afsaneh Fazly and Suzanne Stevenson. 2006. Auto-
matically constructing a lexicon of verb phrase id-
iomatic combinations. In EACL, pages 337–344.
Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko
Toyama. 2009. Supervised synonym acquisition us-
ing distributional features and syntactic patterns. In-
formation and Media Technologies, 4(2):558–582.
Geoffrey E Hinton. 2002. Training products of experts
by minimizing contrastive divergence. Neural com-
putation, 14(8):1771–1800.
Matthew Hoffman, Francis R Bach, and David M Blei.
2010. Online learning for latent Dirichlet allocation.
In NIPS, pages 856–864.
Ray Jackendoff. 2002. Foundations of language:
Brain, meaning, grammar, evolution. Oxford Uni-
versity Press.
Douwe Kiela and Stephen Clark. 2013. Detect-
ing compositionality of multi-word expressions us-
ing nearest neighbours in vector space models. In
EMNLP, pages 1427–1432.
Mike Lewis and Mark Steedman. 2013. Combined
distributional and logical semantics. TACL, 1:179–
192.
Dekang Lin and Patrick Pantel. 2001. DIRT – discov-
ery of inference rules from text. In SIGKDD 2001,
pages 323–328.
Dekang Lin. 1998. Automatic retrieval and clustering
of similar words. In COLING-ACL, pages 768–774.
Dekang Lin. 1999. Automatic identification of non-
compositional phrases. In ACL, pages 317–324.
Edward Loper and Steven Bird. 2002. NLTK: The
natural language toolkit. In ACL Workshop on Ef-
fective tools and methodologies for teaching natural
language processing and computational linguistics,
pages 63–70.
Diana McCarthy, Bill Keller, and John Carroll.
2003. Detecting a continuum of compositionality
in phrasal verbs. In ACL workshop on Multiword
expressions: analysis, acquisition and treatment,
pages 73–80.
Oren Melamud, Jonathan Berant, Ido Dagan, Jacob
Goldberger, and Idan Szpektor. 2013a. A two level
model for context sensitive inference rules. In ACL
2013, pages 1331–1340.
Oren Melamud, Ido Dagan, Jacob Goldberger, and Idan
Szpektor. 2013b. Using lexical expansion to learn
inference rules from sparse data. In ACL: Short Pa-
pers, pages 283–288.
Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006.
Integrating pattern-based and distributional similar-
ity methods for lexical entailment acquisition. In
COLING-ACL: Poster Session, pages 579–586.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Jorge. Nocedal and Stephen J Wright. 1999. Numeri-
cal optimization, volume 2. Springer New York.
</reference>
<page confidence="0.990542">
653
</page>
<reference confidence="0.99988312345679">
Karl Pichotta and John DeNero. 2013. Identify-
ing phrasal verbs using many bilingual corpora. In
EMNLP, pages 636–646.
Carlos Ramisch, Aline Villavicencio, and Valia Kor-
doni. 2013. Introduction to the special issue on
multiword expressions: From theory to practice and
use. ACM Transactions on Speech and Language
Processing (TSLP), 10(2):3.
Deepak Ravichandran and Eduard Hovy. 2002. Learn-
ing surface text patterns for a question answering
system. In ACL, pages 41–47.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in
compound nouns. In IJCNLP, pages 210–218.
Radim Rehurek and Petr Sojka. 2010. Software frame-
work for topic modelling with large corpora. In Pro-
ceedings of LREC 2010 workshop New Challenges
for NLP Frameworks, pages 46–50.
Alan Ritter, Mausam, and Oren Etzioni. 2010. A la-
tent Dirichlet allocation method for selectional pref-
erences. In ACL, pages 424–434.
Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann
Copestake, and Dan Flickinger. 2001. Multiword
expressions: A pain in the neck for NLP. In CI-
CLing, pages 1–15.
Stefan Schoenmackers, Oren Etzioni, Daniel S Weld,
and Jesse Davis. 2010. Learning first-order Horn
clauses from web text. In EMNLP, pages 1088–
1098.
Diarmuid ´O. S´eaghdha. 2010. Latent variable models
of selectional preference. In ACL 2010, pages 435–
444.
Maggie Seaton and Alison Macaulay, editors. 2002.
Collins COBUILD Idioms Dictionary. Harper-
Collins Publishers, 2nd edition.
Satoshi Sekine. 2005. Automatic paraphrase discovery
based on context and keywords between NE pairs.
In IWP, pages 4–6.
Yusuke Shinyama and Satoshi Sekine. 2006. Preemp-
tive information extraction using unrestricted rela-
tion discovery. In HLT-NAACL, pages 304–311.
Idan Szpektor and Ido Dagan. 2008. Learning entail-
ment rules for unary templates. In COLING, pages
849–856.
Yuancheng Tu and Dan Roth. 2011. Learning English
light verb constructions: contextual or statistical. In
ACL HLT 2011, page 31.
Peter D Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of artificial intelligence research,
37(1):141–188.
Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco
Idiart, and Carlos Ramisch. 2007. Validation and
evaluation of automatically acquired multiword ex-
pressions for grammar engineering. In EMNLP-
CoNLL, pages 1034–1043.
Veronika Vincze, Istv´an Nagy T., and Rich´ard Farkas.
2013. Identifying English and Hungarian light verb
constructions: A contrastive approach. In ACL:
Short Papers, pages 255–261.
Julie Weeds and David Weir. 2003. A general frame-
work for distributional similarity. In EMNLP, pages
81–88.
Hila Weisman, Jonathan Berant, Idan Szpektor, and
Ido Dagan. 2012. Learning verb inference rules
from linguistically-motivated evidence. In EMNLP-
CoNLL, pages 194–204.
Dominic Widdows. 2008. Semantic vector products:
Some initial investigations. In Second AAAI Sym-
posium on Quantum Interaction, volume 26, pages
28–35.
Alison Wray. 2008. Formulaic language: Pushing the
boundaries. Oxford University Press.
Fabio Massimo Zanzotto, Marco Pennacchiotti, and
Maria Teresa Pazienza. 2006. Discovering asym-
metric entailment relations between verbs using se-
lectional preferences. In ACL-COLING, pages 849–
856.
Naomi Zeichner, Jonathan Berant, and Ido Dagan.
2012. Crowdsourcing inference-rule evaluation. In
ACL: Short Papers, pages 156–160.
</reference>
<page confidence="0.999027">
654
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.647736">
<title confidence="0.997661">Lexical Inference over Multi-Word Predicates: A Distributional Approach</title>
<author confidence="0.999889">Omri Abend Shay B Cohen Mark Steedman</author>
<affiliation confidence="0.973259">School of Informatics, University of</affiliation>
<note confidence="0.693109">Edinburgh EH8 9AB, United</note>
<abstract confidence="0.998998423076923">Representing predicates in terms of their argument distribution is common practice in NLP. Multi-word predicates (MWPs) in this context are often either disregarded or considered as fixed expressions. The latter treatment is unsatisfactory in two ways: (1) identifying MWPs is notoriously difficult, (2) MWPs show varying degrees of compositionality and could benefit from taking into account the identity of their component parts. We propose a novel approach that integrates the distributional representation of multiple sub-sets of the MWP’s words. We assume a latent distribution over sub-sets of the MWP, and estimate it relative to a downstream prediction task. Focusing on the supervised identification of lexical inference relations, we compare against state-of-the-art baselines that consider a single sub-set of an MWP, obtaining substantial improvements. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alex Alsina</author>
<author>Joan Wanda Bresnan</author>
<author>Peter Sells</author>
</authors>
<title>Complex predicates. Center for the Study of Language and Information.</title>
<date>1997</date>
<contexts>
<context position="1843" citStr="Alsina et al., 1997" startWordPosition="265" endWordPosition="268">ity within distributional semantics. 1 Introduction Multi-word expressions (MWEs) constitute a large part of the lexicon and account for much of its growth (Jackendoff, 2002; Seaton and Macaulay, 2002). However, despite their importance, MWEs remain difficult to define and model, and consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 1997). MWPs encompass a wide range of phenomena, including causatives, light verbs, phrasal verbs, serial verb constructions and many others, and pose considerable challenges to both linguistic theory and NLP applications (see Section 2). Part of the difficulty in treating them stems from their position on the borderline between syntax and the lexicon. It is therefore often unclear whether they should be treated as fixed expressions, as compositional phrases that reflect the properties of their component parts or as both. This work addresses the modelling of MWPs within the context of distributiona</context>
<context position="9637" citStr="Alsina et al., 1997" startWordPosition="1491" endWordPosition="1494">istributional representations themselves or on their incorporation into more elaborate systems, we focus on the integration of the distributional representation of multiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 20</context>
</contexts>
<marker>Alsina, Bresnan, Sells, 1997</marker>
<rawString>Alex Alsina, Joan Wanda Bresnan, and Peter Sells. 1997. Complex predicates. Center for the Study of Language and Information.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Colin Bannard</author>
<author>Takaaki Tanaka</author>
<author>Dominic Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>89--96</pages>
<contexts>
<context position="9814" citStr="Baldwin et al., 2003" startWordPosition="1523" endWordPosition="1526"> to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs </context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of multiword expression decomposability. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatmentVolume 18, pages 89–96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>1183--1193</pages>
<contexts>
<context position="10721" citStr="Baroni and Zamparelli, 2010" startWordPosition="1659" endWordPosition="1663">nes of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 Our Proposal: A Latent LC Approach This section details our approach for distributionally representing MWPs by leveraging their component LCs. </context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In EMNLP, pages 1183–1193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Berant</author>
<author>Jacob Goldberger</author>
<author>Ido Dagan</author>
</authors>
<title>Global learning of typed entailment rules.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>610--619</pages>
<contexts>
<context position="4679" citStr="Berant et al., 2011" startWordPosition="714" endWordPosition="717"> inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation PL → PR is said to hold if the relation denoted by PR generally holds between a set of arguments whenever the relation PL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference relation “have → take” does not generally hold, it does hold in the case of some light verbs, such as “have a look → take a look”, underscoring the importance of taking more inclusive LCs into account. On the other hand, the predicate “likely to give a green light” is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., “likely” or “give a green light”) into account. We present a novel approach to the task that models the selection and relative</context>
<context position="6598" citStr="Berant et al., 2011" startWordPosition="1030" endWordPosition="1033">ts. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (Melamud et al., 2013a</context>
<context position="8337" citStr="Berant et al. (2011)" startWordPosition="1291" endWordPosition="1294">tations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules. Previous work used simple methods to select the predicate’s LC. Some filtered out frequent highly ambiguous verbs (Lewis and Steedman, 2013), others selected a single representative word (Melamud et al., 2013a), while yet others used multi-word LCs but treated them as fixed expressions (Lin and Pantel, 2001; Berant et al., 2011). The goals of the above studies are largely com645 plement</context>
<context position="30856" citStr="Berant et al., 2011" startWordPosition="5117" endWordPosition="5120">NEG, which predicts the most frequent label in the dataset (in our case: “no inference”). The other evaluated systems are formed by taking various subsets of our feature set. We experiment with 4 feature sets. The smallest set, SIM, includes only the similarity features. This feature set is related to the compositional distributional model of Mitchell and Lapata (2010) (see Section 6). We note that despite recent advances in identifying predicate inference relations, the DIRT system (Lin and Pantel, 2001) remains a strong baseline, and is often used as a component in state-of-the-art systems (Berant et al., 2011), and specifically in the two aforementioned works that used the same evaluation corpus. The next feature set BASIC includes the features found to be most useful during the development of the model: the most frequent POS tag, the frequency features and the feature Common. More inclusive is the feature set NO-LDA, which includes all features except the LDA features. Experiments with this set were performed in order to isolate the effect of the LDA features. Finally, ALL includes our complete set of features. The more direct comparison is against partial implementations of our system where the L</context>
</contexts>
<marker>Berant, Goldberger, Dagan, 2011</marker>
<rawString>Jonathan Berant, Jacob Goldberger, and Ido Dagan. 2011. Global learning of typed entailment rules. In ACL, pages 610–619.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Distributional semantics and compositionality 2011: Shared task description and results.</title>
<date>2011</date>
<booktitle>In Workshop on Distributional Semantics and Compositionality,</booktitle>
<pages>21--28</pages>
<contexts>
<context position="9845" citStr="Biemann and Giesbrecht, 2011" startWordPosition="1527" endWordPosition="1530">fication of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositio</context>
</contexts>
<marker>Biemann, Giesbrecht, 2011</marker>
<rawString>Chris Biemann and Eugenie Giesbrecht. 2011. Distributional semantics and compositionality 2011: Shared task description and results. In Workshop on Distributional Semantics and Compositionality, pages 21–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Andrew Y Ng</author>
<author>Michael I Jordan</author>
</authors>
<date>2003</date>
<booktitle>Latent Dirichlet allocation. the Journal of machine Learning research,</booktitle>
<pages>3--993</pages>
<contexts>
<context position="23760" citStr="Blei et al., 2003" startWordPosition="3948" endWordPosition="3951"> encode the conjunction of their POS and the number of times the two lemmas occurred together in R. We also introduce features that capture the statistical correlation between the words of hL. To do so, we use point-wise mutual information, and the conditional probabilities P(hAL|hBL) and P(hBL|hAL). Similar measures have often been used for the unsupervised detection of MWEs (Villavicencio et al., 2007; Fazly and Stevenson, 2006). We also include the analogous set of features for hR. LDA-based Features. We further incorporate features based on a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). Several recent works have underscored the usefulness of using topic models to model a predicate’s selectional preferences (Ritter et al., 2010; Dinu and Lapata, 2010; S´eaghdha, 2010; Lewis and Steedman, 2013; Melamud et al., 2013a). We adopt the approach of Lewis and Steedman (2013), and define a pseudo-document for each LC in the evaluation corpus. We populate the pseudo-documents of an LC with its arguments according to R. We then train an LDA model with 25 topics over these documents. This yields a probability distribution P(topic|h) for each LC h, reflecting the types of arguments h may</context>
</contexts>
<marker>Blei, Ng, Jordan, 2003</marker>
<rawString>David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent Dirichlet allocation. the Journal of machine Learning research, 3:993–1022.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
</authors>
<title>The light verb jungle: still hacking away. In Complex predicates: cross-linguistic perspectives on event structure,</title>
<date>2010</date>
<pages>48--78</pages>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9650" citStr="Butt, 2010" startWordPosition="1495" endWordPosition="1496">ntations themselves or on their incorporation into more elaborate systems, we focus on the integration of the distributional representation of multiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta </context>
</contexts>
<marker>Butt, 2010</marker>
<rawString>Miriam Butt. 2010. The light verb jungle: still hacking away. In Complex predicates: cross-linguistic perspectives on event structure, pages 48–78. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bob Coecke</author>
<author>Mehrnoosh Sadrzadeh</author>
<author>Stephen Clark</author>
</authors>
<title>Mathematical foundations for a compositional distributional model of meaning.</title>
<date>2010</date>
<booktitle>Linguistic Analysis,</booktitle>
<volume>36</volume>
<pages>435--384</pages>
<editor>In J. van Bentham, M. Moortgat, and W. Buszkowski, editors,</editor>
<contexts>
<context position="10743" citStr="Coecke et al., 2010" startWordPosition="1664" endWordPosition="1667">ic MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 Our Proposal: A Latent LC Approach This section details our approach for distributionally representing MWPs by leveraging their component LCs. Section 3.1 describes </context>
</contexts>
<marker>Coecke, Sadrzadeh, Clark, 2010</marker>
<rawString>Bob Coecke, Mehrnoosh Sadrzadeh, and Stephen Clark. 2010. Mathematical foundations for a compositional distributional model of meaning. In J. van Bentham, M. Moortgat, and W. Buszkowski, editors, Linguistic Analysis, volume 36, pages 435–384.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Dan Roth</author>
<author>Mark Sammons</author>
<author>Fabio Massimo Zanzotto</author>
</authors>
<title>Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies,</title>
<date>2013</date>
<pages>6--4</pages>
<contexts>
<context position="6902" citStr="Dagan et al., 2013" startWordPosition="1079" endWordPosition="1082">ed in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (Melamud et al., 2013a), and is often used as a component within larger systems. Schoenmackers et al. (2010) presented an unsupervised system for learning inference rules directly from open-domain web data. Melamud et al. (2013a) used topic models to combine typelevel predicate inference rules with token-level information fr</context>
</contexts>
<marker>Dagan, Roth, Sammons, Zanzotto, 2013</marker>
<rawString>Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto. 2013. Recognizing textual entailment: Models and applications. Synthesis Lectures on Human Language Technologies, 6(4):1–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Mirella Lapata</author>
</authors>
<title>Topic models for meaning similarity in context.</title>
<date>2010</date>
<booktitle>In COLING: Posters,</booktitle>
<pages>250--258</pages>
<contexts>
<context position="23927" citStr="Dinu and Lapata, 2010" startWordPosition="3974" endWordPosition="3977">ion between the words of hL. To do so, we use point-wise mutual information, and the conditional probabilities P(hAL|hBL) and P(hBL|hAL). Similar measures have often been used for the unsupervised detection of MWEs (Villavicencio et al., 2007; Fazly and Stevenson, 2006). We also include the analogous set of features for hR. LDA-based Features. We further incorporate features based on a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). Several recent works have underscored the usefulness of using topic models to model a predicate’s selectional preferences (Ritter et al., 2010; Dinu and Lapata, 2010; S´eaghdha, 2010; Lewis and Steedman, 2013; Melamud et al., 2013a). We adopt the approach of Lewis and Steedman (2013), and define a pseudo-document for each LC in the evaluation corpus. We populate the pseudo-documents of an LC with its arguments according to R. We then train an LDA model with 25 topics over these documents. This yields a probability distribution P(topic|h) for each LC h, reflecting the types of arguments h may take. We further include a feature for the entropy of the topic distribution of the predicate, which reflects its heterogeneity. This feature is motivated by the assu</context>
</contexts>
<marker>Dinu, Lapata, 2010</marker>
<rawString>Georgiana Dinu and Mirella Lapata. 2010. Topic models for meaning similarity in context. In COLING: Posters, pages 250–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Rui Wang</author>
</authors>
<title>Inference rules and their application to recognizing textual entailment.</title>
<date>2009</date>
<booktitle>In EACL,</booktitle>
<pages>211--219</pages>
<contexts>
<context position="6881" citStr="Dinu and Wang, 2009" startWordPosition="1075" endWordPosition="1078">h several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (Melamud et al., 2013a), and is often used as a component within larger systems. Schoenmackers et al. (2010) presented an unsupervised system for learning inference rules directly from open-domain web data. Melamud et al. (2013a) used topic models to combine typelevel predicate inference rules with token</context>
</contexts>
<marker>Dinu, Wang, 2009</marker>
<rawString>Georgiana Dinu and Rui Wang. 2009. Inference rules and their application to recognizing textual entailment. In EACL, pages 211–219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Fader</author>
<author>Stephen Soderland</author>
<author>Oren Etzioni</author>
</authors>
<title>Identifying relations for open information extraction.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1535--1545</pages>
<contexts>
<context position="17494" citStr="Fader et al., 2011" startWordPosition="2858" endWordPosition="2861">e Set This section lists the features used for our experiments. We intentionally select a feature set that relies on either completely unsupervised or shallow processing tools that are available for a wide variety of languages and domains. Given a predicate pair p(i), a label y E 11, −1} and a latent state h E H(i), we define their feature vector as b(p(i), y, h) = y · b(p(i), h). The computation of b(p(i), h) requires a reference corpus R that contains triplets of the type (p, x, y) where p is a binary predicate and x and y are its arguments. We use the Reverb corpus as R in our experiments (Fader et al., 2011; see Section 4). We refrain from encoding features that directly reflect the vocabulary of the training set. Such features are not applicable beyond that set’s vocabulary, and as available datasets contain no more than a few thousand examples, these features are unlikely to generalize well. Table 1 presents the set of features we use in our experiments. The features can be divided into two main categories: similarity features between the LHS and the RHS predicates (table’s top), and features that reflect the individual properties of each 2AHpI is about 15 on average in our dataset, where less</context>
<context position="25760" citStr="Fader et al., 2011" startWordPosition="4284" endWordPosition="4287">s results in 1605 non-constant features. We further note that some LCs that appear in the evaluation corpus do not appear at all in R. In our experiments they amounted to 0.2% of the LCs in our evaluation dataset. While previous work often discarded predicates below a certain frequency from the evaluation, we include them in order to facilitate comparison to future work. We assign the similarity features of such examples a 0 value, and assign their other numerical features the mean value of those features. 4 Experimental Setup Corpora and Preprocessing. As a reference corpus R, we use Reverb (Fader et al., 2011), a web-based corpus consisting of 15M web extractions of binary relations. Each relation is a triplet of a predicate and two arguments, one preceding it and one following it. Relations were extracted using regular expressions over the output of a POS tagger and an NP chunker. Each predicate may consist of a single verb, a verb and a preposition or a sequence of words starting in a verb and ending in a preposition, between which there may nouns, adjectives, adverbs, pronouns, determiners and verbs. The verb may also be a copula. Examples of predicates are “make the most of”, “could be exchange</context>
</contexts>
<marker>Fader, Soderland, Etzioni, 2011</marker>
<rawString>Anthony Fader, Stephen Soderland, and Oren Etzioni. 2011. Identifying relations for open information extraction. In EMNLP, pages 1535–1545.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Afsaneh Fazly</author>
<author>Suzanne Stevenson</author>
</authors>
<title>Automatically constructing a lexicon of verb phrase idiomatic combinations.</title>
<date>2006</date>
<booktitle>In EACL,</booktitle>
<pages>337--344</pages>
<contexts>
<context position="23576" citStr="Fazly and Stevenson, 2006" startWordPosition="3918" endWordPosition="3921">which is used by some light verb identification algorithms (Tu and Roth, 2011). In cases where hL is of size 2, we additionally encode features that apply to the conjunction of hAL and hBL. We encode the conjunction of their POS and the number of times the two lemmas occurred together in R. We also introduce features that capture the statistical correlation between the words of hL. To do so, we use point-wise mutual information, and the conditional probabilities P(hAL|hBL) and P(hBL|hAL). Similar measures have often been used for the unsupervised detection of MWEs (Villavicencio et al., 2007; Fazly and Stevenson, 2006). We also include the analogous set of features for hR. LDA-based Features. We further incorporate features based on a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). Several recent works have underscored the usefulness of using topic models to model a predicate’s selectional preferences (Ritter et al., 2010; Dinu and Lapata, 2010; S´eaghdha, 2010; Lewis and Steedman, 2013; Melamud et al., 2013a). We adopt the approach of Lewis and Steedman (2013), and define a pseudo-document for each LC in the evaluation corpus. We populate the pseudo-documents of an LC with its arguments </context>
</contexts>
<marker>Fazly, Stevenson, 2006</marker>
<rawString>Afsaneh Fazly and Suzanne Stevenson. 2006. Automatically constructing a lexicon of verb phrase idiomatic combinations. In EACL, pages 337–344.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masato Hagiwara</author>
<author>Yasuhiro Ogawa</author>
<author>Katsuhiko Toyama</author>
</authors>
<title>Supervised synonym acquisition using distributional features and syntactic patterns.</title>
<date>2009</date>
<booktitle>Information and Media Technologies,</booktitle>
<pages>4--2</pages>
<contexts>
<context position="8218" citStr="Hagiwara et al. (2009)" startWordPosition="1273" endWordPosition="1276">rove the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules. Previous work used simple methods to select the predicate’s LC. Some filtered out frequent highly ambiguous verbs (Lewis and Steedman, 2013), others selected a single representative word (Melamud et al., 2013a), while yet others used multi-word LCs but treated them as f</context>
</contexts>
<marker>Hagiwara, Ogawa, Toyama, 2009</marker>
<rawString>Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama. 2009. Supervised synonym acquisition using distributional features and syntactic patterns. Information and Media Technologies, 4(2):558–582.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey E Hinton</author>
</authors>
<title>Training products of experts by minimizing contrastive divergence.</title>
<date>2002</date>
<booktitle>Neural computation,</booktitle>
<pages>14--8</pages>
<contexts>
<context position="40477" citStr="Hinton, 2002" startWordPosition="6736" endWordPosition="6737">e to the initialization point. We hereby describe an experiment that tests the sensitivity of our approach to such variance. Selecting the highest scoring feature set on our test set (i.e., BASIC), we ran the model with multiple initializers, by randomly perturbing our standard convex initializer (see Section 3). Concretely, given a convex initializer w, we select the starting point to be w + q, where qi ∼ N(0,α|wi|). We ran this experiment 400 times with α = 0.8. To combine the resulting weight vectors into a single classifier, we apply two types of standard approaches: a Product of Experts (Hinton, 2002), as well as a voting approach that selects the most frequently predicted label. Neither of these experiments yielded any significant performance gain. This demonstrates the robustness of our optimization method to the initialization point. 7 Conclusion We have presented a novel approach to the distributional representation of multi-word predicates. Since MWPs demonstrate varying levels of compositionality, a uniform treatment of MWPs either as fixed expressions or through head words is lacking. Instead, our approach integrates multiple lexical units contained in the predicate. The approach ta</context>
</contexts>
<marker>Hinton, 2002</marker>
<rawString>Geoffrey E Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural computation, 14(8):1771–1800.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Hoffman</author>
<author>Francis R Bach</author>
<author>David M Blei</author>
</authors>
<title>Online learning for latent Dirichlet allocation. In</title>
<date>2010</date>
<booktitle>NIPS,</booktitle>
<pages>856--864</pages>
<contexts>
<context position="29621" citStr="Hoffman et al., 2010" startWordPosition="4921" endWordPosition="4924">e-grained set of POS tags, we collapse the tag set to 7 categories: nouns, verbs, adjectives, adverbs, prepositions, the word “to” and a category that includes all other words. A Reverb argument is represented as the conjunction of its content words that appear more than 10 times in the corpus. Function words are defined according to their POS tags and include determiners, possessive pronouns, existential “there”, numbers and coordinating conjunctions. Auxiliary verbs and copulas are also considered function words. To compute the LDA features, we use the online variational Bayes algorithm of (Hoffman et al., 2010) as implemented in the Gensim software package (Rehurek and Sojka, 2010). Evaluated Algorithms. The only two previous works on this dataset (Melamud et al., 2013a; Melamud et al., 2013b) are not directly comparable, as they used unsupervised systems and evalu4A script that replicates our train-test partition of the corpus can be found here: http://homepages.inf.ed. ac.uk/oabend/mwpreds.html ated on sub-sets of the evaluation dataset. Instead, we use several baselines to demonstrate the usefulness of integrating multiple LCs, as well as the relative usefulness of our feature sets. The simplest </context>
</contexts>
<marker>Hoffman, Bach, Blei, 2010</marker>
<rawString>Matthew Hoffman, Francis R Bach, and David M Blei. 2010. Online learning for latent Dirichlet allocation. In NIPS, pages 856–864.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ray Jackendoff</author>
</authors>
<title>Foundations of language: Brain, meaning, grammar, evolution.</title>
<date>2002</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="1396" citStr="Jackendoff, 2002" startWordPosition="197" endWordPosition="198">P’s words. We assume a latent distribution over sub-sets of the MWP, and estimate it relative to a downstream prediction task. Focusing on the supervised identification of lexical inference relations, we compare against state-of-the-art baselines that consider a single sub-set of an MWP, obtaining substantial improvements. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics. 1 Introduction Multi-word expressions (MWEs) constitute a large part of the lexicon and account for much of its growth (Jackendoff, 2002; Seaton and Macaulay, 2002). However, despite their importance, MWEs remain difficult to define and model, and consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 1997). MWPs encompass a wide range of phenomena, including causatives, light verbs, phrasal verbs, serial verb constructions and many others, and pose conside</context>
<context position="6341" citStr="Jackendoff, 2002" startWordPosition="991" endWordPosition="992">ions between MWPs of varying degrees of compositionality within distributional semantics. We conduct experiments on the dataset of Zeichner et al. (2012) and compare our methods with analogous ones that select a fixed LC, using stateof-the-art feature sets. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used dist</context>
</contexts>
<marker>Jackendoff, 2002</marker>
<rawString>Ray Jackendoff. 2002. Foundations of language: Brain, meaning, grammar, evolution. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douwe Kiela</author>
<author>Stephen Clark</author>
</authors>
<title>Detecting compositionality of multi-word expressions using nearest neighbours in vector space models.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1427--1432</pages>
<contexts>
<context position="10963" citStr="Kiela and Clark, 2013" startWordPosition="1695" endWordPosition="1698">ositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 Our Proposal: A Latent LC Approach This section details our approach for distributionally representing MWPs by leveraging their component LCs. Section 3.1 describes our general approach, Section 3.2 presents our model and Section 3.3 details the feature set. 3.1 General Approach and Notation We propose a method for addressing MWPs of varying degrees of compositionality through the i</context>
</contexts>
<marker>Kiela, Clark, 2013</marker>
<rawString>Douwe Kiela and Stephen Clark. 2013. Detecting compositionality of multi-word expressions using nearest neighbours in vector space models. In EMNLP, pages 1427–1432.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Lewis</author>
<author>Mark Steedman</author>
</authors>
<title>Combined distributional and logical semantics.</title>
<date>2013</date>
<journal>TACL,</journal>
<volume>1</volume>
<pages>192</pages>
<contexts>
<context position="4706" citStr="Lewis and Steedman, 2013" startWordPosition="718" endWordPosition="721">between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation PL → PR is said to hold if the relation denoted by PR generally holds between a set of arguments whenever the relation PL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference relation “have → take” does not generally hold, it does hold in the case of some light verbs, such as “have a look → take a look”, underscoring the importance of taking more inclusive LCs into account. On the other hand, the predicate “likely to give a green light” is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., “likely” or “give a green light”) into account. We present a novel approach to the task that models the selection and relative weighting of the predicate</context>
<context position="7671" citStr="Lewis and Steedman (2013)" startWordPosition="1194" endWordPosition="1197"> unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (Melamud et al., 2013a), and is often used as a component within larger systems. Schoenmackers et al. (2010) presented an unsupervised system for learning inference rules directly from open-domain web data. Melamud et al. (2013a) used topic models to combine typelevel predicate inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach rely</context>
<context position="23970" citStr="Lewis and Steedman, 2013" startWordPosition="3980" endWordPosition="3984">e use point-wise mutual information, and the conditional probabilities P(hAL|hBL) and P(hBL|hAL). Similar measures have often been used for the unsupervised detection of MWEs (Villavicencio et al., 2007; Fazly and Stevenson, 2006). We also include the analogous set of features for hR. LDA-based Features. We further incorporate features based on a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). Several recent works have underscored the usefulness of using topic models to model a predicate’s selectional preferences (Ritter et al., 2010; Dinu and Lapata, 2010; S´eaghdha, 2010; Lewis and Steedman, 2013; Melamud et al., 2013a). We adopt the approach of Lewis and Steedman (2013), and define a pseudo-document for each LC in the evaluation corpus. We populate the pseudo-documents of an LC with its arguments according to R. We then train an LDA model with 25 topics over these documents. This yields a probability distribution P(topic|h) for each LC h, reflecting the types of arguments h may take. We further include a feature for the entropy of the topic distribution of the predicate, which reflects its heterogeneity. This feature is motivated by the assumption that a heterogeneous predicate is mo</context>
<context position="32318" citStr="Lewis and Steedman, 2013" startWordPosition="5355" endWordPosition="5358">. Similar selection strategy was carried out by Melamud et al. (2013a). The second, VPREP, selects h to be the verb along with its following preposition. In cases the predicate contains multiple verbs, the one preceding the preposition is selected, and where the predicate does not contain any non-copula verbs, it regresses to LEFTMOST. This LC selection method approximates a baseline that includes subcategorized prepositions. Such cases are highly frequent and account for a large portion of the MWPs in English. Including a verb’s preposition in its LC was commonly done in previous work (e.g., Lewis and Steedman, 2013). We also attempted to identify verb-preposition constructions using a dependency parser. Unfortunately, our evaluation dataset is only available in 650 a lemmatized version, which posed a difficulty for the parser. Due to the low quality of the resulting parses, we implemented VPREP using POS-based regular expressions as defined above. The full model is denoted with LATENTLC. For each system and feature set, we report results using 10-fold cross-validation on the training set, as well as results on the test set. Both cases use the same set of parameters determined by crossvalidation on the tr</context>
</contexts>
<marker>Lewis, Steedman, 2013</marker>
<rawString>Mike Lewis and Mark Steedman. 2013. Combined distributional and logical semantics. TACL, 1:179– 192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>DIRT – discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In SIGKDD</booktitle>
<pages>323--328</pages>
<contexts>
<context position="4121" citStr="Lin and Pantel, 2001" startWordPosition="615" endWordPosition="619">he verb “take” could be conjoined with its object (e.g., “take care”, “take a bus”). This approach, however, raises the challenge of identifying the sub-set of the predicate’s words that should be taken to represent it (henceforth, its lexical components or LCs). We propose a novel approach that addresses this 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation PL → PR is said to hold if the relation denoted by PR generally holds between a set of arguments whenever the relation PL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, </context>
<context position="7026" citStr="Lin and Pantel (2001)" startWordPosition="1099" endWordPosition="1102">. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (Melamud et al., 2013a), and is often used as a component within larger systems. Schoenmackers et al. (2010) presented an unsupervised system for learning inference rules directly from open-domain web data. Melamud et al. (2013a) used topic models to combine typelevel predicate inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of inf</context>
<context position="8856" citStr="Lin and Pantel, 2001" startWordPosition="1372" endWordPosition="1375">ms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules. Previous work used simple methods to select the predicate’s LC. Some filtered out frequent highly ambiguous verbs (Lewis and Steedman, 2013), others selected a single representative word (Melamud et al., 2013a), while yet others used multi-word LCs but treated them as fixed expressions (Lin and Pantel, 2001; Berant et al., 2011). The goals of the above studies are largely com645 plementary to ours. While previous work focused either on improving the quality of the distributional representations themselves or on their incorporation into more elaborate systems, we focus on the integration of the distributional representation of multiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexic</context>
<context position="20332" citStr="Lin and Pantel, 2001" startWordPosition="3350" endWordPosition="3353">here hL is of size 2. The bottom part lists the LDA-based features. of them. Within the LHS feature set, we distinguish between two sub-types of features: word features that encode the individual properties of hAL and hBL (table’s upper middle part), and pair features that only apply to LCs of size 2 and reflect the relation between hAL and hBL (table’s lower middle part). We further incorporate LDA-based features that reflect the selectional preferences of the predicates (table’s bottom). Distributional Similarity Features. The distributional similarity features are based on the DIRT system (Lin and Pantel, 2001). The score defines for each predicate p and for each argument slot s E {L, R} (corresponding to the arguments to the right and left of that predicate) a vector vps which represents the distribution of arguments appearing in that slot. We take vps(x) to be the number of times that the argument x appeared in the slot s of the predicate p. Given these vectors, the similarity between the predicates p1 and p2 is defined as: Vscore(p1, p2) = sim(vp1L , vp2L ) · sim(vp1R , vp2R ) where situ is some vector similarity measure. We use two common similarity measures: the vector cosine metric, and the BI</context>
<context position="30746" citStr="Lin and Pantel, 2001" startWordPosition="5099" endWordPosition="5102"> integrating multiple LCs, as well as the relative usefulness of our feature sets. The simplest baseline is ALLNEG, which predicts the most frequent label in the dataset (in our case: “no inference”). The other evaluated systems are formed by taking various subsets of our feature set. We experiment with 4 feature sets. The smallest set, SIM, includes only the similarity features. This feature set is related to the compositional distributional model of Mitchell and Lapata (2010) (see Section 6). We note that despite recent advances in identifying predicate inference relations, the DIRT system (Lin and Pantel, 2001) remains a strong baseline, and is often used as a component in state-of-the-art systems (Berant et al., 2011), and specifically in the two aforementioned works that used the same evaluation corpus. The next feature set BASIC includes the features found to be most useful during the development of the model: the most frequent POS tag, the frequency features and the feature Common. More inclusive is the feature set NO-LDA, which includes all features except the LDA features. Experiments with this set were performed in order to isolate the effect of the LDA features. Finally, ALL includes our com</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. DIRT – discovery of inference rules from text. In SIGKDD 2001, pages 323–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic retrieval and clustering of similar words.</title>
<date>1998</date>
<booktitle>In COLING-ACL,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="21293" citStr="Lin (1998)" startWordPosition="3517" endWordPosition="3518"> vectors, the similarity between the predicates p1 and p2 is defined as: Vscore(p1, p2) = sim(vp1L , vp2L ) · sim(vp1R , vp2R ) where situ is some vector similarity measure. We use two common similarity measures: the vector cosine metric, and the BInc (Szpektor and Dagan, 2008) similarity measure. These measures give complementary perspectives on the similarity between the predicates, as the cosine similarity is symmetric between the LHS and RHS predicates, while BInc takes into account the directionality of the inference relation. Preliminary experiments with other measures, such as those of Lin (1998) and Weeds and Weir (2003) did not yield additional improvements. We encode the similarity of all measures for the pair hL and hR as well as the pair hAL and hAR. The latter feature is an approximation to the similarity between the heads of the predicates, as heads in English tend to be to the left of the predicates. These two features coincide for h values of size 1. Word and Pair Features. These features encode the basic properties of the LC. The motivation behind them is to allow a more accurate leveraging of the similarity features, as well as to better determine the relative weights of h </context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. Automatic retrieval and clustering of similar words. In COLING-ACL, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic identification of noncompositional phrases.</title>
<date>1999</date>
<booktitle>In ACL,</booktitle>
<pages>317--324</pages>
<contexts>
<context position="9792" citStr="Lin, 1999" startWordPosition="1521" endWordPosition="1522">ultiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in both theoretical and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly b</context>
</contexts>
<marker>Lin, 1999</marker>
<rawString>Dekang Lin. 1999. Automatic identification of noncompositional phrases. In ACL, pages 317–324.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Loper</author>
<author>Steven Bird</author>
</authors>
<title>NLTK: The natural language toolkit.</title>
<date>2002</date>
<booktitle>In ACL Workshop on Effective tools</booktitle>
<pages>63--70</pages>
<contexts>
<context position="28981" citStr="Loper and Bird, 2002" startWordPosition="4817" endWordPosition="4820"> function words). In 87.3% of the predicate pairs, there was more than one LC (i.e., |Hp |&gt; 1), underscoring the importance of correctly leveraging the different LCs. We randomly partition the corpus into a training set which contains 4,343 instances (∼80%), and a test set that contains 1,068 instances, maintaining the same positive to negative label ratio in both datasets4. Development was carried out using cross-validation on the training data (see below). We use a Maximum Entropy POS Tagger, trained on the Penn Treebank, and the WordNet lemmatizer, both implemented within the NLTK package (Loper and Bird, 2002). To obtain a coarse-grained set of POS tags, we collapse the tag set to 7 categories: nouns, verbs, adjectives, adverbs, prepositions, the word “to” and a category that includes all other words. A Reverb argument is represented as the conjunction of its content words that appear more than 10 times in the corpus. Function words are defined according to their POS tags and include determiners, possessive pronouns, existential “there”, numbers and coordinating conjunctions. Auxiliary verbs and copulas are also considered function words. To compute the LDA features, we use the online variational B</context>
</contexts>
<marker>Loper, Bird, 2002</marker>
<rawString>Edward Loper and Steven Bird. 2002. NLTK: The natural language toolkit. In ACL Workshop on Effective tools and methodologies for teaching natural language processing and computational linguistics, pages 63–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Bill Keller</author>
<author>John Carroll</author>
</authors>
<title>Detecting a continuum of compositionality in phrasal verbs.</title>
<date>2003</date>
<booktitle>In ACL workshop on Multiword expressions:</booktitle>
<pages>73--80</pages>
<contexts>
<context position="10239" citStr="McCarthy et al., 2003" startWordPosition="1591" endWordPosition="1594">Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess</context>
</contexts>
<marker>McCarthy, Keller, Carroll, 2003</marker>
<rawString>Diana McCarthy, Bill Keller, and John Carroll. 2003. Detecting a continuum of compositionality in phrasal verbs. In ACL workshop on Multiword expressions: analysis, acquisition and treatment, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Melamud</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
<author>Idan Szpektor</author>
</authors>
<title>A two level model for context sensitive inference rules.</title>
<date>2013</date>
<booktitle>In ACL 2013,</booktitle>
<pages>1331--1340</pages>
<contexts>
<context position="4171" citStr="Melamud et al., 2013" startWordPosition="624" endWordPosition="627">(e.g., “take care”, “take a bus”). This approach, however, raises the challenge of identifying the sub-set of the predicate’s words that should be taken to represent it (henceforth, its lexical components or LCs). We propose a novel approach that addresses this 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation PL → PR is said to hold if the relation denoted by PR generally holds between a set of arguments whenever the relation PL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference rela</context>
<context position="6620" citStr="Melamud et al., 2013" startWordPosition="1034" endWordPosition="1037">s substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (Melamud et al., 2013a), and is often used a</context>
<context position="8756" citStr="Melamud et al., 2013" startWordPosition="1355" endWordPosition="1358">using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules. Previous work used simple methods to select the predicate’s LC. Some filtered out frequent highly ambiguous verbs (Lewis and Steedman, 2013), others selected a single representative word (Melamud et al., 2013a), while yet others used multi-word LCs but treated them as fixed expressions (Lin and Pantel, 2001; Berant et al., 2011). The goals of the above studies are largely com645 plementary to ours. While previous work focused either on improving the quality of the distributional representations themselves or on their incorporation into more elaborate systems, we focus on the integration of the distributional representation of multiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in</context>
<context position="16533" citStr="Melamud et al., 2013" startWordPosition="2678" endWordPosition="2681">s into the log-linear model leads to a nonconvex objective function. Consequently, BFGS is not guaranteed to converge to the global optimum, but rather to a stationary point. The result may therefore depend on the parameter initialization. Indeed, preliminary experiments showed that both initializing w to be zero and using a random initializer results in lower performance. Instead, we initialize our model with a simplified convex model that fixes the LCs to be the pair of left-most content words comprising each of the predicates. This is a common method for selecting the predicate’s LC (e.g., Melamud et al., 2013a). Once h has been fixed, the model collapses to a convex log-linear model. The optimal w is then taken as an initialization point for the latent variable model. While this method may still not converge to the global maximum, our experiments show that this initialization technique yields high quality values for w (see Section 6). 3.3 Feature Set This section lists the features used for our experiments. We intentionally select a feature set that relies on either completely unsupervised or shallow processing tools that are available for a wide variety of languages and domains. Given a predicate</context>
<context position="23992" citStr="Melamud et al., 2013" startWordPosition="3985" endWordPosition="3988">formation, and the conditional probabilities P(hAL|hBL) and P(hBL|hAL). Similar measures have often been used for the unsupervised detection of MWEs (Villavicencio et al., 2007; Fazly and Stevenson, 2006). We also include the analogous set of features for hR. LDA-based Features. We further incorporate features based on a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). Several recent works have underscored the usefulness of using topic models to model a predicate’s selectional preferences (Ritter et al., 2010; Dinu and Lapata, 2010; S´eaghdha, 2010; Lewis and Steedman, 2013; Melamud et al., 2013a). We adopt the approach of Lewis and Steedman (2013), and define a pseudo-document for each LC in the evaluation corpus. We populate the pseudo-documents of an LC with its arguments according to R. We then train an LDA model with 25 topics over these documents. This yields a probability distribution P(topic|h) for each LC h, reflecting the types of arguments h may take. We further include a feature for the entropy of the topic distribution of the predicate, which reflects its heterogeneity. This feature is motivated by the assumption that a heterogeneous predicate is more likely to benefit f</context>
<context position="29782" citStr="Melamud et al., 2013" startWordPosition="4946" endWordPosition="4949">l other words. A Reverb argument is represented as the conjunction of its content words that appear more than 10 times in the corpus. Function words are defined according to their POS tags and include determiners, possessive pronouns, existential “there”, numbers and coordinating conjunctions. Auxiliary verbs and copulas are also considered function words. To compute the LDA features, we use the online variational Bayes algorithm of (Hoffman et al., 2010) as implemented in the Gensim software package (Rehurek and Sojka, 2010). Evaluated Algorithms. The only two previous works on this dataset (Melamud et al., 2013a; Melamud et al., 2013b) are not directly comparable, as they used unsupervised systems and evalu4A script that replicates our train-test partition of the corpus can be found here: http://homepages.inf.ed. ac.uk/oabend/mwpreds.html ated on sub-sets of the evaluation dataset. Instead, we use several baselines to demonstrate the usefulness of integrating multiple LCs, as well as the relative usefulness of our feature sets. The simplest baseline is ALLNEG, which predicts the most frequent label in the dataset (in our case: “no inference”). The other evaluated systems are formed by taking various</context>
<context position="31761" citStr="Melamud et al. (2013" startWordPosition="5266" endWordPosition="5269">ve is the feature set NO-LDA, which includes all features except the LDA features. Experiments with this set were performed in order to isolate the effect of the LDA features. Finally, ALL includes our complete set of features. The more direct comparison is against partial implementations of our system where the LC h is deterministically selected. Determining h for each predicate yields a regular log-linear binary classification model. We use two variants of this baseline. The first, LEFTMOST, selects the left-most content word for each predicate. Similar selection strategy was carried out by Melamud et al. (2013a). The second, VPREP, selects h to be the verb along with its following preposition. In cases the predicate contains multiple verbs, the one preceding the preposition is selected, and where the predicate does not contain any non-copula verbs, it regresses to LEFTMOST. This LC selection method approximates a baseline that includes subcategorized prepositions. Such cases are highly frequent and account for a large portion of the MWPs in English. Including a verb’s preposition in its LC was commonly done in previous work (e.g., Lewis and Steedman, 2013). We also attempted to identify verb-prepos</context>
</contexts>
<marker>Melamud, Berant, Dagan, Goldberger, Szpektor, 2013</marker>
<rawString>Oren Melamud, Jonathan Berant, Ido Dagan, Jacob Goldberger, and Idan Szpektor. 2013a. A two level model for context sensitive inference rules. In ACL 2013, pages 1331–1340.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Melamud</author>
<author>Ido Dagan</author>
<author>Jacob Goldberger</author>
<author>Idan Szpektor</author>
</authors>
<title>Using lexical expansion to learn inference rules from sparse data. In ACL: Short Papers,</title>
<date>2013</date>
<pages>283--288</pages>
<contexts>
<context position="4171" citStr="Melamud et al., 2013" startWordPosition="624" endWordPosition="627">(e.g., “take care”, “take a bus”). This approach, however, raises the challenge of identifying the sub-set of the predicate’s words that should be taken to represent it (henceforth, its lexical components or LCs). We propose a novel approach that addresses this 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation PL → PR is said to hold if the relation denoted by PR generally holds between a set of arguments whenever the relation PL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference rela</context>
<context position="6620" citStr="Melamud et al., 2013" startWordPosition="1034" endWordPosition="1037">s substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (Melamud et al., 2013a), and is often used a</context>
<context position="8756" citStr="Melamud et al., 2013" startWordPosition="1355" endWordPosition="1358">using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules. Previous work used simple methods to select the predicate’s LC. Some filtered out frequent highly ambiguous verbs (Lewis and Steedman, 2013), others selected a single representative word (Melamud et al., 2013a), while yet others used multi-word LCs but treated them as fixed expressions (Lin and Pantel, 2001; Berant et al., 2011). The goals of the above studies are largely com645 plementary to ours. While previous work focused either on improving the quality of the distributional representations themselves or on their incorporation into more elaborate systems, we focus on the integration of the distributional representation of multiple LCs to improve the identification of inference relations between MWPs. MWP Extraction and Identification. MWPs have received considerable attention over the years in</context>
<context position="16533" citStr="Melamud et al., 2013" startWordPosition="2678" endWordPosition="2681">s into the log-linear model leads to a nonconvex objective function. Consequently, BFGS is not guaranteed to converge to the global optimum, but rather to a stationary point. The result may therefore depend on the parameter initialization. Indeed, preliminary experiments showed that both initializing w to be zero and using a random initializer results in lower performance. Instead, we initialize our model with a simplified convex model that fixes the LCs to be the pair of left-most content words comprising each of the predicates. This is a common method for selecting the predicate’s LC (e.g., Melamud et al., 2013a). Once h has been fixed, the model collapses to a convex log-linear model. The optimal w is then taken as an initialization point for the latent variable model. While this method may still not converge to the global maximum, our experiments show that this initialization technique yields high quality values for w (see Section 6). 3.3 Feature Set This section lists the features used for our experiments. We intentionally select a feature set that relies on either completely unsupervised or shallow processing tools that are available for a wide variety of languages and domains. Given a predicate</context>
<context position="23992" citStr="Melamud et al., 2013" startWordPosition="3985" endWordPosition="3988">formation, and the conditional probabilities P(hAL|hBL) and P(hBL|hAL). Similar measures have often been used for the unsupervised detection of MWEs (Villavicencio et al., 2007; Fazly and Stevenson, 2006). We also include the analogous set of features for hR. LDA-based Features. We further incorporate features based on a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). Several recent works have underscored the usefulness of using topic models to model a predicate’s selectional preferences (Ritter et al., 2010; Dinu and Lapata, 2010; S´eaghdha, 2010; Lewis and Steedman, 2013; Melamud et al., 2013a). We adopt the approach of Lewis and Steedman (2013), and define a pseudo-document for each LC in the evaluation corpus. We populate the pseudo-documents of an LC with its arguments according to R. We then train an LDA model with 25 topics over these documents. This yields a probability distribution P(topic|h) for each LC h, reflecting the types of arguments h may take. We further include a feature for the entropy of the topic distribution of the predicate, which reflects its heterogeneity. This feature is motivated by the assumption that a heterogeneous predicate is more likely to benefit f</context>
<context position="29782" citStr="Melamud et al., 2013" startWordPosition="4946" endWordPosition="4949">l other words. A Reverb argument is represented as the conjunction of its content words that appear more than 10 times in the corpus. Function words are defined according to their POS tags and include determiners, possessive pronouns, existential “there”, numbers and coordinating conjunctions. Auxiliary verbs and copulas are also considered function words. To compute the LDA features, we use the online variational Bayes algorithm of (Hoffman et al., 2010) as implemented in the Gensim software package (Rehurek and Sojka, 2010). Evaluated Algorithms. The only two previous works on this dataset (Melamud et al., 2013a; Melamud et al., 2013b) are not directly comparable, as they used unsupervised systems and evalu4A script that replicates our train-test partition of the corpus can be found here: http://homepages.inf.ed. ac.uk/oabend/mwpreds.html ated on sub-sets of the evaluation dataset. Instead, we use several baselines to demonstrate the usefulness of integrating multiple LCs, as well as the relative usefulness of our feature sets. The simplest baseline is ALLNEG, which predicts the most frequent label in the dataset (in our case: “no inference”). The other evaluated systems are formed by taking various</context>
<context position="31761" citStr="Melamud et al. (2013" startWordPosition="5266" endWordPosition="5269">ve is the feature set NO-LDA, which includes all features except the LDA features. Experiments with this set were performed in order to isolate the effect of the LDA features. Finally, ALL includes our complete set of features. The more direct comparison is against partial implementations of our system where the LC h is deterministically selected. Determining h for each predicate yields a regular log-linear binary classification model. We use two variants of this baseline. The first, LEFTMOST, selects the left-most content word for each predicate. Similar selection strategy was carried out by Melamud et al. (2013a). The second, VPREP, selects h to be the verb along with its following preposition. In cases the predicate contains multiple verbs, the one preceding the preposition is selected, and where the predicate does not contain any non-copula verbs, it regresses to LEFTMOST. This LC selection method approximates a baseline that includes subcategorized prepositions. Such cases are highly frequent and account for a large portion of the MWPs in English. Including a verb’s preposition in its LC was commonly done in previous work (e.g., Lewis and Steedman, 2013). We also attempted to identify verb-prepos</context>
</contexts>
<marker>Melamud, Dagan, Goldberger, Szpektor, 2013</marker>
<rawString>Oren Melamud, Ido Dagan, Jacob Goldberger, and Idan Szpektor. 2013b. Using lexical expansion to learn inference rules from sparse data. In ACL: Short Papers, pages 283–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shachar Mirkin</author>
<author>Ido Dagan</author>
<author>Maayan Geffet</author>
</authors>
<title>Integrating pattern-based and distributional similarity methods for lexical entailment acquisition.</title>
<date>2006</date>
<booktitle>In COLING-ACL: Poster Session,</booktitle>
<pages>579--586</pages>
<contexts>
<context position="8072" citStr="Mirkin et al. (2006)" startWordPosition="1253" endWordPosition="1256">te inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtained a more accurate set of inference rules. Previous work used simple methods to select the predicate’s LC. Some filtered out frequent highly ambiguous verbs (Lewis and</context>
</contexts>
<marker>Mirkin, Dagan, Geffet, 2006</marker>
<rawString>Shachar Mirkin, Ido Dagan, and Maayan Geffet. 2006. Integrating pattern-based and distributional similarity methods for lexical entailment acquisition. In COLING-ACL: Poster Session, pages 579–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="10692" citStr="Mitchell and Lapata, 2010" startWordPosition="1655" endWordPosition="1658">ional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 Our Proposal: A Latent LC Approach This section details our approach for distributionally representing MWPs by lev</context>
<context position="30607" citStr="Mitchell and Lapata (2010)" startWordPosition="5077" endWordPosition="5081">inf.ed. ac.uk/oabend/mwpreds.html ated on sub-sets of the evaluation dataset. Instead, we use several baselines to demonstrate the usefulness of integrating multiple LCs, as well as the relative usefulness of our feature sets. The simplest baseline is ALLNEG, which predicts the most frequent label in the dataset (in our case: “no inference”). The other evaluated systems are formed by taking various subsets of our feature set. We experiment with 4 feature sets. The smallest set, SIM, includes only the similarity features. This feature set is related to the compositional distributional model of Mitchell and Lapata (2010) (see Section 6). We note that despite recent advances in identifying predicate inference relations, the DIRT system (Lin and Pantel, 2001) remains a strong baseline, and is often used as a component in state-of-the-art systems (Berant et al., 2011), and specifically in the two aforementioned works that used the same evaluation corpus. The next feature set BASIC includes the features found to be most useful during the development of the model: the most frequent POS tag, the frequency features and the feature Common. More inclusive is the feature set NO-LDA, which includes all features except t</context>
<context position="36635" citStr="Mitchell and Lapata, 2010" startWordPosition="6077" endWordPosition="6080">i-word phrases (see Section 2). This line of work focuses on compositional predicates, such as “kick the ball” and not on idiosyncratic predicates such as “kick the bucket”. A variant of the CDS approach can be framed within ours. Assume we wish to compute the similarity of the predicates pL = (w1, ..., wn) and pR = (w01, ..., w0m). Let us denote the vector space representations of the individual words as v1, ..., vn and v01, ..., v0m respectively. A standard approach in CDS is to compose distributional representations by taking their vector sum vL = v1 + v2... + vn and vR = v01 + ... + v0 m (Mitchell and Lapata, 2010). One of the most effective similarity measures is the cosine similarity, which is a normalized dot product. The distributional similarity between pL and pR under this model is sim(pL, pR) = �n �m j=1 sim(wi,w0 j), where i=1 sim(wi, w0j) is the dot product between vi and v0j. This similarity score is similar in spirit to a simplified version of our statistical model that restricts the set of allowable LCs Hp to be {({wi}, {w0j})|i ≤ n, j ≤ m}, i.e., only LCs of size 1. Indeed, taking Hp as above, and cosine similarity as the only feature (i.e., w E ][R), yields the distribution 651 Algorithm T</context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nocedal</author>
<author>Stephen J Wright</author>
</authors>
<date>1999</date>
<journal>Numerical optimization,</journal>
<volume>2</volume>
<publisher>Springer</publisher>
<location>New York.</location>
<contexts>
<context position="14548" citStr="Nocedal and Wright, 1999" startWordPosition="2321" endWordPosition="2325">me integer d. We denote the training set by D. 3.2 The Model We address the task with a latent variable loglinear model, representing the LCs of the predicates. We choose this model for its generality, conceptual simplicity, and because it allows to easily incorporate various feature sets and sets of latent variables. We introduce L2 regularization to avoid over-fitting. We use maximum likelihood estimation, and arrive at the following objective function: L(wJD) = 1 log P(y(i)lp(i), w) − A2 l1wl12 = ME i=1 Z(w, i) = E E exp(wTΦ(pi, y, h)). yET−1,11 hEHi We maximize L using the BFGS algorithm (Nocedal and Wright, 1999). The gradient (with respect to w) is the following: OL = Eh[Φ(pi, yi, h)] − Eh,y[Φ(pi, y, h)] − A · w Hp can be defined to be any sub-set of 2W(p) given that taking an expectation over H can be done efficiently. It is therefore possible to use prior linguistic knowledge to consider only sub-sets of p that are likely to be non-compositional (e.g., verbpreposition or verb-noun pairs). In our experiments we attempt to keep the approach maximally general, and define Hp to be the set of all subsets of size 1 or 2 of content words in Wp1. We bound the size of h E Hp in order to retain computational</context>
</contexts>
<marker>Nocedal, Wright, 1999</marker>
<rawString>Jorge. Nocedal and Stephen J Wright. 1999. Numerical optimization, volume 2. Springer New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Pichotta</author>
<author>John DeNero</author>
</authors>
<title>Identifying phrasal verbs using many bilingual corpora.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>636--646</pages>
<contexts>
<context position="10267" citStr="Pichotta and DeNero, 2013" startWordPosition="1595" endWordPosition="1598">tt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWE</context>
</contexts>
<marker>Pichotta, DeNero, 2013</marker>
<rawString>Karl Pichotta and John DeNero. 2013. Identifying phrasal verbs using many bilingual corpora. In EMNLP, pages 636–646.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Ramisch</author>
<author>Aline Villavicencio</author>
<author>Valia Kordoni</author>
</authors>
<title>Introduction to the special issue on multiword expressions: From theory to practice and use.</title>
<date>2013</date>
<journal>ACM Transactions on Speech and Language Processing (TSLP),</journal>
<volume>10</volume>
<issue>2</issue>
<contexts>
<context position="9991" citStr="Ramisch et al., 2013" startWordPosition="1551" endWordPosition="1554">cal and applicative contexts. Their position on the crossroads of syntax and the lexicon, their varying degrees of compositionality, as well as the wealth of linguistic phenomena they exhibit, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite p</context>
</contexts>
<marker>Ramisch, Villavicencio, Kordoni, 2013</marker>
<rawString>Carlos Ramisch, Aline Villavicencio, and Valia Kordoni. 2013. Introduction to the special issue on multiword expressions: From theory to practice and use. ACM Transactions on Speech and Language Processing (TSLP), 10(2):3.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deepak Ravichandran</author>
<author>Eduard Hovy</author>
</authors>
<title>Learning surface text patterns for a question answering system.</title>
<date>2002</date>
<booktitle>In ACL,</booktitle>
<pages>41--47</pages>
<contexts>
<context position="6753" citStr="Ravichandran and Hovy, 2002" startWordPosition="1056" endWordPosition="1059">t cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (Melamud et al., 2013a), and is often used as a component within larger systems. Schoenmackers et al. (2010) presented an unsupervised system for learning inference rules direct</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>Deepak Ravichandran and Eduard Hovy. 2002. Learning surface text patterns for a question answering system. In ACL, pages 41–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>An empirical study on compositionality in compound nouns.</title>
<date>2011</date>
<booktitle>In IJCNLP,</booktitle>
<pages>210--218</pages>
<contexts>
<context position="10913" citStr="Reddy et al., 2011" startWordPosition="1688" endWordPosition="1691">rm treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 Our Proposal: A Latent LC Approach This section details our approach for distributionally representing MWPs by leveraging their component LCs. Section 3.1 describes our general approach, Section 3.2 presents our model and Section 3.3 details the feature set. 3.1 General Approach and Notation We propose a method for addressing MWPs of</context>
</contexts>
<marker>Reddy, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An empirical study on compositionality in compound nouns. In IJCNLP, pages 210–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radim Rehurek</author>
<author>Petr Sojka</author>
</authors>
<title>Software framework for topic modelling with large corpora.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC</booktitle>
<pages>46--50</pages>
<contexts>
<context position="29693" citStr="Rehurek and Sojka, 2010" startWordPosition="4932" endWordPosition="4935">ouns, verbs, adjectives, adverbs, prepositions, the word “to” and a category that includes all other words. A Reverb argument is represented as the conjunction of its content words that appear more than 10 times in the corpus. Function words are defined according to their POS tags and include determiners, possessive pronouns, existential “there”, numbers and coordinating conjunctions. Auxiliary verbs and copulas are also considered function words. To compute the LDA features, we use the online variational Bayes algorithm of (Hoffman et al., 2010) as implemented in the Gensim software package (Rehurek and Sojka, 2010). Evaluated Algorithms. The only two previous works on this dataset (Melamud et al., 2013a; Melamud et al., 2013b) are not directly comparable, as they used unsupervised systems and evalu4A script that replicates our train-test partition of the corpus can be found here: http://homepages.inf.ed. ac.uk/oabend/mwpreds.html ated on sub-sets of the evaluation dataset. Instead, we use several baselines to demonstrate the usefulness of integrating multiple LCs, as well as the relative usefulness of our feature sets. The simplest baseline is ALLNEG, which predicts the most frequent label in the datase</context>
</contexts>
<marker>Rehurek, Sojka, 2010</marker>
<rawString>Radim Rehurek and Petr Sojka. 2010. Software framework for topic modelling with large corpora. In Proceedings of LREC 2010 workshop New Challenges for NLP Frameworks, pages 46–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>A latent Dirichlet allocation method for selectional preferences. In</title>
<date>2010</date>
<booktitle>ACL,</booktitle>
<pages>424--434</pages>
<contexts>
<context position="23904" citStr="Ritter et al., 2010" startWordPosition="3970" endWordPosition="3973"> statistical correlation between the words of hL. To do so, we use point-wise mutual information, and the conditional probabilities P(hAL|hBL) and P(hBL|hAL). Similar measures have often been used for the unsupervised detection of MWEs (Villavicencio et al., 2007; Fazly and Stevenson, 2006). We also include the analogous set of features for hR. LDA-based Features. We further incorporate features based on a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). Several recent works have underscored the usefulness of using topic models to model a predicate’s selectional preferences (Ritter et al., 2010; Dinu and Lapata, 2010; S´eaghdha, 2010; Lewis and Steedman, 2013; Melamud et al., 2013a). We adopt the approach of Lewis and Steedman (2013), and define a pseudo-document for each LC in the evaluation corpus. We populate the pseudo-documents of an LC with its arguments according to R. We then train an LDA model with 25 topics over these documents. This yields a probability distribution P(topic|h) for each LC h, reflecting the types of arguments h may take. We further include a feature for the entropy of the topic distribution of the predicate, which reflects its heterogeneity. This feature i</context>
</contexts>
<marker>Ritter, Mausam, Etzioni, 2010</marker>
<rawString>Alan Ritter, Mausam, and Oren Etzioni. 2010. A latent Dirichlet allocation method for selectional preferences. In ACL, pages 424–434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan A Sag</author>
<author>Timothy Baldwin</author>
<author>Francis Bond</author>
<author>Ann Copestake</author>
<author>Dan Flickinger</author>
</authors>
<title>Multiword expressions: A pain in the neck for NLP. In CICLing,</title>
<date>2001</date>
<pages>1--15</pages>
<contexts>
<context position="1586" citStr="Sag et al., 2001" startWordPosition="223" endWordPosition="226"> relations, we compare against state-of-the-art baselines that consider a single sub-set of an MWP, obtaining substantial improvements. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics. 1 Introduction Multi-word expressions (MWEs) constitute a large part of the lexicon and account for much of its growth (Jackendoff, 2002; Seaton and Macaulay, 2002). However, despite their importance, MWEs remain difficult to define and model, and consequently pose serious difficulties for NLP applications (Sag et al., 2001). Multi-word Predicates (MWPs; sometimes termed Complex Predicates) form an important and much addressed subclass of MWEs and are the focus of this paper. MWPs are informally defined as multiple words that constitute a single predicate (Alsina et al., 1997). MWPs encompass a wide range of phenomena, including causatives, light verbs, phrasal verbs, serial verb constructions and many others, and pose considerable challenges to both linguistic theory and NLP applications (see Section 2). Part of the difficulty in treating them stems from their position on the borderline between syntax and the le</context>
</contexts>
<marker>Sag, Baldwin, Bond, Copestake, Flickinger, 2001</marker>
<rawString>Ivan A. Sag, Timothy Baldwin, Francis Bond, Ann Copestake, and Dan Flickinger. 2001. Multiword expressions: A pain in the neck for NLP. In CICLing, pages 1–15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Schoenmackers</author>
<author>Oren Etzioni</author>
<author>Daniel S Weld</author>
<author>Jesse Davis</author>
</authors>
<title>Learning first-order Horn clauses from web text.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>1088--1098</pages>
<contexts>
<context position="4149" citStr="Schoenmackers et al., 2010" startWordPosition="620" endWordPosition="623">e conjoined with its object (e.g., “take care”, “take a bus”). This approach, however, raises the challenge of identifying the sub-set of the predicate’s words that should be taken to represent it (henceforth, its lexical components or LCs). We propose a novel approach that addresses this 644 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 644–654, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation PL → PR is said to hold if the relation denoted by PR generally holds between a set of arguments whenever the relation PL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. Wh</context>
<context position="6577" citStr="Schoenmackers et al., 2010" startWordPosition="1026" endWordPosition="1029">g stateof-the-art feature sets. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (</context>
</contexts>
<marker>Schoenmackers, Etzioni, Weld, Davis, 2010</marker>
<rawString>Stefan Schoenmackers, Oren Etzioni, Daniel S Weld, and Jesse Davis. 2010. Learning first-order Horn clauses from web text. In EMNLP, pages 1088– 1098.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diarmuid ´O S´eaghdha</author>
</authors>
<title>Latent variable models of selectional preference.</title>
<date>2010</date>
<booktitle>In ACL 2010,</booktitle>
<pages>435--444</pages>
<marker>S´eaghdha, 2010</marker>
<rawString>Diarmuid ´O. S´eaghdha. 2010. Latent variable models of selectional preference. In ACL 2010, pages 435– 444.</rawString>
</citation>
<citation valid="true">
<date>2002</date>
<booktitle>Collins COBUILD Idioms Dictionary. HarperCollins Publishers, 2nd edition.</booktitle>
<editor>Maggie Seaton and Alison Macaulay, editors.</editor>
<marker>2002</marker>
<rawString>Maggie Seaton and Alison Macaulay, editors. 2002. Collins COBUILD Idioms Dictionary. HarperCollins Publishers, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Satoshi Sekine</author>
</authors>
<title>Automatic paraphrase discovery based on context and keywords between NE pairs.</title>
<date>2005</date>
<booktitle>In IWP,</booktitle>
<pages>4--6</pages>
<contexts>
<context position="6526" citStr="Sekine, 2005" startWordPosition="1019" endWordPosition="1020">ous ones that select a fixed LC, using stateof-the-art feature sets. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The syste</context>
</contexts>
<marker>Sekine, 2005</marker>
<rawString>Satoshi Sekine. 2005. Automatic paraphrase discovery based on context and keywords between NE pairs. In IWP, pages 4–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yusuke Shinyama</author>
<author>Satoshi Sekine</author>
</authors>
<title>Preemptive information extraction using unrestricted relation discovery.</title>
<date>2006</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>304--311</pages>
<contexts>
<context position="6805" citStr="Shinyama and Sekine, 2006" startWordPosition="1062" endWordPosition="1065">s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a state-of-the-art baseline (Melamud et al., 2013a), and is often used as a component within larger systems. Schoenmackers et al. (2010) presented an unsupervised system for learning inference rules directly from open-domain web data. Melamud et al. (2013a)</context>
</contexts>
<marker>Shinyama, Sekine, 2006</marker>
<rawString>Yusuke Shinyama and Satoshi Sekine. 2006. Preemptive information extraction using unrestricted relation discovery. In HLT-NAACL, pages 304–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning entailment rules for unary templates. In</title>
<date>2008</date>
<booktitle>COLING,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="4591" citStr="Szpektor and Dagan, 2008" startWordPosition="698" endWordPosition="701">014 Association for Computational Linguistics challenge in the context of identifying lexical inference relations between predicates (Lin and Pantel, 2001; Schoenmackers et al., 2010; Melamud et al., 2013a, inter alia). A (lexical) inference relation PL → PR is said to hold if the relation denoted by PR generally holds between a set of arguments whenever the relation PL does. For instance, an inference relation holds between “annex” and “control” since if a country annexes another, it generally controls it. Most works to this task use distributional similarity, either as their main component (Szpektor and Dagan, 2008; Melamud et al., 2013b), or as part of a more comprehensive system (Berant et al., 2011; Lewis and Steedman, 2013). For example, consider the verb “take”. While the inference relation “have → take” does not generally hold, it does hold in the case of some light verbs, such as “have a look → take a look”, underscoring the importance of taking more inclusive LCs into account. On the other hand, the predicate “likely to give a green light” is unlikely to appear often even within a very large corpus, and could benefit from taking its lexical sub-units (e.g., “likely” or “give a green light”) into</context>
<context position="20961" citStr="Szpektor and Dagan, 2008" startWordPosition="3463" endWordPosition="3466">e score defines for each predicate p and for each argument slot s E {L, R} (corresponding to the arguments to the right and left of that predicate) a vector vps which represents the distribution of arguments appearing in that slot. We take vps(x) to be the number of times that the argument x appeared in the slot s of the predicate p. Given these vectors, the similarity between the predicates p1 and p2 is defined as: Vscore(p1, p2) = sim(vp1L , vp2L ) · sim(vp1R , vp2R ) where situ is some vector similarity measure. We use two common similarity measures: the vector cosine metric, and the BInc (Szpektor and Dagan, 2008) similarity measure. These measures give complementary perspectives on the similarity between the predicates, as the cosine similarity is symmetric between the LHS and RHS predicates, while BInc takes into account the directionality of the inference relation. Preliminary experiments with other measures, such as those of Lin (1998) and Weeds and Weir (2003) did not yield additional improvements. We encode the similarity of all measures for the pair hL and hR as well as the pair hAL and hAR. The latter feature is an approximation to the similarity between the heads of the predicates, as heads in</context>
</contexts>
<marker>Szpektor, Dagan, 2008</marker>
<rawString>Idan Szpektor and Ido Dagan. 2008. Learning entailment rules for unary templates. In COLING, pages 849–856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuancheng Tu</author>
<author>Dan Roth</author>
</authors>
<title>Learning English light verb constructions: contextual or statistical.</title>
<date>2011</date>
<booktitle>In ACL HLT 2011,</booktitle>
<pages>31</pages>
<contexts>
<context position="10176" citStr="Tu and Roth, 2011" startWordPosition="1580" endWordPosition="1583">it, made them the object of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used composi</context>
<context position="23028" citStr="Tu and Roth, 2011" startWordPosition="3825" endWordPosition="3828">ears in both predicates, while “come” and “leave” appear only in one of them. In addition, we use POS-based features that encode the most frequent POS tag for the word lemma and the second most frequent POS tag (according to R). Information about the second most frequent POS tag can be important in identifying light verb constructions, such as “take a swim” or “give a smile”, where the object is derived from a verb. It can thus be interpreted as a generalization 648 of the feature that indicates whether the object is a deverbal noun, which is used by some light verb identification algorithms (Tu and Roth, 2011). In cases where hL is of size 2, we additionally encode features that apply to the conjunction of hAL and hBL. We encode the conjunction of their POS and the number of times the two lemmas occurred together in R. We also introduce features that capture the statistical correlation between the words of hL. To do so, we use point-wise mutual information, and the conditional probabilities P(hAL|hBL) and P(hBL|hAL). Similar measures have often been used for the unsupervised detection of MWEs (Villavicencio et al., 2007; Fazly and Stevenson, 2006). We also include the analogous set of features for </context>
</contexts>
<marker>Tu, Roth, 2011</marker>
<rawString>Yuancheng Tu and Dan Roth. 2011. Learning English light verb constructions: contextual or statistical. In ACL HLT 2011, page 31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of artificial intelligence research,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="2480" citStr="Turney and Pantel, 2010" startWordPosition="363" endWordPosition="367">ass a wide range of phenomena, including causatives, light verbs, phrasal verbs, serial verb constructions and many others, and pose considerable challenges to both linguistic theory and NLP applications (see Section 2). Part of the difficulty in treating them stems from their position on the borderline between syntax and the lexicon. It is therefore often unclear whether they should be treated as fixed expressions, as compositional phrases that reflect the properties of their component parts or as both. This work addresses the modelling of MWPs within the context of distributional semantics (Turney and Pantel, 2010), in which predicates are represented through the distribution of arguments they may take. In order to collect meaningful statistics, the predicate’s lexical unit should be sufficiently frequent and semantically unambiguous. MWPs pose a challenge to such models, as naively collecting statistics over all instances of highly ambiguous verbs is likely to result in noisy representations. For instance, the verb “take” may appear in MWPs as varied as “take time”, “take effect” and “take to the hills”. This heterogeneity of “take” is likely to have a negative effect on downstream systems that use its</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37(1):141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aline Villavicencio</author>
<author>Valia Kordoni</author>
<author>Yi Zhang</author>
<author>Marco Idiart</author>
<author>Carlos Ramisch</author>
</authors>
<title>Validation and evaluation of automatically acquired multiword expressions for grammar engineering. In EMNLPCoNLL,</title>
<date>2007</date>
<pages>1034--1043</pages>
<contexts>
<context position="23548" citStr="Villavicencio et al., 2007" startWordPosition="3914" endWordPosition="3917"> object is a deverbal noun, which is used by some light verb identification algorithms (Tu and Roth, 2011). In cases where hL is of size 2, we additionally encode features that apply to the conjunction of hAL and hBL. We encode the conjunction of their POS and the number of times the two lemmas occurred together in R. We also introduce features that capture the statistical correlation between the words of hL. To do so, we use point-wise mutual information, and the conditional probabilities P(hAL|hBL) and P(hBL|hAL). Similar measures have often been used for the unsupervised detection of MWEs (Villavicencio et al., 2007; Fazly and Stevenson, 2006). We also include the analogous set of features for hR. LDA-based Features. We further incorporate features based on a Latent Dirichlet Allocation (LDA) topic model (Blei et al., 2003). Several recent works have underscored the usefulness of using topic models to model a predicate’s selectional preferences (Ritter et al., 2010; Dinu and Lapata, 2010; S´eaghdha, 2010; Lewis and Steedman, 2013; Melamud et al., 2013a). We adopt the approach of Lewis and Steedman (2013), and define a pseudo-document for each LC in the evaluation corpus. We populate the pseudo-documents </context>
</contexts>
<marker>Villavicencio, Kordoni, Zhang, Idiart, Ramisch, 2007</marker>
<rawString>Aline Villavicencio, Valia Kordoni, Yi Zhang, Marco Idiart, and Carlos Ramisch. 2007. Validation and evaluation of automatically acquired multiword expressions for grammar engineering. In EMNLPCoNLL, pages 1034–1043.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>Istv´an Nagy T</author>
<author>Rich´ard Farkas</author>
</authors>
<title>Identifying English and Hungarian light verb constructions: A contrastive approach.</title>
<date>2013</date>
<booktitle>In ACL: Short Papers,</booktitle>
<pages>255--261</pages>
<contexts>
<context position="10198" citStr="Vincze et al., 2013" startWordPosition="1584" endWordPosition="1587">bject of ongoing linguistic discussion (Alsina et al., 1997; Butt, 2010). In NLP, the discovery and identification of MWEs in general and MWPs in particular has been the focus of much work over the years (Lin, 1999; Baldwin et al., 2003; Biemann and Giesbrecht, 2011). Despite wide interest, the field has yet to converge to a general and widely agreed-upon method for identifying MWPs. See (Ramisch et al., 2013) for an overview. Most work on MWEs emphasized idiosyncratic or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional </context>
</contexts>
<marker>Vincze, T, Farkas, 2013</marker>
<rawString>Veronika Vincze, Istv´an Nagy T., and Rich´ard Farkas. 2013. Identifying English and Hungarian light verb constructions: A contrastive approach. In ACL: Short Papers, pages 255–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
</authors>
<title>A general framework for distributional similarity. In</title>
<date>2003</date>
<booktitle>EMNLP,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="21319" citStr="Weeds and Weir (2003)" startWordPosition="3520" endWordPosition="3523">imilarity between the predicates p1 and p2 is defined as: Vscore(p1, p2) = sim(vp1L , vp2L ) · sim(vp1R , vp2R ) where situ is some vector similarity measure. We use two common similarity measures: the vector cosine metric, and the BInc (Szpektor and Dagan, 2008) similarity measure. These measures give complementary perspectives on the similarity between the predicates, as the cosine similarity is symmetric between the LHS and RHS predicates, while BInc takes into account the directionality of the inference relation. Preliminary experiments with other measures, such as those of Lin (1998) and Weeds and Weir (2003) did not yield additional improvements. We encode the similarity of all measures for the pair hL and hR as well as the pair hAL and hAR. The latter feature is an approximation to the similarity between the heads of the predicates, as heads in English tend to be to the left of the predicates. These two features coincide for h values of size 1. Word and Pair Features. These features encode the basic properties of the LC. The motivation behind them is to allow a more accurate leveraging of the similarity features, as well as to better determine the relative weights of h E H(i). The feature set is</context>
</contexts>
<marker>Weeds, Weir, 2003</marker>
<rawString>Julie Weeds and David Weir. 2003. A general framework for distributional similarity. In EMNLP, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hila Weisman</author>
<author>Jonathan Berant</author>
<author>Idan Szpektor</author>
<author>Ido Dagan</author>
</authors>
<title>Learning verb inference rules from linguistically-motivated evidence. In EMNLPCoNLL,</title>
<date>2012</date>
<pages>194--204</pages>
<contexts>
<context position="7906" citStr="Weisman et al. (2012)" startWordPosition="1227" endWordPosition="1230">presented an unsupervised system for learning inference rules directly from open-domain web data. Melamud et al. (2013a) used topic models to combine typelevel predicate inference rules with token-level information from their arguments in a specific context. Melamud et al. (2013b) used lexical expansion to improve the representation of infrequent predicates. Lewis and Steedman (2013) combined distributional and symbolic representations, evaluating on a Question Answering task, as well as on a quantification-focused entailment dataset. Several studies tackled the task using supervised systems. Weisman et al. (2012) used a set of linguistically motivated features, but evaluated their system on a corpus that consists almost entirely of single-word predicates. Mirkin et al. (2006) presented a system for learning inference rules between nouns, using distributional similarity and pattern-based features. Hagiwara et al. (2009) identified synonyms using a supervised approach relying on distributional and syntactic features. Berant et al. (2011) used distributional similarity between predicates to weight the edges of an entailment graph. By imposing global constraints on the structure of the graph, they obtaine</context>
</contexts>
<marker>Weisman, Berant, Szpektor, Dagan, 2012</marker>
<rawString>Hila Weisman, Jonathan Berant, Idan Szpektor, and Ido Dagan. 2012. Learning verb inference rules from linguistically-motivated evidence. In EMNLPCoNLL, pages 194–204.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominic Widdows</author>
</authors>
<title>Semantic vector products: Some initial investigations.</title>
<date>2008</date>
<booktitle>In Second AAAI Symposium on Quantum Interaction,</booktitle>
<volume>26</volume>
<pages>28--35</pages>
<contexts>
<context position="10665" citStr="Widdows, 2008" startWordPosition="1653" endWordPosition="1654">or non-compositional expressions. Other lines of work focused on specific MWP classes such as light verbs (Tu and Roth, 2011; Vincze et al., 2013) and phrasal verbs (McCarthy et al., 2003; Pichotta and DeNero, 2013). Our work proposes a uniform treatment to MWPs of varying degrees of compositionality, and avoids defining MWPs explicitly by modelling their LCs as latent variables. Compositional Distributional Semantics. Much work in recent years has concentrated on the relation between the distributional representations of composite phrases and the representations of their component sub-parts (Widdows, 2008; Mitchell and Lapata, 2010; Baroni and Zamparelli, 2010; Coecke et al., 2010). Several works have used compositional distributional semantics (CDS) representations to assess the compositionality of MWEs, such as noun compounds (Reddy et al., 2011) or verb-noun combinations (Kiela and Clark, 2013). Despite significant advances, previous work has mostly been concerned with highly compositional cases and does not address the distributional representation of predicates of varying degrees of compositionality. 3 Our Proposal: A Latent LC Approach This section details our approach for distributional</context>
</contexts>
<marker>Widdows, 2008</marker>
<rawString>Dominic Widdows. 2008. Semantic vector products: Some initial investigations. In Second AAAI Symposium on Quantum Interaction, volume 26, pages 28–35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alison Wray</author>
</authors>
<title>Formulaic language: Pushing the boundaries.</title>
<date>2008</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="6354" citStr="Wray, 2008" startWordPosition="993" endWordPosition="994">of varying degrees of compositionality within distributional semantics. We conduct experiments on the dataset of Zeichner et al. (2012) and compare our methods with analogous ones that select a fixed LC, using stateof-the-art feature sets. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional si</context>
</contexts>
<marker>Wray, 2008</marker>
<rawString>Alison Wray. 2008. Formulaic language: Pushing the boundaries. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Massimo Zanzotto</author>
<author>Marco Pennacchiotti</author>
<author>Maria Teresa Pazienza</author>
</authors>
<title>Discovering asymmetric entailment relations between verbs using selectional preferences.</title>
<date>2006</date>
<booktitle>In ACL-COLING,</booktitle>
<pages>849--856</pages>
<contexts>
<context position="6549" citStr="Zanzotto et al., 2006" startWordPosition="1021" endWordPosition="1025">select a fixed LC, using stateof-the-art feature sets. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a central task over the past few years (Sekine, 2005; Zanzotto et al., 2006; Schoenmackers et al., 2010; Berant et al., 2011; Melamud et al., 2013a, inter alia). Inference rules are used in a wide variety of applications including Question Answering (Ravichandran and Hovy, 2002), Information Extraction (Shinyama and Sekine, 2006), and as a main component in Textual Entailment systems (Dinu and Wang, 2009; Dagan et al., 2013). Most approaches to the task used distributional similarity as a major component within their system. Lin and Pantel (2001) introduced DIRT, an unsupervised distributional system for detecting inference relations. The system is still considered a</context>
</contexts>
<marker>Zanzotto, Pennacchiotti, Pazienza, 2006</marker>
<rawString>Fabio Massimo Zanzotto, Marco Pennacchiotti, and Maria Teresa Pazienza. 2006. Discovering asymmetric entailment relations between verbs using selectional preferences. In ACL-COLING, pages 849– 856.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naomi Zeichner</author>
<author>Jonathan Berant</author>
<author>Ido Dagan</author>
</authors>
<title>Crowdsourcing inference-rule evaluation.</title>
<date>2012</date>
<booktitle>In ACL: Short Papers,</booktitle>
<pages>156--160</pages>
<contexts>
<context position="5878" citStr="Zeichner et al. (2012)" startWordPosition="918" endWordPosition="922"> selection and relative weighting of the predicate’s LCs using latent variables. This approach allows the classifier that uses the distributional representations to take into account the most relevant LCs in order to make the prediction. By doing so, we avoid the notoriously difficult problem of defining and identifying MWPs and account for predicates of various sizes and degrees of compositionality. To our knowledge, this is the first work to address lexical relations between MWPs of varying degrees of compositionality within distributional semantics. We conduct experiments on the dataset of Zeichner et al. (2012) and compare our methods with analogous ones that select a fixed LC, using stateof-the-art feature sets. Our method obtains substantial performance gains across all scenarios. Finally, we note that our approach is cognitively appealing. Significant cognitive findings support the claim that a speaker’s lexicon consists of partially overlapping lexical units of various sizes, of which several can be evoked in the interpretation of an utterance (Jackendoff, 2002; Wray, 2008). 2 Background and Related Work Inference Relations. The detection of inference relations between predicates has become a ce</context>
<context position="26796" citStr="Zeichner et al. (2012)" startWordPosition="4457" endWordPosition="4460">ion, between which there may nouns, adjectives, adverbs, pronouns, determiners and verbs. The verb may also be a copula. Examples of predicates are “make the most of”, “could be exchanged for” and “is happy with”. Reverb is an appealing reference corpus for this task for several reasons. First, it uses fairly shallow preprocessing technology which is available for many domains and languages. Second, Reverb applies considerable noise filtering, which results in extractions of fair quality. Third, our evaluation dataset is based on Reverb extractions. We evaluate our algorithm on the dataset of Zeichner et al. (2012). This publicly available corpus3 provides pairs of Reverb binary relations and an indication of whether an inference relation holds between them within the context of a specific pair of argument fillers. The corpus was compiled using distributional methods to detect pairs of relations in Reverb that are likely to have an inference relation between. Annotators, employed through Amazon Mechanical Turk, were then asked to determine whether each pair is meaningful, and if so, to determine whether an inference relation holds. Further measures were taken to monitor the accuracy of the annotation. F</context>
</contexts>
<marker>Zeichner, Berant, Dagan, 2012</marker>
<rawString>Naomi Zeichner, Jonathan Berant, and Ido Dagan. 2012. Crowdsourcing inference-rule evaluation. In ACL: Short Papers, pages 156–160.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>