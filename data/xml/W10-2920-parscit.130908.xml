<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.994957">
Cross-caption coreference resolution for automatic image understanding
</title>
<author confidence="0.998268">
Micah Hodosh Peter Young Cyrus Rashtchian Julia Hockenmaier
</author>
<affiliation confidence="0.999147">
Department of Computer Science
University of Illinois at Urbana-Champaign
</affiliation>
<email confidence="0.98602">
{mhodosh2, pyoung2, crashtc2, juliahmr}@illinois.edu
</email>
<sectionHeader confidence="0.982256" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999911611111111">
Recent work in computer vision has aimed
to associate image regions with keywords
describing the depicted entities, but ac-
tual image ‘understanding’ would also re-
quire identifying their attributes, relations
and activities. Since this information can-
not be conveyed by simple keywords, we
have collected a corpus of “action” photos
each associated with five descriptive cap-
tions. In order to obtain a consistent se-
mantic representation for each image, we
need to first identify which NPs refer to
the same entities. We present three hierar-
chical Bayesian models for cross-caption
coreference resolution. We have also cre-
ated a simple ontology of entity classes
that appear in images and evaluate how
well these can be recovered.
</bodyText>
<sectionHeader confidence="0.995161" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999974524590164">
Many photos capture a moment in time, telling a
brief story of people, animals and objects, their at-
tributes, and their relationship to each other. Al-
though different people may give different inter-
pretations to the same picture, people can read-
ily interpret photos and describe the entities and
events they perceive in complex sentences. This
level of image understanding still remains an elu-
sive goal for computer vision: although current
methods may be able to identify the overall scene
(Quattoni and Torralba, 2009) or some specific
classes of entities (Felzenszwalb et al., 2008), they
are only starting to be able to identify attributes
of entities (Farhadi et al., 2009), and are far from
recovering a complete semantic interpretation of
the depicted situation. Like natural language pro-
cessing, computer vision requires suitable training
data, and there are currently no publicly available
data sets that would enable the development of
such systems.
Photo sharing sites such as Flickr allow users
to annotate images with keywords and other de-
scriptions, and vision researchers have access to
large collections of images annotated with key-
words (e.g. the Corel collection). A lot of recent
work in computer vision has been aimed at pre-
dicting these keywords (Blei et al., 2003; Barnard
et al., 2003; Feng and Lapata, 2008; Deschacht
and Moens, 2007; Jeon et al., 2003). But key-
words alone are not expressive enough to capture
relations between entities. Some research has used
the text that surrounds an image in a news arti-
cle as a proxy (Feng and Lapata, 2008; Deschacht
and Moens, 2007). However, in many cases, the
surrounding text or a user-provided caption does
not simply describe what is depicted in the image
(since this is usually obvious to the human reader
for which this text is intended), but provides ad-
ditional information. We have collected a corpus
of 8108 images associated with several simple de-
scriptive captions. In contrast to the text near an
image on the web, the captions in our corpus pro-
vide direct, if partial and slightly noisy, descrip-
tions of the image content. Our data set differs
from paraphrase corpora (Barzilay and McKeown,
2001; Dolan et al., 2004) in that the different cap-
tions of an image are produced independently by
different writers. There are many ways of describ-
ing the same image, because it is often possible
to focus on different aspects of the depicted situ-
ation, and because certain aspects of the situation
may be unclear to the human viewer.
One of our goals is to use these captions to
obtain a semantic representation of each image
that is consistent with all of its captions. In or-
der to obtain such a representation, it is neces-
sary to identify the entities that appear in the im-
age, and to perform cross-caption coreference res-
olution, i.e. to identify all mentions of the same
entity in the five captions associated with an im-
age. In this paper, we compare different meth-
</bodyText>
<note confidence="0.916162125">
162
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 162–171,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
A golden retriever (ANIMAL) is playing with a smaller black and brown dog(ANIMAL) in a pink collar (CLOTHING).
A smaller black dog (ANIMAL) is fighting with a larger brown dog (ANIMAL) in a forest (NAT_BACKGROUND).
A smaller black and brown dog (ANIMAL) is jumping on a large orange dog (ANIMAL).
Brown dog (ANIMAL) with mouth (BODY_PART) open near head(BODY_PART) of black and tan dog (ANIMAL).
Two dogs (ANIMAL) playing near the woods (NAT_BACKGROUND).
</note>
<figureCaption confidence="0.8951525">
Figure 1: An image with five captions from our corpus. Coreference chains and ontological classes are
indicated in color.
</figureCaption>
<bodyText confidence="0.9996648">
ods of cross-caption coreference resolution on our
corpus. In order to facilitate further computer vi-
sion research, we have also defined a set of coarse-
grained ontological classes that we use to automat-
ically categorize the entities in our data set.
</bodyText>
<sectionHeader confidence="0.797919" genericHeader="introduction">
2 A corpus of action images and captions
</sectionHeader>
<bodyText confidence="0.999040321428571">
Image collection and sentence annotation We
have constructed a corpus consisting of 8108 pho-
tographs from Flickr.com, each paired with five
one-sentence descriptive captions written by Ama-
zon’s Mechanical Turk1 workers. We downloaded
a few thousand images from each of six selected
Flickr groups2. To facilitate future computer vi-
sion research on our data, we filtered out images in
black-and-white or sepia, as well as images with
watermarks, signatures, borders or other obvious
editing. Since our collection focuses on images
depicting actions, we then filtered out images of
scenery, portraits, and mood photography. This
was done independently by two members of our
group and adjudicated by a third.
We paid Turk workers $0.10 to write one de-
scriptive sentence for each of five distinct and ran-
domly chosen images that were displayed one at
a time. We required a small qualification test
that examined the workers’ English grammar and
spelling and we restricted the task to U.S. work-
ers (see Rashtchian et al. (2010) for more details).
Our final corpus contains five sentences for each
of our 8108 images, totaling 478,317 word tokens,
and an average sentence length of 11.8 words.
We first spell-checked3 these sentences, and used
OpenNLP4 to POS-tag them. We identified NPs
using OpenNLP’s chunker, followed by a semi-
</bodyText>
<footnote confidence="0.99872825">
1https://www.mturk.com
2The groups:“strangers!”, “Wild-Child (Kids in Action)”,
“Dogs in Action (Read the Rules)”, “Outdoor Activities”,
“Action Photography”, “Flickr-Social (two or more people in
the photo)”.
3We used Unix’s aspell to generate possible correc-
tions and chose between them based on corpus frequencies.
4http://opennlp.sourceforge.net
</footnote>
<bodyText confidence="0.999487735294117">
automatic procedure to correct for a number of
systematic chunking errors that could easily be
corrected. We randomly selected 200 images for
further manual annotation, to be used as test and
development data in our experiments.
Gold standard coreference annotation We
manually annotated NP chunks, ontological
classes, and cross-caption coreference chains for
each of the 200 images in our test and development
data. Each image was annotated independently by
two annotators and adjudicated by a third.5 The
development set contains 1604 mentions. On av-
erage, each caption has 3.2 mentions, and each im-
age has 5.9 coreference chains (distinct entities).
Ontological annotation of entities In order to
understand the role entities mentioned in the sen-
tences play in the image, we have defined a simple
ontology of entity classes (Table 1). We distin-
guish entities that constitute the background of an
image from those that appear in the foreground.
These entities can be animate (people or animals)
or inanimate. For inanimate objects, we distin-
guish static objects from “movable” objects. We
also distinguish man-made from natural objects
and backgrounds, since this matters for computer
vision algorithms. We have labeled the entity
mentions in our test and development data with
classes from this ontology. Again, two of us an-
notated each image’s mentions, and adjudication
was performed by a single person. Our ontology
is similar to, but smaller than the one proposed
by Hollink and Worring (2005) for video retrieval,
which in turn is based on Hoogs et al. (2003) and
Hunter (2001).
</bodyText>
<sectionHeader confidence="0.811441" genericHeader="method">
3 Predicting image entities from captions
</sectionHeader>
<bodyText confidence="0.988043">
Figure 1 shows an image from our corpus. Dif-
ferent captions use different words to refer to the
</bodyText>
<footnote confidence="0.5486205">
5We used MMAX2 (M¨uller and Strube, 2006) both for
annotation and adjudication.
</footnote>
<page confidence="0.84021">
163
</page>
<table confidence="0.999410238095238">
Ontological Class Examples
animal dog, horse, cow
background man-made street, pool, carpet
background natural ocean, field, air
body part hair, mouth, arms
clothing shirt, hat, sunglasses
event trick, sunset, game
fixed object man-made furniture, statue, ramp
fixed object natural rock, puddle, bush
image attribute camera, picture, closeup
material man-made paint, frosting
material natural water, snow, dirt
movable man-made ball, toy, bowl
movable natural leaves, snowball
nondepictable something, Batman
orientation front, top, [the]distance
part of edge, side, top, tires
person family, skateboarder
property of shadow, shade, theme
vehicle surfboard, bike, boat
writing graffiti, map
</table>
<tableCaption confidence="0.99983">
Table 1: Our ontology for entities in images.
</tableCaption>
<bodyText confidence="0.999973709677419">
same entity, or even seemingly contradictory mod-
ifiers (“orange” vs. “brown” dog). In order to
predict what entities appear in an image from its
captions, we need to identify how many entities
each sentence describes, and what role these enti-
ties play in the image (e.g. person, animal, back-
ground). Because we have five sentences asso-
ciated with each image, we also need to identify
which noun phrases in the different captions of
the same image refer to the same entity. Because
the captions were generated independently, there
are no discourse cues such as anaphora to identify
coreference. This creates problems for standard
coreference resolution systems trained on regular
text. Our data also differs from standard corefer-
ence data sets in that entities are rarely referred to
by proper nouns.
Our first task is to identify which noun phrases
may refer to the same entity. We do this by identi-
fying the set of entity types that each NP may refer
to. We use WordNet (Fellbaum, 1998) to iden-
tify the possible entity types (WordNet synsets) of
each head noun. Since the salient entities in each
image are likely to be mentioned by more than one
caption writer, we then aim to restrict those types
to those that may be shared by some head nouns in
the other captions of the same image. This gives
us an inventory of entity types for each mention,
which we use to identify coreferences, restricted
by the constraint that all coreferent mentions refer
to an entity of the same type.
</bodyText>
<sectionHeader confidence="0.82502" genericHeader="method">
4 Using WordNet to identify entity types
</sectionHeader>
<bodyText confidence="0.923292961538462">
WordNet (Fellbaum, 1998) provides a rich ontol-
ogy of entity types that facilitates our coreference
task.6 We use WordNet to obtain a lexicon of pos-
sible entity types for each mention (based on their
lexical heads, assumed to be the last word with a
nominal POS tag7). We first generate a set of can-
didate synsets based solely on the lexical heads,
and then generate lexicon entries based on rela-
tions between the candidates.
WordNet synsets provide us with synonyms,
and hypernym/hyponym relations. For each men-
tion, we generate a list of candidate synsets.
We require that the candidates are one of the
first four synsets reported and that their fre-
quency is to be at least one-tenth of the most
frequent synset. We limit candidates to ones
with “physical entity#n#1”, “event#n#1”, or “vi-
sual property#n#1” as a hypernym, in order to en-
sure that the synset describes something that is de-
pictable. To avoid word senses that refer to a per-
son in a metaphorical fashion, (e.g. pig meaning
slovenly person or red meaning communist), we
ignore synsets that refer to people if the word has
a synset that is an animal or color.8
In general, we would like for mentions to be
able to take on more specific word senses. For ex-
ample, we would like to be able to identify that
“woman” and “person” may refer to the same
entity, whereas “man” and “woman” typically
would not. However, we also do not want a type
inventory that is too large or too fine-grained.
Once the candidate synsets are generated, we
consider all pairs of nouns (n1, n2) that occur in
different captions of the same image and exam-
ine all corresponding pairs of candidate synsets
(s1, s2). If s2 is a synonym or hypernym of s1, it
is possible that two captions have different words
describing the same entity, so we add s1 and s29
to the lexicon of n1. Adding s2 to n1’s lexicon al-
lows it to act as an umbrella sense covering other
nouns describing the same entity.10 We add s2 to
6For the prediction of ontological classes, we use our own
ontology because WordNet is too fine-grained for this pur-
pose.
7If there are two NP chunks that form a “[NP ... group] of
[NP... ]” construction, we only use the second NP chunk.
8An exception list handles cases (diver, blonde), where the
human sense is more likely than the animal or color sense.
9We don’t add 82 if it is “object#n#1” or “clothing#n#1”.
10This is needed when captions use different aspects of
the entity to describe it (for example, “skier” and “a skiing
man”).
</bodyText>
<page confidence="0.737177">
164
</page>
<bodyText confidence="0.999412">
the lexicon of n2 (since if n1 is using the sense s1,
then n2 must be using the sense s2) and if n1 oc-
curs at least five times in the corpus, we add s1 to
the lexicon of n2.
</bodyText>
<sectionHeader confidence="0.898054" genericHeader="method">
5 A heuristic coreference algorithm
</sectionHeader>
<bodyText confidence="0.991776888888889">
Based on WordNet candidate synsets, we define
a heuristic algorithm that finds the optimal entity
assignment for the mentions associated with each
image. This algorithm is based on the principles
driving our generative model described below, and
on the observation that salient entities will be men-
tioned in many captions and that captions tend to
use similar words to describe the same entity.
Simple heuristic algorithm:
</bodyText>
<listItem confidence="0.9624495">
1. For each noun, choose the synset that appears
in the most number of captions of an image,
and break ties by choosing the synset that
covers the fewest distinct lemmatized nouns.
2. Group all of the noun phrase chunks that
share a synset into a single coreference chain.
</listItem>
<sectionHeader confidence="0.698042" genericHeader="method">
6 Bayesian coreference models
</sectionHeader>
<bodyText confidence="0.999949">
Since we cannot afford to manually annotate our
entire data set with coreference information, we
follow Haghighi and Klein (2007)’s work on un-
supervised coreference resolution, and develop a
series of generative Bayesian models for our task.
</bodyText>
<subsectionHeader confidence="0.998843">
6.1 Model 0: Simple Mixture Model
</subsectionHeader>
<bodyText confidence="0.9999855">
In our first model, based on Haghighi and Klein’s
baseline Dirichlet Process model, each image i
corresponds to the set of observed mentions wi
from across its captions. Image i has a hidden
global topic Ti, drawn from a distribution with a
GEM prior with hyperparameter -y as explained by
Teh et al. (2006). In a Dirichlet process, the GEM
distribution is an infinite analog of the Dirich-
let distribution, allowing for a potentially infinite
number of mixture components. P(Ti = t) is pro-
portional to -y if t is a new component, or to the
number of times t has been drawn before other-
wise. Given a topic choice Ti = t, entity type
assignments Zj for all mentions wj in image i
are in turn drawn from a topic-specific multino-
mial Bt over all possible entity types E that was
drawn from a Dirichlet prior with hyperparameter
Q. Similarly, given an entity type Zi = z, each
corresponding (observed) head word wj is drawn
from an entity type-specific multinomial 0z over
all possible words V, drawn from a finite Dirich-
let prior with hyperparameter a. The set of all im-
ages belonging to the same topic is analogous to
an individual document in Haghighi and Klein’s
baseline model.11 All headwords of the same en-
tity type are assumed to be coreferent, similar to
Haghighi and Klein’s model. As described in sec-
tion 4, we use WordNet to identify the subset of
types that can actually produce the given words.
Therefore, similar to the way Andrzejewski and
Zhu (2009) handled a priori knowledge of topics,
we will define an indicator variable Sij that is 1
iff the WordNet information allows word i to be
produced from entity set j and 0 otherwise.
</bodyText>
<subsubsectionHeader confidence="0.596567">
6.1.1 Sampling Model 0
</subsubsectionHeader>
<bodyText confidence="0.99999075">
We find arg maxZ,TP(Z, TJX) with Gibbs sam-
pling. Here, Z and T are the collection of type
and topic assignments, with Z−j = Z − {Zj} and
T−i = T − {Ti}. This style of notation will be
extended analogously to other variables. Let ne,x
represent the number of times word x is produced
from entity a across all topics and let pj be the
number of images assigned to topic j. Let mt,e
represent the number of times entity type a is
generated by topic t. Each iteration consists of
two steps: first, each Zi is resampled, fixing T;
and then each Ti is resampled based on Z12.
</bodyText>
<listItem confidence="0.730224">
1. Sampling Zj:
</listItem>
<equation confidence="0.9895738">
P(Zj =e|wj E wi,Z−j,T) a P(wj|Zj=e)P(Zj =e|Ti)
„ −j
P(wj = x |Zj = e) a ne,x α P fixe
x0 n−j e,x0 + α
mt,e
−j /J
+ hi
P(Zj = e|Ti = t) = [&apos;� e
Lie M−j0 + N
t
2. Sampling Ti:
P(Ti =j|w,Z,T−i) a P(Ti =j|T−i)P(Z|Ti =j, T−i)
Y
a P(Ti = j|T−i) P(Zk|Ti = j)
k∈w*
Y
= P(Ti = j|T−i)
k∈w•
P(Ti = j|T−i) a !ry, If its a new topic
lPj Otherwise
</equation>
<bodyText confidence="0.9971415">
11Since we do not have multiple images of the same well-
known people or places, referred to by their names, we do not
perform any cross-image coreference
12Sampling on the exponentiated posterior to find the mode
as Haghighi and Klein (2007) did was found to not signifi-
cantly affect results on our tasks
</bodyText>
<figure confidence="0.6510475">
m−i+ N
j,Zk
Pe0 m−i
j,e0 + N
165
Image 21:
</figure>
<table confidence="0.698069">
Caption 1: {x21,1:a golden retriever; x21,2 :a smaller black and brown dog; x21,3:a pink collar}
Caption 2: {x21,4:a smaller black dog; x21,5:a larger brown dog; x21,6:a forest}
Caption 3: {x21,7:small black and brown dog; x21,8:a large orange dog}
Caption 4: {x21,9:brown dog; x21,10:mouth; x21,11:head; x21,12:black and tan dog}
Caption 5: {x21,13:two dogs; x21,14:the woods}
</table>
<figure confidence="0.996445136363636">
x21,8
x21,9
x21,1
x21,5
DOG
attr:5
x21,2
x21,4
x21,7
x21,12
DOG
attr:3
x21,3 x21,6
CLOTHING
attr:2
x21,14
FOREST
attr:8
Restaurant 211
x21,10
MOUTH
attr:0 ...
</figure>
<figureCaption confidence="0.855385">
Figure 2: Models 1 and 2 as Chinese restaurant franchises: each image topic is a franchise, each image
is a restaurant, each entity is a table, each mention is a customer. Model 2 adds attributes (in italics).
</figureCaption>
<subsectionHeader confidence="0.995884">
6.2 Model 1: Explicit Entities
</subsectionHeader>
<bodyText confidence="0.999989431372549">
Model 0 does not have an explicit representation
of entities beyond their type and thus cannot dis-
tinguish multiple entities of the same type in an
image. Although Model 1 also represents men-
tions only by their head words (and thus cannot
distinguish black dog from brown dog), it creates
explicit entities based on the Chinese restaurant
franchise interpretation of the hierarchical Dirich-
let Process model (Teh et al., 2006). Figure 2 (ig-
noring the modifiers / attributes for now) illustrates
the Chinese restaurant franchise interpretation of
our model. Using this metaphor, there are a se-
ries of restaurants (= images), each consisting of
a potentially infinite number of tables (= entities),
that are frequented by customers (= entity men-
tions) who will be seated at the tables. Restau-
rants belong to franchises (= image topics). Each
table is served one dish (= entity type, e.g. DOG,
CLOTHING) shared by all the customers. The head
word of a mention xi,j is generated in the follow-
ing manner: customer j enters restaurant i (be-
longing to franchise Ti) and sits down at one of
the existing tables with probability proportional to
the number of other customers there, or sits at a
new table with probability proportional to a con-
stant. A dish eia (DOG) from a potentially infinite
menu is served to each new table a, with probabil-
ity proportional to the total number of tables it is
served at in the franchise Ti (or to a constant if it
is a new dish). The (observed) head word of the
mention xj,i (dog, retriever) is then drawn from
the multinomial distribution over words defined by
the entity type (DOG) at the table. The menu (set
of dishes) available to each restaurant and table is
restricted by our lexicon of WordNet synsets for
each mention. More formally, each image topic
t defines a distribution over entities drawn from a
global GEM prior with hyperparameter n. There-
fore, the probability of an entity a is proportional
to the number of its existing mentions in images of
the same topic, or to n, if it is previoulsy unmen-
tioned. The type of each entity, ea, is drawn from
a topic-dependent multinomial with global Dirich-
let prior. The head words of mentions are gener-
ated by their entity type as in Model 0. Mentions
assigned to the same entity are considered to be
coreferent. Based on the nature of our corpus, we
again assume that two words cannot be coreferent
within a sentence, restrict the distribution to not
allow inter-sentence coreference and renormalize
the values accordingly.
</bodyText>
<subsubsectionHeader confidence="0.793565">
6.2.1 Sampling Model 1
</subsubsectionHeader>
<bodyText confidence="0.999799416666666">
There are three parts to our resampling procedure:
resampling the entity assignment for each word,
resampling the entity type for each entity, and re-
sampling the topic of each image. The kth word
of image i, sentence j, will now be wij,k; eia is the
entity type of entity a in image i; aij,k is the en-
tity that word k of sentence j is produced from in
image i, and Zij,k represents that entity’s type. a
is the set of all current entity assignments and e
are the type assignments for entities. m is now de-
fined as the number of entities of a certain type be-
ing drawn for an image, n is defined as before and
ci,a is the number of times entity a is expressed in
image i. Topics are resampled as in Model 0.
Entity Assignment Resampling Entity assign-
ments for words are resampled one sentence at a
time in the order the headwords appear in the sen-
tence. For each word in the sentence, entity as-
signments are defined by the distribution of Fig-
ure 3. The headword is assigned to an existing
entity with probability proportional to the number
of entities already assigned to that entity and the
probability that the entity emits that word. The
word is assigned to a new entity with a newly
</bodyText>
<equation confidence="0.9788967">
166
Model 1:
n−ei,a
P (ei a =e|a, w, Ti = t, e−i,a) ∝ (m−ei,a
t,e + β) Q e,x + α
{wi j,k=x|ai P δx,e
j,k=a} y(n−ei,a
e,y + α)
(
c−j,k0
i,a P (wi j,k|Zi j,k = ei a, a−(i,j,k0|k0≥k))ρi j,a, if a is not new
P(ai j,k =a|a−(i,j,k0|k0≥k), e, T) ∝ κP(ei a|e−i,a, Ti)P (wi j,k|Zi j,k = ei a, a−(i,j,k0|k0≥k)), o/w
With:
−(i,j,k0|k0≥k) + α
P(wi j,k =x|Zi j,k =e, a−(i,j,k0|k0≥k)) ∝ ne,x P δx,e
y(n−(i,j,k0|k0≥k) + α)
e,y
Model 2: (
c−j,k0
i,a P (wi j,k|Zi j,k = ei a, a−(i,j,k0|k0≥k))ρi j,aP(di j,k|bi a), if a is not new
P(ai j,k =a|a−(i,j,k0|k0≥k), e, T, b) ∝ κP(ei a|e−i,a, Ti)P(wi j,k|Zi j,k = ei a, a−(i,j,k0|k0≥k))P(bi a)P(di a|bi a), o/w
(s−i,a
b,d+ ζ)
P(bi a =b|Di a,b−i,a)∝P(bi a =b|b−i,a)Q d∈Di P
a d0(s−i,a
b,d0 + ζ)
Figure 3: Sampling equations for Models 1 and 2
t,e + β
P(ei a =e|e−i,a, Ti = t) ∝ m−ei,a
Pe0(m−ei,a t,e0 + β)
</equation>
<bodyText confidence="0.999867214285714">
drawn entity type with probability proportional K,
the probability that the entity type is for an im-
age of the given topic (normalized over WordNet’s
possible entities for the word), and the probability
the drawn type produces the word. pla = 1 iff
entity a of image i does not appear in sentence j
and p�,a = 0 otherwise. a(i,j,k0jk0&gt;k) represents
removing the kth or later words in sentence j of
image i
Entity Type Resampling Fixing the assign-
ments, the type of each entity is redrawn based
on the distribution in Figure 3. It is proportional
to the probability that a certain entity type is in an
image of a given topic and, independently for each
of the words, the probability that the given word
expresses the type. n��i,a
e,x is the number of times
entity type a is expressed as word x not counting
the words attached to the currently entity being re-
sampled and m��i,a
t,e is the number of times an en-
tity of type a appears in an image of topic t not
counting the current entity being resampled. The
probability of a given image belonging to a topic is
proportional the number of images already in the
topic (or &apos;y) followed by the probability that each
of the entities in the image were drawn from that
topic.
</bodyText>
<subsectionHeader confidence="0.994457">
6.3 Model 2: Explicit Entities and Modifiers
</subsectionHeader>
<bodyText confidence="0.9998892">
Certain entities cannot be distinguished simply by
head word alone, such in the example in Figure 2.
Model 2 augments Model 1 with the ability to gen-
erate modifiers. In addition to an entity type, each
entity draws an attribute from a global distribution
drawn from a GEM distribution with hyperparam-
eter q. An attribute is a multinomial distribution,
on possible modifier words, drawn from a Dirich-
let prior with parameter C. From the attribute, each
modifier word is drawn independently. There-
fore given an attribute b and a set of modifiers d:
P(djb) / HdEd(sd + C) where sd is number of
times modifier d is produced by attribute b. In ad-
dition, the probability of a certain attribute b given
all other assignments is given by:
</bodyText>
<equation confidence="0.67874325">
(
η, If its a new attribute
P(bi a = b|b−i,a) ∝
rb, Otherwise
</equation>
<bodyText confidence="0.9999865">
where rb is the number of entities with at-
tribute b. As in Model 1, mentions assigned to
the same entity are considered coreferent. Con-
sider the “smaller black dog” mention in Figure 2.
When the mention is being resampled, the at-
tribute choice for each table will bias the probabil-
ity distribution towards the table whose attribute
is more likely to produce “smaller” and “black”.
Therefore, the model can now better distinguish
the two dogs in the image.
</bodyText>
<subsubsectionHeader confidence="0.71709">
6.3.1 Sampling Model 2
</subsubsectionHeader>
<bodyText confidence="0.9999497">
The addition of modifiers only directly effects the
distribution when resampling entity assignments
since attributes are independent of entity types,
image topics, and headwords of noun phrases. The
sampling distribution are again shown in Figure 3.
In a separate sampling step, it is now necessary to
resample the attribute assigned to each entity: The
probability of drawing a certain attribute is illus-
trated in Figure 3 with Dia as the set of all the mod-
ifiers of all the noun phrases currently assigned to
</bodyText>
<page confidence="0.761502">
167
</page>
<bodyText confidence="0.97602875">
entity a of image i, and s�i,a
b,d as the number of
times attribute b produces modifier d without the
current assignment of entity a of image i.
</bodyText>
<subsectionHeader confidence="0.976175">
6.4 Implementation
</subsectionHeader>
<bodyText confidence="0.999989941176471">
The topic assignments for each image are initial-
ized to correspond to the Flickr groups the images
were taken from. Each mention was initialized as
its own entity with type and attribute sampled from
a uniform distribution.
As our training is unsupervised, each of the
models were ran on the entire dataset. For Model
0, after burn-in, 20 samples of Z were taken
spaced 200 iterations apart, while for Model 1
samples were taken spaced 100 apart, and 25 apart
for Model 2. The implementation of Model 2 ran
too slow to effectively judge when burn in oc-
curred, impacting the results.
The values of parameters α, 0, &apos;y, K, q, C, and
the number of initial attributes were hand-tuned
based on the average performance on our anno-
tated development subset of 100 images.13
</bodyText>
<sectionHeader confidence="0.711994" genericHeader="method">
7 Evaluation of coreference resolution
</sectionHeader>
<bodyText confidence="0.998178944444445">
We evaluate each of the generative models and the
heuristic coreference algorithm on the annotated
test subset of our corpus consisting of 100 images
with both the OpenNLP chunking and the gold
standard chunking. We report our scores based
on the MUC evaluation metric. The results are
reported in Table 2 as the average scores across
all the samples of two independent runs of each
model. We also present results on Model 0 with-
out using WordNet where every word can be an
expression of one of 200 fake entity sets. The
same table also shows the performance of a base-
line model and the upper bound on performance
imposed by WordNet.
A baseline model: In our baseline model, two
noun phrases in captions of the same image are
coreferent if they share the same head noun.
Upper bound on performance: Although
WordNet synsets provide a good indication of
whether two mentions can refer to the same
entity or not, they may also be overly restrictive
in other cases. We measure the upper bound
on performance that our reliance on WordNet
imposes by finding the best-scoring coreference
assignment that is consistent with our lexicon.
13(0.1, 0.1, 100, 0.001875, 100, 0.0002, 20) respectively.
This achieves an F-score of 90.2 on the test data
with gold chunks.
Performance increases in each subsequent
model. The heuristic beats each of the models, but
in some sense it is an extreme version of Model
1. Both it and Model 1 attempt to produce en-
tity sets that cover as many captions as possible,
while minimizing the number of distinct words in-
volved. The heuristic locally forces this case, at
the expense of no longer being a generative model.
</bodyText>
<sectionHeader confidence="0.970281" genericHeader="method">
8 Ontological Class Prediction
</sectionHeader>
<bodyText confidence="0.9999692">
As a further step towards understanding the se-
mantics of images, we develop a model that labels
each entity with one of the ontological classes de-
fined in section 2. The immediate difficulty of this
task is that our ontology includes not only seman-
tic distinctions, but also spatial and visual ones.
While it may be easy to tell which words are an-
imals and which are people, there is only a fine
distinction at the language level whether an object
is movable, fixed, or part of the background.14
</bodyText>
<subsectionHeader confidence="0.993879">
8.1 Model and Features
</subsectionHeader>
<bodyText confidence="0.949950125">
We define our task as a classification problem,
where each entity must be assigned to one of
twenty classes defined by our ontology. We use a
Maximum Entropy classifier, implemented in the
MALLET toolkit (McCallum, 2002), and define
the following text features:
NP Chunk: We include all the words in the NP
chunk, unfiltered.
WordNet Synsets and Hypernyms: The most
likely synset is either the first one that appears in
WordNet or one of the ones predicted by our coref-
erence system. For each of these possibilities, we
include all of that synset’s hypernyms.
Syntactic Role: We parsed our captions with the
C&amp;C parser (Clark and Curran, 2007), and record
whether the word appears as a direct object of a
verb, as the object of a preposition, as the subject
of the sentence, or as a modifier. If it is a modi-
fier, we also add the head word of the phrase being
modified.
14For example, we deem bowls and silverware to be mov-
able objects; furniture, fixed; and carpets, background. More-
over, in all three cases, we must correctly distinguish that
these objects are man-made and not found in nature.
</bodyText>
<page confidence="0.409052">
168
</page>
<table confidence="0.999029555555556">
Model OpenNLP chunks Gold chunks
Rec. Prec. F1 Rec. Prec. F1
Baseline 57.3 89.5 69.9 64.1 96.2 77.0
Upper bound 82.1 100 90.2
WN Heuristic 70.6 84.8 77.0 80.4 90.6 85.2
Model 0 w/o WN 79.7 59.8 68.4 85.1 62.7 72.2
Model 0 66.8 83.1 74.1 75.9 90.3 82.5
Model 1 69.6 83.8 76.0 78.0 90.8 83.9
Model 2 69.2 84.4 76.1 77.9 91.5 84.1
</table>
<tableCaption confidence="0.989966">
Table 2: Coreference resolution results (MUC scores; Models 0-2 are averaged over all samples)
</tableCaption>
<subsectionHeader confidence="0.980887">
8.2 Experiments
</subsectionHeader>
<bodyText confidence="0.999973763157895">
We use two baselines. The naive baseline catego-
rizes words by selecting the most frequent class
of the word. If no instances of the word have oc-
curred, it uses the overall most frequent class. The
WordNet baseline works by finding the most fre-
quent class amongst the most relevant synsets for
a word. It calculates the class frequency for each
synset by assuming each word has the sense of its
first synset and incrementing the frequency of the
first synset and its hypernyms. When categorizing
a word, it finds the set of closest hypernyms of the
word that have a non-zero frequency, and chooses
the class with the greatest sum of frequency counts
amongst those hypernyms.
We train the MaxEnt classifier using semi-
supervised learning. Initially, we train a classifier
using the 500 sentence gold standard development
set. For each class, we use the top 5%15 of the la-
bels to label the unlabeled data and provide addi-
tional training data. We then retrain the classifier
on the newly labeled examples and the develop-
ment set, and run it on the test set. For each coref-
erence chain in the test set, we relabel all of the
mentions in the chain to use the majority class, if
a clear majority exists. If no such majority exists,
we leave the labels as is. The MaxEnt classifier
experiments were conducted by varying the source
of the synset assigned to each word. For each of
our coreference systems, we report two scores (Ta-
ble 3). The first is the average accuracy when us-
ing the output from two runs of each model with
about 20 samples per run, and the second uses the
output that performs best on the coreference task
when scored on the development data.
Discussion Although we use WordNet to clas-
sify our entity mentions, we designed our ontology
by considering only the images and their captions,
with no particular mapping to WordNet in mind.
</bodyText>
<table confidence="0.8793237">
15This was tuned using 10-fold cross-validation of the de-
velopment set.
Classifier (synset prediction) Accuracy (gold chunks)
Naive Baseline 72.0
WordNet Baseline 81.0
MaxEnt (1st-synset) 84.4
MaxEnt (WN heuristic) 82.7
Avg. v Best-Coref
MaxEnt (Model 1) 83.9 0.5 84.5
MaxEnt (Model 2) 84.1 0.4 85.3
</table>
<tableCaption confidence="0.999912">
Table 3: Prediction of ontological classes
</tableCaption>
<bodyText confidence="0.99995925">
Therefore, these experiments provide of a proof of
concept for the semi-supervised labeling of a cor-
pus using any semantic/visual ontology.
Overall, Model 2 had the best performance for
this task. This demonstrates that the additional
features of Model 2 force synset selections that are
consistent across the entire corpus, and are sen-
sitive to the modifiers appearing with them. The
WordNet heuristic selects synsets in a fairly ar-
bitrary manner - all other things being equal, the
synsets are chosen without reference to what other
synsets are chosen by similar clusters of nouns.
</bodyText>
<sectionHeader confidence="0.852353" genericHeader="method">
9 Evaluating entity prediction
</sectionHeader>
<bodyText confidence="0.994822473684211">
Together, the coreference resolution algorithm and
ontology classification model provide us with a set
of distinct, ontologically-categorized entities ap-
pearing in each image. We perform a final experi-
ment to evaluate how well our models can recover
the mentioned entities and their ontological types
for each image. We now represent each entity as a
tuple (L, c), where L is its coreference chain, and
c is the ontological class of these mentions. 16
We compute the precision and recall between
the predicted and gold standard tuples for each im-
age. We consider a tuple (L&apos;, c&apos;) correctly pre-
dicted only when a copy of (L&apos;, c&apos;) occurs both
in the set of predicted tuples and the set of gold
standard tuples.17 Then, as usual, for precision we
16Note that for each image, the tuples of all entities corre-
spond to a partition of the set of the head-word mentions in
an image.
17We assign no partial credit because incorrect typing or
</bodyText>
<page confidence="0.526878">
169
</page>
<table confidence="0.999513857142857">
Model Recall Precision F-score
Baseline 28.4 20.6 23.9
WordNet Heuristic 48.3 43.9 46.0
Model 1 (avg) 51.7 42.8 46.8
Model 1 (best-coref) 50.9 45.4 48.0
Model 2 (avg) 52.2 42.7 47.0
Model 2 (best-coref) 52.3 46.0 49.0
</table>
<tableCaption confidence="0.976389">
Table 4: Overall entity recovery. We measure
</tableCaption>
<bodyText confidence="0.976838342105263">
how many entities we identify correctly (requiring
complete recovery of their coreference chains and
correct prediction of their ontological class.
normalize the number of overlapping tuples by the
number of predicted tuples, and for recall, by the
number of gold standard tuples. We report average
precision and recall over all images in our test set.
We report scores for four different pairs of on-
tological class and coreference chain predictions.
As a baseline, we use the ontological classes pre-
dicted by the our naive baseline and the chains pre-
dicted by the “same-words-are-coreferent” coref-
erence resolution baseline.
We also report results using the classes and
chains predicted by Model 1, Model 2, and the
WordNet Heuristic Algorithm. The influence of
the different coreference algorithms comes from
the entity types that are used to determine corefer-
ence chains, and that also correspond to WordNet
candidate synsets. In other words, although the
final coreference chain may be predicted by two
different models, the synsets they use to do so may
differ, affecting the synset and hypernym features
used for ontological prediction. We present results
in Table 4 for these four different set-ups.
The synsets chosen by the different corefer-
ence algorithms clearly have different applicabil-
ity when it comes to ontological class prediction.
Although Model 2 performs comparably to Model
1 and does worse than the WordNet heuristic al-
gorithm for coreference chain prediction, it cer-
tainly does better on this task. Since our end goal
is creating a unified semantic representation, this
final task judges the effectiveness of our models to
capture the most detailed entity information. The
success of Model 2 means that the incorporation
of adjectives informs the proper choice of synsets
that are useful in predicting ontological classes.
</bodyText>
<sectionHeader confidence="0.990184" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999890109090909">
As a first step towards automatic image under-
standing, we have collected a corpus of images as-
incomplete coreference chaining both completely change the
semantics of an image.
sociated with several simple descriptive captions,
which provide more detailed information about
the image than simple keywords. We plan to make
this data set available for further research in com-
puter vision and natural language processing. In
order to enable the creation of a semantic repre-
sentation of the image content that is consistent
with the captions in our data set, we use Word-
Net and a series of Bayesian models to perform
cross-caption coreference resolution. Similar to
Haghighi and Klein (2009), who find that linguis-
tic heuristics can provide very strong baselines for
standard coreference resultion, relatively simple
heuristics based on WordNet alone perform sur-
prisingly well on our task, although they are out-
performed by our Bayesian models for overall en-
tity prediction. Since our generative models are
based on Dirichlet Process priors, they are de-
signed to favor a small number of unique entities
per image. In the heuristic algorithm, this bias
is built in explicitly, resulting in slightly higher
performance on the coreference resolution task.
However, while the generative models can use
global information to learn what entity type each
word is likely to represent, the heuristic is unable
to capture any non-local information about the en-
tities, and thus provides less useful input for the
prediction of ontological classes.
Future work will aim to improve on these re-
sults by overcoming the upper bound on perfor-
mance imposed by WordNet, and through a more
sophisticated model of modifiers. We will also in-
vestigate how image features can be incorporated
into our model to improve performance on entity
detection. Ultimately, identifying the depicted en-
tities from multiple image captions will require
novel ways to correctly handle the semantics of
plural NPs (i.e. that one caption’s “two dogs” con-
sist of another’s “golden retreiver” and “smaller
black dog”). We foresee similar challenges when
dealing with verbs and events.
The creation of an actual semantic representa-
tion of the image content is a challenging problem
in itself, since the different captions often focus
on different aspects of the depicted situation, or
provide different interpretation of ambiguous sit-
uations. We believe that this poses many inter-
esting challenges for natural language processing,
and will ultimately require ways to integrate the
information conveyed in the caption with visual
features extracted from the image.
</bodyText>
<page confidence="0.756042">
170
</page>
<sectionHeader confidence="0.996597" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.7292432">
This research was funded by NSF grant IIS 08-
03603 INT2-Medium: Understanding the Mean-
ing of Images. We are grateful for David Forsyth
and Dan Roth’s advice, and for Alex Sorokins sup-
port with MTurk.
</bodyText>
<sectionHeader confidence="0.936742" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999857787878788">
David Andrzejewski and Xiaojin Zhu. 2009. Latent
Dirichlet allocation with topic-in-set knowledge. In
NAACL HLT 2009 Workshop on Semi-Supervised
Learning for Natural Language Processing, pages
43–48.
Kobus Barnard, Pinar Duygulu, David Forsyth,
Nando De Freitas, David M. Blei, and Michael I.
Jordan. 2003. Matching words and pictures. Jour-
nal of Machine Learning Research, 3:1107–1135.
Regina Barzilay and Kathleen R. McKeown. 2001.
Extracting paraphrases from a parallel corpus. In
Proceedings of the 39th annual meeting of the Asso-
ciation for Computational Linguistics, pages 50–57,
Toulouse, France, July.
David M. Blei, Michael I, David M. Blei, and Michael
I. 2003. Modeling annotated data. In Proceedings
of the 26th International ACM SIGIR Conference,
pages 127–134.
Stephen Clark and James R. Curran. 2007. Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguistics,
33(4):493–552.
Koen Deschacht and Marie-Francine Moens. 2007.
Text analysis for automatic image annotation. In
Proceedings of the 45th Annual Meeting of the As-
sociation of Computational Linguistics.
Bill Dolan, Chris Quirk, and Chris Brockett. 2004.
Unsupervised construction of large paraphrase cor-
pora: Exploiting massively parallel news sources.
In Proceedings of Coling 2004, pages 350–356,
Geneva, Switzerland, August. COLING.
A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009.
Describing objects by their attributes. In IEEE Con-
ference on Computer Vision and Pattern Recogni-
tion, pages 1778–1785, June.
Christiane Fellbaum, editor. 1998. WordNet An Elec-
tronic Lexical Database. The MIT Press, Cam-
bridge, MA ; London, May.
P. Felzenszwalb, D. McAllester, and D. Ramanan.
2008. A discriminatively trained, multiscale, de-
formable part model. In IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 1–8,
June.
Yansong Feng and Mirella Lapata. 2008. Automatic
image annotation using auxiliary text information.
In Proceedings of ACL-08: HLT, pages 272–280,
Columbus, Ohio, June.
Aria Haghighi and Dan Klein. 2007. Unsupervised
coreference resolution in a nonparametric Bayesian
model. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 848–855, Prague, Czech Republic.
Aria Haghighi and Dan Klein. 2009. Simple coref-
erence resolution with rich syntactic and semantic
features. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing, pages 1152–1161, Singapore, August. Associ-
ation for Computational Linguistics.
L. Hollink and M. Worring. 2005. Building a vi-
sual ontology for video retrieval. In MULTIMEDIA
’05: Proceedings of the 13th annual ACM interna-
tional conference on Multimedia, pages 479–482,
New York, NY, USA. ACM.
A. Hoogs, J. Rittscher, G. Stein, and J. Schmiederer.
2003. Video content annotation using visual anal-
ysis and a large semantic knowledgebase. In IEEE
Computer Society Conference on Computer Vision
and Pattern Recognition, volume 2, pages II–327 –
II–334 vol.2, June.
Jane Hunter. 2001. Adding multimedia to the semantic
web - building an mpeg-7 ontology. In In Interna-
tional Semantic Web Working Symposium (SWWS,
pages 261–281.
Lavrenko Manmatha Jeon, V. Lavrenko, R. Manmatha,
and J. Jeon. 2003. A model for learning the seman-
tics of pictures. In Seventeenth Annual Conference
on Neural Information Processing Systems (NIPS).
MIT Press.
Andrew Kachites McCallum. 2002. Mal-
let: A machine learning for language toolkit.
http://mallet.cs.umass.edu.
Christoph M¨uller and Michael Strube. 2006. Multi-
level annotation of linguistic data with MMAX2. In
Sabine Braun et al, editor, Corpus Technology and
Language Pedagogy, pages 197–214. Peter Lang,
Frankfurt a.M., Germany.
Ariadna Quattoni and Antonio B. Torralba. 2009.
Recognizing indoor scenes. In IEEE Conference
on Computer Vision and Pattern Recognition, pages
413–420. IEEE.
Cyrus Rashtchian, Peter Young, Micah Hodosh, and
Julia Hockenmaier. 2010. Collecting image anno-
tations using amazons mechanical turk. In NAACL
Workshop Creating Speech and Language Data With
Amazons Mechanical Turk.
Yee Whye Teh, Michael I Jordan, Matthew J Beal, and
David M Blei. 2006. Hierarchical dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101(476):1566–1581.
</reference>
<page confidence="0.92095">
171
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.329548">
<title confidence="0.998309">Cross-caption coreference resolution for automatic image understanding</title>
<author confidence="0.999967">Micah Hodosh Peter Young Cyrus Rashtchian Julia</author>
<affiliation confidence="0.9856485">Department of Computer University of Illinois at</affiliation>
<address confidence="0.337166">pyoung2, crashtc2,</address>
<abstract confidence="0.998889">Recent work in computer vision has aimed to associate image regions with keywords describing the depicted entities, but actual image ‘understanding’ would also require identifying their attributes, relations and activities. Since this information cannot be conveyed by simple keywords, we have collected a corpus of “action” photos each associated with five descriptive captions. In order to obtain a consistent semantic representation for each image, we need to first identify which NPs refer to the same entities. We present three hierarchical Bayesian models for cross-caption coreference resolution. We have also created a simple ontology of entity classes that appear in images and evaluate how well these can be recovered.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David Andrzejewski</author>
<author>Xiaojin Zhu</author>
</authors>
<title>Latent Dirichlet allocation with topic-in-set knowledge.</title>
<date>2009</date>
<booktitle>In NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing,</booktitle>
<pages>43--48</pages>
<contexts>
<context position="15908" citStr="Andrzejewski and Zhu (2009)" startWordPosition="2619" endWordPosition="2622">ly, given an entity type Zi = z, each corresponding (observed) head word wj is drawn from an entity type-specific multinomial 0z over all possible words V, drawn from a finite Dirichlet prior with hyperparameter a. The set of all images belonging to the same topic is analogous to an individual document in Haghighi and Klein’s baseline model.11 All headwords of the same entity type are assumed to be coreferent, similar to Haghighi and Klein’s model. As described in section 4, we use WordNet to identify the subset of types that can actually produce the given words. Therefore, similar to the way Andrzejewski and Zhu (2009) handled a priori knowledge of topics, we will define an indicator variable Sij that is 1 iff the WordNet information allows word i to be produced from entity set j and 0 otherwise. 6.1.1 Sampling Model 0 We find arg maxZ,TP(Z, TJX) with Gibbs sampling. Here, Z and T are the collection of type and topic assignments, with Z−j = Z − {Zj} and T−i = T − {Ti}. This style of notation will be extended analogously to other variables. Let ne,x represent the number of times word x is produced from entity a across all topics and let pj be the number of images assigned to topic j. Let mt,e represent the n</context>
</contexts>
<marker>Andrzejewski, Zhu, 2009</marker>
<rawString>David Andrzejewski and Xiaojin Zhu. 2009. Latent Dirichlet allocation with topic-in-set knowledge. In NAACL HLT 2009 Workshop on Semi-Supervised Learning for Natural Language Processing, pages 43–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kobus Barnard</author>
<author>Pinar Duygulu</author>
<author>David Forsyth</author>
<author>Nando De Freitas</author>
<author>David M Blei</author>
<author>Michael I Jordan</author>
</authors>
<title>Matching words and pictures.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1107</pages>
<marker>Barnard, Duygulu, Forsyth, De Freitas, Blei, Jordan, 2003</marker>
<rawString>Kobus Barnard, Pinar Duygulu, David Forsyth, Nando De Freitas, David M. Blei, and Michael I. Jordan. 2003. Matching words and pictures. Journal of Machine Learning Research, 3:1107–1135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Extracting paraphrases from a parallel corpus.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>50--57</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="3186" citStr="Barzilay and McKeown, 2001" startWordPosition="500" endWordPosition="503">Feng and Lapata, 2008; Deschacht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 images associated with several simple descriptive captions. In contrast to the text near an image on the web, the captions in our corpus provide direct, if partial and slightly noisy, descriptions of the image content. Our data set differs from paraphrase corpora (Barzilay and McKeown, 2001; Dolan et al., 2004) in that the different captions of an image are produced independently by different writers. There are many ways of describing the same image, because it is often possible to focus on different aspects of the depicted situation, and because certain aspects of the situation may be unclear to the human viewer. One of our goals is to use these captions to obtain a semantic representation of each image that is consistent with all of its captions. In order to obtain such a representation, it is necessary to identify the entities that appear in the image, and to perform cross-ca</context>
</contexts>
<marker>Barzilay, McKeown, 2001</marker>
<rawString>Regina Barzilay and Kathleen R. McKeown. 2001. Extracting paraphrases from a parallel corpus. In Proceedings of the 39th annual meeting of the Association for Computational Linguistics, pages 50–57, Toulouse, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>I Michael</author>
<author>David M Blei</author>
<author>I Michael</author>
</authors>
<title>Modeling annotated data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th International ACM SIGIR Conference,</booktitle>
<pages>127--134</pages>
<contexts>
<context position="2296" citStr="Blei et al., 2003" startWordPosition="348" endWordPosition="351">al., 2009), and are far from recovering a complete semantic interpretation of the depicted situation. Like natural language processing, computer vision requires suitable training data, and there are currently no publicly available data sets that would enable the development of such systems. Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections of images annotated with keywords (e.g. the Corel collection). A lot of recent work in computer vision has been aimed at predicting these keywords (Blei et al., 2003; Barnard et al., 2003; Feng and Lapata, 2008; Deschacht and Moens, 2007; Jeon et al., 2003). But keywords alone are not expressive enough to capture relations between entities. Some research has used the text that surrounds an image in a news article as a proxy (Feng and Lapata, 2008; Deschacht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 i</context>
</contexts>
<marker>Blei, Michael, Blei, Michael, 2003</marker>
<rawString>David M. Blei, Michael I, David M. Blei, and Michael I. 2003. Modeling annotated data. In Proceedings of the 26th International ACM SIGIR Conference, pages 127–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Clark</author>
<author>James R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="29559" citStr="Clark and Curran, 2007" startWordPosition="5053" endWordPosition="5056">s a classification problem, where each entity must be assigned to one of twenty classes defined by our ontology. We use a Maximum Entropy classifier, implemented in the MALLET toolkit (McCallum, 2002), and define the following text features: NP Chunk: We include all the words in the NP chunk, unfiltered. WordNet Synsets and Hypernyms: The most likely synset is either the first one that appears in WordNet or one of the ones predicted by our coreference system. For each of these possibilities, we include all of that synset’s hypernyms. Syntactic Role: We parsed our captions with the C&amp;C parser (Clark and Curran, 2007), and record whether the word appears as a direct object of a verb, as the object of a preposition, as the subject of the sentence, or as a modifier. If it is a modifier, we also add the head word of the phrase being modified. 14For example, we deem bowls and silverware to be movable objects; furniture, fixed; and carpets, background. Moreover, in all three cases, we must correctly distinguish that these objects are man-made and not found in nature. 168 Model OpenNLP chunks Gold chunks Rec. Prec. F1 Rec. Prec. F1 Baseline 57.3 89.5 69.9 64.1 96.2 77.0 Upper bound 82.1 100 90.2 WN Heuristic 70.</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Stephen Clark and James R. Curran. 2007. Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koen Deschacht</author>
<author>Marie-Francine Moens</author>
</authors>
<title>Text analysis for automatic image annotation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</booktitle>
<contexts>
<context position="2368" citStr="Deschacht and Moens, 2007" startWordPosition="360" endWordPosition="363">rpretation of the depicted situation. Like natural language processing, computer vision requires suitable training data, and there are currently no publicly available data sets that would enable the development of such systems. Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections of images annotated with keywords (e.g. the Corel collection). A lot of recent work in computer vision has been aimed at predicting these keywords (Blei et al., 2003; Barnard et al., 2003; Feng and Lapata, 2008; Deschacht and Moens, 2007; Jeon et al., 2003). But keywords alone are not expressive enough to capture relations between entities. Some research has used the text that surrounds an image in a news article as a proxy (Feng and Lapata, 2008; Deschacht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 images associated with several simple descriptive captions. In contrast t</context>
</contexts>
<marker>Deschacht, Moens, 2007</marker>
<rawString>Koen Deschacht and Marie-Francine Moens. 2007. Text analysis for automatic image annotation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bill Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources.</title>
<date>2004</date>
<booktitle>In Proceedings of Coling</booktitle>
<pages>350--356</pages>
<publisher>COLING.</publisher>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="3207" citStr="Dolan et al., 2004" startWordPosition="504" endWordPosition="507">acht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 images associated with several simple descriptive captions. In contrast to the text near an image on the web, the captions in our corpus provide direct, if partial and slightly noisy, descriptions of the image content. Our data set differs from paraphrase corpora (Barzilay and McKeown, 2001; Dolan et al., 2004) in that the different captions of an image are produced independently by different writers. There are many ways of describing the same image, because it is often possible to focus on different aspects of the depicted situation, and because certain aspects of the situation may be unclear to the human viewer. One of our goals is to use these captions to obtain a semantic representation of each image that is consistent with all of its captions. In order to obtain such a representation, it is necessary to identify the entities that appear in the image, and to perform cross-caption coreference res</context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>Bill Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised construction of large paraphrase corpora: Exploiting massively parallel news sources. In Proceedings of Coling 2004, pages 350–356, Geneva, Switzerland, August. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Farhadi</author>
<author>I Endres</author>
<author>D Hoiem</author>
<author>D Forsyth</author>
</authors>
<title>Describing objects by their attributes.</title>
<date>2009</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1778--1785</pages>
<contexts>
<context position="1689" citStr="Farhadi et al., 2009" startWordPosition="252" endWordPosition="255">ry of people, animals and objects, their attributes, and their relationship to each other. Although different people may give different interpretations to the same picture, people can readily interpret photos and describe the entities and events they perceive in complex sentences. This level of image understanding still remains an elusive goal for computer vision: although current methods may be able to identify the overall scene (Quattoni and Torralba, 2009) or some specific classes of entities (Felzenszwalb et al., 2008), they are only starting to be able to identify attributes of entities (Farhadi et al., 2009), and are far from recovering a complete semantic interpretation of the depicted situation. Like natural language processing, computer vision requires suitable training data, and there are currently no publicly available data sets that would enable the development of such systems. Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections of images annotated with keywords (e.g. the Corel collection). A lot of recent work in computer vision has been aimed at predicting these keywords (Blei et al</context>
</contexts>
<marker>Farhadi, Endres, Hoiem, Forsyth, 2009</marker>
<rawString>A. Farhadi, I. Endres, D. Hoiem, and D. Forsyth. 2009. Describing objects by their attributes. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1778–1785, June.</rawString>
</citation>
<citation valid="true">
<title>WordNet An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA ; London,</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet An Electronic Lexical Database. The MIT Press, Cambridge, MA ; London, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Felzenszwalb</author>
<author>D McAllester</author>
<author>D Ramanan</author>
</authors>
<title>A discriminatively trained, multiscale, deformable part model.</title>
<date>2008</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="1596" citStr="Felzenszwalb et al., 2008" startWordPosition="236" endWordPosition="239">l these can be recovered. 1 Introduction Many photos capture a moment in time, telling a brief story of people, animals and objects, their attributes, and their relationship to each other. Although different people may give different interpretations to the same picture, people can readily interpret photos and describe the entities and events they perceive in complex sentences. This level of image understanding still remains an elusive goal for computer vision: although current methods may be able to identify the overall scene (Quattoni and Torralba, 2009) or some specific classes of entities (Felzenszwalb et al., 2008), they are only starting to be able to identify attributes of entities (Farhadi et al., 2009), and are far from recovering a complete semantic interpretation of the depicted situation. Like natural language processing, computer vision requires suitable training data, and there are currently no publicly available data sets that would enable the development of such systems. Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections of images annotated with keywords (e.g. the Corel collection). A </context>
</contexts>
<marker>Felzenszwalb, McAllester, Ramanan, 2008</marker>
<rawString>P. Felzenszwalb, D. McAllester, and D. Ramanan. 2008. A discriminatively trained, multiscale, deformable part model. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yansong Feng</author>
<author>Mirella Lapata</author>
</authors>
<title>Automatic image annotation using auxiliary text information.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<pages>272--280</pages>
<location>Columbus, Ohio,</location>
<contexts>
<context position="2341" citStr="Feng and Lapata, 2008" startWordPosition="356" endWordPosition="359"> complete semantic interpretation of the depicted situation. Like natural language processing, computer vision requires suitable training data, and there are currently no publicly available data sets that would enable the development of such systems. Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections of images annotated with keywords (e.g. the Corel collection). A lot of recent work in computer vision has been aimed at predicting these keywords (Blei et al., 2003; Barnard et al., 2003; Feng and Lapata, 2008; Deschacht and Moens, 2007; Jeon et al., 2003). But keywords alone are not expressive enough to capture relations between entities. Some research has used the text that surrounds an image in a news article as a proxy (Feng and Lapata, 2008; Deschacht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 images associated with several simple descript</context>
</contexts>
<marker>Feng, Lapata, 2008</marker>
<rawString>Yansong Feng and Mirella Lapata. 2008. Automatic image annotation using auxiliary text information. In Proceedings of ACL-08: HLT, pages 272–280, Columbus, Ohio, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Unsupervised coreference resolution in a nonparametric Bayesian model.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>848--855</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="14300" citStr="Haghighi and Klein (2007)" startWordPosition="2336" endWordPosition="2339">ibed below, and on the observation that salient entities will be mentioned in many captions and that captions tend to use similar words to describe the same entity. Simple heuristic algorithm: 1. For each noun, choose the synset that appears in the most number of captions of an image, and break ties by choosing the synset that covers the fewest distinct lemmatized nouns. 2. Group all of the noun phrase chunks that share a synset into a single coreference chain. 6 Bayesian coreference models Since we cannot afford to manually annotate our entire data set with coreference information, we follow Haghighi and Klein (2007)’s work on unsupervised coreference resolution, and develop a series of generative Bayesian models for our task. 6.1 Model 0: Simple Mixture Model In our first model, based on Haghighi and Klein’s baseline Dirichlet Process model, each image i corresponds to the set of observed mentions wi from across its captions. Image i has a hidden global topic Ti, drawn from a distribution with a GEM prior with hyperparameter -y as explained by Teh et al. (2006). In a Dirichlet process, the GEM distribution is an infinite analog of the Dirichlet distribution, allowing for a potentially infinite number of </context>
<context position="17273" citStr="Haghighi and Klein (2007)" startWordPosition="2890" endWordPosition="2893">each Ti is resampled based on Z12. 1. Sampling Zj: P(Zj =e|wj E wi,Z−j,T) a P(wj|Zj=e)P(Zj =e|Ti) „ −j P(wj = x |Zj = e) a ne,x α P fixe x0 n−j e,x0 + α mt,e −j /J + hi P(Zj = e|Ti = t) = [&apos;� e Lie M−j0 + N t 2. Sampling Ti: P(Ti =j|w,Z,T−i) a P(Ti =j|T−i)P(Z|Ti =j, T−i) Y a P(Ti = j|T−i) P(Zk|Ti = j) k∈w* Y = P(Ti = j|T−i) k∈w• P(Ti = j|T−i) a !ry, If its a new topic lPj Otherwise 11Since we do not have multiple images of the same wellknown people or places, referred to by their names, we do not perform any cross-image coreference 12Sampling on the exponentiated posterior to find the mode as Haghighi and Klein (2007) did was found to not significantly affect results on our tasks m−i+ N j,Zk Pe0 m−i j,e0 + N 165 Image 21: Caption 1: {x21,1:a golden retriever; x21,2 :a smaller black and brown dog; x21,3:a pink collar} Caption 2: {x21,4:a smaller black dog; x21,5:a larger brown dog; x21,6:a forest} Caption 3: {x21,7:small black and brown dog; x21,8:a large orange dog} Caption 4: {x21,9:brown dog; x21,10:mouth; x21,11:head; x21,12:black and tan dog} Caption 5: {x21,13:two dogs; x21,14:the woods} x21,8 x21,9 x21,1 x21,5 DOG attr:5 x21,2 x21,4 x21,7 x21,12 DOG attr:3 x21,3 x21,6 CLOTHING attr:2 x21,14 FOREST at</context>
</contexts>
<marker>Haghighi, Klein, 2007</marker>
<rawString>Aria Haghighi and Dan Klein. 2007. Unsupervised coreference resolution in a nonparametric Bayesian model. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 848–855, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Dan Klein</author>
</authors>
<title>Simple coreference resolution with rich syntactic and semantic features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1152--1161</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="37006" citStr="Haghighi and Klein (2009)" startWordPosition="6301" endWordPosition="6304">e collected a corpus of images asincomplete coreference chaining both completely change the semantics of an image. sociated with several simple descriptive captions, which provide more detailed information about the image than simple keywords. We plan to make this data set available for further research in computer vision and natural language processing. In order to enable the creation of a semantic representation of the image content that is consistent with the captions in our data set, we use WordNet and a series of Bayesian models to perform cross-caption coreference resolution. Similar to Haghighi and Klein (2009), who find that linguistic heuristics can provide very strong baselines for standard coreference resultion, relatively simple heuristics based on WordNet alone perform surprisingly well on our task, although they are outperformed by our Bayesian models for overall entity prediction. Since our generative models are based on Dirichlet Process priors, they are designed to favor a small number of unique entities per image. In the heuristic algorithm, this bias is built in explicitly, resulting in slightly higher performance on the coreference resolution task. However, while the generative models c</context>
</contexts>
<marker>Haghighi, Klein, 2009</marker>
<rawString>Aria Haghighi and Dan Klein. 2009. Simple coreference resolution with rich syntactic and semantic features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1152–1161, Singapore, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Hollink</author>
<author>M Worring</author>
</authors>
<title>Building a visual ontology for video retrieval.</title>
<date>2005</date>
<booktitle>In MULTIMEDIA ’05: Proceedings of the 13th annual ACM international conference on Multimedia,</booktitle>
<pages>479--482</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="8182" citStr="Hollink and Worring (2005)" startWordPosition="1292" endWordPosition="1295">ute the background of an image from those that appear in the foreground. These entities can be animate (people or animals) or inanimate. For inanimate objects, we distinguish static objects from “movable” objects. We also distinguish man-made from natural objects and backgrounds, since this matters for computer vision algorithms. We have labeled the entity mentions in our test and development data with classes from this ontology. Again, two of us annotated each image’s mentions, and adjudication was performed by a single person. Our ontology is similar to, but smaller than the one proposed by Hollink and Worring (2005) for video retrieval, which in turn is based on Hoogs et al. (2003) and Hunter (2001). 3 Predicting image entities from captions Figure 1 shows an image from our corpus. Different captions use different words to refer to the 5We used MMAX2 (M¨uller and Strube, 2006) both for annotation and adjudication. 163 Ontological Class Examples animal dog, horse, cow background man-made street, pool, carpet background natural ocean, field, air body part hair, mouth, arms clothing shirt, hat, sunglasses event trick, sunset, game fixed object man-made furniture, statue, ramp fixed object natural rock, pudd</context>
</contexts>
<marker>Hollink, Worring, 2005</marker>
<rawString>L. Hollink and M. Worring. 2005. Building a visual ontology for video retrieval. In MULTIMEDIA ’05: Proceedings of the 13th annual ACM international conference on Multimedia, pages 479–482, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Hoogs</author>
<author>J Rittscher</author>
<author>G Stein</author>
<author>J Schmiederer</author>
</authors>
<title>Video content annotation using visual analysis and a large semantic knowledgebase.</title>
<date>2003</date>
<booktitle>In IEEE Computer Society Conference on Computer Vision and Pattern Recognition,</booktitle>
<volume>2</volume>
<pages>pages</pages>
<contexts>
<context position="8249" citStr="Hoogs et al. (2003)" startWordPosition="1305" endWordPosition="1308">hese entities can be animate (people or animals) or inanimate. For inanimate objects, we distinguish static objects from “movable” objects. We also distinguish man-made from natural objects and backgrounds, since this matters for computer vision algorithms. We have labeled the entity mentions in our test and development data with classes from this ontology. Again, two of us annotated each image’s mentions, and adjudication was performed by a single person. Our ontology is similar to, but smaller than the one proposed by Hollink and Worring (2005) for video retrieval, which in turn is based on Hoogs et al. (2003) and Hunter (2001). 3 Predicting image entities from captions Figure 1 shows an image from our corpus. Different captions use different words to refer to the 5We used MMAX2 (M¨uller and Strube, 2006) both for annotation and adjudication. 163 Ontological Class Examples animal dog, horse, cow background man-made street, pool, carpet background natural ocean, field, air body part hair, mouth, arms clothing shirt, hat, sunglasses event trick, sunset, game fixed object man-made furniture, statue, ramp fixed object natural rock, puddle, bush image attribute camera, picture, closeup material man-made</context>
</contexts>
<marker>Hoogs, Rittscher, Stein, Schmiederer, 2003</marker>
<rawString>A. Hoogs, J. Rittscher, G. Stein, and J. Schmiederer. 2003. Video content annotation using visual analysis and a large semantic knowledgebase. In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, volume 2, pages II–327 – II–334 vol.2, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Hunter</author>
</authors>
<title>Adding multimedia to the semantic web - building an mpeg-7 ontology.</title>
<date>2001</date>
<booktitle>In In International Semantic Web Working Symposium (SWWS,</booktitle>
<pages>261--281</pages>
<contexts>
<context position="8267" citStr="Hunter (2001)" startWordPosition="1310" endWordPosition="1311">mate (people or animals) or inanimate. For inanimate objects, we distinguish static objects from “movable” objects. We also distinguish man-made from natural objects and backgrounds, since this matters for computer vision algorithms. We have labeled the entity mentions in our test and development data with classes from this ontology. Again, two of us annotated each image’s mentions, and adjudication was performed by a single person. Our ontology is similar to, but smaller than the one proposed by Hollink and Worring (2005) for video retrieval, which in turn is based on Hoogs et al. (2003) and Hunter (2001). 3 Predicting image entities from captions Figure 1 shows an image from our corpus. Different captions use different words to refer to the 5We used MMAX2 (M¨uller and Strube, 2006) both for annotation and adjudication. 163 Ontological Class Examples animal dog, horse, cow background man-made street, pool, carpet background natural ocean, field, air body part hair, mouth, arms clothing shirt, hat, sunglasses event trick, sunset, game fixed object man-made furniture, statue, ramp fixed object natural rock, puddle, bush image attribute camera, picture, closeup material man-made paint, frosting m</context>
</contexts>
<marker>Hunter, 2001</marker>
<rawString>Jane Hunter. 2001. Adding multimedia to the semantic web - building an mpeg-7 ontology. In In International Semantic Web Working Symposium (SWWS, pages 261–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lavrenko Manmatha Jeon</author>
<author>V Lavrenko</author>
<author>R Manmatha</author>
<author>J Jeon</author>
</authors>
<title>A model for learning the semantics of pictures.</title>
<date>2003</date>
<booktitle>In Seventeenth Annual Conference on Neural Information Processing Systems (NIPS).</booktitle>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2388" citStr="Jeon et al., 2003" startWordPosition="364" endWordPosition="367">situation. Like natural language processing, computer vision requires suitable training data, and there are currently no publicly available data sets that would enable the development of such systems. Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections of images annotated with keywords (e.g. the Corel collection). A lot of recent work in computer vision has been aimed at predicting these keywords (Blei et al., 2003; Barnard et al., 2003; Feng and Lapata, 2008; Deschacht and Moens, 2007; Jeon et al., 2003). But keywords alone are not expressive enough to capture relations between entities. Some research has used the text that surrounds an image in a news article as a proxy (Feng and Lapata, 2008; Deschacht and Moens, 2007). However, in many cases, the surrounding text or a user-provided caption does not simply describe what is depicted in the image (since this is usually obvious to the human reader for which this text is intended), but provides additional information. We have collected a corpus of 8108 images associated with several simple descriptive captions. In contrast to the text near an i</context>
</contexts>
<marker>Jeon, Lavrenko, Manmatha, Jeon, 2003</marker>
<rawString>Lavrenko Manmatha Jeon, V. Lavrenko, R. Manmatha, and J. Jeon. 2003. A model for learning the semantics of pictures. In Seventeenth Annual Conference on Neural Information Processing Systems (NIPS). MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Kachites McCallum</author>
</authors>
<title>Mallet: A machine learning for language toolkit.</title>
<date>2002</date>
<note>http://mallet.cs.umass.edu.</note>
<contexts>
<context position="29136" citStr="McCallum, 2002" startWordPosition="4983" endWordPosition="4984">ontological classes defined in section 2. The immediate difficulty of this task is that our ontology includes not only semantic distinctions, but also spatial and visual ones. While it may be easy to tell which words are animals and which are people, there is only a fine distinction at the language level whether an object is movable, fixed, or part of the background.14 8.1 Model and Features We define our task as a classification problem, where each entity must be assigned to one of twenty classes defined by our ontology. We use a Maximum Entropy classifier, implemented in the MALLET toolkit (McCallum, 2002), and define the following text features: NP Chunk: We include all the words in the NP chunk, unfiltered. WordNet Synsets and Hypernyms: The most likely synset is either the first one that appears in WordNet or one of the ones predicted by our coreference system. For each of these possibilities, we include all of that synset’s hypernyms. Syntactic Role: We parsed our captions with the C&amp;C parser (Clark and Curran, 2007), and record whether the word appears as a direct object of a verb, as the object of a preposition, as the subject of the sentence, or as a modifier. If it is a modifier, we als</context>
</contexts>
<marker>McCallum, 2002</marker>
<rawString>Andrew Kachites McCallum. 2002. Mallet: A machine learning for language toolkit. http://mallet.cs.umass.edu.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph M¨uller</author>
<author>Michael Strube</author>
</authors>
<title>Multilevel annotation of linguistic data with MMAX2.</title>
<date>2006</date>
<booktitle>Corpus Technology and Language Pedagogy,</booktitle>
<pages>197--214</pages>
<editor>In Sabine Braun et al, editor,</editor>
<location>Frankfurt a.M., Germany.</location>
<marker>M¨uller, Strube, 2006</marker>
<rawString>Christoph M¨uller and Michael Strube. 2006. Multilevel annotation of linguistic data with MMAX2. In Sabine Braun et al, editor, Corpus Technology and Language Pedagogy, pages 197–214. Peter Lang, Frankfurt a.M., Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ariadna Quattoni</author>
<author>Antonio B Torralba</author>
</authors>
<title>Recognizing indoor scenes.</title>
<date>2009</date>
<booktitle>In IEEE Conference on Computer Vision and Pattern Recognition,</booktitle>
<pages>413--420</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1531" citStr="Quattoni and Torralba, 2009" startWordPosition="226" endWordPosition="229">tology of entity classes that appear in images and evaluate how well these can be recovered. 1 Introduction Many photos capture a moment in time, telling a brief story of people, animals and objects, their attributes, and their relationship to each other. Although different people may give different interpretations to the same picture, people can readily interpret photos and describe the entities and events they perceive in complex sentences. This level of image understanding still remains an elusive goal for computer vision: although current methods may be able to identify the overall scene (Quattoni and Torralba, 2009) or some specific classes of entities (Felzenszwalb et al., 2008), they are only starting to be able to identify attributes of entities (Farhadi et al., 2009), and are far from recovering a complete semantic interpretation of the depicted situation. Like natural language processing, computer vision requires suitable training data, and there are currently no publicly available data sets that would enable the development of such systems. Photo sharing sites such as Flickr allow users to annotate images with keywords and other descriptions, and vision researchers have access to large collections </context>
</contexts>
<marker>Quattoni, Torralba, 2009</marker>
<rawString>Ariadna Quattoni and Antonio B. Torralba. 2009. Recognizing indoor scenes. In IEEE Conference on Computer Vision and Pattern Recognition, pages 413–420. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cyrus Rashtchian</author>
<author>Peter Young</author>
<author>Micah Hodosh</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Collecting image annotations using amazons mechanical turk.</title>
<date>2010</date>
<booktitle>In NAACL Workshop Creating Speech and Language Data With Amazons Mechanical Turk.</booktitle>
<contexts>
<context position="6028" citStr="Rashtchian et al. (2010)" startWordPosition="965" endWordPosition="968">e or sepia, as well as images with watermarks, signatures, borders or other obvious editing. Since our collection focuses on images depicting actions, we then filtered out images of scenery, portraits, and mood photography. This was done independently by two members of our group and adjudicated by a third. We paid Turk workers $0.10 to write one descriptive sentence for each of five distinct and randomly chosen images that were displayed one at a time. We required a small qualification test that examined the workers’ English grammar and spelling and we restricted the task to U.S. workers (see Rashtchian et al. (2010) for more details). Our final corpus contains five sentences for each of our 8108 images, totaling 478,317 word tokens, and an average sentence length of 11.8 words. We first spell-checked3 these sentences, and used OpenNLP4 to POS-tag them. We identified NPs using OpenNLP’s chunker, followed by a semi1https://www.mturk.com 2The groups:“strangers!”, “Wild-Child (Kids in Action)”, “Dogs in Action (Read the Rules)”, “Outdoor Activities”, “Action Photography”, “Flickr-Social (two or more people in the photo)”. 3We used Unix’s aspell to generate possible corrections and chose between them based on</context>
</contexts>
<marker>Rashtchian, Young, Hodosh, Hockenmaier, 2010</marker>
<rawString>Cyrus Rashtchian, Peter Young, Micah Hodosh, and Julia Hockenmaier. 2010. Collecting image annotations using amazons mechanical turk. In NAACL Workshop Creating Speech and Language Data With Amazons Mechanical Turk.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael I Jordan</author>
<author>Matthew J Beal</author>
<author>David M Blei</author>
</authors>
<title>Hierarchical dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>101</volume>
<issue>476</issue>
<contexts>
<context position="14754" citStr="Teh et al. (2006)" startWordPosition="2413" endWordPosition="2416">in. 6 Bayesian coreference models Since we cannot afford to manually annotate our entire data set with coreference information, we follow Haghighi and Klein (2007)’s work on unsupervised coreference resolution, and develop a series of generative Bayesian models for our task. 6.1 Model 0: Simple Mixture Model In our first model, based on Haghighi and Klein’s baseline Dirichlet Process model, each image i corresponds to the set of observed mentions wi from across its captions. Image i has a hidden global topic Ti, drawn from a distribution with a GEM prior with hyperparameter -y as explained by Teh et al. (2006). In a Dirichlet process, the GEM distribution is an infinite analog of the Dirichlet distribution, allowing for a potentially infinite number of mixture components. P(Ti = t) is proportional to -y if t is a new component, or to the number of times t has been drawn before otherwise. Given a topic choice Ti = t, entity type assignments Zj for all mentions wj in image i are in turn drawn from a topic-specific multinomial Bt over all possible entity types E that was drawn from a Dirichlet prior with hyperparameter Q. Similarly, given an entity type Zi = z, each corresponding (observed) head word </context>
<context position="18584" citStr="Teh et al., 2006" startWordPosition="3104" endWordPosition="3107">nchises: each image topic is a franchise, each image is a restaurant, each entity is a table, each mention is a customer. Model 2 adds attributes (in italics). 6.2 Model 1: Explicit Entities Model 0 does not have an explicit representation of entities beyond their type and thus cannot distinguish multiple entities of the same type in an image. Although Model 1 also represents mentions only by their head words (and thus cannot distinguish black dog from brown dog), it creates explicit entities based on the Chinese restaurant franchise interpretation of the hierarchical Dirichlet Process model (Teh et al., 2006). Figure 2 (ignoring the modifiers / attributes for now) illustrates the Chinese restaurant franchise interpretation of our model. Using this metaphor, there are a series of restaurants (= images), each consisting of a potentially infinite number of tables (= entities), that are frequented by customers (= entity mentions) who will be seated at the tables. Restaurants belong to franchises (= image topics). Each table is served one dish (= entity type, e.g. DOG, CLOTHING) shared by all the customers. The head word of a mention xi,j is generated in the following manner: customer j enters restaura</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael I Jordan, Matthew J Beal, and David M Blei. 2006. Hierarchical dirichlet processes. Journal of the American Statistical Association, 101(476):1566–1581.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>