<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997335">
Cross-lingual Dependency Parsing Based on Distributed Representations
</title>
<author confidence="0.997922">
Jiang Guo&apos;∗, Wanxiang Che&apos;, David Yarowsky2,Haifeng Wang3, Ting Liu&apos;
</author>
<affiliation confidence="0.835628333333333">
&apos;Center for Social Computing and Information Retrieval, Harbin Institute of Technology
2Center for Language and Speech Processing, Johns Hopkins University
3Baidu Inc., Beijing, China
</affiliation>
<email confidence="0.856743">
{jguo, car, tliu}@ir.hit.edu.cn
yarowsky@jhu.edu, wanghaifeng@baidu.com
</email>
<sectionHeader confidence="0.997361" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999544">
This paper investigates the problem of
cross-lingual dependency parsing, aim-
ing at inducing dependency parsers for
low-resource languages while using only
training data from a resource-rich lan-
guage (e.g. English). Existing approaches
typically don’t include lexical features,
which are not transferable across lan-
guages. In this paper, we bridge the lex-
ical feature gap by using distributed fea-
ture representations and their composition.
We provide two algorithms for inducing
cross-lingual distributed representations of
words, which map vocabularies from two
different languages into a common vector
space. Consequently, both lexical features
and non-lexical features can be used in our
model for cross-lingual transfer.
Furthermore, our framework is able to in-
corporate additional useful features such
as cross-lingual word clusters. Our com-
bined contributions achieve an average rel-
ative error reduction of 10.9% in labeled
attachment score as compared with the
delexicalized parser, trained on English
universal treebank and transferred to three
other languages. It also significantly out-
performs McDonald et al. (2013) aug-
mented with projected cluster features on
identical data.
</bodyText>
<sectionHeader confidence="0.999623" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998313787234043">
Dependency Parsing has been one of NLP’s long-
standing central problems. The majority of work
on dependency parsing has been dedicated to
resource-rich languages, such as English and Chi-
nese. For these languages, there exist large-scale
∗This work was done while the author was visiting JHU.
annotated treebanks that can be used for super-
vised training of dependency parsers. However,
for most of the languages in the world, there are
few or even no labeled training data for parsing,
and it is labor intensive and time-consuming to
manually build treebanks for all languages. This
fact has given rise to a number of research on un-
supervised methods (Klein and Manning, 2004),
annotation projection methods (Hwa et al., 2005),
and model transfer methods (McDonald et al.,
2011) for predicting linguistic structures. In this
study, we focus on the model transfer methods,
which attempt to build parsers for low-resource
languages by exploiting treebanks from resource-
rich languages.
The major obstacle in transferring a parsing
system from one language to another is the lex-
ical features, e.g. words, which are not directly
transferable across languages. To solve this prob-
lem, McDonald et al. (2011) build a delexical-
ized parser - a parser that only has non-lexical
features. A delexicalized parser makes sense in
that POS tag features are significantly predic-
tive for unlabeled dependency parsing. How-
ever, for labeled dependency parsing, especially
for semantic-oriented dependencies like Stanford-
type dependencies (De Marneffe et al., 2006;
De Marneffe and Manning, 2008), these non-
lexical features are not predictive enough. T¨ack-
str¨om et al. (2012) propose to learn cross-lingual
word clusters from multilingual paralleled un-
labeled data through word alignments, and ap-
ply these clusters as features for semi-supervised
delexicalized parsing. Word clusters can be
thought as a kind of coarse-grained representa-
tions of words. Thus, this approach partially fills
the gap of lexical features in cross-lingual learning
of dependency parsing.
This paper proposes a novel approach for cross-
lingual dependency parsing that is based on pure
distributed feature representations. In contrast to
</bodyText>
<page confidence="0.941949">
1234
</page>
<note confidence="0.976379666666667">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing, pages 1234–1244,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.999831857142857">
the discrete lexical features used in traditional de-
pendency parsers, distributed representations map
symbolic features into a continuous representation
space, that can be shared across languages. There-
fore, our model has the ability to utilize both lexi-
cal and non-lexical features naturally. Specifically,
our framework contains two primary components:
</bodyText>
<listItem confidence="0.974052">
• A neural network-based dependency parser.
We expect a non-linear model for depen-
dency parsing in our study, because dis-
tributed feature representations are shown to
be more effective in non-linear architectures
than in linear architectures (Wang and Man-
ning, 2013). Chen and Manning (2014) pro-
pose a transition-based dependency parser
using a neural network architecture, which
is simple but works well on several datasets.
</listItem>
<bodyText confidence="0.928183125">
Briefly, this model simply replaces the pre-
dictor in transition-based dependency parser
with a well-designed neural network classi-
fier. We will provide explanations for the
merits of this model in Section 3, as well as
how we adapt it to the cross-lingual task.
• Cross-lingual word representation learning.
The key to filling the lexical feature gap is to
project the representations of these features
from different languages into a common vec-
tor space, preserving the translational equiv-
alence. We will study and compare two ap-
proaches of learning cross-lingual word rep-
resentations in Section 4. The first approach
is robust projection, and the second approach
is based on canonical correlation analysis.
Both approaches are simple to implement and
are scalable to large data.
We evaluate our model on the universal multi-
lingual treebanks (McDonald et al., 2013). Case
studies include transferring from English to Ger-
man, Spanish and French. Experiments show that
by incorporating lexical features, the performance
of cross-lingual dependency parsing can be im-
proved significantly. By further embedding cross-
lingual cluster features (T¨ackstr¨om et al., 2012),
we achieve an average relative error reduction of
10.9% in labeled attachment score (LAS), as com-
pared with the delexicalized parsers. It also signif-
icantly outperforms McDonald et al. (2013) aug-
mented with cluster features on identical data. The
original major contributions of this paper include:
</bodyText>
<note confidence="0.4159795">
ROOT He has good control .
PRP VBZ JJ NN .
</note>
<figureCaption confidence="0.744703">
Figure 1: An example labeled dependency tree.
</figureCaption>
<listItem confidence="0.996542615384615">
• We propose a novel and flexible cross-lingual
learning framework for dependency parsing
based on distributed representations, which
can effectively incorporate both lexical and
non-lexical features.
• We present two novel and effective ap-
proaches for inducing cross-lingual word rep-
resentations, that bridge the lexical feature
gap in cross-lingual dependency parsing.
• We show that cross-lingual word cluster fea-
tures can be effectively embedded into our
model, leading to significant additive im-
provements.
</listItem>
<sectionHeader confidence="0.992054" genericHeader="introduction">
2 Background
</sectionHeader>
<subsectionHeader confidence="0.998178">
2.1 Dependency Parsing
</subsectionHeader>
<bodyText confidence="0.99990084">
Given an input sentence x = w0w,...wn, the goal
of dependency parsing is to build a dependency
tree (Figure 1), which can be denoted by d =
{(h,m,l) ∶ 0 ≤ h ≤ n;0 &lt; m ≤ n,l ∈ L}. (h, m, l)
indicates a directed arc from the head word wh to
the modifier wm with a dependency label l, and L
is the label set. The mainstream models that have
been proposed for dependency parsing can be de-
scribed as either graph-based models or transition-
based models (McDonald and Nivre, 2007).
Graph-based models view the parsing problem
as finding the highest scoring tree from a directed
graph. The score of a dependency tree is typi-
cally factored into scores of some small structures
(e.g. arcs) depending on the order of a model.
Transition-based models aim to predict a transi-
tion sequence from an initial parser state to some
terminal states, depending on the parsing history.
This approach has a lot of interest since it is fast
(linear time) and can incorporate rich non-local
features (Zhang and Nivre, 2011).
It has been considered that simple transition-
based parsing using greedy decoding and local
training is not as accurate as graph-based parsers
or transition-based parsers with beam-search and
</bodyText>
<figure confidence="0.6160212">
punct
root
nsubj
dobj
amod
</figure>
<page confidence="0.949445">
1235
</page>
<bodyText confidence="0.997922333333333">
global training (Zhang and Clark, 2011). Recently,
Chen and Manning (2014) show that greedy
transition-based parsers can be greatly improved
by using a well-designed neural network architec-
ture. This approach can be considered as a new
paradigm of parsing, in that it is based on pure dis-
tributed feature representations. In this study, we
choose Chen and Manning’s architecture to build
our basic dependency parsing model.
</bodyText>
<subsectionHeader confidence="0.99835">
2.2 Distributed Representations for NLP
</subsectionHeader>
<bodyText confidence="0.99960637037037">
In recent years, there has been a trend in the NLP
research community of learning distributed rep-
resentations for different natural language units,
from morphemes, words and phrases, to sentences
and documents. Using distributed representations,
these symbolic units are embedded into a low-
dimensional and continuous space, thus it is often
referred to as embeddings.1
In general, there are two major ways of apply-
ing distributed representations to NLP tasks. First,
they can be fed into existing supervised NLP sys-
tems as augmented features in a semi-supervised
manner. This kind of approach has been adopted
in a variety of applications (Turian et al., 2010).
Despite its simplicity and effectiveness, it has been
shown that the potential of distributed representa-
tions cannot be fully exploited in the generalized
linear models which are adopted in most of the ex-
isting NLP systems (Wang and Manning, 2013).
One remedy is to discretize the distributed feature
representations, as studied in Guo et al. (2014).
However, we believe that a non-linear system, e.g.
a neural network, is a more powerful and effec-
tive solution. Some decent progress has already
been made in this paradigm of NLP on various
tasks (Collobert et al., 2011; Chen and Manning,
2014; Sutskever et al., 2014).
</bodyText>
<sectionHeader confidence="0.9987175" genericHeader="method">
3 Transition-based Dependency Parsing:
A Neural Network Architecture
</sectionHeader>
<bodyText confidence="0.999731666666667">
In this section, we first briefly describe transition-
based dependency parsing and the arc-standard
parsing algorithm. Then we revisit the neural net-
work architecture for transition-based dependency
parsing proposed by Chen and Manning (2014).
As discussed in Section 2.1, transition-based
parsing aims to predict a transition sequence from
an initial parser state to the terminal state. Each
state is conventionally regarded as a configuration,
</bodyText>
<footnote confidence="0.898025">
1In this paper, these two terms are used interchangeably.
</footnote>
<figure confidence="0.8806354">
Transition actions
W2
Hidden units
W1
Embeddings
</figure>
<figureCaption confidence="0.95512">
Figure 2: Neural network model for dependency
</figureCaption>
<bodyText confidence="0.99451009375">
parsing. The Cluster features are introduced in
Section 5.2.
which typically consists of a stack S, a buffer B,
and a partially derived forest, i.e. a set of depen-
dency arcs A. Given an input word sequence x =
w1w2, ..., wn, the initial configuration can be rep-
resented as a tuple: ([w0]S, [w1w2, ..., wn]B,O),
and the terminal configuration is ([w0]S, []B, A),
where w0 is a pseudo word indicating the root
of the whole dependency tree. We consider the
arc-standard algorithm (Nivre, 2004) in this pa-
per, which defines three types of transition actions:
LEFT-ARC(l), RIGHT-ARC(l), and SHIFT, l is the
dependency label.
The typical approach for greedy arc-standard
parsing is to build a multi-class classifier (e.g.,
SVM, MaxEnt) of predicting the transition ac-
tion given a feature vector extracted from a spe-
cific configuration. While conventional feature
engineering suffers from the problem of sparsity,
incompleteness and expensive feature computa-
tion (Chen and Manning, 2014), the neural net-
work model provides a potential solution.
The architecture of the neural network-based
dependency parsing model is illustrated in Fig-
ure 2. Primarily, three types of information are
extracted from a configuration in Chen and Man-
ning’s model: word features, POS features and la-
bel features respectively. In this study, we add dis-
tance features indicating the distance between two
items, and valency features indicating the num-
ber of children for a given item (Zhang and Nivre,
</bodyText>
<figure confidence="0.9216569375">
EW Et El
Ed, E&amp;quot;, Ec
POS tags
Arc labels
Distance,
Valency, Cluster
Stack Buffer
ROOT has_VBZ good_JJ
control_NN ._.
nsubj
He_PRP
Configuration
Words
9 X = X3
1236
Word features
</figure>
<equation confidence="0.749386875">
EwSi, EwBi, i = 0, 1, 2
Ewlc1(Si), Ewrc1(Si), Ewlc2(Si), Ewrc2(Si),i = 0, 1
Ewlc1(lc1(Si)), Ewrc1(rc1(Si)),i = 0, 1
POS features
t
ESi EtBi, i = 0,1, 2
Etlc1(Si), Etrc1(Si), Etlc2(Si), Etrc2(Si),i = 0, 1
Etlc1(lc1(Si)), Etrc1(rc1(Si)),i = 0, 1
</equation>
<table confidence="0.674919666666667">
Label features
Distance: Ed(S0,S1), Ed S0,B0)
Valency: ElvS0, ElvS1, ErvS1
</table>
<tableCaption confidence="0.9673355">
Table 1: Feature templates of the neural network
parsing model. Ewp , Etp, Elp, Edp, Elvp , Erv
</tableCaption>
<bodyText confidence="0.999727823529412">
p indi-
cate the {word, POS, label, distance, left/right va-
lency} embeddings of the element at position p,
correspondingly. lc1 / rc1 is the first child in the
left / right, lc2 / rc2 is the second child in the left
/ right. Si and Bi refer to the ith elements respec-
tively in the stack and buffer.
2011). All of these features are projected to an em-
bedding layer via corresponding embedding matri-
ces, which will be estimated through the training
process. The complete feature templates used in
our system are shown in Table 1. Then, feature
compositions are performed at the hidden layer via
a cube activation function: g(x) = x3.
The cube activation function can be viewed as
a special case of low-rank tensor. Formally, g(x)
can be expanded as:
</bodyText>
<equation confidence="0.979065333333333">
g(w1x1 + ... + wmxm + b) =
E (wiwjwk)xixjxk + E b(wiwj)xixj + ...
i,j,k i,j
</equation>
<bodyText confidence="0.99994464516129">
If we treat the bias term as b x x0 where x0 =
1, then the weight corresponding to each feature
combination xixjxk is wiwjwk, which is exactly
the same as a rank-1 component tensor in the low-
rank form using CP tensor decomposition (Cao
and Khudanpur, 2014). Consequently, the cube
activation function implicitly derives full feature
combinations. An advantage of the cube activa-
tion function is that it is flexible for adding extra
features to the input. In fact, we can add as many
features as possible to the input layer to improve
the parsing accuracy. We will show in Section 5.2
that the Brown cluster features can be readily in-
corporated into our model.
Cross-lingual Transfer. The idea of cross-
lingual transfer using the parser we examined
above is straightforward. In contrast to tradi-
tional approaches that have to discard rich lexical
features (delexicalizing) when transferring mod-
els from one language to another, our model can
be transferred using the full model trained on the
source language side, i.e. English.
Since the non-lexical feature (POS, label, dis-
tance, valency) embeddings are directly transfer-
able between languages,2 the key component of
this framework is the cross-lingual learning of lex-
ical feature embeddings, i.e. word embeddings.
Once the cross-lingual word embeddings are in-
duced, we first learn a dependency parser at the
source language side. After that, the parser will be
directly used for parsing target language data.
</bodyText>
<sectionHeader confidence="0.7505245" genericHeader="method">
4 Cross-lingual Word Representation
Learning
</sectionHeader>
<bodyText confidence="0.9999412">
Prior to introducing our approaches for cross-
lingual word representation learning, we briefly
review the basic model for learning monolingual
word embeddings, which constitutes a subproce-
dure of the cross-lingual approaches.
</bodyText>
<subsectionHeader confidence="0.996263">
4.1 Continuous Bag-of-Words Model
</subsectionHeader>
<bodyText confidence="0.999830956521739">
Various approaches have been studied for learn-
ing word embeddings from large-scale plain
texts. In this study, we consider the Continuous
Bag-of-Words (CBOW) model (Mikolov et al.,
2013) as implemented in the open-source toolkit
word2vec.3 The basic principle of the CBOW
model is to predict each individual word in a se-
quence given the bag of its context words within a
fixed window size as input, using a log-linear clas-
sifier. This model avoids the non-linear transfor-
mation in hidden layers, and hence can be trained
with high efficiency.
With large window size, grouped words us-
ing the resulting word embeddings are more topi-
cally similar; whereas with small window size, the
grouped words will be more syntactically similar.
So we set the window size to 1 in our parsing task.
Next, we introduce our approach for inducing
bilingual word embeddings. In general, we ex-
pect our bilingual word embeddings to preserve
translational equivalences. For example, “cook-
ing” (English) should be close to its translation:
“kochen” (German) in the embedding space.
</bodyText>
<footnote confidence="0.853336666666667">
2POS tags are language-independent here since we use the
universal POS tags (Section 5).
3http://code.google.com/p/word2vec/
</footnote>
<equation confidence="0.9918834">
c2(Si), Erc2(Si), i = 0, 1
El lc1(lc1(Si)), El rc1(rc1(Si)),i = 0, 1
l l l l
l
Elc1(Si), Erc1(Si),
</equation>
<page confidence="0.955797">
1237
</page>
<subsectionHeader confidence="0.996288">
4.2 Robust Alignment-based Projection
</subsectionHeader>
<bodyText confidence="0.999965952380953">
Our first method for inducing cross-lingual word
embeddings has two stages. First, we learn word
embeddings from a source language (S) corpora
as in the monolingual case, and then project the
monolingual word embeddings to a target lan-
guage (T), based on word alignments.
Given a sentence-aligned parallel corpus D,
we first conduct unsupervised bidirectional word
alignment, and then collect an alignment dictio-
nary. Specifically, in each word-aligned sentence
pair of D, we keep all alignments with condi-
tional alignment probability exceeding a thresh-
old S = 0.95 and discard the others. Specifically,
let ATIS = {(wT i , wS j ,ci,j),i = 1,2,..., NT; j =
1, 2,..., NS} be the alignment dictionary, where
ci,j is the number of times when the ith target word
wT i is aligned to the jth source word wS j. NS and
NT are vocabulary sizes. We use the shorthand
(i, j) ∈ AT IS to denote a word pair in AT IS. The
projection can be formalized as the weighted aver-
age of the embeddings of translation words:
</bodyText>
<equation confidence="0.995884">
⋅ v(wSj ) (1)
</equation>
<bodyText confidence="0.999946">
where ci,⋅ = Ejci,j, v(w) is the embedding of w.
Obviously, the simple projection method has
one drawback, it only assigns word embeddings
for those target language words that occur in the
word aligned data, which is typically smaller than
the monolingual datasets. Therefore, in order to
improve the robustness of projection, we utilize
a morphology-inspired mechanism, to propagate
embeddings from in-vocabulary words to out-of-
vocabulary (OOV) words. Specifically, for each
OOV word wToov, we extract a list of candidate
words that is similar to it in terms of edit distance,
and then set the averaged vector as the embedding
of wToov. Formally,
</bodyText>
<equation confidence="0.997766">
v(T
woov) = Avg
w′∈C
</equation>
<bodyText confidence="0.911352666666667">
where C = {wIEditDist(wToov, w) ≤ T}
To reduce noise, we choose a small edit distance
threshold T = 1.
</bodyText>
<subsectionHeader confidence="0.995738">
4.3 Canonical Correlation Analysis
</subsectionHeader>
<bodyText confidence="0.9992365">
The second approach we consider is similar
to Faruqui and Dyer (2014), which use CCA to
improve monolingual word embeddings with mul-
tilingual correlation. CCA is a way of measur-
</bodyText>
<figureCaption confidence="0.785015">
Figure 3: CCA for cross-lingual word representa-
tion learning.
</figureCaption>
<bodyText confidence="0.9998084">
ing the linear relationship between multidimen-
sional variables. For two multidimensional vari-
ables, CCA aims to find two projection matrices to
map the original variables to a new basis (lower-
dimensional), such that the correlation between
the two variables is maximized.
Let’s treat CCA as a blackbox here, and see how
to apply CCA for inducing bilingual word embed-
dings. Suppose there are already two pre-trained
monolingual word embeddings (e.g. English and
German): E ∈ Rn1×d1 and Q ∈ Rn2×d2. At the first
step, we extract a one-to-one alignment dictionary
D ∶ E′ ↔ Q′ from the alignment dictionary ASST .4
Here, E′ ⊆ E, indicating that every word in E′ is
translated to one word in Q′ ⊆ Q, and vice versa.
The process is illustrated in Figure 3. Denot-
ing the dimension of resulting word embeddings
by d ≤ min(d1,d2). First, we derive two projec-
tion matrices V ∈ Rd1×d, W ∈ Rd2×d respectively
for E′ and Q′ using CCA:
</bodyText>
<equation confidence="0.573038">
V, W = CCA(E′, Q′) (3)
</equation>
<bodyText confidence="0.8705705">
Then, V and W are used to project the entire vo-
cabulary E and Q:
</bodyText>
<equation confidence="0.995998">
E∗ = EV, Q∗ = QW (4)
</equation>
<bodyText confidence="0.999372285714286">
where E∗ ∈ Rn1×d and Q∗ ∈ Rn2×d are the result-
ing word embeddings for our cross-lingual task.
Contrary to the projection approach, CCA as-
signs embeddings for every word in the monolin-
gual vocabulary. However, one potential limita-
tion is that CCA assumes linear transformation of
word embeddings, which is difficult to satisfy.
</bodyText>
<footnote confidence="0.9461985">
4ATIS is also worth trying, but we observed slight perfor-
mance degradation in our experimental setting.
</footnote>
<figure confidence="0.991234">
d1 d2
n1 E* n2 W
d
d
n1
E,
E
d1
V
d
n2
W
d
n�
�
d2
v(wTi ) = ci,j
(i,j)∈ATIS ci,⋅
(v(w′))
(2)
</figure>
<page confidence="0.966969">
1238
</page>
<bodyText confidence="0.999848714285714">
Note that both approaches can be generalize
to lower-resource languages where parallel bitexts
are not available. In that way, the dictionary A can
be readily obtained either using bilingual lexicon
induction approaches (Koehn and Knight, 2002;
Mann and Yarowsky, 2001; Haghighi et al., 2008),
or from resources like Wiktionary5 and Panlex.6
</bodyText>
<sectionHeader confidence="0.99993" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999194">
5.1 Data and Settings
</subsectionHeader>
<bodyText confidence="0.999183428571429">
For the pre-training of word embeddings, we use
the WMT-2011 monolingual news corpora for En-
glish, German and Spanish.7 For French, we
combined the WMT-2011 and WMT-2012 mono-
lingual news corpora.8 We obtained the word
alignment counts using the fast-align toolkit in
cdec (Dyer et al., 2010) from the parallel news
commentary corpora (WMT 2006-10) combined
with the Europarl corpus for English-{German,
Spanish, French}.9
For the training of the neural network depen-
dency parser, we set the number of hidden units to
400. The dimension of embeddings for different
features are shown in Table 2.
</bodyText>
<table confidence="0.983041">
Word POS Label Dist. Val. Cluster
Dim. 50 50 50 5 5 8
</table>
<tableCaption confidence="0.998057">
Table 2: Dimensions of feature embeddings.
</tableCaption>
<bodyText confidence="0.999762625">
Adaptive stochastic gradient descent (Ada-
Grad) (Duchi et al., 2011) is used for optimization.
For the CCA approach, we use the implementation
of Faruqui and Dyer (2014). The dimensions of
the monolingual embeddings (d1, d2) and the re-
sulting bilingual embeddings are set to 50 equally.
We employ the universal dependency treebanks
proposed by McDonald et al. (2013) for a reli-
able evaluation of our approach for cross-lingual
dependency parsing. The universal multilingual
treebanks are annotated using the universal POS
tagset (Petrov et al., 2011) which contains 12 POS
tags, as well as the universal dependencies which
contains 42 relations. We follow the standard split
of the treebanks for every language (DE, ES, and
FR).10
</bodyText>
<footnote confidence="0.999852333333333">
5https://www.wiktionary.org/
6http://panlex.org/
7http://www.statmt.org/wmt11/
8http://www.statmt.org/wmt12/
9http://www.statmt.org/europarl/
10http://code.google.com/p/uni-dep-tb/.
</footnote>
<subsectionHeader confidence="0.997398">
5.2 Baseline Systems
</subsectionHeader>
<bodyText confidence="0.999649333333334">
We compare our approach with three systems. For
the first baseline, we evaluate the delexicalized
transfer of our parser [DELEX], in which we only
use non-lexical features.
We also compare our approach with the delexi-
calized parser in McDonald et al. (2013) [McD13],
who used a perceptron-trained transition-based
parser with a beam of size 8, along with rich non-
local features (Zhang and Nivre, 2011).
Furthermore, we augment cross-lingual word
clusters to the perceptron-based delexicalized
parser, as proposed in T¨ackstr¨om et al. (2012). We
use the same alignment dictionary as described in
Section 4 to induce the cross-lingual word clus-
ters. We re-implement the PROJECTED cluster
approach in T¨ackstr¨om et al. (2012), which assigns
a target word to the cluster with which it is most
often aligned:
</bodyText>
<equation confidence="0.646735">
ci,j ⋅ ✶[c(wS j ) = k]
</equation>
<bodyText confidence="0.999981833333333">
This method also has the drawback that words that
do not occur in the alignment dictionary (OOV)
cannot be assigned a cluster. Therefore, we use
the same strategy as described in Section 4.2 to
find the most likely clusters for the OOV words.
Instead of the clustering model of Uszkoreit and
Brants (2008), we use Brown clustering (Brown
et al., 1992) to induce hierarchical word clusters,
where each word is represented as a bit-string.
We use the same word cluster feature templates
from T¨ackstr¨om et al. (2012), and set the number
of Brown clusters to 256.
</bodyText>
<subsectionHeader confidence="0.997273">
5.3 Experimental Results
</subsectionHeader>
<bodyText confidence="0.9732402">
All of the parsing models are trained using the de-
velopment data from English for early-stopping.
Table 3 lists the results of the cross-lingual trans-
fer experiments for dependency parsing. Table 4
further summarizes each of the experimental gains
detailed in Table 3.
Our delexicalized system obtains slightly lower
performance than those reported in McDonald
et al. (2013) (McD13), because we’re using
Before this dataset was carried out, the CoNLL multilingual
dependency treebanks (Buchholz and Marsi, 2006) were
often used for evaluation. However, the major problem is
that the dependency annotations vary for different languages
(e.g. the choice of lexical versus functional head), which
makes it impossible to evaluate the LAS.
</bodyText>
<equation confidence="0.863728">
c(wTi ) = arg max E
k (i,j)∈ATIS
</equation>
<page confidence="0.940066">
1239
</page>
<table confidence="0.9994343">
Unlabeled Attachment Score (UAS) AVG EN Labeled Attachment Score (LAS) AVG
EN DE ES FR DE ES FR
DELEX 83.67 57.01 68.05 68.85 64.64 79.42 47.12 56.99 57.78 53.96
PROJ 91.96 60.07 71.42 71.36 67.62 90.48 49.94 61.76 61.55 57.75
PROJ+Cluster 92.33 60.35 71.90 72.93 68.39 90.91 51.54 62.28 63.12 58.98
CCA 90.62† 59.42 68.87 69.58 65.96 88.88† 49.32 59.65 59.50 56.16
CCA+Cluster 92.03† 60.66 71.33 70.87 67.62 90.49† 51.29 61.69 61.50 58.16
MCD13 83.33 58.50 68.07 70.14 65.57 78.54 48.11 56.86 58.20 54.39
MCD13∗ 84.44 57.30 68.15 69.91 65.12 80.30 47.34 57.12 58.80 54.42
MCD13∗+Cluster 90.21 60.55 70.43 72.01 67.66 88.28 50.20 60.96 61.96 57.71
</table>
<tableCaption confidence="0.73498275">
Table 3: Cross-lingual transfer dependency parsing from English on the test dataset of 4 universal multi-
lingual treebanks. Results measured by unlabeled attachment score (UAS) and labeled attachment score
(LAS). * denotes our re-implementation of MCD13. Since the model varies for different target languages
in the CCA-based approach, † indicates the averaged UAS/LAS.
</tableCaption>
<table confidence="0.769720214285714">
Experimental Contribution DE/ES/FR Avg
PROJ vs. DELEX +3.79 (8.2%)
CCA vs. DELEX +2.19 (4.8%)
PROJ vs. MCD13∗ +3.33 (7.3%)
CCA vs. MCD13∗ +1.74 (3.8%)
PROJ+Cluster vs. PROJ +1.23 (2.9%)
CCA+Cluster vs. CCA +2.00 (4.6%)
MCD13∗+Cluster vs. MCD13∗ +3.29 (7.2%)
PROJ+Cluster vs. DELEX +5.02 (10.9%)
CCA+Cluster vs. DELEX +4.20 (9.1%)
PROJ+Cluster vs. MCD13∗ +4.46 (9.8%)
CCA+Cluster vs. MCD13∗ +3.74 (8.2%)
PROJ+Cluster vs. MCD13∗+Cluster +1.27 (3.0%)
CCA+Cluster vs. MCD13∗+Cluster +0.45 (1.1%)
</table>
<tableCaption confidence="0.799496">
Table 4: Summary of each of the experimental
</tableCaption>
<bodyText confidence="0.968077875">
gains detailed in Table 3, in both absolute LAS
gain and relative error reduction. All gains are sta-
tistically significant using MaltEval at p &lt; 0.01.12
greedy decoding and local training. Our re-
implementation of (McDonald et al., 2013) attains
comparable performance with MCD13.
For all languages we consider in this study, by
using cross-lingual word embeddings either from
alignment-based projection or CCA, we obtain
statistically significant improvements against the
delexicalized system, both in UAS and LAS.
Interestingly, we notice that PROJ consistently
performs better than CCA by a significant margin,
and is comparable to McD13*+Cluster. We will
give further analysis to this observation in Sec-
tion 5.3.1 and 5.3.2.
Our framework is flexible for incorporating
richer features simply by embedding them into
continuous vectors. Thus we further embed the
cross-lingual word cluster features into our model,
together with the proposed cross-lingual word em-
beddings. The cluster feature template used here
is similar to the POS tag feature templates:
Cluster features
</bodyText>
<equation confidence="0.960850666666667">
EcSi, Ec�i, i = 0, 1, 2
Eic1(Si), Ecrc1(Si), Eic2(Si), Ecrc2(Si), i = 0, 1
Eic1(lc1(Si)), Ecrc1(rc1(Si)),i = 0, 1
</equation>
<tableCaption confidence="0.886977">
Table 5: Word cluster feature templates.
</tableCaption>
<bodyText confidence="0.999860142857143">
As shown in Table 3, additive improvements are
obtained for both PROJ and CCA. Compared with
our delexicalized system, the relative error is re-
duced by up to 13.1% in UAS, and up to 12.6% in
LAS. The combined system further outperforms
McD13* augmented with cluster features signifi-
cantly .
</bodyText>
<subsectionHeader confidence="0.937537">
5.3.1 Effect of Robust Projection
</subsectionHeader>
<bodyText confidence="0.999995769230769">
Since in both PROJ and the induction of cross-
lingual word clusters, we use edit distance mea-
sure for OOV words, we would like to see how
this affects the performance of parsing.
Intuitively, higher coverage of projected words
in the test dataset should promote the parsing per-
formance more. To verify this, we further con-
duct experiments under both settings using the
PROJ+Cluster model. Results are shown in Ta-
ble 6. Improvements are observed for all lan-
guages when using robust projection with edit dis-
tance measure, especially for FR, where the high-
est coverage gain is obtained by robust projection.
</bodyText>
<subsectionHeader confidence="0.994227">
5.3.2 Fine-tuning of Word Embeddings
</subsectionHeader>
<bodyText confidence="0.969498333333333">
Another reason for the effectiveness of PROJ over
CCA lies in the fine-tuning of word embeddings
while training the parser.
</bodyText>
<page confidence="0.949879">
1240
</page>
<table confidence="0.9998932">
Simple Robust Δ
coverage 91.37 94.70 +3.33
DE UAS 59.74 60.35 +0.61
LAS 50.84 51.54 +0.70
coverage 94.51 96.67 +2.16
ES UAS 70.97 71.90 +0.93
LAS 61.34 62.28 +0.94
coverage 90.83 97.60 +6.77
FR UAS 71.17 72.93 +1.76
LAS 61.72 63.12 +1.40
</table>
<tableCaption confidence="0.999405">
Table 6: Effect of robust projection.
</tableCaption>
<bodyText confidence="0.9976734375">
CCA can be viewed as a joint method for in-
ducing cross-lingual word embeddings. When
training the source language dependency parser
with cross-lingual word embeddings derived from
CCA, the EN word embeddings should be fixed.
Otherwise, the translational equivalence will be
broken. However, for PROJ, there is no such limi-
tation. Word embeddings can be updated as other
non-lexical feature embeddings, in order to obtain
a more accurate dependency parser. We refer to
this procedure as a fine-tuning process to the word
embeddings. To verify the benefits of fine-tuning,
we conduct experiments to see relative loss if word
embeddings are fixed while training. Results are
shown in Table 7, which indicates that fine-tuning
indeed offers considerable help.
</bodyText>
<table confidence="0.999724142857143">
Fix Fine-tune Δ
DE UAS 59.74 60.07 +0.33
LAS 49.44 49.94 +0.50
ES UAS 70.10 71.42 +1.32
LAS 61.31 61.76 +0.45
FR UAS 70.65 71.36 +0.71
LAS 60.69 61.50 +0.81
</table>
<tableCaption confidence="0.999525">
Table 7: Effect of fine-tuning word embeddings.
</tableCaption>
<subsectionHeader confidence="0.9961045">
5.4 Compare with Existing Bilingual Word
Embeddings
</subsectionHeader>
<bodyText confidence="0.999951333333334">
In this section, we compare our bilingual em-
beddings with several previous approaches in the
context of dependency parsing. To the best of
our knowledge, this is the first work on eval-
uation of bilingual word embeddings in syntac-
tic tasks. The approaches we consider include
the multi-task learning approach (Klementiev et
al., 2012) [MTL], the bilingual auto-encoder ap-
proach (Chandar et al., 2014) [BIAE], the bilingual
compositional vector model (Hermann and Blun-
som, 2014) [BICVM], and the bilingual bag-of-
words approach (Gouws et al., 2014) [BILBOWA].
For MTL and BIAE, we adopt their released
word embeddings directly due to the inefficiency
of training.13 For BICVM and BILBOWA, we re-
run their systems on the same dataset as our pre-
vious experiments.14 Results are summarized in
Table 8. CCA and PROJ consistently outperforms
all other approaches in all languages, and PROJ
performs the best. The inferior performance of
MTL and BIAE is partly due to the low word
coverage. For example, they cover only 31% of
words in the universal DE test treebank, whereas
the CCA and PROJ covers over 70%. Moreover,
BIAE, BICVM and BILBOWA are optimized using
semantic-related objectives. So we suggest that
they are probably not well fit for syntactic tasks.
It is worth noting that we don’t assume/require
bilingual parallel data in CCA and PROJ. What
we need in practice is a bilingual lexicon for each
paired languages. This is especially important
for generalizing our approaches to lower-resource
languages, where parallel texts are not available.
</bodyText>
<sectionHeader confidence="0.997433" genericHeader="method">
6 Related Studies
</sectionHeader>
<bodyText confidence="0.99990832">
Existing approaches for cross-lingual dependency
parsing can be divided into three categories: cross-
lingual annotation projection methods, jointly
modeling methods and cross-lingual representa-
tion learning methods.
The cross-lingual annotation projection method
is first proposed in Yarowsky et al. (2001) for shal-
lower NLP tasks (POS tagging, NER, etc.). The
central idea is to project the syntactic annotations
from a resource-rich language to the target lan-
guage through word alignments, and then train a
supervised parser on the projected noisy annota-
tions (Hwa et al., 2005; Smith and Eisner, 2009;
Zhao et al., 2009; Jiang et al., 2011; Tiedemann,
2014; Tiedemann, 2015). Noises and errors intro-
duced by the word alignment and annotation pro-
jection processes can be reduced with robust pro-
jection methods by using graph-based label propa-
gation (Das and Petrov, 2011; Kim and Lee, 2012),
or by incorporating auxiliary resources (Kim et al.,
2012; Khapra et al., 2010).
The jointly modeling methods integrates the
monolingual grammar induction with bilingually-
projected dependency information (Liu et al.,
2013), or linguistic constraints via posterior
</bodyText>
<footnote confidence="0.998812">
13The MTL embeddings are normalized before training.
14BICVM only uses the bilingual parallel dataset.
</footnote>
<page confidence="0.964533">
1241
</page>
<note confidence="0.634706625">
UAS DE UAS ES UAS FR
LAS LAS LAS
MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30
BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47
BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13
BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65
CCA 59.42 49.32 68.87 59.65 69.58 59.50
PROJ 60.07 49.94 71.42 61.76 71.36 61.55
</note>
<tableCaption confidence="0.9757455">
Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their
released bilingual word embeddings.
</tableCaption>
<bodyText confidence="0.995152162162162">
regularization (Ganchev et al., 2009), manu-
ally constructed universal dependency parsing
rules (Naseem et al., 2010) and manually spec-
ified typological features (Naseem et al., 2012).
Besides dependency parsing, the joint modeling
method has also been applied for other multi-
lingual NLP tasks, including NER (Che et al.,
2013; Wang and Manning, 2014), SRL (Zhuang
and Zong, 2010; Titov and Klementiev, 2012) and
WSD (Guo and Diab, 2010).
The cross-lingual representation learning
method aims at building connections across
different languages by inducing language-
independent feature representations. After that, a
parser can be trained at the source-language side
within the induced feature space, and directly be
applied to the target language. Typical approaches
include cross-lingual word clustering (T¨ackstr¨om
et al., 2012) which is employed in this paper as a
baseline, projection features (Durrett et al., 2012).
Xiao and Guo (2014) learns cross-lingual word
embeddings and apply them with MSTParser for
linguistic transfer, which inspires this work.
It is worth mentioning that remarkable re-
sults on the universal dependency treebanks have
been achieved by using annotation projection
method (Tiedemann, 2014), treebank translation
method (Tiedemann and Nivre, 2014), and distri-
bution transferring method (Ma and Xia, 2014).
Unlike our approach, all of these methods in-
volve training a parser at the target language side.
Parallel bitexts are required in these methods,
which limits their scalability to lower-resource
languages. That said, these methods have the ad-
vantage that they are capable of capturing some
language-specific syntactic patterns which our ap-
proach cannot.15 These two kinds of approaches
</bodyText>
<footnote confidence="0.95762675">
15For example, in Spanish and French, adjectives often ap-
pears after nouns, thus forming a right-directed arc labeled
by amod, whereas in English, the amod arcs are mostly left-
directed.
</footnote>
<bodyText confidence="0.9727575">
are complementary, and can be integrated to push
the performance further.
</bodyText>
<sectionHeader confidence="0.998048" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999950238095238">
This paper proposes a novel framework based on
distributed representations for cross-lingual de-
pendency parsing. Two algorithms are proposed
for the induction of cross-lingual word represen-
tations: robust projection and CCA, which bridge
the lexical feature gap.
Experiments show that by using cross-lingual
word embeddings derived from either approach,
the transferred parsing performance can be im-
proved significantly against the delexicalized sys-
tem. A notable observation is that our projection
method performs significantly better than CCA,
a joint method. Additionally, our framework is
flexibly able to incorporate the cross-lingual word
cluster features, with further significant gains in
each use. The combined system significantly
outperforms the delexicalized system on all lan-
guages, by an average of 10.9% error reduction
on LAS, and further significantly outperforms Mc-
Donald et al. (2013) augmented with projected
cluster features.16
</bodyText>
<sectionHeader confidence="0.996978" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9997212">
We are grateful to Manaal Faruqui for providing
the bilingual resources. We thank Ryan McDon-
ald for pointing out the evaluation issue in the ex-
periment. We also thank Sharon Busching for the
proofreading and the anonymous reviewers for the
insightful comments and suggestions. This work
was supported by the National Key Basic Research
Program of China via grant 2014CB340503 and
the National Natural Science Foundation of China
(NSFC) via grant 61133012 and 61370164.
</bodyText>
<footnote confidence="0.9960255">
16Our system is publicly available at https://
github.com/jiangfeng1124/acl15-clnndep.
</footnote>
<page confidence="0.993748">
1242
</page>
<sectionHeader confidence="0.996147" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999284903846154">
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Sabine Buchholz and Erwin Marsi. 2006. Conll-x
shared task on multilingual dependency parsing. In
CoNLL, pages 149–164.
Yuan Cao and Sanjeev Khudanpur. 2014. Online
learning in tensor space. In ACL, pages 666–675.
Sarath Chandar, Stanislas Lauly, Hugo Larochelle,
Mitesh Khapra, Balaraman Ravindran, Vikas
Raykar, and Amrita Saha. 2014. An autoencoder
approach to learning bilingual word representations.
In NIPS, pages 1853–1861.
Wanxiang Che, Mengqiu Wang, Christopher D. Man-
ning, and Ting Liu. 2013. Named entity recognition
with bilingual constraints. In NAACL, pages 52–62.
Danqi Chen and Christopher D Manning. 2014. A fast
and accurate dependency parser using neural net-
works. In EMNLP, pages 740–750.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. JMLR, 12:2493–2537.
Dipanjan Das and Slav Petrov. 2011. Unsupervised
part-of-speech tagging with bilingual graph-based
projections. In ACL, pages 600–609.
Marie-Catherine De Marneffe and Christopher D Man-
ning. 2008. The stanford typed dependencies rep-
resentation. In Coling 2008: Proceedings of the
workshop on Cross-Framework and Cross-Domain
Parser Evaluation, pages 1–8.
Marie-Catherine De Marneffe, Bill MacCartney,
Christopher D Manning, et al. 2006. Generat-
ing typed dependency parses from phrase structure
parses. In LREC, volume 6, pages 449–454.
John Duchi, Elad Hazan, and Yoram Singer. 2011.
Adaptive subgradient methods for online learning
and stochastic optimization. JMLR, 12:2121–2159.
Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syn-
tactic transfer using a bilingual lexicon. In EMNLP,
pages 1–11, July.
Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan
Weese, Ferhan Ture, Phil Blunsom, Hendra Seti-
awan, Vladimir Eidelman, and Philip Resnik. 2010.
cdec: A decoder, alignment, and learning framework
for finite-state and context-free translation models.
In ACL.
Manaal Faruqui and Chris Dyer. 2014. Improving
vector space word representations using multilingual
correlation. In EACL, pages 462–471.
Kuzman Ganchev, Jennifer Gillenwater, and Ben
Taskar. 2009. Dependency grammar induction via
bitext projection constraints. In ACL-IJCNLP, pages
369–377.
Stephan Gouws, Yoshua Bengio, and Greg Corrado.
2014. Bilbowa: Fast bilingual distributed repre-
sentations without word alignments. arXiv preprint
arXiv:1410.2455.
Weiwei Guo and Mona Diab. 2010. Combining or-
thogonal monolingual and multilingual sources of
evidence for all words wsd. In ACL, pages 1542–
1551, July.
Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting
Liu. 2014. Revisiting embedding features for sim-
ple semi-supervised learning. In EMNLP, pages
110–120.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In ACL, volume 2008,
pages 771–779.
Karl Moritz Hermann and Phil Blunsom. 2014. Multi-
lingual models for compositional distributed seman-
tics. In ACL, pages 58–68, June.
Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara
Cabezas, and Okan Kolak. 2005. Bootstrapping
parsers via syntactic projection across parallel texts.
Natural language engineering, 11(03):311–325.
Wenbin Jiang, Qun Liu, and Yajuan Lv. 2011. Re-
laxed cross-lingual projection of constituent syntax.
In EMNLP, pages 1192–1201.
Mitesh Khapra, Saurabh Sohoney, Anup Kulkarni, and
Pushpak Bhattacharyya. 2010. Value for money:
Balancing annotation effort, lexicon building and ac-
curacy for multilingual wsd. In COLING, pages
555–563.
Seokhwan Kim and Gary Geunbae Lee. 2012. A
graph-based cross-lingual projection approach for
weakly supervised relation extraction. In ACL,
pages 48–53.
Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.
2012. Multilingual named entity recognition using
parallel data and metadata from wikipedia. In ACL,
pages 694–702.
Dan Klein and Christopher D Manning. 2004. Corpus-
based induction of syntactic structure: Models of de-
pendency and constituency. In ACL, page 478.
Alexandre Klementiev, Ivan Titov, and Binod Bhat-
tarai. 2012. Inducing crosslingual distributed repre-
sentations of words. In COLING, pages 1459–1474.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
Proceedings of the ACL-02 workshop on Unsuper-
vised lexical acquisition-Volume 9, pages 9–16.
</reference>
<page confidence="0.528736">
1243
</page>
<reference confidence="0.99968597979798">
Kai Liu, Yajuan L¨u, Wenbin Jiang, and Qun Liu. 2013.
Bilingually-guided monolingual dependency gram-
mar induction. In ACL, pages 1063–1072.
Xuezhe Ma and Fei Xia. 2014. Unsupervised depen-
dency parsing with transferring distribution via par-
allel guidance and entropy regularization. In ACL,
pages 1337–1348.
Gideon S Mann and David Yarowsky. 2001. Multipath
translation lexicon induction via bridge languages.
In NAACL, pages 1–8.
Ryan McDonald and Joakim Nivre. 2007. Character-
izing the errors of data-driven dependency parsing
models. In EMNLP-CoNLL, pages 122–131.
Ryan McDonald, Slav Petrov, and Keith Hall. 2011.
Multi-source transfer of delexicalized dependency
parsers. In EMNLP, pages 62–72.
Ryan T McDonald, Joakim Nivre, Yvonne Quirmbach-
Brundage, Yoav Goldberg, Dipanjan Das, Kuzman
Ganchev, Keith B Hall, Slav Petrov, Hao Zhang, Os-
car T¨ackstr¨om, et al. 2013. Universal dependency
annotation for multilingual parsing. In ACL, pages
92–97.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tahira Naseem, Harr Chen, Regina Barzilay, and Mark
Johnson. 2010. Using universal linguistic knowl-
edge to guide grammar induction. In EMNLP, pages
1234–1244.
Tahira Naseem, Regina Barzilay, and Amir Globerson.
2012. Selective sharing for multilingual dependency
parsing. In ACL, pages 629–637.
Joakim Nivre. 2004. Incrementality in deterministic
dependency parsing. In Proceedings of the Work-
shop on Incremental Parsing: Bringing Engineering
and Cognition Together, pages 50–57.
Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011.
A universal part-of-speech tagset. arXiv preprint
arXiv:1104.2086.
David A Smith and Jason Eisner. 2009. Parser adap-
tation and projection with quasi-synchronous gram-
mar features. In EMNLP, pages 822–831. Associa-
tion for Computational Linguistics.
Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014.
Sequence to sequence learning with neural net-
works. In NIPS, pages 3104–3112.
Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszko-
reit. 2012. Cross-lingual word clusters for direct
transfer of linguistic structure. In NAACL, pages
477–487.
J¨org Tiedemann and Joakim Nivre. 2014. Tree-
bank translation for cross-lingual parser induction.
CoNLL-2014, page 130.
J¨org Tiedemann. 2014. Rediscovering annotation pro-
jection for cross-lingual parser induction. In Proc.
COLING.
J¨org Tiedemann. 2015. Improving the cross-lingual
projection of syntactic dependencies. In Nordic
Conference of Computational Linguistics NODAL-
IDA 2015, page 191.
Ivan Titov and Alexandre Klementiev. 2012. Crosslin-
gual induction of semantic roles. In ACL, pages
647–656.
Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.
Word representations: a simple and general method
for semi-supervised learning. In ACL, pages 384–
394.
Jakob Uszkoreit and Thorsten Brants. 2008. Dis-
tributed word clustering for large scale class-based
language modeling in machine translation. In ACL,
pages 755–762.
Mengqiu Wang and Christopher D. Manning. 2013.
Effect of non-linear deep architecture in sequence
labeling. In IJCNLP, pages 1285–1291.
Mengqiu Wang and Christopher D Manning. 2014.
Cross-lingual projected expectation regularization
for weakly supervised learning. TACL, 2:55–66.
Min Xiao and Yuhong Guo. 2014. Distributed word
representation learning for cross-lingual dependency
parsing. In CoNLL, pages 119–129.
David Yarowsky, Grace Ngai, and Richard Wicen-
towski. 2001. Inducing multilingual text analysis
tools via robust projection across aligned corpora.
In Proceedings of the first international conference
on Human language technology research, pages 1–
8.
Yue Zhang and Stephen Clark. 2011. Syntactic pro-
cessing using the generalized perceptron and beam
search. Computational Linguistics, 37(1):105–151.
Yue Zhang and Joakim Nivre. 2011. Transition-based
dependency parsing with rich non-local features. In
ACL, pages 188–193.
Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou.
2009. Cross language dependency parsing using a
bilingual lexicon. In ACL-IJCNLP, pages 55–63.
Tao Zhuang and Chengqing Zong. 2010. Joint in-
ference for bilingual semantic role labeling. In
EMNLP, pages 304–314.
</reference>
<page confidence="0.99371">
1244
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.113524">
<title confidence="0.990544">Cross-lingual Dependency Parsing Based on Distributed Representations</title>
<author confidence="0.759029">Wanxiang David Ting</author>
<title confidence="0.477708">for Social Computing and Information Retrieval, Harbin Institute of for Language and Speech Processing, Johns Hopkins</title>
<address confidence="0.68134">Inc., Beijing,</address>
<email confidence="0.8088295">car,yarowsky@jhu.edu,wanghaifeng@baidu.com</email>
<abstract confidence="0.999148">This paper investigates the problem of cross-lingual dependency parsing, aiming at inducing dependency parsers for low-resource languages while using only training data from a resource-rich language (e.g. English). Existing approaches typically don’t include lexical features, which are not transferable across lan- In this paper, we bridge the lexfeature gap using distributed feature representations and their composition. We provide two algorithms for inducing cross-lingual distributed representations of words, which map vocabularies from two different languages into a common vector space. Consequently, both lexical features and non-lexical features can be used in our model for cross-lingual transfer. Furthermore, our framework is able to incorporate additional useful features such as cross-lingual word clusters. Our combined contributions achieve an average relative error reduction of 10.9% in labeled attachment score as compared with the delexicalized parser, trained on English universal treebank and transferred to three other languages. It also significantly outperforms McDonald et al. (2013) augmented with projected cluster features on identical data.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="23364" citStr="Brown et al., 1992" startWordPosition="3710" endWordPosition="3713">ignment dictionary as described in Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: ci,j ⋅ ✶[c(wS j ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et </context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>Conll-x shared task on multilingual dependency parsing. In CoNLL,</title>
<date>2006</date>
<pages>149--164</pages>
<contexts>
<context position="24111" citStr="Buchholz and Marsi, 2006" startWordPosition="3825" endWordPosition="3828">e templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower performance than those reported in McDonald et al. (2013) (McD13), because we’re using Before this dataset was carried out, the CoNLL multilingual dependency treebanks (Buchholz and Marsi, 2006) were often used for evaluation. However, the major problem is that the dependency annotations vary for different languages (e.g. the choice of lexical versus functional head), which makes it impossible to evaluate the LAS. c(wTi ) = arg max E k (i,j)∈ATIS 1239 Unlabeled Attachment Score (UAS) AVG EN Labeled Attachment Score (LAS) AVG EN DE ES FR DE ES FR DELEX 83.67 57.01 68.05 68.85 64.64 79.42 47.12 56.99 57.78 53.96 PROJ 91.96 60.07 71.42 71.36 67.62 90.48 49.94 61.76 61.55 57.75 PROJ+Cluster 92.33 60.35 71.90 72.93 68.39 90.91 51.54 62.28 63.12 58.98 CCA 90.62† 59.42 68.87 69.58 65.96 88.</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. Conll-x shared task on multilingual dependency parsing. In CoNLL, pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Cao</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Online learning in tensor space.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>666--675</pages>
<contexts>
<context position="13726" citStr="Cao and Khudanpur, 2014" startWordPosition="2139" endWordPosition="2142">The complete feature templates used in our system are shown in Table 1. Then, feature compositions are performed at the hidden layer via a cube activation function: g(x) = x3. The cube activation function can be viewed as a special case of low-rank tensor. Formally, g(x) can be expanded as: g(w1x1 + ... + wmxm + b) = E (wiwjwk)xixjxk + E b(wiwj)xixj + ... i,j,k i,j If we treat the bias term as b x x0 where x0 = 1, then the weight corresponding to each feature combination xixjxk is wiwjwk, which is exactly the same as a rank-1 component tensor in the lowrank form using CP tensor decomposition (Cao and Khudanpur, 2014). Consequently, the cube activation function implicitly derives full feature combinations. An advantage of the cube activation function is that it is flexible for adding extra features to the input. In fact, we can add as many features as possible to the input layer to improve the parsing accuracy. We will show in Section 5.2 that the Brown cluster features can be readily incorporated into our model. Cross-lingual Transfer. The idea of crosslingual transfer using the parser we examined above is straightforward. In contrast to traditional approaches that have to discard rich lexical features (d</context>
</contexts>
<marker>Cao, Khudanpur, 2014</marker>
<rawString>Yuan Cao and Sanjeev Khudanpur. 2014. Online learning in tensor space. In ACL, pages 666–675.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarath Chandar</author>
<author>Stanislas Lauly</author>
<author>Hugo Larochelle</author>
<author>Mitesh Khapra</author>
<author>Balaraman Ravindran</author>
<author>Vikas Raykar</author>
<author>Amrita Saha</author>
</authors>
<title>An autoencoder approach to learning bilingual word representations.</title>
<date>2014</date>
<booktitle>In NIPS,</booktitle>
<pages>1853--1861</pages>
<contexts>
<context position="29940" citStr="Chandar et al., 2014" startWordPosition="4747" endWordPosition="4750">+0.33 LAS 49.44 49.94 +0.50 ES UAS 70.10 71.42 +1.32 LAS 61.31 61.76 +0.45 FR UAS 70.65 71.36 +0.71 LAS 60.69 61.50 +0.81 Table 7: Effect of fine-tuning word embeddings. 5.4 Compare with Existing Bilingual Word Embeddings In this section, we compare our bilingual embeddings with several previous approaches in the context of dependency parsing. To the best of our knowledge, this is the first work on evaluation of bilingual word embeddings in syntactic tasks. The approaches we consider include the multi-task learning approach (Klementiev et al., 2012) [MTL], the bilingual auto-encoder approach (Chandar et al., 2014) [BIAE], the bilingual compositional vector model (Hermann and Blunsom, 2014) [BICVM], and the bilingual bag-ofwords approach (Gouws et al., 2014) [BILBOWA]. For MTL and BIAE, we adopt their released word embeddings directly due to the inefficiency of training.13 For BICVM and BILBOWA, we rerun their systems on the same dataset as our previous experiments.14 Results are summarized in Table 8. CCA and PROJ consistently outperforms all other approaches in all languages, and PROJ performs the best. The inferior performance of MTL and BIAE is partly due to the low word coverage. For example, they </context>
<context position="32499" citStr="Chandar et al., 2014" startWordPosition="5148" endWordPosition="5151"> reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the j</context>
</contexts>
<marker>Chandar, Lauly, Larochelle, Khapra, Ravindran, Raykar, Saha, 2014</marker>
<rawString>Sarath Chandar, Stanislas Lauly, Hugo Larochelle, Mitesh Khapra, Balaraman Ravindran, Vikas Raykar, and Amrita Saha. 2014. An autoencoder approach to learning bilingual word representations. In NIPS, pages 1853–1861.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wanxiang Che</author>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
<author>Ting Liu</author>
</authors>
<title>Named entity recognition with bilingual constraints.</title>
<date>2013</date>
<booktitle>In NAACL,</booktitle>
<pages>52--62</pages>
<contexts>
<context position="33207" citStr="Che et al., 2013" startWordPosition="5258" endWordPosition="5261">.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao</context>
</contexts>
<marker>Che, Wang, Manning, Liu, 2013</marker>
<rawString>Wanxiang Che, Mengqiu Wang, Christopher D. Manning, and Ting Liu. 2013. Named entity recognition with bilingual constraints. In NAACL, pages 52–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Danqi Chen</author>
<author>Christopher D Manning</author>
</authors>
<title>A fast and accurate dependency parser using neural networks.</title>
<date>2014</date>
<booktitle>In EMNLP,</booktitle>
<pages>740--750</pages>
<contexts>
<context position="4711" citStr="Chen and Manning (2014)" startWordPosition="680" endWordPosition="683">atures used in traditional dependency parsers, distributed representations map symbolic features into a continuous representation space, that can be shared across languages. Therefore, our model has the ability to utilize both lexical and non-lexical features naturally. Specifically, our framework contains two primary components: • A neural network-based dependency parser. We expect a non-linear model for dependency parsing in our study, because distributed feature representations are shown to be more effective in non-linear architectures than in linear architectures (Wang and Manning, 2013). Chen and Manning (2014) propose a transition-based dependency parser using a neural network architecture, which is simple but works well on several datasets. Briefly, this model simply replaces the predictor in transition-based dependency parser with a well-designed neural network classifier. We will provide explanations for the merits of this model in Section 3, as well as how we adapt it to the cross-lingual task. • Cross-lingual word representation learning. The key to filling the lexical feature gap is to project the representations of these features from different languages into a common vector space, preservin</context>
<context position="8244" citStr="Chen and Manning (2014)" startWordPosition="1245" endWordPosition="1248">arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy decoding and local training is not as accurate as graph-based parsers or transition-based parsers with beam-search and punct root nsubj dobj amod 1235 global training (Zhang and Clark, 2011). Recently, Chen and Manning (2014) show that greedy transition-based parsers can be greatly improved by using a well-designed neural network architecture. This approach can be considered as a new paradigm of parsing, in that it is based on pure distributed feature representations. In this study, we choose Chen and Manning’s architecture to build our basic dependency parsing model. 2.2 Distributed Representations for NLP In recent years, there has been a trend in the NLP research community of learning distributed representations for different natural language units, from morphemes, words and phrases, to sentences and documents.</context>
<context position="9890" citStr="Chen and Manning, 2014" startWordPosition="1507" endWordPosition="1510">ions (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Sutskever et al., 2014). 3 Transition-based Dependency Parsing: A Neural Network Architecture In this section, we first briefly describe transitionbased dependency parsing and the arc-standard parsing algorithm. Then we revisit the neural network architecture for transition-based dependency parsing proposed by Chen and Manning (2014). As discussed in Section 2.1, transition-based parsing aims to predict a transition sequence from an initial parser state to the terminal state. Each state is conventionally regarded as a configuration, 1In this paper, these two terms are used interchangeably. T</context>
<context position="11564" citStr="Chen and Manning, 2014" startWordPosition="1763" endWordPosition="1766"> A), where w0 is a pseudo word indicating the root of the whole dependency tree. We consider the arc-standard algorithm (Nivre, 2004) in this paper, which defines three types of transition actions: LEFT-ARC(l), RIGHT-ARC(l), and SHIFT, l is the dependency label. The typical approach for greedy arc-standard parsing is to build a multi-class classifier (e.g., SVM, MaxEnt) of predicting the transition action given a feature vector extracted from a specific configuration. While conventional feature engineering suffers from the problem of sparsity, incompleteness and expensive feature computation (Chen and Manning, 2014), the neural network model provides a potential solution. The architecture of the neural network-based dependency parsing model is illustrated in Figure 2. Primarily, three types of information are extracted from a configuration in Chen and Manning’s model: word features, POS features and label features respectively. In this study, we add distance features indicating the distance between two items, and valency features indicating the number of children for a given item (Zhang and Nivre, EW Et El Ed, E&amp;quot;, Ec POS tags Arc labels Distance, Valency, Cluster Stack Buffer ROOT has_VBZ good_JJ control</context>
</contexts>
<marker>Chen, Manning, 2014</marker>
<rawString>Danqi Chen and Christopher D Manning. 2014. A fast and accurate dependency parser using neural networks. In EMNLP, pages 740–750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<pages>12--2493</pages>
<publisher>JMLR,</publisher>
<contexts>
<context position="9866" citStr="Collobert et al., 2011" startWordPosition="1503" endWordPosition="1506">in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Sutskever et al., 2014). 3 Transition-based Dependency Parsing: A Neural Network Architecture In this section, we first briefly describe transitionbased dependency parsing and the arc-standard parsing algorithm. Then we revisit the neural network architecture for transition-based dependency parsing proposed by Chen and Manning (2014). As discussed in Section 2.1, transition-based parsing aims to predict a transition sequence from an initial parser state to the terminal state. Each state is conventionally regarded as a configuration, 1In this paper, these two terms are</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. JMLR, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Slav Petrov</author>
</authors>
<title>Unsupervised part-of-speech tagging with bilingual graph-based projections.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>600--609</pages>
<contexts>
<context position="31978" citStr="Das and Petrov, 2011" startWordPosition="5068" endWordPosition="5071">ion method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 4</context>
</contexts>
<marker>Das, Petrov, 2011</marker>
<rawString>Dipanjan Das and Slav Petrov. 2011. Unsupervised part-of-speech tagging with bilingual graph-based projections. In ACL, pages 600–609.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<marker>De Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine De Marneffe and Christopher D Manning. 2008. The stanford typed dependencies representation. In Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Duchi</author>
<author>Elad Hazan</author>
<author>Yoram Singer</author>
</authors>
<title>Adaptive subgradient methods for online learning and stochastic optimization.</title>
<date>2011</date>
<journal>JMLR,</journal>
<pages>12--2121</pages>
<contexts>
<context position="21318" citStr="Duchi et al., 2011" startWordPosition="3401" endWordPosition="3404">ned the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units to 400. The dimension of embeddings for different features are shown in Table 2. Word POS Label Dist. Val. Cluster Dim. 50 50 50 5 5 8 Table 2: Dimensions of feature embeddings. Adaptive stochastic gradient descent (AdaGrad) (Duchi et al., 2011) is used for optimization. For the CCA approach, we use the implementation of Faruqui and Dyer (2014). The dimensions of the monolingual embeddings (d1, d2) and the resulting bilingual embeddings are set to 50 equally. We employ the universal dependency treebanks proposed by McDonald et al. (2013) for a reliable evaluation of our approach for cross-lingual dependency parsing. The universal multilingual treebanks are annotated using the universal POS tagset (Petrov et al., 2011) which contains 12 POS tags, as well as the universal dependencies which contains 42 relations. We follow the standard</context>
</contexts>
<marker>Duchi, Hazan, Singer, 2011</marker>
<rawString>John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. JMLR, 12:2121–2159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Durrett</author>
<author>Adam Pauls</author>
<author>Dan Klein</author>
</authors>
<title>Syntactic transfer using a bilingual lexicon. In</title>
<date>2012</date>
<booktitle>EMNLP,</booktitle>
<pages>1--11</pages>
<contexts>
<context position="33801" citStr="Durrett et al., 2012" startWordPosition="5344" endWordPosition="5347">uding NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-</context>
</contexts>
<marker>Durrett, Pauls, Klein, 2012</marker>
<rawString>Greg Durrett, Adam Pauls, and Dan Klein. 2012. Syntactic transfer using a bilingual lexicon. In EMNLP, pages 1–11, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Adam Lopez</author>
<author>Juri Ganitkevitch</author>
<author>Johnathan Weese</author>
<author>Ferhan Ture</author>
<author>Phil Blunsom</author>
<author>Hendra Setiawan</author>
<author>Vladimir Eidelman</author>
<author>Philip Resnik</author>
</authors>
<title>cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models.</title>
<date>2010</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="20850" citStr="Dyer et al., 2010" startWordPosition="3324" endWordPosition="3327">r-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units to 400. The dimension of embeddings for different features are shown in Table 2. Word POS Label Dist. Val. Cluster Dim. 50 50 50 5 5 8 Table 2: Dimensions of feature embeddings. Adaptive stochastic gradient descent (AdaGrad) (Duchi et al., 2011) is used for optimization. For the CCA approach, we use the implementation of Faruqui and Dyer (2014). The dimensions of the monolin</context>
</contexts>
<marker>Dyer, Lopez, Ganitkevitch, Weese, Ture, Blunsom, Setiawan, Eidelman, Resnik, 2010</marker>
<rawString>Chris Dyer, Adam Lopez, Juri Ganitkevitch, Johnathan Weese, Ferhan Ture, Phil Blunsom, Hendra Setiawan, Vladimir Eidelman, and Philip Resnik. 2010. cdec: A decoder, alignment, and learning framework for finite-state and context-free translation models. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manaal Faruqui</author>
<author>Chris Dyer</author>
</authors>
<title>Improving vector space word representations using multilingual correlation.</title>
<date>2014</date>
<booktitle>In EACL,</booktitle>
<pages>462--471</pages>
<contexts>
<context position="18450" citStr="Faruqui and Dyer (2014)" startWordPosition="2904" endWordPosition="2907">al datasets. Therefore, in order to improve the robustness of projection, we utilize a morphology-inspired mechanism, to propagate embeddings from in-vocabulary words to out-ofvocabulary (OOV) words. Specifically, for each OOV word wToov, we extract a list of candidate words that is similar to it in terms of edit distance, and then set the averaged vector as the embedding of wToov. Formally, v(T woov) = Avg w′∈C where C = {wIEditDist(wToov, w) ≤ T} To reduce noise, we choose a small edit distance threshold T = 1. 4.3 Canonical Correlation Analysis The second approach we consider is similar to Faruqui and Dyer (2014), which use CCA to improve monolingual word embeddings with multilingual correlation. CCA is a way of measurFigure 3: CCA for cross-lingual word representation learning. ing the linear relationship between multidimensional variables. For two multidimensional variables, CCA aims to find two projection matrices to map the original variables to a new basis (lowerdimensional), such that the correlation between the two variables is maximized. Let’s treat CCA as a blackbox here, and see how to apply CCA for inducing bilingual word embeddings. Suppose there are already two pre-trained monolingual wor</context>
<context position="21419" citStr="Faruqui and Dyer (2014)" startWordPosition="3418" endWordPosition="3421">ing the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units to 400. The dimension of embeddings for different features are shown in Table 2. Word POS Label Dist. Val. Cluster Dim. 50 50 50 5 5 8 Table 2: Dimensions of feature embeddings. Adaptive stochastic gradient descent (AdaGrad) (Duchi et al., 2011) is used for optimization. For the CCA approach, we use the implementation of Faruqui and Dyer (2014). The dimensions of the monolingual embeddings (d1, d2) and the resulting bilingual embeddings are set to 50 equally. We employ the universal dependency treebanks proposed by McDonald et al. (2013) for a reliable evaluation of our approach for cross-lingual dependency parsing. The universal multilingual treebanks are annotated using the universal POS tagset (Petrov et al., 2011) which contains 12 POS tags, as well as the universal dependencies which contains 42 relations. We follow the standard split of the treebanks for every language (DE, ES, and FR).10 5https://www.wiktionary.org/ 6http://p</context>
</contexts>
<marker>Faruqui, Dyer, 2014</marker>
<rawString>Manaal Faruqui and Chris Dyer. 2014. Improving vector space word representations using multilingual correlation. In EACL, pages 462–471.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jennifer Gillenwater</author>
<author>Ben Taskar</author>
</authors>
<title>Dependency grammar induction via bitext projection constraints.</title>
<date>2009</date>
<booktitle>In ACL-IJCNLP,</booktitle>
<pages>369--377</pages>
<contexts>
<context position="32919" citStr="Ganchev et al., 2009" startWordPosition="5214" endWordPosition="5217">d before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained a</context>
</contexts>
<marker>Ganchev, Gillenwater, Taskar, 2009</marker>
<rawString>Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar. 2009. Dependency grammar induction via bitext projection constraints. In ACL-IJCNLP, pages 369–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Gouws</author>
<author>Yoshua Bengio</author>
<author>Greg Corrado</author>
</authors>
<title>Bilbowa: Fast bilingual distributed representations without word alignments. arXiv preprint arXiv:1410.2455.</title>
<date>2014</date>
<contexts>
<context position="30086" citStr="Gouws et al., 2014" startWordPosition="4769" endWordPosition="4772">ning word embeddings. 5.4 Compare with Existing Bilingual Word Embeddings In this section, we compare our bilingual embeddings with several previous approaches in the context of dependency parsing. To the best of our knowledge, this is the first work on evaluation of bilingual word embeddings in syntactic tasks. The approaches we consider include the multi-task learning approach (Klementiev et al., 2012) [MTL], the bilingual auto-encoder approach (Chandar et al., 2014) [BIAE], the bilingual compositional vector model (Hermann and Blunsom, 2014) [BICVM], and the bilingual bag-ofwords approach (Gouws et al., 2014) [BILBOWA]. For MTL and BIAE, we adopt their released word embeddings directly due to the inefficiency of training.13 For BICVM and BILBOWA, we rerun their systems on the same dataset as our previous experiments.14 Results are summarized in Table 8. CCA and PROJ consistently outperforms all other approaches in all languages, and PROJ performs the best. The inferior performance of MTL and BIAE is partly due to the low word coverage. For example, they cover only 31% of words in the universal DE test treebank, whereas the CCA and PROJ covers over 70%. Moreover, BIAE, BICVM and BILBOWA are optimiz</context>
<context position="32635" citStr="Gouws et al., 2014" startWordPosition="5170" endWordPosition="5173">ing auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), S</context>
</contexts>
<marker>Gouws, Bengio, Corrado, 2014</marker>
<rawString>Stephan Gouws, Yoshua Bengio, and Greg Corrado. 2014. Bilbowa: Fast bilingual distributed representations without word alignments. arXiv preprint arXiv:1410.2455.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Guo</author>
<author>Mona Diab</author>
</authors>
<title>Combining orthogonal monolingual and multilingual sources of evidence for all words wsd.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>1542--1551</pages>
<contexts>
<context position="33318" citStr="Guo and Diab, 2010" startWordPosition="5277" endWordPosition="5280"> 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, whi</context>
</contexts>
<marker>Guo, Diab, 2010</marker>
<rawString>Weiwei Guo and Mona Diab. 2010. Combining orthogonal monolingual and multilingual sources of evidence for all words wsd. In ACL, pages 1542– 1551, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiang Guo</author>
<author>Wanxiang Che</author>
<author>Haifeng Wang</author>
<author>Ting Liu</author>
</authors>
<title>Revisiting embedding features for simple semi-supervised learning.</title>
<date>2014</date>
<booktitle>In EMNLP,</booktitle>
<pages>110--120</pages>
<contexts>
<context position="9645" citStr="Guo et al. (2014)" startWordPosition="1465" endWordPosition="1468"> major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Sutskever et al., 2014). 3 Transition-based Dependency Parsing: A Neural Network Architecture In this section, we first briefly describe transitionbased dependency parsing and the arc-standard parsing algorithm. Then we revisit the neural network architecture for transition-based dependency parsing proposed by Chen and Manning (2014). As discussed in </context>
</contexts>
<marker>Guo, Che, Wang, Liu, 2014</marker>
<rawString>Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu. 2014. Revisiting embedding features for simple semi-supervised learning. In EMNLP, pages 110–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<volume>volume</volume>
<pages>771--779</pages>
<contexts>
<context position="20473" citStr="Haghighi et al., 2008" startWordPosition="3264" endWordPosition="3267">However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4ATIS is also worth trying, but we observed slight performance degradation in our experimental setting. d1 d2 n1 E* n2 W d d n1 E, E d1 V d n2 W d n� � d2 v(wTi ) = ci,j (i,j)∈ATIS ci,⋅ (v(w′)) (2) 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the number of hidden units </context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In ACL, volume 2008, pages 771–779.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Moritz Hermann</author>
<author>Phil Blunsom</author>
</authors>
<title>Multilingual models for compositional distributed semantics.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>58--68</pages>
<contexts>
<context position="30017" citStr="Hermann and Blunsom, 2014" startWordPosition="4757" endWordPosition="4761">45 FR UAS 70.65 71.36 +0.71 LAS 60.69 61.50 +0.81 Table 7: Effect of fine-tuning word embeddings. 5.4 Compare with Existing Bilingual Word Embeddings In this section, we compare our bilingual embeddings with several previous approaches in the context of dependency parsing. To the best of our knowledge, this is the first work on evaluation of bilingual word embeddings in syntactic tasks. The approaches we consider include the multi-task learning approach (Klementiev et al., 2012) [MTL], the bilingual auto-encoder approach (Chandar et al., 2014) [BIAE], the bilingual compositional vector model (Hermann and Blunsom, 2014) [BICVM], and the bilingual bag-ofwords approach (Gouws et al., 2014) [BILBOWA]. For MTL and BIAE, we adopt their released word embeddings directly due to the inefficiency of training.13 For BICVM and BILBOWA, we rerun their systems on the same dataset as our previous experiments.14 Results are summarized in Table 8. CCA and PROJ consistently outperforms all other approaches in all languages, and PROJ performs the best. The inferior performance of MTL and BIAE is partly due to the low word coverage. For example, they cover only 31% of words in the universal DE test treebank, whereas the CCA an</context>
<context position="32570" citStr="Hermann and Blunsom, 2014" startWordPosition="5159" endWordPosition="5162"> propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP t</context>
</contexts>
<marker>Hermann, Blunsom, 2014</marker>
<rawString>Karl Moritz Hermann and Phil Blunsom. 2014. Multilingual models for compositional distributed semantics. In ACL, pages 58–68, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Hwa</author>
<author>Philip Resnik</author>
<author>Amy Weinberg</author>
<author>Clara Cabezas</author>
<author>Okan Kolak</author>
</authors>
<title>Bootstrapping parsers via syntactic projection across parallel texts. Natural language engineering,</title>
<date>2005</date>
<pages>11--03</pages>
<contexts>
<context position="2330" citStr="Hwa et al., 2005" startWordPosition="332" endWordPosition="335">parsing has been dedicated to resource-rich languages, such as English and Chinese. For these languages, there exist large-scale ∗This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features. A delexicalized parser makes sense in that POS ta</context>
<context position="31685" citStr="Hwa et al., 2005" startWordPosition="5020" endWordPosition="5023"> are not available. 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings </context>
</contexts>
<marker>Hwa, Resnik, Weinberg, Cabezas, Kolak, 2005</marker>
<rawString>Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas, and Okan Kolak. 2005. Bootstrapping parsers via syntactic projection across parallel texts. Natural language engineering, 11(03):311–325.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
<author>Yajuan Lv</author>
</authors>
<title>Relaxed cross-lingual projection of constituent syntax.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1192--1201</pages>
<contexts>
<context position="31748" citStr="Jiang et al., 2011" startWordPosition="5032" endWordPosition="5035"> cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual</context>
</contexts>
<marker>Jiang, Liu, Lv, 2011</marker>
<rawString>Wenbin Jiang, Qun Liu, and Yajuan Lv. 2011. Relaxed cross-lingual projection of constituent syntax. In EMNLP, pages 1192–1201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitesh Khapra</author>
<author>Saurabh Sohoney</author>
<author>Anup Kulkarni</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Value for money: Balancing annotation effort, lexicon building and accuracy for multilingual wsd. In</title>
<date>2010</date>
<booktitle>COLING,</booktitle>
<pages>555--563</pages>
<contexts>
<context position="32079" citStr="Khapra et al., 2010" startWordPosition="5085" endWordPosition="5088">.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.</context>
</contexts>
<marker>Khapra, Sohoney, Kulkarni, Bhattacharyya, 2010</marker>
<rawString>Mitesh Khapra, Saurabh Sohoney, Anup Kulkarni, and Pushpak Bhattacharyya. 2010. Value for money: Balancing annotation effort, lexicon building and accuracy for multilingual wsd. In COLING, pages 555–563.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seokhwan Kim</author>
<author>Gary Geunbae Lee</author>
</authors>
<title>A graph-based cross-lingual projection approach for weakly supervised relation extraction.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>48--53</pages>
<contexts>
<context position="31998" citStr="Kim and Lee, 2012" startWordPosition="5072" endWordPosition="5075">oposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.</context>
</contexts>
<marker>Kim, Lee, 2012</marker>
<rawString>Seokhwan Kim and Gary Geunbae Lee. 2012. A graph-based cross-lingual projection approach for weakly supervised relation extraction. In ACL, pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sungchul Kim</author>
<author>Kristina Toutanova</author>
<author>Hwanjo Yu</author>
</authors>
<title>Multilingual named entity recognition using parallel data and metadata from wikipedia.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>694--702</pages>
<contexts>
<context position="32057" citStr="Kim et al., 2012" startWordPosition="5081" endWordPosition="5084"> tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.</context>
</contexts>
<marker>Kim, Toutanova, Yu, 2012</marker>
<rawString>Sungchul Kim, Kristina Toutanova, and Hwanjo Yu. 2012. Multilingual named entity recognition using parallel data and metadata from wikipedia. In ACL, pages 694–702.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Corpusbased induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In ACL,</booktitle>
<pages>478</pages>
<contexts>
<context position="2280" citStr="Klein and Manning, 2004" startWordPosition="325" endWordPosition="328">ing central problems. The majority of work on dependency parsing has been dedicated to resource-rich languages, such as English and Chinese. For these languages, there exist large-scale ∗This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features.</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Christopher D Manning. 2004. Corpusbased induction of syntactic structure: Models of dependency and constituency. In ACL, page 478.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ivan Titov</author>
<author>Binod Bhattarai</author>
</authors>
<title>Inducing crosslingual distributed representations of words.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>1459--1474</pages>
<contexts>
<context position="29874" citStr="Klementiev et al., 2012" startWordPosition="4737" endWordPosition="4740"> indeed offers considerable help. Fix Fine-tune Δ DE UAS 59.74 60.07 +0.33 LAS 49.44 49.94 +0.50 ES UAS 70.10 71.42 +1.32 LAS 61.31 61.76 +0.45 FR UAS 70.65 71.36 +0.71 LAS 60.69 61.50 +0.81 Table 7: Effect of fine-tuning word embeddings. 5.4 Compare with Existing Bilingual Word Embeddings In this section, we compare our bilingual embeddings with several previous approaches in the context of dependency parsing. To the best of our knowledge, this is the first work on evaluation of bilingual word embeddings in syntactic tasks. The approaches we consider include the multi-task learning approach (Klementiev et al., 2012) [MTL], the bilingual auto-encoder approach (Chandar et al., 2014) [BIAE], the bilingual compositional vector model (Hermann and Blunsom, 2014) [BICVM], and the bilingual bag-ofwords approach (Gouws et al., 2014) [BILBOWA]. For MTL and BIAE, we adopt their released word embeddings directly due to the inefficiency of training.13 For BICVM and BILBOWA, we rerun their systems on the same dataset as our previous experiments.14 Results are summarized in Table 8. CCA and PROJ consistently outperforms all other approaches in all languages, and PROJ performs the best. The inferior performance of MTL a</context>
<context position="32434" citStr="Klementiev et al., 2012" startWordPosition="5137" endWordPosition="5140">ced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological </context>
</contexts>
<marker>Klementiev, Titov, Bhattarai, 2012</marker>
<rawString>Alexandre Klementiev, Ivan Titov, and Binod Bhattarai. 2012. Inducing crosslingual distributed representations of words. In COLING, pages 1459–1474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume 9,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="20424" citStr="Koehn and Knight, 2002" startWordPosition="3256" endWordPosition="3259">gs for every word in the monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4ATIS is also worth trying, but we observed slight performance degradation in our experimental setting. d1 d2 n1 E* n2 W d d n1 E, E d1 V d n2 W d n� � d2 v(wTi ) = ci,j (i,j)∈ATIS ci,⋅ (v(w′)) (2) 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network depe</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition-Volume 9, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kai Liu</author>
<author>Yajuan L¨u</author>
<author>Wenbin Jiang</author>
<author>Qun Liu</author>
</authors>
<title>Bilingually-guided monolingual dependency grammar induction.</title>
<date>2013</date>
<booktitle>In ACL,</booktitle>
<pages>1063--1072</pages>
<marker>Liu, L¨u, Jiang, Liu, 2013</marker>
<rawString>Kai Liu, Yajuan L¨u, Wenbin Jiang, and Qun Liu. 2013. Bilingually-guided monolingual dependency grammar induction. In ACL, pages 1063–1072.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuezhe Ma</author>
<author>Fei Xia</author>
</authors>
<title>Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization.</title>
<date>2014</date>
<booktitle>In ACL,</booktitle>
<pages>1337--1348</pages>
<contexts>
<context position="34214" citStr="Ma and Xia, 2014" startWordPosition="5403" endWordPosition="5406">pplied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod arcs are mostly leftdirected. are complementary, and can </context>
</contexts>
<marker>Ma, Xia, 2014</marker>
<rawString>Xuezhe Ma and Fei Xia. 2014. Unsupervised dependency parsing with transferring distribution via parallel guidance and entropy regularization. In ACL, pages 1337–1348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Multipath translation lexicon induction via bridge languages. In</title>
<date>2001</date>
<booktitle>NAACL,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="20449" citStr="Mann and Yarowsky, 2001" startWordPosition="3260" endWordPosition="3263"> monolingual vocabulary. However, one potential limitation is that CCA assumes linear transformation of word embeddings, which is difficult to satisfy. 4ATIS is also worth trying, but we observed slight performance degradation in our experimental setting. d1 d2 n1 E* n2 W d d n1 E, E d1 V d n2 W d n� � d2 v(wTi ) = ci,j (i,j)∈ATIS ci,⋅ (v(w′)) (2) 1238 Note that both approaches can be generalize to lower-resource languages where parallel bitexts are not available. In that way, the dictionary A can be readily obtained either using bilingual lexicon induction approaches (Koehn and Knight, 2002; Mann and Yarowsky, 2001; Haghighi et al., 2008), or from resources like Wiktionary5 and Panlex.6 5 Experiments 5.1 Data and Settings For the pre-training of word embeddings, we use the WMT-2011 monolingual news corpora for English, German and Spanish.7 For French, we combined the WMT-2011 and WMT-2012 monolingual news corpora.8 We obtained the word alignment counts using the fast-align toolkit in cdec (Dyer et al., 2010) from the parallel news commentary corpora (WMT 2006-10) combined with the Europarl corpus for English-{German, Spanish, French}.9 For the training of the neural network dependency parser, we set the</context>
</contexts>
<marker>Mann, Yarowsky, 2001</marker>
<rawString>Gideon S Mann and David Yarowsky. 2001. Multipath translation lexicon induction via bridge languages. In NAACL, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>122--131</pages>
<contexts>
<context position="7420" citStr="McDonald and Nivre, 2007" startWordPosition="1113" endWordPosition="1116">ual word cluster features can be effectively embedded into our model, leading to significant additive improvements. 2 Background 2.1 Dependency Parsing Given an input sentence x = w0w,...wn, the goal of dependency parsing is to build a dependency tree (Figure 1), which can be denoted by d = {(h,m,l) ∶ 0 ≤ h ≤ n;0 &lt; m ≤ n,l ∈ L}. (h, m, l) indicates a directed arc from the head word wh to the modifier wm with a dependency label l, and L is the label set. The mainstream models that have been proposed for dependency parsing can be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy d</context>
</contexts>
<marker>McDonald, Nivre, 2007</marker>
<rawString>Ryan McDonald and Joakim Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. In EMNLP-CoNLL, pages 122–131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Slav Petrov</author>
<author>Keith Hall</author>
</authors>
<title>Multi-source transfer of delexicalized dependency parsers.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>62--72</pages>
<contexts>
<context position="2382" citStr="McDonald et al., 2011" startWordPosition="340" endWordPosition="343">guages, such as English and Chinese. For these languages, there exist large-scale ∗This work was done while the author was visiting JHU. annotated treebanks that can be used for supervised training of dependency parsers. However, for most of the languages in the world, there are few or even no labeled training data for parsing, and it is labor intensive and time-consuming to manually build treebanks for all languages. This fact has given rise to a number of research on unsupervised methods (Klein and Manning, 2004), annotation projection methods (Hwa et al., 2005), and model transfer methods (McDonald et al., 2011) for predicting linguistic structures. In this study, we focus on the model transfer methods, which attempt to build parsers for low-resource languages by exploiting treebanks from resourcerich languages. The major obstacle in transferring a parsing system from one language to another is the lexical features, e.g. words, which are not directly transferable across languages. To solve this problem, McDonald et al. (2011) build a delexicalized parser - a parser that only has non-lexical features. A delexicalized parser makes sense in that POS tag features are significantly predictive for unlabele</context>
</contexts>
<marker>McDonald, Petrov, Hall, 2011</marker>
<rawString>Ryan McDonald, Slav Petrov, and Keith Hall. 2011. Multi-source transfer of delexicalized dependency parsers. In EMNLP, pages 62–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan T McDonald</author>
<author>Joakim Nivre</author>
</authors>
<title>Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith B Hall, Slav Petrov,</title>
<date>2013</date>
<booktitle>ACL,</booktitle>
<pages>92--97</pages>
<location>Hao Zhang, Oscar T¨ackstr¨om, et</location>
<marker>McDonald, Nivre, 2013</marker>
<rawString>Ryan T McDonald, Joakim Nivre, Yvonne QuirmbachBrundage, Yoav Goldberg, Dipanjan Das, Kuzman Ganchev, Keith B Hall, Slav Petrov, Hao Zhang, Oscar T¨ackstr¨om, et al. 2013. Universal dependency annotation for multilingual parsing. In ACL, pages 92–97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="15420" citStr="Mikolov et al., 2013" startWordPosition="2397" endWordPosition="2400">rn a dependency parser at the source language side. After that, the parser will be directly used for parsing target language data. 4 Cross-lingual Word Representation Learning Prior to introducing our approaches for crosslingual word representation learning, we briefly review the basic model for learning monolingual word embeddings, which constitutes a subprocedure of the cross-lingual approaches. 4.1 Continuous Bag-of-Words Model Various approaches have been studied for learning word embeddings from large-scale plain texts. In this study, we consider the Continuous Bag-of-Words (CBOW) model (Mikolov et al., 2013) as implemented in the open-source toolkit word2vec.3 The basic principle of the CBOW model is to predict each individual word in a sequence given the bag of its context words within a fixed window size as input, using a log-linear classifier. This model avoids the non-linear transformation in hidden layers, and hence can be trained with high efficiency. With large window size, grouped words using the resulting word embeddings are more topically similar; whereas with small window size, the grouped words will be more syntactically similar. So we set the window size to 1 in our parsing task. Nex</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Harr Chen</author>
<author>Regina Barzilay</author>
<author>Mark Johnson</author>
</authors>
<title>Using universal linguistic knowledge to guide grammar induction.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>1234--1244</pages>
<contexts>
<context position="32998" citStr="Naseem et al., 2010" startWordPosition="5225" endWordPosition="5228"> UAS ES UAS FR LAS LAS LAS MTL (Klementiev et al., 2012)‡ 57.70 47.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be ap</context>
</contexts>
<marker>Naseem, Chen, Barzilay, Johnson, 2010</marker>
<rawString>Tahira Naseem, Harr Chen, Regina Barzilay, and Mark Johnson. 2010. Using universal linguistic knowledge to guide grammar induction. In EMNLP, pages 1234–1244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tahira Naseem</author>
<author>Regina Barzilay</author>
<author>Amir Globerson</author>
</authors>
<title>Selective sharing for multilingual dependency parsing.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>629--637</pages>
<contexts>
<context position="33064" citStr="Naseem et al., 2012" startWordPosition="5235" endWordPosition="5238">.13 68.04 58.78 67.66 57.30 BIAE (Chandar et al., 2014)‡ 53.74 43.68 58.81 46.66 60.10 49.47 BICVM (Hermann and Blunsom, 2014) 56.30 46.99 67.78 58.08 69.13 58.13 BILBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lin</context>
</contexts>
<marker>Naseem, Barzilay, Globerson, 2012</marker>
<rawString>Tahira Naseem, Regina Barzilay, and Amir Globerson. 2012. Selective sharing for multilingual dependency parsing. In ACL, pages 629–637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
</authors>
<title>Incrementality in deterministic dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together,</booktitle>
<pages>50--57</pages>
<contexts>
<context position="11074" citStr="Nivre, 2004" startWordPosition="1693" endWordPosition="1694"> are used interchangeably. Transition actions W2 Hidden units W1 Embeddings Figure 2: Neural network model for dependency parsing. The Cluster features are introduced in Section 5.2. which typically consists of a stack S, a buffer B, and a partially derived forest, i.e. a set of dependency arcs A. Given an input word sequence x = w1w2, ..., wn, the initial configuration can be represented as a tuple: ([w0]S, [w1w2, ..., wn]B,O), and the terminal configuration is ([w0]S, []B, A), where w0 is a pseudo word indicating the root of the whole dependency tree. We consider the arc-standard algorithm (Nivre, 2004) in this paper, which defines three types of transition actions: LEFT-ARC(l), RIGHT-ARC(l), and SHIFT, l is the dependency label. The typical approach for greedy arc-standard parsing is to build a multi-class classifier (e.g., SVM, MaxEnt) of predicting the transition action given a feature vector extracted from a specific configuration. While conventional feature engineering suffers from the problem of sparsity, incompleteness and expensive feature computation (Chen and Manning, 2014), the neural network model provides a potential solution. The architecture of the neural network-based depende</context>
</contexts>
<marker>Nivre, 2004</marker>
<rawString>Joakim Nivre. 2004. Incrementality in deterministic dependency parsing. In Proceedings of the Workshop on Incremental Parsing: Bringing Engineering and Cognition Together, pages 50–57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Dipanjan Das</author>
<author>Ryan McDonald</author>
</authors>
<title>A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</title>
<date>2011</date>
<contexts>
<context position="21800" citStr="Petrov et al., 2011" startWordPosition="3476" endWordPosition="3479">Cluster Dim. 50 50 50 5 5 8 Table 2: Dimensions of feature embeddings. Adaptive stochastic gradient descent (AdaGrad) (Duchi et al., 2011) is used for optimization. For the CCA approach, we use the implementation of Faruqui and Dyer (2014). The dimensions of the monolingual embeddings (d1, d2) and the resulting bilingual embeddings are set to 50 equally. We employ the universal dependency treebanks proposed by McDonald et al. (2013) for a reliable evaluation of our approach for cross-lingual dependency parsing. The universal multilingual treebanks are annotated using the universal POS tagset (Petrov et al., 2011) which contains 12 POS tags, as well as the universal dependencies which contains 42 relations. We follow the standard split of the treebanks for every language (DE, ES, and FR).10 5https://www.wiktionary.org/ 6http://panlex.org/ 7http://www.statmt.org/wmt11/ 8http://www.statmt.org/wmt12/ 9http://www.statmt.org/europarl/ 10http://code.google.com/p/uni-dep-tb/. 5.2 Baseline Systems We compare our approach with three systems. For the first baseline, we evaluate the delexicalized transfer of our parser [DELEX], in which we only use non-lexical features. We also compare our approach with the delex</context>
</contexts>
<marker>Petrov, Das, McDonald, 2011</marker>
<rawString>Slav Petrov, Dipanjan Das, and Ryan McDonald. 2011. A universal part-of-speech tagset. arXiv preprint arXiv:1104.2086.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Parser adaptation and projection with quasi-synchronous grammar features.</title>
<date>2009</date>
<booktitle>In EMNLP,</booktitle>
<pages>822--831</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="31709" citStr="Smith and Eisner, 2009" startWordPosition="5024" endWordPosition="5027">. 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before tr</context>
</contexts>
<marker>Smith, Eisner, 2009</marker>
<rawString>David A Smith and Jason Eisner. 2009. Parser adaptation and projection with quasi-synchronous grammar features. In EMNLP, pages 822–831. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ilya Sutskever</author>
<author>Oriol Vinyals</author>
<author>Quoc Le</author>
</authors>
<title>Sequence to sequence learning with neural networks.</title>
<date>2014</date>
<booktitle>In NIPS,</booktitle>
<pages>3104--3112</pages>
<contexts>
<context position="9915" citStr="Sutskever et al., 2014" startWordPosition="1511" endWordPosition="1514">0). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Sutskever et al., 2014). 3 Transition-based Dependency Parsing: A Neural Network Architecture In this section, we first briefly describe transitionbased dependency parsing and the arc-standard parsing algorithm. Then we revisit the neural network architecture for transition-based dependency parsing proposed by Chen and Manning (2014). As discussed in Section 2.1, transition-based parsing aims to predict a transition sequence from an initial parser state to the terminal state. Each state is conventionally regarded as a configuration, 1In this paper, these two terms are used interchangeably. Transition actions W2 Hidd</context>
</contexts>
<marker>Sutskever, Vinyals, Le, 2014</marker>
<rawString>Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. 2014. Sequence to sequence learning with neural networks. In NIPS, pages 3104–3112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oscar T¨ackstr¨om</author>
<author>Ryan McDonald</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Cross-lingual word clusters for direct transfer of linguistic structure.</title>
<date>2012</date>
<booktitle>In NAACL,</booktitle>
<pages>477--487</pages>
<marker>T¨ackstr¨om, McDonald, Uszkoreit, 2012</marker>
<rawString>Oscar T¨ackstr¨om, Ryan McDonald, and Jakob Uszkoreit. 2012. Cross-lingual word clusters for direct transfer of linguistic structure. In NAACL, pages 477–487.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
<author>Joakim Nivre</author>
</authors>
<title>Treebank translation for cross-lingual parser induction.</title>
<date>2014</date>
<booktitle>CoNLL-2014,</booktitle>
<pages>130</pages>
<contexts>
<context position="34157" citStr="Tiedemann and Nivre, 2014" startWordPosition="5394" endWordPosition="5397">-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-directed arc labeled by amod, whereas in English, the amod </context>
</contexts>
<marker>Tiedemann, Nivre, 2014</marker>
<rawString>J¨org Tiedemann and Joakim Nivre. 2014. Treebank translation for cross-lingual parser induction. CoNLL-2014, page 130.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Rediscovering annotation projection for cross-lingual parser induction.</title>
<date>2014</date>
<booktitle>In Proc. COLING.</booktitle>
<contexts>
<context position="31765" citStr="Tiedemann, 2014" startWordPosition="5036" endWordPosition="5037">dency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset</context>
<context position="34100" citStr="Tiedemann, 2014" startWordPosition="5389" endWordPosition="5390">ter that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. That said, these methods have the advantage that they are capable of capturing some language-specific syntactic patterns which our approach cannot.15 These two kinds of approaches 15For example, in Spanish and French, adjectives often appears after nouns, thus forming a right-di</context>
</contexts>
<marker>Tiedemann, 2014</marker>
<rawString>J¨org Tiedemann. 2014. Rediscovering annotation projection for cross-lingual parser induction. In Proc. COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Improving the cross-lingual projection of syntactic dependencies.</title>
<date>2015</date>
<booktitle>In Nordic Conference of Computational Linguistics NODALIDA 2015,</booktitle>
<pages>191</pages>
<contexts>
<context position="31783" citStr="Tiedemann, 2015" startWordPosition="5038" endWordPosition="5039"> be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM only uses the bilingual parallel dataset. 1241 UAS DE UAS </context>
</contexts>
<marker>Tiedemann, 2015</marker>
<rawString>J¨org Tiedemann. 2015. Improving the cross-lingual projection of syntactic dependencies. In Nordic Conference of Computational Linguistics NODALIDA 2015, page 191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Alexandre Klementiev</author>
</authors>
<title>Crosslingual induction of semantic roles.</title>
<date>2012</date>
<booktitle>In ACL,</booktitle>
<pages>647--656</pages>
<contexts>
<context position="33289" citStr="Titov and Klementiev, 2012" startWordPosition="5271" endWordPosition="5274">.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser</context>
</contexts>
<marker>Titov, Klementiev, 2012</marker>
<rawString>Ivan Titov and Alexandre Klementiev. 2012. Crosslingual induction of semantic roles. In ACL, pages 647–656.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Turian</author>
<author>Lev Ratinov</author>
<author>Yoshua Bengio</author>
</authors>
<title>Word representations: a simple and general method for semi-supervised learning.</title>
<date>2010</date>
<booktitle>In ACL,</booktitle>
<pages>384--394</pages>
<contexts>
<context position="9294" citStr="Turian et al., 2010" startWordPosition="1409" endWordPosition="1412">in the NLP research community of learning distributed representations for different natural language units, from morphemes, words and phrases, to sentences and documents. Using distributed representations, these symbolic units are embedded into a lowdimensional and continuous space, thus it is often referred to as embeddings.1 In general, there are two major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Su</context>
</contexts>
<marker>Turian, Ratinov, Bengio, 2010</marker>
<rawString>Joseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: a simple and general method for semi-supervised learning. In ACL, pages 384– 394.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakob Uszkoreit</author>
<author>Thorsten Brants</author>
</authors>
<title>Distributed word clustering for large scale class-based language modeling in machine translation.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>755--762</pages>
<contexts>
<context position="23318" citStr="Uszkoreit and Brants (2008)" startWordPosition="3702" endWordPosition="3705">posed in T¨ackstr¨om et al. (2012). We use the same alignment dictionary as described in Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: ci,j ⋅ ✶[c(wS j ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as described in Section 4.2 to find the most likely clusters for the OOV words. Instead of the clustering model of Uszkoreit and Brants (2008), we use Brown clustering (Brown et al., 1992) to induce hierarchical word clusters, where each word is represented as a bit-string. We use the same word cluster feature templates from T¨ackstr¨om et al. (2012), and set the number of Brown clusters to 256. 5.3 Experimental Results All of the parsing models are trained using the development data from English for early-stopping. Table 3 lists the results of the cross-lingual transfer experiments for dependency parsing. Table 4 further summarizes each of the experimental gains detailed in Table 3. Our delexicalized system obtains slightly lower p</context>
</contexts>
<marker>Uszkoreit, Brants, 2008</marker>
<rawString>Jakob Uszkoreit and Thorsten Brants. 2008. Distributed word clustering for large scale class-based language modeling in machine translation. In ACL, pages 755–762.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Effect of non-linear deep architecture in sequence labeling.</title>
<date>2013</date>
<booktitle>In IJCNLP,</booktitle>
<pages>1285--1291</pages>
<contexts>
<context position="4686" citStr="Wang and Manning, 2013" startWordPosition="675" endWordPosition="679">s the discrete lexical features used in traditional dependency parsers, distributed representations map symbolic features into a continuous representation space, that can be shared across languages. Therefore, our model has the ability to utilize both lexical and non-lexical features naturally. Specifically, our framework contains two primary components: • A neural network-based dependency parser. We expect a non-linear model for dependency parsing in our study, because distributed feature representations are shown to be more effective in non-linear architectures than in linear architectures (Wang and Manning, 2013). Chen and Manning (2014) propose a transition-based dependency parser using a neural network architecture, which is simple but works well on several datasets. Briefly, this model simply replaces the predictor in transition-based dependency parser with a well-designed neural network classifier. We will provide explanations for the merits of this model in Section 3, as well as how we adapt it to the cross-lingual task. • Cross-lingual word representation learning. The key to filling the lexical feature gap is to project the representations of these features from different languages into a commo</context>
<context position="9543" citStr="Wang and Manning, 2013" startWordPosition="1449" endWordPosition="1452"> lowdimensional and continuous space, thus it is often referred to as embeddings.1 In general, there are two major ways of applying distributed representations to NLP tasks. First, they can be fed into existing supervised NLP systems as augmented features in a semi-supervised manner. This kind of approach has been adopted in a variety of applications (Turian et al., 2010). Despite its simplicity and effectiveness, it has been shown that the potential of distributed representations cannot be fully exploited in the generalized linear models which are adopted in most of the existing NLP systems (Wang and Manning, 2013). One remedy is to discretize the distributed feature representations, as studied in Guo et al. (2014). However, we believe that a non-linear system, e.g. a neural network, is a more powerful and effective solution. Some decent progress has already been made in this paradigm of NLP on various tasks (Collobert et al., 2011; Chen and Manning, 2014; Sutskever et al., 2014). 3 Transition-based Dependency Parsing: A Neural Network Architecture In this section, we first briefly describe transitionbased dependency parsing and the arc-standard parsing algorithm. Then we revisit the neural network arch</context>
</contexts>
<marker>Wang, Manning, 2013</marker>
<rawString>Mengqiu Wang and Christopher D. Manning. 2013. Effect of non-linear deep architecture in sequence labeling. In IJCNLP, pages 1285–1291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mengqiu Wang</author>
<author>Christopher D Manning</author>
</authors>
<title>Cross-lingual projected expectation regularization for weakly supervised learning.</title>
<date>2014</date>
<tech>TACL,</tech>
<pages>2--55</pages>
<contexts>
<context position="33232" citStr="Wang and Manning, 2014" startWordPosition="5262" endWordPosition="5265">LBOWA (Gouws et al., 2014) 51.65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cr</context>
</contexts>
<marker>Wang, Manning, 2014</marker>
<rawString>Mengqiu Wang and Christopher D Manning. 2014. Cross-lingual projected expectation regularization for weakly supervised learning. TACL, 2:55–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min Xiao</author>
<author>Yuhong Guo</author>
</authors>
<title>Distributed word representation learning for cross-lingual dependency parsing. In CoNLL,</title>
<date>2014</date>
<pages>119--129</pages>
<contexts>
<context position="33822" citStr="Xiao and Guo (2014)" startWordPosition="5348" endWordPosition="5351">2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings and apply them with MSTParser for linguistic transfer, which inspires this work. It is worth mentioning that remarkable results on the universal dependency treebanks have been achieved by using annotation projection method (Tiedemann, 2014), treebank translation method (Tiedemann and Nivre, 2014), and distribution transferring method (Ma and Xia, 2014). Unlike our approach, all of these methods involve training a parser at the target language side. Parallel bitexts are required in these methods, which limits their scalability to lower-resource languages. T</context>
</contexts>
<marker>Xiao, Guo, 2014</marker>
<rawString>Min Xiao and Yuhong Guo. 2014. Distributed word representation learning for cross-lingual dependency parsing. In CoNLL, pages 119–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
<author>Grace Ngai</author>
<author>Richard Wicentowski</author>
</authors>
<title>Inducing multilingual text analysis tools via robust projection across aligned corpora.</title>
<date>2001</date>
<booktitle>In Proceedings of the first international conference on Human language technology research,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="31412" citStr="Yarowsky et al. (2001)" startWordPosition="4974" endWordPosition="4977">sks. It is worth noting that we don’t assume/require bilingual parallel data in CCA and PROJ. What we need in practice is a bilingual lexicon for each paired languages. This is especially important for generalizing our approaches to lower-resource languages, where parallel texts are not available. 6 Related Studies Existing approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorp</context>
</contexts>
<marker>Yarowsky, Ngai, Wicentowski, 2001</marker>
<rawString>David Yarowsky, Grace Ngai, and Richard Wicentowski. 2001. Inducing multilingual text analysis tools via robust projection across aligned corpora. In Proceedings of the first international conference on Human language technology research, pages 1– 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Syntactic processing using the generalized perceptron and beam search.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="8209" citStr="Zhang and Clark, 2011" startWordPosition="1240" endWordPosition="1243">es of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy decoding and local training is not as accurate as graph-based parsers or transition-based parsers with beam-search and punct root nsubj dobj amod 1235 global training (Zhang and Clark, 2011). Recently, Chen and Manning (2014) show that greedy transition-based parsers can be greatly improved by using a well-designed neural network architecture. This approach can be considered as a new paradigm of parsing, in that it is based on pure distributed feature representations. In this study, we choose Chen and Manning’s architecture to build our basic dependency parsing model. 2.2 Distributed Representations for NLP In recent years, there has been a trend in the NLP research community of learning distributed representations for different natural language units, from morphemes, words and p</context>
</contexts>
<marker>Zhang, Clark, 2011</marker>
<rawString>Yue Zhang and Stephen Clark. 2011. Syntactic processing using the generalized perceptron and beam search. Computational Linguistics, 37(1):105–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Joakim Nivre</author>
</authors>
<title>Transition-based dependency parsing with rich non-local features.</title>
<date>2011</date>
<booktitle>In ACL,</booktitle>
<pages>188--193</pages>
<contexts>
<context position="7945" citStr="Zhang and Nivre, 2011" startWordPosition="1200" endWordPosition="1203">an be described as either graph-based models or transitionbased models (McDonald and Nivre, 2007). Graph-based models view the parsing problem as finding the highest scoring tree from a directed graph. The score of a dependency tree is typically factored into scores of some small structures (e.g. arcs) depending on the order of a model. Transition-based models aim to predict a transition sequence from an initial parser state to some terminal states, depending on the parsing history. This approach has a lot of interest since it is fast (linear time) and can incorporate rich non-local features (Zhang and Nivre, 2011). It has been considered that simple transitionbased parsing using greedy decoding and local training is not as accurate as graph-based parsers or transition-based parsers with beam-search and punct root nsubj dobj amod 1235 global training (Zhang and Clark, 2011). Recently, Chen and Manning (2014) show that greedy transition-based parsers can be greatly improved by using a well-designed neural network architecture. This approach can be considered as a new paradigm of parsing, in that it is based on pure distributed feature representations. In this study, we choose Chen and Manning’s architect</context>
<context position="22585" citStr="Zhang and Nivre, 2011" startWordPosition="3580" endWordPosition="3583">E, ES, and FR).10 5https://www.wiktionary.org/ 6http://panlex.org/ 7http://www.statmt.org/wmt11/ 8http://www.statmt.org/wmt12/ 9http://www.statmt.org/europarl/ 10http://code.google.com/p/uni-dep-tb/. 5.2 Baseline Systems We compare our approach with three systems. For the first baseline, we evaluate the delexicalized transfer of our parser [DELEX], in which we only use non-lexical features. We also compare our approach with the delexicalized parser in McDonald et al. (2013) [McD13], who used a perceptron-trained transition-based parser with a beam of size 8, along with rich nonlocal features (Zhang and Nivre, 2011). Furthermore, we augment cross-lingual word clusters to the perceptron-based delexicalized parser, as proposed in T¨ackstr¨om et al. (2012). We use the same alignment dictionary as described in Section 4 to induce the cross-lingual word clusters. We re-implement the PROJECTED cluster approach in T¨ackstr¨om et al. (2012), which assigns a target word to the cluster with which it is most often aligned: ci,j ⋅ ✶[c(wS j ) = k] This method also has the drawback that words that do not occur in the alignment dictionary (OOV) cannot be assigned a cluster. Therefore, we use the same strategy as descri</context>
</contexts>
<marker>Zhang, Nivre, 2011</marker>
<rawString>Yue Zhang and Joakim Nivre. 2011. Transition-based dependency parsing with rich non-local features. In ACL, pages 188–193.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Yan Song</author>
<author>Chunyu Kit</author>
<author>Guodong Zhou</author>
</authors>
<title>Cross language dependency parsing using a bilingual lexicon. In</title>
<date>2009</date>
<booktitle>ACL-IJCNLP,</booktitle>
<pages>55--63</pages>
<contexts>
<context position="31728" citStr="Zhao et al., 2009" startWordPosition="5028" endWordPosition="5031">ting approaches for cross-lingual dependency parsing can be divided into three categories: crosslingual annotation projection methods, jointly modeling methods and cross-lingual representation learning methods. The cross-lingual annotation projection method is first proposed in Yarowsky et al. (2001) for shallower NLP tasks (POS tagging, NER, etc.). The central idea is to project the syntactic annotations from a resource-rich language to the target language through word alignments, and then train a supervised parser on the projected noisy annotations (Hwa et al., 2005; Smith and Eisner, 2009; Zhao et al., 2009; Jiang et al., 2011; Tiedemann, 2014; Tiedemann, 2015). Noises and errors introduced by the word alignment and annotation projection processes can be reduced with robust projection methods by using graph-based label propagation (Das and Petrov, 2011; Kim and Lee, 2012), or by incorporating auxiliary resources (Kim et al., 2012; Khapra et al., 2010). The jointly modeling methods integrates the monolingual grammar induction with bilinguallyprojected dependency information (Liu et al., 2013), or linguistic constraints via posterior 13The MTL embeddings are normalized before training. 14BICVM onl</context>
</contexts>
<marker>Zhao, Song, Kit, Zhou, 2009</marker>
<rawString>Hai Zhao, Yan Song, Chunyu Kit, and Guodong Zhou. 2009. Cross language dependency parsing using a bilingual lexicon. In ACL-IJCNLP, pages 55–63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tao Zhuang</author>
<author>Chengqing Zong</author>
</authors>
<title>Joint inference for bilingual semantic role labeling.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>304--314</pages>
<contexts>
<context position="33260" citStr="Zhuang and Zong, 2010" startWordPosition="5267" endWordPosition="5270">65 41.83 65.02 54.35 63.35 51.65 CCA 59.42 49.32 68.87 59.65 69.58 59.50 PROJ 60.07 49.94 71.42 61.76 71.36 61.55 Table 8: Comparison with existing bilingual word embeddings. ‡For MTL and BIAE, we use their released bilingual word embeddings. regularization (Ganchev et al., 2009), manually constructed universal dependency parsing rules (Naseem et al., 2010) and manually specified typological features (Naseem et al., 2012). Besides dependency parsing, the joint modeling method has also been applied for other multilingual NLP tasks, including NER (Che et al., 2013; Wang and Manning, 2014), SRL (Zhuang and Zong, 2010; Titov and Klementiev, 2012) and WSD (Guo and Diab, 2010). The cross-lingual representation learning method aims at building connections across different languages by inducing languageindependent feature representations. After that, a parser can be trained at the source-language side within the induced feature space, and directly be applied to the target language. Typical approaches include cross-lingual word clustering (T¨ackstr¨om et al., 2012) which is employed in this paper as a baseline, projection features (Durrett et al., 2012). Xiao and Guo (2014) learns cross-lingual word embeddings </context>
</contexts>
<marker>Zhuang, Zong, 2010</marker>
<rawString>Tao Zhuang and Chengqing Zong. 2010. Joint inference for bilingual semantic role labeling. In EMNLP, pages 304–314.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>