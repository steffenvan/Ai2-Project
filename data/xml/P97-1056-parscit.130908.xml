<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000111">
<title confidence="0.960489">
Memory-Based Learning: Using Similarity for Smoothing
</title>
<author confidence="0.958303">
Jakub Zavrel and Walter Daelernans
</author>
<affiliation confidence="0.9642085">
Computational Linguistics
Tilburg University
</affiliation>
<address confidence="0.819942666666667">
PO Box 90153
5000 LE Tilburg
The Netherlands
</address>
<email confidence="0.594367">
Izavrel,walterlOkub.n1
</email>
<sectionHeader confidence="0.988652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999845842105263">
This paper analyses the relation between
the use of similarity in Memory-Based
Learning and the notion of backed-off
smoothing in statistical language model-
ing. We show that the two approaches are
closely related, and we argue that feature
weighting methods in the Memory-Based
paradigm can offer the advantage of au-
tomatically specifying a suitable domain-
specific hierarchy between most specific
and most general conditioning information
without the need for a large number of pa-
rameters. We report two applications of
this approach: PP-attachment and POS-
tagging. Our method achieves state-of-the-
art performance in both domains, and al-
lows the easy integration of diverse infor-
mation sources, such as rich lexical repre-
sentations.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999952555555556">
Statistical approaches to disambiguation offer the
advantage of making the most likely decision on the
basis of available evidence. For this purpose a large
number of probabilities has to be estimated from a
training corpus. However, many possible condition-
ing events are not present in the training data, yield-
ing zero Maximum Likelihood (ML) estimates. This
motivates the need for smoothing methods, which re-
estimate the probabilities of low-count events from
more reliable estimates.
Inductive generalization from observed to new
data lies at the heart of machine-learning approaches
to disambiguation. In Memory-Based Learning&apos;
(MBL) induction is based on the use of similarity
(Stanfill &amp; Waltz, 1986; Aha et al., 1991; Cardie,
1994; Daelemans, 1995). In this paper we describe
how the use of similarity between patterns embod-
ies a solution to the sparse data problem, how it
</bodyText>
<footnote confidence="0.632856">
&apos;The Approach is also referred to as Case-based,
Instance-based or Exemplar-based.
</footnote>
<figureCaption confidence="0.423321">
relates to backed-off smoothing methods and what
advantages it offers when combining diverse and rich
information sources.
</figureCaption>
<bodyText confidence="0.9997372">
We illustrate the analysis by applying MBL to
two tasks where combination of information sources
promises to bring improved performance: PP-
attachment disambiguation and Part of Speech tag-
ging.
</bodyText>
<sectionHeader confidence="0.9568595" genericHeader="method">
2 Memory-Based Language
Processing
</sectionHeader>
<bodyText confidence="0.999941071428571">
The basic idea in Memory-Based language process-
ing is that processing and learning are fundamen-
tally interwoven. Each language experience leaves a
memory trace which can be used to guide later pro-
cessing. When a new instance of a task is processed,
a set of relevant instances is selected from memory,
and the output is produced by analogy to that set.
The techniques that are used are variants and
extensions of the classic k-nearest neighbor (k-
NN) classifier algorithm. The instances of a task
are stored in a table as patterns of feature-value
pairs, together with the associated &amp;quot;correct&amp;quot; out-
put. When a new pattern is processed, the k nearest
neighbors of the pattern are retrieved from memory
using some similarity metric. The output is then de-
termined by extrapolation from the k nearest neigh-
bors, i.e. the output is chosen that has the highest
relative frequency among the nearest neighbors.
Note that no abstractions, such as grammatical
rules, stochastic automata, or decision trees are ex-
tracted from the examples. Rule-like behavior re-
sults from the linguistic regularities that are present
in the patterns of usage in memory in combination
with the use of an appropriate similarity metric.
It is our experience that even limited forms of ab-
straction can harm performance on linguistic tasks,
which often contain many subregularities and excep-
tions (Daelemans, 1996).
</bodyText>
<subsectionHeader confidence="0.971544">
2.1 Similarity metrics
</subsectionHeader>
<bodyText confidence="0.995175">
The most basic metric for patterns with symbolic
features is the Overlap metric given in equations 1
</bodyText>
<page confidence="0.998791">
436
</page>
<bodyText confidence="0.9932885">
and 2; where A(X, Y) is the distance between pat-
terns X and Y, represented by n features, wi is a
weight for feature i, and 6 is the distance per fea-
ture. The k-NN algorithm with this metric, and
equal weighting for all features is called 031 (Aha
et al., 1991). Usually k is set to 1.
</bodyText>
<equation confidence="0.994822333333333">
A(X,Y) = E wi (5(x ,y,) (1)
where:
S(x„yz) = 0 if xi = y„ else 1 (2)
</equation>
<bodyText confidence="0.999647260869565">
This metric simply counts the number of
(mis)matching feature values in both patterns. If
we do not have information about the importance
of features, this is a reasonable choice. But if we
do have some information about feature relevance
one possibility would be to add linguistic bias to
weight or select different features (Cardie, 1996). An
alternative--more empiricist—approach, is to look
at the behavior of features in the set of examples
used for training. We can compute statistics about
the relevance of features by looking at which fea-
tures are good predictors of the class labels. Infor-
mation Theory gives us a useful tool for measuring
feature relevance in this way (Quinlan, 1986; Quin-
lan, 1993).
Information Gain (IG) weighting looks at each
feature in isolation, and measures how much infor-
mation it contributes to our knowledge of the cor-
rect class label. The Information Gain of feature f
is measured by computing the difference in uncer-
tainty (i.e. entropy) between the situations with-
out and with knowledge of the value of that feature
(Equation 3).
</bodyText>
<equation confidence="0.9706324">
EvEvf P(v) x H(Civ)
wf H(C) (3)
si(f)
si(f) = — E P(v) log2 P(v) (4)
VEVf
</equation>
<bodyText confidence="0.995132875">
Where C is the set of class labels, Vf is
the set of values for feature f, and H(C) =
— EcEc p(c) log2 P(c) is the entropy of the class la-
bels. The probabilities are estimated from relative
frequencies in the training set. The normalizing fac-
tor si(f) (split info) is included to avoid a bias in
favor of features with more values. It represents the
amount of information needed to represent all val-
ues of the feature (Equation 4). The resulting IG
values can then be used as weights in equation 1.
The k-NN algorithm with this metric is called 031-
1G (Daelemans &amp; Van den Bosch, 1992).
The possibility of automatically determining the
relevance of features implies that many different and
possibly irrelevant features can be added to the fea-
ture set. This is a very convenient methodology if
theory does not constrain the choice enough before-
hand, or if we wish to measure the importance of
various information sources experimentally.
Finally, it should be mentioned that MB-
classifiers, despite their description as table-lookup
algorithms here, can be implemented to work
fast, using e.g. tree-based indexing into the case-
base (Daelemans et al., 1997).
</bodyText>
<sectionHeader confidence="0.611354" genericHeader="method">
3 Smoothing of Estimates
</sectionHeader>
<bodyText confidence="0.999852666666667">
The commonly used method for probabilistic clas-
sification (the Bayesian classifier) chooses a class
for a pattern X by picking the class that has the
maximum conditional probability P(classIX). This
probability is estimated from the data set by looking
at the relative joint frequency of occurrence of the
classes and pattern X. If pattern X is described by
a number of feature-values , , xn, we can write
the conditional probability as P(classlxi,...,xn). If
a particular pattern is not literally present
among the examples, all classes have zero ML prob-
ability estimates. Smoothing methods are needed to
avoid zeroes on events that could occur in the test
material.
There are two main approaches to smoothing:
count re-estimation smoothing such as the Add-One
or Good-Turing methods (Church &amp; Gale, 1991),
and Back-off type methods (Bahl et al., 1983; Katz,
1987; Chen &amp; Goodman, 1996; Samuelsson, 1996).
We will focus here on a comparison with Back-off
type methods, because an experimental comparison
in Chen &amp; Goodman (1996) shows the superiority
of Back-off based methods over count re-estimation
smoothing methods. With the Back-off method the
probabilities of complex conditioning events are ap-
proximated by (a linear interpolation of) the proba-
bilities of more general events:
</bodyText>
<equation confidence="0.9982405">
23(classIX) = Axi5(classIX)+ Ax,P(classIX&apos;)
+.. ± Ax4(classIX11) (5)
</equation>
<bodyText confidence="0.962777785714286">
Where 23 stands for the smoothed estimate, 23 for
the relative frequency estimate, A are interpolation
weights, Ein_o Ax. = 1, and X -‹ X&apos; for all i,
where -‹ is a (partial) ordering from most specific
to most general feature-sets2 (e.g the probabilities
of trigrams (X) can be approximated by bigrams
(X&apos;) and unigrams (X&amp;quot;)). The weights of the lin-
ear interpolation are estimated by maximizing the
probability of held-out data (deleted interpolation)
with the forward-backward algorithm. An alterna-
tive method to determine the interpolation weights
without iterative training on held-out data is given
in Samuelsson (1996).
2.X -‹ X&apos; can be read as X is more specific than X&apos;.
</bodyText>
<page confidence="0.993575">
437
</page>
<bodyText confidence="0.998669457627118">
We can assume for simplicity&apos;s sake that the Ax.
do not depend on the value of X&apos;, but only on i. In
this case, if F is the number of features, there are
2&apos; — 1 more general terms, and we need to estimate
At&apos;s for all of these. In most applications the inter-
polation method is used for tasks with clear order-
ings of feature-sets (e.g. n-gram language modeling)
so that many of the 2&apos; — 1 terms can be omitted
beforehand. More recently, the integration of infor-
mation sources, and the modeling of more complex
language processing tasks in the statistical frame-
work has increased the interest in smoothing meth-
ods (Collins &amp; Brooks, 1995; Ratnaparkhi, 1996;
Magerman, 1994; Ng &amp; Lee, 1996; Collins, 1996).
For such applications with a diverse set of features
it is not necessarily the case that terms can be ex-
cluded beforehand.
If we let the Ax, depend on the value of Xi, the
number of parameters explodes even faster. A prac-
tical solution for this is to make a smaller number
of buckets for the Xi, e.g. by clustering (see e.g.
Magerman (1994)).
Note that linear interpolation (equation 5) actu-
ally performs two functions. In the first place, if the
most specific terms have non-zero frequency, it still
interpolates them with the more general terms. Be-
cause the more general terms should never overrule
the more specific ones, the Ax, for the more general
terms should be quite small. Therefore the inter-
polation effect is usually small or negligible. The
second function is the pure back-off function: if the
more specific terms have zero frequency, the proba-
bilities of the more general terms are used instead.
Only if terms are of a similar specificity, the A&apos;s truly
serve to weight relevance of the interpolation terms.
If we isolate the pure back-off function of the in-
terpolation equation we get an algorithm similar to
the one used in Collins &amp; Brooks (1995). It is given
in a schematic form in Table 1. Each step consists
of a back-off to a lower level of specificity. There
are as many steps as features, and there are a total
of 2F terms, divided over all the steps. Because all
features are considered of equal importance, we call
this the Naive Back-off algorithm.
Usually, not all features x are equally important,
so that not all back-off terms are equally relevant
for the re-estimation. Hence, the problem of fitting
the Ax, parameters is replaced by a term selection
task. To optimize the term selection, an evaluation
of the up to 2F terms on held-out data is still neces-
sary. In summary, the Back-off method does not pro-
vide a principled and practical domain-independent
method to adapt to the structure of a particular do-
main by determining a suitable ordering -&lt; between
events. In the next section, we will argue that a for-
mal operationalization of similarity between events,
as provided by MBL, can be used for this purpose.
In MBL the similarity metric and feature weighting
scheme automatically determine the implicit back-
</bodyText>
<equation confidence="0.990737545454546">
If f(xi,...,x.) &gt; 0:
13(clxi, ..,x„)
,x.)
Else if f(xi,...,xn—i,*) + + f (*, x2, , xn) &gt; 0:
15(c 1x1, , xri) f (c;Xl &amp;quot; • • &apos;X&amp;quot; —1,*)+•••+ f(C 14&apos;7121. • • 7z 71)
k111.••,Xn-1.1*)+•••+f (*,X2,•••
Else if. :
ii(clxi,•••,Xn)
Else if f(xi, *, *) + + f (*, •••,*,xn) &gt; 0:
f(c,si,*,...,*)+ ..+ f(c,*,. .,*,xn)
ii(c1X1, --)X72) = .,*)+...+f(*,...,*,xn)
</equation>
<tableCaption confidence="0.823356">
Table 1: The Naive Back-off smoothing algorithm.
</tableCaption>
<bodyText confidence="0.789825">
f(X) stands for the frequency of pattern X in the
training set. An asterix (*) stands for a wildcard in
a pattern. The terms at a higher level in the back-off
sequence are more specific (-0 than the lower levels.
off ordering using a domain independent heuristic,
with only a few parameters, in which there is no
need for held-out data.
</bodyText>
<sectionHeader confidence="0.993747" genericHeader="method">
4 A Comparison
</sectionHeader>
<bodyText confidence="0.999991222222222">
If we classify pattern X by looking at its nearest
neighbors, we are in fact estimating the probabil-
ity P(classIX), by looking at the relative frequency
of the class in the set defined by simk (X), where
simk(X) is a function from X to the set of most sim-
ilar patterns present in the training data&apos;. Although
the name &amp;quot;k-nearest neighbor&amp;quot; might mislead us by
suggesting that classification is based on exactly k
training patterns, the simk (X) function given by the
Overlap metric groups varying numbers of patterns
into buckets of equal similarity. A bucket is defined
by a particular number of mismatches with respect
to pattern X. Each bucket can further be decom-
posed into a number of schemata characterized by
the position of a wildcard (i.e. a mismatch). Thus
simk (X) specifies a --&lt; ordering in a Collins &amp; Brooks
style back-off sequence, where each bucket is a step
in the sequence, and each schema is a term in the
estimation formula at that step. In fact, the un-
weighted overlap metric specifies exactly the same
ordering as the Naive Back-off algorithm (table 1).
In Figure 1 this is shown for a four-featured pat-
tern. The most specific schema is the schema with
zero mismatches, which corresponds to the retrieval
of an identical pattern from memory, the most gen-
eral schema (not shown in the Figure) has a mis-
match on every feature, which corresponds to the
</bodyText>
<footnote confidence="0.707502333333333">
3Note that MBL is not limited to choosing the best
class. It can also return the conditional distribution of
all the classes.
</footnote>
<page confidence="0.989897">
438
</page>
<figure confidence="0.792396">
Overlap
</figure>
<figureCaption confidence="0.979624333333333">
Figure 1: An analysis of nearest neighbor sets into buckets (from left to right) and schemata (stacked). IG
weights reorder the schemata. The grey schemata are not used if the third feature has a very high weight
(see section 5.1).
</figureCaption>
<figure confidence="0.970361142857143">
exact match I mismatch
2 mismatches 3 mismatches
Overlap IG
rr
••
••
--
</figure>
<bodyText confidence="0.997357203703704">
entire memory being best neighbor.
If Information Gain weights are used in combina-
tion with the Overlap metric, individual schemata
instead of buckets become the steps of the back-off
sequence4. The -‹ ordering becomes slightly more
complicated now, as it depends on the number of
wildcards and on the magnitude of the weights at-
tached to those wildcards. Let S be the most specific
(zero mismatches) schema. We can then define the
-‹ ordering between schemata in the following equa-
tion, where A(X, Y) is the distance as defined in
equation 1.
5&amp;quot; -&lt;5&amp;quot; &lt;#. A(S, S&apos;) &lt;(S,S&amp;quot;) (6)
Note that this approach represents a type of im-
plicit parallelism. The importance of the 2&amp;quot; back-off
terms is specified using only F parameters—the IG
weights–, where F is the number of features. This
advantage is not restricted to the use of IG weights;
many other weighting schemes exist in the machine
learning literature (see Wettschereck et al. (1997)
for an overview).
Using the IG weights causes the algorithm to rely
on the most specific schema only. Although in most
applications this leads to a higher accuracy, because
it rejects schemata which do not match the most
important features, sometimes this constraint needs
to be weakened. This is desirable when: (i) there
are a number of schemata which are almost equally
relevant, (ii) the top ranked schema selects too few
cases to make a reliable estimate, or (iii) the chance
that the few items instantiating the schema are mis-
labeled in the training material is high. In such
cases we wish to include some of the lower-ranked
schemata. For case (i) this can be done by discretiz-
ing the IG weights into bins, so that minor differ-
ences will lose their significance, in effect merging
some schemata back into buckets. For (ii) and (iii),
and for continuous metrics (Stanfill &amp; Waltz, 1986;
Cost &amp; Salzberg, 1993) which extrapolate from ex-
actly k neighbors5, it might be necessary to choose a
k parameter larger than 1. This introduces one addi-
tional parameter, which has to be tuned on held-out
data. We can then use the distance between a pat-
tern and a schema to weight its vote in the nearest
neighbor extrapolation. This results in a back-off
sequence in which the terms at each step in the se-
quence are weighted with respect to each other, but
without the introduction of any additional weight-
ing parameters. A weighted voting function that
was found to work well is due to Dudani (1976): the
nearest neighbor schema receives a weight of 1.0, the
furthest schema a weight of 0.0, and the other neigh-
bors are scaled linearly to the line between these two
points.
</bodyText>
<footnote confidence="0.921628">
4Unless two schemata are exactly tied in their IG 5Note that the schema analysis does not apply to
values. these metrics.
</footnote>
<page confidence="0.995724">
439
</page>
<table confidence="0.99140975">
Method % Accuracy
ml (=Naive Back-off) 83.7 %
ml-IG 84.1 %
LexSpace IG 84.4 %
Back-off model (Collins &amp; Brooks) 84.1 %
C4.5 (Ratnaparkhi et al.) 79.9 %
Max Entropy (Ratnaparkhi et al.) 81.6 %
Brill&apos;s rules (Collins &amp; Brooks) 81.9 %
</table>
<tableCaption confidence="0.999536">
Table 2: Accuracy on the PP-attachment test set.
</tableCaption>
<sectionHeader confidence="0.976916" genericHeader="method">
5 Applications
</sectionHeader>
<subsectionHeader confidence="0.987123">
5.1 PP-attachment
</subsectionHeader>
<bodyText confidence="0.999962849315069">
In this section we describe experiments with MBL
on a data-set of Prepositional Phrase (PP) attach-
ment disambiguation cases. The problem in this
data-set is to disambiguate whether a PP attaches
to the verb (as in I ate pizza with a fork) or to the
noun (as in I ate pizza with cheese). This is a dif-
ficult and important problem, because the semantic
knowledge needed to solve the problem is very diffi-
cult to model, and the ambiguity can lead to a very
large number of interpretations for sentences.
We used a data-set extracted from the Penn
Treebank WSJ corpus by Ratnaparkhi et al. (1994).
It consists of sentences containing the possibly
ambiguous sequence verb noun-phrase PP. Cases
were constructed from these sentences by record-
ing the features: verb, head noun of the first noun
phrase, preposition, and head noun of the noun
phrase contained in the PP. The cases were la-
beled with the attachment decision as made by
the parse annotator of the corpus. So, for the
two example sentences given above we would get
the feature vectors ate,pizza,with,fork,V. and
ate ,pizza,with, cheese,N. The data-set contains
20801 training cases and 3097 separate test cases,
and was also used in Collins &amp; Brooks (1995).
The IG weights for the four features (V ,N,P ,N)
were respectively 0.03, 0.03, 0.10, 0.03. This identi-
fies the preposition as the most important feature:
its weight is higher than the sum of the other three
weights. The composition of the back-off sequence
following from this can be seen in the lower part
of Figure 1. The grey-colored schemata were effec-
tively left out, because they include a mismatch on
the preposition.
Table 2 shows a comparison of accuracy on the
test-set of 3097 cases. We can see that IB1, which
implicitly uses the same specificity ordering as the
Naive Back-off algorithm already performs quite well
in relation to other methods used in the literature.
Collins &amp; Brooks&apos; (1995) Back-off model is more so-
phisticated than the naive model, because they per-
formed a number of validation experiments on held-
out data to determine which terms to include and,
more importantly, which to exclude from the back-
off sequence. They excluded all terms which did
not match in the preposition! Not surprisingly, the
84.1% accuracy they achieve is matched by the per-
formance of IB1-IG. The two methods exactly mimic
each others behavior, in spite of their huge differ-
ence in design. It should however be noted that the
computation of IG-weights is many orders of mag-
nitude faster than the laborious evaluation of terms
on held-out data.
We also experimented with rich lexical represen-
tations obtained in an unsupervised way from word
co-occurrences in raw WSJ text (Zavrel &amp; Veenstra,
1995; Schiitze, 1994). We call these representations
Lexical Space vectors. Each word has a numeric 25
dimensional vector representation. Using these vec-
tors, in combination with the IG weights mentioned
above and a cosine metric, we got even slightly bet-
ter results. Because the cosine metric fails to group
the patterns into discrete schemata, it is necessary
to use a larger number of neighbors (k = 50). The
result in Table 2 is obtained using Dudani&apos;s weighted
voting method.
Note that to devise a back-off scheme on the basis
of these high-dimensional representations (each pat-
tern has 4 x 25 features) one would need to consider
up to 2100 smoothing terms. The MBL framework
is a convenient way to further experiment with even
more complex conditioning events, e.g. with seman-
tic labels added as features.
</bodyText>
<subsectionHeader confidence="0.99924">
5.2 POS-tagging
</subsectionHeader>
<bodyText confidence="0.9425345">
Another NLP problem where combination of differ-
ent sources of statistical information is an impor-
tant issue, is POS-tagging, especially for the guess-
ing of the POS-tag of words not present in the lex-
icon. Relevant information for guessing the tag of
an unknown word includes contextual information
(the words and tags in the context of the word), and
word form information (prefixes and suffixes, first
and last letters of the word as an approximation of
affix information, presence or absence of capitaliza-
tion, numbers, special characters etc.). There is a
large number of potentially informative features that
could play a role in correctly predicting the tag of
an unknown word (Ratnaparkhi, 1996; Weischedel
et al., 1993; Daelemans et al., 1996). A priori, it
is not clear what the relative importance is of these
features.
We compared Naive Back-off estimation and MBL
with two sets of features:
• pDAss: the first letter of the unknown word (p),
the tag of the word to the left of the unknown
word (d), a tag representing the set of possible
lexical categories of the word to the right of the
unknown word (a), and the two last letters (s).
The first letter provides information about cap-
italisation and the prefix, the two last letters
</bodyText>
<page confidence="0.991655">
440
</page>
<listItem confidence="0.804683333333333">
about suffixes.
• PDDDAAASSS: more left and right context fea-
tures, and more suffix information.
</listItem>
<bodyText confidence="0.999921375">
The data set consisted of 100,000 feature value
patterns taken from the Wall Street Journal corpus.
Only open-class words were used during construc-
tion of the training set. For both ml-IG and Naive
Back-off, a 10-fold cross-validation experiment was
run using both PDASS and PDDDAAASSS patterns.
The results are in Table 3. The IG values for the
features are given in Figure 2.
</bodyText>
<figure confidence="0.512382">
a a a a a
feature
</figure>
<figureCaption confidence="0.996114">
Figure 2: IG values for features used in predicting
the tag of unknown words.
</figureCaption>
<table confidence="0.998786333333333">
IB1, Naive Back-off IB1-IG
PDASS 88.5 (0.4) 88.3 (0.4)
PDDDAAASSS 85.9 (0.4) 89.8 (0.4)
</table>
<tableCaption confidence="0.536532333333333">
Table 3: Comparison of generalization accuracy of
Back-off and Memory-Based Learning on prediction
of category of unknown words. All differences are
statistically significant (two-tailed paired t-test, p &lt;
0.05). Standard deviations on the 10 experiments
are between brackets.
</tableCaption>
<bodyText confidence="0.999953933333333">
The results show that for Naive Back-off (and IB1)
the addition of more, possibly irrelevant, features
quickly becomes detrimental (decrease from 88.5 to
85.9), even if these added features do make a gener-
alisation performance increase possible (witness the
increase with IB1-IG from 88.3 to 89.8). Notice that
we did not actually compute the 210 terms of Naive
Back-off in the PDDDAAASSS condition, as IB1 is
guaranteed to provide statistically the same results.
Contrary to Naive Back-off and IB1, memory-based
learning with feature weighting (u31-IG) manages
to integrate diverse information sources by differ-
entially assigning relevance to the different features.
Since noisy features will receive low IG weights, this
also implies that it is much more noise-tolerant.
</bodyText>
<sectionHeader confidence="0.994466" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999954631578947">
We have analysed the relationship between Back-
off smoothing and Memory-Based Learning and es-
tablished a close correspondence between these two
frameworks which were hitherto mostly seen as un-
related. An exception is the use of similarity for al-
leviating the sparse data problem in language mod-
eling (Essen &amp; Steinbiss, 1992; Brown et al., 1992;
Dagan et at., 1994). However, these works differ in
their focus from our analysis in that the emphasis
is put on similarity between values of a feature (e.g.
words), instead of similarity between patterns that
are a (possibly complex) combination of many fea-
tures.
The comparison of MBL and Back-off shows that
the two approaches perform smoothing in a very sim-
ilar way, i.e. by using estimates from more general
patterns if specific patterns are absent in the train-
ing data. The analysis shows that MBL and Back-off
use exactly the same type of data and counts, and
this implies that MBL can safely be incorporated
into a system that is explicitly probabilistic. Since
the underlying k-NN classifier is a method that does
not necessitate any of the common independence or
distribution assumptions, this promises to be a fruit-
ful approach.
A serious advantage of the described approach,
is that in MBL the back-off sequence is specified
by the used similarity metric, without manual in-
tervention or the estimation of smoothing parame-
ters on held-out data, and requires only one param-
eter for each feature instead of an exponential num-
ber of parameters. With a feature-weighting met-
ric such as Information Gain, MBL is particularly
at an advantage for NLP tasks where conditioning
events are complex, where they consist of the fusion
of different information sources, or when the data is
noisy. This was illustrated by the experiments on
PP-attachment and POS-tagging data-sets.
</bodyText>
<page confidence="0.998425">
441
</page>
<sectionHeader confidence="0.997578" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999935125">
This research was done in the context of the &amp;quot;Induc-
tion of Linguistic Knowledge&amp;quot; research programme,
partially supported by the Foundation for Lan-
guage Speech and Logic (TSL), which is funded by
the Netherlands Organization for Scientific Research
(NWO). We would like to thank Peter Berck and
Anders Green for their help with software for the
experiments.
</bodyText>
<sectionHeader confidence="0.998861" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.973984795918368">
D. Aha, D. Kibler, and M. Albert. 1991. Instance-
based Learning Algorithms. Machine Learning,
Vol. 6, pp. 37-66.
L.R. Bahl, F. Jelinek and R.L. Mercer. 1983.
A Maximum Likelihood Approach to Continu-
ous Speech Recognition. IEEE Transactions on
Pattern Analysis and Machine Intelligence, Vol.
PAMI-5 (2), pp. 179-190.
Peter F. Brown, Vincent J. Della Pietra, Peter
V. deSouza, Jennifer C. Lai, and Robert L. Mer-
cer. 1992. Class-based N-gram Models of Natural
Language. Computational Linguistics, Vol. 18(4),
pp. 467-479.
Claire Cardie. 1994. Domain Specific Knowl-
edge Acquisition for Conceptual Sentence Anal-
ysis, PhD Thesis, University of Massachusets,
Amherst, MA.
Claire Cardie. 1996. Automatic Feature Set Selec-
tion for Case-Based Learning of Linguistic Knowl-
edge. In Proc. of the Conference on Empirical
Methods in Natural Language Processing, May 17-
18, 1996, University of Pennsylvania.
Stanley F.Chen and Joshua Goodman. 1996. An
Empirical Study of Smoothing Techniques for
Language Modelling. In Proc. of the 34th Annual
Meeting of the ACL, June 1996, Santa Cruz, CA,
ACL.
Kenneth W. Church and William A. Gale. 1991.
A comparison of the enhanced Good-Turing and
deleted estimation methods for estimating proba-
bilities of English bigrams. Computer Speech and
Language, Vol 19(5), pp. 19-54.
M. Collins. 1996. A New Statistical Parser Based on
Bigram Lexical Dependencies. In Proc. of the 34th
Annual Meeting of the ACL, June 1996, Santa
Cruz, CA, ACL.
M. Collins and J. Brooks. 1995. Prepositional
Phrase Attachment through a Backed-Off Model.
In Proceedings of the Third Workshop on Very
Large Corpora, Cambridge, MA.
S. Cost and S. Salzberg. 1993. A weighted near-
est neighbour algorithm for learning with symbolic
features. Machine Learning, Vol. 10, pp. 57-78.
Walter Daelemans and Antal van den Bosch.
1992. Generalisation Performance of Backprop-
agation Learning on a Syllabification Task. In
M. F. J. Drossaers &amp; A. Nijholt (eds.), TWLTS:
Connectionism and Natural Language Processing.
Enschede: Twente University. pp. 27-37.
Walter Daelemans. 1995. Memory-based lexical
acquisition and processing. In P. Steffens (ed.),
Machine Translation and the Lexicon. Springer
Lecture Notes in Artificial Intelligence, no. 898.
Berlin: Springer Verlag. pp. 85-98
Walter Daelemans. 1996. Abstraction Considered
Harmful: Lazy Learning of Language Process-
ing. In J. van den Herik and T. Weijters (eds.),
Benelearn-96. Proceedings of the 6th Belgian-
Dutch Conference on Machine Learning. MA-
TRIKS: Maastricht, The Netherlands, pp. 3-12.
Walter Daelemans, Jakub Zavrel, Peter Berck, and
Steven Gillis. 1996. MBT: A Memory-Based Part
of Speech Tagger Generator. In E. Ejerhed and
I. Dagan (eds.) Proc. of the Fourth Workshop on
Very Large Corpora, Copenhagen: ACL SIGDAT,
pp. 14-27.
Walter Daelemans, Antal van den Bosch, and Ton
Weijters. 1997. IGTree: Using Trees for Com-
pression and Classification in Lazy Learning Al-
gorithms. In D. Aha (ed.) Artificial Intelligence
Review, special issue on Lazy Learning, Vol. 11(1-
5).
Ido Dagan, Fernando Pereira, and Lillian Lee. 1994.
Similarity-Based Estimation of Word Cooccur-
rence Probabilities. In Proc. of the 32nd Annual
Meeting of the ACL, June 1994, Las Cruces, New
Mexico, ACL.
S.A. Dudani. 1981 The Distance-Weighted k-
Nearest Neighbor Rule IEEE Transactions on
Systems, Man, and Cybernetics, Vol. SMC-6, pp.
325-327.
Ute Essen, and Volker Steinbiss. 1992. Coocurrence
Smoothing for Stochastic Language Modeling. In
Proc. of ICASSP, Vol. 1, pp. 161-164, IEEE.
Slava M. Katz. 1987. Estimation of Probabilities
from Sparse Data for the Language Model Com-
ponent of a Speech Recognizer IEEE Transac-
tions on Acoustics, Speech and Signal Processing,
Vol. ASSP-35, pp. 400-401, March 1987.
David M. Magerman. 1994. Natural Language Pars-
ing as Statistical Pattern Recognition, PhD The-
sis, Department of Computer Science, Stanford
University.
Hwee Tou Ng and Hian Beng Lee. 1996. Integrat-
ing Multiple Knowledge Sources to Disambiguate
Word Sense: An Exemplar-Based Approach. In
Proc. of the 34th Annual Meeting of the ACL,
June 1996, Santa Cruz, CA, ACL.
</reference>
<page confidence="0.979724">
442
</page>
<reference confidence="0.993437302325582">
J. .R. Quinlan. 1986. Induction of Decision Trees.
Machine Learning, Vol. 1, pp. 81-206.
J. .R. Quinlan. 1993. c4.5: Programs for Machine
Learning. San Mateo, CA: Morgan Kaufmann.
Adwait Ratnaparkhi. 1996. A Maximum Entropy
Part-Of-Speech Tagger. In Proc. of the Confer-
ence on Empirical Methods in Natural Language
Processing, May 17-18, 1996, University of Penn-
sylvania.
A. Ratnaparkhi, J. Reynar and S. Roukos. 1994. A
maximum entropy model for Prepositional Phrase
Attachment. In ARPA Workshop on Human Lan-
guage Technology, Plainsboro, NJ.
Christer Samuelsson. 1996. Handling Sparse Data
by Successive Abstraction In Proc. of the Interna-
tional Confercnce on Computational Linguistics
(COLING&apos;96), August 1996, Copenhagen, Den-
mark.
Hinrich Schiitze. 1994. Distributional Part-of-
Speech Tagging. In Proc. of the 7th Conference of
the European Chapter of the Association for Com-
putational Linguistics (EACL&apos;95), Dublin, Ire-
land.
C. Stanfill and D. Waltz. 1986. Toward memory-
based reasoning. Communications of the ACM,
Vol. 29, pp. 1213-1228.
Ralph Weischedel, Marie Meteer, Richard Schwartz,
Lance Ramshaw, and Jeff Palmucci. 1993. Cop-
ing with Ambiguity and Unknown Words through
Probabilistic Models. Computational Linguistics,
Vol. 19(2). pp. 359-382.
D. Wettschereck, D. W. Aha, and T. Mohri.
1997. A Review and Comparative Evaluation of
Feature-Weighting Methods for Lazy Learning Al-
gorithms. In D. Aha (ed.) Artificial Intelligence
Review, special issue on Lazy Learning, Vol. 11(1-
5).
Jakub Zavrel and Jorn B. Veenstra. 1995. The Lan-
guage Environment and Syntactic Word-Class Ac-
quisition. In C.Koster and F.Wijnen (eds.) Proc.
of the Groningen Assembly on Language Acquisi-
tion (GALA 95). Center for Language and Cogni-
tion, Groningen, pp. 365-374.
</reference>
<page confidence="0.999319">
443
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.277974">
<title confidence="0.999865">Memory-Based Learning: Using Similarity for Smoothing</title>
<author confidence="0.725661">Zavrel Daelernans</author>
<affiliation confidence="0.989645">Computational Linguistics Tilburg University</affiliation>
<address confidence="0.847123">PO Box 90153 5000 LE Tilburg The Netherlands Izavrel,walterlOkub.n1</address>
<abstract confidence="0.9958706">This paper analyses the relation between the use of similarity in Memory-Based Learning and the notion of backed-off smoothing in statistical language modeling. We show that the two approaches are closely related, and we argue that feature weighting methods in the Memory-Based paradigm can offer the advantage of automatically specifying a suitable domainspecific hierarchy between most specific and most general conditioning information without the need for a large number of parameters. We report two applications of this approach: PP-attachment and POStagging. Our method achieves state-of-theart performance in both domains, and allows the easy integration of diverse information sources, such as rich lexical representations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Aha</author>
<author>D Kibler</author>
<author>M Albert</author>
</authors>
<title>Instancebased Learning Algorithms.</title>
<date>1991</date>
<journal>Machine Learning,</journal>
<volume>6</volume>
<pages>37--66</pages>
<contexts>
<context position="1681" citStr="Aha et al., 1991" startWordPosition="245" endWordPosition="248">s of available evidence. For this purpose a large number of probabilities has to be estimated from a training corpus. However, many possible conditioning events are not present in the training data, yielding zero Maximum Likelihood (ML) estimates. This motivates the need for smoothing methods, which reestimate the probabilities of low-count events from more reliable estimates. Inductive generalization from observed to new data lies at the heart of machine-learning approaches to disambiguation. In Memory-Based Learning&apos; (MBL) induction is based on the use of similarity (Stanfill &amp; Waltz, 1986; Aha et al., 1991; Cardie, 1994; Daelemans, 1995). In this paper we describe how the use of similarity between patterns embodies a solution to the sparse data problem, how it &apos;The Approach is also referred to as Case-based, Instance-based or Exemplar-based. relates to backed-off smoothing methods and what advantages it offers when combining diverse and rich information sources. We illustrate the analysis by applying MBL to two tasks where combination of information sources promises to bring improved performance: PPattachment disambiguation and Part of Speech tagging. 2 Memory-Based Language Processing The basi</context>
<context position="4043" citStr="Aha et al., 1991" startWordPosition="631" endWordPosition="634"> with the use of an appropriate similarity metric. It is our experience that even limited forms of abstraction can harm performance on linguistic tasks, which often contain many subregularities and exceptions (Daelemans, 1996). 2.1 Similarity metrics The most basic metric for patterns with symbolic features is the Overlap metric given in equations 1 436 and 2; where A(X, Y) is the distance between patterns X and Y, represented by n features, wi is a weight for feature i, and 6 is the distance per feature. The k-NN algorithm with this metric, and equal weighting for all features is called 031 (Aha et al., 1991). Usually k is set to 1. A(X,Y) = E wi (5(x ,y,) (1) where: S(x„yz) = 0 if xi = y„ else 1 (2) This metric simply counts the number of (mis)matching feature values in both patterns. If we do not have information about the importance of features, this is a reasonable choice. But if we do have some information about feature relevance one possibility would be to add linguistic bias to weight or select different features (Cardie, 1996). An alternative--more empiricist—approach, is to look at the behavior of features in the set of examples used for training. We can compute statistics about the relev</context>
</contexts>
<marker>Aha, Kibler, Albert, 1991</marker>
<rawString>D. Aha, D. Kibler, and M. Albert. 1991. Instancebased Learning Algorithms. Machine Learning, Vol. 6, pp. 37-66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Bahl</author>
<author>F Jelinek</author>
<author>R L Mercer</author>
</authors>
<title>A Maximum Likelihood Approach to Continuous Speech Recognition.</title>
<date>1983</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>5</volume>
<issue>2</issue>
<pages>179--190</pages>
<contexts>
<context position="7314" citStr="Bahl et al., 1983" startWordPosition="1186" endWordPosition="1189">ta set by looking at the relative joint frequency of occurrence of the classes and pattern X. If pattern X is described by a number of feature-values , , xn, we can write the conditional probability as P(classlxi,...,xn). If a particular pattern is not literally present among the examples, all classes have zero ML probability estimates. Smoothing methods are needed to avoid zeroes on events that could occur in the test material. There are two main approaches to smoothing: count re-estimation smoothing such as the Add-One or Good-Turing methods (Church &amp; Gale, 1991), and Back-off type methods (Bahl et al., 1983; Katz, 1987; Chen &amp; Goodman, 1996; Samuelsson, 1996). We will focus here on a comparison with Back-off type methods, because an experimental comparison in Chen &amp; Goodman (1996) shows the superiority of Back-off based methods over count re-estimation smoothing methods. With the Back-off method the probabilities of complex conditioning events are approximated by (a linear interpolation of) the probabilities of more general events: 23(classIX) = Axi5(classIX)+ Ax,P(classIX&apos;) +.. ± Ax4(classIX11) (5) Where 23 stands for the smoothed estimate, 23 for the relative frequency estimate, A are interpol</context>
</contexts>
<marker>Bahl, Jelinek, Mercer, 1983</marker>
<rawString>L.R. Bahl, F. Jelinek and R.L. Mercer. 1983. A Maximum Likelihood Approach to Continuous Speech Recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, Vol. PAMI-5 (2), pp. 179-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<date>1992</date>
<journal>Class-based N-gram Models of Natural Language. Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<pages>467--479</pages>
<contexts>
<context position="23904" citStr="Brown et al., 1992" startWordPosition="3994" endWordPosition="3997">d learning with feature weighting (u31-IG) manages to integrate diverse information sources by differentially assigning relevance to the different features. Since noisy features will receive low IG weights, this also implies that it is much more noise-tolerant. 6 Conclusion We have analysed the relationship between Backoff smoothing and Memory-Based Learning and established a close correspondence between these two frameworks which were hitherto mostly seen as unrelated. An exception is the use of similarity for alleviating the sparse data problem in language modeling (Essen &amp; Steinbiss, 1992; Brown et al., 1992; Dagan et at., 1994). However, these works differ in their focus from our analysis in that the emphasis is put on similarity between values of a feature (e.g. words), instead of similarity between patterns that are a (possibly complex) combination of many features. The comparison of MBL and Back-off shows that the two approaches perform smoothing in a very similar way, i.e. by using estimates from more general patterns if specific patterns are absent in the training data. The analysis shows that MBL and Back-off use exactly the same type of data and counts, and this implies that MBL can safel</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based N-gram Models of Natural Language. Computational Linguistics, Vol. 18(4), pp. 467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
</authors>
<title>Domain Specific Knowledge Acquisition for Conceptual Sentence Analysis, PhD Thesis,</title>
<date>1994</date>
<institution>University of Massachusets,</institution>
<location>Amherst, MA.</location>
<contexts>
<context position="1695" citStr="Cardie, 1994" startWordPosition="249" endWordPosition="250">dence. For this purpose a large number of probabilities has to be estimated from a training corpus. However, many possible conditioning events are not present in the training data, yielding zero Maximum Likelihood (ML) estimates. This motivates the need for smoothing methods, which reestimate the probabilities of low-count events from more reliable estimates. Inductive generalization from observed to new data lies at the heart of machine-learning approaches to disambiguation. In Memory-Based Learning&apos; (MBL) induction is based on the use of similarity (Stanfill &amp; Waltz, 1986; Aha et al., 1991; Cardie, 1994; Daelemans, 1995). In this paper we describe how the use of similarity between patterns embodies a solution to the sparse data problem, how it &apos;The Approach is also referred to as Case-based, Instance-based or Exemplar-based. relates to backed-off smoothing methods and what advantages it offers when combining diverse and rich information sources. We illustrate the analysis by applying MBL to two tasks where combination of information sources promises to bring improved performance: PPattachment disambiguation and Part of Speech tagging. 2 Memory-Based Language Processing The basic idea in Memo</context>
</contexts>
<marker>Cardie, 1994</marker>
<rawString>Claire Cardie. 1994. Domain Specific Knowledge Acquisition for Conceptual Sentence Analysis, PhD Thesis, University of Massachusets, Amherst, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claire Cardie</author>
</authors>
<title>Automatic Feature Set Selection for Case-Based Learning of Linguistic Knowledge.</title>
<date>1996</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="4477" citStr="Cardie, 1996" startWordPosition="712" endWordPosition="713">atures, wi is a weight for feature i, and 6 is the distance per feature. The k-NN algorithm with this metric, and equal weighting for all features is called 031 (Aha et al., 1991). Usually k is set to 1. A(X,Y) = E wi (5(x ,y,) (1) where: S(x„yz) = 0 if xi = y„ else 1 (2) This metric simply counts the number of (mis)matching feature values in both patterns. If we do not have information about the importance of features, this is a reasonable choice. But if we do have some information about feature relevance one possibility would be to add linguistic bias to weight or select different features (Cardie, 1996). An alternative--more empiricist—approach, is to look at the behavior of features in the set of examples used for training. We can compute statistics about the relevance of features by looking at which features are good predictors of the class labels. Information Theory gives us a useful tool for measuring feature relevance in this way (Quinlan, 1986; Quinlan, 1993). Information Gain (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label. The Information Gain of feature f is measured by computing the diff</context>
</contexts>
<marker>Cardie, 1996</marker>
<rawString>Claire Cardie. 1996. Automatic Feature Set Selection for Case-Based Learning of Linguistic Knowledge. In Proc. of the Conference on Empirical Methods in Natural Language Processing, May 17-18, 1996, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stanley F Chen</author>
<author>Joshua Goodman</author>
</authors>
<title>An Empirical Study of Smoothing Techniques for Language Modelling.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the ACL,</booktitle>
<location>Santa Cruz, CA, ACL.</location>
<contexts>
<context position="7348" citStr="Chen &amp; Goodman, 1996" startWordPosition="1192" endWordPosition="1195">ve joint frequency of occurrence of the classes and pattern X. If pattern X is described by a number of feature-values , , xn, we can write the conditional probability as P(classlxi,...,xn). If a particular pattern is not literally present among the examples, all classes have zero ML probability estimates. Smoothing methods are needed to avoid zeroes on events that could occur in the test material. There are two main approaches to smoothing: count re-estimation smoothing such as the Add-One or Good-Turing methods (Church &amp; Gale, 1991), and Back-off type methods (Bahl et al., 1983; Katz, 1987; Chen &amp; Goodman, 1996; Samuelsson, 1996). We will focus here on a comparison with Back-off type methods, because an experimental comparison in Chen &amp; Goodman (1996) shows the superiority of Back-off based methods over count re-estimation smoothing methods. With the Back-off method the probabilities of complex conditioning events are approximated by (a linear interpolation of) the probabilities of more general events: 23(classIX) = Axi5(classIX)+ Ax,P(classIX&apos;) +.. ± Ax4(classIX11) (5) Where 23 stands for the smoothed estimate, 23 for the relative frequency estimate, A are interpolation weights, Ein_o Ax. = 1, and </context>
</contexts>
<marker>Chen, Goodman, 1996</marker>
<rawString>Stanley F.Chen and Joshua Goodman. 1996. An Empirical Study of Smoothing Techniques for Language Modelling. In Proc. of the 34th Annual Meeting of the ACL, June 1996, Santa Cruz, CA, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<volume>19</volume>
<issue>5</issue>
<pages>pp.</pages>
<contexts>
<context position="7268" citStr="Church &amp; Gale, 1991" startWordPosition="1178" endWordPosition="1181">assIX). This probability is estimated from the data set by looking at the relative joint frequency of occurrence of the classes and pattern X. If pattern X is described by a number of feature-values , , xn, we can write the conditional probability as P(classlxi,...,xn). If a particular pattern is not literally present among the examples, all classes have zero ML probability estimates. Smoothing methods are needed to avoid zeroes on events that could occur in the test material. There are two main approaches to smoothing: count re-estimation smoothing such as the Add-One or Good-Turing methods (Church &amp; Gale, 1991), and Back-off type methods (Bahl et al., 1983; Katz, 1987; Chen &amp; Goodman, 1996; Samuelsson, 1996). We will focus here on a comparison with Back-off type methods, because an experimental comparison in Chen &amp; Goodman (1996) shows the superiority of Back-off based methods over count re-estimation smoothing methods. With the Back-off method the probabilities of complex conditioning events are approximated by (a linear interpolation of) the probabilities of more general events: 23(classIX) = Axi5(classIX)+ Ax,P(classIX&apos;) +.. ± Ax4(classIX11) (5) Where 23 stands for the smoothed estimate, 23 for t</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Kenneth W. Church and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilities of English bigrams. Computer Speech and Language, Vol 19(5), pp. 19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>A New Statistical Parser Based on Bigram Lexical Dependencies.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the ACL,</booktitle>
<location>Santa Cruz, CA, ACL.</location>
<contexts>
<context position="9201" citStr="Collins, 1996" startWordPosition="1500" endWordPosition="1501">nly on i. In this case, if F is the number of features, there are 2&apos; — 1 more general terms, and we need to estimate At&apos;s for all of these. In most applications the interpolation method is used for tasks with clear orderings of feature-sets (e.g. n-gram language modeling) so that many of the 2&apos; — 1 terms can be omitted beforehand. More recently, the integration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods (Collins &amp; Brooks, 1995; Ratnaparkhi, 1996; Magerman, 1994; Ng &amp; Lee, 1996; Collins, 1996). For such applications with a diverse set of features it is not necessarily the case that terms can be excluded beforehand. If we let the Ax, depend on the value of Xi, the number of parameters explodes even faster. A practical solution for this is to make a smaller number of buckets for the Xi, e.g. by clustering (see e.g. Magerman (1994)). Note that linear interpolation (equation 5) actually performs two functions. In the first place, if the most specific terms have non-zero frequency, it still interpolates them with the more general terms. Because the more general terms should never overru</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>M. Collins. 1996. A New Statistical Parser Based on Bigram Lexical Dependencies. In Proc. of the 34th Annual Meeting of the ACL, June 1996, Santa Cruz, CA, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>J Brooks</author>
</authors>
<title>Prepositional Phrase Attachment through a Backed-Off Model.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora,</booktitle>
<location>Cambridge, MA.</location>
<contexts>
<context position="9134" citStr="Collins &amp; Brooks, 1995" startWordPosition="1488" endWordPosition="1491"> for simplicity&apos;s sake that the Ax. do not depend on the value of X&apos;, but only on i. In this case, if F is the number of features, there are 2&apos; — 1 more general terms, and we need to estimate At&apos;s for all of these. In most applications the interpolation method is used for tasks with clear orderings of feature-sets (e.g. n-gram language modeling) so that many of the 2&apos; — 1 terms can be omitted beforehand. More recently, the integration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods (Collins &amp; Brooks, 1995; Ratnaparkhi, 1996; Magerman, 1994; Ng &amp; Lee, 1996; Collins, 1996). For such applications with a diverse set of features it is not necessarily the case that terms can be excluded beforehand. If we let the Ax, depend on the value of Xi, the number of parameters explodes even faster. A practical solution for this is to make a smaller number of buckets for the Xi, e.g. by clustering (see e.g. Magerman (1994)). Note that linear interpolation (equation 5) actually performs two functions. In the first place, if the most specific terms have non-zero frequency, it still interpolates them with the mor</context>
<context position="10367" citStr="Collins &amp; Brooks (1995)" startWordPosition="1700" endWordPosition="1703">terms. Because the more general terms should never overrule the more specific ones, the Ax, for the more general terms should be quite small. Therefore the interpolation effect is usually small or negligible. The second function is the pure back-off function: if the more specific terms have zero frequency, the probabilities of the more general terms are used instead. Only if terms are of a similar specificity, the A&apos;s truly serve to weight relevance of the interpolation terms. If we isolate the pure back-off function of the interpolation equation we get an algorithm similar to the one used in Collins &amp; Brooks (1995). It is given in a schematic form in Table 1. Each step consists of a back-off to a lower level of specificity. There are as many steps as features, and there are a total of 2F terms, divided over all the steps. Because all features are considered of equal importance, we call this the Naive Back-off algorithm. Usually, not all features x are equally important, so that not all back-off terms are equally relevant for the re-estimation. Hence, the problem of fitting the Ax, parameters is replaced by a term selection task. To optimize the term selection, an evaluation of the up to 2F terms on held</context>
<context position="18264" citStr="Collins &amp; Brooks (1995)" startWordPosition="3068" endWordPosition="3071">994). It consists of sentences containing the possibly ambiguous sequence verb noun-phrase PP. Cases were constructed from these sentences by recording the features: verb, head noun of the first noun phrase, preposition, and head noun of the noun phrase contained in the PP. The cases were labeled with the attachment decision as made by the parse annotator of the corpus. So, for the two example sentences given above we would get the feature vectors ate,pizza,with,fork,V. and ate ,pizza,with, cheese,N. The data-set contains 20801 training cases and 3097 separate test cases, and was also used in Collins &amp; Brooks (1995). The IG weights for the four features (V ,N,P ,N) were respectively 0.03, 0.03, 0.10, 0.03. This identifies the preposition as the most important feature: its weight is higher than the sum of the other three weights. The composition of the back-off sequence following from this can be seen in the lower part of Figure 1. The grey-colored schemata were effectively left out, because they include a mismatch on the preposition. Table 2 shows a comparison of accuracy on the test-set of 3097 cases. We can see that IB1, which implicitly uses the same specificity ordering as the Naive Back-off algorith</context>
</contexts>
<marker>Collins, Brooks, 1995</marker>
<rawString>M. Collins and J. Brooks. 1995. Prepositional Phrase Attachment through a Backed-Off Model. In Proceedings of the Third Workshop on Very Large Corpora, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Cost</author>
<author>S Salzberg</author>
</authors>
<title>A weighted nearest neighbour algorithm for learning with symbolic features.</title>
<date>1993</date>
<booktitle>Machine Learning,</booktitle>
<volume>10</volume>
<pages>57--78</pages>
<contexts>
<context position="15859" citStr="Cost &amp; Salzberg, 1993" startWordPosition="2651" endWordPosition="2654"> is desirable when: (i) there are a number of schemata which are almost equally relevant, (ii) the top ranked schema selects too few cases to make a reliable estimate, or (iii) the chance that the few items instantiating the schema are mislabeled in the training material is high. In such cases we wish to include some of the lower-ranked schemata. For case (i) this can be done by discretizing the IG weights into bins, so that minor differences will lose their significance, in effect merging some schemata back into buckets. For (ii) and (iii), and for continuous metrics (Stanfill &amp; Waltz, 1986; Cost &amp; Salzberg, 1993) which extrapolate from exactly k neighbors5, it might be necessary to choose a k parameter larger than 1. This introduces one additional parameter, which has to be tuned on held-out data. We can then use the distance between a pattern and a schema to weight its vote in the nearest neighbor extrapolation. This results in a back-off sequence in which the terms at each step in the sequence are weighted with respect to each other, but without the introduction of any additional weighting parameters. A weighted voting function that was found to work well is due to Dudani (1976): the nearest neighbo</context>
</contexts>
<marker>Cost, Salzberg, 1993</marker>
<rawString>S. Cost and S. Salzberg. 1993. A weighted nearest neighbour algorithm for learning with symbolic features. Machine Learning, Vol. 10, pp. 57-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
</authors>
<title>Generalisation Performance of Backpropagation Learning on a Syllabification Task.</title>
<date>1992</date>
<booktitle>TWLTS: Connectionism and Natural Language Processing. Enschede: Twente University.</booktitle>
<pages>27--37</pages>
<editor>In M. F. J. Drossaers &amp; A. Nijholt (eds.),</editor>
<marker>Daelemans, van den Bosch, 1992</marker>
<rawString>Walter Daelemans and Antal van den Bosch. 1992. Generalisation Performance of Backpropagation Learning on a Syllabification Task. In M. F. J. Drossaers &amp; A. Nijholt (eds.), TWLTS: Connectionism and Natural Language Processing. Enschede: Twente University. pp. 27-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
</authors>
<title>Memory-based lexical acquisition and processing.</title>
<date>1995</date>
<booktitle>Machine Translation and the Lexicon. Springer Lecture Notes in Artificial Intelligence, no. 898.</booktitle>
<pages>85--98</pages>
<editor>In P. Steffens (ed.),</editor>
<publisher>Springer Verlag.</publisher>
<location>Berlin:</location>
<contexts>
<context position="1713" citStr="Daelemans, 1995" startWordPosition="251" endWordPosition="252">s purpose a large number of probabilities has to be estimated from a training corpus. However, many possible conditioning events are not present in the training data, yielding zero Maximum Likelihood (ML) estimates. This motivates the need for smoothing methods, which reestimate the probabilities of low-count events from more reliable estimates. Inductive generalization from observed to new data lies at the heart of machine-learning approaches to disambiguation. In Memory-Based Learning&apos; (MBL) induction is based on the use of similarity (Stanfill &amp; Waltz, 1986; Aha et al., 1991; Cardie, 1994; Daelemans, 1995). In this paper we describe how the use of similarity between patterns embodies a solution to the sparse data problem, how it &apos;The Approach is also referred to as Case-based, Instance-based or Exemplar-based. relates to backed-off smoothing methods and what advantages it offers when combining diverse and rich information sources. We illustrate the analysis by applying MBL to two tasks where combination of information sources promises to bring improved performance: PPattachment disambiguation and Part of Speech tagging. 2 Memory-Based Language Processing The basic idea in Memory-Based language </context>
</contexts>
<marker>Daelemans, 1995</marker>
<rawString>Walter Daelemans. 1995. Memory-based lexical acquisition and processing. In P. Steffens (ed.), Machine Translation and the Lexicon. Springer Lecture Notes in Artificial Intelligence, no. 898. Berlin: Springer Verlag. pp. 85-98</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
</authors>
<title>Abstraction Considered Harmful: Lazy Learning of Language Processing.</title>
<date>1996</date>
<booktitle>Benelearn-96. Proceedings of the 6th BelgianDutch Conference on Machine Learning. MATRIKS: Maastricht, The Netherlands,</booktitle>
<pages>3--12</pages>
<editor>In J. van den Herik and T. Weijters (eds.),</editor>
<contexts>
<context position="3652" citStr="Daelemans, 1996" startWordPosition="560" endWordPosition="561">xtrapolation from the k nearest neighbors, i.e. the output is chosen that has the highest relative frequency among the nearest neighbors. Note that no abstractions, such as grammatical rules, stochastic automata, or decision trees are extracted from the examples. Rule-like behavior results from the linguistic regularities that are present in the patterns of usage in memory in combination with the use of an appropriate similarity metric. It is our experience that even limited forms of abstraction can harm performance on linguistic tasks, which often contain many subregularities and exceptions (Daelemans, 1996). 2.1 Similarity metrics The most basic metric for patterns with symbolic features is the Overlap metric given in equations 1 436 and 2; where A(X, Y) is the distance between patterns X and Y, represented by n features, wi is a weight for feature i, and 6 is the distance per feature. The k-NN algorithm with this metric, and equal weighting for all features is called 031 (Aha et al., 1991). Usually k is set to 1. A(X,Y) = E wi (5(x ,y,) (1) where: S(x„yz) = 0 if xi = y„ else 1 (2) This metric simply counts the number of (mis)matching feature values in both patterns. If we do not have informatio</context>
</contexts>
<marker>Daelemans, 1996</marker>
<rawString>Walter Daelemans. 1996. Abstraction Considered Harmful: Lazy Learning of Language Processing. In J. van den Herik and T. Weijters (eds.), Benelearn-96. Proceedings of the 6th BelgianDutch Conference on Machine Learning. MATRIKS: Maastricht, The Netherlands, pp. 3-12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Jakub Zavrel</author>
<author>Peter Berck</author>
<author>Steven Gillis</author>
</authors>
<title>MBT: A Memory-Based Part of Speech Tagger Generator.</title>
<date>1996</date>
<booktitle>Proc. of the Fourth Workshop on Very Large Corpora, Copenhagen: ACL SIGDAT,</booktitle>
<pages>14--27</pages>
<editor>In E. Ejerhed and I. Dagan (eds.)</editor>
<contexts>
<context position="21341" citStr="Daelemans et al., 1996" startWordPosition="3578" endWordPosition="3581">pecially for the guessing of the POS-tag of words not present in the lexicon. Relevant information for guessing the tag of an unknown word includes contextual information (the words and tags in the context of the word), and word form information (prefixes and suffixes, first and last letters of the word as an approximation of affix information, presence or absence of capitalization, numbers, special characters etc.). There is a large number of potentially informative features that could play a role in correctly predicting the tag of an unknown word (Ratnaparkhi, 1996; Weischedel et al., 1993; Daelemans et al., 1996). A priori, it is not clear what the relative importance is of these features. We compared Naive Back-off estimation and MBL with two sets of features: • pDAss: the first letter of the unknown word (p), the tag of the word to the left of the unknown word (d), a tag representing the set of possible lexical categories of the word to the right of the unknown word (a), and the two last letters (s). The first letter provides information about capitalisation and the prefix, the two last letters 440 about suffixes. • PDDDAAASSS: more left and right context features, and more suffix information. The d</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>Walter Daelemans, Jakub Zavrel, Peter Berck, and Steven Gillis. 1996. MBT: A Memory-Based Part of Speech Tagger Generator. In E. Ejerhed and I. Dagan (eds.) Proc. of the Fourth Workshop on Very Large Corpora, Copenhagen: ACL SIGDAT, pp. 14-27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Walter Daelemans</author>
<author>Antal van den Bosch</author>
<author>Ton Weijters</author>
</authors>
<title>IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms. In</title>
<date>1997</date>
<journal>Artificial Intelligence Review, special issue on Lazy Learning,</journal>
<volume>Vol.</volume>
<pages>11--1</pages>
<editor>D. Aha (ed.)</editor>
<marker>Daelemans, van den Bosch, Weijters, 1997</marker>
<rawString>Walter Daelemans, Antal van den Bosch, and Ton Weijters. 1997. IGTree: Using Trees for Compression and Classification in Lazy Learning Algorithms. In D. Aha (ed.) Artificial Intelligence Review, special issue on Lazy Learning, Vol. 11(1-5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Fernando Pereira</author>
<author>Lillian Lee</author>
</authors>
<title>Similarity-Based Estimation of Word Cooccurrence Probabilities.</title>
<date>1994</date>
<booktitle>In Proc. of the 32nd Annual Meeting of the ACL,</booktitle>
<location>Las Cruces, New Mexico, ACL.</location>
<marker>Dagan, Pereira, Lee, 1994</marker>
<rawString>Ido Dagan, Fernando Pereira, and Lillian Lee. 1994. Similarity-Based Estimation of Word Cooccurrence Probabilities. In Proc. of the 32nd Annual Meeting of the ACL, June 1994, Las Cruces, New Mexico, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Dudani</author>
</authors>
<title>The Distance-Weighted kNearest Neighbor Rule</title>
<date>1981</date>
<journal>IEEE Transactions on Systems, Man, and Cybernetics,</journal>
<volume>6</volume>
<pages>325--327</pages>
<marker>Dudani, 1981</marker>
<rawString>S.A. Dudani. 1981 The Distance-Weighted kNearest Neighbor Rule IEEE Transactions on Systems, Man, and Cybernetics, Vol. SMC-6, pp. 325-327.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ute Essen</author>
<author>Volker Steinbiss</author>
</authors>
<title>Coocurrence Smoothing for Stochastic Language Modeling.</title>
<date>1992</date>
<booktitle>In Proc. of ICASSP,</booktitle>
<volume>1</volume>
<pages>161--164</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="23884" citStr="Essen &amp; Steinbiss, 1992" startWordPosition="3990" endWordPosition="3993">-off and IB1, memory-based learning with feature weighting (u31-IG) manages to integrate diverse information sources by differentially assigning relevance to the different features. Since noisy features will receive low IG weights, this also implies that it is much more noise-tolerant. 6 Conclusion We have analysed the relationship between Backoff smoothing and Memory-Based Learning and established a close correspondence between these two frameworks which were hitherto mostly seen as unrelated. An exception is the use of similarity for alleviating the sparse data problem in language modeling (Essen &amp; Steinbiss, 1992; Brown et al., 1992; Dagan et at., 1994). However, these works differ in their focus from our analysis in that the emphasis is put on similarity between values of a feature (e.g. words), instead of similarity between patterns that are a (possibly complex) combination of many features. The comparison of MBL and Back-off shows that the two approaches perform smoothing in a very similar way, i.e. by using estimates from more general patterns if specific patterns are absent in the training data. The analysis shows that MBL and Back-off use exactly the same type of data and counts, and this implie</context>
</contexts>
<marker>Essen, Steinbiss, 1992</marker>
<rawString>Ute Essen, and Volker Steinbiss. 1992. Coocurrence Smoothing for Stochastic Language Modeling. In Proc. of ICASSP, Vol. 1, pp. 161-164, IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<volume>35</volume>
<pages>400--401</pages>
<contexts>
<context position="7326" citStr="Katz, 1987" startWordPosition="1190" endWordPosition="1191">t the relative joint frequency of occurrence of the classes and pattern X. If pattern X is described by a number of feature-values , , xn, we can write the conditional probability as P(classlxi,...,xn). If a particular pattern is not literally present among the examples, all classes have zero ML probability estimates. Smoothing methods are needed to avoid zeroes on events that could occur in the test material. There are two main approaches to smoothing: count re-estimation smoothing such as the Add-One or Good-Turing methods (Church &amp; Gale, 1991), and Back-off type methods (Bahl et al., 1983; Katz, 1987; Chen &amp; Goodman, 1996; Samuelsson, 1996). We will focus here on a comparison with Back-off type methods, because an experimental comparison in Chen &amp; Goodman (1996) shows the superiority of Back-off based methods over count re-estimation smoothing methods. With the Back-off method the probabilities of complex conditioning events are approximated by (a linear interpolation of) the probabilities of more general events: 23(classIX) = Axi5(classIX)+ Ax,P(classIX&apos;) +.. ± Ax4(classIX11) (5) Where 23 stands for the smoothed estimate, 23 for the relative frequency estimate, A are interpolation weight</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Slava M. Katz. 1987. Estimation of Probabilities from Sparse Data for the Language Model Component of a Speech Recognizer IEEE Transactions on Acoustics, Speech and Signal Processing, Vol. ASSP-35, pp. 400-401, March 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Natural Language Parsing as Statistical Pattern Recognition,</title>
<date>1994</date>
<tech>PhD Thesis,</tech>
<institution>Department of Computer Science, Stanford University.</institution>
<contexts>
<context position="9169" citStr="Magerman, 1994" startWordPosition="1494" endWordPosition="1495">depend on the value of X&apos;, but only on i. In this case, if F is the number of features, there are 2&apos; — 1 more general terms, and we need to estimate At&apos;s for all of these. In most applications the interpolation method is used for tasks with clear orderings of feature-sets (e.g. n-gram language modeling) so that many of the 2&apos; — 1 terms can be omitted beforehand. More recently, the integration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods (Collins &amp; Brooks, 1995; Ratnaparkhi, 1996; Magerman, 1994; Ng &amp; Lee, 1996; Collins, 1996). For such applications with a diverse set of features it is not necessarily the case that terms can be excluded beforehand. If we let the Ax, depend on the value of Xi, the number of parameters explodes even faster. A practical solution for this is to make a smaller number of buckets for the Xi, e.g. by clustering (see e.g. Magerman (1994)). Note that linear interpolation (equation 5) actually performs two functions. In the first place, if the most specific terms have non-zero frequency, it still interpolates them with the more general terms. Because the more g</context>
</contexts>
<marker>Magerman, 1994</marker>
<rawString>David M. Magerman. 1994. Natural Language Parsing as Statistical Pattern Recognition, PhD Thesis, Department of Computer Science, Stanford University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Hian Beng Lee</author>
</authors>
<title>Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th Annual Meeting of the ACL,</booktitle>
<location>Santa Cruz, CA, ACL.</location>
<contexts>
<context position="9185" citStr="Ng &amp; Lee, 1996" startWordPosition="1496" endWordPosition="1499">lue of X&apos;, but only on i. In this case, if F is the number of features, there are 2&apos; — 1 more general terms, and we need to estimate At&apos;s for all of these. In most applications the interpolation method is used for tasks with clear orderings of feature-sets (e.g. n-gram language modeling) so that many of the 2&apos; — 1 terms can be omitted beforehand. More recently, the integration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods (Collins &amp; Brooks, 1995; Ratnaparkhi, 1996; Magerman, 1994; Ng &amp; Lee, 1996; Collins, 1996). For such applications with a diverse set of features it is not necessarily the case that terms can be excluded beforehand. If we let the Ax, depend on the value of Xi, the number of parameters explodes even faster. A practical solution for this is to make a smaller number of buckets for the Xi, e.g. by clustering (see e.g. Magerman (1994)). Note that linear interpolation (equation 5) actually performs two functions. In the first place, if the most specific terms have non-zero frequency, it still interpolates them with the more general terms. Because the more general terms sho</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>Hwee Tou Ng and Hian Beng Lee. 1996. Integrating Multiple Knowledge Sources to Disambiguate Word Sense: An Exemplar-Based Approach. In Proc. of the 34th Annual Meeting of the ACL, June 1996, Santa Cruz, CA, ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>Induction of Decision Trees.</title>
<date>1986</date>
<booktitle>Machine Learning,</booktitle>
<volume>1</volume>
<pages>81--206</pages>
<contexts>
<context position="4830" citStr="Quinlan, 1986" startWordPosition="770" endWordPosition="771">terns. If we do not have information about the importance of features, this is a reasonable choice. But if we do have some information about feature relevance one possibility would be to add linguistic bias to weight or select different features (Cardie, 1996). An alternative--more empiricist—approach, is to look at the behavior of features in the set of examples used for training. We can compute statistics about the relevance of features by looking at which features are good predictors of the class labels. Information Theory gives us a useful tool for measuring feature relevance in this way (Quinlan, 1986; Quinlan, 1993). Information Gain (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label. The Information Gain of feature f is measured by computing the difference in uncertainty (i.e. entropy) between the situations without and with knowledge of the value of that feature (Equation 3). EvEvf P(v) x H(Civ) wf H(C) (3) si(f) si(f) = — E P(v) log2 P(v) (4) VEVf Where C is the set of class labels, Vf is the set of values for feature f, and H(C) = — EcEc p(c) log2 P(c) is the entropy of the class labels. The p</context>
</contexts>
<marker>Quinlan, 1986</marker>
<rawString>J. .R. Quinlan. 1986. Induction of Decision Trees. Machine Learning, Vol. 1, pp. 81-206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>c4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann.</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="4846" citStr="Quinlan, 1993" startWordPosition="772" endWordPosition="774"> not have information about the importance of features, this is a reasonable choice. But if we do have some information about feature relevance one possibility would be to add linguistic bias to weight or select different features (Cardie, 1996). An alternative--more empiricist—approach, is to look at the behavior of features in the set of examples used for training. We can compute statistics about the relevance of features by looking at which features are good predictors of the class labels. Information Theory gives us a useful tool for measuring feature relevance in this way (Quinlan, 1986; Quinlan, 1993). Information Gain (IG) weighting looks at each feature in isolation, and measures how much information it contributes to our knowledge of the correct class label. The Information Gain of feature f is measured by computing the difference in uncertainty (i.e. entropy) between the situations without and with knowledge of the value of that feature (Equation 3). EvEvf P(v) x H(Civ) wf H(C) (3) si(f) si(f) = — E P(v) log2 P(v) (4) VEVf Where C is the set of class labels, Vf is the set of values for feature f, and H(C) = — EcEc p(c) log2 P(c) is the entropy of the class labels. The probabilities are</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. .R. Quinlan. 1993. c4.5: Programs for Machine Learning. San Mateo, CA: Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Part-Of-Speech Tagger.</title>
<date>1996</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="9153" citStr="Ratnaparkhi, 1996" startWordPosition="1492" endWordPosition="1493">hat the Ax. do not depend on the value of X&apos;, but only on i. In this case, if F is the number of features, there are 2&apos; — 1 more general terms, and we need to estimate At&apos;s for all of these. In most applications the interpolation method is used for tasks with clear orderings of feature-sets (e.g. n-gram language modeling) so that many of the 2&apos; — 1 terms can be omitted beforehand. More recently, the integration of information sources, and the modeling of more complex language processing tasks in the statistical framework has increased the interest in smoothing methods (Collins &amp; Brooks, 1995; Ratnaparkhi, 1996; Magerman, 1994; Ng &amp; Lee, 1996; Collins, 1996). For such applications with a diverse set of features it is not necessarily the case that terms can be excluded beforehand. If we let the Ax, depend on the value of Xi, the number of parameters explodes even faster. A practical solution for this is to make a smaller number of buckets for the Xi, e.g. by clustering (see e.g. Magerman (1994)). Note that linear interpolation (equation 5) actually performs two functions. In the first place, if the most specific terms have non-zero frequency, it still interpolates them with the more general terms. Be</context>
<context position="21291" citStr="Ratnaparkhi, 1996" startWordPosition="3572" endWordPosition="3573">on is an important issue, is POS-tagging, especially for the guessing of the POS-tag of words not present in the lexicon. Relevant information for guessing the tag of an unknown word includes contextual information (the words and tags in the context of the word), and word form information (prefixes and suffixes, first and last letters of the word as an approximation of affix information, presence or absence of capitalization, numbers, special characters etc.). There is a large number of potentially informative features that could play a role in correctly predicting the tag of an unknown word (Ratnaparkhi, 1996; Weischedel et al., 1993; Daelemans et al., 1996). A priori, it is not clear what the relative importance is of these features. We compared Naive Back-off estimation and MBL with two sets of features: • pDAss: the first letter of the unknown word (p), the tag of the word to the left of the unknown word (d), a tag representing the set of possible lexical categories of the word to the right of the unknown word (a), and the two last letters (s). The first letter provides information about capitalisation and the prefix, the two last letters 440 about suffixes. • PDDDAAASSS: more left and right co</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>Adwait Ratnaparkhi. 1996. A Maximum Entropy Part-Of-Speech Tagger. In Proc. of the Conference on Empirical Methods in Natural Language Processing, May 17-18, 1996, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
<author>J Reynar</author>
<author>S Roukos</author>
</authors>
<title>A maximum entropy model for Prepositional Phrase Attachment.</title>
<date>1994</date>
<booktitle>In ARPA Workshop on Human Language Technology,</booktitle>
<location>Plainsboro, NJ.</location>
<contexts>
<context position="17645" citStr="Ratnaparkhi et al. (1994)" startWordPosition="2968" endWordPosition="2971">5 Applications 5.1 PP-attachment In this section we describe experiments with MBL on a data-set of Prepositional Phrase (PP) attachment disambiguation cases. The problem in this data-set is to disambiguate whether a PP attaches to the verb (as in I ate pizza with a fork) or to the noun (as in I ate pizza with cheese). This is a difficult and important problem, because the semantic knowledge needed to solve the problem is very difficult to model, and the ambiguity can lead to a very large number of interpretations for sentences. We used a data-set extracted from the Penn Treebank WSJ corpus by Ratnaparkhi et al. (1994). It consists of sentences containing the possibly ambiguous sequence verb noun-phrase PP. Cases were constructed from these sentences by recording the features: verb, head noun of the first noun phrase, preposition, and head noun of the noun phrase contained in the PP. The cases were labeled with the attachment decision as made by the parse annotator of the corpus. So, for the two example sentences given above we would get the feature vectors ate,pizza,with,fork,V. and ate ,pizza,with, cheese,N. The data-set contains 20801 training cases and 3097 separate test cases, and was also used in Coll</context>
</contexts>
<marker>Ratnaparkhi, Reynar, Roukos, 1994</marker>
<rawString>A. Ratnaparkhi, J. Reynar and S. Roukos. 1994. A maximum entropy model for Prepositional Phrase Attachment. In ARPA Workshop on Human Language Technology, Plainsboro, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christer Samuelsson</author>
</authors>
<title>Handling Sparse Data by Successive Abstraction</title>
<date>1996</date>
<booktitle>In Proc. of the International Confercnce on Computational Linguistics (COLING&apos;96),</booktitle>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="7367" citStr="Samuelsson, 1996" startWordPosition="1196" endWordPosition="1197">occurrence of the classes and pattern X. If pattern X is described by a number of feature-values , , xn, we can write the conditional probability as P(classlxi,...,xn). If a particular pattern is not literally present among the examples, all classes have zero ML probability estimates. Smoothing methods are needed to avoid zeroes on events that could occur in the test material. There are two main approaches to smoothing: count re-estimation smoothing such as the Add-One or Good-Turing methods (Church &amp; Gale, 1991), and Back-off type methods (Bahl et al., 1983; Katz, 1987; Chen &amp; Goodman, 1996; Samuelsson, 1996). We will focus here on a comparison with Back-off type methods, because an experimental comparison in Chen &amp; Goodman (1996) shows the superiority of Back-off based methods over count re-estimation smoothing methods. With the Back-off method the probabilities of complex conditioning events are approximated by (a linear interpolation of) the probabilities of more general events: 23(classIX) = Axi5(classIX)+ Ax,P(classIX&apos;) +.. ± Ax4(classIX11) (5) Where 23 stands for the smoothed estimate, 23 for the relative frequency estimate, A are interpolation weights, Ein_o Ax. = 1, and X -‹ X&apos; for all i, </context>
</contexts>
<marker>Samuelsson, 1996</marker>
<rawString>Christer Samuelsson. 1996. Handling Sparse Data by Successive Abstraction In Proc. of the International Confercnce on Computational Linguistics (COLING&apos;96), August 1996, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Distributional Part-ofSpeech Tagging.</title>
<date>1994</date>
<booktitle>In Proc. of the 7th Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;95),</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="19783" citStr="Schiitze, 1994" startWordPosition="3325" endWordPosition="3326">h to exclude from the backoff sequence. They excluded all terms which did not match in the preposition! Not surprisingly, the 84.1% accuracy they achieve is matched by the performance of IB1-IG. The two methods exactly mimic each others behavior, in spite of their huge difference in design. It should however be noted that the computation of IG-weights is many orders of magnitude faster than the laborious evaluation of terms on held-out data. We also experimented with rich lexical representations obtained in an unsupervised way from word co-occurrences in raw WSJ text (Zavrel &amp; Veenstra, 1995; Schiitze, 1994). We call these representations Lexical Space vectors. Each word has a numeric 25 dimensional vector representation. Using these vectors, in combination with the IG weights mentioned above and a cosine metric, we got even slightly better results. Because the cosine metric fails to group the patterns into discrete schemata, it is necessary to use a larger number of neighbors (k = 50). The result in Table 2 is obtained using Dudani&apos;s weighted voting method. Note that to devise a back-off scheme on the basis of these high-dimensional representations (each pattern has 4 x 25 features) one would ne</context>
</contexts>
<marker>Schiitze, 1994</marker>
<rawString>Hinrich Schiitze. 1994. Distributional Part-ofSpeech Tagging. In Proc. of the 7th Conference of the European Chapter of the Association for Computational Linguistics (EACL&apos;95), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Stanfill</author>
<author>D Waltz</author>
</authors>
<title>Toward memorybased reasoning.</title>
<date>1986</date>
<journal>Communications of the ACM,</journal>
<volume>29</volume>
<pages>1213--1228</pages>
<contexts>
<context position="1663" citStr="Stanfill &amp; Waltz, 1986" startWordPosition="241" endWordPosition="244">ely decision on the basis of available evidence. For this purpose a large number of probabilities has to be estimated from a training corpus. However, many possible conditioning events are not present in the training data, yielding zero Maximum Likelihood (ML) estimates. This motivates the need for smoothing methods, which reestimate the probabilities of low-count events from more reliable estimates. Inductive generalization from observed to new data lies at the heart of machine-learning approaches to disambiguation. In Memory-Based Learning&apos; (MBL) induction is based on the use of similarity (Stanfill &amp; Waltz, 1986; Aha et al., 1991; Cardie, 1994; Daelemans, 1995). In this paper we describe how the use of similarity between patterns embodies a solution to the sparse data problem, how it &apos;The Approach is also referred to as Case-based, Instance-based or Exemplar-based. relates to backed-off smoothing methods and what advantages it offers when combining diverse and rich information sources. We illustrate the analysis by applying MBL to two tasks where combination of information sources promises to bring improved performance: PPattachment disambiguation and Part of Speech tagging. 2 Memory-Based Language P</context>
<context position="15835" citStr="Stanfill &amp; Waltz, 1986" startWordPosition="2647" endWordPosition="2650">eds to be weakened. This is desirable when: (i) there are a number of schemata which are almost equally relevant, (ii) the top ranked schema selects too few cases to make a reliable estimate, or (iii) the chance that the few items instantiating the schema are mislabeled in the training material is high. In such cases we wish to include some of the lower-ranked schemata. For case (i) this can be done by discretizing the IG weights into bins, so that minor differences will lose their significance, in effect merging some schemata back into buckets. For (ii) and (iii), and for continuous metrics (Stanfill &amp; Waltz, 1986; Cost &amp; Salzberg, 1993) which extrapolate from exactly k neighbors5, it might be necessary to choose a k parameter larger than 1. This introduces one additional parameter, which has to be tuned on held-out data. We can then use the distance between a pattern and a schema to weight its vote in the nearest neighbor extrapolation. This results in a back-off sequence in which the terms at each step in the sequence are weighted with respect to each other, but without the introduction of any additional weighting parameters. A weighted voting function that was found to work well is due to Dudani (19</context>
</contexts>
<marker>Stanfill, Waltz, 1986</marker>
<rawString>C. Stanfill and D. Waltz. 1986. Toward memorybased reasoning. Communications of the ACM, Vol. 29, pp. 1213-1228.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Weischedel</author>
<author>Marie Meteer</author>
<author>Richard Schwartz</author>
<author>Lance Ramshaw</author>
<author>Jeff Palmucci</author>
</authors>
<date>1993</date>
<booktitle>Coping with Ambiguity and Unknown Words through Probabilistic Models. Computational Linguistics,</booktitle>
<volume>19</volume>
<issue>2</issue>
<pages>359--382</pages>
<contexts>
<context position="21316" citStr="Weischedel et al., 1993" startWordPosition="3574" endWordPosition="3577">issue, is POS-tagging, especially for the guessing of the POS-tag of words not present in the lexicon. Relevant information for guessing the tag of an unknown word includes contextual information (the words and tags in the context of the word), and word form information (prefixes and suffixes, first and last letters of the word as an approximation of affix information, presence or absence of capitalization, numbers, special characters etc.). There is a large number of potentially informative features that could play a role in correctly predicting the tag of an unknown word (Ratnaparkhi, 1996; Weischedel et al., 1993; Daelemans et al., 1996). A priori, it is not clear what the relative importance is of these features. We compared Naive Back-off estimation and MBL with two sets of features: • pDAss: the first letter of the unknown word (p), the tag of the word to the left of the unknown word (d), a tag representing the set of possible lexical categories of the word to the right of the unknown word (a), and the two last letters (s). The first letter provides information about capitalisation and the prefix, the two last letters 440 about suffixes. • PDDDAAASSS: more left and right context features, and more </context>
</contexts>
<marker>Weischedel, Meteer, Schwartz, Ramshaw, Palmucci, 1993</marker>
<rawString>Ralph Weischedel, Marie Meteer, Richard Schwartz, Lance Ramshaw, and Jeff Palmucci. 1993. Coping with Ambiguity and Unknown Words through Probabilistic Models. Computational Linguistics, Vol. 19(2). pp. 359-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wettschereck</author>
<author>D W Aha</author>
<author>T Mohri</author>
</authors>
<title>A Review and Comparative Evaluation of Feature-Weighting Methods for Lazy Learning Algorithms. In</title>
<date>1997</date>
<journal>Artificial Intelligence Review, special issue on Lazy Learning,</journal>
<volume>Vol.</volume>
<pages>11--1</pages>
<editor>D. Aha (ed.)</editor>
<contexts>
<context position="14943" citStr="Wettschereck et al. (1997)" startWordPosition="2494" endWordPosition="2497">tude of the weights attached to those wildcards. Let S be the most specific (zero mismatches) schema. We can then define the -‹ ordering between schemata in the following equation, where A(X, Y) is the distance as defined in equation 1. 5&amp;quot; -&lt;5&amp;quot; &lt;#. A(S, S&apos;) &lt;(S,S&amp;quot;) (6) Note that this approach represents a type of implicit parallelism. The importance of the 2&amp;quot; back-off terms is specified using only F parameters—the IG weights–, where F is the number of features. This advantage is not restricted to the use of IG weights; many other weighting schemes exist in the machine learning literature (see Wettschereck et al. (1997) for an overview). Using the IG weights causes the algorithm to rely on the most specific schema only. Although in most applications this leads to a higher accuracy, because it rejects schemata which do not match the most important features, sometimes this constraint needs to be weakened. This is desirable when: (i) there are a number of schemata which are almost equally relevant, (ii) the top ranked schema selects too few cases to make a reliable estimate, or (iii) the chance that the few items instantiating the schema are mislabeled in the training material is high. In such cases we wish to </context>
</contexts>
<marker>Wettschereck, Aha, Mohri, 1997</marker>
<rawString>D. Wettschereck, D. W. Aha, and T. Mohri. 1997. A Review and Comparative Evaluation of Feature-Weighting Methods for Lazy Learning Algorithms. In D. Aha (ed.) Artificial Intelligence Review, special issue on Lazy Learning, Vol. 11(1-5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Zavrel</author>
<author>Jorn B Veenstra</author>
</authors>
<title>The Language Environment and Syntactic Word-Class Acquisition.</title>
<date>1995</date>
<booktitle>In C.Koster and F.Wijnen (eds.) Proc. of the Groningen Assembly on Language Acquisition (GALA 95). Center for Language and Cognition, Groningen,</booktitle>
<pages>365--374</pages>
<contexts>
<context position="19766" citStr="Zavrel &amp; Veenstra, 1995" startWordPosition="3321" endWordPosition="3324">d, more importantly, which to exclude from the backoff sequence. They excluded all terms which did not match in the preposition! Not surprisingly, the 84.1% accuracy they achieve is matched by the performance of IB1-IG. The two methods exactly mimic each others behavior, in spite of their huge difference in design. It should however be noted that the computation of IG-weights is many orders of magnitude faster than the laborious evaluation of terms on held-out data. We also experimented with rich lexical representations obtained in an unsupervised way from word co-occurrences in raw WSJ text (Zavrel &amp; Veenstra, 1995; Schiitze, 1994). We call these representations Lexical Space vectors. Each word has a numeric 25 dimensional vector representation. Using these vectors, in combination with the IG weights mentioned above and a cosine metric, we got even slightly better results. Because the cosine metric fails to group the patterns into discrete schemata, it is necessary to use a larger number of neighbors (k = 50). The result in Table 2 is obtained using Dudani&apos;s weighted voting method. Note that to devise a back-off scheme on the basis of these high-dimensional representations (each pattern has 4 x 25 featu</context>
</contexts>
<marker>Zavrel, Veenstra, 1995</marker>
<rawString>Jakub Zavrel and Jorn B. Veenstra. 1995. The Language Environment and Syntactic Word-Class Acquisition. In C.Koster and F.Wijnen (eds.) Proc. of the Groningen Assembly on Language Acquisition (GALA 95). Center for Language and Cognition, Groningen, pp. 365-374.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>