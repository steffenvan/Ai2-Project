<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004250">
<title confidence="0.998763">
Multi-Level Alignments As An Extensible Representation Basis
for Textual Entailment Algorithms
</title>
<author confidence="0.881101">
Tae-Gil Noh (noh@cl.uni-heidelberg.de)1, Sebastian Pad´o (pado@ims.uni-stuttgart.de)2,
Vered Shwartz (vered1986@gmail.com)3, Ido Dagan (dagan@cs.biu.ac.il)3,
Vivi Nastase (nastase@fbk.eu)4, Kathrin Eichler (kathrin.eichler@dfki.de)5,
</author>
<affiliation confidence="0.913253666666667">
Lili Kotlerman (lili.dav@gmail.com)3, and Meni Adler (meni.adler@gmail.com)3
1Institute of Computational Linguistics, Heidelberg University, Germany
2Institute of Natural Language Processing, Stuttgart University, Germany
3Department of Computer Science, Bar-Ilan University, Israel
4Human Language Technologies, Fondazione Bruno Kessler, Italy
5Language Technology Lab, DFKI GmbH, Germany
</affiliation>
<sectionHeader confidence="0.987678" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999669733333333">
A major problem in research on Textual Entail-
ment (TE) is the high implementation effort for
TE systems. Recently, interoperable standards
for annotation and preprocessing have been
proposed. In contrast, the algorithmic level
remains unstandardized, which makes compo-
nent re-use in this area very difficult in prac-
tice. In this paper, we introduce multi-level
alignments as a central, powerful representa-
tion for TE algorithms that encourages modu-
lar, reusable, multilingual algorithm develop-
ment. We demonstrate that a pilot open-source
implementation of multi-level alignment with
minimal features competes with state-of-the-
art open-source TE engines in three languages.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981688888889">
A key challenge of Natural Language Processing is
to determine what conclusions can be drawn from a
natural language text, a task known as Textual Entail-
ment (TE, Dagan and Glickman 2004). The ability to
recognize TE helps dealing with surface variability in
tasks like Question Answering (Harabagiu and Hickl,
2006), Intelligent Tutoring (Nielsen et al., 2009), or
Text Exploration (Berant et al., 2012). Open source
implementations a number of TE algorithms have be-
come available over the last years, including BIUTEE
(Stern and Dagan, 2012) and EDITS (Kouylekov and
Negri, 2010), which has made it much easier for end
users to utilize TE engines.
At the same time, the situation is still more difficult
for researchers and developers. Even though recently
a common platform for TE has been proposed (Pad´o
et al., 2015) that standardizes important aspects like
annotation types, preprocessing, and knowledge re-
sources, it largely ignores the algorithmic level. In
fact, TE algorithms themselves are generally not de-
signed to be extensible or interoperable. Therefore,
changes to the algorithms – like adding support for a
new language or for new analysis aspect – are often
very involved, if not impossible. This often forces
the next generation of TE researchers to develop and
implement their own core algorithms from scratch.
In this paper, we address this problem by propos-
ing a schema for TE algorithms that revolves around
a central representation layer called multi-level align-
ment geared towards encoding the relevant informa-
tion for deciding entailment. The use of multi-level
alignments encourages a modular, extensible devel-
opment of TE algorithms that can be partitioned into
“alignment producers” and “alignment consumers”.
This enables for future researchers and developers
to change analysis components or add new ones in a
straightforward manner.
We also present evaluation results for a very simple
TE algorithm based on multi-level alignments for
English, German and Italian. It utilizes a minimal
set of analyzers and four basic language-independent
features. It can thus be regarded as a baseline of
the performance achievable with this approach. The
results can already compete with the best open-source
engines available for each of the languages.
</bodyText>
<sectionHeader confidence="0.911047" genericHeader="method">
2 TE with Multi-Level Alignments
</sectionHeader>
<bodyText confidence="0.995718">
The quality of the word alignment between a Text (T)
and a Hypothesis (H) has been used very early as a
simple feature to decide about TE. When it was found
</bodyText>
<page confidence="0.990276">
193
</page>
<note confidence="0.605914">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 193–198,
Denver, Colorado, June 4–5, 2015.
</note>
<figure confidence="0.70497">
T-H input sentences
</figure>
<figureCaption confidence="0.992946">
Figure 1: Dataflow for TE algorithms based on multi level
alignment
</figureCaption>
<bodyText confidence="0.999624046875">
that alignment strength can be misleading (MacCart-
ney et al., 2006), alignment was understood as an
intermediate step whose outcome is a set of corre-
spondences between parts of T and H that can be used
to define (mis-)match features. Alignments can be es-
tablished at the word level, phrase level (MacCartney
et al., 2008), or dependency level (Dinu and Wang,
2009). Dagan et al. (2013) generalized this practical
use to an architectural principle: They showed that
various TE algorithms can be mapped onto a uni-
versal alignment-based schema with six steps: pre-
processing, enrichment, candidate alignment genera-
tion, alignment selection, and classification.
Proposal. Our proposal is similar to, but simpler
than, Dagan et al.’s. Figure 1 shows the data flow.
First, the text and the hypothesis are linguistically
pre-processed. Then, the annotated T-H pair becomes
the input for various independent aligners, which
have access to knowledge resources and can compute
any evidence for or against entailment that can be
represented as a weighted alignment between any
linguistic levels of H and T. Note that this includes
many analyses not normally treated as alignment, e.g.
match or mismatch in negation or modality between
parts of T and H. The union of all alignments forms
the central data structure, the Multi-Level Alignments.
The next step is feature extraction. Features can
be extracted on the basis of individual alignments, or
from sets of alignments. We assume that the features
form a vector describing the T-H pair, and that the
last step is supervised entailment classification.
Discussion. The main difference to Dagan et al.’s
schema is that we intentionally leave out the step
of alignment selection which explicitly selects a sin-
gle alignment for each part of H or T, typically the
globally most probable one. Our decision to forgo
selection is grounded in our design of multi-level
alignments as a repository that supports coexistence
of information from different sources. This has the
following benefits: (a) aligners become decoupled
in that adding a new aligner does not have a direct
impact on other aligners; (b) alignments produced by
different aligners can have different semantics, e.g.
positive (match) or negative (mismatch); (c) inter-
actions between alignments can still be captured by
defining features in the feature extraction step.
In this manner, multi-level alignments serve as an
abstraction layer that encourages the development
of TE algorithms composed of small, self-contained
modules that solve specialized tasks in TE recogni-
tion. Each of these modules consists of two parts:
an aligner, and a set of feature extractors. A priori,
each module can be defined independently; to intro-
duce interactions with other modules, it should be
sufficient to extend the feature extractors.
The practical benefit for the developer is that even
relatively complex TE algorithms use a small set
of well-defined interfaces, which makes them easy
to manage, even at the implementation level. The
startup cost is getting acquainted with the common
data structure of multi-level alignments. We believe
that developers are willing to pay this cost, especially
when this provides them with a platform that supports
multilingual pre-processing and resources.
</bodyText>
<figure confidence="0.998987391304348">
1-pre-processing pipeline
T
H
aligner1
aligner2
aligner3
knowledge
resource
2-adding alignments
multi-level
alignment
T-H pair enriched
with various levels
of alignments
T
H
3-extracting features
T-H pair expressed
as a data point
4-classifying entailment
Entailment Decision
T-H with linguistic
annotations
</figure>
<page confidence="0.939044">
194
</page>
<table confidence="0.907414068965517">
3 Implementation and Evaluation Paraphrase Aligner. The paraphrase aligner con-
centrates on surface forms rather than lemmas and
can align sequences of them rather than just individ-
ual tokens. It uses paraphrase tables, e.g. extracted
from parallel corpora (Bannard and Callison-Burch,
2005). The alignment process is similar to the lexical
aligner: any two sequences of tokens in T and H are
aligned if the pair is listed in the resource. The align-
ment links created by this aligner instantiate only
one relation (“paraphrase”) but report the strength of
the relation via the translation probability. We used
the paraphrase tables provided by the METEOR MT
evaluation package (Denkowski and Lavie, 2014),
which are available for numerous languages.
Lemma Identity Aligner. This aligner does not
use any resources. It simply aligns identical lem-
mas between T and H and plays an important role in
practice to deal with named entities.
3.3 A Minimal Feature Set
Similar to the aligners, we concentrate on a small set
of four features in the pilot algorithm. Again, the
features are completely language independent, even
at the implementation level. This is possible because
the linguistic annotations and the alignments, use a
language-independent type system (cf. Section 3.1).
All current features measure some form of cov-
erage on the Hypothesis, i.e. the percentage of H
that can be explained by T. The underlying hypoth-
esis is that a higher coverage of H corresponds to a
higher chance of entailment. Since parts-of-speech
arguably differ in the importance of being covered,
we compute coverage for four sets of words sepa-
rately: (a), for all words; (b), for content words; (c),
for verbs; (d), for proper names (according to the
POS tagger). The features are defined on the union
of all produced alignments: i.e., two words count as
aligned if they were aligned by any aligner. Clearly,
this is an overly simplistic (albeit surprisingly effec-
tive) strategy. It can be considered a baseline for our
approach that can be extended with many features
that suggest themselves from the literature.
4 Experimental Evaluation
Evaluation 1: RTE-3. RTE-3 was the third in-
stance of the yearly benchmarking workshops of the
Textual Entailment community (Giampiccolo et al.,
We describe an implementation of a pilot TE algo-
rithm based on the Multi-Level Alignment approach
and its evaluation in three languages (EN, DE, IT).
The system is available as open-source.1
3.1 Technical Foundations
We implement the algorithm within an open source
TE development platform (Pad´o et al., 2015).
The platform provides various multilingual pre-
processing pipelines and knowledge resources such
as WordNet, VerbOcean, etc., under a shared
API. For pre-processing, we use TreeTagger-based
pipelines for all three languages.
Another important service provided by the plat-
</table>
<figureCaption confidence="0.632478821428571">
form is the ability of storing a wide range of linguistic
annotations in a common, language-independent data
representation. The platform uses UIMA CAS (Fer-
rucci and Lally, 2004) as the data container, adopts
the DKPro type system (de Castilho and Gurevych,
2014), and defines annotation types which can be
extended in a controlled manner. We used this capa-
bility to define a multilingual Multi-Level Alignment
layer with little implementation effort.
3.2 A Minimal Set of Aligners
The pilot algorithm restricts itself to three aligners.
All three are fully language-independent, even if two
use language-specific knowledge resources.
Lexical Aligner. The lexical aligner adds an align-
ment link for a pair of lemmas in T and H if it finds
some kind of semantic relationships between them
in a set of lexical resources. The link is directed,
labeled (by the semantic relation, e.g. “synonym”,
“antonym”) and weighted, with the weight indicating
the strength of the relationship. Note that this aligner
can on its own already produce alignment links with
inconsistent semantics (positive and negative). For
English, WordNet and VerbOcean were used as lexi-
cal resources. Italian WordNet was used for Italian,
and GermaNet and German DerivBase (Zeller et al.,
2013) were used as lexical resources for German.
1As a part of Excitement Open Platform for Textual En-
tailment. https://github.com/hltfbk/EOP-1.2.1/
</figureCaption>
<table confidence="0.984846375">
wiki/AlignmentEDAP1
195
English German Italian
MultiAlign 67.0 64.5 65.4
BIUTEE 67.0 - -
TIE 65.2 63.1 -
EDITS 63.6 - 62.6
RTE3 median 61.8
</table>
<tableCaption confidence="0.999959">
Table 1: Accuracy evaluation on the RTE3 dataset
</tableCaption>
<bodyText confidence="0.992016184210526">
2007). The English dataset created for RTE-3 con-
sists of 800 training and 800 testing T-H pairs. Later,
the RTE-3 dataset was translated into both German
and Italian (Magnini et al., 2014). It is the only Tex-
tual Entailment dataset in multiple languages with
the same content. The task is binary TE recognition,
with baseline of 50% accuracy (balanced classes).
We trained and tested our Multi-Level Alignment
approach (MultiAlign) on the RTE-3 dataset sepa-
rately for each language. We compare against the
other RTE systems from the platform by Pad´o et al.
(2015), namely BIUTEE (Stern and Dagan, 2012),
EDITS (Kouylekov and Negri, 2010), and TIE (Wang
and Zhang, 2009). Each system is configured with
its best known configurations. The pilot system sup-
ports all three languages, while others support one
(BIUTEE) or two languages (EDITS, TIE).
The results are shown in Table 1. The pilot system
performs well in all three languages. It ties with BIU-
TEE on English and it outperforms TIE and EDITS in
their respective results on German and Italian. This
is particularly notable since all three systems have
gone through several years of development, while
MultiAlign is only a pilot implementation.
Evaluation 2: T-H pairs from Application Data.
We perform the second evaluation on real-world ap-
plication data from two application datasets: an en-
tailment graph dataset (for English and Italian), and
an e-mail categorization dataset (for German). En-
tailment graph building is the task of constructing
graphs that hierarchically structure the statements
from a collection (Berant et al., 2012) for the ap-
plication of Text Exploration. In TE-based e-mail
categorization, the goal is to assign the right cate-
gory to an email with TE, using the email as T and a
category description as H. (Eichler et al., 2014).
Due to space constraints, we cannot evaluate these
applications end-to-end. Instead, we focus on the
</bodyText>
<table confidence="0.9980722">
English German Italian
MultiAlign 69.2 72.4 69.5
BIUTEE 71.3 - -
TIE 67.3 72.4 -
EDITS 66.6 - 65.6
</table>
<tableCaption confidence="0.999004">
Table 2: Fl evaluation on application data
</tableCaption>
<bodyText confidence="0.999913054054054">
respective first step, the binary decision of entailment
for individual T-H pairs. This task corresponds to
RTE-3, and the main difference to Evaluation 1 is
that these pairs come from real-world interactions
and were produced by native speakers. All T-H pairs
are sampled from application gold data which were
manually constructed on the basis of anonymized cus-
tomer interactions (Eichler et al. (2014) for German;
Kotlerman et al. (2015) for English and Italian2).
The sets are fairly large (5300 pairs for English, 1700
for Italian, 1274 for German), and were sampled to
be balanced. We report F1 for comparability with
non-balanced setups (our random baseline is F1=50).
Table 2 shows our evaluation results. MultiAlign
system beats EDITS for Italian (+4), and ties with
TIE for German. On English, BIUTEE still outper-
forms MultiAlign (-2). Thus, MultiAlign also per-
forms acceptably on real-world data.
In sum, we find that MultiAlign is already compet-
itive with state-of-the-art open-source TE engines on
three languages. MultiAlign is not only much less
complex, but it is also a single system covering all
three languages, without any language-specific opti-
mizations. We interpret this as a positive sign for the
future of the Multi-Level Alignment approach.
Visualization. The platform also supports visual-
ization of individual Text-Hypothesis pairs, showing
the alignments that were created by the system as
well as the features computed on the basis of the
alignments. The visualization was built on the basis
of the BRAT library.3
Figure 2 shows an example for the Text The judges
made an assessment of Peter’s credibility and the
Hypothesis The judges assessed if Peter was credible.
The top line shows the final prediction, Entailment,
and the confidence (75%). The main part shows the
Text and the Hypothesis below each other, connected
</bodyText>
<footnote confidence="0.9826845">
2Both datasets are publicly available.
3http://brat.nlplab.org/index.html
</footnote>
<page confidence="0.99542">
196
</page>
<figureCaption confidence="0.99984">
Figure 2: Screenshot of the Multi-Level Alignment Visualizer
</figureCaption>
<bodyText confidence="0.999973642857143">
by alignment links that are labeled with their source
and their score. Note that the alignments can link
individual words (assessment and assess are aligned
through a derivational link from WordNet) but also
phrases (The two occurrences of The judges in Text
and Hypothesis are linked by virtue of being identical
lemmas).
The three features currently used by the English
system are are shown below. As can be seen, they
aggregate very simple statistics about the alignments:
5 of 7 tokens in the hypothesis are covered, 4 out
of 5 content words, and the one proper name is also
aligned. This situation motivates nicely the use of
those features: a relatively low alignment coverage
on all tokens is still compatible with entailment as
long as the crucial tokens are aligned.
This visualization enables end users to quickly
take in the justification behind the system’s decision.
Developers can inspect alignments and features for
plausibility and detect possible bugs and assess the
limitations of aligners and their underlying resources.
For example, the current example shows a wrong
link produced by the VerbOcean resource between
the noun judges in the Text and the verb assessed in
the Hypothesis. The reason is that the noun judges
is mistaken for an inflected form of the verb to judge
which indeed stands in a Stronger-than relationship
to to assess.
</bodyText>
<sectionHeader confidence="0.998953" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9999417">
This paper proposed the use of multi-level alignments,
a rich data structure allowing multiple alignments to
co-exist. We argued that multi-level alignments are a
suitable basis for developing Textual Entailment algo-
rithms by virtue of providing a beneficial abstraction
layer that supports extensible and modular entailment
algorithms. A pilot TE algorithm developed in this
schema showed performance comparable to much
more sophisticated state-of-the-art open-source TE
engines and is available as open source software.
</bodyText>
<sectionHeader confidence="0.998334" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.970064">
This work was partially supported by the EC-funded
project EXCITEMENT (FP7ICT-287923).
</bodyText>
<page confidence="0.992193">
197
</page>
<reference confidence="0.982246943396226">
References Meeting of the Association for Computational Linguis-
Colin Bannard and Chris Callison-Burch. 2005. Para- tics, pages 905–912, Sydney, Australia.
phrasing with bilingual parallel corpora. In Proceed- Lili Kotlerman, Ido Dagan, Bernardo Magnini, and Luisa
ings of the 43rd Annual Meeting of the Association for Bentivogli. 2015. Textual entailment graphs. Natural
Computational Linguistics, pages 597–604, Ann Arbor, Language Engineering.
MI. Milen Kouylekov and Matteo Negri. 2010. An open-
Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. source package for recognizing textual entailment. In
Learning entailment relations by global graph structure Proceedings of the ACL 2010 System Demonstrations,
optimization. Computational Linguistics, 38(1):73– pages 42–47, Uppsala, Sweden.
111. Bill MacCartney, Trond Grenager, Marie-Catherine
Ido Dagan and Oren Glickman. 2004. Probabilistic tex- de Marneffe, Daniel Cer, and Christopher D Manning.
tual entailment: Generic applied modeling of language 2006. Learning to recognize features of valid textual
variability. In PASCAL workshop on Learning Methods entailments. In Proceedings of the Human Language
for Text Understanding and Mining, Grenoble, France. Technology Conference of the NAACL, pages 41–48,
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas- New York City, USA.
simo Zanzotto. 2013. Recognizing Textual Entailment: Bill MacCartney, Michel Galley, and Christopher D Man-
Models and Applications. Number 17 in Synthesis Lec- ning. 2008. A phrase-based alignment model for nat-
tures on Human Language Technologies. Morgan &amp; ural language inference. In Proceedings of the Con-
Claypool. ference on Empirical Methods in Natural Language
Richard Eckart de Castilho and Iryna Gurevych. 2014. Processing, pages 802–811, Honolulu, Hawaii.
A broad-coverage collection of portable NLP compo- Bernardo Magnini, Roberto Zanoli, Ido Dagan, Kathrin
nents for building shareable analysis pipelines. In Pro- Eichler, G¨unter Neumann, Tae-Gil Noh, Sebastian
ceedings of the Workshop on Open Infrastructures and Pad´o, Asher Stern, and Omer Levy. 2014. The EX-
Analysis Frameworks for HLT at COLING 2014, pages CITEMENT open platform for textual inferences. In
1–11, Dublin, Ireland, August. Proceedings of the ACL 2014 System Demonstrations,
Michael Denkowski and Alon Lavie. 2014. Meteor uni- pages 43–48, Baltimore, MD.
versal: Language specific translation evaluation for any Rodney D Nielsen, Wayne Ward, and James H Martin.
target language. In Proceedings of the Ninth Workshop 2009. Recognizing entailment in intelligent tutoring
on Statistical Machine Translation, pages 376–380, Bal- systems. Journal of Natural Language Engineering,
timore, MD. 15(4):479–501.
Georgiana Dinu and Rui Wang. 2009. Inference rules and Sebastian Pad´o, Tae-Gil Noh, Asher Stern, Rui Wang, and
their application to recognizing textual entailment. In Roberto Zanoli. 2015. Design and realization of a
Proceedings of the 12th Conference of the European modular architecture for textual entailment. Journal of
Chapter of the ACL, pages 211–219, Athens, Greece. Natural Language Engineering, 21(2):167–200.
Kathrin Eichler, Aleksandra Gabryszak, and G¨unter Neu- Asher Stern and Ido Dagan. 2012. BIUTEE: A modular
mann. 2014. An analysis of textual inference in Ger- open-source system for recognizing textual entailment.
man customer emails. In Proceedings of the Third Joint In Proceedings of the ACL 2012 System Demonstra-
Conference on Lexical and Computational Semantics, tions, pages 73–78, Jeju Island, Korea.
pages 69–74, Dublin, Ireland. Rui Wang and Yi Zhang. 2009. Recognizing textual
David Ferrucci and Adam Lally. 2004. UIMA: An ar- relatedness with predicate-argument structures. In Pro-
chitectural approach to unstructured information pro- ceedings of the Conference on Empirical Methods in
cessing in the corporate research environment. Natural Natural Language Processing, pages 784–792, Singa-
Language Engineering, 10(3–4):327–348. pore.
Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Britta Zeller, Jan ˇSnajder, and Sebastian Pad´o. 2013. DE-
Bill Dolan. 2007. The third PASCAL recognising rivBase: Inducing and evaluating a derivational mor-
textual entailment challenge. In Proceedings of the phology resource for German. In Proceedings of ACL,
ACL-PASCAL Workshop on Textual Entailment and pages 1201–1211, Sofia, Bulgaria.
Paraphrasing, pages 1–9, Prague, Czech Republic.
Sanda Harabagiu and Andrew Hickl. 2006. Methods for
using textual entailment in open-domain question an-
swering. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th Annual
198
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.211582">
<title confidence="0.7527382">Multi-Level Alignments As An Extensible Representation for Textual Entailment Algorithms Noh Pad´o Shwartz Dagan Nastase Eichler</title>
<author confidence="0.79133">Kotlerman</author>
<author confidence="0.79133">Adler</author>
<affiliation confidence="0.94435">of Computational Linguistics, Heidelberg University, of Natural Language Processing, Stuttgart University, of Computer Science, Bar-Ilan University, Language Technologies, Fondazione Bruno Kessler,</affiliation>
<address confidence="0.778273">Technology Lab, DFKI GmbH, Germany</address>
<abstract confidence="0.9988775">A major problem in research on Textual Entailment (TE) is the high implementation effort for TE systems. Recently, interoperable standards for annotation and preprocessing have been proposed. In contrast, the algorithmic level remains unstandardized, which makes component re-use in this area very difficult in prac- In this paper, we introduce a central, powerful representation for TE algorithms that encourages modular, reusable, multilingual algorithm development. We demonstrate that a pilot open-source implementation of multi-level alignment with minimal features competes with state-of-theart open-source TE engines in three languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>References Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Para- phrasing with bilingual parallel corpora.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<booktitle>In Proceed- ings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>38</volume>
<issue>1</issue>
<pages>597--604</pages>
<publisher>Asher</publisher>
<institution>Sebastian Pad´o, Tae-Gil</institution>
<location>Ann</location>
<contexts>
<context position="7997" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1187" endWordPosition="1190"> pre-processing and resources. 1-pre-processing pipeline T H aligner1 aligner2 aligner3 knowledge resource 2-adding alignments multi-level alignment T-H pair enriched with various levels of alignments T H 3-extracting features T-H pair expressed as a data point 4-classifying entailment Entailment Decision T-H with linguistic annotations 194 3 Implementation and Evaluation Paraphrase Aligner. The paraphrase aligner concentrates on surface forms rather than lemmas and can align sequences of them rather than just individual tokens. It uses paraphrase tables, e.g. extracted from parallel corpora (Bannard and Callison-Burch, 2005). The alignment process is similar to the lexical aligner: any two sequences of tokens in T and H are aligned if the pair is listed in the resource. The alignment links created by this aligner instantiate only one relation (“paraphrase”) but report the strength of the relation via the translation probability. We used the paraphrase tables provided by the METEOR MT evaluation package (Denkowski and Lavie, 2014), which are available for numerous languages. Lemma Identity Aligner. This aligner does not use any resources. It simply aligns identical lemmas between T and H and plays an important rol</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>References Colin Bannard and Chris Callison-Burch. 2005. Para- phrasing with bilingual parallel corpora. In Proceed- ings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 597–604, Ann Arbor, MI. Jonathan Berant, Ido Dagan, and Jacob Goldberger. 2012. Learning entailment relations by global graph structure optimization. Computational Linguistics, 38(1):73– 111. Ido Dagan and Oren Glickman. 2004. Probabilistic tex- tual entailment: Generic applied modeling of language variability. In PASCAL workshop on Learning Methods for Text Understanding and Mining, Grenoble, France. Ido Dagan, Dan Roth, Mark Sammons, and Fabio Mas- simo Zanzotto. 2013. Recognizing Textual Entailment: Models and Applications. Number 17 in Synthesis Lec- tures on Human Language Technologies. Morgan &amp; Claypool. Richard Eckart de Castilho and Iryna Gurevych. 2014. A broad-coverage collection of portable NLP compo- nents for building shareable analysis pipelines. In Pro- ceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT at COLING 2014, pages 1–11, Dublin, Ireland, August. Michael Denkowski and Alon Lavie. 2014. Meteor uni- versal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376–380, Bal- timore, MD. Georgiana Dinu and Rui Wang. 2009. Inference rules and their application to recognizing textual entailment. In Proceedings of the 12th Conference of the European Chapter of the ACL, pages 211–219, Athens, Greece. Kathrin Eichler, Aleksandra Gabryszak, and G¨unter Neu- mann. 2014. An analysis of textual inference in Ger- man customer emails. In Proceedings of the Third Joint Conference on Lexical and Computational Semantics, pages 69–74, Dublin, Ireland. David Ferrucci and Adam Lally. 2004. UIMA: An ar- chitectural approach to unstructured information pro- cessing in the corporate research environment. Natural Language Engineering, 10(3–4):327–348. Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. 2007. The third PASCAL recognising textual entailment challenge. In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Paraphrasing, pages 1–9, Prague, Czech Republic. Sanda Harabagiu and Andrew Hickl. 2006. Methods for using textual entailment in open-domain question an- swering. In Proceedings of the 21st International Con- ference on Computational Linguistics and 44th Annual 198 Meeting of the Association for Computational Linguis- tics, pages 905–912, Sydney, Australia. Lili Kotlerman, Ido Dagan, Bernardo Magnini, and Luisa Bentivogli. 2015. Textual entailment graphs. Natural Language Engineering. Milen Kouylekov and Matteo Negri. 2010. An open- source package for recognizing textual entailment. In Proceedings of the ACL 2010 System Demonstrations, pages 42–47, Uppsala, Sweden. Bill MacCartney, Trond Grenager, Marie-Catherine de Marneffe, Daniel Cer, and Christopher D Manning. 2006. Learning to recognize features of valid textual entailments. In Proceedings of the Human Language Technology Conference of the NAACL, pages 41–48, New York City, USA. Bill MacCartney, Michel Galley, and Christopher D Man- ning. 2008. A phrase-based alignment model for nat- ural language inference. In Proceedings of the Con- ference on Empirical Methods in Natural Language Processing, pages 802–811, Honolulu, Hawaii. Bernardo Magnini, Roberto Zanoli, Ido Dagan, Kathrin Eichler, G¨unter Neumann, Tae-Gil Noh, Sebastian Pad´o, Asher Stern, and Omer Levy. 2014. The EX- CITEMENT open platform for textual inferences. In Proceedings of the ACL 2014 System Demonstrations, pages 43–48, Baltimore, MD. Rodney D Nielsen, Wayne Ward, and James H Martin. 2009. Recognizing entailment in intelligent tutoring systems. Journal of Natural Language Engineering, 15(4):479–501. Sebastian Pad´o, Tae-Gil Noh, Asher Stern, Rui Wang, and Roberto Zanoli. 2015. Design and realization of a modular architecture for textual entailment. Journal of Natural Language Engineering, 21(2):167–200. Asher Stern and Ido Dagan. 2012. BIUTEE: A modular open-source system for recognizing textual entailment. In Proceedings of the ACL 2012 System Demonstra- tions, pages 73–78, Jeju Island, Korea. Rui Wang and Yi Zhang. 2009. Recognizing textual relatedness with predicate-argument structures. In Pro- ceedings of the Conference on Empirical Methods in Natural Language Processing, pages 784–792, Singa- pore. Britta Zeller, Jan ˇSnajder, and Sebastian Pad´o. 2013. DE- rivBase: Inducing and evaluating a derivational mor- phology resource for German. In Proceedings of ACL, pages 1201–1211, Sofia, Bulgaria.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>