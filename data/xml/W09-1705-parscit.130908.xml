<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000889">
<title confidence="0.9958715">
Graph Connectivity Measures for Unsupervised Parameter Tuning
of Graph-Based Sense Induction Systems
</title>
<author confidence="0.986969">
Ioannis Korkontzelos, Ioannis Klapaftis and Suresh Manandhar
</author>
<affiliation confidence="0.999572">
Department of Computer Science
The University of York
</affiliation>
<address confidence="0.925756">
Heslington, York, YO10 5NG, UK
</address>
<email confidence="0.979633">
{johnkork, giannis, suresh}@cs.york.ac.uk
</email>
<sectionHeader confidence="0.996272" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996946653846154">
Word Sense Induction (WSI) is the task of
identifying the different senses (uses) of a tar-
get word in a given text. This paper focuses
on the unsupervised estimation of the free pa-
rameters of a graph-based WSI method, and
explores the use of eight Graph Connectiv-
ity Measures (GCM) that assess the degree of
connectivity in a graph. Given a target word
and a set of parameters, GCM evaluate the
connectivity of the produced clusters, which
correspond to subgraphs of the initial (unclus-
tered) graph. Each parameter setting is as-
signed a score according to one of the GCM
and the highest scoring setting is then selected.
Our evaluation on the nouns of SemEval-2007
WSI task (SWSI) shows that: (1) all GCM es-
timate a set of parameters which significantly
outperform the worst performing parameter
setting in both SWSI evaluation schemes, (2)
all GCM estimate a set of parameters which
outperform the Most Frequent Sense (MFS)
baseline by a statistically significant amount
in the supervised evaluation scheme, and (3)
two of the measures estimate a set of parame-
ters that performs closely to a set of parame-
ters estimated in supervised manner.
</bodyText>
<sectionHeader confidence="0.99916" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993381095238095">
Using word senses instead of word forms is essential
in many applications such as information retrieval
(IR) and machine translation (MT) (Pantel and Lin,
2002). Word senses are a prerequisite for word sense
disambiguation (WSD) algorithms. However, they
are usually represented as a fixed-list of definitions
of a manually constructed lexical database. The
36
fixed-list of senses paradigm has several disadvan-
tages. Firstly, lexical databases often contain general
definitions and miss many domain specific senses
(Agirre et al., 2001). Secondly, they suffer from the
lack of explicit semantic and topical relations be-
tween concepts (Agirre et al., 2001). Thirdly, they
often do not reflect the exact content of the context
in which the target word appears (Veronis, 2004).
WSI aims to overcome these limitations of hand-
constructed lexicons.
Most WSI systems are based on the vector-space
model that represents each context of a target word
as a vector of features (e.g. frequency of cooccur-
ring words). Vectors are clustered and the resulting
clusters are taken to represent the induced senses.
Recently, graph-based methods have been employed
to WSI (Dorow and Widdows, 2003; Veronis, 2004;
Agirre and Soroa, 2007b).
Typically, graph-based approaches represent each
word co-occurring with the target word, within a
pre-specified window, as a vertex. Two vertices
are connected via an edge if they co-occur in one
or more contexts of the target word. This co-
occurrence graph is then clustered employing differ-
ent graph clustering algorithms to induce the senses.
Each cluster (induced sense) consists of words ex-
pected to be topically related to the particular sense.
As a result, graph-based approaches assume that
each context word is related to one and only one
sense of the target one.
Recently, Klapaftis and Manandhar (2008) argued
that this assumption might not be always valid, since
a context word may be related to more than one
senses of the target one. As a result, they pro-
</bodyText>
<note confidence="0.9895585">
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 36–44,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999953983606558">
posed the use of a graph-based model for WSI, in
which each vertex of the graph corresponds to a
collocation (word-pair) that co-occurs with the tar-
get word, while edges are drawn based on the co-
occurrence frequency of their associated colloca-
tions. Clustering of this collocational graph would
produce clusters, which consist of a set of collo-
cations. The intuition is that the produced clusters
will be less sense-conflating than those produced
by other graph-based approaches, since collocations
provide strong and consistent clues to the senses of
a target word (Yarowsky, 1995).
The collocational graph-based approach as well
as the majority of state-of-the-art WSI systems es-
timate their parameters either empirically or by em-
ploying supervised techniques. The SemEval-2007
WSI task (SWSI) participating systems UOY and
UBC-AS used labeled data for parameter estimation
(Agirre and Soroa, 2007a), while the authors of I2R,
UPV SI and UMND2 have empirically chosen val-
ues for their parameters. This issue imposes limits
on the unsupervised nature of these algorithms, as
well as on their performance on different datasets.
More specifically, when applying an unsupervised
WSI system on different datasets, one cannot be sure
that the same set of parameters is appropriate for all
datasets (Karakos et al., 2007). In most cases, a new
parameter tuning might be necessary. Unsupervised
estimation of free parameters may enhance the unsu-
pervised nature of systems, making them applicable
to any dataset, even if there are no tagged data avail-
able.
In this paper, we focus on estimating the free
parameters of the collocational graph-based WSI
method (Klapaftis and Manandhar, 2008) using
eight graph connectivity measures (GCM). Given a
parameter setting and the associated induced cluster-
ing solution, each induced cluster corresponds to a
subgraph of the original unclustered graph. A graph
connectivity measure GCMZ scores each cluster by
evaluating the degree of connectivity of its corre-
sponding subgraph. Each clustering solution is then
assigned the average of the scores of its clusters. Fi-
nally, the highest scoring solution is selected.
Our evaluation on the nouns of SWSI shows
that GCM improve the worst performing parame-
ter setting by large margins in both SWSI evaluation
schemes, although they are below the best perform-
ing parameter setting. Moreover, the evaluation in
a WSD setting shows that all GCM estimate a set
of parameters which are above the Most Frequent
Sense (MFS) baseline by a statistically significant
amount. Finally our results show that two of the
measures, i.e. average degree and weighted average
degree, estimate a set of parameters that performs
closely to a set of parameters estimated in a super-
vised manner. All of these findings, suggest that
GCM are able to identify useful differences regard-
ing the quality of the induced clusters for different
parameter combinations, in effect being useful for
unsupervised parameter estimation.
</bodyText>
<sectionHeader confidence="0.925638" genericHeader="method">
2 Collocational graphs for WSI
</sectionHeader>
<bodyText confidence="0.935557433333333">
Let bc, be the base corpus, which consists of para-
graphs containing the target word tw. The aim is
to induce the senses of tw given bc as the only in-
put. Let rc be a large reference corpus. In Klapaftis
and Manandhar (2008) the British National Corpus1
is used as a reference corpus. The WSI algorithm
consists of the following stages.
Corpus pre-processing The target of this stage is
to filter the paragraphs of the base corpus, in order to
keep the words which are topically (and possibly se-
mantically) related to the target one. Initially, tw is
removed from bc and both bc and rc are PoS-tagged.
In the next step, only nouns are kept in the para-
graphs of bc, since they are characterised by higher
discriminative ability than verbs, adverbs or adjec-
tives which may appear in a variety of different con-
texts. At the end of this pre-processing step, each
paragraph of bc and rc is a list of lemmatized nouns
(Klapaftis and Manandhar, 2008).
In the next step, the paragraphs of bc are fil-
tered by removing common nouns which are noisy;
contextually not related to tw. Given a contex-
tual word cw that occurs in the paragraphs of bc, a
log-likelihood ratio (G2) test is employed (Dunning,
1993), which checks if the distribution of cw in bc
is similar to the distribution of cw in rc; p(cwIbc) =
p(cwIrc) (null hypothesis). If this is true, G2 has a
small value. If this value is less than a pre-specified
threshold (parameter p1) the noun is removed from
bc.
</bodyText>
<footnote confidence="0.999263">
1The British National Corpus (BNC) (2001, version 2). Dis-
tributed by Oxford University Computing Services.
</footnote>
<page confidence="0.998064">
37
</page>
<table confidence="0.994963888888889">
Target: cnn nbc Target: nbc news
nbc tv nbc tv
cnn tv soap opera
cnn radio nbc show
news newscast news newscast
radio television nbc newshour
cnn headline cnn headline
nbc politics radio tv
breaking news breaking news
</table>
<tableCaption confidence="0.999979">
Table 1: Collocations connected to cnn nbc and nbc news
</tableCaption>
<bodyText confidence="0.979078071428571">
This process identifies nouns that are more indica-
tive in bc than in rc and vice versa. However, in this
setting we are not interested in nouns which have
a distinctive frequency in rc. As a result, each cw
which has a relative frequency in bc less than in rc
is filtered out. At the end of this stage, each para-
graph of bc is a list of nouns which are assumed to
be contextually related to the target word tw.
Creating the initial collocational graph The tar-
get of this stage is to determine the related nouns,
which will form the collocations, and the weight of
each collocation. Klapaftis and Manandhar (2008)
consider collocations of size 2, i.e. pairs of nouns.
For each paragraph of bc of size n, collocations
are identified by generating all the possible (cn )
2
combinations. The frequency of a collocation c is
the number of paragraphs in the whole SWSI corpus
(27132 paragraphs), in which c occurs.
Each collocation is assigned a weight, measuring
the relative frequency of two nouns co-occurring.
Let freqij denote the number of paragraphs in
which nouns i and j cooccur, and freqj denote the
number of paragraphs, where noun j occurs. The
conditional probability p(i|j) is defined in equation
1, and p(j|i) is computed in a similar way. The
weight of collocation cij is the average of these con-
ditional probabilities wcij = p(i|j) + p(j|i).
</bodyText>
<equation confidence="0.919283">
p(i|j) = freqij (1)
freqj
</equation>
<bodyText confidence="0.996831407407407">
Finally, Klapaftis and Manandhar (2008) only ex-
tract collocations which have frequency (parame-
ter p2) and weight (parameter p3) higher than pre-
specified thresholds. This filtering appears to com-
pensate for inaccuracies in G2, as well as for low-
frequency distant collocations that are ambiguous.
Each weighted collocation is represented as a ver-
tex. Two vertices share an edge, if they co-occur in
one or more paragraphs of bc.
Populating and weighing the collocational graph
The constructed graph, G, is sparse, since the pre-
vious stage attempted to identify rare events, i.e.
co-occurring collocations. To address this problem,
Klapaftis and Manandhar (2008) apply a smooth-
ing technique, similar to the one in Cimiano et
al. (2005), extending the principle that a word is
characterised by the company it keeps (Firth, 1957)
to collocations. The target is to discover new edges
between vertices and to assign weights to all edges.
Each vertex i (collocation ci) is associated to
a vector V Ci containing its neighbouring vertices
(collocations). Table 1 shows an example of two
vertices, cnn nbc and nbc news, which are discon-
nected in G of the target word network. The example
was taken from Klapaftis and Manandhar (2008).
In the next step, the similarity between all vertex
vectors VCi and V Cj is calculated using the Jaccard
</bodyText>
<equation confidence="0.9650725">
V CinV Ccoefficient, i.e. JC(V Ci, V Cj) = jV C
uV C; j . Two
</equation>
<bodyText confidence="0.998501192307692">
collocations ci and cj are mutually similar if ci is the
most similar collocation to cj and vice versa.
Given that collocations ci and cj are mutually
similar, an occurrence of a collocation ck with one
of ci, cj is also counted as an occurrence with the
other collocation. For example in Table 1, if cnn nbc
and nbc news are mutually similar, then the zero-
frequency event between nbc news and cnn tv is
set equal to the joint frequency between cnn nbc
and cnn tv. Marginal frequencies of collocations
are updated and the overall result is consequently a
smoothing of relative frequencies.
The weight applied to each edge connecting ver-
tices i and j (collocations ci and cj ) is the maximum
of their conditional probabilities: p(i|j) = freqij
freqj ,
where freqi is the number of paragraphs collocation
ci occurs. p(j|i) is defined similarly.
Inducing senses and tagging In this final stage,
the collocational graph is clustered to produced the
senses (clusters) of the target word. The clustering
method employed is Chinese Whispers (CW) (Bie-
mann, 2006). CW is linear to the number of graph
edges, while it offers the advantage that it does not
require any input parameters, producing the clusters
of a graph automatically.
</bodyText>
<page confidence="0.998372">
38
</page>
<figureCaption confidence="0.999614">
Figure 1: An example undirected weighted graph.
</figureCaption>
<bodyText confidence="0.99994062962963">
Initially, CW assigns all vertices to different
classes. Each vertex i is processed for a number of
iterations and inherits the strongest class in its lo-
cal neighbourhood (LN) in an update step. LN is
defined as the set of vertices which share an edge
with i. In each iteration for vertex i: each class, cl,
receives a score equal to the sum of the weights of
edges (i, j), where j has been assigned to class cl.
The maximum score determines the strongest class.
In case of multiple strongest classes, one is chosen
randomly. Classes are updated immediately, mean-
ing that a vertex can inherit from its LN classes that
were introduced in the same iteration.
Once CW has produced the clusters of a target
word, each of the instances of tw is tagged with
one of the induced clusters. This process is simi-
lar to Word Sense Disambiguation (WSD) with the
difference that the sense repository has been auto-
matically produced. Particularly, given an instance
of tw in paragraph pi: each induced cluster cl is as-
signed a score equal to the number of its collocations
(i.e. pairs of words) occurring in pi. We observe that
the tagging method exploits the one sense per collo-
cation property (Yarowsky, 1995), which means that
WSD based on collocations is probably finer than
WSD based on simple words, since ambiguity is re-
duced (Klapaftis and Manandhar, 2008).
</bodyText>
<sectionHeader confidence="0.892979" genericHeader="method">
3 Unsupervised parameter tuning
</sectionHeader>
<bodyText confidence="0.999222416666667">
In this section we investigate unsupervised ways to
address the issue of choosing parameter values. To
this end, we employ a variety of GCM, which mea-
sure the relative importance of each vertex and as-
sess the overall connectivity of the corresponding
graph. These measures are average degree, cluster
coefficient, graph entropy and edge density (Navigli
and Lapata, 2007; Zesch and Gurevych, 2007).
GCM quantify the degree of connectivity of the
produced clusters (subgraphs), which represent the
senses (uses) of the target word for a given cluster-
ing solution (parameter setting). Higher values of
GCM indicate subgraphs (clusters) of higher con-
nectivity. Given a parameter setting, the induced
clustering solution and a graph connectivity measure
GCMi, each induced cluster is assigned the result-
ing score of applying GCMi on the corresponding
subgraph of the initial unclustered graph. Each clus-
tering solution is assigned the average of the scores
of its clusters (table 6), and the highest scoring one
is selected.
For each measure, we have developed two ver-
sions, i.e. one which considers the edge weights in
the subgraph, and a second which does not. In the
following description the terms graph and subgraph
are interchangeable.
Let G = (V, E) be an undirected graph (in-
duced sense), where V is a set of vertices and E =
{(u, v) : u, v E V } a set of edges connecting vertex
pairs. Each edge is weighted by a positive weight,
W : wuv —* [0, oc). Figure 1 shows a small example
to explain the computation of GCM. The graph con-
sists of 8 vertices, |V  |= 8, and 10 edges, |E |= 10.
Edge weights appear on edges, e.g. wab = 14.
Average Degree The degree (deg) of a vertex u is
the number of edges connected to u:
</bodyText>
<equation confidence="0.967327">
deg(u) =|{(u,v) E E : v E V 1 |(2)
</equation>
<bodyText confidence="0.753374">
The average degree (AvgDeg) of a graph can be
computed as:
</bodyText>
<equation confidence="0.998928">
AvgDeg(G(V, E)) =
1
|V  |u∈V
</equation>
<bodyText confidence="0.973214166666667">
The first row of table 2 shows the vertex degrees
of the example graph (figure 1) and AvgDeg(G) =
208 = 2.5.
Edge weights can be integrated into the degree
computation. Let mew be the maximum edge
weight in the graph:
</bodyText>
<equation confidence="0.4683875">
mew= max wuv (4)
(u,v)∈E
</equation>
<bodyText confidence="0.992397">
Average Weighted Degree The weighted de-
gree(w deg) of a vertex is defined as:
</bodyText>
<equation confidence="0.9362618">
deg(u) (3)
1 w deg(u) =|V  |wuv (5)
(
mew
u,v)∈E
</equation>
<page confidence="0.987579">
39
</page>
<table confidence="0.988087352941176">
a b c d e f g h
deg(u) 2 2 3 4 3 3 2 1
wdeg(u) 5 1 5 9 7 3 3 1
4 2 4 4 2 2 4
Tu 1 1 1 1 1 2 1 0
CC(u) 1 1 1 1 1 2 1 0
3 6 3 3
WTu 3 1 1 1 1 3 1 0
4 4 4 2 2 4
wCC(u) 3 1 1 1 1 1 1 0
4 12 24 6 2 4
1 1 3 1 3 3 1 1
p(u) 10 10 20 5 20 20 10 20
en(u) * 100 33 33 41 46 41 41 33 22
1 1 1 9 7 3 3 1
wp(u) 16 20 8 80 80 40 40 80
we(u) * 100 25 22 38 35 31 28 28 8
</table>
<tableCaption confidence="0.9807985">
Table 2: Computations of graph connectivity measures
and relevant quantities on the example graph (figure 1).
</tableCaption>
<bodyText confidence="0.99992475">
Average weighted degree (AvgWDeg), similarly to
AvgDeg, is averaged over all vertices of the graph.
In the graph of figure 1, mew = 1. The second row
of table 2 shows the weighted degrees of all vertices.
</bodyText>
<equation confidence="0.980862">
AvgWDeg(G) = 48
36 &apos; 1.33.
</equation>
<bodyText confidence="0.8338165">
Average Cluster Coefficient The cluster coeffi-
cient (cc) of a vertex, u, is defined as:
</bodyText>
<equation confidence="0.978149166666667">
Tu
cc(u) = (6)
2−1ku(ku − 1)
XTu = X 1 (7)
(u,v)∈E (v,x)∈E
x6=u
</equation>
<bodyText confidence="0.98451025">
Tu is the number of edges between the ku neigh-
bours of u. Obviously ku = deg(u). 2−1ku(ku − 1)
would be the number of edges between the neigh-
bours of u if the graph they define was fully con-
nected. Average cluster coefficient (AvgCC) is aver-
aged over all vertices of the graph.
The computations of Tu and cc(u) on the example
graph are shown in the third and fourth rows of table
</bodyText>
<sectionHeader confidence="0.437158" genericHeader="method">
2. Consequently, AvgCC(G) = 916 = 0.5625.
</sectionHeader>
<bodyText confidence="0.99848425">
Average Weighted Cluster Coefficient Let WTu
be the sum of edge weights between the neighbours
of u over mew. Weighted cluster coefficient (wcc)
can be computed as:
</bodyText>
<equation confidence="0.977097">
WTu
wcc(u) = (8)
2−1ku(ku − 1)
wvx (9)
</equation>
<bodyText confidence="0.983489153846154">
Average weighted cluster coefficient (AvgWCC) is
averaged over all vertices of the graph. The com-
putations of WTu and wcc(u) on the example graph
(figure 1) are shown in the fifth and sixth rows of
table 2 and AvgWCC(G)= 67
8∗24 &apos; 0.349.
Graph Entropy Entropy measures the amount of
information (alternatively the uncertainty) in a ran-
dom variable. For a graph, high entropy indicates
that many vertices are equally important and low en-
tropy that only few vertices are relevant (Navigli and
Lapata, 2007). The entropy (en) of a vertex u can be
defined as:
</bodyText>
<equation confidence="0.999682">
en(u) = −p(u) log2 p(u) (10)
</equation>
<bodyText confidence="0.997255">
The probability of a vertex, p(u), is determined by
the degree distribution:
</bodyText>
<equation confidence="0.998505333333333">
�deg(u) �
p(u) = (11)
2|E |u∈V
</equation>
<bodyText confidence="0.99303975">
Graph entropy (GE) is computed by summing all
vertex entropies and normalising by log2 |V |. The
seventh and eighth row of table 2 show the compu-
tations of p(u) and en(u) on the example graph, re-
spectively. Thus, GE &apos; 0.97.
Weighted Graph Entropy Similarly to previous
graph connectivity measures, the weighted entropy
(wen) of a vertex u is defined as:
</bodyText>
<equation confidence="0.999077">
we(u) = −wp(u) log2 wp(u) (12)
r w deg(u) �
2 ∗ mew ∗ |E |u∈V
</equation>
<bodyText confidence="0.996826222222222">
Weighted graph entropy (GE) is computed by sum-
ming all vertex weighted entropies and normalising
by log2 |V |. The last two rows of table 2 show the
computations of wp(u) and we(u) on the example
graph. Consequently, WGE &apos; 0.73.
Edge Density and Weighted Edge Density Edge
density (ed) quantifies how many edges the graph
has, as a ratio over the number of edges of a fully
connected graph of the same size:
</bodyText>
<equation confidence="0.959923666666667">
~|V  |�
A(V ) = 2 (13)
2
X
(u,v)∈E
X
(v,x)∈E
x6=u
1
W Tu =
mew
where: wp(u) =
</equation>
<page confidence="0.966248">
40
</page>
<bodyText confidence="0.997571">
Edge density (ed) is a global graph connectivity
measure; it refers to the whole graph and not a spe-
cific vertex. Edge density (ed) and weighted edge
density (wed) can be defined as follows:
</bodyText>
<equation confidence="0.997325">
ed(G(V, E)) = |E |(14)
A(V )
1 wed(G(V, E)) = A(V ) E mew (15)
(u,v)∈E
</equation>
<bodyText confidence="0.626715">
In the graph of figure 1: A(V ) = 2(8 ) = 28,
</bodyText>
<equation confidence="0.9646465">
2
ed(G) = 10
28 0.357, E wu,v
mew = 6 and wed(G) =
28 ^ 0.214.
6
</equation>
<bodyText confidence="0.999957714285714">
The use of the aforementioned GCM allows the
estimation of a different parameter setting for each
target word. Table 3 shows the parameters of the col-
locational graph-based WSI system (Klapaftis and
Manandhar, 2008). These parameters affect how the
collocational graph is constructed, and in effect the
quality of the induced clusters.
</bodyText>
<sectionHeader confidence="0.99965" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.998384">
4.1 Experimental setting
</subsectionHeader>
<bodyText confidence="0.99996656">
The collocational WSI approach was evaluated un-
der the framework and corpus of SemEval-2007
WSI task (Agirre and Soroa, 2007a). The corpus
consists of text of the Wall Street Journal corpus,
and is hand-tagged with OntoNotes senses (Hovy et
al., 2006). The evaluation focuses on all 35 nouns of
SWSI. SWSI task employs two evaluation schemes.
In unsupervised evaluation, the results are treated as
clusters of contexts and gold standard (GS) senses
as classes. In a perfect clustering solution, each in-
duced cluster contains the same contexts as one of
the classes (Homogeneity), and each class contains
the same contexts as one of the clusters (Complete-
ness). F-Score is used to assess the overall quality of
clustering. Entropy and purity are also used, com-
plementarily. F-Score is a better measure than en-
tropy or purity, since F-Score measures both homo-
geneity and completeness, while entropy and purity
measure only the former. In the second scheme, su-
pervised evaluation, the training corpus is used to
map the induced clusters to GS senses. The testing
corpus is then used to measure WSD performance
(Table 4, Sup. Recall).
The graph-based collocational WSI method is re-
ferred as Col-Sm (where “Col” stands for the “col-
</bodyText>
<table confidence="0.99920075">
Parameter Range Value
G2 threshold 5, 10, 15 P1 = 5
Collocation frequency 4, 6, 8, 10 P2 = 8
Collocation weight 0.2, 0.3, 0.4 P3 = 0.2
</table>
<tableCaption confidence="0.973335">
Table 3: Parameters ranges and values in Klapaftis and
Manandhar (2008)
</tableCaption>
<bodyText confidence="0.998471638888889">
locational WSI” approach and “Sm” for its ver-
sion using “smoothing”). Col-Bl (where “Bl” stands
for “baseline”) refers to the same system without
smoothing. The parameters of Col-Sm were origi-
nally estimated by cross-validation on the training
set of SWSI. Out of 72 parameter combinations, the
setting with the highest F-Score was chosen and ap-
plied to all 35 nouns of the test set. This is referred
as Col-Sm-org (where “org” stands for “original”) in
Table 4. Table 3 shows all values for each parameter,
and the chosen values, under supervised parameter
estimation2. Col-Bl-org (Table 4) induces senses as
Col-Sm-org does, but without smoothing.
In table 4, Col-Sm-w (respectively Col-Bl-w)
refers to the evaluation of Col-Sm (Col-Bl), follow-
ing the same technique for parameter estimation as
in Klapaftis and Manandhar (2008) for each target
word separately (“w” stands for “word”). Given that
GCM are applied for each target word separately,
these baselines will allow to see the performance of
GCM compared to a supervised setting.
The 1c1inst baseline assigns each instance to a
distinct cluster, while the 1c1w baseline groups all
instances of a target word into a single cluster. 1c1w
is equivalent to MFS in this setting. The fifth column
of table 4 shows the average number of clusters.
The SWSI participant systems UOY and UBC-AS
used labeled data for parameter estimation. The au-
thors of I2R, UPV SI and UMND2 have empirically
chosen values for their parameters.
The next subsection presents the evaluation of
GCM as well as the results of SWSI systems. Ini-
tially, we provide a brief discussion on the differ-
ences between the two evaluation schemes of SWSI
that will allow for a better understanding of GCM
performance.
</bodyText>
<subsectionHeader confidence="0.999802">
4.2 Analysis of results and discussion
</subsectionHeader>
<bodyText confidence="0.9887015">
Evaluation of WSI methods is a difficult task. For
instance, 1c1inst (Table 4) achieves perfect purity
</bodyText>
<footnote confidence="0.9958705">
2CW performed 200 iterations for all experiments, because
it is not guaranteed to converge.
</footnote>
<page confidence="0.995933">
41
</page>
<table confidence="0.999606615384615">
System Unsupervised Evaluation Sup.
FSc. Pur. Ent. # Cl. Recall
Col-Sm-org 78.0 88.6 31.0 5.9 86.4
Col-Bl-org 73.1 89.6 29.0 8.0 85.6
Col-Sm-w 80.9 88.0 32.5 4.3 85.5
Col-Bl-w 78.1 88.3 31.7 5.4 84.3
UBC-AS 80.8 83.6 43.5 1.6 80.7
UPV SI 69.9 87.4 30.9 7.2 82.5
I2R 68.0 88.4 29.7 3.1 86.8
UMND2 67.1 85.8 37.6 1.7 84.5
UOY 65.8 89.8 25.5 11.3 81.6
1c1w-MFS 80.7 82.4 46.3 1 80.9
1c1inst 6.6 100 0 73.1 N/A
</table>
<tableCaption confidence="0.99993">
Table 4: Evaluation of WSI systems and baselines.
</tableCaption>
<bodyText confidence="0.993079970588235">
and entropy. However, F-Score of 1c1inst is low,
because the GS senses are spread among clusters,
decreasing unsupervised recall. Supervised recall of
1c1inst is undefined, because each cluster tags only
one instance. Hence, clusters tagging instances in
the test corpus do not tag any instances in the train
corpus and the mapping cannot be performed. 1c1w
achieves high F-Score due to the dominance of MFS
in the testing corpus. However, its purity, entropy
and supervised recall are much lower than other sys-
tems, because it only induces the dominant sense.
Clustering solutions that achieve high supervised
recall do not necessarily achieve high F-Score,
mainly because F-Score penalises systems for in-
ducing more clusters than the corresponding GS
classes, as 1cl1inst does. Supervised evaluation
seems to be more neutral regarding the number of
clusters, since clusters are mapped into a weighted
vector of senses. Thus, inducing a number of clus-
ters similar to the number of senses is not a require-
ment for good results (Agirre and Soroa, 2007a).
High supervised recall means high purity and en-
tropy, as in I2R, but not vice versa, as in UOY. UOY
produces many clean clusters, however these are un-
reliably mapped to senses due to insufficient train-
ing data. On the contrary, I2R produces a few clean
clusters, which are mapped more reliably.
Comparing the performance of SWSI systems
shows that none performs well in both evaluation
settings, in effect being biased against one of the
schemes. However, this is not the case for the collo-
cational WSI method, which achieves a high perfor-
mance in both evaluation settings.
Table 6 presents the results of applying the graph
</bodyText>
<table confidence="0.9997246">
System Bound Unsupervised Evaluation Sup.
type FSc. Pur. Ent. # Cl. Recall
Col-Sm MaxR 79.3 90.5 26.6 7.0 88.6
Col-Sm MinR 62.9 89.0 26.7 12.7 78.8
Col-Bl MaxR 72.9 91.8 23.2 9.6 88.7
Col-Bl MinR 57.5 89.0 26.4 14.4 76.2
Col-Sm MaxF 83.2 90.0 28.7 4.9 86.6
Col-Sm MinF 43.6 90.2 22.1 17.6 83.7
Col-Bl MaxF 81.1 90.0 28.7 5.3 81.8
Col-Bl MinF 34.1 90.5 20.5 20.4 81.5
</table>
<tableCaption confidence="0.983023">
Table 5: Upper and lower performance bounds for sys-
tems Col-Sm and Col-Bl.
</tableCaption>
<bodyText confidence="0.999498611111111">
connectivity measures of section 3 in order to choose
the parameter values for the collocational WSI sys-
tem, for each word separately. The evaluation is
done both for Col-Sm and Col-Bl that use and ignore
smoothing, respectively.
To evaluate the supervised recall performance
using the graph connectivity measures, we com-
puted both the upper and lower bounds of Col-Sm,
i.e. the best and worst supervised recall, respectively
(MaxR and MinR in table 5). In the former case,
we selected the parameter combination per target
word that performs best (Col-Sm, MaxR in table 5),
which resulted in 88.6% supervised recall (F-Score:
79.3%), while in the latter we selected the worst per-
forming one, which resulted in 78.8% supervised re-
call (F-Score: 62.9%). In table 6 we observe that
the supervised recall of all measures is significantly
lower than the upper bound. However, all measures
perform significantly better than the lower bound
(McNemar’s test, confidence level: 95%); the small-
est difference is 4.9%, in the case of weighted edge
density. The picture is the same for Col-Bl.
In the same vein, we computed both the upper and
lower bounds of Col-Sm in terms of F-Score, 83.2%
and 43.6%, respectively (Col-Sm, MinF and MaxF
in table 5). The performance of the system is lower
than the upper bound, for all GCM. Despite that, we
observe that all measures except edge density and
weighted edge density outperform the lower bound
by large margins.
The comparison of GCM performance against
the lower and upper bounds of Col-Sm and Col-Bl
shows that GCM are able to identify useful differ-
ences regarding the degree of connectivity of in-
duced clusters, and in effect suggest parameter val-
ues that perform significantly better than the worst
</bodyText>
<page confidence="0.995588">
42
</page>
<table confidence="0.999596583333333">
Col-Sm Col-Bl
Unsupervised Evaluation Sup. Unsupervised Evaluation Sup.
FSc Pur. Ent. # Cl. Recall FSc Pur. Ent. # Cl. Recall
Graph Connectivity Measure
Average Degree 79.2 87.2 34.2 3.9 84.8 77.5 31.3 88.4 5.7 83.8
Average Weighted Degree 77.1 87.8 32.0 5.5 84.2 75.1 28.3 89.6 8.5 83.3
Average Cluster Coefficient 72.5 88.8 28.5 9.1 83.9 68.7 24.0 90.9 12.9 83.9
Average Weighted Cluster Coefficient 65.8 88.4 28.0 9.6 84.1 68.9 22.4 91.3 13.9 83.7
Graph Entropy 67.0 89.6 25.9 12.3 83.8 68.5 22.1 91.8 14.4 84.4
Weighted Graph Entropy 72.7 89.4 28.1 9.6 84.1 72.2 23.5 91.2 12.5 84.0
Edge Density 47.8 91.8 19.4 18.4 84.8 42.0 16.9 92.8 21.9 84.1
Weighted Edge Density 53.4 90.2 23.1 15.5 83.7 42.2 17.1 92.7 21.9 83.9
</table>
<tableCaption confidence="0.999725">
Table 6: Unsupervised &amp; supervised evaluation of the collocational WSI approach using graph connectivity measures.
</tableCaption>
<bodyText confidence="0.999718055555556">
case. However, they are all unable to approximate
the upper bound for both evaluation schemes, which
is also the case for the supervised estimation of pa-
rameters per target word (Col-Sm-w and Col-Bl-w).
In Table 6, we also observe that all measures
achieve higher supervised recall scores than the
MFS baseline. The increase is statistically signif-
icant (McNemar’s test, confidence level: 95%) in
all cases. This result shows that irrespective of the
number of clusters produced (low F-Score), GCM
are able to estimate a set of parameters that provides
clean clusters (low entropy), which when mapped to
GS senses improve upon the most frequent heuristic,
unlike the majority of unsupervised WSD systems.
Regarding the comparison between different
GCM, we observe that average degree and weighted
average degree for Col-Sm (Col-Bl) perform
closely to Col-Sm-w (Col-Bl-w) for both evaluation
schemes. This is due to the fact that they produce a
number of clusters similar to Col-Sm-w (Col-Bl-w),
while at the same time their distributions of clusters
over the target words’ instances are also similar.
On the contrary, the remaining GCM tend to pro-
duce larger numbers of clusters compared to both
Col-Sm-w (Col-Bl-w) and the GS, in effect being
penalised by F-Score. As it has already been men-
tioned, supervised recall is less affected by a large
number of clusters, which causes small differences
among GCM.
Determining whether the weighted or unweighted
version of GCM performs better depends on the
GCM itself. Weighted graph entropy performs in all
cases better than the unweighted version. For aver-
age cluster coefficient and edge density, we cannot
extract a safe conclusion. Unweighted average de-
gree performs better than the weighted version.
</bodyText>
<sectionHeader confidence="0.977006" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999914909090909">
In this paper, we explored the use of eight graph con-
nectivity measures for unsupervised estimation of
free parameters of a collocational graph-based WSI
system. Given a parameter setting and the associ-
ated induced clustering solution, each cluster was
scored according to the connectivity degree of its
corresponding subgraph, as assessed by a particular
graph connectivity measure. Each clustering solu-
tion was then assigned the average of its clusters’
scores, and the highest scoring one was selected.
Evaluation on the nouns of SemEval-2007 WSI
task (SWSI) showed that all eight graph connectiv-
ity measures choose parameters for which the corre-
sponding performance of the system is significantly
higher than the lower performance bound, for both
the supervised and unsupervised evaluation scheme.
Moreover, the selected parameters produce results
which outperform the MFS baseline by a statisti-
cally significant amount in the supervised evalua-
tion scheme. The best performing measures, average
degree and weighted average degree, perform com-
parably well to the set of parameters chosen by a
supervised parameter estimation. In general, graph
connectivity measures can quantify significant dif-
ferences regarding the degree of connectivity of in-
duced clusters.
Future work focuses on further exploiting graph
connectivity measures. Graph theoretic literature
proposes a variety of measures capturing graph
properties. Some of these measures might help in
improving WSI performance, while at the same time
keeping graph-based WSI systems totally unsuper-
vised.
</bodyText>
<page confidence="0.999705">
43
</page>
<sectionHeader confidence="0.993879" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915649350649">
Eneko Agirre and Aitor Soroa. 2007a. Semeval-2007
task 02: Evaluating word sense induction and discrim-
ination systems. In Proceedings of the Fourth Interna-
tional Workshop on Semantic Evaluations (SemEval-
2007), pages 7–12, Prague, Czech Republic. Associa-
tion for Computational Linguistics.
Eneko Agirre and Aitor Soroa. 2007b. Ubc-as: A graph
based unsupervised system for induction and classi-
fication. In Proceedings of the Fourth International
Workshop on Semantic Evaluations (SemEval-2007),
pages 346–349, Prague, Czech Republic. Association
for Computational Linguistics.
Eneko Agirre, Olatz Ansa, Eduard Hovy, and David Mar-
tinez. 2001. Enriching wordnet concepts with topic
signatures, Sep.
Chris Biemann. 2006. Chinese whispers - an efficient
graph clustering algorithm and its application to nat-
ural language processing problems. In Proceedings
of TextGraphs: the Second Workshop on Graph Based
Methods for Natural Language Processing, pages 73–
80, New York City, June. Association for Computa-
tional Linguistics.
Philipp Cimiano, Andreas Hotho, and Steffen Staab.
2005. Learning concept hierarchies from text corpora
using formal concept analysis. Journal ofArtificial In-
telligence research, 24:305–339.
Beate Dorow and Dominic Widdows. 2003. Discover-
ing corpusspecific word senses. In Proceedings 10th
conference of the European chapter of the ACL, pages
79–82, Budapest, Hungary.
Ted E. Dunning. 1993. Accurate methods for the statis-
tics of surprise and coincidence. Computational Lin-
guistics, 19(1):61–74.
John R. Firth. 1957. A synopsis of linguistic theory,
1930-1955. Studies in Linguistic Analysis, pages 1–
32.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of the Human Lan-
guage Technology Conference of the NAACL, Com-
panion Volume: Short Papers, pages 57–60, New York
City, USA. Association for Computational Linguistics.
Damianos Karakos, Jason Eisner, Sanjeev Khudanpur,
and Carey Priebe. 2007. Cross-instance tuning of un-
supervised document clustering algorithms. In Human
Language Technologies 2007: The Conference of the
North American Chapter of the Association for Com-
putational Linguistics; Proceedings of the Main Con-
ference, pages 252–259, Rochester, New York, April.
Association for Computational Linguistics.
Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word
sense induction using graphs of collocations. In In
Proceedings of the 18th European Conference on Ar-
tificial Intelligence, (ECAI-2008), Patras, Greece.
R. Navigli and M. Lapata. 2007. Graph connectiv-
ity measures for unsupervised word sense disambigua-
tion. In 20th International Joint Conference on Artifi-
cial Intelligence (IJCAI 2007), pages 1683–1688, Hy-
derabad, India, January.
Patrick Pantel and Dekang Lin. 2002. Discovering
word senses from text. In KDD ’02: Proceedings
of the eighth ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 613–
619, New York, NY, USA. ACM Press.
Jean Veronis. 2004. Hyperlex: lexical cartography for
information retrieval. Computer Speech &amp; Language,
18(3):223–252, July.
David Yarowsky. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Meeting of
the Association for Computational Linguistics, pages
189–196.
Torsten Zesch and Iryna Gurevych. 2007. Analysis of
the wikipedia category graph for NLP applications. In
Proceedings of the Second Workshop on TextGraphs:
Graph-Based Algorithms for Natural Language Pro-
cessing, pages 1–8, Rochester, NY, USA. Association
for Computational Linguistics.
</reference>
<page confidence="0.999293">
44
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.864975">
<title confidence="0.999302">Graph Connectivity Measures for Unsupervised Parameter Tuning of Graph-Based Sense Induction Systems</title>
<author confidence="0.946107">Ioannis Korkontzelos</author>
<author confidence="0.946107">Ioannis Klapaftis</author>
<author confidence="0.946107">Suresh</author>
<affiliation confidence="0.9991505">Department of Computer The University of</affiliation>
<address confidence="0.962461">Heslington, York, YO10 5NG,</address>
<email confidence="0.992597">giannis,</email>
<abstract confidence="0.998354962962963">Word Sense Induction (WSI) is the task of identifying the different senses (uses) of a target word in a given text. This paper focuses on the unsupervised estimation of the free parameters of a graph-based WSI method, and explores the use of eight Graph Connectivity Measures (GCM) that assess the degree of connectivity in a graph. Given a target word and a set of parameters, GCM evaluate the connectivity of the produced clusters, which correspond to subgraphs of the initial (unclustered) graph. Each parameter setting is assigned a score according to one of the GCM and the highest scoring setting is then selected. Our evaluation on the nouns of SemEval-2007 WSI task (SWSI) shows that: (1) all GCM estimate a set of parameters which significantly outperform the worst performing parameter setting in both SWSI evaluation schemes, (2) all GCM estimate a set of parameters which outperform the Most Frequent Sense (MFS) baseline by a statistically significant amount in the supervised evaluation scheme, and (3) two of the measures estimate a set of parameters that performs closely to a set of parameters estimated in supervised manner.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Semeval-2007 task 02: Evaluating word sense induction and discrimination systems.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007),</booktitle>
<pages>7--12</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2673" citStr="Agirre and Soroa, 2007" startWordPosition="415" endWordPosition="418"> and topical relations between concepts (Agirre et al., 2001). Thirdly, they often do not reflect the exact content of the context in which the target word appears (Veronis, 2004). WSI aims to overcome these limitations of handconstructed lexicons. Most WSI systems are based on the vector-space model that represents each context of a target word as a vector of features (e.g. frequency of cooccurring words). Vectors are clustered and the resulting clusters are taken to represent the induced senses. Recently, graph-based methods have been employed to WSI (Dorow and Widdows, 2003; Veronis, 2004; Agirre and Soroa, 2007b). Typically, graph-based approaches represent each word co-occurring with the target word, within a pre-specified window, as a vertex. Two vertices are connected via an edge if they co-occur in one or more contexts of the target word. This cooccurrence graph is then clustered employing different graph clustering algorithms to induce the senses. Each cluster (induced sense) consists of words expected to be topically related to the particular sense. As a result, graph-based approaches assume that each context word is related to one and only one sense of the target one. Recently, Klapaftis and </context>
<context position="4546" citStr="Agirre and Soroa, 2007" startWordPosition="707" endWordPosition="710">ional graph would produce clusters, which consist of a set of collocations. The intuition is that the produced clusters will be less sense-conflating than those produced by other graph-based approaches, since collocations provide strong and consistent clues to the senses of a target word (Yarowsky, 1995). The collocational graph-based approach as well as the majority of state-of-the-art WSI systems estimate their parameters either empirically or by employing supervised techniques. The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. This issue imposes limits on the unsupervised nature of these algorithms, as well as on their performance on different datasets. More specifically, when applying an unsupervised WSI system on different datasets, one cannot be sure that the same set of parameters is appropriate for all datasets (Karakos et al., 2007). In most cases, a new parameter tuning might be necessary. Unsupervised estimation of free parameters may enhance the unsupervised nature of systems, making them applicable to any da</context>
<context position="20085" citStr="Agirre and Soroa, 2007" startWordPosition="3503" endWordPosition="3506">(15) (u,v)∈E In the graph of figure 1: A(V ) = 2(8 ) = 28, 2 ed(G) = 10 28 0.357, E wu,v mew = 6 and wed(G) = 28 ^ 0.214. 6 The use of the aforementioned GCM allows the estimation of a different parameter setting for each target word. Table 3 shows the parameters of the collocational graph-based WSI system (Klapaftis and Manandhar, 2008). These parameters affect how the collocational graph is constructed, and in effect the quality of the induced clusters. 4 Evaluation 4.1 Experimental setting The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). The corpus consists of text of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses (Hovy et al., 2006). The evaluation focuses on all 35 nouns of SWSI. SWSI task employs two evaluation schemes. In unsupervised evaluation, the results are treated as clusters of contexts and gold standard (GS) senses as classes. In a perfect clustering solution, each induced cluster contains the same contexts as one of the classes (Homogeneity), and each class contains the same contexts as one of the clusters (Completeness). F-Score is used to assess the overall quality of clustering. En</context>
<context position="24872" citStr="Agirre and Soroa, 2007" startWordPosition="4296" endWordPosition="4299">s. However, its purity, entropy and supervised recall are much lower than other systems, because it only induces the dominant sense. Clustering solutions that achieve high supervised recall do not necessarily achieve high F-Score, mainly because F-Score penalises systems for inducing more clusters than the corresponding GS classes, as 1cl1inst does. Supervised evaluation seems to be more neutral regarding the number of clusters, since clusters are mapped into a weighted vector of senses. Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY. UOY produces many clean clusters, however these are unreliably mapped to senses due to insufficient training data. On the contrary, I2R produces a few clean clusters, which are mapped more reliably. Comparing the performance of SWSI systems shows that none performs well in both evaluation settings, in effect being biased against one of the schemes. However, this is not the case for the collocational WSI method, which achieves a high performance in both evaluation settings. Table 6 presents the r</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007a. Semeval-2007 task 02: Evaluating word sense induction and discrimination systems. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval2007), pages 7–12, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Ubc-as: A graph based unsupervised system for induction and classification.</title>
<date>2007</date>
<booktitle>In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>346--349</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="2673" citStr="Agirre and Soroa, 2007" startWordPosition="415" endWordPosition="418"> and topical relations between concepts (Agirre et al., 2001). Thirdly, they often do not reflect the exact content of the context in which the target word appears (Veronis, 2004). WSI aims to overcome these limitations of handconstructed lexicons. Most WSI systems are based on the vector-space model that represents each context of a target word as a vector of features (e.g. frequency of cooccurring words). Vectors are clustered and the resulting clusters are taken to represent the induced senses. Recently, graph-based methods have been employed to WSI (Dorow and Widdows, 2003; Veronis, 2004; Agirre and Soroa, 2007b). Typically, graph-based approaches represent each word co-occurring with the target word, within a pre-specified window, as a vertex. Two vertices are connected via an edge if they co-occur in one or more contexts of the target word. This cooccurrence graph is then clustered employing different graph clustering algorithms to induce the senses. Each cluster (induced sense) consists of words expected to be topically related to the particular sense. As a result, graph-based approaches assume that each context word is related to one and only one sense of the target one. Recently, Klapaftis and </context>
<context position="4546" citStr="Agirre and Soroa, 2007" startWordPosition="707" endWordPosition="710">ional graph would produce clusters, which consist of a set of collocations. The intuition is that the produced clusters will be less sense-conflating than those produced by other graph-based approaches, since collocations provide strong and consistent clues to the senses of a target word (Yarowsky, 1995). The collocational graph-based approach as well as the majority of state-of-the-art WSI systems estimate their parameters either empirically or by employing supervised techniques. The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. This issue imposes limits on the unsupervised nature of these algorithms, as well as on their performance on different datasets. More specifically, when applying an unsupervised WSI system on different datasets, one cannot be sure that the same set of parameters is appropriate for all datasets (Karakos et al., 2007). In most cases, a new parameter tuning might be necessary. Unsupervised estimation of free parameters may enhance the unsupervised nature of systems, making them applicable to any da</context>
<context position="20085" citStr="Agirre and Soroa, 2007" startWordPosition="3503" endWordPosition="3506">(15) (u,v)∈E In the graph of figure 1: A(V ) = 2(8 ) = 28, 2 ed(G) = 10 28 0.357, E wu,v mew = 6 and wed(G) = 28 ^ 0.214. 6 The use of the aforementioned GCM allows the estimation of a different parameter setting for each target word. Table 3 shows the parameters of the collocational graph-based WSI system (Klapaftis and Manandhar, 2008). These parameters affect how the collocational graph is constructed, and in effect the quality of the induced clusters. 4 Evaluation 4.1 Experimental setting The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). The corpus consists of text of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses (Hovy et al., 2006). The evaluation focuses on all 35 nouns of SWSI. SWSI task employs two evaluation schemes. In unsupervised evaluation, the results are treated as clusters of contexts and gold standard (GS) senses as classes. In a perfect clustering solution, each induced cluster contains the same contexts as one of the classes (Homogeneity), and each class contains the same contexts as one of the clusters (Completeness). F-Score is used to assess the overall quality of clustering. En</context>
<context position="24872" citStr="Agirre and Soroa, 2007" startWordPosition="4296" endWordPosition="4299">s. However, its purity, entropy and supervised recall are much lower than other systems, because it only induces the dominant sense. Clustering solutions that achieve high supervised recall do not necessarily achieve high F-Score, mainly because F-Score penalises systems for inducing more clusters than the corresponding GS classes, as 1cl1inst does. Supervised evaluation seems to be more neutral regarding the number of clusters, since clusters are mapped into a weighted vector of senses. Thus, inducing a number of clusters similar to the number of senses is not a requirement for good results (Agirre and Soroa, 2007a). High supervised recall means high purity and entropy, as in I2R, but not vice versa, as in UOY. UOY produces many clean clusters, however these are unreliably mapped to senses due to insufficient training data. On the contrary, I2R produces a few clean clusters, which are mapped more reliably. Comparing the performance of SWSI systems shows that none performs well in both evaluation settings, in effect being biased against one of the schemes. However, this is not the case for the collocational WSI method, which achieves a high performance in both evaluation settings. Table 6 presents the r</context>
</contexts>
<marker>Agirre, Soroa, 2007</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2007b. Ubc-as: A graph based unsupervised system for induction and classification. In Proceedings of the Fourth International Workshop on Semantic Evaluations (SemEval-2007), pages 346–349, Prague, Czech Republic. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Olatz Ansa</author>
<author>Eduard Hovy</author>
<author>David Martinez</author>
</authors>
<title>Enriching wordnet concepts with topic signatures,</title>
<date>2001</date>
<contexts>
<context position="1993" citStr="Agirre et al., 2001" startWordPosition="306" endWordPosition="309">orms closely to a set of parameters estimated in supervised manner. 1 Introduction Using word senses instead of word forms is essential in many applications such as information retrieval (IR) and machine translation (MT) (Pantel and Lin, 2002). Word senses are a prerequisite for word sense disambiguation (WSD) algorithms. However, they are usually represented as a fixed-list of definitions of a manually constructed lexical database. The 36 fixed-list of senses paradigm has several disadvantages. Firstly, lexical databases often contain general definitions and miss many domain specific senses (Agirre et al., 2001). Secondly, they suffer from the lack of explicit semantic and topical relations between concepts (Agirre et al., 2001). Thirdly, they often do not reflect the exact content of the context in which the target word appears (Veronis, 2004). WSI aims to overcome these limitations of handconstructed lexicons. Most WSI systems are based on the vector-space model that represents each context of a target word as a vector of features (e.g. frequency of cooccurring words). Vectors are clustered and the resulting clusters are taken to represent the induced senses. Recently, graph-based methods have been</context>
</contexts>
<marker>Agirre, Ansa, Hovy, Martinez, 2001</marker>
<rawString>Eneko Agirre, Olatz Ansa, Eduard Hovy, and David Martinez. 2001. Enriching wordnet concepts with topic signatures, Sep.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems.</title>
<date>2006</date>
<booktitle>In Proceedings of TextGraphs: the Second Workshop on Graph Based Methods for Natural Language Processing,</booktitle>
<pages>73--80</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City,</location>
<contexts>
<context position="12325" citStr="Biemann, 2006" startWordPosition="2026" endWordPosition="2028">ween cnn nbc and cnn tv. Marginal frequencies of collocations are updated and the overall result is consequently a smoothing of relative frequencies. The weight applied to each edge connecting vertices i and j (collocations ci and cj ) is the maximum of their conditional probabilities: p(i|j) = freqij freqj , where freqi is the number of paragraphs collocation ci occurs. p(j|i) is defined similarly. Inducing senses and tagging In this final stage, the collocational graph is clustered to produced the senses (clusters) of the target word. The clustering method employed is Chinese Whispers (CW) (Biemann, 2006). CW is linear to the number of graph edges, while it offers the advantage that it does not require any input parameters, producing the clusters of a graph automatically. 38 Figure 1: An example undirected weighted graph. Initially, CW assigns all vertices to different classes. Each vertex i is processed for a number of iterations and inherits the strongest class in its local neighbourhood (LN) in an update step. LN is defined as the set of vertices which share an edge with i. In each iteration for vertex i: each class, cl, receives a score equal to the sum of the weights of edges (i, j), wher</context>
</contexts>
<marker>Biemann, 2006</marker>
<rawString>Chris Biemann. 2006. Chinese whispers - an efficient graph clustering algorithm and its application to natural language processing problems. In Proceedings of TextGraphs: the Second Workshop on Graph Based Methods for Natural Language Processing, pages 73– 80, New York City, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Cimiano</author>
<author>Andreas Hotho</author>
<author>Steffen Staab</author>
</authors>
<title>Learning concept hierarchies from text corpora using formal concept analysis.</title>
<date>2005</date>
<journal>Journal ofArtificial Intelligence research,</journal>
<pages>24--305</pages>
<contexts>
<context position="10611" citStr="Cimiano et al. (2005)" startWordPosition="1727" endWordPosition="1730">t (parameter p3) higher than prespecified thresholds. This filtering appears to compensate for inaccuracies in G2, as well as for lowfrequency distant collocations that are ambiguous. Each weighted collocation is represented as a vertex. Two vertices share an edge, if they co-occur in one or more paragraphs of bc. Populating and weighing the collocational graph The constructed graph, G, is sparse, since the previous stage attempted to identify rare events, i.e. co-occurring collocations. To address this problem, Klapaftis and Manandhar (2008) apply a smoothing technique, similar to the one in Cimiano et al. (2005), extending the principle that a word is characterised by the company it keeps (Firth, 1957) to collocations. The target is to discover new edges between vertices and to assign weights to all edges. Each vertex i (collocation ci) is associated to a vector V Ci containing its neighbouring vertices (collocations). Table 1 shows an example of two vertices, cnn nbc and nbc news, which are disconnected in G of the target word network. The example was taken from Klapaftis and Manandhar (2008). In the next step, the similarity between all vertex vectors VCi and V Cj is calculated using the Jaccard V </context>
</contexts>
<marker>Cimiano, Hotho, Staab, 2005</marker>
<rawString>Philipp Cimiano, Andreas Hotho, and Steffen Staab. 2005. Learning concept hierarchies from text corpora using formal concept analysis. Journal ofArtificial Intelligence research, 24:305–339.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beate Dorow</author>
<author>Dominic Widdows</author>
</authors>
<title>Discovering corpusspecific word senses.</title>
<date>2003</date>
<booktitle>In Proceedings 10th conference of the European chapter of the ACL,</booktitle>
<pages>79--82</pages>
<location>Budapest, Hungary.</location>
<contexts>
<context position="2634" citStr="Dorow and Widdows, 2003" startWordPosition="409" endWordPosition="412">uffer from the lack of explicit semantic and topical relations between concepts (Agirre et al., 2001). Thirdly, they often do not reflect the exact content of the context in which the target word appears (Veronis, 2004). WSI aims to overcome these limitations of handconstructed lexicons. Most WSI systems are based on the vector-space model that represents each context of a target word as a vector of features (e.g. frequency of cooccurring words). Vectors are clustered and the resulting clusters are taken to represent the induced senses. Recently, graph-based methods have been employed to WSI (Dorow and Widdows, 2003; Veronis, 2004; Agirre and Soroa, 2007b). Typically, graph-based approaches represent each word co-occurring with the target word, within a pre-specified window, as a vertex. Two vertices are connected via an edge if they co-occur in one or more contexts of the target word. This cooccurrence graph is then clustered employing different graph clustering algorithms to induce the senses. Each cluster (induced sense) consists of words expected to be topically related to the particular sense. As a result, graph-based approaches assume that each context word is related to one and only one sense of t</context>
</contexts>
<marker>Dorow, Widdows, 2003</marker>
<rawString>Beate Dorow and Dominic Widdows. 2003. Discovering corpusspecific word senses. In Proceedings 10th conference of the European chapter of the ACL, pages 79–82, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted E Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<contexts>
<context position="7850" citStr="Dunning, 1993" startWordPosition="1257" endWordPosition="1258">d rc are PoS-tagged. In the next step, only nouns are kept in the paragraphs of bc, since they are characterised by higher discriminative ability than verbs, adverbs or adjectives which may appear in a variety of different contexts. At the end of this pre-processing step, each paragraph of bc and rc is a list of lemmatized nouns (Klapaftis and Manandhar, 2008). In the next step, the paragraphs of bc are filtered by removing common nouns which are noisy; contextually not related to tw. Given a contextual word cw that occurs in the paragraphs of bc, a log-likelihood ratio (G2) test is employed (Dunning, 1993), which checks if the distribution of cw in bc is similar to the distribution of cw in rc; p(cwIbc) = p(cwIrc) (null hypothesis). If this is true, G2 has a small value. If this value is less than a pre-specified threshold (parameter p1) the noun is removed from bc. 1The British National Corpus (BNC) (2001, version 2). Distributed by Oxford University Computing Services. 37 Target: cnn nbc Target: nbc news nbc tv nbc tv cnn tv soap opera cnn radio nbc show news newscast news newscast radio television nbc newshour cnn headline cnn headline nbc politics radio tv breaking news breaking news Table </context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted E. Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19(1):61–74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John R Firth</author>
</authors>
<title>A synopsis of linguistic theory,</title>
<date>1957</date>
<booktitle>Studies in Linguistic Analysis,</booktitle>
<pages>1930--1955</pages>
<contexts>
<context position="10703" citStr="Firth, 1957" startWordPosition="1744" endWordPosition="1745">racies in G2, as well as for lowfrequency distant collocations that are ambiguous. Each weighted collocation is represented as a vertex. Two vertices share an edge, if they co-occur in one or more paragraphs of bc. Populating and weighing the collocational graph The constructed graph, G, is sparse, since the previous stage attempted to identify rare events, i.e. co-occurring collocations. To address this problem, Klapaftis and Manandhar (2008) apply a smoothing technique, similar to the one in Cimiano et al. (2005), extending the principle that a word is characterised by the company it keeps (Firth, 1957) to collocations. The target is to discover new edges between vertices and to assign weights to all edges. Each vertex i (collocation ci) is associated to a vector V Ci containing its neighbouring vertices (collocations). Table 1 shows an example of two vertices, cnn nbc and nbc news, which are disconnected in G of the target word network. The example was taken from Klapaftis and Manandhar (2008). In the next step, the similarity between all vertex vectors VCi and V Cj is calculated using the Jaccard V CinV Ccoefficient, i.e. JC(V Ci, V Cj) = jV C uV C; j . Two collocations ci and cj are mutua</context>
</contexts>
<marker>Firth, 1957</marker>
<rawString>John R. Firth. 1957. A synopsis of linguistic theory, 1930-1955. Studies in Linguistic Analysis, pages 1– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers,</booktitle>
<pages>57--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>New York City, USA.</location>
<contexts>
<context position="20212" citStr="Hovy et al., 2006" startWordPosition="3524" endWordPosition="3527"> of the aforementioned GCM allows the estimation of a different parameter setting for each target word. Table 3 shows the parameters of the collocational graph-based WSI system (Klapaftis and Manandhar, 2008). These parameters affect how the collocational graph is constructed, and in effect the quality of the induced clusters. 4 Evaluation 4.1 Experimental setting The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). The corpus consists of text of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses (Hovy et al., 2006). The evaluation focuses on all 35 nouns of SWSI. SWSI task employs two evaluation schemes. In unsupervised evaluation, the results are treated as clusters of contexts and gold standard (GS) senses as classes. In a perfect clustering solution, each induced cluster contains the same contexts as one of the classes (Homogeneity), and each class contains the same contexts as one of the clusters (Completeness). F-Score is used to assess the overall quality of clustering. Entropy and purity are also used, complementarily. F-Score is a better measure than entropy or purity, since F-Score measures bot</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of the Human Language Technology Conference of the NAACL, Companion Volume: Short Papers, pages 57–60, New York City, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damianos Karakos</author>
<author>Jason Eisner</author>
<author>Sanjeev Khudanpur</author>
<author>Carey Priebe</author>
</authors>
<title>Cross-instance tuning of unsupervised document clustering algorithms.</title>
<date>2007</date>
<booktitle>In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,</booktitle>
<pages>252--259</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, New York,</location>
<contexts>
<context position="4963" citStr="Karakos et al., 2007" startWordPosition="774" endWordPosition="777">arameters either empirically or by employing supervised techniques. The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. This issue imposes limits on the unsupervised nature of these algorithms, as well as on their performance on different datasets. More specifically, when applying an unsupervised WSI system on different datasets, one cannot be sure that the same set of parameters is appropriate for all datasets (Karakos et al., 2007). In most cases, a new parameter tuning might be necessary. Unsupervised estimation of free parameters may enhance the unsupervised nature of systems, making them applicable to any dataset, even if there are no tagged data available. In this paper, we focus on estimating the free parameters of the collocational graph-based WSI method (Klapaftis and Manandhar, 2008) using eight graph connectivity measures (GCM). Given a parameter setting and the associated induced clustering solution, each induced cluster corresponds to a subgraph of the original unclustered graph. A graph connectivity measure </context>
</contexts>
<marker>Karakos, Eisner, Khudanpur, Priebe, 2007</marker>
<rawString>Damianos Karakos, Jason Eisner, Sanjeev Khudanpur, and Carey Priebe. 2007. Cross-instance tuning of unsupervised document clustering algorithms. In Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 252–259, Rochester, New York, April. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis P Klapaftis</author>
<author>Suresh Manandhar</author>
</authors>
<title>Word sense induction using graphs of collocations. In</title>
<date>2008</date>
<booktitle>In Proceedings of the 18th European Conference on Artificial Intelligence,</booktitle>
<location>(ECAI-2008), Patras, Greece.</location>
<contexts>
<context position="3289" citStr="Klapaftis and Manandhar (2008)" startWordPosition="513" endWordPosition="516">nd Soroa, 2007b). Typically, graph-based approaches represent each word co-occurring with the target word, within a pre-specified window, as a vertex. Two vertices are connected via an edge if they co-occur in one or more contexts of the target word. This cooccurrence graph is then clustered employing different graph clustering algorithms to induce the senses. Each cluster (induced sense) consists of words expected to be topically related to the particular sense. As a result, graph-based approaches assume that each context word is related to one and only one sense of the target one. Recently, Klapaftis and Manandhar (2008) argued that this assumption might not be always valid, since a context word may be related to more than one senses of the target one. As a result, they proProceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 36–44, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics posed the use of a graph-based model for WSI, in which each vertex of the graph corresponds to a collocation (word-pair) that co-occurs with the target word, while edges are drawn based on the cooccurrence frequency of their associated colloc</context>
<context position="5330" citStr="Klapaftis and Manandhar, 2008" startWordPosition="832" endWordPosition="835">f these algorithms, as well as on their performance on different datasets. More specifically, when applying an unsupervised WSI system on different datasets, one cannot be sure that the same set of parameters is appropriate for all datasets (Karakos et al., 2007). In most cases, a new parameter tuning might be necessary. Unsupervised estimation of free parameters may enhance the unsupervised nature of systems, making them applicable to any dataset, even if there are no tagged data available. In this paper, we focus on estimating the free parameters of the collocational graph-based WSI method (Klapaftis and Manandhar, 2008) using eight graph connectivity measures (GCM). Given a parameter setting and the associated induced clustering solution, each induced cluster corresponds to a subgraph of the original unclustered graph. A graph connectivity measure GCMZ scores each cluster by evaluating the degree of connectivity of its corresponding subgraph. Each clustering solution is then assigned the average of the scores of its clusters. Finally, the highest scoring solution is selected. Our evaluation on the nouns of SWSI shows that GCM improve the worst performing parameter setting by large margins in both SWSI evalua</context>
<context position="6879" citStr="Klapaftis and Manandhar (2008)" startWordPosition="1084" endWordPosition="1087">erage degree and weighted average degree, estimate a set of parameters that performs closely to a set of parameters estimated in a supervised manner. All of these findings, suggest that GCM are able to identify useful differences regarding the quality of the induced clusters for different parameter combinations, in effect being useful for unsupervised parameter estimation. 2 Collocational graphs for WSI Let bc, be the base corpus, which consists of paragraphs containing the target word tw. The aim is to induce the senses of tw given bc as the only input. Let rc be a large reference corpus. In Klapaftis and Manandhar (2008) the British National Corpus1 is used as a reference corpus. The WSI algorithm consists of the following stages. Corpus pre-processing The target of this stage is to filter the paragraphs of the base corpus, in order to keep the words which are topically (and possibly semantically) related to the target one. Initially, tw is removed from bc and both bc and rc are PoS-tagged. In the next step, only nouns are kept in the paragraphs of bc, since they are characterised by higher discriminative ability than verbs, adverbs or adjectives which may appear in a variety of different contexts. At the end</context>
<context position="9112" citStr="Klapaftis and Manandhar (2008)" startWordPosition="1481" endWordPosition="1484"> nbc and nbc news This process identifies nouns that are more indicative in bc than in rc and vice versa. However, in this setting we are not interested in nouns which have a distinctive frequency in rc. As a result, each cw which has a relative frequency in bc less than in rc is filtered out. At the end of this stage, each paragraph of bc is a list of nouns which are assumed to be contextually related to the target word tw. Creating the initial collocational graph The target of this stage is to determine the related nouns, which will form the collocations, and the weight of each collocation. Klapaftis and Manandhar (2008) consider collocations of size 2, i.e. pairs of nouns. For each paragraph of bc of size n, collocations are identified by generating all the possible (cn ) 2 combinations. The frequency of a collocation c is the number of paragraphs in the whole SWSI corpus (27132 paragraphs), in which c occurs. Each collocation is assigned a weight, measuring the relative frequency of two nouns co-occurring. Let freqij denote the number of paragraphs in which nouns i and j cooccur, and freqj denote the number of paragraphs, where noun j occurs. The conditional probability p(i|j) is defined in equation 1, and </context>
<context position="10538" citStr="Klapaftis and Manandhar (2008)" startWordPosition="1713" endWordPosition="1716">har (2008) only extract collocations which have frequency (parameter p2) and weight (parameter p3) higher than prespecified thresholds. This filtering appears to compensate for inaccuracies in G2, as well as for lowfrequency distant collocations that are ambiguous. Each weighted collocation is represented as a vertex. Two vertices share an edge, if they co-occur in one or more paragraphs of bc. Populating and weighing the collocational graph The constructed graph, G, is sparse, since the previous stage attempted to identify rare events, i.e. co-occurring collocations. To address this problem, Klapaftis and Manandhar (2008) apply a smoothing technique, similar to the one in Cimiano et al. (2005), extending the principle that a word is characterised by the company it keeps (Firth, 1957) to collocations. The target is to discover new edges between vertices and to assign weights to all edges. Each vertex i (collocation ci) is associated to a vector V Ci containing its neighbouring vertices (collocations). Table 1 shows an example of two vertices, cnn nbc and nbc news, which are disconnected in G of the target word network. The example was taken from Klapaftis and Manandhar (2008). In the next step, the similarity b</context>
<context position="13896" citStr="Klapaftis and Manandhar, 2008" startWordPosition="2298" endWordPosition="2301">e instances of tw is tagged with one of the induced clusters. This process is similar to Word Sense Disambiguation (WSD) with the difference that the sense repository has been automatically produced. Particularly, given an instance of tw in paragraph pi: each induced cluster cl is assigned a score equal to the number of its collocations (i.e. pairs of words) occurring in pi. We observe that the tagging method exploits the one sense per collocation property (Yarowsky, 1995), which means that WSD based on collocations is probably finer than WSD based on simple words, since ambiguity is reduced (Klapaftis and Manandhar, 2008). 3 Unsupervised parameter tuning In this section we investigate unsupervised ways to address the issue of choosing parameter values. To this end, we employ a variety of GCM, which measure the relative importance of each vertex and assess the overall connectivity of the corresponding graph. These measures are average degree, cluster coefficient, graph entropy and edge density (Navigli and Lapata, 2007; Zesch and Gurevych, 2007). GCM quantify the degree of connectivity of the produced clusters (subgraphs), which represent the senses (uses) of the target word for a given clustering solution (par</context>
<context position="19802" citStr="Klapaftis and Manandhar, 2008" startWordPosition="3460" endWordPosition="3463">,x)∈E x6=u 1 W Tu = mew where: wp(u) = 40 Edge density (ed) is a global graph connectivity measure; it refers to the whole graph and not a specific vertex. Edge density (ed) and weighted edge density (wed) can be defined as follows: ed(G(V, E)) = |E |(14) A(V ) 1 wed(G(V, E)) = A(V ) E mew (15) (u,v)∈E In the graph of figure 1: A(V ) = 2(8 ) = 28, 2 ed(G) = 10 28 0.357, E wu,v mew = 6 and wed(G) = 28 ^ 0.214. 6 The use of the aforementioned GCM allows the estimation of a different parameter setting for each target word. Table 3 shows the parameters of the collocational graph-based WSI system (Klapaftis and Manandhar, 2008). These parameters affect how the collocational graph is constructed, and in effect the quality of the induced clusters. 4 Evaluation 4.1 Experimental setting The collocational WSI approach was evaluated under the framework and corpus of SemEval-2007 WSI task (Agirre and Soroa, 2007a). The corpus consists of text of the Wall Street Journal corpus, and is hand-tagged with OntoNotes senses (Hovy et al., 2006). The evaluation focuses on all 35 nouns of SWSI. SWSI task employs two evaluation schemes. In unsupervised evaluation, the results are treated as clusters of contexts and gold standard (GS)</context>
<context position="21393" citStr="Klapaftis and Manandhar (2008)" startWordPosition="3723" endWordPosition="3726">han entropy or purity, since F-Score measures both homogeneity and completeness, while entropy and purity measure only the former. In the second scheme, supervised evaluation, the training corpus is used to map the induced clusters to GS senses. The testing corpus is then used to measure WSD performance (Table 4, Sup. Recall). The graph-based collocational WSI method is referred as Col-Sm (where “Col” stands for the “colParameter Range Value G2 threshold 5, 10, 15 P1 = 5 Collocation frequency 4, 6, 8, 10 P2 = 8 Collocation weight 0.2, 0.3, 0.4 P3 = 0.2 Table 3: Parameters ranges and values in Klapaftis and Manandhar (2008) locational WSI” approach and “Sm” for its version using “smoothing”). Col-Bl (where “Bl” stands for “baseline”) refers to the same system without smoothing. The parameters of Col-Sm were originally estimated by cross-validation on the training set of SWSI. Out of 72 parameter combinations, the setting with the highest F-Score was chosen and applied to all 35 nouns of the test set. This is referred as Col-Sm-org (where “org” stands for “original”) in Table 4. Table 3 shows all values for each parameter, and the chosen values, under supervised parameter estimation2. Col-Bl-org (Table 4) induces</context>
</contexts>
<marker>Klapaftis, Manandhar, 2008</marker>
<rawString>Ioannis P. Klapaftis and Suresh Manandhar. 2008. Word sense induction using graphs of collocations. In In Proceedings of the 18th European Conference on Artificial Intelligence, (ECAI-2008), Patras, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>M Lapata</author>
</authors>
<title>Graph connectivity measures for unsupervised word sense disambiguation.</title>
<date>2007</date>
<booktitle>In 20th International Joint Conference on Artificial Intelligence (IJCAI</booktitle>
<pages>1683--1688</pages>
<location>Hyderabad, India,</location>
<contexts>
<context position="14300" citStr="Navigli and Lapata, 2007" startWordPosition="2361" endWordPosition="2364">od exploits the one sense per collocation property (Yarowsky, 1995), which means that WSD based on collocations is probably finer than WSD based on simple words, since ambiguity is reduced (Klapaftis and Manandhar, 2008). 3 Unsupervised parameter tuning In this section we investigate unsupervised ways to address the issue of choosing parameter values. To this end, we employ a variety of GCM, which measure the relative importance of each vertex and assess the overall connectivity of the corresponding graph. These measures are average degree, cluster coefficient, graph entropy and edge density (Navigli and Lapata, 2007; Zesch and Gurevych, 2007). GCM quantify the degree of connectivity of the produced clusters (subgraphs), which represent the senses (uses) of the target word for a given clustering solution (parameter setting). Higher values of GCM indicate subgraphs (clusters) of higher connectivity. Given a parameter setting, the induced clustering solution and a graph connectivity measure GCMi, each induced cluster is assigned the resulting score of applying GCMi on the corresponding subgraph of the initial unclustered graph. Each clustering solution is assigned the average of the scores of its clusters (</context>
<context position="18121" citStr="Navigli and Lapata, 2007" startWordPosition="3134" endWordPosition="3137"> neighbours of u over mew. Weighted cluster coefficient (wcc) can be computed as: WTu wcc(u) = (8) 2−1ku(ku − 1) wvx (9) Average weighted cluster coefficient (AvgWCC) is averaged over all vertices of the graph. The computations of WTu and wcc(u) on the example graph (figure 1) are shown in the fifth and sixth rows of table 2 and AvgWCC(G)= 67 8∗24 &apos; 0.349. Graph Entropy Entropy measures the amount of information (alternatively the uncertainty) in a random variable. For a graph, high entropy indicates that many vertices are equally important and low entropy that only few vertices are relevant (Navigli and Lapata, 2007). The entropy (en) of a vertex u can be defined as: en(u) = −p(u) log2 p(u) (10) The probability of a vertex, p(u), is determined by the degree distribution: �deg(u) � p(u) = (11) 2|E |u∈V Graph entropy (GE) is computed by summing all vertex entropies and normalising by log2 |V |. The seventh and eighth row of table 2 show the computations of p(u) and en(u) on the example graph, respectively. Thus, GE &apos; 0.97. Weighted Graph Entropy Similarly to previous graph connectivity measures, the weighted entropy (wen) of a vertex u is defined as: we(u) = −wp(u) log2 wp(u) (12) r w deg(u) � 2 ∗ mew ∗ |E </context>
</contexts>
<marker>Navigli, Lapata, 2007</marker>
<rawString>R. Navigli and M. Lapata. 2007. Graph connectivity measures for unsupervised word sense disambiguation. In 20th International Joint Conference on Artificial Intelligence (IJCAI 2007), pages 1683–1688, Hyderabad, India, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Dekang Lin</author>
</authors>
<title>Discovering word senses from text.</title>
<date>2002</date>
<booktitle>In KDD ’02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>613--619</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="1616" citStr="Pantel and Lin, 2002" startWordPosition="252" endWordPosition="255">all GCM estimate a set of parameters which significantly outperform the worst performing parameter setting in both SWSI evaluation schemes, (2) all GCM estimate a set of parameters which outperform the Most Frequent Sense (MFS) baseline by a statistically significant amount in the supervised evaluation scheme, and (3) two of the measures estimate a set of parameters that performs closely to a set of parameters estimated in supervised manner. 1 Introduction Using word senses instead of word forms is essential in many applications such as information retrieval (IR) and machine translation (MT) (Pantel and Lin, 2002). Word senses are a prerequisite for word sense disambiguation (WSD) algorithms. However, they are usually represented as a fixed-list of definitions of a manually constructed lexical database. The 36 fixed-list of senses paradigm has several disadvantages. Firstly, lexical databases often contain general definitions and miss many domain specific senses (Agirre et al., 2001). Secondly, they suffer from the lack of explicit semantic and topical relations between concepts (Agirre et al., 2001). Thirdly, they often do not reflect the exact content of the context in which the target word appears (</context>
</contexts>
<marker>Pantel, Lin, 2002</marker>
<rawString>Patrick Pantel and Dekang Lin. 2002. Discovering word senses from text. In KDD ’02: Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 613– 619, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Veronis</author>
</authors>
<title>Hyperlex: lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<contexts>
<context position="2230" citStr="Veronis, 2004" startWordPosition="347" endWordPosition="348">. Word senses are a prerequisite for word sense disambiguation (WSD) algorithms. However, they are usually represented as a fixed-list of definitions of a manually constructed lexical database. The 36 fixed-list of senses paradigm has several disadvantages. Firstly, lexical databases often contain general definitions and miss many domain specific senses (Agirre et al., 2001). Secondly, they suffer from the lack of explicit semantic and topical relations between concepts (Agirre et al., 2001). Thirdly, they often do not reflect the exact content of the context in which the target word appears (Veronis, 2004). WSI aims to overcome these limitations of handconstructed lexicons. Most WSI systems are based on the vector-space model that represents each context of a target word as a vector of features (e.g. frequency of cooccurring words). Vectors are clustered and the resulting clusters are taken to represent the induced senses. Recently, graph-based methods have been employed to WSI (Dorow and Widdows, 2003; Veronis, 2004; Agirre and Soroa, 2007b). Typically, graph-based approaches represent each word co-occurring with the target word, within a pre-specified window, as a vertex. Two vertices are con</context>
</contexts>
<marker>Veronis, 2004</marker>
<rawString>Jean Veronis. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech &amp; Language, 18(3):223–252, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="4229" citStr="Yarowsky, 1995" startWordPosition="664" endWordPosition="665">on for Computational Linguistics posed the use of a graph-based model for WSI, in which each vertex of the graph corresponds to a collocation (word-pair) that co-occurs with the target word, while edges are drawn based on the cooccurrence frequency of their associated collocations. Clustering of this collocational graph would produce clusters, which consist of a set of collocations. The intuition is that the produced clusters will be less sense-conflating than those produced by other graph-based approaches, since collocations provide strong and consistent clues to the senses of a target word (Yarowsky, 1995). The collocational graph-based approach as well as the majority of state-of-the-art WSI systems estimate their parameters either empirically or by employing supervised techniques. The SemEval-2007 WSI task (SWSI) participating systems UOY and UBC-AS used labeled data for parameter estimation (Agirre and Soroa, 2007a), while the authors of I2R, UPV SI and UMND2 have empirically chosen values for their parameters. This issue imposes limits on the unsupervised nature of these algorithms, as well as on their performance on different datasets. More specifically, when applying an unsupervised WSI s</context>
<context position="13743" citStr="Yarowsky, 1995" startWordPosition="2275" endWordPosition="2276">can inherit from its LN classes that were introduced in the same iteration. Once CW has produced the clusters of a target word, each of the instances of tw is tagged with one of the induced clusters. This process is similar to Word Sense Disambiguation (WSD) with the difference that the sense repository has been automatically produced. Particularly, given an instance of tw in paragraph pi: each induced cluster cl is assigned a score equal to the number of its collocations (i.e. pairs of words) occurring in pi. We observe that the tagging method exploits the one sense per collocation property (Yarowsky, 1995), which means that WSD based on collocations is probably finer than WSD based on simple words, since ambiguity is reduced (Klapaftis and Manandhar, 2008). 3 Unsupervised parameter tuning In this section we investigate unsupervised ways to address the issue of choosing parameter values. To this end, we employ a variety of GCM, which measure the relative importance of each vertex and assess the overall connectivity of the corresponding graph. These measures are average degree, cluster coefficient, graph entropy and edge density (Navigli and Lapata, 2007; Zesch and Gurevych, 2007). GCM quantify t</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Meeting of the Association for Computational Linguistics, pages 189–196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torsten Zesch</author>
<author>Iryna Gurevych</author>
</authors>
<title>Analysis of the wikipedia category graph for NLP applications.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second Workshop on TextGraphs: Graph-Based Algorithms for Natural Language Processing,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Rochester, NY, USA.</location>
<contexts>
<context position="14327" citStr="Zesch and Gurevych, 2007" startWordPosition="2365" endWordPosition="2368">per collocation property (Yarowsky, 1995), which means that WSD based on collocations is probably finer than WSD based on simple words, since ambiguity is reduced (Klapaftis and Manandhar, 2008). 3 Unsupervised parameter tuning In this section we investigate unsupervised ways to address the issue of choosing parameter values. To this end, we employ a variety of GCM, which measure the relative importance of each vertex and assess the overall connectivity of the corresponding graph. These measures are average degree, cluster coefficient, graph entropy and edge density (Navigli and Lapata, 2007; Zesch and Gurevych, 2007). GCM quantify the degree of connectivity of the produced clusters (subgraphs), which represent the senses (uses) of the target word for a given clustering solution (parameter setting). Higher values of GCM indicate subgraphs (clusters) of higher connectivity. Given a parameter setting, the induced clustering solution and a graph connectivity measure GCMi, each induced cluster is assigned the resulting score of applying GCMi on the corresponding subgraph of the initial unclustered graph. Each clustering solution is assigned the average of the scores of its clusters (table 6), and the highest s</context>
</contexts>
<marker>Zesch, Gurevych, 2007</marker>
<rawString>Torsten Zesch and Iryna Gurevych. 2007. Analysis of the wikipedia category graph for NLP applications. In Proceedings of the Second Workshop on TextGraphs: Graph-Based Algorithms for Natural Language Processing, pages 1–8, Rochester, NY, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>