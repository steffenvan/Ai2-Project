<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000857">
<title confidence="0.8956705">
On the use of automatic tools for large scale semantic analyses of causal
connectives
</title>
<author confidence="0.688938">
Liesbeth Degand
</author>
<affiliation confidence="0.656429666666667">
Université catholique de Lou-
vain
Institute of Linguistics
</affiliation>
<email confidence="0.900139">
degand@lige.ucl.ac.be
</email>
<author confidence="0.790509">
Wilbert Spooren
</author>
<affiliation confidence="0.538419">
Vrije Universiteit, Amsterdam
Language &amp; Communication
</affiliation>
<email confidence="0.949899">
w.spooren@let.vu.nl
</email>
<author confidence="0.420207">
Yves Bestgen
</author>
<bodyText confidence="0.953116">
Université catholique de Lou-
vain
Faculty of Psychology
Yves.bestgen@psp.ucl.
ac.be
</bodyText>
<sectionHeader confidence="0.967105" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9928783">
In this paper we describe the (annotation) tools underly-
ing two automatic techniques to analyse the meaning
and use of backward causal connectives in large Dutch
newspaper corpora. With the help of these techniques,
Latent Semantic Analysis and Thematic Text Analysis,
the contexts of more than 14,000 connectives were stud-
ied. We will focus here on the methods of analysis and
on the fairly straightforward (annotation) tools needed
to perform the semantic analyses, i.e. POS-tagging, lem-
matisation and a thesaurus-like thematic dictionary.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999860849056604">
In ongoing work, we explore the possibility to
make use of large corpora to test hypotheses con-
cerning linguistic factors determining the meaning
and use of connectives. Of course, corpus-based
approaches of connectives are not new, but classi-
cally they consist of either fully analysed but rela-
tively small corpora, or of large corpora of which a
random set is analysed. The reason for this quanti-
tative restriction is clear: The data-analyses are
completely hand-based. While these empirical
studies are useful from a qualitative point of view,
they all suffer from the same quantitative draw-
back, namely the relatively small number of data
(rarely more than 100 occurrences are analysed,
mostly only 50). In addition, most of these analy-
ses are still too analyst-dependent, making gener-
alizations and replications difficult. Changing this
situation includes handling exhaustively large cor-
pora (with hundreds and even thousands of occur-
rences of the same linguistic phenomenon) and
implementing the analytic procedures to make
them analyst-independent. In this paper, we test
such a methodology for which we used a number
of linguistic hypotheses found in the literature on
the semantics of causal connectives and tried to
replicate the results. The linguistic material we
worked on are four Dutch backward causal con-
nectives: aangezien (&apos;since&apos;), doordat (&apos;because of
the fact that&apos;), omdat (&apos;because&apos;) and, want (&apos;be-
cause&apos;). This choice was motivated by the fact that
there has already been quite some linguistic work
on this topic, mainly empirically based (Degand,
2001; Degand and Pander Maat, 2003; Pit, 2003).1
We have shown elsewhere how linguistic hypothe-
ses concerning the scaling of these connectives in
terms of subjectivity and their thematic behaviour
could be supported (Bestgen et al., 2003). Since
these first results are very encouraging, we would
like to focus here on the methods of automatic
analysis – Latent Semantic Analysis and Thematic
Text Analysis - and on the fairly straightforward
(annotation) tools needed to perform the semantic
analyses, i.e. POS-tagging, lemmatisation and a
thesaurus-like thematic dictionary. We illustrate
how the combination of the two techniques of
automatic analysis permit to gain deeper insight
into the semantic constraints on the use of the con-
nectives studied. Doing so, we test a number of
new hypotheses concerning the perspectivizing and
polyphonic nature of connectives that remain un-
confirmed in the linguistic literature. We also dis-
cuss the robustness of the techniques and their re-
usability in other contexts and other languages.
</bodyText>
<footnote confidence="0.902612">
1 For lack of space we will not present the linguistic analyses here but will
consider them as given.
</footnote>
<sectionHeader confidence="0.462794" genericHeader="introduction">
2 Techniques and Tools
</sectionHeader>
<bodyText confidence="0.999916454545454">
The techniques used have to fulfil two tasks: they
are needed to extract the relevant linguistic mate-
rial from the corpus, that is to say the four connec-
tives with their context of use; and they are used to
analyse the retrieved elements in order to test a
number of linguistic hypotheses concerning the
meaning and use of these connectives. Our main
objective is to show that with the use of these tech-
niques only fairly straightforward annotation tools
are needed to perform quite profound semantic
analyses on massive quantitative data.
</bodyText>
<subsectionHeader confidence="0.9708795">
2.1 POS-Tagging and the identification of
the causal segments
</subsectionHeader>
<bodyText confidence="0.999983866666667">
The extraction of the relevant linguistic material
was fulfilled by automatic syntactic analysis tech-
niques. As a basis for our analyses we worked with
the first six months of a Dutch newspaper corpus
of more than 30 million words2. This material was
POS-tagged using MBT (Memory Based Tagger)
(Daelemans et al.,1996). We then discarded the
items with few content words: sports results, tele-
vision programs, crosswords and puzzles, stock
exchange reports, service information from the
newspaper editor, etc. We also ‘cleaned’ the cor-
pus material of irregularities caused by the incom-
patibility between the source file and the tagging
program (mostly nonsense words generated by the
program). This eventually led to a data set of ap-
proximately 16,500,000 words.
The POS-tagging permitted to segment the cor-
pus in sentences and to label the words grammati-
cally. Second, POS-tagging allowed us to locate
and extract the connectives from the sentences in
which they occurred. Concretely, we extracted all
sentence-length segments on the basis of the tag
&lt;UT&gt; (‘utterance’). We then did a search on the
four connectives tagged as &lt;conj&gt; by the parser.
Table 1 displays the frequencies of the retrieved
connectives. These figures do not include a number
of sentences that were eliminated because they
were potentially problematic for the analysis. This
was for instance the case for sentences containing
more than one connective out of our list of four.3
</bodyText>
<table confidence="0.998248166666667">
Connective Raw frequency Relative frequency
(per million words)
aangezien 248 30
Doordat 826 101
Omdat 7689 938
Want 5621 686
</table>
<tableCaption confidence="0.9475845">
Table 1: Frequencies of the causal connectives in the
data set
</tableCaption>
<bodyText confidence="0.919942">
The extracted sentences were then analysed in
terms of a series of heuristics to identify the CAUSE
(P) and CONSEQUENCE segments (Q)4. From a
syntactic point of view, the connectives doordat,
omdat and aangezien can occur in two basic types
of causal constructions: medial (Q CONNECTIVE P),
see example (1), and preposed ones (CONNECTIVE
P, Q), see example (2). The connective want only
appears in medial constructions.
</bodyText>
<listItem confidence="0.987425315789474">
(1) Een gezamenlijk beleid is
nodig omdat in het najaar
in het Japanse Kyoto
wereldwijd wordt onderhan-
deld over het klimaat.
‘A common policy is necessary
because worldwide negotia-
tions will take place in the
autumn in the Japanese city
of Kyoto.’
(2) Iedere strenge winter
heeft gevolgen voor de
kerkorgels&apos;, zegt dr. A.J.
Gierveld van de Gerefor-
meerde Organisten-
vereniging. Doordat het
hout krimpt, kunnen er
kieren ontstaan waardoor
lucht ontsnapt.
</listItem>
<bodyText confidence="0.9935262">
‘Every hard winter has conse-
quences for the church or-
gans”, Dr. A.J. Gierveld of
the Reformed Organists Union
says. Because the wood
shrinks, crocks may show,
through which air escapes.’
The heuristics to identify the CAUSE (P) and
CONSEQUENCE (Q) segments were primarily based
on
</bodyText>
<footnote confidence="0.977591285714286">
2 We used the year 1997 of &amp;quot;De Volkskrant&amp;quot; a Dutch national daily newspaper.
The corpus is distributed on CD-rom.
2 These cases were eliminated in order to be sure of the exact influence of the
connective and about the exact contribution of the context.
4 The connectives under investigation are all so-called backward causal connec-
tives, i.e. they express an underlying causal relation of the type CONSEQUENCE –
CAUSE, in which the connective introduces the CAUSE segment.
</footnote>
<listItem confidence="0.970010285714286">
a) the position of the connective in the
sentence (number and type of words
preceding the connective),
b) the number, position and order of finite
verbs in the segment,
c) the presence or absence of punctuation
markers, especially commas.
</listItem>
<bodyText confidence="0.978391346153846">
For example, a sentence beginning with the con-
nective omdat can either be preposed (P-Q) (ex-
ample 3), or medial (Q-P), if Q and P are given in
different sentences (example 4).
(3) Omdat de verdachte niet
eerder was veroordeeld,
bleef de gevangenisstraf
geheel voorwaardelijk.
‘Because the suspect had not
been convicted before, the
sentence was entirely proba-
tional.’
(4) Maar er zijn meer pro-
gramma&apos;s die de moeite
waard zijn en die toch
niet worden bekeken. Omdat
ze onvindbaar zijn tussen
de ramsj.
‘But there are more [TV] pro-
grammes that are worth watch-
ing and still are not being
watched. Because they are
hard to trace among the rub-
bish.’
To extract these segments correctly, a number of
rules enter into play. For example,
</bodyText>
<listItem confidence="0.954284">
a) If CONN = omdat, doordat or aangezien; and
b) If CONN in initial position, look for first fi-
nite verb [vf], if vf appears in segment
&lt;...vf, vf ...&gt; or &lt;... vf vf ...&gt;, then cut be-
fore second vf, and segment containing
CONN is P, the other one is Q.
c) If CONN in initial position and there is only
one vf, then segment containing CONN is P,
and previous sentence is Q.
</listItem>
<bodyText confidence="0.896383275">
Other rules are used to determine whether the
CONN is in initial position or not. In addition to
examples (2-3), example (5) also illustrates a case
of initial connective, even though a word precedes
the connective.
(5) En omdat in Nederland de
voertaal nog steeds het
Nederlands is, worden de
meeste schoolvakken ook in
die taal gedoceerd.
’And because Dutch is still
the main language in the
Netherlands, most subjects
are taught in that language.’
This resulted in 21 heuristic rules, the adequacy of
which was hand-checked on large samples of the
data. In the end, 1.4% of the data were lost because
one of the segments was missing or because none
of the procedures could work out the identification
of P and Q. Ultimately we were able to identify
the causal segments for 14181 sentences. Four syn-
tactic environments can be distinguished, involving
a preposed construction &lt;Conn P Q.&gt; as in exam-
ples (2, 3, 5) above, and three types of medial con-
structions:
a) &lt;Q conn P.&gt; corresponds to a construction
in which Q and P are linked by a connective
within the same sentence (example 1);
b) &lt;Q. Conn P.&gt; corresponds to constructions
in which the previous sentence functions as
Q (examples 4); and
c) &lt;Prev. Q conn P.&gt; corresponds to construc-
tions for which the Q-segment is anaphoric
with the preceding sentence, thus requiring
this previous sentence for the semantic in-
terpretation, as in example (6), in which the
Q “dat komt” (litt. ‘that comes’) picks up the
semantic information from the previous sen-
tence and links it to the P-segment intro-
duced by the connective.
</bodyText>
<listItem confidence="0.894336">
(6) De Europese economie
</listItem>
<bodyText confidence="0.9991769">
raakt hopeloos achterop
bij de Amerikaanse en
Japanse. Dat komt door-
dat Europa niet meedoet
op nieuwe groeimarkten.
‘The European economy is
falling hopelessly behind
the American and Japanese
economy. This is because
Europe is not participating
in new growth markets.’
Actually, only 7.1% of the sentences investigated
belong to the preposed construction type. How-
ever, important divergences exist between the con-
nectives: want is never used in preposed position,
omdat in 10.41% of the cases, and doordat in
14.32% of the cases, a figure which rises to 43.5%
of the cases for aangezien. It is interesting to point
out that this is in total agreement with previous
small-scale corpus research on this matter.
</bodyText>
<subsectionHeader confidence="0.8762245">
2.2 Lemmatisation and the construction of
the LSA semantic space
</subsectionHeader>
<bodyText confidence="0.999994340206186">
The first automatic technique that will be presented
is Latent Semantic Analysis (LSA), a mathematical
technique for extracting a very large “semantic
space” from large text corpora on the basis of the
statistical analysis of the set of co-occurrences in a
text corpus. Landauer et al. (1998) stress that this
technique can be viewed from two sides. At a
theoretical level, it is meant to be used to develop
simulations of the cognitive processes running dur-
ing language comprehension, including, for in-
stance, a computational model of metaphor
treatment (Kintsch, 2000 ; Lemaire et al., 2001),
but also to analyse the coherence of texts (Foltz et
al., 1998 ; Piérard et al., 2004). At a more applied
level, it is a technique which enables to infer and to
represent the meaning of words on the basis of
their actual use in text so that the similarity of the
meaning of words, sentences or paragraphs can be
estimated (Bestgen, 2002; Choi et al., 2001). It is
this latter aspect which draws our attention here.
The point of departure of the analysis is a lexical
table (Lebart and Salem, 1992) containing the fre-
quencies of every word in each of the documents
included in the text material, a document being a
text, a paragraph, or a sentence. To derive semantic
relations between words from the lexical table the
analysis of mere co-occurrences will not do, the
major problem being that even in a large corpus
most words are relatively rare. Consequently the
co-occurrences of words are even rarer. This fact
makes such co-occurrences very sensitive to arbi-
trary variations (Burgess et al., 1998 ; Kintsch,
2001). LSA resolves this problem by replacing the
original frequency table by an approximation pro-
ducing a kind of smoothening effect on the asso-
ciations. To this end, the frequency table
undergoes a singular value decomposition and it is
then recomposed on the basis of only a fraction of
the information it contains. Thus, the thousands of
words from the documents have been substituted
by linear combinations or ‘semantic dimensions’
with respect to which the original words can be
situated again. Contrary to a classical factor analy-
sis the extracted dimensions are very numerous
and non-interpretable.
All original words and segments can then be
placed into this semantic space. The meaning of
each word is represented by a vector, thus indicat-
ing the exact location of the word in this multidi-
mensional semantic space. To calculate the
semantic proximity between two words, the cosine
between the two vectors that represent them is cal-
culated. The more two words are semantically
similar, the more their vectors point in the same
direction, and consequently, the closer their cosine
will be to 1 (coinciding vectors). A cosine of 0
shows an absence of similarity, since the corre-
sponding vectors point in orthogonal directions. It
is also possible to calculate the similarity between
‘higher order’ elements, i.e. between sentences,
paragraphs, and entire documents, or combinations
of those, even if this higher order element isn’t by
itself an analysed element. The vector in question
corresponds to the centroid of the words compos-
ing the segment under investigation. The centroid
results from the weighted sum of the vectors of
these words (Deerwester et al., 1990). This makes
it possible to calculate the semantic proximity be-
tween any two sentences, viz. whether present in
the original corpus or not, whether the original
corpus had been segmented in sentence length
documents or not.
To perform the LSA analyses, we used the
Dutch newspaper corpus to build the semantic
space. To this end, the data set, which had been
lemmatised with MBLEM (Memory Based Lem-
matiser) (Van den Bosch &amp; Daelemans, 1999), was
cut into article-length segments. Elimination of all
digits, special characters, punctuation marks, and
of a list of 222 stopwords (words occurring in
“any” context, like determiners, auxiliaries, con-
junctions, ...), brought the total number of words
back to approximately 6.5 million. For the input
lexical table, the documents were articles of mini-
mally 24 words and maximally 523 words, i.e. all
articles minus the 10% shortest and minus the 10%
longest ones. As to the words, we kept all those
that occurred at least ten times in the data set.
Overall this resulted in a matrix of 36630 terms in
28640 documents. To build the semantic space
proper, the singular value decomposition was real-
ized with the program SVDPACKC (Berry, 1992;
Berry et al., 1993), and the 300 first singular vec-
tors were retained. In the present research we will
use this technique to evaluate the semantic prox-
imity between P&amp; Q, and between the causal seg-
ments and the prior or subsequent sentences.
</bodyText>
<subsectionHeader confidence="0.99517">
2.3 Dictionaries and lexical categorisation
</subsectionHeader>
<bodyText confidence="0.999988433962264">
The second technique used to test the linguistic
hypotheses is alternatively called ‘word count
strategy’ (Pennebaker et al., 2003), automatic iden-
tification of linguistic features (Biber, 1988) or
thematic text analysis (Popping, 2000; Stone,
1997), the aim of which is to determine whether
some categories of words (e.g., words of opinion,
fact, attitude, etc.) or some grammatical categories
(e.g. personal pronouns) occur more often in a
given type of text segment. The first step in this
kind of analysis is to build a dictionary that con-
tains the categories to be investigated and the cor-
responding (lemmatised) lexical entries that signal
their occurrence. The categories may correspond to
grammatical classes, but also to thematic word
grouping. The following step consists in searching
all the text segments containing these lexical en-
tries in order to account for the frequency of each
category in each text segment. These data are put
into a matrix that has one row for each text seg-
ment and one column for each category, each cell
containing the frequency of the respective category
in the respective text segment. Finally, this matrix
is analysed to determine whether some categories
occur more often in a given type of text segment.
To illustrate this technique, let us assume that we
want to test the hypothesis that (nominative) per-
sonal pronouns occur more frequent in text seg-
ments connected by want than by the other
backward causal connectives. In the first step the
&amp;quot;Personal-Pronoun&amp;quot; dictionary is built, containing
the corresponding lexical entries: ik, jij, je, hij, zij,
ze, wij, we, jullie, u. All the text segments contain-
ing these lexical entries are then searched in order
to account for the frequency of the concept &amp;quot;Per-
sonal-Pronoun&amp;quot; in each text segment. These data
are put into a matrix which is analysed to deter-
mine whether the concept &amp;quot;Personal-Pronoun&amp;quot; oc-
curs more often with want-segments than with the
other causal segments.
The two main difficulties we are confronted
with when using this technique in the present stud-
ies are (i) the reduced size of the analysed text
segments (one sentence or even less), and (ii) the
difficulty, or even impossibility, to build an ex-
haustive list of words belonging to a category like
fact, opinion, attitude, etc. With respect to the first
difficulty, we believe that the reduced size of the
segments will be compensated by the large number
of segments of each type being analysed. The sec-
ond difficulty is addressed below where we pro-
pose a number of ways to extend the category lists
automatically.
</bodyText>
<sectionHeader confidence="0.9976305" genericHeader="method">
3 Combining LSA and TTA: an applica-
tion
</sectionHeader>
<subsectionHeader confidence="0.999728">
3.1 Perspective shift
</subsectionHeader>
<bodyText confidence="0.98634668">
There are a number of claims in the literature that
some connectives co-occur with perspective shifts
between the causal segments, while others do not.
Perspectivisation accounts for the fact that there
are more sources of information than the speaker
alone. In relation to our connectives, perspectivisa-
tion has been claimed to play a role in the meaning
differences between want (introducing a perspec-
tive shift) and omdat (no perspective shift). How-
ever, the various corpus studies on this matter have
not univocally confirmed this hypothesis (Degand,
2001; Oversteegen, 1997). We would like to ex-
plore this matter further by comparing the semantic
tightness of the segments related by our connec-
tives. This will be done by calculating the seman-
tic proximity between Q and P for each of the
connectives. Our hypotheses are as follows:
Hypothesis 1: The cosine between Q and P re-
lated by monophonic connectives (omdat)
should be higher than the cosine between Q and
P related by polyphonic connectives (want).
Hypothesis 2: The cosine between the prior sen-
tence and the subsequent sentence should be
higher for monophonic connectives than for
polyphonic connectives.
</bodyText>
<tableCaption confidence="0.981685333333333">
Table 2: Mean Cosine per connective between the
causal segments, and between the prior and subsequent
sentences
</tableCaption>
<bodyText confidence="0.993796551724138">
Table 2 displays the cosines resulting from the
LSA-analysis. Two ANOVAs were performed.
The first one had the connectives as independent
variable and the semantic proximity between the
causal segments as dependent variable. It shows
that hypothesis 1 is borne out (F(3, 10505) =
11.36, p &lt; 0.0001): the causal segments related by
the (monophonic) connective omdat are semanti-
cally closer than the segments related by the (poly-
phonic) connective want. The results furthermore
show that doordat and aangezien should be de-
scribed in terms of monophonic connectives. The
second ANOVA, with the connectives as inde-
pendent variable and the semantic proximity be-
tween the prior and subsequent sentences as
dependent variable, confirms hypothesis 2 (F(3,
10505) = 25.75, p &lt; 0.0001): the monophonic con-
nectives aangezien, doordat and omdat go along
with topic continuity (or at least semantic prox-
imity) between the prior and subsequent sentence
to the causal construction, while this is less the
case for the connective want.
To confirm that these results are indeed related
to the issue of perspectivisation, this LSA-analysis
was completed with a thematic text analysis to test
for the presence vs. absence of perspective indica-
tors. To this end we built a &amp;quot;Perspective&amp;quot; diction-
ary of perspective-indicating elements (Spooren,
1989) such as intensifiers, emphasisers, attitudinal
nouns and adjuncts, etc. (Caenepeel, 1989). The
dictionary was composed of two subcategories:
a) communication markers, like (non-
ambiguous) verbs and adverbs of saying
and thinking, e.g. report, tell, confirm, re-
quire, according to,...
b) markers of the speaker&apos;s attitude, like lin-
guistic elements expressing an expectation
or a denial of expectation, intensifiers and
attitudinals, and evaluative words, e.g.
probably, must, horrible, fantastic, ...
To build the dictionary, we used a Dutch thesaurus
(Brouwers, 1997) and extracted all (unambiguous)
lemmas corresponding to one of the above-
mentioned categories. Multi-word expressions or
separable verbs were not included in the lists. The
lists were composed on two native speaker&apos;s
judgements with a good knowledge of the litera-
ture on perspectivisation.
The idea of the thematic text analysis was to
confirm that the break in semantic tightness occur-
ring with want-segments, as revealed by the LSA-
analysis, could indeed be interpreted in terms of a
perspective shift. We would therefore expect that
the causal segments related by the connective want
show diverging perspectivisation patterns, and that
this will not be the case for the segments related by
omdat, doordat, aangezien. This is reformulated in
hypothesis 3.
</bodyText>
<figureCaption confidence="0.816495">
Hypothesis 3: If the causal segments are re-
lated by the connective want, the Q-segment
contains perspective signals, the P-segment
does not. The causal segments related by the
connectives omdat, doordat, aangezien do not
present such a shift.
</figureCaption>
<table confidence="0.999902909090909">
Communication Attitude markers
markers
Mean Q Mean P Mean Q Mean P
0.173 0.115 0.360 0.273
SD: 0.38 SD: 0.32 SD: 0.48 SD: 0.45
0.129 0.104 0.305 0.326
SD: 0.33 SD: 0.31 SD: 0.46 SD: 0.47
0.179 0.162 0.312 0.312
SD: 0.38 SD: 0.37 SD: 0.46 SD: 0.46
0.175 0.181 0.442 0.394
SD: 0.38 SD: 0.38 SD: 0.50 SD: 0.49
</table>
<tableCaption confidence="0.999865">
Table 3: Mean number of perspective markers in P &amp; Q
</tableCaption>
<bodyText confidence="0.999727">
The results displayed in Table 3 show that the hy-
pothesis is borne out for the subcategory of attitu-
dinal markers: want-segments display a higher
amount of attitudinal markers in Q than in P (F(1,
5588) = 26.84, p &lt; 0.0001). For the other connec-
tives this is not the case. For the communication
markers, the hypothesis is not borne out. Actually,
only omdat displays a higher amount of communi-
</bodyText>
<table confidence="0.941417375">
Cos. Q &amp; P Cos. Prior Subse-
quent
Mean SD Mean SD
0.143 0.17 0.207 0.21
0.154 0.17 0.187 0.19
0.137 0.17 0.182 0.20
0.120 0.17 0.150 0.19
aangezien
</table>
<equation confidence="0.961672533333333">
(N = 200)
doordat (N
= 644)
omdat (N =
5691)
want (N =
3974)
aangezien
(N = 139)
doordat (N
= 699)
omdat (N =
6747)
want (N =
5589)
</equation>
<bodyText confidence="0.999395666666666">
cation markers in Q (F(1, 6746) = 6.53, p &lt; 0.01).
While this latter result might seem counter to ex-
pectation, it actually goes in the direction of prior
observations that omdat-relations frequently dis-
play the explicit introduction of speech acts (De-
gand, 2001; Pit 2003).
All together, these results offer new interesting
insights into the discourse environment of (Dutch)
causal connectives. On the one hand, we have
shown with the LSA analysis that the proximity
between Q and P is lower for want-relations than
for the other connectives and that this is also the
case for the semantic proximity between the sen-
tences prior and subsequent to the causal relations.
We therefore concluded that the connective want is
a marker of thematic shift. On the other hand, the
TTA analysis revealed that the Q-segments in
want-relations display a higher amount of attitudi-
nal markers. In our view, the presence of these
markers leads to the conclusion that the connective
want is indeed a marker of perspective shift, i.e.
the break in semantic tightness should be inter-
preted as a perspective break, as has often been
suggested in the literature. Furthermore, the addi-
tional results for want (absence of communication
markers in Q) also suggest that markers expressing
the speaker&apos;s attitude should be clearly distin-
guished from those that explicit the speaker&apos;s
speech act (verbs of saying) or designate him/her
explicitly as the source of the speech act (adverbs
like aldus, volgens, ... &apos;according to&apos;).
The polyphony/monophony distinction overlaps
with the coordination/subordination distinction
between want vs. the other connectives. The ques-
tion arises which of those two factors is responsi-
ble for the results obtained. One route to follow is
to compare our results with a language like English
in which a same connective (because) has both
monophonic and polyphonic uses, or with a lan-
guage like French where a polyphonic connective
like puisque is subordinating. The latter topic is
object of ongoing research.
</bodyText>
<sectionHeader confidence="0.999852" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999863135135135">
In this paper we have presented a method for the
linguistic investigation of a discourse phenomenon,
viz. connectives, giving very satisfying results
without necessitating heavy, work-intensive (hand-
based) discourse annotation. The research pre-
sented is important to the corpus study of discourse
phenomena for a number of reasons. The first is
that it makes it possible to test linguistic hypothe-
ses about the use of causal connectives on a large
scale basis, whereas previous tests were based on
only small corpora and small amount of data. The
second is that the analysis is mostly fully auto-
matic, especially with respect to the coding of the
fragments. It is especially this latter feature that
should appeal to the linguistic community, and
makes our method more robust. The intercoder
reliability is a constant concern of everyone work-
ing with corpora to test linguistic hypotheses (Car-
letta, 1996), and the more so when one is coding
for semanto-pragmatic interpretations, as in the
case of the analysis of connectives. A third reason
is that our method combines two techniques of
automatic text analysis, which allows us to formu-
late our hypotheses to be tested more fine-grained
than possible with either one separately. Moreover,
hypothesis formulation and testing goes further:
We can use the methodology to formulate new hy-
potheses. An interesting possibility is to use LSA
to find neighbours of terms in the dictionary, thus
extending the dictionary. A further interesting
venue is to test the linguistic hypotheses for differ-
ent genres. This brings us to a further possibility,
namely to reuse the semantic space for different
types of linguistic research. A final possibility is to
use the present semantic space for comparative
research: How do the present results compare to a
similar analysis of French connectives?
</bodyText>
<sectionHeader confidence="0.994955" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999586">
L. Degand and Y. Bestgen are research fellows of
the Belgian National Fund for Scientific Research.
This research was supported by grant n° FRFC
2.4535.02 and by a grant (“Action de Recherche
concertée”) of the government of the French-
language community of Belgium.
</bodyText>
<sectionHeader confidence="0.998602" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999589419047619">
Berry, M.W. (1992). Large scale singular value compu-
tation, International journal of Supercomputer Appli-
cation, 6: 13-49.
Berry, M., Do, T., O&apos;Brien, G., Krishna, V. and Varad-
han, S. (1993). SVDPACKC: Version 1.0 User&apos;s
Guide, Tech. Rep. CS-93-194, University of Tennes-
see, Knoxville, TN, October 1993.
Burgess C., Livesay K., Lund K., &amp;quot; Explorations in Con-
text Space : Words, Sentences, Discourse &amp;quot;, Dis-
course Processes, Vol. 25, 1998, p. 211-257.
Bestgen, Y. (2002). Détermination de la valence affec-
tive de termes dans de grands corpus de textes. Actes
du Colloque International sur la Fouille de Texte
CIFT&apos;02 (pp. 81-94). Nancy : INRIA.
Bestgen, Y., Degand, L. &amp; Spooren, W. (2003). On the
use of automatic techniques to determine the seman-
tics of connectives in large newspaper corpora: an
exploratory study. Lagerwerf L., Spooren W., De-
gand L. (Eds). Determination of Information and
Tenor in Texts: MAD 2003, Stichting Neerlandistiek
VU Amsterdam &amp; Nodus Publikationen Münster,
179-188.
Biber, D. (1998). Variation across speech and writing.
Cambridge: Cambridge University Press.
Brouwers, L. (1997). Het juiste woord, betekeniswoor-
denboek. 6th ed. (ed. by F. Claes). Antwerpen etc.:
Standaard.
Caenepeel, M. (1989). Aspect, Temporal Ordering and
Perspective in Narrative Fiction. Doctoral Disserta-
tion University of Edinburgh.
Carletta, J. (1996). Assessing agreement on classifica-
tion tasks: the kappa statistic. Computational Lin-
guistics 22 (2), 249-254.
Choi, F., Wiemer-Hastings P., &amp; Moore J. (2001) La-
tent Semantic Analysis for Text Segmentation. In L.
Lee &amp; D. Harman (Eds.), Proceedings of the 2001
Conference on Empirical Methods in Natural Lan-
guage Processing , 109-117.
Daelemans, W., Zavrel, J., Berck, P., &amp; Gillis, S.
(1996). MBT: A Memory-Based Part of Speech Tag-
ger-Generator. In E. Ejerhed &amp; I. Dagan (Eds.), Pro-
ceedings of the Fourth Workshop on Very Large
Corpora (pp. 14-27). Copenhagen, Denmark.
Deerwester S., Dumais S.T., Furnas G.W., Landauer
T.K., Harshman R., Indexing by Latent Semantic
Analysis, Journal of the American Society for Infor-
mation Science, Vol. 41, 1990, 391-407.
Degand, L. (2001). Form and Function of Causation. A
theoretical and empirical investigation of causal
constructions in Dutch, Peeters, Leuven, Paris, Ster-
ling.
Degand, L. &amp; Pander Maat, H. (2003) A contrastive
study of Dutch and French causal connectives on the
Speaker Involvement Scale, A. Verhagen &amp; J. van de
Weijer (eds.) Usage based approaches to Dutch (pp.
175-199). Utrecht: LOT.
Foltz, P.W., Kintsch, W., &amp; Landauer T.K. (1998). The
measurement of textual coherence with Latent Se-
mantic Analysis. Discourse Processes, 25, 285-307.
Kintsch, W. (2000). Metaphor comprehension: A com-
putational theory. Psychonomic Bulletin and Review,
7, 257-266.
Kintsch W., (2001).Predication, Cognitive Science 25,
173-202.
Landauer, T.K., Foltz, P.W., and Laham, D. (1998). An
introduction to Latent Semantic Analysis. Discourse
Processes, 25 (2, 3), 259-284.
Lebart, L., Salem, A., and Berry, L. (1998). Exploring
Textual Data. Kluwer Academic Publisher.
Lemaire, B., Bianco, M., Sylvestre, E., &amp; Noveck, I.
(2001). Un modèle de compréhension de textes fondé
sur l&apos;analyse de la sémantique latente. In H. Paugam
Moisy, V. Nyckees, J. Caron-Pargue (Eds.), La Co-
gnition entre Individu et Société : Actes du Colloque
de l&apos;ARCo (pp. 309-320). Paris: Hermès.
Oversteegen, L. (1997). On the pragmatic nature of
causal and contrastive connectives. Discourse
Processes, 24, 51-86.
Pennebaker, J.W., , Mehl, M.R., &amp; Niederhoffer, K.G.
(2003). Psychological aspects of natural language
use: Our words, our selves. Annual Review of Psy-
chology, 54, 547-577.
Piérard, S., Degand, L., &amp; Bestgen Y. (2004). Vers une
recherche automatique des marqueurs de la segmen-
tation du discours. Actes des 7es Journées internatio-
nales d’Analyse statistique des Données Textuelles.
Louvain-la-Neuve.
Pit, M. (2003). How to Express Yourself with a Causal
Connective. Subjectivity and Causal Connectives in
Dutch, German and French. Amsterdam : Rodopi.
Popping, R. (2000). Computer-assisted text analysis.
London: SAGE.
Spooren, W.P.M.S (1989). Some Aspects of the Form
and Interpretation of Global Contrastive Coherence
Relations. Unpublished Dissertation, K.U. Nijmegen.
Stone, P.J. (1997). Thematic text analysis: New agendas
for analyzing text content. In C.W. Roberts (Eds.).
Text Analysis for the Social Sciences: Methods for
Drawing Statistical Inferences from Texts and Tran-
scripts (pp.35-54). Mahwah, NJ: Erlbaum.
van den Bosch, A., &amp; Daelemans, W. (1999). Memory-
based morphological analysis. In Proceedings of the
37th Annual Meeting of the Association for Compu-
tational Linguistics, ACL&apos;99 (pp. 285-292). New
Brunswick, NJ: ACL
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.048542">
<title confidence="0.722255">On the use of automatic tools for large scale semantic analyses of causal</title>
<abstract confidence="0.7409865">connectives Liesbeth catholique de Institute of Linguistics</abstract>
<email confidence="0.670886">degand@lige.ucl.ac.be</email>
<author confidence="0.936824">Wilbert</author>
<affiliation confidence="0.981089">Vrije Universiteit, Language &amp; Communication</affiliation>
<email confidence="0.72701">w.spooren@let.vu.nl</email>
<abstract confidence="0.89769575">Yves catholique de Faculty of Psychology Yves.bestgen@psp.ucl.</abstract>
<email confidence="0.614393">ac.be</email>
<abstract confidence="0.992059636363637">In this paper we describe the (annotation) tools underlying two automatic techniques to analyse the meaning and use of backward causal connectives in large Dutch newspaper corpora. With the help of these techniques, Latent Semantic Analysis and Thematic Text Analysis, the contexts of more than 14,000 connectives were studied. We will focus here on the methods of analysis and on the fairly straightforward (annotation) tools needed to perform the semantic analyses, i.e. POS-tagging, lemmatisation and a thesaurus-like thematic dictionary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M W Berry</author>
</authors>
<title>Large scale singular value computation,</title>
<date>1992</date>
<journal>International journal of Supercomputer Application,</journal>
<volume>6</volume>
<pages>13--49</pages>
<contexts>
<context position="15710" citStr="Berry, 1992" startWordPosition="2555" endWordPosition="2556">ords (words occurring in “any” context, like determiners, auxiliaries, conjunctions, ...), brought the total number of words back to approximately 6.5 million. For the input lexical table, the documents were articles of minimally 24 words and maximally 523 words, i.e. all articles minus the 10% shortest and minus the 10% longest ones. As to the words, we kept all those that occurred at least ten times in the data set. Overall this resulted in a matrix of 36630 terms in 28640 documents. To build the semantic space proper, the singular value decomposition was realized with the program SVDPACKC (Berry, 1992; Berry et al., 1993), and the 300 first singular vectors were retained. In the present research we will use this technique to evaluate the semantic proximity between P&amp; Q, and between the causal segments and the prior or subsequent sentences. 2.3 Dictionaries and lexical categorisation The second technique used to test the linguistic hypotheses is alternatively called ‘word count strategy’ (Pennebaker et al., 2003), automatic identification of linguistic features (Biber, 1988) or thematic text analysis (Popping, 2000; Stone, 1997), the aim of which is to determine whether some categories of w</context>
</contexts>
<marker>Berry, 1992</marker>
<rawString>Berry, M.W. (1992). Large scale singular value computation, International journal of Supercomputer Application, 6: 13-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Berry</author>
<author>T Do</author>
<author>G O&apos;Brien</author>
<author>V Krishna</author>
<author>S Varadhan</author>
</authors>
<title>SVDPACKC: Version 1.0 User&apos;s Guide,</title>
<date>1993</date>
<tech>Tech. Rep. CS-93-194,</tech>
<institution>University of Tennessee,</institution>
<location>Knoxville, TN,</location>
<contexts>
<context position="15731" citStr="Berry et al., 1993" startWordPosition="2557" endWordPosition="2560">ccurring in “any” context, like determiners, auxiliaries, conjunctions, ...), brought the total number of words back to approximately 6.5 million. For the input lexical table, the documents were articles of minimally 24 words and maximally 523 words, i.e. all articles minus the 10% shortest and minus the 10% longest ones. As to the words, we kept all those that occurred at least ten times in the data set. Overall this resulted in a matrix of 36630 terms in 28640 documents. To build the semantic space proper, the singular value decomposition was realized with the program SVDPACKC (Berry, 1992; Berry et al., 1993), and the 300 first singular vectors were retained. In the present research we will use this technique to evaluate the semantic proximity between P&amp; Q, and between the causal segments and the prior or subsequent sentences. 2.3 Dictionaries and lexical categorisation The second technique used to test the linguistic hypotheses is alternatively called ‘word count strategy’ (Pennebaker et al., 2003), automatic identification of linguistic features (Biber, 1988) or thematic text analysis (Popping, 2000; Stone, 1997), the aim of which is to determine whether some categories of words (e.g., words of </context>
</contexts>
<marker>Berry, Do, O&apos;Brien, Krishna, Varadhan, 1993</marker>
<rawString>Berry, M., Do, T., O&apos;Brien, G., Krishna, V. and Varadhan, S. (1993). SVDPACKC: Version 1.0 User&apos;s Guide, Tech. Rep. CS-93-194, University of Tennessee, Knoxville, TN, October 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Burgess</author>
<author>K Livesay</author>
<author>K Lund</author>
</authors>
<title>Explorations in Context Space</title>
<date>1998</date>
<booktitle>Words, Sentences, Discourse &amp;quot;, Discourse Processes,</booktitle>
<volume>25</volume>
<pages>211--257</pages>
<contexts>
<context position="12840" citStr="Burgess et al., 1998" startWordPosition="2086" endWordPosition="2089">ct which draws our attention here. The point of departure of the analysis is a lexical table (Lebart and Salem, 1992) containing the frequencies of every word in each of the documents included in the text material, a document being a text, a paragraph, or a sentence. To derive semantic relations between words from the lexical table the analysis of mere co-occurrences will not do, the major problem being that even in a large corpus most words are relatively rare. Consequently the co-occurrences of words are even rarer. This fact makes such co-occurrences very sensitive to arbitrary variations (Burgess et al., 1998 ; Kintsch, 2001). LSA resolves this problem by replacing the original frequency table by an approximation producing a kind of smoothening effect on the associations. To this end, the frequency table undergoes a singular value decomposition and it is then recomposed on the basis of only a fraction of the information it contains. Thus, the thousands of words from the documents have been substituted by linear combinations or ‘semantic dimensions’ with respect to which the original words can be situated again. Contrary to a classical factor analysis the extracted dimensions are very numerous and </context>
</contexts>
<marker>Burgess, Livesay, Lund, 1998</marker>
<rawString>Burgess C., Livesay K., Lund K., &amp;quot; Explorations in Context Space : Words, Sentences, Discourse &amp;quot;, Discourse Processes, Vol. 25, 1998, p. 211-257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bestgen</author>
</authors>
<title>Détermination de la valence affective de termes dans de grands corpus de textes.</title>
<date>2002</date>
<booktitle>Actes du Colloque International sur la Fouille de Texte CIFT&apos;02</booktitle>
<pages>81--94</pages>
<publisher>INRIA.</publisher>
<location>Nancy :</location>
<contexts>
<context position="12176" citStr="Bestgen, 2002" startWordPosition="1976" endWordPosition="1977">e can be viewed from two sides. At a theoretical level, it is meant to be used to develop simulations of the cognitive processes running during language comprehension, including, for instance, a computational model of metaphor treatment (Kintsch, 2000 ; Lemaire et al., 2001), but also to analyse the coherence of texts (Foltz et al., 1998 ; Piérard et al., 2004). At a more applied level, it is a technique which enables to infer and to represent the meaning of words on the basis of their actual use in text so that the similarity of the meaning of words, sentences or paragraphs can be estimated (Bestgen, 2002; Choi et al., 2001). It is this latter aspect which draws our attention here. The point of departure of the analysis is a lexical table (Lebart and Salem, 1992) containing the frequencies of every word in each of the documents included in the text material, a document being a text, a paragraph, or a sentence. To derive semantic relations between words from the lexical table the analysis of mere co-occurrences will not do, the major problem being that even in a large corpus most words are relatively rare. Consequently the co-occurrences of words are even rarer. This fact makes such co-occurren</context>
</contexts>
<marker>Bestgen, 2002</marker>
<rawString>Bestgen, Y. (2002). Détermination de la valence affective de termes dans de grands corpus de textes. Actes du Colloque International sur la Fouille de Texte CIFT&apos;02 (pp. 81-94). Nancy : INRIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Bestgen</author>
<author>L Degand</author>
<author>W Spooren</author>
</authors>
<title>On the use of automatic techniques to determine the semantics of connectives in large newspaper corpora: an exploratory study.</title>
<date>2003</date>
<journal>Lagerwerf</journal>
<booktitle>Determination of Information and Tenor in Texts: MAD 2003, Stichting Neerlandistiek VU Amsterdam &amp; Nodus Publikationen Münster,</booktitle>
<pages>179--188</pages>
<contexts>
<context position="2745" citStr="Bestgen et al., 2003" startWordPosition="408" endWordPosition="411">s of causal connectives and tried to replicate the results. The linguistic material we worked on are four Dutch backward causal connectives: aangezien (&apos;since&apos;), doordat (&apos;because of the fact that&apos;), omdat (&apos;because&apos;) and, want (&apos;because&apos;). This choice was motivated by the fact that there has already been quite some linguistic work on this topic, mainly empirically based (Degand, 2001; Degand and Pander Maat, 2003; Pit, 2003).1 We have shown elsewhere how linguistic hypotheses concerning the scaling of these connectives in terms of subjectivity and their thematic behaviour could be supported (Bestgen et al., 2003). Since these first results are very encouraging, we would like to focus here on the methods of automatic analysis – Latent Semantic Analysis and Thematic Text Analysis - and on the fairly straightforward (annotation) tools needed to perform the semantic analyses, i.e. POS-tagging, lemmatisation and a thesaurus-like thematic dictionary. We illustrate how the combination of the two techniques of automatic analysis permit to gain deeper insight into the semantic constraints on the use of the connectives studied. Doing so, we test a number of new hypotheses concerning the perspectivizing and poly</context>
</contexts>
<marker>Bestgen, Degand, Spooren, 2003</marker>
<rawString>Bestgen, Y., Degand, L. &amp; Spooren, W. (2003). On the use of automatic techniques to determine the semantics of connectives in large newspaper corpora: an exploratory study. Lagerwerf L., Spooren W., Degand L. (Eds). Determination of Information and Tenor in Texts: MAD 2003, Stichting Neerlandistiek VU Amsterdam &amp; Nodus Publikationen Münster, 179-188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Biber</author>
</authors>
<title>Variation across speech and writing. Cambridge:</title>
<date>1998</date>
<publisher>Cambridge University Press.</publisher>
<marker>Biber, 1998</marker>
<rawString>Biber, D. (1998). Variation across speech and writing. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Brouwers</author>
</authors>
<title>Het juiste woord,</title>
<date>1997</date>
<editor>betekeniswoordenboek. 6th ed. (ed. by F. Claes). Antwerpen etc.:</editor>
<publisher>Standaard.</publisher>
<contexts>
<context position="21813" citStr="Brouwers, 1997" startWordPosition="3530" endWordPosition="3531"> perspective-indicating elements (Spooren, 1989) such as intensifiers, emphasisers, attitudinal nouns and adjuncts, etc. (Caenepeel, 1989). The dictionary was composed of two subcategories: a) communication markers, like (nonambiguous) verbs and adverbs of saying and thinking, e.g. report, tell, confirm, require, according to,... b) markers of the speaker&apos;s attitude, like linguistic elements expressing an expectation or a denial of expectation, intensifiers and attitudinals, and evaluative words, e.g. probably, must, horrible, fantastic, ... To build the dictionary, we used a Dutch thesaurus (Brouwers, 1997) and extracted all (unambiguous) lemmas corresponding to one of the abovementioned categories. Multi-word expressions or separable verbs were not included in the lists. The lists were composed on two native speaker&apos;s judgements with a good knowledge of the literature on perspectivisation. The idea of the thematic text analysis was to confirm that the break in semantic tightness occurring with want-segments, as revealed by the LSAanalysis, could indeed be interpreted in terms of a perspective shift. We would therefore expect that the causal segments related by the connective want show diverging</context>
</contexts>
<marker>Brouwers, 1997</marker>
<rawString>Brouwers, L. (1997). Het juiste woord, betekeniswoordenboek. 6th ed. (ed. by F. Claes). Antwerpen etc.: Standaard.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Caenepeel</author>
</authors>
<title>Aspect, Temporal Ordering and Perspective in Narrative Fiction. Doctoral Dissertation</title>
<date>1989</date>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="21336" citStr="Caenepeel, 1989" startWordPosition="3461" endWordPosition="3462">s aangezien, doordat and omdat go along with topic continuity (or at least semantic proximity) between the prior and subsequent sentence to the causal construction, while this is less the case for the connective want. To confirm that these results are indeed related to the issue of perspectivisation, this LSA-analysis was completed with a thematic text analysis to test for the presence vs. absence of perspective indicators. To this end we built a &amp;quot;Perspective&amp;quot; dictionary of perspective-indicating elements (Spooren, 1989) such as intensifiers, emphasisers, attitudinal nouns and adjuncts, etc. (Caenepeel, 1989). The dictionary was composed of two subcategories: a) communication markers, like (nonambiguous) verbs and adverbs of saying and thinking, e.g. report, tell, confirm, require, according to,... b) markers of the speaker&apos;s attitude, like linguistic elements expressing an expectation or a denial of expectation, intensifiers and attitudinals, and evaluative words, e.g. probably, must, horrible, fantastic, ... To build the dictionary, we used a Dutch thesaurus (Brouwers, 1997) and extracted all (unambiguous) lemmas corresponding to one of the abovementioned categories. Multi-word expressions or se</context>
</contexts>
<marker>Caenepeel, 1989</marker>
<rawString>Caenepeel, M. (1989). Aspect, Temporal Ordering and Perspective in Narrative Fiction. Doctoral Dissertation University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carletta</author>
</authors>
<title>Assessing agreement on classification tasks: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<volume>22</volume>
<issue>2</issue>
<pages>249--254</pages>
<contexts>
<context position="26778" citStr="Carletta, 1996" startWordPosition="4357" endWordPosition="4359">course phenomena for a number of reasons. The first is that it makes it possible to test linguistic hypotheses about the use of causal connectives on a large scale basis, whereas previous tests were based on only small corpora and small amount of data. The second is that the analysis is mostly fully automatic, especially with respect to the coding of the fragments. It is especially this latter feature that should appeal to the linguistic community, and makes our method more robust. The intercoder reliability is a constant concern of everyone working with corpora to test linguistic hypotheses (Carletta, 1996), and the more so when one is coding for semanto-pragmatic interpretations, as in the case of the analysis of connectives. A third reason is that our method combines two techniques of automatic text analysis, which allows us to formulate our hypotheses to be tested more fine-grained than possible with either one separately. Moreover, hypothesis formulation and testing goes further: We can use the methodology to formulate new hypotheses. An interesting possibility is to use LSA to find neighbours of terms in the dictionary, thus extending the dictionary. A further interesting venue is to test t</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Carletta, J. (1996). Assessing agreement on classification tasks: the kappa statistic. Computational Linguistics 22 (2), 249-254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Choi</author>
<author>P Wiemer-Hastings</author>
<author>J Moore</author>
</authors>
<title>Latent Semantic Analysis for Text Segmentation. In</title>
<date>2001</date>
<booktitle>Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing ,</booktitle>
<pages>109--117</pages>
<contexts>
<context position="12196" citStr="Choi et al., 2001" startWordPosition="1978" endWordPosition="1981"> from two sides. At a theoretical level, it is meant to be used to develop simulations of the cognitive processes running during language comprehension, including, for instance, a computational model of metaphor treatment (Kintsch, 2000 ; Lemaire et al., 2001), but also to analyse the coherence of texts (Foltz et al., 1998 ; Piérard et al., 2004). At a more applied level, it is a technique which enables to infer and to represent the meaning of words on the basis of their actual use in text so that the similarity of the meaning of words, sentences or paragraphs can be estimated (Bestgen, 2002; Choi et al., 2001). It is this latter aspect which draws our attention here. The point of departure of the analysis is a lexical table (Lebart and Salem, 1992) containing the frequencies of every word in each of the documents included in the text material, a document being a text, a paragraph, or a sentence. To derive semantic relations between words from the lexical table the analysis of mere co-occurrences will not do, the major problem being that even in a large corpus most words are relatively rare. Consequently the co-occurrences of words are even rarer. This fact makes such co-occurrences very sensitive t</context>
</contexts>
<marker>Choi, Wiemer-Hastings, Moore, 2001</marker>
<rawString>Choi, F., Wiemer-Hastings P., &amp; Moore J. (2001) Latent Semantic Analysis for Text Segmentation. In L. Lee &amp; D. Harman (Eds.), Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing , 109-117.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A Memory-Based Part of Speech Tagger-Generator.</title>
<date>1996</date>
<booktitle>In E. Ejerhed &amp; I. Dagan (Eds.), Proceedings of the Fourth Workshop on Very Large Corpora</booktitle>
<pages>14--27</pages>
<location>Copenhagen,</location>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>Daelemans, W., Zavrel, J., Berck, P., &amp; Gillis, S. (1996). MBT: A Memory-Based Part of Speech Tagger-Generator. In E. Ejerhed &amp; I. Dagan (Eds.), Proceedings of the Fourth Workshop on Very Large Corpora (pp. 14-27). Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis,</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<volume>41</volume>
<pages>391--407</pages>
<contexts>
<context position="14527" citStr="Deerwester et al., 1990" startWordPosition="2357" endWordPosition="2360">consequently, the closer their cosine will be to 1 (coinciding vectors). A cosine of 0 shows an absence of similarity, since the corresponding vectors point in orthogonal directions. It is also possible to calculate the similarity between ‘higher order’ elements, i.e. between sentences, paragraphs, and entire documents, or combinations of those, even if this higher order element isn’t by itself an analysed element. The vector in question corresponds to the centroid of the words composing the segment under investigation. The centroid results from the weighted sum of the vectors of these words (Deerwester et al., 1990). This makes it possible to calculate the semantic proximity between any two sentences, viz. whether present in the original corpus or not, whether the original corpus had been segmented in sentence length documents or not. To perform the LSA analyses, we used the Dutch newspaper corpus to build the semantic space. To this end, the data set, which had been lemmatised with MBLEM (Memory Based Lemmatiser) (Van den Bosch &amp; Daelemans, 1999), was cut into article-length segments. Elimination of all digits, special characters, punctuation marks, and of a list of 222 stopwords (words occurring in “an</context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>Deerwester S., Dumais S.T., Furnas G.W., Landauer T.K., Harshman R., Indexing by Latent Semantic Analysis, Journal of the American Society for Information Science, Vol. 41, 1990, 391-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Degand</author>
</authors>
<title>Form and Function of Causation. A theoretical and empirical investigation of causal constructions in</title>
<date>2001</date>
<location>Dutch, Peeters, Leuven, Paris, Sterling.</location>
<contexts>
<context position="2511" citStr="Degand, 2001" startWordPosition="374" endWordPosition="375">stic phenomenon) and implementing the analytic procedures to make them analyst-independent. In this paper, we test such a methodology for which we used a number of linguistic hypotheses found in the literature on the semantics of causal connectives and tried to replicate the results. The linguistic material we worked on are four Dutch backward causal connectives: aangezien (&apos;since&apos;), doordat (&apos;because of the fact that&apos;), omdat (&apos;because&apos;) and, want (&apos;because&apos;). This choice was motivated by the fact that there has already been quite some linguistic work on this topic, mainly empirically based (Degand, 2001; Degand and Pander Maat, 2003; Pit, 2003).1 We have shown elsewhere how linguistic hypotheses concerning the scaling of these connectives in terms of subjectivity and their thematic behaviour could be supported (Bestgen et al., 2003). Since these first results are very encouraging, we would like to focus here on the methods of automatic analysis – Latent Semantic Analysis and Thematic Text Analysis - and on the fairly straightforward (annotation) tools needed to perform the semantic analyses, i.e. POS-tagging, lemmatisation and a thesaurus-like thematic dictionary. We illustrate how the combi</context>
<context position="19196" citStr="Degand, 2001" startWordPosition="3125" endWordPosition="3126">ng LSA and TTA: an application 3.1 Perspective shift There are a number of claims in the literature that some connectives co-occur with perspective shifts between the causal segments, while others do not. Perspectivisation accounts for the fact that there are more sources of information than the speaker alone. In relation to our connectives, perspectivisation has been claimed to play a role in the meaning differences between want (introducing a perspective shift) and omdat (no perspective shift). However, the various corpus studies on this matter have not univocally confirmed this hypothesis (Degand, 2001; Oversteegen, 1997). We would like to explore this matter further by comparing the semantic tightness of the segments related by our connectives. This will be done by calculating the semantic proximity between Q and P for each of the connectives. Our hypotheses are as follows: Hypothesis 1: The cosine between Q and P related by monophonic connectives (omdat) should be higher than the cosine between Q and P related by polyphonic connectives (want). Hypothesis 2: The cosine between the prior sentence and the subsequent sentence should be higher for monophonic connectives than for polyphonic con</context>
<context position="24106" citStr="Degand, 2001" startWordPosition="3930" endWordPosition="3932"> not borne out. Actually, only omdat displays a higher amount of communiCos. Q &amp; P Cos. Prior Subsequent Mean SD Mean SD 0.143 0.17 0.207 0.21 0.154 0.17 0.187 0.19 0.137 0.17 0.182 0.20 0.120 0.17 0.150 0.19 aangezien (N = 200) doordat (N = 644) omdat (N = 5691) want (N = 3974) aangezien (N = 139) doordat (N = 699) omdat (N = 6747) want (N = 5589) cation markers in Q (F(1, 6746) = 6.53, p &lt; 0.01). While this latter result might seem counter to expectation, it actually goes in the direction of prior observations that omdat-relations frequently display the explicit introduction of speech acts (Degand, 2001; Pit 2003). All together, these results offer new interesting insights into the discourse environment of (Dutch) causal connectives. On the one hand, we have shown with the LSA analysis that the proximity between Q and P is lower for want-relations than for the other connectives and that this is also the case for the semantic proximity between the sentences prior and subsequent to the causal relations. We therefore concluded that the connective want is a marker of thematic shift. On the other hand, the TTA analysis revealed that the Q-segments in want-relations display a higher amount of atti</context>
</contexts>
<marker>Degand, 2001</marker>
<rawString>Degand, L. (2001). Form and Function of Causation. A theoretical and empirical investigation of causal constructions in Dutch, Peeters, Leuven, Paris, Sterling.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Degand</author>
<author>Pander Maat</author>
<author>H</author>
</authors>
<title>A contrastive study of Dutch and French causal connectives on the Speaker Involvement Scale,</title>
<date>2003</date>
<booktitle>Usage based approaches to Dutch</booktitle>
<pages>175--199</pages>
<editor>A. Verhagen &amp; J. van de Weijer (eds.)</editor>
<location>Utrecht: LOT.</location>
<marker>Degand, Maat, H, 2003</marker>
<rawString>Degand, L. &amp; Pander Maat, H. (2003) A contrastive study of Dutch and French causal connectives on the Speaker Involvement Scale, A. Verhagen &amp; J. van de Weijer (eds.) Usage based approaches to Dutch (pp. 175-199). Utrecht: LOT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Foltz</author>
<author>W Kintsch</author>
<author>T K Landauer</author>
</authors>
<title>The measurement of textual coherence with Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<pages>285--307</pages>
<contexts>
<context position="11902" citStr="Foltz et al., 1998" startWordPosition="1923" endWordPosition="1926"> be presented is Latent Semantic Analysis (LSA), a mathematical technique for extracting a very large “semantic space” from large text corpora on the basis of the statistical analysis of the set of co-occurrences in a text corpus. Landauer et al. (1998) stress that this technique can be viewed from two sides. At a theoretical level, it is meant to be used to develop simulations of the cognitive processes running during language comprehension, including, for instance, a computational model of metaphor treatment (Kintsch, 2000 ; Lemaire et al., 2001), but also to analyse the coherence of texts (Foltz et al., 1998 ; Piérard et al., 2004). At a more applied level, it is a technique which enables to infer and to represent the meaning of words on the basis of their actual use in text so that the similarity of the meaning of words, sentences or paragraphs can be estimated (Bestgen, 2002; Choi et al., 2001). It is this latter aspect which draws our attention here. The point of departure of the analysis is a lexical table (Lebart and Salem, 1992) containing the frequencies of every word in each of the documents included in the text material, a document being a text, a paragraph, or a sentence. To derive sema</context>
</contexts>
<marker>Foltz, Kintsch, Landauer, 1998</marker>
<rawString>Foltz, P.W., Kintsch, W., &amp; Landauer T.K. (1998). The measurement of textual coherence with Latent Semantic Analysis. Discourse Processes, 25, 285-307.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<title>Metaphor comprehension: A computational theory.</title>
<date>2000</date>
<journal>Psychonomic Bulletin and Review,</journal>
<volume>7</volume>
<pages>257--266</pages>
<contexts>
<context position="11814" citStr="Kintsch, 2000" startWordPosition="1908" endWordPosition="1909"> the construction of the LSA semantic space The first automatic technique that will be presented is Latent Semantic Analysis (LSA), a mathematical technique for extracting a very large “semantic space” from large text corpora on the basis of the statistical analysis of the set of co-occurrences in a text corpus. Landauer et al. (1998) stress that this technique can be viewed from two sides. At a theoretical level, it is meant to be used to develop simulations of the cognitive processes running during language comprehension, including, for instance, a computational model of metaphor treatment (Kintsch, 2000 ; Lemaire et al., 2001), but also to analyse the coherence of texts (Foltz et al., 1998 ; Piérard et al., 2004). At a more applied level, it is a technique which enables to infer and to represent the meaning of words on the basis of their actual use in text so that the similarity of the meaning of words, sentences or paragraphs can be estimated (Bestgen, 2002; Choi et al., 2001). It is this latter aspect which draws our attention here. The point of departure of the analysis is a lexical table (Lebart and Salem, 1992) containing the frequencies of every word in each of the documents included i</context>
</contexts>
<marker>Kintsch, 2000</marker>
<rawString>Kintsch, W. (2000). Metaphor comprehension: A computational theory. Psychonomic Bulletin and Review, 7, 257-266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Kintsch</author>
</authors>
<date>2001</date>
<journal>Cognitive Science</journal>
<volume>25</volume>
<pages>173--202</pages>
<contexts>
<context position="12857" citStr="Kintsch, 2001" startWordPosition="2091" endWordPosition="2092">ntion here. The point of departure of the analysis is a lexical table (Lebart and Salem, 1992) containing the frequencies of every word in each of the documents included in the text material, a document being a text, a paragraph, or a sentence. To derive semantic relations between words from the lexical table the analysis of mere co-occurrences will not do, the major problem being that even in a large corpus most words are relatively rare. Consequently the co-occurrences of words are even rarer. This fact makes such co-occurrences very sensitive to arbitrary variations (Burgess et al., 1998 ; Kintsch, 2001). LSA resolves this problem by replacing the original frequency table by an approximation producing a kind of smoothening effect on the associations. To this end, the frequency table undergoes a singular value decomposition and it is then recomposed on the basis of only a fraction of the information it contains. Thus, the thousands of words from the documents have been substituted by linear combinations or ‘semantic dimensions’ with respect to which the original words can be situated again. Contrary to a classical factor analysis the extracted dimensions are very numerous and non-interpretable</context>
</contexts>
<marker>Kintsch, 2001</marker>
<rawString>Kintsch W., (2001).Predication, Cognitive Science 25, 173-202.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T K Landauer</author>
<author>P W Foltz</author>
<author>D Laham</author>
</authors>
<title>An introduction to Latent Semantic Analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<volume>25</volume>
<issue>2</issue>
<pages>259--284</pages>
<contexts>
<context position="11537" citStr="Landauer et al. (1998)" startWordPosition="1862" endWordPosition="1865"> preposed position, omdat in 10.41% of the cases, and doordat in 14.32% of the cases, a figure which rises to 43.5% of the cases for aangezien. It is interesting to point out that this is in total agreement with previous small-scale corpus research on this matter. 2.2 Lemmatisation and the construction of the LSA semantic space The first automatic technique that will be presented is Latent Semantic Analysis (LSA), a mathematical technique for extracting a very large “semantic space” from large text corpora on the basis of the statistical analysis of the set of co-occurrences in a text corpus. Landauer et al. (1998) stress that this technique can be viewed from two sides. At a theoretical level, it is meant to be used to develop simulations of the cognitive processes running during language comprehension, including, for instance, a computational model of metaphor treatment (Kintsch, 2000 ; Lemaire et al., 2001), but also to analyse the coherence of texts (Foltz et al., 1998 ; Piérard et al., 2004). At a more applied level, it is a technique which enables to infer and to represent the meaning of words on the basis of their actual use in text so that the similarity of the meaning of words, sentences or par</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Landauer, T.K., Foltz, P.W., and Laham, D. (1998). An introduction to Latent Semantic Analysis. Discourse Processes, 25 (2, 3), 259-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lebart</author>
<author>A Salem</author>
<author>L Berry</author>
</authors>
<title>Exploring Textual Data.</title>
<date>1998</date>
<publisher>Kluwer Academic Publisher.</publisher>
<marker>Lebart, Salem, Berry, 1998</marker>
<rawString>Lebart, L., Salem, A., and Berry, L. (1998). Exploring Textual Data. Kluwer Academic Publisher.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Lemaire</author>
<author>M Bianco</author>
<author>E Sylvestre</author>
<author>I Noveck</author>
</authors>
<title>Un modèle de compréhension de textes fondé sur l&apos;analyse de la sémantique latente. In</title>
<date>2001</date>
<pages>309--320</pages>
<location>Paris: Hermès.</location>
<contexts>
<context position="11838" citStr="Lemaire et al., 2001" startWordPosition="1911" endWordPosition="1914">n of the LSA semantic space The first automatic technique that will be presented is Latent Semantic Analysis (LSA), a mathematical technique for extracting a very large “semantic space” from large text corpora on the basis of the statistical analysis of the set of co-occurrences in a text corpus. Landauer et al. (1998) stress that this technique can be viewed from two sides. At a theoretical level, it is meant to be used to develop simulations of the cognitive processes running during language comprehension, including, for instance, a computational model of metaphor treatment (Kintsch, 2000 ; Lemaire et al., 2001), but also to analyse the coherence of texts (Foltz et al., 1998 ; Piérard et al., 2004). At a more applied level, it is a technique which enables to infer and to represent the meaning of words on the basis of their actual use in text so that the similarity of the meaning of words, sentences or paragraphs can be estimated (Bestgen, 2002; Choi et al., 2001). It is this latter aspect which draws our attention here. The point of departure of the analysis is a lexical table (Lebart and Salem, 1992) containing the frequencies of every word in each of the documents included in the text material, a d</context>
</contexts>
<marker>Lemaire, Bianco, Sylvestre, Noveck, 2001</marker>
<rawString>Lemaire, B., Bianco, M., Sylvestre, E., &amp; Noveck, I. (2001). Un modèle de compréhension de textes fondé sur l&apos;analyse de la sémantique latente. In H. Paugam Moisy, V. Nyckees, J. Caron-Pargue (Eds.), La Cognition entre Individu et Société : Actes du Colloque de l&apos;ARCo (pp. 309-320). Paris: Hermès.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Oversteegen</author>
</authors>
<title>On the pragmatic nature of causal and contrastive connectives.</title>
<date>1997</date>
<booktitle>Discourse Processes,</booktitle>
<volume>24</volume>
<pages>51--86</pages>
<contexts>
<context position="19216" citStr="Oversteegen, 1997" startWordPosition="3127" endWordPosition="3128">: an application 3.1 Perspective shift There are a number of claims in the literature that some connectives co-occur with perspective shifts between the causal segments, while others do not. Perspectivisation accounts for the fact that there are more sources of information than the speaker alone. In relation to our connectives, perspectivisation has been claimed to play a role in the meaning differences between want (introducing a perspective shift) and omdat (no perspective shift). However, the various corpus studies on this matter have not univocally confirmed this hypothesis (Degand, 2001; Oversteegen, 1997). We would like to explore this matter further by comparing the semantic tightness of the segments related by our connectives. This will be done by calculating the semantic proximity between Q and P for each of the connectives. Our hypotheses are as follows: Hypothesis 1: The cosine between Q and P related by monophonic connectives (omdat) should be higher than the cosine between Q and P related by polyphonic connectives (want). Hypothesis 2: The cosine between the prior sentence and the subsequent sentence should be higher for monophonic connectives than for polyphonic connectives. Table 2: M</context>
</contexts>
<marker>Oversteegen, 1997</marker>
<rawString>Oversteegen, L. (1997). On the pragmatic nature of causal and contrastive connectives. Discourse Processes, 24, 51-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Mehl</author>
<author>K G Niederhoffer</author>
</authors>
<title>Psychological aspects of natural language use: Our words, our selves.</title>
<date>2003</date>
<journal>Annual Review of Psychology,</journal>
<volume>54</volume>
<pages>547--577</pages>
<marker>Mehl, Niederhoffer, 2003</marker>
<rawString>Pennebaker, J.W., , Mehl, M.R., &amp; Niederhoffer, K.G. (2003). Psychological aspects of natural language use: Our words, our selves. Annual Review of Psychology, 54, 547-577.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Piérard</author>
<author>L Degand</author>
<author>Y Bestgen</author>
</authors>
<title>Vers une recherche automatique des marqueurs de la segmentation du discours. Actes des 7es Journées internationales d’Analyse statistique des Données Textuelles. Louvain-la-Neuve.</title>
<date>2004</date>
<contexts>
<context position="11926" citStr="Piérard et al., 2004" startWordPosition="1928" endWordPosition="1931">nt Semantic Analysis (LSA), a mathematical technique for extracting a very large “semantic space” from large text corpora on the basis of the statistical analysis of the set of co-occurrences in a text corpus. Landauer et al. (1998) stress that this technique can be viewed from two sides. At a theoretical level, it is meant to be used to develop simulations of the cognitive processes running during language comprehension, including, for instance, a computational model of metaphor treatment (Kintsch, 2000 ; Lemaire et al., 2001), but also to analyse the coherence of texts (Foltz et al., 1998 ; Piérard et al., 2004). At a more applied level, it is a technique which enables to infer and to represent the meaning of words on the basis of their actual use in text so that the similarity of the meaning of words, sentences or paragraphs can be estimated (Bestgen, 2002; Choi et al., 2001). It is this latter aspect which draws our attention here. The point of departure of the analysis is a lexical table (Lebart and Salem, 1992) containing the frequencies of every word in each of the documents included in the text material, a document being a text, a paragraph, or a sentence. To derive semantic relations between w</context>
</contexts>
<marker>Piérard, Degand, Bestgen, 2004</marker>
<rawString>Piérard, S., Degand, L., &amp; Bestgen Y. (2004). Vers une recherche automatique des marqueurs de la segmentation du discours. Actes des 7es Journées internationales d’Analyse statistique des Données Textuelles. Louvain-la-Neuve.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pit</author>
</authors>
<title>How to Express Yourself with a Causal Connective. Subjectivity and Causal Connectives in Dutch, German and French.</title>
<date>2003</date>
<publisher>Rodopi.</publisher>
<location>Amsterdam :</location>
<contexts>
<context position="2553" citStr="Pit, 2003" startWordPosition="381" endWordPosition="382">ic procedures to make them analyst-independent. In this paper, we test such a methodology for which we used a number of linguistic hypotheses found in the literature on the semantics of causal connectives and tried to replicate the results. The linguistic material we worked on are four Dutch backward causal connectives: aangezien (&apos;since&apos;), doordat (&apos;because of the fact that&apos;), omdat (&apos;because&apos;) and, want (&apos;because&apos;). This choice was motivated by the fact that there has already been quite some linguistic work on this topic, mainly empirically based (Degand, 2001; Degand and Pander Maat, 2003; Pit, 2003).1 We have shown elsewhere how linguistic hypotheses concerning the scaling of these connectives in terms of subjectivity and their thematic behaviour could be supported (Bestgen et al., 2003). Since these first results are very encouraging, we would like to focus here on the methods of automatic analysis – Latent Semantic Analysis and Thematic Text Analysis - and on the fairly straightforward (annotation) tools needed to perform the semantic analyses, i.e. POS-tagging, lemmatisation and a thesaurus-like thematic dictionary. We illustrate how the combination of the two techniques of automatic </context>
<context position="24117" citStr="Pit 2003" startWordPosition="3933" endWordPosition="3934">. Actually, only omdat displays a higher amount of communiCos. Q &amp; P Cos. Prior Subsequent Mean SD Mean SD 0.143 0.17 0.207 0.21 0.154 0.17 0.187 0.19 0.137 0.17 0.182 0.20 0.120 0.17 0.150 0.19 aangezien (N = 200) doordat (N = 644) omdat (N = 5691) want (N = 3974) aangezien (N = 139) doordat (N = 699) omdat (N = 6747) want (N = 5589) cation markers in Q (F(1, 6746) = 6.53, p &lt; 0.01). While this latter result might seem counter to expectation, it actually goes in the direction of prior observations that omdat-relations frequently display the explicit introduction of speech acts (Degand, 2001; Pit 2003). All together, these results offer new interesting insights into the discourse environment of (Dutch) causal connectives. On the one hand, we have shown with the LSA analysis that the proximity between Q and P is lower for want-relations than for the other connectives and that this is also the case for the semantic proximity between the sentences prior and subsequent to the causal relations. We therefore concluded that the connective want is a marker of thematic shift. On the other hand, the TTA analysis revealed that the Q-segments in want-relations display a higher amount of attitudinal mar</context>
</contexts>
<marker>Pit, 2003</marker>
<rawString>Pit, M. (2003). How to Express Yourself with a Causal Connective. Subjectivity and Causal Connectives in Dutch, German and French. Amsterdam : Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Popping</author>
</authors>
<title>Computer-assisted text analysis.</title>
<date>2000</date>
<publisher>SAGE.</publisher>
<location>London:</location>
<contexts>
<context position="16233" citStr="Popping, 2000" startWordPosition="2636" endWordPosition="2637">oper, the singular value decomposition was realized with the program SVDPACKC (Berry, 1992; Berry et al., 1993), and the 300 first singular vectors were retained. In the present research we will use this technique to evaluate the semantic proximity between P&amp; Q, and between the causal segments and the prior or subsequent sentences. 2.3 Dictionaries and lexical categorisation The second technique used to test the linguistic hypotheses is alternatively called ‘word count strategy’ (Pennebaker et al., 2003), automatic identification of linguistic features (Biber, 1988) or thematic text analysis (Popping, 2000; Stone, 1997), the aim of which is to determine whether some categories of words (e.g., words of opinion, fact, attitude, etc.) or some grammatical categories (e.g. personal pronouns) occur more often in a given type of text segment. The first step in this kind of analysis is to build a dictionary that contains the categories to be investigated and the corresponding (lemmatised) lexical entries that signal their occurrence. The categories may correspond to grammatical classes, but also to thematic word grouping. The following step consists in searching all the text segments containing these l</context>
</contexts>
<marker>Popping, 2000</marker>
<rawString>Popping, R. (2000). Computer-assisted text analysis. London: SAGE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W P M S Spooren</author>
</authors>
<title>Some Aspects of the Form and Interpretation of Global Contrastive Coherence Relations. Unpublished Dissertation,</title>
<date>1989</date>
<location>K.U. Nijmegen.</location>
<contexts>
<context position="21246" citStr="Spooren, 1989" startWordPosition="3450" endWordPosition="3451">able, confirms hypothesis 2 (F(3, 10505) = 25.75, p &lt; 0.0001): the monophonic connectives aangezien, doordat and omdat go along with topic continuity (or at least semantic proximity) between the prior and subsequent sentence to the causal construction, while this is less the case for the connective want. To confirm that these results are indeed related to the issue of perspectivisation, this LSA-analysis was completed with a thematic text analysis to test for the presence vs. absence of perspective indicators. To this end we built a &amp;quot;Perspective&amp;quot; dictionary of perspective-indicating elements (Spooren, 1989) such as intensifiers, emphasisers, attitudinal nouns and adjuncts, etc. (Caenepeel, 1989). The dictionary was composed of two subcategories: a) communication markers, like (nonambiguous) verbs and adverbs of saying and thinking, e.g. report, tell, confirm, require, according to,... b) markers of the speaker&apos;s attitude, like linguistic elements expressing an expectation or a denial of expectation, intensifiers and attitudinals, and evaluative words, e.g. probably, must, horrible, fantastic, ... To build the dictionary, we used a Dutch thesaurus (Brouwers, 1997) and extracted all (unambiguous) </context>
</contexts>
<marker>Spooren, 1989</marker>
<rawString>Spooren, W.P.M.S (1989). Some Aspects of the Form and Interpretation of Global Contrastive Coherence Relations. Unpublished Dissertation, K.U. Nijmegen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Stone</author>
</authors>
<title>Thematic text analysis: New agendas for analyzing text content. In C.W. Roberts (Eds.). Text Analysis for the Social Sciences: Methods for Drawing Statistical Inferences from Texts and Transcripts (pp.35-54).</title>
<date>1997</date>
<publisher>Erlbaum.</publisher>
<location>Mahwah, NJ:</location>
<contexts>
<context position="16247" citStr="Stone, 1997" startWordPosition="2638" endWordPosition="2639">lar value decomposition was realized with the program SVDPACKC (Berry, 1992; Berry et al., 1993), and the 300 first singular vectors were retained. In the present research we will use this technique to evaluate the semantic proximity between P&amp; Q, and between the causal segments and the prior or subsequent sentences. 2.3 Dictionaries and lexical categorisation The second technique used to test the linguistic hypotheses is alternatively called ‘word count strategy’ (Pennebaker et al., 2003), automatic identification of linguistic features (Biber, 1988) or thematic text analysis (Popping, 2000; Stone, 1997), the aim of which is to determine whether some categories of words (e.g., words of opinion, fact, attitude, etc.) or some grammatical categories (e.g. personal pronouns) occur more often in a given type of text segment. The first step in this kind of analysis is to build a dictionary that contains the categories to be investigated and the corresponding (lemmatised) lexical entries that signal their occurrence. The categories may correspond to grammatical classes, but also to thematic word grouping. The following step consists in searching all the text segments containing these lexical entries</context>
</contexts>
<marker>Stone, 1997</marker>
<rawString>Stone, P.J. (1997). Thematic text analysis: New agendas for analyzing text content. In C.W. Roberts (Eds.). Text Analysis for the Social Sciences: Methods for Drawing Statistical Inferences from Texts and Transcripts (pp.35-54). Mahwah, NJ: Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A van den Bosch</author>
<author>W Daelemans</author>
</authors>
<title>Memorybased morphological analysis.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, ACL&apos;99</booktitle>
<pages>285--292</pages>
<publisher>ACL</publisher>
<location>New Brunswick, NJ:</location>
<marker>van den Bosch, Daelemans, 1999</marker>
<rawString>van den Bosch, A., &amp; Daelemans, W. (1999). Memorybased morphological analysis. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, ACL&apos;99 (pp. 285-292). New Brunswick, NJ: ACL</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>