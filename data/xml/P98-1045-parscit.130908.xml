<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000046">
<title confidence="0.96513">
Automatic Semantic Tagging of Unknown Proper Names
</title>
<note confidence="0.7669596">
Alessandro CUCCHIARELLI
Universita di Ancona
Istituto di Informatica
Via Brecce Bianche
60131 Ancona, Italia
</note>
<email confidence="0.971684">
alex@inform.unian.it
</email>
<note confidence="0.9290184">
Danilo LUZI
Universita di Ancona
Istituto di Informatica
Via Brecce Bianche
60131 Ancona, Italia
</note>
<email confidence="0.974607">
luzi@inform.unian.it
</email>
<note confidence="0.8073492">
Paola VELARDI
Universita di Roma &apos;La Sapienza&apos;
Dip. di Scienze dell&apos;Informazione
Via Salaria 113
00198 Roma, Italia
</note>
<email confidence="0.97129">
velardi@dsi.uniromal.it
</email>
<sectionHeader confidence="0.994343" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954170731707">
Implemented methods for proper names
recognition rely on large gazetteers of
common proper nouns and a set of
heuristic rules (e.g. Mr. as an indicator of a
PERSON entity type). Though the
performance of current PN recognizers is
very high (over 90%), it is important to
note that this problem is by no means a
&amp;quot;solved problem&amp;quot;. Existing systems
perform extremely well on newswire
corpora by virtue of the availability of
large gazetteers and rule bases designed
for specific tasks (e.g. recognition of
Organization and Person entity types as
specified in recent Message Understanding
Conferences MUC).
However, large gazetteers are not available
for most languages and applications other
than newswire texts and, in any case,
proper nouns are an open class.
In this paper we describe a context-based
method to assign an entity type to
unknown proper names (PNs). Like many
others, our system relies on a gazetteer and
a set of context-dependent heuristics to
classify proper nouns. However, due to the
unavailability of large gazetteers in Italian,
over 20% detected PNs cannot be
semantically tagged.
The algorithm that we propose assigns an
entity type to an unknown PN based on
the analysis of syntactically and
semantically similar contexts already seen
in the application corpus.
The performance of the algorithm is
evaluated not only in terms of precision,
following the tradition of MUC
conferences, but also in terms of
Information Gain, an information theoretic
measure that takes into account the
complexity of the classification task.
</bodyText>
<sectionHeader confidence="0.953525" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.996072885714286">
In terms of syntactic categories, proper
nouns are lexical NPs that can be formed
by primitive proper names (Adol-
fo_Battaglia), groups of proper nouns of
different semantic categories (San_Paolo
di Brescia), and also of non-proper nouns
(Banca dei regolamenti internazionali). In
the latter case, capital letters are optional,
making the problem of PN items
identification even more complex.
In the literature, it is accepted that an
adequate treatment of proper nouns
requires the use of a context-sensitive
grammar (McDonald, 1996). McDonald
points out that the context sensitivity
requirement involves two complementary
types of evidence: internal and external.
The internal evidence, can be derived from
the sequence of. words in a text (proper
nouns and trigger words, such as Inc., &amp;,
Ltd., Company, etc.), and is gained in
almost all state-of-art PNs recognisers by
the use of large gazetteers and lists of
trigger words.
The external evidence is the context of a
proper noun, that provides classificatory
criteria to reinforce internal evidence, if
any, or supplies some classificatory
evidence. In fact, proper names form an
open class, making the incompleteness of
gazetteers an obvious problem.
The methods for recognition of proper
nouns (PNs) described in literature closely
reflects this view of the problem.
PN identification typically includes:
</bodyText>
<listItem confidence="0.996367555555555">
• a gazetteer lookup, which locates simple
and complex nominals identifying
common PNs, such as companies,
person names, locations, etc.
• a set of patterns or rules, stated in terms
of part-of-speech, syntactic or lexical
features (e.g. Mr. as an indicator of a
PERSON entity type), orthographic
features (e.g. capitalization), etc.
</listItem>
<page confidence="0.997598">
286
</page>
<bodyText confidence="0.999873131147541">
Proper nouns recognition has recently
attracted much attention especially in the
area of Information Extraction, where this
problem is known as the Named Entity
recognition task. The highest performing
systems include large numbers of hand-
coded rules, or patterns, such as VIE
(Humphreys et al. 1996), the UMass
system (Fisher et al. 1997) and Proteus
(Grishman et al. 1992), but lately a high
performance has been obtained by the use
of statistical methods. For example,
Nymble (Bikel et al. 1997) learns names
using a trained approach based on a
variant of Hidden Markov Models.
However, a 90% success rate is reached at
the price of tagging manually around half
a million words. Since PNs are mostly
domain-specific, presumably a comparable
effort is needed when shifting to different
domains.
High performances of the existing systems
are by no means the result of many years
of studies and research in the area of IE
from newswire English texts, promoted
and funded by the Message Understanding
Conferences (MUC) organizers. Yet, there
is no evidence that a similar performance
could be obtained in other languages and
domains, if not at the price of a similar
effort for rule writing (or manual training),
and for the compilation of a high-
coverage gazetteer. A recent study (Palmer
and Day, 1997) established that the
baseline performances of the PN
recognition task for several languages and
application domains vary between 34%
and 71%. The lower bound is calculated
by considering a simple algorithm that
recognizes PNs on the basis of a list of
frequent proper nouns seen in a training
set.
The method we propose in this paper
combines symbolic and statistical
approaches to classify unknown PNs using
context evidence previously extracted
from the application corpus. The method
can be used to overcome the limitation of
small gazetteers and poorly encoded rule
bases.
Our method is untrained: what is needed is
a learning (raw) corpus, a surface syntactic
analyzer, a dictionary of synonyms, a list
of category names for classifying PNs (we
used the categories proposed in the
forthcoming MUC-7), and a &amp;quot;start-up&amp;quot;
gazetteer and rule base, used to acquire an
initial model of typical PNs contexts.
In the next section, we describe the method
in detail. Section 3 is dedicated to a
discussion of experimental results.
</bodyText>
<sectionHeader confidence="0.949719" genericHeader="method">
2 The Method
</sectionHeader>
<bodyText confidence="0.996532538461539">
The problem of PN recognition has been
considered in our group in the context of
the European project ECRAN, aimed at
improving domain adaptability of IE
systems through the integrated use of
corpora and MRDs.
A first version of the Named Entity (NE)
recognizer, in Italian, closely reproduced
the architecture of the VIE recognizer,
developed at the University of Sheffield
(Humphreys et al. 1996).
Proper noun recognition is initially
performed in two steps:
</bodyText>
<listItem confidence="0.848722">
1) common proper nouns are identified
using a gazetteer, structured in files
</listItem>
<bodyText confidence="0.706767">
and related lists of trigger words for
each proper nouns category (e.g.
&amp;quot;Gulf&amp;quot; for LOCATIONs, or
&amp;quot;Association&amp;quot; for ORGANIZATIONs);
</bodyText>
<listItem confidence="0.916558333333333">
2) a context-sensitive grammar of about
250 rules is used to parse proper
nouns in contexts. The majority of
</listItem>
<bodyText confidence="0.8374188">
rules uses internal evidence to identify
and classify proper nouns made of
complex NPs. For example the
following rule is used to recognize
street names:
</bodyText>
<equation confidence="0.8753994">
rule(tagged_location_np(s_form:[via,&amp;quot;,F2
,&amp;quot;,F.3],sem:A^13),
[nome(s_form:via,sem:_^_),
organ_names_np(s_form:F2,sem:_^_),
num(s_form:F3)])
</equation>
<bodyText confidence="0.9759235">
Ex: &amp;quot;via Giorgio Marini 34 &amp;quot;.
When running these first two modules on a
one million word corpus of economic
news (extracted from the newspaper II Sole
24 Ore), we obtained the following
performances: 84% precision, 85% recall,
about 20% proper nouns correctly
identified as such, but NOT classified.
Unknown proper nouns are identified
initially by the Brill part-of-speech tagger
(Brill, 1995). Complex unknown nominals
(e.g. Quick Take 200) are partly detected
by simple heuristics.
One of the motivations for such a high
percentage of unknowns and relatively low
performance (as compared with state-of-
art PN recognizers) is that at the present
state of implementation the gazetteer has a
</bodyText>
<page confidence="0.976573">
287
</page>
<bodyText confidence="0.983088982142857">
limited coveragel ; yet, the problem of
unknowns is generally recognized as
crucial in real-world applications, because
proper nouns are an open class.
We have therefore devised a method to
reinforce external evidence, using a
corpus-driven algorithm to incrementally
update the gazetteer and classification of
unknown PNs in running texts.
The algorithm to classify unknown proper
nouns uses the following linguistic
resources: a (raw text) learning corpus in
the same domain as the application, a
shallow corpus parser, a &amp;quot;seed&amp;quot; gazetteer,
and a dictionary of synonyms.
The shallow parser (Basili et al. 1994),
extracts from the learning corpus
elementary syntactic relations such as
subject-object, noun-preposition-noun, etc.
A syntactic link (hereafter esl) is
represented as:
esli(wj, mod(lYPebwk))
where w• is the head word, wk is the
J
modifier, and typei is the type of syntactic
relation (e.g. PP(of), PP(for), SUBJ-Verb,
Verb-DirectObject, etc.).
The learning corpus is previously
morphologically and syntactically
processed. Step 1 and 2 described at the
beginning of this section are used to detect
PNs. A database of esls including known
PNs2 is then created and used by the
algorithm to assign a category to unknown
PNs. The algorithm works as follows:
let PN_U be an unknown proper noun, i.e.
a single word or a complex nominal. Let
Cpn (Ci, Cpn2, CpnN) be the set of
semantic categories for proper nouns (e.g.
Person, Organization, Product etc.).
Finally, let ESL be the set of elementary
syntactic links (esl) extracted from the
1 The context sensitive grammar closely reflects,
with extension, that developed for a similar
application in the English VIE system.
Therefore, low performance is likely due to the
low-coverage gazzetteer. The absence of available
linguistic resources in languages other than
English is a well known problem.
2Note that the database is not manually inspected
for correctness (POS tagging and parsing errors).
However, the parser assigns to each detected es1 a
statistical measure of confidence, called
plausibility (Basili et al. 1994b).
learning corpus that include PN_U as one
of its arguments.
</bodyText>
<equation confidence="0.853748">
For each esli in ESL let:
esli(wj,mod(typei,wk))=esli(x,PN_U)
</equation>
<bodyText confidence="0.998534333333333">
where x=wj or wk and PN_U =wk or wj,
typei is the syntactic type of esl (e.g. N-di-
N, N_N, V-per-N ecc), and further let:
</bodyText>
<equation confidence="0.722275">
pl(esli (x, PN_U)
</equation>
<bodyText confidence="0.99936821875">
be the plausibility of a detected esl. The
plausibility is a measure of the statistical
evidence of a detected syntactic link (Basili
et al, 1994b), that depends upon local (i.e.
at the sentence level) syntactic ambiguity
and global corpus evidence.
Finally, let:
ESLA be a set of esls defined as follows:
for each esli(x,PN_U) in ESL put in
ESLA the set of es1j(x,PND, in the
corpus, with type=typei, x in the same
position of esli, and PNi a known proper
noun, in the same position as PN_U in
esli,
ES L B be the set of eslk defined as
follows: for each esli(x,PN_U) in ESL
put in ESLB the set of es1j(w,PND, in the
corpus, with type=typei, w in the same
position of x in esli, Sim(w,x)&gt; 8, and
PNj a known proper noun, in the same
position as PN_U in esli. Sim(w,x) is a
similarity measure between x and w. In
our first experiments, Sim(w,x)&gt; 5 iff w
is a synonym of x.
For each semantic category Cpnj compute
evidence(Cpnp as shown in Figure 1,
where:
- amb(esl(x, PN)) is a measure of the
ambiguity of x and PNj in esli;
— a and f are experimentally determined
weights (currently, a=0.7 and 13=0.3).
The selected category for PN_U is:
</bodyText>
<equation confidence="0.480031">
C=argmax(evidence(Cpna=maxj(evidence(Cpnj))
</equation>
<bodyText confidence="0.99988">
The underlying hypothesis is that, in a
given application corpus, a PN has a
unique sense. This is a reasonable
restriction supported by empirical
evidence (see also (Gale et al. 1992)). An
alternative solution would be to select the
&amp;quot;best performing&amp;quot; tags, and then apply
</bodyText>
<page confidence="0.979635">
288
</page>
<table confidence="0.993146">
(1)evidence(Cpn3) I(pl(esli(x,PNi))* amb(esli(x,PNi)))
es1 ,€ESL A ,C(PN,)=C
Ipl(esli(x,PAri)
esI,EESL A AnyPN
(pl(esl i(w,PN j))* amb (esl i(x,PN j)))
13 es!, EESL B ,C(PN i)=C
,(w,PN j)
est, EESL B ,anvPN
</table>
<figureCaption confidence="0.673263">
Figure 1 - The evidence(C) computation formula
</figureCaption>
<bodyText confidence="0.61548">
some WSD algorithm to predict the precise
sense in running texts.
</bodyText>
<sectionHeader confidence="0.969133" genericHeader="method">
3 Discussion of the Experiment
</sectionHeader>
<bodyText confidence="0.999656875">
In our experiment, we used a corpus of
one million words extracted from articles
in the II Sole 24 Ore economic newspaper.
A database of 76055 esls including proper
nouns was obtained.
Table 1 shows the distribution of esls by
category, and the prior probability (i.e.
relative distribution) of each category.
</bodyText>
<table confidence="0.999701636363636">
Category N° ESLi Prior Prob.
ORGANIZ 26418 0.347
LOCATION 25087 0.330
PERSON 20558 0.270
DATE 544 0.007
TIME 879 0.011
MONEY 1076 0.014
PERCENT 520 0.007
PRODUCT 2671 0.035
OTHERS 1112 0.015
Tot. ESL 76055
</table>
<tableCaption confidence="0.99943">
Table 1 - PN distribution by category
</tableCaption>
<bodyText confidence="0.9970185">
The semantic categories in Table 1, with
the addition of Product, are those that will
be used for Named Entity task evaluation
in the forthcoming MUC-7 contest.
In Figure 2, a complete experiment is
reported. In the figure, an esl is
represented as a list, for example (0.5
G_N_P_N Quick_Take_200 0 1 in
documento). The detected es1 is
&apos;Quick_Take_200 in documento&apos;
(Quick_Take_200 in document), the
syntactic type is G_N_P_N (noun-
preposition-noun), the plausibility is 0.5,
the initial category of Quick_Take_200 is
0 (= unknown) and its ambiguity is
initially set to 1.
It is seen in the figure that some detected
esls do not contribute to the computation
of (1) (e.g. acquisire con
Quick_Take_200 to acquire with
Quick_Take_200) while some other esl
turns out to be particularly informative
(e.g. qualita&apos; di Quick_Take_200 quality
of Quick_Take_200)
For the name Quick_Take_200 (a software
product), the category 8 is finally selected
(PRODUCT, as shown in the figure).
An extended experiment was designed as
follows:
We selected from the corpus 35 PNs for
each of the following categories:
Organization, Person, Location and
Product3. The PNs are selected by ranges
of frequency in the corpus, except for
Producs, that are very rare in our excerpt
of the Il Sole 24 Ore: here we selected the
35 top frequency PNs.
We then removed each of the 140 PNs
from the gazetteer, one at the time, and
attempted a re-classification using our
algorithm.
To evaluate the performances we used, in
addition to the classical Precision measure,
the Information Gain (Kononenko and
Bratko, 1991).
The Information Gain is an information-
theoretic measure that takes into account
the complexity of the classification task.
</bodyText>
<footnote confidence="0.989516">
3The other categories are less interesting in our
view. Numbers, dates etc. are recursive and
regular phenomena that can be detected in a more
general way by the use of specific grammars or
pattern matchers.
</footnote>
<page confidence="0.994184">
289
</page>
<table confidence="0.961238155555556">
PROPER MNE: Quick Take 200
0.5 G N P_N Qiick 200 0 1 in docananto
1.0 G N V Quick Take 200 0 1 nil dotare
ESI.1: 0.333000 G N_P N grande di. Weil 3 1
PLB 0.250000 G_N_P_N grannie di. Europa 2 1
EET. 0.2 G N P_N grande di Casa 1 1
ESLE 1.0 G_N V Apple 1 1 nil fornire 0.333000 G_N_P N qualita&apos; di Qtack Take 200 0 1
1 1
ESLE 1.0 UN V Power Pc 1 1 nil fornire ESLA= 1.0 G_N P N qualita&apos; di Elm 8 1
ELBZ 0.333000 G_N_P N aorta di In 1 1
ESL= 1.0 G_N P N generazione di G 3 1
ESLBr 0.125000 G N_P_N caratteristica di cAqa
ESLI 0.250000 G_N P N caratteristica di
ESL, 1.0 G_N V Tank Francaise Chronoreflex 8 1
nil esintaggiare
0.1 G Agg_P_N thquisito cm Quick Take 200 0 1
Macintosh Perforrna 8 1
FSL 0.250000
8 1
GNP N caratteristica di Vs
EISL) 0.5 G_N_P_N marca di Arese 2 1
0.1 G_pp P N acquisire con Quick Take 200 0 1
0.1 G_V_P_N acquisire con Quick Take 200 0 1
0.333000 G IN_P_N Forza di Qaick Tala 200 0 1
0.2 GVPN utilizzare com Quick Take 200 0 1
ESLA= 0.333000 qL]g p_p Forza di Linea Pret 2 1
Coefficient a: 0.7
0.333000 G N P_N Punti di Qiick Take 200 0 1 Coefficient p: 0.3
MDI CLASS SDNLEELA. SUM ESLB EVIDENCE
- --
0.333000 G_N P N acquisizione di. Quick 200 0 1 1 ORG 0.000 2.658 0.109
2. LOC 0.333 0.750 0.205
3 PERSON 0.000 1.666 0.068
0.333000 G YUJI raparita di cuic3c_Take 200 0 1 4 CATE 0.000 0.000 0.000
5 TIME 0.000 0.000 0.000
ES1.1 0.167000 G_N P N portata di 280 Kg 9 1 6 MCNEY 0.000 0.000 0.000
F:SL,1 0.2 G_N P N partata di 300_I&lt;g 9 1
7 PERCENT 0.000 0.000 0.000
FMB= 0.333000 G N P_N rrezzo di C2xrtier 3 1
8 PRCCUCT 1.000 1.833 0.600
ES:LB= 0.333000 GNPN facilita&apos; di Apple Share 8 1 9 OTHERS 0.000 0.367 0.015
SUM ESLA 1.333 SLILESLE 7.274
0.333000 G_N pjg imagine di Quickjlake 200 0 1
Max evidence category is: PRCET.MT
0.333000 GNPN inportante di Wick Tala 200 0 1 Selected category: PROM=
</table>
<figureCaption confidence="0.957111">
Figure 2 - A complete example
</figureCaption>
<bodyText confidence="0.99635">
If P(C) is the prior (a-priori) probability4
that an instance c is a member of class C,
and P&apos;(C) is the probability of c E C, as
computed by the classifier in a given test
the Information Gain I(ti) is defined as:
</bodyText>
<equation confidence="0.935005">
I(ti) = log(1-P(C)) - log(1-P&apos;(C))
if P(C) &gt; Pi(C)
or
I(ti) = log(P&apos;(C)) - log(P(C))
if Pt(C) &gt; P(C)
</equation>
<bodyText confidence="0.5853955">
That is, if the classification is wrong, I(ti) is
a penalty as high as the classification task
</bodyText>
<tableCaption confidence="0.964437">
4The prior probability can be easily computed in
a learning set as the ratio between the number of
training instances belonging to a class C and the
total number of training instances. In our
experiment, the prior probabilities are listed in
Table 1.
</tableCaption>
<bodyText confidence="0.982457166666667">
was an easy one (i.e. the prior probability
of C was high). If the classification is
correct, I(ti) is a price as high as the
classification task was complex (i.e. the
prior probability of C was low).
Over a test set of T cases, I is given by:
</bodyText>
<equation confidence="0.993720333333333">
1 T
I = -El(ti)
T_.1
</equation>
<bodyText confidence="0.998687545454545">
Table 2 illustrates the results. It is seen that
unknown PNs in the three major categories
(those for which there is evidence in the
corpus and in the gazetteer) have a very
high probability of being correctly
classified (up to 100% for Organizations).
On the contrary, we obtain poor
performances with Products.
However, Product is interesting because:
- there are no more than 50-60 product
names in the gazetteer (which we
</bodyText>
<page confidence="0.986146">
290
</page>
<bodyText confidence="0.999662047619048">
manually added for the purpose of this
experiment)
- there are no contextual rules for
Products in the context-sensitive
grammar.
Thus, both prior probability and prior
knowledge on Products are close to zero.
This is numerically evidenced by the
Information Gain: though we are not
learning much about Products, the
Information Gain is higher than for the
other categories, and also as an absolute
value (in (Kononenko and Bratko, 1991) a
0,5 bit improvement is among the highest
measured values in a comparative
experiment). In addition, the relative
precision of classifying PNs as Product is
100%. This means that most products are
misclassified, but, if something is classified
as Product, this information can be reliably
used to enrich the gazetteer.
</bodyText>
<table confidence="0.9310124">
Category Precision Inf. Gain
ORGANIZ. 100.00% 0.11
LOCATION. 91.43% 0.14
PERSON 80.00% 0.23
PRODUCT 22.86% 0.65
</table>
<tableCaption confidence="0.864052">
Table 2 - Precision and Information Gain
of the method
Table 3 reports an experiment on a small
corpus extracted from another portion of
Il Sole 24 Ore, indexed as &amp;quot;New Products&amp;quot;.
</tableCaption>
<table confidence="0.996595923076923">
Category N° ESL; Prior Prob.
ORGANIZ 735 0.160
LOCATION 583 0.126
PERSON 902 0.196
DATE 7 0.001
TIME 8 0.001
MONEY 31 0.007
PERCENT 114 0.025
PRODUCT 2184 0.473
OTHERS 262 0.057
Tot. ESL 4615
Precision Inf. Gain
PRODUCT 88.57% 0.12
</table>
<tableCaption confidence="0.9623315">
Table 3 - Experiment with a small &amp;quot;New
Product&amp;quot; Corpus
</tableCaption>
<bodyText confidence="0.9999775">
Here, the prior probability of Products is
obviously higher, though -due to the poor
gazetteer- there is an elevated number of
unrecognized products.
In this corpus we selected and then
removed 35 product names, and now the
system correctly classifies 31. Notice that
in this experiment the gazetteer and the PN
grammar are the same as before, The only
difference is that the corpus provides more
evidence (contexts) concerning those
products that have been recognized as
such. Notice on the other side, that the
Information Gain now is very low.
</bodyText>
<sectionHeader confidence="0.998178" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.989728347826087">
Our current implementation of a PN
analyzer still has a limited performance,
caused by a variety of problems that range
from unsatisfactory performance of state-
of-art POS taggers in inflected languages,
to limited availability of linguistic
resources,in Italian, such as PN gazetteers.
The algorithm that we propose has indeed
the purpose of overcoming limitations of
gazetteers and manually defined
contextual rules for PN recognition. In
(Cucchiarelli et al. 1998) we also show
how to extend our method to
incrementally update the initial gazzeteer.
The performance of the proposed
algorithm is more than satisfactory. A
comparison with existing systems is
difficult because in the literature global PN
recognition performances are reported,
without considering the semantic
classification of unknowns as a subtask.
The only exception is in (Wacholder et al,
1997) where the reported performance for
the sole semantic disambiguation task of
PNs is 79%. In that paper, however,
semantic disambiguation is performed
among a lower number of classes5.
The performance of our system is clearly
affected by the dimension of the initial
seed gazetteer and contextual rules. If the
sets ESLA and ESLB are large enough,
obviously more examples of similar
contexts are found, even for unknown PNs
with a single occurrence.
In our test experiment, we always managed
to find at least one or two similar contexts
of an unknown PN, but in some cases they
were misleading and caused a wrong
classification, especially for Products.
However, it may be possible to increase the
evidence provided by the set ESLB by
including contexts in which the words are
50ne of the advantages of Information Gain is
that, if widely adopted, this measure facilitates
the comparison among learning methods with
different complexity of the classification task.
</bodyText>
<page confidence="0.991038">
291
</page>
<bodyText confidence="0.9999893125">
not strictly synonyms, but belong to the
same semantic category.
One such experiment requires a word
taxonomy, like for example WordNet.
WordNet is currently unavailable in Italian
(the first known results of the
EuroWordNet project are too preliminary),
therefore we plan to reproduce our
experiment in English.
Another strategy to improve performances
in absence of a substantial evidence is the
definition of general (not contextual) rules
to capture unknown complex nominals.
For example, looking at the Product
experiment in more detail, we found that
product names are often formed by very
complex nominals, e.g. Fiat- Marea
Weekend 2000 (the name of a car model).
Capturing complex nominals in absence of
anchors and specific contextual rules (here
the only anchor is Fiat, which appears in
the gazetteer as an Organization name)
may be difficult, and if a complex nominal
is not captured as a unit, the resulting
syntactic context may be misleading (e.g.
N_ADJ(Fiat_Marea_Weekend, 2000)).
We believe that finding class-independent
heuristics for capturing complex nominals
is a more &amp;quot;general&amp;quot; way of improving the
performance of the method, rather than
adding specific rules for specific entity
types and enriching the gazetteer.
</bodyText>
<sectionHeader confidence="0.998436" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999698">
The authors would like to thank Mr. Enzo
Peracchia for his support in the software
developent and for aiding with experi-
ments. This research has been funded
under the EC project ECRAN LE-2110.
</bodyText>
<sectionHeader confidence="0.998985" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999821367647059">
Basili, R., Pazienza M.T., Velardi P. (1994) A
(not-so) shallow parser for collocational analy-
sis. Proc. of Coling &apos;94, Kyoto, Japan, 1994.
Basili, R., Marziali A., Pazienza M.T. (1994b)
Modelling syntax uncertainty in lexical acqui-
sition from texts. Journal of Quantitative Lin-
guistics, vol.1, n.1, 1994.
Bike! D.,Miller S., Schwartz R. and Weischedel
R. (1997) Nymble: a High-Performance Learn-
ing Name-finder, in proc. of 5th Conference on
Applied natural Language Processing, Wash-
ington, 1997
Brill, E (1995). Transformation-based Error-
Driven Learning and Natural Language Pro-
cessing: A case study of Part of Speech Tag-
ging. Computational Linguistics, vol. 21, n.
24, 1995
Cucchiarelli A., Luzi D., Velardi P. Using
Corpus evidence for Automatic Gazetteer
Extension in Proc. of first Language Resources
and Evaluation, Granada, May 1988
ECRAN: Extraction of Content: Research at
Near Market. http://www2.echo.lu/langeng/en/
lel/ecran/ecran.html
Fisher D., Soderland S., McCarthy J., Feng F.
and Lenhart W. (1996) Description of the
UMass system as used for MUC-6.
http://ciir.cs.umass.edu/info/psfiles/tepubs/tepu
bs.html
Gale, Church W. K. and Yarowsky D.(1992)
One sense per discourse. in Proc. of the
DARPA speech and and Natural Language
workshop, Harriman, NY, February 1992
Grislunan R., Macleod C. and Meyers A. (1992)
NYU: description of the Proteus System as
used for MUC-4. in Proc. of Fourth Message
Understanding Conference (MUC-4) June 1992
Humphreys (1996) VIE Technical Specifications,
1996/10/1815. ILASH, University of
Sheffield.
Kononenko I. and Bratko I. (1991) Information-
based Evaluation Criterion for Classifier&apos;s Per-
formance. Machine Learning 6, pp. 67-80,
1991
Mani I., McMillian R., Luperfoy S., Lusher E.,
Laskowski S. (1996) Identifying Unknown
Proper Names in Newswire Text. in Corpus
Processing for Lexical Acquisition, J. Puste-
jovslcy and B. Boguraev Eds., MIT Press 1996.
McDonald D. (1996) Internal and External Evi-
dence in the Identification and Semantic Cate-
gorization of Proper Names. in Corpus Pro-
cessing for Lexical Acquisition, J. Pustejovsky
and B. Boguraev Eds., MIT Press 1996.
Paik W., Liddy E., Yu E. and McKenna M.
(1996) Categorizing and standardizing proper
nouns for effcient Information Retrieval. in
Corpus Processing for Lexical Acquisition, J.
Pustejovslcy and B. Boguraev Eds., MIT Press
1996.
Palmer D. and Day D. (1997) A Statistical Pro-
file of the Named Enity Task in Proc. of 5th
Conference on Applied natural Language Pro-
cessing, Washington, 1997
Wacholder N., Ravin Y. and Choi M. (1997)
Disambiguation of Proper Names in Text. in
Proc. of 5th Conference on Applied natural
Language Processing, Washington, 1997
</reference>
<page confidence="0.997475">
292
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.284486">
<title confidence="0.999901">Automatic Semantic Tagging of Unknown Proper Names</title>
<author confidence="0.998408">Alessandro CUCCHIARELLI</author>
<affiliation confidence="0.967167666666667">Universita di Ancona Istituto di Informatica Via Brecce Bianche</affiliation>
<address confidence="0.999872">60131 Ancona, Italia</address>
<email confidence="0.995392">alex@inform.unian.it</email>
<affiliation confidence="0.85666075">Danilo LUZI Universita di Ancona Istituto di Informatica Via Brecce Bianche</affiliation>
<address confidence="0.999872">60131 Ancona, Italia</address>
<email confidence="0.99649">luzi@inform.unian.it</email>
<author confidence="0.972112">Paola VELARDI</author>
<affiliation confidence="0.9821435">Universita di Roma &apos;La Sapienza&apos; Dip. di Scienze dell&apos;Informazione</affiliation>
<address confidence="0.8619555">Via Salaria 113 00198 Roma, Italia</address>
<email confidence="0.995558">velardi@dsi.uniromal.it</email>
<abstract confidence="0.999746095238095">Implemented methods for proper names recognition rely on large gazetteers of common proper nouns and a set of rules (e.g. an indicator of a PERSON entity type). Though the performance of current PN recognizers is very high (over 90%), it is important to note that this problem is by no means a &amp;quot;solved problem&amp;quot;. Existing systems perform extremely well on newswire corpora by virtue of the availability of large gazetteers and rule bases designed for specific tasks (e.g. recognition of Organization and Person entity types as specified in recent Message Understanding Conferences MUC). However, large gazetteers are not available for most languages and applications other than newswire texts and, in any case, proper nouns are an open class. In this paper we describe a context-based method to assign an entity type to unknown proper names (PNs). Like many others, our system relies on a gazetteer and a set of context-dependent heuristics to classify proper nouns. However, due to the unavailability of large gazetteers in Italian, over 20% detected PNs cannot be semantically tagged. The algorithm that we propose assigns an entity type to an unknown PN based on analysis of and contexts already seen in the application corpus. The performance of the algorithm is evaluated not only in terms of precision, following the tradition of MUC conferences, but also in terms of Information Gain, an information theoretic measure that takes into account the complexity of the classification task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>M T Pazienza</author>
<author>P Velardi</author>
</authors>
<title>A (not-so) shallow parser for collocational analysis.</title>
<date>1994</date>
<booktitle>Proc. of Coling &apos;94,</booktitle>
<location>Kyoto, Japan,</location>
<contexts>
<context position="8425" citStr="Basili et al. 1994" startWordPosition="1294" endWordPosition="1297">s a 287 limited coveragel ; yet, the problem of unknowns is generally recognized as crucial in real-world applications, because proper nouns are an open class. We have therefore devised a method to reinforce external evidence, using a corpus-driven algorithm to incrementally update the gazetteer and classification of unknown PNs in running texts. The algorithm to classify unknown proper nouns uses the following linguistic resources: a (raw text) learning corpus in the same domain as the application, a shallow corpus parser, a &amp;quot;seed&amp;quot; gazetteer, and a dictionary of synonyms. The shallow parser (Basili et al. 1994), extracts from the learning corpus elementary syntactic relations such as subject-object, noun-preposition-noun, etc. A syntactic link (hereafter esl) is represented as: esli(wj, mod(lYPebwk)) where w• is the head word, wk is the J modifier, and typei is the type of syntactic relation (e.g. PP(of), PP(for), SUBJ-Verb, Verb-DirectObject, etc.). The learning corpus is previously morphologically and syntactically processed. Step 1 and 2 described at the beginning of this section are used to detect PNs. A database of esls including known PNs2 is then created and used by the algorithm to assign a </context>
<context position="9896" citStr="Basili et al. 1994" startWordPosition="1522" endWordPosition="1525">Finally, let ESL be the set of elementary syntactic links (esl) extracted from the 1 The context sensitive grammar closely reflects, with extension, that developed for a similar application in the English VIE system. Therefore, low performance is likely due to the low-coverage gazzetteer. The absence of available linguistic resources in languages other than English is a well known problem. 2Note that the database is not manually inspected for correctness (POS tagging and parsing errors). However, the parser assigns to each detected es1 a statistical measure of confidence, called plausibility (Basili et al. 1994b). learning corpus that include PN_U as one of its arguments. For each esli in ESL let: esli(wj,mod(typei,wk))=esli(x,PN_U) where x=wj or wk and PN_U =wk or wj, typei is the syntactic type of esl (e.g. N-diN, N_N, V-per-N ecc), and further let: pl(esli (x, PN_U) be the plausibility of a detected esl. The plausibility is a measure of the statistical evidence of a detected syntactic link (Basili et al, 1994b), that depends upon local (i.e. at the sentence level) syntactic ambiguity and global corpus evidence. Finally, let: ESLA be a set of esls defined as follows: for each esli(x,PN_U) in ESL p</context>
</contexts>
<marker>Basili, Pazienza, Velardi, 1994</marker>
<rawString>Basili, R., Pazienza M.T., Velardi P. (1994) A (not-so) shallow parser for collocational analysis. Proc. of Coling &apos;94, Kyoto, Japan, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Basili</author>
<author>A Marziali</author>
<author>M T Pazienza</author>
</authors>
<title>Modelling syntax uncertainty in lexical acquisition from texts.</title>
<date>1994</date>
<journal>Journal of Quantitative Linguistics,</journal>
<volume>1</volume>
<pages>1</pages>
<contexts>
<context position="8425" citStr="Basili et al. 1994" startWordPosition="1294" endWordPosition="1297">s a 287 limited coveragel ; yet, the problem of unknowns is generally recognized as crucial in real-world applications, because proper nouns are an open class. We have therefore devised a method to reinforce external evidence, using a corpus-driven algorithm to incrementally update the gazetteer and classification of unknown PNs in running texts. The algorithm to classify unknown proper nouns uses the following linguistic resources: a (raw text) learning corpus in the same domain as the application, a shallow corpus parser, a &amp;quot;seed&amp;quot; gazetteer, and a dictionary of synonyms. The shallow parser (Basili et al. 1994), extracts from the learning corpus elementary syntactic relations such as subject-object, noun-preposition-noun, etc. A syntactic link (hereafter esl) is represented as: esli(wj, mod(lYPebwk)) where w• is the head word, wk is the J modifier, and typei is the type of syntactic relation (e.g. PP(of), PP(for), SUBJ-Verb, Verb-DirectObject, etc.). The learning corpus is previously morphologically and syntactically processed. Step 1 and 2 described at the beginning of this section are used to detect PNs. A database of esls including known PNs2 is then created and used by the algorithm to assign a </context>
<context position="9896" citStr="Basili et al. 1994" startWordPosition="1522" endWordPosition="1525">Finally, let ESL be the set of elementary syntactic links (esl) extracted from the 1 The context sensitive grammar closely reflects, with extension, that developed for a similar application in the English VIE system. Therefore, low performance is likely due to the low-coverage gazzetteer. The absence of available linguistic resources in languages other than English is a well known problem. 2Note that the database is not manually inspected for correctness (POS tagging and parsing errors). However, the parser assigns to each detected es1 a statistical measure of confidence, called plausibility (Basili et al. 1994b). learning corpus that include PN_U as one of its arguments. For each esli in ESL let: esli(wj,mod(typei,wk))=esli(x,PN_U) where x=wj or wk and PN_U =wk or wj, typei is the syntactic type of esl (e.g. N-diN, N_N, V-per-N ecc), and further let: pl(esli (x, PN_U) be the plausibility of a detected esl. The plausibility is a measure of the statistical evidence of a detected syntactic link (Basili et al, 1994b), that depends upon local (i.e. at the sentence level) syntactic ambiguity and global corpus evidence. Finally, let: ESLA be a set of esls defined as follows: for each esli(x,PN_U) in ESL p</context>
</contexts>
<marker>Basili, Marziali, Pazienza, 1994</marker>
<rawString>Basili, R., Marziali A., Pazienza M.T. (1994b) Modelling syntax uncertainty in lexical acquisition from texts. Journal of Quantitative Linguistics, vol.1, n.1, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bike</author>
<author>S Miller</author>
<author>R Schwartz</author>
<author>R Weischedel</author>
</authors>
<title>Nymble: a High-Performance Learning Name-finder,</title>
<date>1997</date>
<booktitle>in proc. of 5th Conference on Applied natural Language Processing,</booktitle>
<location>Washington,</location>
<marker>Bike, Miller, Schwartz, Weischedel, 1997</marker>
<rawString>Bike! D.,Miller S., Schwartz R. and Weischedel R. (1997) Nymble: a High-Performance Learning Name-finder, in proc. of 5th Conference on Applied natural Language Processing, Washington, 1997</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>Transformation-based ErrorDriven Learning and Natural Language Processing: A case study of Part of Speech Tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<volume>21</volume>
<contexts>
<context position="7513" citStr="Brill, 1995" startWordPosition="1155" endWordPosition="1156">complex NPs. For example the following rule is used to recognize street names: rule(tagged_location_np(s_form:[via,&amp;quot;,F2 ,&amp;quot;,F.3],sem:A^13), [nome(s_form:via,sem:_^_), organ_names_np(s_form:F2,sem:_^_), num(s_form:F3)]) Ex: &amp;quot;via Giorgio Marini 34 &amp;quot;. When running these first two modules on a one million word corpus of economic news (extracted from the newspaper II Sole 24 Ore), we obtained the following performances: 84% precision, 85% recall, about 20% proper nouns correctly identified as such, but NOT classified. Unknown proper nouns are identified initially by the Brill part-of-speech tagger (Brill, 1995). Complex unknown nominals (e.g. Quick Take 200) are partly detected by simple heuristics. One of the motivations for such a high percentage of unknowns and relatively low performance (as compared with state-ofart PN recognizers) is that at the present state of implementation the gazetteer has a 287 limited coveragel ; yet, the problem of unknowns is generally recognized as crucial in real-world applications, because proper nouns are an open class. We have therefore devised a method to reinforce external evidence, using a corpus-driven algorithm to incrementally update the gazetteer and classi</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Brill, E (1995). Transformation-based ErrorDriven Learning and Natural Language Processing: A case study of Part of Speech Tagging. Computational Linguistics, vol. 21, n. 24, 1995</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Cucchiarelli</author>
<author>D Luzi</author>
<author>P Velardi</author>
</authors>
<title>Using Corpus evidence for Automatic Gazetteer Extension in</title>
<date>1988</date>
<booktitle>Proc. of first Language Resources and Evaluation,</booktitle>
<location>Granada,</location>
<note>http://www2.echo.lu/langeng/en/ lel/ecran/ecran.html</note>
<marker>Cucchiarelli, Luzi, Velardi, 1988</marker>
<rawString>Cucchiarelli A., Luzi D., Velardi P. Using Corpus evidence for Automatic Gazetteer Extension in Proc. of first Language Resources and Evaluation, Granada, May 1988 ECRAN: Extraction of Content: Research at Near Market. http://www2.echo.lu/langeng/en/ lel/ecran/ecran.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Fisher</author>
<author>S Soderland</author>
<author>J McCarthy</author>
<author>F Feng</author>
<author>W Lenhart</author>
</authors>
<title>Description of the UMass system as used for MUC-6. http://ciir.cs.umass.edu/info/psfiles/tepubs/tepu bs.html</title>
<date>1996</date>
<marker>Fisher, Soderland, McCarthy, Feng, Lenhart, 1996</marker>
<rawString>Fisher D., Soderland S., McCarthy J., Feng F. and Lenhart W. (1996) Description of the UMass system as used for MUC-6. http://ciir.cs.umass.edu/info/psfiles/tepubs/tepu bs.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Church W K Gale</author>
</authors>
<title>and Yarowsky D.(1992) One sense per discourse.</title>
<date>1992</date>
<booktitle>in Proc. of the DARPA speech and and Natural Language workshop,</booktitle>
<location>Harriman, NY,</location>
<marker>Gale, 1992</marker>
<rawString>Gale, Church W. K. and Yarowsky D.(1992) One sense per discourse. in Proc. of the DARPA speech and and Natural Language workshop, Harriman, NY, February 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grislunan</author>
<author>C Macleod</author>
<author>A Meyers</author>
</authors>
<title>NYU: description of the Proteus System as used for MUC-4. in</title>
<date>1992</date>
<booktitle>Proc. of Fourth Message Understanding Conference (MUC-4)</booktitle>
<marker>Grislunan, Macleod, Meyers, 1992</marker>
<rawString>Grislunan R., Macleod C. and Meyers A. (1992) NYU: description of the Proteus System as used for MUC-4. in Proc. of Fourth Message Understanding Conference (MUC-4) June 1992</rawString>
</citation>
<citation valid="true">
<authors>
<author>Humphreys</author>
</authors>
<title>VIE Technical Specifications,</title>
<date>1996</date>
<institution>ILASH, University of Sheffield.</institution>
<marker>Humphreys, 1996</marker>
<rawString>Humphreys (1996) VIE Technical Specifications, 1996/10/1815. ILASH, University of Sheffield.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Kononenko</author>
<author>I Bratko</author>
</authors>
<title>Informationbased Evaluation Criterion for Classifier&apos;s Performance.</title>
<date>1991</date>
<journal>Machine Learning</journal>
<volume>6</volume>
<pages>67--80</pages>
<contexts>
<context position="14108" citStr="Kononenko and Bratko, 1991" startWordPosition="2226" endWordPosition="2229">wn in the figure). An extended experiment was designed as follows: We selected from the corpus 35 PNs for each of the following categories: Organization, Person, Location and Product3. The PNs are selected by ranges of frequency in the corpus, except for Producs, that are very rare in our excerpt of the Il Sole 24 Ore: here we selected the 35 top frequency PNs. We then removed each of the 140 PNs from the gazetteer, one at the time, and attempted a re-classification using our algorithm. To evaluate the performances we used, in addition to the classical Precision measure, the Information Gain (Kononenko and Bratko, 1991). The Information Gain is an informationtheoretic measure that takes into account the complexity of the classification task. 3The other categories are less interesting in our view. Numbers, dates etc. are recursive and regular phenomena that can be detected in a more general way by the use of specific grammars or pattern matchers. 289 PROPER MNE: Quick Take 200 0.5 G N P_N Qiick 200 0 1 in docananto 1.0 G N V Quick Take 200 0 1 nil dotare ESI.1: 0.333000 G N_P N grande di. Weil 3 1 PLB 0.250000 G_N_P_N grannie di. Europa 2 1 EET. 0.2 G N P_N grande di Casa 1 1 ESLE 1.0 G_N V Apple 1 1 nil forn</context>
<context position="18092" citStr="Kononenko and Bratko, 1991" startWordPosition="2982" endWordPosition="2985">anizations). On the contrary, we obtain poor performances with Products. However, Product is interesting because: - there are no more than 50-60 product names in the gazetteer (which we 290 manually added for the purpose of this experiment) - there are no contextual rules for Products in the context-sensitive grammar. Thus, both prior probability and prior knowledge on Products are close to zero. This is numerically evidenced by the Information Gain: though we are not learning much about Products, the Information Gain is higher than for the other categories, and also as an absolute value (in (Kononenko and Bratko, 1991) a 0,5 bit improvement is among the highest measured values in a comparative experiment). In addition, the relative precision of classifying PNs as Product is 100%. This means that most products are misclassified, but, if something is classified as Product, this information can be reliably used to enrich the gazetteer. Category Precision Inf. Gain ORGANIZ. 100.00% 0.11 LOCATION. 91.43% 0.14 PERSON 80.00% 0.23 PRODUCT 22.86% 0.65 Table 2 - Precision and Information Gain of the method Table 3 reports an experiment on a small corpus extracted from another portion of Il Sole 24 Ore, indexed as &amp;quot;Ne</context>
</contexts>
<marker>Kononenko, Bratko, 1991</marker>
<rawString>Kononenko I. and Bratko I. (1991) Informationbased Evaluation Criterion for Classifier&apos;s Performance. Machine Learning 6, pp. 67-80, 1991</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>R McMillian</author>
<author>S Luperfoy</author>
<author>E Lusher</author>
<author>S Laskowski</author>
</authors>
<title>Identifying Unknown Proper Names in Newswire Text.</title>
<date>1996</date>
<booktitle>in Corpus Processing for Lexical Acquisition,</booktitle>
<publisher>MIT Press</publisher>
<marker>Mani, McMillian, Luperfoy, Lusher, Laskowski, 1996</marker>
<rawString>Mani I., McMillian R., Luperfoy S., Lusher E., Laskowski S. (1996) Identifying Unknown Proper Names in Newswire Text. in Corpus Processing for Lexical Acquisition, J. Pustejovslcy and B. Boguraev Eds., MIT Press 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDonald</author>
</authors>
<title>Internal and External Evidence in the Identification and Semantic Categorization of Proper Names.</title>
<date>1996</date>
<booktitle>in Corpus Processing for Lexical Acquisition,</booktitle>
<publisher>MIT Press</publisher>
<contexts>
<context position="2541" citStr="McDonald, 1996" startWordPosition="380" endWordPosition="381">e that takes into account the complexity of the classification task. Introduction In terms of syntactic categories, proper nouns are lexical NPs that can be formed by primitive proper names (Adolfo_Battaglia), groups of proper nouns of different semantic categories (San_Paolo di Brescia), and also of non-proper nouns (Banca dei regolamenti internazionali). In the latter case, capital letters are optional, making the problem of PN items identification even more complex. In the literature, it is accepted that an adequate treatment of proper nouns requires the use of a context-sensitive grammar (McDonald, 1996). McDonald points out that the context sensitivity requirement involves two complementary types of evidence: internal and external. The internal evidence, can be derived from the sequence of. words in a text (proper nouns and trigger words, such as Inc., &amp;, Ltd., Company, etc.), and is gained in almost all state-of-art PNs recognisers by the use of large gazetteers and lists of trigger words. The external evidence is the context of a proper noun, that provides classificatory criteria to reinforce internal evidence, if any, or supplies some classificatory evidence. In fact, proper names form an</context>
</contexts>
<marker>McDonald, 1996</marker>
<rawString>McDonald D. (1996) Internal and External Evidence in the Identification and Semantic Categorization of Proper Names. in Corpus Processing for Lexical Acquisition, J. Pustejovsky and B. Boguraev Eds., MIT Press 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Paik</author>
<author>E Liddy</author>
<author>E Yu</author>
<author>M McKenna</author>
</authors>
<title>Categorizing and standardizing proper nouns for effcient Information Retrieval.</title>
<date>1996</date>
<booktitle>in Corpus Processing for Lexical Acquisition,</booktitle>
<publisher>MIT Press</publisher>
<marker>Paik, Liddy, Yu, McKenna, 1996</marker>
<rawString>Paik W., Liddy E., Yu E. and McKenna M. (1996) Categorizing and standardizing proper nouns for effcient Information Retrieval. in Corpus Processing for Lexical Acquisition, J. Pustejovslcy and B. Boguraev Eds., MIT Press 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Palmer</author>
<author>D Day</author>
</authors>
<title>A Statistical Profile of the Named Enity Task in</title>
<date>1997</date>
<booktitle>Proc. of 5th Conference on Applied natural Language Processing,</booktitle>
<location>Washington,</location>
<contexts>
<context position="5009" citStr="Palmer and Day, 1997" startWordPosition="770" endWordPosition="773"> Since PNs are mostly domain-specific, presumably a comparable effort is needed when shifting to different domains. High performances of the existing systems are by no means the result of many years of studies and research in the area of IE from newswire English texts, promoted and funded by the Message Understanding Conferences (MUC) organizers. Yet, there is no evidence that a similar performance could be obtained in other languages and domains, if not at the price of a similar effort for rule writing (or manual training), and for the compilation of a highcoverage gazetteer. A recent study (Palmer and Day, 1997) established that the baseline performances of the PN recognition task for several languages and application domains vary between 34% and 71%. The lower bound is calculated by considering a simple algorithm that recognizes PNs on the basis of a list of frequent proper nouns seen in a training set. The method we propose in this paper combines symbolic and statistical approaches to classify unknown PNs using context evidence previously extracted from the application corpus. The method can be used to overcome the limitation of small gazetteers and poorly encoded rule bases. Our method is untraine</context>
</contexts>
<marker>Palmer, Day, 1997</marker>
<rawString>Palmer D. and Day D. (1997) A Statistical Profile of the Named Enity Task in Proc. of 5th Conference on Applied natural Language Processing, Washington, 1997</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Wacholder</author>
<author>Y Ravin</author>
<author>M Choi</author>
</authors>
<title>Disambiguation of Proper Names in Text.</title>
<date>1997</date>
<booktitle>in Proc. of 5th Conference on Applied natural Language Processing,</booktitle>
<location>Washington,</location>
<contexts>
<context position="20435" citStr="Wacholder et al, 1997" startWordPosition="3355" endWordPosition="3358">ources,in Italian, such as PN gazetteers. The algorithm that we propose has indeed the purpose of overcoming limitations of gazetteers and manually defined contextual rules for PN recognition. In (Cucchiarelli et al. 1998) we also show how to extend our method to incrementally update the initial gazzeteer. The performance of the proposed algorithm is more than satisfactory. A comparison with existing systems is difficult because in the literature global PN recognition performances are reported, without considering the semantic classification of unknowns as a subtask. The only exception is in (Wacholder et al, 1997) where the reported performance for the sole semantic disambiguation task of PNs is 79%. In that paper, however, semantic disambiguation is performed among a lower number of classes5. The performance of our system is clearly affected by the dimension of the initial seed gazetteer and contextual rules. If the sets ESLA and ESLB are large enough, obviously more examples of similar contexts are found, even for unknown PNs with a single occurrence. In our test experiment, we always managed to find at least one or two similar contexts of an unknown PN, but in some cases they were misleading and cau</context>
</contexts>
<marker>Wacholder, Ravin, Choi, 1997</marker>
<rawString>Wacholder N., Ravin Y. and Choi M. (1997) Disambiguation of Proper Names in Text. in Proc. of 5th Conference on Applied natural Language Processing, Washington, 1997</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>