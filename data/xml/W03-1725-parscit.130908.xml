<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000203">
<title confidence="0.995682">
A Unicode based Adaptive Segmentor
</title>
<author confidence="0.969907">
Q. Lu, S. T. Chan, R. F. Xu, T. S. Chiu
</author>
<affiliation confidence="0.959477">
Dept. Of Computing,
</affiliation>
<author confidence="0.4993405">
The Hong Kong Polytechnic University,
Hung Hom, Hong Kong
</author>
<email confidence="0.92275">
{csluqin,csrfxu}@comp.polyu.edu.hk
</email>
<author confidence="0.82811">
B. L. Li, S. W. Yu
</author>
<affiliation confidence="0.722747333333333">
The Institute of Computational Linguistics,
Peking University,
Beijing, China
</affiliation>
<email confidence="0.942624">
{libi,yusw}@pku.edu.cn
</email>
<sectionHeader confidence="0.996689" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999443">
This paper presents a Unicode based
Chinese word segmentor. It can handle
Chinese text in Simplified, Traditional, or
mixed mode. The system uses the strategy
of divide-and-conquer to handle the
recognition of personal names, numbers,
time and numerical values, etc in the pre-
processing stage. The segmentor further
uses tagging information to work on
disambiguation. Adopting a modular
design approach, different functional parts
are separately implemented using
different modules and each module
tackles one problem at a time providing
more flexibility and extensibility. Results
show that with added pre-processing
modules and accessorial modules, the
accuracy of the segmentor is increased
and the system is easily adaptive to
different applications.
</bodyText>
<sectionHeader confidence="0.999471" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99981590625">
The most difficult problem in Chinese word
segmentation is due to overlapping ambiguities [1-
2]. The recognition of names, foreign names, and
organizations are quite unique for Chinese. Some
systems can already achieve very high accuracy [3],
but they heavily rely on manual work in getting the
system to be trained to work certain language
environment. However, for many applications, we
need to look at the cost to achieve high accuracy.
In a competitive environment, we also need to
have systems that are quickly adaptive to new
requirements with limited resources available.
In this paper, we report a Unicode based Chinese
word segmentor. The segmentor can handle
Chinese text in Simplified, Traditional, or mixed
mode where internally only one dictionary is
needed. The system uses the strategy of divide-
and-conquer to handle the recognition of personal
names, numbers, time and numerical values. The
system has a built-in new word extractor that can
extract new words from running text, thus save
time on training and getting the system quickly
adaptive to new language environment. The
Bakeoff results in the open text for our system in
all categories have shown that it works reasonably
good for all different corpora.
The rest of the paper is organized as follows.
Section 2 presents our system design objectives
and components. Section 3 discusses more
implementation details. Section 4 gives some
performance evaluations. Section 5 is the
conclusion.
</bodyText>
<sectionHeader confidence="0.959072" genericHeader="method">
2 Design Objectives and Components
</sectionHeader>
<bodyText confidence="0.999367333333333">
With the wide use of Unicode based operating
systems such as Window 2000 and Window XP,
we now see more and more text data written in
both the Simplified form and the Traditional form
to co-exist on the same system. It is also likely that
text written in mixed mode. Because of this reality,
the first design objective of this system is its ability
to handle the segmentation of Chinese text written
in either Simplified Chinese, Traditional Chinese,
or mixed mode. As an example, we should be able
to segment the same sentence in different forms
such as the example given below:
The second design objective is to adopt the
modular design approach where different
functional parts are separately implemented using
independent modules and each module tackles one
problem at a time. Using this modular approach,
we can isolate problems and fine tune each module
with minimal effect on other modules in the system.
Special features like adding new rules or new
dictionary can be easily done without affecting
other modules. Consequently, the system is more
flexible and can be easily extended.
The third design objective of the system is to make
the segmentor adaptive to different application
domains. We consider it having more practical
value if the segmentor can be easily trained using
some semi-automatic process to work in different
domains and work well for text with different
regional variations. We consider it essential that
the segmentor has tools to help it to obtain regional
related information quickly even if annotated
corpora are not available. For instance, when it
runs text from Hong Kong, it must be able to
recognize the personal names such as if
such a name(quadra-gram) appears in the text often.
</bodyText>
<figureCaption confidence="0.995177">
Figure 1. System components
</figureCaption>
<bodyText confidence="0.998194628571429">
Figure 1 shows the two major components, the
segmentor and data manager. The segmentor is the
core component of the system. It has a pre-
processor, the kernel, and a post-processor. As the
system has to maintain a number of tables such as
the dictionaries, family name list, etc., a separate
component called data manager is responsible in
handling the maintenance of these data. The pre-
processor has separate modules to handle
paragraphs, ASCII code, numbers, time, and
proper names including personal names, place and
organizational names, and foreign names. The
kernel supports different segmentation algorithms.
It is the application or user’s choice to invoke the
preferred segmentation algorithms that at current
time include the basic maximum matching and
minimum matching in both forward and backward
mode. These can also be used to build more
complicated algorithms later on. In addition, the
system provides segmentation using part-of-speech
tagging information to help resolve ambiguity. The
post-processor applies morphological rules which
cannot be easily applied using a dictionary.
The data manager helps to maintain the knowledge
base in the system. It also has an accessory
software called the new word extractor which can
collect statistical information based on character
bi-grams, tri-grams and quadra-grams to semi-
automatically extract words and names so that they
can be used by the segmentor to improve
performance especially when switching to a new
domain. Another characteristic of this segmentor is
that it provides tagging information for segmented
text. The tagging information can be optionally
omitted if not needed by an application.
</bodyText>
<sectionHeader confidence="0.996558" genericHeader="method">
3 Implementation Details
</sectionHeader>
<bodyText confidence="0.981856888888889">
The basic dictionary of this system was provided
by Peking University [4] and we also used the
tagging data from [4]. The data structure for our
dictionaries are very similar to that discussed in [5].
As our program needs to handle both Simplified
and Traditional Chinese characters, Unicode is the
only solution for dealing with more than one script
at the same time.
Even though it is our design objective to support
both Simplified and Traditional Chinese, we do not
want to keep two different sets of dictionaries for
Simplified and Traditional Chinese. Even if two
versions are kept, it would not serve well for text
in mixed mode. For example, Traditional Chinese
word of “the day after tomorrow” should be ,
and for Simplified Chinese, it should be .
However sometimes we can see the word
appears in a Traditional Chinese text. We cannot
say that it is wrong because the sentence is still
semantically correct especially in Unicode
environment. Therefore the segmentor should be
able to segment those words correctly such as in
the examples: “ ”, and in “
”. We must also deal with dictionary
maintenance related to Chinese variants. For
example, characters are variants, so are
.
</bodyText>
<figure confidence="0.992748909090909">
Segmentor
Pre-
Processor
Post
Processor
Kernel
Data manager
Knowledge-
base
New Word
Extractor
</figure>
<bodyText confidence="0.995925164179104">
In order to keep the dictionary maintenance simple,
our system uses a single dictionary which only
keeps the so called canonical form of a word. In
our system, the canonical form of a word is its
“simplified form”. We quoted the word
“simplified” because only certain characters have
simplified forms such as to , but for
, there is no simplified form. In the case of
variants, we simply choose one of them as the
canonical character. The canonical characters are
maintained in the traditional-simplified character
conversion table as well as in a variant table.
Whenever a new word, item, is added into the
dictionary, it must be added using a function
CanonicalConversion(), which takes item as an
input. During segmentation, the corresponding
dictionary look up function will first convert the
token to its canonical form before looking up in the
dictionary.
The personal name recognizers (separate for
Chinese names and foreign names) use the
maximum-likelihood algorithm with consideration
of commonly used Chinese family names, given
names, and foreign name characters. It works for
Chinese names of length up to 5 characters. In the
following examples you can see that our system
successfully recognized the name . This
is done using our algorithm, not by putting her
name in our dictionary:
Organization names and place names are
recognized mainly using special purpose
dictionaries. The segmentor uses tagging
information to help resolve ambiguity. The
disambiguation is mostly based on rules such as
p + (n + f) -&gt; p + n + f
which would word to correct
basic statistical data include bi-gram frequency, tri-
gram frequency, and quadra-gram frequencies. In
order to further example whether a bi-gram, say
, is indeed a word, we further collect forward
conditional frequency of , and
the back-ward conditional frequency of ,
. For an i-gram token, we also
use the (i+1)-gram statistics to eliminate those i-
grams that are only a part of (i+1) – gram word.
For instance, if the frequency of bi-gram is
very close to the frequency of tri-gram , it
is less likely that is a word. Of course,
whether is a word depends on quadra-gram
results. Using the statistical result, a set of rules
was applied to these i-grams to eliminate entries
that are not considered new words. Minimal
manual work is required to identify whether the
remaining candidates are new words. Before words
are added into the dictionary, part-of-speech
information are added manually (although not
necessary) before using the canonical function.
The following table shows examples of bi-grams
which are found by the new word extractor using
one year Hong Kong Commercial Daily News data.
For efficiency reasons, our system uses only about
20 rules. The system is flexible enough for new
rules to be added to improve performance.
The new word extractor is an accessory program to
extract new words from running text based on
statistical data which can either be grabbed from
the internet or collected from other sources. The
</bodyText>
<sectionHeader confidence="0.988261" genericHeader="method">
4 Performance Evaluation
</sectionHeader>
<bodyText confidence="0.9950855">
The valuation metrics used in [6] were adopted
here.
</bodyText>
<equation confidence="0.9874864">
N
recall = 3 (1)
N1
N
presicion = 3
(2)
N2
F1 (precision, recall) = x precision (3 )
recall precision
+
</equation>
<bodyText confidence="0.956229071428571">
where N 1 denotes the number of words in the
annotated corpus, N 2 denotes the number of words
identified by the segmentation algorithm , and N 3 is
the number of words correctly identified.
We participated in the open tests for all four
corpora. The results are shown in the following
table.
because we used a very similar dictionary
. The
additional training of data for HK was done using
one year Commercial Daily( ).
The following table summarizes the execution
speed of our program for the 4 different
sources:
</bodyText>
<table confidence="0.9699095">
Data No. of Processi Processin Segmentat chars ng Time g Rate ion Rate
(sec.) (char/sec) (char/sec)
AS 18,743 4.703 3,985 7,641
CTB 62,332 10.110 6,165 7,930
HK 57,432 10.329 5,560 7,109
PK 28,458 4.829 5,893 10,970
</table>
<bodyText confidence="0.9990385">
rate of segmentation on the average is around
7,500 characters for the first three corpora. It
seems that the processing speed for Peking U. data
is faster. This may be because the dictionaries we
used are closer to the PK system, thus it would
take less time to work on disambiguation.
</bodyText>
<sectionHeader confidence="0.99909" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.947604866666667">
purposed Unicode based
is proposed. It
is able to process Simplified and Traditional
Chinese appear in the same text. Sophisticated pre-
processing and other auxiliary modules help
segmenting text more accurately. User interactions
and modules can be easily added with the help of
its modular design.
new word extractor
is also implemented for extracting new words fr
segmentor
A built-in
om
running text. It saves much time on training and
thus it can be quickly adapted to new environments.
</bodyText>
<sectionHeader confidence="0.945496" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9962788">
We thank the PI of ITF Grant by ITC of
HKSARG (ITS/024/01) entitled: Towards Cost-
Effective E-business in the News Media &amp;
Publishing Industry for the use of HK Commercial
Daily.
</bodyText>
<equation confidence="0.350734">
Text ( )
</equation>
<reference confidence="0.989722692307692">
K.Y. Liu,
Commercial Press, 2000
[2] Segmentation Issues in Chinese Information
Processing, (C.N. Huang Issue No.
1, 1997)
[3] The design and Implementation of a Modern General
Purpose Segmentation System (B. Lou, R. Song, W.L.
Li, an
,
d Z.Y. Luo, Journal of Chinese Information
Processing, Issue No. 5, 2001)
[4] (Institute of
Computational Linguistics, Peking Univ., 2002)
</reference>
<bodyText confidence="0.99880415">
The worst performance in the 4 tests were for the
CTB(UPenn) data. From the observation from the
testing data, we found that the main problem with
have with CTB data is the difference in word
granularity. To confirm our observation, we have
done an analysis of combining errors and
overlapping errors. The results show that the ratios
of combining errors in all the error types are
0.8425(AS), 0.87684(CTB), 0.82085(HK), and
0.77102(PK). The biggest problem we have with
AS data, on the other hand is due to out of
vocabulary mistakes. Even though our new word
extractor can help us to reduce this problem, but
we have not trained our system using data from
Taiwan. Our best performance was on PK data
The program initialization needs around 2.25
seconds mainly to load the dictionaries and other
data into the memory before the segmentation can
start
In this paper, design and algorithms of a general-
</bodyText>
<sectionHeader confidence="0.999213" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999598833333333">
[1] Automatic Segmentation and Tagging for Chinese
Journal of Chinese information
processing vol. 14, no. 1,
[6] Chinese Word Segmentation and Information
Retrieval, Palmer D., and Burger J., In AAAI
Symposium Cross-Language Text an
</reference>
<page confidence="0.280053">
2001)
</page>
<figure confidence="0.4287672">
d Speech
Retrieval 1997
2x
recall
. If we only count the segmentation time, the
</figure>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.261997">
<title confidence="0.999748">A Unicode based Adaptive Segmentor</title>
<author confidence="0.971868">Q Lu</author>
<author confidence="0.971868">S T Chan</author>
<author confidence="0.971868">R F Xu</author>
<author confidence="0.971868">T S</author>
<affiliation confidence="0.991695">Dept. Of</affiliation>
<title confidence="0.608028">The Hong Kong Polytechnic</title>
<author confidence="0.856514">Hung Hom</author>
<author confidence="0.856514">Hong Kong</author>
<email confidence="0.895866">csluqin@comp.polyu.edu.hk</email>
<email confidence="0.895866">csrfxu@comp.polyu.edu.hk</email>
<author confidence="0.979346">B L Li</author>
<author confidence="0.979346">S W</author>
<affiliation confidence="0.791206">The Institute of Computational Peking</affiliation>
<address confidence="0.997666">Beijing, China</address>
<email confidence="0.987624">libi@pku.edu.cn</email>
<email confidence="0.987624">yusw@pku.edu.cn</email>
<abstract confidence="0.998806428571429">This paper presents a Unicode based Chinese word segmentor. It can handle Chinese text in Simplified, Traditional, or mixed mode. The system uses the strategy of divide-and-conquer to handle the recognition of personal names, numbers, time and numerical values, etc in the preprocessing stage. The segmentor further uses tagging information to work on disambiguation. Adopting a modular design approach, different functional parts are separately implemented using different modules and each module tackles one problem at a time providing more flexibility and extensibility. Results show that with added pre-processing modules and accessorial modules, the accuracy of the segmentor is increased and the system is easily adaptive to different applications.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>K Y Liu</author>
</authors>
<date>2000</date>
<publisher>Commercial Press,</publisher>
<marker>Liu, 2000</marker>
<rawString> K.Y. Liu, Commercial Press, 2000</rawString>
</citation>
<citation valid="true">
<title>Segmentation Issues in Chinese Information Processing,</title>
<date>1997</date>
<journal>C.N. Huang Issue</journal>
<volume>1</volume>
<marker>[2]</marker>
<rawString>Segmentation Issues in Chinese Information Processing, (C.N. Huang Issue No. 1, 1997)</rawString>
</citation>
<citation valid="true">
<title>The design and Implementation of a Modern General Purpose Segmentation System</title>
<date>2001</date>
<journal>Journal of Chinese Information Processing, Issue</journal>
<volume>5</volume>
<contexts>
<context position="1329" citStr="[3]" startWordPosition="193" endWordPosition="193">rent functional parts are separately implemented using different modules and each module tackles one problem at a time providing more flexibility and extensibility. Results show that with added pre-processing modules and accessorial modules, the accuracy of the segmentor is increased and the system is easily adaptive to different applications. 1 Introduction The most difficult problem in Chinese word segmentation is due to overlapping ambiguities [1- 2]. The recognition of names, foreign names, and organizations are quite unique for Chinese. Some systems can already achieve very high accuracy [3], but they heavily rely on manual work in getting the system to be trained to work certain language environment. However, for many applications, we need to look at the cost to achieve high accuracy. In a competitive environment, we also need to have systems that are quickly adaptive to new requirements with limited resources available. In this paper, we report a Unicode based Chinese word segmentor. The segmentor can handle Chinese text in Simplified, Traditional, or mixed mode where internally only one dictionary is needed. The system uses the strategy of divideand-conquer to handle the recog</context>
</contexts>
<marker>[3]</marker>
<rawString>The design and Implementation of a Modern General Purpose Segmentation System (B. Lou, R. Song, W.L. Li, an , d Z.Y. Luo, Journal of Chinese Information Processing, Issue No. 5, 2001)</rawString>
</citation>
<citation valid="false">
<date>2002</date>
<institution>Institute of Computational Linguistics, Peking Univ.,</institution>
<contexts>
<context position="6096" citStr="[4]" startWordPosition="951" endWordPosition="951">stem. It also has an accessory software called the new word extractor which can collect statistical information based on character bi-grams, tri-grams and quadra-grams to semiautomatically extract words and names so that they can be used by the segmentor to improve performance especially when switching to a new domain. Another characteristic of this segmentor is that it provides tagging information for segmented text. The tagging information can be optionally omitted if not needed by an application. 3 Implementation Details The basic dictionary of this system was provided by Peking University [4] and we also used the tagging data from [4]. The data structure for our dictionaries are very similar to that discussed in [5]. As our program needs to handle both Simplified and Traditional Chinese characters, Unicode is the only solution for dealing with more than one script at the same time. Even though it is our design objective to support both Simplified and Traditional Chinese, we do not want to keep two different sets of dictionaries for Simplified and Traditional Chinese. Even if two versions are kept, it would not serve well for text in mixed mode. For example, Traditional Chinese wor</context>
</contexts>
<marker>[4]</marker>
<rawString>(Institute of Computational Linguistics, Peking Univ., 2002)</rawString>
</citation>
<citation valid="false">
<title>Automatic Segmentation and Tagging for Chinese</title>
<journal>Journal of Chinese information processing</journal>
<volume>14</volume>
<marker>[1]</marker>
<rawString>Automatic Segmentation and Tagging for Chinese Journal of Chinese information processing vol. 14, no. 1,</rawString>
</citation>
<citation valid="false">
<authors>
<author>Chinese Word Segmentation</author>
<author>Information Retrieval</author>
<author>D Palmer</author>
<author>J Burger</author>
</authors>
<booktitle>In AAAI Symposium Cross-Language Text an</booktitle>
<contexts>
<context position="10355" citStr="[6]" startWordPosition="1658" endWordPosition="1658">ally (although not necessary) before using the canonical function. The following table shows examples of bi-grams which are found by the new word extractor using one year Hong Kong Commercial Daily News data. For efficiency reasons, our system uses only about 20 rules. The system is flexible enough for new rules to be added to improve performance. The new word extractor is an accessory program to extract new words from running text based on statistical data which can either be grabbed from the internet or collected from other sources. The 4 Performance Evaluation The valuation metrics used in [6] were adopted here. N recall = 3 (1) N1 N presicion = 3 (2) N2 F1 (precision, recall) = x precision (3 ) recall precision + where N 1 denotes the number of words in the annotated corpus, N 2 denotes the number of words identified by the segmentation algorithm , and N 3 is the number of words correctly identified. We participated in the open tests for all four corpora. The results are shown in the following table. because we used a very similar dictionary . The additional training of data for HK was done using one year Commercial Daily( ). The following table summarizes the execution speed of o</context>
</contexts>
<marker>[6]</marker>
<rawString>Chinese Word Segmentation and Information Retrieval, Palmer D., and Burger J., In AAAI Symposium Cross-Language Text an</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>