<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000122">
<title confidence="0.960549">
A New Objective Function for Word Alignment
</title>
<author confidence="0.993768">
Tugba Bodrumlu Kevin Knight Sujith Ravi
</author>
<affiliation confidence="0.995944">
Information Sciences Institute &amp; Computer Science Department
University of Southern California
</affiliation>
<email confidence="0.989699">
bodrumlu@usc.edu, knight@isi.edu, sravi@isi.edu
</email>
<sectionHeader confidence="0.996648" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999955727272727">
We develop a new objective function for word
alignment that measures the size of the bilin-
gual dictionary induced by an alignment. A
word alignment that results in a small dictio-
nary is preferred over one that results in a large
dictionary. In order to search for the align-
ment that minimizes this objective, we cast the
problem as an integer linear program. We then
extend our objective function to align corpora
at the sub-word level, which we demonstrate
on a small Turkish-English corpus.
</bodyText>
<sectionHeader confidence="0.99878" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.995089772727273">
Word alignment is the problem of annotating a bilin-
gual text with links connecting words that have the
same meanings. Figure 1 shows sample input for
a word aligner (Knight, 1997). After analyzing the
text, we may conclude, for example, that sprok cor-
responds to dat in the first sentence pair.
Word alignment has several downstream con-
sumers. One is machine translation, where pro-
grams extract translation rules from word-aligned
corpora (Och and Ney, 2004; Galley et al., 2004;
Chiang, 2007; Quirk et al., 2005). Other down-
stream processes exploit dictionaries derived by
alignment, in order to translate queries in cross-
lingual IR (Sch¨onhofen et al., 2008) or re-score can-
didate translation outputs (Och et al., 2004).
Many methods of automatic alignment have been
proposed. Probabilistic generative models like IBM
1-5 (Brown et al., 1993), HMM (Vogel et al., 1996),
ITG (Wu, 1997), and LEAF (Fraser and Marcu,
2007) define formulas for P(f I e) or P(e, f), with
ok-voon ororok sprok
at-voon bichat dat
erok sprok izok hihok ghirok
totat dat arrat vat hilat
ok-drubel ok-voon anok plok sprok
at-drubel at-voon pippat rrat dat
ok-voon anok drok brok jok
at-voon krat pippat sat lat
wiwok farok izok stok
totat jjat quat cat
lalok sprok izok jok stok
wat dat krat quat cat
lalok farok ororok lalok sprok izok enemok
wat jjat bichat wat dat vat eneat
lalok brok anok plok nok
iat lat pippat rrat nnat
wiwok nok izok kantok ok-yurp
totat nnat quat oloat at-yurp
lalok mok nok yorok ghirok clok
wat nnat gat mat bat hilat
lalok nok crrrok hihok yorok zanzanok
wat nnat arrat mat zanzanat
lalok rarok nok izok hihok mok
wat nnat forat arrat vat gat
</bodyText>
<figureCaption confidence="0.991777">
Figure 1: Word alignment exercise (Knight, 1997).
</figureCaption>
<page confidence="0.977376">
28
</page>
<note confidence="0.9956525">
Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28–35,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.997437071428571">
hidden alignment variables. EM algorithms estimate
dictionary and other probabilities in order to maxi-
mize those quantities. One can then ask for Viterbi
alignments that maximize P(alignment  |e, f). Dis-
criminative models, e.g. (Taskar et al., 2005), in-
stead set parameters to maximize alignment accu-
racy against a hand-aligned development set. EMD
training (Fraser and Marcu, 2006) combines genera-
tive and discriminative elements.
Low accuracy is a weakness for all systems. Most
practitioners still use 1990s algorithms to align their
data. It stands to reason that we have not yet seen
the last word in alignment models.
In this paper, we develop a new objective function
for alignment, inspired by watching people manu-
ally solve the alignment exercise of Figure 1. When
people attack this problem, we find that once they
create a bilingual dictionary entry, they like to re-
use that entry as much as possible. Previous ma-
chine aligners emulate this to some degree, but they
are not explicitly programmed to do so.
We also address another weakness of current
aligners: they only align full words. With few ex-
ceptions, e.g. (Zhang et al., 2003; Snyder and Barzi-
lay, 2008), aligners do not operate at the sub-word
level, making them much less useful for agglutina-
tive languages such as Turkish.
Our present contributions are as follows:
</bodyText>
<listItem confidence="0.991111875">
• We offer a simple new objective function that
scores a corpus alignment based on how many
distinct bilingual word pairs it contains.
• We use an integer programming solver to
carry out optimization and corpus alignment.
• We extend the system to perform sub-
word alignment, which we demonstrate on a
Turkish-English corpus.
</listItem>
<bodyText confidence="0.994257666666667">
The results in this paper constitute a proof of con-
cept of these ideas, executed on small corpora. We
conclude by listing future directions.
</bodyText>
<sectionHeader confidence="0.892593" genericHeader="method">
2 New Objective Function for Alignment
</sectionHeader>
<bodyText confidence="0.991944566666667">
We search for the legal alignment that minimizes the
size of the induced bilingual dictionary. By dictio-
nary size, we mean the number of distinct word-
pairs linked in the corpus alignment. We can im-
mediately investigate how different alignments stack
up, according to this objective function. Figure 2
garcia and associates
garcia y asociados
his associates are not strong
sus asociados no son fuertes
carlos garcia has three associates
carlos garcia tiene tres asociados
garcia has a company also
garcia tambien tiene una empresa
its clients are angry
sus clientes estan enfadados
the associates are also angry
los asociados tambien estan enfadados
the clients and the associates are enemies
los clientes y los asociados son enemigos
the company has three groups
la empresa tiene tres grupos
its groups are in europe
sus grupos estan en europa
the modern groups sell strong pharmaceuticals
los grupos modernos venden medicinas fuertes
the groups do not sell zenzanine
los grupos no venden zanzanina
the small groups are not modern
los grupos pequenos no son modernos
</bodyText>
<figureCaption confidence="0.861165666666667">
Figure 2: Gold alignment. The induced bilingual dic-
tionary has 28 distinct entries, including garcia/garcia,
are/son, are/estan, not/no, has/tiene, etc.
</figureCaption>
<page confidence="0.99658">
29
</page>
<figureCaption confidence="0.9971465">
Figure 3: IP alignment. The induced bilingual dictionary
has 28 distinct entries.
</figureCaption>
<bodyText confidence="0.999157666666667">
shows the gold alignment for the corpus in Figure 1
(displayed here as English-Spanish), which results
in 28 distinct bilingual dictionary entries. By con-
trast, a monotone alignment induces 39 distinct en-
tries, due to less re-use.
Next we look at how to automatically rifle through
all legal alignments to find the one with the best
score. What is a legal alignment? For now, we con-
sider it to be one where:
</bodyText>
<listItem confidence="0.99023925">
• Every foreign word is aligned exactly once
(Brown et al., 1993).
• Every English word has either 0 or 1 align-
ments (Melamed, 1997).
</listItem>
<bodyText confidence="0.99488">
We formulate our integer program (IP) as follows.
We set up two types of binary variables:
</bodyText>
<listItem confidence="0.972242714285714">
• Alignment link variables. If link-i-j-k = 1, that
means in sentence pair i, the foreign word at
position j aligns to the English words at posi-
tion k.
• Bilingual dictionary variables. If dict-f-e = 1,
that means word pair (f, e) is “in” the dictio-
nary.
</listItem>
<bodyText confidence="0.999962214285714">
We constrain the values of link variables to sat-
isfy the two alignment conditions listed earlier. We
also require that if link-i-j-k = 1 (i.e., we’ve decided
on an alignment link), then dict-fij-eik should also
equal 1 (the linked words are recorded as a dictio-
nary entry).1 We do not require the converse—just
because a word pair is available in the dictionary, the
aligner does not have to link every instance of that
word pair. For example, if an English sentence has
two the tokens, and its Spanish translation has two
la tokens, we should not require that all four links
be active—in fact, this would conflict with the 1-1
link constraints and render the integer program un-
solvable. The IP reads as follows:
</bodyText>
<equation confidence="0.918582333333333">
minimize:
E
f,e dict-f-e
subject to:
�
Vi,j k link-i-j-k = 1
�
Vi,k j link-i-j-k &lt; 1
Vi,j,k link-i-j-k &lt; dict-fij-eik
</equation>
<bodyText confidence="0.998732">
On our Spanish-English corpus, the cplex2 solver
obtains a minimal objective function value of 28. To
</bodyText>
<footnote confidence="0.999288">
1fij is the jth foreign word in the ith sentence pair.
2www.ilog.com/products/cplex
</footnote>
<bodyText confidence="0.997912791666667">
garcia and associates
garcia y asociados
his associates are not strong
sus asociados no son fuertes
carlos garcia has three associates
carlos garcia tiene tres asociados
garcia has a company also
garcia tambien tiene una empresa
its clients are angry
sus clientes estan enfadados
the associates are also angry
los asociados tambien estan enfadados
the clients and the associates are enemies
los clientes y los asociados son enemigos
the company has three groups
la empresa tiene tres grupos
the modern groups sell strong pharmaceuticals
los grupos modernos venden medicinas fuertes
the groups do not sell zenzanine
los grupos no venden zanzanina
the small groups are not modern
los grupos pequenos no son modernos
its groups are in europe
sus grupos estan en europa
</bodyText>
<page confidence="0.990193">
30
</page>
<bodyText confidence="0.999755941176471">
get the second-best alignment, we add a constraint
to our IP requiring the sum of the n variables active
in the previous solution to be less than n, and we
re-run cplex. This forces cplex to choose different
variable settings on the second go-round. We repeat
this procedure to get an ordered list of alignments.3
We find that there are 8 distinct solutions that
yield the same objective function value of 28. Fig-
ure 3 shows one of these. This alignment is not bad,
considering that word-order information is not en-
coded in the IP. We can now compare several align-
ments in terms of both dictionary size and alignment
accuracy. For accuracy, we represent each alignment
as a set of tuples &lt; i, j, k &gt;, where i is the sentence
pair, j is a foreign index, and k is an English index.
We use these tuples to calculate a balanced f-score
against the gold alignment tuples.4
</bodyText>
<table confidence="0.992018666666667">
Method Dict size f-score
Gold 28 100.0
Monotone 39 68.9
IBM-1 (Brown et al., 1993) 30 80.3
IBM-4 (Brown et al., 1993) 29 86.9
IP 28 95.9
</table>
<bodyText confidence="0.864511681818182">
The last line shows an average f-score over the 8 tied
IP solutions.
Figure 4 further investigates the connection be-
tween our objective function and alignment accu-
racy. We sample up to 10 alignments at each of
several objective function values v, by first adding
a constraint that dict variables add to exactly v, then
iterating the n-best list procedure above. We stop
when we have 10 solutions, or when cplex fails to
find another solution with value v. In this figure, we
see a clear relationship between the objective func-
tion and alignment accuracy—minimizing the for-
mer is a good way to maximize the latter.
3This method not only suppresses the IP solutions generated
so far, but it suppresses additional solutions as well. In partic-
ular, it suppresses solutions in which all link and dict variables
have the same values as in some previous solution, but some
additional dict variables are flipped to 1. We consider this a fea-
ture rather than a bug, as it ensures that all alignments in the
n-best list are unique. For what we report in this paper, we only
create n-best lists whose elements possess the same objective
function value, so the issue does not arise.
</bodyText>
<footnote confidence="0.693420666666667">
4P = proportion of proposed links that are in gold,
R = proportion of gold links that are proposed, and f-
score = 2PR/(P+R).
</footnote>
<figure confidence="0.815576666666667">
28 30 32 34 36 38 40 42 44 46
Number of bilingual dictionary entries in solution
(objective function value)
</figure>
<figureCaption confidence="0.825253666666667">
Figure 4: Relationship between IP objective (x-axis =
size of induced bilingual dictionary) and alignment ac-
curacy (y-axis = f-score).
</figureCaption>
<figure confidence="0.579788">
Turkish English
yururum i walk
yururler they walk
</figure>
<figureCaption confidence="0.984844">
Figure 5: Two Turkish-English sentence pairs.
</figureCaption>
<sectionHeader confidence="0.996021" genericHeader="method">
3 Sub-Word Alignment
</sectionHeader>
<bodyText confidence="0.999929842105263">
We now turn to alignment at the sub-word level.
Agglutinative languages like Turkish present chal-
lenges for many standard NLP techniques. An ag-
glutinative language can express in a single word
(e.g., yurumuyoruz) what might require many words
in another language (e.g., we are not walking).
Naively breaking on whitespace results in a very
large vocabulary for Turkish, and it ignores the
multi-morpheme structure inside Turkish words.
Consider the tiny Turkish-English corpus in Fig-
ure 5. Even a non-Turkish speaker might plausi-
bly align yurur to walk, um to I, and ler to they.
However, none of the popular machine aligners
is able to do this, since they align at the whole-
word level. Designers of translation systems some-
times employ language-specific word breakers be-
fore alignment, though these are hard to build and
maintain, and they are usually not only language-
specific, but also language-pair-specific. Good un-
</bodyText>
<figure confidence="0.998140454545454">
Average alignment accuracy 1
(f-score) 0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
</figure>
<page confidence="0.999801">
31
</page>
<bodyText confidence="0.999077970588235">
supervised monolingual morpheme segmenters are
also available (Goldsmith, 2001; Creutz and Lagus,
2005), though again, these do not do joint inference
of alignment and word segmentation.
We extend our objective function straightfor-
wardly to sub-word alignment. To test our exten-
sion, we construct a Turkish-English corpus of 1616
sentence pairs. We first manually construct a regu-
lar tree grammar (RTG) (Gecseg and Steinby, 1984)
for a fragment of English. This grammar produces
English trees; it has 86 rules, 26 states, and 53 ter-
minals (English words). We then construct a tree-to-
string transducer (Rounds, 1970) that converts En-
glish trees into Turkish character strings, including
space. Because it does not explicitly enumerate the
Turkish vocabulary, this transducer can output a very
large number of distinct Turkish words (i.e., charac-
ter sequences preceded and followed by space). This
transducer has 177 rules, 18 states, and 23 termi-
nals (Turkish characters). RTG generation produces
English trees that the transducer converts to Turk-
ish, both via the tree automata toolkit Tiburon (May
and Knight, 2006). From this, we obtain a parallel
Turkish-English corpus. A fragment of the corpus is
shown in Figure 6. Because we will concentrate on
finding Turkish sub-words, we manually break off
the English sub-word -ing, by rule, as seen in the
last line of the figure.
This is a small corpus, but good for demonstrat-
ing our concept. By automatically tracing the inter-
nal operation of the tree transducer, we also produce
a gold alignment for the corpus. We use the gold
alignment to tabulate the number of morphemes per
Turkish word:
</bodyText>
<table confidence="0.8789106">
n % Turkish types % Turkish tokens
with n morphemes with n morphemes
1 23.1% 35.5%
2 63.5% 61.6%
3 13.4% 2.9%
</table>
<bodyText confidence="0.999745348837209">
Naturally, these statistics imply that standard whole-
word aligners will fail. By inspecting the corpus, we
find that 26.8 is the maximium f-score available to
whole-word alignment methods.
Now we adjust our IP formulation. We broaden
the definition of legal alignment to include breaking
any foreign word (token) into one or more sub-word
(tokens). Each resulting sub-word token is aligned
to exactly one English word token, and every En-
glish word aligns to 0 or 1 foreign sub-words. Our
dict-f-e variables now relate Turkish sub-words to
English words. The first sentence pair in Figure 5
would have previously contributed two dict vari-
ables; now it contributes 44, including things like
dict-uru-walk. We consider an alignment to be a set
of tuples &lt; i, j1, j2, k &gt;, where j1 and j2 are start
and end indices into the foreign character string. We
create align-i-j1-j2-k variables that connect Turkish
character spans with English word indices. Align-
ment variables constrain dictionary variables as be-
fore, i.e., an alignment link can only “turn on” when
licensed by the dictionary.
We previously constrained every Turkish word to
align to something. However, we do not want ev-
ery Turkish character span to align—only the spans
explicitly chosen in our word segmentation. So we
introduce span-i-j1-j2 variables to indicate segmen-
tation decisions. Only when span-i-j1-j2 = 1 do we
require Ek align-i-j1-j2-k = 1.
For a coherent segmentation, the set of active span
variables must cover all Turkish letter tokens in the
corpus, and no pair of spans may overlap each other.
To implement these constraints, we create a lattice
where each node represents a Turkish index, and
each transition corresponds to a span variable. In a
coherent segmentation, the sum of all span variables
entering an lattice-internal node equals the sum of
all span variables leaving that node. If the sum of
all variables leaving the start node equals 1, then we
are guaranteed a left-to-right path through the lat-
tice, i.e., a coherent choice of 0 and 1 values for span
variables.
The IP reads as follows:
</bodyText>
<equation confidence="0.973593307692308">
minimize:
E
f,e dict-f-e
subject to:
E
Vi,j1,j2 k align-i-j1-j2-k = span-i-j1-j2
E
Vi,k j1,j2 align-i-j1-j2-k &lt; 1
Vi,j1,j2,k align-i-j1-j2-k &lt; dict-fi,j1,j2-ei,k
E
Vi,j j3 span-i-j3-j = Ej3 span-i-j-j3
E
Vi,w j&gt;w span-i-w-j = 1
</equation>
<bodyText confidence="0.886785">
(w ranges over Turkish word start indices)
With our simple objective function, we obtain an
f-score of 61.4 against the gold standard. Sample
gold and IP alignments are shown in Figure 7.
</bodyText>
<page confidence="0.988552">
32
</page>
<bodyText confidence="0.964919">
Turkish English
onlari gordum i saw them
gidecekler they will go
onu stadyumda gordum i saw him in the stadium
ogretmenlerim tiyatroya yurudu my teachers walked to the theatre
cocuklar yurudu the girls walked
babam restorana gidiyor my father is walk ing to the restaurant
. . . . . .
</bodyText>
<figureCaption confidence="0.9973145">
Figure 6: A Turkish-English corpus produced by an English grammar pipelined with an English-to-Turkish tree-to-
string transducer.
</figureCaption>
<subsectionHeader confidence="0.850662">
Gold alignment
</subsectionHeader>
<bodyText confidence="0.93810175">
you go to his office
onun ofisi- -ne gider- -sin
my teachers ran to their house
ogretmenler- -im onlarin evi- -ne kostu
</bodyText>
<sectionHeader confidence="0.507851" genericHeader="method">
IP sub-word alignment
</sectionHeader>
<bodyText confidence="0.862425476190476">
you go to his office
onun ofisi- -ne gider- -sin
my teachers ran to their house
ogretmenler- -im onlarin evi- -ne kostu
3. i saw him
onu gordu--m
my aunt goes to their house
hala- -m onlarin evi- -ne gider
5. we go to the theatre
tiyatro- -ya gider- -iz
6. they walked to the store
magaza- -ya yurudu- -ler
4.
i saw him
onu gordu--m
my aunt goes to their house
hal- -am onlarin evi- -ne gider
we go to the theatre
tiyatro- -ya gider- -iz
they walked to the store
magaza- -ya yurudu- -ler
</bodyText>
<figureCaption confidence="0.919125666666667">
Figure 7: Sample gold and (initial) IP sub-word alignments on our Turkish-English corpus. Dashes indicate where the
IP search has decided to break Turkish words in the process of aligning. For examples, the word magazaya has been
broken into magaza- and -ya.
</figureCaption>
<page confidence="0.997526">
33
</page>
<bodyText confidence="0.9999419">
The last two incorrect alignments in the figure
are instructive. The system has decided to align
English the to the Turkish noun morphemes tiyatro
and magaza, and to leave English nouns theatre and
store unaligned. This is a tie-break decision. It is
equally good for the objective function to leave the
unaligned instead—either way, there are two rele-
vant dictionary entries.
We fix this problem by introducing a special
NULL Turkish token, and by modifying the IP to re-
quire every English token to align (either to NULL
or something else). This introduces a cost for fail-
ing to align an English token x to Turkish, because
a new x/NULL dictionary entry will have to be cre-
ated. (The NULL token itself is unconstrained in
how many tokens it may align to.)
Under this scheme, the last two incorrect align-
ments in Figure 7 induce four relevant dictio-
nary entries (the/tiyatro, the/magaza, theatre/NULL,
store/NULL) while the gold alignment induces only
three (the/NULL, theatre/tiyatro, store/magaza), be-
cause the/NULL is re-used. The gold alignment is
therefore now preferred by the IP optimizer. There
is a rippling effect, causing the system to correct
many other decisions as well. This revision raises
the alignment f-score from 61.4 to 83.4.
The following table summarizes our alignment re-
sults. In the table, “Dict” refers to the size of the
induced dictionary, and “Sub-words” refers to the
number of induced Turkish sub-word tokens.
</bodyText>
<table confidence="0.995285">
Method Dict Sub-words f-score
Gold (sub-word) 67 8102 100.0
Monotone (word) 512 4851 5.5
IBM-1 (word) 220 4851 21.6
IBM-4 (word) 230 4851 20.3
IP (word) 107 4851 20.1
IP (sub-word, 60 7418 61.4
initial)
IP (sub-word, 65 8105 83.4
revised)
</table>
<bodyText confidence="0.9989695">
Our search for an optimal IP solution is not fast.
It takes 1-5 hours to perform sub-word alignment on
the Turkish-English corpus. Of course, if we wanted
to obtain optimal alignments under IBM Model 4,
that would also be expensive, in fact NP-complete
(Raghavendra and Maji, 2006). Practical Model 4
systems therefore make substantial search approxi-
mations (Brown et al., 1993).
</bodyText>
<sectionHeader confidence="0.999887" genericHeader="evaluation">
4 Related Work
</sectionHeader>
<bodyText confidence="0.9995965">
(Zhang et al., 2003) and (Wu, 1997) tackle the prob-
lem of segmenting Chinese while aligning it to En-
glish. (Snyder and Barzilay, 2008) use multilingual
data to compute segmentations of Arabic, Hebrew,
Aramaic, and English. Their method uses IBM mod-
els to bootstrap alignments, and they measure the re-
sulting segmentation accuracy.
(Taskar et al., 2005) cast their alignment model as
a minimum cost quadratic flow problem, for which
optimal alignments can be computed with off-the-
shelf optimizers. Alignment in the modified model
of (Lacoste-Julien et al., 2006) can be mapped to a
quadratic assignment problem and solved with linear
programming tools. In that work, linear program-
ming is not only used for alignment, but also for
training weights for the discriminative model. These
weights are trained on a manually-aligned subset of
the parallel data. One important “mega” feature for
the discriminative model is the score assigned by an
IBM model, which must be separately trained on the
full parallel data. Our work differs in two ways: (1)
our training is unsupervised, requiring no manually
aligned data, and (2) we do not bootstrap off IBM
models. (DeNero and Klein, 2008) gives an integer
linear programming formulation of another align-
ment model based on phrases. There, integer pro-
gramming is used only for alignment, not for learn-
ing parameter values.
</bodyText>
<sectionHeader confidence="0.997754" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.9992215">
We have presented a novel objective function for
alignment, and we have applied it to whole-word and
sub-word alignment problems. Preliminary results
look good, especially given that new objective func-
tion is simpler than those previously proposed. The
integer programming framework makes the model
easy to implement, and its optimal behavior frees us
from worrying about search errors.
We believe there are good future possibilities for
this work:
</bodyText>
<listItem confidence="0.943045333333333">
• Extend legal alignments to cover n-to-m
and discontinuous cases. While morpheme-
to-morpheme alignment is more frequently a
</listItem>
<page confidence="0.997718">
34
</page>
<bodyText confidence="0.776595">
1-to-1 affair than word-to-word alignment is,
the 1-to-1 assumption is not justified in either
case.
• Develop new components for the IP objec-
tive. Our current objective function makes no
reference to word order, so if the same word
appears twice in a sentence, a tie-break en-
sues.
</bodyText>
<listItem confidence="0.991135428571428">
• Establish complexity bounds for optimiz-
ing dictionary size. We conjecture that opti-
mal alignment according to our model is NP-
complete in the size of the corpus.
• Develop a fast, approximate alignment al-
gorithm for our model.
• Test on large-scale bilingual corpora.
</listItem>
<sectionHeader confidence="0.998861" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.984685">
This work was partially supported under DARPA
GALE, Contract No. HR0011-06-C-0022.
</bodyText>
<sectionHeader confidence="0.997928" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999692046153846">
P. Brown, V. Della Pietra, S. Della Pietra, and R. Mercer.
1993. The mathematics of statistical machine trans-
lation: Parameter estimation. Computational linguis-
tics, 19(2).
D. Chiang. 2007. Hierarchical phrase-based translation.
Computational Linguistics, 33(2).
M. Creutz and K. Lagus. 2005. Inducing the morpho-
logical lexicon of a natural language from unannotated
text. In Proc. AKRR.
J. DeNero and D. Klein. 2008. The complexity of phrase
alignment problems. In Proc. ACL.
A. Fraser and D. Marcu. 2006. Semi-supervised training
for statistical word alignment. In Proc. ACL-COLING.
A. Fraser and D. Marcu. 2007. Getting the structure right
for word alignment: LEAF. In Proc. EMNLP-CoNLL.
M. Galley, M. Hopkins, K. Knight, and D Marcu. 2004.
What’s in a translation rule. In Proc. NAACL-HLT.
F. Gecseg and M. Steinby. 1984. Tree automata.
Akademiai Kiado.
J. Goldsmith. 2001. Unsupervised learning of the mor-
phology of a natural language. Computational Lin-
guistics, 27(2).
K. Knight. 1997. Automating knowledge acquisition for
machine translation. AI Magazine, 18(4).
S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan.
2006. Word alignment via quadratic assignment. In
Proc. HLT-NAACL.
J. May and K. Knight. 2006. Tiburon: A weighted tree
automata toolkit. In Proc. CIAA.
I. D. Melamed. 1997. A word-to-word model of transla-
tional equivalence. In Proc. ACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30(4).
F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Ya-
mada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng,
V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord
of features for statistical machine translation. In Proc.
HLT-NAACL.
C. Quirk, A. Menezes, and C. Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal SMT. In Proc. ACL.
U. Raghavendra and H. K. Maji. 2006. Computational
complexity of statistical machine translation. In Proc.
EACL.
W. Rounds. 1970. Mappings and grammars on trees.
Theory of Computing Systems, 4(3).
P. Sch¨onhofen, A. Bencz´ur, I. Bir´o, and K. Csaloginy,
2008. Cross-language retrieval with wikipedia.
Springer.
B. Snyder and R. Barzilay. 2008. Unsupervised multi-
lingual learning for morphological segmentation. In
Proc. ACL.
B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A dis-
criminative matching approach to word alignment. In
Proc. EMNLP.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based
word alignment in statistical translation. In Proc. ACL.
D. Wu. 1997. Stochastic inversion transduction gram-
mars and bilingual parsing of parallel corpora. Com-
putational Linguistics, 23(3).
Y. Zhang, S. Vogel, and A. Waibel. 2003. Integrated
phrase segmentation and alignment algorithm for sta-
tistical machine translation. In Proc. Intl. Conf. on
NLP and KE.
</reference>
<page confidence="0.999317">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.987260">
<title confidence="0.999907">A New Objective Function for Word Alignment</title>
<author confidence="0.995123">Tugba Bodrumlu Kevin Knight Sujith Ravi</author>
<affiliation confidence="0.999524">Information Sciences Institute &amp; Computer Science Department University of Southern California</affiliation>
<email confidence="0.999813">bodrumlu@usc.edu,knight@isi.edu,sravi@isi.edu</email>
<abstract confidence="0.999439333333333">We develop a new objective function for word alignment that measures the size of the bilingual dictionary induced by an alignment. A word alignment that results in a small dictionary is preferred over one that results in a large dictionary. In order to search for the alignment that minimizes this objective, we cast the problem as an integer linear program. We then extend our objective function to align corpora at the sub-word level, which we demonstrate on a small Turkish-English corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>V Della Pietra</author>
<author>S Della Pietra</author>
<author>R Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="1588" citStr="Brown et al., 1993" startWordPosition="245" endWordPosition="248">or example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f I e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat </context>
<context position="6257" citStr="Brown et al., 1993" startWordPosition="1016" endWordPosition="1019">g garcia/garcia, are/son, are/estan, not/no, has/tiene, etc. 29 Figure 3: IP alignment. The induced bilingual dictionary has 28 distinct entries. shows the gold alignment for the corpus in Figure 1 (displayed here as English-Spanish), which results in 28 distinct bilingual dictionary entries. By contrast, a monotone alignment induces 39 distinct entries, due to less re-use. Next we look at how to automatically rifle through all legal alignments to find the one with the best score. What is a legal alignment? For now, we consider it to be one where: • Every foreign word is aligned exactly once (Brown et al., 1993). • Every English word has either 0 or 1 alignments (Melamed, 1997). We formulate our integer program (IP) as follows. We set up two types of binary variables: • Alignment link variables. If link-i-j-k = 1, that means in sentence pair i, the foreign word at position j aligns to the English words at position k. • Bilingual dictionary variables. If dict-f-e = 1, that means word pair (f, e) is “in” the dictionary. We constrain the values of link variables to satisfy the two alignment conditions listed earlier. We also require that if link-i-j-k = 1 (i.e., we’ve decided on an alignment link), then</context>
<context position="9408" citStr="Brown et al., 1993" startWordPosition="1564" endWordPosition="1567">e are 8 distinct solutions that yield the same objective function value of 28. Figure 3 shows one of these. This alignment is not bad, considering that word-order information is not encoded in the IP. We can now compare several alignments in terms of both dictionary size and alignment accuracy. For accuracy, we represent each alignment as a set of tuples &lt; i, j, k &gt;, where i is the sentence pair, j is a foreign index, and k is an English index. We use these tuples to calculate a balanced f-score against the gold alignment tuples.4 Method Dict size f-score Gold 28 100.0 Monotone 39 68.9 IBM-1 (Brown et al., 1993) 30 80.3 IBM-4 (Brown et al., 1993) 29 86.9 IP 28 95.9 The last line shows an average f-score over the 8 tied IP solutions. Figure 4 further investigates the connection between our objective function and alignment accuracy. We sample up to 10 alignments at each of several objective function values v, by first adding a constraint that dict variables add to exactly v, then iterating the n-best list procedure above. We stop when we have 10 solutions, or when cplex fails to find another solution with value v. In this figure, we see a clear relationship between the objective function and alignment </context>
<context position="19755" citStr="Brown et al., 1993" startWordPosition="3292" endWordPosition="3295">. Method Dict Sub-words f-score Gold (sub-word) 67 8102 100.0 Monotone (word) 512 4851 5.5 IBM-1 (word) 220 4851 21.6 IBM-4 (word) 230 4851 20.3 IP (word) 107 4851 20.1 IP (sub-word, 60 7418 61.4 initial) IP (sub-word, 65 8105 83.4 revised) Our search for an optimal IP solution is not fast. It takes 1-5 hours to perform sub-word alignment on the Turkish-English corpus. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 systems therefore make substantial search approximations (Brown et al., 1993). 4 Related Work (Zhang et al., 2003) and (Wu, 1997) tackle the problem of segmenting Chinese while aligning it to English. (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Their method uses IBM models to bootstrap alignments, and they measure the resulting segmentation accuracy. (Taskar et al., 2005) cast their alignment model as a minimum cost quadratic flow problem, for which optimal alignments can be computed with off-theshelf optimizers. Alignment in the modified model of (Lacoste-Julien et al., 2006) can be mapped to a qu</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, V. Della Pietra, S. Della Pietra, and R. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational linguistics, 19(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="1236" citStr="Chiang, 2007" startWordPosition="193" endWordPosition="194">function to align corpora at the sub-word level, which we demonstrate on a small Turkish-English corpus. 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f I e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok s</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>D. Chiang. 2007. Hierarchical phrase-based translation. Computational Linguistics, 33(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Creutz</author>
<author>K Lagus</author>
</authors>
<title>Inducing the morphological lexicon of a natural language from unannotated text.</title>
<date>2005</date>
<booktitle>In Proc. AKRR.</booktitle>
<contexts>
<context position="12227" citStr="Creutz and Lagus, 2005" startWordPosition="2037" endWordPosition="2040">5. Even a non-Turkish speaker might plausibly align yurur to walk, um to I, and ler to they. However, none of the popular machine aligners is able to do this, since they align at the wholeword level. Designers of translation systems sometimes employ language-specific word breakers before alignment, though these are hard to build and maintain, and they are usually not only languagespecific, but also language-pair-specific. Good unAverage alignment accuracy 1 (f-score) 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 31 supervised monolingual morpheme segmenters are also available (Goldsmith, 2001; Creutz and Lagus, 2005), though again, these do not do joint inference of alignment and word segmentation. We extend our objective function straightforwardly to sub-word alignment. To test our extension, we construct a Turkish-English corpus of 1616 sentence pairs. We first manually construct a regular tree grammar (RTG) (Gecseg and Steinby, 1984) for a fragment of English. This grammar produces English trees; it has 86 rules, 26 states, and 53 terminals (English words). We then construct a tree-tostring transducer (Rounds, 1970) that converts English trees into Turkish character strings, including space. Because it</context>
</contexts>
<marker>Creutz, Lagus, 2005</marker>
<rawString>M. Creutz and K. Lagus. 2005. Inducing the morphological lexicon of a natural language from unannotated text. In Proc. AKRR.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J DeNero</author>
<author>D Klein</author>
</authors>
<title>The complexity of phrase alignment problems.</title>
<date>2008</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="20951" citStr="DeNero and Klein, 2008" startWordPosition="3486" endWordPosition="3489">2006) can be mapped to a quadratic assignment problem and solved with linear programming tools. In that work, linear programming is not only used for alignment, but also for training weights for the discriminative model. These weights are trained on a manually-aligned subset of the parallel data. One important “mega” feature for the discriminative model is the score assigned by an IBM model, which must be separately trained on the full parallel data. Our work differs in two ways: (1) our training is unsupervised, requiring no manually aligned data, and (2) we do not bootstrap off IBM models. (DeNero and Klein, 2008) gives an integer linear programming formulation of another alignment model based on phrases. There, integer programming is used only for alignment, not for learning parameter values. 5 Conclusions and Future Work We have presented a novel objective function for alignment, and we have applied it to whole-word and sub-word alignment problems. Preliminary results look good, especially given that new objective function is simpler than those previously proposed. The integer programming framework makes the model easy to implement, and its optimal behavior frees us from worrying about search errors.</context>
</contexts>
<marker>DeNero, Klein, 2008</marker>
<rawString>J. DeNero and D. Klein. 2008. The complexity of phrase alignment problems. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Semi-supervised training for statistical word alignment.</title>
<date>2006</date>
<booktitle>In Proc. ACL-COLING.</booktitle>
<contexts>
<context position="3019" citStr="Fraser and Marcu, 2006" startWordPosition="482" endWordPosition="485">re 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28–35, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics hidden alignment variables. EM algorithms estimate dictionary and other probabilities in order to maximize those quantities. One can then ask for Viterbi alignments that maximize P(alignment |e, f). Discriminative models, e.g. (Taskar et al., 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set. EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. Low accuracy is a weakness for all systems. Most practitioners still use 1990s algorithms to align their data. It stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dictionary entry, they like to reuse that entry as much as possible. Previous machine aligners emulate this to some degree, but they are n</context>
</contexts>
<marker>Fraser, Marcu, 2006</marker>
<rawString>A. Fraser and D. Marcu. 2006. Semi-supervised training for statistical word alignment. In Proc. ACL-COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fraser</author>
<author>D Marcu</author>
</authors>
<title>Getting the structure right for word alignment: LEAF.</title>
<date>2007</date>
<booktitle>In Proc. EMNLP-CoNLL.</booktitle>
<contexts>
<context position="1665" citStr="Fraser and Marcu, 2007" startWordPosition="259" endWordPosition="262">d alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f I e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat</context>
</contexts>
<marker>Fraser, Marcu, 2007</marker>
<rawString>A. Fraser and D. Marcu. 2007. Getting the structure right for word alignment: LEAF. In Proc. EMNLP-CoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Galley</author>
<author>M Hopkins</author>
<author>K Knight</author>
<author>D Marcu</author>
</authors>
<title>What’s in a translation rule. In</title>
<date>2004</date>
<booktitle>Proc. NAACL-HLT.</booktitle>
<contexts>
<context position="1222" citStr="Galley et al., 2004" startWordPosition="189" endWordPosition="192">extend our objective function to align corpora at the sub-word level, which we demonstrate on a small Turkish-English corpus. 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f I e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-vo</context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>M. Galley, M. Hopkins, K. Knight, and D Marcu. 2004. What’s in a translation rule. In Proc. NAACL-HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Gecseg</author>
<author>M Steinby</author>
</authors>
<title>Tree automata. Akademiai Kiado.</title>
<date>1984</date>
<contexts>
<context position="12553" citStr="Gecseg and Steinby, 1984" startWordPosition="2088" endWordPosition="2091"> build and maintain, and they are usually not only languagespecific, but also language-pair-specific. Good unAverage alignment accuracy 1 (f-score) 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 31 supervised monolingual morpheme segmenters are also available (Goldsmith, 2001; Creutz and Lagus, 2005), though again, these do not do joint inference of alignment and word segmentation. We extend our objective function straightforwardly to sub-word alignment. To test our extension, we construct a Turkish-English corpus of 1616 sentence pairs. We first manually construct a regular tree grammar (RTG) (Gecseg and Steinby, 1984) for a fragment of English. This grammar produces English trees; it has 86 rules, 26 states, and 53 terminals (English words). We then construct a tree-tostring transducer (Rounds, 1970) that converts English trees into Turkish character strings, including space. Because it does not explicitly enumerate the Turkish vocabulary, this transducer can output a very large number of distinct Turkish words (i.e., character sequences preceded and followed by space). This transducer has 177 rules, 18 states, and 23 terminals (Turkish characters). RTG generation produces English trees that the transducer</context>
</contexts>
<marker>Gecseg, Steinby, 1984</marker>
<rawString>F. Gecseg and M. Steinby. 1984. Tree automata. Akademiai Kiado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Goldsmith</author>
</authors>
<title>Unsupervised learning of the morphology of a natural language.</title>
<date>2001</date>
<journal>Computational Linguistics,</journal>
<volume>27</volume>
<issue>2</issue>
<contexts>
<context position="12202" citStr="Goldsmith, 2001" startWordPosition="2035" endWordPosition="2036">corpus in Figure 5. Even a non-Turkish speaker might plausibly align yurur to walk, um to I, and ler to they. However, none of the popular machine aligners is able to do this, since they align at the wholeword level. Designers of translation systems sometimes employ language-specific word breakers before alignment, though these are hard to build and maintain, and they are usually not only languagespecific, but also language-pair-specific. Good unAverage alignment accuracy 1 (f-score) 0.95 0.9 0.85 0.8 0.75 0.7 0.65 0.6 0.55 0.5 31 supervised monolingual morpheme segmenters are also available (Goldsmith, 2001; Creutz and Lagus, 2005), though again, these do not do joint inference of alignment and word segmentation. We extend our objective function straightforwardly to sub-word alignment. To test our extension, we construct a Turkish-English corpus of 1616 sentence pairs. We first manually construct a regular tree grammar (RTG) (Gecseg and Steinby, 1984) for a fragment of English. This grammar produces English trees; it has 86 rules, 26 states, and 53 terminals (English words). We then construct a tree-tostring transducer (Rounds, 1970) that converts English trees into Turkish character strings, in</context>
</contexts>
<marker>Goldsmith, 2001</marker>
<rawString>J. Goldsmith. 2001. Unsupervised learning of the morphology of a natural language. Computational Linguistics, 27(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Knight</author>
</authors>
<title>Automating knowledge acquisition for machine translation.</title>
<date>1997</date>
<journal>AI Magazine,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="923" citStr="Knight, 1997" startWordPosition="143" endWordPosition="144">ize of the bilingual dictionary induced by an alignment. A word alignment that results in a small dictionary is preferred over one that results in a large dictionary. In order to search for the alignment that minimizes this objective, we cast the problem as an integer linear program. We then extend our objective function to align corpora at the sub-word level, which we demonstrate on a small Turkish-English corpus. 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. </context>
<context position="2440" citStr="Knight, 1997" startWordPosition="401" endWordPosition="402">ok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghirok clok wat nnat gat mat bat hilat lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28–35, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics hidden alignment variables. EM algorithms estimate dictionary and other probabilities in order to maximize those quantities. One can then ask for Viterbi alignments that maximize P(alignment |e, f). Discriminative models, e.g. (Taskar et al., 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set. EMD training (Fraser and Marcu, 2006) combines generative </context>
</contexts>
<marker>Knight, 1997</marker>
<rawString>K. Knight. 1997. Automating knowledge acquisition for machine translation. AI Magazine, 18(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Lacoste-Julien</author>
<author>B Taskar</author>
<author>D Klein</author>
<author>M Jordan</author>
</authors>
<title>Word alignment via quadratic assignment.</title>
<date>2006</date>
<booktitle>In Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="20333" citStr="Lacoste-Julien et al., 2006" startWordPosition="3384" endWordPosition="3387">tantial search approximations (Brown et al., 1993). 4 Related Work (Zhang et al., 2003) and (Wu, 1997) tackle the problem of segmenting Chinese while aligning it to English. (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Their method uses IBM models to bootstrap alignments, and they measure the resulting segmentation accuracy. (Taskar et al., 2005) cast their alignment model as a minimum cost quadratic flow problem, for which optimal alignments can be computed with off-theshelf optimizers. Alignment in the modified model of (Lacoste-Julien et al., 2006) can be mapped to a quadratic assignment problem and solved with linear programming tools. In that work, linear programming is not only used for alignment, but also for training weights for the discriminative model. These weights are trained on a manually-aligned subset of the parallel data. One important “mega” feature for the discriminative model is the score assigned by an IBM model, which must be separately trained on the full parallel data. Our work differs in two ways: (1) our training is unsupervised, requiring no manually aligned data, and (2) we do not bootstrap off IBM models. (DeNer</context>
</contexts>
<marker>Lacoste-Julien, Taskar, Klein, Jordan, 2006</marker>
<rawString>S. Lacoste-Julien, B. Taskar, D. Klein, and M. Jordan. 2006. Word alignment via quadratic assignment. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J May</author>
<author>K Knight</author>
</authors>
<title>Tiburon: A weighted tree automata toolkit.</title>
<date>2006</date>
<booktitle>In Proc. CIAA.</booktitle>
<contexts>
<context position="13240" citStr="May and Knight, 2006" startWordPosition="2196" endWordPosition="2199">t has 86 rules, 26 states, and 53 terminals (English words). We then construct a tree-tostring transducer (Rounds, 1970) that converts English trees into Turkish character strings, including space. Because it does not explicitly enumerate the Turkish vocabulary, this transducer can output a very large number of distinct Turkish words (i.e., character sequences preceded and followed by space). This transducer has 177 rules, 18 states, and 23 terminals (Turkish characters). RTG generation produces English trees that the transducer converts to Turkish, both via the tree automata toolkit Tiburon (May and Knight, 2006). From this, we obtain a parallel Turkish-English corpus. A fragment of the corpus is shown in Figure 6. Because we will concentrate on finding Turkish sub-words, we manually break off the English sub-word -ing, by rule, as seen in the last line of the figure. This is a small corpus, but good for demonstrating our concept. By automatically tracing the internal operation of the tree transducer, we also produce a gold alignment for the corpus. We use the gold alignment to tabulate the number of morphemes per Turkish word: n % Turkish types % Turkish tokens with n morphemes with n morphemes 1 23.</context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>J. May and K. Knight. 2006. Tiburon: A weighted tree automata toolkit. In Proc. CIAA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I D Melamed</author>
</authors>
<title>A word-to-word model of translational equivalence.</title>
<date>1997</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="6324" citStr="Melamed, 1997" startWordPosition="1031" endWordPosition="1032">: IP alignment. The induced bilingual dictionary has 28 distinct entries. shows the gold alignment for the corpus in Figure 1 (displayed here as English-Spanish), which results in 28 distinct bilingual dictionary entries. By contrast, a monotone alignment induces 39 distinct entries, due to less re-use. Next we look at how to automatically rifle through all legal alignments to find the one with the best score. What is a legal alignment? For now, we consider it to be one where: • Every foreign word is aligned exactly once (Brown et al., 1993). • Every English word has either 0 or 1 alignments (Melamed, 1997). We formulate our integer program (IP) as follows. We set up two types of binary variables: • Alignment link variables. If link-i-j-k = 1, that means in sentence pair i, the foreign word at position j aligns to the English words at position k. • Bilingual dictionary variables. If dict-f-e = 1, that means word pair (f, e) is “in” the dictionary. We constrain the values of link variables to satisfy the two alignment conditions listed earlier. We also require that if link-i-j-k = 1 (i.e., we’ve decided on an alignment link), then dict-fij-eik should also equal 1 (the linked words are recorded as</context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. D. Melamed. 1997. A word-to-word model of translational equivalence. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>4</issue>
<contexts>
<context position="1201" citStr="Och and Ney, 2004" startWordPosition="185" endWordPosition="188">r program. We then extend our objective function to align corpora at the sub-word level, which we demonstrate on a small Turkish-English corpus. 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f I e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat </context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>D Gildea</author>
<author>S Khudanpur</author>
<author>A Sarkar</author>
<author>K Yamada</author>
<author>A Fraser</author>
<author>S Kumar</author>
<author>L Shen</author>
<author>D Smith</author>
<author>K Eng</author>
<author>V Jain</author>
<author>Z Jin</author>
<author>D Radev</author>
</authors>
<title>A smorgasbord of features for statistical machine translation. In</title>
<date>2004</date>
<booktitle>Proc. HLT-NAACL.</booktitle>
<contexts>
<context position="1465" citStr="Och et al., 2004" startWordPosition="227" endWordPosition="230">ame meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f I e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wa</context>
</contexts>
<marker>Och, Gildea, Khudanpur, Sarkar, Yamada, Fraser, Kumar, Shen, Smith, Eng, Jain, Jin, Radev, 2004</marker>
<rawString>F. J. Och, D. Gildea, S. Khudanpur, A. Sarkar, K. Yamada, A. Fraser, S. Kumar, L. Shen, D. Smith, K. Eng, V. Jain, Z. Jin, and D. Radev. 2004. A smorgasbord of features for statistical machine translation. In Proc. HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Quirk</author>
<author>A Menezes</author>
<author>C Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal SMT.</title>
<date>2005</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1257" citStr="Quirk et al., 2005" startWordPosition="195" endWordPosition="198">ign corpora at the sub-word level, which we demonstrate on a small Turkish-English corpus. 1 Introduction Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Figure 1 shows sample input for a word aligner (Knight, 1997). After analyzing the text, we may conclude, for example, that sprok corresponds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f I e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voo</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>C. Quirk, A. Menezes, and C. Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal SMT. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>U Raghavendra</author>
<author>H K Maji</author>
</authors>
<title>Computational complexity of statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proc. EACL.</booktitle>
<contexts>
<context position="19658" citStr="Raghavendra and Maji, 2006" startWordPosition="3278" endWordPosition="3281">e size of the induced dictionary, and “Sub-words” refers to the number of induced Turkish sub-word tokens. Method Dict Sub-words f-score Gold (sub-word) 67 8102 100.0 Monotone (word) 512 4851 5.5 IBM-1 (word) 220 4851 21.6 IBM-4 (word) 230 4851 20.3 IP (word) 107 4851 20.1 IP (sub-word, 60 7418 61.4 initial) IP (sub-word, 65 8105 83.4 revised) Our search for an optimal IP solution is not fast. It takes 1-5 hours to perform sub-word alignment on the Turkish-English corpus. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 systems therefore make substantial search approximations (Brown et al., 1993). 4 Related Work (Zhang et al., 2003) and (Wu, 1997) tackle the problem of segmenting Chinese while aligning it to English. (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Their method uses IBM models to bootstrap alignments, and they measure the resulting segmentation accuracy. (Taskar et al., 2005) cast their alignment model as a minimum cost quadratic flow problem, for which optimal alignments can be computed with off-theshelf o</context>
</contexts>
<marker>Raghavendra, Maji, 2006</marker>
<rawString>U. Raghavendra and H. K. Maji. 2006. Computational complexity of statistical machine translation. In Proc. EACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Rounds</author>
</authors>
<title>Mappings and grammars on trees.</title>
<date>1970</date>
<journal>Theory of Computing Systems,</journal>
<volume>4</volume>
<issue>3</issue>
<publisher>Springer.</publisher>
<contexts>
<context position="12739" citStr="Rounds, 1970" startWordPosition="2121" endWordPosition="2122">pervised monolingual morpheme segmenters are also available (Goldsmith, 2001; Creutz and Lagus, 2005), though again, these do not do joint inference of alignment and word segmentation. We extend our objective function straightforwardly to sub-word alignment. To test our extension, we construct a Turkish-English corpus of 1616 sentence pairs. We first manually construct a regular tree grammar (RTG) (Gecseg and Steinby, 1984) for a fragment of English. This grammar produces English trees; it has 86 rules, 26 states, and 53 terminals (English words). We then construct a tree-tostring transducer (Rounds, 1970) that converts English trees into Turkish character strings, including space. Because it does not explicitly enumerate the Turkish vocabulary, this transducer can output a very large number of distinct Turkish words (i.e., character sequences preceded and followed by space). This transducer has 177 rules, 18 states, and 23 terminals (Turkish characters). RTG generation produces English trees that the transducer converts to Turkish, both via the tree automata toolkit Tiburon (May and Knight, 2006). From this, we obtain a parallel Turkish-English corpus. A fragment of the corpus is shown in Figu</context>
</contexts>
<marker>Rounds, 1970</marker>
<rawString>W. Rounds. 1970. Mappings and grammars on trees. Theory of Computing Systems, 4(3). P. Sch¨onhofen, A. Bencz´ur, I. Bir´o, and K. Csaloginy, 2008. Cross-language retrieval with wikipedia. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Snyder</author>
<author>R Barzilay</author>
</authors>
<title>Unsupervised multilingual learning for morphological segmentation. In</title>
<date>2008</date>
<booktitle>Proc. ACL.</booktitle>
<contexts>
<context position="3809" citStr="Snyder and Barzilay, 2008" startWordPosition="616" endWordPosition="620">stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dictionary entry, they like to reuse that entry as much as possible. Previous machine aligners emulate this to some degree, but they are not explicitly programmed to do so. We also address another weakness of current aligners: they only align full words. With few exceptions, e.g. (Zhang et al., 2003; Snyder and Barzilay, 2008), aligners do not operate at the sub-word level, making them much less useful for agglutinative languages such as Turkish. Our present contributions are as follows: • We offer a simple new objective function that scores a corpus alignment based on how many distinct bilingual word pairs it contains. • We use an integer programming solver to carry out optimization and corpus alignment. • We extend the system to perform subword alignment, which we demonstrate on a Turkish-English corpus. The results in this paper constitute a proof of concept of these ideas, executed on small corpora. We conclude</context>
<context position="19906" citStr="Snyder and Barzilay, 2008" startWordPosition="3319" endWordPosition="3322"> IP (word) 107 4851 20.1 IP (sub-word, 60 7418 61.4 initial) IP (sub-word, 65 8105 83.4 revised) Our search for an optimal IP solution is not fast. It takes 1-5 hours to perform sub-word alignment on the Turkish-English corpus. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 systems therefore make substantial search approximations (Brown et al., 1993). 4 Related Work (Zhang et al., 2003) and (Wu, 1997) tackle the problem of segmenting Chinese while aligning it to English. (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Their method uses IBM models to bootstrap alignments, and they measure the resulting segmentation accuracy. (Taskar et al., 2005) cast their alignment model as a minimum cost quadratic flow problem, for which optimal alignments can be computed with off-theshelf optimizers. Alignment in the modified model of (Lacoste-Julien et al., 2006) can be mapped to a quadratic assignment problem and solved with linear programming tools. In that work, linear programming is not only used for alignment, but also for trai</context>
</contexts>
<marker>Snyder, Barzilay, 2008</marker>
<rawString>B. Snyder and R. Barzilay. 2008. Unsupervised multilingual learning for morphological segmentation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Taskar</author>
<author>S Lacoste-Julien</author>
<author>D Klein</author>
</authors>
<title>A discriminative matching approach to word alignment.</title>
<date>2005</date>
<booktitle>In Proc. EMNLP.</booktitle>
<contexts>
<context position="2886" citStr="Taskar et al., 2005" startWordPosition="462" endWordPosition="465">lalok nok crrrok hihok yorok zanzanok wat nnat arrat mat zanzanat lalok rarok nok izok hihok mok wat nnat forat arrat vat gat Figure 1: Word alignment exercise (Knight, 1997). 28 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing, pages 28–35, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics hidden alignment variables. EM algorithms estimate dictionary and other probabilities in order to maximize those quantities. One can then ask for Viterbi alignments that maximize P(alignment |e, f). Discriminative models, e.g. (Taskar et al., 2005), instead set parameters to maximize alignment accuracy against a hand-aligned development set. EMD training (Fraser and Marcu, 2006) combines generative and discriminative elements. Low accuracy is a weakness for all systems. Most practitioners still use 1990s algorithms to align their data. It stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dicti</context>
<context position="20124" citStr="Taskar et al., 2005" startWordPosition="3352" endWordPosition="3355">us. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 systems therefore make substantial search approximations (Brown et al., 1993). 4 Related Work (Zhang et al., 2003) and (Wu, 1997) tackle the problem of segmenting Chinese while aligning it to English. (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Their method uses IBM models to bootstrap alignments, and they measure the resulting segmentation accuracy. (Taskar et al., 2005) cast their alignment model as a minimum cost quadratic flow problem, for which optimal alignments can be computed with off-theshelf optimizers. Alignment in the modified model of (Lacoste-Julien et al., 2006) can be mapped to a quadratic assignment problem and solved with linear programming tools. In that work, linear programming is not only used for alignment, but also for training weights for the discriminative model. These weights are trained on a manually-aligned subset of the parallel data. One important “mega” feature for the discriminative model is the score assigned by an IBM model, w</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>B. Taskar, S. Lacoste-Julien, and D. Klein. 2005. A discriminative matching approach to word alignment. In Proc. EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMM-based word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In Proc. ACL.</booktitle>
<contexts>
<context position="1614" citStr="Vogel et al., 1996" startWordPosition="250" endWordPosition="253">responds to dat in the first sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f I e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok m</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-based word alignment in statistical translation. In Proc. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="1630" citStr="Wu, 1997" startWordPosition="255" endWordPosition="256">st sentence pair. Word alignment has several downstream consumers. One is machine translation, where programs extract translation rules from word-aligned corpora (Och and Ney, 2004; Galley et al., 2004; Chiang, 2007; Quirk et al., 2005). Other downstream processes exploit dictionaries derived by alignment, in order to translate queries in crosslingual IR (Sch¨onhofen et al., 2008) or re-score candidate translation outputs (Och et al., 2004). Many methods of automatic alignment have been proposed. Probabilistic generative models like IBM 1-5 (Brown et al., 1993), HMM (Vogel et al., 1996), ITG (Wu, 1997), and LEAF (Fraser and Marcu, 2007) define formulas for P(f I e) or P(e, f), with ok-voon ororok sprok at-voon bichat dat erok sprok izok hihok ghirok totat dat arrat vat hilat ok-drubel ok-voon anok plok sprok at-drubel at-voon pippat rrat dat ok-voon anok drok brok jok at-voon krat pippat sat lat wiwok farok izok stok totat jjat quat cat lalok sprok izok jok stok wat dat krat quat cat lalok farok ororok lalok sprok izok enemok wat jjat bichat wat dat vat eneat lalok brok anok plok nok iat lat pippat rrat nnat wiwok nok izok kantok ok-yurp totat nnat quat oloat at-yurp lalok mok nok yorok ghi</context>
<context position="19807" citStr="Wu, 1997" startWordPosition="3304" endWordPosition="3305">Monotone (word) 512 4851 5.5 IBM-1 (word) 220 4851 21.6 IBM-4 (word) 230 4851 20.3 IP (word) 107 4851 20.1 IP (sub-word, 60 7418 61.4 initial) IP (sub-word, 65 8105 83.4 revised) Our search for an optimal IP solution is not fast. It takes 1-5 hours to perform sub-word alignment on the Turkish-English corpus. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 systems therefore make substantial search approximations (Brown et al., 1993). 4 Related Work (Zhang et al., 2003) and (Wu, 1997) tackle the problem of segmenting Chinese while aligning it to English. (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Their method uses IBM models to bootstrap alignments, and they measure the resulting segmentation accuracy. (Taskar et al., 2005) cast their alignment model as a minimum cost quadratic flow problem, for which optimal alignments can be computed with off-theshelf optimizers. Alignment in the modified model of (Lacoste-Julien et al., 2006) can be mapped to a quadratic assignment problem and solved with linear pr</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>D. Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Zhang</author>
<author>S Vogel</author>
<author>A Waibel</author>
</authors>
<title>Integrated phrase segmentation and alignment algorithm for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. Intl. Conf. on NLP and KE.</booktitle>
<contexts>
<context position="3781" citStr="Zhang et al., 2003" startWordPosition="612" endWordPosition="615">lign their data. It stands to reason that we have not yet seen the last word in alignment models. In this paper, we develop a new objective function for alignment, inspired by watching people manually solve the alignment exercise of Figure 1. When people attack this problem, we find that once they create a bilingual dictionary entry, they like to reuse that entry as much as possible. Previous machine aligners emulate this to some degree, but they are not explicitly programmed to do so. We also address another weakness of current aligners: they only align full words. With few exceptions, e.g. (Zhang et al., 2003; Snyder and Barzilay, 2008), aligners do not operate at the sub-word level, making them much less useful for agglutinative languages such as Turkish. Our present contributions are as follows: • We offer a simple new objective function that scores a corpus alignment based on how many distinct bilingual word pairs it contains. • We use an integer programming solver to carry out optimization and corpus alignment. • We extend the system to perform subword alignment, which we demonstrate on a Turkish-English corpus. The results in this paper constitute a proof of concept of these ideas, executed o</context>
<context position="19792" citStr="Zhang et al., 2003" startWordPosition="3299" endWordPosition="3302">(sub-word) 67 8102 100.0 Monotone (word) 512 4851 5.5 IBM-1 (word) 220 4851 21.6 IBM-4 (word) 230 4851 20.3 IP (word) 107 4851 20.1 IP (sub-word, 60 7418 61.4 initial) IP (sub-word, 65 8105 83.4 revised) Our search for an optimal IP solution is not fast. It takes 1-5 hours to perform sub-word alignment on the Turkish-English corpus. Of course, if we wanted to obtain optimal alignments under IBM Model 4, that would also be expensive, in fact NP-complete (Raghavendra and Maji, 2006). Practical Model 4 systems therefore make substantial search approximations (Brown et al., 1993). 4 Related Work (Zhang et al., 2003) and (Wu, 1997) tackle the problem of segmenting Chinese while aligning it to English. (Snyder and Barzilay, 2008) use multilingual data to compute segmentations of Arabic, Hebrew, Aramaic, and English. Their method uses IBM models to bootstrap alignments, and they measure the resulting segmentation accuracy. (Taskar et al., 2005) cast their alignment model as a minimum cost quadratic flow problem, for which optimal alignments can be computed with off-theshelf optimizers. Alignment in the modified model of (Lacoste-Julien et al., 2006) can be mapped to a quadratic assignment problem and solved</context>
</contexts>
<marker>Zhang, Vogel, Waibel, 2003</marker>
<rawString>Y. Zhang, S. Vogel, and A. Waibel. 2003. Integrated phrase segmentation and alignment algorithm for statistical machine translation. In Proc. Intl. Conf. on NLP and KE.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>