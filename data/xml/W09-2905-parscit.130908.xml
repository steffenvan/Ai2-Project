<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000003">
<title confidence="0.9981">
A Re-examination of Lexical Association Measures
</title>
<author confidence="0.998714">
Hung Huu Hoang
</author>
<affiliation confidence="0.998915">
Dept. of Computer Science
National University
</affiliation>
<address confidence="0.575207">
of Singapore
</address>
<email confidence="0.995116">
hoanghuu@comp.nus.edu.sg
</email>
<author confidence="0.992518">
Su Nam Kim
</author>
<affiliation confidence="0.939604">
Dept. of Computer Science
and Software Engineering
University of Melbourne
</affiliation>
<email confidence="0.994557">
snkim@csse.unimelb.edu.au
</email>
<author confidence="0.994334">
Min-Yen Kan
</author>
<affiliation confidence="0.997347">
Dept. of Computer Science
National University
</affiliation>
<address confidence="0.843261">
of Singapore
</address>
<email confidence="0.996084">
kanmy@comp.nus.edu.sg
</email>
<sectionHeader confidence="0.993831" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99986345">
We review lexical Association Measures
(AMs) that have been employed by past
work in extracting multiword expressions.
Our work contributes to the understanding
of these AMs by categorizing them into
two groups and suggesting the use of rank
equivalence to group AMs with the same
ranking performance. We also examine
how existing AMs can be adapted to better
rank English verb particle constructions
and light verb constructions. Specifically,
we suggest normalizing (Pointwise)
Mutual Information and using marginal
frequencies to construct penalization
terms. We empirically validate the
effectiveness of these modified AMs in
detection tasks in English, performed on
the Penn Treebank, which shows
significant improvement over the original
AMs.
</bodyText>
<sectionHeader confidence="0.99899" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.977850259259259">
Recently, the NLP community has witnessed a
renewed interest in the use of lexical association
measures in extracting Multiword Expressions
(MWEs). Lexical Association Measures
(hereafter, AMs) are mathematical formulas
which can be used to capture the degree of
connection or association between constituents
of a given phrase. Well-known AMs include
Pointwise Mutual Information (PMI),
Pearson’s 2
χ and the Odds Ratio. These AMs
have been applied in many different fields of
study, from information retrieval to hypothesis
testing. In the context of MWE extraction, many
published works have been devoted to comparing
their effectiveness. Krenn and Evert (2001)
evaluate Mutual Information (MI), Dice,
Pearson’s2
χ , log-likelihood
ratio and the T score. In Pearce (2002), AMs
such as Z score, Pointwise MI, cost reduction,
left and right context entropy, odds ratio are
evaluated. Evert (2004) discussed a wide range
of AMs, including exact hypothesis tests such as
the binomial test and Fisher’s exact tests, various
coefficients such as Dice and Jaccard. Later,
Ramisch et al. (2008) evaluated MI,
</bodyText>
<equation confidence="0.4776465">
Pearson’s2
χ and Permutation Entropy. Probably
</equation>
<bodyText confidence="0.999972235294117">
the most comprehensive evaluation of AMs was
presented in Pecina and Schlesinger (2006),
where 82 AMs were assembled and evaluated
over Czech collocations. These collocations
contained a mix of idiomatic expressions,
technical terms, light verb constructions and
stock phrases. In their work, the best
combination of AMs was selected using machine
learning.
While the previous works have evaluated AMs,
there have been few details on why the AMs
perform as they do. A detailed analysis of why
these AMs perform as they do is needed in order
to explain their identification performance, and
to help us recommend AMs for future tasks. This
weakness of previous works motivated us to
address this issue. In this work, we contribute to
further understanding of association measures,
using two different MWE extraction tasks to
motivate and concretize our discussion. Our goal
is to be able to predict, a priori, what types of
AMs are likely to perform well for a particular
MWE class.
We focus on the extraction of two common
types of English MWEs that can be captured by
bigram model: Verb Particle Constructions
(VPCs) and Light Verb Constructions (LVCs).
VPCs consist of a verb and one or more particles,
which can be prepositions (e.g. put on, bolster
up), adjectives (cut short) or verbs (make do).
For simplicity, we focus only on bigram VPCs
that take prepositional particles, the most
common class of VPCs. A special characteristic
of VPCs that affects their extraction is the
</bodyText>
<page confidence="0.998637">
31
</page>
<note confidence="0.994297">
Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 31–39,
Suntec, Singapore, 6 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.99980605882353">
mobility of noun phrase complements in
transitive VPCs. They can appear after the
particle (Take off your hat) or between the verb
and the particle (Take your hat off). However, a
pronominal complement can only appear in the
latter configuration (Take it off).
In comparison, LVCs comprise of a verb and a
complement, which is usually a noun phrase
(make a presentation, give a demonstration).
Their meanings come mostly from their
complements and, as such, verbs in LVCs are
termed semantically light, hence the name light
verb. This explains why modifiers of LVCs
modify the complement instead of the verb
(make a serious mistake vs. *make a mistake
seriously). This phenomenon also shows that an
LVC’s constituents may not occur contiguously.
</bodyText>
<sectionHeader confidence="0.790732" genericHeader="method">
2 Classification of Association Measures
</sectionHeader>
<bodyText confidence="0.999591857142857">
Although different AMs have different
approaches to measuring association, we
observed that they can effectively be classified
into two broad classes. Class I AMs look at the
degree of institutionalization; i.e., the extent to
which the phrase is a semantic unit rather than a
free combination of words. Some of the AMs in
this class directly measure this association
between constituents using various combinations
of co-occurrence and marginal frequencies.
Examples include MI, PMI and their variants as
well as most of the association coefficients such
as Jaccard, Hamann, Brawn-Blanquet, and
others. Other Class I AMs estimate a phrase’s
MWE-hood by judging the significance of the
difference between observed and expected
frequencies. These AMs include, among others,
statistical hypothesis tests such as T score, Z
score and Pearson’s 2
χ test.
Class II AMs feature the use of context to
measure non-compositionality, a peculiar
characteristic of many types of MWEs, including
VPCs and idioms. This is commonly done in one
of the following two ways. First, non-
compositionality can be modeled through the
diversity of contexts, measured using entropy.
The underlying assumption of this approach is
that non-compositional phrases appear in a more
restricted set of contexts than compositional ones.
Second, non-compositionality can also be
measured through context similarity between the
phrase and its constituents. The observation here
is that non-compositional phrases have different
semantics from those of their constituents. It then
follows that contexts in which the phrase and its
constituents appear would be different (Zhai,
1997). Some VPC examples include carry out,
give up. A close approximation stipulates that
contexts of a non-compositional phrase’s
constituents are also different. For instance,
phrases such as hot dog and Dutch courage are
comprised of constituents that have unrelated
meanings. Metrics that are commonly used to
compute context similarity include cosine and
dice similarity; distance metrics such as
Euclidean and Manhattan norm; and probability
distribution measures such as Kullback-Leibler
divergence and Jensen-Shannon divergence.
Table 1 lists all AMs used in our discussion.
The lower left legend defines the variables a, b, c,
and d with respect to the raw co-occurrence
statistics observed in the corpus data. When an
AM is introduced, it is prefixed with its index
given in Table 1(e.g., [M2] Mutual Information)
for the reader’s convenience.
</bodyText>
<sectionHeader confidence="0.99808" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9996904">
We will first present how VPC and LVC
candidates are extracted and used to form our
evaluation data set. Second, we will discuss how
performances of AMs are measured in our
experiments.
</bodyText>
<subsectionHeader confidence="0.99903">
3.1 Evaluation Data
</subsectionHeader>
<bodyText confidence="0.999941476190476">
In this study, we employ the Wall Street Journal
(WSJ) section of one million words in the Penn
Tree Bank. To create the evaluation data set, we
first extract the VPC and LVC candidates from
our corpus as described below. We note here that
the mobility property of both VPC and LVC
constituents have been used in the extraction
process.
For VPCs, we first identify particles using a
pre-compiled set of 38 particles based on
Baldwin (2005) and Quirk et al. (1985)
(Appendix A). Here we do not use the WSJ
particle tag to avoid possible inconsistencies
pointed out in Baldwin (2005). Next, we search
to the left of the located particle for the nearest
verb. As verbs and particles in transitive VPCs
may not occur contiguously, we allow an
intervening NP of up to 5 words, similar to
Baldwin and Villavicencio (2002) and Smadja
(1993), since longer NPs tend to be located after
particles.
</bodyText>
<page confidence="0.996565">
32
</page>
<table confidence="0.816672625">
AM Name Formula AM Name Formula
M1. Joint Probability f (xy) / N M2. Mutual Information 1 f
ij
∑ f ij log ˆ
N f
i j
, ij
M3. Log likelihood f,� M4. Pointwise MI (PMI) P(xy)
ratio 2� . f, log ˆ log
i j
, A
P(x*)P(*y)
M5. Local-PMI f (xy) x PMI M6. PMIk ( ) k
Nf xy
log
f x * f * y
( ) ( )
M7. PMI2 2 M8. Mutual Dependency 2
Nf xy P xy
( ) lo ( )
g
log f (x*)f (*y) P(x*)P(*y)
M9. Driver-Kroeber a M10. Normalized 2a
expectation
</table>
<figure confidence="0.984237395348837">
(a + b)(a + c) 2a + b + c
M11. Jaccard a M12. First Kulczynski a
a+b+c b +c
M13. Second a M14. Third a +d
Sokal-Sneath Sokal-Sneath
a + 2(b + c) +
b c
M15. Sokal-Michiner + M16. Rogers-Tanimoto +
a d a d
a+b+c+d a+2b+2c+d
M17. Hamann + − + M18. Odds ratio ad
(a d) (b c)
a+b+c+d bc
M19. Yule’s w − M20. Yule’s Q −
ad bc ad bc
ad +bc
ad + bc
M21. Brawn- a M22. Simpson a
Blanquet
a + b a + c a + b a + c
max( , ) min( , )
M23. S cost 1 M24*. Adjusted S Cost 1
b c − b c −
min( , ) 2 max( , ) 2
log(1 + ) log(1 + )
a+1 a+1
M25. Laplace a+1 M26*. Adjusted Laplace a+1
b c + b c +
a + min( , ) 2 a + max( , ) 2
M27. Fager − b c M28*. Adjusted Fager [M9] 1 b
[M9] 1 max( , ) − max( , c )
2
aN
M29*. Normalized PMI / NF( α) M30*. Simplified log(ad)
PMIs PMI / NFMax normalized PMI for
VPCs
(1 )
αxb+ −α xc
M31*. Normalized MI / NF( α) NF( α) = αP(x*) + (1 − α)P(*y) α e [0, 1]
MIs MI / NFMax NFMax = max(P(x*), P(*y))
a = f11 = f (xy) b = f12 = f (xy) f (x*)
c = f 21 = f ( xy ) d = f 22 = f ( xy ) f ( x* )
f (*y) f (*y) N
</figure>
<bodyText confidence="0.6096892">
Contingency table of a bigram (x y), recording co-
occurrence and marginal frequencies; w stands for all
words except w; * stands for all words; N is total
number of bigrams. The expected frequency under the
independence assumption is f ˆ(xy) = f (x*)f (*y) / N.
</bodyText>
<tableCaption confidence="0.995169">
Table 1. Association measures discussed in this paper. Starred AMs (*) are developed in this work.
</tableCaption>
<bodyText confidence="0.9996365">
Extraction of LVCs is carried out in a similar
fashion. First, occurrences of light verbs are
located based on the following set of seven
frequently used English light verbs: do, get, give,
have, make, put and take. Next, we search to the
right of the light verbs for the nearest noun,
</bodyText>
<page confidence="0.995178">
33
</page>
<bodyText confidence="0.980742923076923">
permitting a maximum of 4 intervening words to
allow for quantifiers (a/an, the, many, etc.),
adjectival and adverbial modifiers, etc. If this
search fails to find a noun, as when LVCs are
used in the passive (e.g. the presentation was
made), we search to the right of the light verb,
also allowing a maximum of 4 intervening words.
The above extraction process produced a total of
8,652 VPC and 11,465 LVC candidates when
run on the corpus. We then filter out candidates
with observed frequencies less than 6, as
suggested in Pecina and Schlesinger (2006), to
obtain a set of 1,629 VPCs and 1,254 LVCs.
Separately, we use the following two available
sources of annotations: 3,078 VPC candidates
extracted and annotated in (Baldwin, 2005) and
464 annotated LVC candidates used in (Tan et
al., 2006). Both sets of annotations give both
positive and negative examples.
Our final VPC and LVC evaluation datasets
were then constructed by intersecting the gold-
standard datasets with our corresponding sets of
extracted candidates. We also concatenated both
sets of evaluation data for composite evaluation.
This set is referred to as “Mixed”. Statistics of
our three evaluation datasets are summarized in
</bodyText>
<tableCaption confidence="0.985848">
Table 2.
</tableCaption>
<table confidence="0.9998144">
VPC data LVC data Mixed
Total 413 100 513
(freq ≥ 6)
Positive 117 28 145
instances (28.33%) (28%) (23.26%)
</table>
<tableCaption confidence="0.999949">
Table 2. Evaluation data sizes (type count, not token).
</tableCaption>
<bodyText confidence="0.9994668">
While these datasets are small, our primary
goal in this work is to establish initial
comparable baselines and describe interesting
phenomena that we plan to investigate over
larger datasets in future work.
</bodyText>
<subsectionHeader confidence="0.997358">
3.2 Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999970684210526">
To evaluate the performance of AMs, we can use
the standard precision and recall measures, as in
much past work. We note that the ranked list of
candidates generated by an AM is often used as a
classifier by setting a threshold. However, setting
a threshold is problematic and optimal threshold
values vary for different AMs. Additionally,
using the list of ranked candidates directly as a
classifier does not consider the confidence
indicated by actual scores. Another way to avoid
setting threshold values is to measure precision
and recall of only the n most likely candidates
(the n-best method). However, as discussed in
Evert and Krenn (2001), this method depends
heavily on the choice of n. In this paper, we opt
for average precision (AP), which is the average
of precisions at all possible recall values. This
choice also makes our results comparable to
those of Pecina and Schlesinger (2006).
</bodyText>
<subsectionHeader confidence="0.99811">
3.3 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.999688954545455">
Figure 1(a, b) gives the two average precision
profiles of the 82 AMs presented in Pecina and
Schlesinger (2006) when we replicated their
experiments over our English VPC and LVC
datasets. We observe that the average precision
profile for VPCs is slightly concave while the
one for LVCs is more convex. This can be
interpreted as VPCs being more sensitive to the
choice of AM than LVCs. Another point we
observed is that a vast majority of Class I AMs,
including PMI, its variants and association
coefficients (excluding hypothesis tests), perform
reasonably well in our application. In contrast,
the performances of most of context-based and
hypothesis test AMs are very modest. Their
mediocre performance indicates their
inapplicability to our VPC and LVC tasks. In
particular, the high frequencies of particles in
VPCs and light verbs in LVCs both undermine
their contexts’ discriminative power and skew
the difference between observed and expected
frequencies that are relied on in hypothesis tests.
</bodyText>
<sectionHeader confidence="0.993465" genericHeader="method">
4 Rank Equivalence
</sectionHeader>
<bodyText confidence="0.999903">
We note that some AMs, although not
mathematically equivalent (i.e., assigning
identical scores to input candidates) produce the
same lists of ranked candidates on our datasets.
Hence, they achieve the same average precision.
The ability to identify such groups of AMs is
helpful in simplifying their formulas, which in
turn assisting in analyzing their meanings.
</bodyText>
<equation confidence="0.538593">
Definition: Association measures M1 and M2 are
r
</equation>
<bodyText confidence="0.529986142857143">
rank equivalent over a set C, denoted by M1
M2, if and only if M1(cj) &gt; M1(ck) Ù M2(cj) &gt;
M2(ck) and M1(cj) = M1(ck) Ù M2(cj) = M2(ck) for
all cj, ck belongs to C where Mk(ci) denotes the
score assigned to ci by the measure Mk.
As a corollary, the following also holds for rank
equivalent AMs:
</bodyText>
<figure confidence="0.8252098">
≡
C
34
0.6
0.5
AP
0.4
0.3
0.2
0.1
</figure>
<figureCaption confidence="0.98491475">
Figure 1a. AP profile of AMs examined over our VPC data set.
Figure 1b. AP profile of AMs examined over our LVC data set.
Figure 1. Average precision (AP) performance of the 82 AMs from Pecina and Schlesinger (2006), on our
English VPC and LVC datasets. Bold points indicate AMs discussed in this paper.
</figureCaption>
<figure confidence="0.987417">
□ Hypothesis test AMs ◊ Class I AMs, excluding hypothesis test AMs + Context-based AMs.
0.6
0.5
0.4
0.3
0.2
0.1
AP
</figure>
<equation confidence="0.605187333333333">
r
≡M2 then APC(M1) = APC(M2)
C
</equation>
<bodyText confidence="0.999270769230769">
where APC(Mi) stands for the average precision
of the AM Mi over the data set C.
Essentially, M1 and M2 are rank equivalent over
a set C if their ranked lists of all candidates taken
from C are the same, ignoring the actual
calculated scores1. As an example, the following
3 AMs: Odds ratio, Yule’s ω and Yule’s Q (Table
3, row 5), though not mathematically equivalent,
can be shown to be rank equivalent. Five groups
of rank equivalent AMs that we have found are
listed in Table 3. This allows us to replace the
below 15 AMs with their (most simple)
representatives from each rank equivalent group.
</bodyText>
<footnote confidence="0.574087142857143">
1 Two AMs may be rank equivalent with the exception of
some candidates where one AM is undefined due to a zero
in the denominator while the other AM is still well-defined.
We call these cases weakly rank equivalent. With a
reasonably large corpus, such candidates are rare for our
VPC and LVC types. Hence, we still consider such AM
pairs to be rank equivalent.
</footnote>
<table confidence="0.999173923076923">
1) [M2] Mutual Information,
[M3] Log likelihood ratio
2) [M7] PMI2, [M8] Mutual Dependency,
[M9] Driver-Kroeber (a.k.a. Ochiai)
3) [M10] Normalized expectation,
[M11] Jaccard, [M12] First Kulczynski,
[M13]Second Sokal-Sneath
(a.k.a. Anderberg)
4) [M14] Third Sokal-Sneath,
[M15] Sokal-Michiner,
[M16] Rogers-Tanimoto, [M17] Hamann
5) [M18] Odds ratio, [M19] Yule’s ω,
[M20] Yule’s Q
</table>
<tableCaption confidence="0.99926">
Table 3. Five groups of rank equivalent AMs.
</tableCaption>
<sectionHeader confidence="0.919615" genericHeader="method">
5 Examination of Association Measures
</sectionHeader>
<bodyText confidence="0.994007">
We highlight two important findings in our
analysis of the AMs over our English datasets.
Section 5.1 focuses on MI and PMI and Section
5.2 discusses penalization terms.
</bodyText>
<subsectionHeader confidence="0.9505435">
5.1 Mutual Information and Pointwise
Mutual Information
</subsectionHeader>
<bodyText confidence="0.825866666666667">
In Figure 1, over 82 AMs, PMI ranks 11th in
identifying VPCs while MI ranks 35th in
Corollary: If M1
</bodyText>
<page confidence="0.99786">
35
</page>
<bodyText confidence="0.9891395">
identifying LVCs. In this section, we show how
their performances can be improved significantly.
Mutual Information (MI) measures the
common information between two variables or
the reduction in uncertainty of one variable given
knowledge of the
</bodyText>
<equation confidence="0.964521272727273">
other. MI(U; i) _ ∑ p(uv) log p( P( uv). In the
u,v
context of bigrams, the above formula can be
∑ f� log fij . While MI
N i j
, fij
holds between random variables, [M4] Pointwise
MI (PMI) holds between specific values: PMI(x,
P xy
( )
y) = log
</equation>
<bodyText confidence="0.998714125">
been pointed out that PMI favors bigrams with
low-frequency constituents, as evidenced by the
product of two marginal frequencies in its
denominator. To reduce this bias, a common
solution is to assign more weight to the co-
occurrence frequency f (xy) in the numerator by
either raising it to some power k (Daille, 1994) or
multiplying PMI with f (xy) . Table 4 lists these
adjusted versions of PMI and their performance
over our datasets. We can see from Table 4 that
the best performance of PMIk is obtained at k
values less than one, indicating that it is better to
rely less on f(xy) . Similarly, multiplying
f (xy) directly to PMI reduces the performance of
PMI. As such, assigning more weight to f (xy)
does not improve the AP performance of PMI.
</bodyText>
<table confidence="0.998855166666667">
AM VPCs LVCs Mixed
Best [M6] PMIk .547 .573 .544
(k = .13) (k = .85) (k = .32)
[M4] PMI .510 .566 .515
[M5] Local-PMI .259 .393 .272
[M1] Joint Prob. .170 .28 .175
</table>
<tableCaption confidence="0.9985685">
Table 4. AP performance of PMI and its variants. Best
alpha settings shown in parentheses.
</tableCaption>
<bodyText confidence="0.9990509375">
Another shortcoming of (P)MI is that both
grow not only with the degree of dependence but
also with frequency (Manning and Schutze,&amp;&amp; 1999,
p. 66). In particular, we can show that MI(X; Y) ≤
min(H(X), H(Y)), where H(.) denotes entropy,
and PMI(x,y) ≤ min(−logP(x*), −logP(*y) ).
These two inequalities suggest that the
allowed score ranges of different candidates vary
and consequently, MI and PMI scores are not
directly comparable. Furthermore, in the case of
VPCs and LVCs, the differences among score
ranges of different candidates are large, due to
high frequencies of particles and light verbs. This
has motivated us to normalize these scores
before using them for comparison. We suggest
MI and PMI be divided by one of the following
two normalization factors: NF( α) = αP(x*) +
(1− α)P(* y) with α E [0, 1] and NFmax
= max(P(x*), P(*y)) . NF( α) , being dependent on
alpha, can be optimized by setting an appropriate
alpha value, which is inevitably affected by the
MWE type and the corpus statistics. On the other
hand, NFmax is independent of alpha and is
recommended when one needs to apply
normalized (P)MI to a mixed set of different
MWE types or when sufficient data for
parameter tuning is unavailable. As shown in
Table 5, normalized MI and PMI show
considerable improvements of up to 80%. Also,
PMI and MI, after being normalized with NFmax,
rank number one in VPC and LVC task,
respectively. If one re-writes MI as = (1/
</bodyText>
<equation confidence="0.825854666666667">
N) ij
∑ f x PMI , it is easy to see the heavy
iji, j
</equation>
<bodyText confidence="0.933518666666667">
dependence of MI on direct frequencies
compared with PMI and this explains why
normalization is a pressing need for MI.
</bodyText>
<table confidence="0.999511333333333">
AM VPCs LVCs Mixed
MI / NF( α) .508 .583 .516
(α = .48) (α = .47) (α = .5)
MI / NFmax .508 .584 .518
[M2] MI .273 .435 .289
PMI / NF( α) .592 .554 .588
(α = .8) (α = .48) (α = .77)
PMI / NFmax .565 .517 .556
[M4] PMI .510 .566 .515
</table>
<tableCaption confidence="0.969449">
Table 5. AP performance of normalized (P)MI versus
standard (P)MI. Best alpha settings shown in
parentheses.
</tableCaption>
<subsectionHeader confidence="0.999723">
5.2 Penalization Terms
</subsectionHeader>
<bodyText confidence="0.99864">
It can be seen that given equal co-occurrence
frequencies, higher marginal frequencies reduce
the likelihood of being MWEs. This motivates us
to use marginal frequencies to synthesize
penalization terms which are formulae whose
values are inversely proportional to the
likelihood of being MWEs. We hypothesize that
incorporating such penalization terms can
improve the respective AMs detection AP.
Take as an example, the AMs [M21] Brawn-
Blanquet (a.k.a. Minimum Sensitivity) and [M22]
Simpson. These two AMs are identical, except
</bodyText>
<figure confidence="0.954680444444444">
1
simplified to [M2] MI =
P x * P * y
( ) ( )
Nf ( xy)
_
log
f (x*)f (*y)
. It has long
</figure>
<page confidence="0.995038">
36
</page>
<bodyText confidence="0.9984019">
for one difference in the denominator: Brawn-
Blanquet uses max(b, c); Simpson uses min(b, c).
It is intuitive and confirmed by our experiments
that penalizing against the more frequent
constituent by choosing max(b, c) is more
effective. This is further attested in AMs [M23]
S Cost and [M25] Laplace, where we tried to
replace the min(b, c) term with max(b, c). Table
6 shows the average precision on our datasets for
all these AMs.
</bodyText>
<table confidence="0.9997117">
AM VPCs LVCs Mixed
[M21]Brawn- .478 .578 .486
Blanquet
[M22] Simpson .249 .382 .260
[M24] Adjusted .485 .577 .492
S Cost
[M23] S cost .249 .388 .260
[M26] Adjusted .486 .577 .493
Laplace
[M25] Laplace .241 .388 .254
</table>
<tableCaption confidence="0.996862">
Table 6. Replacing min() with max() in selected AMs.
</tableCaption>
<bodyText confidence="0.999795285714286">
In the [M27] Fager AM, the penalization term
max(b, c) is subtracted from the first term, which
is no stranger but rank equivalent to [M7] PMI2.
In our application, this AM is not good since the
second term is far larger than the first term,
which is less than 1. As such, Fager is largely
equivalent to just –%2 max(b, c). In order to make
use of the first term, we need to replace the
constant %2 by a scaled down version of max(b,
c). We have approximately derived 1/ aN as a
lower bound estimate of max(b, c) using the
independence assumption, producing [M28]
Adjusted Fager. We can see from Table 7 that
this adjustment improves Fager on both datasets.
</bodyText>
<table confidence="0.99878675">
AM VPCs LVCs Mixed
[M28] Adjusted .564 .543 .554
Fager
[M27] Fager .552 .439 .525
</table>
<tableCaption confidence="0.988043">
Table 7. Performance of Fager and its adjusted
version.
</tableCaption>
<bodyText confidence="0.9991421875">
The next experiment involves [M14] Third
Sokal Sneath, which can be shown to be rank
equivalent to –b –c. We further notice that
frequencies c of particles are normally much
larger than frequencies b of verbs. Thus, this AM
runs the risk of ranking VPC candidates based on
only frequencies of particles. So, it is necessary
that we scale b and c properly as in
[M14&apos;] −α x b – (1− α) x c . Having scaled the
constituents properly, we still see that [M14&apos;] by
itself is not a good measure as it uses only
constituent frequencies and does not take into
consideration the co-occurrence frequency of the
two constituents. This has led us to experiment
with [MR14&amp;quot;] PMI . The
denominator of [MR14&amp;quot;] is obtained by
removing the minus sign from [MR14&apos;] so that it
can be used as a penalization term. The choice of
PMI in the numerator is due to the fact that the
denominator of [MR14&amp;quot;] is in essence similar to
NF( α) = αP(x*) + (1− α)P(* y) , which has
been successfully used to divide PMI in the
normalized PMI experiment. We heuristically
tried to simplify [MR14&amp;quot;] to the following AM
. The setting of alpha in
Table 8 below is taken from the best alpha
setting obtained the experiment on the
normalized PMI (Table 5). It can be observed
from Table 8 that [MR14&amp;quot;&apos;], being
computationally simpler than normalized PMI,
performs as well as normalized PMI and better
than Third Sokal-Sneath over the VPC data set.
</bodyText>
<table confidence="0.99859575">
AM VPCs LVCs Mixed
PMI / NF( α) .592 .554 .588
(α =.8) (α =.48) (α =.77)
log(ad) .600 .484 .588
[M30] (α =.8) (α =.48) (α =.77)
αxb+(1−α)xc
[M14] Third .565 .453 .546
Sokal Sneath
</table>
<tableCaption confidence="0.978714">
Table 8. AP performance of suggested VPCs’
penalization terms and AMs.
</tableCaption>
<bodyText confidence="0.988061285714286">
With the same intention and method, we have
found that while addition of marginal frequencies
is a good penalization term for VPCs, the
product of marginal frequencies is more suitable
for LVCs (rows 1 and 2, Table 9). As with the
linear combination, the product bc should also be
weighted accordingly as (1 )
</bodyText>
<figure confidence="0.60055">
b c
</figure>
<bodyText confidence="0.997744285714286">
α −α . The best alpha
value is also taken from the normalized PMI
experiments (Table 5), which is nearly .5. Under
this setting, this penalization term is exactly the
denominator of the [M18] Odds Ratio. Table 9
below show our experiment results in deriving
the penalization term for LVCs.
</bodyText>
<figure confidence="0.9912815">
αxb+(1−α)xc
[M30] log(ad)
αxb+ −α xc
(1 )
</figure>
<page confidence="0.989946">
37
</page>
<table confidence="0.99823125">
AM VPCs LVCs Mixed
–b –c .565 .453 .546
1/bc .502 .532 .502
[M18] Odds ratio .443 .567 .456
</table>
<tableCaption confidence="0.9698805">
Table 9. AP performance of suggested LVCs’
penalization terms and AMs.
</tableCaption>
<sectionHeader confidence="0.997336" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999942565217392">
We have conducted an analysis of the 82 AMs
assembled in Pecina and Schlesinger (2006) for
the tasks of English VPC and LVC extraction
over the Wall Street Journal Penn Treebank data.
In our work, we have observed that AMs can be
divided into two classes: ones that do not use
context (Class I) and ones that do (Class II), and
find that the latter is not suitable for our VPC and
LVC detection tasks as the size of our corpus is
too small to rely on the frequency of candidates’
contexts. This phenomenon also revealed the
inappropriateness of hypothesis tests for our
detection task. We have also introduced the
novel notion of rank equivalence to MWE
detection, in which we show that complex AMs
may be replaced by simpler AMs that yield the
same average precision performance.
We further observed that certain modifications
to some AMs are necessary. First, in the context
of ranking, we have proposed normalizing scores
produced by MI and PMI in cases where the
distributions of the two events are markedly
different, as is the case for light verbs and
particles. While our claims are limited to the
datasets analyzed, they show clear
improvements: normalized PMI produces better
performance over our mixed MWE dataset,
yielding an average precision of 58.8%
compared to 51.5% when using standard PMI, a
significant improvement as judged by paired T
test. Normalized MI also yields the best
performance over our LVC dataset with a
significantly improved AP of 58.3%.
We also show that marginal frequencies can
be used to form effective penalization terms. In
particular, we find that α x b + (1− α) x c is a good
penalization term for VPCs, while bαc(1−α) is
suitable for LVCs. Our introduced alpha tuning
parameter should be set to properly scale the
values b and c, and should be optimized per
MWE type. In cases where a common factor is
applied to different MWE types, max(b, c) is a
better choice than min(b, c). In future work, we
plan to expand our investigations over larger,
web-based datasets of English, to verify the
performance gains of our modified AMs.
</bodyText>
<sectionHeader confidence="0.967515" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.994991333333333">
This work was partially supported by a National
Research Foundation grant “Interactive Media
Search” (grant # R 252 000 325 279).
</bodyText>
<sectionHeader confidence="0.998877" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999808720930233">
Baldwin, Timothy (2005). The deep lexical
acquisition of English verb-particle constructions.
Computer Speech and Language, Special
Issue on Multiword Expressions, 19(4):398–414.
Baldwin, Timothy and Villavicencio, Aline (2002).
Extracting the unextractable: A case study on verb-
particles. In Proceedings of the 6th Conference
on Natural Language Learning (CoNLL-2002),
pages 98–104, Taipei, Taiwan.
Daille, Béatrice (1994). Approche mixte pour
l&apos;extraction automatique de terminologie:
statistiques lexicales et filtres linguistiques.
PhD thesis, Université Paris 7.
Evert, Stefan (2004). Online repository of association
measures http://www.collocations.de/, a
companion to The Statistics of Word
Cooccurrences: Word Pairs and Collocations.
Ph.D. dissertation, University of Stuttgart.
Evert, Stefan and Krenn, Brigitte (2001) Methods for
qualitative evaluation of lexical association
measures. In Proceedings of the 39th Annual
Meeting of the Association of Computational
Linguistics, pages 188-915, Toulouse, France.
Katz, Graham and Giesbrecht, Eugenie (2006).
Automatic identification of non-compositional
multi-word expressions using latent semantic
analysis. In Proceedings of the ACL Workshop
on Multiword Expressions: Identifying and
Exploiting Underlying Properties, pages 12-19,
Sydney, Australia.
Krenn, Brigitte and Evert, Stefan (2001). Can we do
better than frequency? A case study on extracting
PP-verb collocations. In Proceedings of the
ACL/EACL 2001 Workshop on the
Computational Extraction, Analysis and
Exploitation of Collocations, pages 39–46,
Toulouse, France.
Manning D. Christopher and Schutze,&amp;&amp; Hinrich
(1999). Foundations of Statistical Natural
Language Processing. The MIT Press, Cambridge,
Massachusetts.
Pearce, Darren (2002). A comparative evaluation of
collocation extraction techniques. In Proc. of the
</reference>
<page confidence="0.985732">
38
</page>
<reference confidence="0.994576177777778">
3rd International Conference on Language
Resources and Evaluation (LREC 2002), Las
Palmas, pages 1530-1536, Canary Islands.
Pecina, Pavel and Schlesinger, Pavel (2006).
Combining association measures for collocation
extraction. In Proceedings of the 21th
International Conference on Computational
Linguistics and 44th Annual Meeting of the
Association for Computational Linguistics
(COLING/ACL 2006), pages 651-658, Sydney,
Australia.
Quirk Randolph, Greenbaum Sidney, Leech Geoffrey
and Svartvik Jan (1985). A Comprehensive
Grammar of the English Language. Longman,
London, UK.
Ramisch Carlos, Schreiner Paulo, Idiart Marco and
Villavicencio Aline (2008). An Evaluation of
Methods for the extraction of Multiword
Expressions. In Proceedings of the LREC-2008
Workshop on Multiword Expressions:
Towards a Shared Task for Multiword
Expressions, pages 50-53, Marrakech, Morocco.
Smadja, Frank (1993). Retrieving collocations from
text: Xtract. Computational Linguistics 19(1):
143–77.
Tan, Y. Fan, Kan M. Yen and Cui, Hang (2006).
Extending corpus-based identification of light verb
constructions using a supervised learning
framework. In Proceedings of the EACL 2006
Workshop on Multi-word-expressions in a
multilingual context, pages 49–56, Trento, Italy.
Zhai, Chengxiang (1997). Exploiting context to
identify lexical atoms – A statistical view of
linguistic context. In International and
Interdisciplinary Conference on Modelling
and Using Context (CONTEXT-97), pages 119-
129, Rio de Janeiro, Brazil.
Appendix A. List of particles used in
identifying verb particle constructions.
about, aback, aboard, above, abroad, across, adrift,
ahead, along, apart, around, aside, astray, away,
back, backward, backwards, behind, by, down,
forth, forward, forwards, in, into, off, on, out, over,
past, round, through, to, together, under, up, upon,
without.
</reference>
<page confidence="0.999523">
39
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.511244">
<title confidence="0.999855">A Re-examination of Lexical Association Measures</title>
<author confidence="0.992658">Hung Huu</author>
<affiliation confidence="0.87769">Dept. of Computer National University of Singapore</affiliation>
<email confidence="0.922663">hoanghuu@comp.nus.edu.sg</email>
<author confidence="0.994604">Su Nam Kim</author>
<affiliation confidence="0.998157333333333">Dept. of Computer and Software Engineering University of</affiliation>
<email confidence="0.963834">snkim@csse.unimelb.edu.au</email>
<author confidence="0.999323">Min-Yen Kan</author>
<affiliation confidence="0.999977">Dept. of Computer National University</affiliation>
<address confidence="0.962375">of</address>
<email confidence="0.961627">kanmy@comp.nus.edu.sg</email>
<abstract confidence="0.99867680952381">We review lexical Association Measures (AMs) that have been employed by past work in extracting multiword expressions. Our work contributes to the understanding of these AMs by categorizing them into two groups and suggesting the use of rank equivalence to group AMs with the same ranking performance. We also examine how existing AMs can be adapted to better rank English verb particle constructions and light verb constructions. Specifically, we suggest normalizing (Pointwise) Mutual Information and using marginal frequencies to construct penalization terms. We empirically validate effectiveness of these modified AMs in detection tasks in English, performed on the Penn Treebank, which shows significant improvement over the original AMs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
</authors>
<title>The deep lexical acquisition of English verb-particle constructions.</title>
<date>2005</date>
<journal>Computer Speech and Language, Special Issue on Multiword Expressions,</journal>
<volume>19</volume>
<issue>4</issue>
<contexts>
<context position="7844" citStr="Baldwin (2005)" startWordPosition="1206" endWordPosition="1207">nd LVC candidates are extracted and used to form our evaluation data set. Second, we will discuss how performances of AMs are measured in our experiments. 3.1 Evaluation Data In this study, we employ the Wall Street Journal (WSJ) section of one million words in the Penn Tree Bank. To create the evaluation data set, we first extract the VPC and LVC candidates from our corpus as described below. We note here that the mobility property of both VPC and LVC constituents have been used in the extraction process. For VPCs, we first identify particles using a pre-compiled set of 38 particles based on Baldwin (2005) and Quirk et al. (1985) (Appendix A). Here we do not use the WSJ particle tag to avoid possible inconsistencies pointed out in Baldwin (2005). Next, we search to the left of the located particle for the nearest verb. As verbs and particles in transitive VPCs may not occur contiguously, we allow an intervening NP of up to 5 words, similar to Baldwin and Villavicencio (2002) and Smadja (1993), since longer NPs tend to be located after particles. 32 AM Name Formula AM Name Formula M1. Joint Probability f (xy) / N M2. Mutual Information 1 f ij ∑ f ij log ˆ N f i j , ij M3. Log likelihood f,� M4. </context>
<context position="11128" citStr="Baldwin, 2005" startWordPosition="1902" endWordPosition="1903">iers, etc. If this search fails to find a noun, as when LVCs are used in the passive (e.g. the presentation was made), we search to the right of the light verb, also allowing a maximum of 4 intervening words. The above extraction process produced a total of 8,652 VPC and 11,465 LVC candidates when run on the corpus. We then filter out candidates with observed frequencies less than 6, as suggested in Pecina and Schlesinger (2006), to obtain a set of 1,629 VPCs and 1,254 LVCs. Separately, we use the following two available sources of annotations: 3,078 VPC candidates extracted and annotated in (Baldwin, 2005) and 464 annotated LVC candidates used in (Tan et al., 2006). Both sets of annotations give both positive and negative examples. Our final VPC and LVC evaluation datasets were then constructed by intersecting the goldstandard datasets with our corresponding sets of extracted candidates. We also concatenated both sets of evaluation data for composite evaluation. This set is referred to as “Mixed”. Statistics of our three evaluation datasets are summarized in Table 2. VPC data LVC data Mixed Total 413 100 513 (freq ≥ 6) Positive 117 28 145 instances (28.33%) (28%) (23.26%) Table 2. Evaluation da</context>
</contexts>
<marker>Baldwin, 2005</marker>
<rawString>Baldwin, Timothy (2005). The deep lexical acquisition of English verb-particle constructions. Computer Speech and Language, Special Issue on Multiword Expressions, 19(4):398–414.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Aline Villavicencio</author>
</authors>
<title>Extracting the unextractable: A case study on verbparticles.</title>
<date>2002</date>
<booktitle>In Proceedings of the 6th Conference on Natural Language Learning (CoNLL-2002),</booktitle>
<pages>98--104</pages>
<location>Taipei, Taiwan.</location>
<contexts>
<context position="8220" citStr="Baldwin and Villavicencio (2002)" startWordPosition="1270" endWordPosition="1273">es from our corpus as described below. We note here that the mobility property of both VPC and LVC constituents have been used in the extraction process. For VPCs, we first identify particles using a pre-compiled set of 38 particles based on Baldwin (2005) and Quirk et al. (1985) (Appendix A). Here we do not use the WSJ particle tag to avoid possible inconsistencies pointed out in Baldwin (2005). Next, we search to the left of the located particle for the nearest verb. As verbs and particles in transitive VPCs may not occur contiguously, we allow an intervening NP of up to 5 words, similar to Baldwin and Villavicencio (2002) and Smadja (1993), since longer NPs tend to be located after particles. 32 AM Name Formula AM Name Formula M1. Joint Probability f (xy) / N M2. Mutual Information 1 f ij ∑ f ij log ˆ N f i j , ij M3. Log likelihood f,� M4. Pointwise MI (PMI) P(xy) ratio 2� . f, log ˆ log i j , A P(x*)P(*y) M5. Local-PMI f (xy) x PMI M6. PMIk ( ) k Nf xy log f x * f * y ( ) ( ) M7. PMI2 2 M8. Mutual Dependency 2 Nf xy P xy ( ) lo ( ) g log f (x*)f (*y) P(x*)P(*y) M9. Driver-Kroeber a M10. Normalized 2a expectation (a + b)(a + c) 2a + b + c M11. Jaccard a M12. First Kulczynski a a+b+c b +c M13. Second a M14. Th</context>
</contexts>
<marker>Baldwin, Villavicencio, 2002</marker>
<rawString>Baldwin, Timothy and Villavicencio, Aline (2002). Extracting the unextractable: A case study on verbparticles. In Proceedings of the 6th Conference on Natural Language Learning (CoNLL-2002), pages 98–104, Taipei, Taiwan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Béatrice Daille</author>
</authors>
<title>Approche mixte pour l&apos;extraction automatique de terminologie: statistiques lexicales et filtres linguistiques.</title>
<date>1994</date>
<tech>PhD thesis,</tech>
<institution>Université Paris</institution>
<contexts>
<context position="17699" citStr="Daille, 1994" startWordPosition="3005" endWordPosition="3006">certainty of one variable given knowledge of the other. MI(U; i) _ ∑ p(uv) log p( P( uv). In the u,v context of bigrams, the above formula can be ∑ f� log fij . While MI N i j , fij holds between random variables, [M4] Pointwise MI (PMI) holds between specific values: PMI(x, P xy ( ) y) = log been pointed out that PMI favors bigrams with low-frequency constituents, as evidenced by the product of two marginal frequencies in its denominator. To reduce this bias, a common solution is to assign more weight to the cooccurrence frequency f (xy) in the numerator by either raising it to some power k (Daille, 1994) or multiplying PMI with f (xy) . Table 4 lists these adjusted versions of PMI and their performance over our datasets. We can see from Table 4 that the best performance of PMIk is obtained at k values less than one, indicating that it is better to rely less on f(xy) . Similarly, multiplying f (xy) directly to PMI reduces the performance of PMI. As such, assigning more weight to f (xy) does not improve the AP performance of PMI. AM VPCs LVCs Mixed Best [M6] PMIk .547 .573 .544 (k = .13) (k = .85) (k = .32) [M4] PMI .510 .566 .515 [M5] Local-PMI .259 .393 .272 [M1] Joint Prob. .170 .28 .175 Tab</context>
</contexts>
<marker>Daille, 1994</marker>
<rawString>Daille, Béatrice (1994). Approche mixte pour l&apos;extraction automatique de terminologie: statistiques lexicales et filtres linguistiques. PhD thesis, Université Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
</authors>
<title>Online repository of association measures http://www.collocations.de/, a companion to The Statistics of Word Cooccurrences: Word Pairs and Collocations.</title>
<date>2004</date>
<institution>University of Stuttgart.</institution>
<note>Ph.D. dissertation,</note>
<contexts>
<context position="2023" citStr="Evert (2004)" startWordPosition="288" endWordPosition="289">ion between constituents of a given phrase. Well-known AMs include Pointwise Mutual Information (PMI), Pearson’s 2 χ and the Odds Ratio. These AMs have been applied in many different fields of study, from information retrieval to hypothesis testing. In the context of MWE extraction, many published works have been devoted to comparing their effectiveness. Krenn and Evert (2001) evaluate Mutual Information (MI), Dice, Pearson’s2 χ , log-likelihood ratio and the T score. In Pearce (2002), AMs such as Z score, Pointwise MI, cost reduction, left and right context entropy, odds ratio are evaluated. Evert (2004) discussed a wide range of AMs, including exact hypothesis tests such as the binomial test and Fisher’s exact tests, various coefficients such as Dice and Jaccard. Later, Ramisch et al. (2008) evaluated MI, Pearson’s2 χ and Permutation Entropy. Probably the most comprehensive evaluation of AMs was presented in Pecina and Schlesinger (2006), where 82 AMs were assembled and evaluated over Czech collocations. These collocations contained a mix of idiomatic expressions, technical terms, light verb constructions and stock phrases. In their work, the best combination of AMs was selected using machin</context>
</contexts>
<marker>Evert, 2004</marker>
<rawString>Evert, Stefan (2004). Online repository of association measures http://www.collocations.de/, a companion to The Statistics of Word Cooccurrences: Word Pairs and Collocations. Ph.D. dissertation, University of Stuttgart.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Evert</author>
<author>Brigitte Krenn</author>
</authors>
<title>Methods for qualitative evaluation of lexical association measures.</title>
<date>2001</date>
<booktitle>In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>188--915</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="12639" citStr="Evert and Krenn (2001)" startWordPosition="2144" endWordPosition="2147">e of AMs, we can use the standard precision and recall measures, as in much past work. We note that the ranked list of candidates generated by an AM is often used as a classifier by setting a threshold. However, setting a threshold is problematic and optimal threshold values vary for different AMs. Additionally, using the list of ranked candidates directly as a classifier does not consider the confidence indicated by actual scores. Another way to avoid setting threshold values is to measure precision and recall of only the n most likely candidates (the n-best method). However, as discussed in Evert and Krenn (2001), this method depends heavily on the choice of n. In this paper, we opt for average precision (AP), which is the average of precisions at all possible recall values. This choice also makes our results comparable to those of Pecina and Schlesinger (2006). 3.3 Evaluation Results Figure 1(a, b) gives the two average precision profiles of the 82 AMs presented in Pecina and Schlesinger (2006) when we replicated their experiments over our English VPC and LVC datasets. We observe that the average precision profile for VPCs is slightly concave while the one for LVCs is more convex. This can be interpr</context>
</contexts>
<marker>Evert, Krenn, 2001</marker>
<rawString>Evert, Stefan and Krenn, Brigitte (2001) Methods for qualitative evaluation of lexical association measures. In Proceedings of the 39th Annual Meeting of the Association of Computational Linguistics, pages 188-915, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Katz</author>
<author>Eugenie Giesbrecht</author>
</authors>
<title>Automatic identification of non-compositional multi-word expressions using latent semantic analysis.</title>
<date>2006</date>
<booktitle>In Proceedings of the ACL Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties,</booktitle>
<pages>12--19</pages>
<location>Sydney, Australia.</location>
<marker>Katz, Giesbrecht, 2006</marker>
<rawString>Katz, Graham and Giesbrecht, Eugenie (2006). Automatic identification of non-compositional multi-word expressions using latent semantic analysis. In Proceedings of the ACL Workshop on Multiword Expressions: Identifying and Exploiting Underlying Properties, pages 12-19, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brigitte Krenn</author>
<author>Stefan Evert</author>
</authors>
<title>Can we do better than frequency? A case study on extracting PP-verb collocations.</title>
<date>2001</date>
<booktitle>In Proceedings of the ACL/EACL 2001 Workshop on the Computational Extraction, Analysis and Exploitation of Collocations,</booktitle>
<pages>39--46</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1790" citStr="Krenn and Evert (2001)" startWordPosition="249" endWordPosition="252">a renewed interest in the use of lexical association measures in extracting Multiword Expressions (MWEs). Lexical Association Measures (hereafter, AMs) are mathematical formulas which can be used to capture the degree of connection or association between constituents of a given phrase. Well-known AMs include Pointwise Mutual Information (PMI), Pearson’s 2 χ and the Odds Ratio. These AMs have been applied in many different fields of study, from information retrieval to hypothesis testing. In the context of MWE extraction, many published works have been devoted to comparing their effectiveness. Krenn and Evert (2001) evaluate Mutual Information (MI), Dice, Pearson’s2 χ , log-likelihood ratio and the T score. In Pearce (2002), AMs such as Z score, Pointwise MI, cost reduction, left and right context entropy, odds ratio are evaluated. Evert (2004) discussed a wide range of AMs, including exact hypothesis tests such as the binomial test and Fisher’s exact tests, various coefficients such as Dice and Jaccard. Later, Ramisch et al. (2008) evaluated MI, Pearson’s2 χ and Permutation Entropy. Probably the most comprehensive evaluation of AMs was presented in Pecina and Schlesinger (2006), where 82 AMs were assemb</context>
</contexts>
<marker>Krenn, Evert, 2001</marker>
<rawString>Krenn, Brigitte and Evert, Stefan (2001). Can we do better than frequency? A case study on extracting PP-verb collocations. In Proceedings of the ACL/EACL 2001 Workshop on the Computational Extraction, Analysis and Exploitation of Collocations, pages 39–46, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manning D Christopher</author>
<author>&amp;&amp; Hinrich Schutze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Christopher, Schutze, 1999</marker>
<rawString>Manning D. Christopher and Schutze,&amp;&amp; Hinrich (1999). Foundations of Statistical Natural Language Processing. The MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Darren Pearce</author>
</authors>
<title>A comparative evaluation of collocation extraction techniques.</title>
<date>2002</date>
<booktitle>In Proc. of the 3rd International Conference on Language Resources and Evaluation (LREC 2002), Las Palmas,</booktitle>
<pages>1530--1536</pages>
<location>Canary Islands.</location>
<contexts>
<context position="1900" citStr="Pearce (2002)" startWordPosition="268" endWordPosition="269">iation Measures (hereafter, AMs) are mathematical formulas which can be used to capture the degree of connection or association between constituents of a given phrase. Well-known AMs include Pointwise Mutual Information (PMI), Pearson’s 2 χ and the Odds Ratio. These AMs have been applied in many different fields of study, from information retrieval to hypothesis testing. In the context of MWE extraction, many published works have been devoted to comparing their effectiveness. Krenn and Evert (2001) evaluate Mutual Information (MI), Dice, Pearson’s2 χ , log-likelihood ratio and the T score. In Pearce (2002), AMs such as Z score, Pointwise MI, cost reduction, left and right context entropy, odds ratio are evaluated. Evert (2004) discussed a wide range of AMs, including exact hypothesis tests such as the binomial test and Fisher’s exact tests, various coefficients such as Dice and Jaccard. Later, Ramisch et al. (2008) evaluated MI, Pearson’s2 χ and Permutation Entropy. Probably the most comprehensive evaluation of AMs was presented in Pecina and Schlesinger (2006), where 82 AMs were assembled and evaluated over Czech collocations. These collocations contained a mix of idiomatic expressions, techni</context>
</contexts>
<marker>Pearce, 2002</marker>
<rawString>Pearce, Darren (2002). A comparative evaluation of collocation extraction techniques. In Proc. of the 3rd International Conference on Language Resources and Evaluation (LREC 2002), Las Palmas, pages 1530-1536, Canary Islands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pavel Pecina</author>
<author>Pavel Schlesinger</author>
</authors>
<title>Combining association measures for collocation extraction.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21th International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL</booktitle>
<pages>651--658</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2364" citStr="Pecina and Schlesinger (2006)" startWordPosition="338" endWordPosition="341">o comparing their effectiveness. Krenn and Evert (2001) evaluate Mutual Information (MI), Dice, Pearson’s2 χ , log-likelihood ratio and the T score. In Pearce (2002), AMs such as Z score, Pointwise MI, cost reduction, left and right context entropy, odds ratio are evaluated. Evert (2004) discussed a wide range of AMs, including exact hypothesis tests such as the binomial test and Fisher’s exact tests, various coefficients such as Dice and Jaccard. Later, Ramisch et al. (2008) evaluated MI, Pearson’s2 χ and Permutation Entropy. Probably the most comprehensive evaluation of AMs was presented in Pecina and Schlesinger (2006), where 82 AMs were assembled and evaluated over Czech collocations. These collocations contained a mix of idiomatic expressions, technical terms, light verb constructions and stock phrases. In their work, the best combination of AMs was selected using machine learning. While the previous works have evaluated AMs, there have been few details on why the AMs perform as they do. A detailed analysis of why these AMs perform as they do is needed in order to explain their identification performance, and to help us recommend AMs for future tasks. This weakness of previous works motivated us to addres</context>
<context position="10946" citStr="Pecina and Schlesinger (2006)" startWordPosition="1871" endWordPosition="1874">Next, we search to the right of the light verbs for the nearest noun, 33 permitting a maximum of 4 intervening words to allow for quantifiers (a/an, the, many, etc.), adjectival and adverbial modifiers, etc. If this search fails to find a noun, as when LVCs are used in the passive (e.g. the presentation was made), we search to the right of the light verb, also allowing a maximum of 4 intervening words. The above extraction process produced a total of 8,652 VPC and 11,465 LVC candidates when run on the corpus. We then filter out candidates with observed frequencies less than 6, as suggested in Pecina and Schlesinger (2006), to obtain a set of 1,629 VPCs and 1,254 LVCs. Separately, we use the following two available sources of annotations: 3,078 VPC candidates extracted and annotated in (Baldwin, 2005) and 464 annotated LVC candidates used in (Tan et al., 2006). Both sets of annotations give both positive and negative examples. Our final VPC and LVC evaluation datasets were then constructed by intersecting the goldstandard datasets with our corresponding sets of extracted candidates. We also concatenated both sets of evaluation data for composite evaluation. This set is referred to as “Mixed”. Statistics of our </context>
<context position="12892" citStr="Pecina and Schlesinger (2006)" startWordPosition="2187" endWordPosition="2190"> and optimal threshold values vary for different AMs. Additionally, using the list of ranked candidates directly as a classifier does not consider the confidence indicated by actual scores. Another way to avoid setting threshold values is to measure precision and recall of only the n most likely candidates (the n-best method). However, as discussed in Evert and Krenn (2001), this method depends heavily on the choice of n. In this paper, we opt for average precision (AP), which is the average of precisions at all possible recall values. This choice also makes our results comparable to those of Pecina and Schlesinger (2006). 3.3 Evaluation Results Figure 1(a, b) gives the two average precision profiles of the 82 AMs presented in Pecina and Schlesinger (2006) when we replicated their experiments over our English VPC and LVC datasets. We observe that the average precision profile for VPCs is slightly concave while the one for LVCs is more convex. This can be interpreted as VPCs being more sensitive to the choice of AM than LVCs. Another point we observed is that a vast majority of Class I AMs, including PMI, its variants and association coefficients (excluding hypothesis tests), perform reasonably well in our appl</context>
<context position="14896" citStr="Pecina and Schlesinger (2006)" startWordPosition="2523" endWordPosition="2526">sisting in analyzing their meanings. Definition: Association measures M1 and M2 are r rank equivalent over a set C, denoted by M1 M2, if and only if M1(cj) &gt; M1(ck) Ù M2(cj) &gt; M2(ck) and M1(cj) = M1(ck) Ù M2(cj) = M2(ck) for all cj, ck belongs to C where Mk(ci) denotes the score assigned to ci by the measure Mk. As a corollary, the following also holds for rank equivalent AMs: ≡ C 34 0.6 0.5 AP 0.4 0.3 0.2 0.1 Figure 1a. AP profile of AMs examined over our VPC data set. Figure 1b. AP profile of AMs examined over our LVC data set. Figure 1. Average precision (AP) performance of the 82 AMs from Pecina and Schlesinger (2006), on our English VPC and LVC datasets. Bold points indicate AMs discussed in this paper. □ Hypothesis test AMs ◊ Class I AMs, excluding hypothesis test AMs + Context-based AMs. 0.6 0.5 0.4 0.3 0.2 0.1 AP r ≡M2 then APC(M1) = APC(M2) C where APC(Mi) stands for the average precision of the AM Mi over the data set C. Essentially, M1 and M2 are rank equivalent over a set C if their ranked lists of all candidates taken from C are the same, ignoring the actual calculated scores1. As an example, the following 3 AMs: Odds ratio, Yule’s ω and Yule’s Q (Table 3, row 5), though not mathematically equival</context>
<context position="25050" citStr="Pecina and Schlesinger (2006)" startWordPosition="4330" endWordPosition="4333">ighted accordingly as (1 ) b c α −α . The best alpha value is also taken from the normalized PMI experiments (Table 5), which is nearly .5. Under this setting, this penalization term is exactly the denominator of the [M18] Odds Ratio. Table 9 below show our experiment results in deriving the penalization term for LVCs. αxb+(1−α)xc [M30] log(ad) αxb+ −α xc (1 ) 37 AM VPCs LVCs Mixed –b –c .565 .453 .546 1/bc .502 .532 .502 [M18] Odds ratio .443 .567 .456 Table 9. AP performance of suggested LVCs’ penalization terms and AMs. 6 Conclusions We have conducted an analysis of the 82 AMs assembled in Pecina and Schlesinger (2006) for the tasks of English VPC and LVC extraction over the Wall Street Journal Penn Treebank data. In our work, we have observed that AMs can be divided into two classes: ones that do not use context (Class I) and ones that do (Class II), and find that the latter is not suitable for our VPC and LVC detection tasks as the size of our corpus is too small to rely on the frequency of candidates’ contexts. This phenomenon also revealed the inappropriateness of hypothesis tests for our detection task. We have also introduced the novel notion of rank equivalence to MWE detection, in which we show that</context>
</contexts>
<marker>Pecina, Schlesinger, 2006</marker>
<rawString>Pecina, Pavel and Schlesinger, Pavel (2006). Combining association measures for collocation extraction. In Proceedings of the 21th International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics (COLING/ACL 2006), pages 651-658, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quirk Randolph</author>
<author>Greenbaum Sidney</author>
</authors>
<title>Leech Geoffrey and Svartvik</title>
<date>1985</date>
<journal>A Comprehensive Grammar of the English Language. Longman,</journal>
<location>London, UK.</location>
<marker>Randolph, Sidney, 1985</marker>
<rawString>Quirk Randolph, Greenbaum Sidney, Leech Geoffrey and Svartvik Jan (1985). A Comprehensive Grammar of the English Language. Longman, London, UK.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ramisch Carlos</author>
<author>Schreiner Paulo</author>
</authors>
<title>Idiart Marco and Villavicencio Aline (2008). An Evaluation of Methods for the extraction of Multiword Expressions.</title>
<booktitle>In Proceedings of the LREC-2008 Workshop on Multiword Expressions: Towards a Shared Task for Multiword Expressions,</booktitle>
<pages>50--53</pages>
<location>Marrakech, Morocco.</location>
<marker>Carlos, Paulo, </marker>
<rawString>Ramisch Carlos, Schreiner Paulo, Idiart Marco and Villavicencio Aline (2008). An Evaluation of Methods for the extraction of Multiword Expressions. In Proceedings of the LREC-2008 Workshop on Multiword Expressions: Towards a Shared Task for Multiword Expressions, pages 50-53, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Retrieving collocations from text:</title>
<date>1993</date>
<journal>Xtract. Computational Linguistics</journal>
<volume>19</volume>
<issue>1</issue>
<pages>143--77</pages>
<contexts>
<context position="8238" citStr="Smadja (1993)" startWordPosition="1275" endWordPosition="1276">. We note here that the mobility property of both VPC and LVC constituents have been used in the extraction process. For VPCs, we first identify particles using a pre-compiled set of 38 particles based on Baldwin (2005) and Quirk et al. (1985) (Appendix A). Here we do not use the WSJ particle tag to avoid possible inconsistencies pointed out in Baldwin (2005). Next, we search to the left of the located particle for the nearest verb. As verbs and particles in transitive VPCs may not occur contiguously, we allow an intervening NP of up to 5 words, similar to Baldwin and Villavicencio (2002) and Smadja (1993), since longer NPs tend to be located after particles. 32 AM Name Formula AM Name Formula M1. Joint Probability f (xy) / N M2. Mutual Information 1 f ij ∑ f ij log ˆ N f i j , ij M3. Log likelihood f,� M4. Pointwise MI (PMI) P(xy) ratio 2� . f, log ˆ log i j , A P(x*)P(*y) M5. Local-PMI f (xy) x PMI M6. PMIk ( ) k Nf xy log f x * f * y ( ) ( ) M7. PMI2 2 M8. Mutual Dependency 2 Nf xy P xy ( ) lo ( ) g log f (x*)f (*y) P(x*)P(*y) M9. Driver-Kroeber a M10. Normalized 2a expectation (a + b)(a + c) 2a + b + c M11. Jaccard a M12. First Kulczynski a a+b+c b +c M13. Second a M14. Third a +d Sokal-Sne</context>
</contexts>
<marker>Smadja, 1993</marker>
<rawString>Smadja, Frank (1993). Retrieving collocations from text: Xtract. Computational Linguistics 19(1): 143–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Fan Tan</author>
<author>Kan M Yen</author>
<author>Hang Cui</author>
</authors>
<title>Extending corpus-based identification of light verb constructions using a supervised learning framework.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Multi-word-expressions in a multilingual context,</booktitle>
<pages>49--56</pages>
<location>Trento, Italy.</location>
<contexts>
<context position="11188" citStr="Tan et al., 2006" startWordPosition="1911" endWordPosition="1914">VCs are used in the passive (e.g. the presentation was made), we search to the right of the light verb, also allowing a maximum of 4 intervening words. The above extraction process produced a total of 8,652 VPC and 11,465 LVC candidates when run on the corpus. We then filter out candidates with observed frequencies less than 6, as suggested in Pecina and Schlesinger (2006), to obtain a set of 1,629 VPCs and 1,254 LVCs. Separately, we use the following two available sources of annotations: 3,078 VPC candidates extracted and annotated in (Baldwin, 2005) and 464 annotated LVC candidates used in (Tan et al., 2006). Both sets of annotations give both positive and negative examples. Our final VPC and LVC evaluation datasets were then constructed by intersecting the goldstandard datasets with our corresponding sets of extracted candidates. We also concatenated both sets of evaluation data for composite evaluation. This set is referred to as “Mixed”. Statistics of our three evaluation datasets are summarized in Table 2. VPC data LVC data Mixed Total 413 100 513 (freq ≥ 6) Positive 117 28 145 instances (28.33%) (28%) (23.26%) Table 2. Evaluation data sizes (type count, not token). While these datasets are s</context>
</contexts>
<marker>Tan, Yen, Cui, 2006</marker>
<rawString>Tan, Y. Fan, Kan M. Yen and Cui, Hang (2006). Extending corpus-based identification of light verb constructions using a supervised learning framework. In Proceedings of the EACL 2006 Workshop on Multi-word-expressions in a multilingual context, pages 49–56, Trento, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chengxiang Zhai</author>
</authors>
<title>Exploiting context to identify lexical atoms – A statistical view of linguistic context.</title>
<date>1997</date>
<booktitle>In International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97),</booktitle>
<pages>119--129</pages>
<location>Rio de Janeiro, Brazil.</location>
<contexts>
<context position="6336" citStr="Zhai, 1997" startWordPosition="964" endWordPosition="965">he following two ways. First, noncompositionality can be modeled through the diversity of contexts, measured using entropy. The underlying assumption of this approach is that non-compositional phrases appear in a more restricted set of contexts than compositional ones. Second, non-compositionality can also be measured through context similarity between the phrase and its constituents. The observation here is that non-compositional phrases have different semantics from those of their constituents. It then follows that contexts in which the phrase and its constituents appear would be different (Zhai, 1997). Some VPC examples include carry out, give up. A close approximation stipulates that contexts of a non-compositional phrase’s constituents are also different. For instance, phrases such as hot dog and Dutch courage are comprised of constituents that have unrelated meanings. Metrics that are commonly used to compute context similarity include cosine and dice similarity; distance metrics such as Euclidean and Manhattan norm; and probability distribution measures such as Kullback-Leibler divergence and Jensen-Shannon divergence. Table 1 lists all AMs used in our discussion. The lower left legend</context>
</contexts>
<marker>Zhai, 1997</marker>
<rawString>Zhai, Chengxiang (1997). Exploiting context to identify lexical atoms – A statistical view of linguistic context. In International and Interdisciplinary Conference on Modelling and Using Context (CONTEXT-97), pages 119-129, Rio de Janeiro, Brazil.</rawString>
</citation>
<citation valid="false">
<authors>
<author>A Appendix</author>
</authors>
<title>List of particles used in identifying verb particle constructions. about, aback, aboard, above, abroad, across, adrift, ahead, along,</title>
<note>apart, around, aside, astray, away, back, backward, backwards, behind, by, down, forth, forward, forwards, in, into, off, on, out, over, past, round, through, to, together, under, up, upon, without.</note>
<marker>Appendix, </marker>
<rawString>Appendix A. List of particles used in identifying verb particle constructions. about, aback, aboard, above, abroad, across, adrift, ahead, along, apart, around, aside, astray, away, back, backward, backwards, behind, by, down, forth, forward, forwards, in, into, off, on, out, over, past, round, through, to, together, under, up, upon, without.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>