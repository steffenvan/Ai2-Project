<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.983873">
Polynomial Learnability and Locality of Formal Grammars
</title>
<author confidence="0.966345">
Naoki Abe*
</author>
<affiliation confidence="0.9935955">
Department of Computer and Information Science,
University of Pennsylvania, Philadelphia, PA19104.
</affiliation>
<sectionHeader confidence="0.881457" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999971333333333">
We apply a complexity theoretic notion of feasible
learnability called &amp;quot;polynomial learnability&amp;quot; to the eval-
uation of grammatical formalisms for linguistic descrip-
tion. We show that a novel, nontrivial constraint on the
degree of &amp;quot;locality&amp;quot; of grammars allows not only con-
text free languages but also a rich class of mildy context
sensitive languages to be polynomially learnable. We
discuss possible implications of this result to the theory
of natural language acquisition.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.966837316666667">
Much of the formal modeling of natural language acqui-
sition has been within the classic paradigm of &amp;quot;identi-
fication in the limit from positive examples&amp;quot; proposed
by Gold [7]. A relatively restricted class of formal lan-
guages has been shown to be unlearnable in this sense,
and the problem of learning formal grammars has long
been considered intractable.&apos; The following two contro-
versial aspects of this paradigm, however, leave the im-
plications of these negative results to the computational
theory of language acquisition inconclusive. First, it
places a very high demand on the accuracy of the learn-
ing that takes place — the hypothesized language must
be exactly equal to the target language for it to be con-
sidered &amp;quot;correct&amp;quot;. Second, it places a very permissive
demand on the time and amount of data that may be
required for the learning — all that is required of the
learner is that it converge to the correct language in the
limit.2
Of the many alternative paradigms of learning pro-
posed, the notion of &amp;quot;polynomial learnability&amp;quot; recently
formulated by Blumer et al. [6] is of particular interest
because it addresses both of these problems in a unified
&apos;Supported by an IBM graduate fellowship. The author
gratefully acknowledges his advisor, Scott Weinstein, for his
guidance and encouragement throughout this research.
1Some interesting learnable subclasses of regular languages
have been discovered and studied by Angluin [3].
2 For a comprehensive survey of various paradigms related to
&amp;quot;identification in the limit&amp;quot; that have been proposed to address
the first issue, see Osherson, Stob and Weinstein [12]. As for the
latter issue, Angluin ([5], [4]) investigates the feasible learnabil-
ity of formal languages with the use of powerful oracles such as
&amp;quot;MEMBERSHIP&apos; and &amp;quot;EQUIVALENCE&amp;quot;.
way. This paradigm relaxes the criterion for learning by
ruling a class of languages to be learnable, if each lan-
guage in the class can be approximated, given only pos-
itive and negative examples,3 with a desired degree of
accuracy and with a desired degree of robustness (prob-
ability), but puts a higher demand on the complexity
by requiring that the learner converge in time polyno-
mial in these parameters (of accuracy and robustness)
as well as the size (complexity) of the language being
learned.
In this paper, we apply the criterion of polynomial
learnability to subclasses of formal grammars that are of
considerable linguistic interest. Specifically, we present
a novel, nontrivial constraint on grammars called &amp;quot;k-
locality&amp;quot;, which enables context free grammars and in-
deed a rich class of mildly context sensitive grammars to
be feasibly learnable. Importantly the constraint of k-
locality is a nontrivial one because each k-local subclass
is an exponential class 4 containing infinitely many infi-
nite languages. To the best of the author&apos;s knowledge,
&amp;quot;k-locality&amp;quot; is the first nontrivial constraint on gram-
mars, which has been shown to allow a rich class of
grammars of considerable linguistic interest to be poly-
nomially learnable. We finally mention some recent neg-
ative result in this paradigm, and discuss possible im-
plications of its contrast with the learnability of k-local
classes.
</bodyText>
<sectionHeader confidence="0.986655" genericHeader="introduction">
2 Polynomial Learnability
</sectionHeader>
<bodyText confidence="0.995882714285714">
&amp;quot;Polynomial learnability&amp;quot; is a complexity theoretic
notion of feasible learnability recently formulated by
Blumer et al. ([6]). This notion generalizes Valiant&apos;s
theory of learnable boolean concepts [15], [14] to infinite
objects such as formal languages. In this paradigm, the
languages are presented via infinite sequences of pos-
3We hold no particular stance on the the validity of the claim
that children make no use of negative examples. We do, however,
maintain that the investigation of learnability of grammars from
both positive and negative examples is a worthwhile endeavour
for at least two reasons: First, it has a potential application for
the design of natural language systems that learn. Second, it is
possible that children do make use of indirect negative informa-
tion.
</bodyText>
<note confidence="0.491104">
4A class of grammars g is an exponential class if each sub-
class of g with bounded size contains exponentially (in that size)
many grammars.
</note>
<page confidence="0.997981">
225
</page>
<bodyText confidence="0.999925578947368">
itive and negative examples&apos; drawn with an arbitrary
but time invariant distribution over the entire space,
that is in our case, Er&apos;. Learners are to hypothesize
a grammar at each finite initial segment of such a se-
quence, in other words, they are functions from finite se-
quences of members of Er&apos; X OM to grammars.&apos; The
criterion for learning is a complexity theoretic, approx-
imate, and probabilistic one. A learner is said to learn
if it can, with an arbitrarily high probability (1 — 6),
converge to an arbitrarily accurate (within e) grammar
in a feasible number of examples. &amp;quot;A feasible num-
ber of examples&amp;quot; means, more precisely, polynomial in
the size of the grammar it is learning and the degrees
of probability and accuracy that it achieves — 8-1 and
e-1. &amp;quot;Accurate within e&amp;quot; means, more precisely, that
the output grammar can predict, with error probability
e, future events (examples) drawn from the same dis-
tribution on which it has been presented examples for
learning. We now formally state this criterion.&apos;
</bodyText>
<construct confidence="0.96976475">
Definition 2.1 (Polynomial Learnability) A col-
lection of languages G with an associated &apos;size&apos; function
with respect to some fixed representation mechanism is
polynomially learnable if and only if?
</construct>
<sectionHeader confidence="0.714868" genericHeader="method">
3 fET
</sectionHeader>
<equation confidence="0.939516714285714">
3 q: a polynomial function
V L1 E
V P: a probability measure on Ere
V e, 6 &gt; 0
V m q(e-1 , , size(Li))
[P.M E CA&apos; (Li) I P(L(f (1,-.)),6,Li) c})
&gt; 1 — 6
</equation>
<bodyText confidence="0.9419325">
and f is computable in time polynomial
in the length of input]
If in addition all of f&apos;s output grammars on example
sequences for languages in G belong to g, then we say
that is polynomially learnable by g.
Suppose we take the sequence of the hypotheses
(grammars) made by a learner on successive initial fi-
nite sequences of examples, and plot the &amp;quot;errors&amp;quot; of
those grammars with respect to the language being
learned. The two learnability criteria, &amp;quot;identification
</bodyText>
<footnote confidence="0.949304615384615">
5We let eX(L) denote the set of infinite sequences which con-
tain only positive and negative examples for L, so indicated.
6We let 7 denote the set of all such functions.
7The following presentation uses concepts and notation of
formal learning theory, cf. [12]
8Note the following notation. The inital_segment of a se-
quence t up to the n-th element is denoted by tn. L denotes some
fixed mapping from grammars to languages: If G is a grammar,
L(G) denotes the language generated by it. If Li is a language,
size(Li) denotes the size of a minimal grammar for L1. AB
denotes the symmetric difference, i.e. (A — B)U(B —A). Finally,
if P is a probability measure on Er&apos;, then P. is the cannonical
product extension of P.
</footnote>
<figure confidence="0.888228">
Identification in the Limit
Error
Error
</figure>
<figureCaption confidence="0.999926">
Figure 1: Convergence behaviour
</figureCaption>
<bodyText confidence="0.875463636363636">
in the limit&amp;quot; and &amp;quot;polynomial learnability&amp;quot;, require dif-
ferent kinds of convergence behavior of such a sequence,
as is illustrated in Figure 1.
Blumer et al. ([6]) shows an interesting connection
between polynomial learnability and data compression.
The connection is one way: If there exists a polyno-
mial time algorithm which reliably &amp;quot;compresses&amp;quot; any
sample of any language in a given collection to a prov-
ably small consistent grammar for it, then such an al-
ogorithm polynomially learns that collection. We state
this theorem in a slightly weaker form.
</bodyText>
<construct confidence="0.834506">
Definition 2.2 Let G be a language collection with an
associated size function &amp;quot;size&amp;quot;, and for each n let Ln =
{L E size(L) &lt; n}. Then A is an Occam algorithm
for C with range eize9 f(m,n) if and only if:
</construct>
<equation confidence="0.486924">
VnEN
VLECn
VI E EX(L)
Vm E N
[A(fm) is consistent with&apos;rng(fm)
</equation>
<subsectionHeader confidence="0.5874305">
and A(im) E Cf(n,m)
and A runs in time polynomial in i,n
</subsectionHeader>
<bodyText confidence="0.7007598">
Theorem 2.1 (Blumer et al.) If A is an Occam al-
gorithm for G with range size f(n, m) = 0(nk ma) for
some k &gt; 1, 0 &lt; cr &lt; 1 (i.e. less than linear in sample
size and polynomial in complexity of language), then A
polynomially learns C.
</bodyText>
<footnote confidence="0.870867142857143">
8In [6], the notion of &amp;quot;range dimension&amp;quot; is used in place of
&amp;quot;range size&amp;quot;, which is the Vapnik-Chervonenkis dimension of the
hypothesis class. Here, we use the fact that the dimension of a
hypothesis class with a size bound is at most equal to that size
bound.
&amp;quot;Grammar G is consistent with a sample S if {z. I (e, 0) E
SI C L(G) &amp; L(G) n {z j (z,1) e sl =m.
</footnote>
<figure confidence="0.79742175">
Tint•
olynomial Lear nability
p(IloOld.n)
Moo
</figure>
<page confidence="0.998219">
226
</page>
<sectionHeader confidence="0.998534" genericHeader="method">
3 K-Local Context Free Grammars : locality(r) = 4
</sectionHeader>
<bodyText confidence="0.999874318181818">
The notion of &amp;quot;k-locality&amp;quot; of a context free grammar is
defined with respect to a formulation of derivations de-
fined originally for TAG&apos;s by Vijay-Shanker, Weir, and
Joshi [16] [17], which is a generalization of the notion
of a parse tree. In their formulation, a derivation is a
tree recording the history of rewritings. Each node of
a derivation tree is labeled by a rewriting rule, and in
particular, the root must be labeled with a rule with
the starting symbol as its left hand side. Each edge
corresponds to the application of a rewriting; the edge
from a rule (host rule) to another rule (applied rule) is
labeled with the &amp;quot;position&amp;quot; of the nonterminal in the
right hand side of the host rule at which the rewriting
takes place.
The degree of locality of a derivation is the num-
ber of distinct kinds of rewritings in it - including the
immediate context in which rewritings take place. In
terms of a derivation tree, the degree of locality is the
number of different kinds of edges in it, where two edges
are equivalent just in case the two end nodes are labeled
by the same rules, and the edges themselves are labeled
by the same node address.
</bodyText>
<construct confidence="0.365475">
Definition 3.1 Let 1)(G) denote the set of all deriva-
</construct>
<bodyText confidence="0.996962428571428">
tion trees of G, and let r E 1)(G). Then, the
degree of locality of r, written locality(r), is defined as
follows. locality(r) = card{(p,g,n) I there is an edge in
r from a node labeled with p to another labeled with q,
and is itself labeled with n}
The degree of locality of a grammar is the maximum of
those of all its derivations.
</bodyText>
<construct confidence="0.9763855">
Definition 3.2 A CFG G is called k-local if
max{locality(r) I r E D(G)} &lt; k.
</construct>
<bodyText confidence="0.914954">
We write k-Local-CFG = {G I G E CFG and G is k-
Local} and k-Local-CFL = {L(G) I G E k-Local-CFG
}-
Example 3.1 L1 = {anbnambm I n,m E Ar} E
4-Local-CFL since all the derivations of G1 =
({S ,S1), {a, b} ,
S, {S Si , aSib, --. A}) generating Ll have
degree of locality at most 4. For example, the derivation
for the string a3b3ab has degree of locality 4 as shown
in Figure 2.
A crucical property of k-local grammars, which we
will utilize in proving the learnability result, is that
for each k-local grammar, there exists another k-local
grammar in a specific normal form, whose size is only
</bodyText>
<figure confidence="0.6336005">
S —41 Si
S1 -4a Si b
2
Si -411 Si b
</figure>
<figureCaption confidence="0.998259">
Figure 2: Degree of locality of a derivation of a3b3ab by
</figureCaption>
<bodyText confidence="0.9976105">
polynomially larger than the original grammar. The
normal form in effect puts the grammar into a disjoint
union of small grammars each with at most k rules and
k nonterminal occurences. By &amp;quot;the disjoint union&amp;quot; of
an arbitrary set of n grammars, gn, we mean the
grammar obtained by first reanaming nonterminals in
each gi so that the nonterminal set of each one is dis-
joint from that of any other, and then taking the union
of the rules in all those grammars, and finally adding
the rule S —P Si for each staring symbol Si of gi, and
making a brand new symbol S the starting symbol of
the grammar so obtained.
</bodyText>
<construct confidence="0.7742215">
Lemma 3.1 (K-Local Normal Form) For every k-
local-CFG H, i n = size(H), then there is a k-local-
CFG G such that
1. L(G)= L(H).
2. G is in k-local normal form, i.e. there is an index
set I such that G = (ET,UiezEi,S, IS --0 Si I I E
./} U (UistRi )), and if we let Gi = (Er, Ei, Si, Ri)
for each i E I, then
</construct>
<figure confidence="0.407438571428571">
(a) Each Gi is &amp;quot;k-simple&amp;quot;; Vi E I I Ri l&lt;
k &amp; NTO(Ri) &lt; k.&amp;quot;
(b) Each Gi has size bounded by size(G); Vi E
I size(Ci) = 0(n)
(c) All C, &apos;s have disjoint nonterminal sets;
Vi,j E /(i j) n E,
3. size(G)= 0(n5+1).
</figure>
<construct confidence="0.8762855">
Definition 3.3 We let 0 and 0 to be any maps that
satisfy: If G is any k-local-CFG in k-local normal form,
</construct>
<bodyText confidence="0.560804">
11If R is a set of production rules, then NTO(Ri) denotes the
number of nonterminal occurrences in those rules.
</bodyText>
<figure confidence="0.996735615384615">
1
Si Si b
Si —■■ Si b
SI —sal Si b
Si
S SI
SI -4a Si b
S -4S1 S1
2
S -94 Si b
Si Si b
2
S1 -A
</figure>
<page confidence="0.981206">
227
</page>
<bodyText confidence="0.995897">
then 0(G) is the set of all of its k-local components (G*
above.) If G = (Gi I i E I) is a set of k-simple gram-
mars, then ING) is a single grammar that is a &amp;quot;disjoint
union&amp;quot; of all of the k-simple grammars in G.
</bodyText>
<sectionHeader confidence="0.932733" genericHeader="method">
4 K-Local Context Free Languages
Are Polynomially Learnable
</sectionHeader>
<bodyText confidence="0.9698675">
In this section, we present a sketch of the proof of our
main learnability result.
Theorem 4.1 For each k E N;
k-local-CFL is polynomially learnable.12
</bodyText>
<sectionHeader confidence="0.482708" genericHeader="method">
Proof:
</sectionHeader>
<bodyText confidence="0.99992665">
We prove this by exhibiting an Occam algorithm A for
k-local-CFL with some fixed k, with range size polyno-
mial in the size of a minimal grammar and less than
linear in the sample size.
We assume that A is given a labeled m-sample&amp;quot;
Si, for some L E k-local-CFL with size(H) = it where
H is its minimal k-local-CFG. We let length(SL) =
Eees length(s) = I.&amp;quot; We let St and St denote
the positive and negative portions of Si, respectively,
i.e., St = I 3s E Si, such that a = (x,0)) and
Sj = {a I 35 E SL such that a = (x,1)}. We fix a mini-
mal grammar in k-local normal form G that is consistent
with SL with size(G) &lt; p(n) for some fixed polynomial
p by Lemma 3.1. and the fact that a minimal consis-
tent k-local-CFG is not larger than H. Further, we let
G be the set of all of &amp;quot;k-simple components&amp;quot; of G and
define L(a) = UGredL(Gi). Then note L(0) = L(G).
Since each k-simple component has at most k floater-
minals,,we assume without loss of generality that each
Gs in G has the same nonterminal set of size k, say
</bodyText>
<subsectionHeader confidence="0.621188">
Ek =
</subsectionHeader>
<bodyText confidence="0.94124235483871">
The idea for constructing A is straightforward.
Step 1. We generate all possible rules that may be
in the portion of G that is relevant to St. That is,
if we fix a set of derivations D, one for each string in
St from G, then the set of rules that we generate will
contain all the rules that participate in any derivation
in D. (We let Rel(G , St) denote the restriction of G
to St with respect to some V in this fashion.) We use
12We use the size of a minimal k-local CFG as the size of a
k-local-CFL, i.e., VL E k-local-CFL size(L) = minfsize(G)
G E k-local-CFG &amp; L(G)= U.
135L is a labeled m-sample for L if S C groph(char(L)) and
card(S) = m. oraph(char(L)) is the grap-I1 of the characteristic
function of L, i.e. is the set 1(x, 0) 1 r e u {(r,1) r L}.
14In the sequel, we refer to the number of strings in a sample
as the sample size, and the total length of the strings in a sample
as the sample length.
k-locality of G to show that such a set will be polyno-
mially bounded in the length of S. Step 2. We then
generate the set of all possible grammars having at most
k of these rules. Since each k-simple component of G
has at most k rules, the generated set of grammars will
include all of the k-simple components of G. Step 3.
We then use the negative portion of the sample, Si. to
filter out the &amp;quot;inconsistent&amp;quot; ones. What we have at this
stage is a polynomially bounded set of k-simple gram-
mars with varying sizes, which do not generate any of
St, and contain all the k-simple grammars of G. Asso-
ciated with each k-simple grammar is the portion of St
that it &amp;quot;covers&amp;quot; and its size. Step 4. What an Occam
algorithm needs to do, then, is to find some subset of
these k-simple grammars that &amp;quot;covers&amp;quot; St, and has a
total size that is provably only polynomially larger than
a minimal total size of a subset that covers St, and is
less than linear is the sample size, m. We formalize
this as a variant of &amp;quot;Set Cover&amp;quot; problem which we call
&amp;quot;Weighted Set Cover&amp;quot;(WSC), and prove the existence of
. an approximation algorithm with a performance guar-
antee which suffices to ensure that the output of A will
be a grammar that is provably only polynomially larger
than the minimal one, and is less than linear in the
sample size. The algorithm runs in time polynomial in
the size of the grammar being learned and the sample
length.
Step 1.
A crucial consequence of the way k-locality is defined
is that the &amp;quot;terminal yield&amp;quot; of any rule body that is
used to derive any string in the language could be split
into at most k 1 intervals. (We define the &amp;quot;terminal
yield* of a rule body R to be h(R), where h is a homo-
morphism that preserves terminal symbols and deletes
nonterminal symbols.)
Definition 4.1 (Subyields) For an arbitrary i E N,
an i-tuple of members of E. w = (v1, v2, ..., vi) is said
to be a subyield of a, if there are some u1,...,t‘i,u:+1 E
E. such that a = u2v1u2v2...uivitii+1. We let
SubYields(i,$) = ft v (E;.)x I z &lt; i dc w is a sub-
yield of a).
We then let SubYieldsk(St) denote the set of all
subyields of strings in St that may have come from
a rule body in a k-local-CFG, i.e. subyields that are
tuples of at most k 1 strings.
</bodyText>
<equation confidence="0.859216333333333">
Definition 4.2
SubY ieldsk(St) = U ,estS ubyields(k + 1, a).
Claim 4.1 card(SubYieldsk(St)) =
</equation>
<bodyText confidence="0.7862895">
Proof:
This is obvious, since given a string s of length a, there
</bodyText>
<page confidence="0.995348">
228
</page>
<bodyText confidence="0.999408846153846">
are only 0(a2(1+1)) ways of choosing 2(k + 1) differ-
ent positions in the string. This completely specifies all
the elements of SubYieldsk+1(s). Since the number of
strings (m) in St and the length of each string in St
are each bounded by the sample length (1), we have at
most 0(1) x 0(12(k+1)) strings in SubYieldsk(St).
Thus we now have a polynomially generable set of
possible yields of rule bodies in G. The next step is
to generate the set of all possible rules having these
yields. Now, by k-locality, in any derivation of G we
have at most k distinct &amp;quot;kinds&amp;quot; of rewritings present.
So, each rule has at most k useful nonterminal oc-
currences and since G is minimal, it is free of useless
nonterminals. We generate all possible rules with at
most k nonterminal occurrences from some fixed set of
k nonterminals (Ek), having as terminal subyields, one
of SubYieldsk(St). We will then have generated all
possible rules of Rel(G , St). In other words, such a
set will provably contain all the rules of Rel(G , St).
We let TFRules(Ek) denote the set of &amp;quot;terminal free
rules&amp;quot; {Ai. In&lt;k&amp;Vj&lt;
n A&amp;quot; E El.) We note that the cardinality of such a set
is a function only of k. We then &amp;quot;assign&amp;quot; members of
SubYieldsk(St) to TFRules(Ek), wherever it is possi-
ble (or the arities agree). We let CRules(k, Sill denote
the set of &amp;quot;candidate rules&amp;quot; so obtained.
</bodyText>
<equation confidence="0.767766333333333">
Definition 4.3 C Rules(k, St)
{R(wilri,..•,Wn/Xn) I R E TFRules(Ek) &amp; w E
SubY ieldsk(St) &amp; arity(w) = arity(R) = n}
</equation>
<bodyText confidence="0.9713445">
It is easy to see that the number of rules in such a set
is also polynomially bounded.
</bodyText>
<subsectionHeader confidence="0.62253">
Claim 4.2 card(C Rules(k, Si)) = 0(1244)
</subsectionHeader>
<bodyText confidence="0.9813795">
Step 2.
Recall that we have assumed that they each have a non-
terminal set contained in some fixed set of k nontermi-
nals, E. So if we generate all subsets of C Rules(k, St)
with at most k rules, hen these will include all the k-
simple grammars in G.
</bodyText>
<subsectionHeader confidence="0.404644">
Definition 4.4
</subsectionHeader>
<bodyText confidence="0.595722">
CGrams(k,St)=Pk(CRules(k,Bt)).&apos;
</bodyText>
<subsectionHeader confidence="0.388355">
Step 3.
</subsectionHeader>
<bodyText confidence="0.998655333333333">
Now we finally make use of the negative portion of the
sample, SE, to ensure that we do not include any in-
consistent grammars in our candidates.
</bodyText>
<footnote confidence="0.3577295">
151&amp;quot;k(X) in general denotes the set of all subsets of X with
cardinality at most k.
</footnote>
<construct confidence="0.717265">
Definition 4.5 FGrams(k, S 1,) = {H I H E
CGrams(k, St) &amp; L(H) n si- =
</construct>
<bodyText confidence="0.818974882352941">
This filtering can be computed in time polynomial in
the length of SL, because for testing consistency of each
grammar in CGrams(k, St), all that is involved is the
membership question for strings in Sy— with that gram-
mar.
Step 4.
What we have at this stage is a set of `subcovers&apos; of St,
each with a size (or &apos;weight&apos;) associated with it, and we
wish to find a subset of these `subcovers&apos; that cover the
entire St, but has a provably small &apos;total weight&apos;. We
abstract this as the following problem.
WEIGHTED-SET-COVER(WSC)
INSTANCE: (X, Y, w) where X is a finite set and Y is
a subset of P(X) and w is a function from Y to N+.
Intuitively, Y is a set of subcovers of the set X, each
associated with its &apos;weight&apos;.
NOTATION: For every subset Z of Y, we let cover(Z)=
</bodyText>
<figure confidence="0.688582">
U{z z E Z}, and totalweight(Z) =Ezez w(z).
QUESTION: What subset of Y is a set-cover of X with
a minimal total weight, i.e. find Z C Y with the follow-
ing properties:
(i) cover(Z)= X.
VZ&apos; C Y if cover(r) = X then totalweight(r) &gt;
totalweight(Z).
</figure>
<bodyText confidence="0.998115333333333">
We now prove the existence of an approximation
algorithm for this problem with the desired performance
guarantee.
</bodyText>
<construct confidence="0.99455175">
Lemma 4.1 There is an algorithm B and a polyno-
mial p such that given an arbitrary instance (X,Y,w)
of WEIGHTED-SET-COVER with I X I= n, always
outputs Z such that;
</construct>
<listItem confidence="0.921847333333333">
1. Z C Y
2. Z is a cover for X, i.e. UZ = X
3. If Z&apos; is a minimal weight set cover for (X,Y,w),
then Evez w(y) &lt; p(Evez, w(y)) x log n.
4. B runs in time polynomial in the size of the in-
stance.
</listItem>
<bodyText confidence="0.7993195">
Proof: To exhibit an algorithm with this property, we
make use of the greedy algorithm C for the standard
</bodyText>
<page confidence="0.996953">
229
</page>
<bodyText confidence="0.999757">
set-cover problem due to Johnson ([8]), with a perfor-
mance guarantee. SET-COVER can be thought of as a
special case of WEIGHTED-SET-COVER with weight
function being the constant funtion 1.
</bodyText>
<construct confidence="0.591691666666667">
Theorem 4.2 (David S. Johnson)
There is a greedy algorithm C for SET-COVER such
that given an arbitrary instance (X, Y) with an optimal
solution Z&apos;, outputs a solution Z, such that card(Z) =
0(log I X I xcard(V)) and runs in time polynomial in
the instance size.
</construct>
<bodyText confidence="0.9985505">
Now we present the algorithm for WSC. The idea
of the algorithm is simple. It applies C on X and suc-
cessive subclasses of Y with bounded weights, upto the
maximum weight there is, but using only powers of 2 as
the bounds. It then outputs one with a minimal total
weight among those.
</bodyText>
<figure confidence="0.70699575">
Algorithm B: ((X, Y, to))
maxweight := max{w(y) y E Y}
m := [log maxweightl
/* this loop gets an approximate solution using C
for subsets of Y each defined by putting an upperbound
on the weights */
For i = 1 to m do:
Y[i] := fyIyEY &amp; w(y) 2i)
s[t] := C((X,
End /* For */
/* this loop replaces all &apos;bad&apos; (i.e. does not cover X)
solutions with Y — the solution with the maximum
total weight */
For i = 1 to m do:
s[i] := 411 if cover(s[i])= X
:= Y otherwise
End /* For */
mintotaiweight := min{totalweight(s[j]) I j E [m]}
Return s[minfi I totalweight(s[i])= mintotalweight}]
End /* Algorithm B *I
</figure>
<subsectionHeader confidence="0.982215">
Time Analysis
</subsectionHeader>
<bodyText confidence="0.9985474">
Clearly, Algorithm B runs in time polynomial in
the instance size, since Algorithm C runs in time poly-
nomial in the instance size and there are only m =
flog maxweightl calls to it, which certainly does not
exceed the instance size.
</bodyText>
<subsectionHeader confidence="0.963674">
Performance Guarantee
</subsectionHeader>
<bodyText confidence="0.998654739130435">
Let (X, Y, to) be a given instance with card(X) =
n. Then let Z&apos; be an optimal solution of that in-
stance, i.e., it is a minimal total weight set cover. Let
totalweight(V) = w&apos;. Now let m* = [log max{w(z)
z E 211. Then m* &lt; min(n, flog max weightl). So
when C is called with an instance (X,Y[m]) in the
m&amp;quot;-th iteration of the first Tor&apos;-loop in the algorithm,
every member of Z is in Y[ml. Hence, the optimal
solution of this instance equals Z. Thus, by the per-
formance guarantee of C, srms] will be a cover of X
with cardinality at most card(Z°) x log n. Thus, we
have card(s[m]) &lt; card(Ze) x log n. Now, for every
member t of s[m&amp;quot;], w(t) &lt; 2&amp;quot;‘. &lt; 24&amp;quot;.1 &lt; 2e.
Therefore, totalweight(s[m]) = card(Z) x log n x
0(2e) = 0(e) x log n x 0(2w&amp;quot;), since to&amp;quot; certainly
is at least as large as card(r). Hence, we have
totalweight(strep = 0(e2 x log n). Now it is clear
that the output of B will be a cover, and its total weight
will not exceed the total weight of s[ml. We conclude
therefore that B((X ,Y, w)) will be a set-cover for X,
with total weight bounded above by 0(tos2 x log n),
where to&apos; is the total weight of a minimal weight cover
and n =I X I.
Now, to apply algorithm B to our learning problem,
we let Y = fst n L(H) H E FGrams(k, SL)} and de-
fine the weight function to: Y N+ by Vy E Y w(y) =
min{size(H) I HE FGrams(k, S z,) &amp; y = L(H) n St)
and call B on (St, Y, w). We then output the gram-
mar &apos;corresponding&apos; to B((St, Y, to)). In other words,
we let H = {mingrammar(y) I y E B((St, w))}
where mingrammar(y) is a minimal-size grammar H
in FGrams(k,SL) such that L(H) n S = y. The
final output grammar H will be the &amp;quot;disjoint union&amp;quot;
of all the grammars in H, i.e. H = b(H). H is
clearly consistent with SL, and since the minimal to-
tal weight solution of this instance of WSC is no larger
than Rel(d, St), by the performance guarantee on the
algorithm B, size(H) &lt; p(size(Rel(G, St))) x 0(log m)
for some polynomial p, where m is the sample size.
size(G) &gt; size(Rel(G, St)) is also bounded by a poly-
nomial in the size of a minimal grammar consistent with
SL. We therefore have shown the existence of an Occam
algorithm with range size polymomial in the size of a
minimal consistent grammar and less than linear in the
sample size. Hence, Theorem 4.1 has been proved.
Q.E.D.
</bodyText>
<sectionHeader confidence="0.808021" genericHeader="method">
5 Extension to Mildly Context Sen-
sitive Languages
</sectionHeader>
<bodyText confidence="0.99792075">
The learnability of k-local subclasses of CFG may ap-
pear to be quite restricted. It turns out, however, that
the learnability of k-local subclasses extends to a rich
class of mildly context sensitive grammars which we
</bodyText>
<page confidence="0.98532">
230
</page>
<bodyText confidence="0.971498977777778">
call &amp;quot;Ranked Node Rewriting Grammars&amp;quot; (RNRG&apos;s).
RNRG&apos;s are based on the underlying ideas of Tree Ad-
joining Grammars (TAG&apos;s) &amp;quot;, and are also a specical
case of context free tree grammars [13] in which unre-
stricted use of variables for moving, copying and delet-
ing, is not permitted. In other words each rewriting
in this system replaces a &amp;quot;ranked&amp;quot; nonterminal node of
say rank j with an &amp;quot;incomplete&amp;quot; tree containing exactly
j edges that have no descendants. If we define a hier-
archy of languages generated by subclasses of RNRG&apos;s
having nodes and rules with bounded rank j (RNRL,),
then RNRL0 = CFL, and RNRL, = TAT,&amp;quot; It turns
out that each k-local subclass of each RNRL, is poly-
nomially learnable. Further, the constraint of k-locality
on RNRG&apos;s is an interesting one because not only each
k-local subclass is an exponential class containing in-
finitely many infinite languages, but also k-local sub-
classes of the RNRG hierarchy become progressively
more complex as we go higher in the hierarchy. In par-
ticular, for each j, RNRG, can &amp;quot;count up to&amp;quot; 2(j + 1)
and for each k &gt; 2, k-local-RNRG, can also count up
to 2(j -I- 1).&amp;quot;
We will omit a detailed definition of RNRG&apos;s (see
[2]), and informally illustrate them by some examples.&amp;quot;
Example 5.1 Ll = {anbn In E N} E CFL is gen-
erated by the following RN RG0 grammar, where a is
shown in Figure 3. GI = ({S},{s,a,b},11,{S},{S
a, S s(A)})
Example 5.2 1.2 = {anbnCndn I n E N] E
TAL is generated by the following RNRG, gram-
mar, where is shown in Figure 3. G2 =
({S},{3, a, b, c, 411, {(S(A))}, {S --• 13, S s(t)})
Example 5.3 1.3 = {anbnCndnCnfn n E N}
TAL is generated by the following RN RG2 gram-
mar, where 7 is shown in Figure 3. G3 =
({S}, {s, a, b, c, d, e, f}, tt, {(S (A, A))) , {S —■ 7,S
8(4,)}). An example of a tree in the tree language of
G3 having as its yield `aabbccddeef 1&apos; is also shown in
Figure 3.
16ee adjoining grammars were introduced as a formalism
for linguistic description by Joshi et al. [10], [9]. Various formal
and computational properties of TAG&apos;s were studied in [16]. Its
linguistic relevance was demonstrated in [11].
17This hierarchy is different from the hierarchy of &amp;quot;meta-
TAL&apos;s&amp;quot; invented and studied extensively by Weir in [18].
</bodyText>
<subsectionHeader confidence="0.531226">
18A class of grammars g is said to be able to &amp;quot;count up to&amp;quot;
</subsectionHeader>
<bodyText confidence="0.973958142857143">
j, just in case { n E N} E {L(G) I G E g} but
fal&apos;ai&apos;...(27+1 I n E N} g {L(G)I G E g}.
18Simpler trees are represented as term structures, whereas
more involved trees are shown in the figure. Also note that we
use uppercase letters for nonterminals and lowercase for termi-
nals. Note the use of the special symbol 1 to indicate an edge
with no descendent.
</bodyText>
<figure confidence="0.994184714285714">
a: -y : derived:
a S b
a s f
a sf
• S d
b scd se
b I c b Xcd Xe
</figure>
<figureCaption confidence="0.999775">
Figure 3: a,19, 7 and deriving aabbccddee f f&apos; by G3
</figureCaption>
<bodyText confidence="0.99809475">
We state the learnability result of RNRL, &apos;s below
as a theorem, and again refer the reader to [2] for details.
Note that this theorem sumsumes Theorem 4.1 as the
case j = 0.
</bodyText>
<construct confidence="0.8871955">
Theorem 5.1 Vj, k E N k-local- RNRL, is polynomi-
ally learnable.&amp;quot;
</construct>
<sectionHeader confidence="0.991943" genericHeader="method">
6 Some Negative Results
</sectionHeader>
<bodyText confidence="0.990361333333333">
The reader&apos;s reaction to the result described above may
be an illusion that the learnability of k-local grammars
follows from &amp;quot;bounding by k&amp;quot;. On the contrary, we
present a case where &amp;quot;bounding by k&amp;quot; not only does
not help feasible learning, but in some senae makes it
harder to learn. Let us consider Tree Adjoining Gram-
mars without local constraints, TAG(wolc) for the sake
of comparison.21 Then an anlogous argument to the one
for the learnability of k-local-CFL shows that k-local-
TAL(wok) is polynornially learnable for any k.
Theorem 8.1 Vk E N+ k-local-TAL(wolc) is polyno-
mially learnable.
Now let us define subclasses of TAG(wolc) with
a bounded number of initial trees; k-initial-tree-
TAG(wolc) is the class of TAG(wolc) with at most k
initial trees. Then surprisingly, for the case of single
letter alphabet, we already have the following striking
result. (For full detail, see [1].)
</bodyText>
<construct confidence="0.9427935">
Theorem 6.2 (i) TAL(wolc) on 1-letter alphabet is
polynomially learnable.
</construct>
<footnote confidence="0.4098792">
28We use the size of a minimal k-local RNRG, as the size of
a k-local RNRLj, i.e., Vj E N VL E k-local-RNRLj size(L) =
min{size(G) G E k-local-RNRG, &amp; L(G)= L}.
21T1ee Adjoining Grammar formalism was never defined with-
out local constrains.
</footnote>
<page confidence="0.993036">
231
</page>
<bodyText confidence="0.996406041666667">
(ii) Vk &gt; 3 k-initial-tree-TAL(wolc) on 1-letter al-
phabet is not polynomially learnable by k-initial-tree-
TAG(wolc).
As a corollary to the second part of the above theorem,
we have that k-initial-tree-TAL(wolc) on an arbitrary
alphabet is not polynomially learnable (by k-initial-tree-
TAG(wolc)). This is because we would be able to use
a learning algorithm for an arbitrary alphabet to con-
struct one for the single letter alphabet case.
Corollary 6.1 k-initial-tree-TAL(wolc) is not polyno-
mially learnable by k-initial-tree-TAG(wolc).
The learnability of k-local-TAL(wolc) and the non-
learnability of k-initial-tree-TAL(wok) is an interesting
contrast. Intuitively, in the former case, the &amp;quot;k-bound&amp;quot;
is placed so that the grammar is forced to be an ar-
bitrarily &amp;quot;wide&amp;quot; union of boundedly small grammars,
whereas, in the latter, the grammar is forced to be a
boundedly &amp;quot;narrow&amp;quot; union of arbitrarily large gram-
mars. It is suggestive of the possibility that in fact
human infants when acquiring her native tongue may
start developing small special purpose grammars for dif-
ferent uses and contexts and slowly start to generalize
and compress the large set of similar grammars into a
smaller set.
</bodyText>
<sectionHeader confidence="0.999701" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.9999928">
We have investigated the use of complexity theory to
the evaluation of grammatical systems as linguistic for-
malisms from the point of view of feasible learnabil-
ity. In particular, we have demonstrated that a single,
natural and non-trivial constraint of &amp;quot;locality&amp;quot; on the
grammars allows a rich class of mildly context sensi-
tive languages to be feasibly learnable, in a well-defined
complexity theoretic sense. Our work differs from re-
cent works on efficient learning of formal languages,
for example by Angluin ([4]), in that it uses only ex-
amples and no other powerful oracles. We hope to
have demonstrated that learning formal grammars need
not be doomed to be necessarily computationally in-
tractable, and the investigation of alternative formula-
tions of this problem is a worthwhile endeavour.
</bodyText>
<sectionHeader confidence="0.999647" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999977052631579">
[1] Naoki Abe. Polynomial leamability of semilinear
sets. 1988. Unpublished manuscript.
[2] Naoki Abe. Polynomially learnable subclasses of
mildy context sensitive languages. In Proceedings
of COLING, August 1988.
[3] Dana Angluin. Inference of reversible languages.
Journal of A.C.M., 29:741-765, 1982.
[4] Dana Angluin. Learing k-bounded context-free
grammars. Technical Report YALEU/DCS/TR-
557, Yale University, August 1987.
[5] Dana Angluin. Learning Regular
Sets from Queries and Counter-examples. Techni-
cal Report YALEU/DCS/TR-464, Yale University,
March 1986.
[6] A. Blumer, A. Ehrenfeucht, D. Haussler, and M.
Warmuth. Classifying Learnable Geometric Con-
cepts with the Vapnik-Chervonenkis Dimension.
Technical Report UCSC CRL-86-5, University of
California at Santa Cruz, March 1986.
[7] E. Mark Gold. Language identification in the limit.
Information and Control, 10:447-474, 1967.
[8] David S. Johnson. Approximation algorithms for
combinatorial problems. Journal of Computer and
System Sciences, 9:256-278, 1974. &apos;
[9] A. K. Joshi. How much context-sensitivity is neces-
sary for characterizing structural description - tree
adjoining grammars. In D. Dowty, L. Karttunen,
and A. Zwicky, editors, Natural Language Pro-
cessing - Theoretical, Computational, and Psycho-
logical Perspectives, Cambridege University Press,
1983.
[10] Aravind K. Joshi, Leon Levy, and Masako Taka-
hashi. Tree adjunct grammars. Journal of Com-
puter and System Sciences, 10:136-163, 1975.
[11] A. Kroch and A. K. Joshi. Linguistic relevance
of tree adjoining grammars. 1989. To appear in
Linguistics and Philosophy.
[12] Daniel N. Osherson, Michael Stob, and Scott We-
instein. Systems That Learn. The MIT Press, 1986.
[13] William C. Rounds. Context-free grammars on
trees. In ACM Symposium on Theory of Comput-
ing, pages 143-148, 1969.
[14] Leslie G. Valiant. Learning disjunctions of conjunc-
tions. In The 9th IJCAI, 1985.
[15] Leslie G. Valiant. A theory of the learnable. Com-
munications of A.C.M., 27:1134-1142, 1984.
[16] K. Vijay-Shanker and A. K. Joshi. Some compu-
tational properties of tree adjoining grammars. In
23rd Meeting of A.C.L., 1985.
[17] K. Vijay-Shanker, D. J. Weir, and A. K. Joshi.
Characterizing structural descriptions produced by
various grammatical formalisms. In 25th Meeting
of A.C.L., 1987.
[18] David J. Weir. From Context-Free Grammars to
Tree Adjoining Grammars and Beyond - A disser-
tation proposal. Technical Report MS-CIS-87-42,
University of Pennsylvania, 1987.
</reference>
<page confidence="0.995184">
232
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.702445">
<title confidence="0.999892">Polynomial Learnability and Locality of Formal Grammars</title>
<author confidence="0.997765">Naoki Abe</author>
<affiliation confidence="0.999996">Department of Computer and Information Science,</affiliation>
<address confidence="0.716096">University of Pennsylvania, Philadelphia, PA19104.</address>
<abstract confidence="0.9982482">We apply a complexity theoretic notion of feasible learnability called &amp;quot;polynomial learnability&amp;quot; to the evaluation of grammatical formalisms for linguistic description. We show that a novel, nontrivial constraint on the degree of &amp;quot;locality&amp;quot; of grammars allows not only context free languages but also a rich class of mildy context sensitive languages to be polynomially learnable. We discuss possible implications of this result to the theory of natural language acquisition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Naoki Abe</author>
</authors>
<title>Polynomial leamability of semilinear sets.</title>
<date>1988</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="29354" citStr="[1]" startWordPosition="5404" endWordPosition="5404">Let us consider Tree Adjoining Grammars without local constraints, TAG(wolc) for the sake of comparison.21 Then an anlogous argument to the one for the learnability of k-local-CFL shows that k-localTAL(wok) is polynornially learnable for any k. Theorem 8.1 Vk E N+ k-local-TAL(wolc) is polynomially learnable. Now let us define subclasses of TAG(wolc) with a bounded number of initial trees; k-initial-treeTAG(wolc) is the class of TAG(wolc) with at most k initial trees. Then surprisingly, for the case of single letter alphabet, we already have the following striking result. (For full detail, see [1].) Theorem 6.2 (i) TAL(wolc) on 1-letter alphabet is polynomially learnable. 28We use the size of a minimal k-local RNRG, as the size of a k-local RNRLj, i.e., Vj E N VL E k-local-RNRLj size(L) = min{size(G) G E k-local-RNRG, &amp; L(G)= L}. 21T1ee Adjoining Grammar formalism was never defined without local constrains. 231 (ii) Vk &gt; 3 k-initial-tree-TAL(wolc) on 1-letter alphabet is not polynomially learnable by k-initial-treeTAG(wolc). As a corollary to the second part of the above theorem, we have that k-initial-tree-TAL(wolc) on an arbitrary alphabet is not polynomially learnable (by k-initial-</context>
</contexts>
<marker>[1]</marker>
<rawString>Naoki Abe. Polynomial leamability of semilinear sets. 1988. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoki Abe</author>
</authors>
<title>Polynomially learnable subclasses of mildy context sensitive languages.</title>
<date>1988</date>
<booktitle>In Proceedings of COLING,</booktitle>
<contexts>
<context position="26637" citStr="[2]" startWordPosition="4892" endWordPosition="4892">NRL,), then RNRL0 = CFL, and RNRL, = TAT,&amp;quot; It turns out that each k-local subclass of each RNRL, is polynomially learnable. Further, the constraint of k-locality on RNRG&apos;s is an interesting one because not only each k-local subclass is an exponential class containing infinitely many infinite languages, but also k-local subclasses of the RNRG hierarchy become progressively more complex as we go higher in the hierarchy. In particular, for each j, RNRG, can &amp;quot;count up to&amp;quot; 2(j + 1) and for each k &gt; 2, k-local-RNRG, can also count up to 2(j -I- 1).&amp;quot; We will omit a detailed definition of RNRG&apos;s (see [2]), and informally illustrate them by some examples.&amp;quot; Example 5.1 Ll = {anbn In E N} E CFL is generated by the following RN RG0 grammar, where a is shown in Figure 3. GI = ({S},{s,a,b},11,{S},{S a, S s(A)}) Example 5.2 1.2 = {anbnCndn I n E N] E TAL is generated by the following RNRG, grammar, where is shown in Figure 3. G2 = ({S},{3, a, b, c, 411, {(S(A))}, {S --• 13, S s(t)}) Example 5.3 1.3 = {anbnCndnCnfn n E N} TAL is generated by the following RN RG2 grammar, where 7 is shown in Figure 3. G3 = ({S}, {s, a, b, c, d, e, f}, tt, {(S (A, A))) , {S —■ 7,S 8(4,)}). An example of a tree in the t</context>
<context position="28299" citStr="[2]" startWordPosition="5227" endWordPosition="5227">aid to be able to &amp;quot;count up to&amp;quot; j, just in case { n E N} E {L(G) I G E g} but fal&apos;ai&apos;...(27+1 I n E N} g {L(G)I G E g}. 18Simpler trees are represented as term structures, whereas more involved trees are shown in the figure. Also note that we use uppercase letters for nonterminals and lowercase for terminals. Note the use of the special symbol 1 to indicate an edge with no descendent. a: -y : derived: a S b a s f a sf • S d b scd se b I c b Xcd Xe Figure 3: a,19, 7 and deriving aabbccddee f f&apos; by G3 We state the learnability result of RNRL, &apos;s below as a theorem, and again refer the reader to [2] for details. Note that this theorem sumsumes Theorem 4.1 as the case j = 0. Theorem 5.1 Vj, k E N k-local- RNRL, is polynomially learnable.&amp;quot; 6 Some Negative Results The reader&apos;s reaction to the result described above may be an illusion that the learnability of k-local grammars follows from &amp;quot;bounding by k&amp;quot;. On the contrary, we present a case where &amp;quot;bounding by k&amp;quot; not only does not help feasible learning, but in some senae makes it harder to learn. Let us consider Tree Adjoining Grammars without local constraints, TAG(wolc) for the sake of comparison.21 Then an anlogous argument to the one for </context>
</contexts>
<marker>[2]</marker>
<rawString>Naoki Abe. Polynomially learnable subclasses of mildy context sensitive languages. In Proceedings of COLING, August 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
</authors>
<title>Inference of reversible languages.</title>
<date>1982</date>
<journal>Journal of A.C.M.,</journal>
<pages>29--741</pages>
<contexts>
<context position="2101" citStr="[3]" startWordPosition="326" endWordPosition="326">e learning — all that is required of the learner is that it converge to the correct language in the limit.2 Of the many alternative paradigms of learning proposed, the notion of &amp;quot;polynomial learnability&amp;quot; recently formulated by Blumer et al. [6] is of particular interest because it addresses both of these problems in a unified &apos;Supported by an IBM graduate fellowship. The author gratefully acknowledges his advisor, Scott Weinstein, for his guidance and encouragement throughout this research. 1Some interesting learnable subclasses of regular languages have been discovered and studied by Angluin [3]. 2 For a comprehensive survey of various paradigms related to &amp;quot;identification in the limit&amp;quot; that have been proposed to address the first issue, see Osherson, Stob and Weinstein [12]. As for the latter issue, Angluin ([5], [4]) investigates the feasible learnability of formal languages with the use of powerful oracles such as &amp;quot;MEMBERSHIP&apos; and &amp;quot;EQUIVALENCE&amp;quot;. way. This paradigm relaxes the criterion for learning by ruling a class of languages to be learnable, if each language in the class can be approximated, given only positive and negative examples,3 with a desired degree of accuracy and with </context>
</contexts>
<marker>[3]</marker>
<rawString>Dana Angluin. Inference of reversible languages. Journal of A.C.M., 29:741-765, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
</authors>
<title>Learing k-bounded context-free grammars.</title>
<date>1987</date>
<tech>Technical Report YALEU/DCS/TR557,</tech>
<institution>Yale University,</institution>
<contexts>
<context position="2327" citStr="[4]" startWordPosition="363" endWordPosition="363"> Blumer et al. [6] is of particular interest because it addresses both of these problems in a unified &apos;Supported by an IBM graduate fellowship. The author gratefully acknowledges his advisor, Scott Weinstein, for his guidance and encouragement throughout this research. 1Some interesting learnable subclasses of regular languages have been discovered and studied by Angluin [3]. 2 For a comprehensive survey of various paradigms related to &amp;quot;identification in the limit&amp;quot; that have been proposed to address the first issue, see Osherson, Stob and Weinstein [12]. As for the latter issue, Angluin ([5], [4]) investigates the feasible learnability of formal languages with the use of powerful oracles such as &amp;quot;MEMBERSHIP&apos; and &amp;quot;EQUIVALENCE&amp;quot;. way. This paradigm relaxes the criterion for learning by ruling a class of languages to be learnable, if each language in the class can be approximated, given only positive and negative examples,3 with a desired degree of accuracy and with a desired degree of robustness (probability), but puts a higher demand on the complexity by requiring that the learner converge in time polynomial in these parameters (of accuracy and robustness) as well as the size (complexit</context>
</contexts>
<marker>[4]</marker>
<rawString>Dana Angluin. Learing k-bounded context-free grammars. Technical Report YALEU/DCS/TR557, Yale University, August 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dana Angluin</author>
</authors>
<title>Learning Regular Sets from Queries and Counter-examples.</title>
<date>1986</date>
<tech>Technical Report YALEU/DCS/TR-464,</tech>
<institution>Yale University,</institution>
<contexts>
<context position="2322" citStr="[5]" startWordPosition="362" endWordPosition="362">ed by Blumer et al. [6] is of particular interest because it addresses both of these problems in a unified &apos;Supported by an IBM graduate fellowship. The author gratefully acknowledges his advisor, Scott Weinstein, for his guidance and encouragement throughout this research. 1Some interesting learnable subclasses of regular languages have been discovered and studied by Angluin [3]. 2 For a comprehensive survey of various paradigms related to &amp;quot;identification in the limit&amp;quot; that have been proposed to address the first issue, see Osherson, Stob and Weinstein [12]. As for the latter issue, Angluin ([5], [4]) investigates the feasible learnability of formal languages with the use of powerful oracles such as &amp;quot;MEMBERSHIP&apos; and &amp;quot;EQUIVALENCE&amp;quot;. way. This paradigm relaxes the criterion for learning by ruling a class of languages to be learnable, if each language in the class can be approximated, given only positive and negative examples,3 with a desired degree of accuracy and with a desired degree of robustness (probability), but puts a higher demand on the complexity by requiring that the learner converge in time polynomial in these parameters (of accuracy and robustness) as well as the size (comp</context>
</contexts>
<marker>[5]</marker>
<rawString>Dana Angluin. Learning Regular Sets from Queries and Counter-examples. Technical Report YALEU/DCS/TR-464, Yale University, March 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Blumer</author>
<author>A Ehrenfeucht</author>
<author>D Haussler</author>
<author>M Warmuth</author>
</authors>
<title>Classifying Learnable Geometric Concepts with the Vapnik-Chervonenkis Dimension.</title>
<date>1986</date>
<tech>Technical Report UCSC CRL-86-5,</tech>
<institution>University of California at Santa Cruz,</institution>
<contexts>
<context position="1742" citStr="[6]" startWordPosition="275" endWordPosition="275">o the computational theory of language acquisition inconclusive. First, it places a very high demand on the accuracy of the learning that takes place — the hypothesized language must be exactly equal to the target language for it to be considered &amp;quot;correct&amp;quot;. Second, it places a very permissive demand on the time and amount of data that may be required for the learning — all that is required of the learner is that it converge to the correct language in the limit.2 Of the many alternative paradigms of learning proposed, the notion of &amp;quot;polynomial learnability&amp;quot; recently formulated by Blumer et al. [6] is of particular interest because it addresses both of these problems in a unified &apos;Supported by an IBM graduate fellowship. The author gratefully acknowledges his advisor, Scott Weinstein, for his guidance and encouragement throughout this research. 1Some interesting learnable subclasses of regular languages have been discovered and studied by Angluin [3]. 2 For a comprehensive survey of various paradigms related to &amp;quot;identification in the limit&amp;quot; that have been proposed to address the first issue, see Osherson, Stob and Weinstein [12]. As for the latter issue, Angluin ([5], [4]) investigates </context>
<context position="4008" citStr="[6]" startWordPosition="626" endWordPosition="626"> is an exponential class 4 containing infinitely many infinite languages. To the best of the author&apos;s knowledge, &amp;quot;k-locality&amp;quot; is the first nontrivial constraint on grammars, which has been shown to allow a rich class of grammars of considerable linguistic interest to be polynomially learnable. We finally mention some recent negative result in this paradigm, and discuss possible implications of its contrast with the learnability of k-local classes. 2 Polynomial Learnability &amp;quot;Polynomial learnability&amp;quot; is a complexity theoretic notion of feasible learnability recently formulated by Blumer et al. ([6]). This notion generalizes Valiant&apos;s theory of learnable boolean concepts [15], [14] to infinite objects such as formal languages. In this paradigm, the languages are presented via infinite sequences of pos3We hold no particular stance on the the validity of the claim that children make no use of negative examples. We do, however, maintain that the investigation of learnability of grammars from both positive and negative examples is a worthwhile endeavour for at least two reasons: First, it has a potential application for the design of natural language systems that learn. Second, it is possibl</context>
<context position="7609" citStr="[6]" startWordPosition="1243" endWordPosition="1243"> is denoted by tn. L denotes some fixed mapping from grammars to languages: If G is a grammar, L(G) denotes the language generated by it. If Li is a language, size(Li) denotes the size of a minimal grammar for L1. AB denotes the symmetric difference, i.e. (A — B)U(B —A). Finally, if P is a probability measure on Er&apos;, then P. is the cannonical product extension of P. Identification in the Limit Error Error Figure 1: Convergence behaviour in the limit&amp;quot; and &amp;quot;polynomial learnability&amp;quot;, require different kinds of convergence behavior of such a sequence, as is illustrated in Figure 1. Blumer et al. ([6]) shows an interesting connection between polynomial learnability and data compression. The connection is one way: If there exists a polynomial time algorithm which reliably &amp;quot;compresses&amp;quot; any sample of any language in a given collection to a provably small consistent grammar for it, then such an alogorithm polynomially learns that collection. We state this theorem in a slightly weaker form. Definition 2.2 Let G be a language collection with an associated size function &amp;quot;size&amp;quot;, and for each n let Ln = {L E size(L) &lt; n}. Then A is an Occam algorithm for C with range eize9 f(m,n) if and only if: Vn</context>
</contexts>
<marker>[6]</marker>
<rawString>A. Blumer, A. Ehrenfeucht, D. Haussler, and M. Warmuth. Classifying Learnable Geometric Concepts with the Vapnik-Chervonenkis Dimension. Technical Report UCSC CRL-86-5, University of California at Santa Cruz, March 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Mark Gold</author>
</authors>
<title>Language identification in the limit.</title>
<date>1967</date>
<journal>Information and Control,</journal>
<pages>10--447</pages>
<contexts>
<context position="839" citStr="[7]" startWordPosition="123" endWordPosition="123">ty called &amp;quot;polynomial learnability&amp;quot; to the evaluation of grammatical formalisms for linguistic description. We show that a novel, nontrivial constraint on the degree of &amp;quot;locality&amp;quot; of grammars allows not only context free languages but also a rich class of mildy context sensitive languages to be polynomially learnable. We discuss possible implications of this result to the theory of natural language acquisition. 1 Introduction Much of the formal modeling of natural language acquisition has been within the classic paradigm of &amp;quot;identification in the limit from positive examples&amp;quot; proposed by Gold [7]. A relatively restricted class of formal languages has been shown to be unlearnable in this sense, and the problem of learning formal grammars has long been considered intractable.&apos; The following two controversial aspects of this paradigm, however, leave the implications of these negative results to the computational theory of language acquisition inconclusive. First, it places a very high demand on the accuracy of the learning that takes place — the hypothesized language must be exactly equal to the target language for it to be considered &amp;quot;correct&amp;quot;. Second, it places a very permissive demand</context>
</contexts>
<marker>[7]</marker>
<rawString>E. Mark Gold. Language identification in the limit. Information and Control, 10:447-474, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David S Johnson</author>
</authors>
<title>Approximation algorithms for combinatorial problems.</title>
<date>1974</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>9--256</pages>
<publisher></publisher>
<contexts>
<context position="21344" citStr="[8]" startWordPosition="3895" endWordPosition="3895"> existence of an approximation algorithm for this problem with the desired performance guarantee. Lemma 4.1 There is an algorithm B and a polynomial p such that given an arbitrary instance (X,Y,w) of WEIGHTED-SET-COVER with I X I= n, always outputs Z such that; 1. Z C Y 2. Z is a cover for X, i.e. UZ = X 3. If Z&apos; is a minimal weight set cover for (X,Y,w), then Evez w(y) &lt; p(Evez, w(y)) x log n. 4. B runs in time polynomial in the size of the instance. Proof: To exhibit an algorithm with this property, we make use of the greedy algorithm C for the standard 229 set-cover problem due to Johnson ([8]), with a performance guarantee. SET-COVER can be thought of as a special case of WEIGHTED-SET-COVER with weight function being the constant funtion 1. Theorem 4.2 (David S. Johnson) There is a greedy algorithm C for SET-COVER such that given an arbitrary instance (X, Y) with an optimal solution Z&apos;, outputs a solution Z, such that card(Z) = 0(log I X I xcard(V)) and runs in time polynomial in the instance size. Now we present the algorithm for WSC. The idea of the algorithm is simple. It applies C on X and successive subclasses of Y with bounded weights, upto the maximum weight there is, but u</context>
</contexts>
<marker>[8]</marker>
<rawString>David S. Johnson. Approximation algorithms for combinatorial problems. Journal of Computer and System Sciences, 9:256-278, 1974. &apos;</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
</authors>
<title>How much context-sensitivity is necessary for characterizing structural description - tree adjoining grammars. In</title>
<date>1983</date>
<booktitle>Natural Language Processing - Theoretical, Computational, and Psychological Perspectives, Cambridege</booktitle>
<editor>D. Dowty, L. Karttunen, and A. Zwicky, editors,</editor>
<publisher>University Press,</publisher>
<contexts>
<context position="27426" citStr="[9]" startWordPosition="5053" endWordPosition="5053">,{S},{S a, S s(A)}) Example 5.2 1.2 = {anbnCndn I n E N] E TAL is generated by the following RNRG, grammar, where is shown in Figure 3. G2 = ({S},{3, a, b, c, 411, {(S(A))}, {S --• 13, S s(t)}) Example 5.3 1.3 = {anbnCndnCnfn n E N} TAL is generated by the following RN RG2 grammar, where 7 is shown in Figure 3. G3 = ({S}, {s, a, b, c, d, e, f}, tt, {(S (A, A))) , {S —■ 7,S 8(4,)}). An example of a tree in the tree language of G3 having as its yield `aabbccddeef 1&apos; is also shown in Figure 3. 16ee adjoining grammars were introduced as a formalism for linguistic description by Joshi et al. [10], [9]. Various formal and computational properties of TAG&apos;s were studied in [16]. Its linguistic relevance was demonstrated in [11]. 17This hierarchy is different from the hierarchy of &amp;quot;metaTAL&apos;s&amp;quot; invented and studied extensively by Weir in [18]. 18A class of grammars g is said to be able to &amp;quot;count up to&amp;quot; j, just in case { n E N} E {L(G) I G E g} but fal&apos;ai&apos;...(27+1 I n E N} g {L(G)I G E g}. 18Simpler trees are represented as term structures, whereas more involved trees are shown in the figure. Also note that we use uppercase letters for nonterminals and lowercase for terminals. Note the use of the</context>
</contexts>
<marker>[9]</marker>
<rawString>A. K. Joshi. How much context-sensitivity is necessary for characterizing structural description - tree adjoining grammars. In D. Dowty, L. Karttunen, and A. Zwicky, editors, Natural Language Processing - Theoretical, Computational, and Psychological Perspectives, Cambridege University Press, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Leon Levy</author>
<author>Masako Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal of Computer and System Sciences,</journal>
<pages>10--136</pages>
<contexts>
<context position="27421" citStr="[10]" startWordPosition="5052" endWordPosition="5052">,b},11,{S},{S a, S s(A)}) Example 5.2 1.2 = {anbnCndn I n E N] E TAL is generated by the following RNRG, grammar, where is shown in Figure 3. G2 = ({S},{3, a, b, c, 411, {(S(A))}, {S --• 13, S s(t)}) Example 5.3 1.3 = {anbnCndnCnfn n E N} TAL is generated by the following RN RG2 grammar, where 7 is shown in Figure 3. G3 = ({S}, {s, a, b, c, d, e, f}, tt, {(S (A, A))) , {S —■ 7,S 8(4,)}). An example of a tree in the tree language of G3 having as its yield `aabbccddeef 1&apos; is also shown in Figure 3. 16ee adjoining grammars were introduced as a formalism for linguistic description by Joshi et al. [10], [9]. Various formal and computational properties of TAG&apos;s were studied in [16]. Its linguistic relevance was demonstrated in [11]. 17This hierarchy is different from the hierarchy of &amp;quot;metaTAL&apos;s&amp;quot; invented and studied extensively by Weir in [18]. 18A class of grammars g is said to be able to &amp;quot;count up to&amp;quot; j, just in case { n E N} E {L(G) I G E g} but fal&apos;ai&apos;...(27+1 I n E N} g {L(G)I G E g}. 18Simpler trees are represented as term structures, whereas more involved trees are shown in the figure. Also note that we use uppercase letters for nonterminals and lowercase for terminals. Note the use o</context>
</contexts>
<marker>[10]</marker>
<rawString>Aravind K. Joshi, Leon Levy, and Masako Takahashi. Tree adjunct grammars. Journal of Computer and System Sciences, 10:136-163, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kroch</author>
<author>A K Joshi</author>
</authors>
<title>Linguistic relevance of tree adjoining grammars.</title>
<date>1989</date>
<note>To appear in Linguistics and Philosophy.</note>
<contexts>
<context position="27552" citStr="[11]" startWordPosition="5071" endWordPosition="5071"> Figure 3. G2 = ({S},{3, a, b, c, 411, {(S(A))}, {S --• 13, S s(t)}) Example 5.3 1.3 = {anbnCndnCnfn n E N} TAL is generated by the following RN RG2 grammar, where 7 is shown in Figure 3. G3 = ({S}, {s, a, b, c, d, e, f}, tt, {(S (A, A))) , {S —■ 7,S 8(4,)}). An example of a tree in the tree language of G3 having as its yield `aabbccddeef 1&apos; is also shown in Figure 3. 16ee adjoining grammars were introduced as a formalism for linguistic description by Joshi et al. [10], [9]. Various formal and computational properties of TAG&apos;s were studied in [16]. Its linguistic relevance was demonstrated in [11]. 17This hierarchy is different from the hierarchy of &amp;quot;metaTAL&apos;s&amp;quot; invented and studied extensively by Weir in [18]. 18A class of grammars g is said to be able to &amp;quot;count up to&amp;quot; j, just in case { n E N} E {L(G) I G E g} but fal&apos;ai&apos;...(27+1 I n E N} g {L(G)I G E g}. 18Simpler trees are represented as term structures, whereas more involved trees are shown in the figure. Also note that we use uppercase letters for nonterminals and lowercase for terminals. Note the use of the special symbol 1 to indicate an edge with no descendent. a: -y : derived: a S b a s f a sf • S d b scd se b I c b Xcd Xe Figu</context>
</contexts>
<marker>[11]</marker>
<rawString>A. Kroch and A. K. Joshi. Linguistic relevance of tree adjoining grammars. 1989. To appear in Linguistics and Philosophy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel N Osherson</author>
<author>Michael Stob</author>
<author>Scott Weinstein</author>
</authors>
<title>Systems That Learn.</title>
<date>1986</date>
<publisher>The MIT Press,</publisher>
<contexts>
<context position="2283" citStr="[12]" startWordPosition="355" endWordPosition="355">lynomial learnability&amp;quot; recently formulated by Blumer et al. [6] is of particular interest because it addresses both of these problems in a unified &apos;Supported by an IBM graduate fellowship. The author gratefully acknowledges his advisor, Scott Weinstein, for his guidance and encouragement throughout this research. 1Some interesting learnable subclasses of regular languages have been discovered and studied by Angluin [3]. 2 For a comprehensive survey of various paradigms related to &amp;quot;identification in the limit&amp;quot; that have been proposed to address the first issue, see Osherson, Stob and Weinstein [12]. As for the latter issue, Angluin ([5], [4]) investigates the feasible learnability of formal languages with the use of powerful oracles such as &amp;quot;MEMBERSHIP&apos; and &amp;quot;EQUIVALENCE&amp;quot;. way. This paradigm relaxes the criterion for learning by ruling a class of languages to be learnable, if each language in the class can be approximated, given only positive and negative examples,3 with a desired degree of accuracy and with a desired degree of robustness (probability), but puts a higher demand on the complexity by requiring that the learner converge in time polynomial in these parameters (of accuracy an</context>
<context position="6918" citStr="[12]" startWordPosition="1122" endWordPosition="1122">ces for languages in G belong to g, then we say that is polynomially learnable by g. Suppose we take the sequence of the hypotheses (grammars) made by a learner on successive initial finite sequences of examples, and plot the &amp;quot;errors&amp;quot; of those grammars with respect to the language being learned. The two learnability criteria, &amp;quot;identification 5We let eX(L) denote the set of infinite sequences which contain only positive and negative examples for L, so indicated. 6We let 7 denote the set of all such functions. 7The following presentation uses concepts and notation of formal learning theory, cf. [12] 8Note the following notation. The inital_segment of a sequence t up to the n-th element is denoted by tn. L denotes some fixed mapping from grammars to languages: If G is a grammar, L(G) denotes the language generated by it. If Li is a language, size(Li) denotes the size of a minimal grammar for L1. AB denotes the symmetric difference, i.e. (A — B)U(B —A). Finally, if P is a probability measure on Er&apos;, then P. is the cannonical product extension of P. Identification in the Limit Error Error Figure 1: Convergence behaviour in the limit&amp;quot; and &amp;quot;polynomial learnability&amp;quot;, require different kinds of</context>
</contexts>
<marker>[12]</marker>
<rawString>Daniel N. Osherson, Michael Stob, and Scott Weinstein. Systems That Learn. The MIT Press, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Rounds</author>
</authors>
<title>Context-free grammars on trees.</title>
<date>1969</date>
<booktitle>In ACM Symposium on Theory of Computing,</booktitle>
<pages>143--148</pages>
<contexts>
<context position="25650" citStr="[13]" startWordPosition="4717" endWordPosition="4717">ze polymomial in the size of a minimal consistent grammar and less than linear in the sample size. Hence, Theorem 4.1 has been proved. Q.E.D. 5 Extension to Mildly Context Sensitive Languages The learnability of k-local subclasses of CFG may appear to be quite restricted. It turns out, however, that the learnability of k-local subclasses extends to a rich class of mildly context sensitive grammars which we 230 call &amp;quot;Ranked Node Rewriting Grammars&amp;quot; (RNRG&apos;s). RNRG&apos;s are based on the underlying ideas of Tree Adjoining Grammars (TAG&apos;s) &amp;quot;, and are also a specical case of context free tree grammars [13] in which unrestricted use of variables for moving, copying and deleting, is not permitted. In other words each rewriting in this system replaces a &amp;quot;ranked&amp;quot; nonterminal node of say rank j with an &amp;quot;incomplete&amp;quot; tree containing exactly j edges that have no descendants. If we define a hierarchy of languages generated by subclasses of RNRG&apos;s having nodes and rules with bounded rank j (RNRL,), then RNRL0 = CFL, and RNRL, = TAT,&amp;quot; It turns out that each k-local subclass of each RNRL, is polynomially learnable. Further, the constraint of k-locality on RNRG&apos;s is an interesting one because not only each </context>
</contexts>
<marker>[13]</marker>
<rawString>William C. Rounds. Context-free grammars on trees. In ACM Symposium on Theory of Computing, pages 143-148, 1969.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie G Valiant</author>
</authors>
<title>Learning disjunctions of conjunctions.</title>
<date>1985</date>
<booktitle>In The 9th IJCAI,</booktitle>
<contexts>
<context position="4092" citStr="[14]" startWordPosition="637" endWordPosition="637">st of the author&apos;s knowledge, &amp;quot;k-locality&amp;quot; is the first nontrivial constraint on grammars, which has been shown to allow a rich class of grammars of considerable linguistic interest to be polynomially learnable. We finally mention some recent negative result in this paradigm, and discuss possible implications of its contrast with the learnability of k-local classes. 2 Polynomial Learnability &amp;quot;Polynomial learnability&amp;quot; is a complexity theoretic notion of feasible learnability recently formulated by Blumer et al. ([6]). This notion generalizes Valiant&apos;s theory of learnable boolean concepts [15], [14] to infinite objects such as formal languages. In this paradigm, the languages are presented via infinite sequences of pos3We hold no particular stance on the the validity of the claim that children make no use of negative examples. We do, however, maintain that the investigation of learnability of grammars from both positive and negative examples is a worthwhile endeavour for at least two reasons: First, it has a potential application for the design of natural language systems that learn. Second, it is possible that children do make use of indirect negative information. 4A class of grammars g</context>
</contexts>
<marker>[14]</marker>
<rawString>Leslie G. Valiant. Learning disjunctions of conjunctions. In The 9th IJCAI, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leslie G Valiant</author>
</authors>
<title>A theory of the learnable.</title>
<date>1984</date>
<journal>Communications of A.C.M.,</journal>
<pages>27--1134</pages>
<contexts>
<context position="4086" citStr="[15]" startWordPosition="636" endWordPosition="636">the best of the author&apos;s knowledge, &amp;quot;k-locality&amp;quot; is the first nontrivial constraint on grammars, which has been shown to allow a rich class of grammars of considerable linguistic interest to be polynomially learnable. We finally mention some recent negative result in this paradigm, and discuss possible implications of its contrast with the learnability of k-local classes. 2 Polynomial Learnability &amp;quot;Polynomial learnability&amp;quot; is a complexity theoretic notion of feasible learnability recently formulated by Blumer et al. ([6]). This notion generalizes Valiant&apos;s theory of learnable boolean concepts [15], [14] to infinite objects such as formal languages. In this paradigm, the languages are presented via infinite sequences of pos3We hold no particular stance on the the validity of the claim that children make no use of negative examples. We do, however, maintain that the investigation of learnability of grammars from both positive and negative examples is a worthwhile endeavour for at least two reasons: First, it has a potential application for the design of natural language systems that learn. Second, it is possible that children do make use of indirect negative information. 4A class of gram</context>
</contexts>
<marker>[15]</marker>
<rawString>Leslie G. Valiant. A theory of the learnable. Communications of A.C.M., 27:1134-1142, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>A K Joshi</author>
</authors>
<title>Some computational properties of tree adjoining grammars.</title>
<date>1985</date>
<booktitle>In 23rd Meeting of A.C.L.,</booktitle>
<contexts>
<context position="9196" citStr="[16]" startWordPosition="1539" endWordPosition="1539">imension&amp;quot; is used in place of &amp;quot;range size&amp;quot;, which is the Vapnik-Chervonenkis dimension of the hypothesis class. Here, we use the fact that the dimension of a hypothesis class with a size bound is at most equal to that size bound. &amp;quot;Grammar G is consistent with a sample S if {z. I (e, 0) E SI C L(G) &amp; L(G) n {z j (z,1) e sl =m. Tint• olynomial Lear nability p(IloOld.n) Moo 226 3 K-Local Context Free Grammars : locality(r) = 4 The notion of &amp;quot;k-locality&amp;quot; of a context free grammar is defined with respect to a formulation of derivations defined originally for TAG&apos;s by Vijay-Shanker, Weir, and Joshi [16] [17], which is a generalization of the notion of a parse tree. In their formulation, a derivation is a tree recording the history of rewritings. Each node of a derivation tree is labeled by a rewriting rule, and in particular, the root must be labeled with a rule with the starting symbol as its left hand side. Each edge corresponds to the application of a rewriting; the edge from a rule (host rule) to another rule (applied rule) is labeled with the &amp;quot;position&amp;quot; of the nonterminal in the right hand side of the host rule at which the rewriting takes place. The degree of locality of a derivation i</context>
<context position="27501" citStr="[16]" startWordPosition="5064" endWordPosition="5064">d by the following RNRG, grammar, where is shown in Figure 3. G2 = ({S},{3, a, b, c, 411, {(S(A))}, {S --• 13, S s(t)}) Example 5.3 1.3 = {anbnCndnCnfn n E N} TAL is generated by the following RN RG2 grammar, where 7 is shown in Figure 3. G3 = ({S}, {s, a, b, c, d, e, f}, tt, {(S (A, A))) , {S —■ 7,S 8(4,)}). An example of a tree in the tree language of G3 having as its yield `aabbccddeef 1&apos; is also shown in Figure 3. 16ee adjoining grammars were introduced as a formalism for linguistic description by Joshi et al. [10], [9]. Various formal and computational properties of TAG&apos;s were studied in [16]. Its linguistic relevance was demonstrated in [11]. 17This hierarchy is different from the hierarchy of &amp;quot;metaTAL&apos;s&amp;quot; invented and studied extensively by Weir in [18]. 18A class of grammars g is said to be able to &amp;quot;count up to&amp;quot; j, just in case { n E N} E {L(G) I G E g} but fal&apos;ai&apos;...(27+1 I n E N} g {L(G)I G E g}. 18Simpler trees are represented as term structures, whereas more involved trees are shown in the figure. Also note that we use uppercase letters for nonterminals and lowercase for terminals. Note the use of the special symbol 1 to indicate an edge with no descendent. a: -y : derived: </context>
</contexts>
<marker>[16]</marker>
<rawString>K. Vijay-Shanker and A. K. Joshi. Some computational properties of tree adjoining grammars. In 23rd Meeting of A.C.L., 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>D J Weir</author>
<author>A K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In 25th Meeting of A.C.L.,</booktitle>
<contexts>
<context position="9201" citStr="[17]" startWordPosition="1540" endWordPosition="1540">ion&amp;quot; is used in place of &amp;quot;range size&amp;quot;, which is the Vapnik-Chervonenkis dimension of the hypothesis class. Here, we use the fact that the dimension of a hypothesis class with a size bound is at most equal to that size bound. &amp;quot;Grammar G is consistent with a sample S if {z. I (e, 0) E SI C L(G) &amp; L(G) n {z j (z,1) e sl =m. Tint• olynomial Lear nability p(IloOld.n) Moo 226 3 K-Local Context Free Grammars : locality(r) = 4 The notion of &amp;quot;k-locality&amp;quot; of a context free grammar is defined with respect to a formulation of derivations defined originally for TAG&apos;s by Vijay-Shanker, Weir, and Joshi [16] [17], which is a generalization of the notion of a parse tree. In their formulation, a derivation is a tree recording the history of rewritings. Each node of a derivation tree is labeled by a rewriting rule, and in particular, the root must be labeled with a rule with the starting symbol as its left hand side. Each edge corresponds to the application of a rewriting; the edge from a rule (host rule) to another rule (applied rule) is labeled with the &amp;quot;position&amp;quot; of the nonterminal in the right hand side of the host rule at which the rewriting takes place. The degree of locality of a derivation is the</context>
</contexts>
<marker>[17]</marker>
<rawString>K. Vijay-Shanker, D. J. Weir, and A. K. Joshi. Characterizing structural descriptions produced by various grammatical formalisms. In 25th Meeting of A.C.L., 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Weir</author>
</authors>
<title>From Context-Free Grammars to Tree Adjoining Grammars and Beyond - A dissertation proposal.</title>
<date>1987</date>
<tech>Technical Report MS-CIS-87-42,</tech>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="27666" citStr="[18]" startWordPosition="5089" endWordPosition="5089"> generated by the following RN RG2 grammar, where 7 is shown in Figure 3. G3 = ({S}, {s, a, b, c, d, e, f}, tt, {(S (A, A))) , {S —■ 7,S 8(4,)}). An example of a tree in the tree language of G3 having as its yield `aabbccddeef 1&apos; is also shown in Figure 3. 16ee adjoining grammars were introduced as a formalism for linguistic description by Joshi et al. [10], [9]. Various formal and computational properties of TAG&apos;s were studied in [16]. Its linguistic relevance was demonstrated in [11]. 17This hierarchy is different from the hierarchy of &amp;quot;metaTAL&apos;s&amp;quot; invented and studied extensively by Weir in [18]. 18A class of grammars g is said to be able to &amp;quot;count up to&amp;quot; j, just in case { n E N} E {L(G) I G E g} but fal&apos;ai&apos;...(27+1 I n E N} g {L(G)I G E g}. 18Simpler trees are represented as term structures, whereas more involved trees are shown in the figure. Also note that we use uppercase letters for nonterminals and lowercase for terminals. Note the use of the special symbol 1 to indicate an edge with no descendent. a: -y : derived: a S b a s f a sf • S d b scd se b I c b Xcd Xe Figure 3: a,19, 7 and deriving aabbccddee f f&apos; by G3 We state the learnability result of RNRL, &apos;s below as a theorem, </context>
</contexts>
<marker>[18]</marker>
<rawString>David J. Weir. From Context-Free Grammars to Tree Adjoining Grammars and Beyond - A dissertation proposal. Technical Report MS-CIS-87-42, University of Pennsylvania, 1987.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>