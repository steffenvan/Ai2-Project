<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014845">
<title confidence="0.999362">
A Simple Generative Pipeline Approach to Dependency Parsing and Se-
mantic Role Labeling
</title>
<author confidence="0.920113">
Daniel Zeman
</author>
<bodyText confidence="0.6816475">
Ústav formální a aplikované lingvistiky
Univerzita Karlova v Praze
Malostranské námestí 25, Praha, CZ-11800, Czechia
zeman@ufal.mff.cuni.cz
</bodyText>
<sectionHeader confidence="0.992131" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998834">
We describe our CoNLL 2009 Shared Task
system in the present paper. The system in-
cludes three cascaded components: a genera-
tive dependency parser, a classifier for
syntactic dependency labels and a semantic
classifier. The experimental results show that
the labeled macro F1 scores of our system on
the joint task range from 43.50% (Chinese) to
57.95% (Czech), with an average of 51.07%.
</bodyText>
<sectionHeader confidence="0.999515" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996491076923077">
The CoNLL 2009 shared task is an extension of
the tasks addressed in previous years: unlike the
English-only 2008 task, the present year deals with
seven languages; and unlike 2006 and 2007, se-
mantic role labeling is performed atop the surface
dependency parsing.
We took part in the closed challenge of the joint
task.1 The input of our system contained gold stan-
dard lemma, part of speech and morphological fea-
tures for each token. Tokens which were
considered predicates were marked in the input
data. The system was required to find the follow-
ing information:
</bodyText>
<listItem confidence="0.951635384615385">
• parent (syntactic dependency) for each to-
ken
1 For more details on the two tasks and challenges, see Hajic et
al. (2009).
• label for each syntactic dependency (to-
ken)
• label for every predicate
• for every token (predicate or non-
predicate) A and every predicate P in the
sentence, say whether there is a semantic
relation between P and A (A is an argu-
ment of P) and if so, provide a label for the
relation (role of the argument)
</listItem>
<bodyText confidence="0.99946">
The organizers of the shared task provided train-
ing and evaluation data (Hajic et al., 2006; Sur-
deanu et al., 2008; Burchardt et al., 2006; Taulé et
al., 2008; Kawahara et al., 2002; Xue and Palmer,
2009) converted to a uniform CoNLL Shared Task
format.
</bodyText>
<sectionHeader confidence="0.993864" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999967">
The system is a sequence of three components: a
surface syntactic parser, a syntactic tagger that as-
signs labels to the syntactic dependencies and a
semantic classifier (labels both the predicates and
the roles of their arguments). We did not attempt to
gain advantage from training a joint classifier for
all the subtasks. We did not have time to do much
beyond putting together the basic infrastructure.
The components 2 and 3 are thus fairly primitive.
</bodyText>
<subsectionHeader confidence="0.998232">
2.1 Surface Dependency Parser
</subsectionHeader>
<bodyText confidence="0.999873333333333">
We use the parser described by Zeman (2004). The
parser takes a generative approach. It has a model
of dependency statistics in which a dependency is
</bodyText>
<page confidence="0.965577">
120
</page>
<note confidence="0.7627775">
Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 120–125,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.999935961538461">
specified by the lemma and tag of the parent and
the child nodes, by direction (left or right) and ad-
jacency. The core of the algorithm can be de-
scribed as repeated greedy selecting of best-
weighted allowed dependencies and adding them
to the dependency tree.
There are other components which affect the de-
pendency selection, too. They range from support-
ing statistical models to a few hard-coded rules.
However, some features of the parser are designed
to work with Czech, or even with the Prague De-
pendency Treebank. For instance, there is a spe-
cialized model for coordinative constructions. The
model itself is statistical but it depends on the PDT
annotation guidelines in various ways. Most nota-
bly, the training component recognizes coordina-
tion by the Coord dependency label, which is not
present in other treebanks. Other rules (e.g. the
constraints on the set of allowed dependencies)
rely on correct interpretation of the part-of-speech
tags.
In order to make the parser less language-
dependent in the multilingual environment of the
shared task, we disabled most of the abovemen-
tioned treebank-bound features. Of course, it led to
decreased performance on the Czech data.2
</bodyText>
<subsectionHeader confidence="0.999923">
2.2 Assignment of Dependency Labels
</subsectionHeader>
<bodyText confidence="0.999906071428572">
The system learns surface dependency labels as a
function of the part-of-speech tags and features of
the parent and the child node. Almost no back-off
is applied. The most frequent label for the given
pair of tags (and feature structures) is always se-
lected. If the pair of tags is unknown, the label is
based on the features of the child node, and if it is
unknown, too, the most frequent label of the train-
ing data is selected.
Obviously, both the training and the labeling
procedures have to know the dependencies. Gold
standard dependencies are examined during train-
ing while parser-generated dependencies are used
for real labeling.
</bodyText>
<subsectionHeader confidence="0.999568">
2.3 Semantic Classifier
</subsectionHeader>
<bodyText confidence="0.999204">
The semantic component solves several tasks.
First, all predicates have to be labeled. Tokens that
</bodyText>
<footnote confidence="0.669835333333333">
2 However, the parser – without adaptation – would not do
well on Czech anyway because the PDT tags are presented in
a different format in the shared task data.
</footnote>
<bodyText confidence="0.99996">
are considered predicates in the particular treebank
are marked on input, so this is a simple classifica-
tion problem. Again, we took the path of least re-
sistance and trained the PRED labels as a function
of gold-standard lemmas.
Second, we have to find semantic dependencies.
Any token (predicate or not) can be the argument
of one or more predicates. These relations may or
may not be parallel to a syntactic dependency. For
each token, we need to find out 1. which predicates
it depends on, and 2. what is the label of its seman-
tic role in this relation?
The task is complex and there are apparently no
simple solutions to it. We learn the semantic role
labels as a function of the gold-standard part of
speech of the argument, the gold-standard lemma
of the predicate and the flag whether there is a syn-
tactic dependency between the two nodes or not.
This approach makes it theoretically possible to
make one token semantically dependent on more
than one predicate. However, we have no means to
control the number of the dependencies.
</bodyText>
<sectionHeader confidence="0.999928" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.999374708333333">
The official results of our system are given in
Table 1. The system made the least syntactic errors
(attachment and labels) for Japanese. The Japanese
treebank seems to be relatively easy to parse, as
many other systems achieved very high scores on
this data. At the other end of the rating scale, Chi-
nese seems to be the syntactically hardest lan-
guage. Our second-worst syntactic score was for
Czech, most likely owing to the turning off all lan-
guage-dependent (and Czech-biased) features of
the parser.
An obvious feature of the table is the extremely
poor semantic scores (in contrast to the accuracy of
surface dependency attachment and labels). While
the simplicity of the additional models does not
seem to hurt too much the dependency labeling, it
apparently is too primitive for semantic role label-
ing. We analyze the errors in more detail in Sec-
tion 4.
The system is platform-independent;3 we have
been running all the experiments under Linux on
an AMD Opteron 848 processor, 2 GHz, with
32 GB RAM. The running times and memory re-
quirements are shown in Table 2.
</bodyText>
<footnote confidence="0.795421">
3 It is written entirely in Perl.
</footnote>
<page confidence="0.982537">
121
</page>
<table confidence="0.999726">
Language Average Cs En De Es Ca Ja Zh
Labeled macro F1 51.07 57.95 50.27 49.57 48.90 49.61 57.69 43.50
OOD lab mac F1 43.67 54.49 48.56 27.97
Labeled syn accur 64.92 57.06 61.82 69.79 65.98 67.68 82.66 49.48
Unlab syn accur 70.84 66.04 70.68 72.91 71.22 73.81 83.36 57.87
Syn labeling accur 79.20 69.10 74.24 84.63 81.83 82.46 95.98 66.13
OOD lab syn acc 50.20 51.45 62.83 36.31
OOD unl syn acc 58.08 60.56 71.78 41.90
OOD syn labeling 69.65 65.64 75.22 68.08
Semantic lab F1 32.14 58.13 36.05 16.44 25.36 24.19 30.13 34.71
OOD sem lab F1 32.86 56.83 31.77 9.98
</table>
<tableCaption confidence="0.9902655">
Table 1. The official results of the system. ISO 639-1 language codes are used (cs = Czech, en = English, de = Ger-
man, es = Spanish, ca = Catalan, ja = Japanese, zh = Chinese). “OOD” means “out-of-domain test data”.
</tableCaption>
<table confidence="0.999914071428571">
Language Cs En De Es Ca Ja Zh
Training sentences 43955 40613 38020 15984 14924 4643 24039
Training tokens 740532 991535 680710 477810 443317 119144 658680
Average sentence length 17 24 18 30 30 26 27
Training minutes 9:21 10:41 8:28 6:17 5:42 1:24 7:01
Training sentences per secnd 78 63 75 42 44 55 57
Training tokens per second 1320 1547 1340 1267 1296 1418 1565
Training rsize memory 3.9 GB 2.2 GB 2.7 GB 2.7 GB 2.4 GB 416 MB 1.5 GB
Test sentences 4213 2399 2000 1725 1862 500 2556
Test tokens 70348 57676 31622 50630 53355 13615 73153
Parsing minutes 6:36 3:11 2:24 5:47 6:05 0:46 5:45
Parsing sentences per second 10.6 12.6 13.9 5.0 5.1 10.9 7.4
Parsing tokens per second 178 302 220 146 146 296 212
Parsing rsize memory 980 MB 566 MB 779 MB 585 MB 487 MB 121 MB 444 MB
</table>
<tableCaption confidence="0.999759">
Table 2. Time and space requirements of the syntactic parser.
</tableCaption>
<bodyText confidence="0.999921074074074">
To assess the need for data, Table 3 presents se-
lected points on the learning curve of our system.
The system has been retrained on 25, 50 and 75%
of the training data for each language (the selection
process was simple: the first N% of sentences of
the training data set were used).
Generally, our method does not seem very data-
hungry. Even for Japanese, with the smallest train-
ing data set, reducing training data to 25% of the
original size makes the scores drop less than 1%
point. The drop for other languages lies mostly
between 1 and 2 points. The exceptions are (unla-
beled) syntactic attachment accuracies of Czech
and Spanish, and labeled semantic F1 of Spanish
and Chinese. The Chinese learning curve also con-
tains a nonmonotonic anomaly of syntactic de-
pendency labeling between data sizes of 50 and
75% (shown in boldface). This can be probably
explained by uneven distribution of the labels in
training data.
As to the comparison of the various languages
and corpora, Japanese seems to be the most spe-
cific (relatively high scores even with such small
data). Spanish and Catalan are related languages,
their treebanks are of similar size, conform to simi-
lar guidelines and were prepared by the same team.
Their scores are very similar.
</bodyText>
<sectionHeader confidence="0.999676" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.998842333333333">
In order to estimate sources of errors, we are now
going to provide some analysis of the data and the
errors our system does.
</bodyText>
<subsectionHeader confidence="0.905028">
4.1 DEPREL Coverage
</subsectionHeader>
<bodyText confidence="0.999752">
The syntactic tagger (assigns DEPREL syntactic
labels) and the semantic tagger (assigns PRED and
APRED labels) are based on simple statistical
models without sophisticated back-off techniques.
</bodyText>
<page confidence="0.973873">
122
</page>
<table confidence="0.999928">
Score TrSize Average Cs En De Es Ca Ja Zh
UnLab 25% 69.38 63.72 69.70 71.36 68.99 72.41 82.58 56.90
Syn
Attach
50% 70.14 64.96 70.13 72.11 70.37 72.83 82.99 57.58
75% 70.51 65.50 70.37 72.50 70.83 73.47 83.17 57.73
100% 70.84 66.04 70.68 72.91 71.22 73.81 83.36 57.87
Syn 25% 78.47 68.28 73.79 84.21 80.67 81.92 95.70 64.71
Label
50% 78.94 68.68 74.08 84.44 81.59 81.99 95.86 65.94
75% 79.03 68.87 74.14 84.51 81.67 82.19 95.97 65.83
100% 79.20 69.10 74.24 84.63 81.83 82.46 95.98 66.13
Labeled 25% 30.10 56.29 34.47 15.51 22.78 22.14 28.91 30.58
Sem F1
50% 33.85 57.24 35.34 16.03 24.46 23.13 29.60 33.31
75% 31.76 57.76 35.85 16.29 24.96 23.77 29.96 33.71
100% 32.14 58.13 36.05 16.44 25.36 24.19 30.13 34.71
Labeled 25% 49.19 55.87 49.06 48.10 46.22 47.76 56.66 40.64
Macro
F1
50% 50.28 56.99 49.66 48.90 47.97 48.53 57.23 42.66
75% 50.68 57.53 50.01 49.26 48.47 49.21 57.52 42.73
100% 51.07 57.95 50.27 49.57 48.90 49.61 57.69 43.50
</table>
<tableCaption confidence="0.999816">
Table 3. The learning curve of the principal scores.
</tableCaption>
<bodyText confidence="0.995020333333334">
Sparse data could pose a serious problem. So how
sparse are the data? Some cue could be drawn from
Table 3. However, we should also know how often
the labels had to be assigned to an unknown set of
input features.
DEPREL (syntactic dependency label) is esti-
mated based on morphological tag (i.e. POS +
FEAT) of both the child and parent. If the pair of
tags is unknown, then it is based on the tag of the
child, and if it is unknown, too, the most frequent
label is chosen. Coverage is high: 93 (Czech) to
97 % (Chinese) of the pairs of tags in test data
were known from training data. Moreover, the er-
ror rate on the unknown pairs is actually much
lower than on the whole data!4
</bodyText>
<subsectionHeader confidence="0.967877">
4.2 PRED Coverage
</subsectionHeader>
<bodyText confidence="0.9999586">
PRED (predicate sense label) is estimated based on
lemma. For most languages, this seems to be a
good selection. Japanese predicate labels are al-
ways identical to lemmas; elsewhere, there are by
average 1.05 (Chinese) to 1.48 (Spanish) labels per
lemma; the exception is German with a label-
lemma ratio of 2.33.
Our accuracy of PRED label assignment ranges
from 71% (German) to 100% (Japanese). We al-
ways assign the most probable label for the given
</bodyText>
<footnote confidence="0.787355666666667">
4 This might suggest that the input features are chosen inap-
propriately and that the DEPREL label should be based just on
the morphology of the child.
</footnote>
<bodyText confidence="0.986713428571429">
lemma; if the lemma is unknown, we copy the
lemma to the PRED column. Coverage is not an
issue here. It goes from 94% (Czech) to almost
100% (German).5 The accuracy on unknown lem-
mas could probably be improved using the sub-
categorization dictionaries accompanying the
training data.
</bodyText>
<table confidence="0.999584764705882">
Language Lemma PREDs
Cs 1. mít 77
2. prijmout 8
En 1. take 20
2. go 18
De 1. kommen 28
2. nehmen 25
1. pasar 10
Es 1. dar 10
3. llevar 9
3. hacer 9
Ca 1. fer 11
2. pasar 9
Ja Always 1 PRED per lemma
1. 5! (yào) 8
Zh 1. -_ff (you) 8
1. tT (da) 8
</table>
<tableCaption confidence="0.99858">
Table 4. Most homonymous predicates.
</tableCaption>
<footnote confidence="0.875451333333333">
5 The coverage of Japanese is 88% but since Japanese PRED
labels are exact copies of lemmas, even unknown lemmas
yield 100%-correct labels.
</footnote>
<page confidence="0.985809">
123
</page>
<table confidence="0.999274909090909">
Language Cs En De Es Ca Ja Zh
Potential APRED slots 1287545 195029 12066 192103 197976 57394 329757
Filled in APREDs 87934 32968 10480 49904 52786 6547 49047
Feature pair coverage (%) 46.05 40.04 14.99 29.34 29.89 18.31 38.08
Non-empty APRED accuracy 73.19 64.65 67.37 56.90 57.89 59.20 68.77
Unlabeled precision 34.94 26.86 10.88 21.71 20.25 9.13 25.66
Unlabeled recall 62.61 63.86 97.52 93.40 92.72 22.10 67.82
Unlabeled F 44.86 37.81 19.57 35.23 33.24 12.93 37.23
Labeled precision 25.58 17.36 7.33 12.35 11.72 5.41 17.64
Labeled recall 45.83 41.28 65.70 53.15 53.67 13.08 46.64
Labeled F 32.83 24.44 13.19 20.05 19.24 7.65 25.60
</table>
<tableCaption confidence="0.97643275">
Table 5. APRED detailed analysis. Non-empty APRED accuracy includes only APRED cells that were non-empty
both in gold standard and system output. Feature-pair coverage includes all cells filled by the system. Unlabeled preci-
sion and recall count non-empty vs. empty APREDs without respect to their actual labels. Counted on development
data with gold-standard surface syntax.
</tableCaption>
<subsectionHeader confidence="0.995597">
4.3 APRED Assignment Analysis
</subsectionHeader>
<bodyText confidence="0.99927875">
The most complicated part of the task is the as-
signment of the APRED labels. In a sense, APRED
labeling is dependency parsing on a deeper level. It
consists of several sub-problems:
</bodyText>
<listItem confidence="0.9994459375">
• Is the node an argument of any predicate at
all?
• If so, how many predicates is the node ar-
gument of? Should the predicate be, say,
coordination, then the node would seman-
tically depend on all members of the coor-
dination.
• In what way is the semantic dependency
related to the syntactic dependency be-
tween the node and its syntactic parent? In
majority of cases, syntactic and semantic
dependencies go parallel; however, there
are still a significant number of semantic
relations for which this assumption does
not hold.6
• Once we know that there is a semantic re-
</listItem>
<bodyText confidence="0.873667260869565">
lation (an APRED field should not be
empty), we still have to figure out the cor-
rect APRED label. This is the semantic
role labeling (or tagging) proper.
6 Nearly all Spanish and Catalan semantic dependencies are
parallel to syntactic ones (but not all syntactic dependencies
are also semantic); in most other languages, about two thirds
of semantic relations match syntax. Japanese is the only lan-
guage in which this behavior does not prevail.
Our system always makes semantic roles paral-
lel to surface syntax. It even does not allow for
empty APRED if there is a syntactic dependency—
this turned out to be one of the major sources of
errors.7
The role labels are estimated based on the
lemma of the predicate and the part of speech of
the argument. Low coverage of this pair of features
in the training data turns to be another major
source of errors. If the pair is not known from
training data, the system selects the most frequent
APRED in the given treebank. Table 5 gives an
overview of the principal statistics relevant to the
analysis of APRED errors.
</bodyText>
<sectionHeader confidence="0.998558" genericHeader="method">
5 Post-evaluation Experiments
</sectionHeader>
<bodyText confidence="0.999964416666666">
Finally, we performed some preliminary experi-
ments focused on the syntactic parser. As men-
tioned in Section 2.1, many features of the parser
have to be turned off unless the parser understands
the part-of-speech and morphological features. We
used DZ Interset (Zeman, 2008) to convert Czech
and English CoNLL POS+FEAT strings to PDT-
like positional tags. Then we switched back on the
parser options that use up the tags and re-ran pars-
ing. The results (Table 6) confirm that the tag ma-
nipulation significantly improves Czech parsing
while it does not help with English.
</bodyText>
<footnote confidence="0.992663">
7 This is a design flaw that we overlooked. Most likely, mak-
ing empty APRED one of the predictable values would im-
prove accuracy.
</footnote>
<page confidence="0.984744">
124
</page>
<table confidence="0.990978">
Cs En
Before 65.81 69.48
After 71.76 68.92
</table>
<tableCaption confidence="0.921997">
Table 6. Unlabeled attachment accuracy on de-
velopment data before and after tagset conversion.
</tableCaption>
<sectionHeader confidence="0.997433" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995272727273">
We described one of the systems that participated
in the CoNLL 2009 Shared Task. We analyzed the
weaknesses of the system and identified possible
room for improvement. The most important point
to focus on in future work is specifying where
APRED should be filled in. The links between syn-
tactic and semantic structures have to be studied
further. Subcategorization frames could probably
help improve these decisions, too—our present
system ignores the subcategorization dictionaries
that accompany the participating treebanks.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998628333333333">
This research has been supported by the Ministry
of Education of the Czech Republic, project No.
MSM0021620838.
</bodyText>
<sectionHeader confidence="0.99941" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999873173076923">
Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea
Kowalski, Sebastian Padó and Manfred Pinkal. 2006.
The SALSA Corpus: a German Corpus Resource for
Lexical Semantics. Proceedings of the 5th Interna-
tional Conference on Language Resources and
Evaluation (LREC-2006). Genova, Italy.
Jan Hajic, Massimiliano Ciaramita, Richard Johansson,
Daisuke Kawahara, Maria Antonia Martí, Lluís
Màrquez, Adam Meyers, Joakim Nivre, Sebastian
Padó, Jan Štepánek, Pavel Stranák, Mihai Surdeanu,
Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009
Shared Task: Syntactic and Semantic Dependencies
in Multiple Languages. Proceedings of the 13th Con-
ference on Computational Natural Language Lear-
ning (CoNLL-2009). June 4-5. pp. 3-22. Boulder,
Colorado, USA.
Jan Hajic, Jarmila Panevová, Eva Hajicová, Petr Sgall,
Petr Pajas, Jan Štepánek, Jifí Havelka, Marie Miku-
lová and Zdenek Žabokrtský. 2006. The Prague De-
pendency Treebank 2.0. CD-ROM. Linguistic Data
Consortium, Philadelphia, Pennsylvania, USA. ISBN
1-58563-370-4. LDC Cat. No. LDC2006T01. URL:
http://ldc.upenn.edu/.
Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida.
2002. Construction of a Japanese Relevance-tagged
Corpus. Proceedings of the 3rd International Confer-
ence on Language Resources and Evaluation (LREC-
2002). pp. 2008-2013. Las Palmas, Spain.
Mihai Surdeanu, Richard Johansson, Adam Meyers,
Lluís Màrquez and Joakim Nivre. 2008. The CoNLL-
2008 Shared Task on Joint Parsing of Syntactic and
Semantic Dependencies. In Proceedings of the 12th
Conference on Computational Natural Language
Learning (CoNLL-2008). August 16 – 17. Manches-
ter, UK.
Mariona Taulé, Maria Antònia Martí and Marta Reca-
sens. 2008. AnCora: Multilevel Annotated Corpora
for Catalan and Spanish. Proceedings of the 6th In-
ternational Conference on Language Resources and
Evaluation (LREC-2008). Marrakech, Morocco.
Nianwen Xue and Martha Palmer. 2009. Adding seman-
tic roles to the Chinese Treebank. Natural Language
Engineering, 15(1):143-172.
Daniel Zeman. 2004. Parsing with a Statistical Depend-
ency Model (PhD thesis). Univerzita Karlova, Praha,
Czechia. URL: http://ufal.mff.cuni.cz/~zeman/pro-
jekty/parser/index.html
Daniel Zeman. 2008. Reusable Tagset Conversion Us-
ing Tagset Drivers. In Proceedings of the 6th Interna-
tional Conference on Language Resources and
Evaluation (LREC-2008). ISBN 2-9517408-4-0.
Marrakech, Morocco.
</reference>
<page confidence="0.998495">
125
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595659">
<title confidence="0.998657">Simple Generative Pipeline Approach to Dependency Parsing and mantic Role Labeling</title>
<author confidence="0.978891">Daniel</author>
<affiliation confidence="0.9275635">Ústav formální a aplikované Univerzita Karlova v</affiliation>
<address confidence="0.988649">25, Praha, CZ-11800,</address>
<email confidence="0.989713">zeman@ufal.mff.cuni.cz</email>
<abstract confidence="0.9614371">We describe our CoNLL 2009 Shared Task system in the present paper. The system includes three cascaded components: a generative dependency parser, a classifier for syntactic dependency labels and a semantic classifier. The experimental results show that the labeled macro F1 scores of our system on the joint task range from 43.50% (Chinese) to 57.95% (Czech), with an average of 51.07%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aljoscha Burchardt</author>
<author>Katrin Erk</author>
<author>Anette Frank</author>
<author>Andrea Kowalski</author>
<author>Sebastian Padó</author>
<author>Manfred Pinkal</author>
</authors>
<title>The SALSA Corpus: a German Corpus Resource for Lexical Semantics.</title>
<date>2006</date>
<booktitle>Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006).</booktitle>
<location>Genova, Italy.</location>
<contexts>
<context position="1787" citStr="Burchardt et al., 2006" startWordPosition="296" endWordPosition="299">em was required to find the following information: • parent (syntactic dependency) for each token 1 For more details on the two tasks and challenges, see Hajic et al. (2009). • label for each syntactic dependency (token) • label for every predicate • for every token (predicate or nonpredicate) A and every predicate P in the sentence, say whether there is a semantic relation between P and A (A is an argument of P) and if so, provide a label for the relation (role of the argument) The organizers of the shared task provided training and evaluation data (Hajic et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Taulé et al., 2008; Kawahara et al., 2002; Xue and Palmer, 2009) converted to a uniform CoNLL Shared Task format. 2 System Description The system is a sequence of three components: a surface syntactic parser, a syntactic tagger that assigns labels to the syntactic dependencies and a semantic classifier (labels both the predicates and the roles of their arguments). We did not attempt to gain advantage from training a joint classifier for all the subtasks. We did not have time to do much beyond putting together the basic infrastructure. The components 2 and 3 are thus fairly primitive. 2.1 Sur</context>
</contexts>
<marker>Burchardt, Erk, Frank, Kowalski, Padó, Pinkal, 2006</marker>
<rawString>Aljoscha Burchardt, Katrin Erk, Anette Frank, Andrea Kowalski, Sebastian Padó and Manfred Pinkal. 2006. The SALSA Corpus: a German Corpus Resource for Lexical Semantics. Proceedings of the 5th International Conference on Language Resources and Evaluation (LREC-2006). Genova, Italy.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajic</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
</authors>
<title>Maria Antonia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Štepánek, Pavel Stranák, Mihai Surdeanu, Nianwen Xue and Yi Zhang.</title>
<date>2009</date>
<booktitle>The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009).</booktitle>
<pages>3--22</pages>
<location>Boulder, Colorado, USA.</location>
<contexts>
<context position="1338" citStr="Hajic et al. (2009)" startWordPosition="211" endWordPosition="214">n previous years: unlike the English-only 2008 task, the present year deals with seven languages; and unlike 2006 and 2007, semantic role labeling is performed atop the surface dependency parsing. We took part in the closed challenge of the joint task.1 The input of our system contained gold standard lemma, part of speech and morphological features for each token. Tokens which were considered predicates were marked in the input data. The system was required to find the following information: • parent (syntactic dependency) for each token 1 For more details on the two tasks and challenges, see Hajic et al. (2009). • label for each syntactic dependency (token) • label for every predicate • for every token (predicate or nonpredicate) A and every predicate P in the sentence, say whether there is a semantic relation between P and A (A is an argument of P) and if so, provide a label for the relation (role of the argument) The organizers of the shared task provided training and evaluation data (Hajic et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Taulé et al., 2008; Kawahara et al., 2002; Xue and Palmer, 2009) converted to a uniform CoNLL Shared Task format. 2 System Description The system is </context>
</contexts>
<marker>Hajic, Ciaramita, Johansson, Kawahara, 2009</marker>
<rawString>Jan Hajic, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Antonia Martí, Lluís Màrquez, Adam Meyers, Joakim Nivre, Sebastian Padó, Jan Štepánek, Pavel Stranák, Mihai Surdeanu, Nianwen Xue and Yi Zhang. 2009. The CoNLL-2009 Shared Task: Syntactic and Semantic Dependencies in Multiple Languages. Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL-2009). June 4-5. pp. 3-22. Boulder, Colorado, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Hajic</author>
<author>Jarmila Panevová</author>
<author>Eva Hajicová</author>
<author>Petr Sgall</author>
<author>Petr Pajas</author>
<author>Jan Štepánek</author>
<author>Jifí Havelka</author>
</authors>
<title>Marie Mikulová and Zdenek Žabokrtský.</title>
<date>2006</date>
<booktitle>The Prague Dependency Treebank 2.0. CD-ROM. Linguistic Data Consortium,</booktitle>
<tech>ISBN 1-58563-370-4. LDC Cat. No. LDC2006T01. URL: http://ldc.upenn.edu/.</tech>
<location>Philadelphia, Pennsylvania, USA.</location>
<contexts>
<context position="1740" citStr="Hajic et al., 2006" startWordPosition="287" endWordPosition="290">tes were marked in the input data. The system was required to find the following information: • parent (syntactic dependency) for each token 1 For more details on the two tasks and challenges, see Hajic et al. (2009). • label for each syntactic dependency (token) • label for every predicate • for every token (predicate or nonpredicate) A and every predicate P in the sentence, say whether there is a semantic relation between P and A (A is an argument of P) and if so, provide a label for the relation (role of the argument) The organizers of the shared task provided training and evaluation data (Hajic et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Taulé et al., 2008; Kawahara et al., 2002; Xue and Palmer, 2009) converted to a uniform CoNLL Shared Task format. 2 System Description The system is a sequence of three components: a surface syntactic parser, a syntactic tagger that assigns labels to the syntactic dependencies and a semantic classifier (labels both the predicates and the roles of their arguments). We did not attempt to gain advantage from training a joint classifier for all the subtasks. We did not have time to do much beyond putting together the basic infrastructure. The compon</context>
</contexts>
<marker>Hajic, Panevová, Hajicová, Sgall, Pajas, Štepánek, Havelka, 2006</marker>
<rawString>Jan Hajic, Jarmila Panevová, Eva Hajicová, Petr Sgall, Petr Pajas, Jan Štepánek, Jifí Havelka, Marie Mikulová and Zdenek Žabokrtský. 2006. The Prague Dependency Treebank 2.0. CD-ROM. Linguistic Data Consortium, Philadelphia, Pennsylvania, USA. ISBN 1-58563-370-4. LDC Cat. No. LDC2006T01. URL: http://ldc.upenn.edu/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Kawahara</author>
<author>Sadao Kurohashi</author>
<author>Koiti Hasida</author>
</authors>
<title>Construction of a Japanese Relevance-tagged Corpus.</title>
<date>2002</date>
<booktitle>Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002).</booktitle>
<pages>2008--2013</pages>
<location>Las Palmas,</location>
<contexts>
<context position="1830" citStr="Kawahara et al., 2002" startWordPosition="304" endWordPosition="307">ation: • parent (syntactic dependency) for each token 1 For more details on the two tasks and challenges, see Hajic et al. (2009). • label for each syntactic dependency (token) • label for every predicate • for every token (predicate or nonpredicate) A and every predicate P in the sentence, say whether there is a semantic relation between P and A (A is an argument of P) and if so, provide a label for the relation (role of the argument) The organizers of the shared task provided training and evaluation data (Hajic et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Taulé et al., 2008; Kawahara et al., 2002; Xue and Palmer, 2009) converted to a uniform CoNLL Shared Task format. 2 System Description The system is a sequence of three components: a surface syntactic parser, a syntactic tagger that assigns labels to the syntactic dependencies and a semantic classifier (labels both the predicates and the roles of their arguments). We did not attempt to gain advantage from training a joint classifier for all the subtasks. We did not have time to do much beyond putting together the basic infrastructure. The components 2 and 3 are thus fairly primitive. 2.1 Surface Dependency Parser We use the parser de</context>
</contexts>
<marker>Kawahara, Kurohashi, Hasida, 2002</marker>
<rawString>Daisuke Kawahara, Sadao Kurohashi and Koiti Hasida. 2002. Construction of a Japanese Relevance-tagged Corpus. Proceedings of the 3rd International Conference on Language Resources and Evaluation (LREC2002). pp. 2008-2013. Las Palmas, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Richard Johansson</author>
<author>Adam Meyers</author>
</authors>
<title>Lluís Màrquez and Joakim Nivre.</title>
<date>2008</date>
<booktitle>The CoNLL2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008). August 16 – 17.</booktitle>
<location>Manchester, UK.</location>
<contexts>
<context position="1763" citStr="Surdeanu et al., 2008" startWordPosition="291" endWordPosition="295">he input data. The system was required to find the following information: • parent (syntactic dependency) for each token 1 For more details on the two tasks and challenges, see Hajic et al. (2009). • label for each syntactic dependency (token) • label for every predicate • for every token (predicate or nonpredicate) A and every predicate P in the sentence, say whether there is a semantic relation between P and A (A is an argument of P) and if so, provide a label for the relation (role of the argument) The organizers of the shared task provided training and evaluation data (Hajic et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Taulé et al., 2008; Kawahara et al., 2002; Xue and Palmer, 2009) converted to a uniform CoNLL Shared Task format. 2 System Description The system is a sequence of three components: a surface syntactic parser, a syntactic tagger that assigns labels to the syntactic dependencies and a semantic classifier (labels both the predicates and the roles of their arguments). We did not attempt to gain advantage from training a joint classifier for all the subtasks. We did not have time to do much beyond putting together the basic infrastructure. The components 2 and 3 are thus f</context>
</contexts>
<marker>Surdeanu, Johansson, Meyers, 2008</marker>
<rawString>Mihai Surdeanu, Richard Johansson, Adam Meyers, Lluís Màrquez and Joakim Nivre. 2008. The CoNLL2008 Shared Task on Joint Parsing of Syntactic and Semantic Dependencies. In Proceedings of the 12th Conference on Computational Natural Language Learning (CoNLL-2008). August 16 – 17. Manchester, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mariona Taulé</author>
</authors>
<title>Maria Antònia Martí and Marta Recasens.</title>
<date>2008</date>
<booktitle>Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008).</booktitle>
<location>Marrakech, Morocco.</location>
<marker>Taulé, 2008</marker>
<rawString>Mariona Taulé, Maria Antònia Martí and Marta Recasens. 2008. AnCora: Multilevel Annotated Corpora for Catalan and Spanish. Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008). Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Martha Palmer</author>
</authors>
<title>Adding semantic roles to the Chinese Treebank.</title>
<date>2009</date>
<journal>Natural Language Engineering,</journal>
<pages>15--1</pages>
<contexts>
<context position="1853" citStr="Xue and Palmer, 2009" startWordPosition="308" endWordPosition="311">tic dependency) for each token 1 For more details on the two tasks and challenges, see Hajic et al. (2009). • label for each syntactic dependency (token) • label for every predicate • for every token (predicate or nonpredicate) A and every predicate P in the sentence, say whether there is a semantic relation between P and A (A is an argument of P) and if so, provide a label for the relation (role of the argument) The organizers of the shared task provided training and evaluation data (Hajic et al., 2006; Surdeanu et al., 2008; Burchardt et al., 2006; Taulé et al., 2008; Kawahara et al., 2002; Xue and Palmer, 2009) converted to a uniform CoNLL Shared Task format. 2 System Description The system is a sequence of three components: a surface syntactic parser, a syntactic tagger that assigns labels to the syntactic dependencies and a semantic classifier (labels both the predicates and the roles of their arguments). We did not attempt to gain advantage from training a joint classifier for all the subtasks. We did not have time to do much beyond putting together the basic infrastructure. The components 2 and 3 are thus fairly primitive. 2.1 Surface Dependency Parser We use the parser described by Zeman (2004)</context>
</contexts>
<marker>Xue, Palmer, 2009</marker>
<rawString>Nianwen Xue and Martha Palmer. 2009. Adding semantic roles to the Chinese Treebank. Natural Language Engineering, 15(1):143-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Parsing with a Statistical Dependency Model (PhD thesis). Univerzita Karlova,</title>
<date>2004</date>
<location>Praha, Czechia. URL: http://ufal.mff.cuni.cz/~zeman/projekty/parser/index.html</location>
<contexts>
<context position="2453" citStr="Zeman (2004)" startWordPosition="409" endWordPosition="410">almer, 2009) converted to a uniform CoNLL Shared Task format. 2 System Description The system is a sequence of three components: a surface syntactic parser, a syntactic tagger that assigns labels to the syntactic dependencies and a semantic classifier (labels both the predicates and the roles of their arguments). We did not attempt to gain advantage from training a joint classifier for all the subtasks. We did not have time to do much beyond putting together the basic infrastructure. The components 2 and 3 are thus fairly primitive. 2.1 Surface Dependency Parser We use the parser described by Zeman (2004). The parser takes a generative approach. It has a model of dependency statistics in which a dependency is 120 Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 120–125, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics specified by the lemma and tag of the parent and the child nodes, by direction (left or right) and adjacency. The core of the algorithm can be described as repeated greedy selecting of bestweighted allowed dependencies and adding them to the dependency tree. There are other components whi</context>
</contexts>
<marker>Zeman, 2004</marker>
<rawString>Daniel Zeman. 2004. Parsing with a Statistical Dependency Model (PhD thesis). Univerzita Karlova, Praha, Czechia. URL: http://ufal.mff.cuni.cz/~zeman/projekty/parser/index.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Zeman</author>
</authors>
<title>Reusable Tagset Conversion Using Tagset Drivers.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008). ISBN</booktitle>
<pages>2--9517408</pages>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="16432" citStr="Zeman, 2008" startWordPosition="2834" endWordPosition="2835">ow coverage of this pair of features in the training data turns to be another major source of errors. If the pair is not known from training data, the system selects the most frequent APRED in the given treebank. Table 5 gives an overview of the principal statistics relevant to the analysis of APRED errors. 5 Post-evaluation Experiments Finally, we performed some preliminary experiments focused on the syntactic parser. As mentioned in Section 2.1, many features of the parser have to be turned off unless the parser understands the part-of-speech and morphological features. We used DZ Interset (Zeman, 2008) to convert Czech and English CoNLL POS+FEAT strings to PDTlike positional tags. Then we switched back on the parser options that use up the tags and re-ran parsing. The results (Table 6) confirm that the tag manipulation significantly improves Czech parsing while it does not help with English. 7 This is a design flaw that we overlooked. Most likely, making empty APRED one of the predictable values would improve accuracy. 124 Cs En Before 65.81 69.48 After 71.76 68.92 Table 6. Unlabeled attachment accuracy on development data before and after tagset conversion. 6 Conclusion We described one of</context>
</contexts>
<marker>Zeman, 2008</marker>
<rawString>Daniel Zeman. 2008. Reusable Tagset Conversion Using Tagset Drivers. In Proceedings of the 6th International Conference on Language Resources and Evaluation (LREC-2008). ISBN 2-9517408-4-0. Marrakech, Morocco.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>