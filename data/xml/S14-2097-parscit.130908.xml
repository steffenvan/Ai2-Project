<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001586">
<title confidence="0.994741">
ShrdLite: Semantic Parsing Using a Handmade Grammar
</title>
<author confidence="0.997134">
Peter Ljunglöf
</author>
<affiliation confidence="0.831883333333333">
Department of Computer Science and Engineering
University of Gothenburg and Chalmers University of Technology
Gothenburg, Sweden
</affiliation>
<email confidence="0.997242">
peter.ljunglof@cse.gu.se
</email>
<sectionHeader confidence="0.997366" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999621238095238">
This paper describes my approach for
parsing robot commands, which was
task 6 at SemEval 2014. My solution
is to manually create a compact unifica-
tion grammar. The grammar is highly am-
biguous, and relies heavily on filtering the
parse results by checking their consistency
with the current world.
The grammar is small, consisting of not
more than 25 grammatical and 60 lexical
rules. The parser uses simple error correc-
tion together with a straightforward itera-
tive deepening search. Nevertheless, with
these very basic algorithms, the system
still managed to get 86.1% correctness on
the evaluation data. Even more interesting
is that by making the parser slightly more
robust, the accuracy of the system rises
to 93.5%, and by adding one single word
to the lexicon, the accuracy is boosted to
98.0%.
</bodyText>
<sectionHeader confidence="0.999474" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99988475">
SemEval 2014, task 6, was about parsing com-
mands to a robot operating in a blocks world. The
goal is to parse utterances in natural language into
commands in a formal language, the Robot Con-
trol Language (RCL). As a guide the system can
use a spatial planner which can tell whether an
RCL command is meaningful in a given blocks
world.
The utterances are taken from the Robot Com-
mands Treebank (Dukes, 2013), which pairs 3409
sentences with semantic annotations consisting of
an RCL command together with a description of
</bodyText>
<footnote confidence="0.81775375">
This work is licensed under a Creative Commons Attribu-
tion 4.0 International Licence. Page numbers and proceed-
ings footer are added by the organisers. Licence details:
http://creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.984875">
a world where the command is meaningful. The
corpus was divided into 2500 training sentences
and 909 evaluation sentences.
The system that is described in this paper, to-
gether with the evaluation data, is available online
from GitHub:
https://github.com/heatherleaf/semeval-2014-task6
</bodyText>
<sectionHeader confidence="0.992135" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999969529411765">
The Shrdlite system is based on a small unification
grammar, together with a naive robust parser im-
plemented using iterative deepening. After pars-
ing, the resulting parse trees are modified accord-
ing to six extra-grammatical semantic rules.
The grammar and the semantic rules were hand-
crafted using manual analysis of the available
2500 training sentences, and an incremental and
iterative process to select and fine-tune the gram-
mar. The total amount of work for creating the
grammar consisted of about 3–4 days for one per-
son. This excludes programming the robust parser
and the rest of the system, which took another 2–3
days.
I did not have any access to the 909 evalua-
tion sentences while developing the grammar or
the other parts of the system.
</bodyText>
<subsectionHeader confidence="0.969309">
2.1 Grammar
</subsectionHeader>
<bodyText confidence="0.9999653">
The grammar is a Prolog DCG unification gram-
mar (Pereira and Warren, 1980) which builds a se-
mantic parse tree during parsing. The core gram-
mar is shown in figure 1. For presentation pur-
poses, the DCG arguments (including the semantic
parse trees) are left out of the figure. The lexicon
consists of ca 150 surface words (or multi-word
units) divided into 23 lexical categories. The lex-
ical categories are the lowercase italic symbols in
the core grammar.
</bodyText>
<page confidence="0.955101">
556
</page>
<note confidence="0.7465025">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 556–559,
Dublin, Ireland, August 23-24, 2014.
</note>
<table confidence="0.838501357142857">
Main −&gt; Event ?periods
Event −&gt; take-verb Entity take-suffix
 |drop-verb Entity drop-suffix
 |move-verb Entity ?commas move-suffix Destination
 |take-verb Entity take-suffix and-then move-verb RefEntity move-suffix Destination
RefEntity −&gt; ?reference-pronoun
Destination −&gt; SpatialRelation
SpatialRelation −&gt; (Measure  |entity-relation  |Measure entity-relation) Entity
RelativeClause −&gt; ?commas relative-pronoun SpatialRelation
Measure −&gt; Entity
Entity −&gt; ?determiner BasicEntity
BasicEntity −&gt; (cardinal  |indicator  |color  |cube-group-indicator) BasicEntity
 |(type  |one) ?RelativeClause
 |?(its  |the) region-indicator ?of-board
</table>
<figureCaption confidence="0.758303333333333">
Figure 1: The core grammar. Lowercase italic symbols are lexicon entries, and a question mark indicates
that the following symbol is optional. DCG arguments (semantic trees and syntactic coordination) are
left out for presentation reasons.
</figureCaption>
<subsectionHeader confidence="0.997896">
2.2 Semantic Modifications After Parsing
</subsectionHeader>
<bodyText confidence="0.9984814">
After parsing, each resulting semantic parse tree
is modified according to the following rules. Most
of these rules should be possible to implement as
grammar rules, but I felt that this would make the
grammar unnecessarily complicated.
</bodyText>
<listItem confidence="0.998846227272728">
• If a color is specified before an indicator,
change the order between them.
• If an entity of type CUBE is described using
two colours, its type is changed to CUBE-
GROUP.
• Relation words such as “above”, “oppo-
site”, “over”, etc., correspond to the relation
WITHIN() if the entity is of type CORNER or
REGION; for all other entities the relation will
be ABOVE().
• The relation FRONT() is changed to FOR-
WARD() if the entity is of type TILE.
• Add a reference id to the subject of a TAKE-
AND-DROP command sequence, unless it al-
ready has a reference id.
• If the destination of a move command
is of type TYPE-REFERENCE or TYPE-
REFERENCE-GROUP, add a reference id to
the subject; unless the subject is of type
PRISM and it has a spatial relation, in which
case the reference id is added to its spatial re-
lation instead.
</listItem>
<subsectionHeader confidence="0.994216">
2.3 Robust Parsing
</subsectionHeader>
<bodyText confidence="0.99953275">
The parser is a standard Prolog recursive-descent
parser, augmented with simple support for han-
dling robustness. The algorithm is shown in fig-
ure 2.
</bodyText>
<subsectionHeader confidence="0.731474">
2.3.1 Misspellings and Junk Words
</subsectionHeader>
<bodyText confidence="0.999951933333334">
The parser tries to compensate for misspellings
and junk words. Any word can be recognized
as a misspelled word, penalized using the Leven-
shtein edit distance (Levenshtein, 1966), or it can
be skipped as a junk word with a fixed penalty.1
The parser first tries to find an exact match of
the sentence in the grammar, then it gradually al-
lows higher penalties until the sentence is parsed.
This is done using iterative deepening on the edit
penalty of the sentence, until it reaches the maxi-
mum edit penalty – if the sentence still cannot be
parsed, it fails. In the original evaluation I used a
maximum edit penalty of 5, but by just increasing
this penalty, the accuracy was boosted consider-
ably as discussed in sections 2.4 and 3.1.
</bodyText>
<subsectionHeader confidence="0.495206">
2.3.2 Filtering Through the Spatial Planner
</subsectionHeader>
<bodyText confidence="0.999719714285714">
The parser uses the spatial planner that was dis-
tributed together with the task as a black box. It
takes a semantic parse tree and the current world
configuration, and decides if the tree is meaningful
in the given world.
When the sentence is recognized, all its parse
trees are filtered through the spatial planner. If
</bodyText>
<footnote confidence="0.99467">
1The penalty of skipping over a word is 3 if the word is
already in the lexicon, and 2 otherwise.
</footnote>
<page confidence="0.968714">
557
</page>
<equation confidence="0.76685125">
function robust-parse(sentence, world):
for penalty in 0... 5:
trees = { t&apos;  |t E parse-dcg(sentence, edit-penalty=penalty),
t&apos; = modify-tree(t),
</equation>
<bodyText confidence="0.64598725">
spatial-planner(t&apos;, world) = MEANINGFUL }
if trees 7� /0:
return min(trees, key=treesize)
return FAILURE
</bodyText>
<figureCaption confidence="0.999233">
Figure 2: The robust parsing algorithm.
</figureCaption>
<bodyText confidence="0.999477666666667">
none of the trees are meaningful in the world, the
parser tries to parse the sentence with a higher edit
penalty.
</bodyText>
<subsectionHeader confidence="0.982279">
2.3.3 Selecting the Best Tree
</subsectionHeader>
<bodyText confidence="0.999202666666667">
If there is more than one possible semantic tree,
the system returns the tree with the smallest num-
ber of nodes.
</bodyText>
<subsectionHeader confidence="0.973011">
2.4 Minor Modifications After Evaluation
</subsectionHeader>
<bodyText confidence="0.973937482758621">
As explained in section 3.1, the error analysis of
the final evaluation revealed one construction and
one lexical item that did not occur in the training
corpus:
• Utterances can start with 2–3 periods. The
reason why this was not caught by the robust
parser is that each of these periods are consid-
ered a word of its own, and as mentioned in
section 2.3.1, the penalty for skipping a lex-
icon word is 3 which means that the penalty
for parsing a sentence with 2–3 initial periods
is 6 or 9. Unfortunately I had chosen a maxi-
mum penalty of 5 which meant that the orig-
inal evaluation missed all these sentences.
By just increasing the maximum penalty
from 5 to 9, the accuracy increased from
86.1% to 93.5%.
• The word “cell” occurs in the evaluation
data as a synonym for the entity type
TILE, in addition to the existing tile words
“square”, “grid”, “space”, etc. Unfortu-
nately, the parser tries to correct “cell” into
the Levenshtein-similar “cube”, giving the
wrong semantics.
By adding “cell” to the lexicon, the accuracy
increased further from 93.5% to 98.0%.
The results of these minimal modifications are
substantial, and are discussed further in sec-
tion 3.2.
</bodyText>
<sectionHeader confidence="0.996776" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9999485">
The system was evaluated on 909 sentences from
the treebank, and I only tested for exact matches.
The result of the initial evaluation was that 86% of
the sentences returned a correct result, when using
the spatial planner as a guide for selecting parses.
Without the planner, the accuracy was only 51%.
The results are shown in the top rows in tables 1
and 2.
The grammar is ambiguous and the system re-
lies heavily on the spatial planner to filter out can-
didates. Without the planner, 42% of the utter-
ances are ambiguous returning between 2 and 18
trees, but with the planner, only 4 utterances are
ambiguous (i.e., 0.4%).
</bodyText>
<subsectionHeader confidence="0.99337">
3.1 Error Analysis
</subsectionHeader>
<bodyText confidence="0.999142">
As already mentioned in section 2.4, almost all of
the errors that the system makes are of two forms
that are very easy to correct:
</bodyText>
<listItem confidence="0.839371625">
• None of the training sentences start with a se-
quence of periods, but 58 of the evaluation
sentences do. This was solved by increasing
the maximum edit penalty to 9.
• The word “cell” does not occur in the train-
ing sentences, but in does appear in 45 of the
evaluation sentences. To solve this error I just
added that word to the lexicon.
</listItem>
<subsectionHeader confidence="0.999712">
3.2 Evaluation Results
</subsectionHeader>
<bodyText confidence="0.983711444444444">
As already mentioned, the accuracy of the initial
grammar was 86.1% with the spatial planner. The
two minor modifications described in section 2.4
improve the results significantly, as can be seen
in table 1. Increasing the maximum edit penalty
solves 67 of the 126 failing sentences, and adding
the word “cell” solves 41 of the remaining sen-
tences. These two improvements together solve
108 sentences, leaving only 18 failing sentences.
</bodyText>
<page confidence="0.989537">
558
</page>
<table confidence="0.999676">
Max. Unique Correct Total Ambiguous Incorrect Total
penalty Ambiguous Miss Fail
Original grammar 5 782 1 86.1% 0 19 107 13.9%
Original grammar 9 845 5 93.5% 0 50 9 6.5%
Adding “cell” 9 886 5 98.0% 0 10 8 2.0%
</table>
<tableCaption confidence="0.99516">
Table 1: Evaluation results with the spatial planner.
</tableCaption>
<table confidence="0.9998038">
Max. Unique Correct Total Ambiguous Incorrect Total
penalty Ambiguous Miss Fail
Original grammar 5 450 18 51.5% 315 64 62 48.5%
Original grammar 9 493 20 56.4% 330 65 1 43.6%
Adding “cell” 9 498 24 57.4% 366 20 1 42.6%
</table>
<tableCaption confidence="0.999622">
Table 2: Evaluation results without the spatial planner.
</tableCaption>
<bodyText confidence="0.999889258064516">
The final accuracy was therefore boosted to an im-
pressive 98.0%.
The columns in the result tables are as follows:
Unique are the number of sentences for which the
system returns one single tree which is correct.
Ambiguous are the number of sentences where the
parser returns several trees, and the correct tree is
among them: if the tree that the system selects
(i.e., the smallest tree) is correct, it is counted
as a correct ambiguous sentence, otherwise it is
counted as incorrect. Miss are the number of sen-
tences where all the returned trees are incorrect,
and Fail are the sentences for which the system
could not find a tree at all.
Table 2 shows that the modifications also im-
prove the accuracy when the spatial planner is
not used, but the improvement is not as impres-
sive. The reason for this is that many of the failed
sentences become ambiguous, and since the plan-
ner cannot be used for disambiguation, there is
still a risk that the returned tree is not the cor-
rect one. The number of sentences for which the
system returns the correct tree somewhere among
the results is the sum of all unique and ambigu-
ous sentences, which amounts to 450+18+315 =
783 (i.e., 86.1%) for the original grammar and
498+24+366 = 888 (i.e., 97.7%) for the updated
grammar. Note that these are almost the same re-
sults as in table 1, which is consistent with the fact
that the system uses the planner to filter out incor-
rect interpretations.
</bodyText>
<sectionHeader confidence="0.99994" genericHeader="conclusions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999742944444444">
In this paper I have showed that a traditional
symbol-based grammatical approach can be as
good as, or even superior to, a data-based machine
learning approach, in specific domains where the
language and the possible actions are restricted.
The grammar-based system gets an accuracy of
86.1% on the evaluation data. By increasing the
penalty threshold the accuracy rises to 93.5%, and
with a single addition to the lexicon it reaches
98.0%.
This suggests that grammar-based approaches
can be useful when developing interactive systems
for limited domains. In particular it seems that
a grammar-based system could be well suited for
systems that are built using an iterative and incre-
mental development process (Larman and Basili,
2003), where the system is updated frequently and
continuously evaluated by users.
</bodyText>
<sectionHeader confidence="0.999549" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.996187166666667">
Kais Dukes. 2013. Semantic annotation of robotic spa-
tial commands. In Proceedings of LTC’13: 6th Lan-
guage and Technology Conference, Pozna´n, Poland.
Craig Larman and Victor R. Basili. 2003. Iterative
and incremental development: A brief history. Com-
puter, 36(6):47–56.
Vladimir I. Levenshtein. 1966. Binary codes capa-
ble of correcting deletions, insertions, and reversals.
Soviet Physics Doklady, 10(8):707–710.
Fernando C. N. Pereira and David H. D. Warren. 1980.
Definie clause grammars for language analysis. Ar-
tificialIntelligence, 13:231–278.
</reference>
<page confidence="0.998615">
559
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.385741">
<title confidence="0.999515">ShrdLite: Semantic Parsing Using a Handmade Grammar</title>
<author confidence="0.990675">Peter</author>
<affiliation confidence="0.851386">Department of Computer Science and University of Gothenburg and Chalmers University of Gothenburg,</affiliation>
<email confidence="0.954548">peter.ljunglof@cse.gu.se</email>
<abstract confidence="0.986871227272727">This paper describes my approach for parsing robot commands, which was task 6 at SemEval 2014. My solution is to manually create a compact unification grammar. The grammar is highly ambiguous, and relies heavily on filtering the parse results by checking their consistency with the current world. The grammar is small, consisting of not more than 25 grammatical and 60 lexical rules. The parser uses simple error correction together with a straightforward iterative deepening search. Nevertheless, with these very basic algorithms, the system still managed to get 86.1% correctness on the evaluation data. Even more interesting is that by making the parser slightly more robust, the accuracy of the system rises to 93.5%, and by adding one single word to the lexicon, the accuracy is boosted to 98.0%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kais Dukes</author>
</authors>
<title>Semantic annotation of robotic spatial commands.</title>
<date>2013</date>
<booktitle>In Proceedings of LTC’13: 6th Language and Technology Conference,</booktitle>
<location>Pozna´n, Poland.</location>
<contexts>
<context position="1454" citStr="Dukes, 2013" startWordPosition="233" endWordPosition="234">ore interesting is that by making the parser slightly more robust, the accuracy of the system rises to 93.5%, and by adding one single word to the lexicon, the accuracy is boosted to 98.0%. 1 Introduction SemEval 2014, task 6, was about parsing commands to a robot operating in a blocks world. The goal is to parse utterances in natural language into commands in a formal language, the Robot Control Language (RCL). As a guide the system can use a spatial planner which can tell whether an RCL command is meaningful in a given blocks world. The utterances are taken from the Robot Commands Treebank (Dukes, 2013), which pairs 3409 sentences with semantic annotations consisting of an RCL command together with a description of This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ a world where the command is meaningful. The corpus was divided into 2500 training sentences and 909 evaluation sentences. The system that is described in this paper, together with the evaluation data, is available online from GitHub: https://github.com/heatherleaf/semeval</context>
</contexts>
<marker>Dukes, 2013</marker>
<rawString>Kais Dukes. 2013. Semantic annotation of robotic spatial commands. In Proceedings of LTC’13: 6th Language and Technology Conference, Pozna´n, Poland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Larman</author>
<author>Victor R Basili</author>
</authors>
<title>Iterative and incremental development: A brief history.</title>
<date>2003</date>
<journal>Computer,</journal>
<volume>36</volume>
<issue>6</issue>
<marker>Larman, Basili, 2003</marker>
<rawString>Craig Larman and Victor R. Basili. 2003. Iterative and incremental development: A brief history. Computer, 36(6):47–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="5783" citStr="Levenshtein, 1966" startWordPosition="898" endWordPosition="899">move command is of type TYPE-REFERENCE or TYPEREFERENCE-GROUP, add a reference id to the subject; unless the subject is of type PRISM and it has a spatial relation, in which case the reference id is added to its spatial relation instead. 2.3 Robust Parsing The parser is a standard Prolog recursive-descent parser, augmented with simple support for handling robustness. The algorithm is shown in figure 2. 2.3.1 Misspellings and Junk Words The parser tries to compensate for misspellings and junk words. Any word can be recognized as a misspelled word, penalized using the Levenshtein edit distance (Levenshtein, 1966), or it can be skipped as a junk word with a fixed penalty.1 The parser first tries to find an exact match of the sentence in the grammar, then it gradually allows higher penalties until the sentence is parsed. This is done using iterative deepening on the edit penalty of the sentence, until it reaches the maximum edit penalty – if the sentence still cannot be parsed, it fails. In the original evaluation I used a maximum edit penalty of 5, but by just increasing this penalty, the accuracy was boosted considerably as discussed in sections 2.4 and 3.1. 2.3.2 Filtering Through the Spatial Planner</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>David H D Warren</author>
</authors>
<title>Definie clause grammars for language analysis.</title>
<date>1980</date>
<journal>ArtificialIntelligence,</journal>
<pages>13--231</pages>
<contexts>
<context position="2924" citStr="Pereira and Warren, 1980" startWordPosition="458" endWordPosition="461">-grammatical semantic rules. The grammar and the semantic rules were handcrafted using manual analysis of the available 2500 training sentences, and an incremental and iterative process to select and fine-tune the grammar. The total amount of work for creating the grammar consisted of about 3–4 days for one person. This excludes programming the robust parser and the rest of the system, which took another 2–3 days. I did not have any access to the 909 evaluation sentences while developing the grammar or the other parts of the system. 2.1 Grammar The grammar is a Prolog DCG unification grammar (Pereira and Warren, 1980) which builds a semantic parse tree during parsing. The core grammar is shown in figure 1. For presentation purposes, the DCG arguments (including the semantic parse trees) are left out of the figure. The lexicon consists of ca 150 surface words (or multi-word units) divided into 23 lexical categories. The lexical categories are the lowercase italic symbols in the core grammar. 556 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 556–559, Dublin, Ireland, August 23-24, 2014. Main −&gt; Event ?periods Event −&gt; take-verb Entity take-suffix |drop-verb Entity</context>
</contexts>
<marker>Pereira, Warren, 1980</marker>
<rawString>Fernando C. N. Pereira and David H. D. Warren. 1980. Definie clause grammars for language analysis. ArtificialIntelligence, 13:231–278.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>