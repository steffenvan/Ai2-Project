<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000300">
<title confidence="0.989648">
Corpus-based Semantic Lexicon Induction with Web-based Corroboration
</title>
<author confidence="0.996891">
Sean P. Igo
</author>
<affiliation confidence="0.919843">
Center for High Performance Computing
University of Utah
Salt Lake City, UT 84112 USA
</affiliation>
<email confidence="0.993536">
Sean.Igo@utah.edu
</email>
<author confidence="0.996017">
Ellen Riloff
</author>
<affiliation confidence="0.920160333333333">
School of Computing
University of Utah
Salt Lake City, UT 84112 USA
</affiliation>
<email confidence="0.9987">
riloff@cs.utah.edu
</email>
<sectionHeader confidence="0.995613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99882495">
Various techniques have been developed to au-
tomatically induce semantic dictionaries from
text corpora and from the Web. Our research
combines corpus-based semantic lexicon in-
duction with statistics acquired from the Web
to improve the accuracy of automatically ac-
quired domain-specific dictionaries. We use
a weakly supervised bootstrapping algorithm
to induce a semantic lexicon from a text cor-
pus, and then issue Web queries to generate
co-occurrence statistics between each lexicon
entry and semantically related terms. The Web
statistics provide a source of independent ev-
idence to confirm, or disconfirm, that a word
belongs to the intended semantic category. We
evaluate this approach on 7 semantic cate-
gories representing two domains. Our results
show that the Web statistics dramatically im-
prove the ranking of lexicon entries, and can
also be used to filter incorrect entries.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.990316042553191">
Semantic resources are extremely valuable for many
natural language processing (NLP) tasks, as evi-
denced by the wide popularity of WordNet (Miller,
1990) and a multitude of efforts to create similar
“WordNets” for additional languages (e.g. (Atserias
et al., 1997; Vossen, 1998; Stamou et al., 2002)).
Semantic resources can take many forms, but one of
the most basic types is a dictionary that associates
a word (or word sense) with one or more semantic
categories (hypernyms). For example, truck might
be identified as a VEHICLE, and dog might be identi-
fied as an ANIMAL. Automated methods for generat-
18
ing such dictionaries have been developed under the
rubrics of lexical acquisition, hyponym learning, se-
mantic class induction, and Web-based information
extraction. These techniques can be used to rapidly
create semantic lexicons for new domains and lan-
guages, and to automatically increase the coverage
of existing resources.
Techniques for semantic lexicon induction can be
subdivided into two groups: corpus-based methods
and Web-based methods. Although the Web can be
viewed as a (gigantic) corpus, these two approaches
tend to have different goals. Corpus-based methods
are typically designed to induce domain-specific se-
mantic lexicons from a collection of domain-specific
texts. In contrast, Web-based methods are typically
designed to induce broad-coverage resources, simi-
lar to WordNet. Ideally, one would hope that broad-
coverage resources would be sufficient for any do-
main, but this is often not the case. Many domains
use specialized vocabularies and jargon that are not
adequately represented in broad-coverage resources
(e.g., medicine, genomics, etc.). Furthermore, even
relatively general text genres, such as news, con-
tain subdomains that require extensive knowledge
of specific semantic categories. For example, our
work uses a corpus of news articles about terror-
ism that includes many arcane weapon terms (e.g.,
M-79, AR-15, an-fo, and gelignite). Similarly, our
disease-related documents mention obscure diseases
(e.g., psittacosis) and contain many informal terms,
abbreviations, and spelling variants that do not even
occur in most medical dictionaries. For example, yf
refers to yellow fever, tularaemia is an alternative
spelling for tularemia, and nv-cjd is frequently used
</bodyText>
<note confidence="0.8977225">
Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 18–26,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.998346237288136">
to refer to new variant Creutzfeldt Jacob Disease. methods have utilized noun co-occurrence statis-
The Web is such a vast repository of knowledge tics (Riloff and Shepherd, 1997; Roark and Char-
that specialized terminology for nearly any domain niak, 1998), syntactic information (Widdows and
probably exists in some niche or cranny, but find- Dorow, 2002; Phillips and Riloff, 2002; Pantel and
ing the appropriate corner of the Web to tap into is a Ravichandran, 2004; Tanev and Magnini, 2006),
challenge. You have to know where to look to find and lexico-syntactic contextual patterns (e.g., “re-
specialized knowledge. In contrast, corpus-based sides in &lt;location&gt;” or “moved to &lt;location&gt;”)
methods can learn specialized terminology directly (Riloff and Jones, 1999; Thelen and Riloff, 2002).
from a domain-specific corpus, but accuracy can be Due to the need for POS tagging and/or parsing,
a problem because most corpora are relatively small. these types of methods have been evaluated only
In this paper, we seek to exploit the best of both on fixed corpora1, although (Pantel et al., 2004)
worlds by combining a weakly supervised corpus- demonstrated how to scale up their algorithms for
based method for semantic lexicon induction with the Web. The goal of our work is to improve upon
statistics obtained from the Web. First, we use corpus-based bootstrapping algorithms by using co-
a bootstrapping algorithm, Basilisk (Thelen and occurrence statistics obtained from the Web to re-
Riloff, 2002), to automatically induce a semantic rank and filter the hypothesized category members.
lexicon from a domain-specific corpus. This pro- Techniques for semantic class learning have also
duces a set of words that are hypothesized to be- been developed specifically for the Web. Sev-
long to the targeted semantic category. Second, we eral Web-based semantic class learners build upon
use the Web as a source of corroborating evidence Hearst’s early work (Hearst, 1992) with hyponym
to confirm, or disconfirm, whether each term truly patterns. Hearst exploited patterns that explicitly
belongs to the semantic category. For each candi- identify a hyponym relation between a semantic
date word, we search the Web for pages that con- class and a word (e.g., “such authors as &lt;X&gt;”) to
tain both the word and a semantically related term. automatically acquire new hyponyms. (Pas¸ca, 2004)
We expect that true semantic category members will applied hyponym patterns to the Web and learned se-
co-occur with semantically similar words more of- mantic class instances and groups by acquiring con-
ten than non-members. texts around the patterns. Later, (Pasca, 2007) cre-
This paper is organized as follows. Section 2 dis- ated context vectors for a group of seed instances by
cusses prior work on weakly supervised methods for searching Web query logs, and used them to learn
semantic lexicon induction. Section 3 overviews similar instances. The KnowItAll system (Etzioni
our approach: we briefly describe the weakly su- et al., 2005) also uses hyponym patterns to extract
pervised bootstrapping algorithm that we use for class instances from the Web and evaluates them fur-
corpus-based semantic lexicon induction, and then ther by computing mutual information scores based
present our procedure for gathering corroborating on Web queries. (Kozareva et al., 2008) proposed
evidence from the Web. Section 4 presents exper- the use of a doubly-anchored hyponym pattern and
imental results on seven semantic categories repre- a graph to represent the links between hyponym oc-
senting two domains: Latin American terrorism and currences in these patterns.
disease-related documents. Section 5 summarizes Our work builds upon Turney’s work on seman-
our results and discusses future work. tic orientation (Turney, 2002) and synonym learning
2 Related Work (Turney, 2001), in which he used a PMI-IR algo-
Our research focuses on semantic lexicon induc- rithm to measure the similarity of words and phrases
tion, where the goal is to create a list of words based on Web queries. We use a similar PMI (point-
that belong to a desired semantic class. A sub- wise mutual information) metric for the purposes of
stantial amount of previous work has been done on semantic class verification.
weakly supervised and unsupervised creation of se- There has also been work on fully unsupervised
mantic lexicons. Weakly supervised corpus-based
19
1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated
on Web pages, but used a precompiled corpus of downloaded
Web pages.
semantic clustering (e.g., (Lin, 1998; Lin and Pan-
tel, 2002; Davidov and Rappoport, 2006; Davidov et
al., 2007)), however clustering methods may or may
not produce the types and granularities of seman-
tic classes desired by a user. Another related line
of work is automated ontology construction, which
aims to create lexical hierarchies based on semantic
classes (e.g., (Caraballo, 1999; Cimiano and Volker,
2005; Mann, 2002)).
</bodyText>
<sectionHeader confidence="0.8099755" genericHeader="method">
3 Semantic Lexicon Induction with
Web-based Corroboration
</sectionHeader>
<bodyText confidence="0.9999785">
Our approach combines a weakly supervised learn-
ing algorithm for corpus-based semantic lexicon in-
duction with a follow-on procedure that gathers cor-
roborating statistical evidence from the Web. In
this section, we describe both of these components.
First, we give a brief overview of the Basilisk boot-
strapping algorithm that we use for corpus-based se-
mantic lexicon induction. Second, we present our
new strategies for acquiring and utilizing corrobo-
rating statistical evidence from the Web.
</bodyText>
<subsectionHeader confidence="0.999239">
3.1 Corpus-based Semantic Lexicon Induction
via Bootstrapping
</subsectionHeader>
<bodyText confidence="0.999973228571429">
For corpus-based semantic lexicon induction, we
use a weakly supervised bootstrapping algorithm
called Basilisk (Thelen and Riloff, 2002). As in-
put, Basilisk requires a small set of seed words for
each semantic category, and a collection of (unanno-
tated) texts. Basilisk iteratively generates new words
that are hypothesized to belong to the same seman-
tic class as the seeds. Here we give an overview of
Basilisk’s algorithm and refer the reader to (Thelen
and Riloff, 2002) for more details.
The key idea behind Basilisk is to use pattern con-
texts around a word to identify its semantic class.
Basilisk’s bootstrapping process has two main steps:
Pattern Pool Creation and Candidate Word Selec-
tion. First, Basilisk applies the AutoSlog pattern
generator (Riloff, 1996) to create a set of lexico-
syntactic patterns that, collectively, can extract every
noun phrase in the corpus. Basilisk then ranks the
patterns according to how often they extract the seed
words, under the assumption that patterns which ex-
tract known category members are likely to extract
other category members as well. The highest-ranked
patterns are placed in a pattern pool.
Second, Basilisk gathers every noun phrase that is
extracted by at least one pattern in the pattern pool,
and designates each head noun as a candidate for the
semantic category. The candidates are then scored
and ranked. For each candidate, Basilisk collects all
of the patterns that extracted that word, computes the
logarithm of the number of seeds extracted by each
of those patterns, and finally computes the average
of these log values as the score for the candidate.
Intuitively, a candidate word receives a high score
if it was extracted by patterns that, on average, also
extract many known category members.
The N highest ranked candidates are automati-
cally added to the list of seed words, taking a leap
of faith that they are true members of the semantic
category. The bootstrapping process then repeats,
using the larger set of seed words as known category
members in the next iteration.
Basilisk learns many good category members,
but its accuracy varies a lot across semantic cate-
gories (Thelen and Riloff, 2002). One problem with
Basilisk, and bootstrapping algorithms in general, is
that accuracy tends to deteriorate as bootstrapping
progresses. Basilisk generates candidates by iden-
tifying the contexts in which they occur and words
unrelated to the desired category can sometimes also
occur in those contexts. Some patterns consistently
extract members of several semantic classes; for ex-
ample, “attack on &lt;NP&gt;” will extract both people
(“attack on the president”) and buildings (“attack
on the U.S. embassy”). Idiomatic expressions and
parsing errors can also lead to undesirable words be-
ing learned. Incorrect words tend to accumulate as
bootstrapping progresses, which can lead to gradu-
ally deteriorating performance.
(Thelen and Riloff, 2002) tried to address this
problem by learning multiple semantic categories si-
multaneously. This helps to keep the bootstrapping
focused by flagging words that are potentially prob-
lematic because they are strongly associated with a
competing category. This improved Basilisk’s accu-
racy, but by a relatively small amount, and this ap-
proach depends on the often unrealistic assumption
that a word cannot belong to more than one seman-
tic category. In our work, we use the single-category
version of Basilisk that learns each semantic cate-
gory independently so that we do not need to make
</bodyText>
<page confidence="0.766697">
20
</page>
<bodyText confidence="0.9112215">
this assumption. on dairy farms” or `An animal such as a cow
has...”.
</bodyText>
<subsectionHeader confidence="0.999609">
3.2 Web-based Semantic Class Corroboration
</subsectionHeader>
<bodyText confidence="0.999769255813953">
The novel aspect of our work is that we introduce a
new mechanism to independently verify each candi-
date word’s category membership using the Web as
an external knowledge source. We gather statistics
from the Web to provide evidence for (or against)
the semantic class of a word in a manner completely
independent of Basilisk’s criteria. Our approach
is based on the distributional hypothesis (Harris,
1954), which says that words that occur in the same
contexts tend to have similar meanings. We seek to
corroborate a word’s semantic class through statis-
tics that measure how often the word co-occurs with
semantically related words.
For each candidate word produced by Basilisk, we
construct a Web query that pairs the word with a se-
mantically related word. Our goal is not just to find
Web pages that contain both terms, but to find Web
pages that contain both terms in close proximity to
one another. We consider two terms to be collo-
cated if they occur within ten words of each other
on the same Web page, which corresponds to the
functionality of the NEAR operator used by the Al-
taVista search engine2. Turney (Turney, 2001; Tur-
ney, 2002) reported that the NEAR operator outper-
formed simple page co-occurrence for his purposes;
our early experiments informally showed the same
for this work.
We want our technique to remain weakly super-
vised, so we do not want to require additional hu-
man input or effort beyond what is already required
for Basilisk. With this in mind, we investigated two
types of collocation relations as possible indicators
of semantic class membership:
Hypernym Collocation: We compute co-
occurrence statistics between the candidate word
and the name of the targeted semantic class (i.e.,
the word’s hypothesized hypernym). For example,
given the candidate word jeep and the semantic
category VEHICLE, we would issue the Web query
“jeep NEAR vehicle”. Our intuition is that such
queries would identify definition-type Web hits.
For example, the query “cow NEAR animal” might
retrieve snippets such as `A cow is an animal found
</bodyText>
<footnote confidence="0.879149">
2http://www.altavista.com
</footnote>
<bodyText confidence="0.982675545454545">
Seed Collocation: We compute co-occurrence
statistics between the candidate word and each seed
word that was given to Basilisk as input. For ex-
ample, given the candidate word jeep and the seed
word truck, we would issue the Web query “jeep
NEAR truck”. Here the intuition is that members of
the same semantic category tend to occur near one
another - in lists, for example.
As a statistical measure of co-occurrence, we
compute a variation of Pointwise Mutual Informa-
tion (PMI), which is defined as:
</bodyText>
<equation confidence="0.9967655">
PMI(x, y) = log( p(x,y)
p(x)∗p(y))
</equation>
<bodyText confidence="0.8969165">
where p(x, y) is the probability that x and y are col-
located (near each other) on a Web page, p(x) is the
probability that x occurs on a Web page, and p(y) is
the probability that y occurs on a Web page.
p(x) is calculated as count(x)
N , where count(x) is
the number of hits returned by AltaVista, searching
for x by itself, and N is the total number of docu-
ments on the World Wide Web at the time the query
is made. Similarly, p(x, y) is count(x NEAR y) .
N
Given this, the PMI equation can be rewritten as:
</bodyText>
<equation confidence="0.9918275">
log(N) + log(count(x NEAR y) )
count(x)∗count(y)
</equation>
<bodyText confidence="0.998772">
N is not known, but it is the same for every
query (assuming the queries were made at roughly
the same time). We will use these scores solely to
compare the relative goodness of candidates, so we
can omit N from the equation because it will not
change the relative ordering of the scores. Thus, our
PMI score3 for a candidate word and related term
(hypernym or seed) is:
</bodyText>
<equation confidence="0.954175">
log(count(x NEAR y) )
count (x) ∗count(y)
</equation>
<bodyText confidence="0.9284746">
Finally, we created three different scoring func-
tions that use PMI values in different ways to cap-
ture different types of co-occurrence information:
Hypernym Score: PMI based on collocation be-
tween the hypernym term and candidate word.
</bodyText>
<footnote confidence="0.996516333333333">
3In the rare cases when a term had a zero hit count, we as-
signed -99999 as the PMI score, which effectively ranks it at the
bottom.
</footnote>
<page confidence="0.999052">
21
</page>
<bodyText confidence="0.966494358974359">
Average of Seeds Score: The mean of the PMI
scores computed for the candidate and each
seed word:
PMI(candidate, seedi)
Max of Seeds Score: The maximum (highest) of
the PMI scores computed for the candidate and
each seed word.
The rationale for the Average of Seeds Score is
that the seeds are all members of the semantic cat-
egory, so we might expect other members to occur
near many of them. Averaging over all of the seeds
can diffuse unusually high or low collocation counts
that might result from an anomalous seed. The ra-
tionale for the Max of Seeds Score is that a word
may naturally co-occur with some category mem-
bers more often than others. For example, one would
expect dog to co-occur with cat much more fre-
quently than with frog. A high Max of Seeds Score
indicates that there is at least one seed word that fre-
quently co-occurs with the candidate.
Since Web queries are relatively expensive, it is
worth taking stock of how many queries are nec-
essary. Let N be the number of candidate words
produced by Basilisk, and 5 be the number of
seed words given to Basilisk as input. To com-
pute the Hypernym Score for a candidate, we need
3 queries: count(hypernym), count(candidate),
and count(hypernym NEAR candidate). The
first query is the same for all candidates, so for N
candidate words we need 2N + 1 queries in total.
To compute the Average or Max of Seeds Score for
a candidate, we need 5 queries for count(seedi), 5
queries for count(seedi NEAR candidate), and 1
query for count(candidate). So for N candidate
words we need N ∗ (25 + 1) queries. 5 is typically
small for weakly supervised algorithms (5=10 in our
experiments), which means that this Web-based cor-
roboration process requires O(N) queries to process
a semantic lexicon of size N.
</bodyText>
<sectionHeader confidence="0.999303" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.962451">
4.1 Data Sets
</subsectionHeader>
<bodyText confidence="0.919414472222222">
We ran experiments on two corpora: 1700 MUC-4
terrorism articles (MUC-4 Proceedings, 1992) and
a combination of 6000 disease-related documents,
consisting of 2000 ProMed disease outbreak re-
ports (ProMed-mail, 2006) and 4000 disease-related
PubMed abstracts (PubMed, 2009). For the terror-
ism domain, we created lexicons for four semantic
categories: BUILDING, HUMAN, LOCATION, and
WEAPON. For the disease domain, we created lexi-
cons for three semantic categories: ANIMAL4, DIS-
EASE, and SYMPTOM. For each category, we gave
Basilisk 10 seed words as input. The seeds were
chosen by applying a shallow parser to each corpus,
extracting the head nouns of all the NPs, and sort-
ing the nouns by frequency. A human then walked
down the sorted list and identified the 10 most fre-
quent nouns that belong to each semantic category5.
This strategy ensures that the bootstrapping process
is given seed words that occur in the corpus with
high frequency. The seed words are shown in Ta-
ble 1.
BUILDING: embassy office headquarters church
offices house home residence hospital airport
HUMAN: people guerrillas members troops
Cristiani rebels president terrorists soldiers leaders
LOCATION: country El Salvador Salvador
United States area Colombia city countries
department Nicaragua
WEAPON: weapons bomb bombs explosives arms
missiles dynamite rifles materiel bullets
ANIMAL: bird mosquito cow horse pig chicken
sheep dog deer fish
DISEASE: SARS BSE anthrax influenza WNV
FMD encephalitis malaria pneumonia flu
SYMPTOM: fever diarrhea vomiting rash paralysis
weakness necrosis chills headaches hemorrhage
</bodyText>
<tableCaption confidence="0.993603">
Table 1: Seed Words
</tableCaption>
<bodyText confidence="0.999917625">
To evaluate our results, we used the gold standard
answer key that Thelen &amp; Riloff created to evaluate
Basilisk on the MUC4 corpus (Thelen and Riloff,
2002); they manually labeled every head noun in the
corpus with its semantic class. For the ProMed /
PubMed disease corpus, we created our own answer
key. For all of the lexicon entries hypothesized by
Basilisk, a human annotator (not any of the authors)
</bodyText>
<footnote confidence="0.978821333333333">
4ANIMAL was chosen because many of the ProMed disease
outbreak stories concerned outbreaks among animal popula-
tions.
5The disease domain seed words were chosen from a larger
set of ProMed documents, which included the 2000 used for
lexicon induction.
</footnote>
<table confidence="0.886345052631579">
1 Iseedsl
�
i=1
Iseedsl
22
N Ba Hy BUILDING Mx Ba Hy HUMAN Mx Ba Hy LOCATION Mx Ba Hy WEAPON Mx
Av Av Av Av
25 .40 .56 .52 .56 .40 .72 .80 .84 .68 .88 .88 1.0 .56 .84 1.0 1.0
50 .44 .56 .46 .40 .56 .80 .88 .86 .80 .86 .84 .98 .52 .74 .76 .90
75 .44 .45 .41 .39 .65 .84 .85 .85 .80 .88 .80 .99 .52 .63 .65 .79
100 .42 .41 .38 .36 .69 .81 .80 .87 .81 .85 .78 .95 .55 .55 .56 .63
300 .22 .82 .75 .26
ANIMAL DISEASE SYMPTOM
N Ba Hy Av Mx Ba Hy Av Mx Ba Hy Av Mx
25 .48 .88 .92 .92 .64 .84 .80 .84 .64 .84 .92 .80
50 .58 .82 .84 .80 .72 .84 .60 .82 .62 .76 .90 .74
75 .55 .68 .67 .69 .69 .83 .59 .81 .61 .68 .79 .71
100 .45 .55 .54 .57 .69 .78 .58 .80 .59 .71 .77 .64
300 .20 .62 .38
</table>
<tableCaption confidence="0.9559145">
Table 2: Ranking results for 7 semantic categories, showing accuracies for the top-ranked N words.
(Ba=Basilisk, Hy=Hypernym Re-ranking, Av=Average of Seeds Re-ranking, Mx=Max of Seeds Re-ranking
</tableCaption>
<bodyText confidence="0.99676225">
labeled each word as either correct or incorrect for
the hypothesized semantic class. A word is consid-
ered to be correct if any sense of the word is seman-
tically correct.
</bodyText>
<subsectionHeader confidence="0.998243">
4.2 Ranking Results
</subsectionHeader>
<bodyText confidence="0.999368380952381">
We ran Basilisk for 60 iterations, learning 5 new
words in each bootstrapping cycle, which produced
a lexicon of 300 words for each semantic category.
The columns labeled Ba in Table 2 show the accu-
racy results for Basilisk.6 As we explained in Sec-
tion 3.1, accuracy tends to decrease as bootstrapping
progresses, so we computed accuracy scores for the
top-ranked 100 words, in increments of 25, and also
for the entire lexicon of 300 words.
Overall, we see that Basilisk learns many cor-
rect words for each semantic category, and the top-
ranked terms are generally more accurate than the
lower-ranked terms. For the top 100 words, accu-
racies are generally in the 50-70% range, except for
LOCATION which achieves about 80% accuracy. For
the HUMAN category, Basilisk obtained 82% accu-
racy over all 300 words, but the top-ranked words
actually produced lower accuracy.
Basilisk’s ranking is clearly not as good as it could
be because there are correct terms co-mingled with
incorrect terms throughout the ranked lists. This has
</bodyText>
<footnote confidence="0.986797">
6These results are not comparable to the Basilisk results re-
ported by (Thelen and Riloff, 2002) because our implementa-
tion only does single-category learning while the results in that
paper are based on simultaneously learning multiple categories.
</footnote>
<bodyText confidence="0.999714066666667">
two ramifications. First, if we want a human to man-
ually review each lexicon before adding the words
to an external resource, then the rankings may not
be very helpful (i.e., the human will need to review
all of the words), and (2) incorrect terms generated
during the early stages of bootstrapping may be hin-
dering the learning process because they introduce
noise during bootstrapping. The HUMAN category
seems to have recovered from early mistakes, but
the lower accuracies for some other categories may
be the result of this problem. The purpose of our
Web-based corroboration process is to automatically
re-evaluate the lexicons produced by Basilisk, using
Web-based statistics to create more separation be-
tween the good entries and the bad ones.
Our first set of experiments uses the Web-based
co-occurrence statistics to re-rank the lexicon en-
tries. The Hy, Av, and Mx columns in Ta-
ble 2 show the re-ranking results using each of the
Hypernym, Average of Seeds, and Maximum of
Seeds scoring functions. In all cases, Web-based
re-ranking outperforms Basilisk’s original rank-
ings. Every semantic category except for BUILDING
yielded accuracies of 80-100% among the top can-
didates. For each row, the highest accuracy for each
semantic category is shown in boldface (as are any
tied for highest).
Overall, the Max of Seeds Scores were best, per-
forming better than or as well as the other scoring
functions on 5 of the 7 categories. It was only out-
</bodyText>
<page confidence="0.993712">
23
</page>
<table confidence="0.998556090909091">
BUILDING HUMAN LOCATION WEAPON ANIMAL DISEASE SYMPTOM
consulate guerrilla San Salvador shotguns bird-to-bird meningo-encephalitis nausea
pharmacies extremists Las Hojas carbines cervids bse).austria diarrhoea
aiport sympathizers Tejutepeque armaments goats inhalational myalgias
zacamil assassins Ayutuxtepeque revolvers ewes anthrax disease chlorosis
airports patrols Copinol detonators ruminants otitis media myalgia
parishes militiamen Cuscatancingo pistols swine airport malaria salivation
Masariegos battalion Jiboa car bombs calf taeniorhynchus dysentery
chancery Ellacuria Chinameca calibers lambs hyopneumonia cramping
residences rebel Zacamil M-16 wolsington monkeypox dizziness
police station policemen Chalantenango grenades piglets kala-azar inappetance
</table>
<tableCaption confidence="0.999955">
Table 3: Top 10 words ranked by Max of Seeds Scores.
</tableCaption>
<bodyText confidence="0.999953965517241">
performed once by the Hypernym Scores (BUILD-
ING) and once by the Average of Seeds Scores
(SYMPTOM).
The strong performance of the Max of Seeds
scores suggests that one seed is often an especially
good collocation indicator for category membership
– though it may not be the same seed word for all of
the lexicon words. The relatively poor performance
of the Average of Seeds scores may be attributable
to the same principle; perhaps even if one seed is
especially strong, averaging over the less-effective
seeds’ scores dilutes the results. Averaging is also
susceptible to damage from words that receive the
special-case score of -99999 when a hit count is zero
(see Section 3.2).
Table 3 shows the 10 top-ranked candidates for
each semantic category based on the Max of Seeds
scores. The table illustrates that this scoring func-
tion does a good job of identifying semantically cor-
rect words, although of course there are some mis-
takes. Mistakes can happen due to parsing errors
(e.g., bird-to-bird is an adjective and not a noun, as
in bird-to-bird transmission), and some are due to
issues associated with Web querying. For exam-
ple, the nonsense term “bse).austria” was ranked
highly because Altavista split this term into 2 sep-
arate words because of the punctuation, and bse by
itself is indeed a disease term (bovine spongiform
encephalitis).
</bodyText>
<subsectionHeader confidence="0.999862">
4.3 Filtering Results
</subsectionHeader>
<bodyText confidence="0.997384444444444">
Table 2 revealed that the 300-word lexicons pro-
duced by Basilisk vary widely in the number of true
category words that they contain. The least dense
category is ANIMAL, with only 61 correct words,
and the most dense is HUMAN with 247 correct
words. Interestingly, the densest categories are not
always the easiest to rank. For example, the HU-
MAN category is the densest category but Basilisk’s
ranking of the human terms was poor.
</bodyText>
<table confidence="0.999723772727273">
θ Category Acc Cor/Tot
WEAPON .88 46/52
LOCATION .98 59/60
HUMAN .80 8/10
-22 BUILDING .83 5/6
ANIMAL .91 30/33
DISEASE .82 64/78
SYMPTOM .65 64/99
WEAPON .79 59/75
LOCATION .96 82/85
HUMAN .85 23/27
-23 BUILDING .71 12/17
ANIMAL .87 40/46
DISEASE .78 82/105
SYMPTOM .62 86/139
WEAPON .63 63/100
LOCATION .93 111/120
HUMAN .87 54/62
-24 BUILDING .45 17/38
ANIMAL .75 47/63
DISEASE .74 94/127
SYMPTOM .60 100/166
</table>
<tableCaption confidence="0.999721">
Table 4: Filtering results using the Max of Seeds Scores.
</tableCaption>
<bodyText confidence="0.999846">
The ultimate goal behind a better ranking mech-
anism is to completely automate the process of se-
mantic lexicon induction. If we can produce high-
quality rankings, then we can discard the lower
ranked words and keep only the highest ranked
words for our semantic dictionary. However, this
</bodyText>
<page confidence="0.954498">
24
</page>
<bodyText confidence="0.999757808510638">
presupposes that we know where to draw the line be- prove the ranking of lexicon entries produced by
tween the good and bad entries, and Table 2 shows a weakly-supervised corpus-based bootstrapping al-
that this boundary varies across categories. For HU- gorithm, without requiring any additional supervi-
MANS, the top 100 words are 87% accurate, and in sion. We found that computing Web-based co-
fact we get 82% accuracy over all 300 words. But occurrence statistics across a set of seed words and
for ANIMALS we achieve 80% accuracy only for the then using the highest score was the most success-
top 50 words. It is paramount for semantic dictio- ful approach. Co-occurrence with a hypernym term
naries to have high integrity, so accuracy must be also performed well for some categories, and could
high if we want to use the resulting lexicons without be easily combined with the Max of Seeds approach
manual review. by choosing the highest value among the seeds as
As an alternative to ranking, another way that we well as the hypernym.
could use the Web-based corroboration statistics is In future work, we would like to incorporate this
to automatically filter words that do not receive a Web-based re-ranking procedure into the bootstrap-
high score. The key question is whether the values ping algorithm itself to dynamically “clean up” the
of the scores are consistent enough across categories learned words before they are cycled back into the
to set a single threshold that will work well across bootstrapping process. Basilisk could consult the
the different categories. Web-based statistics to select the best 5 words to
Table 4 shows the results of using the Max of generate before the next bootstrapping cycle begins.
Seeds Scores as a filtering mechanism: given a This integrated approach has the potential to sub-
threshold 0, all words that have a score &lt; 0 are dis- stantially improve Basilisk’s performance because
carded. For each threshold value 0 and semantic cat- it would improve the precision of the induced lex-
egory, we computed the accuracy (Acc) of the lex- icon entries during the earliest stages of bootstrap-
icon after all words with a score &lt; 0 have been re- ping when the learning process is most fragile.
moved. The Cor/Tot column shows the number of Acknowledgments
correct category members and the number of total Many thanks to Julia James for annotating the gold
words that passed the threshold. standards for the disease domain. This research was
We experimented with a variety of threshold val- supported in part by Department of Homeland Secu-
ues and found that 0=-22 performed best. Table 4 rity Grant N0014-07-1-0152.
shows that this threshold produces a relatively high- References
precision filtering mechanism, with 6 of the 7 cat- J. Atserias, S. Climent, X. Farreres, G. Rigau, and H. Ro-
egories achieving lexicon accuracies ≥ 80%. As driguez. 1997. Combining Multiple Methods for the
expected, the Cor/Tot column shows that the num- Automatic Construction of Multilingual WordNets. In
ber of words varies widely across categories. Au- Proceedings ofthe International Conference on Recent
tomatic filtering represents a trade-off: a relatively Advances in Natural Language Processing.
high-precision lexicon can be created, but some cor- S. Caraballo. 1999. Automatic Acquisition of a
rect words will be lost. The threshold can be ad- Hypernym-Labeled Noun Hierarchy from Text. In
justed to increase the number of learned words, but Proc. of the 37th Annual Meeting of the Association
with a corresponding drop in precision. Depending for Computational Linguistics, pages 120–126.
upon a user’s needs, a high threshold may be desir- P. Cimiano and J. Volker. 2005. Towards large-scale,
able to identify only the most confident lexicon en- open-domain and ontology-based named entity classi-
tries, or a lower threshold may be desirable to retain fication. In Proc. of Recent Advances in Natural Lan-
most of the correct entries while reliably removing guage Processing, pages 166–172.
some of the incorrect ones. D. Davidov and A. Rappoport. 2006. Efficient unsu-
5 Conclusions pervised discovery of word categories using symmet-
We have demonstrated that co-occurrence statis- ric patterns and high frequency words. In Proc. of the
tics gathered from the Web can dramatically im- 21st International Conference on Computational Lin-
25 guistics and the 44th annual meeting of the ACL.
</bodyText>
<reference confidence="0.998053831775701">
D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully
unsupervised discovery of concept-specific relation-
ships by web mining. In Proc. of the 45th Annual
Meeting of the Association of Computational Linguis-
tics, pages 232–239, June.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu,
T. Shaked, S. Soderland, D. Weld, and A. Yates.
2005. Unsupervised named-entity extraction from the
web: an experimental study. Artificial Intelligence,
165(1):91–134, June.
Z. Harris. 1954. Distributional Structure. In J. A. Fodor
and J. J. Katz, editor, The Structure ofLanguage, pages
33–49. Prentice-Hall.
M. Hearst. 1992. Automatic acquisition of hyponyms
from large text corpora. In Proc. of the 14th In-
ternational Conference on Computational Linguistics
(COLING-92).
Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic
Class Learning from the Web with Hyponym Pattern
Linkage Graphs. In Proceedings of the 46th Annual
Meeting ofthe Association for Computational Linguis-
tics: Human Language Technologies (ACL-08).
D. Lin and P. Pantel. 2002. Concept discovery from text.
In Proc. of the 19th International Conference on Com-
putational linguistics, pages 1–7.
D. Lin. 1998. Dependency-based Evaluation of MINI-
PAR. In Workshop on the Evaluation of Parsing Sys-
tems, Granada, Spain.
G. Mann. 2002. Fine-grained proper noun ontologies for
question answering. In Proc. of the 19th International
Conference on Computational Linguistics, pages 1–7.
G. Miller. 1990. Wordnet: An On-line Lexical Database.
International Journal ofLexicography, 3(4).
MUC-4 Proceedings. 1992. Proceedings of the Fourth
Message Understanding Conference (MUC-4). Mor-
gan Kaufmann.
M. Pas¸ca. 2004. Acquisition of categorized named en-
tities for web search. In Proc. of the Thirteenth ACM
International Conference on Information and Knowl-
edge Management, pages 137–145.
P. Pantel and D. Ravichandran. 2004. Automatically
labeling semantic classes. In Proc. of Conference of
HLT /North American Chapter of the Association for
Computational Linguistics, pages 321–328.
P. Pantel, D. Ravichandran, and E. Hovy. 2004. To-
wards terascale knowledge acquisition. In Proc. of the
20th international conference on Computational Lin-
guistics, page 771.
M. Pasca. 2007. weakly-supervised Discovery of Named
Entities using Web Search Queries. In CIKM, pages
683–690.
W. Phillips and E. Riloff. 2002. Exploiting Strong Syn-
tactic Heuristics and Co-Training to Learn Semantic
Lexicons. In Proceedings of the 2002 Conference on
Empirical Methods in Natural Language Processing,
pages 125–132.
ProMed-mail. 2006. http://www.promedmail.org/.
PubMed. 2009. http://www.ncbi.nlm.nih.gov/sites/entrez.
E. Riloff and R. Jones. 1999. Learning Dictionaries for
Information Extraction by Multi-Level Bootstrapping.
In Proceedings of the Sixteenth National Conference
on Artificial Intelligence.
E. Riloff and J. Shepherd. 1997. A Corpus-Based Ap-
proach for Building Semantic Lexicons. In Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 117–124.
E. Riloff. 1996. Automatically Generating Extraction
Patterns from Untagged Text. In Proceedings of the
Thirteenth National Conference on Artificial Intelli-
gence, pages 1044–1049. The AAAI Press/MIT Press.
B. Roark and E. Charniak. 1998. Noun-phrase Co-
occurrence Statistics for Semi-automatic Semantic
Lexicon Construction. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics, pages 1110–1116.
Sofia Stamou, Kemal Oflazer, Karel Pala, Dimitris Chris-
toudoulakis, Dan Cristea, Dan Tufis, Svetla Koeva,
George Totkov, Dominique Dutoit, and Maria Grigo-
riadou. 2002. Balkanet: A multilingual semantic net-
work for the balkan languages. In Proceedings of the
1st Global WordNet Association conference.
H. Tanev and B. Magnini. 2006. Weakly supervised ap-
proaches for ontology population. In Proc. of 11st
Conference of the European Chapter of the Associa-
tion for Computational Linguistics.
M. Thelen and E. Riloff. 2002. A Bootstrapping Method
for Learning Semantic Lexicons Using Extraction Pa
ttern Contexts. In Proceedings of the 2002 Conference
on Empirical Methods in Natural Language Process-
ing, pages 214–221.
Peter D. Turney. 2001. Mining the Web for Syn-
onyms: PMI-IR versus LSA on TOEFL. In EMCL
’01: Proceedings of the 12th European Conference
on Machine Learning, pages 491–502, London, UK.
Springer-Verlag.
P. D. Turney. 2002. Thumbs up or thumbs down? se-
mantic orientation applied to unsupervised classifica-
tion of reviews. In Proceedings of the 40th Annual
Meeting ofthe Association for Computational Linguis-
tics (ACL’02), pages 417–424.
Piek Vossen, editor. 1998. EuroWordNet: A Multilingual
Database with Lexical Semantic Networks. Kluwer
Academic Publishers, Norwell, MA, USA.
D. Widdows and B. Dorow. 2002. A graph model for
unsupervised lexical acquisition. In Proc. of the 19th
International Conference on Computational Linguis-
tics, pages 1–7.
</reference>
<page confidence="0.998056">
26
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.828264">
<title confidence="0.999436">Corpus-based Semantic Lexicon Induction with Web-based Corroboration</title>
<author confidence="0.999788">P Sean</author>
<affiliation confidence="0.9877875">Center for High Performance University of</affiliation>
<address confidence="0.964962">Salt Lake City, UT 84112</address>
<email confidence="0.933361">Sean.Igo@utah.edu</email>
<author confidence="0.979515">Ellen</author>
<affiliation confidence="0.998814">School of University of</affiliation>
<address confidence="0.97032">Salt Lake City, UT 84112</address>
<email confidence="0.999793">riloff@cs.utah.edu</email>
<abstract confidence="0.999478714285714">Various techniques have been developed to automatically induce semantic dictionaries from text corpora and from the Web. Our research combines corpus-based semantic lexicon induction with statistics acquired from the Web to improve the accuracy of automatically acquired domain-specific dictionaries. We use a weakly supervised bootstrapping algorithm to induce a semantic lexicon from a text corpus, and then issue Web queries to generate co-occurrence statistics between each lexicon entry and semantically related terms. The Web statistics provide a source of independent evidence to confirm, or disconfirm, that a word belongs to the intended semantic category. We evaluate this approach on 7 semantic categories representing two domains. Our results show that the Web statistics dramatically improve the ranking of lexicon entries, and can also be used to filter incorrect entries.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Davidov</author>
<author>A Rappoport</author>
<author>M Koppel</author>
</authors>
<title>Fully unsupervised discovery of concept-specific relationships by web mining.</title>
<date>2007</date>
<booktitle>In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>232--239</pages>
<contexts>
<context position="8347" citStr="Davidov et al., 2007" startWordPosition="1274" endWordPosition="1277">ords based on Web queries. We use a similar PMI (pointthat belong to a desired semantic class. A sub- wise mutual information) metric for the purposes of stantial amount of previous work has been done on semantic class verification. weakly supervised and unsupervised creation of se- There has also been work on fully unsupervised mantic lexicons. Weakly supervised corpus-based 19 1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)). 3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical evidence from the Web. In this section, we describe bo</context>
</contexts>
<marker>Davidov, Rappoport, Koppel, 2007</marker>
<rawString>D. Davidov, A. Rappoport, and M. Koppel. 2007. Fully unsupervised discovery of concept-specific relationships by web mining. In Proc. of the 45th Annual Meeting of the Association of Computational Linguistics, pages 232–239, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. Weld, and A. Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artificial Intelligence, 165(1):91–134, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Harris</author>
</authors>
<title>Distributional Structure. In</title>
<date>1954</date>
<booktitle>The Structure ofLanguage,</booktitle>
<pages>33--49</pages>
<editor>J. A. Fodor and J. J. Katz, editor,</editor>
<publisher>Prentice-Hall.</publisher>
<contexts>
<context position="13294" citStr="Harris, 1954" startWordPosition="2054" endWordPosition="2055"> Basilisk that learns each semantic category independently so that we do not need to make 20 this assumption. on dairy farms” or `An animal such as a cow has...”. 3.2 Web-based Semantic Class Corroboration The novel aspect of our work is that we introduce a new mechanism to independently verify each candidate word’s category membership using the Web as an external knowledge source. We gather statistics from the Web to provide evidence for (or against) the semantic class of a word in a manner completely independent of Basilisk’s criteria. Our approach is based on the distributional hypothesis (Harris, 1954), which says that words that occur in the same contexts tend to have similar meanings. We seek to corroborate a word’s semantic class through statistics that measure how often the word co-occurs with semantically related words. For each candidate word produced by Basilisk, we construct a Web query that pairs the word with a semantically related word. Our goal is not just to find Web pages that contain both terms, but to find Web pages that contain both terms in close proximity to one another. We consider two terms to be collocated if they occur within ten words of each other on the same Web pa</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Z. Harris. 1954. Distributional Structure. In J. A. Fodor and J. J. Katz, editor, The Structure ofLanguage, pages 33–49. Prentice-Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Hearst</author>
</authors>
<title>Automatic acquisition of hyponyms from large text corpora.</title>
<date>1992</date>
<booktitle>In Proc. of the 14th International Conference on Computational Linguistics (COLING-92).</booktitle>
<contexts>
<context position="5667" citStr="Hearst, 1992" startWordPosition="854" endWordPosition="855">d bootstrapping algorithms by using coa bootstrapping algorithm, Basilisk (Thelen and occurrence statistics obtained from the Web to reRiloff, 2002), to automatically induce a semantic rank and filter the hypothesized category members. lexicon from a domain-specific corpus. This pro- Techniques for semantic class learning have also duces a set of words that are hypothesized to be- been developed specifically for the Web. Sevlong to the targeted semantic category. Second, we eral Web-based semantic class learners build upon use the Web as a source of corroborating evidence Hearst’s early work (Hearst, 1992) with hyponym to confirm, or disconfirm, whether each term truly patterns. Hearst exploited patterns that explicitly belongs to the semantic category. For each candi- identify a hyponym relation between a semantic date word, we search the Web for pages that con- class and a word (e.g., “such authors as &lt;X&gt;”) to tain both the word and a semantically related term. automatically acquire new hyponyms. (Pas¸ca, 2004) We expect that true semantic category members will applied hyponym patterns to the Web and learned seco-occur with semantically similar words more of- mantic class instances and groups</context>
</contexts>
<marker>Hearst, 1992</marker>
<rawString>M. Hearst. 1992. Automatic acquisition of hyponyms from large text corpora. In Proc. of the 14th International Conference on Computational Linguistics (COLING-92).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Kozareva</author>
<author>E Riloff</author>
<author>E Hovy</author>
</authors>
<title>Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics: Human Language Technologies (ACL-08).</booktitle>
<contexts>
<context position="7049" citStr="Kozareva et al., 2008" startWordPosition="1068" endWordPosition="1071">s for a group of seed instances by cusses prior work on weakly supervised methods for searching Web query logs, and used them to learn semantic lexicon induction. Section 3 overviews similar instances. The KnowItAll system (Etzioni our approach: we briefly describe the weakly su- et al., 2005) also uses hyponym patterns to extract pervised bootstrapping algorithm that we use for class instances from the Web and evaluates them furcorpus-based semantic lexicon induction, and then ther by computing mutual information scores based present our procedure for gathering corroborating on Web queries. (Kozareva et al., 2008) proposed evidence from the Web. Section 4 presents exper- the use of a doubly-anchored hyponym pattern and imental results on seven semantic categories repre- a graph to represent the links between hyponym ocsenting two domains: Latin American terrorism and currences in these patterns. disease-related documents. Section 5 summarizes Our work builds upon Turney’s work on semanour results and discusses future work. tic orientation (Turney, 2002) and synonym learning 2 Related Work (Turney, 2001), in which he used a PMI-IR algoOur research focuses on semantic lexicon induc- rithm to measure the </context>
</contexts>
<marker>Kozareva, Riloff, Hovy, 2008</marker>
<rawString>Z. Kozareva, E. Riloff, and E. Hovy. 2008. Semantic Class Learning from the Web with Hyponym Pattern Linkage Graphs. In Proceedings of the 46th Annual Meeting ofthe Association for Computational Linguistics: Human Language Technologies (ACL-08).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
<author>P Pantel</author>
</authors>
<title>Concept discovery from text.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="8295" citStr="Lin and Pantel, 2002" startWordPosition="1265" endWordPosition="1269">rases tion, where the goal is to create a list of words based on Web queries. We use a similar PMI (pointthat belong to a desired semantic class. A sub- wise mutual information) metric for the purposes of stantial amount of previous work has been done on semantic class verification. weakly supervised and unsupervised creation of se- There has also been work on fully unsupervised mantic lexicons. Weakly supervised corpus-based 19 1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)). 3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical ev</context>
</contexts>
<marker>Lin, Pantel, 2002</marker>
<rawString>D. Lin and P. Pantel. 2002. Concept discovery from text. In Proc. of the 19th International Conference on Computational linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-based Evaluation of MINIPAR.</title>
<date>1998</date>
<booktitle>In Workshop on the Evaluation of Parsing Systems,</booktitle>
<location>Granada,</location>
<contexts>
<context position="8273" citStr="Lin, 1998" startWordPosition="1263" endWordPosition="1264">ords and phrases tion, where the goal is to create a list of words based on Web queries. We use a similar PMI (pointthat belong to a desired semantic class. A sub- wise mutual information) metric for the purposes of stantial amount of previous work has been done on semantic class verification. weakly supervised and unsupervised creation of se- There has also been work on fully unsupervised mantic lexicons. Weakly supervised corpus-based 19 1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)). 3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corrob</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>D. Lin. 1998. Dependency-based Evaluation of MINIPAR. In Workshop on the Evaluation of Parsing Systems, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Mann</author>
</authors>
<title>Fine-grained proper noun ontologies for question answering.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="8661" citStr="Mann, 2002" startWordPosition="1324" endWordPosition="1325">rvised mantic lexicons. Weakly supervised corpus-based 19 1Meta-bootstrapping (Riloff and Jones, 1999) was evaluated on Web pages, but used a precompiled corpus of downloaded Web pages. semantic clustering (e.g., (Lin, 1998; Lin and Pantel, 2002; Davidov and Rappoport, 2006; Davidov et al., 2007)), however clustering methods may or may not produce the types and granularities of semantic classes desired by a user. Another related line of work is automated ontology construction, which aims to create lexical hierarchies based on semantic classes (e.g., (Caraballo, 1999; Cimiano and Volker, 2005; Mann, 2002)). 3 Semantic Lexicon Induction with Web-based Corroboration Our approach combines a weakly supervised learning algorithm for corpus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical evidence from the Web. In this section, we describe both of these components. First, we give a brief overview of the Basilisk bootstrapping algorithm that we use for corpus-based semantic lexicon induction. Second, we present our new strategies for acquiring and utilizing corroborating statistical evidence from the Web. 3.1 Corpus-based Semantic Lexicon Induction vi</context>
</contexts>
<marker>Mann, 2002</marker>
<rawString>G. Mann. 2002. Fine-grained proper noun ontologies for question answering. In Proc. of the 19th International Conference on Computational Linguistics, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Miller</author>
</authors>
<title>Wordnet: An On-line Lexical Database.</title>
<date>1990</date>
<journal>International Journal ofLexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1349" citStr="Miller, 1990" startWordPosition="199" endWordPosition="200">-occurrence statistics between each lexicon entry and semantically related terms. The Web statistics provide a source of independent evidence to confirm, or disconfirm, that a word belongs to the intended semantic category. We evaluate this approach on 7 semantic categories representing two domains. Our results show that the Web statistics dramatically improve the ranking of lexicon entries, and can also be used to filter incorrect entries. 1 Introduction Semantic resources are extremely valuable for many natural language processing (NLP) tasks, as evidenced by the wide popularity of WordNet (Miller, 1990) and a multitude of efforts to create similar “WordNets” for additional languages (e.g. (Atserias et al., 1997; Vossen, 1998; Stamou et al., 2002)). Semantic resources can take many forms, but one of the most basic types is a dictionary that associates a word (or word sense) with one or more semantic categories (hypernyms). For example, truck might be identified as a VEHICLE, and dog might be identified as an ANIMAL. Automated methods for generat18 ing such dictionaries have been developed under the rubrics of lexical acquisition, hyponym learning, semantic class induction, and Web-based infor</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>G. Miller. 1990. Wordnet: An On-line Lexical Database. International Journal ofLexicography, 3(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>MUC-4 Proceedings</author>
</authors>
<date>1992</date>
<booktitle>Proceedings of the Fourth Message Understanding Conference (MUC-4).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="18720" citStr="Proceedings, 1992" startWordPosition="3003" endWordPosition="3004">ll candidates, so for N candidate words we need 2N + 1 queries in total. To compute the Average or Max of Seeds Score for a candidate, we need 5 queries for count(seedi), 5 queries for count(seedi NEAR candidate), and 1 query for count(candidate). So for N candidate words we need N ∗ (25 + 1) queries. 5 is typically small for weakly supervised algorithms (5=10 in our experiments), which means that this Web-based corroboration process requires O(N) queries to process a semantic lexicon of size N. 4 Evaluation 4.1 Data Sets We ran experiments on two corpora: 1700 MUC-4 terrorism articles (MUC-4 Proceedings, 1992) and a combination of 6000 disease-related documents, consisting of 2000 ProMed disease outbreak reports (ProMed-mail, 2006) and 4000 disease-related PubMed abstracts (PubMed, 2009). For the terrorism domain, we created lexicons for four semantic categories: BUILDING, HUMAN, LOCATION, and WEAPON. For the disease domain, we created lexicons for three semantic categories: ANIMAL4, DISEASE, and SYMPTOM. For each category, we gave Basilisk 10 seed words as input. The seeds were chosen by applying a shallow parser to each corpus, extracting the head nouns of all the NPs, and sorting the nouns by fr</context>
</contexts>
<marker>Proceedings, 1992</marker>
<rawString>MUC-4 Proceedings. 1992. Proceedings of the Fourth Message Understanding Conference (MUC-4). Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pas¸ca</author>
</authors>
<title>Acquisition of categorized named entities for web search.</title>
<date>2004</date>
<booktitle>In Proc. of the Thirteenth ACM International Conference on Information and Knowledge Management,</booktitle>
<pages>137--145</pages>
<marker>Pas¸ca, 2004</marker>
<rawString>M. Pas¸ca. 2004. Acquisition of categorized named entities for web search. In Proc. of the Thirteenth ACM International Conference on Information and Knowledge Management, pages 137–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In Proc. of Conference of HLT /North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>321--328</pages>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>P. Pantel and D. Ravichandran. 2004. Automatically labeling semantic classes. In Proc. of Conference of HLT /North American Chapter of the Association for Computational Linguistics, pages 321–328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>D Ravichandran</author>
<author>E Hovy</author>
</authors>
<title>Towards terascale knowledge acquisition.</title>
<date>2004</date>
<booktitle>In Proc. of the 20th international conference on Computational Linguistics,</booktitle>
<pages>771</pages>
<contexts>
<context position="4798" citStr="Pantel et al., 2004" startWordPosition="717" endWordPosition="720"> and Magnini, 2006), challenge. You have to know where to look to find and lexico-syntactic contextual patterns (e.g., “respecialized knowledge. In contrast, corpus-based sides in &lt;location&gt;” or “moved to &lt;location&gt;”) methods can learn specialized terminology directly (Riloff and Jones, 1999; Thelen and Riloff, 2002). from a domain-specific corpus, but accuracy can be Due to the need for POS tagging and/or parsing, a problem because most corpora are relatively small. these types of methods have been evaluated only In this paper, we seek to exploit the best of both on fixed corpora1, although (Pantel et al., 2004) worlds by combining a weakly supervised corpus- demonstrated how to scale up their algorithms for based method for semantic lexicon induction with the Web. The goal of our work is to improve upon statistics obtained from the Web. First, we use corpus-based bootstrapping algorithms by using coa bootstrapping algorithm, Basilisk (Thelen and occurrence statistics obtained from the Web to reRiloff, 2002), to automatically induce a semantic rank and filter the hypothesized category members. lexicon from a domain-specific corpus. This pro- Techniques for semantic class learning have also duces a se</context>
</contexts>
<marker>Pantel, Ravichandran, Hovy, 2004</marker>
<rawString>P. Pantel, D. Ravichandran, and E. Hovy. 2004. Towards terascale knowledge acquisition. In Proc. of the 20th international conference on Computational Linguistics, page 771.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pasca</author>
</authors>
<title>weakly-supervised Discovery of Named Entities using Web Search Queries. In</title>
<date>2007</date>
<booktitle>CIKM,</booktitle>
<pages>683--690</pages>
<contexts>
<context position="6353" citStr="Pasca, 2007" startWordPosition="962" endWordPosition="963"> Hearst exploited patterns that explicitly belongs to the semantic category. For each candi- identify a hyponym relation between a semantic date word, we search the Web for pages that con- class and a word (e.g., “such authors as &lt;X&gt;”) to tain both the word and a semantically related term. automatically acquire new hyponyms. (Pas¸ca, 2004) We expect that true semantic category members will applied hyponym patterns to the Web and learned seco-occur with semantically similar words more of- mantic class instances and groups by acquiring conten than non-members. texts around the patterns. Later, (Pasca, 2007) creThis paper is organized as follows. Section 2 dis- ated context vectors for a group of seed instances by cusses prior work on weakly supervised methods for searching Web query logs, and used them to learn semantic lexicon induction. Section 3 overviews similar instances. The KnowItAll system (Etzioni our approach: we briefly describe the weakly su- et al., 2005) also uses hyponym patterns to extract pervised bootstrapping algorithm that we use for class instances from the Web and evaluates them furcorpus-based semantic lexicon induction, and then ther by computing mutual information scores</context>
</contexts>
<marker>Pasca, 2007</marker>
<rawString>M. Pasca. 2007. weakly-supervised Discovery of Named Entities using Web Search Queries. In CIKM, pages 683–690.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Phillips</author>
<author>E Riloff</author>
</authors>
<title>Exploiting Strong Syntactic Heuristics and Co-Training to Learn Semantic Lexicons.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>125--132</pages>
<contexts>
<context position="4085" citStr="Phillips and Riloff, 2002" startWordPosition="603" endWordPosition="606">r tularemia, and nv-cjd is frequently used Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 18–26, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics to refer to new variant Creutzfeldt Jacob Disease. methods have utilized noun co-occurrence statisThe Web is such a vast repository of knowledge tics (Riloff and Shepherd, 1997; Roark and Charthat specialized terminology for nearly any domain niak, 1998), syntactic information (Widdows and probably exists in some niche or cranny, but find- Dorow, 2002; Phillips and Riloff, 2002; Pantel and ing the appropriate corner of the Web to tap into is a Ravichandran, 2004; Tanev and Magnini, 2006), challenge. You have to know where to look to find and lexico-syntactic contextual patterns (e.g., “respecialized knowledge. In contrast, corpus-based sides in &lt;location&gt;” or “moved to &lt;location&gt;”) methods can learn specialized terminology directly (Riloff and Jones, 1999; Thelen and Riloff, 2002). from a domain-specific corpus, but accuracy can be Due to the need for POS tagging and/or parsing, a problem because most corpora are relatively small. these types of methods have been ev</context>
</contexts>
<marker>Phillips, Riloff, 2002</marker>
<rawString>W. Phillips and E. Riloff. 2002. Exploiting Strong Syntactic Heuristics and Co-Training to Learn Semantic Lexicons. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 125–132.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ProMed-mail</author>
</authors>
<title>Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping.</title>
<date>2006</date>
<booktitle>In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="18844" citStr="ProMed-mail, 2006" startWordPosition="3020" endWordPosition="3021">candidate, we need 5 queries for count(seedi), 5 queries for count(seedi NEAR candidate), and 1 query for count(candidate). So for N candidate words we need N ∗ (25 + 1) queries. 5 is typically small for weakly supervised algorithms (5=10 in our experiments), which means that this Web-based corroboration process requires O(N) queries to process a semantic lexicon of size N. 4 Evaluation 4.1 Data Sets We ran experiments on two corpora: 1700 MUC-4 terrorism articles (MUC-4 Proceedings, 1992) and a combination of 6000 disease-related documents, consisting of 2000 ProMed disease outbreak reports (ProMed-mail, 2006) and 4000 disease-related PubMed abstracts (PubMed, 2009). For the terrorism domain, we created lexicons for four semantic categories: BUILDING, HUMAN, LOCATION, and WEAPON. For the disease domain, we created lexicons for three semantic categories: ANIMAL4, DISEASE, and SYMPTOM. For each category, we gave Basilisk 10 seed words as input. The seeds were chosen by applying a shallow parser to each corpus, extracting the head nouns of all the NPs, and sorting the nouns by frequency. A human then walked down the sorted list and identified the 10 most frequent nouns that belong to each semantic cat</context>
</contexts>
<marker>ProMed-mail, 2006</marker>
<rawString>ProMed-mail. 2006. http://www.promedmail.org/. PubMed. 2009. http://www.ncbi.nlm.nih.gov/sites/entrez. E. Riloff and R. Jones. 1999. Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
<author>J Shepherd</author>
</authors>
<title>A Corpus-Based Approach for Building Semantic Lexicons.</title>
<date>1997</date>
<booktitle>In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>117--124</pages>
<contexts>
<context position="3881" citStr="Riloff and Shepherd, 1997" startWordPosition="572" endWordPosition="575"> and contain many informal terms, abbreviations, and spelling variants that do not even occur in most medical dictionaries. For example, yf refers to yellow fever, tularaemia is an alternative spelling for tularemia, and nv-cjd is frequently used Proceedings of the NAACL HLT Workshop on Unsupervised and Minimally Supervised Learning of Lexical Semantics, pages 18–26, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics to refer to new variant Creutzfeldt Jacob Disease. methods have utilized noun co-occurrence statisThe Web is such a vast repository of knowledge tics (Riloff and Shepherd, 1997; Roark and Charthat specialized terminology for nearly any domain niak, 1998), syntactic information (Widdows and probably exists in some niche or cranny, but find- Dorow, 2002; Phillips and Riloff, 2002; Pantel and ing the appropriate corner of the Web to tap into is a Ravichandran, 2004; Tanev and Magnini, 2006), challenge. You have to know where to look to find and lexico-syntactic contextual patterns (e.g., “respecialized knowledge. In contrast, corpus-based sides in &lt;location&gt;” or “moved to &lt;location&gt;”) methods can learn specialized terminology directly (Riloff and Jones, 1999; Thelen an</context>
</contexts>
<marker>Riloff, Shepherd, 1997</marker>
<rawString>E. Riloff and J. Shepherd. 1997. A Corpus-Based Approach for Building Semantic Lexicons. In Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 117–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<publisher>The AAAI Press/MIT Press.</publisher>
<contexts>
<context position="10046" citStr="Riloff, 1996" startWordPosition="1535" endWordPosition="1536">k requires a small set of seed words for each semantic category, and a collection of (unannotated) texts. Basilisk iteratively generates new words that are hypothesized to belong to the same semantic class as the seeds. Here we give an overview of Basilisk’s algorithm and refer the reader to (Thelen and Riloff, 2002) for more details. The key idea behind Basilisk is to use pattern contexts around a word to identify its semantic class. Basilisk’s bootstrapping process has two main steps: Pattern Pool Creation and Candidate Word Selection. First, Basilisk applies the AutoSlog pattern generator (Riloff, 1996) to create a set of lexicosyntactic patterns that, collectively, can extract every noun phrase in the corpus. Basilisk then ranks the patterns according to how often they extract the seed words, under the assumption that patterns which extract known category members are likely to extract other category members as well. The highest-ranked patterns are placed in a pattern pool. Second, Basilisk gathers every noun phrase that is extracted by at least one pattern in the pattern pool, and designates each head noun as a candidate for the semantic category. The candidates are then scored and ranked. </context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>E. Riloff. 1996. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, pages 1044–1049. The AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Roark</author>
<author>E Charniak</author>
</authors>
<title>Noun-phrase Cooccurrence Statistics for Semi-automatic Semantic Lexicon Construction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1110--1116</pages>
<marker>Roark, Charniak, 1998</marker>
<rawString>B. Roark and E. Charniak. 1998. Noun-phrase Cooccurrence Statistics for Semi-automatic Semantic Lexicon Construction. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 1110–1116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sofia Stamou</author>
<author>Kemal Oflazer</author>
<author>Karel Pala</author>
<author>Dimitris Christoudoulakis</author>
<author>Dan Cristea</author>
<author>Dan Tufis</author>
<author>Svetla Koeva</author>
<author>George Totkov</author>
<author>Dominique Dutoit</author>
<author>Maria Grigoriadou</author>
</authors>
<title>Balkanet: A multilingual semantic network for the balkan languages.</title>
<date>2002</date>
<booktitle>In Proceedings of the 1st Global WordNet Association conference.</booktitle>
<contexts>
<context position="1495" citStr="Stamou et al., 2002" startWordPosition="220" endWordPosition="223">ce to confirm, or disconfirm, that a word belongs to the intended semantic category. We evaluate this approach on 7 semantic categories representing two domains. Our results show that the Web statistics dramatically improve the ranking of lexicon entries, and can also be used to filter incorrect entries. 1 Introduction Semantic resources are extremely valuable for many natural language processing (NLP) tasks, as evidenced by the wide popularity of WordNet (Miller, 1990) and a multitude of efforts to create similar “WordNets” for additional languages (e.g. (Atserias et al., 1997; Vossen, 1998; Stamou et al., 2002)). Semantic resources can take many forms, but one of the most basic types is a dictionary that associates a word (or word sense) with one or more semantic categories (hypernyms). For example, truck might be identified as a VEHICLE, and dog might be identified as an ANIMAL. Automated methods for generat18 ing such dictionaries have been developed under the rubrics of lexical acquisition, hyponym learning, semantic class induction, and Web-based information extraction. These techniques can be used to rapidly create semantic lexicons for new domains and languages, and to automatically increase t</context>
</contexts>
<marker>Stamou, Oflazer, Pala, Christoudoulakis, Cristea, Tufis, Koeva, Totkov, Dutoit, Grigoriadou, 2002</marker>
<rawString>Sofia Stamou, Kemal Oflazer, Karel Pala, Dimitris Christoudoulakis, Dan Cristea, Dan Tufis, Svetla Koeva, George Totkov, Dominique Dutoit, and Maria Grigoriadou. 2002. Balkanet: A multilingual semantic network for the balkan languages. In Proceedings of the 1st Global WordNet Association conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Tanev</author>
<author>B Magnini</author>
</authors>
<title>Weakly supervised approaches for ontology population.</title>
<date>2006</date>
<booktitle>In Proc. of 11st Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="4197" citStr="Tanev and Magnini, 2006" startWordPosition="623" endWordPosition="626">pervised Learning of Lexical Semantics, pages 18–26, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics to refer to new variant Creutzfeldt Jacob Disease. methods have utilized noun co-occurrence statisThe Web is such a vast repository of knowledge tics (Riloff and Shepherd, 1997; Roark and Charthat specialized terminology for nearly any domain niak, 1998), syntactic information (Widdows and probably exists in some niche or cranny, but find- Dorow, 2002; Phillips and Riloff, 2002; Pantel and ing the appropriate corner of the Web to tap into is a Ravichandran, 2004; Tanev and Magnini, 2006), challenge. You have to know where to look to find and lexico-syntactic contextual patterns (e.g., “respecialized knowledge. In contrast, corpus-based sides in &lt;location&gt;” or “moved to &lt;location&gt;”) methods can learn specialized terminology directly (Riloff and Jones, 1999; Thelen and Riloff, 2002). from a domain-specific corpus, but accuracy can be Due to the need for POS tagging and/or parsing, a problem because most corpora are relatively small. these types of methods have been evaluated only In this paper, we seek to exploit the best of both on fixed corpora1, although (Pantel et al., 2004</context>
</contexts>
<marker>Tanev, Magnini, 2006</marker>
<rawString>H. Tanev and B. Magnini. 2006. Weakly supervised approaches for ontology population. In Proc. of 11st Conference of the European Chapter of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Thelen</author>
<author>E Riloff</author>
</authors>
<title>A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pa ttern Contexts.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>214--221</pages>
<contexts>
<context position="4496" citStr="Thelen and Riloff, 2002" startWordPosition="665" endWordPosition="668">erd, 1997; Roark and Charthat specialized terminology for nearly any domain niak, 1998), syntactic information (Widdows and probably exists in some niche or cranny, but find- Dorow, 2002; Phillips and Riloff, 2002; Pantel and ing the appropriate corner of the Web to tap into is a Ravichandran, 2004; Tanev and Magnini, 2006), challenge. You have to know where to look to find and lexico-syntactic contextual patterns (e.g., “respecialized knowledge. In contrast, corpus-based sides in &lt;location&gt;” or “moved to &lt;location&gt;”) methods can learn specialized terminology directly (Riloff and Jones, 1999; Thelen and Riloff, 2002). from a domain-specific corpus, but accuracy can be Due to the need for POS tagging and/or parsing, a problem because most corpora are relatively small. these types of methods have been evaluated only In this paper, we seek to exploit the best of both on fixed corpora1, although (Pantel et al., 2004) worlds by combining a weakly supervised corpus- demonstrated how to scale up their algorithms for based method for semantic lexicon induction with the Web. The goal of our work is to improve upon statistics obtained from the Web. First, we use corpus-based bootstrapping algorithms by using coa bo</context>
<context position="9414" citStr="Thelen and Riloff, 2002" startWordPosition="1429" endWordPosition="1432">pus-based semantic lexicon induction with a follow-on procedure that gathers corroborating statistical evidence from the Web. In this section, we describe both of these components. First, we give a brief overview of the Basilisk bootstrapping algorithm that we use for corpus-based semantic lexicon induction. Second, we present our new strategies for acquiring and utilizing corroborating statistical evidence from the Web. 3.1 Corpus-based Semantic Lexicon Induction via Bootstrapping For corpus-based semantic lexicon induction, we use a weakly supervised bootstrapping algorithm called Basilisk (Thelen and Riloff, 2002). As input, Basilisk requires a small set of seed words for each semantic category, and a collection of (unannotated) texts. Basilisk iteratively generates new words that are hypothesized to belong to the same semantic class as the seeds. Here we give an overview of Basilisk’s algorithm and refer the reader to (Thelen and Riloff, 2002) for more details. The key idea behind Basilisk is to use pattern contexts around a word to identify its semantic class. Basilisk’s bootstrapping process has two main steps: Pattern Pool Creation and Candidate Word Selection. First, Basilisk applies the AutoSlog </context>
<context position="11451" citStr="Thelen and Riloff, 2002" startWordPosition="1765" endWordPosition="1768">mputes the average of these log values as the score for the candidate. Intuitively, a candidate word receives a high score if it was extracted by patterns that, on average, also extract many known category members. The N highest ranked candidates are automatically added to the list of seed words, taking a leap of faith that they are true members of the semantic category. The bootstrapping process then repeats, using the larger set of seed words as known category members in the next iteration. Basilisk learns many good category members, but its accuracy varies a lot across semantic categories (Thelen and Riloff, 2002). One problem with Basilisk, and bootstrapping algorithms in general, is that accuracy tends to deteriorate as bootstrapping progresses. Basilisk generates candidates by identifying the contexts in which they occur and words unrelated to the desired category can sometimes also occur in those contexts. Some patterns consistently extract members of several semantic classes; for example, “attack on &lt;NP&gt;” will extract both people (“attack on the president”) and buildings (“attack on the U.S. embassy”). Idiomatic expressions and parsing errors can also lead to undesirable words being learned. Incor</context>
<context position="20395" citStr="Thelen and Riloff, 2002" startWordPosition="3257" endWordPosition="3260">rists soldiers leaders LOCATION: country El Salvador Salvador United States area Colombia city countries department Nicaragua WEAPON: weapons bomb bombs explosives arms missiles dynamite rifles materiel bullets ANIMAL: bird mosquito cow horse pig chicken sheep dog deer fish DISEASE: SARS BSE anthrax influenza WNV FMD encephalitis malaria pneumonia flu SYMPTOM: fever diarrhea vomiting rash paralysis weakness necrosis chills headaches hemorrhage Table 1: Seed Words To evaluate our results, we used the gold standard answer key that Thelen &amp; Riloff created to evaluate Basilisk on the MUC4 corpus (Thelen and Riloff, 2002); they manually labeled every head noun in the corpus with its semantic class. For the ProMed / PubMed disease corpus, we created our own answer key. For all of the lexicon entries hypothesized by Basilisk, a human annotator (not any of the authors) 4ANIMAL was chosen because many of the ProMed disease outbreak stories concerned outbreaks among animal populations. 5The disease domain seed words were chosen from a larger set of ProMed documents, which included the 2000 used for lexicon induction. 1 Iseedsl � i=1 Iseedsl 22 N Ba Hy BUILDING Mx Ba Hy HUMAN Mx Ba Hy LOCATION Mx Ba Hy WEAPON Mx Av </context>
<context position="23081" citStr="Thelen and Riloff, 2002" startWordPosition="3758" endWordPosition="3761">ds for each semantic category, and the topranked terms are generally more accurate than the lower-ranked terms. For the top 100 words, accuracies are generally in the 50-70% range, except for LOCATION which achieves about 80% accuracy. For the HUMAN category, Basilisk obtained 82% accuracy over all 300 words, but the top-ranked words actually produced lower accuracy. Basilisk’s ranking is clearly not as good as it could be because there are correct terms co-mingled with incorrect terms throughout the ranked lists. This has 6These results are not comparable to the Basilisk results reported by (Thelen and Riloff, 2002) because our implementation only does single-category learning while the results in that paper are based on simultaneously learning multiple categories. two ramifications. First, if we want a human to manually review each lexicon before adding the words to an external resource, then the rankings may not be very helpful (i.e., the human will need to review all of the words), and (2) incorrect terms generated during the early stages of bootstrapping may be hindering the learning process because they introduce noise during bootstrapping. The HUMAN category seems to have recovered from early mista</context>
</contexts>
<marker>Thelen, Riloff, 2002</marker>
<rawString>M. Thelen and E. Riloff. 2002. A Bootstrapping Method for Learning Semantic Lexicons Using Extraction Pa ttern Contexts. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 214–221.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
</authors>
<title>Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL.</title>
<date>2001</date>
<booktitle>In EMCL ’01: Proceedings of the 12th European Conference on Machine Learning,</booktitle>
<pages>491--502</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="7548" citStr="Turney, 2001" startWordPosition="1145" endWordPosition="1146">information scores based present our procedure for gathering corroborating on Web queries. (Kozareva et al., 2008) proposed evidence from the Web. Section 4 presents exper- the use of a doubly-anchored hyponym pattern and imental results on seven semantic categories repre- a graph to represent the links between hyponym ocsenting two domains: Latin American terrorism and currences in these patterns. disease-related documents. Section 5 summarizes Our work builds upon Turney’s work on semanour results and discusses future work. tic orientation (Turney, 2002) and synonym learning 2 Related Work (Turney, 2001), in which he used a PMI-IR algoOur research focuses on semantic lexicon induc- rithm to measure the similarity of words and phrases tion, where the goal is to create a list of words based on Web queries. We use a similar PMI (pointthat belong to a desired semantic class. A sub- wise mutual information) metric for the purposes of stantial amount of previous work has been done on semantic class verification. weakly supervised and unsupervised creation of se- There has also been work on fully unsupervised mantic lexicons. Weakly supervised corpus-based 19 1Meta-bootstrapping (Riloff and Jones, 1</context>
<context position="14016" citStr="Turney, 2001" startWordPosition="2182" endWordPosition="2183">a word’s semantic class through statistics that measure how often the word co-occurs with semantically related words. For each candidate word produced by Basilisk, we construct a Web query that pairs the word with a semantically related word. Our goal is not just to find Web pages that contain both terms, but to find Web pages that contain both terms in close proximity to one another. We consider two terms to be collocated if they occur within ten words of each other on the same Web page, which corresponds to the functionality of the NEAR operator used by the AltaVista search engine2. Turney (Turney, 2001; Turney, 2002) reported that the NEAR operator outperformed simple page co-occurrence for his purposes; our early experiments informally showed the same for this work. We want our technique to remain weakly supervised, so we do not want to require additional human input or effort beyond what is already required for Basilisk. With this in mind, we investigated two types of collocation relations as possible indicators of semantic class membership: Hypernym Collocation: We compute cooccurrence statistics between the candidate word and the name of the targeted semantic class (i.e., the word’s hyp</context>
</contexts>
<marker>Turney, 2001</marker>
<rawString>Peter D. Turney. 2001. Mining the Web for Synonyms: PMI-IR versus LSA on TOEFL. In EMCL ’01: Proceedings of the 12th European Conference on Machine Learning, pages 491–502, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P D Turney</author>
</authors>
<title>Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL’02),</booktitle>
<pages>417--424</pages>
<contexts>
<context position="7497" citStr="Turney, 2002" startWordPosition="1137" endWordPosition="1138">xicon induction, and then ther by computing mutual information scores based present our procedure for gathering corroborating on Web queries. (Kozareva et al., 2008) proposed evidence from the Web. Section 4 presents exper- the use of a doubly-anchored hyponym pattern and imental results on seven semantic categories repre- a graph to represent the links between hyponym ocsenting two domains: Latin American terrorism and currences in these patterns. disease-related documents. Section 5 summarizes Our work builds upon Turney’s work on semanour results and discusses future work. tic orientation (Turney, 2002) and synonym learning 2 Related Work (Turney, 2001), in which he used a PMI-IR algoOur research focuses on semantic lexicon induc- rithm to measure the similarity of words and phrases tion, where the goal is to create a list of words based on Web queries. We use a similar PMI (pointthat belong to a desired semantic class. A sub- wise mutual information) metric for the purposes of stantial amount of previous work has been done on semantic class verification. weakly supervised and unsupervised creation of se- There has also been work on fully unsupervised mantic lexicons. Weakly supervised corpu</context>
<context position="14031" citStr="Turney, 2002" startWordPosition="2184" endWordPosition="2186">tic class through statistics that measure how often the word co-occurs with semantically related words. For each candidate word produced by Basilisk, we construct a Web query that pairs the word with a semantically related word. Our goal is not just to find Web pages that contain both terms, but to find Web pages that contain both terms in close proximity to one another. We consider two terms to be collocated if they occur within ten words of each other on the same Web page, which corresponds to the functionality of the NEAR operator used by the AltaVista search engine2. Turney (Turney, 2001; Turney, 2002) reported that the NEAR operator outperformed simple page co-occurrence for his purposes; our early experiments informally showed the same for this work. We want our technique to remain weakly supervised, so we do not want to require additional human input or effort beyond what is already required for Basilisk. With this in mind, we investigated two types of collocation relations as possible indicators of semantic class membership: Hypernym Collocation: We compute cooccurrence statistics between the candidate word and the name of the targeted semantic class (i.e., the word’s hypothesized hyper</context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>P. D. Turney. 2002. Thumbs up or thumbs down? semantic orientation applied to unsupervised classification of reviews. In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL’02), pages 417–424.</rawString>
</citation>
<citation valid="true">
<title>EuroWordNet: A Multilingual Database with Lexical Semantic Networks.</title>
<date>1998</date>
<editor>Piek Vossen, editor.</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Norwell, MA, USA.</location>
<marker>1998</marker>
<rawString>Piek Vossen, editor. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer Academic Publishers, Norwell, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Widdows</author>
<author>B Dorow</author>
</authors>
<title>A graph model for unsupervised lexical acquisition.</title>
<date>2002</date>
<booktitle>In Proc. of the 19th International Conference on Computational Linguistics,</booktitle>
<pages>1--7</pages>
<marker>Widdows, Dorow, 2002</marker>
<rawString>D. Widdows and B. Dorow. 2002. A graph model for unsupervised lexical acquisition. In Proc. of the 19th International Conference on Computational Linguistics, pages 1–7.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>