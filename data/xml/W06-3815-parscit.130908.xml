<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000161">
<title confidence="0.991695">
Context Comparison as a Minimum Cost Flow Problem
</title>
<author confidence="0.998256">
Vivian Tsang and Suzanne Stevenson
</author>
<affiliation confidence="0.9987455">
Department of Computer Science
University of Toronto
</affiliation>
<address confidence="0.542099">
Canada
</address>
<email confidence="0.99473">
vyctsang,suzanne @cs.utoronto.ca
</email>
<sectionHeader confidence="0.993735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99963052631579">
Comparing word contexts is a key compo-
nent of many NLP tasks, but rarely is it
used in conjunction with additional onto-
logical knowledge. One problem is that
the amount of overhead required can be
high. In this paper, we provide a graphi-
cal method which easily combines an on-
tology with contextual information. We
take advantage of the intrinsic graphical
structure of an ontology for representing
a context. In addition, we turn the on-
tology into a metric space, such that sub-
graphs within it, which represent contexts,
can be compared. We develop two vari-
ants of our graphical method for compar-
ing contexts. Our analysis indicates that
our method performs the comparison effi-
ciently and offers a competitive alternative
to non-graphical methods.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999266875">
Many natural language problems can be cast as a
problem of comparing “contexts” (units of text). For
example, the local context of a word can be used to
resolve its ambiguity (e.g., Sch¨utze, 1998), assum-
ing that words used in similar contexts are closely
related semantically (Miller and Charles, 1991). Ex-
tending the meaning of context, the content of a
document may reveal which document class(es) it
belongs to (e.g., Xu et al., 2003). In any appli-
cation, once a sensible view of context is formu-
lated, the next step is to choose a representation that
makes comparisons possible. For example, in word
sense disambiguation, a context of an ambiguous
instance can be represented as a vector of the fre-
quencies of words surrounding it. Until recently, the
dominant approach has been a non-graphical one—
context comparison is reduced to a task of measuring
distributional distance between context vectors. The
difference in the frequency characteristics of con-
texts is used as an indicator of the semantic distance
between them.
We present a graphical alternative that combines
both distributional and ontological knowledge. We
begin with the use of a different context represen-
tation that allows easy incorporation of ontological
information. Treating an ontology as a network, we
can represent a context as a set of nodes in the net-
work (i.e., concepts in the ontology), each with a
weight (i.e., frequency). To contrast our work with
that of Navigli and Velardi (2005) and Mihalcea
(2006), the goal is not merely to provide a graph-
ical representation for a context in which the rele-
vant concepts are connected. Rather, contexts are
treated as weighted subgraphs within a larger graph
in which they are connected via a set of paths. By in-
corporating the semantic distance between individ-
ual concepts, the graph (representing the ontology)
becomes a metric space in which we can measure the
distance between subgraphs (representing the con-
texts to be compared).
More specifically, measuring the distance be-
tween two contexts can be viewed as solving a min-
imum cost flow (MCF) problem by calculating the
amount of “effort” required for transporting the flow
from one context to the other. Our method has
the advantage of including semantic information (by
making use of the graphical structure of an ontol-
ogy) without losing distributional information (by
</bodyText>
<page confidence="0.995608">
97
</page>
<bodyText confidence="0.940127708333333">
Workshop on TextGraphs, at HLT-NAACL 2006, pages 97–104,
New York City, June 2006. c�2006 Association for Computational Linguistics
using the concept frequencies derived from corpus
data).
This network flow formulation, though support-
ing the inclusion of an ontology in context compari-
son, is not flexible enough. The problem is rooted in
the choice of concept-to-concept distance (i.e., the
distance between two concepts, to contrast it from
the overall semantic distance between two contexts).
Certain concept-to-concept distances may result in a
difficult-to-process network which severely compro-
mises efficiency. To remedy this, we propose a novel
network transformation method for constructing a
pared-down network which mimics the structure of
the more precise network, but without the expensive
processing or any significant information loss as a
result of the transformation.
In the remainder of this paper, we first present the
underlying network flow framework, and develop a
more efficient variant of it. We then evaluate the
robustness of our methods on a context comparison
task. Finally, we conclude with an analysis and some
future directions.
</bodyText>
<sectionHeader confidence="0.97951" genericHeader="method">
2 The Network Flow Method
</sectionHeader>
<subsectionHeader confidence="0.956438">
2.1 Minimum Cost Flow
</subsectionHeader>
<bodyText confidence="0.993463466666667">
As a standard example of an MCF problem, consider
the graphical representation of a route map for deliv-
ering fresh produce from grocers (supply nodes) to
homes (demand nodes). The remaining nodes (e.g.,
intersections, gas stations) have neither a supply nor
a demand. Assuming there are sufficient supplies,
the optimal solution is to find the cheapest set of
routes from grocers to homes such that all demands
are satisfied.
Mathematically, let be a connected
network, where is the set of nodes, and is the
set of edges.l Each edge has a cost
which is the distance of the edge. Each node
is associated with a value such that
indicates its available supply ( ), its demand
( ), or neither ( ). The goal is to find a
solution for each node such that all the flow passing
through satisfies its supply or demand requirement
( ). The flow passing through node is captured
by such that we can observe the com-
&apos;Most ontologies are hierarchical, thus, in the case of a for-
est, adding an arbitrary root node yields a connected graph.
bined incoming flow, , from the
entering edges , as well as the combined outgo-
ing flow, , via the exiting edges
. (See Figure 1.) If a feasible solution can be
found, the net flow (the difference between the en-
tering and exiting flow) at each node must fulfill the
corresponding supply or demand requirement.
Formally, the MCF problem can be stated as:
</bodyText>
<equation confidence="0.7952454">
Minimize
(1)
subject to
(2)
(3)
</equation>
<bodyText confidence="0.99656475">
The constraint specified by (2) ensures that the dif-
ference between the flow entering and exiting each
node matches its supply or demand exactly.
The next constraint (3) ensures that the flow is trans-
ported from the supply to the demand but not in
the opposite direction. Finally, selecting route
requires a transportation “effort” of (cost
of the route) multiplied by the amount of supply
transported (the term inside the summation
in eqn. (1)). Taking the summation of the effort,
, of cheapest routes yields the desired
distance between the supply and the demand.
</bodyText>
<subsectionHeader confidence="0.998762">
2.2 Semantic Distance as MCF
</subsectionHeader>
<bodyText confidence="0.999087142857143">
To cast our context comparison task into this frame-
work, we first represent each context as a vector of
concept frequencies (or a context profile for the re-
mainder of this paper). The profile of one context is
chosen as the supply and the other as the demand.
The concept frequencies of the profiles are normal-
ized, so that the total supply always equals the total
</bodyText>
<figureCaption confidence="0.994469">
Figure 1: An illustration of flow entering and exiting node.
</figureCaption>
<figure confidence="0.326353">
,
</figure>
<page confidence="0.997932">
98
</page>
<bodyText confidence="0.997952205128205">
demand. The cost of the routes between nodes is
determined by a semantic distance measure defined
over any two nodes in the ontology. Now, as in the
grocery delivery domain, the goal is to find the MCF
from supply to demand.
We can treat any ontology as the transport net-
work. A relation (such as hyponymy) between two
concepts and is represented by an edge , and
the cost on each edge can be defined as the seman-
tic distance between the two concepts. This seman-
tic distance can be as simple as the number of edges
separating the concepts, or more sophisticated, such
as Lin’s (1998) information-theoretic measure. (See
Budanitsky and Hirst (2006) for a survey of such
measures).
Numerous methods are possible for converting
the word frequency vector of a context to a concept
frequency vector (i.e., a context profile). One simple
method is to transfer each element in the word vector
(i.e., the frequency of each word) to the correspond-
ing concepts in the ontology, resulting in a vector
of concept frequencies. In this paper, we have cho-
sen a uniform distribution of word frequency counts
among concepts, instead of a weighted distribution
towards the relevant concepts for a particular text.
Since we wish to evaluate the strength of our method
alone without any additional NLP effort, we bypass
the issue of approximating the true distribution of
the concepts via word sense disambiguation or class-
based approximation methods, such as those by Li
and Abe (1998) and Clark and Weir (2002).
To calculate the distance between two profiles, we
need to cast one profile as the supply ( ) and the
other as the demand ( ). Note that our distance
is symmetric, so the choice of the supply and the
demand is arbitrary. Next, we must determine the
value of at each concept node ; this is just
the difference between the (normalized) supply fre-
quency and demand frequency
</bodyText>
<equation confidence="0.545723">
(4)
</equation>
<bodyText confidence="0.999354722222222">
This formula yields the net supply/demand, , at
node. Recall that our goal is to transport all the sup-
ply to meet the demand—the final step is to deter-
mine the cheapest routes between and such that
the constraints in (2) and (3) are satisfied. The total
distance of the routes, or the MCF, in eqn. (1),
is the distance between the two context profiles.
Finally, it is important to note that the MCF for-
mulation does not simply find the shortest paths
from the concept nodes in the supply to those in the
demand. Because a profile is a frequency-weighted
concept vector, some concept nodes are weighted
more heavily than others, and the routes between
such nodes across the two profiles are also weighted
more heavily. Indeed, in eqn. (1), the cost of each
route, , is weighted by (how much sup-
ply, or frequency weight, is transported between
nodes and ).
</bodyText>
<sectionHeader confidence="0.992858" genericHeader="method">
3 Graphical Issues
</sectionHeader>
<bodyText confidence="0.999949">
As alluded to in the introduction, certain concept-
to-concept distances pose a problem to solving the
MCF problem easily. The details are described next.
</bodyText>
<subsectionHeader confidence="0.995784">
3.1 Additivity
</subsectionHeader>
<bodyText confidence="0.985126913043478">
In theory, our method has the flexibility to incorpo-
rate different concept-to-concept distances. The is-
sue lies in the algorithms for solving MCF problems.
Existing algorithms are greedy—they take a step-
wise “localist” approach on the set of edges connect-
ing the supply and the demand; i.e., at each node,
the cheapest outgoing edge is selected. The assump-
tion is that the concept-to-concept distance function
is additive. Mathematically, for any path from node
to node ,, where
and , the distance between nodes and is
the sum of the distance of the edges along the path:
(5)
The additivity of a concept-to-concept distance en-
tails that selecting the cheapest edge at each step
(i.e., locally) yields the overall cheapest set of routes
(i.e., globally). Note that some of the most success-
ful concept-to-concept distances proposed in the CL
literature are non-additive (e.g., Lin, 1998; Resnik,
1995). This poses a problem in solving our network
flow problem—the global distance between any con-
cepts, and , cannot be correctly determined by the
greedy method.
</bodyText>
<subsectionHeader confidence="0.7725135">
3.2 Constructing an Equivalent Bipartite
Network
</subsectionHeader>
<bodyText confidence="0.907619666666667">
The issue of non-additive distances can be addressed
in the following way. We map the relevant portion
:
</bodyText>
<page confidence="0.968509">
99
</page>
<figureCaption confidence="0.992433333333333">
Figure 2: An illustration of the transformations (left to right) from the original network (a) to the bipartite network (b), and finally,
to the network produced by our transformation (c), given two profiles S and D. Nodes labelled with either “S” or “D” belong to the
corresponding profile. Nodes labelled with “ ” or “ ” are junction nodes (see section 4.2).
</figureCaption>
<bodyText confidence="0.99997284">
of the network into a new network such that the
concept-to-concept distance is preserved, but with-
out the problem introduced by non-additivity. One
possible solution is to construct a complete bipar-
tite graph between the supply nodes and the demand
nodes (the nodes in the two context profiles). We set
the cost of each edge in the bipartite graph to
be the concept-to-concept distance between and
in the original network. Since there is exactly one
edge between any pair of nodes, the non-additivity
is removed entirely. (See Figures 2(a) and 2(b).)
Now, we can apply a network flow solver on the new
graph.
However, one problem arises from performing the
above mapping—there is a processing bottleneck as
a result of the quadratic increase in the number of
edges in the new network. Unfortunately, though
tractable, polynomial complexity is not always prac-
tical. For example, with an average of 900 nodes
per profile, making 120 profile comparisons in addi-
tion to network re-structuring can take as long as 10
days.2 If we choose to use a non-additive distance,
the method described above does not scale up well
for a large number of comparisons. Next, we present
a method to alleviate the complexity issue.
</bodyText>
<sectionHeader confidence="0.998739" genericHeader="method">
4 Network Transformation
</sectionHeader>
<bodyText confidence="0.998267">
One method of alleviating the bottleneck is to reduce
the processing load from generating a large number
</bodyText>
<footnote confidence="0.976509">
2This is tested on a context comparison task not reported in
this paper. The code is scripted in perl. The experiment was
performed on a machine with two P4 Xeon CPUs running at
3.6GHz, with a 1MB cache and 6GB of memory.
</footnote>
<bodyText confidence="0.709840125">
of edges. Instead of generating a complete bipar-
tite network, we generate a network which approx-
imates both the structure of the original network as
well as that of the complete bipartite network. The
goal is to construct a pared-down network such that
(a) a reduction in the number of edges improves effi-
ciency, and (b) the resulting distance distortion does
not hamper performance significantly.
</bodyText>
<subsectionHeader confidence="0.997626">
4.1 Path Shape in a Hierarchy
</subsectionHeader>
<bodyText confidence="0.999967695652174">
To understand our transformation method, let us fur-
ther examine the graphical properties of an ontology
as a network. In a hierarchical network (e.g., Word-
Net, Gene Ontology, UMLS), calculating the dis-
tance between two concept nodes usually involves
travelling “up” and “down” the hierarchy. The sim-
plest route is a single hop from a child to its parent
or vice versa. Generally, travelling from one node
to another node consists of an A-shaped path as-
cending from node to a common ancestor of and
, and then descending to node .
Interestingly, our description of the A-shaped
path matches the design of a number of concept-to-
concept distances. For example, distances that in-
corporate Resnik’s (1995) information content (IC),
, such as those of Jiang and Con-
rath (1997) and Lin (1998), consider both the (low-
est) common ancestor as well as the two nodes of
interest in their calculation.
The complete bipartite graph considered in sec-
tion 3.2 directly connects each node s in profile
to node in profile , eliminating the typical A-
shaped path in an ontology. This structure solves the
</bodyText>
<page confidence="0.964382">
100
</page>
<bodyText confidence="0.9999578">
non-additivity issue, by generating an edge with the
exact concept-to-concept distance for each potential
node comparison, but, as noted above, is too inef-
ficient. Our solution here is to construct a network
that uses the idea of a pared-down A-shaped path to
mostly avoid non-additivity, but without the ineffi-
ciency of the complete bipartite graph. Thus, as ex-
plained in more detail in the following subsections,
we trade off the exactness of the distance calculation
against the efficiency of the network construction.
</bodyText>
<subsectionHeader confidence="0.886398">
4.2 Network Construction
</subsectionHeader>
<bodyText confidence="0.99741845">
In our network construction, we exploit the general
notion of an A-shaped path between any two nodes,
but replace the “tip” of the A with two nodes. Then
for each node and in profiles and , we gen-
erate an edge from s to an ancestor of (the
left “branch” of the A), an edge from d to an an-
cestor of (the right “branch” of the A), and an
edge between and (the two nodes forming the
“elongated tip” of the A). Each edge has the exact
concept-to-concept distance from the original net-
work, so that the distance between any two nodes
and is the sum of three exact distances.
The set of ancestor nodes, and , comprise the
“junction” points at which the supply from can be
transported across to the nodes in to satisfy their
demand. The set of junction nodes, , for a pro-
file , must be selected such that for each node
in , contains at least one ancestor of. (See
section 4.4 for details on the junction selection pro-
cess.) The resulting network is constructed by di-
rectly connecting each profile to its corresponding
junction, then connecting the two junctions in the
middle (Figure 2(c)).
The difference between the complete bipartite
network and the transformed network here is that,
instead of connecting each node in to every node
in , we connect each node in to every node
in . Compare the transformed network in Fig-
ure 2(c) with the complete bipartite network in Fig-
ure 2(b). The complete bipartite component in the
transformed network (the middle portion between
the junction nodes labelled and ) is consid-
erably smaller in size. Thus, the number of edges
in the transformed network is significantly fewer as
well.
Next, we can proceed to define the cost function
on the transformed network. Observe that each edge
, with cost , in the complete bipartite
network, where ,, is now instead repre-
sented by three edges: ,
</bodyText>
<equation confidence="0.740101">
(6)
</equation>
<bodyText confidence="0.999482">
where is the precise concept-to-concept
distance between and in the original network.
Once we have set up the transformed network, we
can solve the MCF in this network, yielding the dis-
tance between the two (supply and demand) profiles.
</bodyText>
<subsectionHeader confidence="0.994101">
4.3 Distance Distortion
</subsectionHeader>
<bodyText confidence="0.979488125">
Because the distance between nodes and is now
calculated as the sum of three distances (eqn. (6)),
some distortion may result for non-additive concept-
to-concept distances. To illustrate the distortion ef-
fect, consider Jiang and Conrath’s (1997) distance:
where is the information content of a node
, and is the lowest common subsumer
of nodes and . This distance measures the dif-
ference in information content between the concepts
and their lowest common subsumers.
After the transformation, the distance is distorted
in the following way. If and have no common
junction ancestor, then becomes:
,
where the term in eqn. (8) is re-
placed by . In either case, the transformation
replaces the lowest common subsumer
in eqn. (7) with some other common subsumer
( or , mentioned above). Un-
less , the distance is distorted
by using a less precise quantity,
,
where and . Thus, the transformed
distance between and ,, becomes:
, and
where and are the junction ancestors of
and , respectively. Otherwise, if and
share a common ancestor at the junction, then
becomes
.
Note that the information content of a concept is
given by its maximum likelihood estimate based on
</bodyText>
<page confidence="0.997839">
101
</page>
<bodyText confidence="0.984236384615385">
its frequency in a large corpus. An increment in the
frequency of a concept leads to an increment in the
frequency of all its ancestors. Due to the frequency
percolation, concepts with a small depth tend to ac-
cumulate higher counts than those deeper in the hi-
erarchy (note the difference in depth:
). Thus, we expect the informa-
tion content of a concept to be higher than its an-
cestors, i.e., a concept is more semantically specific
than its ancestors, which is captured by the use of
the negative function in the definition of IC.
The transformed distance is distorted accordingly
( ).
</bodyText>
<subsectionHeader confidence="0.994159">
4.4 Junction Selection
</subsectionHeader>
<bodyText confidence="0.999922818181818">
Selection of junction nodes is a key component of
the network transformation. Trivially, a junction
consisting of profile nodes yields a network equiva-
lent to the complete bipartite network. The key is to
select a junction that is considerably smaller in size
than its corresponding profile, hence, cutting down
the number of edges generated, which results in sig-
nificant savings in complexity.
Note that there is a tradeoff between the over-
all computational efficiency and the similarity be-
tween the transformed network and the complete bi-
partite network. The closer the junctions are to the
corresponding profiles, the closer the transformed
network resembles the complete bipartite network.
Though the distance calculation is more accurate,
such a network is also more expensive to process.
On the other hand, there are fewer nodes in a junc-
tion as it approaches the root level, but there is more
distortion in the transformed concept-to-concept dis-
tance. Clearly, it is important to balance the two fac-
tors.
Selecting junction nodes involves finding a
smaller set of ancestor nodes representing the pro-
file nodes in a hierarchy. In other words, the junc-
tion can be viewed as an alternative representation
which is a generalization of the profile nodes. In
addition to the profile nodes, the junction nodes are
also included in the transformed network. They may
provide extra information about the corresponding
context.
Finding a generalization of a profile is explored in
the works of Clark and Weir (2002) and Li and Abe
(1998). Unfortunately, the complexity of these algo-
rithms is quadratic (the former) or cubic (the latter)
in the number of nodes in a network, which is unac-
ceptably expensive for our transformation method.
Note that to ensure every profile node has an ances-
tor node in the junction, the selection process has a
linear lower bound. To keep the cost low, it is best
to keep a linear complexity for the junction selection
process. However, if this is not possible, it should
be significantly less expensive than a quadratic com-
plexity. We will empirically explore the process fur-
ther in section 5.3.
</bodyText>
<sectionHeader confidence="0.963219" genericHeader="method">
5 Context Comparison
</sectionHeader>
<bodyText confidence="0.9999691">
As alluded to earlier, our network flow method pro-
vides an alternative to a purely distributional and
non-graphical approach to context comparison. In
this paper, we will test both variants of our method
(with or without the transformation in section 4) in
a name disambiguation task in which the context
words within a small window surrounding the am-
biguous words are compared. Our preliminary anal-
ysis shows that our general network flow framework
is robust and efficient.
</bodyText>
<subsectionHeader confidence="0.992561">
5.1 Name Disambiguation
</subsectionHeader>
<bodyText confidence="0.999997777777778">
The goal for name disambiguation is to classify each
ambiguous instance on the basis of its surrounding
context. One approach is to use an unsupervised
method such as clustering. This involves making a
large number of pairwise comparisons between in-
dividual contexts. Given that there is an overhead
to incorporating ontological information, our net-
work flow method does not compute distances as ef-
ficiently as calculating a purely arithmetic distance
such as cosine or Euclidean distance. Our alterna-
tive approach is to use minimal training data. Us-
ing a handful of contexts, we can build a “gold stan-
dard” profile for each sense of an ambiguous name
by using the context words of a small number of
instances. We then compare the context profile of
each instance to the gold standards. Each instance is
given the label of the gold standard profile to which
its context profile is the closest.
</bodyText>
<subsectionHeader confidence="0.997069">
5.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999623333333333">
In our name disambiguation experiment, we use the
data collected by Pedersen et al. (2005) for their
name discrimination task. This data is taken from
</bodyText>
<page confidence="0.992964">
102
</page>
<table confidence="0.999887375">
Name Pairs Baseline 200 (Full) 200 (Trans) 100 (Full) 100 (Trans)
Ronaldo/David Beckham 0.69 0.80 0.88 0.79 0.84
Tajik/RolfEkeus 0.74 0.97 0.99 0.98 0.99
Microsoft/IBM 0.59 0.73 0.75 0.73 0.71
Shimon Peres/Slobodan Milosevic 0.56 0.96 0.99 0.97 0.99
Jordan/Egyptian 0.54 0.77 0.76 0.74 0.76
Japan/France 0.51 0.75 0.82 0.75 0.83
Weighted Average 0.53 0.77 0.82 0.76 0.82
</table>
<tableCaption confidence="0.999374">
Table 1: Name disambiguation results (accuracy/F-measure) at a glance. The baseline is the relative frequency of the majority
</tableCaption>
<bodyText confidence="0.945936458333334">
name. “200” and “100” give the averaged results (over five different runs) using 200 and 100 randomly selected training instances
per ambiguous name. The weighted average is calculated based on the number of test instances per task. “Full” and “Trans” refer
to the results using the full network (pre-transformation) or the pared-down network (with transformation), respectively.
the Agence France Press English Service portion of
the GigaWord English corpus distributed by the Lin-
guistic Data Consortium. It consists of the contexts
of six pairs of names, including: the names of two
soccer players (Ronaldo and David Beckham); an
ethnic group and a diplomat (Tajik and Rolf Ekeus);
two companies (Microsoft and IBM); two politicians
(Shimon Peres and Slobodan Milosevic); a nation
and a nationality (Jordan and Egyptian); and two
countries (France and Japan). These name pairs are
selected by Pedersen et al. (2005) to reflect a range
of confusability between names.
Each pair of names serves as one of six name
disambiguation tasks. Each name instance con-
sists of a context window of 50 words (25 words
to the left and to the right of the target name),
with the target name obfuscated. For example, for
the task of distinguishing “David Beckham” and
“Ronaldo”, the target name in each instance be-
comes “David BeckhamRonaldo”. The goal is to
recover the correct target name in each instance.
</bodyText>
<subsectionHeader confidence="0.995967">
5.3 Junction Selection
</subsectionHeader>
<bodyText confidence="0.999977571428571">
We reported earlier that a complete bipartite graph
with 900 nodes is too expensive to process. Our
first attempt is to select a junction on the basis of
the number of nodes it contains. Here, the junctions
we select are simple to find by taking a top-down ap-
proach. We start at the top nine root nodes of Word-
Net (nodes of zero depth) and proceed downwards.
We limit the search within the top two levels because
the second level consists of 158 nodes, while the fol-
lowing level consists of 1307 nodes, which, clearly,
exceeds 900 nodes. Here, we select the junction
which consists of eight of the top root nodes (sil-
bings of entity) and the children of entity, given that
entity is semantically more general than its siblings.3
In our current experiment, we use Jiang and
Conrath’s distance for its ease of analysis. As
shown in section 4.3, only one term in the distance,
, is replaced because of the use of the
junction nodes. Any change in the performance (in
comparison to our method without the transforma-
tion) can be attributed to the distance distortion as
a result of this term being replaced. The analysis
of experimental results (next section) is made easy
because we can assess the goodness of the trans-
formation given the selected junction—a significant
degradation in performance is an indication that the
junction nodes should be brought closer to the pro-
file nodes, yielding a more precise distance.
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="evaluation">
6 Results and Analysis
</sectionHeader>
<bodyText confidence="0.925712904761905">
To compare the two variants of our method, we
perform our name disambiguation experiment us-
ing 100 and 200 training instances per ambiguous
name to create the gold standard profiles. See Ta-
ble 1 for the results. Comparing the results using
the full network and the transformed network, ob-
serve that there is very little performance degrada-
tion; in fact, in most cases, there is an increase in
accuracy (the difference is significant, paired t-test
with ).
Distance Transformation In Jiang and Conrath’s
formulation, the network transformation replaces
the term with
3Note that the complexity of this selection process is linear,
since all profile nodes must be examined to ensure they have an
ancestor in the junction; any profile node of which no junction
node is an ancestor is added to the junction. This process can
only be avoided by using junction nodes of zero depth exclu-
sively.
,
where is some common ancestor of and
</bodyText>
<page confidence="0.983494">
103
</page>
<bodyText confidence="0.999974944444444">
, whose depth is small. Junction nodes with a small
depth distort the distance more than those with a
larger depth. Surprisingly, our experiment indicates
that using such nodes produces equally good or bet-
ter performance. This suggests that selecting a junc-
tion with a larger depth, at least for the data in this
task, is not necessary.
Speed Improvement In comparison to our re-
ported running time on the pre-transformation net-
work (120 comparisons running for 10 days), on
the same machine, making 12,000 comparisons can
now be accomplished within two hours. In terms of
complexity, if we have profile nodes and junc-
tion nodes, the number of edges to be processed is
. Given that our junctions have signif-
icantly fewer nodes than the original profiles, the
running time is significantly less than quadratic in
the number of profile nodes.
</bodyText>
<sectionHeader confidence="0.99954" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999998441860465">
We have given an overview of our network flow for-
malism which seamlessly combines distributional
and ontological information. Given a suitable on-
tology, a context vector of word frequencies can
be transformed into a context profile—a frequency
distribution over the concepts in the ontology. In
contrast to traditional non-graphical approaches to
measuring only the distributional distance between
context vectors, we provide a graphical formalism
which incorporates both the semantic distance of the
component nodes as well as the distributional differ-
ences between the context profiles. By taking advan-
tage of the graphical structure of an ontology, our
method allows a systematic and meaningful way of
abstracting over words in a context, and by exten-
sion, a meaningful way of comparing contexts.
One concern with our method in its pre-
transformation form is its inability to incorporate
sophisticated concept-to-concept semantic distances
efficiently. To remedy this, we propose a novel tech-
nique that mimics the structure of the more compu-
tationally intensive network. Our preliminary eval-
uation shows that the transformation does not ham-
per the method’s ability to make fine-grained seman-
tic distinctions, and the computational complexity is
drastically reduced as well. Generally, our network
flow method presents a highly competitive alterna-
tive to a purely distributional and non-graphical ap-
proach.
In our on-going work, we are further exploring
how the choice of junction influences the perfor-
mance of different types of concept-to-concept se-
mantic distances. For example, would a bottom-up
junction selection approach (from the profile nodes
instead of from the root level) result in better per-
formance? In addition, we intend to examine the
graphical properties of the individual profiles as well
as the routes between the concepts across profiles
selected by our network flow methods. Such analy-
ses will help us gain insight into the strengths (and
weaknesses) of taking advantage of a graphical rep-
resentation of contexts as well as treating an ontol-
ogy as a metric space for context comparisons.
</bodyText>
<sectionHeader confidence="0.999435" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999542621621622">
Budanitsky, A. and Hirst, G. (2006). Evaluating WordNet-based
measures of semantic distance. Computational Linguistics.
To appear.
Clark, S. and Weir, D. (2002). Class-based probability estima-
tion using a semantic hierarchy. Computational Linguistics,
28(2):187–206.
Jiang, J. and Conrath, D. (1997). Semantic similarity based on
corpus statistics and lexical taxonomy. In Proceedings on
the International Conference on Research in Computational
Linguistics, pages 19–33.
Li, H. and Abe, N. (1998). Word clustering and disambiguation
based on co-occurrence data. In Proceedings of COLING-
ACL 1998, pages 749–755.
Lin, D. (1998). An information-theoretic definition of similar-
ity. In Proceedings ofInternational Conference on Machine
Learning.
Mihalcea, R. (2006). Random walks on text structures. In Pro-
ceedings of CICLing 2006, pages 249–262.
Miller, G. A. and Charles, W. G. (1991). Contextual correlates
of semantic similarity. Language and Cognitive Processes,
6(1):1–28.
Navigli, R. and Velardi, P. (2005). Structural semantic inter-
connections: A knowledge-based approach to word sense
disambiguation. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 27(7).
Pedersen, T., Purandare, A., and Kulkarni, A. (2005). Name
discrimination by clustering similar context. In Proceedings
of the Sixth International Conference on Intelligent Text Pro-
cessing and Computational Linguistics.
Resnik, P. (1995). Using information content to evaluate se-
mantic similarity in a taxonomy. In Proceedings of the 14th
International Joint Conference on Artificial Intelligence.
Sch¨utze, H. (1998). Automatic word sense discrimination.
Computational Linguistics, 24(1):97–123.
Xu, W., Liu, X., and Gong, Y. (2003). Document clustering
based on non-negative matrix factorization. In Proceedings
of the 26th ACM SIGIR Conference.
</reference>
<page confidence="0.998781">
104
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.552743">
<title confidence="0.999696">Context Comparison as a Minimum Cost Flow Problem</title>
<author confidence="0.999507">Vivian Tsang</author>
<author confidence="0.999507">Suzanne</author>
<affiliation confidence="0.997885">Department of Computer University of</affiliation>
<abstract confidence="0.978463142857143">vyctsang,suzanne @cs.utoronto.ca Abstract Comparing word contexts is a key component of many NLP tasks, but rarely is it used in conjunction with additional ontological knowledge. One problem is that the amount of overhead required can be high. In this paper, we provide a graphical method which easily combines an ontology with contextual information. We take advantage of the intrinsic graphical structure of an ontology for representing a context. In addition, we turn the ontology into a metric space, such that subgraphs within it, which represent contexts, can be compared. We develop two variants of our graphical method for comparing contexts. Our analysis indicates that our method performs the comparison efficiently and offers a competitive alternative to non-graphical methods.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNet-based measures of semantic distance. Computational Linguistics.</title>
<date>2006</date>
<note>To appear.</note>
<contexts>
<context position="7601" citStr="Budanitsky and Hirst (2006)" startWordPosition="1257" endWordPosition="1260">the routes between nodes is determined by a semantic distance measure defined over any two nodes in the ontology. Now, as in the grocery delivery domain, the goal is to find the MCF from supply to demand. We can treat any ontology as the transport network. A relation (such as hyponymy) between two concepts and is represented by an edge , and the cost on each edge can be defined as the semantic distance between the two concepts. This semantic distance can be as simple as the number of edges separating the concepts, or more sophisticated, such as Lin’s (1998) information-theoretic measure. (See Budanitsky and Hirst (2006) for a survey of such measures). Numerous methods are possible for converting the word frequency vector of a context to a concept frequency vector (i.e., a context profile). One simple method is to transfer each element in the word vector (i.e., the frequency of each word) to the corresponding concepts in the ontology, resulting in a vector of concept frequencies. In this paper, we have chosen a uniform distribution of word frequency counts among concepts, instead of a weighted distribution towards the relevant concepts for a particular text. Since we wish to evaluate the strength of our metho</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>Budanitsky, A. and Hirst, G. (2006). Evaluating WordNet-based measures of semantic distance. Computational Linguistics. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>D Weir</author>
</authors>
<title>Class-based probability estimation using a semantic hierarchy.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>2</issue>
<contexts>
<context position="8446" citStr="Clark and Weir (2002)" startWordPosition="1397" endWordPosition="1400">he word vector (i.e., the frequency of each word) to the corresponding concepts in the ontology, resulting in a vector of concept frequencies. In this paper, we have chosen a uniform distribution of word frequency counts among concepts, instead of a weighted distribution towards the relevant concepts for a particular text. Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or classbased approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). To calculate the distance between two profiles, we need to cast one profile as the supply ( ) and the other as the demand ( ). Note that our distance is symmetric, so the choice of the supply and the demand is arbitrary. Next, we must determine the value of at each concept node ; this is just the difference between the (normalized) supply frequency and demand frequency (4) This formula yields the net supply/demand, , at node. Recall that our goal is to transport all the supply to meet the demand—the final step is to determine the cheapest routes between and such that the constraints in (2) a</context>
<context position="20437" citStr="Clark and Weir (2002)" startWordPosition="3433" endWordPosition="3436">e is more distortion in the transformed concept-to-concept distance. Clearly, it is important to balance the two factors. Selecting junction nodes involves finding a smaller set of ancestor nodes representing the profile nodes in a hierarchy. In other words, the junction can be viewed as an alternative representation which is a generalization of the profile nodes. In addition to the profile nodes, the junction nodes are also included in the transformed network. They may provide extra information about the corresponding context. Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). Unfortunately, the complexity of these algorithms is quadratic (the former) or cubic (the latter) in the number of nodes in a network, which is unacceptably expensive for our transformation method. Note that to ensure every profile node has an ancestor node in the junction, the selection process has a linear lower bound. To keep the cost low, it is best to keep a linear complexity for the junction selection process. However, if this is not possible, it should be significantly less expensive than a quadratic complexity. We will empirically explore the process further in </context>
</contexts>
<marker>Clark, Weir, 2002</marker>
<rawString>Clark, S. and Weir, D. (2002). Class-based probability estimation using a semantic hierarchy. Computational Linguistics, 28(2):187–206.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Jiang</author>
<author>D Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings on the International Conference on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="14203" citStr="Jiang and Conrath (1997)" startWordPosition="2372" endWordPosition="2376">.g., WordNet, Gene Ontology, UMLS), calculating the distance between two concept nodes usually involves travelling “up” and “down” the hierarchy. The simplest route is a single hop from a child to its parent or vice versa. Generally, travelling from one node to another node consists of an A-shaped path ascending from node to a common ancestor of and , and then descending to node . Interestingly, our description of the A-shaped path matches the design of a number of concept-toconcept distances. For example, distances that incorporate Resnik’s (1995) information content (IC), , such as those of Jiang and Conrath (1997) and Lin (1998), consider both the (lowest) common ancestor as well as the two nodes of interest in their calculation. The complete bipartite graph considered in section 3.2 directly connects each node s in profile to node in profile , eliminating the typical Ashaped path in an ontology. This structure solves the 100 non-additivity issue, by generating an edge with the exact concept-to-concept distance for each potential node comparison, but, as noted above, is too inefficient. Our solution here is to construct a network that uses the idea of a pared-down A-shaped path to mostly avoid non-addi</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jiang, J. and Conrath, D. (1997). Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings on the International Conference on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Li</author>
<author>N Abe</author>
</authors>
<title>Word clustering and disambiguation based on co-occurrence data.</title>
<date>1998</date>
<booktitle>In Proceedings of COLINGACL</booktitle>
<pages>749--755</pages>
<contexts>
<context position="8420" citStr="Li and Abe (1998)" startWordPosition="1392" endWordPosition="1395">sfer each element in the word vector (i.e., the frequency of each word) to the corresponding concepts in the ontology, resulting in a vector of concept frequencies. In this paper, we have chosen a uniform distribution of word frequency counts among concepts, instead of a weighted distribution towards the relevant concepts for a particular text. Since we wish to evaluate the strength of our method alone without any additional NLP effort, we bypass the issue of approximating the true distribution of the concepts via word sense disambiguation or classbased approximation methods, such as those by Li and Abe (1998) and Clark and Weir (2002). To calculate the distance between two profiles, we need to cast one profile as the supply ( ) and the other as the demand ( ). Note that our distance is symmetric, so the choice of the supply and the demand is arbitrary. Next, we must determine the value of at each concept node ; this is just the difference between the (normalized) supply frequency and demand frequency (4) This formula yields the net supply/demand, , at node. Recall that our goal is to transport all the supply to meet the demand—the final step is to determine the cheapest routes between and such tha</context>
<context position="20459" citStr="Li and Abe (1998)" startWordPosition="3438" endWordPosition="3441">e transformed concept-to-concept distance. Clearly, it is important to balance the two factors. Selecting junction nodes involves finding a smaller set of ancestor nodes representing the profile nodes in a hierarchy. In other words, the junction can be viewed as an alternative representation which is a generalization of the profile nodes. In addition to the profile nodes, the junction nodes are also included in the transformed network. They may provide extra information about the corresponding context. Finding a generalization of a profile is explored in the works of Clark and Weir (2002) and Li and Abe (1998). Unfortunately, the complexity of these algorithms is quadratic (the former) or cubic (the latter) in the number of nodes in a network, which is unacceptably expensive for our transformation method. Note that to ensure every profile node has an ancestor node in the junction, the selection process has a linear lower bound. To keep the cost low, it is best to keep a linear complexity for the junction selection process. However, if this is not possible, it should be significantly less expensive than a quadratic complexity. We will empirically explore the process further in section 5.3. 5 Context</context>
</contexts>
<marker>Li, Abe, 1998</marker>
<rawString>Li, H. and Abe, N. (1998). Word clustering and disambiguation based on co-occurrence data. In Proceedings of COLINGACL 1998, pages 749–755.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings ofInternational Conference on Machine Learning.</booktitle>
<contexts>
<context position="10750" citStr="Lin, 1998" startWordPosition="1795" endWordPosition="1796">he demand; i.e., at each node, the cheapest outgoing edge is selected. The assumption is that the concept-to-concept distance function is additive. Mathematically, for any path from node to node ,, where and , the distance between nodes and is the sum of the distance of the edges along the path: (5) The additivity of a concept-to-concept distance entails that selecting the cheapest edge at each step (i.e., locally) yields the overall cheapest set of routes (i.e., globally). Note that some of the most successful concept-to-concept distances proposed in the CL literature are non-additive (e.g., Lin, 1998; Resnik, 1995). This poses a problem in solving our network flow problem—the global distance between any concepts, and , cannot be correctly determined by the greedy method. 3.2 Constructing an Equivalent Bipartite Network The issue of non-additive distances can be addressed in the following way. We map the relevant portion : 99 Figure 2: An illustration of the transformations (left to right) from the original network (a) to the bipartite network (b), and finally, to the network produced by our transformation (c), given two profiles S and D. Nodes labelled with either “S” or “D” belong to the</context>
<context position="14218" citStr="Lin (1998)" startWordPosition="2378" endWordPosition="2379">UMLS), calculating the distance between two concept nodes usually involves travelling “up” and “down” the hierarchy. The simplest route is a single hop from a child to its parent or vice versa. Generally, travelling from one node to another node consists of an A-shaped path ascending from node to a common ancestor of and , and then descending to node . Interestingly, our description of the A-shaped path matches the design of a number of concept-toconcept distances. For example, distances that incorporate Resnik’s (1995) information content (IC), , such as those of Jiang and Conrath (1997) and Lin (1998), consider both the (lowest) common ancestor as well as the two nodes of interest in their calculation. The complete bipartite graph considered in section 3.2 directly connects each node s in profile to node in profile , eliminating the typical Ashaped path in an ontology. This structure solves the 100 non-additivity issue, by generating an edge with the exact concept-to-concept distance for each potential node comparison, but, as noted above, is too inefficient. Our solution here is to construct a network that uses the idea of a pared-down A-shaped path to mostly avoid non-additivity, but wit</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). An information-theoretic definition of similarity. In Proceedings ofInternational Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mihalcea</author>
</authors>
<title>Random walks on text structures.</title>
<date>2006</date>
<booktitle>In Proceedings of CICLing</booktitle>
<pages>249--262</pages>
<contexts>
<context position="2442" citStr="Mihalcea (2006)" startWordPosition="392" endWordPosition="393">ance between context vectors. The difference in the frequency characteristics of contexts is used as an indicator of the semantic distance between them. We present a graphical alternative that combines both distributional and ontological knowledge. We begin with the use of a different context representation that allows easy incorporation of ontological information. Treating an ontology as a network, we can represent a context as a set of nodes in the network (i.e., concepts in the ontology), each with a weight (i.e., frequency). To contrast our work with that of Navigli and Velardi (2005) and Mihalcea (2006), the goal is not merely to provide a graphical representation for a context in which the relevant concepts are connected. Rather, contexts are treated as weighted subgraphs within a larger graph in which they are connected via a set of paths. By incorporating the semantic distance between individual concepts, the graph (representing the ontology) becomes a metric space in which we can measure the distance between subgraphs (representing the contexts to be compared). More specifically, measuring the distance between two contexts can be viewed as solving a minimum cost flow (MCF) problem by cal</context>
</contexts>
<marker>Mihalcea, 2006</marker>
<rawString>Mihalcea, R. (2006). Random walks on text structures. In Proceedings of CICLing 2006, pages 249–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>W G Charles</author>
</authors>
<title>Contextual correlates of semantic similarity.</title>
<date>1991</date>
<booktitle>Language and Cognitive Processes,</booktitle>
<pages>6--1</pages>
<contexts>
<context position="1253" citStr="Miller and Charles, 1991" startWordPosition="196" endWordPosition="199">y into a metric space, such that subgraphs within it, which represent contexts, can be compared. We develop two variants of our graphical method for comparing contexts. Our analysis indicates that our method performs the comparison efficiently and offers a competitive alternative to non-graphical methods. 1 Introduction Many natural language problems can be cast as a problem of comparing “contexts” (units of text). For example, the local context of a word can be used to resolve its ambiguity (e.g., Sch¨utze, 1998), assuming that words used in similar contexts are closely related semantically (Miller and Charles, 1991). Extending the meaning of context, the content of a document may reveal which document class(es) it belongs to (e.g., Xu et al., 2003). In any application, once a sensible view of context is formulated, the next step is to choose a representation that makes comparisons possible. For example, in word sense disambiguation, a context of an ambiguous instance can be represented as a vector of the frequencies of words surrounding it. Until recently, the dominant approach has been a non-graphical one— context comparison is reduced to a task of measuring distributional distance between context vecto</context>
</contexts>
<marker>Miller, Charles, 1991</marker>
<rawString>Miller, G. A. and Charles, W. G. (1991). Contextual correlates of semantic similarity. Language and Cognitive Processes, 6(1):1–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>P Velardi</author>
</authors>
<title>Structural semantic interconnections: A knowledge-based approach to word sense disambiguation.</title>
<date>2005</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>27</volume>
<issue>7</issue>
<contexts>
<context position="2422" citStr="Navigli and Velardi (2005)" startWordPosition="387" endWordPosition="390">f measuring distributional distance between context vectors. The difference in the frequency characteristics of contexts is used as an indicator of the semantic distance between them. We present a graphical alternative that combines both distributional and ontological knowledge. We begin with the use of a different context representation that allows easy incorporation of ontological information. Treating an ontology as a network, we can represent a context as a set of nodes in the network (i.e., concepts in the ontology), each with a weight (i.e., frequency). To contrast our work with that of Navigli and Velardi (2005) and Mihalcea (2006), the goal is not merely to provide a graphical representation for a context in which the relevant concepts are connected. Rather, contexts are treated as weighted subgraphs within a larger graph in which they are connected via a set of paths. By incorporating the semantic distance between individual concepts, the graph (representing the ontology) becomes a metric space in which we can measure the distance between subgraphs (representing the contexts to be compared). More specifically, measuring the distance between two contexts can be viewed as solving a minimum cost flow </context>
</contexts>
<marker>Navigli, Velardi, 2005</marker>
<rawString>Navigli, R. and Velardi, P. (2005). Structural semantic interconnections: A knowledge-based approach to word sense disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 27(7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>A Purandare</author>
<author>A Kulkarni</author>
</authors>
<title>Name discrimination by clustering similar context.</title>
<date>2005</date>
<booktitle>In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics.</booktitle>
<contexts>
<context position="22577" citStr="Pedersen et al. (2005)" startWordPosition="3790" endWordPosition="3793">stances as efficiently as calculating a purely arithmetic distance such as cosine or Euclidean distance. Our alternative approach is to use minimal training data. Using a handful of contexts, we can build a “gold standard” profile for each sense of an ambiguous name by using the context words of a small number of instances. We then compare the context profile of each instance to the gold standards. Each instance is given the label of the gold standard profile to which its context profile is the closest. 5.2 Experimental Setup In our name disambiguation experiment, we use the data collected by Pedersen et al. (2005) for their name discrimination task. This data is taken from 102 Name Pairs Baseline 200 (Full) 200 (Trans) 100 (Full) 100 (Trans) Ronaldo/David Beckham 0.69 0.80 0.88 0.79 0.84 Tajik/RolfEkeus 0.74 0.97 0.99 0.98 0.99 Microsoft/IBM 0.59 0.73 0.75 0.73 0.71 Shimon Peres/Slobodan Milosevic 0.56 0.96 0.99 0.97 0.99 Jordan/Egyptian 0.54 0.77 0.76 0.74 0.76 Japan/France 0.51 0.75 0.82 0.75 0.83 Weighted Average 0.53 0.77 0.82 0.76 0.82 Table 1: Name disambiguation results (accuracy/F-measure) at a glance. The baseline is the relative frequency of the majority name. “200” and “100” give the average</context>
<context position="24056" citStr="Pedersen et al. (2005)" startWordPosition="4018" endWordPosition="4021"> (pre-transformation) or the pared-down network (with transformation), respectively. the Agence France Press English Service portion of the GigaWord English corpus distributed by the Linguistic Data Consortium. It consists of the contexts of six pairs of names, including: the names of two soccer players (Ronaldo and David Beckham); an ethnic group and a diplomat (Tajik and Rolf Ekeus); two companies (Microsoft and IBM); two politicians (Shimon Peres and Slobodan Milosevic); a nation and a nationality (Jordan and Egyptian); and two countries (France and Japan). These name pairs are selected by Pedersen et al. (2005) to reflect a range of confusability between names. Each pair of names serves as one of six name disambiguation tasks. Each name instance consists of a context window of 50 words (25 words to the left and to the right of the target name), with the target name obfuscated. For example, for the task of distinguishing “David Beckham” and “Ronaldo”, the target name in each instance becomes “David BeckhamRonaldo”. The goal is to recover the correct target name in each instance. 5.3 Junction Selection We reported earlier that a complete bipartite graph with 900 nodes is too expensive to process. Our </context>
</contexts>
<marker>Pedersen, Purandare, Kulkarni, 2005</marker>
<rawString>Pedersen, T., Purandare, A., and Kulkarni, A. (2005). Name discrimination by clustering similar context. In Proceedings of the Sixth International Conference on Intelligent Text Processing and Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Resnik</author>
</authors>
<title>Using information content to evaluate semantic similarity in a taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="10765" citStr="Resnik, 1995" startWordPosition="1797" endWordPosition="1798">i.e., at each node, the cheapest outgoing edge is selected. The assumption is that the concept-to-concept distance function is additive. Mathematically, for any path from node to node ,, where and , the distance between nodes and is the sum of the distance of the edges along the path: (5) The additivity of a concept-to-concept distance entails that selecting the cheapest edge at each step (i.e., locally) yields the overall cheapest set of routes (i.e., globally). Note that some of the most successful concept-to-concept distances proposed in the CL literature are non-additive (e.g., Lin, 1998; Resnik, 1995). This poses a problem in solving our network flow problem—the global distance between any concepts, and , cannot be correctly determined by the greedy method. 3.2 Constructing an Equivalent Bipartite Network The issue of non-additive distances can be addressed in the following way. We map the relevant portion : 99 Figure 2: An illustration of the transformations (left to right) from the original network (a) to the bipartite network (b), and finally, to the network produced by our transformation (c), given two profiles S and D. Nodes labelled with either “S” or “D” belong to the corresponding </context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Resnik, P. (1995). Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>Sch¨utze, H. (1998). Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Xu</author>
<author>X Liu</author>
<author>Y Gong</author>
</authors>
<title>Document clustering based on non-negative matrix factorization.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th ACM SIGIR Conference.</booktitle>
<contexts>
<context position="1388" citStr="Xu et al., 2003" startWordPosition="220" endWordPosition="223"> for comparing contexts. Our analysis indicates that our method performs the comparison efficiently and offers a competitive alternative to non-graphical methods. 1 Introduction Many natural language problems can be cast as a problem of comparing “contexts” (units of text). For example, the local context of a word can be used to resolve its ambiguity (e.g., Sch¨utze, 1998), assuming that words used in similar contexts are closely related semantically (Miller and Charles, 1991). Extending the meaning of context, the content of a document may reveal which document class(es) it belongs to (e.g., Xu et al., 2003). In any application, once a sensible view of context is formulated, the next step is to choose a representation that makes comparisons possible. For example, in word sense disambiguation, a context of an ambiguous instance can be represented as a vector of the frequencies of words surrounding it. Until recently, the dominant approach has been a non-graphical one— context comparison is reduced to a task of measuring distributional distance between context vectors. The difference in the frequency characteristics of contexts is used as an indicator of the semantic distance between them. We prese</context>
</contexts>
<marker>Xu, Liu, Gong, 2003</marker>
<rawString>Xu, W., Liu, X., and Gong, Y. (2003). Document clustering based on non-negative matrix factorization. In Proceedings of the 26th ACM SIGIR Conference.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>