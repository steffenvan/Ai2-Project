<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016299">
<title confidence="0.869969">
SemEval-2013 Task 12: Multilingual Word Sense Disambiguation
</title>
<author confidence="0.872868">
Roberto Navigli, David Jurgens and Daniele Vannella
</author>
<affiliation confidence="0.742375">
Dipartimento di Informatica
</affiliation>
<address confidence="0.753825">
Sapienza Universit`a di Roma
Viale Regina Elena, 295 – 00161 Roma Italy
</address>
<email confidence="0.998464">
{navigli,jurgens,vannella}@di.uniroma1.it
</email>
<sectionHeader confidence="0.998595" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999908">
This paper presents the SemEval-2013 task on
multilingual Word Sense Disambiguation. We
describe our experience in producing a mul-
tilingual sense-annotated corpus for the task.
The corpus is tagged with BabelNet 1.1.1,
a freely-available multilingual encyclopedic
dictionary and, as a byproduct, WordNet 3.0
and the Wikipedia sense inventory. We present
and analyze the results of participating sys-
tems, and discuss future directions.
</bodyText>
<sectionHeader confidence="0.999501" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998762867924528">
Word Sense Disambiguation (WSD), the task of au-
tomatically assigning predefined meanings to words
occurring in context, is a fundamental task in com-
putational lexical semantics (Navigli, 2009; Navigli,
2012). Several Senseval and SemEval tasks have
been organized in the past to study the performance
and limits of disambiguation systems and, even
more importantly, disambiguation settings. While
an ad-hoc sense inventory was originally chosen for
the first Senseval edition (Kilgarriff, 1998; Kilgarriff
and Palmer, 2000), later tasks (Edmonds and Cot-
ton, 2001; Snyder and Palmer, 2004; Mihalcea et
al., 2004) focused on WordNet (Miller et al., 1990;
Fellbaum, 1998) as a sense inventory. In 2007 the
issue of the fine sense granularity of WordNet was
addressed in two different SemEval disambiguation
tasks, leading to the beneficial creation of coarser-
grained sense inventories from WordNet itself (Nav-
igli et al., 2007) and from OntoNotes (Pradhan et al.,
2007).
In recent years, with the exponential growth of
the Web and, consequently, the increase of non-
English speaking surfers, we have witnessed an up-
surge of interest in multilinguality. SemEval-2010
tasks on cross-lingual Word Sense Disambiguation
(Lefever and Hoste, 2010) and cross-lingual lexi-
cal substitution (Mihalcea et al., 2010) were orga-
nized. While these tasks addressed the multilin-
gual aspect of sense-level text understanding, they
departed from the traditional WSD paradigm, i.e.,
the automatic assignment of senses from an existing
inventory, and instead focused on lexical substitu-
tion (McCarthy and Navigli, 2009). The main factor
hampering traditional WSD from going multilingual
was the lack of a freely-available large-scale multi-
lingual dictionary.
The recent availability of huge collaboratively-
built repositories of knowledge such as Wikipedia
has enabled the automated creation of large-scale
lexical knowledge resources (Hovy et al., 2013).
Over the past few years, a wide-coverage multi-
lingual “encyclopedic” dictionary, called BabelNet,
has been developed (Navigli and Ponzetto, 2012a).
BabelNet1 brings together WordNet and Wikipedia
and provides a multilingual sense inventory that cur-
rently covers 6 languages. We therefore decided to
put the BabelNet 1.1.1 sense inventory to the test
and organize a traditional Word Sense Disambigua-
tion task on a given English test set translated into 4
other languages (namely, French, German, Spanish
and Italian). Not only does BabelNet enable mul-
tilinguality, but it also provides coverage for both
lexicographic (e.g., apple as fruit) and encyclopedic
</bodyText>
<footnote confidence="0.995646">
1http://babelnet.org
</footnote>
<page confidence="0.941543">
222
</page>
<bodyText confidence="0.7998368">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 222–231, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
meanings (e.g., Apple Inc. as company). In this pa-
per we describe our task and disambiguation dataset
and report on the system results.
</bodyText>
<sectionHeader confidence="0.990626" genericHeader="method">
2 Task Setup
</sectionHeader>
<bodyText confidence="0.999941777777778">
The task required participating systems to annotate
nouns in a test corpus with the most appropriate
sense from the BabelNet sense inventory or, alter-
natively, from two main subsets of it, namely the
WordNet or Wikipedia sense inventories. In contrast
to previous all-words WSD tasks we did not focus
on the other three open classes (i.e., verbs, adjec-
tives and adverbs) since BabelNet does not currently
provide non-English coverage for them.
</bodyText>
<subsectionHeader confidence="0.992919">
2.1 Test Corpus
</subsectionHeader>
<bodyText confidence="0.999983428571429">
The test set consisted of 13 articles obtained from
the datasets available from the 2010, 2011 and 2012
editions of the workshop on Statistical Machine
Translation (WSMT).2 The articles cover different
domains, ranging from sports to financial news.
The same article was available in 4 different lan-
guages (English, French, German and Spanish). In
order to cover Italian, an Italian native speaker man-
ually translated each article from English into Ital-
ian, with the support of an English mother tongue
advisor. In Table 1 we show for each language the
number of words of running text, together with the
number of multiword expressions and named enti-
ties annotated, from the 13 articles.
</bodyText>
<subsectionHeader confidence="0.996386">
2.2 Sense Inventories
2.2.1 BabelNet inventory
</subsectionHeader>
<bodyText confidence="0.999897692307692">
To semantically annotate all the single- and multi-
word expressions, as well as the named entities, oc-
curring in our test corpus we used BabelNet 1.1.1
(Navigli and Ponzetto, 2012a). BabelNet is a mul-
tilingual “encyclopedic dictionary” and a semantic
network currently covering 6 languages, namely:
English, Catalan, French, German, Italian and Span-
ish. BabelNet is obtained as a result of a novel inte-
gration and enrichment methodology. This resource
is created by linking the largest multilingual Web en-
cyclopedia – i.e., Wikipedia – to the most popular
computational lexicon – i.e., WordNet 3.0. The inte-
gration is performed via an automatic mapping and
</bodyText>
<footnote confidence="0.840435">
2http://www.statmt.org/wmt12/
</footnote>
<bodyText confidence="0.999144277777778">
by filling in lexical gaps in resource-poor languages
with the aid of Machine Translation (MT).
Its lexicon includes lemmas which denote both
lexicographic meanings (e.g., balloon) and ency-
clopedic ones (e.g., Montgolfier brothers). The
basic meaning unit in BabelNet is the Babel
synset, modeled after the WordNet synset (Miller
et al., 1990; Fellbaum, 1998). A Babel synset
is a set of synonyms which express a concept
in different languages. For instance, { Globus
aerost`aticCA, BalloonEN, A´erostationFR, BallonDE,
Pallone aerostaticoIT, ... , Globo aerost´aticoES } is
the Babel synset for the balloon aerostat, where the
language of each synonym is provided as a subscript
label. Thanks to their multilingual nature, we were
able to use Babel synsets as interlingual concept tags
for nouns occurring within text written in any of the
covered languages.
</bodyText>
<subsubsectionHeader confidence="0.753653">
2.2.2 WordNet and Wikipedia inventories
</subsubsectionHeader>
<bodyText confidence="0.999987">
Since BabelNet 1.1.1 is a superset of the Word-
Net 3.0 and Wikipedia sense inventories,3 once text
is annotated with Babel synsets, it turns out to
be annotated also according to either WordNet or
Wikipedia, or both. In fact, in order to induce the
WordNet annotations, one can restrict to those lex-
ical items annotated with Babel synsets which con-
tain WordNet senses for the target lemma; similarly,
for Wikipedia, we restrict to those items tagged with
Babel synsets which contain Wikipedia pages for the
target lemma.
</bodyText>
<subsectionHeader confidence="0.999571">
2.3 BabelNet sense inventory validation
</subsectionHeader>
<bodyText confidence="0.999945461538461">
Because BabelNet is an automatic integration of
WordNet and Wikipedia, the resulting Babel synsets
may contain WordNet and Wikipedia entries about
different meanings of the same lemma. The under-
lying cause is a wrong mapping between the two
original resources. For instance, in BabelNet 1.1
the WordNet synset { arsenic, As, atomic number
33 } was mapped to the Wikipedia page AS (RO-
MAN COIN), and therefore the same Babel synset
mixed the two meanings.
In order to avoid an inconsistent semantic tag-
ging of text, we decided to manually check all the
mappings in BabelNet 1.1 between Wikipedia pages
</bodyText>
<footnote confidence="0.956276">
3For version 1.1.1 we used the English Wikipedia database
dump from October 1, 2012.
</footnote>
<page confidence="0.996424">
223
</page>
<table confidence="0.9998245">
Language Instances Single- Multiword Named Mean senses Mean senses
words expressions Entities per instance per lemma
BabelNet
English 1931 1604 127 200 1.02 1.09
French 1656 1389 89 176 1.05 1.15
German 1467 1267 21 176 1.00 1.05
Italian 1706 1454 211 41 1.22 1.27
Spanish 1481 1103 129 249 1.15 1.19
Wikipedia
English 1242 945 102 195 1.15 1.16
French 1039 790 72 175 1.18 1.14
German 1156 957 21 176 1.07 1.08
Italian 1977 869 85 41 1.20 1.18
Spanish 1103 758 107 248 1.11 1.10
WordNet
English 1644 1502 85 57 1.01 1.10
</table>
<tableCaption confidence="0.999312">
Table 1: Statistics for the sense annotations of the test set.
</tableCaption>
<bodyText confidence="0.999873875">
and WordNet senses involving lemmas in our En-
glish test set for the task. Overall, we identified 8306
synsets for 978 lemmas to be manually checked. We
recruited 8 annotators in our research group and as-
signed each lemma to two annotators. Each anno-
tator was instructed to check each Babel synset and
determine whether any of the following three opera-
tions was needed:
</bodyText>
<listItem confidence="0.990891875">
• Delete a mapping and separate the WordNet
sense from the Wikipedia page (like in the ar-
senic vs. AS (ROMAN COIN) example above);
• Add a mapping between a WordNet sense and a
Wikipedia page (formerly available as two sep-
arate Babel synsets);
• Merge two Babel synsets which express the
same concept.
</listItem>
<bodyText confidence="0.999419571428572">
After disagreement adjudication carried out by
the first author, the number of delete, add and merge
operations was 493, 203 and 43, respectively, for a
total of 739 operations (i.e., 8.8% of synsets cor-
rected). As a result of our validation of BabelNet
1.1, we obtained version 1.1.1, which is currently
available online.
</bodyText>
<subsectionHeader confidence="0.997452">
2.4 Sense Annotation
</subsectionHeader>
<bodyText confidence="0.999996208333333">
To ensure high quality annotations, the annotation
process was completed in three phases. Because
BabelNet is a superset of both the WordNet and
Wikipedia sense inventories, all annotators used the
BabelNet 1.1.1 sense inventory for their respective
language. These BabelNet annotations were then
projected into WordNet and Wikipedia senses. An-
notation was performed by one native speaker each
for English, French, German and Spanish and, for
Italian, by two native speakers who annotated dif-
ferent subsets of the corpus.
In the first phase, each annotator was instructed
to inspect each instance to check that (1) the lemma
was tagged with the correct part of speech, (2) lem-
mas were correctly annotated as named entity or
multiword expressions, and (3) the meaning of the
instance’s lemma had an associated sense in Ba-
belNet. Based on these criteria, annotators removed
dozens of instances from the original data.
In the second phase, each instance in the En-
glish dataset was annotated using BabelNet senses.
To reduce the time required for annotation in the
other languages, the sense annotations for the En-
glish dataset were then projected onto the other four
</bodyText>
<page confidence="0.995255">
224
</page>
<table confidence="0.99889">
Language Projected Valid Invalid
instances projections projections
French 1016 791 225
German 592 373 219
Italian 1029 774 255
Spanish 911 669 242
</table>
<tableCaption confidence="0.996408">
Table 2: Statistics when using the English sense an-
</tableCaption>
<bodyText confidence="0.997789517647059">
notations to project the correct sense of a lemma in
another language of the sentence-aligned test data.
languages using the sense translation API of Babel-
Net (Navigli and Ponzetto, 2012d). The projection
operated as follows, using the aligned sentences in
the English and non-English texts. For an instance
in the non-English text, all of the senses for that in-
stance’s lemma were compared with the sense an-
notations in the English sentence. If any of that
lemma’s senses was used in the English sentence,
then that sense was selected for the non-English
instance. The matching procedure operates at the
sentence-aligned level because the instances them-
selves are not aligned; i.e., different languages have
different numbers of instances per sentence, which
are potentially ordered differently due to language-
specific construction. Ultimately, this projection la-
beled approximately 50-70% of the instances in the
other four languages. Given the projected senses,
the annotators for the other four languages were then
asked to (1) correct the projected sense labels and
(2) annotate those still without senses.4 These anno-
tations were recorded in text in a stand-off file; no
further annotation tools were used.
The resulting sense projection proved highly use-
ful for selecting the correct sense. Table 2 shows
the number of corrections made by the annotators
to the projected senses, who changed only 22-37%
of the labels. While simple, the projection method
offers significant potential for generating good qual-
ity sense-annotated data from sentence-aligned mul-
tilingual text.
In the third phase, an independent annotator re-
viewed the labels for the high-frequency lemmas for
4During the second phase, annotators were also allowed
to add and remove instances that were missed during the first
phase, which resulted in small number of changes.
all languages to check for systematic errors and dis-
cuss possible changes to the labeling. This review
resulted in only a small number of changes to less
than 5% of the total instances, except for German
which had a slightly higher percentage of changes.
Table 1 summarizes the sense annotation statis-
tics for the test set. Annotators were allowed to use
multiple senses in the case of ambiguity, but en-
couraged to use a single sense whenever possible.
In rare cases, a lemma was annotated with senses
from a different lemma. For example, WordNet does
not contain a sense for “card” that corresponds to
the penalty card meaning (as used in sports such
as football). In contrast, BabelNet has a sense for
“penalty card” from Wikipedia which, however, is
not mapped to the lemma “card”. In such cases,
we add both the closest meaning from the original
lemma (e.g., the rectangual piece of paper sense in
WordNet) and the most suitable sense that may have
a different lemma form (e.g., PENALTY CARD).
Previous annotation studies have shown that,
when a fine-grained sense inventory is used, annota-
tors will often label ambiguous instances with multi-
ple senses if allowed (Erk and McCarthy, 2009; Jur-
gens and Klapaftis, 2013). Since BabelNet is a com-
bination of a fine-grained inventory (WordNet) and
contains additional senses from Wikipedia, we ana-
lyzed the average number of BabelNet sense anno-
tations per instance, shown in column six of Table 1.
Surprisingly, Table 1 suggests that the rate of mul-
tiple sense annotation varies significantly between
languages.
BabelNet may combine multiple Wikipedia pages
into a single BabelNet synset. As a result, when
Wikipedia is used as a sense inventory, instances are
annotated with all of the Wikipedia pages associated
with each BabelNet synset. Indeed, Table 1 shows a
markedly increased multi-sense annotation rate for
three languages when using Wikipedia.
As a second analysis, we considered the observed
level of polysemy for each of the unique lemmas.
The last column of Table 1 shows the average num-
ber of different senses seen for each lemma across
the test sets. In all languages, often only a single
sense of a lemma was used. Because the test set is
constructed based on topical documents, infrequent
lemmas mostly occurred within a single document
where they were used with a consistent interpreta-
</bodyText>
<page confidence="0.992458">
225
</page>
<bodyText confidence="0.999547">
tion. However, we note that in the case of lem-
mas that were only seen with a single sense, this
sense does not always correspond to the most fre-
quent sense as seen in SemCor.
</bodyText>
<sectionHeader confidence="0.999092" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999971967741935">
Task 12 uses the standard definitions of precision
and recall for WSD evaluation (see, e.g., (Navigli,
2009)). Precision measures the percentage of the
sense assignments provided by the system that are
identical to the gold standard; Recall measures the
percentage of instances that are correctly labeled by
the system. When a system provides sense labels
for all instances, precision and recall are equivalent.
Systems using BabelNet and WordNet senses are
compared against the Most Frequent Sense (MFS)
baseline obtained by using the WordNet most fre-
quent sense. For the Wikipedia sense inventory, we
constructed a pseudo-MFS baseline by selecting (1)
the Wikipedia page associated with the highest rank-
ing WordNet sense, as ranked by SemCor frequency,
or (2) when no synset for a lemma was associ-
ated with a WordNet sense, the first Wikipedia page
sorted using BabelNet’s ordering criteria, i.e., lexi-
cographic sorting. We note that, in the second case,
this procedure frequently selected the page with the
same name as the lemma itself. For instance, the
first sense of Dragon Ball is the cartoon with title
DRAGON BALL, followed by two films (DRAGON
BALL (1990 FILM) and DRAGON BALL EVOLU-
TION).
Systems were scored separately for each sense in-
ventory. We note that because the instances in each
test set are filtered to include only those that can
be labeled with the respective inventory, both the
Wikipedia and WordNet test sets are subsets of the
instances in the BabelNet test set.
</bodyText>
<sectionHeader confidence="0.968462" genericHeader="method">
4 Participating Systems
</sectionHeader>
<bodyText confidence="0.999902740740741">
Three teams submitted a total of seven systems for
the task, with at least one participant attempting
all of the sense inventory and language combina-
tions. Six systems participated in the WSD task
with BabelNet senses; two teams submitted four sys-
tems using WordNet senses; and one team submitted
three systems for Wikipedia-based senses. Notably,
all systems used graph-based approaches for sense
disambiguation, either using WordNet or BabelNet’s
synset graphs. We summarize the teams’ systems as
follows.
DAEBAK! DAEBAK! submitted one system
called PD (Peripheral Diversity) based on BabelNet
path indices from the BabelNet synset graph. Us-
ing a f5 sentence window around the target word,
a graph is constructed for all senses of co-occurring
lemmas following the procedure proposed by Nav-
igli and Lapata (2010). The final sense is selected
based on measuring connectivity to the synsets of
neighboring lemmas. The MFS is used as a backoff
strategy when no appropriate sense can be picked
out.
GETALP GETALP submitted three systems, two
for BabelNet and one for WordNet, all based on
the ant-colony algorithm of (Schwab et al., 2012),
which uses the sense inventory network structure
to identify paths connecting synsets of the target
lemma to the synsets of other lemmas in context.
The algorithm requires setting several parameters
for the weighting of the structure of the context-
based graph, which vary across the three systems.
The BN1 system optimizes its parameters from the
trial data, while the BN2 and WN1 systems are
completely unsupervised and optimize their param-
eters directly from the structure of the BabelNet and
WordNet graphs.
UMCC-DLSI UMCC-DLSI submitted three
systems based on the ISR-WN resource (Guti´errez
et al., 2011), which enriches the WordNet se-
mantic network using edges from multiple lexical
resources, such as WordNet Domains and the
eXtended WordNet. WSD was then performed
using the ISR-WN network in combination with
the algorithm of Guti´errez (2012), which is an
extension of the Personalized PageRank algorithm
for WSD (Agirre and Soroa, 2009) which includes
senses frequency. The algorithm requires initial-
izing the PageRank algorithm with a set of seed
synsets (vertices) in the network; this initialization
represents the key variation among UMCC’s three
approaches. The RUN-1 system performs WSD
using all noun instances from the sentence context.
In contrast, the RUN-2 works at the discourse level
and initializes the PageRank using the synsets of all
</bodyText>
<page confidence="0.99712">
226
</page>
<table confidence="0.998841625">
Team System English French German Italian Spanish
DAEBAK! PD 0.604 0.538 0.591 0.613 0.600
GETALP BN-1 0.263 0.261 0.404 0.324 -
GETALP BN-2 0.266 0.257 0.400 0.324 0.371
UMCC-DLSI RUN-1 0.677 0.605 0.618 0.657 0.705
UMCC-DLSI RUN-2 0.685 0.605 0.621 0.658 0.710
UMCC-DLSI RUN-3 0.680 - - - -
MFS 0.665 0.453 0.674 0.575 0.645
</table>
<tableCaption confidence="0.9943905">
Table 3: System performance, reported as F1, for all five languages in the test set when using BabelNet
senses. Top performing systems are marked in bold.
</tableCaption>
<table confidence="0.389332666666667">
nouns in the document. Finally, the RUN-3 system DAEBAK! PD UMCC-DLSI Run-2
initializes using all words in the sentence. GETALP BN-2
WSD F1
</table>
<sectionHeader confidence="0.998592" genericHeader="evaluation">
5 Results and Discussion
</sectionHeader>
<bodyText confidence="0.999855833333333">
All teams submitted at least one system using the
BabelNet inventory, shown in Table 3. The UMCC-
DLSI systems were consistently able to outperform
the MFS baseline (a notoriously hard-to-beat heuris-
tic) in all languages except German. Additionally,
the DAEBAK! system outperformed the MFS base-
line on French and Italian. The UMCC-DLSI RUN-
2 system performed the best for all languages. No-
tably, this system leverages the single-sense per dis-
course heuristic (Yarowsky, 1995), which uses the
same sense label for all occurrences of a lemma in a
document.
UMCC-DLSI submitted the only three sys-
tems to use Wikipedia-based senses. Table 4 shows
their performance. Of the three sense inventories,
Wikipedia had the most competitive MFS baseline,
scoring at least 0.694 on all languages. Notably,
the Wikipedia-based system has the lowest recall of
all systems. Despite having superior precision to the
MFS baseline, the low recall brought the resulting
F1 measure below the MFS.
Two teams submitted four total systems for Word-
Net, shown in Table 5. The UMCC-DLSI RUN-2
system was again the top-performing system, under-
scoring the benefit of using discourse information in
selecting senses. The other two UMCC-DLSI sys-
tems also surpassed the MFS baseline. Though still
performing worse than the MFS baseline, when us-
ing the WordNet sense graph, the GETALP system
sees a noticeable improvement of 0.14 over its per-
</bodyText>
<figure confidence="0.997768083333333">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
0 5 10 15 20 25 30 35 40 45
Number of senses for the instance
</figure>
<figureCaption confidence="0.982248333333333">
Figure 1: F1 measure according to the degree of
instance polysemy, reported when at least ten in-
stances have the specified polysemy.
</figureCaption>
<bodyText confidence="0.970138266666667">
formance on English data when using the WordNet
sense graph.
The disambiguation task encompasses multiple
types of entities. Therefore, we partitioned the Ba-
belNet test data according to the type of instance be-
ing disambiguated; Table 6 highlights the results per
instance type, averaged across all languages.5 Both
multiword expressions and named entities are less
polysemous, resulting in a substantially higher MFS
baseline that no system was able to outperform on
the two classes. However, for instances made of a
single term, both of the UMCC-DLSI systems were
able to outperform the MFS baseline.
BabelNet adds many Wikipedia senses to the ex-
isting WordNet senses, which increases the poly-
</bodyText>
<footnote confidence="0.9934425">
5We omit the UMCC-DLSI Run-3 system from analysis, as
it participated in only a single language.
</footnote>
<page confidence="0.985095">
227
</page>
<table confidence="0.9977975">
Team System English French German Italian Spanish
Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
UMCC-DLSI RUN-1 0.619 0.484 0.543 0.817 0.480 0.605 0.758 0.460 0.572 0.785 0.458 0.578 0.773 0.493 0.602
UMCC-DLSI RUN-2 0.620 0.487 0.546 0.815 0.478 0.603 0.769 0.467 0.581 0.787 0.463 0.583 0.778 0.502 0.610
UMCC-DLSI RUN-3 0.622 0.489 0.548 - - - - - - - - - - - -
MFS 0.860 0.753 0.803 0.698 0.691 0.694 0.836 0.827 0.831 0.833 0.813 0.823 0.830 0.819 0.824
</table>
<tableCaption confidence="0.9830655">
Table 4: The F1 measure for each system across all five languages in the test set when using Wikipedia-based
senses.
</tableCaption>
<table confidence="0.999852666666667">
Team System Precision Recall F1
GETALP WN-1 0.406 0.406 0.406
UMCC-DLSI RUN-1 0.639 0.635 0.637
UMCC-DLSI RUN-2 0.649 0.645 0.647
UMCC-DLSI RUN-3 0.642 0.639 0.640
MFS 0.630 0.630 0.630
</table>
<tableCaption confidence="0.991354">
Table 5: System performance when using WordNet senses. Top performing systems are marked in bold.
</tableCaption>
<table confidence="0.999881571428571">
Team System Single term Multiword expression Named Entity
DAEBAK! PD 0.502 0.801 0.910
GETALP BN-1 0.232 0.724 0.677
GETALP BN-2 0.235 0.740 0.656
UMCC-DLSI RUN-1 0.582 0.806 0.865
UMCC-DLSI RUN-2 0.584 0.809 0.864
MFS 0.511 0.853 0.920
</table>
<tableCaption confidence="0.9544425">
Table 6: System F1 per instance type, averaged across all submitted languages, with the highest system
scores in bold.
</tableCaption>
<table confidence="0.999783333333333">
English French German Italian Spanish
Team System Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1 Prec. Rec. F1
DAEBAK PD 0.769 0.364 0.494 0.747 0.387 0.510 0.762 0.307 0.438 0.778 0.425 0.550 0.778 0.450 0.570
GETALP BN-2 0.793 0.111 0.195 0.623 0.130 0.215 0.679 0.124 0.210 0.647 0.141 0.231 0.688 0.177 0.282
UMCC-DLSI RUN-1 0.787 0.421 0.549 0.754 0.441 0.557 0.741 0.330 0.457 0.796 0.461 0.584 0.830 0.525 0.643
UMCC-DLSI RUN-2 0.791 0.419 0.548 0.760 0.436 0.554 0.746 0.332 0.460 0.799 0.453 0.578 0.837 0.530 0.649
</table>
<tableCaption confidence="0.9259355">
Table 7: System performance when the system’s annotations are restricted to only those senses that it also
uses in the aligned sentences of at least two other languages.
</tableCaption>
<bodyText confidence="0.999540928571429">
semy of most instances. As a further analysis, we
consider the relationship between the polysemy of
an instance’s target and system performance. In-
stances were grouped according to the number of
BabelNet senses that their lemma had; following,
systems were scored on each grouping. Figure 1
shows the performance of the best system from each
team on each polysemy-based instance grouping,
with a general trend of performance decay as the
number of senses increases. Indeed, all systems’
performances are negatively correlated with the de-
gree of polysemy, ranging from -0.401 (UMCC-
DLSI RUN-1) to -0.654 (GETALP BN-1) when
measured using Pearson’s correlation. All systems’
</bodyText>
<page confidence="0.995113">
228
</page>
<bodyText confidence="0.999103818181818">
correlations are significant at p &lt; 0.05.
Last, we note that all systems operated by sense-
annotating each language individually without tak-
ing advantage of either the multilingual structure of
BabelNet or the sentence alignment of the test data.
For example, the sense projection method used to
create the initial set of multilingual annotations on
our test data (cf. Table 2) suggests that the sense
translation API could be used as a reliable source for
estimating the correctness of an annotation; specifi-
cally, given the sense annotations for each language,
the translation API could be used to test whether the
sense is also present in the aligned sentence in the
other languages.
Therefore, we performed a post-hoc analysis of
the benefit of multilingual sense alignment using the
results of the four systems that submitted for all lan-
guages in BabelNet. For each language, we filter
the sense annotations such that an annotation for an
instance is retained only if the system assigned the
same sense to some word in the aligned sentence
from at least two other languages.
Table 7 shows the resulting performance for the
four systems. As expected, the systems exhibit sig-
nificantly lower recall due to omitting all language-
specific instances. However, the resulting precision
is significantly higher than the original performance,
shown in Table 3. Additionally, we analyzed the set
of instances reported for each system and confirmed
that the improvement is not due to selecting only
monosemous lemmas. Despite the GETALP system
having the lower performance of the four systems
when all instances are considered, the system ob-
tains the highest precision for the English dataset.
Furthermore, the UMCC-DLSI systems still obtain
moderate recall, while enjoying 0.106-0.155 abso-
lute improvements in precision across all languages.
While the resulting F1 is lower due to a loss of recall,
we view this result as a solid starting point for other
methods to sense-tag the remaining instances. Over-
all, these results corroborate previous studies sug-
gesting that highly precise sense annotations can be
obtained by leveraging multiple languages (Navigli
and Ponzetto, 2012b; Navigli and Ponzetto, 2012c).
</bodyText>
<sectionHeader confidence="0.995633" genericHeader="conclusions">
6 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.998235136363636">
Following recent SemEval efforts with word senses
in multilingual settings, we have introduced a new
task on multilingual WSD that uses the recently
released BabelNet 1.1.1 sense inventory. Using a
data set of 13 articles in five languages, all nomi-
nal instances were annotated with BabelNet senses.
Because BabelNet is a superset of WordNet and
Wikipedia, the task also facilitates analysis in those
sense inventories.
Three teams submitted seven systems, with all
systems leveraging the graph-based structure of
WordNet and BabelNet. Several systems were able
to outperform the competitive MFS baseline, except
in the case of Wikipedia, but current performance
leaves significant room for future improvement. In
addition, we believe that future research could lever-
age sense parallelism available in sentence-aligned
multilingual corpora, together with enriched infor-
mation available in future versions of BabelNet. All
of the resources for this task, including the newest
1.1.1 version of BabelNet, were released on the task
website.6
</bodyText>
<sectionHeader confidence="0.998391" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999810823529412">
The authors gratefully acknowledge
the support of the ERC Starting
Grant MultiJEDI No. 259234.
A large group of people assisted with SemEval-
2013 Task 12, and without whose help this
task would not have been possible. In particular,
we would like to thank Philipp Cimiano, Maud
Erhmann, Sascha Hinte, Jes´us Roque Campa˜na
G´omez, and Andreas Soos for their assistance
in sense annotation; our fellow LCL team mem-
bers: Moreno De Vincenzi, Stefano Faralli, Tiziano
Flati, Marc Franco Salvador, Andrea Moro, Silvia
Necs¸ulescu, and Taher Pilehvar for their invaluable
assistance in creating BabelNet 1.1.1, preparing and
validating sense annotations, and sense-tagging the
Italian corpus; last, we thank Jim McManus for his
help in producing the Italian test data.
</bodyText>
<footnote confidence="0.998557">
6http://www.cs.york.ac.uk/semeval-2013/
task12/
</footnote>
<page confidence="0.998049">
229
</page>
<sectionHeader confidence="0.996105" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999697553398058">
Eneko Agirre and Aitor Soroa. 2009. Personalizing
PageRank for Word Sense Disambiguation. In Pro-
ceedings of EACL, Athens, Greece, pages 33–41.
Philip Edmonds and Scott Cotton. 2001. Senseval-2:
Overview. In Proceedings of The Second International
Workshop on Evaluating Word Sense Disambiguation
Systems, pages 1–6, Toulouse, France.
Katrin Erk and Diana McCarthy. 2009. Graded word
sense assignment. In Proceedings of Empirical Meth-
ods in Natural Language Processing, pages 440–449,
Singapore.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Yoan Guti´errez, Antonio Fern´andez Orquin, Sonia
V´azquez, and Andr´es Montoyo. 2011. Enriching the
integration of semantic resources based on wordnet.
Procesamiento del Lenguaje Natural, 47:249–257.
Yoan Guti´errez. 2012. An´alisis sem´antico multidimen-
sional aplicado a la desambiguaci´on del lenguaje nat-
ural. Ph.D. thesis, Universidad de Alicante.
Eduard H. Hovy, Roberto Navigli, and Simone Paolo
Ponzetto. 2013. Collaboratively built semi-structured
content and artificial intelligence: The story so far. Ar-
tificialIntelligence, 194:2–27.
David Jurgens and Ioannis Klapaftis. 2013. Semeval-
2013 task 13: Word sense induction for graded and
non-graded senses. In Proceedings of the 7th Interna-
tional Workshop on Semantic Evaluation.
Adam Kilgarriff and Martha Palmer. 2000. Introduction
to the special issue on senseval. Computers and the
Humanities, 34(1-2):1–13.
Adam Kilgarriff. 1998. Senseval: An exercise in eval-
uating word sense disambiguation programs. In Pro-
ceedings of the First International Conference on Lan-
guage Resources and Evaluation, pages 1255–1258,
Granada, Spain.
Els Lefever and Veronique Hoste. 2010. Semeval-2010
task 3: Cross-lingual word sense disambiguation. In
Proceedings of the 5th International Workshop on Se-
mantic Evaluation, pages 15–20, Uppsala, Sweden.
Association for Computational Linguistics.
Diana McCarthy and Roberto Navigli. 2009. The En-
glish lexical substitution task. Language Resources
and Evaluation, 43(2):139–159.
Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-
riff. 2004. The Senseval-3 English lexical sample
task. In Proceedings of the 3rd International Work-
shop on the Evaluation of Systems for the Semantic
Analysis of Text (SENSEVAL-3) atACL-04, Barcelona,
Spain, 25–26 July 2004, pages 25–28.
Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010.
Semeval-2010 task 2: Cross-lingual lexical substitu-
tion. In Proceedings of the 5th international workshop
on semantic evaluation, pages 9–14, Uppsala, Sweden.
Association for Computational Linguistics.
George A. Miller, R.T. Beckwith, Christiane D. Fell-
baum, D. Gross, and K. Miller. 1990. WordNet: an
online lexical database. International Journal of Lexi-
cography, 3(4):235–244.
Roberto Navigli and Mirella Lapata. 2010. An exper-
imental study on graph connectivity for unsupervised
Word Sense Disambiguation. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 32(4):678–
692.
Roberto Navigli and Simone Paolo Ponzetto. 2012a. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217–250.
Roberto Navigli and Simone Paolo Ponzetto. 2012b.
BabelRelate! a joint multilingual approach to com-
puting semantic relatedness. In Proceedings of the
Twenty-Sixth AAAI Conference on Artificial Intelli-
gence (AAAI), Toronto, Ontario, Canada.
Roberto Navigli and Simone Paolo Ponzetto. 2012c.
Joining forces pays off: Multilingual Joint Word Sense
Disambiguation. In Proceedings of EMNLP-CoNLL,
pages 1399–1410, Jeju Island, Korea.
Roberto Navigli and Simone Paolo Ponzetto. 2012d.
Multilingual WSD with just a few lines of code: the
BabelNet API. In Proceedings of the 50th Annual
Meeting of the Association for Computational Linguis-
tics (ACL 2012), Jeju, Korea.
Roberto Navigli, Kenneth C. Litkowski, and Orin Har-
graves. 2007. SemEval-2007 Task 07: Coarse-
grained English all-words task. In Proceedings of the
4th International Workshop on Semantic Evaluations
(SemEval-2007), Prague, Czech Republic, pages 30–
35.
Roberto Navigli. 2009. Word Sense Disambiguation: A
survey. ACM Computing Surveys, 41(2):1–69.
Roberto Navigli. 2012. A quick tour of Word Sense
Disambiguation, Induction and related approaches. In
Proceedings of the 38th Conference on Current Trends
in Theory and Practice of Computer Science (SOF-
SEM), pages 115–129.
Sameer Pradhan, Edward Loper, Dmitriy Dligach, and
Martha Palmer. 2007. SemEval-2007 Task-17: En-
glish lexical sample, SRL and all words. In Proceed-
ings of the 4th International Workshop on Semantic
Evaluations (SemEval-2007), Prague, Czech Repub-
lic, pages 87–92.
Didier Schwab, J´erˆome Goulian, Andon Tchechmedjiev,
and Herv´e Blanchon. 2012. Ant colony algorithm for
</reference>
<page confidence="0.952559">
230
</page>
<reference confidence="0.997521384615385">
the unsupervised word sense disambiguation of texts:
Comparison and evaluation. In Proceedings of the
241h International Conference on Computational Lin-
guistics (COLING), pages 8–15, Mumbai, India.
Benjamin Snyder and Martha Palmer. 2004. The en-
glish all-words task. In Proceedings of ACL 2004
SENSEVAL-3 Workshop, pages 41–43, Barcelona,
Spain.
David Yarowsky. 1995. Unsupervised word sense dis-
ambiguation rivaling supervised methods. In Proceed-
ings of the 33Td Annual Meeting of the Association
for Computational Linguistics, pages 189–196, Cam-
bridge, MA, USA.
</reference>
<page confidence="0.997936">
231
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.365520">
<title confidence="0.992859">SemEval-2013 Task 12: Multilingual Word Sense Disambiguation</title>
<author confidence="0.662552">David Jurgens Navigli</author>
<affiliation confidence="0.5852675">Dipartimento di Sapienza Universit`a di</affiliation>
<address confidence="0.559516">Viale Regina Elena, 295 – 00161 Roma</address>
<abstract confidence="0.991777909090909">This paper presents the SemEval-2013 task on multilingual Word Sense Disambiguation. We describe our experience in producing a multilingual sense-annotated corpus for the task. The corpus is tagged with BabelNet 1.1.1, a freely-available multilingual encyclopedic dictionary and, as a byproduct, WordNet 3.0 and the Wikipedia sense inventory. We present and analyze the results of participating systems, and discuss future directions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Aitor Soroa</author>
</authors>
<title>Personalizing PageRank for Word Sense Disambiguation.</title>
<date>2009</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>33--41</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="18711" citStr="Agirre and Soroa, 2009" startWordPosition="2963" endWordPosition="2966">ts parameters from the trial data, while the BN2 and WN1 systems are completely unsupervised and optimize their parameters directly from the structure of the BabelNet and WordNet graphs. UMCC-DLSI UMCC-DLSI submitted three systems based on the ISR-WN resource (Guti´errez et al., 2011), which enriches the WordNet semantic network using edges from multiple lexical resources, such as WordNet Domains and the eXtended WordNet. WSD was then performed using the ISR-WN network in combination with the algorithm of Guti´errez (2012), which is an extension of the Personalized PageRank algorithm for WSD (Agirre and Soroa, 2009) which includes senses frequency. The algorithm requires initializing the PageRank algorithm with a set of seed synsets (vertices) in the network; this initialization represents the key variation among UMCC’s three approaches. The RUN-1 system performs WSD using all noun instances from the sentence context. In contrast, the RUN-2 works at the discourse level and initializes the PageRank using the synsets of all 226 Team System English French German Italian Spanish DAEBAK! PD 0.604 0.538 0.591 0.613 0.600 GETALP BN-1 0.263 0.261 0.404 0.324 - GETALP BN-2 0.266 0.257 0.400 0.324 0.371 UMCC-DLSI </context>
</contexts>
<marker>Agirre, Soroa, 2009</marker>
<rawString>Eneko Agirre and Aitor Soroa. 2009. Personalizing PageRank for Word Sense Disambiguation. In Proceedings of EACL, Athens, Greece, pages 33–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Edmonds</author>
<author>Scott Cotton</author>
</authors>
<title>Senseval-2: Overview.</title>
<date>2001</date>
<booktitle>In Proceedings of The Second International Workshop on Evaluating Word Sense Disambiguation Systems,</booktitle>
<pages>1--6</pages>
<location>Toulouse, France.</location>
<contexts>
<context position="1276" citStr="Edmonds and Cotton, 2001" startWordPosition="172" endWordPosition="176">pating systems, and discuss future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 </context>
</contexts>
<marker>Edmonds, Cotton, 2001</marker>
<rawString>Philip Edmonds and Scott Cotton. 2001. Senseval-2: Overview. In Proceedings of The Second International Workshop on Evaluating Word Sense Disambiguation Systems, pages 1–6, Toulouse, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Diana McCarthy</author>
</authors>
<title>Graded word sense assignment.</title>
<date>2009</date>
<booktitle>In Proceedings of Empirical Methods in Natural Language Processing,</booktitle>
<pages>440--449</pages>
<contexts>
<context position="13758" citStr="Erk and McCarthy, 2009" startWordPosition="2164" endWordPosition="2167">se for “card” that corresponds to the penalty card meaning (as used in sports such as football). In contrast, BabelNet has a sense for “penalty card” from Wikipedia which, however, is not mapped to the lemma “card”. In such cases, we add both the closest meaning from the original lemma (e.g., the rectangual piece of paper sense in WordNet) and the most suitable sense that may have a different lemma form (e.g., PENALTY CARD). Previous annotation studies have shown that, when a fine-grained sense inventory is used, annotators will often label ambiguous instances with multiple senses if allowed (Erk and McCarthy, 2009; Jurgens and Klapaftis, 2013). Since BabelNet is a combination of a fine-grained inventory (WordNet) and contains additional senses from Wikipedia, we analyzed the average number of BabelNet sense annotations per instance, shown in column six of Table 1. Surprisingly, Table 1 suggests that the rate of multiple sense annotation varies significantly between languages. BabelNet may combine multiple Wikipedia pages into a single BabelNet synset. As a result, when Wikipedia is used as a sense inventory, instances are annotated with all of the Wikipedia pages associated with each BabelNet synset. I</context>
</contexts>
<marker>Erk, McCarthy, 2009</marker>
<rawString>Katrin Erk and Diana McCarthy. 2009. Graded word sense assignment. In Proceedings of Empirical Methods in Natural Language Processing, pages 440–449, Singapore.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoan Guti´errez</author>
<author>Antonio Fern´andez Orquin</author>
<author>Sonia V´azquez</author>
<author>Andr´es Montoyo</author>
</authors>
<title>Enriching the integration of semantic resources based on wordnet.</title>
<date>2011</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>47--249</pages>
<marker>Guti´errez, Orquin, V´azquez, Montoyo, 2011</marker>
<rawString>Yoan Guti´errez, Antonio Fern´andez Orquin, Sonia V´azquez, and Andr´es Montoyo. 2011. Enriching the integration of semantic resources based on wordnet. Procesamiento del Lenguaje Natural, 47:249–257.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoan Guti´errez</author>
</authors>
<title>An´alisis sem´antico multidimensional aplicado a la desambiguaci´on del lenguaje natural.</title>
<date>2012</date>
<tech>Ph.D. thesis,</tech>
<institution>Universidad de Alicante.</institution>
<marker>Guti´errez, 2012</marker>
<rawString>Yoan Guti´errez. 2012. An´alisis sem´antico multidimensional aplicado a la desambiguaci´on del lenguaje natural. Ph.D. thesis, Universidad de Alicante.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard H Hovy</author>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Collaboratively built semi-structured content and artificial intelligence: The story so far.</title>
<date>2013</date>
<journal>ArtificialIntelligence,</journal>
<pages>194--2</pages>
<contexts>
<context position="2641" citStr="Hovy et al., 2013" startWordPosition="376" endWordPosition="379">nized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has enabled the automated creation of large-scale lexical knowledge resources (Hovy et al., 2013). Over the past few years, a wide-coverage multilingual “encyclopedic” dictionary, called BabelNet, has been developed (Navigli and Ponzetto, 2012a). BabelNet1 brings together WordNet and Wikipedia and provides a multilingual sense inventory that currently covers 6 languages. We therefore decided to put the BabelNet 1.1.1 sense inventory to the test and organize a traditional Word Sense Disambiguation task on a given English test set translated into 4 other languages (namely, French, German, Spanish and Italian). Not only does BabelNet enable multilinguality, but it also provides coverage for </context>
</contexts>
<marker>Hovy, Navigli, Ponzetto, 2013</marker>
<rawString>Eduard H. Hovy, Roberto Navigli, and Simone Paolo Ponzetto. 2013. Collaboratively built semi-structured content and artificial intelligence: The story so far. ArtificialIntelligence, 194:2–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Jurgens</author>
<author>Ioannis Klapaftis</author>
</authors>
<title>Semeval2013 task 13: Word sense induction for graded and non-graded senses.</title>
<date>2013</date>
<booktitle>In Proceedings of the 7th International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="13788" citStr="Jurgens and Klapaftis, 2013" startWordPosition="2168" endWordPosition="2172">sponds to the penalty card meaning (as used in sports such as football). In contrast, BabelNet has a sense for “penalty card” from Wikipedia which, however, is not mapped to the lemma “card”. In such cases, we add both the closest meaning from the original lemma (e.g., the rectangual piece of paper sense in WordNet) and the most suitable sense that may have a different lemma form (e.g., PENALTY CARD). Previous annotation studies have shown that, when a fine-grained sense inventory is used, annotators will often label ambiguous instances with multiple senses if allowed (Erk and McCarthy, 2009; Jurgens and Klapaftis, 2013). Since BabelNet is a combination of a fine-grained inventory (WordNet) and contains additional senses from Wikipedia, we analyzed the average number of BabelNet sense annotations per instance, shown in column six of Table 1. Surprisingly, Table 1 suggests that the rate of multiple sense annotation varies significantly between languages. BabelNet may combine multiple Wikipedia pages into a single BabelNet synset. As a result, when Wikipedia is used as a sense inventory, instances are annotated with all of the Wikipedia pages associated with each BabelNet synset. Indeed, Table 1 shows a markedl</context>
</contexts>
<marker>Jurgens, Klapaftis, 2013</marker>
<rawString>David Jurgens and Ioannis Klapaftis. 2013. Semeval2013 task 13: Word sense induction for graded and non-graded senses. In Proceedings of the 7th International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
<author>Martha Palmer</author>
</authors>
<title>Introduction to the special issue on senseval.</title>
<date>2000</date>
<booktitle>Computers and the Humanities,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="1237" citStr="Kilgarriff and Palmer, 2000" startWordPosition="166" endWordPosition="169"> present and analyze the results of participating systems, and discuss future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of int</context>
</contexts>
<marker>Kilgarriff, Palmer, 2000</marker>
<rawString>Adam Kilgarriff and Martha Palmer. 2000. Introduction to the special issue on senseval. Computers and the Humanities, 34(1-2):1–13.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adam Kilgarriff</author>
</authors>
<title>Senseval: An exercise in evaluating word sense disambiguation programs.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation,</booktitle>
<pages>1255--1258</pages>
<location>Granada,</location>
<contexts>
<context position="1207" citStr="Kilgarriff, 1998" startWordPosition="164" endWordPosition="165">ense inventory. We present and analyze the results of participating systems, and discuss future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we ha</context>
</contexts>
<marker>Kilgarriff, 1998</marker>
<rawString>Adam Kilgarriff. 1998. Senseval: An exercise in evaluating word sense disambiguation programs. In Proceedings of the First International Conference on Language Resources and Evaluation, pages 1255–1258, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>Veronique Hoste</author>
</authors>
<title>Semeval-2010 task 3: Cross-lingual word sense disambiguation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th International Workshop on Semantic Evaluation,</booktitle>
<pages>15--20</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="1950" citStr="Lefever and Hoste, 2010" startWordPosition="278" endWordPosition="281">focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has en</context>
</contexts>
<marker>Lefever, Hoste, 2010</marker>
<rawString>Els Lefever and Veronique Hoste. 2010. Semeval-2010 task 3: Cross-lingual word sense disambiguation. In Proceedings of the 5th International Workshop on Semantic Evaluation, pages 15–20, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana McCarthy</author>
<author>Roberto Navigli</author>
</authors>
<title>The English lexical substitution task.</title>
<date>2009</date>
<journal>Language Resources and Evaluation,</journal>
<volume>43</volume>
<issue>2</issue>
<contexts>
<context position="2307" citStr="McCarthy and Navigli, 2009" startWordPosition="330" endWordPosition="333">. In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has enabled the automated creation of large-scale lexical knowledge resources (Hovy et al., 2013). Over the past few years, a wide-coverage multilingual “encyclopedic” dictionary, called BabelNet, has been developed (Navigli and Ponzetto, 2012a). BabelNet1 brings together WordNet and Wikipedia and provides a multilingual sense inventory that currently covers 6 </context>
</contexts>
<marker>McCarthy, Navigli, 2009</marker>
<rawString>Diana McCarthy and Roberto Navigli. 2009. The English lexical substitution task. Language Resources and Evaluation, 43(2):139–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Timothy Chklovski</author>
<author>Adam Kilgarriff</author>
</authors>
<title>The Senseval-3 English lexical sample task.</title>
<date>2004</date>
<booktitle>In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3) atACL-04,</booktitle>
<pages>25--28</pages>
<location>Barcelona,</location>
<contexts>
<context position="1325" citStr="Mihalcea et al., 2004" startWordPosition="181" endWordPosition="184">troduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation </context>
</contexts>
<marker>Mihalcea, Chklovski, Kilgarriff, 2004</marker>
<rawString>Rada Mihalcea, Timothy Chklovski, and Adam Kilgarriff. 2004. The Senseval-3 English lexical sample task. In Proceedings of the 3rd International Workshop on the Evaluation of Systems for the Semantic Analysis of Text (SENSEVAL-3) atACL-04, Barcelona, Spain, 25–26 July 2004, pages 25–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ravi Sinha</author>
<author>Diana McCarthy</author>
</authors>
<title>Semeval-2010 task 2: Cross-lingual lexical substitution.</title>
<date>2010</date>
<booktitle>In Proceedings of the 5th international workshop on semantic evaluation,</booktitle>
<pages>9--14</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="2013" citStr="Mihalcea et al., 2010" startWordPosition="287" endWordPosition="290">se inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has enabled the automated creation of large-scale lexical knowledge r</context>
</contexts>
<marker>Mihalcea, Sinha, McCarthy, 2010</marker>
<rawString>Rada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010. Semeval-2010 task 2: Cross-lingual lexical substitution. In Proceedings of the 5th international workshop on semantic evaluation, pages 9–14, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
<author>R T Beckwith</author>
<author>Christiane D Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>WordNet: an online lexical database.</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<contexts>
<context position="1365" citStr="Miller et al., 1990" startWordPosition="188" endWordPosition="191"> the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-ling</context>
<context position="5948" citStr="Miller et al., 1990" startWordPosition="882" endWordPosition="885">and enrichment methodology. This resource is created by linking the largest multilingual Web encyclopedia – i.e., Wikipedia – to the most popular computational lexicon – i.e., WordNet 3.0. The integration is performed via an automatic mapping and 2http://www.statmt.org/wmt12/ by filling in lexical gaps in resource-poor languages with the aid of Machine Translation (MT). Its lexicon includes lemmas which denote both lexicographic meanings (e.g., balloon) and encyclopedic ones (e.g., Montgolfier brothers). The basic meaning unit in BabelNet is the Babel synset, modeled after the WordNet synset (Miller et al., 1990; Fellbaum, 1998). A Babel synset is a set of synonyms which express a concept in different languages. For instance, { Globus aerost`aticCA, BalloonEN, A´erostationFR, BallonDE, Pallone aerostaticoIT, ... , Globo aerost´aticoES } is the Babel synset for the balloon aerostat, where the language of each synonym is provided as a subscript label. Thanks to their multilingual nature, we were able to use Babel synsets as interlingual concept tags for nouns occurring within text written in any of the covered languages. 2.2.2 WordNet and Wikipedia inventories Since BabelNet 1.1.1 is a superset of the </context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>George A. Miller, R.T. Beckwith, Christiane D. Fellbaum, D. Gross, and K. Miller. 1990. WordNet: an online lexical database. International Journal of Lexicography, 3(4):235–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Mirella Lapata</author>
</authors>
<title>An experimental study on graph connectivity for unsupervised Word Sense Disambiguation.</title>
<date>2010</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>32</volume>
<issue>4</issue>
<pages>692</pages>
<contexts>
<context position="17440" citStr="Navigli and Lapata (2010)" startWordPosition="2761" endWordPosition="2765">SD task with BabelNet senses; two teams submitted four systems using WordNet senses; and one team submitted three systems for Wikipedia-based senses. Notably, all systems used graph-based approaches for sense disambiguation, either using WordNet or BabelNet’s synset graphs. We summarize the teams’ systems as follows. DAEBAK! DAEBAK! submitted one system called PD (Peripheral Diversity) based on BabelNet path indices from the BabelNet synset graph. Using a f5 sentence window around the target word, a graph is constructed for all senses of co-occurring lemmas following the procedure proposed by Navigli and Lapata (2010). The final sense is selected based on measuring connectivity to the synsets of neighboring lemmas. The MFS is used as a backoff strategy when no appropriate sense can be picked out. GETALP GETALP submitted three systems, two for BabelNet and one for WordNet, all based on the ant-colony algorithm of (Schwab et al., 2012), which uses the sense inventory network structure to identify paths connecting synsets of the target lemma to the synsets of other lemmas in context. The algorithm requires setting several parameters for the weighting of the structure of the contextbased graph, which vary acro</context>
</contexts>
<marker>Navigli, Lapata, 2010</marker>
<rawString>Roberto Navigli and Mirella Lapata. 2010. An experimental study on graph connectivity for unsupervised Word Sense Disambiguation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32(4):678– 692.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>193--217</pages>
<contexts>
<context position="2787" citStr="Navigli and Ponzetto, 2012" startWordPosition="396" endWordPosition="399">igm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has enabled the automated creation of large-scale lexical knowledge resources (Hovy et al., 2013). Over the past few years, a wide-coverage multilingual “encyclopedic” dictionary, called BabelNet, has been developed (Navigli and Ponzetto, 2012a). BabelNet1 brings together WordNet and Wikipedia and provides a multilingual sense inventory that currently covers 6 languages. We therefore decided to put the BabelNet 1.1.1 sense inventory to the test and organize a traditional Word Sense Disambiguation task on a given English test set translated into 4 other languages (namely, French, German, Spanish and Italian). Not only does BabelNet enable multilinguality, but it also provides coverage for both lexicographic (e.g., apple as fruit) and encyclopedic 1http://babelnet.org 222 Second Joint Conference on Lexical and Computational Semantics</context>
<context position="5098" citStr="Navigli and Ponzetto, 2012" startWordPosition="755" endWordPosition="758">ifferent languages (English, French, German and Spanish). In order to cover Italian, an Italian native speaker manually translated each article from English into Italian, with the support of an English mother tongue advisor. In Table 1 we show for each language the number of words of running text, together with the number of multiword expressions and named entities annotated, from the 13 articles. 2.2 Sense Inventories 2.2.1 BabelNet inventory To semantically annotate all the single- and multiword expressions, as well as the named entities, occurring in our test corpus we used BabelNet 1.1.1 (Navigli and Ponzetto, 2012a). BabelNet is a multilingual “encyclopedic dictionary” and a semantic network currently covering 6 languages, namely: English, Catalan, French, German, Italian and Spanish. BabelNet is obtained as a result of a novel integration and enrichment methodology. This resource is created by linking the largest multilingual Web encyclopedia – i.e., Wikipedia – to the most popular computational lexicon – i.e., WordNet 3.0. The integration is performed via an automatic mapping and 2http://www.statmt.org/wmt12/ by filling in lexical gaps in resource-poor languages with the aid of Machine Translation (M</context>
<context position="10907" citStr="Navigli and Ponzetto, 2012" startWordPosition="1705" endWordPosition="1708">ond phase, each instance in the English dataset was annotated using BabelNet senses. To reduce the time required for annotation in the other languages, the sense annotations for the English dataset were then projected onto the other four 224 Language Projected Valid Invalid instances projections projections French 1016 791 225 German 592 373 219 Italian 1029 774 255 Spanish 911 669 242 Table 2: Statistics when using the English sense annotations to project the correct sense of a lemma in another language of the sentence-aligned test data. languages using the sense translation API of BabelNet (Navigli and Ponzetto, 2012d). The projection operated as follows, using the aligned sentences in the English and non-English texts. For an instance in the non-English text, all of the senses for that instance’s lemma were compared with the sense annotations in the English sentence. If any of that lemma’s senses was used in the English sentence, then that sense was selected for the non-English instance. The matching procedure operates at the sentence-aligned level because the instances themselves are not aligned; i.e., different languages have different numbers of instances per sentence, which are potentially ordered di</context>
<context position="27004" citStr="Navigli and Ponzetto, 2012" startWordPosition="4310" endWordPosition="4313">e lower performance of the four systems when all instances are considered, the system obtains the highest precision for the English dataset. Furthermore, the UMCC-DLSI systems still obtain moderate recall, while enjoying 0.106-0.155 absolute improvements in precision across all languages. While the resulting F1 is lower due to a loss of recall, we view this result as a solid starting point for other methods to sense-tag the remaining instances. Overall, these results corroborate previous studies suggesting that highly precise sense annotations can be obtained by leveraging multiple languages (Navigli and Ponzetto, 2012b; Navigli and Ponzetto, 2012c). 6 Conclusion and Future Directions Following recent SemEval efforts with word senses in multilingual settings, we have introduced a new task on multilingual WSD that uses the recently released BabelNet 1.1.1 sense inventory. Using a data set of 13 articles in five languages, all nominal instances were annotated with BabelNet senses. Because BabelNet is a superset of WordNet and Wikipedia, the task also facilitates analysis in those sense inventories. Three teams submitted seven systems, with all systems leveraging the graph-based structure of WordNet and BabelN</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012a. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelRelate! a joint multilingual approach to computing semantic relatedness.</title>
<date>2012</date>
<booktitle>In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI),</booktitle>
<location>Toronto, Ontario, Canada.</location>
<contexts>
<context position="2787" citStr="Navigli and Ponzetto, 2012" startWordPosition="396" endWordPosition="399">igm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has enabled the automated creation of large-scale lexical knowledge resources (Hovy et al., 2013). Over the past few years, a wide-coverage multilingual “encyclopedic” dictionary, called BabelNet, has been developed (Navigli and Ponzetto, 2012a). BabelNet1 brings together WordNet and Wikipedia and provides a multilingual sense inventory that currently covers 6 languages. We therefore decided to put the BabelNet 1.1.1 sense inventory to the test and organize a traditional Word Sense Disambiguation task on a given English test set translated into 4 other languages (namely, French, German, Spanish and Italian). Not only does BabelNet enable multilinguality, but it also provides coverage for both lexicographic (e.g., apple as fruit) and encyclopedic 1http://babelnet.org 222 Second Joint Conference on Lexical and Computational Semantics</context>
<context position="5098" citStr="Navigli and Ponzetto, 2012" startWordPosition="755" endWordPosition="758">ifferent languages (English, French, German and Spanish). In order to cover Italian, an Italian native speaker manually translated each article from English into Italian, with the support of an English mother tongue advisor. In Table 1 we show for each language the number of words of running text, together with the number of multiword expressions and named entities annotated, from the 13 articles. 2.2 Sense Inventories 2.2.1 BabelNet inventory To semantically annotate all the single- and multiword expressions, as well as the named entities, occurring in our test corpus we used BabelNet 1.1.1 (Navigli and Ponzetto, 2012a). BabelNet is a multilingual “encyclopedic dictionary” and a semantic network currently covering 6 languages, namely: English, Catalan, French, German, Italian and Spanish. BabelNet is obtained as a result of a novel integration and enrichment methodology. This resource is created by linking the largest multilingual Web encyclopedia – i.e., Wikipedia – to the most popular computational lexicon – i.e., WordNet 3.0. The integration is performed via an automatic mapping and 2http://www.statmt.org/wmt12/ by filling in lexical gaps in resource-poor languages with the aid of Machine Translation (M</context>
<context position="10907" citStr="Navigli and Ponzetto, 2012" startWordPosition="1705" endWordPosition="1708">ond phase, each instance in the English dataset was annotated using BabelNet senses. To reduce the time required for annotation in the other languages, the sense annotations for the English dataset were then projected onto the other four 224 Language Projected Valid Invalid instances projections projections French 1016 791 225 German 592 373 219 Italian 1029 774 255 Spanish 911 669 242 Table 2: Statistics when using the English sense annotations to project the correct sense of a lemma in another language of the sentence-aligned test data. languages using the sense translation API of BabelNet (Navigli and Ponzetto, 2012d). The projection operated as follows, using the aligned sentences in the English and non-English texts. For an instance in the non-English text, all of the senses for that instance’s lemma were compared with the sense annotations in the English sentence. If any of that lemma’s senses was used in the English sentence, then that sense was selected for the non-English instance. The matching procedure operates at the sentence-aligned level because the instances themselves are not aligned; i.e., different languages have different numbers of instances per sentence, which are potentially ordered di</context>
<context position="27004" citStr="Navigli and Ponzetto, 2012" startWordPosition="4310" endWordPosition="4313">e lower performance of the four systems when all instances are considered, the system obtains the highest precision for the English dataset. Furthermore, the UMCC-DLSI systems still obtain moderate recall, while enjoying 0.106-0.155 absolute improvements in precision across all languages. While the resulting F1 is lower due to a loss of recall, we view this result as a solid starting point for other methods to sense-tag the remaining instances. Overall, these results corroborate previous studies suggesting that highly precise sense annotations can be obtained by leveraging multiple languages (Navigli and Ponzetto, 2012b; Navigli and Ponzetto, 2012c). 6 Conclusion and Future Directions Following recent SemEval efforts with word senses in multilingual settings, we have introduced a new task on multilingual WSD that uses the recently released BabelNet 1.1.1 sense inventory. Using a data set of 13 articles in five languages, all nominal instances were annotated with BabelNet senses. Because BabelNet is a superset of WordNet and Wikipedia, the task also facilitates analysis in those sense inventories. Three teams submitted seven systems, with all systems leveraging the graph-based structure of WordNet and BabelN</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012b. BabelRelate! a joint multilingual approach to computing semantic relatedness. In Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI), Toronto, Ontario, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Joining forces pays off: Multilingual Joint Word Sense Disambiguation.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP-CoNLL,</booktitle>
<pages>1399--1410</pages>
<location>Jeju Island,</location>
<contexts>
<context position="2787" citStr="Navigli and Ponzetto, 2012" startWordPosition="396" endWordPosition="399">igm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has enabled the automated creation of large-scale lexical knowledge resources (Hovy et al., 2013). Over the past few years, a wide-coverage multilingual “encyclopedic” dictionary, called BabelNet, has been developed (Navigli and Ponzetto, 2012a). BabelNet1 brings together WordNet and Wikipedia and provides a multilingual sense inventory that currently covers 6 languages. We therefore decided to put the BabelNet 1.1.1 sense inventory to the test and organize a traditional Word Sense Disambiguation task on a given English test set translated into 4 other languages (namely, French, German, Spanish and Italian). Not only does BabelNet enable multilinguality, but it also provides coverage for both lexicographic (e.g., apple as fruit) and encyclopedic 1http://babelnet.org 222 Second Joint Conference on Lexical and Computational Semantics</context>
<context position="5098" citStr="Navigli and Ponzetto, 2012" startWordPosition="755" endWordPosition="758">ifferent languages (English, French, German and Spanish). In order to cover Italian, an Italian native speaker manually translated each article from English into Italian, with the support of an English mother tongue advisor. In Table 1 we show for each language the number of words of running text, together with the number of multiword expressions and named entities annotated, from the 13 articles. 2.2 Sense Inventories 2.2.1 BabelNet inventory To semantically annotate all the single- and multiword expressions, as well as the named entities, occurring in our test corpus we used BabelNet 1.1.1 (Navigli and Ponzetto, 2012a). BabelNet is a multilingual “encyclopedic dictionary” and a semantic network currently covering 6 languages, namely: English, Catalan, French, German, Italian and Spanish. BabelNet is obtained as a result of a novel integration and enrichment methodology. This resource is created by linking the largest multilingual Web encyclopedia – i.e., Wikipedia – to the most popular computational lexicon – i.e., WordNet 3.0. The integration is performed via an automatic mapping and 2http://www.statmt.org/wmt12/ by filling in lexical gaps in resource-poor languages with the aid of Machine Translation (M</context>
<context position="10907" citStr="Navigli and Ponzetto, 2012" startWordPosition="1705" endWordPosition="1708">ond phase, each instance in the English dataset was annotated using BabelNet senses. To reduce the time required for annotation in the other languages, the sense annotations for the English dataset were then projected onto the other four 224 Language Projected Valid Invalid instances projections projections French 1016 791 225 German 592 373 219 Italian 1029 774 255 Spanish 911 669 242 Table 2: Statistics when using the English sense annotations to project the correct sense of a lemma in another language of the sentence-aligned test data. languages using the sense translation API of BabelNet (Navigli and Ponzetto, 2012d). The projection operated as follows, using the aligned sentences in the English and non-English texts. For an instance in the non-English text, all of the senses for that instance’s lemma were compared with the sense annotations in the English sentence. If any of that lemma’s senses was used in the English sentence, then that sense was selected for the non-English instance. The matching procedure operates at the sentence-aligned level because the instances themselves are not aligned; i.e., different languages have different numbers of instances per sentence, which are potentially ordered di</context>
<context position="27004" citStr="Navigli and Ponzetto, 2012" startWordPosition="4310" endWordPosition="4313">e lower performance of the four systems when all instances are considered, the system obtains the highest precision for the English dataset. Furthermore, the UMCC-DLSI systems still obtain moderate recall, while enjoying 0.106-0.155 absolute improvements in precision across all languages. While the resulting F1 is lower due to a loss of recall, we view this result as a solid starting point for other methods to sense-tag the remaining instances. Overall, these results corroborate previous studies suggesting that highly precise sense annotations can be obtained by leveraging multiple languages (Navigli and Ponzetto, 2012b; Navigli and Ponzetto, 2012c). 6 Conclusion and Future Directions Following recent SemEval efforts with word senses in multilingual settings, we have introduced a new task on multilingual WSD that uses the recently released BabelNet 1.1.1 sense inventory. Using a data set of 13 articles in five languages, all nominal instances were annotated with BabelNet senses. Because BabelNet is a superset of WordNet and Wikipedia, the task also facilitates analysis in those sense inventories. Three teams submitted seven systems, with all systems leveraging the graph-based structure of WordNet and BabelN</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012c. Joining forces pays off: Multilingual Joint Word Sense Disambiguation. In Proceedings of EMNLP-CoNLL, pages 1399–1410, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>Multilingual WSD with just a few lines of code: the BabelNet API.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<location>Jeju,</location>
<contexts>
<context position="2787" citStr="Navigli and Ponzetto, 2012" startWordPosition="396" endWordPosition="399">igm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has enabled the automated creation of large-scale lexical knowledge resources (Hovy et al., 2013). Over the past few years, a wide-coverage multilingual “encyclopedic” dictionary, called BabelNet, has been developed (Navigli and Ponzetto, 2012a). BabelNet1 brings together WordNet and Wikipedia and provides a multilingual sense inventory that currently covers 6 languages. We therefore decided to put the BabelNet 1.1.1 sense inventory to the test and organize a traditional Word Sense Disambiguation task on a given English test set translated into 4 other languages (namely, French, German, Spanish and Italian). Not only does BabelNet enable multilinguality, but it also provides coverage for both lexicographic (e.g., apple as fruit) and encyclopedic 1http://babelnet.org 222 Second Joint Conference on Lexical and Computational Semantics</context>
<context position="5098" citStr="Navigli and Ponzetto, 2012" startWordPosition="755" endWordPosition="758">ifferent languages (English, French, German and Spanish). In order to cover Italian, an Italian native speaker manually translated each article from English into Italian, with the support of an English mother tongue advisor. In Table 1 we show for each language the number of words of running text, together with the number of multiword expressions and named entities annotated, from the 13 articles. 2.2 Sense Inventories 2.2.1 BabelNet inventory To semantically annotate all the single- and multiword expressions, as well as the named entities, occurring in our test corpus we used BabelNet 1.1.1 (Navigli and Ponzetto, 2012a). BabelNet is a multilingual “encyclopedic dictionary” and a semantic network currently covering 6 languages, namely: English, Catalan, French, German, Italian and Spanish. BabelNet is obtained as a result of a novel integration and enrichment methodology. This resource is created by linking the largest multilingual Web encyclopedia – i.e., Wikipedia – to the most popular computational lexicon – i.e., WordNet 3.0. The integration is performed via an automatic mapping and 2http://www.statmt.org/wmt12/ by filling in lexical gaps in resource-poor languages with the aid of Machine Translation (M</context>
<context position="10907" citStr="Navigli and Ponzetto, 2012" startWordPosition="1705" endWordPosition="1708">ond phase, each instance in the English dataset was annotated using BabelNet senses. To reduce the time required for annotation in the other languages, the sense annotations for the English dataset were then projected onto the other four 224 Language Projected Valid Invalid instances projections projections French 1016 791 225 German 592 373 219 Italian 1029 774 255 Spanish 911 669 242 Table 2: Statistics when using the English sense annotations to project the correct sense of a lemma in another language of the sentence-aligned test data. languages using the sense translation API of BabelNet (Navigli and Ponzetto, 2012d). The projection operated as follows, using the aligned sentences in the English and non-English texts. For an instance in the non-English text, all of the senses for that instance’s lemma were compared with the sense annotations in the English sentence. If any of that lemma’s senses was used in the English sentence, then that sense was selected for the non-English instance. The matching procedure operates at the sentence-aligned level because the instances themselves are not aligned; i.e., different languages have different numbers of instances per sentence, which are potentially ordered di</context>
<context position="27004" citStr="Navigli and Ponzetto, 2012" startWordPosition="4310" endWordPosition="4313">e lower performance of the four systems when all instances are considered, the system obtains the highest precision for the English dataset. Furthermore, the UMCC-DLSI systems still obtain moderate recall, while enjoying 0.106-0.155 absolute improvements in precision across all languages. While the resulting F1 is lower due to a loss of recall, we view this result as a solid starting point for other methods to sense-tag the remaining instances. Overall, these results corroborate previous studies suggesting that highly precise sense annotations can be obtained by leveraging multiple languages (Navigli and Ponzetto, 2012b; Navigli and Ponzetto, 2012c). 6 Conclusion and Future Directions Following recent SemEval efforts with word senses in multilingual settings, we have introduced a new task on multilingual WSD that uses the recently released BabelNet 1.1.1 sense inventory. Using a data set of 13 articles in five languages, all nominal instances were annotated with BabelNet senses. Because BabelNet is a superset of WordNet and Wikipedia, the task also facilitates analysis in those sense inventories. Three teams submitted seven systems, with all systems leveraging the graph-based structure of WordNet and BabelN</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012d. Multilingual WSD with just a few lines of code: the BabelNet API. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL 2012), Jeju, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Kenneth C Litkowski</author>
<author>Orin Hargraves</author>
</authors>
<title>SemEval-2007 Task 07: Coarsegrained English all-words task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>30--35</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1638" citStr="Navigli et al., 2007" startWordPosition="230" endWordPosition="234">d limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>Roberto Navigli, Kenneth C. Litkowski, and Orin Hargraves. 2007. SemEval-2007 Task 07: Coarsegrained English all-words task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, pages 30– 35.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>Word Sense Disambiguation: A survey.</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<issue>2</issue>
<contexts>
<context position="905" citStr="Navigli, 2009" startWordPosition="121" endWordPosition="122">val-2013 task on multilingual Word Sense Disambiguation. We describe our experience in producing a multilingual sense-annotated corpus for the task. The corpus is tagged with BabelNet 1.1.1, a freely-available multilingual encyclopedic dictionary and, as a byproduct, WordNet 3.0 and the Wikipedia sense inventory. We present and analyze the results of participating systems, and discuss future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval di</context>
<context position="2307" citStr="Navigli, 2009" startWordPosition="332" endWordPosition="333">ears, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (McCarthy and Navigli, 2009). The main factor hampering traditional WSD from going multilingual was the lack of a freely-available large-scale multilingual dictionary. The recent availability of huge collaborativelybuilt repositories of knowledge such as Wikipedia has enabled the automated creation of large-scale lexical knowledge resources (Hovy et al., 2013). Over the past few years, a wide-coverage multilingual “encyclopedic” dictionary, called BabelNet, has been developed (Navigli and Ponzetto, 2012a). BabelNet1 brings together WordNet and Wikipedia and provides a multilingual sense inventory that currently covers 6 </context>
<context position="15219" citStr="Navigli, 2009" startWordPosition="2409" endWordPosition="2410">average number of different senses seen for each lemma across the test sets. In all languages, often only a single sense of a lemma was used. Because the test set is constructed based on topical documents, infrequent lemmas mostly occurred within a single document where they were used with a consistent interpreta225 tion. However, we note that in the case of lemmas that were only seen with a single sense, this sense does not always correspond to the most frequent sense as seen in SemCor. 3 Evaluation Task 12 uses the standard definitions of precision and recall for WSD evaluation (see, e.g., (Navigli, 2009)). Precision measures the percentage of the sense assignments provided by the system that are identical to the gold standard; Recall measures the percentage of instances that are correctly labeled by the system. When a system provides sense labels for all instances, precision and recall are equivalent. Systems using BabelNet and WordNet senses are compared against the Most Frequent Sense (MFS) baseline obtained by using the WordNet most frequent sense. For the Wikipedia sense inventory, we constructed a pseudo-MFS baseline by selecting (1) the Wikipedia page associated with the highest ranking</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>Roberto Navigli. 2009. Word Sense Disambiguation: A survey. ACM Computing Surveys, 41(2):1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
</authors>
<title>A quick tour of Word Sense Disambiguation, Induction and related approaches.</title>
<date>2012</date>
<booktitle>In Proceedings of the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM),</booktitle>
<pages>115--129</pages>
<contexts>
<context position="921" citStr="Navigli, 2012" startWordPosition="123" endWordPosition="124">n multilingual Word Sense Disambiguation. We describe our experience in producing a multilingual sense-annotated corpus for the task. The corpus is tagged with BabelNet 1.1.1, a freely-available multilingual encyclopedic dictionary and, as a byproduct, WordNet 3.0 and the Wikipedia sense inventory. We present and analyze the results of participating systems, and discuss future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tas</context>
</contexts>
<marker>Navigli, 2012</marker>
<rawString>Roberto Navigli. 2012. A quick tour of Word Sense Disambiguation, Induction and related approaches. In Proceedings of the 38th Conference on Current Trends in Theory and Practice of Computer Science (SOFSEM), pages 115–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Edward Loper</author>
<author>Dmitriy Dligach</author>
<author>Martha Palmer</author>
</authors>
<title>SemEval-2007 Task-17: English lexical sample, SRL and all words.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>87--92</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1680" citStr="Pradhan et al., 2007" startWordPosition="238" endWordPosition="241">en more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2010) and cross-lingual lexical substitution (Mihalcea et al., 2010) were organized. While these tasks addressed the multilingual aspect of sense-level text understanding, they departed from the traditional WSD paradigm, i.e., the automatic assignment of senses from an existing inventory, and instead focused on lexical substitution (</context>
</contexts>
<marker>Pradhan, Loper, Dligach, Palmer, 2007</marker>
<rawString>Sameer Pradhan, Edward Loper, Dmitriy Dligach, and Martha Palmer. 2007. SemEval-2007 Task-17: English lexical sample, SRL and all words. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), Prague, Czech Republic, pages 87–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Didier Schwab</author>
<author>J´erˆome Goulian</author>
<author>Andon Tchechmedjiev</author>
<author>Herv´e Blanchon</author>
</authors>
<title>Ant colony algorithm for the unsupervised word sense disambiguation of texts: Comparison and evaluation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 241h International Conference on Computational Linguistics (COLING),</booktitle>
<pages>8--15</pages>
<location>Mumbai, India.</location>
<contexts>
<context position="17762" citStr="Schwab et al., 2012" startWordPosition="2816" endWordPosition="2819">DAEBAK! submitted one system called PD (Peripheral Diversity) based on BabelNet path indices from the BabelNet synset graph. Using a f5 sentence window around the target word, a graph is constructed for all senses of co-occurring lemmas following the procedure proposed by Navigli and Lapata (2010). The final sense is selected based on measuring connectivity to the synsets of neighboring lemmas. The MFS is used as a backoff strategy when no appropriate sense can be picked out. GETALP GETALP submitted three systems, two for BabelNet and one for WordNet, all based on the ant-colony algorithm of (Schwab et al., 2012), which uses the sense inventory network structure to identify paths connecting synsets of the target lemma to the synsets of other lemmas in context. The algorithm requires setting several parameters for the weighting of the structure of the contextbased graph, which vary across the three systems. The BN1 system optimizes its parameters from the trial data, while the BN2 and WN1 systems are completely unsupervised and optimize their parameters directly from the structure of the BabelNet and WordNet graphs. UMCC-DLSI UMCC-DLSI submitted three systems based on the ISR-WN resource (Guti´errez et</context>
</contexts>
<marker>Schwab, Goulian, Tchechmedjiev, Blanchon, 2012</marker>
<rawString>Didier Schwab, J´erˆome Goulian, Andon Tchechmedjiev, and Herv´e Blanchon. 2012. Ant colony algorithm for the unsupervised word sense disambiguation of texts: Comparison and evaluation. In Proceedings of the 241h International Conference on Computational Linguistics (COLING), pages 8–15, Mumbai, India.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Snyder</author>
<author>Martha Palmer</author>
</authors>
<title>The english all-words task.</title>
<date>2004</date>
<booktitle>In Proceedings of ACL 2004 SENSEVAL-3 Workshop,</booktitle>
<pages>41--43</pages>
<location>Barcelona,</location>
<contexts>
<context position="1301" citStr="Snyder and Palmer, 2004" startWordPosition="177" endWordPosition="180">s future directions. 1 Introduction Word Sense Disambiguation (WSD), the task of automatically assigning predefined meanings to words occurring in context, is a fundamental task in computational lexical semantics (Navigli, 2009; Navigli, 2012). Several Senseval and SemEval tasks have been organized in the past to study the performance and limits of disambiguation systems and, even more importantly, disambiguation settings. While an ad-hoc sense inventory was originally chosen for the first Senseval edition (Kilgarriff, 1998; Kilgarriff and Palmer, 2000), later tasks (Edmonds and Cotton, 2001; Snyder and Palmer, 2004; Mihalcea et al., 2004) focused on WordNet (Miller et al., 1990; Fellbaum, 1998) as a sense inventory. In 2007 the issue of the fine sense granularity of WordNet was addressed in two different SemEval disambiguation tasks, leading to the beneficial creation of coarsergrained sense inventories from WordNet itself (Navigli et al., 2007) and from OntoNotes (Pradhan et al., 2007). In recent years, with the exponential growth of the Web and, consequently, the increase of nonEnglish speaking surfers, we have witnessed an upsurge of interest in multilinguality. SemEval-2010 tasks on cross-lingual Wo</context>
</contexts>
<marker>Snyder, Palmer, 2004</marker>
<rawString>Benjamin Snyder and Martha Palmer. 2004. The english all-words task. In Proceedings of ACL 2004 SENSEVAL-3 Workshop, pages 41–43, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33Td Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="20249" citStr="Yarowsky, 1995" startWordPosition="3211" endWordPosition="3212"> Finally, the RUN-3 system DAEBAK! PD UMCC-DLSI Run-2 initializes using all words in the sentence. GETALP BN-2 WSD F1 5 Results and Discussion All teams submitted at least one system using the BabelNet inventory, shown in Table 3. The UMCCDLSI systems were consistently able to outperform the MFS baseline (a notoriously hard-to-beat heuristic) in all languages except German. Additionally, the DAEBAK! system outperformed the MFS baseline on French and Italian. The UMCC-DLSI RUN2 system performed the best for all languages. Notably, this system leverages the single-sense per discourse heuristic (Yarowsky, 1995), which uses the same sense label for all occurrences of a lemma in a document. UMCC-DLSI submitted the only three systems to use Wikipedia-based senses. Table 4 shows their performance. Of the three sense inventories, Wikipedia had the most competitive MFS baseline, scoring at least 0.694 on all languages. Notably, the Wikipedia-based system has the lowest recall of all systems. Despite having superior precision to the MFS baseline, the low recall brought the resulting F1 measure below the MFS. Two teams submitted four total systems for WordNet, shown in Table 5. The UMCC-DLSI RUN-2 system wa</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>David Yarowsky. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33Td Annual Meeting of the Association for Computational Linguistics, pages 189–196, Cambridge, MA, USA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>