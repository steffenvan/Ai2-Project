<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.953031">
Contrasting the Interaction Structure of an Email and a Telephone
Corpus: A Machine Learning Approach to Annotation of Dialogue
Function Units
</title>
<author confidence="0.999046">
Jun Hu
</author>
<affiliation confidence="0.996877">
Department of Computer Science
Columbia University
</affiliation>
<address confidence="0.915103">
New York, NY, USA
</address>
<email confidence="0.99873">
jh2740@columbia.edu
</email>
<note confidence="0.8900595">
Rebecca J. Passonneau
CCLS
</note>
<affiliation confidence="0.697085">
Columbia University
</affiliation>
<address confidence="0.919897">
New York, NY, USA
</address>
<email confidence="0.998821">
becky@cs.columbia.edu
</email>
<author confidence="0.740918">
Owen Rambow
</author>
<affiliation confidence="0.7561155">
CCLS
Columbia University
</affiliation>
<address confidence="0.91805">
New York, NY, USA
</address>
<email confidence="0.99936">
rambow@ccls.columbia.edu
</email>
<sectionHeader confidence="0.994816" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999836">
We present a dialogue annotation scheme
for both spoken and written interaction,
and use it in a telephone transaction cor-
pus and an email corpus. We train classi-
fiers, comparing regular SVM and struc-
tured SVM against a heuristic baseline.
We provide a novel application of struc-
tured SVM to predicting relations between
instance pairs.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999981176470588">
We present an annotation scheme for verbal inter-
action which can be applied to corpora that vary
across many dimensions: modality of signal (oral,
textual), medium (e.g., email, voice alone, voice
over electronic channel), register (such as infor-
mal conversation versus formal legal interroga-
tion), number of participants, immediacy (online
versus offline), and so on.1 We test it by anno-
tating transcribed phone conversations and email
threads. We then use three algorithms, two of
which use machine learning (including a novel ap-
proach to using Structured SVM), to predict labels
and links (a generalization of adjacency pairs) on
unseen data. We conclude that we can indeed use
a common annotation scheme, and that the email
modality is easier to tag for dialogue acts, but that
it is harder in email to find the links.
</bodyText>
<sectionHeader confidence="0.999853" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.998773333333333">
Annotation for dialogue acts (DAs), inspired by
Searle and Austin’s work on speech acts, arose
largely as a means to understand, evaluate and
</bodyText>
<footnote confidence="0.955442625">
1This research was supported in part by the National Sci-
ence Foundation under grants IIS-0745369 and IIS-0713548,
and by the Human Language Technology Center of Excel-
lence. Any opinions, findings, and conclusions or recom-
mendations expressed in this material are those of the au-
thors and do not necessarily reflect the views of the sponsors.
We would like to thank three anonymous reviewers for their
thoughtful comments.
</footnote>
<bodyText confidence="0.999890170731707">
model human-human and human-machine com-
munication. The need for the enterprise derives
from the fact that the relationship between lexico-
grammatical form (including mood, e.g., interrog-
ative) and communicative actions cannot be enu-
merated; there are complex dependencies on the
linguistic and situational contexts of use. Many
DA schemes exist: they can be hierarchical or
flat (Popescu-Belis, 2008), can comprise a large
(Devillers et al., 2002; Hardy et al., 2003) or small
repertoire (Komatani et al., 2005), or can be ori-
ented towards human-human dialogue (Allen and
Core, 1997; Devillers et al., 2002; Thompson et
al., 1993; Traum and Heeman, 1996; Stolcke et
al., 2000) or multi-party interactions (Galley et al.,
2004), or human-computer interaction (Walker
and Passonneau, 2001; Hardy et al., 2003), in-
cluding multimodal ones (Thompson et al., 1993;
Kruijff-Korbayov´a et al., 2006).
A major focus of the cited work is on how to
recognize or generate speech acts for interactive
systems, or how to classify speech acts for dis-
tributional analyses. The focus can be on a spe-
cific type of speech act (e.g., grounding and re-
pairs (Traum and Heeman, 1996; Frampton and
Lemon, 2008)), or on more general comparisons,
such as the contrast between human-human and
human-computer dialogues (Doran et al., 2001).
While there is a large degree of overlap across
schemes, the set of DA types will differ due to dif-
ferences in the nature of the communicative goals;
thus information-seeking versus task-oriented di-
alogues differ in the set of speech acts and their
relative frequencies.
Our motivation in providing a new DA annota-
tion scheme is that our focus differs from much
of this prior work. We aim for a relatively ab-
stract annotation scheme in order to make compar-
isons across interactions of widely differing prop-
erties. Our initial focus is less on speech act types
and more on the patterns of local alternation be-
</bodyText>
<note confidence="0.710226">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 357–366,
</note>
<affiliation confidence="0.662308">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.997783">
357
</page>
<bodyText confidence="0.999906616666667">
tween an initiating speech act and a responding
one–the analog of adjacency pairs (Sacks et al.,
1974). The most closely related effort is (Gal-
ley et al., 2004), which aims to automatically
identify adjacency pairs in the ICSI Meeting cor-
pus, a large corpus of 75 meetings, using a small
tagset. Their maximum entropy ranking approach
achieved 90% accuracy on the 4-way classifica-
tion into agreement, disagreement, backchannel
and other. Using the switchboard corpus, (Stolcke
et al., 2000) achieved good dialogue act labeling
accuracy (71% on manual transcriptions) for a set
of 42 dialogue act types, and constructed proba-
bilistic models of dialogue act sequencing in order
to test the hypothesis that dialogue act sequence
information could boost speech recognition per-
formance.
There has been far less work on developing
manual and automatic dialogue act annotation
schemes for email. We summarize some salient
recent work. Carvalho and Cohen (2006) use word
n-grams (with extensive preprocssing) to classify
entire emails into a complex ontology of speech
acts. However, in their experiments, they con-
centrate on detecting only a subset of speech acts,
which is comparable in size to ours. Speech acts
are assigned for entire emails, but several speech
acts can be assigned to one email. Apparently,
they develop separate binary classifiers for each
speech act. Corston-Oliver et al. (2004) are in-
terested in identifying tasks in email. They label
each sentence in email with tags from a set which
describes the type of content of the sentence (de-
scribing a task, scheduling a meeting), but are less
interested in the interactive aspect of email com-
munication (creating an obligation to respond).
There has been some work which relates to find-
ing links, but limited to finding question-answer
pairs. Shrestha and McKeown (2004) first de-
tect questions using lexical and part-of-speech fea-
tures, and then find the paragraph that answers the
question. They use features related to the structure
of the email thread, as well as lexical features. As
do we, they find that classifying is easier than link-
ing.
Ding et al. (2008) argue that in order to do
well at finding answers to questions, one must
also find the context of the question, since it of-
ten contains the information needed to identify the
answer. They use a corpus of online discussion
forums, and use slip-CRFs and two-dimensional
CRFs, models related to those we use. We will
investigate their proposal to consider the question
context in future work.
While they do not use dialogue act tagging
to compare modalities, as we do, Murray and
Carenini (2008) compare spoken conversation
with email by comparing a common summariza-
tion architecture across both modalities. They get
similar performance, but the features differ.
</bodyText>
<tableCaption confidence="0.997949">
Table 1: DFU speech act labels
</tableCaption>
<figure confidence="0.989015777777778">
Request-Information (R-I)
Request-Action (R-A)
Inform (Inf)
Commit (Comm)
Conventional (Conv)
Perform (Perf)
Backchannel (Bch) (+/- Grounding)
Other
3 Annotation Scheme
</figure>
<figureCaption confidence="0.9837245">
Figure 1: Example DFU illustrating the relation of
extent (segmentation) to speech act type
</figureCaption>
<bodyText confidence="0.931153888888889">
M1.2 I have completed the invoices for April,
May and June
M1.3 and we owe Pasadena each month for a to-
tal of $3,615,910.62.
M1.4 I am waiting to hear back from Patti on May
and June to make sure they are okay with her.
[Inform(1.2-1.4): status of Pasadena invoicing-
completed &amp; pending approval – versus amount
due]
</bodyText>
<equation confidence="0.3642696">
Sffink(1.2-1.4)
M2.1 That’s fine.
[Inform(2.1): acknowledgement of status of
Pasadena invoicing]
Blink(1.2-1.4)
</equation>
<bodyText confidence="0.9999606">
The annotation scheme presented here consists
of Dialogue Function Units (DFUs), which are
intended to represent abstract units of interac-
tion. The last two authors developed the annota-
tion on three contrasting corpora: email threads,
telephone conversations, and court transcripts. It
builds on our previous work in intention-based
segmentation (Passonneau and Litman, 1997),
and on mixing a formal schema with natural lan-
guage descriptions (Nenkova et al., 2007). In this
</bodyText>
<page confidence="0.997636">
358
</page>
<bodyText confidence="0.999900807228916">
paper, we investigate the modalities of telephone
two-person conversation in a library setting, and
multi-party email in a workplace setting. Our ini-
tial focus is on the structure of turn-taking. By
using a relatively abstract annotation scheme, we
can compare and contrast this behavior across dif-
ferent types of interaction.
Our unit of annotation is the DFU. DFUs have
an extent, a dialogue act (DA) label along with
a description, and possibly one or more forward
and/or backward links. We explain each compo-
nent of the annotation in turn. We use the exam-
ple in Figure 1; the example is drawn from actual
messages, but has been modified to yield a more
succinct example.
The extent of a DFU roughly corresponds to that
portion of a turn (conversational turn; email mes-
sage; etc.) that corresponds to a coherent com-
municative intention. Because we do not address
automatic identification of the segmentation into
DFU units in this paper, we do not discuss how
annotators are instructed to identify extent.
As illustrated in Figure 1, the communicative
function of a DFU is captured by a speech act
type, and a natural language description. This is
somewhat analogous to the natural language de-
scriptions associated with Summary Content Units
(SCUs) in pyramid annotation (Nenkova et al.,
2007), or with the intention-based segmentation
of (Passonneau and Litman, 1997). The pur-
pose in all cases is to require annotators to artic-
ulate briefly but specifically the unifying intention
(Passonneau and Litman, 1997), semantic content
(Nenkova et al., 2007), or speech act. We use the
eight dialogue act types listed in the upper left of
Table 1. To accommodate discontinuous speech
acts, due to the interruptions that are common to
conversation, each speech act can have an oper-
ator affix such as “-Continue”. We have previ-
ously shown (Passonneau and Litman, 1997) that
intention-based segmentation can be done reliably
by multiple annotators. For twenty narratives each
segmented by the same seven annotators, using
Cochran’s Q (Cochran, 1950), we found the prob-
abilities associated with the null hypothesis that
the observed distributions could have arisen by
chance to be at or below p=0.1 x10−6. Partition-
ing Q by number of annotators gave significant re-
sults for all values of A ranging over the number
of annotators apart from A = 2. We would expect
similar patterns of agreement on DFU segmen-
tation, but have not collected segmentation data
from multiple annotators on the two corpora pre-
sented here.
DFU Links, or simply Links, correspond to ad-
jacency pairs, but need not be adjacent. A forward
link (Flink) is the analog of a “first pair-part” of
an adjacency pair (Sacks et al., 1974), and is sim-
ilarly restricted to specific speech act types. All
Request-Information and Request-Action DFUs
are assigned Flinks. The responses to such re-
quests are assigned a backward link (Blink). In
principle, a response can be any of the speech act
types, thus it can be an answer to a question (In-
form), a rejection of a Request-Action or a com-
mitment to take the requested action (Commit),
a request for clarification (Request-Information),
and so on. In most but not all cases, requests are
responded to, thus most Flinks and Blinks come in
pairs. We refer to Flinks with no matching Blink
as dangling links. If an utterance can be inter-
preted as a response to a preceding DFU, it will
get a Blink even where the preceding DFU has no
Flink. The preceding DFU taken to be the “first
pair-part” of the Link will be assigned a secondary
forward link (Sflink). All links except dangling
links are annotated with the address of the DFU
from which they originate. Figure 1 illustrates an
email message (M2) containing a single sentence
(“That’s fine”) that is a response to a DFU in a
prior email (M1), where the prior email had no
Flink because it only contains Inform DAs; thus
M1 gets an Sflink.
</bodyText>
<sectionHeader confidence="0.996137" genericHeader="method">
4 Corpora
</sectionHeader>
<bodyText confidence="0.999948529411765">
The Loqui corpus consists of 82 transcribed dia-
logues from a larger set of 175 dialogues that were
recorded at New York City’s Andrew Heiskell
Braille and Talking Book Library during the sum-
mer of 2005. All of the transcribed dialogues per-
tain to one or more book requests. Forty-eight
dialogues were annotated; the annotators worked
from a combination of the transcription and the au-
dio. Three annotators were trained together, anno-
tated up to a dozen dialogues independently, then
discussed, adjudicated and merged ten of them.
During this phase, the annotation guidelines were
refined and revised. One of the three annotators
subsequently annotated 38 additional dialogues.
We also annotated 122 email threads of the En-
ron email corpus, consisting of email messages
in the inboxes and outboxes of Enron corporation
</bodyText>
<page confidence="0.999251">
359
</page>
<tableCaption confidence="0.9178775">
Table 2: Distributional Characteristics of Dialogue
Acts in Enron and Loqui
</tableCaption>
<table confidence="0.9997075">
Loqui Enron
Words 21097 17924
DFUs 3845 1400
Speech Act Labels
Inform 1928 50% 853 61%
Request-Inf. 761 20% 149 11%
Request-Action 39 1% 37 3%
Commit 338 9% 3 0%
Conventional 254 7% 356 25%
Backchannel 507 13% 0 0
Other 18 0% 2 0%
Total 3845 100% 1400 100%
Links
Paired Links 1204 63% 193 28%
Flink/Blink 702 58% 83 43%
Sflink/Blink 502 42% 110 57%
Dangling Links 90 2% 97 7%
Mutliple Blinks 4 0% 4 0%
Links by Speech Act Labels
Inform 1003 83% 142 74%
Request-Inf. 170 14% 44 23%
Request-Action 1 0% 5 3%
Commit 13 1% 2 1%
Conventional 2 0% 0 0
Backchannel 15 1% 0 0
1204 100% 193 100%
</table>
<bodyText confidence="0.999812754716981">
employees. Most of the emails are concerned with
exchanging information, scheduling meetings, and
solving problems, but there are also purely social
emails. We used a version of the corpus with some
missing messages restored from other emails in
which they were quoted (Yeh and Harnly, 2006).
The annotator of the majority of the Loqui corpus
also annotated the Enron corpus. She received ad-
ditional training and guidance based on our experi-
ence with a pilot annotator who helped us develop
the initial guidelines.
Table 2 illustrates differences between the two
corpora. The DFUs in the Loqui data are much
shorter, with 5.5 words on average compared with
12.8 words in Enron. The distribution of DFU la-
bels shows a similarly high proportion of Inform
acts, comprising 50% of all Loqui DFUs and 61%
of all Enron DFUs. Otherwise, the distributions
are quite distinct. The Loqui interactions are all
two party telephone dialogues where the callers
(library patrons) tend to have limited goals (re-
questing books). The Enron threads consist of
two or more parties, and exhibit a much broader
range of communicative goals. In the Loqui data,
backchannels are relatively frequent (13%) but do
not occur in the email corpus for obvious reasons.
There are some Commits (9%), typically reflect-
ing cases where the librarian indicates she will
send requested items to the caller by mail, or place
them on reserve. There are no Commits in the
Enron data. Neither corpus has many Request-
Actions; the Loqui corpus has many more requests
for information, which includes requests made by
the librarian, e.g., for the patrons’ identifying in-
formation, or by the caller.
The most striking differences between the two
corpora pertain to the distribution of DFU Links.
In Loqui, 63% of the DFUs are the first pair-part
or the second pair-part of a Link compared with
28% in Enron. In Loqui, the majority of Links
are initiated by overt requests (58% of Links are
Flink/Blink pairs), whereas in Enron, the major-
ity of Links involve SFlinks (57%). There are
relatively few dangling Links in either dataset,
with more than three times as many in Enron (7%
versus 2% in Loqui). Most of the DFU types
in the second pair-part of Links are Informs and
Request-Information, with a different proportion
in each dataset. In Loqui, 83% of DFUs that are
second pair-part of a Link are Informs compared
with 74% in Enron; correspondingly, only 14% of
DFUs in Links are Request-Information in Loqui
versus 23% in Enron.
</bodyText>
<sectionHeader confidence="0.9618565" genericHeader="method">
5 Dialogue Act Tagging and Link
Prediction
</sectionHeader>
<bodyText confidence="0.999797266666667">
There are two machine learning tasks in our prob-
lem. The first is Dialogue Act (DA) Tagging, in
which we assign DAs to every Dialogue Func-
tional Unit (DFU). The second is Link predic-
tion, in which we predict if two DFUs form a link
pair. In this paper, we assume that the DFUs are
given. We propose three systems to tackle the
problem. The first system is a non-strawman Base-
line Heuristics system, which uses the structural
characteristics of dialogue. The second is Regu-
lar SVM. The third is Structured SVM. Structured
SVM is a discriminative method that can predict
complex structured output. Recently, discrimi-
native Probabilistic Graphical Models have been
widely applied in structural problems (Getoor and
</bodyText>
<page confidence="0.987363">
360
</page>
<bodyText confidence="0.999504454545454">
Taskar, 2007) such as link prediction. However,
Structured SVM (Taskar et al., 2003; Tsochan-
taridis et al., 2005) is also a compelling method
which has the potential to handle the interdepen-
dence between labeling and sequencing, due to its
ability to handle dependencies among features and
prediction results within the structure. sequence
labeling (Tsochantaridis et al., 2005). We have
adapted Structured SVM to our problem, provided
a novel method for link prediction, and shown that
it is superior in some aspects to Regular SVM.
</bodyText>
<subsectionHeader confidence="0.713364">
5.1 Features
</subsectionHeader>
<bodyText confidence="0.9998902">
We have two sets of features. DFU features are as-
sociated with a particular DFU, and link features
describe the relationship between two DFUs. DFU
features are used in both tasks. Link features are
only used in link prediction. The feature vector of
a link contains two sets of DFU features and the
link features that are defined over the two DFUs.
Table 3 gives the features we used, which are al-
most identical for both corpora, so we could com-
pare the performance.
Because a lot of Flinks are questions, we
chose some features that are tailored to Question-
Answer detection, such as presence of a question
mark. Dialogue fillers and acceptance words af-
fect the accuracy of Part-Of-Speech tagging. On
the other hand, they are helpful indicators of dis-
fluency or confirmation. So we hand-picked a list
of filler and acceptance words, removed them from
the sentence, and added features counting their oc-
currences.
</bodyText>
<subsectionHeader confidence="0.994046">
5.2 Baseline Heuristics
</subsectionHeader>
<bodyText confidence="0.983328875">
Dialogue Act Tagging We use the most frequent
DA as the heuristic for prediction. In both Enron
and Loqui, this DA is Inform.
Link Prediction In link prediction, the heuris-
tics for Enron and Loqui corpora are different due
to structural differences. In Loqui, whenever we
see a DFU with a Forward Link (DA is Request-
Information or Request-Action), we predict that
the target of the link is the first following DFU that
is available and acceptable. “Available” means
that the second DFU has not been assigned a Back-
ward Link yet. “Acceptable” means that the sec-
ond DFU has a DA that is very frequent in a Back-
ward Link and it is of a different speaker to the
first DFU. We enforce similar constraints in Enron
corpus for link prediction, except that the second
</bodyText>
<tableCaption confidence="0.991206">
Table 3: DFU features (E: Enron, L: Loqui)
</tableCaption>
<table confidence="0.995746294117647">
Structural for DA prediction
E,L First three POS
E,L Relative Position in the Dialogue
E Existence of Question Mark
E,L Does the first POS start with “w” or “v”
E,L Length of the DFU
E Head, body, tail of the Message
E,L Dialogue Act (Only used in link prediction)
Lexical for DA prediction
E,L Bag of Words
E,L Number of Content Words
L Number of Filler Words, as “uh”, “hmm”
E,L Number of Acceptance Words, as “yes”
Structural for Link prediction
E,L The distance between two DFUs
Lexical for Link prediction
E,L Overlapping number of content words
</table>
<bodyText confidence="0.998217777777778">
DFU not only has to be from a different author,
but also has to be in a message which is a direct
descendant in the reply chain of the message that
contains the first DFU. The baseline link predic-
tion algorithm uses the DAs as predicted by the
Regular SVM. If we used the baseline DA predic-
tion, the result would be too low to make a valid
comparison against other systems in terms of link
prediction because all DAs would be identical.
</bodyText>
<subsectionHeader confidence="0.968724">
5.3 Regular SVM
</subsectionHeader>
<bodyText confidence="0.999627222222222">
We have used the Yamcha support vector machine
package (chasen.org/∼taku/software/yamcha/).
The advantage of Yamcha is that it extends the
traditional SVM by enabling using dynamically
generated features such as preceding labels.
Dialogue Act Tagging We use the feature vector
of the current DFU as well as the predicted DA of
the preceding DFU as features to predict the DA
of the current DFU.
Link Prediction First, in order to limit search
space, we specify a certain window size to produce
a space S of DFU pairs under consideration. For
a particular DFU, we look at all succeeding DFUs
and check if these two DFUs satisfy the follow-
ing constraint: in Loqui, they must be of different
speakers; in Email, one must be another’s ancestor
and they must be of different authors. We consider
all valid pairs starting from the current DFU until
</bodyText>
<page confidence="0.997215">
361
</page>
<bodyText confidence="0.999905857142857">
the number of considered valid pairs reaches the
window size. Then we proceed to the next DFU
and collect more DFU pairs into our consideration
space.
Second, we train a link binary classifier with all
DFU pairs in this consideration space along with a
binary classification correct/not correct as training
data. This classifier takes the feature vectors of the
two DFUs as well as the link features such as the
distance between these two DFUs as features.
Third, we apply a greedy algorithm to gener-
ate links in the test data with the binary classifier.
The algorithm firstly uses the classifier to generate
scores for all DFU pairs in the consideration space
of the test data, then it scans the dialogue sequen-
tially, checks all preceding DFUs that are allowed
to link to the current DFU (i.e., the DFU pair is in
the consideration space), and assigns correspond-
ing links to the most likely DFU pair. We impose a
restriction that there can be at most one Flink, one
Sflink and one Blink for any given DFU.
</bodyText>
<subsectionHeader confidence="0.978783">
5.4 Structured SVM
</subsectionHeader>
<bodyText confidence="0.9996705">
A Structured SVM is able to predict com-
plex output instead of simply a binary result
as in a regular SVM. There are several vari-
ants. We have followed the margin-rescaling ap-
proach (Tsochantaridis et al., 2005), and im-
plemented our systems using SVMpython, which
is a python interface to the SVMst&amp;quot;&apos; package
(svmlight.joachims.org/svm struct.html). Gener-
ally, Structured SVM learns a discriminant func-
tion F : X x Y —* R, which estimates a score of
how likely the output y is given the input x. Cru-
cially, y can be a complex structure. Section A in
the appendix; here, we summarize the main intu-
itions.
Dialogue Act Tagging The input x is a sequence
of DFUs, and y is the corresponding sequence of
DAs to predict. Compared to Regular SVM, in-
stead of predicting yt one at a time, Structured
SVM optimizes the sequence as a whole and pre-
dicts all labels simultaneously. Due to the similar-
ity to HMM, the maximization problem is solved
by the Viterbi algorithm (Tsochantaridis et al.,
2005).
Link Prediction The input now contains the DFU
sequence, a link consideration space, as well as
a label sequence, which we get from the previ-
ous stage. The output structure chooses among
the possible links in the link consideration space,
such that there is at most one Flink/SFlink or Blink
for any given DFU, and that there are no crossing
links. (Note that all the constraints are only en-
forced in training and prediction; in testing, we
compare results against the complete manual an-
notations which do not follow these constraints.)
Then the maximization problem can be solved by a
straightforward dynamic programming algorithm.
</bodyText>
<tableCaption confidence="0.998964">
Table 4: Result of DA prediction
</tableCaption>
<table confidence="0.998784333333333">
Baseline Regular Struct
Loqui 50.14% 68.30% 70.26%
Enron 60.93% 88.34% 88.71%
</table>
<tableCaption confidence="0.316068666666667">
Note: Structured SVM parameters for Loqui are C =
300, α = 1; Structured SVM parameters for Enron
are C = 1000, α = 1.
</tableCaption>
<sectionHeader confidence="0.990928" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.99796871875">
We have three hypotheses for our experiments:
Hypothesis 1 Link prediction is harder than Dia-
logue Act prediction.
Hypothesis 2 Enron is harder than Loqui.
Hypothesis 3 Structured SVM is better than Reg-
ular SVM, and Baseline is the worst.
We have applied the algorithm described in Sec-
tion 5 to both the Enron and Loqui corpora. The
data set is annotated with DFUs; we focus on the
DA labels and Links. As discussed before, every
system is a pipeline that would preprocess the data
into separate DFUs, predict the Dialogue Acts,
and then feed the Dialogue Acts into the link pre-
diction algorithm. The size of the data set is shown
in Table 2. We do five-fold cross-validation.
Table 4 shows the accuracy of three systems on
Enron and Loqui. Structured SVM has a clear lead
to Regular SVM in Loqui; but the advantage is less
clear in Enron. Tables 6 and 7 give detailed results
of DA prediction.We do not show DAs that do not
exist in the corpora, or that were not predicted by
the algorithms. Both Regular SVM and Structured
SVM performed consistently for the two corpora.
Table 5 gives Link prediction results. Note that
when we compute the combined result for both
types of links, we are only concerned with the
Link position. The seperate results for Flink/Blink
and Sflink/Blink require us to identify the types of
links first, so here we not only compare the posi-
tion of predicted links against the gold, but also
require predicted DAs to indicate the link type
(e.g., the DA of the first DFU must be Request-
</bodyText>
<page confidence="0.997787">
362
</page>
<tableCaption confidence="0.997049">
Table 5: Link Prediction for Enron and Loqui
</tableCaption>
<table confidence="0.9955191">
Baseline Regular Struct
Enron R P F R P F R P R
Paired Links 16.66% 40% 23.52% 18.75% 55.38% 28.01% 31.25% 39.47% 34.88%
Flink/Blink 32.53% 33.75% 33.13% 26.50% 61.11% 36.97% 34.93% 47.54% 40.27%
Sflink/Blink 0.0% 0.0% 0.0% 11.92% 44.82% 18.83% 22.93% 27.47% 25.00%
Loqui
Paired Links 30% 56.15% 39.11% 43.59% 60.60% 50.71% 44.15% 56.02% 49.38%
Flink/Blink 43.30% 46.47% 44.83% 40.58% 57.73% 47.66% 43.55% 60.04% 50.48%
Sflink/Blink 0.0% 0.0% 0.0% 21.76% 29.36% 25.00% 22.88% 26.24% 24.45%
Note: Structured SVM parameters for Enron are C = 2000, Q = 2., for Loqui C = 1000, Q = 4.
</table>
<tableCaption confidence="0.854187">
Information or Request-Action to qualify as a
Flink/Blink).
Table 6: Recall/Precision/F-measure of DA pre-
diction for Loqui (in %)
</tableCaption>
<table confidence="0.999897125">
Regular Struct
P R F P R F
R-A 50.0 51.7 50.9 43.3 43.3 43.3
R-I 51.3 61.1 55.8 52.3 71.2 60.3
Inf 73.9 73.0 73.5 76.9 74.1 75.5
Bch 65.3 51.7 57.7 65.1 53.6 58.8
Com 5.6 33.3 9.5 5.6 33.3 9.5
Conv 81.2 84.0 82.6 83.7 83.3 83.5
</table>
<tableCaption confidence="0.9737395">
Table 7: Recall/Precision/F-measure of DA pre-
diction for Enron (in %)
</tableCaption>
<table confidence="0.999705">
Regular Struct
R P F R P F
R-A 27.8 55.6 37.0 25.0 75.0 37.5
R-I 77.9 82.3 80.0 77.2 83.3 80.1
Inf 92.5 90.6 91.5 92.1 91.2 91.7
Conv 90.5 87.3 88.9 93.4 85.6 89.3
</table>
<sectionHeader confidence="0.996121" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999910815789474">
Hypothesis 1 The result of DA prediction is dras-
tically better than link prediction. There are usu-
ally indicators of DA types such as “thank you” for
Conventional, so learning algorithms could easily
capture them. But in link prediction, we frequently
need to handle deep semantic inference and some-
times useful information exists in the surrounding
context rather than the DFU itself. Both of these
scenarios imply that in order to predict links or re-
lationships better, we need more sophisticated fea-
tures.
Hypothesis 2 This hypothesis turns out to be half-
correct. The DA prediction accuracy for Enron
is better than that of Loqui. The higher percent-
age of Inform and less diversity of DAs in Enron
(See Appendix for statistics) may be part of the
reason. Another possible explanation is that as a
set of spoken dialogue data, Loqui is inherently
more difficult to process than written form, since
some common tasks such Part-Of-Speech tagging
have lower accuracy for spoken data. On the other
hand, the result of link prediction did confirm our
hypothesis. The first reason is that there are far
fewer links in Enron than in Loqui, so we have less
training data. The tree structure of the reply chain
in the email threads also makes prediction more
difficult. And the link distance is longer, because
in email, people can respond to a very early mes-
sage, while in a phone conversation, people tend
to respond to immediate requests.
Hypothesis 3 Both SVM models perform better
than the baseline. Generally, Structured SVM per-
forms better than Regular SVM, especially in link
prediction for Enron. This confirms the advan-
tage of using Structured SVM for output involv-
ing inter-dependencies. The only exception is the
Sflink prediction in Loqui, which in turn affects
the overall accuracy of link prediction.
</bodyText>
<sectionHeader confidence="0.972067" genericHeader="references">
References
</sectionHeader>
<subsectionHeader confidence="0.943513">
James Allen and Mark Core. 1997. Damsl:
</subsectionHeader>
<bodyText confidence="0.559041142857143">
Dialogue act markup in several layers.
http://www.cs.rochester.edu/research/cisd/resources/damsl.
Vitor Carvalho and William Cohen. 2006. Improving ”email
speech acts” analysis via n-gram selection. In Proceed-
ings of the Analyzing Conversations in Text and Speech.
William G. Cochran. 1950. The comparison of percentages
in matched samples. Biometrika, 37:256–266.
</bodyText>
<page confidence="0.997599">
363
</page>
<reference confidence="0.977100376470588">
Simon Corston-Oliver, Eric Ringger, Michael Gamon, and
Richard Campbell. 2004. Task-focused summarization of
email. In Stan Szpakowicz Marie-Francine Moens, edi-
tor, Text Summarization Branches Out: Proceedings of the
ACL-04 Workshop.
Laurence Devillers, Sophie Rosset, Bonneau-Helene May-
nard, and Lamel Lori. 2002. Annotations for dynamic
diagnosis of the dialog state. In LREC.
Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu.
2008. Using conditional random fields to extract contexts
and answers of questions from online forums. In Proceed-
ings of ACL-08: HLT, Columbus, Ohio.
Christine Doran, John Aberdeen, Laurie Damianos, and
Lynette Hirschman. 2001. Comparing several aspects of
human-computer and human-human dialogues. In Pro-
ceedings of the 2nd SIGDIAL Workshop on Discourse and
Dialogue.
Matthew Frampton and Oliver Lemon. 2008. Using dialogue
acts to learn better repair strategies for spoken dialogue
systems. In ICASSP.
Michel Galley, Kathleen McKeown, Julia Hirschberg, and
Elizabeth Shriberg. 2004. Identifying agreement and dis-
agreement in conversational speech: use of Bayesian net-
works to model pragmatic dependencies. In Proceedings
of the 42nd Annual Meeting of the Association for Com-
putational Linguistics, pages 669–676.
Lise Getoor and Ben Taskar, editors. 2007. Introduction to
Statistical Relational Learning. The MIT Press.
Hilda Hardy, Kirk Baker, Bonneau-Helene Maynard, Lau-
rence Devillers, Sophie Rosset, and Tomek Strza-
lkowski. 2003. Semantic and dialogic annotation
for automated multilingual customer service. In Eu-
rospeech/Interspeech.
Kazunori Komatani, Nayouki Kanda, Tetsuya Ogata, and Hi-
roshi G. Okuno. 2005. Contextual constraints based on
dialogue models in database search task for spoken dia-
logue systems. In Eurospeech.
Ivana Kruijff-Korbayov´a, Tilman Becker, Nate Blaylock,
Ciprian Gerstenberger, Michael Kaisser, Peter Poller, Ver-
ena Rieser, and Jan Schehl. 2006. The Sammie corpus of
multimodal dialogues with an mp3 player. In LREC.
Gabriel Murray and Giuseppe Carenini. 2008. Summarizing
spoken and written conversations. In EMNLP.
Ani Nenkova, Rebecca J. Passonneau, and Kathleen McKe-
own. 2007. The pyramid method: incorporating human
content selection variation in summarization evaluation.
ACM Transactions on Speech and Language Processing,
4(2).
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Computational Linguistics, 23(1).
Andrei Popescu-Belis. 2008. Dimensionality of dialogue act
tagsets: An empirical analysis of large corpora. LREC,
42(1).
Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson.
1974. A simplest systemics for the organization of turn-
taking for conversation. Language, 50(4).
Lokesh Shrestha and Kathleen McKeown. 2004. Detection
of question-answer pairs in email conversations. In COL-
ING.
Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth
Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor,
Rachel Martin, Carol Van Ess-Dykena, and Meteer Marie.
2000. Dialogue act modeling for automatic tagging and
recognition of conversational speech. International Jour-
nal of Computational Linguistics, 26(3).
Ben Taskar, Crlos Guestrin, and Daphne Koller. 2003. Max-
margin markov networks. In NIPS.
Henry S. Thompson, Anne H. Anderson, Ellen Gurman Bard,
Gwyneth Doherty-Sneddon, Alison Newlands, and Cathy
Sotillo. 1993. The HCRC map task corpus: Natural
dialogue for speech recognition. In Proceedings of the
DARPA Human Language Technology Workshop.
David Traum and Peter Heeman. 1996. Utterance units and
grounding in spoken dialogue. In Interspeech/ICSLP.
Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-
mann, and Yasemin Altun. 2005. Large margin methods
for structured and interdependent output variables. JMLR,
6.
Marilyn A. Walker and Rebecca Passonneau. 2001. Date:
A dialogue act tagging scheme for evaluation of spoken
dialogue systems. In HLT.
Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread re-
assembly using similarity matching. In Conference on
Email and Anti-Spam.
</reference>
<page confidence="0.997465">
364
</page>
<note confidence="0.647371">
A Appendix: Structured SVM
</note>
<bodyText confidence="0.960658">
This section provides mathematical background
for Secton 5.4. The hypothesis function is given
by:
f(x, w) = argmaxyEYF(x, y : w)
And in addition, we assume F to be linear to a
joint feature map Ψ(x, y).
</bodyText>
<equation confidence="0.810251">
F(x, y : w) = hw, Ψ(x, y)i
</equation>
<bodyText confidence="0.997018166666667">
We also define a loss function Δ(y, y) which de-
fines the deviation of the predicted output y to the
correct output.
As a result, given a sequence of training
examples,(x1, y1) · · · (xn, yn) ∈ X × Y, the
function we need to optimize becomes:
</bodyText>
<equation confidence="0.996348">
min �,,,,�2 Iwk2 + n Ei=1 Si
s.t. ∀i∀y ∈ Y\y(i) : hw, sΨi(y)i &gt;
Δ(y(i), y) − �i where,
C �
hw,�Ψi(y)i = w, Ψ(x(i),y(i)) − Ψ(x(i),y)
</equation>
<bodyText confidence="0.9980474">
w is optimized towards maximizing the margin
between the true structured output y and any
other suboptimal configurations for all training in-
stances.
A cutting plane optimization algorithm is im-
plemented in SVMstruct. However, for any prob-
lem, we need to implement the feature map
Ψ(x, y), the loss function Δ(y, y), and a maxi-
mization problem which enables the cutting plane
optimization, i.e.
</bodyText>
<equation confidence="0.86128">
y = argmaxyEYΔ(y(i), y) − hw, 60i(y)i
</equation>
<bodyText confidence="0.952551">
Only certain feature maps that would make
solving this maximization effectively, usually by
dynamic programming, could be handled this way.
For Dialogue Act Tagging, let x =
(x1, x2 ... xT) be the sequence of DFUs,
</bodyText>
<equation confidence="0.692445">
(y1, y2 . . . yT�
</equation>
<bodyText confidence="0.97853">
and y = the corresponding se-
quence of dialogue acts. O(xt) represents the DFU
features and O(xt) ∈ RD. yt ∈ L = {l1, ... , lK}
where L contains the set of available DAs. The
feature map is (Tsochantaridis et al., 2005):
</bodyText>
<equation confidence="0.776451666666667">
*(x y) — ETt= 1 O(xt) ⊗ A(yt) /
A(yt−1)⊗ A(yt)
—
</equation>
<bodyText confidence="0.999964785714286">
In analogy to an HMM, the lower part in
*(x, y) encodes the histogram of adjacent DA
transitions in y ; the upper part encodes the DA
emissions from a specific label to one dimension
in the DFU feature vector. Hence, the total num-
ber of dimensions in *(x, y) is K2 + DK. As
a result, F(x, y : w) = hw, *(x, y)i gives a
global score based on all transitions and emissions
in the sequence, which captures the dependecies
among nearby labels and mimics the behaviour of
an HMM. Figure 2 gives an example of how to
compute the feature map.
The loss function is the sum of all zero-one
losses across the sequence, i.e.
</bodyText>
<equation confidence="0.852793">
Δ(y, y) = α ETt=1 A(yt, yt)
</equation>
<bodyText confidence="0.982493">
α denotes a cost assigned to every DA loss.
For Link Prediction, the input contains the
DFU sequence x, a link consideration space
s = {(i, j) :, DFU i and j is being considered},
as well as label sequence y which we get from
the previous stage. cp(xi, xj) is the link feature
defined over two DFUs. Let the dimension of
link feature be B. The output structure u =
S u1, u2 ... u T I specifies the link plan. ut denotes
tlhat there is a link from DFU t − ut to t with the
exception that ut being zero denotes there is no
link pointing to t. The setup of u constraints that
there can be at most one Flink/SFlink or Blink for
any given DFU. In addition u is also subject to the
constraint that all specified links must be in the
link consideration space.
The discriminant function becomes F : X ×
Y×S×U → R. Similar to structured DA predic-
tion, the discriminant function should give a global
evaluation as to how likely is the link plan spec-
ified by U with respect to all the input vectors.
Our solution is to decompose the score, and cor-
respondingly, the feature representation into two
components, link emission and no-link emission;
the details can be found in Figure 3 in the appendix
and an example is in Figure 2.
Similarly, we could define the loss function as
the sum of all zero-one losses across the sequence
, i.e.
where A(yt) = [A(l1, y), ... , A(lk, y)] and A is
an indicator function that returns 1 if two parame-
ters are equal. ⊗-operator is defined as:
</bodyText>
<equation confidence="0.8692445">
RD × RK → RD-K, (a ⊗ b)i+(j−1)D ≡ ai · bj
Δ(u, u) = Q ETt=1 A(ut, ut)
</equation>
<bodyText confidence="0.81265">
Q denotes a cost assigned to every Link loss.
</bodyText>
<page confidence="0.99776">
365
</page>
<figureCaption confidence="0.997679">
Figure 2: A full example of feature map for Structured SVM
</figureCaption>
<equation confidence="0.936927222222222">
x1 = “are you you sure” Ψda = ⎛ 0 ⎞ Inform to Inform Ψlink = ⎛ 1 ⎞ 1st link pair-part with“are”
x2 = “sure” ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 0 ⎠⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟⎟ Inform to Req-Info ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ 2 ⎠ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ ⎟ 1st link pair-part with“you”
y1 = “Req-Info” 0 Req-Info to Inform 1 1st link pair-part with“sure”
y2 = “Inform” 1 Req-Info to Inform 0 1st link pair-part with Inform
u1 = 0 0 Inform with “are” 1 1st link pair-part with Req-Info
u2 = 1 0 Inform with “you” 0 2nd link pair-part with“are”
O(x1) = (1, 2, 1) 1 Inform with “sure” 0 2nd link pair-part with“you”
O(x2) = (0, 0, 1) 1 Req-Info with “are” 1 2nd link pair-part with“sure”
W(x1, x2) = (1, 1) 2 Req-Info with “you” 1 2nd link pair-part with Inform
</equation>
<table confidence="0.99796275">
1 Req-Info with “sure” 0 2nd link pair-part with Req-Info
1 distance of link
1 overlap of link
1 No-Link with“are”
2 No-Link with“you”
1 No-Link with“sure”
0 No-Link with Inform
1 No-Link with Req-Info
</table>
<reference confidence="0.39681225">
Note: In this example, O(xt) extracts the bag-of-words features from xt. “are”,“you”,“sure” are the 1st, 2nd
and 3rd DFU feature respectively. W(xi, xj) extracts the distance and number of the overlap content, which are
the link features, from the 1st and 2nd pair-part in a DFU link pair. There is a link from DFU 1 to DFU 2 as
specified by j − uj = i, but there is no link pointing to DFU 1.
</reference>
<figureCaption confidence="0.994954">
Figure 3: The feature map of link prediction for
the structured SVM
</figureCaption>
<equation confidence="0.982276214285714">
ΨL =
PT j=i+1 Λ(yi
i=1
, j − uj)
PT −1 PT j=i+1 φ(xj)λ(i, j − uj)
PT−1i=1 PT j=i+1 Λ(yj)λ(i, j − uj)
PT −1
i=1 PT j=i+1 ϕ(xi,xj)λ(i, j − uj)
i=1
PT i=1 φ(xi)λ(0, ui)
ΨNL =
PTi=1 Λ(yi)λ(0, ui)
Ψ(x, y, s, u) = ΨL !
ΨNL
</equation>
<bodyText confidence="0.94984275">
Note: ΨL and ΨNL correspond to the link and no-
link emissions in the feature map Ψ(x, y, s, u) re-
spectively as shown in the equations. The total di-
mension of the feature map is 3D + 3K + B.
</bodyText>
<figure confidence="0.965949222222222">
⎛
⎜ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝
PT −1 PT j=i+1 φ(xi)λ(i, j − uj)
PT−1
i=1
)λ(i
⎞
⎠⎟⎟⎟⎟⎟⎟
!
</figure>
<page confidence="0.981874">
366
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.451248">
<title confidence="0.998668666666667">Contrasting the Interaction Structure of an Email and a Corpus: A Machine Learning Approach to Annotation of Function Units</title>
<author confidence="0.995042">Jun</author>
<affiliation confidence="0.999963">Department of Computer</affiliation>
<address confidence="0.9036975">Columbia New York, NY, USA</address>
<email confidence="0.998344">jh2740@columbia.edu</email>
<author confidence="0.988116">J Rebecca</author>
<affiliation confidence="0.795548">Columbia</affiliation>
<address confidence="0.998313">New York, NY, USA</address>
<email confidence="0.999832">becky@cs.columbia.edu</email>
<author confidence="0.865673">Owen</author>
<affiliation confidence="0.787643">Columbia</affiliation>
<address confidence="0.998323">New York, NY, USA</address>
<email confidence="0.999862">rambow@ccls.columbia.edu</email>
<abstract confidence="0.9978921">We present a dialogue annotation scheme for both spoken and written interaction, and use it in a telephone transaction corpus and an email corpus. We train classifiers, comparing regular SVM and structured SVM against a heuristic baseline. We provide a novel application of structured SVM to predicting relations between instance pairs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Simon Corston-Oliver</author>
<author>Eric Ringger</author>
<author>Michael Gamon</author>
<author>Richard Campbell</author>
</authors>
<title>Task-focused summarization of email.</title>
<date>2004</date>
<booktitle>In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</booktitle>
<contexts>
<context position="5711" citStr="Corston-Oliver et al. (2004)" startWordPosition="900" endWordPosition="903">n performance. There has been far less work on developing manual and automatic dialogue act annotation schemes for email. We summarize some salient recent work. Carvalho and Cohen (2006) use word n-grams (with extensive preprocssing) to classify entire emails into a complex ontology of speech acts. However, in their experiments, they concentrate on detecting only a subset of speech acts, which is comparable in size to ours. Speech acts are assigned for entire emails, but several speech acts can be assigned to one email. Apparently, they develop separate binary classifiers for each speech act. Corston-Oliver et al. (2004) are interested in identifying tasks in email. They label each sentence in email with tags from a set which describes the type of content of the sentence (describing a task, scheduling a meeting), but are less interested in the interactive aspect of email communication (creating an obligation to respond). There has been some work which relates to finding links, but limited to finding question-answer pairs. Shrestha and McKeown (2004) first detect questions using lexical and part-of-speech features, and then find the paragraph that answers the question. They use features related to the structur</context>
</contexts>
<marker>Corston-Oliver, Ringger, Gamon, Campbell, 2004</marker>
<rawString>Simon Corston-Oliver, Eric Ringger, Michael Gamon, and Richard Campbell. 2004. Task-focused summarization of email. In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laurence Devillers</author>
<author>Sophie Rosset</author>
<author>Bonneau-Helene Maynard</author>
<author>Lamel Lori</author>
</authors>
<title>Annotations for dynamic diagnosis of the dialog state.</title>
<date>2002</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="2614" citStr="Devillers et al., 2002" startWordPosition="402" endWordPosition="405">this material are those of the authors and do not necessarily reflect the views of the sponsors. We would like to thank three anonymous reviewers for their thoughtful comments. model human-human and human-machine communication. The need for the enterprise derives from the fact that the relationship between lexicogrammatical form (including mood, e.g., interrogative) and communicative actions cannot be enumerated; there are complex dependencies on the linguistic and situational contexts of use. Many DA schemes exist: they can be hierarchical or flat (Popescu-Belis, 2008), can comprise a large (Devillers et al., 2002; Hardy et al., 2003) or small repertoire (Komatani et al., 2005), or can be oriented towards human-human dialogue (Allen and Core, 1997; Devillers et al., 2002; Thompson et al., 1993; Traum and Heeman, 1996; Stolcke et al., 2000) or multi-party interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional</context>
</contexts>
<marker>Devillers, Rosset, Maynard, Lori, 2002</marker>
<rawString>Laurence Devillers, Sophie Rosset, Bonneau-Helene Maynard, and Lamel Lori. 2002. Annotations for dynamic diagnosis of the dialog state. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shilin Ding</author>
<author>Gao Cong</author>
<author>Chin-Yew Lin</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Using conditional random fields to extract contexts and answers of questions from online forums.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="6442" citStr="Ding et al. (2008)" startWordPosition="1024" endWordPosition="1027">ibes the type of content of the sentence (describing a task, scheduling a meeting), but are less interested in the interactive aspect of email communication (creating an obligation to respond). There has been some work which relates to finding links, but limited to finding question-answer pairs. Shrestha and McKeown (2004) first detect questions using lexical and part-of-speech features, and then find the paragraph that answers the question. They use features related to the structure of the email thread, as well as lexical features. As do we, they find that classifying is easier than linking. Ding et al. (2008) argue that in order to do well at finding answers to questions, one must also find the context of the question, since it often contains the information needed to identify the answer. They use a corpus of online discussion forums, and use slip-CRFs and two-dimensional CRFs, models related to those we use. We will investigate their proposal to consider the question context in future work. While they do not use dialogue act tagging to compare modalities, as we do, Murray and Carenini (2008) compare spoken conversation with email by comparing a common summarization architecture across both modali</context>
</contexts>
<marker>Ding, Cong, Lin, Zhu, 2008</marker>
<rawString>Shilin Ding, Gao Cong, Chin-Yew Lin, and Xiaoyan Zhu. 2008. Using conditional random fields to extract contexts and answers of questions from online forums. In Proceedings of ACL-08: HLT, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christine Doran</author>
<author>John Aberdeen</author>
<author>Laurie Damianos</author>
<author>Lynette Hirschman</author>
</authors>
<title>Comparing several aspects of human-computer and human-human dialogues.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue.</booktitle>
<contexts>
<context position="3479" citStr="Doran et al., 2001" startWordPosition="543" endWordPosition="546">interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses. The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001). While there is a large degree of overlap across schemes, the set of DA types will differ due to differences in the nature of the communicative goals; thus information-seeking versus task-oriented dialogues differ in the set of speech acts and their relative frequencies. Our motivation in providing a new DA annotation scheme is that our focus differs from much of this prior work. We aim for a relatively abstract annotation scheme in order to make comparisons across interactions of widely differing properties. Our initial focus is less on speech act types and more on the patterns of local alte</context>
</contexts>
<marker>Doran, Aberdeen, Damianos, Hirschman, 2001</marker>
<rawString>Christine Doran, John Aberdeen, Laurie Damianos, and Lynette Hirschman. 2001. Comparing several aspects of human-computer and human-human dialogues. In Proceedings of the 2nd SIGDIAL Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Frampton</author>
<author>Oliver Lemon</author>
</authors>
<title>Using dialogue acts to learn better repair strategies for spoken dialogue systems.</title>
<date>2008</date>
<booktitle>In ICASSP.</booktitle>
<contexts>
<context position="3354" citStr="Frampton and Lemon, 2008" startWordPosition="525" endWordPosition="528">(Allen and Core, 1997; Devillers et al., 2002; Thompson et al., 1993; Traum and Heeman, 1996; Stolcke et al., 2000) or multi-party interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses. The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001). While there is a large degree of overlap across schemes, the set of DA types will differ due to differences in the nature of the communicative goals; thus information-seeking versus task-oriented dialogues differ in the set of speech acts and their relative frequencies. Our motivation in providing a new DA annotation scheme is that our focus differs from much of this prior work. We aim for a relatively abstract annotation scheme in order to make comparisons across inte</context>
</contexts>
<marker>Frampton, Lemon, 2008</marker>
<rawString>Matthew Frampton and Oliver Lemon. 2008. Using dialogue acts to learn better repair strategies for spoken dialogue systems. In ICASSP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Kathleen McKeown</author>
<author>Julia Hirschberg</author>
<author>Elizabeth Shriberg</author>
</authors>
<title>Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model pragmatic dependencies.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>669--676</pages>
<contexts>
<context position="2894" citStr="Galley et al., 2004" startWordPosition="448" endWordPosition="451"> the relationship between lexicogrammatical form (including mood, e.g., interrogative) and communicative actions cannot be enumerated; there are complex dependencies on the linguistic and situational contexts of use. Many DA schemes exist: they can be hierarchical or flat (Popescu-Belis, 2008), can comprise a large (Devillers et al., 2002; Hardy et al., 2003) or small repertoire (Komatani et al., 2005), or can be oriented towards human-human dialogue (Allen and Core, 1997; Devillers et al., 2002; Thompson et al., 1993; Traum and Heeman, 1996; Stolcke et al., 2000) or multi-party interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses. The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001). While there i</context>
<context position="4476" citStr="Galley et al., 2004" startWordPosition="708" endWordPosition="712"> work. We aim for a relatively abstract annotation scheme in order to make comparisons across interactions of widely differing properties. Our initial focus is less on speech act types and more on the patterns of local alternation beProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 357–366, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 357 tween an initiating speech act and a responding one–the analog of adjacency pairs (Sacks et al., 1974). The most closely related effort is (Galley et al., 2004), which aims to automatically identify adjacency pairs in the ICSI Meeting corpus, a large corpus of 75 meetings, using a small tagset. Their maximum entropy ranking approach achieved 90% accuracy on the 4-way classification into agreement, disagreement, backchannel and other. Using the switchboard corpus, (Stolcke et al., 2000) achieved good dialogue act labeling accuracy (71% on manual transcriptions) for a set of 42 dialogue act types, and constructed probabilistic models of dialogue act sequencing in order to test the hypothesis that dialogue act sequence information could boost speech rec</context>
</contexts>
<marker>Galley, McKeown, Hirschberg, Shriberg, 2004</marker>
<rawString>Michel Galley, Kathleen McKeown, Julia Hirschberg, and Elizabeth Shriberg. 2004. Identifying agreement and disagreement in conversational speech: use of Bayesian networks to model pragmatic dependencies. In Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics, pages 669–676.</rawString>
</citation>
<citation valid="true">
<title>Introduction to Statistical Relational Learning.</title>
<date>2007</date>
<editor>Lise Getoor and Ben Taskar, editors.</editor>
<publisher>The MIT Press.</publisher>
<marker>2007</marker>
<rawString>Lise Getoor and Ben Taskar, editors. 2007. Introduction to Statistical Relational Learning. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hilda Hardy</author>
<author>Kirk Baker</author>
<author>Bonneau-Helene Maynard</author>
<author>Laurence Devillers</author>
<author>Sophie Rosset</author>
<author>Tomek Strzalkowski</author>
</authors>
<title>Semantic and dialogic annotation for automated multilingual customer service.</title>
<date>2003</date>
<booktitle>In Eurospeech/Interspeech.</booktitle>
<contexts>
<context position="2635" citStr="Hardy et al., 2003" startWordPosition="406" endWordPosition="409">of the authors and do not necessarily reflect the views of the sponsors. We would like to thank three anonymous reviewers for their thoughtful comments. model human-human and human-machine communication. The need for the enterprise derives from the fact that the relationship between lexicogrammatical form (including mood, e.g., interrogative) and communicative actions cannot be enumerated; there are complex dependencies on the linguistic and situational contexts of use. Many DA schemes exist: they can be hierarchical or flat (Popescu-Belis, 2008), can comprise a large (Devillers et al., 2002; Hardy et al., 2003) or small repertoire (Komatani et al., 2005), or can be oriented towards human-human dialogue (Allen and Core, 1997; Devillers et al., 2002; Thompson et al., 1993; Traum and Heeman, 1996; Stolcke et al., 2000) or multi-party interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses. The focus </context>
</contexts>
<marker>Hardy, Baker, Maynard, Devillers, Rosset, Strzalkowski, 2003</marker>
<rawString>Hilda Hardy, Kirk Baker, Bonneau-Helene Maynard, Laurence Devillers, Sophie Rosset, and Tomek Strzalkowski. 2003. Semantic and dialogic annotation for automated multilingual customer service. In Eurospeech/Interspeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kazunori Komatani</author>
<author>Nayouki Kanda</author>
<author>Tetsuya Ogata</author>
<author>Hiroshi G Okuno</author>
</authors>
<title>Contextual constraints based on dialogue models in database search task for spoken dialogue systems.</title>
<date>2005</date>
<booktitle>In Eurospeech.</booktitle>
<contexts>
<context position="2679" citStr="Komatani et al., 2005" startWordPosition="413" endWordPosition="416">lect the views of the sponsors. We would like to thank three anonymous reviewers for their thoughtful comments. model human-human and human-machine communication. The need for the enterprise derives from the fact that the relationship between lexicogrammatical form (including mood, e.g., interrogative) and communicative actions cannot be enumerated; there are complex dependencies on the linguistic and situational contexts of use. Many DA schemes exist: they can be hierarchical or flat (Popescu-Belis, 2008), can comprise a large (Devillers et al., 2002; Hardy et al., 2003) or small repertoire (Komatani et al., 2005), or can be oriented towards human-human dialogue (Allen and Core, 1997; Devillers et al., 2002; Thompson et al., 1993; Traum and Heeman, 1996; Stolcke et al., 2000) or multi-party interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses. The focus can be on a specific type of speech act (e.g</context>
</contexts>
<marker>Komatani, Kanda, Ogata, Okuno, 2005</marker>
<rawString>Kazunori Komatani, Nayouki Kanda, Tetsuya Ogata, and Hiroshi G. Okuno. 2005. Contextual constraints based on dialogue models in database search task for spoken dialogue systems. In Eurospeech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivana Kruijff-Korbayov´a</author>
<author>Tilman Becker</author>
<author>Nate Blaylock</author>
<author>Ciprian Gerstenberger</author>
<author>Michael Kaisser</author>
<author>Peter Poller</author>
<author>Verena Rieser</author>
<author>Jan Schehl</author>
</authors>
<title>The Sammie corpus of multimodal dialogues with an mp3 player.</title>
<date>2006</date>
<booktitle>In LREC.</booktitle>
<marker>Kruijff-Korbayov´a, Becker, Blaylock, Gerstenberger, Kaisser, Poller, Rieser, Schehl, 2006</marker>
<rawString>Ivana Kruijff-Korbayov´a, Tilman Becker, Nate Blaylock, Ciprian Gerstenberger, Michael Kaisser, Peter Poller, Verena Rieser, and Jan Schehl. 2006. The Sammie corpus of multimodal dialogues with an mp3 player. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gabriel Murray</author>
<author>Giuseppe Carenini</author>
</authors>
<title>Summarizing spoken and written conversations.</title>
<date>2008</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="6935" citStr="Murray and Carenini (2008)" startWordPosition="1108" endWordPosition="1111">ure of the email thread, as well as lexical features. As do we, they find that classifying is easier than linking. Ding et al. (2008) argue that in order to do well at finding answers to questions, one must also find the context of the question, since it often contains the information needed to identify the answer. They use a corpus of online discussion forums, and use slip-CRFs and two-dimensional CRFs, models related to those we use. We will investigate their proposal to consider the question context in future work. While they do not use dialogue act tagging to compare modalities, as we do, Murray and Carenini (2008) compare spoken conversation with email by comparing a common summarization architecture across both modalities. They get similar performance, but the features differ. Table 1: DFU speech act labels Request-Information (R-I) Request-Action (R-A) Inform (Inf) Commit (Comm) Conventional (Conv) Perform (Perf) Backchannel (Bch) (+/- Grounding) Other 3 Annotation Scheme Figure 1: Example DFU illustrating the relation of extent (segmentation) to speech act type M1.2 I have completed the invoices for April, May and June M1.3 and we owe Pasadena each month for a total of $3,615,910.62. M1.4 I am waiti</context>
</contexts>
<marker>Murray, Carenini, 2008</marker>
<rawString>Gabriel Murray and Giuseppe Carenini. 2008. Summarizing spoken and written conversations. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ani Nenkova</author>
<author>Rebecca J Passonneau</author>
<author>Kathleen McKeown</author>
</authors>
<title>The pyramid method: incorporating human content selection variation in summarization evaluation.</title>
<date>2007</date>
<journal>ACM Transactions on Speech and Language Processing,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="8287" citStr="Nenkova et al., 2007" startWordPosition="1309" endWordPosition="1312">eted &amp; pending approval – versus amount due] Sffink(1.2-1.4) M2.1 That’s fine. [Inform(2.1): acknowledgement of status of Pasadena invoicing] Blink(1.2-1.4) The annotation scheme presented here consists of Dialogue Function Units (DFUs), which are intended to represent abstract units of interaction. The last two authors developed the annotation on three contrasting corpora: email threads, telephone conversations, and court transcripts. It builds on our previous work in intention-based segmentation (Passonneau and Litman, 1997), and on mixing a formal schema with natural language descriptions (Nenkova et al., 2007). In this 358 paper, we investigate the modalities of telephone two-person conversation in a library setting, and multi-party email in a workplace setting. Our initial focus is on the structure of turn-taking. By using a relatively abstract annotation scheme, we can compare and contrast this behavior across different types of interaction. Our unit of annotation is the DFU. DFUs have an extent, a dialogue act (DA) label along with a description, and possibly one or more forward and/or backward links. We explain each component of the annotation in turn. We use the example in Figure 1; the exampl</context>
<context position="9597" citStr="Nenkova et al., 2007" startWordPosition="1526" endWordPosition="1529">The extent of a DFU roughly corresponds to that portion of a turn (conversational turn; email message; etc.) that corresponds to a coherent communicative intention. Because we do not address automatic identification of the segmentation into DFU units in this paper, we do not discuss how annotators are instructed to identify extent. As illustrated in Figure 1, the communicative function of a DFU is captured by a speech act type, and a natural language description. This is somewhat analogous to the natural language descriptions associated with Summary Content Units (SCUs) in pyramid annotation (Nenkova et al., 2007), or with the intention-based segmentation of (Passonneau and Litman, 1997). The purpose in all cases is to require annotators to articulate briefly but specifically the unifying intention (Passonneau and Litman, 1997), semantic content (Nenkova et al., 2007), or speech act. We use the eight dialogue act types listed in the upper left of Table 1. To accommodate discontinuous speech acts, due to the interruptions that are common to conversation, each speech act can have an operator affix such as “-Continue”. We have previously shown (Passonneau and Litman, 1997) that intention-based segmentatio</context>
</contexts>
<marker>Nenkova, Passonneau, McKeown, 2007</marker>
<rawString>Ani Nenkova, Rebecca J. Passonneau, and Kathleen McKeown. 2007. The pyramid method: incorporating human content selection variation in summarization evaluation. ACM Transactions on Speech and Language Processing, 4(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Discourse segmentation by human and automated means.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="8198" citStr="Passonneau and Litman, 1997" startWordPosition="1294" endWordPosition="1297">nd June to make sure they are okay with her. [Inform(1.2-1.4): status of Pasadena invoicingcompleted &amp; pending approval – versus amount due] Sffink(1.2-1.4) M2.1 That’s fine. [Inform(2.1): acknowledgement of status of Pasadena invoicing] Blink(1.2-1.4) The annotation scheme presented here consists of Dialogue Function Units (DFUs), which are intended to represent abstract units of interaction. The last two authors developed the annotation on three contrasting corpora: email threads, telephone conversations, and court transcripts. It builds on our previous work in intention-based segmentation (Passonneau and Litman, 1997), and on mixing a formal schema with natural language descriptions (Nenkova et al., 2007). In this 358 paper, we investigate the modalities of telephone two-person conversation in a library setting, and multi-party email in a workplace setting. Our initial focus is on the structure of turn-taking. By using a relatively abstract annotation scheme, we can compare and contrast this behavior across different types of interaction. Our unit of annotation is the DFU. DFUs have an extent, a dialogue act (DA) label along with a description, and possibly one or more forward and/or backward links. We exp</context>
<context position="9672" citStr="Passonneau and Litman, 1997" startWordPosition="1536" endWordPosition="1539">onversational turn; email message; etc.) that corresponds to a coherent communicative intention. Because we do not address automatic identification of the segmentation into DFU units in this paper, we do not discuss how annotators are instructed to identify extent. As illustrated in Figure 1, the communicative function of a DFU is captured by a speech act type, and a natural language description. This is somewhat analogous to the natural language descriptions associated with Summary Content Units (SCUs) in pyramid annotation (Nenkova et al., 2007), or with the intention-based segmentation of (Passonneau and Litman, 1997). The purpose in all cases is to require annotators to articulate briefly but specifically the unifying intention (Passonneau and Litman, 1997), semantic content (Nenkova et al., 2007), or speech act. We use the eight dialogue act types listed in the upper left of Table 1. To accommodate discontinuous speech acts, due to the interruptions that are common to conversation, each speech act can have an operator affix such as “-Continue”. We have previously shown (Passonneau and Litman, 1997) that intention-based segmentation can be done reliably by multiple annotators. For twenty narratives each s</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Rebecca J. Passonneau and Diane J. Litman. 1997. Discourse segmentation by human and automated means. Computational Linguistics, 23(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Popescu-Belis</author>
</authors>
<title>Dimensionality of dialogue act tagsets: An empirical analysis of large corpora.</title>
<date>2008</date>
<journal>LREC,</journal>
<volume>42</volume>
<issue>1</issue>
<contexts>
<context position="2568" citStr="Popescu-Belis, 2008" startWordPosition="396" endWordPosition="397">conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the sponsors. We would like to thank three anonymous reviewers for their thoughtful comments. model human-human and human-machine communication. The need for the enterprise derives from the fact that the relationship between lexicogrammatical form (including mood, e.g., interrogative) and communicative actions cannot be enumerated; there are complex dependencies on the linguistic and situational contexts of use. Many DA schemes exist: they can be hierarchical or flat (Popescu-Belis, 2008), can comprise a large (Devillers et al., 2002; Hardy et al., 2003) or small repertoire (Komatani et al., 2005), or can be oriented towards human-human dialogue (Allen and Core, 1997; Devillers et al., 2002; Thompson et al., 1993; Traum and Heeman, 1996; Stolcke et al., 2000) or multi-party interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or </context>
</contexts>
<marker>Popescu-Belis, 2008</marker>
<rawString>Andrei Popescu-Belis. 2008. Dimensionality of dialogue act tagsets: An empirical analysis of large corpora. LREC, 42(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harvey Sacks</author>
<author>Emanuel A Schegloff</author>
<author>Gail Jefferson</author>
</authors>
<title>A simplest systemics for the organization of turntaking for conversation.</title>
<date>1974</date>
<journal>Language,</journal>
<volume>50</volume>
<issue>4</issue>
<contexts>
<context position="4418" citStr="Sacks et al., 1974" startWordPosition="698" endWordPosition="701"> scheme is that our focus differs from much of this prior work. We aim for a relatively abstract annotation scheme in order to make comparisons across interactions of widely differing properties. Our initial focus is less on speech act types and more on the patterns of local alternation beProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 357–366, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 357 tween an initiating speech act and a responding one–the analog of adjacency pairs (Sacks et al., 1974). The most closely related effort is (Galley et al., 2004), which aims to automatically identify adjacency pairs in the ICSI Meeting corpus, a large corpus of 75 meetings, using a small tagset. Their maximum entropy ranking approach achieved 90% accuracy on the 4-way classification into agreement, disagreement, backchannel and other. Using the switchboard corpus, (Stolcke et al., 2000) achieved good dialogue act labeling accuracy (71% on manual transcriptions) for a set of 42 dialogue act types, and constructed probabilistic models of dialogue act sequencing in order to test the hypothesis tha</context>
<context position="10994" citStr="Sacks et al., 1974" startWordPosition="1757" endWordPosition="1760">sociated with the null hypothesis that the observed distributions could have arisen by chance to be at or below p=0.1 x10−6. Partitioning Q by number of annotators gave significant results for all values of A ranging over the number of annotators apart from A = 2. We would expect similar patterns of agreement on DFU segmentation, but have not collected segmentation data from multiple annotators on the two corpora presented here. DFU Links, or simply Links, correspond to adjacency pairs, but need not be adjacent. A forward link (Flink) is the analog of a “first pair-part” of an adjacency pair (Sacks et al., 1974), and is similarly restricted to specific speech act types. All Request-Information and Request-Action DFUs are assigned Flinks. The responses to such requests are assigned a backward link (Blink). In principle, a response can be any of the speech act types, thus it can be an answer to a question (Inform), a rejection of a Request-Action or a commitment to take the requested action (Commit), a request for clarification (Request-Information), and so on. In most but not all cases, requests are responded to, thus most Flinks and Blinks come in pairs. We refer to Flinks with no matching Blink as d</context>
</contexts>
<marker>Sacks, Schegloff, Jefferson, 1974</marker>
<rawString>Harvey Sacks, Emanuel A. Schegloff, and Gail Jefferson. 1974. A simplest systemics for the organization of turntaking for conversation. Language, 50(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lokesh Shrestha</author>
<author>Kathleen McKeown</author>
</authors>
<title>Detection of question-answer pairs in email conversations.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="6148" citStr="Shrestha and McKeown (2004)" startWordPosition="973" endWordPosition="976">s are assigned for entire emails, but several speech acts can be assigned to one email. Apparently, they develop separate binary classifiers for each speech act. Corston-Oliver et al. (2004) are interested in identifying tasks in email. They label each sentence in email with tags from a set which describes the type of content of the sentence (describing a task, scheduling a meeting), but are less interested in the interactive aspect of email communication (creating an obligation to respond). There has been some work which relates to finding links, but limited to finding question-answer pairs. Shrestha and McKeown (2004) first detect questions using lexical and part-of-speech features, and then find the paragraph that answers the question. They use features related to the structure of the email thread, as well as lexical features. As do we, they find that classifying is easier than linking. Ding et al. (2008) argue that in order to do well at finding answers to questions, one must also find the context of the question, since it often contains the information needed to identify the answer. They use a corpus of online discussion forums, and use slip-CRFs and two-dimensional CRFs, models related to those we use.</context>
</contexts>
<marker>Shrestha, McKeown, 2004</marker>
<rawString>Lokesh Shrestha and Kathleen McKeown. 2004. Detection of question-answer pairs in email conversations. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
<author>Klaus Ries</author>
<author>Noah Coccaro</author>
<author>Elizabeth Shriberg</author>
<author>Rebecca Bates</author>
<author>Daniel Jurafsky</author>
<author>Paul Taylor</author>
<author>Rachel Martin</author>
<author>Carol Van Ess-Dykena</author>
<author>Meteer Marie</author>
</authors>
<title>Dialogue act modeling for automatic tagging and recognition of conversational speech.</title>
<date>2000</date>
<journal>International Journal of Computational Linguistics,</journal>
<volume>26</volume>
<issue>3</issue>
<marker>Stolcke, Ries, Coccaro, Shriberg, Bates, Jurafsky, Taylor, Martin, Van Ess-Dykena, Marie, 2000</marker>
<rawString>Andreas Stolcke, Klaus Ries, Noah Coccaro, Elizabeth Shriberg, Rebecca Bates, Daniel Jurafsky, Paul Taylor, Rachel Martin, Carol Van Ess-Dykena, and Meteer Marie. 2000. Dialogue act modeling for automatic tagging and recognition of conversational speech. International Journal of Computational Linguistics, 26(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Crlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Maxmargin markov networks.</title>
<date>2003</date>
<booktitle>In NIPS.</booktitle>
<contexts>
<context position="17017" citStr="Taskar et al., 2003" startWordPosition="2789" endWordPosition="2792">iction, in which we predict if two DFUs form a link pair. In this paper, we assume that the DFUs are given. We propose three systems to tackle the problem. The first system is a non-strawman Baseline Heuristics system, which uses the structural characteristics of dialogue. The second is Regular SVM. The third is Structured SVM. Structured SVM is a discriminative method that can predict complex structured output. Recently, discriminative Probabilistic Graphical Models have been widely applied in structural problems (Getoor and 360 Taskar, 2007) such as link prediction. However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure. sequence labeling (Tsochantaridis et al., 2005). We have adapted Structured SVM to our problem, provided a novel method for link prediction, and shown that it is superior in some aspects to Regular SVM. 5.1 Features We have two sets of features. DFU features are associated with a particular DFU, and link features describe the relationship between two</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Ben Taskar, Crlos Guestrin, and Daphne Koller. 2003. Maxmargin markov networks. In NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Henry S Thompson</author>
<author>Anne H Anderson</author>
<author>Ellen Gurman Bard</author>
<author>Gwyneth Doherty-Sneddon</author>
<author>Alison Newlands</author>
<author>Cathy Sotillo</author>
</authors>
<title>The HCRC map task corpus: Natural dialogue for speech recognition.</title>
<date>1993</date>
<booktitle>In Proceedings of the DARPA Human Language Technology Workshop.</booktitle>
<contexts>
<context position="2797" citStr="Thompson et al., 1993" startWordPosition="433" endWordPosition="436">uman-human and human-machine communication. The need for the enterprise derives from the fact that the relationship between lexicogrammatical form (including mood, e.g., interrogative) and communicative actions cannot be enumerated; there are complex dependencies on the linguistic and situational contexts of use. Many DA schemes exist: they can be hierarchical or flat (Popescu-Belis, 2008), can comprise a large (Devillers et al., 2002; Hardy et al., 2003) or small repertoire (Komatani et al., 2005), or can be oriented towards human-human dialogue (Allen and Core, 1997; Devillers et al., 2002; Thompson et al., 1993; Traum and Heeman, 1996; Stolcke et al., 2000) or multi-party interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses. The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as </context>
</contexts>
<marker>Thompson, Anderson, Bard, Doherty-Sneddon, Newlands, Sotillo, 1993</marker>
<rawString>Henry S. Thompson, Anne H. Anderson, Ellen Gurman Bard, Gwyneth Doherty-Sneddon, Alison Newlands, and Cathy Sotillo. 1993. The HCRC map task corpus: Natural dialogue for speech recognition. In Proceedings of the DARPA Human Language Technology Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Traum</author>
<author>Peter Heeman</author>
</authors>
<title>Utterance units and grounding in spoken dialogue.</title>
<date>1996</date>
<booktitle>In Interspeech/ICSLP.</booktitle>
<contexts>
<context position="2821" citStr="Traum and Heeman, 1996" startWordPosition="437" endWordPosition="440">chine communication. The need for the enterprise derives from the fact that the relationship between lexicogrammatical form (including mood, e.g., interrogative) and communicative actions cannot be enumerated; there are complex dependencies on the linguistic and situational contexts of use. Many DA schemes exist: they can be hierarchical or flat (Popescu-Belis, 2008), can comprise a large (Devillers et al., 2002; Hardy et al., 2003) or small repertoire (Komatani et al., 2005), or can be oriented towards human-human dialogue (Allen and Core, 1997; Devillers et al., 2002; Thompson et al., 1993; Traum and Heeman, 1996; Stolcke et al., 2000) or multi-party interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses. The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between hum</context>
</contexts>
<marker>Traum, Heeman, 1996</marker>
<rawString>David Traum and Peter Heeman. 1996. Utterance units and grounding in spoken dialogue. In Interspeech/ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ioannis Tsochantaridis</author>
<author>Thorsten Joachims</author>
<author>Thomas Hofmann</author>
<author>Yasemin Altun</author>
</authors>
<title>Large margin methods for structured and interdependent output variables.</title>
<date>2005</date>
<journal>JMLR,</journal>
<volume>6</volume>
<contexts>
<context position="17047" citStr="Tsochantaridis et al., 2005" startWordPosition="2793" endWordPosition="2797">redict if two DFUs form a link pair. In this paper, we assume that the DFUs are given. We propose three systems to tackle the problem. The first system is a non-strawman Baseline Heuristics system, which uses the structural characteristics of dialogue. The second is Regular SVM. The third is Structured SVM. Structured SVM is a discriminative method that can predict complex structured output. Recently, discriminative Probabilistic Graphical Models have been widely applied in structural problems (Getoor and 360 Taskar, 2007) such as link prediction. However, Structured SVM (Taskar et al., 2003; Tsochantaridis et al., 2005) is also a compelling method which has the potential to handle the interdependence between labeling and sequencing, due to its ability to handle dependencies among features and prediction results within the structure. sequence labeling (Tsochantaridis et al., 2005). We have adapted Structured SVM to our problem, provided a novel method for link prediction, and shown that it is superior in some aspects to Regular SVM. 5.1 Features We have two sets of features. DFU features are associated with a particular DFU, and link features describe the relationship between two DFUs. DFU features are used i</context>
<context position="22308" citStr="Tsochantaridis et al., 2005" startWordPosition="3708" endWordPosition="3711">scores for all DFU pairs in the consideration space of the test data, then it scans the dialogue sequentially, checks all preceding DFUs that are allowed to link to the current DFU (i.e., the DFU pair is in the consideration space), and assigns corresponding links to the most likely DFU pair. We impose a restriction that there can be at most one Flink, one Sflink and one Blink for any given DFU. 5.4 Structured SVM A Structured SVM is able to predict complex output instead of simply a binary result as in a regular SVM. There are several variants. We have followed the margin-rescaling approach (Tsochantaridis et al., 2005), and implemented our systems using SVMpython, which is a python interface to the SVMst&amp;quot;&apos; package (svmlight.joachims.org/svm struct.html). Generally, Structured SVM learns a discriminant function F : X x Y —* R, which estimates a score of how likely the output y is given the input x. Crucially, y can be a complex structure. Section A in the appendix; here, we summarize the main intuitions. Dialogue Act Tagging The input x is a sequence of DFUs, and y is the corresponding sequence of DAs to predict. Compared to Regular SVM, instead of predicting yt one at a time, Structured SVM optimizes the se</context>
</contexts>
<marker>Tsochantaridis, Joachims, Hofmann, Altun, 2005</marker>
<rawString>Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hofmann, and Yasemin Altun. 2005. Large margin methods for structured and interdependent output variables. JMLR, 6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Rebecca Passonneau</author>
</authors>
<title>Date: A dialogue act tagging scheme for evaluation of spoken dialogue systems.</title>
<date>2001</date>
<booktitle>In HLT.</booktitle>
<contexts>
<context position="2954" citStr="Walker and Passonneau, 2001" startWordPosition="455" endWordPosition="458">luding mood, e.g., interrogative) and communicative actions cannot be enumerated; there are complex dependencies on the linguistic and situational contexts of use. Many DA schemes exist: they can be hierarchical or flat (Popescu-Belis, 2008), can comprise a large (Devillers et al., 2002; Hardy et al., 2003) or small repertoire (Komatani et al., 2005), or can be oriented towards human-human dialogue (Allen and Core, 1997; Devillers et al., 2002; Thompson et al., 1993; Traum and Heeman, 1996; Stolcke et al., 2000) or multi-party interactions (Galley et al., 2004), or human-computer interaction (Walker and Passonneau, 2001; Hardy et al., 2003), including multimodal ones (Thompson et al., 1993; Kruijff-Korbayov´a et al., 2006). A major focus of the cited work is on how to recognize or generate speech acts for interactive systems, or how to classify speech acts for distributional analyses. The focus can be on a specific type of speech act (e.g., grounding and repairs (Traum and Heeman, 1996; Frampton and Lemon, 2008)), or on more general comparisons, such as the contrast between human-human and human-computer dialogues (Doran et al., 2001). While there is a large degree of overlap across schemes, the set of DA ty</context>
</contexts>
<marker>Walker, Passonneau, 2001</marker>
<rawString>Marilyn A. Walker and Rebecca Passonneau. 2001. Date: A dialogue act tagging scheme for evaluation of spoken dialogue systems. In HLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jen-Yuan Yeh</author>
<author>Aaron Harnly</author>
</authors>
<title>Email thread reassembly using similarity matching.</title>
<date>2006</date>
<booktitle>In Conference on Email and Anti-Spam.</booktitle>
<contexts>
<context position="13979" citStr="Yeh and Harnly, 2006" startWordPosition="2279" endWordPosition="2282">Links Paired Links 1204 63% 193 28% Flink/Blink 702 58% 83 43% Sflink/Blink 502 42% 110 57% Dangling Links 90 2% 97 7% Mutliple Blinks 4 0% 4 0% Links by Speech Act Labels Inform 1003 83% 142 74% Request-Inf. 170 14% 44 23% Request-Action 1 0% 5 3% Commit 13 1% 2 1% Conventional 2 0% 0 0 Backchannel 15 1% 0 0 1204 100% 193 100% employees. Most of the emails are concerned with exchanging information, scheduling meetings, and solving problems, but there are also purely social emails. We used a version of the corpus with some missing messages restored from other emails in which they were quoted (Yeh and Harnly, 2006). The annotator of the majority of the Loqui corpus also annotated the Enron corpus. She received additional training and guidance based on our experience with a pilot annotator who helped us develop the initial guidelines. Table 2 illustrates differences between the two corpora. The DFUs in the Loqui data are much shorter, with 5.5 words on average compared with 12.8 words in Enron. The distribution of DFU labels shows a similarly high proportion of Inform acts, comprising 50% of all Loqui DFUs and 61% of all Enron DFUs. Otherwise, the distributions are quite distinct. The Loqui interactions </context>
</contexts>
<marker>Yeh, Harnly, 2006</marker>
<rawString>Jen-Yuan Yeh and Aaron Harnly. 2006. Email thread reassembly using similarity matching. In Conference on Email and Anti-Spam.</rawString>
</citation>
<citation valid="false">
<title>Note: In this example, O(xt) extracts the bag-of-words features from xt. “are”,“you”,“sure” are the 1st, 2nd and 3rd DFU feature respectively. W(xi, xj) extracts the distance and number of the overlap content, which are the link features, from the 1st and 2nd pair-part in a DFU link pair. There is a link from DFU 1 to DFU 2 as specified by j − uj = i, but there is no link pointing to DFU 1.</title>
<marker></marker>
<rawString>Note: In this example, O(xt) extracts the bag-of-words features from xt. “are”,“you”,“sure” are the 1st, 2nd and 3rd DFU feature respectively. W(xi, xj) extracts the distance and number of the overlap content, which are the link features, from the 1st and 2nd pair-part in a DFU link pair. There is a link from DFU 1 to DFU 2 as specified by j − uj = i, but there is no link pointing to DFU 1.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>