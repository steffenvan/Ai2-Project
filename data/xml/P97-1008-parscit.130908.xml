<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.612623">
Similarity-Based Methods For Word Sense Disambiguation
</title>
<author confidence="0.96731">
Ido Dagan
</author>
<affiliation confidence="0.980022">
Dept. of Mathematics and
Computer Science
Bar Ilan University
</affiliation>
<address confidence="0.892949">
Ramat Gan 52900, Israel
</address>
<email confidence="0.997799">
dagan@macs.biu.ac.il
</email>
<author confidence="0.852224">
Lillian Lee
</author>
<affiliation confidence="0.815752666666667">
Div. of Engineering and
Applied Sciences
Harvard University
</affiliation>
<address confidence="0.808192">
Cambridge, MA 01238, USA
</address>
<email confidence="0.928049">
lleeeleecs.harvard.edu
</email>
<note confidence="0.607142">
Fernando Pereira
AT&amp;T Labs - Research
600 Mountain Ave.
</note>
<address confidence="0.726451">
Murray Hill, NJ 07974, USA
</address>
<email confidence="0.990353">
pereira@research.att.com
</email>
<sectionHeader confidence="0.994609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999728">
We compare four similarity-based esti-
mation methods against back-off and
maximum-likelihood estimation meth-
ods on a pseudo-word sense disam-
biguation task in which we controlled
for both unigram and bigram fre-
quency. The similarity-based meth-
ods perform up to 40% better on this
particular task. We also conclude
that events that occur only once in
the training set have major impact on
similarity-based estimates.
</bodyText>
<sectionHeader confidence="0.99843" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999976814814815">
The problem of data sparseness affects all sta-
tistical methods for natural language process-
ing. Even large training sets tend to misrep-
resent low-probability events, since rare events
may not appear in the training corpus at all.
We concentrate here on the problem of es-
timating the probability of unseen word pairs,
that is, pairs that do not occur in the train-
ing set. Katz&apos;s back-off scheme (Katz, 1987),
widely used in bigram language modeling, esti-
mates the probability of an unseen bigram by
utilizing unigram estimates. This has the un-
desirable result of assigning unseen bigrams the
same probability if they are made up of uni-
grams of the same frequency.
Class-based methods (Brown et al., 1992;
Pereira, Tishby, and Lee, 1993; Resnik, 1992)
cluster words into classes of similar words, so
that one can base the estimate of a word pair&apos;s
probability on the averaged cooccurrence prob-
ability of the classes to which the two words be-
long. However, a word is therefore modeled by
the average behavior of many words, which may
cause the given word&apos;s idiosyncrasies to be ig-
nored. For instance, the word &amp;quot;red&amp;quot; might well
act like a generic color word in most cases, but
it has distinctive cooccurrence patterns with re-
spect to words like &amp;quot;apple,&amp;quot; &amp;quot;banana,&amp;quot; and so
on.
We therefore consider similarity-based esti-
mation schemes that do not require building
general word classes. Instead, estimates for
the most similar words to a word w are com-
bined; the evidence provided by word w&apos; is
weighted by a function of its similarity to w.
Dagan, Markus, and Markovitch (1993) pro-
pose such a scheme for predicting which un-
seen cooccurrences are more likely than others.
However, their scheme does not assign probabil-
ities. In what follows, we focus on probabilistic
similarity-based estimation methods.
We compared several such methods, in-
cluding that of Dagan, Pereira, and Lee (1994)
and the cooccurrence smoothing method of
Essen and Steinbiss (1992), against classical es-
timation methods, including that of Katz, in a
decision task involving unseen pairs of direct ob-
jects and verbs, where unigram frequency was
eliminated from being a factor. We found that
all the similarity-based schemes performed al-
most 40% better than back-off, which is ex-
pected to yield about 50% accuracy in our
experimental setting. Furthermore, a scheme
based on the total divergence of empirical dis-
</bodyText>
<page confidence="0.994374">
56
</page>
<bodyText confidence="0.99956325">
tributions to their average&apos; yielded statistically
significant improvement in error rate over cooc-
currence smoothing.
We also investigated the effect of removing
extremely low-frequency events from the train-
ing set. We found that, in contrast to back-
off smoothing, where such events are often dis-
carded from training with little discernible ef-
fect, similarity-based smoothing methods suf-
fer noticeable performance degradation when
singletons (events that occur exactly once) are
omitted.
</bodyText>
<sectionHeader confidence="0.969495" genericHeader="method">
2 Distributional Similarity Models
</sectionHeader>
<bodyText confidence="0.999922088235294">
We wish to model conditional probability distri-
butions arising from the coocurrence of linguis-
tic objects, typically words, in certain configura-
tions. We thus consider pairs (w1, w2) E V, x V2
for appropriate sets VI and V2, not necessar-
ily disjoint. In what follows, we use subscript
i for the ith element of a pair: thus P(w2lwi)
is the conditional probability (or rather, some
empirical estimate, the true probability being
unknown) that a pair has second element w2
given that its first element is wi; and P(wiiw2)
denotes the probability estimate, according to
the base language model, that wi is the first
word of a pair given that the second word is w2.
P(w) denotes the base estimate for the unigram
probability of word to.
A similarity-based language model consists
of three parts: a scheme for deciding which
word pairs require a similarity-based estimate,
a method for combining information from simi-
lar words, and, of course, a function measuring
the similarity between words. We give the de-
tails of each of these three parts in the following
three sections. We will only be concerned with
similarity between words in VI.
&apos;To the best of our knowledge, this is the first use
of this particular distribution dissimilarity function in
statistical language processing. The function itself is im-
plicit in earlier work on distributional clustering (Pereira,
Tishby, and Lee, 1993), has been used by Tishby (p.c.)
in other distributional similarity work, and, as sug-
gested by Yoav Freund (p.c.), it is related to results of
Hoeffding (1965) on the probability that a given sample
was drawn from a given joint distribution.
</bodyText>
<subsectionHeader confidence="0.993943">
2.1 Discounting and Redistribution
</subsectionHeader>
<bodyText confidence="0.8959658">
Data sparseness makes the maximum likelihood
estimate (MLE) for word pair probabilities un-
reliable. The MLE for the probability of a word
pair (w1, w2), conditional on the appearance of
word w1, is simply
</bodyText>
<equation confidence="0.991104333333333">
c(wi, w2)
PmL(w2lwi) = (1)
c(wi)
</equation>
<bodyText confidence="0.999899619047619">
where c(wi, w2) is the frequency of (w1, w2) in
the training corpus and c(wi) is the frequency
of w1. However, PML is zero for any unseen
word pair, which leads to extremely inaccurate
estimates for word pair probabilities.
Previous proposals for remedying the above
problem (Good, 1953; Jelinek, Mercer, and
Roukos, 1992; Katz, 1987; Church and Gale,
1991) adjust the MLE in so that the total prob-
ability of seen word pairs is less than one, leav-
ing some probability mass to be redistributed
among the unseen pairs. In general, the ad-
justment involves either interpolation, in which
the MLE is used in linear combination with an
estimator guaranteed to be nonzero for unseen
word pairs, or discounting, in which a reduced
MLE is used for seen word pairs, with the prob-
ability mass left over from this reduction used
to model unseen pairs.
The discounting approach is the one adopted
by Katz (1987):
</bodyText>
<equation confidence="0.97572875">
{
Pd(W211111) C(Wi, W2) &gt;0
15(W2IW1) p \
râ€˜W211Vi, 0.W.
</equation>
<bodyText confidence="0.9903715">
(2)
where Pd represents the Good-Turing dis-
counted estimate (Katz, 1987) for seen word
pairs, and Pr denotes the model for probabil-
ity redistribution among the unseen word pairs.
ce(wi) is a normalization factor.
Following Dagan, Pereira, and Lee (1994),
we modify Katz&apos;s formulation by writing
Pr(W2IW1) instead P(w2), enabling us to use
similarity-based estimates for unseen word pairs
instead of basing the estimate for the pair on un-
igram frequency P(w2). Observe that similarity
estimates are used for unseen word pairs only.
We next investigate estimates for Pr (w2iwi)
</bodyText>
<page confidence="0.991013">
57
</page>
<bodyText confidence="0.986633">
derived by averaging information from words
that are distributionally similar to w1.
</bodyText>
<subsectionHeader confidence="0.998628">
2.2 Combining Evidence
</subsectionHeader>
<bodyText confidence="0.999387642857143">
Similarity-based models assume that if word
is &amp;quot;similar&amp;quot; to word w1, then w&apos;1 can yield in-
formation about the probability of unseen word
pairs involving w1. We use a weighted aver-
age of the evidence provided by similar words,
where the weight given to a particular word wc.
depends on its similarity to w1.
More precisely, let W(wi, 01) denote an in-
creasing function of the similarity between w1
and w, and let 8(w1) denote the set of words
most similar to w1. Then the general form of
similarity model we consider is a W-weighted
linear combination of predictions of similar
words:
</bodyText>
<equation confidence="0.9825458">
W(Wil
PSIM(W2IW1) E p(w2m),
N(wi)
wiEs(.,)
(3)
</equation>
<bodyText confidence="0.998692285714286">
where N(w1) = EwlEs(.0 w(wi, wc) is a nor-
malization factor. According to this formula,
w2 is more likely to occur with w1 if it tends to
occur with the words that are most similar to
wi
Considerable latitude is allowed in defining
the set S(wi), as is evidenced by previous work
that can be put in the above form. Essen
and Steinbiss (1992) and Karov and Edelman
(1996) (implicitly) set S(wi) =V1. However,
it may be desirable to restrict S(wi) in some
fashion, especially if VI is large. For instance,
Dagan. Pereira, and Lee (1994) use the closest
k or fewer words w such that the dissimilarity
between w1 and w is less than a threshold value
t; k and t are tuned experimentally.
Now, we could directly replace Pr(w2lw1)
in the back-off equation (2) with Psim(w2iwi)â€¢
However, other variations are possible, such
as interpolating with the unigram probability
P(w2):
</bodyText>
<equation confidence="0.999305">
Pr(w2iw1) = 7P(w2) + (1 â€” 7)Psim(w2lw1).
</equation>
<bodyText confidence="0.999864222222222">
where -y is determined experimentally (Dagan,
Pereira, and Lee. 1994). This represents, in ef-
fect, a linear combination of the similarity es-
timate and the back-off estimate: if y = 1,
then we have exactly Katz&apos;s back-off scheme.
As we focus in this paper on alternatives for
Psim, we will not consider this approach here;
that is, for the rest of this paper, Pr(w2lwi) =
Psim (w2i wi)â€¢
</bodyText>
<subsectionHeader confidence="0.998534">
2.3 Measures of Similarity
</subsectionHeader>
<bodyText confidence="0.999926">
We now consider several word similarity func-
tions that can be derived automatically from
the statistics of a training corpus, as opposed
to functions derived from manually-constructed
word classes (Resnik, 1992). All the similarity
functions we describe below depend just on the
base language model P(1.), not the discounted
model P(1-) from Section 2.1 above.
</bodyText>
<subsubsectionHeader confidence="0.809479">
2.3.1 KL divergence
</subsubsectionHeader>
<bodyText confidence="0.999478333333333">
Kullback-Leibler (KL) divergence is a stan-
dard information-theoretic measure of the dis-
similarity between two probability mass func-
tions (Cover and Thomas, 1991). We can ap-
ply it to the conditional distribution P(â€¢Iw1) in-
duced by w1 on words in 172:
</bodyText>
<equation confidence="0.9942055">
E P(w2iwo log P(w2ltvi) â€¢ (4)
P(w2iwo
</equation>
<bodyText confidence="0.967215235294118">
For D(willw&apos;i) to be defined it must be the
case that P(w2Iwc) &gt; 0 whenever P(w2iwi) &gt;
0. Unfortunately, this will not in general be
the case for MLEs based on samples, so we
would need smoothed estimates of P(w21wD
that redistribute some probability mass to zero-
frequency events. However, using smoothed es-
timates for P(w2I wi) as well requires a sum
over all w2 E V2, which is expensive for the
large vocabularies under consideration. Given
the smoothed denominator distribution, we set
W(wi, = 10-13D
where /3 is a free parameter.
2.3.2 Total divergence to the average
A related measure is based on the total KL
divergence to the average of the two distribu-
tions:
</bodyText>
<equation confidence="0.640927666666667">
A(ivi,w1) = D (w1
1.02
wi
</equation>
<page confidence="0.7495025">
2
58
</page>
<bodyText confidence="0.995228">
where (w1 w1)/2 shorthand for the distribu-
</bodyText>
<equation confidence="0.94356375">
tion
1
:2(P(.iwi) P(iwi))
Since D(â€¢II.) &gt; 0, A(wi, &gt; 0. Furthermore,
</equation>
<bodyText confidence="0.98988175">
letting p(w2) = P(w2iwi), 11(w2) = P(w2iwi)
and C = {w2 P(w2) &gt; 0,71(w2) &gt; 0}, it is
straightforward to show by grouping terms ap-
propriately that
</bodyText>
<equation confidence="0.95706">
A(wi,w2EC {H(P(w2) +11(w2))
â€”H(p(w2)) â€” H (71(w2))
+ 2 log 2,
</equation>
<bodyText confidence="0.9999643">
where H(x) = â€”xlogx. Therefore, A(wi, wc)
is bounded, ranging between 0 and 2 log 2, and
smoothed estimates are not required because
probability ratios are not involved. In addi-
tion, the calculation of A(wi, 01) requires sum-
ming only over those w2 for which P(w2jwi) and
P(w2jwc) are both non-zero, which, for sparse
data, makes the computation quite fast.
As in the KL divergence case, we set
W(wi, wii) to be 10-0A(tvi,w1).
</bodyText>
<equation confidence="0.979611363636363">
2.3.3 L1 norm
The L1 norm is defined as
L(wi,wi) = E iP(w2Itu1) â€” P(tv2iiv)1. (6)
W2
By grouping terms as before, we can express
L(wi, u4) in a form depending only on the
&amp;quot;common&amp;quot; 1112:
L (wi, = 2 â€” E p(w2)â€” E pi(w2)
w,ec w2ec
+ E Ip(102) Pi(W2)1-
u&apos;2EC
</equation>
<bodyText confidence="0.92671775">
This last form makes it clear that 0 &lt;
L(wi, &lt;2, with equality if and only if there
are no words w2 such that both P(w2lwi) and
P(w2iwi)
Since we require a weighting scheme that is
decreasing in L, we set
W(wi, = (2 â€” L(wi, wii))13
with again free.
</bodyText>
<subsubsectionHeader confidence="0.679512">
2.3.4 Confusion probability
</subsubsectionHeader>
<bodyText confidence="0.999963666666667">
Essen and Steinbiss (1992) introduced confu-
sion probability 2, which estimates the probabil-
ity that word w can be substituted for word
</bodyText>
<equation confidence="0.998471">
Pc(w&apos;ilwi) = w(wi,w/)
EP(tviltv2)P(w/ilw2)P(w2)
P(wi)
</equation>
<bodyText confidence="0.991619">
Unlike the measures described above, w1 may
not necessarily be the &amp;quot;closest&amp;quot; word to itself,
that is, there may exist a word &amp;I such that
</bodyText>
<equation confidence="0.676379">
Pc(4101) &gt; Pc(wilwi)
</equation>
<bodyText confidence="0.999788">
The confusion probability can be computed
from empirical estimates provided all unigram
estimates are nonzero (as we assume through-
out). In fact, the use of smoothed estimates
like those of Katz&apos;s back-off scheme is problem-
atic, because those estimates typically do not
preserve consistency with respect to marginal
estimates and Bayes&apos;s rule. However, using con-
sistent estimates (such as the MLE), we can
rewrite Pc as follows:
</bodyText>
<equation confidence="0.993803">
p(w2lwi) p(w2itv)P(w1).
Pc(wilwi) P(w2)
w2
</equation>
<bodyText confidence="0.999952363636364">
This form reveals another important difference
between the confusion probability and the func-
tions D, A, and L described in the previous sec-
tions. Those functions rate zu&apos;l as similar to w1
if, roughly, P(w2lw&apos;i) is high when P(w21wi) is.
Pc(wc.jwi), however, is greater for those w&apos;1 for
which P(w&apos;i,w2) is large when P(w2lwi)/P(w2)
is. When the ratio P(w2lwi)/P(w2) is large, we
may think of w2 as being exceptional, since if w2
is infrequent, we do not expect P(w2iwi) to be
large.
</bodyText>
<subsectionHeader confidence="0.74232">
2.3.5 Summary
</subsectionHeader>
<bodyText confidence="0.972965875">
Several features of the measures of similarity
listed above are summarized in table 1. &amp;quot;Base
LM constraints&amp;quot; are conditions that must be
satisfied by the probability estimates of the base
2Actually, they present two alternative definitions.
We use their model 2-B, which they found yielded the
best experimental results.
are strictly positive.
</bodyText>
<page confidence="0.6882525">
W2
59
</page>
<bodyText confidence="0.99984625">
language model. The last column indicates
whether the weight W(wl, wc) associated with
each similarity function depends on a parameter
that needs to be tuned experimentally.
</bodyText>
<sectionHeader confidence="0.994902" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999724">
We evaluated the similarity measures listed
above on a word sense disambiguation task, in
which each method is presented with a noun and
two verbs, and decides which verb is more likely
to have the noun as a direct object. Thus, we do
not measure the absolute quality of the assign-
ment of probabilities, as would be the case in
a perplexity evaluation, but rather the relative
quality. We are therefore able to ignore constant
factors, and so we neither normalize the similar-
ity measures nor calculate the denominator in
equation (3).
</bodyText>
<subsectionHeader confidence="0.779703">
3.1 Task: Pseudo-word Sense
Disambiguation
</subsectionHeader>
<bodyText confidence="0.999838285714286">
In the usual word sense disambiguation prob-
lem, the method to be tested is presented with
an ambiguous word in some context, and is
asked to identify the correct sense of the word
from the context. For example, a test instance
might be the sentence fragment &amp;quot;robbed the
bank&amp;quot;; the disambiguation method must decide
whether &amp;quot;bank&amp;quot; refers to a river bank, a savings
bank, or perhaps some other alternative.
While sense disambiguation is clearly an im-
portant task, it presents numerous experimen-
tal difficulties. First, the very notion of &amp;quot;sense&amp;quot;
is not clearly defined; for instance, dictionaries
may provide sense distinctions that are too fine
or too coarse for the data at hand. Also, one
needs to have training data for which the cor-
rect senses have been assigned, which can re-
quire considerable human effort.
To circumvent these and other difficulties,
we set up a pseudo-word disambiguation ex-
periment (Schiitze, 1992; Gale, Church, and
Yarowsky, 1992) the general format of which is
as follows. We first construct a list of pseudo-
words, each of which is the combination of two
different words in V2. Each word in V2 con-
tributes to exactly one pseudo-word. Then, we
replace each w2 in the test set with its cor-
responding pseudo-word. For example, if we
choose to create a pseudo-word out of the words
&amp;quot;make&amp;quot; and &amp;quot;take&amp;quot;, we would change the test
data like this:
make plans = {make, take} plans
take action {make, take} action
The method being tested must choose between
the two words that make up the pseudo-word.
</bodyText>
<subsectionHeader confidence="0.995878">
3.2 Data
</subsectionHeader>
<bodyText confidence="0.999708615384615">
We used a statistical part-of-speech tagger
(Church, 1988) and pattern matching and con-
cordancing tools (due to David Yarowsky) to
identify transitive main verbs and head nouns
of the corresponding direct objects in 44 mil-
lion words of 1988 Associated Press newswire.
We selected the noun-verb pairs for the 1000
most frequent nouns in the corpus. These pairs
are undoubtedly somewhat noisy given the er-
rors inherent in the part-of-speech tagging and
pattern matching.
We used 80%, or 587833, of the pairs so de-
rived, for building base bigram language mod-
els, reserving 20% for testing purposes. As
some, but not all, of the similarity measures re-
quire smoothed language models, we calculated
both a Katz back-off language model (P = P
(equation (2)), with P,.(w2iw1) = P(w2)), and
a maximum-likelihood model (P = PML). Fur-
thermore, we wished to investigate Katz&apos;s claim
that one can delete singletons, word pairs that
occur only once, from the training set with-
out affecting model performance (Katz, 1987);
our training set contained 82407 singletons. We
therefore built four base language models, sum-
marized in Table 2.
</bodyText>
<tableCaption confidence="0.864826">
Table 2: Base Language Models
</tableCaption>
<bodyText confidence="0.871685666666667">
Since we wished to test the effectiveness of us-
ing similarity for unseen word cooccurrences, we
removed from the test set any verb-object pairs
</bodyText>
<figure confidence="0.9748489">
with singletons
(587833 pairs)
MLE-1
B0-1
no singletons
(505426 pairs)
MLE-o1
BO-ol
MLE
Katz
</figure>
<page confidence="0.971234">
60
</page>
<table confidence="0.9583218">
name range base LM constraints tune?
[0, co] P(w2101) 0 0 if P(w2lwi) 0 0 yes
A [0,2 log 2] none yes
[0,2] none yes
PC [0, maxâ€ž P(w2)] Bayes consistency no
</table>
<tableCaption confidence="0.999855">
Table 1: Summary of similarity function properties
</tableCaption>
<bodyText confidence="0.9477365">
that occurred in the training set; this resulted
in 17152 unseen pairs (some occurred multiple
times). The unseen pairs were further divided
into five equal-sized parts, T1 through Ts, which
formed the basis for fivefold cross-validation: in
each of five runs, one of the T, was used as a
performance test set, with the other 4 sets com-
bined into one set used for tuning parameters
(if necessary) via a simple grid search. Finally,
test pseudo-words were created from pairs of
verbs with similar frequencies, so as to control
for word frequency in the decision task. We use
error rate as our performance metric, defined as
1
â€”N(# of incorrect choices + (# of ties)/2)
where N was the size of the test corpus. A tie
occurs when the two words making up a pseudo-
word are deemed equally likely.
</bodyText>
<subsectionHeader confidence="0.996059">
3.3 Baseline Experiments
</subsectionHeader>
<bodyText confidence="0.99946825">
The performances of the four base language
models are shown in table 3. MLE-1 and
MLE-o1 both have error rates of exactly .5 be-
cause the test sets consist of unseen bigrams,
which are all assigned a probability of 0 by
maximum-likelihood estimates, and thus are all
ties for this method. The back-off models B0-1
and BO-ol also perform similarly.
</bodyText>
<table confidence="0.9848564">
T1 T2 T3 T4 T5
MLE-1 .5 .5 .5 .5 .5
MLE-ol
B0-1 0.517 0.520 0.512 0.513 0.516
BO-ol 0.517 0.520 0.512 0.513 0.516
</table>
<tableCaption confidence="0.99893">
Table 3: Base Language Model Error Rates
</tableCaption>
<bodyText confidence="0.999545333333333">
Since the back-off models consistently per-
formed worse than the MLE models, we chose
to use only the MLE models in our subse-
quent experiments. Therefore, we only ran com-
parisons between the measures that could uti-
lize unsmoothed data, namely, the L1 norm,
L(wi, wil); the total divergence to the aver-
age, A(wi, wc); and the confusion probability,
3 In the full paper, we give de-
tailed examples showing the different neighbor-
hoods induced by the different measures, which
we omit here for reasons of space.
</bodyText>
<subsectionHeader confidence="0.4791485">
3.4 Performance of Similarity-Based
Methods
</subsectionHeader>
<bodyText confidence="0.990002807692307">
Figure 1 shows the results on the five test sets,
using MLE-1 as the base language model. The
parameter was always set to the optimal value
for the corresponding training set. RAND,
which is shown for comparison purposes, sim-
ply chooses the weights W(wi, &amp;I) randomly.
S(wi) was set equal to V1 in all cases.
The similarity-based methods consistently
outperform the MLE method (which, recall, al-
ways has an error rate of .5) and Katz&apos;s back-
off method (which always had an error rate of
about .51) by a huge margin; therefore, we con-
clude that information from other word pairs is
very useful for unseen pairs where unigram fre-
quency is not informative. The similarity-based
methods also do much better than RAND,
which indicates that it is not enough to simply
combine information from other words arbitrar-
ily: it is quite important to take word similarity
into account. In all cases, A edged out the other
methods. The average improvement in using A
instead of Pc is .0082; this difference is signifi-
cant to the .1 level (p &lt; .085), according to the
paired t-test.
31t should be noted, however, that on B0-1 data, KL-
divergence performed slightly better than the L1 norm.
</bodyText>
<page confidence="0.991477">
61
</page>
<figure confidence="0.997156523809524">
Ertet Rates on Tose Sets, Base Langan. Motaa ULEt
.RANDMLE1&apos;
1.MLEV
AULEI â€”
- ----
Eno, Rates en Teel Sets. 900. Language Model MLE.ol
&apos;FIANDSILEolâ€¢ â€”
TONFAILE01.
0.5
0.45
0.4
0.35
0.3
0.25
0.55
0.5
0.45
0.4
0.35
0.3
Ti 22 23 24 15 Ti 12 13 24 TO
</figure>
<figureCaption confidence="0.999563">
Figure 1: Error rates for each test set, where the
</figureCaption>
<bodyText confidence="0.998829090909091">
base language model was MLE-1. The methods,
going from left to right, are RAND , Pc, L, and
A. The performances shown are for settings of i3
that were optimal for the corresponding training
set. 3 ranged from 4.0 to 4.5 for L and from 10
to 13 for A.
The results for the MLE-ol case are depicted
in figure 2. Again, we see the similarity-based
methods achieving far lower error rates than the
NILE, back-off, and RAND methods, and again,
.4 always performed the best. However, with
singletons omitted the difference between A and
Pc is even greater, the average difference being
.024, which is significant to the .01 level (paired
t-test).
An important observation is that all meth-
ods, including RAND, were much more effective
if singletons were included in the base language
model; thus, in the case of unseen word pairs,
Katz&apos;s claim that singletons can be safely ig-
nored in the back-off model does not hold for
similarity-based models.
</bodyText>
<sectionHeader confidence="0.999422" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.973355863636364">
Similarity-based language models provide an
appealing approach for dealing with data
sparseness. We have described and compared
the performance of four such models against two
classical estimation methods, the MLE method
and Katz&apos;s back-off scheme, on a pseudo-word
disambiguation task. We observed that the
similarity-based methods perform much better
on unseen word pairs, with the measure based
Figure 2: Error rates for each test set, where
the base language model was MLE-ol. 3 ranged
from 6 to 11 for L and from 21 to 22 for A.
on the KL divergence to the average, being the
best overall.
We also investigated Katz&apos;s claim that one
can discard singletons in the training data, re-
sulting in a more compact language model,
without significant loss of performance. Our re-
sults indicate that for similarity-based language
modeling, singletons are quite important; their
omission leads to significant degradation of per-
formance.
</bodyText>
<sectionHeader confidence="0.99651" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999808">
We thank Hiyan Alshawi, Joshua Goodman,
Rebecca Hwa, Stuart Shieber, and Yoram
Singer for many helpful comments and discus-
sions. Part of this work was done while the first
and second authors were visiting AT&amp;T Labs.
This material is based upon work supported in
part by the National Science Foundation under
Grant No. IRI-9350192. The second author
also gratefully acknowledges support from a Na-
tional Science Foundation Graduate Fellowship
and an AT&amp;T GRPW/ALFP grant.
</bodyText>
<sectionHeader confidence="0.998651" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9995964">
Brown, Peter F., Vincent J. DellaPietra, Peter V.
deSouza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467-479,
December.
</reference>
<page confidence="0.980843">
62
</page>
<reference confidence="0.999538379310345">
Church, Kenneth. 1988. A stochastic parts program
and noun phrase parser for unrestricted text. In
Proceedings of the Second Conference on Applied
Natural Language Processing, pages 136-143.
Church, Kenneth W. and William A. Gale. 1991.
A comparison of the enhanced Good-Turing and
deleted estimation methods for estimating proba-
bilites of english bigrams. Computer Speech and
Language, 5:19-54.
Cover, Thomas M. and Joy A. Thomas. 1991. Ele-
ments of Information Theory. John Wiley.
Dagan, Ido, Fernando Pereira, and Lillian Lee. 1994.
Similarity-based estimation of word cooccurrence
probabilities. In Proceedings of the 32nd Annual
Meeting of the ACL, pages 272-278, Las Cruces,
NM.
Essen, Ute and Volker Steinbiss. 1992. Co-
occurrence smoothing for stochastic language
modeling. In Proceedings of ICASSP, volume 1,
pages 161-164.
Gale, William, Kenneth Church, and David
Yarowsky. 1992. Work on statistcal methods for
word sense disambiguation. In Working Notes,
AAAI Fall Symposium Series, Probabilistic Ap-
proaches to Natural Language, pages 54-60.
Good, I.J. 1953. The population frequencies of
species and the estimation of population parame-
ters. Biometrika, 40(3 and 4):237-264.
Hoeffding, Wassily. 1965. Asymptotically optimal
tests for multinomial distributions. Annals of
Mathematical Statistics, pages 369-401.
Jelinek, Frederick, Robert L. Mercer, and Salim
Roukos. 1992. Principles of lexical language
modeling for speech recognition. In In Sadaoki
Furui and M. Mohan Sondhi, editors, Advances
in Speech Signal Processing. Mercer Dekker, Inc.,
pages 651-699.
Karov, Yael and Shimon Edelman. 1996. Learning
similarity-based word sense disambiguation from
sparse data. In 4rth Workshop on Very Large
Corpora.
Katz, Slava M. 1987. Estimation of probabilities
from sparse data for the language model com-
ponent of a speech recognizer. IEEE Transac-
tions on Acoustics, Speech and Signal Processing,
ASSP-35(3):400-401, March.
Pereira, Fernando, Naftali Tishby, and Lillian Lee.
1993. Distributional clustering of English words.
In Proceedings of the 31st Annual Meeting of the
ACL, pages 183-190, Columbus, OH.
Resnik, Philip. 1992. Wordnet and distributional
analysis: A class-based approach to lexical discov-
ery. AAAI Workshop on Statistically-based Natu-
ral Language Processing Techniques, pages 56-64,
July.
Schiitze, Hinrich. 1992. Context space. In Work-
ing Notes, AAAI Fall Symposium on Probabilistic
Approaches to Natural Language.
</reference>
<page confidence="0.99946">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.956540">
<title confidence="0.999814">Similarity-Based Methods For Word Sense Disambiguation</title>
<author confidence="0.997583">Ido Dagan</author>
<affiliation confidence="0.99664">Dept. of Mathematics and Computer Science Bar Ilan University</affiliation>
<address confidence="0.99739">Ramat Gan 52900, Israel</address>
<email confidence="0.99726">dagan@macs.biu.ac.il</email>
<author confidence="0.996685">Lillian Lee</author>
<affiliation confidence="0.997189666666667">Div. of Engineering and Applied Sciences Harvard University</affiliation>
<address confidence="0.999992">Cambridge, MA 01238, USA</address>
<email confidence="0.999811">lleeeleecs.harvard.edu</email>
<author confidence="0.999311">Fernando Pereira</author>
<affiliation confidence="0.998345">AT&amp;T Labs - Research</affiliation>
<address confidence="0.999635">600 Mountain Ave. Murray Hill, NJ 07974, USA</address>
<email confidence="0.999813">pereira@research.att.com</email>
<abstract confidence="0.999051769230769">We compare four similarity-based estimation methods against back-off and maximum-likelihood estimation methods on a pseudo-word sense disambiguation task in which we controlled for both unigram and bigram frequency. The similarity-based methods perform up to 40% better on this particular task. We also conclude that events that occur only once in the training set have major impact on similarity-based estimates.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J DellaPietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="1540" citStr="Brown et al., 1992" startWordPosition="233" endWordPosition="236">processing. Even large training sets tend to misrepresent low-probability events, since rare events may not appear in the training corpus at all. We concentrate here on the problem of estimating the probability of unseen word pairs, that is, pairs that do not occur in the training set. Katz&apos;s back-off scheme (Katz, 1987), widely used in bigram language modeling, estimates the probability of an unseen bigram by utilizing unigram estimates. This has the undesirable result of assigning unseen bigrams the same probability if they are made up of unigrams of the same frequency. Class-based methods (Brown et al., 1992; Pereira, Tishby, and Lee, 1993; Resnik, 1992) cluster words into classes of similar words, so that one can base the estimate of a word pair&apos;s probability on the averaged cooccurrence probability of the classes to which the two words belong. However, a word is therefore modeled by the average behavior of many words, which may cause the given word&apos;s idiosyncrasies to be ignored. For instance, the word &amp;quot;red&amp;quot; might well act like a generic color word in most cases, but it has distinctive cooccurrence patterns with respect to words like &amp;quot;apple,&amp;quot; &amp;quot;banana,&amp;quot; and so on. We therefore consider similarit</context>
</contexts>
<marker>Brown, DellaPietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Brown, Peter F., Vincent J. DellaPietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="15908" citStr="Church, 1988" startWordPosition="2635" endWordPosition="2636">rmat of which is as follows. We first construct a list of pseudowords, each of which is the combination of two different words in V2. Each word in V2 contributes to exactly one pseudo-word. Then, we replace each w2 in the test set with its corresponding pseudo-word. For example, if we choose to create a pseudo-word out of the words &amp;quot;make&amp;quot; and &amp;quot;take&amp;quot;, we would change the test data like this: make plans = {make, take} plans take action {make, take} action The method being tested must choose between the two words that make up the pseudo-word. 3.2 Data We used a statistical part-of-speech tagger (Church, 1988) and pattern matching and concordancing tools (due to David Yarowsky) to identify transitive main verbs and head nouns of the corresponding direct objects in 44 million words of 1988 Associated Press newswire. We selected the noun-verb pairs for the 1000 most frequent nouns in the corpus. These pairs are undoubtedly somewhat noisy given the errors inherent in the part-of-speech tagging and pattern matching. We used 80%, or 587833, of the pairs so derived, for building base bigram language models, reserving 20% for testing purposes. As some, but not all, of the similarity measures require smoot</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>Church, Kenneth. 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>William A Gale</author>
</authors>
<title>A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilites of english bigrams.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--19</pages>
<contexts>
<context position="5999" citStr="Church and Gale, 1991" startWordPosition="957" endWordPosition="960">ounting and Redistribution Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (w1, w2), conditional on the appearance of word w1, is simply c(wi, w2) PmL(w2lwi) = (1) c(wi) where c(wi, w2) is the frequency of (w1, w2) in the training corpus and c(wi) is the frequency of w1. However, PML is zero for any unseen word pair, which leads to extremely inaccurate estimates for word pair probabilities. Previous proposals for remedying the above problem (Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz, 1987; Church and Gale, 1991) adjust the MLE in so that the total probability of seen word pairs is less than one, leaving some probability mass to be redistributed among the unseen pairs. In general, the adjustment involves either interpolation, in which the MLE is used in linear combination with an estimator guaranteed to be nonzero for unseen word pairs, or discounting, in which a reduced MLE is used for seen word pairs, with the probability mass left over from this reduction used to model unseen pairs. The discounting approach is the one adopted by Katz (1987): { Pd(W211111) C(Wi, W2) &gt;0 15(W2IW1) p \ râ€˜W211Vi, 0.W. (</context>
</contexts>
<marker>Church, Gale, 1991</marker>
<rawString>Church, Kenneth W. and William A. Gale. 1991. A comparison of the enhanced Good-Turing and deleted estimation methods for estimating probabilites of english bigrams. Computer Speech and Language, 5:19-54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas M Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley.</publisher>
<contexts>
<context position="9779" citStr="Cover and Thomas, 1991" startWordPosition="1589" endWordPosition="1592">or the rest of this paper, Pr(w2lwi) = Psim (w2i wi)â€¢ 2.3 Measures of Similarity We now consider several word similarity functions that can be derived automatically from the statistics of a training corpus, as opposed to functions derived from manually-constructed word classes (Resnik, 1992). All the similarity functions we describe below depend just on the base language model P(1.), not the discounted model P(1-) from Section 2.1 above. 2.3.1 KL divergence Kullback-Leibler (KL) divergence is a standard information-theoretic measure of the dissimilarity between two probability mass functions (Cover and Thomas, 1991). We can apply it to the conditional distribution P(â€¢Iw1) induced by w1 on words in 172: E P(w2iwo log P(w2ltvi) â€¢ (4) P(w2iwo For D(willw&apos;i) to be defined it must be the case that P(w2Iwc) &gt; 0 whenever P(w2iwi) &gt; 0. Unfortunately, this will not in general be the case for MLEs based on samples, so we would need smoothed estimates of P(w21wD that redistribute some probability mass to zerofrequency events. However, using smoothed estimates for P(w2I wi) as well requires a sum over all w2 E V2, which is expensive for the large vocabularies under consideration. Given the smoothed denominator distr</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Cover, Thomas M. and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Fernando Pereira</author>
<author>Lillian Lee</author>
</authors>
<title>Similarity-based estimation of word cooccurrence probabilities.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the ACL,</booktitle>
<pages>272--278</pages>
<location>Las Cruces, NM.</location>
<marker>Dagan, Pereira, Lee, 1994</marker>
<rawString>Dagan, Ido, Fernando Pereira, and Lillian Lee. 1994. Similarity-based estimation of word cooccurrence probabilities. In Proceedings of the 32nd Annual Meeting of the ACL, pages 272-278, Las Cruces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ute Essen</author>
<author>Volker Steinbiss</author>
</authors>
<title>Cooccurrence smoothing for stochastic language modeling.</title>
<date>1992</date>
<booktitle>In Proceedings of ICASSP,</booktitle>
<volume>1</volume>
<pages>161--164</pages>
<contexts>
<context position="2786" citStr="Essen and Steinbiss (1992)" startWordPosition="440" endWordPosition="443">chemes that do not require building general word classes. Instead, estimates for the most similar words to a word w are combined; the evidence provided by word w&apos; is weighted by a function of its similarity to w. Dagan, Markus, and Markovitch (1993) propose such a scheme for predicting which unseen cooccurrences are more likely than others. However, their scheme does not assign probabilities. In what follows, we focus on probabilistic similarity-based estimation methods. We compared several such methods, including that of Dagan, Pereira, and Lee (1994) and the cooccurrence smoothing method of Essen and Steinbiss (1992), against classical estimation methods, including that of Katz, in a decision task involving unseen pairs of direct objects and verbs, where unigram frequency was eliminated from being a factor. We found that all the similarity-based schemes performed almost 40% better than back-off, which is expected to yield about 50% accuracy in our experimental setting. Furthermore, a scheme based on the total divergence of empirical dis56 tributions to their average&apos; yielded statistically significant improvement in error rate over cooccurrence smoothing. We also investigated the effect of removing extreme</context>
<context position="8255" citStr="Essen and Steinbiss (1992)" startWordPosition="1342" endWordPosition="1345">an increasing function of the similarity between w1 and w, and let 8(w1) denote the set of words most similar to w1. Then the general form of similarity model we consider is a W-weighted linear combination of predictions of similar words: W(Wil PSIM(W2IW1) E p(w2m), N(wi) wiEs(.,) (3) where N(w1) = EwlEs(.0 w(wi, wc) is a normalization factor. According to this formula, w2 is more likely to occur with w1 if it tends to occur with the words that are most similar to wi Considerable latitude is allowed in defining the set S(wi), as is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set S(wi) =V1. However, it may be desirable to restrict S(wi) in some fashion, especially if VI is large. For instance, Dagan. Pereira, and Lee (1994) use the closest k or fewer words w such that the dissimilarity between w1 and w is less than a threshold value t; k and t are tuned experimentally. Now, we could directly replace Pr(w2lw1) in the back-off equation (2) with Psim(w2iwi)â€¢ However, other variations are possible, such as interpolating with the unigram probability P(w2): Pr(w2iw1) = 7P(w2) + (1 â€” 7)Psim(w2lw1). where -y is determined experime</context>
<context position="11906" citStr="Essen and Steinbiss (1992)" startWordPosition="1977" endWordPosition="1980">As in the KL divergence case, we set W(wi, wii) to be 10-0A(tvi,w1). 2.3.3 L1 norm The L1 norm is defined as L(wi,wi) = E iP(w2Itu1) â€” P(tv2iiv)1. (6) W2 By grouping terms as before, we can express L(wi, u4) in a form depending only on the &amp;quot;common&amp;quot; 1112: L (wi, = 2 â€” E p(w2)â€” E pi(w2) w,ec w2ec + E Ip(102) Pi(W2)1- u&apos;2EC This last form makes it clear that 0 &lt; L(wi, &lt;2, with equality if and only if there are no words w2 such that both P(w2lwi) and P(w2iwi) Since we require a weighting scheme that is decreasing in L, we set W(wi, = (2 â€” L(wi, wii))13 with again free. 2.3.4 Confusion probability Essen and Steinbiss (1992) introduced confusion probability 2, which estimates the probability that word w can be substituted for word Pc(w&apos;ilwi) = w(wi,w/) EP(tviltv2)P(w/ilw2)P(w2) P(wi) Unlike the measures described above, w1 may not necessarily be the &amp;quot;closest&amp;quot; word to itself, that is, there may exist a word &amp;I such that Pc(4101) &gt; Pc(wilwi) The confusion probability can be computed from empirical estimates provided all unigram estimates are nonzero (as we assume throughout). In fact, the use of smoothed estimates like those of Katz&apos;s back-off scheme is problematic, because those estimates typically do not preserve</context>
</contexts>
<marker>Essen, Steinbiss, 1992</marker>
<rawString>Essen, Ute and Volker Steinbiss. 1992. Cooccurrence smoothing for stochastic language modeling. In Proceedings of ICASSP, volume 1, pages 161-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Gale</author>
<author>Kenneth Church</author>
<author>David Yarowsky</author>
</authors>
<title>Work on statistcal methods for word sense disambiguation.</title>
<date>1992</date>
<booktitle>In Working Notes, AAAI Fall Symposium Series, Probabilistic Approaches to Natural Language,</booktitle>
<pages>54--60</pages>
<contexts>
<context position="15279" citStr="Gale, Church, and Yarowsky, 1992" startWordPosition="2520" endWordPosition="2524">&amp;quot;bank&amp;quot; refers to a river bank, a savings bank, or perhaps some other alternative. While sense disambiguation is clearly an important task, it presents numerous experimental difficulties. First, the very notion of &amp;quot;sense&amp;quot; is not clearly defined; for instance, dictionaries may provide sense distinctions that are too fine or too coarse for the data at hand. Also, one needs to have training data for which the correct senses have been assigned, which can require considerable human effort. To circumvent these and other difficulties, we set up a pseudo-word disambiguation experiment (Schiitze, 1992; Gale, Church, and Yarowsky, 1992) the general format of which is as follows. We first construct a list of pseudowords, each of which is the combination of two different words in V2. Each word in V2 contributes to exactly one pseudo-word. Then, we replace each w2 in the test set with its corresponding pseudo-word. For example, if we choose to create a pseudo-word out of the words &amp;quot;make&amp;quot; and &amp;quot;take&amp;quot;, we would change the test data like this: make plans = {make, take} plans take action {make, take} action The method being tested must choose between the two words that make up the pseudo-word. 3.2 Data We used a statistical part-of</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, William, Kenneth Church, and David Yarowsky. 1992. Work on statistcal methods for word sense disambiguation. In Working Notes, AAAI Fall Symposium Series, Probabilistic Approaches to Natural Language, pages 54-60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I J Good</author>
</authors>
<title>The population frequencies of species and the estimation of population parameters.</title>
<date>1953</date>
<journal>Biometrika,</journal>
<volume>40</volume>
<issue>3</issue>
<pages>4--237</pages>
<contexts>
<context position="5928" citStr="Good, 1953" startWordPosition="948" endWordPosition="949"> sample was drawn from a given joint distribution. 2.1 Discounting and Redistribution Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (w1, w2), conditional on the appearance of word w1, is simply c(wi, w2) PmL(w2lwi) = (1) c(wi) where c(wi, w2) is the frequency of (w1, w2) in the training corpus and c(wi) is the frequency of w1. However, PML is zero for any unseen word pair, which leads to extremely inaccurate estimates for word pair probabilities. Previous proposals for remedying the above problem (Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz, 1987; Church and Gale, 1991) adjust the MLE in so that the total probability of seen word pairs is less than one, leaving some probability mass to be redistributed among the unseen pairs. In general, the adjustment involves either interpolation, in which the MLE is used in linear combination with an estimator guaranteed to be nonzero for unseen word pairs, or discounting, in which a reduced MLE is used for seen word pairs, with the probability mass left over from this reduction used to model unseen pairs. The discounting approach is the one adopted by</context>
</contexts>
<marker>Good, 1953</marker>
<rawString>Good, I.J. 1953. The population frequencies of species and the estimation of population parameters. Biometrika, 40(3 and 4):237-264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wassily Hoeffding</author>
</authors>
<title>Asymptotically optimal tests for multinomial distributions.</title>
<date>1965</date>
<journal>Annals of Mathematical Statistics,</journal>
<pages>369--401</pages>
<contexts>
<context position="5286" citStr="Hoeffding (1965)" startWordPosition="842" endWordPosition="843">course, a function measuring the similarity between words. We give the details of each of these three parts in the following three sections. We will only be concerned with similarity between words in VI. &apos;To the best of our knowledge, this is the first use of this particular distribution dissimilarity function in statistical language processing. The function itself is implicit in earlier work on distributional clustering (Pereira, Tishby, and Lee, 1993), has been used by Tishby (p.c.) in other distributional similarity work, and, as suggested by Yoav Freund (p.c.), it is related to results of Hoeffding (1965) on the probability that a given sample was drawn from a given joint distribution. 2.1 Discounting and Redistribution Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (w1, w2), conditional on the appearance of word w1, is simply c(wi, w2) PmL(w2lwi) = (1) c(wi) where c(wi, w2) is the frequency of (w1, w2) in the training corpus and c(wi) is the frequency of w1. However, PML is zero for any unseen word pair, which leads to extremely inaccurate estimates for word pair probabilities. Previous proposals f</context>
</contexts>
<marker>Hoeffding, 1965</marker>
<rawString>Hoeffding, Wassily. 1965. Asymptotically optimal tests for multinomial distributions. Annals of Mathematical Statistics, pages 369-401.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Jelinek</author>
<author>Robert L Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Principles of lexical language modeling for speech recognition.</title>
<date>1992</date>
<booktitle>In In Sadaoki Furui</booktitle>
<pages>651--699</pages>
<editor>and M. Mohan Sondhi, editors,</editor>
<publisher>Mercer Dekker, Inc.,</publisher>
<contexts>
<context position="5963" citStr="Jelinek, Mercer, and Roukos, 1992" startWordPosition="950" endWordPosition="954">drawn from a given joint distribution. 2.1 Discounting and Redistribution Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (w1, w2), conditional on the appearance of word w1, is simply c(wi, w2) PmL(w2lwi) = (1) c(wi) where c(wi, w2) is the frequency of (w1, w2) in the training corpus and c(wi) is the frequency of w1. However, PML is zero for any unseen word pair, which leads to extremely inaccurate estimates for word pair probabilities. Previous proposals for remedying the above problem (Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz, 1987; Church and Gale, 1991) adjust the MLE in so that the total probability of seen word pairs is less than one, leaving some probability mass to be redistributed among the unseen pairs. In general, the adjustment involves either interpolation, in which the MLE is used in linear combination with an estimator guaranteed to be nonzero for unseen word pairs, or discounting, in which a reduced MLE is used for seen word pairs, with the probability mass left over from this reduction used to model unseen pairs. The discounting approach is the one adopted by Katz (1987): { Pd(W211111) C(Wi, W</context>
</contexts>
<marker>Jelinek, Mercer, Roukos, 1992</marker>
<rawString>Jelinek, Frederick, Robert L. Mercer, and Salim Roukos. 1992. Principles of lexical language modeling for speech recognition. In In Sadaoki Furui and M. Mohan Sondhi, editors, Advances in Speech Signal Processing. Mercer Dekker, Inc., pages 651-699.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yael Karov</author>
<author>Shimon Edelman</author>
</authors>
<title>Learning similarity-based word sense disambiguation from sparse data.</title>
<date>1996</date>
<booktitle>In 4rth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="8284" citStr="Karov and Edelman (1996)" startWordPosition="1347" endWordPosition="1350">imilarity between w1 and w, and let 8(w1) denote the set of words most similar to w1. Then the general form of similarity model we consider is a W-weighted linear combination of predictions of similar words: W(Wil PSIM(W2IW1) E p(w2m), N(wi) wiEs(.,) (3) where N(w1) = EwlEs(.0 w(wi, wc) is a normalization factor. According to this formula, w2 is more likely to occur with w1 if it tends to occur with the words that are most similar to wi Considerable latitude is allowed in defining the set S(wi), as is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set S(wi) =V1. However, it may be desirable to restrict S(wi) in some fashion, especially if VI is large. For instance, Dagan. Pereira, and Lee (1994) use the closest k or fewer words w such that the dissimilarity between w1 and w is less than a threshold value t; k and t are tuned experimentally. Now, we could directly replace Pr(w2lw1) in the back-off equation (2) with Psim(w2iwi)â€¢ However, other variations are possible, such as interpolating with the unigram probability P(w2): Pr(w2iw1) = 7P(w2) + (1 â€” 7)Psim(w2lw1). where -y is determined experimentally (Dagan, Pereira, and L</context>
</contexts>
<marker>Karov, Edelman, 1996</marker>
<rawString>Karov, Yael and Shimon Edelman. 1996. Learning similarity-based word sense disambiguation from sparse data. In 4rth Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slava M Katz</author>
</authors>
<title>Estimation of probabilities from sparse data for the language model component of a speech recognizer.</title>
<date>1987</date>
<journal>IEEE Transactions on Acoustics, Speech and Signal Processing,</journal>
<pages>35--3</pages>
<contexts>
<context position="1244" citStr="Katz, 1987" startWordPosition="186" endWordPosition="187">larity-based methods perform up to 40% better on this particular task. We also conclude that events that occur only once in the training set have major impact on similarity-based estimates. 1 Introduction The problem of data sparseness affects all statistical methods for natural language processing. Even large training sets tend to misrepresent low-probability events, since rare events may not appear in the training corpus at all. We concentrate here on the problem of estimating the probability of unseen word pairs, that is, pairs that do not occur in the training set. Katz&apos;s back-off scheme (Katz, 1987), widely used in bigram language modeling, estimates the probability of an unseen bigram by utilizing unigram estimates. This has the undesirable result of assigning unseen bigrams the same probability if they are made up of unigrams of the same frequency. Class-based methods (Brown et al., 1992; Pereira, Tishby, and Lee, 1993; Resnik, 1992) cluster words into classes of similar words, so that one can base the estimate of a word pair&apos;s probability on the averaged cooccurrence probability of the classes to which the two words belong. However, a word is therefore modeled by the average behavior </context>
<context position="5975" citStr="Katz, 1987" startWordPosition="955" endWordPosition="956">on. 2.1 Discounting and Redistribution Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (w1, w2), conditional on the appearance of word w1, is simply c(wi, w2) PmL(w2lwi) = (1) c(wi) where c(wi, w2) is the frequency of (w1, w2) in the training corpus and c(wi) is the frequency of w1. However, PML is zero for any unseen word pair, which leads to extremely inaccurate estimates for word pair probabilities. Previous proposals for remedying the above problem (Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz, 1987; Church and Gale, 1991) adjust the MLE in so that the total probability of seen word pairs is less than one, leaving some probability mass to be redistributed among the unseen pairs. In general, the adjustment involves either interpolation, in which the MLE is used in linear combination with an estimator guaranteed to be nonzero for unseen word pairs, or discounting, in which a reduced MLE is used for seen word pairs, with the probability mass left over from this reduction used to model unseen pairs. The discounting approach is the one adopted by Katz (1987): { Pd(W211111) C(Wi, W2) &gt;0 15(W2I</context>
<context position="16856" citStr="Katz, 1987" startWordPosition="2792" endWordPosition="2793">n the errors inherent in the part-of-speech tagging and pattern matching. We used 80%, or 587833, of the pairs so derived, for building base bigram language models, reserving 20% for testing purposes. As some, but not all, of the similarity measures require smoothed language models, we calculated both a Katz back-off language model (P = P (equation (2)), with P,.(w2iw1) = P(w2)), and a maximum-likelihood model (P = PML). Furthermore, we wished to investigate Katz&apos;s claim that one can delete singletons, word pairs that occur only once, from the training set without affecting model performance (Katz, 1987); our training set contained 82407 singletons. We therefore built four base language models, summarized in Table 2. Table 2: Base Language Models Since we wished to test the effectiveness of using similarity for unseen word cooccurrences, we removed from the test set any verb-object pairs with singletons (587833 pairs) MLE-1 B0-1 no singletons (505426 pairs) MLE-o1 BO-ol MLE Katz 60 name range base LM constraints tune? [0, co] P(w2101) 0 0 if P(w2lwi) 0 0 yes A [0,2 log 2] none yes [0,2] none yes PC [0, maxâ€ž P(w2)] Bayes consistency no Table 1: Summary of similarity function properties that oc</context>
</contexts>
<marker>Katz, 1987</marker>
<rawString>Katz, Slava M. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE Transactions on Acoustics, Speech and Signal Processing, ASSP-35(3):400-401, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the ACL,</booktitle>
<pages>183--190</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="1572" citStr="Pereira, Tishby, and Lee, 1993" startWordPosition="237" endWordPosition="241">ge training sets tend to misrepresent low-probability events, since rare events may not appear in the training corpus at all. We concentrate here on the problem of estimating the probability of unseen word pairs, that is, pairs that do not occur in the training set. Katz&apos;s back-off scheme (Katz, 1987), widely used in bigram language modeling, estimates the probability of an unseen bigram by utilizing unigram estimates. This has the undesirable result of assigning unseen bigrams the same probability if they are made up of unigrams of the same frequency. Class-based methods (Brown et al., 1992; Pereira, Tishby, and Lee, 1993; Resnik, 1992) cluster words into classes of similar words, so that one can base the estimate of a word pair&apos;s probability on the averaged cooccurrence probability of the classes to which the two words belong. However, a word is therefore modeled by the average behavior of many words, which may cause the given word&apos;s idiosyncrasies to be ignored. For instance, the word &amp;quot;red&amp;quot; might well act like a generic color word in most cases, but it has distinctive cooccurrence patterns with respect to words like &amp;quot;apple,&amp;quot; &amp;quot;banana,&amp;quot; and so on. We therefore consider similarity-based estimation schemes that </context>
<context position="5126" citStr="Pereira, Tishby, and Lee, 1993" startWordPosition="812" endWordPosition="816">age model consists of three parts: a scheme for deciding which word pairs require a similarity-based estimate, a method for combining information from similar words, and, of course, a function measuring the similarity between words. We give the details of each of these three parts in the following three sections. We will only be concerned with similarity between words in VI. &apos;To the best of our knowledge, this is the first use of this particular distribution dissimilarity function in statistical language processing. The function itself is implicit in earlier work on distributional clustering (Pereira, Tishby, and Lee, 1993), has been used by Tishby (p.c.) in other distributional similarity work, and, as suggested by Yoav Freund (p.c.), it is related to results of Hoeffding (1965) on the probability that a given sample was drawn from a given joint distribution. 2.1 Discounting and Redistribution Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (w1, w2), conditional on the appearance of word w1, is simply c(wi, w2) PmL(w2lwi) = (1) c(wi) where c(wi, w2) is the frequency of (w1, w2) in the training corpus and c(wi) is the</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Pereira, Fernando, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the ACL, pages 183-190, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Wordnet and distributional analysis: A class-based approach to lexical discovery.</title>
<date>1992</date>
<booktitle>AAAI Workshop on Statistically-based Natural Language Processing Techniques,</booktitle>
<pages>56--64</pages>
<contexts>
<context position="1587" citStr="Resnik, 1992" startWordPosition="242" endWordPosition="243">esent low-probability events, since rare events may not appear in the training corpus at all. We concentrate here on the problem of estimating the probability of unseen word pairs, that is, pairs that do not occur in the training set. Katz&apos;s back-off scheme (Katz, 1987), widely used in bigram language modeling, estimates the probability of an unseen bigram by utilizing unigram estimates. This has the undesirable result of assigning unseen bigrams the same probability if they are made up of unigrams of the same frequency. Class-based methods (Brown et al., 1992; Pereira, Tishby, and Lee, 1993; Resnik, 1992) cluster words into classes of similar words, so that one can base the estimate of a word pair&apos;s probability on the averaged cooccurrence probability of the classes to which the two words belong. However, a word is therefore modeled by the average behavior of many words, which may cause the given word&apos;s idiosyncrasies to be ignored. For instance, the word &amp;quot;red&amp;quot; might well act like a generic color word in most cases, but it has distinctive cooccurrence patterns with respect to words like &amp;quot;apple,&amp;quot; &amp;quot;banana,&amp;quot; and so on. We therefore consider similarity-based estimation schemes that do not require </context>
<context position="9448" citStr="Resnik, 1992" startWordPosition="1541" endWordPosition="1542"> determined experimentally (Dagan, Pereira, and Lee. 1994). This represents, in effect, a linear combination of the similarity estimate and the back-off estimate: if y = 1, then we have exactly Katz&apos;s back-off scheme. As we focus in this paper on alternatives for Psim, we will not consider this approach here; that is, for the rest of this paper, Pr(w2lwi) = Psim (w2i wi)â€¢ 2.3 Measures of Similarity We now consider several word similarity functions that can be derived automatically from the statistics of a training corpus, as opposed to functions derived from manually-constructed word classes (Resnik, 1992). All the similarity functions we describe below depend just on the base language model P(1.), not the discounted model P(1-) from Section 2.1 above. 2.3.1 KL divergence Kullback-Leibler (KL) divergence is a standard information-theoretic measure of the dissimilarity between two probability mass functions (Cover and Thomas, 1991). We can apply it to the conditional distribution P(â€¢Iw1) induced by w1 on words in 172: E P(w2iwo log P(w2ltvi) â€¢ (4) P(w2iwo For D(willw&apos;i) to be defined it must be the case that P(w2Iwc) &gt; 0 whenever P(w2iwi) &gt; 0. Unfortunately, this will not in general be the case </context>
</contexts>
<marker>Resnik, 1992</marker>
<rawString>Resnik, Philip. 1992. Wordnet and distributional analysis: A class-based approach to lexical discovery. AAAI Workshop on Statistically-based Natural Language Processing Techniques, pages 56-64, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Context space.</title>
<date>1992</date>
<booktitle>In Working Notes, AAAI Fall Symposium on Probabilistic Approaches to Natural Language.</booktitle>
<contexts>
<context position="15245" citStr="Schiitze, 1992" startWordPosition="2518" endWordPosition="2519"> decide whether &amp;quot;bank&amp;quot; refers to a river bank, a savings bank, or perhaps some other alternative. While sense disambiguation is clearly an important task, it presents numerous experimental difficulties. First, the very notion of &amp;quot;sense&amp;quot; is not clearly defined; for instance, dictionaries may provide sense distinctions that are too fine or too coarse for the data at hand. Also, one needs to have training data for which the correct senses have been assigned, which can require considerable human effort. To circumvent these and other difficulties, we set up a pseudo-word disambiguation experiment (Schiitze, 1992; Gale, Church, and Yarowsky, 1992) the general format of which is as follows. We first construct a list of pseudowords, each of which is the combination of two different words in V2. Each word in V2 contributes to exactly one pseudo-word. Then, we replace each w2 in the test set with its corresponding pseudo-word. For example, if we choose to create a pseudo-word out of the words &amp;quot;make&amp;quot; and &amp;quot;take&amp;quot;, we would change the test data like this: make plans = {make, take} plans take action {make, take} action The method being tested must choose between the two words that make up the pseudo-word. 3.2 </context>
</contexts>
<marker>Schiitze, 1992</marker>
<rawString>Schiitze, Hinrich. 1992. Context space. In Working Notes, AAAI Fall Symposium on Probabilistic Approaches to Natural Language.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>