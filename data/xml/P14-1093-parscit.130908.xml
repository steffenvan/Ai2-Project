<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.98918">
Toward Future Scenario Generation: Extracting Event Causality
Exploiting Semantic Relation, Context, and Association Features
</title>
<author confidence="0.682436">
Chikara Hashimoto∗ Kentaro Torisawa† Julien Kloetzer‡ Motoki Sano§
Istv´an Varga¶ Jong-Hoon Ohll Yutaka Kidawara∗∗
</author>
<affiliation confidence="0.4867905">
∗ † ‡ § II ∗∗National Institute of Information and Communications Technology, Kyoto, 619-0289, Japan
¶NEC Knowledge Discovery Research Laboratories, Nara, 630-0101, Japan
</affiliation>
<email confidence="0.807565">
J∗ ch, † torisawa, ‡ julien, § msano, 11 rovellia, ∗∗kidawaral@nict.go.jp
</email>
<sectionHeader confidence="0.987063" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99988775">
We propose a supervised method of
extracting event causalities like conduct
slash-and-burn agriculture—*exacerbate
desertification from the web using se-
mantic relation (between nouns), context,
and association features. Experiments
show that our method outperforms base-
lines that are based on state-of-the-art
methods. We also propose methods of
generating future scenarios like conduct
slash-and-burn agriculture—*exacerbate
desertification—*increase Asian dust (from
China)—*asthma gets worse. Experi-
ments show that we can generate 50,000
scenarios with 68% precision. We also
generated a scenario deforestation con-
tinues—*global warming worsens—*sea
temperatures rise—*vibrio parahaemolyti-
cus fouls (water), which is written in no
document in our input web corpus crawled
in 2007. But the vibrio risk due to global
warming was observed in Baker-Austin
et al. (2013). Thus, we “predicted” the
future event sequence in a sense.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999828740740741">
The world can be seen as a network of causal-
ity where people, organizations, and other kinds
of entities causally depend on each other. This
network is so huge and complex that it is almost
impossible for humans to exhaustively predict the
consequences of a given event. Indeed, after the
Great East Japan Earthquake in 2011, few ex-
pected that it would lead to an enormous trade
deficit in Japan due to a sharp increase in en-
ergy imports. For effective decision making that
carefully considers any form of future risks and
chances, we need a system that helps humans do
scenario planning (Schwartz, 1991), which is a
decision-making scheme that examines possible
future events and assesses their potential chances
and risks. Our ultimate goal is to develop a system
that supports scenario planning through generat-
ing possible future events using big data, which
would contain what Donald Rumsfeld called “un-
known unknowns”1 (Torisawa et al., 2010).
To this end, we propose a supervised method
of extracting such event causality as conduct
slash-and-burn agriculture—*exacerbate desertifi-
cation and use its output to generate future sce-
narios (scenarios), which are chains of causal-
ity that have been or might be observed in
this world like conduct slash-and-burn agricul-
ture—*exacerbate desertification—*increase Asian
dust (from China)—*asthma gets worse. Note that,
in this paper, A—*B denotes that A causes B, which
means that “if A happens, the probability of B in-
creases.” Our notion of causality should be inter-
preted probabilistically rather than logically.
Our method extracts event causality based on
three assumptions that are embodied as features
of our classifier. First, we assume that two nouns
(e.g. slash-and-burn agriculture and desertifica-
tion) that take some specific binary semantic rela-
tions (e.g. A CAUSES B) tend to constitute event
causality if combined with two predicates (e.g.
conduct and exacerbate). Note that semantic re-
lations are not restricted to those directly relevant
to causality like A CAUSES B but can be those that
might seem irrelevant to causality like A IS AN
INGREDIENT FOR B (e.g. plutonium and atomic
bomb as in plutonium is stolen—*atomic bomb is
made). Our underlying intuition is the observation
that event causality tends to hold between two en-
tities linked by semantic relations which roughly
entail that one entity strongly affects the other.
Such semantic relations can be expressed by (oth-
erwise unintuitive) patterns like A IS AN INGRE-
DIENT FOR B. As such, semantic relations like the
MATERIAL relation can also be useful. (See Sec-
</bodyText>
<footnote confidence="0.986088">
1http://youtu.be/GiPe1OiKQuk
</footnote>
<page confidence="0.860586">
987
</page>
<note confidence="0.835141">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 987–997,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.998842632183908">
tion 3.2.1 for a more intuitive explanation.)
Our second assumption is that there are gram-
matical contexts in which event causality is more
likely to appear. We implement what we con-
sider likely contexts for event causality as con-
text features. For example, a likely context of
event causality (underlined) would be: CO2 levels
rose, so climatic anomalies were observed, while
an unlikely context would be: It remains uncertain
whether if the recession is bottomed the declining
birth rate is halted. Useful context information in-
cludes the mood of the sentences (e.g., the uncer-
tainty mood expressed by uncertain above), which
is represented by lexical features (Section 3.2.2).
The last assumption embodied in our associa-
tion features is that each word of the cause phrase
must have a strong association (i.e., PMI, for ex-
ample) with that of the effect phrase as slash-and-
burn agriculture and desertification in the above
example, as in Do et al. (2011).
Our method exploits these features on top of our
base features such as nouns and predicates. Exper-
iments using 600 million web pages (Akamine et
al., 2010) show that our method outperforms base-
lines based on state-of-the-art methods (Do et al.,
2011; Hashimoto et al., 2012) by more than 19%
of average precision.
We require that event causality be self-
contained, i.e., intelligible as causality without the
sentences from which it was extracted. For ex-
ample, omit toothbrushing—*get a cavity is self-
contained, but omit toothbrushing—*get a girl-
friend is not since this is not intelligible without a
context: He omitted toothbrushing every day and
got a girlfriend who was a dental assistant of den-
tal clinic he went to for his cavity. This is im-
portant since future scenarios, which are gener-
ated by chaining event causality as described be-
low, must be self-contained, unlike Hashimoto et
al. (2012). To make event causality self-contained,
we wrote guidelines for manually annotating train-
ing/development/test data. Annotators regarded
as event causality only phrase pairs that were
interpretable as event causality without contexts
(i.e., self-contained). From the training data, our
method seemed to successfully learn what self-
contained event causality is.
Our scenario generation method generates sce-
narios by chaining extracted event causality; gen-
erating A—*B—*C from A—*B and B—*C. The chal-
lenge is that many acceptable scenarios are over-
looked if we require the joint part of the chain (B
above) to be an exact match. To increase the num-
ber of acceptable scenarios, our method identifies
compatibility w.r.t causality between two phrases
by a recently proposed semantic polarity, exci-
tation (Hashimoto et al., 2012), which properly
relaxes the chaining condition (Section 3.1 de-
scribes it). For example, our method can iden-
tify the compatibility between sea temperatures
are high and sea temperatures rise to chain global
warming worsens—*sea temperatures are high
and sea temperatures rise—*vibrio parahaemolyti-
cus2 fouls (water). Accordingly, we generated
a scenario deforestation continues—*global warm-
ing worsens—*sea temperatures rise—*vibrio para-
haemolyticus fouls (water), which is written in
no document in our input web corpus that was
crawled in 2007, but the vibrio risk due to global
warming has actually been observed in the Baltic
sea and reported in Baker-Austin et al. (2013). In
a sense, we “predicted” the event sequence re-
ported in 2013 by documents written in 2007. Our
experiments also show that we generated 50,000
scenarios with 68% precision, which include con-
duct terrorist operations—*terrorist bombing oc-
curs—*cause fatalities and injuries—*cause eco-
nomic losses and the above “slash-and-burn agri-
culture” scenario (Section 5.2). Neither is written
in any document in our input corpus.
In this paper, our target language is Japanese.
However, we believe that our ideas and methods
are applicable to many languages. Examples are
translated into English for ease of explanation.
Supplementary notes of this paper are available
at http://khn.nict.go.jp/analysis/
member/ch/acl2014-sup.pdf.
</bodyText>
<sectionHeader confidence="0.999766" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999704166666667">
For event causality extraction, clues used by
previous methods can roughly be categorized
as lexico-syntactic patterns (Abe et al., 2008;
Radinsky et al., 2012), words in context (Oh et
al., 2013), associations among words (Torisawa,
2006; Riaz and Girju, 2010; Do et al., 2011), and
predicate semantics (Hashimoto et al., 2012). Be-
sides features similar to those described above, we
propose semantic relation features3 that include
those that are not obviously related to causality.
We show that such thorough exploitation of new
and existing features leads to high performance.
</bodyText>
<footnote confidence="0.998557333333333">
2A bacterium in the sea causing food-poisoning.
3Radinsky et al. (2012) and Tanaka et al. (2012) used se-
mantic relations to generalize acquired causality instances.
</footnote>
<page confidence="0.996746">
988
</page>
<bodyText confidence="0.999674791666667">
Other clues include shared arguments (Torisawa,
2006; Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009), which we ignore since we tar-
get event causality about two distinct entities.
To the best of our knowledge, future scenario
generation is a new task, although previous works
have addressed similar tasks (Radinsky et al.,
2012; Radinsky and Horvitz, 2013). Neither in-
volves chaining and restricts themselves to only
one event causality step. Besides, the events they
predict must be those for which similar events
have previously been observed, and their method
only applies to news domain.
Some of the scenarios we generated are written
on no page in our input web corpus. Similarly,
Tsuchida et al. (2011) generated semantic knowl-
edge like causality that is written in no sentence.
However, their method cannot combine more than
two pieces of knowledge unlike ours, and their tar-
get knowledge consists of nouns, but ours consists
of verb phrases, which are more informative.
Tanaka et al. (2013)’s web information analy-
sis system provides a what-happens-if QA service,
which is based on our scenario generation method.
</bodyText>
<sectionHeader confidence="0.98825" genericHeader="method">
3 Event Causality Extraction Method
</sectionHeader>
<bodyText confidence="0.9993458">
This section describes our event causality extrac-
tion method. Section 3.1 describes how to extract
event causality candidates, and Section 3.2 details
our features. Section 3.3 shows how to rank event
causality candidates.
</bodyText>
<subsectionHeader confidence="0.998559">
3.1 Event Causality Candidate Extraction
</subsectionHeader>
<bodyText confidence="0.9999691875">
We extract the event causality between two events
represented by two phrases from single sentences
that are dependency parsed.4 We obtained sen-
tences from 600 million web pages. Each phrase
in the event causality must consist of a predicate
with an argument position (template, hereafter)
like conduct X and a noun like slash-and-burn
agriculture that completes X. We also require the
predicate of the cause phrase to syntactically de-
pend on the effect phrase in the sentence from
which the event causality was extracted; we guar-
antee this by verifying the dependencies of the
original sentence. In Japanese, since the tempo-
ral order between events is usually determined by
precedence in a sentence, we require the cause
phrase to precede the effect phrase. For context
</bodyText>
<footnote confidence="0.997856333333333">
4We used a Japanese dependency parser called J.DepP
(Yoshinaga and Kitsuregawa, 2009), available at http://
www.tkl.iis.u-tokyo.ac.jp/∼ynaga/jdepp/.
</footnote>
<bodyText confidence="0.999949133333333">
feature extraction, the event causality candidates
are accompanied by the original sentences from
which they were extracted.
Excitation We only keep the event causality
candidates each phrase of which consists of exci-
tation templates, which have been shown to be ef-
fective for causality extraction (Hashimoto et al.,
2012) and other semantic NLP tasks (Oh et al.,
2013; Varga et al., 2013; Kloetzer et al., 2013a).
Excitation is a semantic property of templates that
classifies them into excitatory, inhibitory, and neu-
tral. Excitatory templates such as cause X entail
that the function, effect, purpose or role of their ar-
gument’s referent is activated, enhanced, or man-
ifested, while inhibitory templates such as lower
X entail that it is deactivated or suppressed. Neu-
tral ones like proportional to X belong to neither
of them. We collectively call both excitatory and
inhibitory templates excitation templates. We ac-
quired 43,697 excitation templates by Hashimoto
et al.’s method and the manual annotation of exci-
tation template candidates.5 We applied the exci-
tation filter to all 272,025,401 event causality can-
didates from the web and 132,528,706 remained.
After applying additional filters (see Section A
in the supplementary notes) including those based
on a stop-word list and a causal connective list
to remove unlikely event causality candidates that
are not removed by the above filter, we finally ac-
quired 2,451,254 event causality candidates.
</bodyText>
<subsectionHeader confidence="0.898654">
3.2 Features for Event Causality Classifier
3.2.1 Semantic Relation Features
</subsectionHeader>
<bodyText confidence="0.991357428571429">
We hypothesize that two nouns with some particu-
lar semantic relations are more likely to constitute
event causality. Below we describe the semantic
relations that we believe are likely to constitute
event causality.
CAUSATION is the causal relation between two
entities and is expressed by binary patterns like
A CAUSES B. Deforestation and global warming
might complete the A and B slots. We manually
collected 748 binary patterns for this relation. (See
Section B in the supplementary notes for examples
of our binary patterns.)
MATERIAL is the relation between a material
and a product made of it (e.g. plutonium and
</bodyText>
<footnote confidence="0.9433024">
5Hashimoto et al.’s method constructs a network of tem-
plates based on their co-occurrence in web sentences with a
small number of polarity-assigned seed templates and infers
the polarity of all the templates in the network by a constraint
solver based on the spin model (Takamura et al., 2005).
</footnote>
<page confidence="0.997501">
989
</page>
<bodyText confidence="0.95949870967742">
atomic bomb) and can be expressed by A IS MADE
OF B. Its relation to event causality might seem
unclear, but a material can be seen as a “cause”
of a product. Indeed materials can participate
in event causality with the help of such template
pairs as A is stolen→B is made as in plutonium is
stolen→atomic bomb is made. We manually col-
lected 187 binary patterns for this relation.
NECESSITY’s patterns include A IS NECES-
SARY FOR B, which can be filled with verbal apti-
tude and ability to think. Noun pairs with this rela-
tion can constitute event causality when combined
with template pairs like improve A→cultivate B.
We collected 257 patterns for this relation.
USE is the relation between means (or instru-
ments) and the purpose for using them. A IS USED
FOR B is a pattern of the relation, which can be
filled with e-mailer and exchanges of e-mail mes-
sages. Note that means can be seen as “causing”
or “realizing” the purpose of using the means in
this relation, and actually event causality can be
obtained by incorporating noun pairs of this rela-
tion into template pairs like activate A→conduct
B. 2,178 patterns were collected for this relation.
PREVENTION is the relation expressed by pat-
terns like A PREVENTS B, which can be filled with
toothbrushing and periodontal disease. This rela-
tion is, so to speak, “negative CAUSATION” since
the entity denoted by the noun completing the A
slot makes the entity denoted by the B noun NOT
realized. Such noun pairs mean event causality
by substituting them into template pairs like omit
A→get B. The number of patterns is 490.
The experiments in Section 5.1.1 show that not
only CAUSATION and PREVENTION (“negative
CAUSATION”) but the other relations are also ef-
fective for event causality extraction.
In addition, we invented the EXCITATION rela-
tion that is expressed by binary patterns made of
excitatory and inhibitory templates (Section 3.1).
For instance, we make binary patterns A RISES B
and A LOWERS B from excitatory template rise X
and inhibitory template lower X respectively. The
EXCITATION relation roughly means that A acti-
vates B (excitatory) or suppresses it (inhibitory).
We simply add an additional argument position to
each template in the 43,697 excitation templates to
make binary patterns. We restricted the argument
positions (represented by Japanese postpositions)
of the A slot to either ha (topic marker), ga (nomi-
native), or de (instrumental) and those of the B slot
to either ha, ga, de, wo (accusative), or ni (dative),
SR1: Binary pattern of our semantic relations that co-
occurs with two nouns of an event causality candi-
date in our web corpus.
SR2: Semantic relation types (e.g CAUSATION and EN-
TAILMENT) of the binary pattern of SR1. EXCITA-
TION is divided into six sub types based on the ex-
citation polarity of the binary patterns, the argument
positions, and the existence of causative markers. A
CAUSATION pattern, B BY A, constitutes an indepen-
dent relation called the BY relation.
</bodyText>
<tableCaption confidence="0.991876">
Table 1: Semantic relation features.
</tableCaption>
<bodyText confidence="0.9998225">
and obtained 55,881 patterns.
Moreover, for broader coverage, we acquired
binary patterns that entail or are entailed by one
of the patterns of the above six semantic relations.
Those patterns were acquired from our web cor-
pus by Kloetzer et al. (2013b)’s method, which ac-
quired 185 million entailment pairs with 80% pre-
cision from our web corpus and was used for con-
tradiction acquisition (Kloetzer et al., 2013a). We
acquired 335,837 patterns by this method. They
are class-dependent patterns, which have seman-
tic class restrictions on arguments. The semantic
classes were obtained from our web corpus based
on Kazama and Torisawa (2008). See De Saeger
et al. (2009), De Saeger et al. (2011) and Kloet-
zer et al. (2013a) for more on our patterns. They
collectively constitute the ENTAILMENT relation.
Table 1 shows our semantic relation features. To
use them, we first make a database that records
which noun pairs co-occur with each binary pat-
tern. Then we check a noun pair (the nouns of the
cause and effect phrases) for each event causality
candidate, and give the candidate all the patterns
in the database that co-occur with the noun pair.
</bodyText>
<subsectionHeader confidence="0.733587">
3.2.2 Context Features
</subsectionHeader>
<bodyText confidence="0.99998025">
We believe that contexts exist where event causal-
ity candidates are more likely to appear, as de-
scribed in Section 1. We developed features that
capture the characteristics of likely contexts for
Japanese event causality (See Section C in the sup-
plementary notes). In a nutshell, they represent a
connective (C1 and C2 in Section C), the distance
between the elements of event causality candidate
(C3 and C4), words in context (C5 to C8), the ex-
istence of adnominal modifier (9 to C10), and the
existence of additional arguments of cause and ef-
fect predicates (C13 to C20), among others.
</bodyText>
<subsectionHeader confidence="0.784346">
3.2.3 Association Features
</subsectionHeader>
<bodyText confidence="0.9974785">
These features measure the association strength
between slash-and-burn agriculture and deser-
</bodyText>
<page confidence="0.974622">
990
</page>
<bodyText confidence="0.590272">
AC1: The CEA value, the sum of AC2, AC3, and AC4.
</bodyText>
<listItem confidence="0.821238153846154">
AC2: Do et al.’s Spp. This is the association measure
between predicates, which is the product of AC5,
AC6 and AC7 below. They are calculated from the
132,528,706 event causality candidates in Section
3.1. We omit Do et al.’s Dist, which is a constant
since we set our window size to one.
AC3: Do et al.’s Spa. This is the association measure be-
tween arguments and predicates, which is the sum
of AC8 and AC9. They are calculated from the
132,528,706 event causality candidates.
AC4: Do et al.’s Saa, which is PMI between arguments.
We obtained it in the same way as Filter 5 in the sup-
plementary notes.
</listItem>
<table confidence="0.79932175">
AC5: PMI between predicates.
AC6 / AC7: Do et al.’s max / IDF.
AC8: PMI between a cause noun and an effect predicate.
AC9: PMI between a cause predicate and an effect noun.
</table>
<tableCaption confidence="0.990261">
Table 2: CEA-based association features.
</tableCaption>
<bodyText confidence="0.996079857142857">
tification in conduct slash-and-burn agricul-
ture→exacerbate desertification for instance and
consist of CEA-, Wikipedia-, definition-, and web-
based features. CEA-based features are based
on the Cause Effect Association (CEA) measure
of Do et al. (2011). It consists of association
measures like PMI between arguments (nouns),
between arguments and predicates, and between
predicates (Table 2). Do et al. used it (along
with discourse relations) to extract event causality.
Wikipedia-based features are the co-occurrence
counts and the PMI values between cause and ef-
fect nouns calculated using Wikipedia (as of 2013-
Sep-19). We also checked whether an Wikipedia
article whose title is a cause (effect) noun con-
tains its effect (cause) noun, as detailed in Section
D.1 in the supplementary notes. Definition-based
features, as detailed in Section D.2 in the sup-
plementary notes, resemble the Wikipedia-based
features except that the information source is the
definition sentences automatically acquired from
our 600 million web pages using the method of
Hashimoto et al. (2011). Web-based features
provide association measures between nouns us-
ing various window sizes in the 600 million web
pages. See Section D.3 for detail. Web-based as-
sociation measures were obtained from the same
database as AC4 in Table 2.
</bodyText>
<subsectionHeader confidence="0.911488">
3.2.4 Base Features
</subsectionHeader>
<bodyText confidence="0.983082">
Base features represent the basic properties of
event causality like nouns, templates, and their ex-
citation polarities (See Section E in the supple-
mentary notes). For B3 and B4, 500 semantic
classes were obtained from our web corpus using
the method of Kazama and Torisawa (2008).
</bodyText>
<subsectionHeader confidence="0.982314">
3.3 Event Causality Scoring
</subsectionHeader>
<bodyText confidence="0.946450866666667">
Using the above features, a classifier6 classifies
each event causality candidate into causality and
non-causality. An event causality candidate is
given a causality score CScore, which is the SVM
score (distance from the hyperplane) that is nor-
malized to [0, 1] by the sigmoid function 1
1+e−x .
Each event causality candidate may be given mul-
tiple original sentences, since a phrase pair can ap-
pear in multiple sentences, in which case it is given
more than one SVM score. For such candidates,
we give the largest score and keep only one origi-
nal sentence that corresponds to the largest score.7
Original sentences are also used for scenario gen-
eration, as described below.
</bodyText>
<sectionHeader confidence="0.996716" genericHeader="method">
4 Future Scenario Generation Method
</sectionHeader>
<bodyText confidence="0.999948321428571">
Our future scenario generation method creates
scenarios by chaining event causalities. A naive
approach chains two phrase pairs by exact match-
ing. However, this approach would overlook many
acceptable scenarios as discussed in Section 1. For
example, global warming worsens→sea tempera-
tures are high and sea temperatures rise→vibrio
parahaemolyticus fouls (water) can be chained to
constitute an acceptable scenario, but the joint part
is not the same string. Note that the two phrases
are not simply paraphrases; temperatures may be
rising but remain cold, or they may be decreasing
even though they remain high.
What characterizes two phrases that can be the
joint part of acceptable scenarios? Although we
have no definite answer yet, we name it the causal-
compatibility of two phrases and provide its pre-
liminary characterization based on the excitation
polarity. Remember that excitatory templates like
cause X entail that X’s function or effect is acti-
vated, but inhibitory templates like lower X entail
that it is suppressed (Section 3.1). Two phrases
are causally-compatible if they mention the same
entity (typically described by a noun) that is pred-
icated by the templates of the same excitation po-
larity. Indeed, both X rise and X are high are ex-
citatory and hence sea temperatures are high and
sea temperatures rise are causally-compatible.8
</bodyText>
<footnote confidence="0.998785833333333">
6We used SVMlight with the polynominal kernel (d = 2),
available at http://svmlight.joachims.org.
7Future work will exploit other original sentences, as sug-
gested by an anonymous reviewer.
8Using other knowledge like verb entailment (Hashimoto
et al., 2009) can be helpful too, which is further future work.
</footnote>
<page confidence="0.99377">
991
</page>
<bodyText confidence="0.92959">
Scenarios (scs) generated by chaining causally-
compatible phrase pairs are scored by 5core(sc),
which embodies our assumption that an acceptable
scenario consists of plausible event causality pairs:
</bodyText>
<table confidence="0.958673">
Method Ave. prec. (%)
Proposed 46.27
w/o Context features 45.68
w/o Association features 45.66
w/o Semantic relation features 44.44
Base features only 41.29
5core(sc) = 11 C5core(cs) Table 3: Ablation tests.
cs∈CAUS(sc)
</table>
<bodyText confidence="0.999934842105263">
where CAU5(sc) is a set of event causality
pairs that constitutes sc and cs is a member of
CAU5(sc). C5core(cs), which is cs’s score,
was described in Section 3.3.
Our method optionally applies the following
two filters to scenarios for better precision: An
original sentence filter removes a scenario if two
event causality pairs that are chained in it are ex-
tracted from original sentences between which no
word overlap exists other than words constituting
causality pairs. In this case, the two event causal-
ity pairs tend to be about different topics and con-
stitute an incoherent scenario. A common argu-
ment filter removes a scenario if a joint part con-
sists of two templates that share no argument in
our (argument, template) database, which is com-
piled from the syntactic dependency data between
arguments and templates extracted from our web
corpus. Such a scenario tends to be incoherent too.
</bodyText>
<sectionHeader confidence="0.999837" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.995833">
5.1 Event Causality Extraction
</subsectionHeader>
<bodyText confidence="0.999774909090909">
Next we describe our experiments on event causal-
ity extraction and show (a) that most of our fea-
tures are effective and (b) that our method outper-
forms the baselines based on state-of-the-art meth-
ods (Do et al., 2011; Hashimoto et al., 2012). Our
method achieved 70% precision at 13% recall; we
can extract about 69,700 event causality pairs with
70% precision, as described below.
For the test data, we randomly sampled 23,650
examples of (event causality candidate, origi-
nal sentence) among which 3,645 were positive
from 2,451,254 event causality candidates ex-
tracted from our web corpus (Section 3.1). For
the development data, we identically collected
11,711 examples among which 1,898 were posi-
tive. These datasets were annotated by three anno-
tators (not the authors), who annotated the event
causality candidates without looking at the origi-
nal sentences. The final label was determined by
majority vote. The training data were created
by the annotators through our preliminary experi-
ments and consists of 112,110 among which 9,657
</bodyText>
<table confidence="0.999471">
Semantic relations Ave. prec. (%)
All semantic relations (Proposed) 46.27
CAUSATION 45.86
CAUSATION and PREVENTION 45.78
None (w/o Semantic relation features) 44.44
</table>
<tableCaption confidence="0.999925">
Table 4: Ablation tests on semantic relations.
</tableCaption>
<bodyText confidence="0.99915475">
were positive. The Kappa (Fleiss, 1971) of their
judgments was 0.67 (substantial agreement (Lan-
dis and Koch, 1977)). These three datasets have
no overlap in terms of phrase pairs. About nine
man-months were required to prepare the data.
Our evaluation is based on average precision;9
we believe that it is important to rank the plausible
event causality candidates higher.
</bodyText>
<subsubsectionHeader confidence="0.582554">
5.1.1 Ablation Tests
</subsubsectionHeader>
<bodyText confidence="0.99985604">
We evaluated the features of our method by ab-
lation tests. Table 3 shows the results of remov-
ing the semantic relation, the context, and the as-
sociation features from our method. All the fea-
ture types are effective and contribute to the per-
formance gain that was about 5% higher than the
Base features only. Proposed achieved 70% pre-
cision at 13% recall. We then estimated that, with
the precision rate, we can extract 69,700 event
causality pairs from the 2,451,254 event causality
candidates, among which the estimated number of
positive examples is 377,794.
Next we examined whether the semantic rela-
tions that do not seem directly relevant to causality
like MATERIAL are effective. Table 4 shows that
the performance degraded (46.27 → 45.86) when
we only used the CAUSATION binary patterns and
their entailing and entailed patterns compared to
Proposed. Even when adding the PREVENTION
(“negative CAUSATION”) patterns and their entail-
ing and entailed patterns, the performance was still
slightly worse than Proposed. The performance
was even worse when using no semantic relation
(“None” in Table 4). Consequently we conclude
that not only semantic relations directly relevant
</bodyText>
<footnote confidence="0.998476">
9It is obtained by computing the precision for each point
in the ranked list where we find a positive sample and aver-
aging all the precision figures (Manning and Sch¨utze, 1999).
</footnote>
<page confidence="0.984218">
992
</page>
<table confidence="0.971292833333333">
Method Ave. prec. (%)
w/o Wikipedia-based features 46.52
Proposed 46.27
w/o definition-based features 46.21
w/o Web-based features 46.15
w/o CEA-based features 45.80
</table>
<tableCaption confidence="0.991671">
Table 5: Ablation tests on association features.
</tableCaption>
<table confidence="0.9943728">
Method Ave. prec. (%)
Proposed 46.27
Proposed-CEA 45.80
CEAsup 21.77
CEAuns 16.57
</table>
<tableCaption confidence="0.960336">
Table 6: Average precision of our proposed meth-
ods and baselines using CEA.
</tableCaption>
<bodyText confidence="0.929507875">
to causality like CAUSATION but also those that
seem to lack direct relevance to causality like MA-
TERIAL are somewhat effective.
Finally, Table 5 shows the performance drop
by removing the Wikipedia-, definition-, web-,
and CEA-based features. The CEA-based fea-
tures were the most effective, while the Wikipedia-
based ones slightly degraded the performance.
</bodyText>
<subsubsectionHeader confidence="0.761083">
5.1.2 Comparison to Baseline Methods
</subsubsectionHeader>
<bodyText confidence="0.999946807692308">
We compared our method and two baselines based
on Do et al. (2011): CEAuns is an unsupervised
method that uses CEA to rank event causality can-
didates, and CEAsup is a supervised method us-
ing SVM and the CEA features, whose ranking is
based on the SVM scores. The baselines are not
complete implementations of Do et al.’s method
which uses discourse relations identified based on
Lin et al. (2010) and exploits them with CEA
within an ILP framework. Nonetheless, we believe
that this comparison is informative since CEA can
be seen as the main component; they achieved a
F1 of 41.7% for extracting causal event relations,
but with only CEA they still achieved 38.6%.
Table 6 shows the average precision of the com-
pared methods. Proposed is our proposed method.
Proposed-CEA is Proposed without the CEA-
features and shows their contribution. Proposed
is the best and the CEA features slightly contribute
to the performance, as Proposed-CEA indicates.
We observed that CEAsup and CEAuns performed
poorly and tended to favor event causality candi-
dates whose phrase pairs were highly relevant to
each other but described the contrasts of events
rather than event causality (e.g. build a slow mus-
cle and build a fast muscle) probably because their
</bodyText>
<figure confidence="0.956951">
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.9011035">
Figure 1: Precision-recall curves of proposed
methods and baselines using CEA.
</figureCaption>
<tableCaption confidence="0.669211">
Table 7: Average precision of our proposed
method and baselines using Cs.
</tableCaption>
<bodyText confidence="0.993916290322581">
main components are PMI values. Figure 1 shows
their precision-recall curves.
Next we compared our method with the base-
lines based on Hashimoto et al. (2012). They de-
veloped an automatic excitation template acqui-
sition method that assigns each template an ex-
citation value in range [−1, 1] that is positive if
the template is excitatory and negative if it is in-
hibitory. They ranked event causality candidates
by Cs(p1, p2) = |s1 |x |s2|, where p1 and p2 are
the two phrases of event causality candidates, and
|s1 |and |s2 |are the absolute excitation values of
p1’s and p2’s templates. The baselines are as fol-
lows: Csuns is an unsupervised method that uses
Cs for ranking, and Cssup is a supervised method
using SVM with Cs as the only feature that uses
SVM scores for ranking. Note that some event
causality candidates were not given excitation val-
ues for their templates, since some templates were
acquired by manual annotation without Hashimoto
et al.’s method. To favor the baselines for fairness,
the event causality candidates of the development
and test data were restricted to those with excita-
tion values. Since Cssup performed slightly better
when using all of the training data in our prelimi-
nary experiments, we used all of it.
Table 7 shows the average precision of the com-
pared methods. Proposed is our method. Its av-
erage precision is different from that in Table 6
due to the difference in test data described above.
Csuns and Cssup did not perform well. Many
</bodyText>
<figure confidence="0.995437684210526">
0.8
0.6
0.4
0.2
0
1
Proposed—CEA
CEA�sup
CEA�uns
Proposed
Method
Proposed
Csuns
Cssup
Ave. prec. (%)
49.64
30.38
27.49
Precision
</figure>
<page confidence="0.907873">
993
</page>
<table confidence="0.8440216">
Two-step Three-step
Exact 1,000 (44.10) 1,000 (23.50)
Proposed 2,000 (32.25) 2,000 (12.55)
Proposed+Orig 995 (36.28) 602 (17.28)
Proposed+Orig+Comm 708 (38.70) 339 (17.99)
</table>
<tableCaption confidence="0.4905365">
Table 8: Number of scenario samples and their
precision (%) in parentheses.
</tableCaption>
<figure confidence="0.97042">
Precision
0.8
0.6
0.4
0.2
0
1
Proposed
Cs-uns
Cs-sup
0 0.2 0.4 0.6 0.8 1
Recall
</figure>
<figureCaption confidence="0.9778225">
Figure 2: Precision-recall curves of proposed
methods and baselines using Cs.
</figureCaption>
<bodyText confidence="0.999540333333333">
phrase pairs described two events that often hap-
pen in parallel but are not event causality (e.g. re-
duce the intake of energy and increase the energy
consumption) in the highly ranked event causality
candidates of Csuns and Cssup. Figure 2 shows
their precision-recall curves.
Hashimoto et al. (2012) extracted 500,000 event
causalities with about 70% precision. However, as
described in Section 1, our event causality crite-
ria are different; since they regarded phrase pairs
that were not self-contained as event causality
(their annotators checked the original sentences of
phrase pairs to see if they were event causality),
their judgments tended to be more lenient than
ours, which explains the performance difference.
In preliminary experiments, since our proposed
method’s performance degraded when Cs was in-
corporated, we did not use it in our method.
</bodyText>
<subsectionHeader confidence="0.999411">
5.2 Future Scenario Generation
</subsectionHeader>
<bodyText confidence="0.999962704918033">
To show that our future scenario generation meth-
ods can generate many acceptable scenarios with
reasonable precision, we experimentally com-
pared four methods: Proposed, our scenario
generation method without the two filters, Pro-
posed+Orig, our method with the original sen-
tence filter, Proposed+Orig+Comm, our method
with the original sentence and common argument
filters, and Exact, a method that chains event
causality by exact matching.
Beginning events As the beginning event of a
scenario, we extracted nouns that describe social
problems (social problem nouns, e.g. deforesta-
tion) from Wikipedia to focus our evaluation on
the ability to generate scenarios about them, which
is a realistic use-case of scenario generation. We
extracted 557 social problem nouns and used the
cause phrases of the event causality candidates that
consisted of one of the social problem nouns as the
scenario’s beginning event.
Event causality We applied our event causality
extraction method to 2,451,254 candidates (Sec-
tion 3.1) and culled the top 1,200,000 phrase pairs
from them (See Section F in the supplementary
notes for examples). Some phrase pairs have the
same noun pairs and the same template polar-
ity pairs (e.g. omit toothbrushing→get a cavity
and neglect toothbrushing→have a cavity, where
omit X and neglect X are inhibitory and get X and
have X are excitatory). We removed such phrase
pairs except those with the highest CScore, and
960,561 phrase pairs remained, from which we
generated two- or three-step scenarios that con-
sisted of two or three phrase pairs.
Evaluation samples The numbers of two- and
three-step scenarios generated by Proposed were
217,836 and 5,288,352, while those of Exact were
22,910 and 72,746. We sampled 2,000 from Pro-
posed’s two- and three-step scenarios and 1,000
from those of Exact. We applied the filters to the
sampled scenarios of Proposed, and the results
were regarded as the sample scenarios of Pro-
posed+Orig and Proposed+Orig+Comm. Table
8 shows the number and precision of the samples.
Note that, for the diversity of the sampled scenar-
ios, our sampling proceeded as follows: (i) Ran-
domly sample a beginning event phrase from the
generated scenarios. (ii) Randomly sample an ef-
fect phrase for the beginning event phrase from the
scenarios. (iii) Regarding the effect phrase as a
cause phrase, randomly sample an effect phrase
for it, and repeat (iii) up to the specified number
of steps (2 or 3). The samples were annotated by
three annotators (not the authors), who were in-
structed to regard a sample as acceptable if each
event causality that constitutes it is plausible and
the sample as a whole constitutes a single coherent
story. Final judgment was made by majority vote.
Fleiss’ kappa of their judgments was 0.53 (moder-
ate agreement), which is lower than the kappa for
the causality judgment. This is probably because
</bodyText>
<page confidence="0.994658">
994
</page>
<table confidence="0.9995348">
Two-step Three-step
Exact 2,085 1,237
Proposed 5,773 0
Proposed+Orig 4,107 0
Proposed+Orig+Comm 3,293 21,153
</table>
<tableCaption confidence="0.946708">
Table 9: Estimated number of acceptable scenar-
ios with a 70% precision rate.
</tableCaption>
<figure confidence="0.8957355">
0 10000 20000 30000 40000 50000 60000 70000
Estimated number of acceptable scenarios
</figure>
<figureCaption confidence="0.999768">
Figure 3: Precision-scenario curves (2-step).
</figureCaption>
<bodyText confidence="0.999893666666667">
scenario judgment requires careful consideration
about various possible futures for which individ-
ual annotators tend to draw different conclusions.
Result 1 Table 9 shows the estimated number
of acceptable scenarios generated with 70% pre-
cision. The estimated number is calculated as the
product of the recall at 70% precision and the
number of acceptable scenarios in all the gener-
ated scenarios, which is estimated by the anno-
tated samples. Figures 3 and 4 show the precision-
scenario curves for the two- and three-step sce-
narios, which illustrate how many acceptable sce-
narios can be generated with what precision. The
curve is drawn in the same way as the precision-
recall curve except that the X-axis indicates the
estimated number of acceptable scenarios. At
70% precision, all of the proposed methods out-
performed Exact in the two-step setting, and Pro-
posed+Orig+Comm outperformed Exact in the
three-step setting.
Result 2 To evaluate the top-ranked scenarios
of Proposed+Orig+Comm in the three-step set-
ting with more samples, the annotators labeled 500
samples from the top 50,000 of its output. 341
(68.20%) were acceptable, and the estimated num-
ber of acceptable scenarios at a precision rate of
70% and 80% are 26,700 and 5,200 (See Section H
in the supplementary notes). The “terrorist oper-
ations” scenario and the “slash-and-burn agricul-
ture” scenario in Section 1 were ranked 16,386th
</bodyText>
<figure confidence="0.936956">
0 100000 200000 300000 400000 500000 600000 700000
Estimated number of acceptable scenarios
</figure>
<figureCaption confidence="0.999888">
Figure 4: Precision-scenario curves (3-step).
</figureCaption>
<bodyText confidence="0.998182291666667">
and 21,968th. Next we examined how many of
the top 50,000 scenarios were acceptable and non-
trivial, i.e., found in no page in our input web cor-
pus, using the 341 acceptable samples. A scenario
was regarded as non-trivial if its nouns co-occur in
no page of the corpus. 22 among the 341 samples
were non-trivial. Accordingly, we estimate that
we can generate 2,200 (50,000×22
500 ) acceptable and
non-trivial scenarios from the top 50,000. (See
Section G in the supplementary notes for exam-
ples of the generated scenarios.)
Discussion Scenario deforestation contin-
ues→global warming worsens→sea temperatures
rise→vibrio parahaemolyticus fouls (water)
was generated by Proposed+Orig+Comm. It
is written in no page in our input web corpus,
which was crawled in 2007.10 But we did find
a paper Baker-Austin et al. (2013) that observed
the emerging vibrio risk in the Baltic sea due to
global warming. In a sense, we “predicted” an
event observed in 2013 from documents written
in 2007, although the scenario was ranked as low
as 240,738th.
</bodyText>
<sectionHeader confidence="0.999249" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.964321153846154">
We proposed a supervised method for event
causality extraction that exploits semantic rela-
tion, context, and association features. We also
proposed methods for our new task, future sce-
nario generation. The methods chain event causal-
ity by causal-compatibility. We generated non-
trivial scenarios with reasonable precision, and
“predicted” future events from web documents.
Increasing their rank is future work.
10The corpus has pages where global warming, sea tem-
peratures, and vibrio parahaemolyticus happen to co-occur.
But they are either diaries where the three words appear sep-
arately in different topics or lists of arbitrary words.
</bodyText>
<figure confidence="0.998747636363636">
Precision
0.8
0.6
0.4
0.2
0
1
Exact
Proposed
Proposed+Orig
Proposed+Orig+Comm
0.8
0.6
0.4
0.2
0
1
Exact
Proposed+Orig+Comm
Proposed
Proposed+Orig
Precision
</figure>
<page confidence="0.994237">
995
</page>
<sectionHeader confidence="0.988452" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999344277777778">
Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008.
Two-phrased event relation acquisition: Coupling
the relation-oriented and argument-oriented ap-
proaches. In Proceedings of the 22nd International
Conference on Computational Linguistics (COLIIG
2008), pages 1–8.
Susumu Akamine, Daisuke Kawahara, Yoshikiyo
Kato, Tetsuji Nakagawa, Yutaka I. Leon-Suematsu,
Takuya Kawada, Kentaro Inui, Sadao Kurohashi,
and Yutaka Kidawara. 2010. Organizing informa-
tion on the web to support user judgments on in-
formation credibility. In Proceedings of 2010 4th
International Universal Communication Symposium
Proceedings (IUCS 2010), pages 122–129.
Craig Baker-Austin, Joaquin A. Trinanes, Nick G. H.
Taylor, Rachel Hartnell, Anja Siitonen, and Jaime
Martinez-Urtaza. 2013. Emerging vibrio risk at
high latitudes in response to ocean warming. Iature
Climate Change, 3:73–77.
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings of the 48th Annual Meeting of the Asso-
ciation of Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 789–
797.
Nathanael Chambers and Dan Jurafsky. 2009. Unsu-
pervised learning of narrative schemas and their par-
ticipants. In Proceedings of the 47th Annual Meet-
ing of the ACL and the 4th IJCILP of the AFILP
(ACL-IJCILP 2009), pages 602–610.
Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama,
Kow Kuroda, and Masaki Murata. 2009. Large
scale relation acquisition using class dependent pat-
terns. In Proceedings of the IEEE International
Conference on Data Mining (ICDM 2009), pages
764–769.
Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida,
Jun’ichi Kazama, Chikara Hashimoto, Ichiro Ya-
mada, Jong-Hoon Oh, Istv´an Varga, and Yulan Yan.
2011. Relation acquisition using word classes and
partial patterns. In Proceedings of the Conference
on Empirical Methods in Iatural Language Pro-
cessing (EMILP 2011), pages 825–835.
Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011.
Minimally supervised event causality identification.
In Proceedings of the 2011 Conference on Empirical
Methods in Iatural Language Processing (EMILP
2011), pages 294–303.
Joseph L. Fleiss. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76(5):378–382.
Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda,
Masaki Murata, and Jun’ichi Kazama. 2009. Large-
scale verb entailment acquisition from the web. In
Proceedings of EMILP 2009: Conference on Em-
pirical Methods in Iatural Language Processing,
pages 1172–1181.
Chikara Hashimoto, Kentaro Torisawa, Stijn
De Saeger, Jun’ichi Kazama, and Sadao Kuro-
hashi. 2011. Extracting paraphrases from definition
sentences on the web. In Proceedings of the 49th
Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies,
pages 1087–1097.
Chikara Hashimoto, Kentaro Torisawa, Stijn De
Saeger, Jong-Hoon Oh, and Jun’ichi Kazama. 2012.
Excitatory or inhibitory: A new semantic orienta-
tion extracts contradiction and causality from the
web. In Proceedings ofEMILP-CoILL 2012: Con-
ference on Empirical Methods in Iatural Language
Processing and Iatural Language Learning, pages
619–630.
Jun’ichi Kazama and Kentaro Torisawa. 2008. Induc-
ing gazetteers for named entity recognition by large-
scale clustering of dependency relations. In Pro-
ceedings of the 46th Annual Meeting of the Associ-
ation for Computational Linguistics: Human Lan-
guage Technologies (ACL-08: HLT), pages 407–
415.
Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa,
Chikara Hashimoto, Jong-Hoon Oh, and Kiyonori
Ohtake. 2013a. Two-stage method for large-scale
acquisition of contradiction pattern pairs using en-
tailment. In Proceedings of the Conference on Em-
pirical Methods in Iatural Language Processing
(EMILP 2013), pages 693–703.
Julien Kloetzer, Kentaro Torisawa, Stijn De Saeger,
Motoki Sano, Chikara Hashimoto, and Jun Gotoh.
2013b. Large-scale acquisition of entailment pattern
pairs. In Information Processing Society of Japan
(IPSJ) Kansai-Branch Convention 2013.
J. Richard Landis and Gary G. Koch. 1977. The mea-
surement of observer agreement for categorical data.
Biometrics, 33(1):159–174.
Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A
pdtb-styled end-to-end discourse parser. Technical
report, School of Computing, National University of
Singapore.
Chris Manning and Hinrich Sch¨utze. 1999. Foun-
dations of Statistical Iatural Language Processing.
MIT Press.
Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,
Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake.
2013. Why-question answering using intra- and
inter-sentential causal relations. In Proceedings of
the 51st Annual Meeting of the Association for Com-
putational Linguistics (ACL 2013), pages 1733–
1743.
</reference>
<page confidence="0.985345">
996
</page>
<reference confidence="0.998725573529412">
Kira Radinsky and Eric Horvitz. 2013. Mining the
web to predict future events. In Proceedings ofSixth
ACM International Conference on Web Search and
Data Mining (WSDM2013), pages 255–264.
Kira Radinsky, Sagie Davidovich, and Shaul
Markovitch. 2012. Learning causality for news
events prediction. In Proceedings of International
World Wide Web Conference 2012 (WWW 2012),
pages 909–918.
Mehwish Riaz and Roxana Girju. 2010. Another look
at causality: Discovering scenario-specific contin-
gency relationships with no supervision. In 2010
IEEE Fourth International Conference on Semantic
Computing, pages 361–368.
Peter Schwartz. 1991. The Art of the Long View. Dou-
bleday.
Hiroya Takamura, Takashi Inui, and Manabu Okumura.
2005. Extracting semantic orientation of words us-
ing spin model. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2005), pages 133–140.
Shohei Tanaka, Naoaki Okazaki, and Mitsuru Ishizuka.
2012. Acquiring and generalizing causal inference
rules from deverbal noun constructions. In Proceed-
ings of 24th International Conference on Compu-
tational Linguistics (COLING 2012), pages 1209–
1218.
Masahiro Tanaka, Stijn De Saeger, Kiyonori Ohtake,
Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii,
and Kentaro Torisawa. 2013. WISDOM2013: A
large-scale web information analysis system. In
Companion Volume of the Proceedings of the 6th In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2013) (Demo Track), pages
45–48.
Kentaro Torisawa, Stijn de Saeger, Jun’ichi Kazama,
Asuka Sumida, Daisuke Noguchi, Yasunari Kak-
izawa, Masaki Murata, Kow Kuroda, and Ichiro Ya-
mada. 2010. Organizing the web’s information ex-
plosion to discover unknown unknowns. New Gen-
eration Computing (Special Issue on Information
Explosion), 28(3):217–236.
Kentaro Torisawa. 2006. Acquiring inference rules
with temporal constraints by using japanese coordi-
nated sentences and noun-verb co-occurrences. In
Proceedings of the Human Language Technology
Conference of the North American Chapter of the
ACL (HLT-NAACL2006), pages 57–64.
Masaaki Tsuchida, Kentaro Torisawa, Stijn De
Saeger, Jong Hoon Oh, Jun’ichi Kazama, Chikara
Hashimoto, and Hayato Ohwada. 2011. Toward
finding semantic relations not written in a single sen-
tence: An inference method using auto-discovered
rules. In Proceedings of the 5th International Joint
Conference on Natural Language Processing (IJC-
NLP 2011), pages 902–910.
Istv´an Varga, Motoki Sano, Kentaro Torisawa, Chikara
Hashimoto, Kiyonori Ohtake, Takao Kawai, Jong-
Hoon Oh, and Stijn De Saeger. 2013. Aid is out
there: Looking for help from tweets during a large
scale disaster. In Proceedings of the 51st Annual
Meeting of the Association for Computational Lin-
guistics (ACL 2013), pages 1619–1629.
Naoki Yoshinaga and Masaru Kitsuregawa. 2009.
Polynomial to linear: Efficient classification with
conjunctive features. In Proceedings of the 2009
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP 2009), pages 542–1551.
</reference>
<page confidence="0.997251">
997
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.208943">
<title confidence="0.998982">Toward Future Scenario Generation: Extracting Event Exploiting Semantic Relation, Context, and Association Features</title>
<note confidence="0.647683">Kentaro † ‡ § II Institute of Information and Communications Technology, Kyoto, 619-0289, Knowledge Discovery Research Laboratories, Nara, 630-0101, 11rovellia,</note>
<abstract confidence="0.999574608695652">We propose a supervised method of event causalities like the web using semantic relation (between nouns), context, and association features. Experiments show that our method outperforms baselines that are based on state-of-the-art methods. We also propose methods of scenarios Asian dust (from gets Experiments show that we can generate 50,000 scenarios with 68% precision. We also a scenario conwarming parahaemolytifouls which is written in no document in our input web corpus crawled in 2007. But the vibrio risk due to global warming was observed in Baker-Austin et al. (2013). Thus, we “predicted” the future event sequence in a sense.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Shuya Abe</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Two-phrased event relation acquisition: Coupling the relation-oriented and argument-oriented approaches.</title>
<date>2008</date>
<booktitle>In Proceedings of the 22nd International Conference on Computational Linguistics (COLIIG</booktitle>
<pages>1--8</pages>
<contexts>
<context position="8486" citStr="Abe et al., 2008" startWordPosition="1286" endWordPosition="1289">ties and injuries—*cause economic losses and the above “slash-and-burn agriculture” scenario (Section 5.2). Neither is written in any document in our input corpus. In this paper, our target language is Japanese. However, we believe that our ideas and methods are applicable to many languages. Examples are translated into English for ease of explanation. Supplementary notes of this paper are available at http://khn.nict.go.jp/analysis/ member/ch/acl2014-sup.pdf. 2 Related Work For event causality extraction, clues used by previous methods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2A bacterium in the sea causing food-poisoning. 3Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality in</context>
</contexts>
<marker>Abe, Inui, Matsumoto, 2008</marker>
<rawString>Shuya Abe, Kentaro Inui, and Yuji Matsumoto. 2008. Two-phrased event relation acquisition: Coupling the relation-oriented and argument-oriented approaches. In Proceedings of the 22nd International Conference on Computational Linguistics (COLIIG 2008), pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Akamine</author>
<author>Daisuke Kawahara</author>
<author>Yoshikiyo Kato</author>
<author>Tetsuji Nakagawa</author>
<author>Yutaka I Leon-Suematsu</author>
</authors>
<title>Takuya Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka Kidawara.</title>
<date>2010</date>
<booktitle>In Proceedings of 2010 4th International Universal Communication Symposium Proceedings (IUCS 2010),</booktitle>
<pages>122--129</pages>
<contexts>
<context position="5366" citStr="Akamine et al., 2010" startWordPosition="814" endWordPosition="817">s halted. Useful context information includes the mood of the sentences (e.g., the uncertainty mood expressed by uncertain above), which is represented by lexical features (Section 3.2.2). The last assumption embodied in our association features is that each word of the cause phrase must have a strong association (i.e., PMI, for example) with that of the effect phrase as slash-andburn agriculture and desertification in the above example, as in Do et al. (2011). Our method exploits these features on top of our base features such as nouns and predicates. Experiments using 600 million web pages (Akamine et al., 2010) show that our method outperforms baselines based on state-of-the-art methods (Do et al., 2011; Hashimoto et al., 2012) by more than 19% of average precision. We require that event causality be selfcontained, i.e., intelligible as causality without the sentences from which it was extracted. For example, omit toothbrushing—*get a cavity is selfcontained, but omit toothbrushing—*get a girlfriend is not since this is not intelligible without a context: He omitted toothbrushing every day and got a girlfriend who was a dental assistant of dental clinic he went to for his cavity. This is important s</context>
</contexts>
<marker>Akamine, Kawahara, Kato, Nakagawa, Leon-Suematsu, 2010</marker>
<rawString>Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato, Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, Takuya Kawada, Kentaro Inui, Sadao Kurohashi, and Yutaka Kidawara. 2010. Organizing information on the web to support user judgments on information credibility. In Proceedings of 2010 4th International Universal Communication Symposium Proceedings (IUCS 2010), pages 122–129.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Baker-Austin</author>
<author>Joaquin A Trinanes</author>
<author>Nick G H Taylor</author>
<author>Rachel Hartnell</author>
<author>Anja Siitonen</author>
<author>Jaime Martinez-Urtaza</author>
</authors>
<title>Emerging vibrio risk at high latitudes in response to ocean warming. Iature Climate Change,</title>
<date>2013</date>
<pages>3--73</pages>
<contexts>
<context position="1363" citStr="Baker-Austin et al. (2013)" startWordPosition="176" endWordPosition="179">our method outperforms baselines that are based on state-of-the-art methods. We also propose methods of generating future scenarios like conduct slash-and-burn agriculture—*exacerbate desertification—*increase Asian dust (from China)—*asthma gets worse. Experiments show that we can generate 50,000 scenarios with 68% precision. We also generated a scenario deforestation continues—*global warming worsens—*sea temperatures rise—*vibrio parahaemolyticus fouls (water), which is written in no document in our input web corpus crawled in 2007. But the vibrio risk due to global warming was observed in Baker-Austin et al. (2013). Thus, we “predicted” the future event sequence in a sense. 1 Introduction The world can be seen as a network of causality where people, organizations, and other kinds of entities causally depend on each other. This network is so huge and complex that it is almost impossible for humans to exhaustively predict the consequences of a given event. Indeed, after the Great East Japan Earthquake in 2011, few expected that it would lead to an enormous trade deficit in Japan due to a sharp increase in energy imports. For effective decision making that carefully considers any form of future risks and c</context>
<context position="7612" citStr="Baker-Austin et al. (2013)" startWordPosition="1160" endWordPosition="1163">ection 3.1 describes it). For example, our method can identify the compatibility between sea temperatures are high and sea temperatures rise to chain global warming worsens—*sea temperatures are high and sea temperatures rise—*vibrio parahaemolyticus2 fouls (water). Accordingly, we generated a scenario deforestation continues—*global warming worsens—*sea temperatures rise—*vibrio parahaemolyticus fouls (water), which is written in no document in our input web corpus that was crawled in 2007, but the vibrio risk due to global warming has actually been observed in the Baltic sea and reported in Baker-Austin et al. (2013). In a sense, we “predicted” the event sequence reported in 2013 by documents written in 2007. Our experiments also show that we generated 50,000 scenarios with 68% precision, which include conduct terrorist operations—*terrorist bombing occurs—*cause fatalities and injuries—*cause economic losses and the above “slash-and-burn agriculture” scenario (Section 5.2). Neither is written in any document in our input corpus. In this paper, our target language is Japanese. However, we believe that our ideas and methods are applicable to many languages. Examples are translated into English for ease of </context>
<context position="38679" citStr="Baker-Austin et al. (2013)" startWordPosition="6134" endWordPosition="6137">io was regarded as non-trivial if its nouns co-occur in no page of the corpus. 22 among the 341 samples were non-trivial. Accordingly, we estimate that we can generate 2,200 (50,000×22 500 ) acceptable and non-trivial scenarios from the top 50,000. (See Section G in the supplementary notes for examples of the generated scenarios.) Discussion Scenario deforestation continues→global warming worsens→sea temperatures rise→vibrio parahaemolyticus fouls (water) was generated by Proposed+Orig+Comm. It is written in no page in our input web corpus, which was crawled in 2007.10 But we did find a paper Baker-Austin et al. (2013) that observed the emerging vibrio risk in the Baltic sea due to global warming. In a sense, we “predicted” an event observed in 2013 from documents written in 2007, although the scenario was ranked as low as 240,738th. 6 Conclusion We proposed a supervised method for event causality extraction that exploits semantic relation, context, and association features. We also proposed methods for our new task, future scenario generation. The methods chain event causality by causal-compatibility. We generated nontrivial scenarios with reasonable precision, and “predicted” future events from web docume</context>
</contexts>
<marker>Baker-Austin, Trinanes, Taylor, Hartnell, Siitonen, Martinez-Urtaza, 2013</marker>
<rawString>Craig Baker-Austin, Joaquin A. Trinanes, Nick G. H. Taylor, Rachel Hartnell, Anja Siitonen, and Jaime Martinez-Urtaza. 2013. Emerging vibrio risk at high latitudes in response to ocean warming. Iature Climate Change, 3:73–77.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies (ACL-08: HLT),</booktitle>
<pages>789--797</pages>
<contexts>
<context position="9180" citStr="Chambers and Jurafsky, 2008" startWordPosition="1392" endWordPosition="1395">iations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2A bacterium in the sea causing food-poisoning. 3Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 988 Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), which we ignore since we target event causality about two distinct entities. To the best of our knowledge, future scenario generation is a new task, although previous works have addressed similar tasks (Radinsky et al., 2012; Radinsky and Horvitz, 2013). Neither involves chaining and restricts themselves to only one event causality step. Besides, the events they predict must be those for which similar events have previously been observed, and their method only applies to news domain. Some of the scenarios we generated are written on no page in our input web corp</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of the 48th Annual Meeting of the Association of Computational Linguistics: Human Language Technologies (ACL-08: HLT), pages 789– 797.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCILP of the AFILP (ACL-IJCILP</booktitle>
<pages>602--610</pages>
<contexts>
<context position="9210" citStr="Chambers and Jurafsky, 2009" startWordPosition="1396" endWordPosition="1399">, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2A bacterium in the sea causing food-poisoning. 3Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 988 Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), which we ignore since we target event causality about two distinct entities. To the best of our knowledge, future scenario generation is a new task, although previous works have addressed similar tasks (Radinsky et al., 2012; Radinsky and Horvitz, 2013). Neither involves chaining and restricts themselves to only one event causality step. Besides, the events they predict must be those for which similar events have previously been observed, and their method only applies to news domain. Some of the scenarios we generated are written on no page in our input web corpus. Similarly, Tsuchida et al.</context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCILP of the AFILP (ACL-IJCILP 2009), pages 602–610.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn De Saeger</author>
<author>Kentaro Torisawa</author>
<author>Jun’ichi Kazama</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
</authors>
<title>Large scale relation acquisition using class dependent patterns.</title>
<date>2009</date>
<booktitle>In Proceedings of the IEEE International Conference on Data Mining (ICDM</booktitle>
<pages>764--769</pages>
<marker>De Saeger, Torisawa, Kazama, Kuroda, Murata, 2009</marker>
<rawString>Stijn De Saeger, Kentaro Torisawa, Jun’ichi Kazama, Kow Kuroda, and Masaki Murata. 2009. Large scale relation acquisition using class dependent patterns. In Proceedings of the IEEE International Conference on Data Mining (ICDM 2009), pages 764–769.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stijn De Saeger</author>
<author>Kentaro Torisawa</author>
<author>Masaaki Tsuchida</author>
<author>Jun’ichi Kazama</author>
<author>Chikara Hashimoto</author>
<author>Ichiro Yamada</author>
<author>Jong-Hoon Oh</author>
<author>Istv´an Varga</author>
<author>Yulan Yan</author>
</authors>
<title>Relation acquisition using word classes and partial patterns.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Iatural Language Processing (EMILP</booktitle>
<pages>825--835</pages>
<marker>De Saeger, Torisawa, Tsuchida, Kazama, Hashimoto, Yamada, Oh, Varga, Yan, 2011</marker>
<rawString>Stijn De Saeger, Kentaro Torisawa, Masaaki Tsuchida, Jun’ichi Kazama, Chikara Hashimoto, Ichiro Yamada, Jong-Hoon Oh, Istv´an Varga, and Yulan Yan. 2011. Relation acquisition using word classes and partial patterns. In Proceedings of the Conference on Empirical Methods in Iatural Language Processing (EMILP 2011), pages 825–835.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Quang Xuan Do</author>
<author>Yee Seng Chan</author>
<author>Dan Roth</author>
</authors>
<title>Minimally supervised event causality identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing (EMILP</booktitle>
<pages>294--303</pages>
<contexts>
<context position="5209" citStr="Do et al. (2011)" startWordPosition="787" endWordPosition="790">imatic anomalies were observed, while an unlikely context would be: It remains uncertain whether if the recession is bottomed the declining birth rate is halted. Useful context information includes the mood of the sentences (e.g., the uncertainty mood expressed by uncertain above), which is represented by lexical features (Section 3.2.2). The last assumption embodied in our association features is that each word of the cause phrase must have a strong association (i.e., PMI, for example) with that of the effect phrase as slash-andburn agriculture and desertification in the above example, as in Do et al. (2011). Our method exploits these features on top of our base features such as nouns and predicates. Experiments using 600 million web pages (Akamine et al., 2010) show that our method outperforms baselines based on state-of-the-art methods (Do et al., 2011; Hashimoto et al., 2012) by more than 19% of average precision. We require that event causality be selfcontained, i.e., intelligible as causality without the sentences from which it was extracted. For example, omit toothbrushing—*get a cavity is selfcontained, but omit toothbrushing—*get a girlfriend is not since this is not intelligible without </context>
<context position="8628" citStr="Do et al., 2011" startWordPosition="1310" endWordPosition="1313"> in our input corpus. In this paper, our target language is Japanese. However, we believe that our ideas and methods are applicable to many languages. Examples are translated into English for ease of explanation. Supplementary notes of this paper are available at http://khn.nict.go.jp/analysis/ member/ch/acl2014-sup.pdf. 2 Related Work For event causality extraction, clues used by previous methods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2A bacterium in the sea causing food-poisoning. 3Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 988 Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), which we ignore </context>
<context position="19906" citStr="Do et al. (2011)" startWordPosition="3141" endWordPosition="3144">nt causality candidates. AC4: Do et al.’s Saa, which is PMI between arguments. We obtained it in the same way as Filter 5 in the supplementary notes. AC5: PMI between predicates. AC6 / AC7: Do et al.’s max / IDF. AC8: PMI between a cause noun and an effect predicate. AC9: PMI between a cause predicate and an effect noun. Table 2: CEA-based association features. tification in conduct slash-and-burn agriculture→exacerbate desertification for instance and consist of CEA-, Wikipedia-, definition-, and webbased features. CEA-based features are based on the Cause Effect Association (CEA) measure of Do et al. (2011). It consists of association measures like PMI between arguments (nouns), between arguments and predicates, and between predicates (Table 2). Do et al. used it (along with discourse relations) to extract event causality. Wikipedia-based features are the co-occurrence counts and the PMI values between cause and effect nouns calculated using Wikipedia (as of 2013- Sep-19). We also checked whether an Wikipedia article whose title is a cause (effect) noun contains its effect (cause) noun, as detailed in Section D.1 in the supplementary notes. Definition-based features, as detailed in Section D.2 i</context>
<context position="25252" citStr="Do et al., 2011" startWordPosition="3990" endWordPosition="3993"> topics and constitute an incoherent scenario. A common argument filter removes a scenario if a joint part consists of two templates that share no argument in our (argument, template) database, which is compiled from the syntactic dependency data between arguments and templates extracted from our web corpus. Such a scenario tends to be incoherent too. 5 Experiments 5.1 Event Causality Extraction Next we describe our experiments on event causality extraction and show (a) that most of our features are effective and (b) that our method outperforms the baselines based on state-of-the-art methods (Do et al., 2011; Hashimoto et al., 2012). Our method achieved 70% precision at 13% recall; we can extract about 69,700 event causality pairs with 70% precision, as described below. For the test data, we randomly sampled 23,650 examples of (event causality candidate, original sentence) among which 3,645 were positive from 2,451,254 event causality candidates extracted from our web corpus (Section 3.1). For the development data, we identically collected 11,711 examples among which 1,898 were positive. These datasets were annotated by three annotators (not the authors), who annotated the event causality candida</context>
<context position="28878" citStr="Do et al. (2011)" startWordPosition="4555" endWordPosition="4558">thod Ave. prec. (%) Proposed 46.27 Proposed-CEA 45.80 CEAsup 21.77 CEAuns 16.57 Table 6: Average precision of our proposed methods and baselines using CEA. to causality like CAUSATION but also those that seem to lack direct relevance to causality like MATERIAL are somewhat effective. Finally, Table 5 shows the performance drop by removing the Wikipedia-, definition-, web-, and CEA-based features. The CEA-based features were the most effective, while the Wikipediabased ones slightly degraded the performance. 5.1.2 Comparison to Baseline Methods We compared our method and two baselines based on Do et al. (2011): CEAuns is an unsupervised method that uses CEA to rank event causality candidates, and CEAsup is a supervised method using SVM and the CEA features, whose ranking is based on the SVM scores. The baselines are not complete implementations of Do et al.’s method which uses discourse relations identified based on Lin et al. (2010) and exploits them with CEA within an ILP framework. Nonetheless, we believe that this comparison is informative since CEA can be seen as the main component; they achieved a F1 of 41.7% for extracting causal event relations, but with only CEA they still achieved 38.6%. </context>
</contexts>
<marker>Do, Chan, Roth, 2011</marker>
<rawString>Quang Xuan Do, Yee Seng Chan, and Dan Roth. 2011. Minimally supervised event causality identification. In Proceedings of the 2011 Conference on Empirical Methods in Iatural Language Processing (EMILP 2011), pages 294–303.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<issue>5</issue>
<contexts>
<context position="26326" citStr="Fleiss, 1971" startWordPosition="4154" endWordPosition="4155">g which 1,898 were positive. These datasets were annotated by three annotators (not the authors), who annotated the event causality candidates without looking at the original sentences. The final label was determined by majority vote. The training data were created by the annotators through our preliminary experiments and consists of 112,110 among which 9,657 Semantic relations Ave. prec. (%) All semantic relations (Proposed) 46.27 CAUSATION 45.86 CAUSATION and PREVENTION 45.78 None (w/o Semantic relation features) 44.44 Table 4: Ablation tests on semantic relations. were positive. The Kappa (Fleiss, 1971) of their judgments was 0.67 (substantial agreement (Landis and Koch, 1977)). These three datasets have no overlap in terms of phrase pairs. About nine man-months were required to prepare the data. Our evaluation is based on average precision;9 we believe that it is important to rank the plausible event causality candidates higher. 5.1.1 Ablation Tests We evaluated the features of our method by ablation tests. Table 3 shows the results of removing the semantic relation, the context, and the association features from our method. All the feature types are effective and contribute to the performa</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Joseph L. Fleiss. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76(5):378–382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Kow Kuroda</author>
<author>Masaki Murata</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Largescale verb entailment acquisition from the web.</title>
<date>2009</date>
<booktitle>In Proceedings of EMILP 2009: Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<pages>1172--1181</pages>
<contexts>
<context position="23618" citStr="Hashimoto et al., 2009" startWordPosition="3726" endWordPosition="3729">s like lower X entail that it is suppressed (Section 3.1). Two phrases are causally-compatible if they mention the same entity (typically described by a noun) that is predicated by the templates of the same excitation polarity. Indeed, both X rise and X are high are excitatory and hence sea temperatures are high and sea temperatures rise are causally-compatible.8 6We used SVMlight with the polynominal kernel (d = 2), available at http://svmlight.joachims.org. 7Future work will exploit other original sentences, as suggested by an anonymous reviewer. 8Using other knowledge like verb entailment (Hashimoto et al., 2009) can be helpful too, which is further future work. 991 Scenarios (scs) generated by chaining causallycompatible phrase pairs are scored by 5core(sc), which embodies our assumption that an acceptable scenario consists of plausible event causality pairs: Method Ave. prec. (%) Proposed 46.27 w/o Context features 45.68 w/o Association features 45.66 w/o Semantic relation features 44.44 Base features only 41.29 5core(sc) = 11 C5core(cs) Table 3: Ablation tests. cs∈CAUS(sc) where CAU5(sc) is a set of event causality pairs that constitutes sc and cs is a member of CAU5(sc). C5core(cs), which is cs’s </context>
</contexts>
<marker>Hashimoto, Torisawa, Kuroda, Murata, Kazama, 2009</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa, Kow Kuroda, Masaki Murata, and Jun’ichi Kazama. 2009. Largescale verb entailment acquisition from the web. In Proceedings of EMILP 2009: Conference on Empirical Methods in Iatural Language Processing, pages 1172–1181.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Stijn De Saeger</author>
<author>Jun’ichi Kazama</author>
<author>Sadao Kurohashi</author>
</authors>
<title>Extracting paraphrases from definition sentences on the web.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>1087--1097</pages>
<marker>Hashimoto, Torisawa, De Saeger, Kazama, Kurohashi, 2011</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger, Jun’ichi Kazama, and Sadao Kurohashi. 2011. Extracting paraphrases from definition sentences on the web. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 1087–1097.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chikara Hashimoto</author>
<author>Kentaro Torisawa</author>
<author>Stijn De Saeger</author>
<author>Jong-Hoon Oh</author>
<author>Jun’ichi Kazama</author>
</authors>
<title>Excitatory or inhibitory: A new semantic orientation extracts contradiction and causality from the web.</title>
<date>2012</date>
<booktitle>In Proceedings ofEMILP-CoILL 2012: Conference on Empirical Methods in Iatural Language Processing and Iatural Language Learning,</booktitle>
<pages>619--630</pages>
<marker>Hashimoto, Torisawa, De Saeger, Oh, Kazama, 2012</marker>
<rawString>Chikara Hashimoto, Kentaro Torisawa, Stijn De Saeger, Jong-Hoon Oh, and Jun’ichi Kazama. 2012. Excitatory or inhibitory: A new semantic orientation extracts contradiction and causality from the web. In Proceedings ofEMILP-CoILL 2012: Conference on Empirical Methods in Iatural Language Processing and Iatural Language Learning, pages 619–630.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun’ichi Kazama</author>
<author>Kentaro Torisawa</author>
</authors>
<title>Inducing gazetteers for named entity recognition by largescale clustering of dependency relations.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT),</booktitle>
<pages>407--415</pages>
<contexts>
<context position="17544" citStr="Kazama and Torisawa (2008)" startWordPosition="2741" endWordPosition="2744">55,881 patterns. Moreover, for broader coverage, we acquired binary patterns that entail or are entailed by one of the patterns of the above six semantic relations. Those patterns were acquired from our web corpus by Kloetzer et al. (2013b)’s method, which acquired 185 million entailment pairs with 80% precision from our web corpus and was used for contradiction acquisition (Kloetzer et al., 2013a). We acquired 335,837 patterns by this method. They are class-dependent patterns, which have semantic class restrictions on arguments. The semantic classes were obtained from our web corpus based on Kazama and Torisawa (2008). See De Saeger et al. (2009), De Saeger et al. (2011) and Kloetzer et al. (2013a) for more on our patterns. They collectively constitute the ENTAILMENT relation. Table 1 shows our semantic relation features. To use them, we first make a database that records which noun pairs co-occur with each binary pattern. Then we check a noun pair (the nouns of the cause and effect phrases) for each event causality candidate, and give the candidate all the patterns in the database that co-occur with the noun pair. 3.2.2 Context Features We believe that contexts exist where event causality candidates are m</context>
<context position="21266" citStr="Kazama and Torisawa (2008)" startWordPosition="3353" endWordPosition="3356">tically acquired from our 600 million web pages using the method of Hashimoto et al. (2011). Web-based features provide association measures between nouns using various window sizes in the 600 million web pages. See Section D.3 for detail. Web-based association measures were obtained from the same database as AC4 in Table 2. 3.2.4 Base Features Base features represent the basic properties of event causality like nouns, templates, and their excitation polarities (See Section E in the supplementary notes). For B3 and B4, 500 semantic classes were obtained from our web corpus using the method of Kazama and Torisawa (2008). 3.3 Event Causality Scoring Using the above features, a classifier6 classifies each event causality candidate into causality and non-causality. An event causality candidate is given a causality score CScore, which is the SVM score (distance from the hyperplane) that is normalized to [0, 1] by the sigmoid function 1 1+e−x . Each event causality candidate may be given multiple original sentences, since a phrase pair can appear in multiple sentences, in which case it is given more than one SVM score. For such candidates, we give the largest score and keep only one original sentence that corresp</context>
</contexts>
<marker>Kazama, Torisawa, 2008</marker>
<rawString>Jun’ichi Kazama and Kentaro Torisawa. 2008. Inducing gazetteers for named entity recognition by largescale clustering of dependency relations. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL-08: HLT), pages 407– 415.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julien Kloetzer</author>
<author>Stijn De Saeger</author>
<author>Kentaro Torisawa</author>
<author>Chikara Hashimoto</author>
<author>Jong-Hoon Oh</author>
<author>Kiyonori Ohtake</author>
</authors>
<title>Two-stage method for large-scale acquisition of contradiction pattern pairs using entailment.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Iatural Language Processing (EMILP</booktitle>
<pages>693--703</pages>
<marker>Kloetzer, De Saeger, Torisawa, Hashimoto, Oh, Ohtake, 2013</marker>
<rawString>Julien Kloetzer, Stijn De Saeger, Kentaro Torisawa, Chikara Hashimoto, Jong-Hoon Oh, and Kiyonori Ohtake. 2013a. Two-stage method for large-scale acquisition of contradiction pattern pairs using entailment. In Proceedings of the Conference on Empirical Methods in Iatural Language Processing (EMILP 2013), pages 693–703.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julien Kloetzer</author>
</authors>
<title>Kentaro Torisawa, Stijn De Saeger, Motoki Sano, Chikara Hashimoto, and Jun Gotoh. 2013b. Large-scale acquisition of entailment pattern pairs.</title>
<date>2013</date>
<booktitle>In Information Processing Society of Japan (IPSJ) Kansai-Branch Convention</booktitle>
<marker>Kloetzer, 2013</marker>
<rawString>Julien Kloetzer, Kentaro Torisawa, Stijn De Saeger, Motoki Sano, Chikara Hashimoto, and Jun Gotoh. 2013b. Large-scale acquisition of entailment pattern pairs. In Information Processing Society of Japan (IPSJ) Kansai-Branch Convention 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Richard Landis</author>
<author>Gary G Koch</author>
</authors>
<title>The measurement of observer agreement for categorical data.</title>
<date>1977</date>
<journal>Biometrics,</journal>
<volume>33</volume>
<issue>1</issue>
<contexts>
<context position="26401" citStr="Landis and Koch, 1977" startWordPosition="4163" endWordPosition="4167">e annotators (not the authors), who annotated the event causality candidates without looking at the original sentences. The final label was determined by majority vote. The training data were created by the annotators through our preliminary experiments and consists of 112,110 among which 9,657 Semantic relations Ave. prec. (%) All semantic relations (Proposed) 46.27 CAUSATION 45.86 CAUSATION and PREVENTION 45.78 None (w/o Semantic relation features) 44.44 Table 4: Ablation tests on semantic relations. were positive. The Kappa (Fleiss, 1971) of their judgments was 0.67 (substantial agreement (Landis and Koch, 1977)). These three datasets have no overlap in terms of phrase pairs. About nine man-months were required to prepare the data. Our evaluation is based on average precision;9 we believe that it is important to rank the plausible event causality candidates higher. 5.1.1 Ablation Tests We evaluated the features of our method by ablation tests. Table 3 shows the results of removing the semantic relation, the context, and the association features from our method. All the feature types are effective and contribute to the performance gain that was about 5% higher than the Base features only. Proposed ach</context>
</contexts>
<marker>Landis, Koch, 1977</marker>
<rawString>J. Richard Landis and Gary G. Koch. 1977. The measurement of observer agreement for categorical data. Biometrics, 33(1):159–174.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ziheng Lin</author>
<author>Hwee Tou Ng</author>
<author>Min-Yen Kan</author>
</authors>
<title>A pdtb-styled end-to-end discourse parser.</title>
<date>2010</date>
<tech>Technical report,</tech>
<institution>School of Computing, National University of Singapore.</institution>
<contexts>
<context position="29208" citStr="Lin et al. (2010)" startWordPosition="4612" endWordPosition="4615">by removing the Wikipedia-, definition-, web-, and CEA-based features. The CEA-based features were the most effective, while the Wikipediabased ones slightly degraded the performance. 5.1.2 Comparison to Baseline Methods We compared our method and two baselines based on Do et al. (2011): CEAuns is an unsupervised method that uses CEA to rank event causality candidates, and CEAsup is a supervised method using SVM and the CEA features, whose ranking is based on the SVM scores. The baselines are not complete implementations of Do et al.’s method which uses discourse relations identified based on Lin et al. (2010) and exploits them with CEA within an ILP framework. Nonetheless, we believe that this comparison is informative since CEA can be seen as the main component; they achieved a F1 of 41.7% for extracting causal event relations, but with only CEA they still achieved 38.6%. Table 6 shows the average precision of the compared methods. Proposed is our proposed method. Proposed-CEA is Proposed without the CEAfeatures and shows their contribution. Proposed is the best and the CEA features slightly contribute to the performance, as Proposed-CEA indicates. We observed that CEAsup and CEAuns performed poo</context>
</contexts>
<marker>Lin, Ng, Kan, 2010</marker>
<rawString>Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010. A pdtb-styled end-to-end discourse parser. Technical report, School of Computing, National University of Singapore.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Manning</author>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Foundations of Statistical Iatural Language Processing.</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<marker>Manning, Sch¨utze, 1999</marker>
<rawString>Chris Manning and Hinrich Sch¨utze. 1999. Foundations of Statistical Iatural Language Processing. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jong-Hoon Oh</author>
<author>Kentaro Torisawa</author>
<author>Chikara Hashimoto</author>
<author>Motoki Sano</author>
<author>Stijn De Saeger</author>
<author>Kiyonori Ohtake</author>
</authors>
<title>Why-question answering using intra- and inter-sentential causal relations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1733--1743</pages>
<marker>Oh, Torisawa, Hashimoto, Sano, De Saeger, Ohtake, 2013</marker>
<rawString>Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto, Motoki Sano, Stijn De Saeger, and Kiyonori Ohtake. 2013. Why-question answering using intra- and inter-sentential causal relations. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1733– 1743.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Eric Horvitz</author>
</authors>
<title>Mining the web to predict future events.</title>
<date>2013</date>
<booktitle>In Proceedings ofSixth ACM International Conference on Web Search and Data Mining (WSDM2013),</booktitle>
<pages>255--264</pages>
<contexts>
<context position="9465" citStr="Radinsky and Horvitz, 2013" startWordPosition="1437" endWordPosition="1440">how that such thorough exploitation of new and existing features leads to high performance. 2A bacterium in the sea causing food-poisoning. 3Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 988 Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), which we ignore since we target event causality about two distinct entities. To the best of our knowledge, future scenario generation is a new task, although previous works have addressed similar tasks (Radinsky et al., 2012; Radinsky and Horvitz, 2013). Neither involves chaining and restricts themselves to only one event causality step. Besides, the events they predict must be those for which similar events have previously been observed, and their method only applies to news domain. Some of the scenarios we generated are written on no page in our input web corpus. Similarly, Tsuchida et al. (2011) generated semantic knowledge like causality that is written in no sentence. However, their method cannot combine more than two pieces of knowledge unlike ours, and their target knowledge consists of nouns, but ours consists of verb phrases, which </context>
</contexts>
<marker>Radinsky, Horvitz, 2013</marker>
<rawString>Kira Radinsky and Eric Horvitz. 2013. Mining the web to predict future events. In Proceedings ofSixth ACM International Conference on Web Search and Data Mining (WSDM2013), pages 255–264.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kira Radinsky</author>
<author>Sagie Davidovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Learning causality for news events prediction.</title>
<date>2012</date>
<booktitle>In Proceedings of International World Wide Web Conference</booktitle>
<pages>909--918</pages>
<contexts>
<context position="8510" citStr="Radinsky et al., 2012" startWordPosition="1290" endWordPosition="1293">*cause economic losses and the above “slash-and-burn agriculture” scenario (Section 5.2). Neither is written in any document in our input corpus. In this paper, our target language is Japanese. However, we believe that our ideas and methods are applicable to many languages. Examples are translated into English for ease of explanation. Supplementary notes of this paper are available at http://khn.nict.go.jp/analysis/ member/ch/acl2014-sup.pdf. 2 Related Work For event causality extraction, clues used by previous methods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2A bacterium in the sea causing food-poisoning. 3Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 988 Other clues</context>
</contexts>
<marker>Radinsky, Davidovich, Markovitch, 2012</marker>
<rawString>Kira Radinsky, Sagie Davidovich, and Shaul Markovitch. 2012. Learning causality for news events prediction. In Proceedings of International World Wide Web Conference 2012 (WWW 2012), pages 909–918.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehwish Riaz</author>
<author>Roxana Girju</author>
</authors>
<title>Another look at causality: Discovering scenario-specific contingency relationships with no supervision.</title>
<date>2010</date>
<booktitle>In 2010 IEEE Fourth International Conference on Semantic Computing,</booktitle>
<pages>361--368</pages>
<contexts>
<context position="8610" citStr="Riaz and Girju, 2010" startWordPosition="1306" endWordPosition="1309">ritten in any document in our input corpus. In this paper, our target language is Japanese. However, we believe that our ideas and methods are applicable to many languages. Examples are translated into English for ease of explanation. Supplementary notes of this paper are available at http://khn.nict.go.jp/analysis/ member/ch/acl2014-sup.pdf. 2 Related Work For event causality extraction, clues used by previous methods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2A bacterium in the sea causing food-poisoning. 3Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 988 Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009)</context>
</contexts>
<marker>Riaz, Girju, 2010</marker>
<rawString>Mehwish Riaz and Roxana Girju. 2010. Another look at causality: Discovering scenario-specific contingency relationships with no supervision. In 2010 IEEE Fourth International Conference on Semantic Computing, pages 361–368.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Schwartz</author>
</authors>
<title>The Art of the Long View.</title>
<date>1991</date>
<publisher>Doubleday.</publisher>
<contexts>
<context position="2043" citStr="Schwartz, 1991" startWordPosition="295" endWordPosition="296">troduction The world can be seen as a network of causality where people, organizations, and other kinds of entities causally depend on each other. This network is so huge and complex that it is almost impossible for humans to exhaustively predict the consequences of a given event. Indeed, after the Great East Japan Earthquake in 2011, few expected that it would lead to an enormous trade deficit in Japan due to a sharp increase in energy imports. For effective decision making that carefully considers any form of future risks and chances, we need a system that helps humans do scenario planning (Schwartz, 1991), which is a decision-making scheme that examines possible future events and assesses their potential chances and risks. Our ultimate goal is to develop a system that supports scenario planning through generating possible future events using big data, which would contain what Donald Rumsfeld called “unknown unknowns”1 (Torisawa et al., 2010). To this end, we propose a supervised method of extracting such event causality as conduct slash-and-burn agriculture—*exacerbate desertification and use its output to generate future scenarios (scenarios), which are chains of causality that have been or m</context>
</contexts>
<marker>Schwartz, 1991</marker>
<rawString>Peter Schwartz. 1991. The Art of the Long View. Doubleday.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroya Takamura</author>
<author>Takashi Inui</author>
<author>Manabu Okumura</author>
</authors>
<title>Extracting semantic orientation of words using spin model.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>133--140</pages>
<contexts>
<context position="13898" citStr="Takamura et al., 2005" startWordPosition="2130" endWordPosition="2133">inary patterns like A CAUSES B. Deforestation and global warming might complete the A and B slots. We manually collected 748 binary patterns for this relation. (See Section B in the supplementary notes for examples of our binary patterns.) MATERIAL is the relation between a material and a product made of it (e.g. plutonium and 5Hashimoto et al.’s method constructs a network of templates based on their co-occurrence in web sentences with a small number of polarity-assigned seed templates and infers the polarity of all the templates in the network by a constraint solver based on the spin model (Takamura et al., 2005). 989 atomic bomb) and can be expressed by A IS MADE OF B. Its relation to event causality might seem unclear, but a material can be seen as a “cause” of a product. Indeed materials can participate in event causality with the help of such template pairs as A is stolen→B is made as in plutonium is stolen→atomic bomb is made. We manually collected 187 binary patterns for this relation. NECESSITY’s patterns include A IS NECESSARY FOR B, which can be filled with verbal aptitude and ability to think. Noun pairs with this relation can constitute event causality when combined with template pairs like</context>
</contexts>
<marker>Takamura, Inui, Okumura, 2005</marker>
<rawString>Hiroya Takamura, Takashi Inui, and Manabu Okumura. 2005. Extracting semantic orientation of words using spin model. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005), pages 133–140.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shohei Tanaka</author>
<author>Naoaki Okazaki</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>Acquiring and generalizing causal inference rules from deverbal noun constructions.</title>
<date>2012</date>
<booktitle>In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012),</booktitle>
<pages>1209--1218</pages>
<contexts>
<context position="9026" citStr="Tanaka et al. (2012)" startWordPosition="1371" endWordPosition="1374">ethods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2A bacterium in the sea causing food-poisoning. 3Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 988 Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009), which we ignore since we target event causality about two distinct entities. To the best of our knowledge, future scenario generation is a new task, although previous works have addressed similar tasks (Radinsky et al., 2012; Radinsky and Horvitz, 2013). Neither involves chaining and restricts themselves to only one event causality step. Besides, the events they predict must be those for which similar events ha</context>
</contexts>
<marker>Tanaka, Okazaki, Ishizuka, 2012</marker>
<rawString>Shohei Tanaka, Naoaki Okazaki, and Mitsuru Ishizuka. 2012. Acquiring and generalizing causal inference rules from deverbal noun constructions. In Proceedings of 24th International Conference on Computational Linguistics (COLING 2012), pages 1209– 1218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masahiro Tanaka</author>
<author>Stijn De Saeger</author>
</authors>
<title>Kiyonori Ohtake, Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii, and Kentaro Torisawa.</title>
<date>2013</date>
<booktitle>In Companion Volume of the Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP 2013) (Demo Track),</booktitle>
<pages>45--48</pages>
<marker>Tanaka, De Saeger, 2013</marker>
<rawString>Masahiro Tanaka, Stijn De Saeger, Kiyonori Ohtake, Chikara Hashimoto, Makoto Hijiya, Hideaki Fujii, and Kentaro Torisawa. 2013. WISDOM2013: A large-scale web information analysis system. In Companion Volume of the Proceedings of the 6th International Joint Conference on Natural Language Processing (IJCNLP 2013) (Demo Track), pages 45–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
<author>Stijn de Saeger</author>
<author>Jun’ichi Kazama</author>
<author>Asuka Sumida</author>
<author>Daisuke Noguchi</author>
<author>Yasunari Kakizawa</author>
<author>Masaki Murata</author>
<author>Kow Kuroda</author>
<author>Ichiro Yamada</author>
</authors>
<title>Organizing the web’s information explosion to discover unknown unknowns.</title>
<date>2010</date>
<journal>New Generation Computing (Special Issue on Information Explosion),</journal>
<volume>28</volume>
<issue>3</issue>
<marker>Torisawa, de Saeger, Kazama, Sumida, Noguchi, Kakizawa, Murata, Kuroda, Yamada, 2010</marker>
<rawString>Kentaro Torisawa, Stijn de Saeger, Jun’ichi Kazama, Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa, Masaki Murata, Kow Kuroda, and Ichiro Yamada. 2010. Organizing the web’s information explosion to discover unknown unknowns. New Generation Computing (Special Issue on Information Explosion), 28(3):217–236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kentaro Torisawa</author>
</authors>
<title>Acquiring inference rules with temporal constraints by using japanese coordinated sentences and noun-verb co-occurrences.</title>
<date>2006</date>
<booktitle>In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT-NAACL2006),</booktitle>
<pages>57--64</pages>
<contexts>
<context position="8588" citStr="Torisawa, 2006" startWordPosition="1304" endWordPosition="1305">2). Neither is written in any document in our input corpus. In this paper, our target language is Japanese. However, we believe that our ideas and methods are applicable to many languages. Examples are translated into English for ease of explanation. Supplementary notes of this paper are available at http://khn.nict.go.jp/analysis/ member/ch/acl2014-sup.pdf. 2 Related Work For event causality extraction, clues used by previous methods can roughly be categorized as lexico-syntactic patterns (Abe et al., 2008; Radinsky et al., 2012), words in context (Oh et al., 2013), associations among words (Torisawa, 2006; Riaz and Girju, 2010; Do et al., 2011), and predicate semantics (Hashimoto et al., 2012). Besides features similar to those described above, we propose semantic relation features3 that include those that are not obviously related to causality. We show that such thorough exploitation of new and existing features leads to high performance. 2A bacterium in the sea causing food-poisoning. 3Radinsky et al. (2012) and Tanaka et al. (2012) used semantic relations to generalize acquired causality instances. 988 Other clues include shared arguments (Torisawa, 2006; Chambers and Jurafsky, 2008; Chambe</context>
</contexts>
<marker>Torisawa, 2006</marker>
<rawString>Kentaro Torisawa. 2006. Acquiring inference rules with temporal constraints by using japanese coordinated sentences and noun-verb co-occurrences. In Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL (HLT-NAACL2006), pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Tsuchida</author>
<author>Kentaro Torisawa</author>
<author>Stijn De Saeger</author>
<author>Jong Hoon Oh</author>
<author>Jun’ichi Kazama</author>
<author>Chikara Hashimoto</author>
<author>Hayato Ohwada</author>
</authors>
<title>Toward finding semantic relations not written in a single sentence: An inference method using auto-discovered rules.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP</booktitle>
<pages>902--910</pages>
<marker>Tsuchida, Torisawa, De Saeger, Oh, Kazama, Hashimoto, Ohwada, 2011</marker>
<rawString>Masaaki Tsuchida, Kentaro Torisawa, Stijn De Saeger, Jong Hoon Oh, Jun’ichi Kazama, Chikara Hashimoto, and Hayato Ohwada. 2011. Toward finding semantic relations not written in a single sentence: An inference method using auto-discovered rules. In Proceedings of the 5th International Joint Conference on Natural Language Processing (IJCNLP 2011), pages 902–910.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Istv´an Varga</author>
<author>Motoki Sano</author>
</authors>
<title>Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<pages>1619--1629</pages>
<marker>Varga, Sano, 2013</marker>
<rawString>Istv´an Varga, Motoki Sano, Kentaro Torisawa, Chikara Hashimoto, Kiyonori Ohtake, Takao Kawai, JongHoon Oh, and Stijn De Saeger. 2013. Aid is out there: Looking for help from tweets during a large scale disaster. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (ACL 2013), pages 1619–1629.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naoki Yoshinaga</author>
<author>Masaru Kitsuregawa</author>
</authors>
<title>Polynomial to linear: Efficient classification with conjunctive features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP</booktitle>
<pages>542--1551</pages>
<contexts>
<context position="11386" citStr="Yoshinaga and Kitsuregawa, 2009" startWordPosition="1740" endWordPosition="1743">st of a predicate with an argument position (template, hereafter) like conduct X and a noun like slash-and-burn agriculture that completes X. We also require the predicate of the cause phrase to syntactically depend on the effect phrase in the sentence from which the event causality was extracted; we guarantee this by verifying the dependencies of the original sentence. In Japanese, since the temporal order between events is usually determined by precedence in a sentence, we require the cause phrase to precede the effect phrase. For context 4We used a Japanese dependency parser called J.DepP (Yoshinaga and Kitsuregawa, 2009), available at http:// www.tkl.iis.u-tokyo.ac.jp/∼ynaga/jdepp/. feature extraction, the event causality candidates are accompanied by the original sentences from which they were extracted. Excitation We only keep the event causality candidates each phrase of which consists of excitation templates, which have been shown to be effective for causality extraction (Hashimoto et al., 2012) and other semantic NLP tasks (Oh et al., 2013; Varga et al., 2013; Kloetzer et al., 2013a). Excitation is a semantic property of templates that classifies them into excitatory, inhibitory, and neutral. Excitatory </context>
</contexts>
<marker>Yoshinaga, Kitsuregawa, 2009</marker>
<rawString>Naoki Yoshinaga and Masaru Kitsuregawa. 2009. Polynomial to linear: Efficient classification with conjunctive features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing (EMNLP 2009), pages 542–1551.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>