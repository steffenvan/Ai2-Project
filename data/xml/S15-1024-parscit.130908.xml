<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010341">
<title confidence="0.961902">
Learning to predict script events from domain-specific text
</title>
<author confidence="0.822513">
Rachel Rudinger&apos;, Vera Demberg3, Ashutosh Modi3,
Benjamin Van Durme&apos;,&apos;, Manfred Pinkal3
</author>
<affiliation confidence="0.802639">
Center for Language and Speech Processing&apos;
Human Language Technology Center of Excellence&apos;
Johns Hopkins University, Baltimore, MD USA
MMCI Cluster of Excellence3
Saarland University, Saarbr¨ucken, Germany
</affiliation>
<email confidence="0.9852515">
rudinger@jhu.edu, {vera,pinkal}@coli.uni-saarland.de,
amodi@mmci.uni-saarland.de, vandurme@cs.jhu.edu
</email>
<sectionHeader confidence="0.995617" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997718454545454">
The automatic induction of scripts (Schank
and Abelson, 1977) has been the focus of
many recent works. In this paper, we employ
a variety of these methods to learn Schank
and Abelson’s canonical restaurant script, us-
ing a novel dataset of restaurant narratives we
have compiled from a website called “Din-
ners from Hell.” Our models learn narrative
chains, script-like structures that we evaluate
with the “narrative cloze” task (Chambers and
Jurafsky, 2008).
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999989074074074">
A well-known theory from the intersection of psy-
chology and artificial intelligence posits that humans
organize certain kinds of general knowledge in the
form of scripts, or common sequences of events
(Schank and Abelson, 1977). Though many early AI
systems employed hand-encoded scripts, more re-
cent work has attempted to induce scripts with auto-
matic and scalable techniques. In particular, several
related techniques approach the problem of script in-
duction as one of learning narrative chains from text
corpora (Chambers and Jurafsky, 2008; Chambers
and Jurafsky, 2009; Jans et al., 2012; Pichotta and
Mooney, 2014). These statistical approaches have
focused on open-domain script acquisition, in which
a large number of scripts may be learned, but the ac-
quisition of any particular set of scripts is not guar-
anteed. For many specialized applications, however,
knowledge of a few relevant scripts may be more
useful than knowledge of many irrelevant scripts.
With this scenario in mind, we attempt to learn
the famous “restaurant script” (Schank and Abel-
son, 1977) by applying the aforementioned narrative
chain learning methods to a specialized corpus of
dinner narratives we compile from the website “Din-
ners from Hell.” Our results suggest that applying
these techniques to a domain-specific dataset may
be reasonable way to learn domain-specific scripts.
</bodyText>
<sectionHeader confidence="0.98111" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.99995688">
Previous work in the automatic induction of scripts
or script-like structures has taken a number of dif-
ferent approaches. Regneri et al. (2010) attempt
to learn the structure of specific scripts by eliciting
event sequence descriptions (ESDs) from humans
to which they apply multiple sequence alignment
(MSA) to yield one global structure per script. (Orr
et al. (2014) learn similar structures in a probabilis-
tic framework with Hidden Markov Models.) Al-
though Regneri et al. (2010), like us, are concerned
with learning pre-specified scripts, our approach is
different in that we apply unsupervised techniques to
scenario-specific collections of natural, pre-existing
texts.
Note that while the applicability of our approach
to script learning may appear limited to domains
for which a corpus conveniently already exists, pre-
vious work demonstrates the feasibility of assem-
bling such a corpus by automatically retrieving rel-
evant documents from a larger collection. For ex-
ample, Chambers and Jurafsky (2011) use informa-
tion retrieval techniques to gather a small number of
bombing-related documents from the Gigaword cor-
pus, which they successfully use to learn a MUC-
style (Sundheim, 1991) information extraction tem-
</bodyText>
<page confidence="0.983808">
205
</page>
<note confidence="0.441501">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.999682480769231">
plate for bombing events.
Following the work of Church and Hanks (1990)
in learning word associations via mutual informa-
tion, and the DIRT system introduced by Lin and
Pantel (2001), Chambers and Jurafsky (2008) pro-
pose a PMI-based system for learning script-like
structures called narrative chains. Several follow-
up papers introduce variations and improvements
on this original model for learning narrative chains
(Chambers and Jurafsky, 2009; Jans et al., 2012; Pi-
chotta and Mooney, 2014). It is from this body of
work that we borrow techniques to apply to the Din-
ners from Hell dataset.
As defined by Chambers and Jurafsky (2008), a
narrative chain is “a partially ordered set of narrative
events that share a common actor,” where a narrative
event is “a tuple of an event (most simply a verb) and
its participants, represented as typed dependencies.”
To learn narrative chains from text, Chambers and
Jurafsky extract chains of narrative events linked by
a common coreferent within a document. For exam-
ple, the sentence “John drove to the store where he
bought some ice cream.” would generate two nar-
rative events corresponding to the protagonist John:
(DRIVE, nsubj) followed by (BUY, nsubj). Over
these extracted chains of narrative events, pointwise
mutual information (PMI) is computed between all
pairs of events. These PMI scores are then used to
predict missing events from such chains, i.e. the nar-
rative cloze evaluation.
Jans et al. (2012) expand on this approach, intro-
ducing an ordered PMI model, a bigram probabil-
ity model, skip n-gram counting methods, corefer-
ence chain selection, and an alternative scoring met-
ric (recall at 50). Their bigram probability model
outperforms the original PMI model on the narra-
tive cloze task under many conditions. Pichotta and
Mooney (2014) introduce an extended notion of nar-
rative event that includes information about subjects
and objects. They also introduce a competitive “un-
igram model” as a baseline for the narrative cloze
task.
To learn the restaurant script from our dataset,
we implement the models of Chambers and Juraf-
sky (2008) and Jans et al. (2012), as well as the
unigram baseline of Pichotta and Mooney (2014).
To evaluate our success in learning the restaurant
script, we perform a modified version of the nar-
rative cloze task, predicting only verbs that we an-
notate as “restaurant script-relevant” and comparing
the performance of each model. Note that these an-
notations are not used for training.
</bodyText>
<sectionHeader confidence="0.997283" genericHeader="method">
3 Methods
</sectionHeader>
<bodyText confidence="0.9998877">
This section provides an overview of each of the dif-
ferent methods and parameter settings we employ to
learn narrative chains from the Dinners from Hell
corpus, starting with the original model (Chambers
and Jurafsky, 2008) and extending to the modifica-
tions of Jans et al. (2012). As part of this work,
we are releasing a program called NaChos, our inte-
grated Python implementation of each of the meth-
ods for learning narrative chains described in this
section.1
</bodyText>
<subsectionHeader confidence="0.971227">
3.1 Counting methods for PMI
</subsectionHeader>
<bodyText confidence="0.999424125">
Formally, a narrative event, e := (v, d), is a verb,
v, paired with a typed dependency (De Marneffe
et al., 2006), d, defining the role a “protagonist”
(coreference mention) plays in an event (verb). The
main computational component of learning narrative
chains in Chambers and Jurafsky’s model is to learn
the pointwise mutual information for any pair of nar-
rative events:
</bodyText>
<equation confidence="0.998399">
C(e1, e2)
pmi(e1, e2) := log C(e1, ∗)C(∗, e2) (1)
</equation>
<bodyText confidence="0.995911">
where C(e1, e2) is the number of times that narrative
events e1 and e2 “co-occur” and
</bodyText>
<equation confidence="0.9089065">
�C(e, ∗) := C(e, e�) (2)
el
</equation>
<bodyText confidence="0.999545769230769">
Chambers and Jurafsky define C(e1, e2) as “the
number of times the two events e1 and e2 had a core-
ferring entity filling the values of the dependencies
d1 and d2.” This is a symmetric value with respect
to e1 and e2.
We implement the following counting variants:
Skip N-gram By default, C(e1,e2) is incre-
mented if e1 and e2 occur anywhere within the same
chain of events derived from a single coreference
chain (skip-all); we also implement an option to re-
strict the distance between e1 and e2 to 0 though 5
intervening events (skip-0 through skip-5). (Jans et
al., 2012)
</bodyText>
<footnote confidence="0.977856">
1www.clsp.jhu.edu/people/rachel-rudinger
</footnote>
<page confidence="0.997958">
206
</page>
<bodyText confidence="0.996921111111111">
Coreference Chain Length The original model
counts co-occurrences in all coreference chains; we
include Jans et al. (2012)’s option to count over only
the longest chains in each document, or to count
only over chains of length 5 or greater (long).
Count Threshold Because PMI favors low-count
events, we add an option to set C(e1, e2) to zero for
any e1, e2 for which C(e1, e2) is below some thresh-
old, T, up to 5.
</bodyText>
<subsectionHeader confidence="0.998841">
3.2 Predictive Models for Narrative Cloze
</subsectionHeader>
<bodyText confidence="0.999128833333333">
In order to perform the narrative cloze task, we need
a model for predicting the missing narrative event, e,
from a chain of observed narrative events, e1 ... en,
at insertion point k. The original model, proposed
by Chambers and Jurafsky (2008), predicts the event
that maximizes unordered pmi,
</bodyText>
<equation confidence="0.994447">
n
eˆ = arg max pmi(e, ei) (3)
eEV i=1
</equation>
<bodyText confidence="0.9992756">
where V is the set of all observed events (the vo-
cabulary) and C(e1,e2) is symmetric. Two addi-
tional models are introduced by Jans et al. (2012)
and we use them here, as well. First, the ordered
pmi model,
</bodyText>
<equation confidence="0.9739935">
eˆ = arg max
eEV
</equation>
<bodyText confidence="0.9301295">
(4)
where C(e1,e2) is asymmetric, i.e., C(e1,e2)
counts only cases in which e1 occurs before e2. Sec-
ond, the bigram probability model:
</bodyText>
<equation confidence="0.94037375">
p(eile) (5)
where p(e2|e1) = C(e1,e2)
C(e1,∗) and C(e1, e2) is asym-
metric.
</equation>
<bodyText confidence="0.999608357142857">
Discounting For each model, we add an option
for discounting the computed scores. In the case
of the two PMI-based models, we use the discount
score described in Pantel and Ravichandran (2004)
and used by Chambers and Jurafsky (2008). For the
bigram probability model, this PMI discount score
would be inappropriate, so we instead use absolute
discounting.
Document Threshold We include a document
threshold parameter, D, that ensures that, in any nar-
rative cloze test, any event e that was observed dur-
ing training in fewer than D distinct documents will
receive a worse score (i.e. be ranked behind) any
event e&apos; whose count meets the document threshold.
</bodyText>
<sectionHeader confidence="0.990974" genericHeader="method">
4 Dataset: Dinners From Hell
</sectionHeader>
<bodyText confidence="0.999963666666667">
The source of our data for this experiment is a blog
called “Dinners From Hell”2 where readers submit
stories about their terrible restaurant experiences.
For an example story, see Figure 1. To process the
raw data, we stripped all HTML and other non-story
content from each file and processed the remain-
ing text with the Stanford CoreNLP pipeline version
3.3.1 (Manning et al., 2014). Of the 237 stories ob-
tained, we manually filtered out 94 stories that were
“off-topic” (e.g., letters to the webmaster, dinners
not at restaurants), leaving a total of 143 stories. The
average story length is 352 words.
</bodyText>
<subsectionHeader confidence="0.984871">
4.1 Annotation
</subsectionHeader>
<bodyText confidence="0.9999718">
For the purposes of evaluation only, we hired four
undergraduates to annotate every non-copular verb
in each story as either corresponding to an event
“related to the experience of eating in a restaurant”
(e.g., ordered a steak), “unrelated to the experience
of eating in a restaurant” (e.g., answered the phone),
or uncertain. We used the WebAnno platform for
annotation (Yimam et al., 2013).
A total of 8,202 verb (tokens) were annotated,
each by three annotators. 70.3% of verbs anno-
tated achieved 3-way agreement; 99.4% had at least
2-way agreement. After merging the annotations
(simple majority vote), 30.7% of verbs were labeled
as restaurant-script-related, 68.6% were labeled as
restaurant-script-unrelated, and the remaining 0.7%
as uncertain.
Corresponding to the 8,202 annotated verb to-
kens, there are 1,481 narrative events at the type
level. 580 of these narrative event types were anno-
tated as script-relevant in at least one token instance.
</bodyText>
<footnote confidence="0.866449">
2www.dinnersfromhell.com
</footnote>
<equation confidence="0.9918275">
k n
pmi(ei, e) + E pmi(e, ei)
i=1 i=k+1
eˆ = arg max k p(elei) n
eEV i=1 H
i=k+1
</equation>
<page confidence="0.995554">
207
</page>
<figureCaption confidence="0.995689333333333">
Figure 1: Example story from Dinners from Hell corpus. Bold
words indicate events in the “we” coreference chain (the longest
chain). Boxed words (blue) indicate best narrative chain of
length three (see Section 5.2); underlined words (orange) are
corresponding subjects and bracketed words (green) are corre-
sponding objects.
</figureCaption>
<sectionHeader confidence="0.997884" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.995273">
5.1 Narrative Cloze
</subsectionHeader>
<bodyText confidence="0.999216214285714">
We evaluate the various models on the narrative
cloze task. What is different about our version of
the narrative cloze task here is that we limit the cloze
tests to only “interesting” events, i.e., those that have
been identified as relevant to the restaurant script by
our annotators (see Section 4.1).
Because our dataset is small (143 documents), we
perform leave-one-out testing at the document level,
training on 133 folds total. (Ten documents are ex-
cluded for a development set.) For each fold of train-
ing, we extract all of the narrative chains (mapped
directly from coreference chains) in the held out test
document. For each test chain, we generate one nar-
rative cloze test per “script-relevant” event in that
</bodyText>
<table confidence="0.992087785714286">
MODEL AVGRNK MRR R@50
unigram model (baseline) 298.13 0.062 0.50
1. unordered pmi; avgrnk 276.88 0.063 0.36
2. unordered pmi; mrr 376.25 0.058 0.33
3. unordered pmi; R@50 400.36 0.050 0.50
4. ordered pmi; avgrnk 284.68 0.061 0.32
5. ordered pmi; mrr 381.44 0.054 0.25
6. ordered pmi; R@50 401.69 0.047 0.50
7. bigram; avgrnk 281.07 0.077 0.38
8. bigram; mrr 378.06 0.066 0.30
9. bigram; R@50 271.78 0.084 0.43
10. bigram disc; avgrnk 283.01 0.077 0.38
11. bigram disc; mrr 378.10 0.067 0.30
12. bigram disc; R@50 271.62 0.089 0.43
</table>
<figureCaption confidence="0.9872308">
Figure 2: Narrative cloze evaluation. Shaded blue cells indi-
cate which scoring metric that row’s parameter settings have
been optimized to. Bold numbers indicate a result that beats
the baseline. Row 12 representes the best model performance
overall.
</figureCaption>
<table confidence="0.552374769230769">
ROW SKIP T D COREF PMI DISC ABS DISC
1 0 1 3 all yes N/A
2 1 3 5 long no N/A
3 1 5 4 longest yes N/A
4 0 1 3 all yes N/A
5 3 5 5 long no N/A
6 0 3 4 longest yes N/A
7 all 1 3 all N/A no
8 3 5 5 long N/A no
9 all 1 5 all N/A no
10 all 1 3 all N/A yes
11 3 5 5 long N/A yes
12 all 1 5 all N/A yes
</table>
<figureCaption confidence="0.999977">
Figure 3: Parameter settings corresponding to each model in
Fig 2.
</figureCaption>
<bodyText confidence="0.986161756097562">
chain. For example, if a chain contains ten events,
three of which are “script-relevant,” then three cloze
tests will be generated, each containing nine “ob-
served” events. Chains with fewer than two events
are excluded. In this way, we generate a total of
2,273 cloze tests.
Scoring We employ three different scoring met-
rics: average rank (Chambers and Jurafsky, 2008),
mean reciprocal rank, and recall at 50 (Jans et al.,
2012).
Baseline The baseline we use for the narrative
cloze task is to rank events by frequency. This
is the “unigram model” employed by Pichotta and
Mooney (2014), a competitive baseline on this task.
“A long time ago when I was still in college, my family
decided to take me out for pizza on my birthday. We
decided to try the new location for a favorite pizza
chain of ours. It was all adults and there were about 8
of us, so we ordered 3 large pizzas. We got to chatting
and soon realized that the pizzas should’ve been ready
quite a bit ago, so we called the waitress over and she
went to check on our pizzas. She did not come back.
We waited about another 10 minutes, then called over
another waitress, who went to check on our pizzas and
waitress. It now been over an hour. About 10 minutes
later, my Dad goes up to the check-out and asks the girl
there to send the manager to our table. A few minutes
later the manager comes out. He explains to us that
our pizzas got stuck in the oven and burned. They were
out of large pizza dough bread, so they were making us
6 medium pizzas for the price of 3 large pizzas. We
had so many [pizzas] on our table we barely had
[room] to eat! Luckily my family is pretty easy going
so we just laughed about the whole thing. We did tell
the manager that it would have been nice if someone,
anyone, had said something earlier to us, instead of
just disappearing, and he agreed. He even said it was
his responsibility, but that he had been busy trying to
fix what caused the pizzas to jam up in the oven. He
went so far as to give us 1/2 off our bill, which was
really nice. It was definitely a memorable birthday!”
</bodyText>
<page confidence="0.995917">
208
</page>
<bodyText confidence="0.9990966">
For each model and scoring metric, we perform
a complete grid search over all possible parameter
settings to find the best-scoring combination on a
cloze tests from a set-aside development set of ten
documents. The parameter space is defined as the
Cartesian product of each of the following possible
parameter values: skip-n (all,0-5), coreference chain
length (all, long, longest), count threshold (T=1-
5), document threshold (D=1-5), and discounting
(yes/no). Bigram probability with and without dis-
counting are treated as two separate models.
Figure 2 reports the results of the narrative cloze
evalutation. Each of the four models (unordered
pmi, ordered pmi, bigram, and bigram with dis-
counting) outperform the baseline on the average
rank metric when the parameters are optimized for
that metric. Both bigram models beat the baseline on
mean reciprocal rank not only for MRR-optimized
parameter settings, but for the average-rank- and
recall-at-50-optimized settings. None of the param-
eter settings are able to ouperform the baseline on
recall at 50, though both PMI models tie the base-
line. Overall, the model that performs the best is the
bigram probability model with discounting (row 12
of Figure 2) which has the following parameter set-
tings: skip-all, coref-all, T=1, and D=5.
The fact that several model settings outperform an
informed baseline on average rank and mean recip-
rocal rank indicates that these methods may in gen-
eral be applicable to smaller, domain-specific cor-
pora. Furthermore, it is apparent from the results
that the bigram probability models perform better
overall than PMI-based models, a finding also re-
ported in Jans et al. (2012). This replication is futher
evidence that these methods do in fact transfer.
</bodyText>
<subsectionHeader confidence="0.999708">
5.2 Qualitative Example
</subsectionHeader>
<bodyText confidence="0.999972894736842">
To get a qualitative sense of the narrative events
these models are learning to associate from this data,
we use the conditional probabilities learned in the
bigram model (Fig 2, row 12) to select the high-
est probability narrative chain of length three out of
the 12 possible events in the “we” coreference chain
in Figure 1 (bolded). The three events selected are
boxed and highlighted in blue. The bigram model
selects the “deciding” event (selecting restaurant)
and the “having” event (having pizza), both reason-
able components of the restaurant script. The third
event selected is “having room,” which is not part of
the restaurant script. This mistake illustrates a weak-
ness of the narrative chains model; without consid-
ering the verb’s object, the model is unable to distin-
guish “have pizza” from “have room.” Incorporating
object information in future experiments, as in Pi-
chotta and Mooney (2014), might resolve this issue,
although it could introduce sparsity problems.
</bodyText>
<sectionHeader confidence="0.999173" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999990703703704">
In this work, we describe the collection and anno-
tation of a corpus of natural descriptions of restau-
rant visits from the website “Dinners from Hell.” We
use this dataset in an attempt to learn the restaurant
script, using a variety of related methods for learn-
ing narrative chains and evaluating on the narrative
cloze task. Our results suggest that it may be pos-
sible in general to use these methods on domain-
specific corpora in order to learn particular scripts
from a pre-specified domain, although further exper-
iments in other domains would help bolster this con-
clusion. In principle, a domain-specific corpus need
not come from a website like Dinners from Hell;
it could instead be sub-sampled from a larger cor-
pus, retrieved from the web, or directly elicited. Our
domain-specific approach to script learning is poten-
tially useful for specialized NLP applications that re-
quire knowledge of only a particular set of scripts.
One feature of the Dinners from Hell corpus that
bears further inspection in future work is the fact that
its stories contain many violations of the restaurant
script. A question to investigate is whether these vi-
olations impact how the restaurant script is learned.
Other avenues for future work include incorporat-
ing object information into event representations and
applying domain adaptation techniques in order to
leverage larger general-domain corpora.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998692">
This research was sponsored in part by the NSF
under grant 0530118 (PIRE) and with additional
support from the Allen Institute for Artificial In-
telligence (AI2). The authors would also like to
thank Michaela Regneri, Annemarie Friedrich, Ste-
fan Thater, Alexis Palmer, Andrea Horbach, Diana
Steffen, Asad Sayeed, and Frank Ferraro for their
insightful contributions.
</bodyText>
<page confidence="0.99819">
209
</page>
<sectionHeader confidence="0.98957" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999768924050633">
Nathanael Chambers and Dan Jurafsky. 2008. Unsu-
pervised learning of narrative event chains. In Pro-
ceedings ofACL-08: HLT, pages 789–797, Columbus,
Ohio. Association for Computational Linguistics.
Nathanael Chambers and Dan Jurafsky. 2009. Unsuper-
vised learning of narrative schemas and their partici-
pants. In Proceedings of the Joint Conference of the
47th Annual Meeting of the ACL and the 4th Interna-
tional Joint Conference on Natural Language Process-
ing of the AFNLP, pages 602–610, Suntec, Singapore.
Association for Computational Linguistics.
Nathanael Chambers and Dan Jurafsky. 2011. Template-
based information extraction without the templates. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 976–986, Portland, Ore-
gon, USA. Association for Computational Linguistics.
Kenneth Ward Church and Patrick Hanks. 1990. Word
association norms, mutual information, and lexicogra-
phy. Computational linguistics, 16(1):22–29.
Marie-Catherine De Marneffe, Bill MacCartney, Christo-
pher D Manning, et al. 2006. Generating typed de-
pendency parses from phrase structure parses. In Pro-
ceedings of LREC, volume 6, pages 449–454.
Bram Jans, Steven Bethard, Ivan Vuli´c, and Marie-
Francine Moens. 2012. Skip n-grams and ranking
functions for predicting script events. In Proceedings
of the 13th Conference of the European Chapter of
the Association for Computational Linguistics, pages
336–344, Avignon, France. Association for Computa-
tional Linguistics.
Dekang Lin and Patrick Pantel. 2001. Dirt - discov-
ery of inference rules from text. In Proceedings of
the seventh ACM SIGKDD international conference
on Knowledge discovery and data mining, pages 323–
328. ACM.
Christopher D. Manning, Mihai Surdeanu, John Bauer,
Jenny Finkel, Steven J. Bethard, and David McClosky.
2014. The Stanford CoreNLP natural language pro-
cessing toolkit. In Proceedings of 52nd Annual Meet-
ing of the Association for Computational Linguistics:
System Demonstrations, pages 55–60.
J Walker Orr, Prasad Tadepalli, Janardhan Rao Doppa,
Xiaoli Fern, and Thomas G Dietterich. 2014. Learn-
ing scripts as hidden markov models. In Twenty-
Eighth AAAI Conference on Artificial Intelligence.
Patrick Pantel and Deepak Ravichandran. 2004. Auto-
matically labeling semantic classes. In Daniel Marcu
Susan Dumais and Salim Roukos, editors, HLT-
NAACL 2004: Main Proceedings, pages 321–328,
Boston, Massachusetts, USA. Association for Compu-
tational Linguistics.
Karl Pichotta and Raymond Mooney. 2014. Statisti-
cal script learning with multi-argument events. In
Proceedings of the 14th Conference of the European
Chapter of the Association for Computational Linguis-
tics, pages 220–229, Gothenburg, Sweden. Associa-
tion for Computational Linguistics.
Michaela Regneri, Alexander Koller, and Manfred
Pinkal. 2010. Learning script knowledge with web
experiments. In Proceedings of the 48th Annual Meet-
ing of the Association for Computational Linguistics,
pages 979–988, Uppsala, Sweden. Association for
Computational Linguistics.
Roger Schank and Robert Abelson. 1977. Scripts,
plans, goals and understanding: An inquiry into hu-
man knowledge structures. Lawrence Erlbaum Asso-
ciates, Hillsdale, NJ.
Beth M. Sundheim. 1991. Third message understand-
ing evaluation and conference (muc-3): Phase 1 status
report. In Proceedings of the Message Understanding
Conference.
Seid Muhie Yimam, Iryna Gurevych, Richard Eckart de
Castilho, and Chris Biemann. 2013. Webanno: A
flexible, web-based and visually supported system for
distributed annotations. In Proceedings of the 51st
Annual Meeting of the Association for Computational
Linguistics: System Demonstrations, pages 1–6, Sofia,
Bulgaria. Association for Computational Linguistics.
</reference>
<page confidence="0.998965">
210
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.100642">
<title confidence="0.997104">Learning to predict script events from domain-specific text</title>
<author confidence="0.924726">Vera Ashutosh Van</author>
<affiliation confidence="0.794187">for Language and Speech Language Technology Center of Johns Hopkins University, Baltimore, MD USA Cluster of Saarland University, Saarbr¨ucken,</affiliation>
<email confidence="0.999182">amodi@mmci.uni-saarland.de,vandurme@cs.jhu.edu</email>
<abstract confidence="0.989278">The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank Abelson’s canonical using a novel dataset of restaurant narratives we have compiled from a website called “Dinfrom Hell.” Our models learn script-like structures that we evaluate with the “narrative cloze” task (Chambers and</abstract>
<note confidence="0.532888">Jurafsky, 2008).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative event chains.</title>
<date>2008</date>
<booktitle>In Proceedings ofACL-08: HLT,</booktitle>
<pages>789--797</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="922" citStr="Chambers and Jurafsky, 2008" startWordPosition="119" endWordPosition="122">r of Excellence3 Saarland University, Saarbr¨ucken, Germany rudinger@jhu.edu, {vera,pinkal}@coli.uni-saarland.de, amodi@mmci.uni-saarland.de, vandurme@cs.jhu.edu Abstract The automatic induction of scripts (Schank and Abelson, 1977) has been the focus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson’s canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called “Dinners from Hell.” Our models learn narrative chains, script-like structures that we evaluate with the “narrative cloze” task (Chambers and Jurafsky, 2008). 1 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et a</context>
<context position="3891" citStr="Chambers and Jurafsky (2008)" startWordPosition="573" endWordPosition="576">larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most simply a verb) and its partic</context>
<context position="5774" citStr="Chambers and Jurafsky (2008)" startWordPosition="876" endWordPosition="880">is approach, introducing an ordered PMI model, a bigram probability model, skip n-gram counting methods, coreference chain selection, and an alternative scoring metric (recall at 50). Their bigram probability model outperforms the original PMI model on the narrative cloze task under many conditions. Pichotta and Mooney (2014) introduce an extended notion of narrative event that includes information about subjects and objects. They also introduce a competitive “unigram model” as a baseline for the narrative cloze task. To learn the restaurant script from our dataset, we implement the models of Chambers and Jurafsky (2008) and Jans et al. (2012), as well as the unigram baseline of Pichotta and Mooney (2014). To evaluate our success in learning the restaurant script, we perform a modified version of the narrative cloze task, predicting only verbs that we annotate as “restaurant script-relevant” and comparing the performance of each model. Note that these annotations are not used for training. 3 Methods This section provides an overview of each of the different methods and parameter settings we employ to learn narrative chains from the Dinners from Hell corpus, starting with the original model (Chambers and Juraf</context>
<context position="8511" citStr="Chambers and Jurafsky (2008)" startWordPosition="1347" endWordPosition="1350">oreference chains; we include Jans et al. (2012)’s option to count over only the longest chains in each document, or to count only over chains of length 5 or greater (long). Count Threshold Because PMI favors low-count events, we add an option to set C(e1, e2) to zero for any e1, e2 for which C(e1, e2) is below some threshold, T, up to 5. 3.2 Predictive Models for Narrative Cloze In order to perform the narrative cloze task, we need a model for predicting the missing narrative event, e, from a chain of observed narrative events, e1 ... en, at insertion point k. The original model, proposed by Chambers and Jurafsky (2008), predicts the event that maximizes unordered pmi, n eˆ = arg max pmi(e, ei) (3) eEV i=1 where V is the set of all observed events (the vocabulary) and C(e1,e2) is symmetric. Two additional models are introduced by Jans et al. (2012) and we use them here, as well. First, the ordered pmi model, eˆ = arg max eEV (4) where C(e1,e2) is asymmetric, i.e., C(e1,e2) counts only cases in which e1 occurs before e2. Second, the bigram probability model: p(eile) (5) where p(e2|e1) = C(e1,e2) C(e1,∗) and C(e1, e2) is asymmetric. Discounting For each model, we add an option for discounting the computed scor</context>
<context position="13997" citStr="Chambers and Jurafsky, 2008" startWordPosition="2296" endWordPosition="2299"> 0 1 3 all yes N/A 5 3 5 5 long no N/A 6 0 3 4 longest yes N/A 7 all 1 3 all N/A no 8 3 5 5 long N/A no 9 all 1 5 all N/A no 10 all 1 3 all N/A yes 11 3 5 5 long N/A yes 12 all 1 5 all N/A yes Figure 3: Parameter settings corresponding to each model in Fig 2. chain. For example, if a chain contains ten events, three of which are “script-relevant,” then three cloze tests will be generated, each containing nine “observed” events. Chains with fewer than two events are excluded. In this way, we generate a total of 2,273 cloze tests. Scoring We employ three different scoring metrics: average rank (Chambers and Jurafsky, 2008), mean reciprocal rank, and recall at 50 (Jans et al., 2012). Baseline The baseline we use for the narrative cloze task is to rank events by frequency. This is the “unigram model” employed by Pichotta and Mooney (2014), a competitive baseline on this task. “A long time ago when I was still in college, my family decided to take me out for pizza on my birthday. We decided to try the new location for a favorite pizza chain of ours. It was all adults and there were about 8 of us, so we ordered 3 large pizzas. We got to chatting and soon realized that the pizzas should’ve been ready quite a bit ago</context>
</contexts>
<marker>Chambers, Jurafsky, 2008</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings ofACL-08: HLT, pages 789–797, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Unsupervised learning of narrative schemas and their participants.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP,</booktitle>
<pages>602--610</pages>
<institution>Suntec, Singapore. Association for Computational Linguistics.</institution>
<contexts>
<context position="1511" citStr="Chambers and Jurafsky, 2009" startWordPosition="208" endWordPosition="211">ze” task (Chambers and Jurafsky, 2008). 1 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn the famous “restaurant script” (Schank and Abelson, 1977) by applying the aforementioned narrative chain learning methods to a specialized corpus of dinner narra</context>
<context position="4123" citStr="Chambers and Jurafsky, 2009" startWordPosition="605" endWordPosition="608">im, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.” To learn narrative chains from text, Chambers and Jurafsky extract chains of narrative events linked by a common coreferent within a document. For example, the sentence “John drove to the </context>
</contexts>
<marker>Chambers, Jurafsky, 2009</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2009. Unsupervised learning of narrative schemas and their participants. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 602–610, Suntec, Singapore. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nathanael Chambers</author>
<author>Dan Jurafsky</author>
</authors>
<title>Templatebased information extraction without the templates.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>976--986</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Portland, Oregon, USA.</location>
<contexts>
<context position="3323" citStr="Chambers and Jurafsky (2011)" startWordPosition="486" endWordPosition="489">lar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for </context>
</contexts>
<marker>Chambers, Jurafsky, 2011</marker>
<rawString>Nathanael Chambers and Dan Jurafsky. 2011. Templatebased information extraction without the templates. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 976–986, Portland, Oregon, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Ward Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word association norms, mutual information, and lexicography.</title>
<date>1990</date>
<booktitle>Computational linguistics,</booktitle>
<pages>16--1</pages>
<contexts>
<context position="3751" citStr="Church and Hanks (1990)" startWordPosition="551" endWordPosition="554">y exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by Chambers and Jurafsky (2008), a narrative chain is “a partially ord</context>
</contexts>
<marker>Church, Hanks, 1990</marker>
<rawString>Kenneth Ward Church and Patrick Hanks. 1990. Word association norms, mutual information, and lexicography. Computational linguistics, 16(1):22–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine De Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating typed dependency parses from phrase structure parses.</title>
<date>2006</date>
<booktitle>In Proceedings of LREC,</booktitle>
<volume>6</volume>
<pages>449--454</pages>
<marker>De Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine De Marneffe, Bill MacCartney, Christopher D Manning, et al. 2006. Generating typed dependency parses from phrase structure parses. In Proceedings of LREC, volume 6, pages 449–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bram Jans</author>
<author>Steven Bethard</author>
<author>Ivan Vuli´c</author>
<author>MarieFrancine Moens</author>
</authors>
<title>Skip n-grams and ranking functions for predicting script events.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>336--344</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Avignon, France.</location>
<marker>Jans, Bethard, Vuli´c, Moens, 2012</marker>
<rawString>Bram Jans, Steven Bethard, Ivan Vuli´c, and MarieFrancine Moens. 2012. Skip n-grams and ranking functions for predicting script events. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 336–344, Avignon, France. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Dirt - discovery of inference rules from text.</title>
<date>2001</date>
<booktitle>In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>323--328</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="3861" citStr="Lin and Pantel (2001)" startWordPosition="569" endWordPosition="572">evant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Dirt - discovery of inference rules from text. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pages 323– 328. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher D Manning</author>
<author>Mihai Surdeanu</author>
<author>John Bauer</author>
<author>Jenny Finkel</author>
<author>Steven J Bethard</author>
<author>David McClosky</author>
</authors>
<title>The Stanford CoreNLP natural language processing toolkit.</title>
<date>2014</date>
<booktitle>In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>55--60</pages>
<contexts>
<context position="10103" citStr="Manning et al., 2014" startWordPosition="1620" endWordPosition="1623">arrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e&apos; whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit stories about their terrible restaurant experiences. For an example story, see Figure 1. To process the raw data, we stripped all HTML and other non-story content from each file and processed the remaining text with the Stanford CoreNLP pipeline version 3.3.1 (Manning et al., 2014). Of the 237 stories obtained, we manually filtered out 94 stories that were “off-topic” (e.g., letters to the webmaster, dinners not at restaurants), leaving a total of 143 stories. The average story length is 352 words. 4.1 Annotation For the purposes of evaluation only, we hired four undergraduates to annotate every non-copular verb in each story as either corresponding to an event “related to the experience of eating in a restaurant” (e.g., ordered a steak), “unrelated to the experience of eating in a restaurant” (e.g., answered the phone), or uncertain. We used the WebAnno platform for an</context>
</contexts>
<marker>Manning, Surdeanu, Bauer, Finkel, Bethard, McClosky, 2014</marker>
<rawString>Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 55–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Walker Orr</author>
<author>Prasad Tadepalli</author>
<author>Janardhan Rao Doppa</author>
<author>Xiaoli Fern</author>
<author>Thomas G Dietterich</author>
</authors>
<title>Learning scripts as hidden markov models.</title>
<date>2014</date>
<booktitle>In TwentyEighth AAAI Conference on Artificial Intelligence.</booktitle>
<contexts>
<context position="2684" citStr="Orr et al. (2014)" startWordPosition="391" endWordPosition="394">hods to a specialized corpus of dinner narratives we compile from the website “Dinners from Hell.” Our results suggest that applying these techniques to a domain-specific dataset may be reasonable way to learn domain-specific scripts. 2 Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by eliciting event sequence descriptions (ESDs) from humans to which they apply multiple sequence alignment (MSA) to yield one global structure per script. (Orr et al. (2014) learn similar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. Fo</context>
</contexts>
<marker>Orr, Tadepalli, Doppa, Fern, Dietterich, 2014</marker>
<rawString>J Walker Orr, Prasad Tadepalli, Janardhan Rao Doppa, Xiaoli Fern, and Thomas G Dietterich. 2014. Learning scripts as hidden markov models. In TwentyEighth AAAI Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Patrick Pantel</author>
<author>Deepak Ravichandran</author>
</authors>
<title>Automatically labeling semantic classes.</title>
<date>2004</date>
<booktitle>In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLTNAACL 2004: Main Proceedings,</booktitle>
<pages>321--328</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Boston, Massachusetts, USA.</location>
<contexts>
<context position="9225" citStr="Pantel and Ravichandran (2004)" startWordPosition="1475" endWordPosition="1478"> i=1 where V is the set of all observed events (the vocabulary) and C(e1,e2) is symmetric. Two additional models are introduced by Jans et al. (2012) and we use them here, as well. First, the ordered pmi model, eˆ = arg max eEV (4) where C(e1,e2) is asymmetric, i.e., C(e1,e2) counts only cases in which e1 occurs before e2. Second, the bigram probability model: p(eile) (5) where p(e2|e1) = C(e1,e2) C(e1,∗) and C(e1, e2) is asymmetric. Discounting For each model, we add an option for discounting the computed scores. In the case of the two PMI-based models, we use the discount score described in Pantel and Ravichandran (2004) and used by Chambers and Jurafsky (2008). For the bigram probability model, this PMI discount score would be inappropriate, so we instead use absolute discounting. Document Threshold We include a document threshold parameter, D, that ensures that, in any narrative cloze test, any event e that was observed during training in fewer than D distinct documents will receive a worse score (i.e. be ranked behind) any event e&apos; whose count meets the document threshold. 4 Dataset: Dinners From Hell The source of our data for this experiment is a blog called “Dinners From Hell”2 where readers submit stor</context>
</contexts>
<marker>Pantel, Ravichandran, 2004</marker>
<rawString>Patrick Pantel and Deepak Ravichandran. 2004. Automatically labeling semantic classes. In Daniel Marcu Susan Dumais and Salim Roukos, editors, HLTNAACL 2004: Main Proceedings, pages 321–328, Boston, Massachusetts, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karl Pichotta</author>
<author>Raymond Mooney</author>
</authors>
<title>Statistical script learning with multi-argument events.</title>
<date>2014</date>
<booktitle>In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>220--229</pages>
<institution>Gothenburg, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="1558" citStr="Pichotta and Mooney, 2014" startWordPosition="216" endWordPosition="219">uction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For many specialized applications, however, knowledge of a few relevant scripts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn the famous “restaurant script” (Schank and Abelson, 1977) by applying the aforementioned narrative chain learning methods to a specialized corpus of dinner narratives we compile from the website “Dinners from</context>
<context position="4170" citStr="Pichotta and Mooney, 2014" startWordPosition="613" endWordPosition="617">ngs of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). It is from this body of work that we borrow techniques to apply to the Dinners from Hell dataset. As defined by Chambers and Jurafsky (2008), a narrative chain is “a partially ordered set of narrative events that share a common actor,” where a narrative event is “a tuple of an event (most simply a verb) and its participants, represented as typed dependencies.” To learn narrative chains from text, Chambers and Jurafsky extract chains of narrative events linked by a common coreferent within a document. For example, the sentence “John drove to the store where he bought some ice cream.” would ge</context>
<context position="5473" citStr="Pichotta and Mooney (2014)" startWordPosition="828" endWordPosition="831">ubj) followed by (BUY, nsubj). Over these extracted chains of narrative events, pointwise mutual information (PMI) is computed between all pairs of events. These PMI scores are then used to predict missing events from such chains, i.e. the narrative cloze evaluation. Jans et al. (2012) expand on this approach, introducing an ordered PMI model, a bigram probability model, skip n-gram counting methods, coreference chain selection, and an alternative scoring metric (recall at 50). Their bigram probability model outperforms the original PMI model on the narrative cloze task under many conditions. Pichotta and Mooney (2014) introduce an extended notion of narrative event that includes information about subjects and objects. They also introduce a competitive “unigram model” as a baseline for the narrative cloze task. To learn the restaurant script from our dataset, we implement the models of Chambers and Jurafsky (2008) and Jans et al. (2012), as well as the unigram baseline of Pichotta and Mooney (2014). To evaluate our success in learning the restaurant script, we perform a modified version of the narrative cloze task, predicting only verbs that we annotate as “restaurant script-relevant” and comparing the perf</context>
<context position="14215" citStr="Pichotta and Mooney (2014)" startWordPosition="2334" endWordPosition="2337">s corresponding to each model in Fig 2. chain. For example, if a chain contains ten events, three of which are “script-relevant,” then three cloze tests will be generated, each containing nine “observed” events. Chains with fewer than two events are excluded. In this way, we generate a total of 2,273 cloze tests. Scoring We employ three different scoring metrics: average rank (Chambers and Jurafsky, 2008), mean reciprocal rank, and recall at 50 (Jans et al., 2012). Baseline The baseline we use for the narrative cloze task is to rank events by frequency. This is the “unigram model” employed by Pichotta and Mooney (2014), a competitive baseline on this task. “A long time ago when I was still in college, my family decided to take me out for pizza on my birthday. We decided to try the new location for a favorite pizza chain of ours. It was all adults and there were about 8 of us, so we ordered 3 large pizzas. We got to chatting and soon realized that the pizzas should’ve been ready quite a bit ago, so we called the waitress over and she went to check on our pizzas. She did not come back. We waited about another 10 minutes, then called over another waitress, who went to check on our pizzas and waitress. It now b</context>
<context position="18385" citStr="Pichotta and Mooney (2014)" startWordPosition="3052" endWordPosition="3056">ents in the “we” coreference chain in Figure 1 (bolded). The three events selected are boxed and highlighted in blue. The bigram model selects the “deciding” event (selecting restaurant) and the “having” event (having pizza), both reasonable components of the restaurant script. The third event selected is “having room,” which is not part of the restaurant script. This mistake illustrates a weakness of the narrative chains model; without considering the verb’s object, the model is unable to distinguish “have pizza” from “have room.” Incorporating object information in future experiments, as in Pichotta and Mooney (2014), might resolve this issue, although it could introduce sparsity problems. 6 Conclusion In this work, we describe the collection and annotation of a corpus of natural descriptions of restaurant visits from the website “Dinners from Hell.” We use this dataset in an attempt to learn the restaurant script, using a variety of related methods for learning narrative chains and evaluating on the narrative cloze task. Our results suggest that it may be possible in general to use these methods on domainspecific corpora in order to learn particular scripts from a pre-specified domain, although further e</context>
</contexts>
<marker>Pichotta, Mooney, 2014</marker>
<rawString>Karl Pichotta and Raymond Mooney. 2014. Statistical script learning with multi-argument events. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 220–229, Gothenburg, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michaela Regneri</author>
<author>Alexander Koller</author>
<author>Manfred Pinkal</author>
</authors>
<title>Learning script knowledge with web experiments.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>979--988</pages>
<institution>Uppsala, Sweden. Association for Computational Linguistics.</institution>
<contexts>
<context position="2458" citStr="Regneri et al. (2010)" startWordPosition="356" endWordPosition="359">ts may be more useful than knowledge of many irrelevant scripts. With this scenario in mind, we attempt to learn the famous “restaurant script” (Schank and Abelson, 1977) by applying the aforementioned narrative chain learning methods to a specialized corpus of dinner narratives we compile from the website “Dinners from Hell.” Our results suggest that applying these techniques to a domain-specific dataset may be reasonable way to learn domain-specific scripts. 2 Background Previous work in the automatic induction of scripts or script-like structures has taken a number of different approaches. Regneri et al. (2010) attempt to learn the structure of specific scripts by eliciting event sequence descriptions (ESDs) from humans to which they apply multiple sequence alignment (MSA) to yield one global structure per script. (Orr et al. (2014) learn similar structures in a probabilistic framework with Hidden Markov Models.) Although Regneri et al. (2010), like us, are concerned with learning pre-specified scripts, our approach is different in that we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learnin</context>
</contexts>
<marker>Regneri, Koller, Pinkal, 2010</marker>
<rawString>Michaela Regneri, Alexander Koller, and Manfred Pinkal. 2010. Learning script knowledge with web experiments. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 979–988, Uppsala, Sweden. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Schank</author>
<author>Robert Abelson</author>
</authors>
<title>Scripts, plans, goals and understanding: An inquiry into human knowledge structures. Lawrence Erlbaum Associates,</title>
<date>1977</date>
<location>Hillsdale, NJ.</location>
<contexts>
<context position="1166" citStr="Schank and Abelson, 1977" startWordPosition="156" endWordPosition="159">ocus of many recent works. In this paper, we employ a variety of these methods to learn Schank and Abelson’s canonical restaurant script, using a novel dataset of restaurant narratives we have compiled from a website called “Dinners from Hell.” Our models learn narrative chains, script-like structures that we evaluate with the “narrative cloze” task (Chambers and Jurafsky, 2008). 1 Introduction A well-known theory from the intersection of psychology and artificial intelligence posits that humans organize certain kinds of general knowledge in the form of scripts, or common sequences of events (Schank and Abelson, 1977). Though many early AI systems employed hand-encoded scripts, more recent work has attempted to induce scripts with automatic and scalable techniques. In particular, several related techniques approach the problem of script induction as one of learning narrative chains from text corpora (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Jans et al., 2012; Pichotta and Mooney, 2014). These statistical approaches have focused on open-domain script acquisition, in which a large number of scripts may be learned, but the acquisition of any particular set of scripts is not guaranteed. For ma</context>
</contexts>
<marker>Schank, Abelson, 1977</marker>
<rawString>Roger Schank and Robert Abelson. 1977. Scripts, plans, goals and understanding: An inquiry into human knowledge structures. Lawrence Erlbaum Associates, Hillsdale, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beth M Sundheim</author>
</authors>
<title>Third message understanding evaluation and conference (muc-3): Phase 1 status report.</title>
<date>1991</date>
<booktitle>In Proceedings of the Message Understanding Conference.</booktitle>
<contexts>
<context position="3505" citStr="Sundheim, 1991" startWordPosition="517" endWordPosition="518">at we apply unsupervised techniques to scenario-specific collections of natural, pre-existing texts. Note that while the applicability of our approach to script learning may appear limited to domains for which a corpus conveniently already exists, previous work demonstrates the feasibility of assembling such a corpus by automatically retrieving relevant documents from a larger collection. For example, Chambers and Jurafsky (2011) use information retrieval techniques to gather a small number of bombing-related documents from the Gigaword corpus, which they successfully use to learn a MUCstyle (Sundheim, 1991) information extraction tem205 Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 205–210, Denver, Colorado, June 4–5, 2015. plate for bombing events. Following the work of Church and Hanks (1990) in learning word associations via mutual information, and the DIRT system introduced by Lin and Pantel (2001), Chambers and Jurafsky (2008) propose a PMI-based system for learning script-like structures called narrative chains. Several followup papers introduce variations and improvements on this original model for learning narrative chains (Chambers </context>
</contexts>
<marker>Sundheim, 1991</marker>
<rawString>Beth M. Sundheim. 1991. Third message understanding evaluation and conference (muc-3): Phase 1 status report. In Proceedings of the Message Understanding Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seid Muhie Yimam</author>
<author>Iryna Gurevych</author>
<author>Richard Eckart de Castilho</author>
<author>Chris Biemann</author>
</authors>
<title>Webanno: A flexible, web-based and visually supported system for distributed annotations.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,</booktitle>
<pages>1--6</pages>
<institution>Sofia, Bulgaria. Association for Computational Linguistics.</institution>
<marker>Yimam, Gurevych, de Castilho, Biemann, 2013</marker>
<rawString>Seid Muhie Yimam, Iryna Gurevych, Richard Eckart de Castilho, and Chris Biemann. 2013. Webanno: A flexible, web-based and visually supported system for distributed annotations. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 1–6, Sofia, Bulgaria. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>