<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000004">
<title confidence="0.915212">
Modeling User Satisfaction with Hidden Markov Models
</title>
<note confidence="0.488429666666667">
Klaus-Peter Engelbrecht, Florian Gödde, Felix Hartard, Hamed Ketabdar, Sebastian
Möller
Deutsche Telekom Laboratories, Quality &amp; Usability Lab, TU Berlin,
</note>
<author confidence="0.56604">
Ernst-Reuter-Platz 7, 10587 Berlin, Germany
</author>
<affiliation confidence="0.431923">
{Klaus-Peter.Engelbrecht,Florian.Goedde,
</affiliation>
<email confidence="0.8934005">
Hamed.Ketabdar,Sebastian.Moeller}@telekom.de
Felix.Hartard@Berlin.de
</email>
<sectionHeader confidence="0.993873" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999051545454545">
Models for predicting judgments about
the quality of Spoken Dialog Systems
have been used as overall evaluation
metric or as optimization functions in
adaptive systems. We describe a new
approach to such models, using Hidden
Markov Models (HMMs). The user’s
opinion is regarded as a continuous
process evolving over time. We present
the data collection method and results
achieved with the HMM model.
</bodyText>
<sectionHeader confidence="0.998979" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999692950819672">
Spoken Dialog Systems (SDSs) are now widely
used, and are becoming more complex as a result
of the increased solidity of advanced techniques,
mainly in the realm of natural language
understanding (Steimel et al. 2008). At the same
time, the evaluation of such systems increasingly
demands for testing the entire system, as
components for speech recognition, language
understanding and dialog management are
interacting more deeply. For example, the system
might search for web content on the basis of
meaning extracted from an n-best list, and
generate the reply and speech recognition
grammars depending on the content found
(Wootton et al. 2007). The performance of single
components strongly depends on each other
component in this case.
While performance parameters become less
meaningful in such a system, the system’s
overall quality, which can only be measured by
asking the user (Jekosch 2005), gains interest for
the evaluation. Typically, users fill out
questionnaires after the interaction, which cover
various perceptional dimensions such as
efficiency, dialog smoothness, or the overall
evaluation of the system (Hone and Graham,
2001; ITU-T Rec. P.851, 2003; Möller 2005a).
Judgments of the system’s overall quality can be
used to compare systems with respect to a single
measure, which however comprises all relevant
aspects of the interaction. Thus, the complexity
of the evaluation task is reduced.
In addition, user simulation is increasingly
used to address the difficulty of foreseeing all
possible problems a user might encounter with
the system (e.g. Ai and Weng, 2008; Engelbrecht
et al., 2008a; Chung, 2004; López-Cózar et al.,
2003). In order to evaluate results from such
simulations, some approaches utilize prediction
models of user judgments (e.g. Ai and Weng,
2008; Engelbrecht et al., 2008a).
Currently, prediction models for user
judgments are based on the PARADISE
framework introduced by Walker et al. (1997).
PARADISE assumes that user satisfaction
judgments describe the overall quality of the
system, and are causally related to task success
and dialog costs, i.e. efficiency and quality of the
dialog. Therefore, a linear regression function
can be trained with interaction parameters
describing dialog costs and task success as
predictors, and satisfaction ratings as the target.
The resulting equation can then be used to
predict user satisfaction with unseen dialogs.
In follow-up studies, it could be shown that
such models are to some degree generalizable
(Walker et al., 2000). However, also limitations
of the models in predicting judgments for other
user groups, or for systems with different levels
of ASR performance, were reported (Walker et
al., 1998). In the same study, prediction
</bodyText>
<note confidence="0.71">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 170–177,
</note>
<affiliation confidence="0.662581">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.998506">
170
</page>
<bodyText confidence="0.999299066666667">
functions for user satisfaction were proposed to
serve as optimization function in a system
adapting its dialog strategy during the interaction.
This idea is taken up by Rieser and Lemon
(2008).
The prediction accuracy of PARADISE
functions typically lies around an R2 of 0.5,
meaning that 50% of the variance in the
judgments is explained by the model. While this
number is not absolutely satisfying, it could be
shown that mean values for groups of dialogs
(e.g. with a specific system configuration) can be
predicted more accurately than single dialogs
with the same models (Engelbrecht and Möller,
2007). Low R2 for the predictions of ratings of
individual dialogs seems to be due to inter-rater
differences at least to some degree. Such
differences have been described, and may
concern the actual perception of the judged issue
(Guski, 1999), or the way the perception is
described by the participant (Okun and Weir,
1990; Engelbrecht et al., 2008b)
We have tested the PARADISE framework
extensively, using different classifier models and
interaction parameters. Precise and general
models are hard to achieve, even if the set of
parameters describing the interaction is widely
extended (Möller et al., 2008). In an effort to
improve such prediction models, we developed
two ideas:
</bodyText>
<listItem confidence="0.896653416666667">
• Predict the distribution of ratings which
can be expected for a representative group
of users given the same stimulus. This
takes into account that in most cases the
relevant user characteristics determining
the judgment cannot be tracked, or even
are unknown.
• Consider the time relations between
events by modeling user opinion as a
variable evolving over the course of the
dialog. This way, time relations like co-
occurrence of events, which affect quality
</listItem>
<bodyText confidence="0.927791545454545">
perception, attention, or memory can be
modeled most effectively.
In this paper, we present a new modeling
approach considering these ideas. In Section 2,
we introduce the topology of the model.
Following this, we report how training data for
the model were obtained from user tests in
Section 3. Evaluation results are presented in
Section 4 and discussed in Section 5, before we
conclude with some remarks on follow-up
research.
</bodyText>
<sectionHeader confidence="0.720835" genericHeader="method">
2 Modeling Judgments with HMMs
</sectionHeader>
<bodyText confidence="0.999872528301887">
Hidden Markov Models (HMMs) are often used
for classifying sequential stochastic processes,
e.g. in computational linguistics or bio-
informatics. An HMM models a sequence of
events as a sequence of states, in which each
state emits certain symbols with some probability.
In addition, the transitions between states are
probabilistic. The model is defined by a set of
state symbols, a set of emission symbols, the
probabilities for the initial state, the state
transition matrix, and the emission matrix. The
transition matrix contains the probabilities for
transitions from each state to each other state or
itself. The emission matrix contains the
probabilities for each emission symbol to occur
at each state.
While the sequence of emissions can be
observed, the state sequence is hidden. However,
given an emission sequence, standard algorithms
defined for the HMM allow to calculate the
probability of each state at each point in the
sequence. The probability for the model to be in
a state is dependent on the previous state and the
emissions observed at the current state.
As illustrated by Figure 1, the development of
the users’ opinions can be modelled as an HMM.
The user judgment about the dialog is modelled
as states, each state representing a specific
judgment (think of it as “emotional states”). A
prediction is made at each dialog turn. In the
model depicted, the user judgment can either be
“bad” or “good”. Each judgment has a
probabilistic relation to the current events in the
dialog. In the picture, the events are described in
the form of understanding errors and
confirmation types, i.e. there are two features
which can take a number of different values,
each with a certain probability.
Although the judgments do not “emit” the
events at each turn (the causal relation is
opposite), the probabilistic relation between them
can be captured and evaluated with the HMM
and the associated algorithms.
Apart from the dialog events, the current
judgment is also determined by the previous
judgment. For example, we expect that the
judgments are varying smoothly, i.e. the
probability for a transition becomes lower with
increasing (semantic) distance between the state
labels.
Although events in previous turns cannot
impact the current judgment given this model
topology, it is possible to incorporate dialog
</bodyText>
<page confidence="0.990675">
171
</page>
<bodyText confidence="0.994222321428571">
history by creating features with a time lag. E.g.,
a feature could represent the understanding error
in the previous turn. Also, simultaneity of
different events affecting the quality perception
can be evaluated by calculating probabilities for
each judgment given the observed combination
of features. If the features are interacting (i.e. the
probability of one feature changes in dependence
of another feature), this is modelled by directly
specifying the emission probabilities for each
combination of features. We call this a layer of
emissions. Additional layers with other features
can be created. In this case, the likelihood of
each judgment given probabilities from each
layer can be calculated by multiplication of the
probabilities from each layer.
For the calculation of state probabilities, we
can use forward recursion (Rabiner, 1989). The
algorithm proceeds through the observed
sequence, and at each step calculates the
probability for each state given the probabilities
of the observation, the probabilities of each state
at the previous step, and the transition
probabilities.
Figure 1. Topology of an HMM to model user
judgments (“good” or “bad”) in their
probabilistic relation to dialog events (error and
confirmation strategy) and the previous rating.
</bodyText>
<sectionHeader confidence="0.988236" genericHeader="method">
3 Data Collection
</sectionHeader>
<bodyText confidence="0.983689607843137">
In order to train the model, data is needed in
which the association between dialog events and
judgments at each turn is represented. Therefore,
we conducted an experiment in which users
interacted with an SDS and judged the
interaction after each dialog turn. In addition, all
users followed the same five dialogs, which was
possible by using the Wizard-of-Oz technique
(Fraser and Gilbert 1991) and relatively strict
scenario descriptions.
As the interactions were predefined, care was
taken that a representative sample of relevant
usability problems were passed in the
interactions. Such problems were collected in a
brainstorming session, and comprise
understanding errors, task failure, prompt
wording issues (such as incomplete information),
missing control options, or not being able to
input one of the criteria specified in the scenario
description. As further features of the dialog,
which might impact the quality judgment, the
number of concepts which can be specified, as
well as the confirmation strategy, should vary
along the interactions.
We then designed a dialog system with a
consistent strategy, which however allowed for
the quality-relevant issues mentioned above to
occur in the dialogs. The system was derived
from the BoRIS restaurant information system
(Möller 2005b), which enables the search of
restaurants in the German city Bochum
according to the criteria day, time, location, type
of cuisine and pricing. For our system, we
simplified the dialog strategy to system initiative,
however, at the first turn, the users could say
both day and time. If both were recognized,
explicit confirmation was requested. If just one
constraint was understood, confirmation was im-
plicit. The last constraint specified (pricing) was
not confirmed, and instead the restaurant
information was output directly. If no restaurant
was found according to the specified criteria, the
user could change one of the constraints. Table 1
shows an example of a dialog with the system.
We also reformulated some prompts to fit the
changed dialog strategy, and partly to
incorporate potential wording problems. Prompts
were recorded with a non-professional speaker,
S: Welcome to the telephone-based restaurant
information system for Bochum. When would you
like to eat?
</bodyText>
<table confidence="0.996263153846154">
U: Tomorrow night.
S: So you want to eat tonight?
U: No.
S: On which day would you like to eat?
....
U: Medium pricing
S: Unfortunately, I could not find a restaurant
matching your query. Would you like to change
time, location, cuisine or pricing?
U: Yes, cuisine.
S: Which type of cuisine are you searching for?
U: Italian.
S: The possible restaurants are: {name, address}
</table>
<tableCaption confidence="0.794383666666667">
Table 1. Example dialog with the BoRIS
restaurant information system, version as used
in the experiment.
</tableCaption>
<page confidence="0.993646">
172
</page>
<bodyText confidence="0.999566666666667">
using high-quality audio equipment. During the
interactions, the wizard simply replayed the
prompt foreseen at the current state of the
predefined dialog script. In addition to the
foreseen prompts, the wizard had at hand no-
input and help prompts in case the user would
behave unexpectedly.
25 users (13 females, 12 males), recruited
from the campus, but not all students,
participated in the experiment. Participants were
aged between 20 and 46 years (M=26.5;
STD=6.6). Ratings were given on a 5-point scale,
where the points were labeled “bad”, “poor”,
“fair”, “good”, and “excellent”. Ratings were
input through a number pad attached to the scale.
Each participant rehearsed the procedure with a
test dialog. Before the experiment, all users filled
out a questionnaire measuring their technical
affinity.
As the data collected in the described experi-
ment are all needed to train the prediction model
for as many combinations of feature values as
possible, we conducted a second experiment to
generate test data. For this test, we asked 17 per-
sons from our lab to conduct two dialogs with
the system mock-up. The test setup was the same
as in the previous experiment, except that new
dialogs were created without particular
requirements or restrictions.
In both experiments, not all users behaved as
we hoped. Therefore, not all of the predefined
dialog scripts were judged by all participants
(N=15...23 for training corpus, N=9...13 for test
</bodyText>
<table confidence="0.96449275">
Feature Values
understanding PA:PA (partially correct)
errors PA:FA (failed)
PA:IC (incorrect)
confirmation explicit
strategy implicit
none
system speech ask for 2 constraints
</table>
<tableCaption confidence="0.965231466666667">
act ask for 1 constraint
ask for selection of a constraint
provide info
user speech act provide info
repeat info
confirm
meta communication
no-input
contextual manner
appropriateness quality
(Grice’s quantity
maxims) relevance
task success success
failure
Table 2. Annotated dialog features.
</tableCaption>
<bodyText confidence="0.996248363636364">
corpus; N: number of valid dialogs). For one
dialog script in the training corpus, the deviating
interactions were all equal (N=9), so
distributions of ratings per turn could be
calculated for comparison with the predicted
distributions for this dialog. For the training and
calculation of initial state probabilities, all dia-
logs in the training corpus were used.
The model derived from the data includes five
possible states (one for each rating). For a list of
features annotated in the dialogs see Figure 2.
</bodyText>
<sectionHeader confidence="0.999958" genericHeader="method">
4 Results
</sectionHeader>
<bodyText confidence="0.990236">
In order to evaluate the modeling approach, we
first searched for the best model given the
training data from the first experiment. We then
applied this model to the test data from the
second experiment in order to evaluate the model
accuracy given unseen data. Afterwards, we
examined if another model trained on the
training set can predict the test set better, i.e. we
“optimized” the model on the test data. Finally,
we cross-check how well the model optimized on
the test data performs on the training data, which
gives a glimpse at how much the model is biased
towards the test data.
As the criterion for the optimization, we deter-
mined the mean squared error (MSE), and
averaged across all dialog script in the corpus on
which the model was optimized. For each dialog
script, all 5 probabilities (ratings “bad” to
“excellent”) at each dialog turn were taken into
account, i.e. the squared prediction errors were
added. If rate is the rating, then
[pemp (rate) − ppred (rate)
</bodyText>
<sectionHeader confidence="0.340407" genericHeader="method">
MSEdial
n
</sectionHeader>
<bodyText confidence="0.999960266666667">
As this measure, in the particular way we
applied it here, is not easily comparable to other
results, we add two pictures illustrating the
accuracy represented either by a rather low or by
a rather high MSE. In addition, we report the
mean absolute error (MAEmax) of the models in
predicting the most likely rating at each state
(mean rating if two ratings with equal probability)
and the baseline performance when the
unconditional distribution of ratings is predicted.
We first optimized a model on the training
data, meaning that we selected parameters,
trained the HMM with these parameters on the
training data and then predicted results for all 6
dialog scripts contained in the training set (top of
</bodyText>
<equation confidence="0.7504746">
n 5
turn rate
= 1
∑ ∑
=1
</equation>
<page confidence="0.996946">
173
</page>
<bodyText confidence="0.992462272727273">
Table 3). The optimized model was chosen as the
one returning the smallest MSE (mean of all
tasks). The best model included understanding
errors interacting with confirmation type at each
turn, and interacting with task success. As we
analyzed the prediction results, we found that
whenever the system changed from asking two
constraints at a time to just one (which is done in
order to avoid multiple errors in a row), the
predictions were too positive. We therefore
introduced a new feature, which is annotated
whenever the system asks for a single constraint
which has been asked in a more complex
question before (“dummy”). In the model
optimized on training data, this parameter was
included on a separate feature layer. That is, this
feature impacts quality perception independent
of the other features’ values.
We then used this model to predict the test
data collected in the second experiment (top of
Table 4). As expected, the MSE clearly increases;
however, this was partly due to the difference in
the sample of participants. As in the second
experiment participants were recruited from our
lab, their technical affinity was relatively high.
Therefore, we retrained the HMM with only
those 50% of the users from the training set who
got the highest score on the technical affinity
questionnaire. With this model, the prediction of
test data improved.
In a next step, we optimized the model on the
test set meaning that we searched for the
parameter combination achieving the best result
on the two test dialogs. However, the model was
still trained on the training data from the first
experiment. As expected, the MSE could be
improved. However, only minor changes in the
feature configuration are necessary: Still, errors
and confirmation type are interacting on the
same layer. However, task success is included as
independent variable on a second layer, and
instead, the error in the previous turn determines
the impact of errors and confirmation on the
ratings. Again, we tested if the prediction can be
</bodyText>
<table confidence="0.8623173">
Predicted: training dialogs Dial 1 Dial 2 Dial 3 Dial 4 Dial 5 Dial 6 Mean (basel.)
Optimized on training Layer 1: Error, Confirm, Task Success
dialogs Layer 2: Dummy
MSE: 0.0185 0.0307 0.0166 0.0216 0.0333 0.0477 0.0281 (0.1201)
MAEmax: 0.7000 0.5714 0.2857 0.0556 0.3636 0.3333 0.3849 (0.6167)
Optimized on test dialogs Layer 1: Errors, Errors_lag, Confirmation
Layer 2: TaskSuccess
MSE: 0.0272 0.0358 0.0247 0.0374 0.0400 0.0574 0.0371 (0.1201)
MAEmax: 0.5000 0.4286 0.4286 0.3889 0.4545 0.3333 0.4223 (0.6167)
Number of valid dialogs (N): 22 15 23 17 17 9
</table>
<tableCaption confidence="0.899329666666667">
Table 3. Evaluation of predictions of training dialogs (mean squared error and mean absolute error
in predicting the most probable state at each turn). Baseline results are given in brackets. The feature
combinations with which results were obtained are also reported.
</tableCaption>
<table confidence="0.980486470588235">
Predicted: test dialogs Dial 1 Dial 2 Mean (baseline)
Optimized on training dialogs Layer 1: Error, Confirm, Task Success
Layer 2: Dummy
MSE: 0.1039 0.0429 0.0734 (0.1583)
MAEmax: 0.4444 0.6250 0.5347 (0.6944)
Optimized on training dialogs (tah) Layer 1: Error, Confirm, Task Success
Layer 2: Dummy
MSE: 0.0957 0.0387 0.0672 (0.1636)
MAEmax: 0.3333 0 0.1667 (0.6944)
Optimized on test dialogs (rf) Layer 1: Errors, Errors_lag, Confirm
Layer 2: TaskSuccess
MSE: 0.0789 0.0349 0.0569 (0.1583)
MAEmax: 0.4444 0.6250 0.5347 (0.6944)
Optimized on test dialogs (tah; rf) Layer 1: Errors, Confirm
MSE: 0.0860 0.0374 0.0617 (0.1636)
MAEmax: 0.3333 0 0.1667 (0.6944)
Number of valid dialogs (N): 9 13
</table>
<tableCaption confidence="0.997162">
Table 4. Evaluation of predictions of training dialogs (tah=model trained on users with high
technical affinity; rf=user speech act feature exclude from analysis)
</tableCaption>
<page confidence="0.998065">
174
</page>
<bodyText confidence="0.999845952380952">
improved by considering differences between the
users’ technical affinity. However, repeating the
procedure for only those users with high
technical affinity did not improve the result this
time. Concerning the parameters, error and
confirmation type were confirmed to be
significant predictors of quality judgments. The
dummy parameter created to improve the
accuracy on training data was not proven useful
for the prediction of the test set ratings.
In order to cross-check the validity of the
model optimized on test data, we finally
predicted the ratings of the 6 dialogs from the
training set with the same model (bottom of
Table 3). As can be seen, the prediction is worse
than that from the model optimized on the
training set. However, the quality of the
prediction is still reasonable, showing that the
two datasets do not demand for completely
different models. All predictions are above the
baseline.
</bodyText>
<sectionHeader confidence="0.998898" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.96099485">
In the previous section, we presented results
achieved with our models in terms of MSE. In
order to gain meaning to the values of MSE, we
added the mean absolute error of predicting the
most probable judgment at each state. A closer
look at the relation between MSE and MAE,,ax
reveals that both measures are not strictly
correlated (see e.g. the first two models in Table
4). While the MSE measures the distance at each
measurement point in the distribution, the
MAE,,ax is a rough indicator of the similarity of
the shape of the predicted and observed
probability curves. The results for MAE,,ax are
promising, as predictions of test data are in the
range of predictions of training data and better
than the baseline. Also, predictions made from
participants with high technical affinity achieve
better results on the test data in all cases, which
was expected, but not found for the MSE results.
Figure 2 presents examples of prediction
results graphically. We chose one example of an
average, and one of a relatively bad prediction, to
allow extrapolation to other results presented.
The pictures show that even a relatively high
MSE corresponds to a fair quality of the
prediction. The probability curves are mostly
similar, mainly smoother than the observed
probability distributions. Sometimes the
predictions are too optimistic, however, usually
the change in judgments is predicted, just not the
extent of this change. We can only hypothesize
Figure 2. Examples of predictions on test
data made with the model, illustrating the
meaning of MSE values. Depicted are two
dialogs (columns) with 9 (left) and 8 (right)
turns (rows). For each turn, the empirical
(solid line) and predicted (dotted line) rating
distributions are given. Left: MSE=0.0957;
N(emp)=9. Right: MSE=0.0349;
N(emp)=13.
</bodyText>
<page confidence="0.997971">
175
</page>
<bodyText confidence="0.999979461538462">
about the reasons for the participants to judge the
respective dialog worse than predicted by the
model. A possible reason is that users more
easily decrease their judgments when the dialog
has a longer history of problematic situations.
According to our data, the users were relatively
forgiving and increased their judgments if the
dialog went well, even if previously errors had
occurred. However, the errors might not really be
forgot, and be reflected in the judgment of later
problems and errors. Unfortunately, for reasons
of data scarcity, the wider dialog history cannot
be considered in the models.
Another source of prediction error might be
the sample size available for the predicted
dialogs. If sample size (N) and MSE values are
compared among the dialogs, it can be observed
that both values are correlated. This might be due
to less smooth probability distribution curves if
few ratings are available at each turn. While the
curves depicted in Figure 2 are sometimes spiky,
with increasing sample size normal distribution
should be more likely. This might to some
degree explain the clearly higher MSE for the test
data predictions despite the relatively small error
in predicting the most probable ratings.
</bodyText>
<sectionHeader confidence="0.999235" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999958677419355">
In this paper, we presented a new approach to the
prediction of user judgments about SDSs, using
HMMs. The approach allows predicting the
users’ judgments at each step of a dialog. In
predicting the distribution of ratings of many
users, the approach takes into account
differences between the users’ judgment
behaviors. This increases the usefulness of the
model for a number of applications. E.g., in
adaptive systems, the decision process can take
into account differences between the users which
cannot be attributed to user characteristics known
to the system. If the model is applied to
automatically generated dialogs, e.g. in the
MeMo workbench (Engelbrecht et al., 2008a), a
more detailed prediction of user satisfaction is
enabled, allowing analysis on a turn-by-turn
basis.
In addition, the approach facilitates the
analysis of models and features affecting the
quality ratings, as results can be compared to the
empirical ratings with more detail. We hope to
gain further insight into the relations between
interaction parameters and user judgments by
running simulations under different assumptions
of relations between these entities.
A drawback of the approach is the generation
of training data. The models presented in this
paper cannot be assumed to be general, and in
particular are lacking important parameters
reflecting the timing in the dialogs. Therefore, as
a next step the acquisition of judgments should
be improved to be less disruptive for the
interaction. In addition, it would be interesting to
find a method for deriving the correct
distributions of ratings at each dialog turn from a
corpus of different dialogs, e.g. by grouping
situations which are comparable. At the moment,
we are also investigating if judgments can be
acquired after the interactions without a loss in
validity.
After all, the results we achieved with the
model suggest that HMMs are suitable for
modeling the users’ quality perception of dialogs
with SDSs. Further research on the topic will
hopefully show if the dialog history has to be
considered to a wider degree than in our present
models.
Concerning dialog features and their relation
to the judgments, the role of understanding errors
in combination with the confirmation type could
be established so far. More rich data are needed
to work towards a general model for judgment
predictions, including all relevant parameters. If
judgments can be acquired after the interactions,
we will be able to easily get the data needed for a
better (and maybe complete) model. In any case,
we are confident that the approach taken will
allow a deeper analysis of the quality judgment
process, which will enable progress by more
analytical methods, such as formulating and
testing hypotheses about this process.
</bodyText>
<sectionHeader confidence="0.998174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.972861666666667">
Hua Ai, Fuliang Weng. 2008. User Simulation as
Testing for Spoken Dialog Systems. Proc. of the 9th
SIGdial Workshop on Discourse and Dialogue,
Columbus, Ohio.
Grace Chung. 2004. Developing a flexible spoken
dialog system using simulation. Proc. of the 42nd
Annual Meeting on Association for Computational
Linguistics, Barcelona, Spain.
Klaus-Peter Engelbrecht, Sebastian Möller. 2007.
Pragmatic Usage of Linear Regression Models for
the Prediction of User Judgments. Proc. of 8th
SIGdial Workshop on Discourse and Dialogue,
Antwerp, Belgium.
Klaus-Peter Engelbrecht, Michael Kruppa, Sebastian
Möller, Michael Quade. 2008a. MeMo Workbench
</reference>
<page confidence="0.985945">
176
</page>
<reference confidence="0.999666102564103">
for Semi-Automated Usability Testing. Proc. of 9th
Interspeech, Brisbane, Australia.
Klaus-Peter Engelbrecht, Sebastian Möller, Robert
Schleicher, Ina Wechsung. 2008b. Analysis of
PARADISE Models for Individual Users of a
Spoken Dialog System. Proc. of ESSV 2008,
Frankfurt/Main, Germany.
Klaus-Peter Engelbrecht, Felix Hartard, Florian
Gödde, Sebastian Möller. 2009. A Closer Look at
Quality Judgments of Spoken Dialog Systems,
submitted to Interspeech 2009.
Norman M. Fraser, G. Nigel Gilbert. 1991.
Simulating speech systems. Computer Speech and
Language, 5:81–99.
Rainer Guski. 1999. Personal and Social Variables as
Co-determinants of Noise Annoyance. Noise &amp;
Health, 3:45-56.
Kate S. Hone, Robert Graham. 2001. Subjective
Assessment of Speech-system Interface Usability.
Proc. of EUROSPEECH, Aalborg, Denmark.
ITU-T Rec. P.851, 2003. Subjective Quality
Evaluation of Telephone Services Based on Spoken
Dialogue Systems, International
Telecommunication Union, Geneva, Switzerland.
Ute Jekosch. 2005. Voice and Speech Quality
Perception. Assessment and Evaluation, Springer,
Berlin, Germany.
Ramón López-Cózar, Ángel de la Torre, José C.
Segura and Antonio J. Rubio. 2003. Assessment of
Dialogue Systems by Means of a New Simulation
Technique. Speech Communication, 40(3):387-
407.
Sebastian Möller. 2005a. Perceptual Quality
Dimensions of Spoken Dialog Systems: A Review
and New Experimental Results, Proc. of Forum
Acusticum, Budapest, Hungary.
Sebastian Möller. 2005b. Quality of Telephone-based
Spoken Dialog Systems. Springer, New York.
Sebastian Möller, Klaus-Peter Engelbrecht, Robert
Schleicher. 2008. Predicting the Quality and
Usability of Spoken Dialogue Services, Speech
Communication, 50:730-744.
Morris A. Okun, Renee M. Weir. 1990. Toward a
Judgment Model of College Satisfaction.
Educational Psychological Review, 2(1):59-76.
Lawrence R. Rabiner. 1989. A tutorial on HMM and
selected applications in speech recognition. Proc.
IEEE, 77(2):257-286.
Verena Rieser, Oliver Lemon. 2008. Automatic
Learning and Evaluation of User-Centered
Objective Functions for Dialogue System
Optimisation. Proc. of LREC&apos;08, Marrakech,
Morocco.
Bernhard Steimel, Oliver Jacobs, Norbert Pfleger,
Sebastian Paulke. 2008. Testbericht VOICE Award
2008: Die besten deutschsprachigen
Sprachapplikationen. Initiative Voice Business,
Bad Homburg, Germany.
Marilyn A. Walker, Diane J. Litman, Candace A.
Kamm, Alicia Abella. 1997. PARADISE: A
Framework for Evaluating Spoken Dialogue
Agents. Proc. of ACL/EACL 35th Ann. Meeting of
the Assoc. for Computational Linguistics, Madrid,
Spain.
Marilyn A. Walker, Diane J. Litman, Candace A.
Kamm, Alicia Abella. 1998. Evaluating Spoken
Dialog Agents with PARADISE: Two Case
Studies. Computer Speech and Language, 12:317-
347.
Marilyn Walker, Candace Kamm, Diane Litman.
2000. Towards Developing General Models of
Usability with PARADISE. Natural Language
Engineering, 6(3-4):363-377.
Craig Wootton, Michael McTear, Terry Anderson.
2007. Utilizing Online Content as Domain
Knowledge in a Multi-Domain Dynamic Dialogue
System. Proc. of Interspeech 2007, Antwerp,
Belgium.
</reference>
<page confidence="0.997701">
177
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.243141">
<title confidence="0.999181">Modeling User Satisfaction with Hidden Markov Models</title>
<author confidence="0.80725">Klaus-Peter Engelbrecht</author>
<author confidence="0.80725">Florian Gödde</author>
<author confidence="0.80725">Felix Hartard</author>
<author confidence="0.80725">Hamed Ketabdar</author>
<affiliation confidence="0.300978">Deutsche Telekom Laboratories, Quality &amp; Usability Lab, TU</affiliation>
<address confidence="0.662105">Ernst-Reuter-Platz 7, 10587 Berlin, Germany</address>
<email confidence="0.928224">Hamed.Ketabdar,Sebastian.Moeller}@telekom.deFelix.Hartard@Berlin.de</email>
<abstract confidence="0.996765583333333">Models for predicting judgments about the quality of Spoken Dialog Systems have been used as overall evaluation metric or as optimization functions in adaptive systems. We describe a new approach to such models, using Hidden Markov Models (HMMs). The user’s opinion is regarded as a continuous process evolving over time. We present the data collection method and results achieved with the HMM model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hua Ai</author>
<author>Fuliang Weng</author>
</authors>
<title>User Simulation as Testing for Spoken Dialog Systems.</title>
<date>2008</date>
<booktitle>Proc. of the 9th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Columbus, Ohio.</location>
<contexts>
<context position="2377" citStr="Ai and Weng, 2008" startWordPosition="340" endWordPosition="343">aires after the interaction, which cover various perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a; Chung, 2004; López-Cózar et al., 2003). In order to evaluate results from such simulations, some approaches utilize prediction models of user judgments (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a). Currently, prediction models for user judgments are based on the PARADISE framework introduced by Walker et al. (1997). PARADISE assumes that user satisfaction judgments describe the overall quality of the system, and are causally related to task success and dialog costs, i.e. efficiency and quality of the dialog. Therefore, a linear regression function can be tra</context>
</contexts>
<marker>Ai, Weng, 2008</marker>
<rawString>Hua Ai, Fuliang Weng. 2008. User Simulation as Testing for Spoken Dialog Systems. Proc. of the 9th SIGdial Workshop on Discourse and Dialogue, Columbus, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grace Chung</author>
</authors>
<title>Developing a flexible spoken dialog system using simulation.</title>
<date>2004</date>
<booktitle>Proc. of the 42nd Annual Meeting on Association for Computational Linguistics,</booktitle>
<location>Barcelona,</location>
<contexts>
<context position="2417" citStr="Chung, 2004" startWordPosition="348" endWordPosition="349">us perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a; Chung, 2004; López-Cózar et al., 2003). In order to evaluate results from such simulations, some approaches utilize prediction models of user judgments (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a). Currently, prediction models for user judgments are based on the PARADISE framework introduced by Walker et al. (1997). PARADISE assumes that user satisfaction judgments describe the overall quality of the system, and are causally related to task success and dialog costs, i.e. efficiency and quality of the dialog. Therefore, a linear regression function can be trained with interaction parameters describ</context>
</contexts>
<marker>Chung, 2004</marker>
<rawString>Grace Chung. 2004. Developing a flexible spoken dialog system using simulation. Proc. of the 42nd Annual Meeting on Association for Computational Linguistics, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus-Peter Engelbrecht</author>
<author>Sebastian Möller</author>
</authors>
<title>Pragmatic Usage of Linear Regression Models for the Prediction of User Judgments.</title>
<date>2007</date>
<booktitle>Proc. of 8th SIGdial Workshop on Discourse and Dialogue,</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="4354" citStr="Engelbrecht and Möller, 2007" startWordPosition="644" endWordPosition="647">ional Linguistics 170 functions for user satisfaction were proposed to serve as optimization function in a system adapting its dialog strategy during the interaction. This idea is taken up by Rieser and Lemon (2008). The prediction accuracy of PARADISE functions typically lies around an R2 of 0.5, meaning that 50% of the variance in the judgments is explained by the model. While this number is not absolutely satisfying, it could be shown that mean values for groups of dialogs (e.g. with a specific system configuration) can be predicted more accurately than single dialogs with the same models (Engelbrecht and Möller, 2007). Low R2 for the predictions of ratings of individual dialogs seems to be due to inter-rater differences at least to some degree. Such differences have been described, and may concern the actual perception of the judged issue (Guski, 1999), or the way the perception is described by the participant (Okun and Weir, 1990; Engelbrecht et al., 2008b) We have tested the PARADISE framework extensively, using different classifier models and interaction parameters. Precise and general models are hard to achieve, even if the set of parameters describing the interaction is widely extended (Möller et al.,</context>
</contexts>
<marker>Engelbrecht, Möller, 2007</marker>
<rawString>Klaus-Peter Engelbrecht, Sebastian Möller. 2007. Pragmatic Usage of Linear Regression Models for the Prediction of User Judgments. Proc. of 8th SIGdial Workshop on Discourse and Dialogue, Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus-Peter Engelbrecht</author>
<author>Michael Kruppa</author>
<author>Sebastian Möller</author>
<author>Michael Quade</author>
</authors>
<title>MeMo Workbench for Semi-Automated Usability Testing.</title>
<date>2008</date>
<booktitle>Proc. of 9th Interspeech,</booktitle>
<location>Brisbane, Australia.</location>
<contexts>
<context position="2403" citStr="Engelbrecht et al., 2008" startWordPosition="344" endWordPosition="347">eraction, which cover various perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a; Chung, 2004; López-Cózar et al., 2003). In order to evaluate results from such simulations, some approaches utilize prediction models of user judgments (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a). Currently, prediction models for user judgments are based on the PARADISE framework introduced by Walker et al. (1997). PARADISE assumes that user satisfaction judgments describe the overall quality of the system, and are causally related to task success and dialog costs, i.e. efficiency and quality of the dialog. Therefore, a linear regression function can be trained with interaction para</context>
<context position="4699" citStr="Engelbrecht et al., 2008" startWordPosition="701" endWordPosition="704">s explained by the model. While this number is not absolutely satisfying, it could be shown that mean values for groups of dialogs (e.g. with a specific system configuration) can be predicted more accurately than single dialogs with the same models (Engelbrecht and Möller, 2007). Low R2 for the predictions of ratings of individual dialogs seems to be due to inter-rater differences at least to some degree. Such differences have been described, and may concern the actual perception of the judged issue (Guski, 1999), or the way the perception is described by the participant (Okun and Weir, 1990; Engelbrecht et al., 2008b) We have tested the PARADISE framework extensively, using different classifier models and interaction parameters. Precise and general models are hard to achieve, even if the set of parameters describing the interaction is widely extended (Möller et al., 2008). In an effort to improve such prediction models, we developed two ideas: • Predict the distribution of ratings which can be expected for a representative group of users given the same stimulus. This takes into account that in most cases the relevant user characteristics determining the judgment cannot be tracked, or even are unknown. • </context>
<context position="24854" citStr="Engelbrecht et al., 2008" startWordPosition="3914" endWordPosition="3917">iction of user judgments about SDSs, using HMMs. The approach allows predicting the users’ judgments at each step of a dialog. In predicting the distribution of ratings of many users, the approach takes into account differences between the users’ judgment behaviors. This increases the usefulness of the model for a number of applications. E.g., in adaptive systems, the decision process can take into account differences between the users which cannot be attributed to user characteristics known to the system. If the model is applied to automatically generated dialogs, e.g. in the MeMo workbench (Engelbrecht et al., 2008a), a more detailed prediction of user satisfaction is enabled, allowing analysis on a turn-by-turn basis. In addition, the approach facilitates the analysis of models and features affecting the quality ratings, as results can be compared to the empirical ratings with more detail. We hope to gain further insight into the relations between interaction parameters and user judgments by running simulations under different assumptions of relations between these entities. A drawback of the approach is the generation of training data. The models presented in this paper cannot be assumed to be general</context>
</contexts>
<marker>Engelbrecht, Kruppa, Möller, Quade, 2008</marker>
<rawString>Klaus-Peter Engelbrecht, Michael Kruppa, Sebastian Möller, Michael Quade. 2008a. MeMo Workbench for Semi-Automated Usability Testing. Proc. of 9th Interspeech, Brisbane, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus-Peter Engelbrecht</author>
<author>Sebastian Möller</author>
<author>Robert Schleicher</author>
<author>Ina Wechsung</author>
</authors>
<title>Analysis of PARADISE Models for Individual Users of a Spoken Dialog System.</title>
<date>2008</date>
<booktitle>Proc. of ESSV</booktitle>
<location>Frankfurt/Main, Germany.</location>
<contexts>
<context position="2403" citStr="Engelbrecht et al., 2008" startWordPosition="344" endWordPosition="347">eraction, which cover various perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a; Chung, 2004; López-Cózar et al., 2003). In order to evaluate results from such simulations, some approaches utilize prediction models of user judgments (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a). Currently, prediction models for user judgments are based on the PARADISE framework introduced by Walker et al. (1997). PARADISE assumes that user satisfaction judgments describe the overall quality of the system, and are causally related to task success and dialog costs, i.e. efficiency and quality of the dialog. Therefore, a linear regression function can be trained with interaction para</context>
<context position="4699" citStr="Engelbrecht et al., 2008" startWordPosition="701" endWordPosition="704">s explained by the model. While this number is not absolutely satisfying, it could be shown that mean values for groups of dialogs (e.g. with a specific system configuration) can be predicted more accurately than single dialogs with the same models (Engelbrecht and Möller, 2007). Low R2 for the predictions of ratings of individual dialogs seems to be due to inter-rater differences at least to some degree. Such differences have been described, and may concern the actual perception of the judged issue (Guski, 1999), or the way the perception is described by the participant (Okun and Weir, 1990; Engelbrecht et al., 2008b) We have tested the PARADISE framework extensively, using different classifier models and interaction parameters. Precise and general models are hard to achieve, even if the set of parameters describing the interaction is widely extended (Möller et al., 2008). In an effort to improve such prediction models, we developed two ideas: • Predict the distribution of ratings which can be expected for a representative group of users given the same stimulus. This takes into account that in most cases the relevant user characteristics determining the judgment cannot be tracked, or even are unknown. • </context>
<context position="24854" citStr="Engelbrecht et al., 2008" startWordPosition="3914" endWordPosition="3917">iction of user judgments about SDSs, using HMMs. The approach allows predicting the users’ judgments at each step of a dialog. In predicting the distribution of ratings of many users, the approach takes into account differences between the users’ judgment behaviors. This increases the usefulness of the model for a number of applications. E.g., in adaptive systems, the decision process can take into account differences between the users which cannot be attributed to user characteristics known to the system. If the model is applied to automatically generated dialogs, e.g. in the MeMo workbench (Engelbrecht et al., 2008a), a more detailed prediction of user satisfaction is enabled, allowing analysis on a turn-by-turn basis. In addition, the approach facilitates the analysis of models and features affecting the quality ratings, as results can be compared to the empirical ratings with more detail. We hope to gain further insight into the relations between interaction parameters and user judgments by running simulations under different assumptions of relations between these entities. A drawback of the approach is the generation of training data. The models presented in this paper cannot be assumed to be general</context>
</contexts>
<marker>Engelbrecht, Möller, Schleicher, Wechsung, 2008</marker>
<rawString>Klaus-Peter Engelbrecht, Sebastian Möller, Robert Schleicher, Ina Wechsung. 2008b. Analysis of PARADISE Models for Individual Users of a Spoken Dialog System. Proc. of ESSV 2008, Frankfurt/Main, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus-Peter Engelbrecht</author>
<author>Felix Hartard</author>
<author>Florian Gödde</author>
<author>Sebastian Möller</author>
</authors>
<title>A Closer Look at Quality Judgments of Spoken Dialog Systems, submitted to Interspeech</title>
<date>2009</date>
<marker>Engelbrecht, Hartard, Gödde, Möller, 2009</marker>
<rawString>Klaus-Peter Engelbrecht, Felix Hartard, Florian Gödde, Sebastian Möller. 2009. A Closer Look at Quality Judgments of Spoken Dialog Systems, submitted to Interspeech 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norman M Fraser</author>
<author>G Nigel Gilbert</author>
</authors>
<title>Simulating speech systems.</title>
<date>1991</date>
<journal>Computer Speech and Language,</journal>
<pages>5--81</pages>
<contexts>
<context position="9981" citStr="Fraser and Gilbert 1991" startWordPosition="1533" endWordPosition="1536"> the transition probabilities. Figure 1. Topology of an HMM to model user judgments (“good” or “bad”) in their probabilistic relation to dialog events (error and confirmation strategy) and the previous rating. 3 Data Collection In order to train the model, data is needed in which the association between dialog events and judgments at each turn is represented. Therefore, we conducted an experiment in which users interacted with an SDS and judged the interaction after each dialog turn. In addition, all users followed the same five dialogs, which was possible by using the Wizard-of-Oz technique (Fraser and Gilbert 1991) and relatively strict scenario descriptions. As the interactions were predefined, care was taken that a representative sample of relevant usability problems were passed in the interactions. Such problems were collected in a brainstorming session, and comprise understanding errors, task failure, prompt wording issues (such as incomplete information), missing control options, or not being able to input one of the criteria specified in the scenario description. As further features of the dialog, which might impact the quality judgment, the number of concepts which can be specified, as well as th</context>
</contexts>
<marker>Fraser, Gilbert, 1991</marker>
<rawString>Norman M. Fraser, G. Nigel Gilbert. 1991. Simulating speech systems. Computer Speech and Language, 5:81–99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rainer Guski</author>
</authors>
<title>Personal and Social Variables as Co-determinants of Noise Annoyance.</title>
<date>1999</date>
<journal>Noise &amp; Health,</journal>
<pages>3--45</pages>
<contexts>
<context position="4593" citStr="Guski, 1999" startWordPosition="685" endWordPosition="686">ctions typically lies around an R2 of 0.5, meaning that 50% of the variance in the judgments is explained by the model. While this number is not absolutely satisfying, it could be shown that mean values for groups of dialogs (e.g. with a specific system configuration) can be predicted more accurately than single dialogs with the same models (Engelbrecht and Möller, 2007). Low R2 for the predictions of ratings of individual dialogs seems to be due to inter-rater differences at least to some degree. Such differences have been described, and may concern the actual perception of the judged issue (Guski, 1999), or the way the perception is described by the participant (Okun and Weir, 1990; Engelbrecht et al., 2008b) We have tested the PARADISE framework extensively, using different classifier models and interaction parameters. Precise and general models are hard to achieve, even if the set of parameters describing the interaction is widely extended (Möller et al., 2008). In an effort to improve such prediction models, we developed two ideas: • Predict the distribution of ratings which can be expected for a representative group of users given the same stimulus. This takes into account that in most c</context>
</contexts>
<marker>Guski, 1999</marker>
<rawString>Rainer Guski. 1999. Personal and Social Variables as Co-determinants of Noise Annoyance. Noise &amp; Health, 3:45-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kate S Hone</author>
<author>Robert Graham</author>
</authors>
<title>Subjective Assessment of Speech-system Interface Usability.</title>
<date>2001</date>
<booktitle>Proc. of EUROSPEECH,</booktitle>
<location>Aalborg, Denmark.</location>
<contexts>
<context position="1934" citStr="Hone and Graham, 2001" startWordPosition="270" endWordPosition="273"> n-best list, and generate the reply and speech recognition grammars depending on the content found (Wootton et al. 2007). The performance of single components strongly depends on each other component in this case. While performance parameters become less meaningful in such a system, the system’s overall quality, which can only be measured by asking the user (Jekosch 2005), gains interest for the evaluation. Typically, users fill out questionnaires after the interaction, which cover various perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a; Chung, 2004; López-Cózar et al., 2003). In order to evaluate results from such simulations, some approaches utilize prediction m</context>
</contexts>
<marker>Hone, Graham, 2001</marker>
<rawString>Kate S. Hone, Robert Graham. 2001. Subjective Assessment of Speech-system Interface Usability. Proc. of EUROSPEECH, Aalborg, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P 851</author>
</authors>
<title>Subjective Quality Evaluation of Telephone Services Based on Spoken Dialogue Systems,</title>
<date>2003</date>
<institution>International Telecommunication Union,</institution>
<location>Geneva, Switzerland.</location>
<marker>851, 2003</marker>
<rawString>ITU-T Rec. P.851, 2003. Subjective Quality Evaluation of Telephone Services Based on Spoken Dialogue Systems, International Telecommunication Union, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ute Jekosch</author>
</authors>
<title>Voice and Speech Quality Perception. Assessment and Evaluation,</title>
<date>2005</date>
<publisher>Springer,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="1688" citStr="Jekosch 2005" startWordPosition="238" endWordPosition="239"> for testing the entire system, as components for speech recognition, language understanding and dialog management are interacting more deeply. For example, the system might search for web content on the basis of meaning extracted from an n-best list, and generate the reply and speech recognition grammars depending on the content found (Wootton et al. 2007). The performance of single components strongly depends on each other component in this case. While performance parameters become less meaningful in such a system, the system’s overall quality, which can only be measured by asking the user (Jekosch 2005), gains interest for the evaluation. Typically, users fill out questionnaires after the interaction, which cover various perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of forese</context>
</contexts>
<marker>Jekosch, 2005</marker>
<rawString>Ute Jekosch. 2005. Voice and Speech Quality Perception. Assessment and Evaluation, Springer, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ramón López-Cózar</author>
<author>Ángel de la Torre</author>
<author>José C Segura</author>
<author>Antonio J Rubio</author>
</authors>
<title>Assessment of Dialogue Systems by Means of a New Simulation Technique. Speech Communication,</title>
<date>2003</date>
<pages>40--3</pages>
<contexts>
<context position="2444" citStr="López-Cózar et al., 2003" startWordPosition="350" endWordPosition="353">al dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a; Chung, 2004; López-Cózar et al., 2003). In order to evaluate results from such simulations, some approaches utilize prediction models of user judgments (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a). Currently, prediction models for user judgments are based on the PARADISE framework introduced by Walker et al. (1997). PARADISE assumes that user satisfaction judgments describe the overall quality of the system, and are causally related to task success and dialog costs, i.e. efficiency and quality of the dialog. Therefore, a linear regression function can be trained with interaction parameters describing dialog costs and task s</context>
</contexts>
<marker>López-Cózar, Torre, Segura, Rubio, 2003</marker>
<rawString>Ramón López-Cózar, Ángel de la Torre, José C. Segura and Antonio J. Rubio. 2003. Assessment of Dialogue Systems by Means of a New Simulation Technique. Speech Communication, 40(3):387-407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Möller</author>
</authors>
<title>Perceptual Quality Dimensions of Spoken Dialog Systems: A Review and New Experimental Results,</title>
<date>2005</date>
<booktitle>Proc. of Forum Acusticum,</booktitle>
<location>Budapest, Hungary.</location>
<contexts>
<context position="1971" citStr="Möller 2005" startWordPosition="278" endWordPosition="279"> recognition grammars depending on the content found (Wootton et al. 2007). The performance of single components strongly depends on each other component in this case. While performance parameters become less meaningful in such a system, the system’s overall quality, which can only be measured by asking the user (Jekosch 2005), gains interest for the evaluation. Typically, users fill out questionnaires after the interaction, which cover various perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a; Chung, 2004; López-Cózar et al., 2003). In order to evaluate results from such simulations, some approaches utilize prediction models of user judgments (e.g. Ai and </context>
<context position="10878" citStr="Möller 2005" startWordPosition="1667" endWordPosition="1668">k failure, prompt wording issues (such as incomplete information), missing control options, or not being able to input one of the criteria specified in the scenario description. As further features of the dialog, which might impact the quality judgment, the number of concepts which can be specified, as well as the confirmation strategy, should vary along the interactions. We then designed a dialog system with a consistent strategy, which however allowed for the quality-relevant issues mentioned above to occur in the dialogs. The system was derived from the BoRIS restaurant information system (Möller 2005b), which enables the search of restaurants in the German city Bochum according to the criteria day, time, location, type of cuisine and pricing. For our system, we simplified the dialog strategy to system initiative, however, at the first turn, the users could say both day and time. If both were recognized, explicit confirmation was requested. If just one constraint was understood, confirmation was implicit. The last constraint specified (pricing) was not confirmed, and instead the restaurant information was output directly. If no restaurant was found according to the specified criteria, the </context>
</contexts>
<marker>Möller, 2005</marker>
<rawString>Sebastian Möller. 2005a. Perceptual Quality Dimensions of Spoken Dialog Systems: A Review and New Experimental Results, Proc. of Forum Acusticum, Budapest, Hungary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Möller</author>
</authors>
<title>Quality of Telephone-based Spoken Dialog Systems.</title>
<date>2005</date>
<publisher>Springer,</publisher>
<location>New York.</location>
<contexts>
<context position="1971" citStr="Möller 2005" startWordPosition="278" endWordPosition="279"> recognition grammars depending on the content found (Wootton et al. 2007). The performance of single components strongly depends on each other component in this case. While performance parameters become less meaningful in such a system, the system’s overall quality, which can only be measured by asking the user (Jekosch 2005), gains interest for the evaluation. Typically, users fill out questionnaires after the interaction, which cover various perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to compare systems with respect to a single measure, which however comprises all relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a; Chung, 2004; López-Cózar et al., 2003). In order to evaluate results from such simulations, some approaches utilize prediction models of user judgments (e.g. Ai and </context>
<context position="10878" citStr="Möller 2005" startWordPosition="1667" endWordPosition="1668">k failure, prompt wording issues (such as incomplete information), missing control options, or not being able to input one of the criteria specified in the scenario description. As further features of the dialog, which might impact the quality judgment, the number of concepts which can be specified, as well as the confirmation strategy, should vary along the interactions. We then designed a dialog system with a consistent strategy, which however allowed for the quality-relevant issues mentioned above to occur in the dialogs. The system was derived from the BoRIS restaurant information system (Möller 2005b), which enables the search of restaurants in the German city Bochum according to the criteria day, time, location, type of cuisine and pricing. For our system, we simplified the dialog strategy to system initiative, however, at the first turn, the users could say both day and time. If both were recognized, explicit confirmation was requested. If just one constraint was understood, confirmation was implicit. The last constraint specified (pricing) was not confirmed, and instead the restaurant information was output directly. If no restaurant was found according to the specified criteria, the </context>
</contexts>
<marker>Möller, 2005</marker>
<rawString>Sebastian Möller. 2005b. Quality of Telephone-based Spoken Dialog Systems. Springer, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sebastian Möller</author>
<author>Klaus-Peter Engelbrecht</author>
<author>Robert Schleicher</author>
</authors>
<date>2008</date>
<booktitle>Predicting the Quality and Usability of Spoken Dialogue Services, Speech Communication,</booktitle>
<pages>50--730</pages>
<contexts>
<context position="4960" citStr="Möller et al., 2008" startWordPosition="739" endWordPosition="742"> Möller, 2007). Low R2 for the predictions of ratings of individual dialogs seems to be due to inter-rater differences at least to some degree. Such differences have been described, and may concern the actual perception of the judged issue (Guski, 1999), or the way the perception is described by the participant (Okun and Weir, 1990; Engelbrecht et al., 2008b) We have tested the PARADISE framework extensively, using different classifier models and interaction parameters. Precise and general models are hard to achieve, even if the set of parameters describing the interaction is widely extended (Möller et al., 2008). In an effort to improve such prediction models, we developed two ideas: • Predict the distribution of ratings which can be expected for a representative group of users given the same stimulus. This takes into account that in most cases the relevant user characteristics determining the judgment cannot be tracked, or even are unknown. • Consider the time relations between events by modeling user opinion as a variable evolving over the course of the dialog. This way, time relations like cooccurrence of events, which affect quality perception, attention, or memory can be modeled most effectively</context>
</contexts>
<marker>Möller, Engelbrecht, Schleicher, 2008</marker>
<rawString>Sebastian Möller, Klaus-Peter Engelbrecht, Robert Schleicher. 2008. Predicting the Quality and Usability of Spoken Dialogue Services, Speech Communication, 50:730-744.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Morris A Okun</author>
<author>Renee M Weir</author>
</authors>
<title>Toward a Judgment Model of College Satisfaction.</title>
<date>1990</date>
<journal>Educational Psychological Review,</journal>
<pages>2--1</pages>
<contexts>
<context position="4673" citStr="Okun and Weir, 1990" startWordPosition="697" endWordPosition="700">ce in the judgments is explained by the model. While this number is not absolutely satisfying, it could be shown that mean values for groups of dialogs (e.g. with a specific system configuration) can be predicted more accurately than single dialogs with the same models (Engelbrecht and Möller, 2007). Low R2 for the predictions of ratings of individual dialogs seems to be due to inter-rater differences at least to some degree. Such differences have been described, and may concern the actual perception of the judged issue (Guski, 1999), or the way the perception is described by the participant (Okun and Weir, 1990; Engelbrecht et al., 2008b) We have tested the PARADISE framework extensively, using different classifier models and interaction parameters. Precise and general models are hard to achieve, even if the set of parameters describing the interaction is widely extended (Möller et al., 2008). In an effort to improve such prediction models, we developed two ideas: • Predict the distribution of ratings which can be expected for a representative group of users given the same stimulus. This takes into account that in most cases the relevant user characteristics determining the judgment cannot be tracke</context>
</contexts>
<marker>Okun, Weir, 1990</marker>
<rawString>Morris A. Okun, Renee M. Weir. 1990. Toward a Judgment Model of College Satisfaction. Educational Psychological Review, 2(1):59-76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence R Rabiner</author>
</authors>
<title>A tutorial on HMM and selected applications in speech recognition.</title>
<date>1989</date>
<booktitle>Proc. IEEE,</booktitle>
<pages>77--2</pages>
<contexts>
<context position="9141" citStr="Rabiner, 1989" startWordPosition="1404" endWordPosition="1405">ities for each judgment given the observed combination of features. If the features are interacting (i.e. the probability of one feature changes in dependence of another feature), this is modelled by directly specifying the emission probabilities for each combination of features. We call this a layer of emissions. Additional layers with other features can be created. In this case, the likelihood of each judgment given probabilities from each layer can be calculated by multiplication of the probabilities from each layer. For the calculation of state probabilities, we can use forward recursion (Rabiner, 1989). The algorithm proceeds through the observed sequence, and at each step calculates the probability for each state given the probabilities of the observation, the probabilities of each state at the previous step, and the transition probabilities. Figure 1. Topology of an HMM to model user judgments (“good” or “bad”) in their probabilistic relation to dialog events (error and confirmation strategy) and the previous rating. 3 Data Collection In order to train the model, data is needed in which the association between dialog events and judgments at each turn is represented. Therefore, we conducte</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Lawrence R. Rabiner. 1989. A tutorial on HMM and selected applications in speech recognition. Proc. IEEE, 77(2):257-286.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Verena Rieser</author>
<author>Oliver Lemon</author>
</authors>
<title>Automatic Learning and Evaluation of User-Centered Objective Functions for Dialogue System Optimisation.</title>
<date>2008</date>
<booktitle>Proc. of LREC&apos;08,</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="3940" citStr="Rieser and Lemon (2008)" startWordPosition="577" endWordPosition="580"> also limitations of the models in predicting judgments for other user groups, or for systems with different levels of ASR performance, were reported (Walker et al., 1998). In the same study, prediction Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 170–177, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 170 functions for user satisfaction were proposed to serve as optimization function in a system adapting its dialog strategy during the interaction. This idea is taken up by Rieser and Lemon (2008). The prediction accuracy of PARADISE functions typically lies around an R2 of 0.5, meaning that 50% of the variance in the judgments is explained by the model. While this number is not absolutely satisfying, it could be shown that mean values for groups of dialogs (e.g. with a specific system configuration) can be predicted more accurately than single dialogs with the same models (Engelbrecht and Möller, 2007). Low R2 for the predictions of ratings of individual dialogs seems to be due to inter-rater differences at least to some degree. Such differences have been described, and may concern th</context>
</contexts>
<marker>Rieser, Lemon, 2008</marker>
<rawString>Verena Rieser, Oliver Lemon. 2008. Automatic Learning and Evaluation of User-Centered Objective Functions for Dialogue System Optimisation. Proc. of LREC&apos;08, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Steimel</author>
<author>Oliver Jacobs</author>
<author>Norbert Pfleger</author>
<author>Sebastian Paulke</author>
</authors>
<title>Testbericht VOICE Award 2008: Die besten deutschsprachigen Sprachapplikationen. Initiative Voice Business,</title>
<date>2008</date>
<location>Bad Homburg, Germany.</location>
<contexts>
<context position="1004" citStr="Steimel et al. 2008" startWordPosition="131" endWordPosition="134">edicting judgments about the quality of Spoken Dialog Systems have been used as overall evaluation metric or as optimization functions in adaptive systems. We describe a new approach to such models, using Hidden Markov Models (HMMs). The user’s opinion is regarded as a continuous process evolving over time. We present the data collection method and results achieved with the HMM model. 1 Introduction Spoken Dialog Systems (SDSs) are now widely used, and are becoming more complex as a result of the increased solidity of advanced techniques, mainly in the realm of natural language understanding (Steimel et al. 2008). At the same time, the evaluation of such systems increasingly demands for testing the entire system, as components for speech recognition, language understanding and dialog management are interacting more deeply. For example, the system might search for web content on the basis of meaning extracted from an n-best list, and generate the reply and speech recognition grammars depending on the content found (Wootton et al. 2007). The performance of single components strongly depends on each other component in this case. While performance parameters become less meaningful in such a system, the sy</context>
</contexts>
<marker>Steimel, Jacobs, Pfleger, Paulke, 2008</marker>
<rawString>Bernhard Steimel, Oliver Jacobs, Norbert Pfleger, Sebastian Paulke. 2008. Testbericht VOICE Award 2008: Die besten deutschsprachigen Sprachapplikationen. Initiative Voice Business, Bad Homburg, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candace A Kamm</author>
<author>Alicia Abella</author>
</authors>
<title>PARADISE: A Framework for Evaluating Spoken Dialogue Agents.</title>
<date>1997</date>
<booktitle>Proc. of ACL/EACL 35th Ann. Meeting of the Assoc. for Computational Linguistics,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="2729" citStr="Walker et al. (1997)" startWordPosition="393" endWordPosition="396"> relevant aspects of the interaction. Thus, the complexity of the evaluation task is reduced. In addition, user simulation is increasingly used to address the difficulty of foreseeing all possible problems a user might encounter with the system (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a; Chung, 2004; López-Cózar et al., 2003). In order to evaluate results from such simulations, some approaches utilize prediction models of user judgments (e.g. Ai and Weng, 2008; Engelbrecht et al., 2008a). Currently, prediction models for user judgments are based on the PARADISE framework introduced by Walker et al. (1997). PARADISE assumes that user satisfaction judgments describe the overall quality of the system, and are causally related to task success and dialog costs, i.e. efficiency and quality of the dialog. Therefore, a linear regression function can be trained with interaction parameters describing dialog costs and task success as predictors, and satisfaction ratings as the target. The resulting equation can then be used to predict user satisfaction with unseen dialogs. In follow-up studies, it could be shown that such models are to some degree generalizable (Walker et al., 2000). However, also limita</context>
</contexts>
<marker>Walker, Litman, Kamm, Abella, 1997</marker>
<rawString>Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, Alicia Abella. 1997. PARADISE: A Framework for Evaluating Spoken Dialogue Agents. Proc. of ACL/EACL 35th Ann. Meeting of the Assoc. for Computational Linguistics, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn A Walker</author>
<author>Diane J Litman</author>
<author>Candace A Kamm</author>
</authors>
<title>Evaluating Spoken Dialog Agents with PARADISE: Two Case Studies. Computer Speech and Language,</title>
<date>1998</date>
<pages>12--317</pages>
<location>Alicia Abella.</location>
<contexts>
<context position="3488" citStr="Walker et al., 1998" startWordPosition="509" endWordPosition="512">d dialog costs, i.e. efficiency and quality of the dialog. Therefore, a linear regression function can be trained with interaction parameters describing dialog costs and task success as predictors, and satisfaction ratings as the target. The resulting equation can then be used to predict user satisfaction with unseen dialogs. In follow-up studies, it could be shown that such models are to some degree generalizable (Walker et al., 2000). However, also limitations of the models in predicting judgments for other user groups, or for systems with different levels of ASR performance, were reported (Walker et al., 1998). In the same study, prediction Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 170–177, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 170 functions for user satisfaction were proposed to serve as optimization function in a system adapting its dialog strategy during the interaction. This idea is taken up by Rieser and Lemon (2008). The prediction accuracy of PARADISE functions typically lies around an R2 of 0.5, meaning that 50% of the variance in the judgments is explained b</context>
</contexts>
<marker>Walker, Litman, Kamm, 1998</marker>
<rawString>Marilyn A. Walker, Diane J. Litman, Candace A. Kamm, Alicia Abella. 1998. Evaluating Spoken Dialog Agents with PARADISE: Two Case Studies. Computer Speech and Language, 12:317-347.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marilyn Walker</author>
<author>Candace Kamm</author>
<author>Diane Litman</author>
</authors>
<title>Towards Developing General Models of Usability with PARADISE.</title>
<date>2000</date>
<journal>Natural Language Engineering,</journal>
<pages>6--3</pages>
<contexts>
<context position="3307" citStr="Walker et al., 2000" startWordPosition="481" endWordPosition="484">amework introduced by Walker et al. (1997). PARADISE assumes that user satisfaction judgments describe the overall quality of the system, and are causally related to task success and dialog costs, i.e. efficiency and quality of the dialog. Therefore, a linear regression function can be trained with interaction parameters describing dialog costs and task success as predictors, and satisfaction ratings as the target. The resulting equation can then be used to predict user satisfaction with unseen dialogs. In follow-up studies, it could be shown that such models are to some degree generalizable (Walker et al., 2000). However, also limitations of the models in predicting judgments for other user groups, or for systems with different levels of ASR performance, were reported (Walker et al., 1998). In the same study, prediction Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 170–177, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 170 functions for user satisfaction were proposed to serve as optimization function in a system adapting its dialog strategy during the interaction. This idea is ta</context>
</contexts>
<marker>Walker, Kamm, Litman, 2000</marker>
<rawString>Marilyn Walker, Candace Kamm, Diane Litman. 2000. Towards Developing General Models of Usability with PARADISE. Natural Language Engineering, 6(3-4):363-377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Craig Wootton</author>
<author>Michael McTear</author>
<author>Terry Anderson</author>
</authors>
<title>Utilizing Online Content as Domain Knowledge in a Multi-Domain Dynamic Dialogue System.</title>
<date>2007</date>
<booktitle>Proc. of Interspeech</booktitle>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="1434" citStr="Wootton et al. 2007" startWordPosition="197" endWordPosition="200">) are now widely used, and are becoming more complex as a result of the increased solidity of advanced techniques, mainly in the realm of natural language understanding (Steimel et al. 2008). At the same time, the evaluation of such systems increasingly demands for testing the entire system, as components for speech recognition, language understanding and dialog management are interacting more deeply. For example, the system might search for web content on the basis of meaning extracted from an n-best list, and generate the reply and speech recognition grammars depending on the content found (Wootton et al. 2007). The performance of single components strongly depends on each other component in this case. While performance parameters become less meaningful in such a system, the system’s overall quality, which can only be measured by asking the user (Jekosch 2005), gains interest for the evaluation. Typically, users fill out questionnaires after the interaction, which cover various perceptional dimensions such as efficiency, dialog smoothness, or the overall evaluation of the system (Hone and Graham, 2001; ITU-T Rec. P.851, 2003; Möller 2005a). Judgments of the system’s overall quality can be used to co</context>
</contexts>
<marker>Wootton, McTear, Anderson, 2007</marker>
<rawString>Craig Wootton, Michael McTear, Terry Anderson. 2007. Utilizing Online Content as Domain Knowledge in a Multi-Domain Dynamic Dialogue System. Proc. of Interspeech 2007, Antwerp, Belgium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>