<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9991235">
A joint model of word segmentation and phonological variation for
English word-final /t/-deletion
</title>
<author confidence="0.996345">
Benjamin B¨orschinger&apos;,3 and Mark Johnson&apos; and Katherine Demuth2
</author>
<listItem confidence="0.634034666666667">
(1) Department of Computing, Macquarie University
(2) Department of Linguistics, Macquarie University
(3) Department of Computational Linguistics, Heidelberg University
</listItem>
<email confidence="0.983505">
{benjamin.borschinger, mark.johnson, katherine.demuth}@mq.edu.au
</email>
<sectionHeader confidence="0.993848" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999914">
Word-final /t/-deletion refers to a common
phenomenon in spoken English where
words such as /west/ “west” are pro-
nounced as [wes] “wes” in certain con-
texts. Phonological variation like this is
common in naturally occurring speech.
Current computational models of unsu-
pervised word segmentation usually as-
sume idealized input that is devoid of
these kinds of variation. We extend a
non-parametric model of word segmenta-
tion by adding phonological rules that map
from underlying forms to surface forms
to produce a mathematically well-defined
joint model as a first step towards han-
dling variation and segmentation in a sin-
gle model. We analyse how our model
handles /t/-deletion on a large corpus of
transcribed speech, and show that the joint
model can perform word segmentation and
recover underlying /t/s. We find that Bi-
gram dependencies are important for per-
forming well on real data and for learning
appropriate deletion probabilities for dif-
ferent contexts.1
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993780056603774">
Computational models of word segmentation try
to solve one of the first problems language learn-
ers have to face: breaking an unsegmented stream
of sound segments into individual words. Cur-
rently, most such models assume that the input
consists of sequences of phonemes with no pro-
nunciation variation across different occurrences
of the same word type. In this paper we describe
1The implementation of our model as well as
scripts to prepare the data will be made available at
http://web.science.mq.edu.au/~bborschi.
We can’t release our version of the Buckeye Corpus (Pitt et
al., 2007) directly because of licensing issues.
an extension of the Bayesian models of Gold-
water et al. (2009) that incorporates phonologi-
cal rules to “explain away” surface variation. As
a concrete example, we focus on word-final /t/-
deletion in English, although our approach is not
limited to this case. We choose /t/-deletion be-
cause it is a very common and well-studied phe-
nomenon (see Coetzee (2004, Chapter 5) for a
review) and segmental deletion is an interesting
test-case for our architecture. Recent work has
found that /t/-deletion (among other things) is in-
deed common in child-directed speech (CDS) and,
importantly, that its distribution is similar to that in
adult-directed speech (ADS) (Dilley et al., to ap-
pear). This justifies our using ADS to evaluate our
model, as discussed below.
Our experiments are consistent with long-
standing and recent findings in linguistics, in par-
ticular that /t/-deletion heavily depends on the im-
mediate context and that models ignoring context
work poorly on real data. We also examine how
well our models identify the probability of /t/-
deletion in different contexts. We find that models
that capture bigram dependencies between under-
lying forms provide considerably more accurate
estimates of those probabilities than correspond-
ing unigram or “bag of words” models of underly-
ing forms.
In section 2 we discuss related work on han-
dling variation in computational models and on /t/-
deletion. Section 3 describes our computational
model and section 4 discusses its performance for
recovering deleted /t/s. We look at both a sit-
uation where word boundaries are pre-specified
and only inference for underlying forms has to
be performed; and the problem of jointly finding
the word boundaries and recovering deleted un-
derlying /t/s. Section 5 discusses our findings, and
section 6 concludes with directions for further re-
search.
</bodyText>
<page confidence="0.94296">
1508
</page>
<note confidence="0.9427965">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1508–1516,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.44355" genericHeader="introduction">
2 Background and related work
</sectionHeader>
<bodyText confidence="0.999894041666667">
The work of Elsner et al. (2012) is most closely
related to our goal of building a model that han-
dles variation. They propose a pipe-line archi-
tecture involving two separate generative models,
one for word-segmentation and one for phonolog-
ical variation. They model the mapping to sur-
face forms using a probabilistic finite-state trans-
ducer. This allows their architecture to handle
virtually arbitrary pronunciation variation. How-
ever, as they point out, combining the segmenta-
tion and the variation model into one joint model
is not straight-forward and usual inference proce-
dures are infeasible, which requires the use of sev-
eral heuristics. We pursue an alternative research
strategy here, starting with a single well-studied
example of phonological variation. This permits
us to develop a joint generative model for both
word segmentation and variation which we plan to
extend to handle more phenomena in future work.
An earlier work that is close to the spirit of our
approach is Naradowsky and Goldwater (2009),
who learn spelling rules jointly with a simple
stem-suffix model of English verb morphology.
Their model, however, doesn’t naturally extend to
the segmentation of entire utterances.
/t/-deletion has received a lot of attention within
linguistics, and we point the interested reader to
Coetzee (2004, Chapter 5) for a thorough review.
Briefly, the phenomenon is as follows: word-final
instances of /t/ may undergo deletion in natural
speech, such that /west/ “west” is actually pro-
nounced as [wes] “wes”.2 While the frequency of
this phenomenon varies across social and dialectal
groups, within groups it has been found to be ro-
bust, and the probability of deletion depends on
its phonological context: a /t/ is more likely to
be dropped when followed by a consonant than
a vowel or a pause, and it is more likely to be
dropped when following a consonant than a vowel
as well. We point out two recent publications that
are of direct relevance to our research. Dilley et al.
(to appear) study word-final variation in stop con-
sonants in CDS, the kind of input we ideally would
like to evaluate our models on. They find that “in-
fants largely experience statistical distributions of
non-canonical consonantal pronunciation variants
[including deletion] that mirror those experienced
by adults.” This both directly establishes the need
</bodyText>
<footnote confidence="0.728772">
2Following the convention in phonology, we give under-
lying forms within “/... /” and surface forms within “[... ]”.
</footnote>
<bodyText confidence="0.999965470588235">
for computational models to handle this dimension
of variation, and justifies our choice of using ADS
for evaluation, as mentioned above.
Coetzee and Kawahara (2013) provide a com-
putational study of (among other things) /t/-
deletion within the framework of Harmonic Gram-
mar. They do not aim for a joint model that also
handles word segmentation, however, and rather
than training their model on an actual corpus, they
evaluate on constructed lists of examples, mimick-
ing frequencies of real data. Overall, our findings
agree with theirs, in particular that capturing the
probability of deletion in different contexts does
not automatically result in good performance for
recovering individual deleted /t/s. We will come
back to this point in our discussion at the end of
the paper.
</bodyText>
<sectionHeader confidence="0.989687" genericHeader="method">
3 The computational model
</sectionHeader>
<bodyText confidence="0.994259896551724">
Our models build on the Unigram and the Bigram
model introduced in Goldwater et al. (2009). Fig-
ure 1 shows the graphical model for our joint Bi-
gram model (the Unigram case is trivially recov-
ered by generating the Ui,js directly from L rather
than from LU,,�−1). Figure 2 gives the mathemati-
cal description of the graphical model and Table 1
provides a key to the variables of our model.
The model generates a latent sequence of un-
derlying word-tokens U1, ... , Un. Each word to-
ken is itself a non-empty sequence of segments or
phonemes, and each Uj corresponds to an under-
lying word form, prior to the application of any
phonological rule. This generative process is re-
peated for each utterance i, leading to multiple
utterances of the form Ui,1, ... , Ui,n, where ni is
the number of words in the ith utterance, and Ui,j
is the jth word in the ith utterance. Each utter-
ance is padded by an observed utterance bound-
ary symbol $ to the left and to the right, hence
Ui,0 = Ui,nz+1 = $.3 Each Ui,j+1 is generated
conditionally on its predecessor Ui,j from LU,,,,
as shown in the first row of the lower plate in Fig-
ure 1. Each Lw is a distribution over the pos-
sible words that can follow a token of w and L
is a global distribution over possible words, used
as back-off for all Lw. Just as in Goldwater et
al. (2009), L is drawn from a Dirichlet Process
(DP) with base distribution B and concentration
</bodyText>
<footnote confidence="0.991235">
3Each utterance terminates as soon as a $ is generated,
thus determining the number of words nz in the ith utterance.
See Goldwater et al. (2009) for discussion.
</footnote>
<page confidence="0.995695">
1509
</page>
<figureCaption confidence="0.782379">
Figure 1: The graphical model for our joint
model of word-final /t/-deletion and Bigram
word segmentation. The corresponding math-
</figureCaption>
<bodyText confidence="0.960839">
ematical description is given in Figure 2. The
generative process mimics the intuitively plau-
sible idea of generating underlying forms from
some kind of syntactic model (here, a Bi-
gram language model) and then mapping the
underlying form to an observed surface-form
through the application of a phonological rule
component, here represented by the collection
of rule probabilities ρc.
</bodyText>
<equation confidence="0.968923375">
L  |&apos;Y, α0 — DP (α0, B(·  |&apos;Y))
Lw  |L, α1 — DP(α1, L)
ρc |β — Beta(1,1)
Ui,0 = $
Si,0 =$
Ui,j+1  |Ui,j, LUz,j — LUz,j
Si,j  |Ui,j, Ui,j+1, P = PR(·  |Ui,j, Ui,j+1)
Wi |Si,1, ... , Si,nz = CAT(Si,0,... , Si,nz)
</equation>
<figureCaption confidence="0.965243">
Figure 2: Mathematical description of our joint
Bigram model. The lexical generator B(·  |&apos;Y)
</figureCaption>
<bodyText confidence="0.90540875">
is specified in Figure 3 and PR is explained in
the text below. CAT stands for concatenation
without word-boundaries, ni refers to the num-
ber of words in utterance i.
</bodyText>
<figure confidence="0.595741375">
Variable Explanation
B base distribution over possible words
L back-off distribution over words
Lw distribution over words following w
Ui,j underlying form, a word
Si,j surface realization of Ui,j, a word
ρc /t/-deletion probability in context c
Wi observed segments for ith utterance
</figure>
<figureCaption confidence="0.914245">
Table 1: Key for the variables in Figure 1 and
Figure 2. See Figure 3 for the definition of B.
</figureCaption>
<bodyText confidence="0.999206655172414">
parameter α0, and the word type specific distri-
butions Lw are drawn from a DP(L, α1), result-
ing in a hierarchical DP model (Teh et al., 2006).
The base distribution B functions as a lexical gen-
erator, defining a prior distribution over possible
words. In principle, B can incorporate arbitrary
prior knowledge about possible words, for exam-
ple syllable structure (cf. Johnson (2008)). In-
spired by Norris et al. (1997), we use a simpler
possible word constraint that only rules out se-
quences that lack a vowel (see Figure 3). While
this is clearly a simplification it is a plausible as-
sumption for English data.
Instead of generating the observed sequence of
segments W directly by concatenating the under-
lying forms as in Goldwater et al. (2009), we
map each Ui,j to a corresponding surface-form
Si,j by a probabilistic rule component PR. The
values over which the Si,j range are determined
by the available phonological processes. In the
model we study here, the phonological processes
only include a rule for deleting word-final /t/s
but in principle, PR can be used to encode a
wide variety of phonological rules. Here, Si,j E
{Ui,j, DELF(Ui,j)} if Ui,j ends in a /t/, and Si,j =
Ui,j otherwise, where DELF(u) refers to the same
word as u except that it lacks u’s final segment.
We look at three kinds of contexts on which a
rule’s probability of applying depends:
</bodyText>
<listItem confidence="0.998662166666667">
1. a uniform context that applies to every word-
final position
2. a right context that also considers the follow-
ing segment
3. a left-right context that additionally takes the
preceeding segment into account
</listItem>
<bodyText confidence="0.988750333333333">
For each possible context c there is a prob-
ability ρc which stands for the probability of
the rule applying in this context. Writing
</bodyText>
<equation confidence="0.9080042">
1510
-y — Dir((0.01, ... , 0.01))
�[r1n i=1 γxi]γ# if V(w)
B(w = x1:n  |-y) = Z
0.0 if -,V(w)
</equation>
<bodyText confidence="0.961126928571428">
Figure 3: Lexical generator with possible word-
constraint for words in E+, E being the alphabet
of available phonemes. x1:n is a sequence of ele-
ments of E of length n. -y is a probability vector
of length |E |+ 1 drawn from a sparse Dirichlet
prior, giving the probability for each phoneme and
the special word-boundary symbol #. The pred-
icate V holds of all sequences containing at least
one vowel. Z is a normalization constant that ad-
justs for the mass assigned to the empty and non-
possible words.
contexts in the notation familiar from genera-
tive phonology (Chomsky and Halle, 1968), our
model can be seen as implementing the fol-
lowing rules under the different assumptions:4
uniform /t/ 0 / ]word
right /t/ 0 / ]word 0
left-right /t/ 0 / α ]word 0
We let 0 range over V(owel), C(onsonant) and $
(utterance-boundary), and α over V and C. We
define a function CONT that maps a pair of ad-
jacent underlying forms Ui,j, Ui,j+1 to the con-
text of the final segment of Ui,j. For example,
CONT(/west/,/@v/) returns “C ]word V” in the
left-right setting, or simply “ ]word” in the uni-
form setting. CONT returns a special NOT con-
text if Ui,j doesn’t end in a /t/. We stipulate that
pNOT = 0.0. Then we can define PR as follows:
</bodyText>
<equation confidence="0.9998485">
PR(DELFINAL(u)  |u, r)) = pCONT(u,r)
PR(u  |u,r) = 1 — pCONT(u,r)
</equation>
<bodyText confidence="0.9999783">
Depending on the context setting used, our
model includes one (uniform), three (right) or six
(left-right) /t/-deletion probabilities pc. We place a
uniform Beta prior on each of those so as to learn
their values in the LEARN-p experiments below.
Finally, the observed unsegmented utterances
Wi are generated by concatenating all Si,j using
the function CAT.
We briefly comment on the central intuition
of this model, i.e. why it can infer underlying
</bodyText>
<footnote confidence="0.822195">
4For right there are three and for left-right six different
rules, one for every instantiation of the context-template.
</footnote>
<bodyText confidence="0.999833444444445">
from surface forms. Bayesian word segmentation
models try to compactly represent the observed
data in terms of a small set of units (word types)
and a short analysis (a small number of word
tokens). Phonological rules such as /t/-deletion
can “explain away” an observed surface type such
as [wes]] in terms of the underlying type /west/
which is independently needed for surface tokens
of [west]. Thus, the /t/ 0 rule makes possi-
ble a smaller lexicon for a given number of sur-
face tokens. Obviously, human learners have ac-
cess to additional cues, such as the meaning of
words, knowledge of phonological similarity be-
tween segments and so forth. One of the advan-
tages of an explicitly defined generative model
such as ours is that it is straight-forward to grad-
ually extend it by adding more cues, as we point
out in the discussion.
</bodyText>
<subsectionHeader confidence="0.776562">
3.1 Inference
</subsectionHeader>
<bodyText confidence="0.999846724137931">
Just as for the Goldwater et al. (2009) segmen-
tation models, exact inference is infeasible for
our joint model. We extend the collapsed Gibbs
breakpoint-sampler described in Goldwater et al.
(2009) to perform inference for our extended mod-
els. We refer the reader to their paper for addi-
tional details such as how to calculate the Bigram
probabilities in Figure 4. Here we focus on the
required changes to the sampler so as to perform
inference under our richer model. We consider the
case of a single surface string W, so we drop the
i-index in the following discussion.
Knowing W, the problem is to recover the un-
derlying forms U1, ... , Un and the surface forms
S1, ... , Sn for unknown n. A major insight in
Goldwater’s work is that rather than sampling over
the latent variables in the model directly (the num-
ber of which we don’t even know), we can instead
perform Gibbs sampling over a set of boundary
variables b1, ... , b|W|−1 that jointly determine the
values for our variables of interest where |W  |is
the length of the surface string W. For our model,
each bj E 10, 1, t}, where bj = 0 indicates ab-
sence of a word boundary, bj = 1 indicates pres-
ence of a boundary and bj = t indicates pres-
ence of a boundary with a preceeding underlying
/t/. The relation between the bj and the S1, ... , Sn
and U1, ... , Un is illustrated in Figure 5. The re-
quired sampling equations are given in Figure 4.
</bodyText>
<page confidence="0.582448">
1511
</page>
<equation confidence="0.9995398">
P(bj = 0  |b−j) a P(w12,u  |wl,u, b−j) X Pr(w12,s  |w12,u, wr,u) X P(wr,u  |w12,u, b−j ® (wl,u, w12,u)) (1)
P(bj = t  |b−j) a P(w1,t  |wl,u, b−j) X Pr(w1,s  |w1,t, w2,u) X P(w2,u  |w1,t, b−j ® (wl,u, w1,t))
X Pr(w2,s  |w2,u, wr,u) X P(wr,u  |w2,u, b−j ® (wl,u, w1,t) ® (w1,t, w2,u)) (2)
P(bj = 1  |b−j) a P(w1,s  |wl,u, b−j) X Pr(w1,s  |w1,s, w2,u) X P(w2,u  |w1,s, b−j ® (wl,u, w1,s))
X Pr(w2,s  |w2,u, wr,u) X P(wr,u  |w2,u, b−j ® (wl,u, w1,s) ® (w1,s, w2,u)) (3)
</equation>
<figureCaption confidence="0.98948025">
Figure 4: Sampling equations for our Gibbs sampler, see figure 5 for illustration. bj = 0 corresponds
to no boundary at this position, bj = t to a boundary with a preceeding underlying /t/ and bj = 1 to a
boundary with no additional underlying /t/. We use b−j for the statistics determined by all but the jth
position and b−j ® (r, l) for these statistics plus an additional count of the bigram (r, l). P(w  |l, b)
refers to the bigram probability of (l, w) given the the statistics b; we refer the reader to Goldwater et
al. (2009) for the details of calculating these bigram probabilities and details about the required statistics
for the collapsed sampler. PR is defined in the text.
Figure 5: The relation between the observed se-
</figureCaption>
<bodyText confidence="0.936297916666667">
quence of segments (bottom), the boundary vari-
ables b1, ... , b|W|−1 the Gibbs sampler operates
over (in squares), the latent sequence of sur-
face forms and the latent sequence of underly-
ing forms. When sampling a new value for
b3 = t, the different word-variables in fig-
ure 4 are: w12,u=w12,s=hiit, w1,t=hit and w1,s=hi,
w2,u=w2,s=it, wl,u=I, wr,u=$. Note that we need
a boundary variable at the end of the utterance as
there might be an underlying /t/ at this position as
well. The final boundary variable is set to 1, not t,
because the /t/ in it is observed.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994303">
4.1 The data
</subsectionHeader>
<bodyText confidence="0.999975692307692">
We are interested in how well our model han-
dles /t/-deletion in real data. Ideally, we’d eval-
uate it on CDS but as of now, we know of no
available large enough corpus of accurately hand-
transcribed CDS. Instead, we used the Buckeye
Corpus (Pitt et al., 2007) for our experiments,
a large ADS corpus of interviews with English
speakers that have been transcribed with relatively
fine phonetic detail, with /t/-deletion among the
things manually annotated. Pointing to the re-
cent work by Dilley et al. (to appear) we want
to emphasize that the statistical distribution of /t/-
deletion has been found to be similar for ADS and
</bodyText>
<figure confidence="0.6636895">
orthographic I don’t intend to
transcript /ar R oU n r n ten d @/
idealized /ar d oU n t r n t e n d t U/
t-drop /ar d oU n r n ten d t U/
</figure>
<figureCaption confidence="0.899208">
Figure 6: An example fragment from the Buckeye-
</figureCaption>
<bodyText confidence="0.976090413793104">
corpus in orthographic form, the fine transcript
available in the Buckeye corpus, a fully idealized
pronunciation with canonical dictionary pronunci-
ations and our version of the data with dropped
/t/s.
CDS, at least for read speech.
We automatically derived a corpus of 285,792
word tokens across 48,795 utterances from the
Buckeye Corpus by collecting utterances across all
interviews and heuristically splitting utterances at
speaker-turn changes and indicated silences. The
Buckeye corpus lists for each word token a man-
ually transcribed pronunciation in context as well
as its canonical pronunciation as given in a pro-
nouncing dictionary. As input to our model, we
use the canonical pronunciation unless the pronun-
ciation in context indicates that the final /t/ has
been deleted in which case we also delete the final
/t/ of the canonical pronunciation Figure 6 shows
an example from the Buckeye Corpus, indicating
how the original data, a fully idealized version
and our derived input that takes into account /t/-
deletions looks like.
Overall, /t/-deletion is a quite frequent phe-
nomenon with roughly 29% of all underlying /t/s
being dropped. The probabilities become more
peaked when looking at finer context; see Table 3
for the empirical distribution of /t/-dropping for
the six different contexts of the left-right setting.
</bodyText>
<figure confidence="0.995963647058824">
underlying
surface
boundaries
observed
I h i t i t $
I h i i t $
I
h
i
i
t
$
1
0
t
1
1
</figure>
<page confidence="0.941721">
1512
</page>
<subsectionHeader confidence="0.791296">
4.2 Recovering deleted /t/s, given word
boundaries
</subsectionHeader>
<bodyText confidence="0.999992545454546">
In this set of experiments we are interested in how
well our model recovers /t/s when it is provided
with the gold word boundaries. This allows us
to investigate the strength of the statistical sig-
nal for the deletion rule without confounding it
with the word segmentation performance, and to
see how the different contextual settings uniform,
right and left-right handle the data. Concretely,
for the example in Figure 6 this means that we tell
the model that there are boundaries between /aI/,
/down/, /Intend/, /tu/ and /liv/ but we don’t tell it
whether or not these words end in an underlying
/t/. Even in this simple example, there are 5 possi-
ble positions for the model to posit an underlying
/t/. We evaluate the model in terms of F-score, the
harmonic mean of recall (the fraction of underly-
ing /t/s the model correctly recovered) and preci-
sion (the fraction of underlying /t/s the model pre-
dicted that were correct).
In these experiments, we ran a total of 2500 it-
erations with a burnin of 2000. We collect sam-
ples with a lag of 10 for the last 500 iterations and
perform maximum marginal decoding over these
samples (Johnson and Goldwater, 2009), as well
as running two chains so as to get an idea of the
variance.5
We are also interested in how well the model
can infer the rule probabilities from the data, that
is, whether it can learn values for the different p,
parameters. We compare two settings, one where
we perform inference for these parameters assum-
ing a uniform Beta prior on each p, (LEARN-p)
and one where we provide the model with the em-
pirical probabilities for each p, as estimated off
the gold-data (GOLD-p), e.g., for the uniform con-
dition 0.29. The results are shown in Table 2.
Best performance for both the Unigram and
the Bigram model in the GOLD-p condition is
achieved under the left-right setting, in line with
the standard analyses of /t/-deletion as primarily
being determined by the preceding and the follow-
ing context. For the LEARN-p condition, the Bi-
gram model still performs best in the left-right set-
ting but the Unigram model’s performance drops
</bodyText>
<footnote confidence="0.510385">
5As manually setting the hyper-parameters for the DPs in
our model proved to be complicated and may be objected to
on principled grounds, we perform inference for them under
a vague gamma prior, as suggested by Teh et al. (2006) and
Johnson and Goldwater (2009), using our own implementa-
tion of a slice-sampler (Neal, 2003).
</footnote>
<table confidence="0.999364142857143">
uniform right left-right
LEARN-P 56.52 39.28 23.59
Unigram
GOLD-P 62.08 60.80 66.15
LEARN-P 60.85 62.98 77.76
Bigram
GOLD-P 69.06 69.98 73.45
</table>
<tableCaption confidence="0.977342">
Table 2: F-score of recovered /t/s with known
</tableCaption>
<bodyText confidence="0.905293666666667">
word boundaries on real data for the three differ-
ent context settings, averaged over two runs (all
standard errors below 2%). Note how the Uni-
gram model always suffers in the LEARN-p condi-
tion whereas the Bigram model’s performance is
actually best for LEARN-p in the left-right setting.
</bodyText>
<table confidence="0.99782725">
C C C V C $ V C V V V $
empirical 0.62 0.42 0.36 0.23 0.15 0.07
Unigram 0.41 0.33 0.17 0.07 0.05 0.00
Bigram 0.70 0.58 0.43 0.17 0.13 0.06
</table>
<tableCaption confidence="0.996794">
Table 3: Inferred rule-probabilities for different
</tableCaption>
<bodyText confidence="0.988607842105263">
contexts in the left-right setting from one of the
runs. “C C” stands for the context where the
deleted /t/ is preceded and followed by a conso-
nant, “V $” stands for the context where it is pre-
ceded by a vowel and followed by the utterance
boundary. Note how the Unigram model severely
under-estimates and the Bigram model slightly
over-estimates the probabilities.
in all settings and is now worst in the left-right and
best in the uniform setting.
In fact, comparing the inferred probabilities
to the “ground truth” indicates that the Bigram
model estimates the true probabilities more ac-
curately than the Unigram model, as illustrated
in Table 3 for the left-right setting. The Bi-
gram model somewhat overestimates the probabil-
ity for all post-consonantal contexts but the Uni-
gram model severely underestimates the probabil-
ity of /t/-deletion across all contexts.
</bodyText>
<subsectionHeader confidence="0.999647">
4.3 Artificial data experiments
</subsectionHeader>
<bodyText confidence="0.936455083333333">
To test our Gibbs sampling inference procedure,
we ran it on artificial data generated according to
the model itself. If our inference procedure fails
to recover the underlying /t/s accurately in this set-
ting, we should not expect it to work well on actual
data. We generated our artificial data as follows.
We transformed the sequence of canonical pronun-
ciations in the Buckeye corpus (which we take to
be underlying forms here) by randomly deleting
final /t/s using empirical probabilities as shown in
Table 3 to generate a sequence of artificial sur-
face forms that serve as input to our models. We
</bodyText>
<page confidence="0.796798">
1513
</page>
<table confidence="0.999537714285714">
uniform right left-right
LEARN-p 94.35 23.55 (+) 63.06
Unigram 94.45 94.20 91.83
GOLD-p
LEARN-p 92.72 91.64 88.48
Bigram 92.88 92.33 89.32
GOLD-p
</table>
<tableCaption confidence="0.809685">
Table 4: F-score of /t/-recovery with known word
boundaries on artificial data, each condition tested
on data that corresponds to the assumption, aver-
aged over two runs (standard errors less than 2%
</tableCaption>
<equation confidence="0.2952835">
except (+) = 3.68%)).
Unigram
</equation>
<bodyText confidence="0.963980590909091">
LEARN-p
GOLD-p
Table 5: /t/-recovery F-scores when performing
joint word segmention in the left-right setting, av-
eraged over two runs (standard errors less than
2%). See Table 6 for the corresponding segmenta-
tion F-scores.
did this for all three context settings, always es-
timating the deletion probability for each context
from the gold-standard. The results of these exper-
iments are given in table 4. Interestingly, perfor-
mance on these artificial data is considerably bet-
ter than on the real data. In particular the Bigram
model is able to get consistently high F-scores for
both the LEARN-p and the GOLD-p setting. For
the Unigram model, we again observe the severe
drop in the LEARN-p setting for the right and left-
right settings although it does remarkably well in
the uniform setting, and performs well across all
settings in the GOLD-p condition. We take this to
show that our inference algorithm is in fact work-
ing as expected.
</bodyText>
<subsectionHeader confidence="0.999822">
4.4 Segmentation experiments
</subsectionHeader>
<bodyText confidence="0.99999044">
Finally, we are also interested to learn how well
we can do word segmentation and underlying /t/-
recovery jointly. Again, we look at both the
LEARN-p and GOLD-p conditions but focus on the
left-right setting as this worked best in the exper-
iments above. For these experiments, we perform
simulated annealing throughout the initial 2000 it-
erations, gradually cooling the temperature from
5 to 1, following the observation by Goldwater
et al. (2009) that without annealing, the Bigram
model gets stuck in sub-optimal parts of the solu-
tion space early on. During the annealing stage,
we prevent the model from performing inference
for underlying /t/s so that the annealing stage can
be seen as an elaborate initialisation scheme, and
we perform joint inference for the remaining 500
iterations, evaluating on the last sample and av-
eraging over two runs. As neither the Unigram
nor the Bigram model performs “perfect” word
segmentation, we expect to see a degradation in
/t/-recovery performance and this is what we find
indeed. To give an impression of the impact of
/t/-deletion, we also report numbers for running
only the segmentation model on the Buckeye data
with no deleted /t/s and on the data with deleted
/t/s. The /t/-recovery scores are given in Table 5
and segmentation scores in Table 6. Again the
Unigram model’s /t/-recovery score degrades dra-
matically in the LEARN-p condition. Looking at
the segmentation performance this isn’t too sur-
prising: the Unigram model’s poorer token F-
score, the standard measure of segmentation per-
formance on a word token level, suggests that it
misses many more boundaries than the Bigram
model to begin with and, consequently, can’t re-
cover any potential underlying /t/s at these bound-
aries. Also note that in the GOLD-p condition, our
joint Bigram model performs almost as well on
data with /t/-deletions as the word segmentation
model on data that includes no variation at all.
The generally worse performance of handling
variation as measured by /t/-recovery F-score
when performing joint segmentation is consistent
with the finding of Elsner et al. (2012) who report
considerable performance drops for their phono-
logical learner when working with induced bound-
aries (note, however, that their model does not per-
form joint inference, rather the induced boundaries
are given to their phonological learner as ground-
truth).
</bodyText>
<sectionHeader confidence="0.99812" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.99995325">
There are two interesting findings from our exper-
iments. First of all, we find a much larger differ-
ence between the Unigram and the Bigram model
in the LEARN-p condition than in the GOLD-p con-
dition. We suggest that this is due to the Unigram
model’s lack of dependencies between underlying
forms, depriving it of an important source of ev-
idence. Bigram dependencies provide additional
evidence for underlying /t/ that are deleted on the
surface, and because the Bigram model identifies
these underlying /t/ more accurately, it can also es-
timate the /t/ deletion probability more accurately.
</bodyText>
<table confidence="0.739984">
33.58
55.92
Bigram
55.64
57.62
1514
Unigram Bigram
LEARN-p 54.53 72.55 (2.3%)
GOLD-p 54.51 73.18
NO-p 54.61 70.12
NO-VAR 54.12 73.99
</table>
<tableCaption confidence="0.96845">
Table 6: Word segmentation F-scores for the /t/-
</tableCaption>
<bodyText confidence="0.995813142857143">
recovery F-scores in Table 5 averaged over two
runs (standard errors less than 2% unless given).
NO-p are scores for running just the word segmen-
tation model with no /t/-deletion rule on the data
that includes /t/-deletion, NO-VAR for running just
the word segmentation model on the data with no
/t/-deletions.
For example, /t/ dropping in “don’t you” yields
surface forms “don you”. Because the word bi-
gram probability P(you I don’t) is high, the bi-
gram model prefers to analyse surface “don” as
underlying “don’t”. The Unigram model does not
have access to word bigram information so the
underlying forms it posits are less accurate (as
shown in Table 2), and hence the estimate of the
/t/-deletion probability is also less accurate. When
the probabilities of deletion are pre-specified the
Unigram model performs better but still consider-
ably worse than the Bigram model when the word
boundaries are known, suggesting the importance
of non-phonological contextual effects that the Bi-
gram model but not the Unigram model can cap-
ture. This suggests that for example word pre-
dictability in context might be an important factor
contributing to /t/-deletion.
The other striking finding is the considerable
drop in performance between running on natural-
istic and artificially created data. This suggests
that the natural distribution of /t/-deletion is much
more complex than can be captured by statistics
over the phonological contexts we examined. Fol-
lowing Guy (1991), a finer-grained distinction for
the preceeding segments might address this prob-
lem.
Yet another suggestion comes from the recent
work in Coetzee and Kawahara (2013) who claim
that “[a] model that accounts perfectly for the
overall rate of application of some variable pro-
cess therefore does not necessarily account very
well for the actual application of the process to in-
dividual words.” They argue that in particular the
extremely high deletion rates typical of high fre-
quency items aren’t accurately captured when the
deletion probability is estimated across all types.
A look at the error patterns of our model on a sam-
ple from the Bigram model in the LEARN-p setting
on the naturalistic data suggests that this is in fact a
problem. For example, the word “just” has an ex-
tremely high rate of deletion with 1746
2442 = 0.71%.
While many tokens of “jus” are “explained away”
through predicting underlying /t/s, the (literally)
extra-ordinary frequency of “jus”-tokens lets our
model still posit it as an underlying form, although
with a much dampened frequency (of the 1746 sur-
face tokens, 1081 are analysed as being realiza-
tions of an underlying “just”).
The /t/-recovery performance drop when per-
forming joint word segmentation isn’t surprising
as even the Bigram model doesn’t deliver a very
high-quality segmentation to begin with, leading
to both sparsity (through missed word-boundaries)
and potential noise (through misplaced word-
boundaries). Using a more realistic generative
process for the underlying forms, for example an
Adaptor Grammar (Johnson et al., 2007), could
address this shortcoming in future work without
changing the overall architecture of the model al-
though novel inference algorithms might be re-
quired.
</bodyText>
<sectionHeader confidence="0.990588" genericHeader="conclusions">
6 Conclusion and outlook
</sectionHeader>
<bodyText confidence="0.999995478260869">
We presented a joint model for word segmentation
and the learning of phonological rule probabili-
ties from a corpus of transcribed speech. We find
that our Bigram model reaches 77% /t/-recovery
F-score when run with knowledge of true word-
boundaries and when it can make use of both the
preceeding and the following phonological con-
text, and that unlike the Unigram model it is able
to learn the probability of /t/-deletion in different
contexts. When performing joint word segmen-
tation on the Buckeye corpus, our Bigram model
reaches around above 55% F-score for recovering
deleted /t/s with a word segmentation F-score of
around 72% which is 2% better than running a Bi-
gram model that does not model /t/-deletion.
We identified additional factors that might help
handling /t/-deletion and similar phenomena. A
major advantage of our generative model is the
ease and transparency with which its assump-
tions can be modified and extended. For fu-
ture work we plan to incorporate into our model
richer phonological contexts, item- and frequency-
specific probabilities and more direct use of word
</bodyText>
<page confidence="0.962875">
1515
</page>
<bodyText confidence="0.999916642857143">
predictability. We also plan to extend our model
to handle additional phenomena, an obvious can-
didate being /d/-deletion.
Also, the two-level architecture we present is
not limited to the mapping being defined in terms
of rules rather than constraints in the spirit of Op-
timality Theory (Prince and Smolensky, 2004); we
plan to explore this alternative path as well in fu-
ture work.
To conclude, we presented a model that pro-
vides a clean framework to test the usefulness of
different factors for word segmentation and han-
dling phonological variation in a controlled man-
ner.
</bodyText>
<sectionHeader confidence="0.9976" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9982594">
We thank the anonymous reviewers for their
valuable comments. This research was sup-
ported under Australian Research Council’s Dis-
covery Projects funding scheme (project numbers
DP110102506 and DP110102593).
</bodyText>
<sectionHeader confidence="0.999025" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999770109589041">
Noam Chomsky and Morris Halle. 1968. The Sound
Pattern of English. Haper &amp; Row, New York.
Andries W. Coetzee and Shigeto Kawahara. 2013. Fre-
quency biases in phonological variation. Natural
Language and Linguisic Theory, 31:47–89.
Andries W. Coetzee. 2004. What it Means to be a
Loser: Non-Optimal Candidates in Optimality The-
ory. Ph.D. thesis, University of Massachusetts ,
Amherst.
Laura Dilley, Amanda Millett, J. Devin McAuley, and
Tonya R. Bergeson. to appear. Phonetic variation
in consonants in infant-directed and adult-directed
speech: The case of regressive place assimilation
in word-final alveolar stops. Journal of Child Lan-
guage.
Micha Elsner, Sharon Goldwater, and Jacob Eisenstein.
2012. Bootstrapping a unified model of lexical and
phonetic acquisition. In Proceedings of the 50th An-
nual Meeting of the Association for Computational
Linguistics, pages 184–193, Jeju Island, Korea. As-
sociation for Computational Linguistics.
Sharon Goldwater, Thomas L. Griffiths, and Mark
Johnson. 2009. A Bayesian framework for word
segmentation: Exploring the effects of context.
Cognition, 112(1):21–54.
Gregory R. Guy. 1991. Contextual conditioning in
variable lexical phonology. Language Variation and
Change, 3(2):223–39.
Mark Johnson and Sharon Goldwater. 2009. Im-
proving nonparameteric Bayesian inference: exper-
iments on unsupervised word segmentation with
adaptor grammars. In Proceedings of Human Lan-
guage Technologies: The 2009 Annual Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics, pages 317–325,
Boulder, Colorado, June. Association for Computa-
tional Linguistics.
Mark Johnson, Thomas L. Griffiths, and Sharon Gold-
water. 2007. Adaptor Grammars: A framework for
specifying compositional nonparametric Bayesian
models. In B. Sch¨olkopf, J. Platt, and T. Hoffman,
editors, Advances in Neural Information Processing
Systems 19, pages 641–648. MIT Press, Cambridge,
MA.
Mark Johnson. 2008. Using Adaptor Grammars to
identify synergies in the unsupervised acquisition of
linguistic structure. In Proceedings of the 46th An-
nual Meeting of the Association of Computational
Linguistics, pages 398–406, Columbus, Ohio. Asso-
ciation for Computational Linguistics.
Jason Naradowsky and Sharon Goldwater. 2009. Im-
proving morphology induction by learning spelling
rules. In Proceedings of the 21st international jont
conference on Artifical intelligence, pages 1531–
1536, San Francisco, CA, USA. Morgan Kaufmann
Publishers Inc.
Radford M. Neal. 2003. Slice sampling. Annals of
Statistics, 31:705–767.
Dennis Norris, James M. Mcqueen, Anne Cutler, and
Sally Butterfield. 1997. The possible-word con-
straint in the segmentation of continuous speech.
Cognitive Psychology, 34(3):191 – 243.
Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kies-
ling, William Raymond, Elizabeth Hume, and Eric
Fosler-Lussier. 2007. Buckeye corpus of conversa-
tional speech.
Alan Prince and Paul Smolensky. 2004. Optimality
Theory: Constraint Interaction in Generative Gram-
mar. Blackwell.
Yee Whye Teh, Michael Jordan, Matthew Beal, and
David Blei. 2006. Hierarchical Dirichlet pro-
cesses. Journal of the American Statistical Associa-
tion, 101:1566–1581.
</reference>
<page confidence="0.992265">
1516
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.646718">
<title confidence="0.9308805">A joint model of word segmentation and phonological variation English word-final /t/-deletion</title>
<affiliation confidence="0.992028">(1) Department of Computing, Macquarie University (2) Department of Linguistics, Macquarie University</affiliation>
<address confidence="0.92585">(3) Department of Computational Linguistics, Heidelberg</address>
<email confidence="0.975917">mark.johnson,</email>
<abstract confidence="0.99452208">Word-final /t/-deletion refers to a common phenomenon in spoken English where such as “west” are proas “wes” in certain contexts. Phonological variation like this is common in naturally occurring speech. Current computational models of unsupervised word segmentation usually assume idealized input that is devoid of these kinds of variation. We extend a non-parametric model of word segmentation by adding phonological rules that map from underlying forms to surface forms to produce a mathematically well-defined joint model as a first step towards handling variation and segmentation in a single model. We analyse how our model handles /t/-deletion on a large corpus of transcribed speech, and show that the joint model can perform word segmentation and recover underlying /t/s. We find that Bigram dependencies are important for performing well on real data and for learning appropriate deletion probabilities for dif-</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
<author>Morris Halle</author>
</authors>
<title>The Sound Pattern of English.</title>
<date>1968</date>
<publisher>Haper &amp; Row,</publisher>
<location>New York.</location>
<contexts>
<context position="12651" citStr="Chomsky and Halle, 1968" startWordPosition="2077" endWordPosition="2080">V(w) B(w = x1:n |-y) = Z 0.0 if -,V(w) Figure 3: Lexical generator with possible wordconstraint for words in E+, E being the alphabet of available phonemes. x1:n is a sequence of elements of E of length n. -y is a probability vector of length |E |+ 1 drawn from a sparse Dirichlet prior, giving the probability for each phoneme and the special word-boundary symbol #. The predicate V holds of all sequences containing at least one vowel. Z is a normalization constant that adjusts for the mass assigned to the empty and nonpossible words. contexts in the notation familiar from generative phonology (Chomsky and Halle, 1968), our model can be seen as implementing the following rules under the different assumptions:4 uniform /t/ 0 / ]word right /t/ 0 / ]word 0 left-right /t/ 0 / α ]word 0 We let 0 range over V(owel), C(onsonant) and $ (utterance-boundary), and α over V and C. We define a function CONT that maps a pair of adjacent underlying forms Ui,j, Ui,j+1 to the context of the final segment of Ui,j. For example, CONT(/west/,/@v/) returns “C ]word V” in the left-right setting, or simply “ ]word” in the uniform setting. CONT returns a special NOT context if Ui,j doesn’t end in a /t/. We stipulate that pNOT = 0.0</context>
</contexts>
<marker>Chomsky, Halle, 1968</marker>
<rawString>Noam Chomsky and Morris Halle. 1968. The Sound Pattern of English. Haper &amp; Row, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andries W Coetzee</author>
<author>Shigeto Kawahara</author>
</authors>
<title>Frequency biases in phonological variation.</title>
<date>2013</date>
<booktitle>Natural Language and Linguisic Theory,</booktitle>
<pages>31--47</pages>
<contexts>
<context position="6697" citStr="Coetzee and Kawahara (2013)" startWordPosition="1035" endWordPosition="1038"> study word-final variation in stop consonants in CDS, the kind of input we ideally would like to evaluate our models on. They find that “infants largely experience statistical distributions of non-canonical consonantal pronunciation variants [including deletion] that mirror those experienced by adults.” This both directly establishes the need 2Following the convention in phonology, we give underlying forms within “/... /” and surface forms within “[... ]”. for computational models to handle this dimension of variation, and justifies our choice of using ADS for evaluation, as mentioned above. Coetzee and Kawahara (2013) provide a computational study of (among other things) /t/- deletion within the framework of Harmonic Grammar. They do not aim for a joint model that also handles word segmentation, however, and rather than training their model on an actual corpus, they evaluate on constructed lists of examples, mimicking frequencies of real data. Overall, our findings agree with theirs, in particular that capturing the probability of deletion in different contexts does not automatically result in good performance for recovering individual deleted /t/s. We will come back to this point in our discussion at the </context>
<context position="30951" citStr="Coetzee and Kawahara (2013)" startWordPosition="5210" endWordPosition="5213">he Unigram model can capture. This suggests that for example word predictability in context might be an important factor contributing to /t/-deletion. The other striking finding is the considerable drop in performance between running on naturalistic and artificially created data. This suggests that the natural distribution of /t/-deletion is much more complex than can be captured by statistics over the phonological contexts we examined. Following Guy (1991), a finer-grained distinction for the preceeding segments might address this problem. Yet another suggestion comes from the recent work in Coetzee and Kawahara (2013) who claim that “[a] model that accounts perfectly for the overall rate of application of some variable process therefore does not necessarily account very well for the actual application of the process to individual words.” They argue that in particular the extremely high deletion rates typical of high frequency items aren’t accurately captured when the deletion probability is estimated across all types. A look at the error patterns of our model on a sample from the Bigram model in the LEARN-p setting on the naturalistic data suggests that this is in fact a problem. For example, the word “jus</context>
</contexts>
<marker>Coetzee, Kawahara, 2013</marker>
<rawString>Andries W. Coetzee and Shigeto Kawahara. 2013. Frequency biases in phonological variation. Natural Language and Linguisic Theory, 31:47–89.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andries W Coetzee</author>
</authors>
<title>What it Means to be a Loser: Non-Optimal Candidates in Optimality Theory.</title>
<date>2004</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Massachusetts ,</institution>
<location>Amherst.</location>
<contexts>
<context position="2369" citStr="Coetzee (2004" startWordPosition="355" endWordPosition="356">describe 1The implementation of our model as well as scripts to prepare the data will be made available at http://web.science.mq.edu.au/~bborschi. We can’t release our version of the Buckeye Corpus (Pitt et al., 2007) directly because of licensing issues. an extension of the Bayesian models of Goldwater et al. (2009) that incorporates phonological rules to “explain away” surface variation. As a concrete example, we focus on word-final /t/- deletion in English, although our approach is not limited to this case. We choose /t/-deletion because it is a very common and well-studied phenomenon (see Coetzee (2004, Chapter 5) for a review) and segmental deletion is an interesting test-case for our architecture. Recent work has found that /t/-deletion (among other things) is indeed common in child-directed speech (CDS) and, importantly, that its distribution is similar to that in adult-directed speech (ADS) (Dilley et al., to appear). This justifies our using ADS to evaluate our model, as discussed below. Our experiments are consistent with longstanding and recent findings in linguistics, in particular that /t/-deletion heavily depends on the immediate context and that models ignoring context work poorl</context>
<context position="5384" citStr="Coetzee (2004" startWordPosition="823" endWordPosition="824">ith a single well-studied example of phonological variation. This permits us to develop a joint generative model for both word segmentation and variation which we plan to extend to handle more phenomena in future work. An earlier work that is close to the spirit of our approach is Naradowsky and Goldwater (2009), who learn spelling rules jointly with a simple stem-suffix model of English verb morphology. Their model, however, doesn’t naturally extend to the segmentation of entire utterances. /t/-deletion has received a lot of attention within linguistics, and we point the interested reader to Coetzee (2004, Chapter 5) for a thorough review. Briefly, the phenomenon is as follows: word-final instances of /t/ may undergo deletion in natural speech, such that /west/ “west” is actually pronounced as [wes] “wes”.2 While the frequency of this phenomenon varies across social and dialectal groups, within groups it has been found to be robust, and the probability of deletion depends on its phonological context: a /t/ is more likely to be dropped when followed by a consonant than a vowel or a pause, and it is more likely to be dropped when following a consonant than a vowel as well. We point out two recen</context>
</contexts>
<marker>Coetzee, 2004</marker>
<rawString>Andries W. Coetzee. 2004. What it Means to be a Loser: Non-Optimal Candidates in Optimality Theory. Ph.D. thesis, University of Massachusetts , Amherst.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Laura Dilley</author>
<author>Amanda Millett</author>
<author>J Devin McAuley</author>
<author>Tonya R Bergeson</author>
</authors>
<title>to appear. Phonetic variation in consonants in infant-directed and adult-directed speech: The case of regressive place assimilation in word-final alveolar stops.</title>
<journal>Journal of Child Language.</journal>
<marker>Dilley, Millett, McAuley, Bergeson, </marker>
<rawString>Laura Dilley, Amanda Millett, J. Devin McAuley, and Tonya R. Bergeson. to appear. Phonetic variation in consonants in infant-directed and adult-directed speech: The case of regressive place assimilation in word-final alveolar stops. Journal of Child Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Micha Elsner</author>
<author>Sharon Goldwater</author>
<author>Jacob Eisenstein</author>
</authors>
<title>Bootstrapping a unified model of lexical and phonetic acquisition.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>184--193</pages>
<institution>Jeju Island, Korea. Association for Computational Linguistics.</institution>
<contexts>
<context position="4099" citStr="Elsner et al. (2012)" startWordPosition="622" endWordPosition="625">ts performance for recovering deleted /t/s. We look at both a situation where word boundaries are pre-specified and only inference for underlying forms has to be performed; and the problem of jointly finding the word boundaries and recovering deleted underlying /t/s. Section 5 discusses our findings, and section 6 concludes with directions for further research. 1508 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1508–1516, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Background and related work The work of Elsner et al. (2012) is most closely related to our goal of building a model that handles variation. They propose a pipe-line architecture involving two separate generative models, one for word-segmentation and one for phonological variation. They model the mapping to surface forms using a probabilistic finite-state transducer. This allows their architecture to handle virtually arbitrary pronunciation variation. However, as they point out, combining the segmentation and the variation model into one joint model is not straight-forward and usual inference procedures are infeasible, which requires the use of several</context>
<context position="28266" citStr="Elsner et al. (2012)" startWordPosition="4782" endWordPosition="4785">r token Fscore, the standard measure of segmentation performance on a word token level, suggests that it misses many more boundaries than the Bigram model to begin with and, consequently, can’t recover any potential underlying /t/s at these boundaries. Also note that in the GOLD-p condition, our joint Bigram model performs almost as well on data with /t/-deletions as the word segmentation model on data that includes no variation at all. The generally worse performance of handling variation as measured by /t/-recovery F-score when performing joint segmentation is consistent with the finding of Elsner et al. (2012) who report considerable performance drops for their phonological learner when working with induced boundaries (note, however, that their model does not perform joint inference, rather the induced boundaries are given to their phonological learner as groundtruth). 5 Discussion There are two interesting findings from our experiments. First of all, we find a much larger difference between the Unigram and the Bigram model in the LEARN-p condition than in the GOLD-p condition. We suggest that this is due to the Unigram model’s lack of dependencies between underlying forms, depriving it of an impor</context>
</contexts>
<marker>Elsner, Goldwater, Eisenstein, 2012</marker>
<rawString>Micha Elsner, Sharon Goldwater, and Jacob Eisenstein. 2012. Bootstrapping a unified model of lexical and phonetic acquisition. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 184–193, Jeju Island, Korea. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sharon Goldwater</author>
<author>Thomas L Griffiths</author>
<author>Mark Johnson</author>
</authors>
<title>A Bayesian framework for word segmentation: Exploring the effects of context.</title>
<date>2009</date>
<journal>Cognition,</journal>
<volume>112</volume>
<issue>1</issue>
<contexts>
<context position="2074" citStr="Goldwater et al. (2009)" startWordPosition="304" endWordPosition="308"> first problems language learners have to face: breaking an unsegmented stream of sound segments into individual words. Currently, most such models assume that the input consists of sequences of phonemes with no pronunciation variation across different occurrences of the same word type. In this paper we describe 1The implementation of our model as well as scripts to prepare the data will be made available at http://web.science.mq.edu.au/~bborschi. We can’t release our version of the Buckeye Corpus (Pitt et al., 2007) directly because of licensing issues. an extension of the Bayesian models of Goldwater et al. (2009) that incorporates phonological rules to “explain away” surface variation. As a concrete example, we focus on word-final /t/- deletion in English, although our approach is not limited to this case. We choose /t/-deletion because it is a very common and well-studied phenomenon (see Coetzee (2004, Chapter 5) for a review) and segmental deletion is an interesting test-case for our architecture. Recent work has found that /t/-deletion (among other things) is indeed common in child-directed speech (CDS) and, importantly, that its distribution is similar to that in adult-directed speech (ADS) (Dille</context>
<context position="7431" citStr="Goldwater et al. (2009)" startWordPosition="1155" endWordPosition="1158">They do not aim for a joint model that also handles word segmentation, however, and rather than training their model on an actual corpus, they evaluate on constructed lists of examples, mimicking frequencies of real data. Overall, our findings agree with theirs, in particular that capturing the probability of deletion in different contexts does not automatically result in good performance for recovering individual deleted /t/s. We will come back to this point in our discussion at the end of the paper. 3 The computational model Our models build on the Unigram and the Bigram model introduced in Goldwater et al. (2009). Figure 1 shows the graphical model for our joint Bigram model (the Unigram case is trivially recovered by generating the Ui,js directly from L rather than from LU,,�−1). Figure 2 gives the mathematical description of the graphical model and Table 1 provides a key to the variables of our model. The model generates a latent sequence of underlying word-tokens U1, ... , Un. Each word token is itself a non-empty sequence of segments or phonemes, and each Uj corresponds to an underlying word form, prior to the application of any phonological rule. This generative process is repeated for each utter</context>
<context position="8884" citStr="Goldwater et al. (2009)" startWordPosition="1426" endWordPosition="1429">ymbol $ to the left and to the right, hence Ui,0 = Ui,nz+1 = $.3 Each Ui,j+1 is generated conditionally on its predecessor Ui,j from LU,,,, as shown in the first row of the lower plate in Figure 1. Each Lw is a distribution over the possible words that can follow a token of w and L is a global distribution over possible words, used as back-off for all Lw. Just as in Goldwater et al. (2009), L is drawn from a Dirichlet Process (DP) with base distribution B and concentration 3Each utterance terminates as soon as a $ is generated, thus determining the number of words nz in the ith utterance. See Goldwater et al. (2009) for discussion. 1509 Figure 1: The graphical model for our joint model of word-final /t/-deletion and Bigram word segmentation. The corresponding mathematical description is given in Figure 2. The generative process mimics the intuitively plausible idea of generating underlying forms from some kind of syntactic model (here, a Bigram language model) and then mapping the underlying form to an observed surface-form through the application of a phonological rule component, here represented by the collection of rule probabilities ρc. L |&apos;Y, α0 — DP (α0, B(· |&apos;Y)) Lw |L, α1 — DP(α1, L) ρc |β — Beta</context>
<context position="11010" citStr="Goldwater et al. (2009)" startWordPosition="1786" endWordPosition="1789"> (Teh et al., 2006). The base distribution B functions as a lexical generator, defining a prior distribution over possible words. In principle, B can incorporate arbitrary prior knowledge about possible words, for example syllable structure (cf. Johnson (2008)). Inspired by Norris et al. (1997), we use a simpler possible word constraint that only rules out sequences that lack a vowel (see Figure 3). While this is clearly a simplification it is a plausible assumption for English data. Instead of generating the observed sequence of segments W directly by concatenating the underlying forms as in Goldwater et al. (2009), we map each Ui,j to a corresponding surface-form Si,j by a probabilistic rule component PR. The values over which the Si,j range are determined by the available phonological processes. In the model we study here, the phonological processes only include a rule for deleting word-final /t/s but in principle, PR can be used to encode a wide variety of phonological rules. Here, Si,j E {Ui,j, DELF(Ui,j)} if Ui,j ends in a /t/, and Si,j = Ui,j otherwise, where DELF(u) refers to the same word as u except that it lacks u’s final segment. We look at three kinds of contexts on which a rule’s probabilit</context>
<context position="14807" citStr="Goldwater et al. (2009)" startWordPosition="2454" endWordPosition="2457">bserved surface type such as [wes]] in terms of the underlying type /west/ which is independently needed for surface tokens of [west]. Thus, the /t/ 0 rule makes possible a smaller lexicon for a given number of surface tokens. Obviously, human learners have access to additional cues, such as the meaning of words, knowledge of phonological similarity between segments and so forth. One of the advantages of an explicitly defined generative model such as ours is that it is straight-forward to gradually extend it by adding more cues, as we point out in the discussion. 3.1 Inference Just as for the Goldwater et al. (2009) segmentation models, exact inference is infeasible for our joint model. We extend the collapsed Gibbs breakpoint-sampler described in Goldwater et al. (2009) to perform inference for our extended models. We refer the reader to their paper for additional details such as how to calculate the Bigram probabilities in Figure 4. Here we focus on the required changes to the sampler so as to perform inference under our richer model. We consider the case of a single surface string W, so we drop the i-index in the following discussion. Knowing W, the problem is to recover the underlying forms U1, ... ,</context>
<context position="17157" citStr="Goldwater et al. (2009)" startWordPosition="2900" endWordPosition="2903">(wl,u, w1,s)) X Pr(w2,s |w2,u, wr,u) X P(wr,u |w2,u, b−j ® (wl,u, w1,s) ® (w1,s, w2,u)) (3) Figure 4: Sampling equations for our Gibbs sampler, see figure 5 for illustration. bj = 0 corresponds to no boundary at this position, bj = t to a boundary with a preceeding underlying /t/ and bj = 1 to a boundary with no additional underlying /t/. We use b−j for the statistics determined by all but the jth position and b−j ® (r, l) for these statistics plus an additional count of the bigram (r, l). P(w |l, b) refers to the bigram probability of (l, w) given the the statistics b; we refer the reader to Goldwater et al. (2009) for the details of calculating these bigram probabilities and details about the required statistics for the collapsed sampler. PR is defined in the text. Figure 5: The relation between the observed sequence of segments (bottom), the boundary variables b1, ... , b|W|−1 the Gibbs sampler operates over (in squares), the latent sequence of surface forms and the latent sequence of underlying forms. When sampling a new value for b3 = t, the different word-variables in figure 4 are: w12,u=w12,s=hiit, w1,t=hit and w1,s=hi, w2,u=w2,s=it, wl,u=I, wr,u=$. Note that we need a boundary variable at the end</context>
<context position="26610" citStr="Goldwater et al. (2009)" startWordPosition="4513" endWordPosition="4516"> and performs well across all settings in the GOLD-p condition. We take this to show that our inference algorithm is in fact working as expected. 4.4 Segmentation experiments Finally, we are also interested to learn how well we can do word segmentation and underlying /t/- recovery jointly. Again, we look at both the LEARN-p and GOLD-p conditions but focus on the left-right setting as this worked best in the experiments above. For these experiments, we perform simulated annealing throughout the initial 2000 iterations, gradually cooling the temperature from 5 to 1, following the observation by Goldwater et al. (2009) that without annealing, the Bigram model gets stuck in sub-optimal parts of the solution space early on. During the annealing stage, we prevent the model from performing inference for underlying /t/s so that the annealing stage can be seen as an elaborate initialisation scheme, and we perform joint inference for the remaining 500 iterations, evaluating on the last sample and averaging over two runs. As neither the Unigram nor the Bigram model performs “perfect” word segmentation, we expect to see a degradation in /t/-recovery performance and this is what we find indeed. To give an impression </context>
</contexts>
<marker>Goldwater, Griffiths, Johnson, 2009</marker>
<rawString>Sharon Goldwater, Thomas L. Griffiths, and Mark Johnson. 2009. A Bayesian framework for word segmentation: Exploring the effects of context. Cognition, 112(1):21–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory R Guy</author>
</authors>
<title>Contextual conditioning in variable lexical phonology.</title>
<date>1991</date>
<journal>Language Variation and Change,</journal>
<volume>3</volume>
<issue>2</issue>
<contexts>
<context position="30785" citStr="Guy (1991)" startWordPosition="5187" endWordPosition="5188">the Bigram model when the word boundaries are known, suggesting the importance of non-phonological contextual effects that the Bigram model but not the Unigram model can capture. This suggests that for example word predictability in context might be an important factor contributing to /t/-deletion. The other striking finding is the considerable drop in performance between running on naturalistic and artificially created data. This suggests that the natural distribution of /t/-deletion is much more complex than can be captured by statistics over the phonological contexts we examined. Following Guy (1991), a finer-grained distinction for the preceeding segments might address this problem. Yet another suggestion comes from the recent work in Coetzee and Kawahara (2013) who claim that “[a] model that accounts perfectly for the overall rate of application of some variable process therefore does not necessarily account very well for the actual application of the process to individual words.” They argue that in particular the extremely high deletion rates typical of high frequency items aren’t accurately captured when the deletion probability is estimated across all types. A look at the error patte</context>
</contexts>
<marker>Guy, 1991</marker>
<rawString>Gregory R. Guy. 1991. Contextual conditioning in variable lexical phonology. Language Variation and Change, 3(2):223–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars.</title>
<date>2009</date>
<booktitle>In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>317--325</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Boulder, Colorado,</location>
<contexts>
<context position="21389" citStr="Johnson and Goldwater, 2009" startWordPosition="3639" endWordPosition="3642"> but we don’t tell it whether or not these words end in an underlying /t/. Even in this simple example, there are 5 possible positions for the model to posit an underlying /t/. We evaluate the model in terms of F-score, the harmonic mean of recall (the fraction of underlying /t/s the model correctly recovered) and precision (the fraction of underlying /t/s the model predicted that were correct). In these experiments, we ran a total of 2500 iterations with a burnin of 2000. We collect samples with a lag of 10 for the last 500 iterations and perform maximum marginal decoding over these samples (Johnson and Goldwater, 2009), as well as running two chains so as to get an idea of the variance.5 We are also interested in how well the model can infer the rule probabilities from the data, that is, whether it can learn values for the different p, parameters. We compare two settings, one where we perform inference for these parameters assuming a uniform Beta prior on each p, (LEARN-p) and one where we provide the model with the empirical probabilities for each p, as estimated off the gold-data (GOLD-p), e.g., for the uniform condition 0.29. The results are shown in Table 2. Best performance for both the Unigram and the</context>
</contexts>
<marker>Johnson, Goldwater, 2009</marker>
<rawString>Mark Johnson and Sharon Goldwater. 2009. Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 317–325, Boulder, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
<author>Thomas L Griffiths</author>
<author>Sharon Goldwater</author>
</authors>
<title>Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems 19,</booktitle>
<pages>641--648</pages>
<editor>In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="32361" citStr="Johnson et al., 2007" startWordPosition="5436" endWordPosition="5439">y of “jus”-tokens lets our model still posit it as an underlying form, although with a much dampened frequency (of the 1746 surface tokens, 1081 are analysed as being realizations of an underlying “just”). The /t/-recovery performance drop when performing joint word segmentation isn’t surprising as even the Bigram model doesn’t deliver a very high-quality segmentation to begin with, leading to both sparsity (through missed word-boundaries) and potential noise (through misplaced wordboundaries). Using a more realistic generative process for the underlying forms, for example an Adaptor Grammar (Johnson et al., 2007), could address this shortcoming in future work without changing the overall architecture of the model although novel inference algorithms might be required. 6 Conclusion and outlook We presented a joint model for word segmentation and the learning of phonological rule probabilities from a corpus of transcribed speech. We find that our Bigram model reaches 77% /t/-recovery F-score when run with knowledge of true wordboundaries and when it can make use of both the preceeding and the following phonological context, and that unlike the Unigram model it is able to learn the probability of /t/-dele</context>
</contexts>
<marker>Johnson, Griffiths, Goldwater, 2007</marker>
<rawString>Mark Johnson, Thomas L. Griffiths, and Sharon Goldwater. 2007. Adaptor Grammars: A framework for specifying compositional nonparametric Bayesian models. In B. Sch¨olkopf, J. Platt, and T. Hoffman, editors, Advances in Neural Information Processing Systems 19, pages 641–648. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Using Adaptor Grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>398--406</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio.</location>
<contexts>
<context position="10647" citStr="Johnson (2008)" startWordPosition="1724" endWordPosition="1725">ng form, a word Si,j surface realization of Ui,j, a word ρc /t/-deletion probability in context c Wi observed segments for ith utterance Table 1: Key for the variables in Figure 1 and Figure 2. See Figure 3 for the definition of B. parameter α0, and the word type specific distributions Lw are drawn from a DP(L, α1), resulting in a hierarchical DP model (Teh et al., 2006). The base distribution B functions as a lexical generator, defining a prior distribution over possible words. In principle, B can incorporate arbitrary prior knowledge about possible words, for example syllable structure (cf. Johnson (2008)). Inspired by Norris et al. (1997), we use a simpler possible word constraint that only rules out sequences that lack a vowel (see Figure 3). While this is clearly a simplification it is a plausible assumption for English data. Instead of generating the observed sequence of segments W directly by concatenating the underlying forms as in Goldwater et al. (2009), we map each Ui,j to a corresponding surface-form Si,j by a probabilistic rule component PR. The values over which the Si,j range are determined by the available phonological processes. In the model we study here, the phonological proce</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>Mark Johnson. 2008. Using Adaptor Grammars to identify synergies in the unsupervised acquisition of linguistic structure. In Proceedings of the 46th Annual Meeting of the Association of Computational Linguistics, pages 398–406, Columbus, Ohio. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Naradowsky</author>
<author>Sharon Goldwater</author>
</authors>
<title>Improving morphology induction by learning spelling rules.</title>
<date>2009</date>
<booktitle>In Proceedings of the 21st international jont conference on Artifical intelligence,</booktitle>
<pages>1531--1536</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="5084" citStr="Naradowsky and Goldwater (2009)" startWordPosition="777" endWordPosition="780">tually arbitrary pronunciation variation. However, as they point out, combining the segmentation and the variation model into one joint model is not straight-forward and usual inference procedures are infeasible, which requires the use of several heuristics. We pursue an alternative research strategy here, starting with a single well-studied example of phonological variation. This permits us to develop a joint generative model for both word segmentation and variation which we plan to extend to handle more phenomena in future work. An earlier work that is close to the spirit of our approach is Naradowsky and Goldwater (2009), who learn spelling rules jointly with a simple stem-suffix model of English verb morphology. Their model, however, doesn’t naturally extend to the segmentation of entire utterances. /t/-deletion has received a lot of attention within linguistics, and we point the interested reader to Coetzee (2004, Chapter 5) for a thorough review. Briefly, the phenomenon is as follows: word-final instances of /t/ may undergo deletion in natural speech, such that /west/ “west” is actually pronounced as [wes] “wes”.2 While the frequency of this phenomenon varies across social and dialectal groups, within grou</context>
</contexts>
<marker>Naradowsky, Goldwater, 2009</marker>
<rawString>Jason Naradowsky and Sharon Goldwater. 2009. Improving morphology induction by learning spelling rules. In Proceedings of the 21st international jont conference on Artifical intelligence, pages 1531– 1536, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radford M Neal</author>
</authors>
<title>Slice sampling.</title>
<date>2003</date>
<journal>Annals of Statistics,</journal>
<pages>31--705</pages>
<contexts>
<context position="22649" citStr="Neal, 2003" startWordPosition="3859" endWordPosition="3860"> under the left-right setting, in line with the standard analyses of /t/-deletion as primarily being determined by the preceding and the following context. For the LEARN-p condition, the Bigram model still performs best in the left-right setting but the Unigram model’s performance drops 5As manually setting the hyper-parameters for the DPs in our model proved to be complicated and may be objected to on principled grounds, we perform inference for them under a vague gamma prior, as suggested by Teh et al. (2006) and Johnson and Goldwater (2009), using our own implementation of a slice-sampler (Neal, 2003). uniform right left-right LEARN-P 56.52 39.28 23.59 Unigram GOLD-P 62.08 60.80 66.15 LEARN-P 60.85 62.98 77.76 Bigram GOLD-P 69.06 69.98 73.45 Table 2: F-score of recovered /t/s with known word boundaries on real data for the three different context settings, averaged over two runs (all standard errors below 2%). Note how the Unigram model always suffers in the LEARN-p condition whereas the Bigram model’s performance is actually best for LEARN-p in the left-right setting. C C C V C $ V C V V V $ empirical 0.62 0.42 0.36 0.23 0.15 0.07 Unigram 0.41 0.33 0.17 0.07 0.05 0.00 Bigram 0.70 0.58 0.4</context>
</contexts>
<marker>Neal, 2003</marker>
<rawString>Radford M. Neal. 2003. Slice sampling. Annals of Statistics, 31:705–767.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dennis Norris</author>
<author>James M Mcqueen</author>
<author>Anne Cutler</author>
<author>Sally Butterfield</author>
</authors>
<title>The possible-word constraint in the segmentation of continuous speech.</title>
<date>1997</date>
<journal>Cognitive Psychology,</journal>
<volume>34</volume>
<issue>3</issue>
<pages>243</pages>
<contexts>
<context position="10682" citStr="Norris et al. (1997)" startWordPosition="1729" endWordPosition="1732">realization of Ui,j, a word ρc /t/-deletion probability in context c Wi observed segments for ith utterance Table 1: Key for the variables in Figure 1 and Figure 2. See Figure 3 for the definition of B. parameter α0, and the word type specific distributions Lw are drawn from a DP(L, α1), resulting in a hierarchical DP model (Teh et al., 2006). The base distribution B functions as a lexical generator, defining a prior distribution over possible words. In principle, B can incorporate arbitrary prior knowledge about possible words, for example syllable structure (cf. Johnson (2008)). Inspired by Norris et al. (1997), we use a simpler possible word constraint that only rules out sequences that lack a vowel (see Figure 3). While this is clearly a simplification it is a plausible assumption for English data. Instead of generating the observed sequence of segments W directly by concatenating the underlying forms as in Goldwater et al. (2009), we map each Ui,j to a corresponding surface-form Si,j by a probabilistic rule component PR. The values over which the Si,j range are determined by the available phonological processes. In the model we study here, the phonological processes only include a rule for deleti</context>
</contexts>
<marker>Norris, Mcqueen, Cutler, Butterfield, 1997</marker>
<rawString>Dennis Norris, James M. Mcqueen, Anne Cutler, and Sally Butterfield. 1997. The possible-word constraint in the segmentation of continuous speech. Cognitive Psychology, 34(3):191 – 243.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark A Pitt</author>
<author>Laura Dilley</author>
<author>Keith Johnson</author>
</authors>
<title>Hume, and Eric Fosler-Lussier.</title>
<date>2007</date>
<location>Scott Kiesling, William Raymond, Elizabeth</location>
<contexts>
<context position="1973" citStr="Pitt et al., 2007" startWordPosition="288" endWordPosition="291">rent contexts.1 1 Introduction Computational models of word segmentation try to solve one of the first problems language learners have to face: breaking an unsegmented stream of sound segments into individual words. Currently, most such models assume that the input consists of sequences of phonemes with no pronunciation variation across different occurrences of the same word type. In this paper we describe 1The implementation of our model as well as scripts to prepare the data will be made available at http://web.science.mq.edu.au/~bborschi. We can’t release our version of the Buckeye Corpus (Pitt et al., 2007) directly because of licensing issues. an extension of the Bayesian models of Goldwater et al. (2009) that incorporates phonological rules to “explain away” surface variation. As a concrete example, we focus on word-final /t/- deletion in English, although our approach is not limited to this case. We choose /t/-deletion because it is a very common and well-studied phenomenon (see Coetzee (2004, Chapter 5) for a review) and segmental deletion is an interesting test-case for our architecture. Recent work has found that /t/-deletion (among other things) is indeed common in child-directed speech (</context>
<context position="18204" citStr="Pitt et al., 2007" startWordPosition="3085" endWordPosition="3088">3 = t, the different word-variables in figure 4 are: w12,u=w12,s=hiit, w1,t=hit and w1,s=hi, w2,u=w2,s=it, wl,u=I, wr,u=$. Note that we need a boundary variable at the end of the utterance as there might be an underlying /t/ at this position as well. The final boundary variable is set to 1, not t, because the /t/ in it is observed. 4 Experiments 4.1 The data We are interested in how well our model handles /t/-deletion in real data. Ideally, we’d evaluate it on CDS but as of now, we know of no available large enough corpus of accurately handtranscribed CDS. Instead, we used the Buckeye Corpus (Pitt et al., 2007) for our experiments, a large ADS corpus of interviews with English speakers that have been transcribed with relatively fine phonetic detail, with /t/-deletion among the things manually annotated. Pointing to the recent work by Dilley et al. (to appear) we want to emphasize that the statistical distribution of /t/- deletion has been found to be similar for ADS and orthographic I don’t intend to transcript /ar R oU n r n ten d @/ idealized /ar d oU n t r n t e n d t U/ t-drop /ar d oU n r n ten d t U/ Figure 6: An example fragment from the Buckeyecorpus in orthographic form, the fine transcript</context>
</contexts>
<marker>Pitt, Dilley, Johnson, 2007</marker>
<rawString>Mark A. Pitt, Laura Dilley, Keith Johnson, Scott Kiesling, William Raymond, Elizabeth Hume, and Eric Fosler-Lussier. 2007. Buckeye corpus of conversational speech.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Prince</author>
<author>Paul Smolensky</author>
</authors>
<title>Optimality Theory: Constraint Interaction in Generative Grammar.</title>
<date>2004</date>
<publisher>Blackwell.</publisher>
<marker>Prince, Smolensky, 2004</marker>
<rawString>Alan Prince and Paul Smolensky. 2004. Optimality Theory: Constraint Interaction in Generative Grammar. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yee Whye Teh</author>
<author>Michael Jordan</author>
<author>Matthew Beal</author>
<author>David Blei</author>
</authors>
<title>Hierarchical Dirichlet processes.</title>
<date>2006</date>
<journal>Journal of the American Statistical Association,</journal>
<pages>101--1566</pages>
<contexts>
<context position="10406" citStr="Teh et al., 2006" startWordPosition="1686" endWordPosition="1689">nds for concatenation without word-boundaries, ni refers to the number of words in utterance i. Variable Explanation B base distribution over possible words L back-off distribution over words Lw distribution over words following w Ui,j underlying form, a word Si,j surface realization of Ui,j, a word ρc /t/-deletion probability in context c Wi observed segments for ith utterance Table 1: Key for the variables in Figure 1 and Figure 2. See Figure 3 for the definition of B. parameter α0, and the word type specific distributions Lw are drawn from a DP(L, α1), resulting in a hierarchical DP model (Teh et al., 2006). The base distribution B functions as a lexical generator, defining a prior distribution over possible words. In principle, B can incorporate arbitrary prior knowledge about possible words, for example syllable structure (cf. Johnson (2008)). Inspired by Norris et al. (1997), we use a simpler possible word constraint that only rules out sequences that lack a vowel (see Figure 3). While this is clearly a simplification it is a plausible assumption for English data. Instead of generating the observed sequence of segments W directly by concatenating the underlying forms as in Goldwater et al. (2</context>
<context position="22554" citStr="Teh et al. (2006)" startWordPosition="3842" endWordPosition="3845">ble 2. Best performance for both the Unigram and the Bigram model in the GOLD-p condition is achieved under the left-right setting, in line with the standard analyses of /t/-deletion as primarily being determined by the preceding and the following context. For the LEARN-p condition, the Bigram model still performs best in the left-right setting but the Unigram model’s performance drops 5As manually setting the hyper-parameters for the DPs in our model proved to be complicated and may be objected to on principled grounds, we perform inference for them under a vague gamma prior, as suggested by Teh et al. (2006) and Johnson and Goldwater (2009), using our own implementation of a slice-sampler (Neal, 2003). uniform right left-right LEARN-P 56.52 39.28 23.59 Unigram GOLD-P 62.08 60.80 66.15 LEARN-P 60.85 62.98 77.76 Bigram GOLD-P 69.06 69.98 73.45 Table 2: F-score of recovered /t/s with known word boundaries on real data for the three different context settings, averaged over two runs (all standard errors below 2%). Note how the Unigram model always suffers in the LEARN-p condition whereas the Bigram model’s performance is actually best for LEARN-p in the left-right setting. C C C V C $ V C V V V $ emp</context>
</contexts>
<marker>Teh, Jordan, Beal, Blei, 2006</marker>
<rawString>Yee Whye Teh, Michael Jordan, Matthew Beal, and David Blei. 2006. Hierarchical Dirichlet processes. Journal of the American Statistical Association, 101:1566–1581.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>