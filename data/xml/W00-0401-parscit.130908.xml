<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000058">
<title confidence="0.997324">
Concept Identification and Presentation in the Context of
Technical Text Summarization
</title>
<author confidence="0.964659">
Horacio Saggion*and Guy Lapalme
</author>
<affiliation confidence="0.81025">
Departement d&apos;Informatique et Recherche Operationnelle
Universite de Montréal
</affiliation>
<address confidence="0.921338666666667">
CP 6128, Succ Centre-Ville
Montréal, Québec, Canada, H3C 3J7
Fax: +1-514-343-5834
</address>
<email confidence="0.997177">
Isaggion,lapalmel@iro.umontreal.ca
</email>
<sectionHeader confidence="0.997284" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996216">
We describe a method of text summarization
that produces indicative-informative abstracts
for technical papers. The abstracts are gener-
ated by a process of conceptual identification,
topic extraction and re-generation. We have
carried out an evaluation to assess indicative-
ness and text acceptability relying on human
judgment. The results so far indicate good per-
formance in both tasks when compared with
other summarization technologies.
</bodyText>
<sectionHeader confidence="0.999395" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999724166666667">
We have specified a method of text summa-
rization which produces indicative-informative
abstracts for technical documents. The method
was designed to identify the &amp;quot;topics&amp;quot; of a
document and present them in an indicative
abstract. Eventually, they can be elaborated in •
</bodyText>
<listItem confidence="0.962352">
•specific ways.
• In Figure 1, we present an indicative abstract
for the document &amp;quot;Facilitating designer-.
• customer communication in the World Wide
Web&amp;quot; (Internet Research: Electronic Net-
working Applications and Policy, Vol 8, Issue
5,1998) produced with our implementation
of this method. The abstract includes a list
of topics which are terms appearing in the
automatic abstract (e.g. WebShaman) or
obtained from the source document by the
process of term expansion (e.g. WWW
technique obtained from technique). It also
</listItem>
<footnote confidence="0.686124428571429">
* The first author is supported by Agence Canadienne
de Developpement International (ACDI) and Fundacion
Antorchas (A-13671/1-47), Argentina. He was previ-
ously supported by Ministerio de Educacion de la Na.cion
de la Reptiblica Argentina (Resolucian 1041/96) and De-
partamento de Computacion, Facultad de Ciencias Exac-
tas y Naturales, UBA, Argentina.
</footnote>
<bodyText confidence="0.9977785">
includes term elaborations which can be used
to answer specific questions about the topics
such as what is topic? how topic is used?
who developed topic? and what are the
</bodyText>
<subsectionHeader confidence="0.784394">
advantages of topic?
</subsectionHeader>
<bodyText confidence="0.99977225">
In this paper, we will describe how we dealt
with the problem of content selection and
presentation and how we have evaluated our
method of text summarization.
</bodyText>
<sectionHeader confidence="0.994259" genericHeader="introduction">
2 Text Summarization
</sectionHeader>
<bodyText confidence="0.999981153846154">
The process of producing a summary from
a source text consists of the following steps:
(i) the interpretation of the text; (ii) the
extraction of the relevant information which
ideally includes the &amp;quot;topics&amp;quot; of the source; (iii)
the condensation of the extracted information
and construction of a summary representation;
and (iv) the presentation of the summary rep-
resentation to the reader in natural language.
While some techniques exist for producing
summaries for domain independent texts
(Luhn, 1958; Marcu, 1997) it seems that
domain specific texts require domain specific
techniques (DeJong, 1982; Paice and Jones,
1993). In our case, we are dealing with techni-
cal articles which are the result of the complex
process of scientific inquiry that starts with
the identification of a knowledge problem and
eventually culminates with the discovery of
an answer to it. Even if authors of technical
articles write about several concepts in their
articles, not all of them are topics. In order to
address the issue of topic identification, content
selection and presentation, we have studied
alignments (manually produced) of sentences
from professional abstracts with sentences from
</bodyText>
<page confidence="0.984637">
1
</page>
<sectionHeader confidence="0.948649" genericHeader="method">
Abstract Introducing the Topics
</sectionHeader>
<bodyText confidence="0.998579588235294">
Virtual prototyping is a technique which has been suggested for use in, for example, telecommuni-
cation product development as a high-end technology to achieve a quick digital model that could
be used in the same way as a real prototype. Presents the design rationale of WebShaman, starting
from the concept design perspective by introducing a set of requirements to support communication
via a concept model between industrial designer and a customer. In the article, the authors suggest
that virtual prototyping in collaborative use between designers is a potential technique to facilitate
design and alleviate the problems created by geographical distance and complexities in the work
between different parties. The technique, was implemented in the VRP project, allows compo-
nent level manipulation of a virtual prototype in a WWW (World Wide Web) browser. The user
services, the software architecture, and the techniques of WebShaman were developed iteratively
during the fieldwork in order to illustrate the ideas and the feasibility of the system. The server is
not much different from the other servers constructed to support synchronous collaboration.
Identified Topics: 3D model - VIRPI project - WWW - WWW technique - WebShaman -
CAD system - conceptualmodel - customer - object-oriented model - product - product
concept - product design - requirement - simulation model - smart virtual prototype
- software component - system - technique - technology - use - virtual component -
virtual prototype - virtual prototype system - virtual prototyping
</bodyText>
<subsectionHeader confidence="0.838671">
Information about the Topics
</subsectionHeader>
<bodyText confidence="0.833877454545454">
An example of a conceptual model, a pen-shaped wireless user interface for a mobile
telephone.
A virtual prototype is a computer-based simulation of a prototype or a subsystem with a
degree of functional realism, comparable to that of a physical prototype.
A computer system implementing the high-end aspects of virtual prototyping has been
developed in the VRP project (VRP, 1998) at VTT Electronics, in Oulu, Finland.
The two-and-a-half-year VIRPI project consists of three parts.
Nowadays, CAD (computer-aided design) systems are used as an aid in industrial, mechan-
ical and electronics design for the specification and development of a product.
A virtual prototype system can be used for concept testing in the early phase of product
development.
</bodyText>
<figureCaption confidence="0.999043">
Figure 1: Indicative Abstract, Topics and Topic Elaboration
</figureCaption>
<bodyText confidence="0.953101052631579">
source documents. One of the alignments is
presented in Table 1. The first column contains
the information of the professional abstract.
The second and third columns contain the infor-
mation from the source document that matches
the sentences of the professional abstract, and
its location in the source document. We have
produced 100 of these tables containing a
total of 309 sentences of professional abstracts
aligned with 568 sentences of source documents.
These alignments allowed us to identify on
one hand, concepts, relations and types of
information usually conveyed in abstracts; and
on the other hand, valid transformations in
the source in order to produce a compact and
coherent text. The transformations include
verb transformation, concept deletion, concept
reformulation, structural deletion, parenthetical
deletion, clause deletion, acronym expansion,
</bodyText>
<page confidence="0.996383">
2
</page>
<table confidence="0.998435533333333">
Professional Abstract Source Document P /T
Presents a more efficient Efficient distributed breadth-first search algo- -/Title
Distributed Breadth-First rithm.
Search algorithm for an
asynchronous communication
network.
In this paper we have presented a more effi- Lst/-
cient distributed algorithm which construct a
breadth-first search tree in an asynchronous
communication network
Presents a model and gives an First we present a model and give overview of 1st!-
overview of related research. related research.
Analyzes the complexity of the algo- We analyse the complexity of our algorithm, lst/-
rithm, and gives some examples of per- and give some examples of performance on
formance on typical networks. typical networks.
</table>
<tableCaption confidence="0.9561135">
Table 1: LISA Abstract 1955 - Source Document: &amp;quot;Efficient distributed breadth-first search algo-
rithm.&amp;quot; S.A.M. Makki. Computer Communications, 19(8) Jul 96, p628-36.
</tableCaption>
<bodyText confidence="0.997978885714285">
abbreviation, merge and split. In our corpus,
89% of the sentences from the professional
abstracts included at least one transformation.
Results of the corpus study are detailed in
(Saggion and Lapalme, 1998) and (Saggion and
Lapalme, 2000).
We have identified a total of 52 different
types of information (coming from the corpus
and from technical articles) for technical text
summarization that we use to identify some of
the main themes. These types include: the
explicit topic of the document, the situa-
tion, the identification of the problem, the
identification of the solution, the research
goal, the explicit topic of a section, the
. authors&apos; development, the inferences, the
description of a topical entity, the definition.
of a topical entity, the relevance of a topical
entity, the advantages, etc. Information
types are classified as indicative or informative
depending on the type of abstract they con-
tribute to (i.e. the topic of a document is
indicative while the description of a topical
entity is informative). Types of information are
identified in sentences of the source document
using co-occurrence of concepts and relations
and specific linguistic patterns. Technical
articles from different domains refer to specific
concepts and relations (diseases and treatments
in Medicine, atoms and chemical reactions
in Chemistry, and theorems and proofs in
Mathematics). We have focused on concepts
and relations that are common across domains
such as problem, solution, research need,
experiment, relevance, researchers, etc.
</bodyText>
<sectionHeader confidence="0.995117" genericHeader="method">
3 Text Interpretation
</sectionHeader>
<bodyText confidence="0.999984482758621">
Our approach to text summarization is based
on a superficial analysis of the source docu-
ment and on the implementation of some text
re-generation techniques such as merging of top-
ical information, re-expression of concepts and
acronym expansion. The article (plain text
in English without mark-up) is segmented in
main units (title, author information, author
abstract, keywords, main sections and refer-
ences) using typographic information and some
keywords. Each unit is passed through a bi-
pos statistical tagger. In each unit, the sys-
tem identifies titles, sentences and paragraphs,
and then, sentences are interpreted using finite
state transducers identifying and packing lin-
guistic constructions and domain specific con-
structions. Following that, a conceptual dictio-
nary that relates lexical items to domain con-
cepts and relations is used to associate seman-
tic tags to the different structural elements in
the sentence. Subsequently, terms (canonical
form of noun groups), their associated semantic
(head of the noun group) and theirs positions
are extracted from each sentence and stored in
an AVL tree (term tree) along with their fre-
quency. A conceptual index is created which
specifies to which particular type of informa-
tion each sentence could contribute. Finally,
terms and words are extracted from titles and
</bodyText>
<page confidence="0.994935">
3
</page>
<bodyText confidence="0.995182">
stored in a list (the topical structure) and content.
acronyms and their expansions are recorded.
</bodyText>
<subsectionHeader confidence="0.999798">
3.1 Content Selection
</subsectionHeader>
<bodyText confidence="0.999956461538462">
In order to represent types of information we use
templates. In Table 2, we present the Topic
of the Document, Topic of the Section
and Signaling Information templates. Also
presented are some indicative and informative
patterns. Indicative patterns contain variables,
syntactic constructions, domain concepts and
relations. Informative patterns also include one
specific position for the topic under considera-
tion. Each element of the pattern matches one
or more elements of the sentence (conceptual,
syntactic and lexical elements match one ele-
ment while variables match zero or more).
</bodyText>
<subsectionHeader confidence="0.576769">
3.1.1 Indicativeness
</subsectionHeader>
<bodyText confidence="0.999933377777778">
The system considers sentences that were iden-
tified as carrying indicative information (their
position is found in the conceptual index).
Given a sentence S and a type of information
T the system verifies if the sentence matches
some of the patterns associated with type T.
For each matched pattern, the system extracts
information from the sentence and instantiates
a template of type T. For example, the
Content slot of the problem identification
template is instantiated with all the sentence
,(avoiding references, structural elements and
parenthetical expressions) while the What slot
&apos;of the topic of the document template is
instantiated with a parsed sentence fragment
to the left or to the right of the make known
relation depending on the attribute voice of the
verb (active vs. passive). All the instantiated
templates constitute the Indicative Data Base
(IDB).
The system matches the topical structure
with the topic candidate slots from the IDB.
The system selects one template for each term
in that structure: the one with the greatest
weight (heuristics are applied if there are more
than one). The selected templates constitute
the indicative content and the terms ap-
pearing in the topic candidate slots and their
expansions constitute the potential topics
of the document. Expansions are obtained
looking for terms in the term tree sharing
the semantic of some terms in the indicative
The indicative content is sorted using &amp;quot;
positional information and the following con-
ceptual order: situation, need for research,
problem, solution, entity introduction,
topical information, goal of concep-
tual entity, focus of conceptual entity,
methodological aspects, inferences and
structural information. Templates of the
same type are grouped together if they ap-
peared in sequence in the list. The types
considered in this process are: the topic,
section topic and structural information.
The sorted templates constitute the text plan.
</bodyText>
<subsectionHeader confidence="0.455533">
3.1.2 Informativeness
</subsectionHeader>
<bodyText confidence="0.999988">
For each potential topic and sentence where
it appears (that information is found on the
term tree) the system verifies if the sentence
contains an informative marker (conceptual
index) and satisfies an informative pattern. If
so, the potential topic is considered a topic
of the document and a link will be created be-
tween the topic and the sentence which will be
part of the informative abstract.
</bodyText>
<sectionHeader confidence="0.972176" genericHeader="method">
4 Content Presentation
</sectionHeader>
<bodyText confidence="0.999831181818182">
Our approach to text generation is based
on the regularities observed in the corpus
of professional abstracts and so, it does not
implement a general theory of text generation
by computers. Each element in the text plan
is used to produce a sentence. The structure of
the sentence depends on the type of template.
The information about the situation, the
problem, the need for research, etc. is
reported as in the original document with few
modifications (concept re-expression). Instead
other types require additional re-generation:
for the topic of the document template the
generation procedure is as follows: (i) the verb
form for the predicate in the Predicate slot
is generated in the present tense (topical
information is always reported in present
tense), 3rd person of singular in active
voice at the beginning of the sentence;
(ii) the parsed sentence fragment from the What
slot is generated in the middle of the sentence
(so the appropriate case for the first element
</bodyText>
<page confidence="0.988663">
4
</page>
<table confidence="0.999727525">
Topic of the Document
Type: topic
Id: integer identifier
Predicate: instance of make known
Where: instance of {research paper, study, work, research}
Who: instance of {research paper, author, study, work, research, none}
What: parsed sentence fragment
Position: section and sentence id
Topic candidates: list of terms from the What filler
Weight: number
Topic of Section
Type: sec_desc
Id: integer identifier
Predicate: instance of make known
Section: instance of paper component
Argument: parsed sentence fragment
Position: section and sentence id
Topic candidates: list of terms from the Argument filler
Weight: number
Signaling Information
Type: structure:2 .
Id: integer identifier
Predicate: instance of show graphical material
Structural: instance of structural element
Argument: parsed sentence fragment
Position: section and sentence id
Topic candidates: list of terms from the Argument filler
Weight: number
Signaling SK/P1 + structural + SK/P2 + show graphically + ARGUMENT + eos
(indicative)
Topic (indica- noun group + author + make known + preposition + research paper +
tive) DESCRIPTION + eos
Author&apos;s Goal SK/P1 + goal of author + define + GOAL + eos
(indicative)
Goal of SKIP + goal + preposition + TOPIC + define + GOAL + eos
TOPIC (in-
formative)
Definition SKIP + TOPIC + define + noun group
of TOPIC
(informative)
</table>
<tableCaption confidence="0.996441">
Table 2: Templates and Patterns.
</tableCaption>
<bodyText confidence="0.999929529411765">
has to be generated); and (iii) a full stop is
generated. This schema of generation avoids
the formulation of expressions like &amp;quot;X will be
presented&amp;quot;, &amp;quot;X have been presented&amp;quot; or &amp;quot;We
have presented here X&amp;quot; which are usually found
on source documents but which are awkward
in the context of the abstract text-type. Note
that each type of information prescribes its
own schema of generation.
Some elements in the parsed sentence frag-
ment require re-expression while others are
presented in &amp;quot;the words of the author.&amp;quot; If the
system detects an acronym without expansion
in the string it would expand it and record that
situation in order to avoid repetitions. Note
that as the templates contain parsed sentence
fragments, the correct punctuation has to
</bodyText>
<page confidence="0.965754">
5
</page>
<bodyText confidence="0.9999541875">
be re-generated. For merged templates the
generator implements the following patterns
of production: if n adjacent templates are to
be presented using the same predicate, only
one verb will be generated whose argument is
the conjunction of the arguments from the n
templates. If the sequence of templates have
no common predicate, the information will
be presented as a conjunction of propositions.
These patterns of sentence production are
exemplified in Table 3.
The elaboration of the topics is presented
upon reader&apos;s demand. The information is pre-
sented in the order of the original text. The in-
formative abstract is the information obtained
by this process as it is shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.892159" genericHeader="method">
5 Limitations of the Approach
</sectionHeader>
<bodyText confidence="0.998645371428571">
Our approach is based on the empirical ex-
amination of abstracts published by second
services. In our first study, we examined 100
abstracts and source documents in order to
deduce a conceptual and linguistic model for
the task of summarization of technical articles.
Then, we expanded the corpus with 100 more
items in order to validate the model. We
believe that the concepts, relations and types
of information identified account for interesting
phenomena appearing in the corpus and con-
stitute a sound basis for text summarization.
&apos; Nevertheless, we have identified only a few
linguistic expressions used in order to express
particular elements of the conceptual model
(241 domain verbs, 163 domain nouns, 129
adjectives, 174 indicative patterns, 87 informa-
tive patterns). This is because we are mainly
concerned with the development of a general
method of automatic abstracting and the task
of constructing such linguistic resources is time
consuming as recent work have shown (Minel
et al., 2000).
The implementation of our method relies on
state-of-the-art techniques in natural language
processing including noun and verb group iden-
tification and conceptual tagging. The inter-
preter relies on the output produced by a shal-
low text segmenter and on a statistical POS-
tagger. Our prototype only analyses sentences
for the specific purpose of text summarization
and implements some patterns of generation ob-
served in the corpus. Additional analysis could
be done on the obtained representation to pro-
duce better results.
</bodyText>
<sectionHeader confidence="0.999838" genericHeader="method">
6 Related Work
</sectionHeader>
<bodyText confidence="0.999860941176471">
(Paice and Jones, 1993) have already addressed
the issue of content identification and expression
in technical summarization using templates, but
while they produced indicative abstracts for a
specific domain, we are producing domain inde-
pendent indicative-informative abstracts. Being
designed for one specific domain, their abstracts
are fixed in structure while our abstracts are
dynamically constructed. Radev and McKeown
(1998) also used instantiated templates, but in
order to produce summaries of multiple docu-
ments in one specific domain. They focus on
the generation of the text while we are address-
ing the overall process of automatic abstracting.
Our concern regarding the presentation of the
information is now being addressed by other re-
searchers as well Ping and McKeown, 1999).
</bodyText>
<sectionHeader confidence="0.847169" genericHeader="method">
7 Evaluating Content and Quality in
</sectionHeader>
<subsectionHeader confidence="0.588729">
Text Summarization
</subsectionHeader>
<bodyText confidence="0.99993476">
Abstracts are texts used in tasks such as assess-
ing the content of the document and deciding
if the source is worth reading. If text summa-
rization systems are designed to fulfill those re-
quirements, the generated texts have to be eval-
uated according to their intended function and
its quality. The quality and success of human
produced abstracts have already been addressed
in the literature (Grant, 1992; Gibson, 1993) us-
ing linguistic criteria such as cohesion and co-
herence, thematic structure, sentence structure
and lexical density. But in automatic text sum-
marization, this is an emergent research topic.
(Mind l et al., 1997) have proposed two meth-
ods of evaluation addressing the content of the
abstract and its quality. For content evalua-
tion, they asked human judges to classify sum-
maries in broad categories and also verify if
the key ideas of source documents are appropri-
ately expressed in the summaries. For text qual-
ity, they asked human judges to identify prob-
lems such as dangling anaphora and broken tex-
tual segments and also to make subjective judg-
ments about readability. In the context of the
TIPSTER program, (Firmin and Chrzanowski,
</bodyText>
<page confidence="0.997722">
6
</page>
<bodyText confidence="0.745977470588235">
Re-Generated Sentences Sentences from Source Documents
Illustrates the principle of virtual prototyping and Figure 1 Virtual prototyping models and techniques
the different techniques and models required. illustrates the principle of virtual prototyping and the
different techniques and models required.
Presents the mechanical and electronic design of After a brief introduction, we present the mechanical
the robot harvester including all subsystems, and electronic design of the robot harvester includ-
namely, fruit localisation module, harvesting arm ing all subsystems, namely , fruit localisation module,
and gripper-cutter as well as the integration of harvesting arm and gripper-cutter as well as the inte-
subsystems and the specific mechanical design of gration of subsystems.
the picking arm addressing the reduction of
undesirable dynamic effects during high velocity
operation.
Throughout this work, we present the specific mechan-
ical design of the picking arm addressing the reduction
of undesirable dynamic effects during high velocity op-
eration.
Shows configuration of the robotic fruit harvester The final prototype consists of two jointed harvesting
Agribot and schematic view of the detaching tool. arms mounted on a human guided vehicle as shown
schematically in Figure 1 Configuration of the robotic
, fruit harvester Agribot.
Schematic representation of the operations involved in
the detaching step can be seen in Figure 5 Schematic
view of the detaching tool and operation.
PAWS (the programmable automated welding sys- PAWS was designed to provide an automated means of
tem) was designed to provide an automated means planning, controlling, and performing critical welding
of planning, controlling, and performing critical operations for improving productivity and quality.
welding operations for improving productivity and
quality.
Describes HuDL (local autonomy) in greater Section 2 describes HuDL in greater detail and section
detail; discusses system integration and the IMA 3 discusses system integration and the IMA .
(the intelligent machine architecture); and also
gives an example implementation.
An example implementation is given in section 4 and
section 5 contains the conclusions.
</bodyText>
<tableCaption confidence="0.971308">
Table 3: Re-Generated Sentences
</tableCaption>
<bodyText confidence="0.72289">
1999) and (Mani et al., 1998) also used a cat-
•egorization task using TREC topics. For text
quality, they addressed subjective aspects such
as the length of the summary, its intelligibility
and its usefulness. We have carried out an eval-
uation of our summarization method in order to
assess the function of the abstract and its text
quality.
</bodyText>
<subsectionHeader confidence="0.995625">
7.1 Experiment
</subsectionHeader>
<bodyText confidence="0.999937935483871">
We compared abstracts produced by our
method with abstracts produced by Mi-
crosoft&apos;97 Summarizer and with others
published with source documents (usually au-
thor abstracts). We have chosen Microsoft&apos;97
Summarizer because, even if it only produces
extracts, it was the only summarizer available
in order to carry out this evaluation and
because it has already been used in other eval-
uations (Marcu, 1997; Barzilay and Elhadad,
1997).
In order to evaluate content, we presented
judges with randomly selected abstracts and
five lists of keywords (content indicators). The
judges had to decide to which list of keywords
the abstract belongs given that different lists
share some keywords and that they belong
to the same technical domain. Those. lists
were obtained from the journals where the
source documents were published. The idea
behind this evaluation is to see if the abstract
convey the very essential content of the source
document.
In order to evaluate the quality of the text, we
asked the judges to provide an acceptability
score between 0-5 for the abstract (0 for un-
acceptable and 5 for acceptable) based on the
following criteria taken from (Rowley, 1982)
(they were only suggestions to the evaluators
and were not enforced): good spelling and
grammar; clear indication of the topic of
</bodyText>
<page confidence="0.998847">
7
</page>
<bodyText confidence="0.999987454545455">
the source document; impersonal style; one
paragraph; conciseness; readable and under-
standable; acronyms are presented along with
their expansions; and other criteria that the
judge considered important as an experienced
reader of abstracts of technical documents.
We told the judges that we would consider
the abstracts with scores above 2.5 as accept-
able. Some criteria are more important than
other, for example judges do not care about
impersonal style but care about readability.
</bodyText>
<subsectionHeader confidence="0.655122">
7.1.1 Materials
</subsectionHeader>
<bodyText confidence="0.924667103448276">
Source Documents: we used twelve source
documents from the journal Industrial Robots
found on the Emerald Electronic Library (all
technical articles). The articles were down-
loaded in plain text format. These documents
are quite long texts with an average of 23K
characters (minimum of 11K characters and a
maximum of 41K characters). They contain
an average of 3472 words (minimum of 1756
words and a maximum of 6196 words excluding
punctuation), and an average of 154 sentences
(with a minimum of 85 and a maximum of 288).
Abstracts: we produced twelve abstracts us-
ing our method and computed the compression
ratio in number of words, then we produced
twelve abstracts by Microsoft&apos;97 Summarizerl
• using a compression rate at least as high as
our (i.e. if our method produced an abstract
with a compression rate of 3.3% of the source,
we produced the Microsoft abstract with a
compression rate of 4% of the source). We
extracted the twelve abstracts and the twelve
lists of keywords published with the source
documents. We thus obtained 36 different
abstracts and twelve lists of keywords.
Forms: we produced 6 different forms each con-
taining six different abstracts randomly2 chosen
out of twelve different documents (for a total of
36 abstracts). Each abstract was printed in a
</bodyText>
<footnote confidence="0.992140833333333">
1We had to format the source document in order for
the Microsoft Summarizer to be able to recognize the
structure of the document (titles, sections, paragraphs
and sentences).
2Random numbers for this evaluation were produced
using software provided by SICSTus Prolog.
</footnote>
<bodyText confidence="0.999042692307692">
different page. It included 5 lists of keywords, a
field to be completed with the quality score as-
sociated to the abstract and a field to be filled -
with comments about the abstract. One of the
lists of keywords was the one published with the
source document, the other four were randomly
selected from the set of 11 remaining keyword
lists, they were printed in the form in random
order. One page was also available to be com-
pleted with comments about the task, in partic-
ular with the time it took to the judges to com-
plete the evaluation. We produced three copies
of each form for a total of 18 forms.
</bodyText>
<subsectionHeader confidence="0.662639">
7.1.2 Subjects
</subsectionHeader>
<bodyText confidence="0.9906774">
We had a total of 18 human judges or eval-
uators. Our evaluators were 18 students of
the M.Sc. program in Information Science at
McGill Graduate School of Library Sc Informa-
tion Studies. All of the subjects had good read-
ing and comprehension skills in English. This
group was chosen because they have knowledge
about what constitutes a good abstract and
they are educated to become professionals in In-
formation Science.
</bodyText>
<subsubsectionHeader confidence="0.539971">
7.1.3 Evaluation Procedure
</subsubsectionHeader>
<bodyText confidence="0.9999961875">
The evaluation was performed in one hour ses-
sion at McGill University. Each human judge
received a form (so he/she evaluated six dif-
ferent abstracts) and an instruction booklet.
No other material was required for the evalu-
ation (i.e. dictionary). We asked the judges to
read carefully the abstract. They had to decide
which was the list of keywords that matched
the abstract (they could chose more than one
or none at all) and then, they had to associate
a numeric score to the abstract representing its
quality based on the given criteria. This pro-
cedure produced three different evaluations of
content and text quality for each of the 36 ab-
stracts. The overall evaluation was completed
in a maximum of 40 minutes.
</bodyText>
<sectionHeader confidence="0.501281" genericHeader="evaluation">
7.2 Results
</sectionHeader>
<bodyText confidence="0.999880571428571">
For each abstract, we computed the average
quality using the scores given by the judges.
We considered that the abstract indicated the
essential content of the source document if two
or more judges were able to chose the correct
list of keywords for the abstract. The results for
individual articles and the average information
</bodyText>
<page confidence="0.996024">
8
</page>
<table confidence="0.999255">
Microsoft Abstract Our Method S.D. Abstract
# Art. Indic? Quality Indic? Quality Indic? Quality
1 yes 2.66 yes 2.93 yes 4.16
2 no 1.36 yes 3.66 yes 4.00
3 no 1.16 no 3.00 no 4.06
4 yes 3.00 yes 4.00 yes 4.33
5 no 2.16 no 1.76 yes 4.00
6 yes 2.16 yes 4.00 no 4.53
7 no 0.83 yes 2.50 yes 4.40
8 yes 2.33 yes 3.00 yes 4.00
9 yes 2.16 no 2.66 yes 3.66
10 yes 2.16 yes 4.00 yes 3.31
11 yes 2.40 no 2.70 no 4.26
12 no 1.16 no 3.33 no 4.00
Average 70% 1.98 70% 3.15 80% 4.04
esu ts with Different Documents and Subjects
Average 80% 1.46 80% 3.23 100% 4.25
</table>
<tableCaption confidence="0.999841">
Table 4: Results of Human Judgment about Indicativeness and Text Quality
</tableCaption>
<bodyText confidence="0.995905464285714">
are shown in Table 4. For a given source docu-
ment and type of abstract, the value in column
&apos;Indic?&apos; contains the value &apos;yes&apos; if the majority
of the evaluator have chosen the source docu-
ment list of keywords for the abstract and &apos;no&apos;
on the contrary. The value in column &apos;Qual-
ity&apos; is the average acceptability for the abstract.
Content: In 80% of the cases, the abstracts
published with the source documents were •
correctly classified by the evaluators. Instead,
the automatic abstracts were correctly classi-
fied in 70% of the cases. It is worth noting
• that the automatic systems did not use the
journal abstracts nor the lists of keywords or
the,information about the journal.
Quality: The figures about text acceptabil-
ity indicate that the abstracts produced by
Microsoft&apos;97 Summarizer are below the accept-
ability level of 2.5, the abstracts produced by
our method are above the acceptability level of
2.5 and that the human abstracts are highly
acceptable.
In a run of this experiment using 30 ab-
stracts from a different set of 10 articles and 15
judges from Ecole de Bibliotheconomie et des
Sciences de l&apos;Information (EBSI) at Universite
de. Montreal we have obtained similar results
(last row in Table 4).
</bodyText>
<sectionHeader confidence="0.998444" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999902083333334">
In this paper, we have presented a method of
text summarization which produces indicative-
informative abstracts. We have described the
techniques we are using to implement our
method and some experiments showing the
viability of the approach.
Our method was specified for summarization
of one specific type of text: the scientific and
technical document. Nevertheless, it is domain
independent because the concepts, relations
and types of information we use are common
across different domains. The question of the
coverage of the model will be addressed in
our future work. Our method was designed
without any particular reader in mind and
with the assumption that a text does have
a &amp;quot;main&amp;quot; topic. If readers were known, the
abstract could be tailored towards their specific
profiles. User profiles could be used in order to
produce the informative abstracts elaborating
those specific aspects the reader is &amp;quot;usually&amp;quot;
interested in. This aspect will be elaborated in
future work.
The experiments reported here addressed
</bodyText>
<page confidence="0.991883">
9
</page>
<bodyText confidence="0.9988015">
the evaluation of the indicative abstracts using
a categorization task. Using the automatic
abstracts reader have chosen the correct
category for the articles in 70% of the cases
compared with 80% of the cases when using the
author abstracts. Readers found the abstracts
produced by our method of better quality than
a sentence-extraction based system.
</bodyText>
<sectionHeader confidence="0.998933" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999894444444445">
We would like to thank three anonymous re-
viewers for their comments which helped us im-
prove the final version of this paper. We are
grateful to Professor Michele Hudon from Uni-
versite de Montréal for fruitful discussion and to
Professor John E. Leide from McGill University
and to Mme Gracia Pagola from Universite de
Montréal for their help in recruiting informants
for the experiments.
</bodyText>
<sectionHeader confidence="0.999445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99968835443038">
R. Barzilay and M. Elhadad. 1997. Using
Lexical Chains for Text Summarization. In
Proceedings of the ACL/EACL &apos;97 Workshop
on Intelligent Scalable Text Summarization,
pages 10-17, Madrid, Spain, July.
G. DeJong. 1982. An Overview of the FRUMP
System. In W.G. Lehnert and M.H. Ringle,
editors, Strategies for Natural Language Pro-
cessing, pages 149-176. Lawrence Erlbaum
Associates, Publishers.
T. Firmin and M.J. Chrzanowski. 1999. An
Evaluation of Automatic Text Summariza-
tion Systems. In I. Mani and M.T. Maybury,.
editors, Advances in Automatic Text Summa-
iization, pages 325-336.
T.R. Gibson. 1993. Towards a Discourse The-
ory of Abstracts and Abstracting. Depart-
ment of English Studies. University of Not-
tingham.
P. Grant. 1992. The Integration of Theory and
Practice in the Development of Summary-
Writting Strategies. Ph.D. thesis, Universite
de Montréal. Faculte des etudes sup4rieures.
H. Jing and K.R. McKeown. 1999. The Decom-
position of Human-Written Summary Sen-
tences. In M. Hearst, Gey. F., and R. Tong,
editors, Proceedings of SIGIR&apos;99. 22nd Inter-
national Conference on Research and Devel-
opment in Information Retrieval, pages 129-
136, University of California, Beekely, Au-
gust.
H.P. Luhn. 1958. The Automatic Creation of &amp;quot;
Literature Abstracts. IBM Journal of Re-
search Development, 2(2):159-165.
I. Mani, D. House, G. Klein, L. Hirshman,
L. Obrst, T. Firmin, M. Chrzanowski, and
B. Sundheim. 1998. The TIPSTER SUM-
MAC Text Summarization Evaluation. Tech-
nical report, The Mitre Corporation.
D. Marcu. 1997. From Discourse Structures
to Text Summaries. In The Proceedings of
the ACL &apos;97/EACL &apos;97 Workshop on Intelli-
gent Scalable Text Summarization, pages 82-
88, Madrid, Spain, July 11.
J-L. Minel, S. Nugier, and G. Piat. 1997. Com-
ment Apprecier la Qualite des Résumés Au-
tomatiques de Textes? Les Exemples des Pro-
tocoles FAN et MLUCE et leurs Resultats
sur SERAPHIN. In 1eres Journees Scientific-
ques et Techniques du Reseau Francophone
de l&apos;Ingenierie de la Langue de l&apos;AUPELF-
UREF., pages 227-232,15-16 avril.
J-L. Minel, J-P. Descles, E. Cartier,
G. Crispino, S.B. Hazez, and A. Jack-
iewicz. 2000. Résumé automatique par
filtrage semantique d&apos;informations dans des
textes. TSI, X(X12000):1-23.
C.D. Paice and P.A. Jones. 1993. The Iden-
tification of Important Concepts in Highly
Structured Technical Papers. In R. Korthage,
E. Rasmussen, and P. Willett, editors, Proc.
of the 16th ACM-SIGIR Conference, pages
69-78.
D.R. Radev and K.R. McKeown. 1998. Gen-
erating Natural Language Summaries from
Multiple On-Line Sources. Computational
Linguistics, 24(3):469-500.
J. Rowley. 1982. Abstracting and Indexing.
Clive Bingley, London.
H. Saggion and G. Lapalme. 1998. Where does
Information come from? Corpus Analysis for
Automatic Abstracting. In RIFRA &apos;98. Ren-
contre Internationale sur l&apos;extraction le Fil-
trage et le Résumé Automatique, pages 72-83.
H. Saggion and G. Lapalme. 2000. Evaluation
of Content and Text Quality in the Context
of Technical Text Summarization. In Pro-
ceedings of RIA0&apos;2000, Paris, France, 12-14
April, 2000.
</reference>
<page confidence="0.997797">
10
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.226804">
<title confidence="0.999143">Concept Identification and Presentation in the Context Technical Text Summarization</title>
<author confidence="0.99921">Horacio Saggionand Guy</author>
<affiliation confidence="0.981723">Departement d&apos;Informatique et Recherche Universite de</affiliation>
<note confidence="0.41087875">CP 6128, Succ Montréal, Québec, Canada, H3C Fax: Isaggion,lapalmel@iro.umontreal.ca</note>
<abstract confidence="0.999823909090909">We describe a method of text summarization that produces indicative-informative abstracts for technical papers. The abstracts are generated by a process of conceptual identification, topic extraction and re-generation. We have out an evaluation to indicativeness and text acceptability relying on human judgment. The results so far indicate good performance in both tasks when compared with other summarization technologies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Barzilay</author>
<author>M Elhadad</author>
</authors>
<title>Using Lexical Chains for Text Summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="24072" citStr="Barzilay and Elhadad, 1997" startWordPosition="3688" endWordPosition="3691">e summary, its intelligibility and its usefulness. We have carried out an evaluation of our summarization method in order to assess the function of the abstract and its text quality. 7.1 Experiment We compared abstracts produced by our method with abstracts produced by Microsoft&apos;97 Summarizer and with others published with source documents (usually author abstracts). We have chosen Microsoft&apos;97 Summarizer because, even if it only produces extracts, it was the only summarizer available in order to carry out this evaluation and because it has already been used in other evaluations (Marcu, 1997; Barzilay and Elhadad, 1997). In order to evaluate content, we presented judges with randomly selected abstracts and five lists of keywords (content indicators). The judges had to decide to which list of keywords the abstract belongs given that different lists share some keywords and that they belong to the same technical domain. Those. lists were obtained from the journals where the source documents were published. The idea behind this evaluation is to see if the abstract convey the very essential content of the source document. In order to evaluate the quality of the text, we asked the judges to provide an acceptabilit</context>
</contexts>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>R. Barzilay and M. Elhadad. 1997. Using Lexical Chains for Text Summarization. In Proceedings of the ACL/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization, pages 10-17, Madrid, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G DeJong</author>
</authors>
<title>An Overview of the FRUMP System.</title>
<date>1982</date>
<booktitle>Strategies for Natural Language Processing,</booktitle>
<pages>149--176</pages>
<editor>In W.G. Lehnert and M.H. Ringle, editors,</editor>
<publisher>Lawrence Erlbaum Associates, Publishers.</publisher>
<contexts>
<context position="2896" citStr="DeJong, 1982" startWordPosition="424" endWordPosition="425">arization The process of producing a summary from a source text consists of the following steps: (i) the interpretation of the text; (ii) the extraction of the relevant information which ideally includes the &amp;quot;topics&amp;quot; of the source; (iii) the condensation of the extracted information and construction of a summary representation; and (iv) the presentation of the summary representation to the reader in natural language. While some techniques exist for producing summaries for domain independent texts (Luhn, 1958; Marcu, 1997) it seems that domain specific texts require domain specific techniques (DeJong, 1982; Paice and Jones, 1993). In our case, we are dealing with technical articles which are the result of the complex process of scientific inquiry that starts with the identification of a knowledge problem and eventually culminates with the discovery of an answer to it. Even if authors of technical articles write about several concepts in their articles, not all of them are topics. In order to address the issue of topic identification, content selection and presentation, we have studied alignments (manually produced) of sentences from professional abstracts with sentences from 1 Abstract Introduc</context>
</contexts>
<marker>DeJong, 1982</marker>
<rawString>G. DeJong. 1982. An Overview of the FRUMP System. In W.G. Lehnert and M.H. Ringle, editors, Strategies for Natural Language Processing, pages 149-176. Lawrence Erlbaum Associates, Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Firmin</author>
<author>M J Chrzanowski</author>
</authors>
<title>An Evaluation of Automatic Text Summarization Systems.</title>
<date>1999</date>
<booktitle>Advances in Automatic Text Summaiization,</booktitle>
<pages>325--336</pages>
<editor>In I. Mani and M.T. Maybury,. editors,</editor>
<marker>Firmin, Chrzanowski, 1999</marker>
<rawString>T. Firmin and M.J. Chrzanowski. 1999. An Evaluation of Automatic Text Summarization Systems. In I. Mani and M.T. Maybury,. editors, Advances in Automatic Text Summaiization, pages 325-336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T R Gibson</author>
</authors>
<title>Towards a Discourse Theory of Abstracts and Abstracting.</title>
<date>1993</date>
<institution>Department of English Studies. University of Nottingham.</institution>
<contexts>
<context position="20299" citStr="Gibson, 1993" startWordPosition="3117" endWordPosition="3118">ng. Our concern regarding the presentation of the information is now being addressed by other researchers as well Ping and McKeown, 1999). 7 Evaluating Content and Quality in Text Summarization Abstracts are texts used in tasks such as assessing the content of the document and deciding if the source is worth reading. If text summarization systems are designed to fulfill those requirements, the generated texts have to be evaluated according to their intended function and its quality. The quality and success of human produced abstracts have already been addressed in the literature (Grant, 1992; Gibson, 1993) using linguistic criteria such as cohesion and coherence, thematic structure, sentence structure and lexical density. But in automatic text summarization, this is an emergent research topic. (Mind l et al., 1997) have proposed two methods of evaluation addressing the content of the abstract and its quality. For content evaluation, they asked human judges to classify summaries in broad categories and also verify if the key ideas of source documents are appropriately expressed in the summaries. For text quality, they asked human judges to identify problems such as dangling anaphora and broken t</context>
</contexts>
<marker>Gibson, 1993</marker>
<rawString>T.R. Gibson. 1993. Towards a Discourse Theory of Abstracts and Abstracting. Department of English Studies. University of Nottingham.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Grant</author>
</authors>
<title>The Integration of Theory and Practice in the Development of SummaryWritting Strategies.</title>
<date>1992</date>
<tech>Ph.D. thesis,</tech>
<institution>Universite de Montréal.</institution>
<contexts>
<context position="20284" citStr="Grant, 1992" startWordPosition="3115" endWordPosition="3116">tic abstracting. Our concern regarding the presentation of the information is now being addressed by other researchers as well Ping and McKeown, 1999). 7 Evaluating Content and Quality in Text Summarization Abstracts are texts used in tasks such as assessing the content of the document and deciding if the source is worth reading. If text summarization systems are designed to fulfill those requirements, the generated texts have to be evaluated according to their intended function and its quality. The quality and success of human produced abstracts have already been addressed in the literature (Grant, 1992; Gibson, 1993) using linguistic criteria such as cohesion and coherence, thematic structure, sentence structure and lexical density. But in automatic text summarization, this is an emergent research topic. (Mind l et al., 1997) have proposed two methods of evaluation addressing the content of the abstract and its quality. For content evaluation, they asked human judges to classify summaries in broad categories and also verify if the key ideas of source documents are appropriately expressed in the summaries. For text quality, they asked human judges to identify problems such as dangling anapho</context>
</contexts>
<marker>Grant, 1992</marker>
<rawString>P. Grant. 1992. The Integration of Theory and Practice in the Development of SummaryWritting Strategies. Ph.D. thesis, Universite de Montréal. Faculte des etudes sup4rieures.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Jing</author>
<author>K R McKeown</author>
</authors>
<title>The Decomposition of Human-Written Summary Sentences.</title>
<date>1999</date>
<booktitle>Proceedings of SIGIR&apos;99. 22nd International Conference on Research and Development in Information Retrieval,</booktitle>
<pages>129--136</pages>
<editor>In M. Hearst, Gey. F., and R. Tong, editors,</editor>
<institution>University of California, Beekely,</institution>
<marker>Jing, McKeown, 1999</marker>
<rawString>H. Jing and K.R. McKeown. 1999. The Decomposition of Human-Written Summary Sentences. In M. Hearst, Gey. F., and R. Tong, editors, Proceedings of SIGIR&apos;99. 22nd International Conference on Research and Development in Information Retrieval, pages 129-136, University of California, Beekely, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H P Luhn</author>
</authors>
<title>The Automatic Creation of &amp;quot; Literature Abstracts.</title>
<date>1958</date>
<journal>IBM Journal of Research Development,</journal>
<pages>2--2</pages>
<contexts>
<context position="2797" citStr="Luhn, 1958" startWordPosition="410" endWordPosition="411">election and presentation and how we have evaluated our method of text summarization. 2 Text Summarization The process of producing a summary from a source text consists of the following steps: (i) the interpretation of the text; (ii) the extraction of the relevant information which ideally includes the &amp;quot;topics&amp;quot; of the source; (iii) the condensation of the extracted information and construction of a summary representation; and (iv) the presentation of the summary representation to the reader in natural language. While some techniques exist for producing summaries for domain independent texts (Luhn, 1958; Marcu, 1997) it seems that domain specific texts require domain specific techniques (DeJong, 1982; Paice and Jones, 1993). In our case, we are dealing with technical articles which are the result of the complex process of scientific inquiry that starts with the identification of a knowledge problem and eventually culminates with the discovery of an answer to it. Even if authors of technical articles write about several concepts in their articles, not all of them are topics. In order to address the issue of topic identification, content selection and presentation, we have studied alignments (</context>
</contexts>
<marker>Luhn, 1958</marker>
<rawString>H.P. Luhn. 1958. The Automatic Creation of &amp;quot; Literature Abstracts. IBM Journal of Research Development, 2(2):159-165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>D House</author>
<author>G Klein</author>
<author>L Hirshman</author>
<author>L Obrst</author>
<author>T Firmin</author>
<author>M Chrzanowski</author>
<author>B Sundheim</author>
</authors>
<title>The TIPSTER SUMMAC Text Summarization Evaluation.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>The Mitre Corporation.</institution>
<contexts>
<context position="23316" citStr="Mani et al., 1998" startWordPosition="3567" endWordPosition="3570">g, and performing critical welding of planning, controlling, and performing critical operations for improving productivity and quality. welding operations for improving productivity and quality. Describes HuDL (local autonomy) in greater Section 2 describes HuDL in greater detail and section detail; discusses system integration and the IMA 3 discusses system integration and the IMA . (the intelligent machine architecture); and also gives an example implementation. An example implementation is given in section 4 and section 5 contains the conclusions. Table 3: Re-Generated Sentences 1999) and (Mani et al., 1998) also used a cat•egorization task using TREC topics. For text quality, they addressed subjective aspects such as the length of the summary, its intelligibility and its usefulness. We have carried out an evaluation of our summarization method in order to assess the function of the abstract and its text quality. 7.1 Experiment We compared abstracts produced by our method with abstracts produced by Microsoft&apos;97 Summarizer and with others published with source documents (usually author abstracts). We have chosen Microsoft&apos;97 Summarizer because, even if it only produces extracts, it was the only su</context>
</contexts>
<marker>Mani, House, Klein, Hirshman, Obrst, Firmin, Chrzanowski, Sundheim, 1998</marker>
<rawString>I. Mani, D. House, G. Klein, L. Hirshman, L. Obrst, T. Firmin, M. Chrzanowski, and B. Sundheim. 1998. The TIPSTER SUMMAC Text Summarization Evaluation. Technical report, The Mitre Corporation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>From Discourse Structures to Text Summaries.</title>
<date>1997</date>
<booktitle>In The Proceedings of the ACL &apos;97/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>82--88</pages>
<location>Madrid, Spain,</location>
<contexts>
<context position="2811" citStr="Marcu, 1997" startWordPosition="412" endWordPosition="413"> presentation and how we have evaluated our method of text summarization. 2 Text Summarization The process of producing a summary from a source text consists of the following steps: (i) the interpretation of the text; (ii) the extraction of the relevant information which ideally includes the &amp;quot;topics&amp;quot; of the source; (iii) the condensation of the extracted information and construction of a summary representation; and (iv) the presentation of the summary representation to the reader in natural language. While some techniques exist for producing summaries for domain independent texts (Luhn, 1958; Marcu, 1997) it seems that domain specific texts require domain specific techniques (DeJong, 1982; Paice and Jones, 1993). In our case, we are dealing with technical articles which are the result of the complex process of scientific inquiry that starts with the identification of a knowledge problem and eventually culminates with the discovery of an answer to it. Even if authors of technical articles write about several concepts in their articles, not all of them are topics. In order to address the issue of topic identification, content selection and presentation, we have studied alignments (manually produ</context>
<context position="24043" citStr="Marcu, 1997" startWordPosition="3686" endWordPosition="3687"> length of the summary, its intelligibility and its usefulness. We have carried out an evaluation of our summarization method in order to assess the function of the abstract and its text quality. 7.1 Experiment We compared abstracts produced by our method with abstracts produced by Microsoft&apos;97 Summarizer and with others published with source documents (usually author abstracts). We have chosen Microsoft&apos;97 Summarizer because, even if it only produces extracts, it was the only summarizer available in order to carry out this evaluation and because it has already been used in other evaluations (Marcu, 1997; Barzilay and Elhadad, 1997). In order to evaluate content, we presented judges with randomly selected abstracts and five lists of keywords (content indicators). The judges had to decide to which list of keywords the abstract belongs given that different lists share some keywords and that they belong to the same technical domain. Those. lists were obtained from the journals where the source documents were published. The idea behind this evaluation is to see if the abstract convey the very essential content of the source document. In order to evaluate the quality of the text, we asked the judg</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>D. Marcu. 1997. From Discourse Structures to Text Summaries. In The Proceedings of the ACL &apos;97/EACL &apos;97 Workshop on Intelligent Scalable Text Summarization, pages 82-88, Madrid, Spain, July 11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-L Minel</author>
<author>S Nugier</author>
<author>G Piat</author>
</authors>
<title>Comment Apprecier la Qualite des Résumés Automatiques de Textes?</title>
<date>1997</date>
<booktitle>Les Exemples des Protocoles FAN et MLUCE et leurs Resultats sur SERAPHIN. In 1eres Journees Scientificques et Techniques du Reseau Francophone de l&apos;Ingenierie de la Langue de l&apos;AUPELFUREF.,</booktitle>
<pages>227--232</pages>
<marker>Minel, Nugier, Piat, 1997</marker>
<rawString>J-L. Minel, S. Nugier, and G. Piat. 1997. Comment Apprecier la Qualite des Résumés Automatiques de Textes? Les Exemples des Protocoles FAN et MLUCE et leurs Resultats sur SERAPHIN. In 1eres Journees Scientificques et Techniques du Reseau Francophone de l&apos;Ingenierie de la Langue de l&apos;AUPELFUREF., pages 227-232,15-16 avril.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-L Minel</author>
<author>J-P Descles</author>
<author>E Cartier</author>
<author>G Crispino</author>
<author>S B Hazez</author>
<author>A Jackiewicz</author>
</authors>
<title>Résumé automatique par filtrage semantique d&apos;informations dans des textes.</title>
<date>2000</date>
<journal>TSI,</journal>
<pages>12000--1</pages>
<contexts>
<context position="18494" citStr="Minel et al., 2000" startWordPosition="2836" endWordPosition="2839">ions and types of information identified account for interesting phenomena appearing in the corpus and constitute a sound basis for text summarization. &apos; Nevertheless, we have identified only a few linguistic expressions used in order to express particular elements of the conceptual model (241 domain verbs, 163 domain nouns, 129 adjectives, 174 indicative patterns, 87 informative patterns). This is because we are mainly concerned with the development of a general method of automatic abstracting and the task of constructing such linguistic resources is time consuming as recent work have shown (Minel et al., 2000). The implementation of our method relies on state-of-the-art techniques in natural language processing including noun and verb group identification and conceptual tagging. The interpreter relies on the output produced by a shallow text segmenter and on a statistical POStagger. Our prototype only analyses sentences for the specific purpose of text summarization and implements some patterns of generation observed in the corpus. Additional analysis could be done on the obtained representation to produce better results. 6 Related Work (Paice and Jones, 1993) have already addressed the issue of co</context>
</contexts>
<marker>Minel, Descles, Cartier, Crispino, Hazez, Jackiewicz, 2000</marker>
<rawString>J-L. Minel, J-P. Descles, E. Cartier, G. Crispino, S.B. Hazez, and A. Jackiewicz. 2000. Résumé automatique par filtrage semantique d&apos;informations dans des textes. TSI, X(X12000):1-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Paice</author>
<author>P A Jones</author>
</authors>
<title>The Identification of Important Concepts in Highly Structured Technical Papers. In</title>
<date>1993</date>
<booktitle>Proc. of the 16th ACM-SIGIR Conference,</booktitle>
<pages>69--78</pages>
<editor>R. Korthage, E. Rasmussen, and P. Willett, editors,</editor>
<contexts>
<context position="2920" citStr="Paice and Jones, 1993" startWordPosition="426" endWordPosition="429">process of producing a summary from a source text consists of the following steps: (i) the interpretation of the text; (ii) the extraction of the relevant information which ideally includes the &amp;quot;topics&amp;quot; of the source; (iii) the condensation of the extracted information and construction of a summary representation; and (iv) the presentation of the summary representation to the reader in natural language. While some techniques exist for producing summaries for domain independent texts (Luhn, 1958; Marcu, 1997) it seems that domain specific texts require domain specific techniques (DeJong, 1982; Paice and Jones, 1993). In our case, we are dealing with technical articles which are the result of the complex process of scientific inquiry that starts with the identification of a knowledge problem and eventually culminates with the discovery of an answer to it. Even if authors of technical articles write about several concepts in their articles, not all of them are topics. In order to address the issue of topic identification, content selection and presentation, we have studied alignments (manually produced) of sentences from professional abstracts with sentences from 1 Abstract Introducing the Topics Virtual p</context>
<context position="19055" citStr="Paice and Jones, 1993" startWordPosition="2923" endWordPosition="2926">time consuming as recent work have shown (Minel et al., 2000). The implementation of our method relies on state-of-the-art techniques in natural language processing including noun and verb group identification and conceptual tagging. The interpreter relies on the output produced by a shallow text segmenter and on a statistical POStagger. Our prototype only analyses sentences for the specific purpose of text summarization and implements some patterns of generation observed in the corpus. Additional analysis could be done on the obtained representation to produce better results. 6 Related Work (Paice and Jones, 1993) have already addressed the issue of content identification and expression in technical summarization using templates, but while they produced indicative abstracts for a specific domain, we are producing domain independent indicative-informative abstracts. Being designed for one specific domain, their abstracts are fixed in structure while our abstracts are dynamically constructed. Radev and McKeown (1998) also used instantiated templates, but in order to produce summaries of multiple documents in one specific domain. They focus on the generation of the text while we are addressing the overall</context>
</contexts>
<marker>Paice, Jones, 1993</marker>
<rawString>C.D. Paice and P.A. Jones. 1993. The Identification of Important Concepts in Highly Structured Technical Papers. In R. Korthage, E. Rasmussen, and P. Willett, editors, Proc. of the 16th ACM-SIGIR Conference, pages 69-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D R Radev</author>
<author>K R McKeown</author>
</authors>
<date>1998</date>
<booktitle>Generating Natural Language Summaries from Multiple On-Line Sources. Computational Linguistics,</booktitle>
<pages>24--3</pages>
<contexts>
<context position="19464" citStr="Radev and McKeown (1998)" startWordPosition="2978" endWordPosition="2981">ext summarization and implements some patterns of generation observed in the corpus. Additional analysis could be done on the obtained representation to produce better results. 6 Related Work (Paice and Jones, 1993) have already addressed the issue of content identification and expression in technical summarization using templates, but while they produced indicative abstracts for a specific domain, we are producing domain independent indicative-informative abstracts. Being designed for one specific domain, their abstracts are fixed in structure while our abstracts are dynamically constructed. Radev and McKeown (1998) also used instantiated templates, but in order to produce summaries of multiple documents in one specific domain. They focus on the generation of the text while we are addressing the overall process of automatic abstracting. Our concern regarding the presentation of the information is now being addressed by other researchers as well Ping and McKeown, 1999). 7 Evaluating Content and Quality in Text Summarization Abstracts are texts used in tasks such as assessing the content of the document and deciding if the source is worth reading. If text summarization systems are designed to fulfill those</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>D.R. Radev and K.R. McKeown. 1998. Generating Natural Language Summaries from Multiple On-Line Sources. Computational Linguistics, 24(3):469-500.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Rowley</author>
</authors>
<title>Abstracting and Indexing.</title>
<date>1982</date>
<location>Clive Bingley, London.</location>
<contexts>
<context position="24808" citStr="Rowley, 1982" startWordPosition="3811" endWordPosition="3812">dicators). The judges had to decide to which list of keywords the abstract belongs given that different lists share some keywords and that they belong to the same technical domain. Those. lists were obtained from the journals where the source documents were published. The idea behind this evaluation is to see if the abstract convey the very essential content of the source document. In order to evaluate the quality of the text, we asked the judges to provide an acceptability score between 0-5 for the abstract (0 for unacceptable and 5 for acceptable) based on the following criteria taken from (Rowley, 1982) (they were only suggestions to the evaluators and were not enforced): good spelling and grammar; clear indication of the topic of 7 the source document; impersonal style; one paragraph; conciseness; readable and understandable; acronyms are presented along with their expansions; and other criteria that the judge considered important as an experienced reader of abstracts of technical documents. We told the judges that we would consider the abstracts with scores above 2.5 as acceptable. Some criteria are more important than other, for example judges do not care about impersonal style but care a</context>
</contexts>
<marker>Rowley, 1982</marker>
<rawString>J. Rowley. 1982. Abstracting and Indexing. Clive Bingley, London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>G Lapalme</author>
</authors>
<title>Where does Information come from? Corpus Analysis for Automatic Abstracting. In RIFRA &apos;98. Rencontre Internationale sur l&apos;extraction le Filtrage et le Résumé Automatique,</title>
<date>1998</date>
<pages>72--83</pages>
<contexts>
<context position="7879" citStr="Saggion and Lapalme, 1998" startWordPosition="1175" endWordPosition="1178">f related research. related research. Analyzes the complexity of the algo- We analyse the complexity of our algorithm, lst/- rithm, and gives some examples of per- and give some examples of performance on formance on typical networks. typical networks. Table 1: LISA Abstract 1955 - Source Document: &amp;quot;Efficient distributed breadth-first search algorithm.&amp;quot; S.A.M. Makki. Computer Communications, 19(8) Jul 96, p628-36. abbreviation, merge and split. In our corpus, 89% of the sentences from the professional abstracts included at least one transformation. Results of the corpus study are detailed in (Saggion and Lapalme, 1998) and (Saggion and Lapalme, 2000). We have identified a total of 52 different types of information (coming from the corpus and from technical articles) for technical text summarization that we use to identify some of the main themes. These types include: the explicit topic of the document, the situation, the identification of the problem, the identification of the solution, the research goal, the explicit topic of a section, the . authors&apos; development, the inferences, the description of a topical entity, the definition. of a topical entity, the relevance of a topical entity, the advantages, etc</context>
</contexts>
<marker>Saggion, Lapalme, 1998</marker>
<rawString>H. Saggion and G. Lapalme. 1998. Where does Information come from? Corpus Analysis for Automatic Abstracting. In RIFRA &apos;98. Rencontre Internationale sur l&apos;extraction le Filtrage et le Résumé Automatique, pages 72-83.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Saggion</author>
<author>G Lapalme</author>
</authors>
<title>Evaluation of Content and Text Quality in the Context of Technical Text Summarization.</title>
<date>2000</date>
<booktitle>In Proceedings of RIA0&apos;2000,</booktitle>
<location>Paris,</location>
<contexts>
<context position="7911" citStr="Saggion and Lapalme, 2000" startWordPosition="1180" endWordPosition="1183">arch. Analyzes the complexity of the algo- We analyse the complexity of our algorithm, lst/- rithm, and gives some examples of per- and give some examples of performance on formance on typical networks. typical networks. Table 1: LISA Abstract 1955 - Source Document: &amp;quot;Efficient distributed breadth-first search algorithm.&amp;quot; S.A.M. Makki. Computer Communications, 19(8) Jul 96, p628-36. abbreviation, merge and split. In our corpus, 89% of the sentences from the professional abstracts included at least one transformation. Results of the corpus study are detailed in (Saggion and Lapalme, 1998) and (Saggion and Lapalme, 2000). We have identified a total of 52 different types of information (coming from the corpus and from technical articles) for technical text summarization that we use to identify some of the main themes. These types include: the explicit topic of the document, the situation, the identification of the problem, the identification of the solution, the research goal, the explicit topic of a section, the . authors&apos; development, the inferences, the description of a topical entity, the definition. of a topical entity, the relevance of a topical entity, the advantages, etc. Information types are classifi</context>
</contexts>
<marker>Saggion, Lapalme, 2000</marker>
<rawString>H. Saggion and G. Lapalme. 2000. Evaluation of Content and Text Quality in the Context of Technical Text Summarization. In Proceedings of RIA0&apos;2000, Paris, France, 12-14 April, 2000.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>