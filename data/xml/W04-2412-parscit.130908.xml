<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000328">
<title confidence="0.9985785">
Introduction to the CoNLL-2004 Shared Task:
Semantic Role Labeling
</title>
<author confidence="0.985226">
Xavier Carreras and Lluis M`arquez
</author>
<affiliation confidence="0.9880275">
TALP Research Centre
Technical University of Catalonia (UPC)
</affiliation>
<email confidence="0.996353">
{carreras,lluism}@lsi.upc.es
</email>
<sectionHeader confidence="0.995609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999107142857143">
In this paper we describe the CoNLL-2004
shared task: semantic role labeling. We intro-
duce the specification and goal of the task, de-
scribe the data sets and evaluation methods, and
present a general overview of the systems that
have contributed to the task, providing compar-
ative description.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996247826087">
In recent years there has been an increasing interest in
semantic parsing of natural language, which is becoming
a key issue in Information Extraction, Question Answer-
ing, Summarization, and, in general, in all NLP applica-
tions requiring some kind of semantic interpretation.
The shared task of CoNLL-2004 1 concerns the recog-
nition of semantic roles, for the English language. We
will refer to it as Semantic Role Labeling (SRL). Given a
sentence, the task consists of analyzing the propositions
expressed by some target verbs of the sentence. In par-
ticular, for each target verb all the constituents in the sen-
tence which fill a semantic role of the verb have to be
extracted (see Figure 1 for a detailed example). Typical
semantic arguments include Agent, Patient, Instrument,
etc. and also adjuncts such as Locative, Temporal, Man-
ner, Cause, etc.
Most existing systems for automatic semantic role la-
beling make use of a full syntactic parse of the sentence
in order to define argument boundaries and to extract rel-
evant information for training classifiers to disambiguate
between role labels. Thus, the task has been usually ap-
proached as a two phase procedure consisting of recogni-
tion and labeling of arguments.
</bodyText>
<footnote confidence="0.805672">
&apos;CoNLL-2004 Shared Task web page —with
data, software and systems’ outputs available— at
http://cnts.uia.ac.be/conll2004/roles.
</footnote>
<bodyText confidence="0.999303589285714">
Regarding the learning component of the systems,
we find pure probabilistic models (Gildea and Juraf-
sky, 2002; Gildea and Palmer, 2002; Gildea and Hock-
enmaier, 2003), Maximum Entropy (Fleischman et al.,
2003), generative models (Thompson et al., 2003), De-
cision Trees (Surdeanu et al., 2003; Chen and Ram-
bow, 2003), and Support Vector Machines (Hacioglu and
Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b).
There have also been some attempts at relaxing the ne-
cessity of using syntactic information derived from full
parse trees. For instance, in (Pradhan et al., 2003a; Ha-
cioglu and Ward, 2003), a SVM-based SRL system is
devised which performs an IOB sequence tagging using
only shallow syntactic information at the level of phrase
chunks.
Nowadays, there exist two main English corpora with
semantic annotations from which to train SRL systems:
PropBank (Palmer et al., 2004) and FrameNet (Fillmore
et al., 2001). In the CoNLL-2004 shared task we concen-
trate on the PropBank corpus, which is the Penn Treebank
corpus enriched with predicate–argument structures. It
addresses predicates expressed by verbs and labels core
arguments with consecutive numbers (A0 to A5), try-
ing to maintain coherence along different predicates. A
number of adjuncts, derived from the Treebank functional
tags, are also included in PropBank annotations.
To date, the best results reported on the PropBank cor-
respond to a Fl measure slightly over 83, when using
the gold standard parse trees from Penn Treebank as the
main source of information (Pradhan et al., 2003b). This
performance drops to 77 when a real parser is used in-
stead. Comparatively, the best SRL system based solely
on shallow syntactic information (Pradhan et al., 2003a)
performs more than 15 points below. Although these re-
sults are not directly comparable to the ones obtained in
the CoNLL-2004 shared task (different datasets, differ-
ent version of PropBank, etc.) they give an idea about the
state-of-the art results on the task.
The challenge for CoNLL-2004 shared task is to come
up with machine learning strategies which address the
SRL problem on the basis of only partial syntactic in-
formation, avoiding the use of full parsers and external
lexico-semantic knowledge bases. The annotations pro-
vided for the development of systems include, apart from
the argument boundaries and role labels, the levels ofpro-
cessing treated in the previous editions of the CoNLL
shared task, i.e., words, PoS tags, base chunks, clauses,
and named entities.
The rest of the paper is organized as follows. Section
2 describes the general setting of the task. Section 3 pro-
vides a detailed description of training, development and
test data. Participant systems are described and compared
in section 4. In particular, information about learning
techniques, SRL strategies, and feature development is
provided, together with performance results on the devel-
opment and test sets. Finally, section 5 concludes.
</bodyText>
<sectionHeader confidence="0.991977" genericHeader="method">
2 Task Description
</sectionHeader>
<bodyText confidence="0.99999185">
The goal of the task is to develop a machine learning sys-
tem to recognize arguments of verbs in a sentence, and
label them with their semantic role. A verb and its set of
arguments form a proposition in the sentence, and typi-
cally, a sentence will contain a number of propositions.
There are two properties that characterize the structure
of the arguments in a proposition. First, arguments do not
overlap, and are organized sequentially. Second, an argu-
ment may appear split into a number of non-contiguous
phrases. For instance, in the sentence “[A1 The apple],
said John, [C−A1 is on the table]”, the utterance argument
(labeled with type A1) appears split into two phrases.
Thus, there is a set of non-overlapping arguments la-
beled with semantic roles associated with each proposi-
tion. The set of arguments of a proposition can be seen as
a chunking of the sentence, in which chunks are parts of
the semantic roles of the proposition predicate.
In practice, number of target verbs are marked in a sen-
tence, each governing one proposition. A system has to
recognize and label the arguments of each target verb.
</bodyText>
<subsectionHeader confidence="0.995213">
2.1 Methodological Setting
</subsectionHeader>
<bodyText confidence="0.999924866666667">
Training and development data are provided to build the
learning system. Apart from the correct output, both data
sets contain the correct input, as well as predictions of the
input made by state-of-the-art processors. The training
set is used for training systems, whereas the development
set is used to tune parameters of the learning systems and
select the best model.
Systems have to be developed strictly with the data
provided, which consists of input and output data and the
official external resources (described below). Since the
correct annotations for the input data are provided, a sys-
tem is allowed either to be trained to predict the input
part, or to make use of an external tool developed strictly
within this setting, such as previous CoNLL shared task
systems.
</bodyText>
<subsectionHeader confidence="0.99471">
2.2 Evaluation
</subsectionHeader>
<bodyText confidence="0.999930666666667">
Evaluation is performed on a separate test set, which in-
cludes only predicted input data. A system is evaluated
with respect to precision, recall and the F1 measure. Pre-
cision (p) is the proportion of arguments predicted by a
system which are correct. Recall (r) is the proportion of
correct arguments which are predicted by a system. Fi-
nally, the F1 measure computes the harmonic mean of
precision and recall, and is the final measure to com-
pare the performance of systems. It is formulated as:
</bodyText>
<equation confidence="0.900034">
Fp=1 = 2pr/(p + r).
</equation>
<bodyText confidence="0.999983466666667">
For an argument to be correctly recognized, the words
spanning the argument as well as its semantic role have
to be correct. 2
As an exceptional case, the verb argument of each
proposition is excluded from the evaluation. This argu-
ment is the lexicalization of the predicate of the proposi-
tion. Most of the time, the verb corresponds to the target
verb of the proposition, which is provided as input, and
only in few cases the verb participant spans more words
than the target verb.
Except for non-trivial cases, this situation makes the
verb fairly easy to identify and, since there is one verb
with each proposition, evaluating its recognition over-
estimates the overall performance of a system. For this
reason, the verb argument is excluded from evaluation.
</bodyText>
<sectionHeader confidence="0.997292" genericHeader="method">
3 Data
</sectionHeader>
<bodyText confidence="0.999927125">
The data consists of six sections of the Wall Street Jour-
nal part of the Penn Treebank (Marcus et al., 1993), and
follows the setting of past editions of the CoNLL shared
task: training set (sections 15-18), development set (sec-
tion 20) and test set (section 21). We first describe anno-
tations related to argument structure. Then, we describe
the preprocessing of input data. Finally, we describe the
format of the data sets.
</bodyText>
<subsectionHeader confidence="0.995931">
3.1 PropBank
</subsectionHeader>
<bodyText confidence="0.99911125">
The Proposition Bank (PropBank) (Palmer et al., 2004)
annotates the Penn Treebank with verb argument struc-
ture. The semantic roles covered by PropBank are the
following:
</bodyText>
<listItem confidence="0.99919275">
• Numbered arguments (A0–A5, AA): Arguments
defining verb-specific roles. Their semantics de-
pends on the verb and the verb usage in a sentence,
or verb sense. In general, A0 stands for the agent
</listItem>
<footnote confidence="0.955937">
2The srl-eval.pl program is the official program to
evaluate the performance of a system. It is available at the
Shared Task web page.
</footnote>
<bodyText confidence="0.997825">
and A1 corresponds to the patient or theme of the
proposition, and these two are the most frequent
roles. However, no consistent generalization can be
made across different verbs or different senses of the
same verb. PropBank takes the definition of verb
senses from VerbNet, and for each verb and each
sense defines the set of possible roles for that verb
usage, called the roleset. The definition of rolesets
is provided in the PropBank Frames files, which is
made available for the shared task as an official re-
source to develop systems.
</bodyText>
<listItem confidence="0.995782">
• Adjuncts (AM-): General arguments that any verb
may take optionally. There are 13 types of adjuncts:
</listItem>
<table confidence="0.817130714285714">
AM-ADV: general-purpose AM-MOD : modal verb
AM-CAU : cause AM-NEG: negation marker
AM-DIR : direction AM-PNC : purpose
AM-DIS : discourse marker AM-PRD : predication
AM-EXT : extent AM-REC : reciprocal
AM-LOC: location AM-TMP : temporal
AM-MNR : manner
</table>
<listItem confidence="0.950037142857143">
• References (R-): Arguments representing argu-
ments realized in other parts of the sentence. The
role of a reference is the same as the role of the ref-
erenced argument. The label is an R- tag prefixed to
the label of the referent, e.g. R-A1.
• Verbs (V): Participant realizing the verb of the
proposition, with exactly one verb for each one.
</listItem>
<bodyText confidence="0.99999205882353">
We used the February 2004 release of PropBank. Most
predicative verbs were annotated, although not all of
them (for example, most of the occurrences of the verb
“to have” and “to be” were not annotated). We applied
procedures to check consistency of propositions, looking
for overlapping arguments, and incorrect semantic role
labels. Also, co-referenced arguments were annotated as
a single item in PropBank, and we automatically distin-
guished between the referent and the reference with sim-
ple rules matching pronominal expressions, which were
tagged as R arguments. A total number of 68 proposi-
tions were not compliant with our procedures, and were
filtered out from the CoNLL data sets. The predicate-
argument annotations, thus, are not necessarily complete
in a sentence. Table 1 provides counts of the number of
sentences, annotated propositions, distinct verbs and ar-
guments in the three data sets.
</bodyText>
<subsectionHeader confidence="0.998674">
3.2 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999407666666667">
In this section we describe the pipeline of processors to
compute the annotations which form the input part of
the data: part-of-speech (PoS) tags, chunks, clauses and
named entities. The preprocessors correspond to the fol-
lowing state-of-the-art systems for each level of annota-
tion:
</bodyText>
<table confidence="0.995632527777777">
Training Devel. Test
Sentences 8,936 2,012 1,671
Tokens 211,727 47,377 40,039
Propositions 19,098 4,305 3,627
Distinct Verbs 1,838 978 855
All Arguments 50,182 11,121 9,598
A0 12,709 2,875 2,579
A1 18,046 4,064 3,429
A2 4,223 954 714
A3 784 149 150
A4 626 147 50
A5 14 4 2
AA 5 0 0
AM-ADV 1,727 352 307
AM-CAU 283 53 49
AM-DIR 231 60 50
AM-DIS 1,077 204 213
AM-EXT 152 49 14
AM-LOC 1,279 230 228
AM-MNR 1,337 334 255
AM-MOD 1,753 389 337
AM-NEG 687 131 127
AM-PNC 446 100 85
AM-PRD 10 3 3
AM-REC 2 1 0
AM-TMP 3,567 759 747
R-A0 738 162 159
R-A1 360 74 70
R-A2 49 17 9
R-A3 8 0 1
R-AA 1 0 0
R-AM-ADV 1 0 0
R-AM-LOC 27 4 4
R-AM-MNR 4 0 1
R-AM-PNC 1 0 1
R-AM-TMP 35 6 14
</table>
<tableCaption confidence="0.999476">
Table 1: Counts on the three data sets.
</tableCaption>
<listItem confidence="0.996020285714286">
• PoS tagger: (Gim´enez and M`arquez, 2003), based
on Support Vector Machines, and trained on Penn
Treebank sections 0–18.
• Chunker and Clause Recognizer: (Carreras and
M`arquez, 2003), based on Voted Perceptrons, and
following the CoNLL settings of 2000 and 2001
tasks (Tjong Kim Sang and Buchholz, 2000; Tjong
Kim Sang and D´ejean, 2001). These two processors
form a coherent partial syntax of a sentence, that is,
chunks and clauses form a tree.
• Named entities with (Chieu and Ng, 2003), based
on Maximum-Entropy classifiers, and following the
CoNLL-2003 task setting (Tjong Kim Sang and
De Meulder, 2003).
</listItem>
<table confidence="0.9959225">
Precision Recall Fl/Acc.
PoS Dev. (acc.) – – 96.88
PoS Test (acc.) – – 96.70
Chunking Dev. 94.28% 93.65% 93.96
Chunking Test 93.80% 92.93% 93.36
Clauses Dev. 90.51% 86.12% 88.26
Clauses Test 88.73% 82.92% 85.73
Named Entities 88.12% 88.51% 88.31
</table>
<tableCaption confidence="0.983122666666667">
Table 2: Results of the preprocessing modules on the de-
velopment and test sets. Named Entity figures are based
on the CoNLL-2003 test set.
</tableCaption>
<bodyText confidence="0.999892333333333">
Such processors were ran in a pipeline, from PoS tags,
to chunks, clauses and finally named entities. Table 2
summarizes the performance of the processors on the de-
velopment and test sections. These figures differ from the
original results in the original due to a better quality of the
input information in our runs. The figures of the named
entity extractor are based on the corpus of the CoNLL-
2003 shared task, since gold annotations of named enti-
ties were not available for the current corpus.
</bodyText>
<subsectionHeader confidence="0.990039">
3.3 Format
</subsectionHeader>
<bodyText confidence="0.9998886">
Figure 1 shows an example of a fully-annotated sentence.
Annotations of a sentence are given using a flat represen-
tation in columns, separated by spaces. Each column en-
codes an annotation by associating a tag with every word.
For each sentence, the following columns are provided:
</bodyText>
<listItem confidence="0.9170502">
1. Words.
2. Part of Speech tags.
3. Chunks in IOB2 format.
4. Clauses in Start-End format.
5. Named Entities in IOB2 format.
6. Target verbs, marking n predicative verbs. This
column, provided as input, specifies the governing
verbs of the propositions to be analyzed. Each target
verb is in the base form. Occasionally this column
does not mark any verb (i.e., n may be 0).
</listItem>
<bodyText confidence="0.979726724137931">
7. For each of the n target verbs, a column in Start-End
format specifying the arguments of the proposition.
These columns are the output of a system, that is,
the ones to be predicted, and are not available for
the test set.
IOB2 format. Represents chunks which do not overlap
nor embed. Words outside a chunk receive the tag O. For
words forming a chunk of type k, the first word receives
the B-k tag (Begin), and the remaining words receive the
tag z-k (Inside).
Start-End format. Represents non-overlapping
phrases (clauses or arguments) which may be embed-
ded3 inside one another. Each tag indicates whether
a clause starts or ends at that word and is of the form
START*END. The START part is a concatenation of (k
parentheses, each representing that a phrase of type k
starts at that word. The END part is a concatenation of
k) parentheses, each representing that a phrase of type
k ends at that word. For example, the * tag represents
a word with no starts and ends; the (A0*A0) tag
represents a word constituting an A0 argument; and the
(S(S*S) tag represents a word which constitutes a
base clause (labeled S) and starts another higher-level
clause. Finally, the concatenation of all tags constitutes
a well-formed bracketing. For the particular case of split
arguments, of type k, the first part appears as a phrase
with label k, and the remaining as phrases with label
C-k (continuation prefix). See examples of annotations
at columns 4th, 7th and 8th of Figure 1.
</bodyText>
<sectionHeader confidence="0.984997" genericHeader="method">
4 Participating Systems
</sectionHeader>
<bodyText confidence="0.999951428571429">
Ten systems have participated in the CoNLL-2004 shared
task. They approached the task in several ways, using dif-
ferent learning components and labeling strategies. The
following subsections briefly summarize the most impor-
tant properties of each system and provide a qualitative
comparison between them, together with a quantitative
evaluation on the development and test sets.
</bodyText>
<subsectionHeader confidence="0.998169">
4.1 Learning techniques
</subsectionHeader>
<bodyText confidence="0.999875041666667">
Up to six different learning algorithms have been ap-
plied in the CoNLL-2004 shared task. None of them
is new with respect to the past editions. Two teams
used the Maximum Entropy (ME) statistical framework
(Baldewein et al., 2004; Lim et al., 2004). Two teams
used Brill’s Transformation-based Error-driven Learning
(TBL) (Higgins, 2004; Williams et al., 2004). Two other
groups applied Memory-Based Learning (MBL) (van den
Bosch et al., 2004; Kouchnir, 2004). The remaining four
teams employed vector-based linear classifiers of differ-
ent types: Hacioglu et al. (2004) and Park et al. (2004)
used Support Vector Machines (SVM) with polyno-
mial kernels, Carreras et al. (2004) used Voted Percep-
trons (VP) also with polynomial kernels, and finally,
Punyakanok et al. (2004) used SNoW, a Winnow-based
network of linear separators. Additionally, the team of
Baldewein et al. (2004) used a EM–based clustering al-
gorithm for feature development (see section 4.3).
As a main difference with respect to past editions, less
effort has been put into combining different learning al-
gorithms and outputs. Instead, the main effort of partici-
pants went into developing useful SRL strategies and into
the development of features (see sections 4.2 and 4.3).
As an exception, van den Bosch et al. (2004) applied a
</bodyText>
<footnote confidence="0.614371">
3Arguments in data do not embed, though format allows so.
</footnote>
<table confidence="0.998664619047619">
The DT B-NP (S* O - (A0* *
San NNP I-NP * B-ORG - * *
Francisco NNP I-NP * I-ORG - * *
Examiner NNP I-NP * I-ORG - *A0) *
issued VBD B-VP * O issue (V*V) *
a DT B-NP * O - (A1* (A1*
special JJ I-NP * O - * *
edition NN I-NP * O - *A1) *A1)
around IN B-PP * O - (AM-TMP* *
noon NN B-NP * O - *AM-TMP) *
yesterday NN B-NP * O - (AM-TMP*AM-TMP) *
that WDT B-NP (S* O - (C-A1* (R-A1*R-A1)
was VBD B-VP (S* O - * *
filled VBN I-VP * O fill * (V*V)
entirely RB B-ADVP * O - * (AM-MNR*AM-MNR)
with IN B-PP * O - * *
earthquake NN B-NP * O - * (A2*
news NN I-NP * O - * *
and CC I-NP * O - * *
information NN I-NP *S)S) O - *C-A1) *A2)
. . O *S) O - * *
</table>
<figureCaption confidence="0.9935105">
Figure 1: An example of an annotated sentence, in columns. Input consists of words (1st), PoS tags (2nd), base chunks
(3rd), clauses (4th) and named entities (5th). The 6th column marks target verbs, and their propositions are found in
</figureCaption>
<bodyText confidence="0.940124">
remaining columns. According to the PropBank Frames, for issue (7th), the A0 annotates the issuer, and the A1 the
thing issued, which appears split into two parts. For fill (8th), A1 is the the destination, and A2 the theme.
voting strategy to derive the final sequence tagging as
a voted combination of three overlapping n-gram output
sequences. The same team also applied a meta-learning
step, by using iterative classifier stacking, for correcting
systematic errors committed by the low–level classifiers.
This work is also worth mentioning because of the exten-
sive work done on parameter tuning and feature selection.
</bodyText>
<subsectionHeader confidence="0.974629">
4.2 SRL approaches
</subsectionHeader>
<bodyText confidence="0.999801928571429">
SRL is a complex task which has to be decomposed into
a number of simpler decisions and tagging schemes in
order to be addressed by learning techniques.
One first issue is the annotation of the different propo-
sitions of a sentence. Most of the groups treated the
annotation of semantic roles for each verb predicate as
an independent problem. An exception is the system of
Carreras et al. (2004), which performs the annotation of
all propositions simultaneously. As a consequence, the
former teams treat the problem as the recognition of se-
quential structures (a.k.a. chunking), while the latter di-
rectly derives a hierarchical structure formed by the argu-
ments of all propositions. Table 3 summarizes the main
properties of each system regarding the SRL strategy im-
plemented. This property corresponds to the first column.
Regarding the labeling strategy, we can distinguish at
least three different strategies. The first one consists of
performing role identification directly by a IOB-type se-
quence tagging. The second approach consists of divid-
ing the problem into two independent phases: recogni-
tion, in which the arguments are recognized, and label-
ing, in which the already recognized arguments are as-
signed role labels. The third approach also proceeds in
two phases: filtering, in which a set of argument can-
didates are decided and labeling, in which the set of
optimal arguments is derived from the proposed can-
didates. As a variant of the first two-phase strategy,
van den Bosch et al. (2004) first perform a direct classi-
fication of chunks into argument labels, and then decide
the actual arguments in a post-process by joining previ-
ously classified argument fragments. All this information
is summarized in the second column of Table 3.
An implication of implementing the two-phase strat-
egy is the ability to work with argument candidates in
the second phase, allowing to develop feature patterns for
complete arguments. Regarding the first phase, the recog-
nition of candidate arguments is performed by means
of a IOB or open–close tagging using classifiers, either
argument–independent, or specialized by argument type.
It is also worth noting that all participant systems per-
formed learning of predicate-independent classifiers in-
stead of specializing by the verb predicate. Information
about verb predicates is captured through features and
some global restrictions.
Another important issue is the granularity at which
the sentence elements are processed. It has become very
clear that a good election for this problem is phrase-by-
phrase processing (P-by-P, using the notation introduced
by Hacioglu et al. (2004)) instead of word-by-word (W-
by-W). The motivation is twofold: (1) phrase boundaries
are almost always consistent with argument boundaries;
(2) P-by-P processing is computationally less expensive
and allows to explore a relatively larger context. Most of
the groups performed a P-by-P processing, but admitting
a processing by words within the target verb chunks. The
system by Baldewein et al. (2004) works with a bit more
general elements called “chunk sequences”, extracted in
a preprocess using heuristic rules. This information is
presented in the third column of Table 3.
Information regarding clauses has proven to be very
useful, as can be seen in section 4.3. All systems captured
some kind of clause information through feature codifica-
tion. However, some of the systems restrict the search for
arguments only to the immediate clause (Park et al., 2004;
Williams et al., 2004) and others use the clause hierarchy
to guide the exploration of the sentence (Lim et al., 2004;
Carreras et al., 2004).
Very relevant to the SRL strategy is the availability of
global sentential information when decisions are taken.
Almost all of the systems try to capture some global level
information by collecting features describing the target
predicate and its context, the “syntactic path” from the
element under consideration to the predicate, etc. (see
section 4.3). But only some of them include a global
optimization procedure at sentence level in the labeling
strategy. The systems working with Maximum Entropy
Models (Baldewein et al., 2004; Lim et al., 2004) use
beam search to find taggings that maximize the prob-
ability of the output sequence. Carreras et al. (2004)
and Punyakanok et al. (2004) also define a global scor-
ing function to maximize. At this point, the system of
Punyakanok et al. (2004) deserves special consideration,
since it formally implements a set of structural and lin-
guistic constraints directly in the global cost function to
maximize. These constraints act as a filter for valid out-
put sequences and ensure coherence of the output. Au-
thors refer to this part of the system as the inference
layer and they implement it using integer linear program-
ming. The iterative classifier stacking mechanism used
by van den Bosch et al. (2004) also tries to alleviate the
problem of locality of the low-level classifiers. This in-
formation is found in the fourth column of Table 3.
Finally, some systems use some kind of postprocess-
ing to ensure coherence of the final labeling, correct some
systematic errors, or to treat some types of adjunctive ar-
guments. In most of the cases, this postprocess is per-
formed on the basis of simple ad-hoc rules. This infor-
mation is included in the last column of Table 3.
</bodyText>
<subsectionHeader confidence="0.902983">
4.3 Features
</subsectionHeader>
<bodyText confidence="0.98932867">
With a very few exceptions all the participant systems
have used all levels of linguistic information provided in
the training data sets, that is, words, PoS and chunk la-
bels, clauses, and named entities.
It is worth mentioning that the general type of features
prop. lab. gran. glob. post
hacioglu s t P-by-P no no
punyakanok s fl W-by-W yes no
carreras j fl P-by-P yes no
lim s t P-by-P yes no
park s rc P-by-P no yes
higgins s t W-by-W no yes
van den bosch s cj P-by-P part. yes
kouchnir s rc P-by-P no yes
baldewein s rc P-by-P yes no
williams s t mixed no no
Table 3: Main properties of the SRL strategies imple-
mented by the ten participant teams (sorted by perfor-
mance on the test set). “prop.” stands for the treatment of
all propositions of a sentence; possible values are: s (sep-
arate) and j (joint). “lab.” stands for labeling strategy;
possible values are: t (one step tagging), rc (recognition
+ classification), fl (filtering + labeling), cj (classifica-
tion + joining). “gran.” stands for granularity; “glob.”
stands for global optimization. “post” stands for post-
processing.
derived from the basic information are strongly inspired
by previous works on the SRL task (Gildea and Jurafsky,
2002; Surdeanu et al., 2003; Pradhan et al., 2003a). Many
systems used the same kind of ideas but implemented
in different ways, since the particular learning strategies
used (see section 4.2) impose different constraints on the
type of information available or the way of expressing it.
As a general idea, we can divide the features into four
types: (1) basic features, evaluating some kind of local
information on the context of the word or constituent be-
ing treated; (2) Features characterizing the internal struc-
ture of a candidate argument; (3) Features describing
properties of the target verb predicate; (4) Features that
capture the relations between the verb predicate and the
constituent under consideration.
All systems used some kind of basic features. Roughly
speaking, they consist of words, PoS tags, chunks, clause
labels, and named entities extracted from a window-
based context. These values can be considered with
or without the relative position with respect to the el-
ement under consideration, and some n-grams of them
can also be computed. If the granularity of the sys-
tem is at phrase level then typically a representative
head word of the phrase is used as lexical information.
As an exception to the general approach, the system of
Williams et al. (2004) does not make use of word forms.
The rest of the features are more interesting since they
are task dependent, and deserve special attention. Table
4 summarizes the type of features exploited by systems.
To represent an argument itself, few attributes are of
general usage. Some systems count the length of it,
with different granularities. Others make use of heuris-
tics to derive its syntactic type. There are systems that
extract a structured representation of the argument, ei-
ther homogeneous (capturing different sequences of head
words, PoS tags, chunks or clauses), or heterogeneous
(combining all elements, based on the syntactic hierar-
chy). A few systems have captured the existence of
neighboring arguments, previously identified in the pro-
cess. Interestingly, the system of Lim et al. (2004) rep-
resents the context of an argument relative to the syntac-
tic hierarchy by means of relative constituent sequences
and syntactic levels. Concerning lexicalization of the
argument, most of the techniques rely on head word
rules based on Collins’, or content word rules as in
Surdeanu et al. (2003). Only Carreras et al. (2004) de-
cide to use a bag-of-words model, apart from heuristic-
based lexicalization.
Regarding the target verb, the voice feature of the verb
is generally used, in addition to basic features capturing
the form and PoS tag of the verb. Some systems captured
statistics on frequent argument patterns for each predi-
cate. Also, systems represented the elements in the prox-
imity of the target verb, inspired by local subcategoriza-
tion patterns of a predicate.
As for features related to a constituent-predicate pair,
all systems use the simple feature describing the relative
position between them, and to a lesser degree, the dis-
tance and the difference in clausal levels. Again, there is
a general tendency to describe the structured path from
the argument to the verb. Its design goes from sim-
ple homogeneous sequences of head words or chunks, to
more sophisticated paths combining chunks and clauses,
and capturing hierarchical properties. The system of
Park et al. (2004) also tracks the number of different syn-
tactic elements found between the pair. Remarkably, the
system of Baldewein et al. (2004) uses an EM clustering
technique to derive features representing the affinity of an
argument and a predicate.
On top of basic feature extraction, all teams work-
ing with SVM and VP used polynomial kernels of de-
gree 2. Similar in expressiveness, the system designed
by Punyakanok et al. (2004) expanded the feature space
with all pairs of basic features.
</bodyText>
<subsectionHeader confidence="0.991216">
4.4 Evaluation
</subsectionHeader>
<bodyText confidence="0.9869704">
A baseline rate was computed for the task. It was pro-
duced by a system developed by Erik Tjong Kim Sang,
from the University of Antwerp, Belgium. The base-
line processor finds semantic roles based on the following
seven rules:
</bodyText>
<listItem confidence="0.982682727272727">
• Tag target verb and successive particles as V.
• Tag not and n’t in target verb chunk as AM-NEG.
• Tag modal verbs in target verb chunk as AM-MOD.
• Tag first NP before target verb as A0.
• Tag first NP after target verb as A1.
• Tag that, which and who before target verb as
R-A0.
• Switch A0 and A1, and R-A0 and R-A1 if the target
verb is part of a passive VP chunk. A VP chunk is
considered in passive voice if it contains a form of
to be and the verb does not end in ing.
</listItem>
<bodyText confidence="0.9999148">
Table 5 presents the overall results obtained by the
ten participating systems, on the development and test
sets. The best performance was obtained by the SVM-
based IOB tagger of (Hacioglu et al., 2004), which al-
most reached the performance of 70 in F1 on the test.
The seven best systems obtained F1 scores in the range
of 60-70, and only three systems scored below that.
Comparing the results across development and test cor-
pora, most systems experienced a decrease in perfor-
mance between 1.5 and 3 points. As in previous editions
of the shared task, we attribute this behavior to a greater
difficulty of the test set instead of an overfitting effect.
Interestingly, the three systems performing below 60 in
the development set did not experienced this decrease. In
fact (Williams et al., 2004) and (Baldewein et al., 2004)
even improved the results on the test set.
Table 6 details the performance of systems for the A0-
A4 arguments, on the test set. Consistently, the best per-
forming system of the task also outperforms all other sys-
tems on these semantic roles.
</bodyText>
<sectionHeader confidence="0.998904" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999790434782609">
We have described the CoNLL-2004 shared task on se-
mantic role labeling. The task was based on the Prop-
Bank corpus, and the challenge was to come up with ma-
chine learning techniques to recognize and label semantic
roles on the basis of partial syntactic structure. Ten sys-
tems have participated to the task, contributing with a va-
riety of standard or novel learning architectures. The best
system, presented by the most experienced group on the
task (Hacioglu et al., 2004), achieved a moderate perfor-
mance of 69.49 at the F1 measure. It is based on a SVM
tagging system, performing IOB decisions on the chunks
of the sentence, and exploiting a wide variety of features
based on partial syntax.
Most of the systems advance the state-of-the-art on se-
mantic role labeling on the basis of partial syntax. How-
ever, state-of-the-art systems working with full syntax
still perform substantially better, although far from a de-
sired behavior for real-task application. Two questions
remain open: which syntactic structures are needed as in-
put for the task, and what other sources of information are
required to obtain a real-world, accurate performance.
As a future line, a more thorough experimental eval-
uation is required to see which are the components that
</bodyText>
<equation confidence="0.806906909090909">
sy ne al at as aw an vv vs vf vc rp di pa ex
hacioglu + + + – – + – + + – + + + + +
punyakanok + + + + + + – + – + + + – + +
carreras + – – – + + – + – – – + – + +
lim + – – – – + + + – – – + – + –
park + – – – – – – + – – + + + + +
higgins + + – – – – + + – – – + + + –
van den bosch + + – – – – – + + – – + + – –
kouchnir + – + – + + – + – + – + + – –
baldewein + + + + + + – + + – – + + – –
williams + + – – – – – – – – – + – – –
</equation>
<tableCaption confidence="0.9635354">
Table 4: Main feature types used by the 10 participating systems in the CoNLL-2004 shared task, sorted by perfor-
mance on the test set. “sy”: use of partial syntax (all levels); “ne”: use of named entities; “al”: argument length; “at”:
argument type; “as”: argument internal structure; “aw”: head-word lexicalization of arguments; “an”: neighboring
arguments; “vv”: verb voice; “vs”: verb statistics; “vf”: verb features derived from PropBank frames; “vc”: verb local
context; “rp”: relative position; “di”: distance (horizontal or in the hierarchy); “pa”: path; “ex”: feature expansion.
</tableCaption>
<bodyText confidence="0.929282">
most contributed to the performance of systems.
</bodyText>
<sectionHeader confidence="0.963913" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99934525">
Authors would like to thank the following people and
institutions. The PropBank team, and specially Martha
Palmer and Scott Cotton, for making the corpus available.
The CoNLL-2004 board for fruitful discussions and sug-
gestions. In particular, Erik Tjong Kim Sang for useful
comments from his valuable experience, and for making
the baseline SRL processor available. Lluis Padr´o and
Mihai Surdeanu, Grzegorz Chrupała, and Hwee Tou Ng
for helping us in the reviewing process and the prepara-
tion of this document. Finally, the teams contributing to
shared task, for their great interest in participating.
This work has been partially funded by the European
Commission (Meaning, IST-2001-34460) and the Span-
ish Research Department (Aliado, TIC2002-04447-C02).
Xavier Carreras is supported by a pre-doctoral grant from
the Catalan Research Department.
</bodyText>
<sectionHeader confidence="0.998601" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.7937896">
Ulrike Baldewein, Katrin Erk, Sebastian Pad´o, and Detlef
Prescher. 2004. Semantic role labeling with chunk
sequences. In Proceedings of CoNLL-2004.
Xavier Carreras and Lluis M`arquez. 2003. Phrase recog-
nition by filtering and ranking with perceptrons. In
Proceedings of RANLP-2003, Borovets, Bulgaria.
Xavier Carreras, Lluis M`arquez, and Grzegorz Chrupała.
2004. Hierarchical recognition of propositional argu-
ments with perceptrons. In Proceedings of CoNLL-
2004.
John Chen and Owen Rambow. 2003. Use of deep lin-
guistic features for the recognition and labeling of se-
mantic arguments. In Proceedings of EMNLP-2003,
Sapporo, Japan.
Hai Leong Chieu and Hwee Tou Ng. 2003. Named en-
tity recognition with a maximum entropy approach. In
Proceedings of CoNLL-2003, Edmonton, Canada.
Charles J. Fillmore, Charles Wooters, and Collin F.
Baker. 2001. Building a large lexical databank which
provides deep semantics. In Proceedings of the Pa-
cific Asian Conference on Language, Informa tion and
Computation, Hong Kong, China.
Michael Fleischman, Namhee Kwon, and Eduard Hovy.
2003. Maximum entropy models for framenet clas-
sification. In Proceedings of EMNLP-2003, Sapporo,
Japan.
Daniel Gildea and Julia Hockenmaier. 2003. Identifying
semantic roles using combinatory categorial grammar.
In Proceedings of EMNLP-2003, Sapporo, Japan.
Daniel Gildea and Daniel Jurafsky. 2002. Automatic la-
beling of semantic roles. Computational Linguistics,
28(3):245–288.
Daniel Gildea and Martha Palmer. 2002. The necessity
of parsing for predicate argument recognition. In Pro-
ceedings ofACL 2002, Philadelphia, USA.
Jes´us Gim´enez and Lluis M`arquez. 2003. Fast and accu-
rate part-of-speech tagging: The svm approach revis-
ited. In Proceedings of RANLP-2003, Borovets, Bul-
garia.
Kadri Hacioglu and Wayne Ward. 2003. Target word de-
tection and semantic role chunking using support vec-
tor machines. In Proceedings of HLT-NAACL 2003,
Edmonton, Canada.
Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2004. Semantic role la-
beling by tagging syntactic chunks. In Proceedings of
CoNLL-2004.
Derrick Higgins. 2004. A transformation-based ap-
proach to argument labeling. In Proceedings of
CoNLL-2004.
</reference>
<table confidence="0.532346">
development Precision Recall F1
hacioglu 74.18% 69.43% 71.72
punyakanok 71.96% 64.93% 68.26
carreras 73.40% 63.70% 68.21
lim 69.78% 62.57% 65.97
park 67.27% 64.36% 65.78
higgins 65.59% 60.16% 62.76
van den bosch 69.06% 57.84% 62.95
kouchnir 44.93% 63.12% 52.50
baldewein 64.90% 41.61% 50.71
williams 53.37% 32.43% 40.35
baseline 50.63% 30.30% 37.91
test Precision Recall F1
hacioglu 72.43% 66.77% 69.49
punyakanok 70.07% 63.07% 66.39
carreras 71.81% 61.11% 66.03
lim 68.42% 61.47% 64.76
park 65.63% 62.43% 63.99
higgins 64.17% 57.52% 60.66
van den bosch 67.12% 54.46% 60.13
kouchnir 56.86% 49.95% 53.18
baldewein 65.73% 42.60% 51.70
williams 58.08% 34.75% 43.48
baseline 54.60% 31.39% 39.87
</table>
<tableCaption confidence="0.463265666666667">
Table 5: Overall precision, recall and F1 rates obtained by
the ten participating systems in the CoNLL-2004 shared
task on the development and test sets.
</tableCaption>
<reference confidence="0.9922292">
Beata Kouchnir. 2004. A memory-based approach for
semantic role labeling. In Proceedings of CoNLL-
2004.
Joon-Ho Lim, Young-Sook Hwang, So-Young Park, and
Hae-Chang Rim. 2004. Semantic role labeling using
maximum entropy model. In Proceedings of CoNLL-
2004.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: the Penn Treebank. Computational
Linguistics, 19.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2004. The proposition bank: An annotated corpus of
semantic roles. Computational Linguistics. Submit-
ted.
Kyung-Mi Park, Young-Sook Hwang, and Hae-Chang
Rim. 2004. Two-phase semantic role labeling
based on support vector machines. In Proceedings of
CoNLL-2004.
Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,
Wayne Ward, James H. Martin, and Daniel Jurafsky.
2003a. Support vector learning for semantic argument
classification. Technical Report TR-CSLR-2003-03,
Center for Spoken Language Research, University of
Colorado.
</reference>
<table confidence="0.99203425">
A0 A1 A2 A3 A4
hacioglu 81.37 71.63 49.33 51.11 66.67
punyakanok 79.38 68.16 46.69 34.04 65.22
carreras 79.05 66.96 43.28 31.22 62.07
lim 77.42 66.00 49.07 41.77 54.55
park 76.38 66.14 46.57 42.32 51.76
higgins 70.67 62.72 45.52 40.00 39.64
van den bosch 74.95 60.83 40.41 37.44 62.37
kouchnir 65.49 54.48 30.95 19.71 36.07
baldewein 66.76 53.37 37.60 22.89 27.69
williams 56.24 49.05 00.00 00.00 00.00
baseline 57.65 34.19 00.00 00.00 00.00
</table>
<tableCaption confidence="0.93911">
Table 6: F1 scores on the most frequent core argument
types obtained by the ten participating systems in the
</tableCaption>
<reference confidence="0.988247051282051">
CoNLL-2004 shared task on the test set. Systems sorted
by overall performance on the test set.
Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.
Martin, and Daniel Jurafsky. 2003b. Semantic role
parsing: Adding semantic structure to unstructured
text. In Proceedings of the International Conference
on Data Mining (ICDM-2003), Melbourne, USA.
Vasin Punyakanok, Dan Roth, Wen-Tau Yih, Dav Zimak,
and Yuancheng Tu. 2004. Semantic role labeling via
generalized inference over classifiers. In Proceedings
of CoNLL-2004.
Mihai Surdeanu, Sanda Harabagiu, John Williams, and
Paul Aarseth. 2003. Using predicate-argument struc-
tures for information extraction. In Proceedings of
ACL 2003, Sapporo, Japan.
Cynthia A. Thompson, Roger Levy, and Christopher D.
Manning. 2003. A generative model for semantic
role labeling. In Proceedings ofECML’03, Dubrovnik,
Croatia.
E. F. Tjong Kim Sang and S. Buchholz. 2000. Intro-
duction to the CoNLL-2000 shared task: Chunking.
In Proceedings of the 4th Conference on Natural Lan-
guage Learning, CoNLL-2000.
Erik F. Tjong Kim Sang and Fien De Meulder. 2003. In-
troduction to the CoNLL-2003 shared task: Language-
independent named entity recognition. In Proceedings
of CoNLL-2003.
Erik F. Tjong Kim Sang and Herv´e D´ejean. 2001. Intro-
duction to the CoNLL-2001 shared task: Clause identi-
fication. In Proceedings of the 5th Conference on Nat-
ural Language Learning, CoNLL-2001.
Antal van den Bosch, Sander Canisius, Walter Daele-
mans, Iris Hendrickx, and Erik Tjong Kim Sang.
2004. Memory-based semantic role labeling: Optimiz-
ing features, algorithm, and output. In Proceedings of
CoNLL-2004.
Ken Williams, Christopher Dozier, and Andrew McCul-
loh. 2004. Learning transformation rules for semantic
role labeling. In Proceedings of CoNLL-2004.
</reference>
</variant>
</algorithm>

<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ulrike Baldewein</author>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
<author>Detlef Prescher</author>
</authors>
<title>Semantic role labeling with chunk sequences.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<marker>Baldewein, Erk, Pad´o, Prescher, 2004</marker>
<rawString>Ulrike Baldewein, Katrin Erk, Sebastian Pad´o, and Detlef Prescher. 2004. Semantic role labeling with chunk sequences. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
</authors>
<title>Phrase recognition by filtering and ranking with perceptrons.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP-2003, Borovets,</booktitle>
<marker>Carreras, M`arquez, 2003</marker>
<rawString>Xavier Carreras and Lluis M`arquez. 2003. Phrase recognition by filtering and ranking with perceptrons. In Proceedings of RANLP-2003, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
<author>Lluis M`arquez</author>
<author>Grzegorz Chrupała</author>
</authors>
<title>Hierarchical recognition of propositional arguments with perceptrons.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL2004.</booktitle>
<marker>Carreras, M`arquez, Chrupała, 2004</marker>
<rawString>Xavier Carreras, Lluis M`arquez, and Grzegorz Chrupała. 2004. Hierarchical recognition of propositional arguments with perceptrons. In Proceedings of CoNLL2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Chen</author>
<author>Owen Rambow</author>
</authors>
<title>Use of deep linguistic features for the recognition and labeling of semantic arguments.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2171" citStr="Chen and Rambow, 2003" startWordPosition="332" endWordPosition="336"> training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et</context>
</contexts>
<marker>Chen, Rambow, 2003</marker>
<rawString>John Chen and Owen Rambow. 2003. Use of deep linguistic features for the recognition and labeling of semantic arguments. In Proceedings of EMNLP-2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Leong Chieu</author>
<author>Hwee Tou Ng</author>
</authors>
<title>Named entity recognition with a maximum entropy approach.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="12636" citStr="Chieu and Ng, 2003" startWordPosition="2088" endWordPosition="2091">AA 1 0 0 R-AM-ADV 1 0 0 R-AM-LOC 27 4 4 R-AM-MNR 4 0 1 R-AM-PNC 1 0 1 R-AM-TMP 35 6 14 Table 1: Counts on the three data sets. • PoS tagger: (Gim´enez and M`arquez, 2003), based on Support Vector Machines, and trained on Penn Treebank sections 0–18. • Chunker and Clause Recognizer: (Carreras and M`arquez, 2003), based on Voted Perceptrons, and following the CoNLL settings of 2000 and 2001 tasks (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001). These two processors form a coherent partial syntax of a sentence, that is, chunks and clauses form a tree. • Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). Precision Recall Fl/Acc. PoS Dev. (acc.) – – 96.88 PoS Test (acc.) – – 96.70 Chunking Dev. 94.28% 93.65% 93.96 Chunking Test 93.80% 92.93% 93.36 Clauses Dev. 90.51% 86.12% 88.26 Clauses Test 88.73% 82.92% 85.73 Named Entities 88.12% 88.51% 88.31 Table 2: Results of the preprocessing modules on the development and test sets. Named Entity figures are based on the CoNLL-2003 test set. Such processors were ran in a pipeline, from PoS tags, to chunks, clauses and finally named en</context>
</contexts>
<marker>Chieu, Ng, 2003</marker>
<rawString>Hai Leong Chieu and Hwee Tou Ng. 2003. Named entity recognition with a maximum entropy approach. In Proceedings of CoNLL-2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Charles J Fillmore</author>
<author>Charles Wooters</author>
<author>Collin F Baker</author>
</authors>
<title>Building a large lexical databank which provides deep semantics.</title>
<date>2001</date>
<booktitle>In Proceedings of the Pacific Asian Conference on Language, Informa tion and Computation,</booktitle>
<location>Hong Kong, China.</location>
<contexts>
<context position="2782" citStr="Fillmore et al., 2001" startWordPosition="431" endWordPosition="434">mbow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001). In the CoNLL-2004 shared task we concentrate on the PropBank corpus, which is the Penn Treebank corpus enriched with predicate–argument structures. It addresses predicates expressed by verbs and labels core arguments with consecutive numbers (A0 to A5), trying to maintain coherence along different predicates. A number of adjuncts, derived from the Treebank functional tags, are also included in PropBank annotations. To date, the best results reported on the PropBank correspond to a Fl measure slightly over 83, when using the gold standard parse trees from Penn Treebank as the main source of i</context>
</contexts>
<marker>Fillmore, Wooters, Baker, 2001</marker>
<rawString>Charles J. Fillmore, Charles Wooters, and Collin F. Baker. 2001. Building a large lexical databank which provides deep semantics. In Proceedings of the Pacific Asian Conference on Language, Informa tion and Computation, Hong Kong, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Fleischman</author>
<author>Namhee Kwon</author>
<author>Eduard Hovy</author>
</authors>
<title>Maximum entropy models for framenet classification.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2065" citStr="Fleischman et al., 2003" startWordPosition="315" endWordPosition="318">ntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with sema</context>
</contexts>
<marker>Fleischman, Kwon, Hovy, 2003</marker>
<rawString>Michael Fleischman, Namhee Kwon, and Eduard Hovy. 2003. Maximum entropy models for framenet classification. In Proceedings of EMNLP-2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Julia Hockenmaier</author>
</authors>
<title>Identifying semantic roles using combinatory categorial grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of EMNLP-2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2022" citStr="Gildea and Hockenmaier, 2003" startWordPosition="308" endWordPosition="312">tic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, the</context>
</contexts>
<marker>Gildea, Hockenmaier, 2003</marker>
<rawString>Daniel Gildea and Julia Hockenmaier. 2003. Identifying semantic roles using combinatory categorial grammar. In Proceedings of EMNLP-2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<issue>3</issue>
<contexts>
<context position="1966" citStr="Gildea and Jurafsky, 2002" startWordPosition="299" endWordPosition="303">Manner, Cause, etc. Most existing systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic </context>
<context position="25655" citStr="Gildea and Jurafsky, 2002" startWordPosition="4285" endWordPosition="4288">perties of the SRL strategies implemented by the ten participant teams (sorted by performance on the test set). “prop.” stands for the treatment of all propositions of a sentence; possible values are: s (separate) and j (joint). “lab.” stands for labeling strategy; possible values are: t (one step tagging), rc (recognition + classification), fl (filtering + labeling), cj (classification + joining). “gran.” stands for granularity; “glob.” stands for global optimization. “post” stands for postprocessing. derived from the basic information are strongly inspired by previous works on the SRL task (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2003a). Many systems used the same kind of ideas but implemented in different ways, since the particular learning strategies used (see section 4.2) impose different constraints on the type of information available or the way of expressing it. As a general idea, we can divide the features into four types: (1) basic features, evaluating some kind of local information on the context of the word or constituent being treated; (2) Features characterizing the internal structure of a candidate argument; (3) Features describing properties of the target verb pred</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Daniel Gildea and Daniel Jurafsky. 2002. Automatic labeling of semantic roles. Computational Linguistics, 28(3):245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
<author>Martha Palmer</author>
</authors>
<title>The necessity of parsing for predicate argument recognition.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL 2002,</booktitle>
<location>Philadelphia, USA.</location>
<contexts>
<context position="1991" citStr="Gildea and Palmer, 2002" startWordPosition="304" endWordPosition="307">isting systems for automatic semantic role labeling make use of a full syntactic parse of the sentence in order to define argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level </context>
</contexts>
<marker>Gildea, Palmer, 2002</marker>
<rawString>Daniel Gildea and Martha Palmer. 2002. The necessity of parsing for predicate argument recognition. In Proceedings ofACL 2002, Philadelphia, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Lluis M`arquez</author>
</authors>
<title>Fast and accurate part-of-speech tagging: The svm approach revisited.</title>
<date>2003</date>
<booktitle>In Proceedings of RANLP-2003, Borovets,</booktitle>
<marker>Gim´enez, M`arquez, 2003</marker>
<rawString>Jes´us Gim´enez and Lluis M`arquez. 2003. Fast and accurate part-of-speech tagging: The svm approach revisited. In Proceedings of RANLP-2003, Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Wayne Ward</author>
</authors>
<title>Target word detection and semantic role chunking using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of HLT-NAACL 2003,</booktitle>
<location>Edmonton, Canada.</location>
<contexts>
<context position="2225" citStr="Hacioglu and Ward, 2003" startWordPosition="341" endWordPosition="344">bels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001). In the CoNLL-2004 shared task we concentr</context>
</contexts>
<marker>Hacioglu, Ward, 2003</marker>
<rawString>Kadri Hacioglu and Wayne Ward. 2003. Target word detection and semantic role chunking using support vector machines. In Proceedings of HLT-NAACL 2003, Edmonton, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kadri Hacioglu</author>
<author>Sameer Pradhan</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role labeling by tagging syntactic chunks.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<contexts>
<context position="16778" citStr="Hacioglu et al. (2004)" startWordPosition="2774" endWordPosition="2777">e development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs. Instead, the main effort of participants went into developing useful SRL str</context>
<context position="21715" citStr="Hacioglu et al. (2004)" startWordPosition="3625" endWordPosition="3628">ormed by means of a IOB or open–close tagging using classifiers, either argument–independent, or specialized by argument type. It is also worth noting that all participant systems performed learning of predicate-independent classifiers instead of specializing by the verb predicate. Information about verb predicates is captured through features and some global restrictions. Another important issue is the granularity at which the sentence elements are processed. It has become very clear that a good election for this problem is phrase-byphrase processing (P-by-P, using the notation introduced by Hacioglu et al. (2004)) instead of word-by-word (Wby-W). The motivation is twofold: (1) phrase boundaries are almost always consistent with argument boundaries; (2) P-by-P processing is computationally less expensive and allows to explore a relatively larger context. Most of the groups performed a P-by-P processing, but admitting a processing by words within the target verb chunks. The system by Baldewein et al. (2004) works with a bit more general elements called “chunk sequences”, extracted in a preprocess using heuristic rules. This information is presented in the third column of Table 3. Information regarding c</context>
<context position="30426" citStr="Hacioglu et al., 2004" startWordPosition="5087" endWordPosition="5090"> and n’t in target verb chunk as AM-NEG. • Tag modal verbs in target verb chunk as AM-MOD. • Tag first NP before target verb as A0. • Tag first NP after target verb as A1. • Tag that, which and who before target verb as R-A0. • Switch A0 and A1, and R-A0 and R-A1 if the target verb is part of a passive VP chunk. A VP chunk is considered in passive voice if it contains a form of to be and the verb does not end in ing. Table 5 presents the overall results obtained by the ten participating systems, on the development and test sets. The best performance was obtained by the SVMbased IOB tagger of (Hacioglu et al., 2004), which almost reached the performance of 70 in F1 on the test. The seven best systems obtained F1 scores in the range of 60-70, and only three systems scored below that. Comparing the results across development and test corpora, most systems experienced a decrease in performance between 1.5 and 3 points. As in previous editions of the shared task, we attribute this behavior to a greater difficulty of the test set instead of an overfitting effect. Interestingly, the three systems performing below 60 in the development set did not experienced this decrease. In fact (Williams et al., 2004) and (</context>
<context position="31777" citStr="Hacioglu et al., 2004" startWordPosition="5319" endWordPosition="5322">ments, on the test set. Consistently, the best performing system of the task also outperforms all other systems on these semantic roles. 5 Conclusion We have described the CoNLL-2004 shared task on semantic role labeling. The task was based on the PropBank corpus, and the challenge was to come up with machine learning techniques to recognize and label semantic roles on the basis of partial syntactic structure. Ten systems have participated to the task, contributing with a variety of standard or novel learning architectures. The best system, presented by the most experienced group on the task (Hacioglu et al., 2004), achieved a moderate performance of 69.49 at the F1 measure. It is based on a SVM tagging system, performing IOB decisions on the chunks of the sentence, and exploiting a wide variety of features based on partial syntax. Most of the systems advance the state-of-the-art on semantic role labeling on the basis of partial syntax. However, state-of-the-art systems working with full syntax still perform substantially better, although far from a desired behavior for real-task application. Two questions remain open: which syntactic structures are needed as input for the task, and what other sources o</context>
</contexts>
<marker>Hacioglu, Pradhan, Ward, Martin, Jurafsky, 2004</marker>
<rawString>Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2004. Semantic role labeling by tagging syntactic chunks. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Derrick Higgins</author>
</authors>
<title>A transformation-based approach to argument labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<contexts>
<context position="16545" citStr="Higgins, 2004" startWordPosition="2741" endWordPosition="2742">ng components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature developme</context>
</contexts>
<marker>Higgins, 2004</marker>
<rawString>Derrick Higgins. 2004. A transformation-based approach to argument labeling. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Beata Kouchnir</author>
</authors>
<title>A memory-based approach for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL2004.</booktitle>
<contexts>
<context position="16668" citStr="Kouchnir, 2004" startWordPosition="2760" endWordPosition="2761">system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different</context>
</contexts>
<marker>Kouchnir, 2004</marker>
<rawString>Beata Kouchnir. 2004. A memory-based approach for semantic role labeling. In Proceedings of CoNLL2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joon-Ho Lim</author>
<author>Young-Sook Hwang</author>
<author>So-Young Park</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Semantic role labeling using maximum entropy model.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL2004.</booktitle>
<contexts>
<context position="16457" citStr="Lim et al., 2004" startWordPosition="2729" endWordPosition="2732">the CoNLL-2004 shared task. They approached the task in several ways, using different learning components and labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the te</context>
<context position="22695" citStr="Lim et al., 2004" startWordPosition="3781" endWordPosition="3784">he system by Baldewein et al. (2004) works with a bit more general elements called “chunk sequences”, extracted in a preprocess using heuristic rules. This information is presented in the third column of Table 3. Information regarding clauses has proven to be very useful, as can be seen in section 4.3. All systems captured some kind of clause information through feature codification. However, some of the systems restrict the search for arguments only to the immediate clause (Park et al., 2004; Williams et al., 2004) and others use the clause hierarchy to guide the exploration of the sentence (Lim et al., 2004; Carreras et al., 2004). Very relevant to the SRL strategy is the availability of global sentential information when decisions are taken. Almost all of the systems try to capture some global level information by collecting features describing the target predicate and its context, the “syntactic path” from the element under consideration to the predicate, etc. (see section 4.3). But only some of them include a global optimization procedure at sentence level in the labeling strategy. The systems working with Maximum Entropy Models (Baldewein et al., 2004; Lim et al., 2004) use beam search to fi</context>
<context position="27730" citStr="Lim et al. (2004)" startWordPosition="4618" endWordPosition="4621">ploited by systems. To represent an argument itself, few attributes are of general usage. Some systems count the length of it, with different granularities. Others make use of heuristics to derive its syntactic type. There are systems that extract a structured representation of the argument, either homogeneous (capturing different sequences of head words, PoS tags, chunks or clauses), or heterogeneous (combining all elements, based on the syntactic hierarchy). A few systems have captured the existence of neighboring arguments, previously identified in the process. Interestingly, the system of Lim et al. (2004) represents the context of an argument relative to the syntactic hierarchy by means of relative constituent sequences and syntactic levels. Concerning lexicalization of the argument, most of the techniques rely on head word rules based on Collins’, or content word rules as in Surdeanu et al. (2003). Only Carreras et al. (2004) decide to use a bag-of-words model, apart from heuristicbased lexicalization. Regarding the target verb, the voice feature of the verb is generally used, in addition to basic features capturing the form and PoS tag of the verb. Some systems captured statistics on frequen</context>
</contexts>
<marker>Lim, Hwang, Park, Rim, 2004</marker>
<rawString>Joon-Ho Lim, Young-Sook Hwang, So-Young Park, and Hae-Chang Rim. 2004. Semantic role labeling using maximum entropy model. In Proceedings of CoNLL2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: the Penn Treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<contexts>
<context position="8158" citStr="Marcus et al., 1993" startWordPosition="1320" endWordPosition="1323">n of the predicate of the proposition. Most of the time, the verb corresponds to the target verb of the proposition, which is provided as input, and only in few cases the verb participant spans more words than the target verb. Except for non-trivial cases, this situation makes the verb fairly easy to identify and, since there is one verb with each proposition, evaluating its recognition overestimates the overall performance of a system. For this reason, the verb argument is excluded from evaluation. 3 Data The data consists of six sections of the Wall Street Journal part of the Penn Treebank (Marcus et al., 1993), and follows the setting of past editions of the CoNLL shared task: training set (sections 15-18), development set (section 20) and test set (section 21). We first describe annotations related to argument structure. Then, we describe the preprocessing of input data. Finally, we describe the format of the data sets. 3.1 PropBank The Proposition Bank (PropBank) (Palmer et al., 2004) annotates the Penn Treebank with verb argument structure. The semantic roles covered by PropBank are the following: • Numbered arguments (A0–A5, AA): Arguments defining verb-specific roles. Their semantics depends o</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: the Penn Treebank. Computational Linguistics, 19.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2004</date>
<journal>Computational Linguistics. Submitted.</journal>
<contexts>
<context position="2745" citStr="Palmer et al., 2004" startWordPosition="425" endWordPosition="428">(Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001). In the CoNLL-2004 shared task we concentrate on the PropBank corpus, which is the Penn Treebank corpus enriched with predicate–argument structures. It addresses predicates expressed by verbs and labels core arguments with consecutive numbers (A0 to A5), trying to maintain coherence along different predicates. A number of adjuncts, derived from the Treebank functional tags, are also included in PropBank annotations. To date, the best results reported on the PropBank correspond to a Fl measure slightly over 83, when using the gold standard parse trees from </context>
<context position="8542" citStr="Palmer et al., 2004" startWordPosition="1382" endWordPosition="1385">ition overestimates the overall performance of a system. For this reason, the verb argument is excluded from evaluation. 3 Data The data consists of six sections of the Wall Street Journal part of the Penn Treebank (Marcus et al., 1993), and follows the setting of past editions of the CoNLL shared task: training set (sections 15-18), development set (section 20) and test set (section 21). We first describe annotations related to argument structure. Then, we describe the preprocessing of input data. Finally, we describe the format of the data sets. 3.1 PropBank The Proposition Bank (PropBank) (Palmer et al., 2004) annotates the Penn Treebank with verb argument structure. The semantic roles covered by PropBank are the following: • Numbered arguments (A0–A5, AA): Arguments defining verb-specific roles. Their semantics depends on the verb and the verb usage in a sentence, or verb sense. In general, A0 stands for the agent 2The srl-eval.pl program is the official program to evaluate the performance of a system. It is available at the Shared Task web page. and A1 corresponds to the patient or theme of the proposition, and these two are the most frequent roles. However, no consistent generalization can be ma</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2004</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2004. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics. Submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kyung-Mi Park</author>
<author>Young-Sook Hwang</author>
<author>Hae-Chang Rim</author>
</authors>
<title>Two-phase semantic role labeling based on support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<contexts>
<context position="16801" citStr="Park et al. (2004)" startWordPosition="2779" endWordPosition="2782">. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs. Instead, the main effort of participants went into developing useful SRL strategies and into the de</context>
<context position="22576" citStr="Park et al., 2004" startWordPosition="3760" endWordPosition="3763">. Most of the groups performed a P-by-P processing, but admitting a processing by words within the target verb chunks. The system by Baldewein et al. (2004) works with a bit more general elements called “chunk sequences”, extracted in a preprocess using heuristic rules. This information is presented in the third column of Table 3. Information regarding clauses has proven to be very useful, as can be seen in section 4.3. All systems captured some kind of clause information through feature codification. However, some of the systems restrict the search for arguments only to the immediate clause (Park et al., 2004; Williams et al., 2004) and others use the clause hierarchy to guide the exploration of the sentence (Lim et al., 2004; Carreras et al., 2004). Very relevant to the SRL strategy is the availability of global sentential information when decisions are taken. Almost all of the systems try to capture some global level information by collecting features describing the target predicate and its context, the “syntactic path” from the element under consideration to the predicate, etc. (see section 4.3). But only some of them include a global optimization procedure at sentence level in the labeling str</context>
<context position="29023" citStr="Park et al. (2004)" startWordPosition="4828" endWordPosition="4831">nts in the proximity of the target verb, inspired by local subcategorization patterns of a predicate. As for features related to a constituent-predicate pair, all systems use the simple feature describing the relative position between them, and to a lesser degree, the distance and the difference in clausal levels. Again, there is a general tendency to describe the structured path from the argument to the verb. Its design goes from simple homogeneous sequences of head words or chunks, to more sophisticated paths combining chunks and clauses, and capturing hierarchical properties. The system of Park et al. (2004) also tracks the number of different syntactic elements found between the pair. Remarkably, the system of Baldewein et al. (2004) uses an EM clustering technique to derive features representing the affinity of an argument and a predicate. On top of basic feature extraction, all teams working with SVM and VP used polynomial kernels of degree 2. Similar in expressiveness, the system designed by Punyakanok et al. (2004) expanded the feature space with all pairs of basic features. 4.4 Evaluation A baseline rate was computed for the task. It was produced by a system developed by Erik Tjong Kim Sang</context>
</contexts>
<marker>Park, Hwang, Rim, 2004</marker>
<rawString>Kyung-Mi Park, Young-Sook Hwang, and Hae-Chang Rim. 2004. Two-phase semantic role labeling based on support vector machines. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Valerie Krugler</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Support vector learning for semantic argument classification.</title>
<date>2003</date>
<tech>Technical Report TR-CSLR-2003-03,</tech>
<institution>Center for Spoken Language Research, University of Colorado.</institution>
<contexts>
<context position="2247" citStr="Pradhan et al., 2003" startWordPosition="345" endWordPosition="348">been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001). In the CoNLL-2004 shared task we concentrate on the PropBank co</context>
<context position="3585" citStr="Pradhan et al., 2003" startWordPosition="559" endWordPosition="562"> by verbs and labels core arguments with consecutive numbers (A0 to A5), trying to maintain coherence along different predicates. A number of adjuncts, derived from the Treebank functional tags, are also included in PropBank annotations. To date, the best results reported on the PropBank correspond to a Fl measure slightly over 83, when using the gold standard parse trees from Penn Treebank as the main source of information (Pradhan et al., 2003b). This performance drops to 77 when a real parser is used instead. Comparatively, the best SRL system based solely on shallow syntactic information (Pradhan et al., 2003a) performs more than 15 points below. Although these results are not directly comparable to the ones obtained in the CoNLL-2004 shared task (different datasets, different version of PropBank, etc.) they give an idea about the state-of-the art results on the task. The challenge for CoNLL-2004 shared task is to come up with machine learning strategies which address the SRL problem on the basis of only partial syntactic information, avoiding the use of full parsers and external lexico-semantic knowledge bases. The annotations provided for the development of systems include, apart from the argume</context>
<context position="25700" citStr="Pradhan et al., 2003" startWordPosition="4293" endWordPosition="4296">en participant teams (sorted by performance on the test set). “prop.” stands for the treatment of all propositions of a sentence; possible values are: s (separate) and j (joint). “lab.” stands for labeling strategy; possible values are: t (one step tagging), rc (recognition + classification), fl (filtering + labeling), cj (classification + joining). “gran.” stands for granularity; “glob.” stands for global optimization. “post” stands for postprocessing. derived from the basic information are strongly inspired by previous works on the SRL task (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2003a). Many systems used the same kind of ideas but implemented in different ways, since the particular learning strategies used (see section 4.2) impose different constraints on the type of information available or the way of expressing it. As a general idea, we can divide the features into four types: (1) basic features, evaluating some kind of local information on the context of the word or constituent being treated; (2) Features characterizing the internal structure of a candidate argument; (3) Features describing properties of the target verb predicate; (4) Features that capture the relation</context>
</contexts>
<marker>Pradhan, Hacioglu, Krugler, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2003a. Support vector learning for semantic argument classification. Technical Report TR-CSLR-2003-03, Center for Spoken Language Research, University of Colorado.</rawString>
</citation>
<citation valid="false">
<title>CoNLL-2004 shared task on the test set. Systems sorted by overall performance on the test set.</title>
<marker></marker>
<rawString>CoNLL-2004 shared task on the test set. Systems sorted by overall performance on the test set.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer Pradhan</author>
<author>Kadri Hacioglu</author>
<author>Wayne Ward</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Semantic role parsing: Adding semantic structure to unstructured text.</title>
<date>2003</date>
<booktitle>In Proceedings of the International Conference on Data Mining (ICDM-2003),</booktitle>
<location>Melbourne, USA.</location>
<contexts>
<context position="2247" citStr="Pradhan et al., 2003" startWordPosition="345" endWordPosition="348">been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) and FrameNet (Fillmore et al., 2001). In the CoNLL-2004 shared task we concentrate on the PropBank co</context>
<context position="3585" citStr="Pradhan et al., 2003" startWordPosition="559" endWordPosition="562"> by verbs and labels core arguments with consecutive numbers (A0 to A5), trying to maintain coherence along different predicates. A number of adjuncts, derived from the Treebank functional tags, are also included in PropBank annotations. To date, the best results reported on the PropBank correspond to a Fl measure slightly over 83, when using the gold standard parse trees from Penn Treebank as the main source of information (Pradhan et al., 2003b). This performance drops to 77 when a real parser is used instead. Comparatively, the best SRL system based solely on shallow syntactic information (Pradhan et al., 2003a) performs more than 15 points below. Although these results are not directly comparable to the ones obtained in the CoNLL-2004 shared task (different datasets, different version of PropBank, etc.) they give an idea about the state-of-the art results on the task. The challenge for CoNLL-2004 shared task is to come up with machine learning strategies which address the SRL problem on the basis of only partial syntactic information, avoiding the use of full parsers and external lexico-semantic knowledge bases. The annotations provided for the development of systems include, apart from the argume</context>
<context position="25700" citStr="Pradhan et al., 2003" startWordPosition="4293" endWordPosition="4296">en participant teams (sorted by performance on the test set). “prop.” stands for the treatment of all propositions of a sentence; possible values are: s (separate) and j (joint). “lab.” stands for labeling strategy; possible values are: t (one step tagging), rc (recognition + classification), fl (filtering + labeling), cj (classification + joining). “gran.” stands for granularity; “glob.” stands for global optimization. “post” stands for postprocessing. derived from the basic information are strongly inspired by previous works on the SRL task (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2003a). Many systems used the same kind of ideas but implemented in different ways, since the particular learning strategies used (see section 4.2) impose different constraints on the type of information available or the way of expressing it. As a general idea, we can divide the features into four types: (1) basic features, evaluating some kind of local information on the context of the word or constituent being treated; (2) Features characterizing the internal structure of a candidate argument; (3) Features describing properties of the target verb predicate; (4) Features that capture the relation</context>
</contexts>
<marker>Pradhan, Hacioglu, Ward, Martin, Jurafsky, 2003</marker>
<rawString>Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Martin, and Daniel Jurafsky. 2003b. Semantic role parsing: Adding semantic structure to unstructured text. In Proceedings of the International Conference on Data Mining (ICDM-2003), Melbourne, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Wen-Tau Yih</author>
<author>Dav Zimak</author>
<author>Yuancheng Tu</author>
</authors>
<title>Semantic role labeling via generalized inference over classifiers.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<contexts>
<context position="16980" citStr="Punyakanok et al. (2004)" startWordPosition="2807" endWordPosition="2810"> teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As a main difference with respect to past editions, less effort has been put into combining different learning algorithms and outputs. Instead, the main effort of participants went into developing useful SRL strategies and into the development of features (see sections 4.2 and 4.3). As an exception, van den Bosch et al. (2004) applied a 3Arguments in data do not embed, though format allows so. The DT B-NP (S* </context>
<context position="23412" citStr="Punyakanok et al. (2004)" startWordPosition="3895" endWordPosition="3898">tential information when decisions are taken. Almost all of the systems try to capture some global level information by collecting features describing the target predicate and its context, the “syntactic path” from the element under consideration to the predicate, etc. (see section 4.3). But only some of them include a global optimization procedure at sentence level in the labeling strategy. The systems working with Maximum Entropy Models (Baldewein et al., 2004; Lim et al., 2004) use beam search to find taggings that maximize the probability of the output sequence. Carreras et al. (2004) and Punyakanok et al. (2004) also define a global scoring function to maximize. At this point, the system of Punyakanok et al. (2004) deserves special consideration, since it formally implements a set of structural and linguistic constraints directly in the global cost function to maximize. These constraints act as a filter for valid output sequences and ensure coherence of the output. Authors refer to this part of the system as the inference layer and they implement it using integer linear programming. The iterative classifier stacking mechanism used by van den Bosch et al. (2004) also tries to alleviate the problem of </context>
<context position="29443" citStr="Punyakanok et al. (2004)" startWordPosition="4898" endWordPosition="4901">Its design goes from simple homogeneous sequences of head words or chunks, to more sophisticated paths combining chunks and clauses, and capturing hierarchical properties. The system of Park et al. (2004) also tracks the number of different syntactic elements found between the pair. Remarkably, the system of Baldewein et al. (2004) uses an EM clustering technique to derive features representing the affinity of an argument and a predicate. On top of basic feature extraction, all teams working with SVM and VP used polynomial kernels of degree 2. Similar in expressiveness, the system designed by Punyakanok et al. (2004) expanded the feature space with all pairs of basic features. 4.4 Evaluation A baseline rate was computed for the task. It was produced by a system developed by Erik Tjong Kim Sang, from the University of Antwerp, Belgium. The baseline processor finds semantic roles based on the following seven rules: • Tag target verb and successive particles as V. • Tag not and n’t in target verb chunk as AM-NEG. • Tag modal verbs in target verb chunk as AM-MOD. • Tag first NP before target verb as A0. • Tag first NP after target verb as A1. • Tag that, which and who before target verb as R-A0. • Switch A0 a</context>
</contexts>
<marker>Punyakanok, Roth, Yih, Zimak, Tu, 2004</marker>
<rawString>Vasin Punyakanok, Dan Roth, Wen-Tau Yih, Dav Zimak, and Yuancheng Tu. 2004. Semantic role labeling via generalized inference over classifiers. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mihai Surdeanu</author>
<author>Sanda Harabagiu</author>
<author>John Williams</author>
<author>Paul Aarseth</author>
</authors>
<title>Using predicate-argument structures for information extraction.</title>
<date>2003</date>
<booktitle>In Proceedings of ACL 2003,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="2147" citStr="Surdeanu et al., 2003" startWordPosition="328" endWordPosition="331">elevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL systems: PropBank (Palmer et al., 2004) a</context>
<context position="25678" citStr="Surdeanu et al., 2003" startWordPosition="4289" endWordPosition="4292">es implemented by the ten participant teams (sorted by performance on the test set). “prop.” stands for the treatment of all propositions of a sentence; possible values are: s (separate) and j (joint). “lab.” stands for labeling strategy; possible values are: t (one step tagging), rc (recognition + classification), fl (filtering + labeling), cj (classification + joining). “gran.” stands for granularity; “glob.” stands for global optimization. “post” stands for postprocessing. derived from the basic information are strongly inspired by previous works on the SRL task (Gildea and Jurafsky, 2002; Surdeanu et al., 2003; Pradhan et al., 2003a). Many systems used the same kind of ideas but implemented in different ways, since the particular learning strategies used (see section 4.2) impose different constraints on the type of information available or the way of expressing it. As a general idea, we can divide the features into four types: (1) basic features, evaluating some kind of local information on the context of the word or constituent being treated; (2) Features characterizing the internal structure of a candidate argument; (3) Features describing properties of the target verb predicate; (4) Features tha</context>
<context position="28029" citStr="Surdeanu et al. (2003)" startWordPosition="4667" endWordPosition="4670">ither homogeneous (capturing different sequences of head words, PoS tags, chunks or clauses), or heterogeneous (combining all elements, based on the syntactic hierarchy). A few systems have captured the existence of neighboring arguments, previously identified in the process. Interestingly, the system of Lim et al. (2004) represents the context of an argument relative to the syntactic hierarchy by means of relative constituent sequences and syntactic levels. Concerning lexicalization of the argument, most of the techniques rely on head word rules based on Collins’, or content word rules as in Surdeanu et al. (2003). Only Carreras et al. (2004) decide to use a bag-of-words model, apart from heuristicbased lexicalization. Regarding the target verb, the voice feature of the verb is generally used, in addition to basic features capturing the form and PoS tag of the verb. Some systems captured statistics on frequent argument patterns for each predicate. Also, systems represented the elements in the proximity of the target verb, inspired by local subcategorization patterns of a predicate. As for features related to a constituent-predicate pair, all systems use the simple feature describing the relative positi</context>
</contexts>
<marker>Surdeanu, Harabagiu, Williams, Aarseth, 2003</marker>
<rawString>Mihai Surdeanu, Sanda Harabagiu, John Williams, and Paul Aarseth. 2003. Using predicate-argument structures for information extraction. In Proceedings of ACL 2003, Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia A Thompson</author>
<author>Roger Levy</author>
<author>Christopher D Manning</author>
</authors>
<title>A generative model for semantic role labeling.</title>
<date>2003</date>
<booktitle>In Proceedings ofECML’03,</booktitle>
<location>Dubrovnik, Croatia.</location>
<contexts>
<context position="2108" citStr="Thompson et al., 2003" startWordPosition="321" endWordPosition="324">ine argument boundaries and to extract relevant information for training classifiers to disambiguate between role labels. Thus, the task has been usually approached as a two phase procedure consisting of recognition and labeling of arguments. &apos;CoNLL-2004 Shared Task web page —with data, software and systems’ outputs available— at http://cnts.uia.ac.be/conll2004/roles. Regarding the learning component of the systems, we find pure probabilistic models (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Gildea and Hockenmaier, 2003), Maximum Entropy (Fleischman et al., 2003), generative models (Thompson et al., 2003), Decision Trees (Surdeanu et al., 2003; Chen and Rambow, 2003), and Support Vector Machines (Hacioglu and Ward, 2003; Pradhan et al., 2003a; Pradhan et al., 2003b). There have also been some attempts at relaxing the necessity of using syntactic information derived from full parse trees. For instance, in (Pradhan et al., 2003a; Hacioglu and Ward, 2003), a SVM-based SRL system is devised which performs an IOB sequence tagging using only shallow syntactic information at the level of phrase chunks. Nowadays, there exist two main English corpora with semantic annotations from which to train SRL sy</context>
</contexts>
<marker>Thompson, Levy, Manning, 2003</marker>
<rawString>Cynthia A. Thompson, Roger Levy, and Christopher D. Manning. 2003. A generative model for semantic role labeling. In Proceedings ofECML’03, Dubrovnik, Croatia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F Tjong Kim Sang</author>
<author>S Buchholz</author>
</authors>
<title>Introduction to the CoNLL-2000 shared task: Chunking.</title>
<date>2000</date>
<booktitle>In Proceedings of the 4th Conference on Natural Language Learning, CoNLL-2000.</booktitle>
<contexts>
<context position="12449" citStr="Sang and Buchholz, 2000" startWordPosition="2055" endWordPosition="2058">0 228 AM-MNR 1,337 334 255 AM-MOD 1,753 389 337 AM-NEG 687 131 127 AM-PNC 446 100 85 AM-PRD 10 3 3 AM-REC 2 1 0 AM-TMP 3,567 759 747 R-A0 738 162 159 R-A1 360 74 70 R-A2 49 17 9 R-A3 8 0 1 R-AA 1 0 0 R-AM-ADV 1 0 0 R-AM-LOC 27 4 4 R-AM-MNR 4 0 1 R-AM-PNC 1 0 1 R-AM-TMP 35 6 14 Table 1: Counts on the three data sets. • PoS tagger: (Gim´enez and M`arquez, 2003), based on Support Vector Machines, and trained on Penn Treebank sections 0–18. • Chunker and Clause Recognizer: (Carreras and M`arquez, 2003), based on Voted Perceptrons, and following the CoNLL settings of 2000 and 2001 tasks (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001). These two processors form a coherent partial syntax of a sentence, that is, chunks and clauses form a tree. • Named entities with (Chieu and Ng, 2003), based on Maximum-Entropy classifiers, and following the CoNLL-2003 task setting (Tjong Kim Sang and De Meulder, 2003). Precision Recall Fl/Acc. PoS Dev. (acc.) – – 96.88 PoS Test (acc.) – – 96.70 Chunking Dev. 94.28% 93.65% 93.96 Chunking Test 93.80% 92.93% 93.36 Clauses Dev. 90.51% 86.12% 88.26 Clauses Test 88.73% 82.92% 85.73 Named Entities 88.12% 88.51% 88.31 Table 2: Results of the preprocessing modules </context>
</contexts>
<marker>Sang, Buchholz, 2000</marker>
<rawString>E. F. Tjong Kim Sang and S. Buchholz. 2000. Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of the 4th Conference on Natural Language Learning, CoNLL-2000.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Erik</author>
</authors>
<title>Tjong Kim Sang and Fien De Meulder.</title>
<date>2003</date>
<booktitle>In Proceedings of CoNLL-2003.</booktitle>
<marker>Erik, 2003</marker>
<rawString>Erik F. Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the CoNLL-2003 shared task: Languageindependent named entity recognition. In Proceedings of CoNLL-2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong Kim Sang</author>
<author>Herv´e D´ejean</author>
</authors>
<title>Introduction to the CoNLL-2001 shared task: Clause identification.</title>
<date>2001</date>
<booktitle>In Proceedings of the 5th Conference on Natural Language Learning, CoNLL-2001.</booktitle>
<marker>Sang, D´ejean, 2001</marker>
<rawString>Erik F. Tjong Kim Sang and Herv´e D´ejean. 2001. Introduction to the CoNLL-2001 shared task: Clause identification. In Proceedings of the 5th Conference on Natural Language Learning, CoNLL-2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antal van den Bosch</author>
</authors>
<title>Sander Canisius, Walter Daelemans, Iris Hendrickx, and Erik Tjong Kim Sang.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<marker>van den Bosch, 2004</marker>
<rawString>Antal van den Bosch, Sander Canisius, Walter Daelemans, Iris Hendrickx, and Erik Tjong Kim Sang. 2004. Memory-based semantic role labeling: Optimizing features, algorithm, and output. In Proceedings of CoNLL-2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Williams</author>
<author>Christopher Dozier</author>
<author>Andrew McCulloh</author>
</authors>
<title>Learning transformation rules for semantic role labeling.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<contexts>
<context position="16569" citStr="Williams et al., 2004" startWordPosition="2743" endWordPosition="2746">nd labeling strategies. The following subsections briefly summarize the most important properties of each system and provide a qualitative comparison between them, together with a quantitative evaluation on the development and test sets. 4.1 Learning techniques Up to six different learning algorithms have been applied in the CoNLL-2004 shared task. None of them is new with respect to the past editions. Two teams used the Maximum Entropy (ME) statistical framework (Baldewein et al., 2004; Lim et al., 2004). Two teams used Brill’s Transformation-based Error-driven Learning (TBL) (Higgins, 2004; Williams et al., 2004). Two other groups applied Memory-Based Learning (MBL) (van den Bosch et al., 2004; Kouchnir, 2004). The remaining four teams employed vector-based linear classifiers of different types: Hacioglu et al. (2004) and Park et al. (2004) used Support Vector Machines (SVM) with polynomial kernels, Carreras et al. (2004) used Voted Perceptrons (VP) also with polynomial kernels, and finally, Punyakanok et al. (2004) used SNoW, a Winnow-based network of linear separators. Additionally, the team of Baldewein et al. (2004) used a EM–based clustering algorithm for feature development (see section 4.3). As</context>
<context position="22600" citStr="Williams et al., 2004" startWordPosition="3764" endWordPosition="3767">s performed a P-by-P processing, but admitting a processing by words within the target verb chunks. The system by Baldewein et al. (2004) works with a bit more general elements called “chunk sequences”, extracted in a preprocess using heuristic rules. This information is presented in the third column of Table 3. Information regarding clauses has proven to be very useful, as can be seen in section 4.3. All systems captured some kind of clause information through feature codification. However, some of the systems restrict the search for arguments only to the immediate clause (Park et al., 2004; Williams et al., 2004) and others use the clause hierarchy to guide the exploration of the sentence (Lim et al., 2004; Carreras et al., 2004). Very relevant to the SRL strategy is the availability of global sentential information when decisions are taken. Almost all of the systems try to capture some global level information by collecting features describing the target predicate and its context, the “syntactic path” from the element under consideration to the predicate, etc. (see section 4.3). But only some of them include a global optimization procedure at sentence level in the labeling strategy. The systems worki</context>
<context position="26929" citStr="Williams et al. (2004)" startWordPosition="4492" endWordPosition="4495">en the verb predicate and the constituent under consideration. All systems used some kind of basic features. Roughly speaking, they consist of words, PoS tags, chunks, clause labels, and named entities extracted from a windowbased context. These values can be considered with or without the relative position with respect to the element under consideration, and some n-grams of them can also be computed. If the granularity of the system is at phrase level then typically a representative head word of the phrase is used as lexical information. As an exception to the general approach, the system of Williams et al. (2004) does not make use of word forms. The rest of the features are more interesting since they are task dependent, and deserve special attention. Table 4 summarizes the type of features exploited by systems. To represent an argument itself, few attributes are of general usage. Some systems count the length of it, with different granularities. Others make use of heuristics to derive its syntactic type. There are systems that extract a structured representation of the argument, either homogeneous (capturing different sequences of head words, PoS tags, chunks or clauses), or heterogeneous (combining </context>
<context position="31020" citStr="Williams et al., 2004" startWordPosition="5188" endWordPosition="5191">r of (Hacioglu et al., 2004), which almost reached the performance of 70 in F1 on the test. The seven best systems obtained F1 scores in the range of 60-70, and only three systems scored below that. Comparing the results across development and test corpora, most systems experienced a decrease in performance between 1.5 and 3 points. As in previous editions of the shared task, we attribute this behavior to a greater difficulty of the test set instead of an overfitting effect. Interestingly, the three systems performing below 60 in the development set did not experienced this decrease. In fact (Williams et al., 2004) and (Baldewein et al., 2004) even improved the results on the test set. Table 6 details the performance of systems for the A0- A4 arguments, on the test set. Consistently, the best performing system of the task also outperforms all other systems on these semantic roles. 5 Conclusion We have described the CoNLL-2004 shared task on semantic role labeling. The task was based on the PropBank corpus, and the challenge was to come up with machine learning techniques to recognize and label semantic roles on the basis of partial syntactic structure. Ten systems have participated to the task, contribu</context>
</contexts>
<marker>Williams, Dozier, McCulloh, 2004</marker>
<rawString>Ken Williams, Christopher Dozier, and Andrew McCulloh. 2004. Learning transformation rules for semantic role labeling. In Proceedings of CoNLL-2004.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>