<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000010">
<title confidence="0.9710385">
Exploring Word Order Universals: a Probabilistic Graphical Model
Approach
</title>
<author confidence="0.998356">
Xia Lu
</author>
<affiliation confidence="0.8155655">
Department of Linguistics
University at Buffalo
</affiliation>
<address confidence="0.921537">
Buffalo, NY USA
</address>
<email confidence="0.99968">
xialu@buffalo.edu
</email>
<sectionHeader confidence="0.994565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99348472">
In this paper we propose a probabilistic graph-
ical model as an innovative framework for
studying typological universals. We view lan-
guage as a system and linguistic features as its
components whose relationships are encoded
in a Directed Acyclic Graph (DAG). Taking
discovery of the word order universals as a
knowledge discovery task we learn the graph-
ical representation of a word order sub-system
which reveals a finer structure such as direct
and indirect dependencies among word order
features. Then probabilistic inference enables
us to see the strength of such relationships:
given the observed value of one feature (or
combination of features), the probabilities of
values of other features can be calculated. Our
model is not restricted to using only two val-
ues of a feature. Using imputation technique
and EM algorithm it can handle missing val-
ues well. Model averaging technique solves
the problem of limited data. In addition the in-
cremental and divide-and-conquer method ad-
dresses the areal and genetic effects simulta-
neously instead of separately as in Daumé III
and Campbell (2007).
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992001724138">
Ever since Greenberg (1963) proposed 45 uni-
versals of language based on a sample of 30 lan-
guages, typologists have been pursuing this topic
actively for the past half century. Since some of
them do not agree with the term (or concept) of
“universal” they use other terminology such as
“correlation”, “co-occurrence”, “dependency”,
“interaction” and “implication” to refer to the
relationships between/among linguistic feature
pairs most of which concern morpheme and
word order. Indeed the definition of “universals”
has never been clear until recently, when most
typologists agreed that such universals should be
statistical universals which are “statistical
tendencies” discovered from data samples by
using statistical methods as used in any other
science. Only those tendencies that can be ex-
trapolated to make general conclusions about the
population can be claimed to be “universals”
since they reflect the global preferences of value
distribution of linguistic features across genea-
logical hierarchy and geographical areas.
Previous statistical methods in the research of
word order universals have yielded interesting
results but they have to make strong assumptions
and do a considerable amount of data prepro-
cessing to make the data fit the statistical model
(Greenberg, 1963; Hawkins, 1982; Dryer, 1989;
Nichols, 1986; Justeson &amp; Stephens, 1990). Re-
cent studies using probabilistic models are much
more flexible and can handle noise and uncer-
tainty better (Daumé III &amp; Campbell, 2007; Dunn
et al., 2011). However these models still rely on
strong theoretic assumptions and heavy data
treatment, such as using only two values of word
order pairs while discarding other values, pur-
posefully selecting a subset of the languages to
study, or selecting partial data with complete
values. In this paper we introduce a novel ap-
proach of using a probabilistic graphical model
to study word order universals. Using this model
we can have a graphic representation of the
structure of language as a complex system com-
posed of linguistic features. Then the relationship
among these features can be quantified as proba-
bilities. Such a model does not rely on strong
assumptions and has little constraint on data.
The paper is organized as follows: in Section 2
we discuss the rationale of using a probabilistic
graphic model to study word order universals
and introduce our two models; Section 3 is about
learning structures and parameters for the two
models. Section 4 discusses the quantitative
analysis while Section 5 gives qualitative analy-
sis of the results. Section 6 is about inference
such as MAP query and in Section 6 we discuss
the advantage of using PGM to study word order
universals.
</bodyText>
<page confidence="0.976676">
150
</page>
<note confidence="0.686127">
Proceedings of the ACL Student Research Workshop, pages 150–157,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.9854161">
2 A new approach: probabilistic graph-
ical modeling
2.1 Rationale for using PGM in word order
study
The probabilistic graphical model is the marriage
of probabilistic theory and graph theory. It com-
bines a graphical representation with a complex
distribution over a high-dimensional space.
There are two major types of graphical represen-
tations of distributions. One is a Directed Acyclic
Graph (DAG) which is also known as a Bayesian
network with all edges having a source and a
target. The other is an Undirected Acyclic Graph,
which is also called a Markov network with all
edges undirected. A mixture of these two types is
also possible (Koller &amp; Friedman, 2009).
There are two advantages of using this model to
study word order universals. First the graphical
structure can reveal much finer structure of lan-
guage as a complex system. Most studies on
word order correlations examine the pairwise
relationship, for example, how the order of verb
and object correlates with the order of noun and
adjective. However linguists have also noticed
other possible interactions among the word order
features, like chains of overlapping implications:
Prep ـ ((NAdj ـ NGen) &amp; (NGen ـ NRel))
proposed by Hawkins (1983); multi-conditional
implications (Daumé III, 2007); correlations
among six word order pairs and three-way inter-
actions (Justeson &amp; Stephens, 1990); spurious
word order correlations (Croft et al., 2011);
chains of associations, e.g. if C predicts B and B
predicts A, then C predicts A redundantly (Bick-
el, 2010b). These claims about the possible inter-
actions among word order features imply com-
plex relationships among the features. The study
of word order correlations started with pairwise
comparison, probably because that was what ty-
pologists could do given the limited resources of
statistical methods. However when we study the
properties of a language, by knowing just several
word orders such as order of verb and object,
noun and adpositions, etc., we are unable to say
anything about the language as a whole. Here we
want to introduce a new perspective of seeing
language as a complex system. We assume there
is a meta-language that has the universal proper-
ties of all languages in the world. We want a
model that can represent this meta-language and
make inferences about linguistic properties of
new languages. This system is composed of mul-
tiple sub-systems such as phonology, morpholo-
gy, syntax, etc. which correspond to the subfields
in linguistics. In this paper we focus on the sub-
system of word order only.
The other advantage of PGM is that it enables
us to quantify the relationships among word or-
der features. Justeson &amp; Stephens (1990) men-
tioned the notion of “correlation strength” when
they found out that N/A order appears less
strongly related to basic V/S/O order and/or
adposition type than is N/G order. This is the
best a log-linear model can do, to indicate
whether a correlation is “strong”, “less strong”,
“weak” or “less weak”. Dunn et al. (2011) used
Bayes factor value to quantify the relationships
between the word order pairs but they mistook
the strength of evidence for an effect as the
strength of the effect itself (Levy &amp; Daumé III,
2011). A PGM model for a word order subsys-
tem encodes a joint probabilistic distribution of
all word order feature pairs. Using probability we
can describe the degree of confidence about the
uncertain nature of word order correlations. For
example, if we set the specific value as evidence,
then we can get the values of other features using
an inference method. Such values can be seen as
quantified strength of relationship between val-
ues of features.
</bodyText>
<subsectionHeader confidence="0.99477">
2.2 Our model
</subsectionHeader>
<bodyText confidence="0.999985851851852">
In our word order universal modeling we will use
DAG structure since we think the direction of
influence matters when talking about the rela-
tionship among features. In Greenberg (1966a)
most of the universals are unidirectional, such as
“If a language has object-verb order, then it also
has subject-verb order” while few are bidirec-
tional universals. The term “directionality” does
not capture the full nature of the different status-
es word order features have in the complex lan-
guage system. We notice in all the word order
studies the order of SOV or OV was given spe-
cial attention. In Dryer’s study VO order is the
dominant one which determines the set of word
order pairs correlated with it (or not). We assume
word order features have different statuses in the
language system and such differences should be
manifested by directionality of relationships be-
tween feature pairs. Therefore we choose DAG
structure as our current model framework.
Another issue is the sampling problem. Some
typologists (Dryer 1989, Croft 2003) have ar-
gued that the language samples in the WALS
database (Haspelmath et al., 2005) are not inde-
pendent and identically distributed (i.i.d.) be-
cause languages can share the same feature val-
ues due to either genetic or areal effect. While
</bodyText>
<page confidence="0.992185">
151
</page>
<bodyText confidence="0.99995345">
others (Maslova, 2010) argue that languages
within a family have developed into distinct ones
through the long history. We notice that even we
can control the areal and genetic factors there are
still many other factors that can influence the
typological data distribution, such as 1) language
speakers: cognitive, physiological, social, and
communicative factors; 2) data collection: diffi-
culty in identifying features; political biases
(some languages are well documented); 3) ran-
dom noise such as historical accidents. Here we
do not make any assumption about the i.i.d prop-
erty of the language samples and propose two
models: one is FLAT, which assumes samples
are independent and identically distributed (i.i.d.);
the other is UNIV, which takes care of the possi-
ble dependencies among the samples. By com-
paring the predictive power of these two models
we hope to find one that is closer to the real dis-
tribution.
</bodyText>
<sectionHeader confidence="0.993521" genericHeader="introduction">
3 Learning
</sectionHeader>
<bodyText confidence="0.9999095">
To build our models we need to learn both struc-
ture and parameters for the two models. We used
Murphy (2001)’s Bayesian Network Toolbox
(BNT) and Leray &amp; Francois (2004)’s BNT
Structure Learning Package (BNT_SLP) for this
purpose.
</bodyText>
<subsectionHeader confidence="0.996175">
3.1 Data
</subsectionHeader>
<bodyText confidence="0.99984225">
As we mentioned earlier we will restrict our at-
tention to the domain of word order only in this
paper. In the WALS database there are 56 fea-
tures belonging to the “Word Order” category.
Because some of the features are redundant, we
chose 15 sets of word order features which are:
S_O_V1 (order of subject, object and verb) [72],
S_V (order of subject and verb) [3], O_V (order
of object and verb) [3], O_Obl_V (order of Ob-
ject, Oblique, and Verb) [6], ADP_NP (order of
adposition and noun phrase) [5], G_N (order of
genitive and noun) [3], A_N (order of adjective
and noun) [4], Dem_N (order of demonstrative
and noun) [4], Num_N (order of numeral and
noun) [4], R_N (order of relative clause and
noun) [7], Deg_A (order of degree word and ad-
jective) [3], PoQPar (position of polar question
particles) [6], IntPhr (position of interrogative
phrases in content questions) [3], AdSub_Cl (or-
der of adverbial subordinator and clause) [5],
</bodyText>
<footnote confidence="0.7918935">
1 The detailed descriptions of these word order features and
values can be found at http://wals.info/.
2 The number in the square brackets indicates the number of
values for that feature.
</footnote>
<bodyText confidence="0.999460166666667">
Neg_V (order of negative morpheme and verb)
[4]. We did some minimal treatment of data. For
Neg_V which has 17 values we collapsed its val-
ues 7-17 to 6 (“Mixed”). For Dem_N and Neg_V,
we treat word and suffix as the same and col-
lapsed values 1 and 3 to 1, and values 2 and 4 to
2. After deleting those languages with no value
for all 15 word order features we have 1646 data
entries. This database is very sparse: in overall
the percentage of missing values is 31%. For
seven features more than 50% of the languages
have values missing.
</bodyText>
<subsectionHeader confidence="0.999873">
3.2 Learning the FLAT model
</subsectionHeader>
<bodyText confidence="0.999931176470588">
There are two big problems in learning DAG
structure for the FLAT model. One is caused by
large number of missing values. Because EM
method for structures from incomplete data takes
very long time to converge due to the large pa-
rameter space of our model, we decided to use
imputation method to handle the missing data
problem (Singh, 1997). The other difficulty is
caused by limited data. To solve this problem we
used model averaging by using bootstrap repli-
cates (Friedman et al., 1999). We use GES
(greedy search in the space of equivalent classes)
algorithm in BNT_SLP to learn structure from a
bootstrap dataset because it uses CPDAGs to
represent Markov equivalent classes which
makes graph fusion easier. The algorithm is as
follows:
</bodyText>
<listItem confidence="0.9512642">
1) Use nearest-neighbor method to impute missing
values in the original dataset D and create a com-
plete dataset DS.
2) Create T=200 bootstrap resamples by resampling
the same number of instances as the original da-
taset with replacement from DS . Then for each
resample DS� learn the highest scoring structure GS�.
3) Fuse the 200 graphs into a single graph GS using
the “Intergroup Undirected Networks Integration”
method (Liu et al., 2007). Then use
cpdag_to_dag.m in BNT_SLP to change GS into a
directed graph GS&apos;.
4) Compute the BIC scores of GS&apos; using the 200
resamples and choose the highest one. If the con-
vergence criterion (change of BIC is less than
10-4 compared with the previous iteration) is met,
stop. Otherwise go to Step 5.
5) Learn 200 sets of parameters BS� for GS� using the
200 resamples and take a weighted-average as the
final parameters BS&apos;. Also use EM algorithm and
dataset D to learn parameters BEM for GS&apos;. Choose
the parameters B between BS&apos; and BEM that gives
the highest BIC score. Use MAP estimation to fill
in the missing values in D and generate a complete
dataset DS,,. Go to Step 2.
</listItem>
<page confidence="0.992481">
152
</page>
<figureCaption confidence="0.961265333333333">
The structure for the FLAT model is shown in
Figure 1.
Figure 1. DAG structure of the FLAT model
</figureCaption>
<subsectionHeader confidence="0.996619">
3.3 Learning the UNIV model
</subsectionHeader>
<bodyText confidence="0.94895059375">
As discussed in Section 2.2, the possible depend-
encies among language samples pose difficulty
for statistical methods using the WALS data.
Daumé III &amp; Campbell (2007)’s hierarchical
models provided a good solution to this problem;
however their two models LINGHIER and DIS-
THIER dealt with genetic and areal influences
separately and the two separate results still do
not tell us what the “true universals” are.
Instead of trying to control the areal and genet-
ic and other factors, we propose a different per-
spective here. As we have mentioned, the kind of
universals we care about are the stable properties
of language, which means they can be found
across all subsets of languages. Therefore to
solve the problem of dependence among the lan-
guages we take an incremental and divide-and-
conquer approach. Using clustering algorithm we
identified five clusters in the WALS data. In
each cluster we picked 1/n of the data and com-
bine them to make a subset. In this way we can
have n subsets of data which have decreased de-
gree of dependencies among the samples. We
learn a structure for each subset and fuse the n
graphs into one single graph. The algorithm is as
follows:
1) Use nearest-neighbor method to impute missing
values and create M complete datasets ܦ௠ (1 ൑
݉ ൑ ܯ).
2) For each ܦ௠ divide the samples into n subsets.
Then for each subset ܦ௠௡ learn the highest scoring
structure ܩ௠௡.
</bodyText>
<listItem confidence="0.985239846153846">
3) Fuse the n graphs into a single graph ܩ௠ using the
“Intragroup Undirected Networks Integration”
method (Liu et al., 2007).
4) Fuse the M graphs to make a single directed graph
ܩெԢ as in Step 3 in the previous section.
5) Compute the BIC score of ܩெԢ using datasets ܦ௠
(1 ൑ ݉ ൑ ܯ) and choose the highest score. If the
convergence criterion (same as in the previous sec-
tion) is met, stop. Otherwise go to Step 6.
6) Learn parameters ߠ௠ for ܩெԢ using datasets ܦ௠
(1 ൑ ݉ ൑ ܯ) and take a weighted-average as the
final parameters ߠܯԢ. Also use EM algorithm and
original dataset to learn parameters ߠாெ for ܩெԢ.
</listItem>
<bodyText confidence="0.857260666666667">
Choose the parameters ߠ among ߠெԢ and ߠாெ that
gives the highest BIC score. Use MAP estimation
to fill in the missing values in D and generate an-
other M complete dataset. Go to Step 2.
The final structure for the UNIV model is
shown in Figure 2.
</bodyText>
<figureCaption confidence="0.999735">
Figure 2. DAG structure of the UNIV model
</figureCaption>
<bodyText confidence="0.999977607142857">
The semantics of a DAG structure cannot be
simply interpreted as causality (Koller &amp; Fried-
man, 2009). From this graph we can see word
order features are on different tiers in the hierar-
chy. The root S_O_V seems to “dominate” all
the other features; noun modifiers and noun are
in the middle tier while O_Obl_V, AdSub_Cl,
Deg_A, Num_N, R, Neg_V and PoQPar are the
leaf nodes which might indicate their smallest
contribution to the word order properties of a
language. O_V seems to be an important node
since most paths start from it indicating its influ-
ence can flow to many other nodes.
We can also see there are two types of connec-
tions among the nodes: 1) direct connection: any
two nodes connected with an arc directly have
influence on each other. This construction induc-
es a correlation between the two features regard-
less of the evidence. This type of dependency
was the one most explored in the previous litera-
tures. 2) three cases of indirect connections: a.
indirect causal effect: e.g. O_V does not influ-
ence G_N directly, but via ADP_NP; b. indirect
evidential effect: knowing G_N will change our
belief about O_V indirectly; c. common cause:
e.g. ADP_NP and O_Obl_V can influence each
other without O_V being observed. Our model
reveals a much finer structure of the word order
</bodyText>
<page confidence="0.996418">
153
</page>
<bodyText confidence="0.99974375">
sub-system by distinguishing different types of
dependencies that might have been categorized
simply as “correlation” in the traditional statisti-
cal methods.
</bodyText>
<sectionHeader confidence="0.985344" genericHeader="method">
4 Quantitative Analysis of Results
</sectionHeader>
<bodyText confidence="0.9996132">
The word order universal results are difficult to
evaluate because we do not know the correct an-
swers. Nonetheless we did a quantitative evalua-
tion following Daumé III and Campbell (2007)’s
method. The results are shown in Figure 3.
</bodyText>
<figureCaption confidence="0.998015">
Figure 3. Results of Quantitative Evaluation
</figureCaption>
<bodyText confidence="0.99994112">
As we can see the predictive power of the
UNIV model is much better than that of the
FLAT model. The accuracy of our both models
is lower than those of Daumé III and Campbell’s.
But this does not mean our models are worse
considering the complexity in model learning.
Instead our UNIV model shows steady accurate
prediction for the top ten universals and has more
stable performance compared with other models.
Using the UNIV model we can do many types
of computation. Besides pairwise feature values,
we can calculate the probability of any combina-
tion of word order feature values. If we want to
know how value “GN” of feature “G_N” is de-
pendent on value “POST” of feature “ADP_NP”
we set POST to be evidence (probability=100%)
and get the probability of having “GN”. Such a
probability can be taken as a measurement of
dependence strength between these two values.
We need more evidence for setting a threshold
value to define a word order universal but for
now we just use 0.5. We calculated the probabili-
ties of all pairwise feature values in the UNIV
model which can found at
http://www.acsu.buffalo.edu/~xialu/univ.html.
</bodyText>
<sectionHeader confidence="0.991323" genericHeader="method">
5 Qualitative Analysis of Results
</sectionHeader>
<bodyText confidence="0.9666044">
We also did qualitative evaluation through com-
parison with the well-known findings in word
order correlation studies. We compared our re-
sults with three major works: those of Green-
berg’s, Dryer’s, and Daumé III and Campbell’s.
5.1 Evaluation: compare with Greenberg’s
and Dryer’s work
Comparison with Greenberg’s work is shown in
Table 1 (in Appendix A). If the probability is
above 0.5 we say it is a universal and mark it red.
We think values like 0.4-0.5 can also give us
some suggestive estimates therefore we mark
these green. For Universal 2, 3, 4, 5, 10, 18 and
19, our results conform to Greenberg’s. But for
others there are discrepancies of different de-
grees. For example, for U12 our results show that
“VSO” can predict “Initial” but not very strongly
compared with “SOV” predicting “Not_Initial”.
Table 2 (in Appendix A) shows our compari-
son with Dryer (1992)’s work. We noticed there
is an asymmetry in terms of V_O’s influence on
other word order pairs, which was not discussed
in previous work. In the correlated pairs, only
ADP NP and G N show bidirectional correla-
tion with O_V while PoQPar becomes a non-
correlated pair. In the non-correlated pairs,
Dem_N becomes a correlated pair and other
pairs also show correlation of weak strength.
Most of our results therefore do not confirm
Dryer’s findings.
</bodyText>
<subsectionHeader confidence="0.4500255">
5.2 Evaluation: compare with Daumé III
and Campbell’s work
</subsectionHeader>
<bodyText confidence="0.99998175">
We compared the probabilities of single value
pairs of the top ten word order universals with
Daumé III and Campbell’s results, which are
shown in the following figures.
</bodyText>
<figure confidence="0.869362">
1 2 3 4 5 6 7 8 9 10
the first ten universals
</figure>
<figureCaption confidence="0.991304">
Figure 4. Compare with Daumé III and Campbell’s
</figureCaption>
<figure confidence="0.997274243902439">
HIER model
p(true) prob PGM
0.5
1 2 3 4 5 6 7 8 9 10
the first ten universals
0 1 2 3 4 5 6 7 8 9 10
Number of Universals ( )
Prediction Accuracy
0.95
0.85
0.75
0.65
0.55
0.9
0.8
0.7
0.6
0.5
FLAT UNIV
p(true) prob PGM
probabilities 1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
0.5
probabilities 1
0.95
0.9
0.85
0.8
0.75
0.7
0.65
0.6
0.55
</figure>
<page confidence="0.994075">
154
</page>
<figureCaption confidence="0.98617">
Figure 5. Compare with Daumé III and Campbell’s
</figureCaption>
<bodyText confidence="0.972197086956522">
DIST model
P(true) is the probability of having the particu-
lar implication; prob is the probability calculated
in a different way which is not specified in Dau-
mé III and Campbell’s work. PGM is our model.
It can be seen that our model provides moderate
numbers which fall between the two probabilities
in Daumé III and Campbell’s results. In Figure 4
the two universals that have the biggest gaps are:
9) Prepositions -&gt;VO and 10) Adjective-Noun-
&gt;Demonstrative-Noun. In Figure 5 the three uni-
versals that have the biggest gaps are: 3) Noun-
Genitive-&gt;Initial subordinator word, 6) Noun-
Genitive-&gt;Prepositions and 8) OV-&gt;SV. It is
hard to tell which model does a better job just by
doing comparison like this. Daumé III and
Campbell’s model computes the probabilities of
3442 feature pairs separately. Their model with
two values as nodes does not consider the more
complex dependencies among more than two
features. Our model provides a better solution by
trying to maximize the joint probabilities of all
word order feature pairs.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="method">
6 Inference
</sectionHeader>
<bodyText confidence="0.999832">
Besides discovering word order universals, our
model can reveal more properties of word order
sub-system through various inference queries. At
present we use SamIam3 for inference because it
has an easy-to-use interface for probabilistic in-
ference queries. Figure 6 (in Appendix B) gives
an example: when we know the language is sub-
ject preceding verb and negative morpheme pre-
ceding verb, then we know the probability for
this language to have postpositions is 0.5349, as
well as the probabilities for the values of all oth-
er features.
The other type of query is MAP which aims to
find the most likely assignments to all of the un-
observed variables. For example, when we only
know that language is VO, we can use MAP que-
ry to find the combination of values which has
the highest probability (0.0032 as shown in Table
3 in Appendix C).
One more useful function is to calculate the
likelihood of a language in terms of word order
properties. If all values of 13 features of a lan-
guage are known, then the probability (likelihood)
of having such a language can be calculated. We
calculated the likelihood of eight languages and
got the results as shown in Figure 7 (in Appendix
</bodyText>
<footnote confidence="0.87613">
3 SamIam is a tool for modeling and reasoning with Bayesi-
an networks ( http://reasoning.cs.ucla.edu/samiam/).
</footnote>
<bodyText confidence="0.999375125">
C). As we can see, English has the highest likeli-
hood to be a language while Hakka Chinese has
the lowest. German and French have similar like-
lihood; Portuguese and Spanish are similar but
are less than German and French. In other words
English is a typical language regarding word or-
der properties while Hakka Chinese is an atypi-
cal one.
</bodyText>
<sectionHeader confidence="0.996998" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999922526315789">
Probabilistic graphic modeling provides solu-
tions to the problems we noticed in the previous
studies of word order universals. By modeling
language as a complex system we shift our atten-
tion to the language itself instead of just features.
Using PGM we can infer properties about a lan-
guage given the known values and we can also
infer the likelihood of a language given all the
values. In the future if we include other domains,
such as phonology, morphology and syntax, we
will be able to discover more properties about
language as a whole complex system.
Regarding the relationships among the fea-
tures since PGM can give a finer structure we are
able to see how the features are related directly
or indirectly. By using probability theory we
overcome the shortcomings of traditional statisti-
cal methods based on NHST. Probabilities cap-
ture our uncertainty about word order correla-
tions. Instead of saying “A is correlated with B”,
we can say “A is correlated with B to a certain
extent”. PGM enables us to quantify our
knowledge about the word order properties of
languages.
Regarding the data treatment, we did very lit-
tle preprocessing of data, therefore reducing the
possibility of bringing in additional bias from
other processes such as family construction in
Dunn et al.&apos;s experiment. In addition we did not
remove most of the values so that we can make
inferences based on values such as “no determi-
nant order” and “both orders”. In this way we
retain the information in our data to the largest
extent.
We think PGM has the potential to become a
new methodology for studying word order uni-
versals. It also opens up many new possibilities
for studying linguistic typology as well:
</bodyText>
<listItem confidence="0.881413833333333">
• It can include other domains to build a more
complex network and to discover more typologi-
cal properties of languages.
• It can be used in field work for linguists to
make predictions about properties of unknown
languages.
</listItem>
<page confidence="0.998942">
155
</page>
<sectionHeader confidence="0.990302" genericHeader="conclusions">
References
</sectionHeader>
<reference confidence="0.996268354166667">
Bickel, B. 2010a. Absolute and statistical universals.
In Hogan, P. C. (ed.) The Cambridge Encyclopedia
of the Language Sciences, 77-79. Cambridge:
Cambridge University Press.
Bickel, B. 2010b. Capturing particulars and univer-
sals in clause linkage: a multivariate analysis. In
Bril, I. (ed.) Clause-hierarchy and clause-linking:
the syntax and pragmatics interface, pp. 51 - 101.
Amsterdam: Benjamins.
Croft, William. 2003. Typology and universals. 2nd
edn. Cambridge: Cambridge University Press.
Daphne Koller and Nir Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques
(Adaptive Computation and Machine Learning se-
ries). MIT Press, Aug 31, 2009
Daumé, H., &amp; Campbell, L. (2007). A Bayesian mod-
el for discovering typological implications.
In Annual Meeting –Association For Computation-
al Linguistics (Vol. 45, No. 1, p. 65).
D.M. Chickering, D. Heckerman, and C. Meek. 1997.
A Bayesian approach to learning Bayesian net-
works with local structure. Proceeding UAI&apos;97
Proceedings of the Thirteenth conference on Un-
certainty in artificial intelligence.
Dryer, M. S. 1989. Large linguistic areas and lan-
guage sampling. Studies in Language 13, 257 –
292.
Dryer, Matthew S. &amp; Martin Haspelmath (eds.). 2011.
The world atlas of language structures online.
München: Max Planck Digital Library.
Dryer, Matthew S. 2011. The evidence for word order
correlations. Linguistic Typology 15. 335–380.
Dunn, Michael, Simon J. Greenhill, Stephen C. Lev-
inson &amp; Russell D. Gray. 2011. Evolved structure
of language shows lineage-specific trends in word-
order universals. Nature 473. 79–82.
E. T. Jaynes. 2003. Probability Theory: The Logic of
Science. Cambridge University Press, Apr 10, 2003.
Friedman, N. (1998, July). The Bayesian structural
EM algorithm. In Proceedings of the Fourteenth
conference on Uncertainty in artificial intelli-
gence (pp. 129-138). Morgan Kaufmann Publishers
Inc.
Friedman, N., Nachman, I., &amp; Peér, D. (1999, July).
Learning bayesian network structure from massive
datasets: the “sparse candidate” algorithm.
In Proceedings of the Fifteenth conference on Un-
certainty in artificial intelligence (pp. 206-215).
Morgan Kaufmann Publishers Inc.
Greenberg, J. H. 1963. Some universals of grammar
with particular reference to the order of meaning-
ful elements. In Universals of Language, J. H.
Greenberg, Ed. MIT Press, Cambridge, MA, 73-
113.
Greenberg, Joseph H. 1966. Synchronic and diachron-
ic universals in phonology. Language 42. 508–517.
Greenberg, J. H. (1969). Some methods of dynamic
comparison in linguistics. Substance and structure
of language, 147-203.
Hawkins, John A. 1983. Word Order Universals. Ac-
ademic Press, 1983.
Justeson, J. S., &amp; Stephens, L. D. (1990). Explana-
tions for word order universals: a log-linear analy-
sis. In Proceedings of the XIV International Con-
gress of Linguists (Vol. 3, pp. 2372-76).
Leray, P., &amp; Francois, O. (2004). BNT structure
learning package: Documentation and experiments.
Levy, R., &amp; Daumé III, H. (2011). Computational
methods are invaluable for typology, but the mod-
els must match the questions: Commentary on
Dunn et al.(2011).Linguistic Typology.(To appear).
Liu, F., Tian, F., &amp; Zhu, Q. (2007). Bayesian network
structure ensemble learning. In Advanced Data
Mining and Applications (pp. 454-465). Springer
Berlin Heidelberg.
Maslova, Elena &amp; Tatiana Nikitina. 2010. Language
universals and stochastic regularity of language
change: Evidence from cross-linguistic distribu-
tions of case marking patterns. Manuscript.
Murphy, K. (2001). The bayes net toolbox for
matlab. Computing science and statistics, 33(2),
1024-1034.
Perkins, Revere D. 1989. Statistical techniques for
determining language sample size. Studies in Lan-
guage 13. 293–315.
Singh, M. (1997, July). Learning Bayesian networks
from incomplete data. In Proceedings of the Na-
tional conference on Artificial Intelligence (pp.
534-539). JOHN WILEY &amp; SONS LTD.
William Croft, Tanmoy Bhattacharya, Dave Klein-
schmidt, D. Eric Smith and T. Florian Jaeger. 2011.
Greenbergian universals, diachrony and statistical
analyses [commentary on Dunn et al., Evolved
structure of language shows lineage-specific trends
in word order universals]. Linguistic Typology
15.433-53.
</reference>
<page confidence="0.999108">
156
</page>
<sectionHeader confidence="0.810591" genericHeader="references">
Appendices
</sectionHeader>
<table confidence="0.980726592592593">
A. Comparison with others’ work
Universals Dependencies UNIV
U2: ADP_NP&lt;=&gt;N_G POST-&gt;GN 83.59
PRE-&gt;NG 70.29
GN-&gt;POST 78.45
NG-&gt;PRE 81.91
U3: VSO-&gt;PRE VSO-&gt;PRE 74.41
U4: SOV-&gt;POST SOV-&gt;POST 85.28
U5: SOV&amp;NG-&gt;NA SOV&amp;NG-&gt;NA 68.95
U9: PoQPar&lt;=&gt;ADP_NP Initial-&gt;PRE 41.87
Final-&gt;POST 49.67
PRE-&gt;Initial 15.80
POST-&gt;Final 31.73
U10: PoQPar&lt;=&gt; VSO all values of below
PoQPar: 10%
VSO below 10%
U11: IntPhr-&gt;VS Initial-&gt;VS 24.12
U12: VSO-&gt;IntPhr VSO-&gt;Initial 50.54
SOV-&gt;Initial 28.52
SOV-&gt;Not_Initial 60.41
U17: VSO-&gt;A_N VSO-&gt;A_N 24.86
U18&amp;19: AN-&gt;NumN 68.86
A_N&lt;=&gt;Num_N&lt;=&gt;Dem_ AN-&gt;DemN 73.74
N NA-&gt;NNum 61.74
NA-&gt;NDem 61.00
U24: RN-&gt;POST (or AN) RN-&gt;POST 65.73
RN-&gt;AN 29.23
</table>
<tableCaption confidence="0.999937">
Table 1. Comparison with Greenberg’s work
</tableCaption>
<table confidence="0.998812">
OV UNIV VO UNIV
correlated pairs
ADP_NP(POST) 90.48 ADP_NP(PRE) 82.72
G_N(GN) 79.38 G_N(NG) 61.49
R_N(RN) 19.66 R_N(NR) 75.17
PoQPar(Final) 31.89 PoQPar(Initial) 15.79
AdSub_Cl (Final) 20.90 AdSub_Cl (Initial) 49.22
IntPhr(Not_Initial) 58.74 IntPhr(Initial) 34.36
non-correlated pairs
A_N(AN) 29.48 A_N(NA) 65.00
Dem_N(Dem_N) 52.27 Dem_N(N_Dem) 54.25
Num_N(NumN) 41.6 Num_N(NNum) 49.25
Deg_A(Deg_A) 43.48 Deg_A(A_Deg) 38.44
Neg_V(NegV) 48.06 Neg_V(VNeg) 25.13
</table>
<tableCaption confidence="0.999899">
Table 2. Comparison with Dryer’s work
</tableCaption>
<figure confidence="0.618947">
B. Probabilistic query example in SamIam
</figure>
<figureCaption confidence="0.934158">
Figure 6. One query example
</figureCaption>
<table confidence="0.478118944444444">
C. Inference examples
P(MAP,e)=0.0015052949102098631
P(MAP|e)=0.003213814742532023
Variable Value
A_N NA
ADP_NP PRE
AdSub_Cl Initial
Deg_A Deg_A
Dem_N N_Dem
G_N NG
IntPhr Not_Initial
Neg_V NegV
Num_N NNum
O_Obl_V VOX
PoQPar Final
R_N NR
S_O_V SVO
S_V SV
</table>
<tableCaption confidence="0.937006">
Table 3. MAP query example
</tableCaption>
<figureCaption confidence="0.9978005">
Figure 7. Likelihood of eight languages in terms of
word order properties
</figureCaption>
<figure confidence="0.997612">
0.00E+00
2.50E-06
2.00E-06
5.00E-07
1.50E-06
1.00E-06
</figure>
<page confidence="0.881794">
157
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.515084">
<title confidence="0.9925085">Exploring Word Order Universals: a Probabilistic Graphical Model Approach</title>
<author confidence="0.989329">Xia</author>
<affiliation confidence="0.9960055">Department of University at</affiliation>
<address confidence="0.811391">Buffalo, NY USA</address>
<email confidence="0.999741">xialu@buffalo.edu</email>
<abstract confidence="0.99589568">In this paper we propose a probabilistic graphical model as an innovative framework for studying typological universals. We view language as a system and linguistic features as its components whose relationships are encoded in a Directed Acyclic Graph (DAG). Taking discovery of the word order universals as a knowledge discovery task we learn the graphical representation of a word order sub-system which reveals a finer structure such as direct and indirect dependencies among word order features. Then probabilistic inference enables us to see the strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated. Our model is not restricted to using only two values of a feature. Using imputation technique and EM algorithm it can handle missing values well. Model averaging technique solves the problem of limited data. In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daumé III</abstract>
<note confidence="0.750896">and Campbell (2007).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B Bickel</author>
</authors>
<title>Absolute and statistical universals.</title>
<date>2010</date>
<booktitle>The Cambridge Encyclopedia of the Language Sciences,</booktitle>
<pages>77--79</pages>
<editor>In Hogan, P. C. (ed.)</editor>
<publisher>Cambridge University Press.</publisher>
<location>Cambridge:</location>
<contexts>
<context position="5652" citStr="Bickel, 2010" startWordPosition="886" endWordPosition="888">nship, for example, how the order of verb and object correlates with the order of noun and adjective. However linguists have also noticed other possible interactions among the word order features, like chains of overlapping implications: Prep ـ ((NAdj ـ NGen) &amp; (NGen ـ NRel)) proposed by Hawkins (1983); multi-conditional implications (Daumé III, 2007); correlations among six word order pairs and three-way interactions (Justeson &amp; Stephens, 1990); spurious word order correlations (Croft et al., 2011); chains of associations, e.g. if C predicts B and B predicts A, then C predicts A redundantly (Bickel, 2010b). These claims about the possible interactions among word order features imply complex relationships among the features. The study of word order correlations started with pairwise comparison, probably because that was what typologists could do given the limited resources of statistical methods. However when we study the properties of a language, by knowing just several word orders such as order of verb and object, noun and adpositions, etc., we are unable to say anything about the language as a whole. Here we want to introduce a new perspective of seeing language as a complex system. We assu</context>
</contexts>
<marker>Bickel, 2010</marker>
<rawString>Bickel, B. 2010a. Absolute and statistical universals. In Hogan, P. C. (ed.) The Cambridge Encyclopedia of the Language Sciences, 77-79. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="false">
<authors>
<author>B 2010b Bickel</author>
</authors>
<title>Capturing particulars and universals in clause linkage: a multivariate analysis.</title>
<booktitle>Clause-hierarchy and clause-linking: the syntax and pragmatics interface,</booktitle>
<pages>51--101</pages>
<editor>In Bril, I. (ed.)</editor>
<location>Amsterdam: Benjamins.</location>
<marker>Bickel, </marker>
<rawString>Bickel, B. 2010b. Capturing particulars and universals in clause linkage: a multivariate analysis. In Bril, I. (ed.) Clause-hierarchy and clause-linking: the syntax and pragmatics interface, pp. 51 - 101. Amsterdam: Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Croft</author>
</authors>
<title>Typology and universals. 2nd edn. Cambridge:</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="8815" citStr="Croft 2003" startWordPosition="1415" endWordPosition="1416">different statuses word order features have in the complex language system. We notice in all the word order studies the order of SOV or OV was given special attention. In Dryer’s study VO order is the dominant one which determines the set of word order pairs correlated with it (or not). We assume word order features have different statuses in the language system and such differences should be manifested by directionality of relationships between feature pairs. Therefore we choose DAG structure as our current model framework. Another issue is the sampling problem. Some typologists (Dryer 1989, Croft 2003) have argued that the language samples in the WALS database (Haspelmath et al., 2005) are not independent and identically distributed (i.i.d.) because languages can share the same feature values due to either genetic or areal effect. While 151 others (Maslova, 2010) argue that languages within a family have developed into distinct ones through the long history. We notice that even we can control the areal and genetic factors there are still many other factors that can influence the typological data distribution, such as 1) language speakers: cognitive, physiological, social, and communicative </context>
</contexts>
<marker>Croft, 2003</marker>
<rawString>Croft, William. 2003. Typology and universals. 2nd edn. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daphne Koller</author>
<author>Nir Friedman</author>
</authors>
<date>2009</date>
<booktitle>Probabilistic Graphical Models: Principles and Techniques (Adaptive Computation and Machine Learning series).</booktitle>
<publisher>MIT Press,</publisher>
<contexts>
<context position="4798" citStr="Koller &amp; Friedman, 2009" startWordPosition="751" endWordPosition="754"> modeling 2.1 Rationale for using PGM in word order study The probabilistic graphical model is the marriage of probabilistic theory and graph theory. It combines a graphical representation with a complex distribution over a high-dimensional space. There are two major types of graphical representations of distributions. One is a Directed Acyclic Graph (DAG) which is also known as a Bayesian network with all edges having a source and a target. The other is an Undirected Acyclic Graph, which is also called a Markov network with all edges undirected. A mixture of these two types is also possible (Koller &amp; Friedman, 2009). There are two advantages of using this model to study word order universals. First the graphical structure can reveal much finer structure of language as a complex system. Most studies on word order correlations examine the pairwise relationship, for example, how the order of verb and object correlates with the order of noun and adjective. However linguists have also noticed other possible interactions among the word order features, like chains of overlapping implications: Prep ـ ((NAdj ـ NGen) &amp; (NGen ـ NRel)) proposed by Hawkins (1983); multi-conditional implications (Daumé III, 2007); cor</context>
<context position="16290" citStr="Koller &amp; Friedman, 2009" startWordPosition="2720" endWordPosition="2724">on) is met, stop. Otherwise go to Step 6. 6) Learn parameters ߠ for ܩெ using datasets ܦ (1  ݉  ܯ) and take a weighted-average as the final parameters ߠܯ. Also use EM algorithm and original dataset to learn parameters ߠாெ for ܩெ. Choose the parameters ߠ among ߠெ and ߠாெ that gives the highest BIC score. Use MAP estimation to fill in the missing values in D and generate another M complete dataset. Go to Step 2. The final structure for the UNIV model is shown in Figure 2. Figure 2. DAG structure of the UNIV model The semantics of a DAG structure cannot be simply interpreted as causality (Koller &amp; Friedman, 2009). From this graph we can see word order features are on different tiers in the hierarchy. The root S_O_V seems to “dominate” all the other features; noun modifiers and noun are in the middle tier while O_Obl_V, AdSub_Cl, Deg_A, Num_N, R, Neg_V and PoQPar are the leaf nodes which might indicate their smallest contribution to the word order properties of a language. O_V seems to be an important node since most paths start from it indicating its influence can flow to many other nodes. We can also see there are two types of connections among the nodes: 1) direct connection: any two nodes connected</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>Daphne Koller and Nir Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques (Adaptive Computation and Machine Learning series). MIT Press, Aug 31, 2009</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Daumé</author>
<author>L Campbell</author>
</authors>
<title>A Bayesian model for discovering typological implications.</title>
<date>2007</date>
<journal>In Annual Meeting –Association For Computational Linguistics</journal>
<volume>45</volume>
<pages>65</pages>
<marker>Daumé, Campbell, 2007</marker>
<rawString>Daumé, H., &amp; Campbell, L. (2007). A Bayesian model for discovering typological implications. In Annual Meeting –Association For Computational Linguistics (Vol. 45, No. 1, p. 65).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M Chickering</author>
<author>D Heckerman</author>
<author>C Meek</author>
</authors>
<title>A Bayesian approach to learning Bayesian networks with local structure.</title>
<date>1997</date>
<booktitle>Proceeding UAI&apos;97 Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence.</booktitle>
<marker>Chickering, Heckerman, Meek, 1997</marker>
<rawString>D.M. Chickering, D. Heckerman, and C. Meek. 1997. A Bayesian approach to learning Bayesian networks with local structure. Proceeding UAI&apos;97 Proceedings of the Thirteenth conference on Uncertainty in artificial intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M S Dryer</author>
</authors>
<title>Large linguistic areas and language sampling.</title>
<date>1989</date>
<journal>Studies in Language</journal>
<volume>13</volume>
<pages>257--292</pages>
<contexts>
<context position="2598" citStr="Dryer, 1989" startWordPosition="394" endWordPosition="395">y using statistical methods as used in any other science. Only those tendencies that can be extrapolated to make general conclusions about the population can be claimed to be “universals” since they reflect the global preferences of value distribution of linguistic features across genealogical hierarchy and geographical areas. Previous statistical methods in the research of word order universals have yielded interesting results but they have to make strong assumptions and do a considerable amount of data preprocessing to make the data fit the statistical model (Greenberg, 1963; Hawkins, 1982; Dryer, 1989; Nichols, 1986; Justeson &amp; Stephens, 1990). Recent studies using probabilistic models are much more flexible and can handle noise and uncertainty better (Daumé III &amp; Campbell, 2007; Dunn et al., 2011). However these models still rely on strong theoretic assumptions and heavy data treatment, such as using only two values of word order pairs while discarding other values, purposefully selecting a subset of the languages to study, or selecting partial data with complete values. In this paper we introduce a novel approach of using a probabilistic graphical model to study word order universals. Us</context>
<context position="8802" citStr="Dryer 1989" startWordPosition="1413" endWordPosition="1414">ture of the different statuses word order features have in the complex language system. We notice in all the word order studies the order of SOV or OV was given special attention. In Dryer’s study VO order is the dominant one which determines the set of word order pairs correlated with it (or not). We assume word order features have different statuses in the language system and such differences should be manifested by directionality of relationships between feature pairs. Therefore we choose DAG structure as our current model framework. Another issue is the sampling problem. Some typologists (Dryer 1989, Croft 2003) have argued that the language samples in the WALS database (Haspelmath et al., 2005) are not independent and identically distributed (i.i.d.) because languages can share the same feature values due to either genetic or areal effect. While 151 others (Maslova, 2010) argue that languages within a family have developed into distinct ones through the long history. We notice that even we can control the areal and genetic factors there are still many other factors that can influence the typological data distribution, such as 1) language speakers: cognitive, physiological, social, and c</context>
</contexts>
<marker>Dryer, 1989</marker>
<rawString>Dryer, M. S. 1989. Large linguistic areas and language sampling. Studies in Language 13, 257 – 292.</rawString>
</citation>
<citation valid="true">
<title>The world atlas of language structures online.</title>
<date>2011</date>
<editor>Dryer, Matthew S. &amp; Martin Haspelmath (eds.).</editor>
<publisher>Max Planck Digital Library.</publisher>
<location>München:</location>
<contexts>
<context position="7122" citStr="(2011)" startWordPosition="1134" endWordPosition="1134">h as phonology, morphology, syntax, etc. which correspond to the subfields in linguistics. In this paper we focus on the subsystem of word order only. The other advantage of PGM is that it enables us to quantify the relationships among word order features. Justeson &amp; Stephens (1990) mentioned the notion of “correlation strength” when they found out that N/A order appears less strongly related to basic V/S/O order and/or adposition type than is N/G order. This is the best a log-linear model can do, to indicate whether a correlation is “strong”, “less strong”, “weak” or “less weak”. Dunn et al. (2011) used Bayes factor value to quantify the relationships between the word order pairs but they mistook the strength of evidence for an effect as the strength of the effect itself (Levy &amp; Daumé III, 2011). A PGM model for a word order subsystem encodes a joint probabilistic distribution of all word order feature pairs. Using probability we can describe the degree of confidence about the uncertain nature of word order correlations. For example, if we set the specific value as evidence, then we can get the values of other features using an inference method. Such values can be seen as quantified str</context>
</contexts>
<marker>2011</marker>
<rawString>Dryer, Matthew S. &amp; Martin Haspelmath (eds.). 2011. The world atlas of language structures online. München: Max Planck Digital Library.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew S Dryer</author>
</authors>
<title>The evidence for word order correlations.</title>
<date>2011</date>
<journal>Linguistic Typology</journal>
<volume>15</volume>
<pages>335--380</pages>
<marker>Dryer, 2011</marker>
<rawString>Dryer, Matthew S. 2011. The evidence for word order correlations. Linguistic Typology 15. 335–380.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Dunn</author>
<author>Simon J Greenhill</author>
<author>Stephen C Levinson</author>
<author>Russell D Gray</author>
</authors>
<title>Evolved structure of language shows lineage-specific trends in wordorder universals.</title>
<date>2011</date>
<journal>Nature</journal>
<volume>473</volume>
<pages>79--82</pages>
<contexts>
<context position="2799" citStr="Dunn et al., 2011" startWordPosition="425" endWordPosition="428">they reflect the global preferences of value distribution of linguistic features across genealogical hierarchy and geographical areas. Previous statistical methods in the research of word order universals have yielded interesting results but they have to make strong assumptions and do a considerable amount of data preprocessing to make the data fit the statistical model (Greenberg, 1963; Hawkins, 1982; Dryer, 1989; Nichols, 1986; Justeson &amp; Stephens, 1990). Recent studies using probabilistic models are much more flexible and can handle noise and uncertainty better (Daumé III &amp; Campbell, 2007; Dunn et al., 2011). However these models still rely on strong theoretic assumptions and heavy data treatment, such as using only two values of word order pairs while discarding other values, purposefully selecting a subset of the languages to study, or selecting partial data with complete values. In this paper we introduce a novel approach of using a probabilistic graphical model to study word order universals. Using this model we can have a graphic representation of the structure of language as a complex system composed of linguistic features. Then the relationship among these features can be quantified as pro</context>
<context position="7122" citStr="Dunn et al. (2011)" startWordPosition="1131" endWordPosition="1134">-systems such as phonology, morphology, syntax, etc. which correspond to the subfields in linguistics. In this paper we focus on the subsystem of word order only. The other advantage of PGM is that it enables us to quantify the relationships among word order features. Justeson &amp; Stephens (1990) mentioned the notion of “correlation strength” when they found out that N/A order appears less strongly related to basic V/S/O order and/or adposition type than is N/G order. This is the best a log-linear model can do, to indicate whether a correlation is “strong”, “less strong”, “weak” or “less weak”. Dunn et al. (2011) used Bayes factor value to quantify the relationships between the word order pairs but they mistook the strength of evidence for an effect as the strength of the effect itself (Levy &amp; Daumé III, 2011). A PGM model for a word order subsystem encodes a joint probabilistic distribution of all word order feature pairs. Using probability we can describe the degree of confidence about the uncertain nature of word order correlations. For example, if we set the specific value as evidence, then we can get the values of other features using an inference method. Such values can be seen as quantified str</context>
</contexts>
<marker>Dunn, Greenhill, Levinson, Gray, 2011</marker>
<rawString>Dunn, Michael, Simon J. Greenhill, Stephen C. Levinson &amp; Russell D. Gray. 2011. Evolved structure of language shows lineage-specific trends in wordorder universals. Nature 473. 79–82.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Jaynes</author>
</authors>
<title>Probability Theory: The Logic of Science.</title>
<date>2003</date>
<publisher>Cambridge University Press,</publisher>
<marker>Jaynes, 2003</marker>
<rawString>E. T. Jaynes. 2003. Probability Theory: The Logic of Science. Cambridge University Press, Apr 10, 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Friedman</author>
</authors>
<title>The Bayesian structural EM algorithm.</title>
<date>1998</date>
<booktitle>In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence</booktitle>
<pages>129--138</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<marker>Friedman, 1998</marker>
<rawString>Friedman, N. (1998, July). The Bayesian structural EM algorithm. In Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence (pp. 129-138). Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Friedman</author>
<author>I Nachman</author>
<author>D Peér</author>
</authors>
<title>Learning bayesian network structure from massive datasets: the “sparse candidate” algorithm.</title>
<date>1999</date>
<booktitle>In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</booktitle>
<pages>206--215</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="12406" citStr="Friedman et al., 1999" startWordPosition="2028" endWordPosition="2031">centage of missing values is 31%. For seven features more than 50% of the languages have values missing. 3.2 Learning the FLAT model There are two big problems in learning DAG structure for the FLAT model. One is caused by large number of missing values. Because EM method for structures from incomplete data takes very long time to converge due to the large parameter space of our model, we decided to use imputation method to handle the missing data problem (Singh, 1997). The other difficulty is caused by limited data. To solve this problem we used model averaging by using bootstrap replicates (Friedman et al., 1999). We use GES (greedy search in the space of equivalent classes) algorithm in BNT_SLP to learn structure from a bootstrap dataset because it uses CPDAGs to represent Markov equivalent classes which makes graph fusion easier. The algorithm is as follows: 1) Use nearest-neighbor method to impute missing values in the original dataset D and create a complete dataset DS. 2) Create T=200 bootstrap resamples by resampling the same number of instances as the original dataset with replacement from DS . Then for each resample DS� learn the highest scoring structure GS�. 3) Fuse the 200 graphs into a sin</context>
</contexts>
<marker>Friedman, Nachman, Peér, 1999</marker>
<rawString>Friedman, N., Nachman, I., &amp; Peér, D. (1999, July). Learning bayesian network structure from massive datasets: the “sparse candidate” algorithm. In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence (pp. 206-215). Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Greenberg</author>
</authors>
<title>Some universals of grammar with particular reference to the order of meaningful elements.</title>
<date>1963</date>
<booktitle>In Universals of</booktitle>
<pages>73--113</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="1312" citStr="Greenberg (1963)" startWordPosition="201" endWordPosition="202">ence enables us to see the strength of such relationships: given the observed value of one feature (or combination of features), the probabilities of values of other features can be calculated. Our model is not restricted to using only two values of a feature. Using imputation technique and EM algorithm it can handle missing values well. Model averaging technique solves the problem of limited data. In addition the incremental and divide-and-conquer method addresses the areal and genetic effects simultaneously instead of separately as in Daumé III and Campbell (2007). 1 Introduction Ever since Greenberg (1963) proposed 45 universals of language based on a sample of 30 languages, typologists have been pursuing this topic actively for the past half century. Since some of them do not agree with the term (or concept) of “universal” they use other terminology such as “correlation”, “co-occurrence”, “dependency”, “interaction” and “implication” to refer to the relationships between/among linguistic feature pairs most of which concern morpheme and word order. Indeed the definition of “universals” has never been clear until recently, when most typologists agreed that such universals should be statistical u</context>
<context position="2570" citStr="Greenberg, 1963" startWordPosition="390" endWordPosition="391">” discovered from data samples by using statistical methods as used in any other science. Only those tendencies that can be extrapolated to make general conclusions about the population can be claimed to be “universals” since they reflect the global preferences of value distribution of linguistic features across genealogical hierarchy and geographical areas. Previous statistical methods in the research of word order universals have yielded interesting results but they have to make strong assumptions and do a considerable amount of data preprocessing to make the data fit the statistical model (Greenberg, 1963; Hawkins, 1982; Dryer, 1989; Nichols, 1986; Justeson &amp; Stephens, 1990). Recent studies using probabilistic models are much more flexible and can handle noise and uncertainty better (Daumé III &amp; Campbell, 2007; Dunn et al., 2011). However these models still rely on strong theoretic assumptions and heavy data treatment, such as using only two values of word order pairs while discarding other values, purposefully selecting a subset of the languages to study, or selecting partial data with complete values. In this paper we introduce a novel approach of using a probabilistic graphical model to stu</context>
</contexts>
<marker>Greenberg, 1963</marker>
<rawString>Greenberg, J. H. 1963. Some universals of grammar with particular reference to the order of meaningful elements. In Universals of Language, J. H. Greenberg, Ed. MIT Press, Cambridge, MA, 73-113.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph H Greenberg</author>
</authors>
<title>Synchronic and diachronic universals in phonology.</title>
<date>1966</date>
<journal>Language</journal>
<volume>42</volume>
<pages>508--517</pages>
<contexts>
<context position="7969" citStr="Greenberg (1966" startWordPosition="1276" endWordPosition="1277">subsystem encodes a joint probabilistic distribution of all word order feature pairs. Using probability we can describe the degree of confidence about the uncertain nature of word order correlations. For example, if we set the specific value as evidence, then we can get the values of other features using an inference method. Such values can be seen as quantified strength of relationship between values of features. 2.2 Our model In our word order universal modeling we will use DAG structure since we think the direction of influence matters when talking about the relationship among features. In Greenberg (1966a) most of the universals are unidirectional, such as “If a language has object-verb order, then it also has subject-verb order” while few are bidirectional universals. The term “directionality” does not capture the full nature of the different statuses word order features have in the complex language system. We notice in all the word order studies the order of SOV or OV was given special attention. In Dryer’s study VO order is the dominant one which determines the set of word order pairs correlated with it (or not). We assume word order features have different statuses in the language system </context>
</contexts>
<marker>Greenberg, 1966</marker>
<rawString>Greenberg, Joseph H. 1966. Synchronic and diachronic universals in phonology. Language 42. 508–517.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Greenberg</author>
</authors>
<title>Some methods of dynamic comparison in linguistics. Substance and structure of language,</title>
<date>1969</date>
<pages>147--203</pages>
<marker>Greenberg, 1969</marker>
<rawString>Greenberg, J. H. (1969). Some methods of dynamic comparison in linguistics. Substance and structure of language, 147-203.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John A Hawkins</author>
</authors>
<title>Word Order Universals.</title>
<date>1983</date>
<publisher>Academic Press,</publisher>
<contexts>
<context position="5343" citStr="Hawkins (1983)" startWordPosition="840" endWordPosition="841">mixture of these two types is also possible (Koller &amp; Friedman, 2009). There are two advantages of using this model to study word order universals. First the graphical structure can reveal much finer structure of language as a complex system. Most studies on word order correlations examine the pairwise relationship, for example, how the order of verb and object correlates with the order of noun and adjective. However linguists have also noticed other possible interactions among the word order features, like chains of overlapping implications: Prep ـ ((NAdj ـ NGen) &amp; (NGen ـ NRel)) proposed by Hawkins (1983); multi-conditional implications (Daumé III, 2007); correlations among six word order pairs and three-way interactions (Justeson &amp; Stephens, 1990); spurious word order correlations (Croft et al., 2011); chains of associations, e.g. if C predicts B and B predicts A, then C predicts A redundantly (Bickel, 2010b). These claims about the possible interactions among word order features imply complex relationships among the features. The study of word order correlations started with pairwise comparison, probably because that was what typologists could do given the limited resources of statistical me</context>
</contexts>
<marker>Hawkins, 1983</marker>
<rawString>Hawkins, John A. 1983. Word Order Universals. Academic Press, 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J S Justeson</author>
<author>L D Stephens</author>
</authors>
<title>Explanations for word order universals: a log-linear analysis.</title>
<date>1990</date>
<booktitle>In Proceedings of the XIV International Congress of Linguists</booktitle>
<volume>3</volume>
<pages>2372--76</pages>
<contexts>
<context position="2641" citStr="Justeson &amp; Stephens, 1990" startWordPosition="398" endWordPosition="401">as used in any other science. Only those tendencies that can be extrapolated to make general conclusions about the population can be claimed to be “universals” since they reflect the global preferences of value distribution of linguistic features across genealogical hierarchy and geographical areas. Previous statistical methods in the research of word order universals have yielded interesting results but they have to make strong assumptions and do a considerable amount of data preprocessing to make the data fit the statistical model (Greenberg, 1963; Hawkins, 1982; Dryer, 1989; Nichols, 1986; Justeson &amp; Stephens, 1990). Recent studies using probabilistic models are much more flexible and can handle noise and uncertainty better (Daumé III &amp; Campbell, 2007; Dunn et al., 2011). However these models still rely on strong theoretic assumptions and heavy data treatment, such as using only two values of word order pairs while discarding other values, purposefully selecting a subset of the languages to study, or selecting partial data with complete values. In this paper we introduce a novel approach of using a probabilistic graphical model to study word order universals. Using this model we can have a graphic repres</context>
<context position="5489" citStr="Justeson &amp; Stephens, 1990" startWordPosition="857" endWordPosition="860">r universals. First the graphical structure can reveal much finer structure of language as a complex system. Most studies on word order correlations examine the pairwise relationship, for example, how the order of verb and object correlates with the order of noun and adjective. However linguists have also noticed other possible interactions among the word order features, like chains of overlapping implications: Prep ـ ((NAdj ـ NGen) &amp; (NGen ـ NRel)) proposed by Hawkins (1983); multi-conditional implications (Daumé III, 2007); correlations among six word order pairs and three-way interactions (Justeson &amp; Stephens, 1990); spurious word order correlations (Croft et al., 2011); chains of associations, e.g. if C predicts B and B predicts A, then C predicts A redundantly (Bickel, 2010b). These claims about the possible interactions among word order features imply complex relationships among the features. The study of word order correlations started with pairwise comparison, probably because that was what typologists could do given the limited resources of statistical methods. However when we study the properties of a language, by knowing just several word orders such as order of verb and object, noun and adpositi</context>
<context position="6799" citStr="Justeson &amp; Stephens (1990)" startWordPosition="1076" endWordPosition="1079"> to introduce a new perspective of seeing language as a complex system. We assume there is a meta-language that has the universal properties of all languages in the world. We want a model that can represent this meta-language and make inferences about linguistic properties of new languages. This system is composed of multiple sub-systems such as phonology, morphology, syntax, etc. which correspond to the subfields in linguistics. In this paper we focus on the subsystem of word order only. The other advantage of PGM is that it enables us to quantify the relationships among word order features. Justeson &amp; Stephens (1990) mentioned the notion of “correlation strength” when they found out that N/A order appears less strongly related to basic V/S/O order and/or adposition type than is N/G order. This is the best a log-linear model can do, to indicate whether a correlation is “strong”, “less strong”, “weak” or “less weak”. Dunn et al. (2011) used Bayes factor value to quantify the relationships between the word order pairs but they mistook the strength of evidence for an effect as the strength of the effect itself (Levy &amp; Daumé III, 2011). A PGM model for a word order subsystem encodes a joint probabilistic distr</context>
</contexts>
<marker>Justeson, Stephens, 1990</marker>
<rawString>Justeson, J. S., &amp; Stephens, L. D. (1990). Explanations for word order universals: a log-linear analysis. In Proceedings of the XIV International Congress of Linguists (Vol. 3, pp. 2372-76).</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Leray</author>
<author>O Francois</author>
</authors>
<title>BNT structure learning package: Documentation and experiments.</title>
<date>2004</date>
<contexts>
<context position="10157" citStr="Leray &amp; Francois (2004)" startWordPosition="1634" endWordPosition="1637"> 3) random noise such as historical accidents. Here we do not make any assumption about the i.i.d property of the language samples and propose two models: one is FLAT, which assumes samples are independent and identically distributed (i.i.d.); the other is UNIV, which takes care of the possible dependencies among the samples. By comparing the predictive power of these two models we hope to find one that is closer to the real distribution. 3 Learning To build our models we need to learn both structure and parameters for the two models. We used Murphy (2001)’s Bayesian Network Toolbox (BNT) and Leray &amp; Francois (2004)’s BNT Structure Learning Package (BNT_SLP) for this purpose. 3.1 Data As we mentioned earlier we will restrict our attention to the domain of word order only in this paper. In the WALS database there are 56 features belonging to the “Word Order” category. Because some of the features are redundant, we chose 15 sets of word order features which are: S_O_V1 (order of subject, object and verb) [72], S_V (order of subject and verb) [3], O_V (order of object and verb) [3], O_Obl_V (order of Object, Oblique, and Verb) [6], ADP_NP (order of adposition and noun phrase) [5], G_N (order of genitive and</context>
</contexts>
<marker>Leray, Francois, 2004</marker>
<rawString>Leray, P., &amp; Francois, O. (2004). BNT structure learning package: Documentation and experiments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Levy</author>
<author>H Daumé</author>
</authors>
<title>Computational methods are invaluable for typology, but the models must match the questions: Commentary on Dunn et al.(2011).Linguistic Typology.(To appear).</title>
<date>2011</date>
<marker>Levy, Daumé, 2011</marker>
<rawString>Levy, R., &amp; Daumé III, H. (2011). Computational methods are invaluable for typology, but the models must match the questions: Commentary on Dunn et al.(2011).Linguistic Typology.(To appear).</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Liu</author>
<author>F Tian</author>
<author>Q Zhu</author>
</authors>
<title>Bayesian network structure ensemble learning.</title>
<date>2007</date>
<booktitle>In Advanced Data Mining and Applications</booktitle>
<pages>454--465</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="13099" citStr="Liu et al., 2007" startWordPosition="2142" endWordPosition="2145"> BNT_SLP to learn structure from a bootstrap dataset because it uses CPDAGs to represent Markov equivalent classes which makes graph fusion easier. The algorithm is as follows: 1) Use nearest-neighbor method to impute missing values in the original dataset D and create a complete dataset DS. 2) Create T=200 bootstrap resamples by resampling the same number of instances as the original dataset with replacement from DS . Then for each resample DS� learn the highest scoring structure GS�. 3) Fuse the 200 graphs into a single graph GS using the “Intergroup Undirected Networks Integration” method (Liu et al., 2007). Then use cpdag_to_dag.m in BNT_SLP to change GS into a directed graph GS&apos;. 4) Compute the BIC scores of GS&apos; using the 200 resamples and choose the highest one. If the convergence criterion (change of BIC is less than 10-4 compared with the previous iteration) is met, stop. Otherwise go to Step 5. 5) Learn 200 sets of parameters BS� for GS� using the 200 resamples and take a weighted-average as the final parameters BS&apos;. Also use EM algorithm and dataset D to learn parameters BEM for GS&apos;. Choose the parameters B between BS&apos; and BEM that gives the highest BIC score. Use MAP estimation to fill i</context>
<context position="15418" citStr="Liu et al., 2007" startWordPosition="2553" endWordPosition="2556">picked 1/n of the data and combine them to make a subset. In this way we can have n subsets of data which have decreased degree of dependencies among the samples. We learn a structure for each subset and fuse the n graphs into one single graph. The algorithm is as follows: 1) Use nearest-neighbor method to impute missing values and create M complete datasets ܦ (1  ݉  ܯ). 2) For each ܦ divide the samples into n subsets. Then for each subset ܦ learn the highest scoring structure ܩ. 3) Fuse the n graphs into a single graph ܩ using the “Intragroup Undirected Networks Integration” method (Liu et al., 2007). 4) Fuse the M graphs to make a single directed graph ܩெ as in Step 3 in the previous section. 5) Compute the BIC score of ܩெ using datasets ܦ (1  ݉  ܯ) and choose the highest score. If the convergence criterion (same as in the previous section) is met, stop. Otherwise go to Step 6. 6) Learn parameters ߠ for ܩெ using datasets ܦ (1  ݉  ܯ) and take a weighted-average as the final parameters ߠܯ. Also use EM algorithm and original dataset to learn parameters ߠாெ for ܩெ. Choose the parameters ߠ among ߠெ and ߠாெ that gives the highest BIC score. Use MAP estimation to fill in the missin</context>
</contexts>
<marker>Liu, Tian, Zhu, 2007</marker>
<rawString>Liu, F., Tian, F., &amp; Zhu, Q. (2007). Bayesian network structure ensemble learning. In Advanced Data Mining and Applications (pp. 454-465). Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elena Maslova</author>
<author>Tatiana Nikitina</author>
</authors>
<title>Language universals and stochastic regularity of language change: Evidence from cross-linguistic distributions of case marking patterns.</title>
<date>2010</date>
<tech>Manuscript.</tech>
<marker>Maslova, Nikitina, 2010</marker>
<rawString>Maslova, Elena &amp; Tatiana Nikitina. 2010. Language universals and stochastic regularity of language change: Evidence from cross-linguistic distributions of case marking patterns. Manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Murphy</author>
</authors>
<title>The bayes net toolbox for matlab. Computing science and statistics,</title>
<date>2001</date>
<volume>33</volume>
<issue>2</issue>
<pages>1024--1034</pages>
<contexts>
<context position="10096" citStr="Murphy (2001)" startWordPosition="1627" endWordPosition="1628">itical biases (some languages are well documented); 3) random noise such as historical accidents. Here we do not make any assumption about the i.i.d property of the language samples and propose two models: one is FLAT, which assumes samples are independent and identically distributed (i.i.d.); the other is UNIV, which takes care of the possible dependencies among the samples. By comparing the predictive power of these two models we hope to find one that is closer to the real distribution. 3 Learning To build our models we need to learn both structure and parameters for the two models. We used Murphy (2001)’s Bayesian Network Toolbox (BNT) and Leray &amp; Francois (2004)’s BNT Structure Learning Package (BNT_SLP) for this purpose. 3.1 Data As we mentioned earlier we will restrict our attention to the domain of word order only in this paper. In the WALS database there are 56 features belonging to the “Word Order” category. Because some of the features are redundant, we chose 15 sets of word order features which are: S_O_V1 (order of subject, object and verb) [72], S_V (order of subject and verb) [3], O_V (order of object and verb) [3], O_Obl_V (order of Object, Oblique, and Verb) [6], ADP_NP (order o</context>
</contexts>
<marker>Murphy, 2001</marker>
<rawString>Murphy, K. (2001). The bayes net toolbox for matlab. Computing science and statistics, 33(2), 1024-1034.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Revere D Perkins</author>
</authors>
<title>Statistical techniques for determining language sample size.</title>
<date>1989</date>
<journal>Studies in Language</journal>
<volume>13</volume>
<pages>293--315</pages>
<marker>Perkins, 1989</marker>
<rawString>Perkins, Revere D. 1989. Statistical techniques for determining language sample size. Studies in Language 13. 293–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Singh</author>
</authors>
<title>Learning Bayesian networks from incomplete data.</title>
<date>1997</date>
<booktitle>In Proceedings of the National conference on Artificial Intelligence</booktitle>
<pages>534--539</pages>
<publisher>JOHN WILEY &amp; SONS LTD.</publisher>
<contexts>
<context position="12257" citStr="Singh, 1997" startWordPosition="2005" endWordPosition="2006">ng those languages with no value for all 15 word order features we have 1646 data entries. This database is very sparse: in overall the percentage of missing values is 31%. For seven features more than 50% of the languages have values missing. 3.2 Learning the FLAT model There are two big problems in learning DAG structure for the FLAT model. One is caused by large number of missing values. Because EM method for structures from incomplete data takes very long time to converge due to the large parameter space of our model, we decided to use imputation method to handle the missing data problem (Singh, 1997). The other difficulty is caused by limited data. To solve this problem we used model averaging by using bootstrap replicates (Friedman et al., 1999). We use GES (greedy search in the space of equivalent classes) algorithm in BNT_SLP to learn structure from a bootstrap dataset because it uses CPDAGs to represent Markov equivalent classes which makes graph fusion easier. The algorithm is as follows: 1) Use nearest-neighbor method to impute missing values in the original dataset D and create a complete dataset DS. 2) Create T=200 bootstrap resamples by resampling the same number of instances as </context>
</contexts>
<marker>Singh, 1997</marker>
<rawString>Singh, M. (1997, July). Learning Bayesian networks from incomplete data. In Proceedings of the National conference on Artificial Intelligence (pp. 534-539). JOHN WILEY &amp; SONS LTD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Croft</author>
<author>Tanmoy Bhattacharya</author>
<author>Dave Kleinschmidt</author>
<author>D Eric Smith</author>
<author>T Florian Jaeger</author>
</authors>
<date>2011</date>
<contexts>
<context position="5544" citStr="Croft et al., 2011" startWordPosition="865" endWordPosition="868">ner structure of language as a complex system. Most studies on word order correlations examine the pairwise relationship, for example, how the order of verb and object correlates with the order of noun and adjective. However linguists have also noticed other possible interactions among the word order features, like chains of overlapping implications: Prep ـ ((NAdj ـ NGen) &amp; (NGen ـ NRel)) proposed by Hawkins (1983); multi-conditional implications (Daumé III, 2007); correlations among six word order pairs and three-way interactions (Justeson &amp; Stephens, 1990); spurious word order correlations (Croft et al., 2011); chains of associations, e.g. if C predicts B and B predicts A, then C predicts A redundantly (Bickel, 2010b). These claims about the possible interactions among word order features imply complex relationships among the features. The study of word order correlations started with pairwise comparison, probably because that was what typologists could do given the limited resources of statistical methods. However when we study the properties of a language, by knowing just several word orders such as order of verb and object, noun and adpositions, etc., we are unable to say anything about the lang</context>
</contexts>
<marker>Croft, Bhattacharya, Kleinschmidt, Smith, Jaeger, 2011</marker>
<rawString>William Croft, Tanmoy Bhattacharya, Dave Kleinschmidt, D. Eric Smith and T. Florian Jaeger. 2011.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Greenbergian universals</author>
</authors>
<title>diachrony and statistical analyses [commentary on Dunn et al., Evolved structure of language shows lineage-specific trends in word order universals]. Linguistic Typology</title>
<pages>15--433</pages>
<marker>universals, </marker>
<rawString>Greenbergian universals, diachrony and statistical analyses [commentary on Dunn et al., Evolved structure of language shows lineage-specific trends in word order universals]. Linguistic Typology 15.433-53.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>