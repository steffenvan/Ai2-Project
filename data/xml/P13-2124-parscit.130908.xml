<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000027">
<title confidence="0.997603">
A Learner Corpus-based Approach to Verb Suggestion for ESL
</title>
<author confidence="0.994025">
Yu Sawai Mamoru Komachi* Yuji Matsumoto
</author>
<affiliation confidence="0.99667">
Graduate School of Information Science
Nara Institute of Science and Technology
</affiliation>
<address confidence="0.805356">
8916-5 Takayama, Ikoma, Nara, 630-0192, Japan
</address>
<email confidence="0.997116">
{yu-s, komachi, matsu}@is.naist.jp
</email>
<sectionHeader confidence="0.993878" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980928571429">
We propose a verb suggestion method
which uses candidate sets and domain
adaptation to incorporate error patterns
produced by ESL learners. The candi-
date sets are constructed from a large scale
learner corpus to cover various error pat-
terns made by learners. Furthermore, the
model is trained using both a native cor-
pus and the learner corpus via a domain
adaptation technique. Experiments on two
learner corpora show that the candidate
sets increase the coverage of error patterns
and domain adaptation improves the per-
formance for verb suggestion.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999610117647059">
In this study, we address verb selection errors in
the writing of English learners. Selecting the right
verb based on the context of a sentence is difficult
for the learners of English as a Second Language
(ESL). This error type is one of the most common
errors in various learner corpora ranging from ele-
mentary to proficient levels1.
They ?connect/communicate with other
businessmen and do their jobs with the
help of computers.2
This sentence is grammatically acceptable with
either verb. However, native speakers of En-
glish would less likely use “connect”, which
means “forming a relationship (with other busi-
nessmen)”, whereas “communicate” means “ex-
changing information or ideas”, which is what the
sentence is trying to convey.
</bodyText>
<footnote confidence="0.8676065">
*Now at Tokyo Metropolitan University.
1For example, in the CLC-FCE dataset, the replacement
error of verbs is the third most common out of 75 error types.
In the KJ corpus, lexical choice of verb is the sixth most com-
mon out of 47 error types.
2This sentence is taken from the CLC-FCE dataset.
</footnote>
<bodyText confidence="0.999777216216216">
Previous work on verb selection usually treats
the task as a multi-class classification problem
(Wu et al., 2010; Wang and Hirst, 2010; Liu et
al., 2010; Liu et al., 2011). In this formaliza-
tion, it is important to restrict verbs by a candi-
date set because verb vocabulary is more numer-
ous than other classes, such as determiners. Can-
didate sets for verb selection are often extracted
from thesauri and/or round-trip translations. How-
ever, these resources may not cover certain error
patterns found in actual learner corpora, and suffer
from low-coverage. Furthermore, all the existing
classifier models are trained only using a native
corpus, which may not be adequate for correcting
learner errors.
In this paper, we propose to use error patterns
in ESL writing for verb suggestion task by using
candidate sets and a domain adaptation technique.
First, to increase the coverage, candidate sets are
extracted from a large scale learner corpus derived
from a language learning website. Second, a do-
main adaptation technique is applied to the model
to fill the gap between two domains: native cor-
pus and ESL corpus. Experiments are carried out
on publicly available learner corpora, the Cam-
bridge Learner Corpus First Certificate of English
dataset (CLC-FCE) and the Konan JIEM corpus
(KJ). The results show that the proposed candidate
sets improve the coverage, compared to the base-
line candidate sets derived from the WordNet and
a round-trip translation table. Domain adaptation
also boosts the suggestion performance.
To our knowledge, this is the first work for
verb suggestion that uses (1) a learner corpus as
a source of candidate sets and (2) the domain
adaptation technique to take learner errors into ac-
count.
</bodyText>
<page confidence="0.959468">
708
</page>
<note confidence="0.5356495">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 708–713,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.8408305" genericHeader="method">
2 Verb Suggestion Considering Error
Patterns
</sectionHeader>
<bodyText confidence="0.9999924">
The proposed verb suggestion system follows the
standard approach in related tasks (Rozovskaya
and Roth, 2011; Wu et al., 2010), where the candi-
date selection is formalized as a multi-class classi-
fication problem with predefined candidate sets.
</bodyText>
<subsectionHeader confidence="0.995307">
2.1 Candidate Sets
</subsectionHeader>
<bodyText confidence="0.999926052631579">
For reflecting tendency of learner errors to the can-
didate sets, we use a large scale corpus obtained
from learners’ writing on an SNS (Social Net-
working Service), Lang-83. An advantage of using
the learner corpus from such website is the size of
annotated portion (Mizumoto et al., 2011). This
SNS has over 1 million manually annotated En-
glish sentences written by ESL learners. We have
collected the learner writings on the site, and re-
leased the dataset for research purpose4.
First, we performed POS tagging for the dataset
using the treebank POS tagger in the NLTK toolkit
2.10. Second, we extracted the correction pairs
which have “VB*” tag. The set of correction pairs
given an incorrect verb is considered as a candi-
date set for the verb.
We then performed the following preprocessing
for the dataset because we focus on lexical selec-
tion of verbs:
</bodyText>
<listItem confidence="0.99329925">
• Lemmatize verbs to reduce data sparseness.
• Remove non-English verbs using WordNet.
• Remove incorrect verbs which occur only
once in the dataset.
</listItem>
<bodyText confidence="0.998837222222222">
The target verbs are limited to the 500 most
common verbs in the CLC-FCE corpus5. There-
fore, verbs that do not appear in the target list are
not included in the candidate sets. The topmost
500 verbs cover almost 90 percent of the vocabu-
lary of verbs in the CLC-FCE corpus6.
The average number of candidates in a set is
20.37. Note that the number of candidates varies
across each target verb8.
</bodyText>
<footnote confidence="0.997356666666667">
3http://lang-8.com
4Further details can be found at http://cl.naist.
jp/nldata/lang-8/. Candidate sets will also be avail-
able at the same URL.
5They are extracted from all “VB” tagged tokens, and
they contain 1,292 unique verbs after removing non-English
words.
6This number excludes “be”.
7In this paper, we limit the maximum number of candi-
dates in each set to 50.
8For instance, the candidate set for “get” has 315 correc-
tion pairs, whereas “refund” has only 4.
</footnote>
<subsectionHeader confidence="0.983862">
2.2 Suggestion Model
</subsectionHeader>
<bodyText confidence="0.999972142857143">
The verb suggestion model consists of multi-class
classifiers for each target verb; and based on the
classifiers’ output, it suggests alternative verbs.
Instances are in a fill-in-the-blank format, where
the labels are verbs. Features in this format are
extracted from the surrounding context of a verb.
When testing on the learner corpus, the model sug-
gests a ranking of the possible verbs for the blank
corresponding to a given context. Note that un-
like the fill-in-the-blank task, the candidate sets
and domain adaptation can be applied to this task
to take the original word into account.
The model is trained on a huge native corpus,
namely the ukWaC corpus, because the data-size
of learner corpora is limited compared to native
corpora. It is then adapted to the target domain,
i.e., learner writing. In our experiment, the Lang-
8 corpus is used as the target domain corpus, since
we assume that it shares the same characteristics
with the CLC-FCE and the KJ corpora used for
testing.
</bodyText>
<subsectionHeader confidence="0.998169">
2.3 Domain Adaptation
</subsectionHeader>
<bodyText confidence="0.99993725">
To adapt the models to the learner corpus, we em-
ploy a domain adaptation technique to emphasize
the importance of learner domain information. Al-
though there are many studies on domain adap-
tation, we chose to use Feature Augmentation
technique introduced by (Daum´e III, 2007) for its
simplicity. Recently, (Imamura et al., 2012) pro-
posed to apply this method to grammatical error
correction for writings of Japanese learners and
confirmed that this is more effective for correct-
ing learner errors than simply adding the target do-
main instances.
In this study, the source domain is the native
writing, and the target domain is the ESL writing.
Our motivation is to use the ESL corpus together
with the huge native corpus to employ both an ad-
vantage of the size of training data and the ESL
writing specific features.
In this method, adapting a model to another
model is achieved by extending the feature space.
Given a feature vector of F dimensions as x E
lRF(F &gt; 0), using simple mapping, the aug-
mented feature vectors for source and target do-
mains are obtained as follows,
</bodyText>
<construct confidence="0.9371655">
Source domain: &lt; xS, xS, 0 &gt; (1)
Target domain: &lt; xT, 0, xT &gt; (2)
</construct>
<page confidence="0.996781">
709
</page>
<bodyText confidence="0.9999734">
where 0 denotes a zero-vector of F dimensions.
The three partitions mean a common, a source-
specific, and a target-specific feature space. When
testing on the ESL corpora, the target-specific fea-
tures are emphasized.
</bodyText>
<subsectionHeader confidence="0.797176">
2.4 Features
</subsectionHeader>
<bodyText confidence="0.999979181818181">
In previous work, various features were used: lex-
ical and POS n-grams, dependencies, and argu-
ments in the verb context. (Liu et al., 2011) has
shown that shallow parse features, such as lexi-
cal n-grams and chunks, work well in realistic set-
tings, in which the input sentence may not be cor-
rectly parsed. Considering this, we use shallow
parse features as context features for robustness.
The features include lexical and POS n-grams,
and lexical head words of the nearest NPs, and
clustering features of these head words. An ex-
ample of extracted features is shown in Table 2.4.
Note that those features are also used when ex-
tracting examples from the target domain dataset
(the learner domain corpus). As shown in Table
2.4, the n-gram features are 3-gram and extracted
from ±2 context window. The nearest NP’s head
features are divided into two (Left, Right).
The additional clustering features are used for
reducing sparseness, because the NP’s head words
are usually proper nouns. To create the word clus-
ters, we employ Brown clustering, a hierarchical
clustering algorithm proposed by (Brown et al.,
1992). The structure of clusters is a complete bi-
nary tree, in which each node is represented as a
bit-string. By varying the length of the prefix of
bit-string, it is possible to change the granularity
of cluster representation. As illustrated in Table
2.4, we use the clustering features with three lev-
els of granularity: 256, 128, and 64 dimensions.
We used Percy Liang’s implementation9 to create
256 dimensional model from the ukWaC corpus,
which is used as the native corpus.
</bodyText>
<sectionHeader confidence="0.999738" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999822285714286">
Performance of verb suggestion is evaluated on
two error-tagged learner corpora: CLC-FCE and
KJ. In the experiments, we assume that the tar-
get verb and its context for suggestion are already
given.
For the experiment on the CLC-FCE dataset,
the targets are all words tagged with “RV” (re-
</bodyText>
<footnote confidence="0.9661145">
9https://github.com/percyliang/
brown-cluster
</footnote>
<table confidence="0.947221153846154">
Feature Example
n-grams they-*V*-with
(surface) &lt;S&gt;-they-*V*
*V*-with-other
n-grams PRP-*V*-IN
(POS) &lt;S&gt;-PRP-*V*
*V*-IN-JJ
NP head L they, L PRP
(Left, Right) R businessmen, R NNS
NP head cluster L 01110001, L 0111000, L 011100
(Left, Right) R 11011001, R 1101100, R 110110
(e.g., They (communicate) with other businessmen and do
their jobs with the help of computers.)
</table>
<tableCaption confidence="0.87079325">
“&lt;S&gt;” denotes the beginning of the sentence, “*V*”
denotes the blanked out verb.
Table 1: Example of extracted features as the fill-
in-the-blank form.
</tableCaption>
<bodyText confidence="0.986345909090909">
placement error of verbs). We assume that all the
verb selection errors are covered with this error
tag. All error tagged parts with nested correction
or multi-word expressions are excluded. The re-
sulting number of “true” targets is 1,083, which
amounts to 4% of all verbs. Therefore the dataset
is highly skewed to correct usages, though this set-
ting expresses well the reality of ESL writing, as
shown in (Chodorow et al., 2012).
We carried out experiments with a variety of re-
sources used for creating candidate sets.
</bodyText>
<listItem confidence="0.872933">
• WordNet
</listItem>
<bodyText confidence="0.724965666666667">
Candidates are retrieved from the synsets and
verbs sharing the same hypernyms in the
WordNet 3.0.
</bodyText>
<listItem confidence="0.8819455">
• LearnerSmall
Candidates are retrieved from following
learner corpora: NUS corpus of learner
English (NUCLE), Konan-JIEM (KJ), and
NICT Japanese learner English (JLE) corpus.
• Roundtrip
Candidates are collected by performing
“round-trip” translation, which is similar to
(Bannard and Callison-Burch, 2005) 10.
• WordNet+Roundtrip
</listItem>
<bodyText confidence="0.942887666666667">
A combination of the thesaurus-based and the
translation table-based candidate sets, similar
to (Liu et al., 2010) and (Liu et al., 2011).
</bodyText>
<listItem confidence="0.861493">
• Lang-8
</listItem>
<bodyText confidence="0.9868525">
The proposed candidate sets obtained from a
large scale learner corpus.
</bodyText>
<listItem confidence="0.580601">
• Lang-8+DA
Lang-8 candidate sets with domain adapta-
</listItem>
<footnote confidence="0.990143333333333">
10Our roundtrip translation lexicons are built using a subset
of the WIT3 corpus (Cettolo et al., 2012), which is available
at http://wit3.fbk.eu.
</footnote>
<page confidence="0.964343">
710
</page>
<table confidence="0.997907428571429">
Settings Candidates/set (Avg.)
WordNet 14.8
LearnerSmall 5.1
Roundtrip 50
Roundtrip (En-Ja-En) 50
WordNet+Roundtrip 50
Lang-8 20.3
</table>
<tableCaption confidence="0.889407666666667">
Table 2: Comparison of candidate set size for each
setting.
tion via feature augmentation.
</tableCaption>
<bodyText confidence="0.958355666666667">
Table 3 shows a comparison of the average
number of candidates in each setting. In all config-
urations above, the parameters of the models un-
derlying the system are identical. We used a L2-
regularized generalized linear model with log-loss
function via Scikit-learn ver. 0.13.
</bodyText>
<sectionHeader confidence="0.785891" genericHeader="method">
Inter-corpus Evaluation
</sectionHeader>
<bodyText confidence="0.999962230769231">
We also evaluate the suggestion performance on
the KJ corpus. The corpus contains diary-style
writing by Japanese university students. The pro-
ficiency of the learners ranges from elementary to
intermediate, so it is lower than that of the CLC-
FCE learners. The targets are all verbs tagged with
“v lxc” (lexical selection error of verbs).
To see the effect of L1 on the verb sugges-
tion task, we added an alternative setting for
the Roundtrip using only the English-Japanese
and Japanese-English round-trip translation tables
(En-Ja-En). For the experiment on this test-
corpus, the LearnerSmall is not included.
</bodyText>
<subsectionHeader confidence="0.520149">
Datasets
</subsectionHeader>
<bodyText confidence="0.999962">
The ukWaC web-corpus (Ferraresi et al., 2008) is
used as a native corpus for training the suggestion
model. Although this corpus consists of over 40
million sentences, 20,000 randomly selected sen-
tences are used for each verb11.
The Lang-8 learner corpus is used for domain
adaptation of the model in the Lang-8+DA config-
uration. The portion of data is the same as that
used for constructing candidate sets.
</bodyText>
<subsectionHeader confidence="0.648145">
Metrics
</subsectionHeader>
<bodyText confidence="0.9983194">
Mean Reciprocal Rank (MRR) is used for evalu-
ating the performance of alternative suggestions.
The mean reciprocal rank is calculated by taking
11e.g., a classifier with a candidate set containing 50 verbs
is trained with 1 million sentences in total.
the average of the reciprocal ranks for each in-
stance. Given r goldi as the position of the gold
correction candidate in the suggestion list Si for i-
th checkpoint, the reciprocal rank RRi is defined
as,
</bodyText>
<figure confidence="0.613679666666667">
1 (goldi ∈ Si ) (3)
r goldi
0 (otherwise)
</figure>
<sectionHeader confidence="0.99636" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999995888888889">
Tables 5 and 5 show the results of suggestion per-
formance on the CLC-FCE dataset and the KJ cor-
pus, respectively. In both cases, the Lang-8 and its
domain adaptation variant outperformed the oth-
ers. The coverage of error patterns in the tables
is the percentage of the cases where the sugges-
tion list includes the gold correction. Generally,
the suggestion performance and the coverage im-
prove as the size of the candidate sets increases.
</bodyText>
<sectionHeader confidence="0.9945" genericHeader="conclusions">
5 Discussions
</sectionHeader>
<bodyText confidence="0.999971482758621">
Although the expert-annotated learner corpora
contain candidates which are more reliable than
a web-crawled Lang-8 corpus, the Lang-8 setting
performed better as shown in Table 5. This can be
explained by the broader coverage by the Lang-8
candidate sets than that of the LearnerSmall. Sim-
ilarly, the WordNet performed the worst because
it contains only synonym-like candidates. We can
conclude that, for the verb suggestion task, the
coverage (recall) of candidate sets is more impor-
tant than the quality (precision).
We see little influence of learners’ L1 in the re-
sults of Table 5, since the Roundtrip performed
better than the Roundtrip (En-Ja-En). As already
mentioned, the number of error patterns contained
in the candidate sets seems to have more impor-
tance than the quality.
As shown in Tables 5 and 5, a positive ef-
fect of domain adaptation technique appeared in
both test-corpora. In the case of the CLC-FCE,
280 out of 624 suggestions were improved com-
pared to the setting without domain adaptation.
For instance, confusions between synonyms such
as “?live/stay”, “?say/tell”, and “?solve/resolve”
are improved, because sentences containing these
confusions appear more frequently in the Lang-
8 corpus. Although the number of test-cases for
the KJ corpus is smaller than the CLC-FCE, we
can see the improvements for 33 out of 66 sug-
</bodyText>
<figure confidence="0.91962425">
⎧
⎨
⎩
RRi =
</figure>
<page confidence="0.981796">
711
</page>
<table confidence="0.8779492">
Settings MRR Coverage
WordNet 0.066 14.0 %
LearnerSmall 0.128 23.5 %
Roundtrip 0.185 48.1 %
WordNet+Roundtrip 0.173 48.1 %
Lang-8 0.220 57.6 %
Lang-8+DA 0.269* 57.6 %
The value marked with the asterisk indicates statistically sig-
nificant improvement over the baselines, where p G 0.05
bootstrap test.
</table>
<tableCaption confidence="0.921458">
Table 3: Suggestion performance on the CLC-
FCE dataset.
</tableCaption>
<table confidence="0.998988285714286">
Settings MRR Coverage
WordNet 0.044 5.0 %
Roundtrip 0.241 53.8 %
Roundtrip (En-Ja-En) 0.188 38.8 %
WordNet+Roundtrip 0.162 53.8 %
Lang-8 0.253 68.9 %
Lang-8+DA 0.412* 68.9 %
</table>
<bodyText confidence="0.733711333333333">
The value marked with the asterisk indicates statistically sig-
nificant improvement over the baselines, except “Roundtrip”,
where p G 0.05 bootstrap test.
</bodyText>
<tableCaption confidence="0.6254855">
Table 4: Suggestion performance on the KJ cor-
pus.
</tableCaption>
<bodyText confidence="0.999923045454546">
gestions. The improvements appeared for fre-
quent confusions of Japanese ESL learners such
as “?see/watch” and “?tell/teach”.
Comparing the results of the Lang-8+DA on
both test-corpora, the domain adaptation tech-
nique worked more effectively on the KJ cor-
pus than on the CLC-FCE. This can be explained
by the fact that the style of writing of the addi-
tional data, i.e., the Lang-8 corpus, is closer to
KJ than it is to CLC-FCE. More precisely, unlike
the examination-type writing style of CLC-FCE,
the KJ corpus consists of diary writing similar in
style to the Lang-8 corpus, and it expresses more
closely the proficiency of the learners.
We think that the next step is to refine the sug-
gestion models, since we currently take a simple
fill-in-the-blank approach. As future work, we
plan to extend the models as follows: (1) use both
incorrect and correct sentences in learner corpora
for training, and (2) employ ESL writing specific
features such as learners’ L1 for domain adapta-
tion.
</bodyText>
<sectionHeader confidence="0.99549" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9994956">
We thank YangYang Xi of Lang-8, Inc. for kindly
allowing us to use the Lang-8 learner corpus. We
also thank the anonymous reviewers for their in-
sightful comments. This work was partially sup-
ported by Microsoft Research CORE Project.
</bodyText>
<sectionHeader confidence="0.998427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999549954545454">
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics, pages 597–604.
Peter F Brown, Vincent J Della Pietra, Peter V DeS-
ouza, Jenifer C Lai, Robert L Mercer, and Vincent
J Della Pietra. 1992. Class-Based n-gram Models
of Natural Language. Computational Linguistics,
18(4):467–479, December.
M Cettolo, Christian Girardi, and Marcello Federico.
2012. WITS: Web Inventory of Transcribed and
Translated Talks. In Proceedings of the 16th Con-
ference of the European Association for Machine
Translation (EAMT), pages 28–30.
Martin Chodorow, Markus Dickinson, Ross Israel, and
Joel Tetreault. 2012. Problems in Evaluating Gram-
matical Error Detection Systems. In Proceedings of
the 24th International Conference on Computational
Linguistics (Coling2012), pages 611–628.
Hal Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proceedings of the 45th Annual Meeting of
the Association of Computational Linguistics, pages
256–263.
Adriano Ferraresi, Eros Zanchetta, and Marco Baroni.
2008. Introducing and evaluating ukWaC, a very
large web-derived corpus of English. In Proceed-
ings of the 4th Web as Corpus Workshop (WAC-4),
pages 45–54.
Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and
Hitoshi Nishikawa. 2012. Grammar error correc-
tion using pseudo-error sentences and domain adap-
tation. In Proceedings of the 50th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 388–392.
Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun
Stiller, and Ming Zhou. 2010. SRL-based verb se-
lection for ESL. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, pages 1068–1076.
Xiaohua Liu, Bo Han, and Ming Zhou. 2011. Cor-
recting verb selection errors for ESL with the per-
ceptron. In 12th International Conference on Intel-
ligent Text Processing and Computational Linguis-
tics, pages 411–423.
</reference>
<page confidence="0.969308">
712
</page>
<reference confidence="0.999279227272727">
Tomoya Mizumoto, Mamoru Komachi, Masaaki Na-
gata, and Yuji Matsumoto. 2011. Mining revi-
sion log of language learning SNS for automated
Japanese error correction of second language learn-
ers. In Proceedings of the 5th International Joint
Conference on Natural Language Processing, pages
147–155.
Alla Rozovskaya and Dan Roth. 2011. Algorithm
selection and model adaptation for ESL correction
tasks. In Proceedings of the 49th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 924–933.
Tong Wang and Graeme Hirst. 2010. Near-synonym
lexical choice in latent semantic space. In Proceed-
ings of the 23rd International Conference on Com-
putational Linguistics (Coling 2010), pages 1182–
1190.
Jian-Cheng Wu, Yu-Chia Chang, Teruko Mitamura,
and Jason S Chang. 2010. Automatic collocation
suggestion in academic writing. In Proceedings of
the 48th Annual Meeting of the Association for Com-
putational Linguistics Short Papers, pages 115–119.
</reference>
<page confidence="0.998985">
713
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951933">
<title confidence="0.999768">A Learner Corpus-based Approach to Verb Suggestion for ESL</title>
<author confidence="0.986195">Sawai Mamoru Matsumoto</author>
<affiliation confidence="0.999326">Graduate School of Information Science Nara Institute of Science and Technology</affiliation>
<address confidence="0.99981">8916-5 Takayama, Ikoma, Nara, 630-0192, Japan</address>
<email confidence="0.992131">komachi,</email>
<abstract confidence="0.998285666666666">We propose a verb suggestion method which uses candidate sets and domain adaptation to incorporate error patterns produced by ESL learners. The candidate sets are constructed from a large scale learner corpus to cover various error patterns made by learners. Furthermore, the model is trained using both a native corpus and the learner corpus via a domain adaptation technique. Experiments on two learner corpora show that the candidate sets increase the coverage of error patterns and domain adaptation improves the performance for verb suggestion.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="11748" citStr="Bannard and Callison-Burch, 2005" startWordPosition="1911" endWordPosition="1914">y skewed to correct usages, though this setting expresses well the reality of ESL writing, as shown in (Chodorow et al., 2012). We carried out experiments with a variety of resources used for creating candidate sets. • WordNet Candidates are retrieved from the synsets and verbs sharing the same hypernyms in the WordNet 3.0. • LearnerSmall Candidates are retrieved from following learner corpora: NUS corpus of learner English (NUCLE), Konan-JIEM (KJ), and NICT Japanese learner English (JLE) corpus. • Roundtrip Candidates are collected by performing “round-trip” translation, which is similar to (Bannard and Callison-Burch, 2005) 10. • WordNet+Roundtrip A combination of the thesaurus-based and the translation table-based candidate sets, similar to (Liu et al., 2010) and (Liu et al., 2011). • Lang-8 The proposed candidate sets obtained from a large scale learner corpus. • Lang-8+DA Lang-8 candidate sets with domain adapta10Our roundtrip translation lexicons are built using a subset of the WIT3 corpus (Cettolo et al., 2012), which is available at http://wit3.fbk.eu. 710 Settings Candidates/set (Avg.) WordNet 14.8 LearnerSmall 5.1 Roundtrip 50 Roundtrip (En-Ja-En) 50 WordNet+Roundtrip 50 Lang-8 20.3 Table 2: Comparison o</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V DeSouza</author>
<author>Jenifer C Lai</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
</authors>
<title>Class-Based n-gram Models of Natural Language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="9467" citStr="Brown et al., 1992" startWordPosition="1552" endWordPosition="1555">g features of these head words. An example of extracted features is shown in Table 2.4. Note that those features are also used when extracting examples from the target domain dataset (the learner domain corpus). As shown in Table 2.4, the n-gram features are 3-gram and extracted from ±2 context window. The nearest NP’s head features are divided into two (Left, Right). The additional clustering features are used for reducing sparseness, because the NP’s head words are usually proper nouns. To create the word clusters, we employ Brown clustering, a hierarchical clustering algorithm proposed by (Brown et al., 1992). The structure of clusters is a complete binary tree, in which each node is represented as a bit-string. By varying the length of the prefix of bit-string, it is possible to change the granularity of cluster representation. As illustrated in Table 2.4, we use the clustering features with three levels of granularity: 256, 128, and 64 dimensions. We used Percy Liang’s implementation9 to create 256 dimensional model from the ukWaC corpus, which is used as the native corpus. 3 Experiments Performance of verb suggestion is evaluated on two error-tagged learner corpora: CLC-FCE and KJ. In the exper</context>
</contexts>
<marker>Brown, Pietra, DeSouza, Lai, Mercer, Pietra, 1992</marker>
<rawString>Peter F Brown, Vincent J Della Pietra, Peter V DeSouza, Jenifer C Lai, Robert L Mercer, and Vincent J Della Pietra. 1992. Class-Based n-gram Models of Natural Language. Computational Linguistics, 18(4):467–479, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Cettolo</author>
<author>Christian Girardi</author>
<author>Marcello Federico</author>
</authors>
<title>WITS: Web Inventory of Transcribed and Translated Talks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT),</booktitle>
<pages>28--30</pages>
<contexts>
<context position="12148" citStr="Cettolo et al., 2012" startWordPosition="1974" endWordPosition="1977">learner English (NUCLE), Konan-JIEM (KJ), and NICT Japanese learner English (JLE) corpus. • Roundtrip Candidates are collected by performing “round-trip” translation, which is similar to (Bannard and Callison-Burch, 2005) 10. • WordNet+Roundtrip A combination of the thesaurus-based and the translation table-based candidate sets, similar to (Liu et al., 2010) and (Liu et al., 2011). • Lang-8 The proposed candidate sets obtained from a large scale learner corpus. • Lang-8+DA Lang-8 candidate sets with domain adapta10Our roundtrip translation lexicons are built using a subset of the WIT3 corpus (Cettolo et al., 2012), which is available at http://wit3.fbk.eu. 710 Settings Candidates/set (Avg.) WordNet 14.8 LearnerSmall 5.1 Roundtrip 50 Roundtrip (En-Ja-En) 50 WordNet+Roundtrip 50 Lang-8 20.3 Table 2: Comparison of candidate set size for each setting. tion via feature augmentation. Table 3 shows a comparison of the average number of candidates in each setting. In all configurations above, the parameters of the models underlying the system are identical. We used a L2- regularized generalized linear model with log-loss function via Scikit-learn ver. 0.13. Inter-corpus Evaluation We also evaluate the suggesti</context>
</contexts>
<marker>Cettolo, Girardi, Federico, 2012</marker>
<rawString>M Cettolo, Christian Girardi, and Marcello Federico. 2012. WITS: Web Inventory of Transcribed and Translated Talks. In Proceedings of the 16th Conference of the European Association for Machine Translation (EAMT), pages 28–30.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Chodorow</author>
<author>Markus Dickinson</author>
<author>Ross Israel</author>
<author>Joel Tetreault</author>
</authors>
<title>Problems in Evaluating Grammatical Error Detection Systems.</title>
<date>2012</date>
<booktitle>In Proceedings of the 24th International Conference on Computational Linguistics (Coling2012),</booktitle>
<pages>611--628</pages>
<contexts>
<context position="11241" citStr="Chodorow et al., 2012" startWordPosition="1837" endWordPosition="1840">o their jobs with the help of computers.) “&lt;S&gt;” denotes the beginning of the sentence, “*V*” denotes the blanked out verb. Table 1: Example of extracted features as the fillin-the-blank form. placement error of verbs). We assume that all the verb selection errors are covered with this error tag. All error tagged parts with nested correction or multi-word expressions are excluded. The resulting number of “true” targets is 1,083, which amounts to 4% of all verbs. Therefore the dataset is highly skewed to correct usages, though this setting expresses well the reality of ESL writing, as shown in (Chodorow et al., 2012). We carried out experiments with a variety of resources used for creating candidate sets. • WordNet Candidates are retrieved from the synsets and verbs sharing the same hypernyms in the WordNet 3.0. • LearnerSmall Candidates are retrieved from following learner corpora: NUS corpus of learner English (NUCLE), Konan-JIEM (KJ), and NICT Japanese learner English (JLE) corpus. • Roundtrip Candidates are collected by performing “round-trip” translation, which is similar to (Bannard and Callison-Burch, 2005) 10. • WordNet+Roundtrip A combination of the thesaurus-based and the translation table-based</context>
</contexts>
<marker>Chodorow, Dickinson, Israel, Tetreault, 2012</marker>
<rawString>Martin Chodorow, Markus Dickinson, Ross Israel, and Joel Tetreault. 2012. Problems in Evaluating Grammatical Error Detection Systems. In Proceedings of the 24th International Conference on Computational Linguistics (Coling2012), pages 611–628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hal Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hal Daum´e III. 2007. Frustratingly easy domain adaptation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adriano Ferraresi</author>
<author>Eros Zanchetta</author>
<author>Marco Baroni</author>
</authors>
<title>Introducing and evaluating ukWaC, a very large web-derived corpus of English.</title>
<date>2008</date>
<booktitle>In Proceedings of the 4th Web as Corpus Workshop (WAC-4),</booktitle>
<pages>45--54</pages>
<contexts>
<context position="13382" citStr="Ferraresi et al., 2008" startWordPosition="2162" endWordPosition="2165">nce on the KJ corpus. The corpus contains diary-style writing by Japanese university students. The proficiency of the learners ranges from elementary to intermediate, so it is lower than that of the CLCFCE learners. The targets are all verbs tagged with “v lxc” (lexical selection error of verbs). To see the effect of L1 on the verb suggestion task, we added an alternative setting for the Roundtrip using only the English-Japanese and Japanese-English round-trip translation tables (En-Ja-En). For the experiment on this testcorpus, the LearnerSmall is not included. Datasets The ukWaC web-corpus (Ferraresi et al., 2008) is used as a native corpus for training the suggestion model. Although this corpus consists of over 40 million sentences, 20,000 randomly selected sentences are used for each verb11. The Lang-8 learner corpus is used for domain adaptation of the model in the Lang-8+DA configuration. The portion of data is the same as that used for constructing candidate sets. Metrics Mean Reciprocal Rank (MRR) is used for evaluating the performance of alternative suggestions. The mean reciprocal rank is calculated by taking 11e.g., a classifier with a candidate set containing 50 verbs is trained with 1 millio</context>
</contexts>
<marker>Ferraresi, Zanchetta, Baroni, 2008</marker>
<rawString>Adriano Ferraresi, Eros Zanchetta, and Marco Baroni. 2008. Introducing and evaluating ukWaC, a very large web-derived corpus of English. In Proceedings of the 4th Web as Corpus Workshop (WAC-4), pages 45–54.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Imamura</author>
<author>Kuniko Saito</author>
<author>Kugatsu Sadamitsu</author>
<author>Hitoshi Nishikawa</author>
</authors>
<title>Grammar error correction using pseudo-error sentences and domain adaptation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>388--392</pages>
<contexts>
<context position="7312" citStr="Imamura et al., 2012" startWordPosition="1183" endWordPosition="1186">ared to native corpora. It is then adapted to the target domain, i.e., learner writing. In our experiment, the Lang8 corpus is used as the target domain corpus, since we assume that it shares the same characteristics with the CLC-FCE and the KJ corpora used for testing. 2.3 Domain Adaptation To adapt the models to the learner corpus, we employ a domain adaptation technique to emphasize the importance of learner domain information. Although there are many studies on domain adaptation, we chose to use Feature Augmentation technique introduced by (Daum´e III, 2007) for its simplicity. Recently, (Imamura et al., 2012) proposed to apply this method to grammatical error correction for writings of Japanese learners and confirmed that this is more effective for correcting learner errors than simply adding the target domain instances. In this study, the source domain is the native writing, and the target domain is the ESL writing. Our motivation is to use the ESL corpus together with the huge native corpus to employ both an advantage of the size of training data and the ESL writing specific features. In this method, adapting a model to another model is achieved by extending the feature space. Given a feature ve</context>
</contexts>
<marker>Imamura, Saito, Sadamitsu, Nishikawa, 2012</marker>
<rawString>Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, and Hitoshi Nishikawa. 2012. Grammar error correction using pseudo-error sentences and domain adaptation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 388–392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Bo Han</author>
<author>Kuan Li</author>
<author>Stephan Hyeonjun Stiller</author>
<author>Ming Zhou</author>
</authors>
<title>SRL-based verb selection for ESL.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1068--1076</pages>
<contexts>
<context position="2016" citStr="Liu et al., 2010" startWordPosition="318" endWordPosition="321"> means “forming a relationship (with other businessmen)”, whereas “communicate” means “exchanging information or ideas”, which is what the sentence is trying to convey. *Now at Tokyo Metropolitan University. 1For example, in the CLC-FCE dataset, the replacement error of verbs is the third most common out of 75 error types. In the KJ corpus, lexical choice of verb is the sixth most common out of 47 error types. 2This sentence is taken from the CLC-FCE dataset. Previous work on verb selection usually treats the task as a multi-class classification problem (Wu et al., 2010; Wang and Hirst, 2010; Liu et al., 2010; Liu et al., 2011). In this formalization, it is important to restrict verbs by a candidate set because verb vocabulary is more numerous than other classes, such as determiners. Candidate sets for verb selection are often extracted from thesauri and/or round-trip translations. However, these resources may not cover certain error patterns found in actual learner corpora, and suffer from low-coverage. Furthermore, all the existing classifier models are trained only using a native corpus, which may not be adequate for correcting learner errors. In this paper, we propose to use error patterns in </context>
<context position="11887" citStr="Liu et al., 2010" startWordPosition="1931" endWordPosition="1934">with a variety of resources used for creating candidate sets. • WordNet Candidates are retrieved from the synsets and verbs sharing the same hypernyms in the WordNet 3.0. • LearnerSmall Candidates are retrieved from following learner corpora: NUS corpus of learner English (NUCLE), Konan-JIEM (KJ), and NICT Japanese learner English (JLE) corpus. • Roundtrip Candidates are collected by performing “round-trip” translation, which is similar to (Bannard and Callison-Burch, 2005) 10. • WordNet+Roundtrip A combination of the thesaurus-based and the translation table-based candidate sets, similar to (Liu et al., 2010) and (Liu et al., 2011). • Lang-8 The proposed candidate sets obtained from a large scale learner corpus. • Lang-8+DA Lang-8 candidate sets with domain adapta10Our roundtrip translation lexicons are built using a subset of the WIT3 corpus (Cettolo et al., 2012), which is available at http://wit3.fbk.eu. 710 Settings Candidates/set (Avg.) WordNet 14.8 LearnerSmall 5.1 Roundtrip 50 Roundtrip (En-Ja-En) 50 WordNet+Roundtrip 50 Lang-8 20.3 Table 2: Comparison of candidate set size for each setting. tion via feature augmentation. Table 3 shows a comparison of the average number of candidates in eac</context>
</contexts>
<marker>Liu, Han, Li, Stiller, Zhou, 2010</marker>
<rawString>Xiaohua Liu, Bo Han, Kuan Li, Stephan Hyeonjun Stiller, and Ming Zhou. 2010. SRL-based verb selection for ESL. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1068–1076.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohua Liu</author>
<author>Bo Han</author>
<author>Ming Zhou</author>
</authors>
<title>Correcting verb selection errors for ESL with the perceptron.</title>
<date>2011</date>
<booktitle>In 12th International Conference on Intelligent Text Processing and Computational Linguistics,</booktitle>
<pages>411--423</pages>
<contexts>
<context position="2035" citStr="Liu et al., 2011" startWordPosition="322" endWordPosition="325">relationship (with other businessmen)”, whereas “communicate” means “exchanging information or ideas”, which is what the sentence is trying to convey. *Now at Tokyo Metropolitan University. 1For example, in the CLC-FCE dataset, the replacement error of verbs is the third most common out of 75 error types. In the KJ corpus, lexical choice of verb is the sixth most common out of 47 error types. 2This sentence is taken from the CLC-FCE dataset. Previous work on verb selection usually treats the task as a multi-class classification problem (Wu et al., 2010; Wang and Hirst, 2010; Liu et al., 2010; Liu et al., 2011). In this formalization, it is important to restrict verbs by a candidate set because verb vocabulary is more numerous than other classes, such as determiners. Candidate sets for verb selection are often extracted from thesauri and/or round-trip translations. However, these resources may not cover certain error patterns found in actual learner corpora, and suffer from low-coverage. Furthermore, all the existing classifier models are trained only using a native corpus, which may not be adequate for correcting learner errors. In this paper, we propose to use error patterns in ESL writing for ver</context>
<context position="8496" citStr="Liu et al., 2011" startWordPosition="1392" endWordPosition="1395">feature space. Given a feature vector of F dimensions as x E lRF(F &gt; 0), using simple mapping, the augmented feature vectors for source and target domains are obtained as follows, Source domain: &lt; xS, xS, 0 &gt; (1) Target domain: &lt; xT, 0, xT &gt; (2) 709 where 0 denotes a zero-vector of F dimensions. The three partitions mean a common, a sourcespecific, and a target-specific feature space. When testing on the ESL corpora, the target-specific features are emphasized. 2.4 Features In previous work, various features were used: lexical and POS n-grams, dependencies, and arguments in the verb context. (Liu et al., 2011) has shown that shallow parse features, such as lexical n-grams and chunks, work well in realistic settings, in which the input sentence may not be correctly parsed. Considering this, we use shallow parse features as context features for robustness. The features include lexical and POS n-grams, and lexical head words of the nearest NPs, and clustering features of these head words. An example of extracted features is shown in Table 2.4. Note that those features are also used when extracting examples from the target domain dataset (the learner domain corpus). As shown in Table 2.4, the n-gram fe</context>
<context position="11910" citStr="Liu et al., 2011" startWordPosition="1936" endWordPosition="1939">rces used for creating candidate sets. • WordNet Candidates are retrieved from the synsets and verbs sharing the same hypernyms in the WordNet 3.0. • LearnerSmall Candidates are retrieved from following learner corpora: NUS corpus of learner English (NUCLE), Konan-JIEM (KJ), and NICT Japanese learner English (JLE) corpus. • Roundtrip Candidates are collected by performing “round-trip” translation, which is similar to (Bannard and Callison-Burch, 2005) 10. • WordNet+Roundtrip A combination of the thesaurus-based and the translation table-based candidate sets, similar to (Liu et al., 2010) and (Liu et al., 2011). • Lang-8 The proposed candidate sets obtained from a large scale learner corpus. • Lang-8+DA Lang-8 candidate sets with domain adapta10Our roundtrip translation lexicons are built using a subset of the WIT3 corpus (Cettolo et al., 2012), which is available at http://wit3.fbk.eu. 710 Settings Candidates/set (Avg.) WordNet 14.8 LearnerSmall 5.1 Roundtrip 50 Roundtrip (En-Ja-En) 50 WordNet+Roundtrip 50 Lang-8 20.3 Table 2: Comparison of candidate set size for each setting. tion via feature augmentation. Table 3 shows a comparison of the average number of candidates in each setting. In all confi</context>
</contexts>
<marker>Liu, Han, Zhou, 2011</marker>
<rawString>Xiaohua Liu, Bo Han, and Ming Zhou. 2011. Correcting verb selection errors for ESL with the perceptron. In 12th International Conference on Intelligent Text Processing and Computational Linguistics, pages 411–423.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomoya Mizumoto</author>
<author>Mamoru Komachi</author>
<author>Masaaki Nagata</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Mining revision log of language learning SNS for automated Japanese error correction of second language learners.</title>
<date>2011</date>
<booktitle>In Proceedings of the 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>147--155</pages>
<contexts>
<context position="4372" citStr="Mizumoto et al., 2011" startWordPosition="691" endWordPosition="694">omputational Linguistics 2 Verb Suggestion Considering Error Patterns The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya and Roth, 2011; Wu et al., 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. 2.1 Candidate Sets For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83. An advantage of using the learner corpus from such website is the size of annotated portion (Mizumoto et al., 2011). This SNS has over 1 million manually annotated English sentences written by ESL learners. We have collected the learner writings on the site, and released the dataset for research purpose4. First, we performed POS tagging for the dataset using the treebank POS tagger in the NLTK toolkit 2.10. Second, we extracted the correction pairs which have “VB*” tag. The set of correction pairs given an incorrect verb is considered as a candidate set for the verb. We then performed the following preprocessing for the dataset because we focus on lexical selection of verbs: • Lemmatize verbs to reduce dat</context>
</contexts>
<marker>Mizumoto, Komachi, Nagata, Matsumoto, 2011</marker>
<rawString>Tomoya Mizumoto, Mamoru Komachi, Masaaki Nagata, and Yuji Matsumoto. 2011. Mining revision log of language learning SNS for automated Japanese error correction of second language learners. In Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alla Rozovskaya</author>
<author>Dan Roth</author>
</authors>
<title>Algorithm selection and model adaptation for ESL correction tasks.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>924--933</pages>
<contexts>
<context position="3929" citStr="Rozovskaya and Roth, 2011" startWordPosition="618" endWordPosition="621">rip translation table. Domain adaptation also boosts the suggestion performance. To our knowledge, this is the first work for verb suggestion that uses (1) a learner corpus as a source of candidate sets and (2) the domain adaptation technique to take learner errors into account. 708 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 708–713, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Verb Suggestion Considering Error Patterns The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya and Roth, 2011; Wu et al., 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. 2.1 Candidate Sets For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83. An advantage of using the learner corpus from such website is the size of annotated portion (Mizumoto et al., 2011). This SNS has over 1 million manually annotated English sentences written by ESL learners. We have collected the learner writings on the site, and released </context>
</contexts>
<marker>Rozovskaya, Roth, 2011</marker>
<rawString>Alla Rozovskaya and Dan Roth. 2011. Algorithm selection and model adaptation for ESL correction tasks. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 924–933.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Wang</author>
<author>Graeme Hirst</author>
</authors>
<title>Near-synonym lexical choice in latent semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics (Coling</booktitle>
<pages>1182--1190</pages>
<contexts>
<context position="1998" citStr="Wang and Hirst, 2010" startWordPosition="314" endWordPosition="317">y use “connect”, which means “forming a relationship (with other businessmen)”, whereas “communicate” means “exchanging information or ideas”, which is what the sentence is trying to convey. *Now at Tokyo Metropolitan University. 1For example, in the CLC-FCE dataset, the replacement error of verbs is the third most common out of 75 error types. In the KJ corpus, lexical choice of verb is the sixth most common out of 47 error types. 2This sentence is taken from the CLC-FCE dataset. Previous work on verb selection usually treats the task as a multi-class classification problem (Wu et al., 2010; Wang and Hirst, 2010; Liu et al., 2010; Liu et al., 2011). In this formalization, it is important to restrict verbs by a candidate set because verb vocabulary is more numerous than other classes, such as determiners. Candidate sets for verb selection are often extracted from thesauri and/or round-trip translations. However, these resources may not cover certain error patterns found in actual learner corpora, and suffer from low-coverage. Furthermore, all the existing classifier models are trained only using a native corpus, which may not be adequate for correcting learner errors. In this paper, we propose to use </context>
</contexts>
<marker>Wang, Hirst, 2010</marker>
<rawString>Tong Wang and Graeme Hirst. 2010. Near-synonym lexical choice in latent semantic space. In Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1182– 1190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian-Cheng Wu</author>
<author>Yu-Chia Chang</author>
<author>Teruko Mitamura</author>
<author>Jason S Chang</author>
</authors>
<title>Automatic collocation suggestion in academic writing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics Short Papers,</booktitle>
<pages>115--119</pages>
<contexts>
<context position="1976" citStr="Wu et al., 2010" startWordPosition="310" endWordPosition="313"> would less likely use “connect”, which means “forming a relationship (with other businessmen)”, whereas “communicate” means “exchanging information or ideas”, which is what the sentence is trying to convey. *Now at Tokyo Metropolitan University. 1For example, in the CLC-FCE dataset, the replacement error of verbs is the third most common out of 75 error types. In the KJ corpus, lexical choice of verb is the sixth most common out of 47 error types. 2This sentence is taken from the CLC-FCE dataset. Previous work on verb selection usually treats the task as a multi-class classification problem (Wu et al., 2010; Wang and Hirst, 2010; Liu et al., 2010; Liu et al., 2011). In this formalization, it is important to restrict verbs by a candidate set because verb vocabulary is more numerous than other classes, such as determiners. Candidate sets for verb selection are often extracted from thesauri and/or round-trip translations. However, these resources may not cover certain error patterns found in actual learner corpora, and suffer from low-coverage. Furthermore, all the existing classifier models are trained only using a native corpus, which may not be adequate for correcting learner errors. In this pap</context>
<context position="3947" citStr="Wu et al., 2010" startWordPosition="622" endWordPosition="625">in adaptation also boosts the suggestion performance. To our knowledge, this is the first work for verb suggestion that uses (1) a learner corpus as a source of candidate sets and (2) the domain adaptation technique to take learner errors into account. 708 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 708–713, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics 2 Verb Suggestion Considering Error Patterns The proposed verb suggestion system follows the standard approach in related tasks (Rozovskaya and Roth, 2011; Wu et al., 2010), where the candidate selection is formalized as a multi-class classification problem with predefined candidate sets. 2.1 Candidate Sets For reflecting tendency of learner errors to the candidate sets, we use a large scale corpus obtained from learners’ writing on an SNS (Social Networking Service), Lang-83. An advantage of using the learner corpus from such website is the size of annotated portion (Mizumoto et al., 2011). This SNS has over 1 million manually annotated English sentences written by ESL learners. We have collected the learner writings on the site, and released the dataset for re</context>
</contexts>
<marker>Wu, Chang, Mitamura, Chang, 2010</marker>
<rawString>Jian-Cheng Wu, Yu-Chia Chang, Teruko Mitamura, and Jason S Chang. 2010. Automatic collocation suggestion in academic writing. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics Short Papers, pages 115–119.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>