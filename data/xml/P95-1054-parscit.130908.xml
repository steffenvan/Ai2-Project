<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.071685">
<title confidence="0.9940945">
Quantifying lexical influence:
Giving direction to context
</title>
<author confidence="0.75761">
V Kripasundar
</author>
<affiliation confidence="0.516804">
kripacs.buffalo.edu
</affiliation>
<address confidence="0.735221666666667">
CEDAR 86 Dept. of Computer Science
SUNY at Buffalo
Buffalo NY 14260, USA
</address>
<sectionHeader confidence="0.926899" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998106066666667">
The relevance of context in disambiguat-
ing natural language input has been widely
acknowledged in the literature. However,
most attempts at formalising the intuitive
notion of context tend to treat the word and
its context symmetrically. We demonstrate
here that traditional measures such as mu-
tual information score are likely to overlook
a significant fraction of all co-occurrence
phenomena in natural language. We also
propose metrics for measuring directed lex-
ical influence and compare performances.
Keywords: contextual post-processing,
defining context, lexical influence, direc-
tionality of context
</bodyText>
<sectionHeader confidence="0.99952" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999928558823529">
It is widely accepted that context plays a significant
role in shaping all aspects of language. Indeed, com-
prehension would be utterly impossible without the
extensive application of contextual information. Ev-
idence from psycholinguistic and cognitive psycho-
logical studies also demonstrates that contextual in-
formation affects the activation levels of lexical can-
didates during the process of perception (Weinreich,
1980; McClelland, 1987). Garvin (1972) describes
the role of context as follows:
[The meaning of] a particular text [is] not
the system-derived meaning as a whole, but
that part of it which is included in the con-
textually and situationally derived mean-
ing proper to the text in question. (p. 69-
70)
In effect, this means that the context of a word serves
to restrict its sense.
The problem addressed in this research is that
of improving the performance of a natural-language
recogniser (such as a recognition system for hand-
written or spoken language). The recogniser out-
put typically consists of an ordered set of candidate
words (word-choices) for each word position in the
input stream. Since natural language abounds in
contextual information, it is reasonable to utilise this
in improving the performance of the recogniser (by
disambiguating among the word-choices).
The word-choices (together with their confidence
values) constitute a confusion set. The recogniser
may further associate a confidence-value with each of
its word choices to communicate finer resolution in
its output. The language module must update these
confidence values to reflect contextual knowledge.
</bodyText>
<sectionHeader confidence="0.972792" genericHeader="method">
2 Linguistic post-processing
</sectionHeader>
<bodyText confidence="0.9915786">
The language module can, in principle, perform
several types of &amp;quot;post-processing&amp;quot; on the word-
candidate lists that the recogniser outputs for the
different word-positions. The most promising possi-
bilities are:
</bodyText>
<listItem confidence="0.990219666666667">
• re-ranking the confusion set (and assigning new
confidence-values to its entries), and,
• deleting low-confidence entries from the confu-
</listItem>
<bodyText confidence="0.995200318181818">
sion set (after applying contextual knowledge)
Several researchers in NLP have acknowledged the
relevance of context in disambiguating natural lan-
guage input ((Evett et al., 1991); (Zernik, 1991);
(Hindle &amp; Rooth, 1993); (Rosenfeld, 1994)). In fact,
the recent revival of interest in statistical language
processing is partly because of its (comparative) suc-
cess in modelling context. However, a theoretically
sound definition of context is needed to ensure that
such re-ranking and deleting of word-choices helps
and not hinders (Gale &amp; Church, 1990).
Researchers in information theory have come up
with many inter-related formalisations of the ideas of
context and contextual influence, such as mutual in-
formation and joint entropy. However, to our knowl-
edge, all attempts at arriving at a theoretical basis
for formalising the intuitive notion of context have
treated the word and its context symmetrically.
Many researchers ((Smadja, 1991); (rihari &amp; Bal-
tus, 1993)) have suggested that the information-
theoretic notion of mutual information score (MIS)
directly captures the idea of context. However, MIS
</bodyText>
<page confidence="0.991503">
332
</page>
<bodyText confidence="0.999830411764706">
is deficient in its ability to detect one-sided correla-
tions (ef. Table 1), and our research indicates that
asymmetric influence measures are required to prop-
erly handle them (Kripesundar, 1994).
For example, it seems quite unlikely that any
symmetric information measure can accurately cap-
ture the co-occurrence relationship between the two
words &apos;Paleolithic&apos; and &apos;age&apos; in the phrase &apos;Pale-
olithic age&apos;. The suggestion that &apos;age&apos; exerts as much
influence on &apos;Paleolithic&apos; as vice versa seems ridicu-
lous, to say the least. What is needed here is a di-
rected (ie, one-sided) influence measure (DIM), some-
thing that serves as a measure of influence of one
word on another, rather than as a simple, symmet-
ric, &amp;quot;co-existence probability&amp;quot; of two words. Table 1
illustrates how a DIM can be effective in detecting
lexical and lexico-semantic associations.
</bodyText>
<sectionHeader confidence="0.7704315" genericHeader="method">
3 Comparing measures of lexical
influence
</sectionHeader>
<bodyText confidence="0.999778962962963">
We used a section of the Wall Street Journal (WSJ)
corpus containing 102K sentences (over two million
words) as the training corpus for the partial results
described here. The lexicon used was a simple 30K-
word superset of the vocabulary of the training cor-
pus.
The results shown here serve to strengthen our
hypothesis that non-standard information measures
are needed for the proper utilisation of linguistic
context. Table 1 shows some pairs of words that
exhibit differing degrees of influence on each other.
It also demonstrates very effectively that one-sided
information measures are much better than sym-
metric measures at utilising context properly. The
arrow between each pair of words in the table in-
dicates the direction of influence (or flow of infor-
mation). The preponderance of word-pairs that ex-
hibit only one direction of significant influence (eg,
&apos;according&apos;--).`to&apos;) shows that no symmetric score
could have captured the correlations in all of these
phrases.
Our formulation of directed influence is still evolv-
ing. The word-pairs in Table 1 have been selected
randomly from the test-set with the criterion that
they scored &amp;quot;significantly&amp;quot; (ie, &gt; 0.9) on at least
one of the three measures D1, D2 and D3. The four
measures (including MIS) are defined as follows:
</bodyText>
<equation confidence="0.9985985">
MIS(wi w2) = log(pr,„(wolp7,22))
Dl(wi/w2)
D2(wi/w2)
D3(wi/w2)
</equation>
<bodyText confidence="0.994629333333333">
In these definitions, #wiw2 denotes the frequency
of co-occurrence of the words wi and w2,1 while
&apos;Note that the exact word order of wi and w2 is ir-
relevant here.
#wi, and #w2 represent (respectively) the frequen-
cies of their (unconditional) occurrence.
</bodyText>
<equation confidence="0.839306333333333">
def
#Cmax = max(#wiw2) is defined to be the
Wj tv2
</equation>
<bodyText confidence="0.999939115384616">
maximum co-occurrence frequency in the corpus,
and appears to be a better normalisation factor than
the size of the corpus itself.
The definition of MIS implicitly incorporates the
size of the corpus, since it has two PO terms in the
denominator, and only one in the numerator. The
DIM&apos;s, on the other hand, have balanced fractions.
Therefore, we have not included a log-term in the
definitions of D1, D2, and D3 above.
D1 is a straightforward estimation of the condi-
tional probability of co-occurrence. It forms a base-
line for performance evaluations, but is prone to
sparse data problems (Dunning, 1993).
The step() functions in D2 and D3 represent two
attempts at minimising such errors. These functions
are piecewise-linear mappings of the normalised co-
occurrence frequency, and are used as scaling factors.
Their effect is apparent in Table 1, especially in the
bottom third of the table, where the low frequency
of the primer pushes D3 down to insignificant levels.
The metrics D2 and D3 can and should be nor-
malised, perhaps to the 0-1 range, in order to fa-
cilitate integration with other metrics such as the
recogniser&apos;s confidence value. Similarly, the lack of
normalisation of MIS hampers direct comparison of
scores with the three DIM&apos;s.
</bodyText>
<sectionHeader confidence="0.998444" genericHeader="evaluation">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999958636363636">
Of the several different types of word-level associ-
ations, lexical and lexico-semantic associations are
among the most significant local associations. Lexi-
cal (or associative) context is characterised by rigid
word order, and usually implies that the primer and
the primed together act as one lexical unit. Lexico-
semantic associations are exemplified by phrasal verbs
(eg, &apos;fix up&apos;), and are characterised by morphological
complexity in the verb part and spatial flexibility in
the phrase as a whole.
It is noteworthy that all the three DIM&apos;s capture
the notions of lexical (ie, fixed) and lexico-semantic
associations in one formula (albeit to differing de-
grees of success). Thus we have &apos;staff&apos; and &apos;re-
porter&apos; influencing each other almost equally, while
the asymmetric influence on &apos;in&apos; from its right con-
text (&apos;addition&apos;) is also detected by the DIM&apos;s.
It is our contention that symmetric measures
constrain the re-ranking/proposing process signifi-
cantly, since they are essentially blind to a signif-
icant fraction (perhaps more than half) of all co-
occurrence phenomena in natural language.
</bodyText>
<sectionHeader confidence="0.795591" genericHeader="conclusions">
5 Summary and Future Work
</sectionHeader>
<bodyText confidence="0.960368">
The preliminary results described in this work es-
tablish clearly that non-standard metrics of lexical
</bodyText>
<equation confidence="0.9929958">
P(wiw2) _ #tviw2
P(w2) #w2
step1(#c&amp;quot;&amp;quot;) x DI
max
step2(#cffil a.) x DI
</equation>
<page confidence="0.996223">
333
</page>
<table confidence="0.9489213">
Word-pair WL WR (#WL)#WR)#WLWR)- MIS D1 D2 D3
new 4- york (6927, 2697, 2338) 5.551 0.866 3.463 3.463
according --* to (1084, 54580, 1083) 3.629 0.999 2.996 2.996
staff 4-- reporter (1613, 1205, 1157) 7.111 0.960 2.879 2.879
staff ---&gt; reporter (1613, 1205, 1157) 7.111 0.717 2.150 2.150
new ---&gt; york (6927, 2697, 2338) 5.551 0.337 1.348 1.348
on --&gt; the (13025, 116356, 3483) 1.554 0.267 1.334 1.334
vice ---&gt; president (1017, 2678, 784) 6.384 0.770 1.540 1.285
at 4- least (11158, 795, 665) 5.039 0.836 1.671 1.247
compared ---&gt; with (585, 11362, 551) 5.139 0.941 1.881 1.244
</table>
<tableCaption confidence="0.998077">
Table 1: Asymmetry in co-occurrence relationships: Word-pairs with &amp;quot;significant&amp;quot; influence in either
</tableCaption>
<bodyText confidence="0.915013666666667">
direction have been selected randomly from the test-set. Note that very few of these pairs exhibit comparable
influence on each other. The arrows indicate the direction of lexical influence (or information flow). A DIM
score of 1 or more implies a significant association, whereas an MIS below 4 is considered a chance association.
influence bear much promise. In fact, what we re-
ally need is a generalised information score, a measure
that takes into account several factors, such as:
</bodyText>
<listItem confidence="0.989963">
• directionality in correlation
• multiple words participating in a lexical rela-
tionship
• different (morphological) forms of words, and,
• spatial flexibility in the components of a collo-
cation
</listItem>
<bodyText confidence="0.9996384">
The generalised information score would capture all
the variations that are introduced by the above fac-
tors, and allow for the variants so as to reflect a
&amp;quot;normalised&amp;quot; measure of contextual influence.
We have also been working with experimental
measures which attach higher significance to the
collocation frequency, (measures which, in essence,
&amp;quot;trust&amp;quot; the recogniser more often). Our future work
will involve bringing these various factors together
into one integrated formalism.
</bodyText>
<sectionHeader confidence="0.997927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99840306">
Max Coltheart, editor. 1987. Attention and Perfor-
mance XII: The Psychology of Reading. Lawrence
Erlbaum.
Ted Dunning. 1993. Accurate methods for the
statistics of surprise and coincidence. Computa-
tional Linguistics, 19:1:61-74.
LJ Evett, CJ Wells, FG Keenan, T Rose, and
RJ Whitrow. 1991. Using linguistic information
to aid handwriting recognition. Proceedings of
the International Workshop on Frontiers in Hand-
writing Recognition, pages 303-311.
William A Gale and Kenneth W Church. 1990. Poor
estimates of context are worse than none. In Pro-
ceedings of the DARPA Speech and Natural Lan-
guage Workshop, pages 283-287.
Paul L Garvin. 1972. On Machine Translation.
Mouton.
Donald Hindle and Mats Rooth. 1993. Structural
ambiguity and lexical relations. Computational
Linguistics, 19:1:103-120.
V Kripisundar. 1994. Drawing on Linguistic Con-
text to Resolve Ambiguities OR How to imrove re-
congition in noisy domains. Ph.D. thesis, Com-
puter Science, SUNY@Buffalo. (proposal).
James L McClelland. 1987. The case for interaction-
ism in language processing. In (Coltheart, 1987).
Lawrence Erlbaum.
Ronald Rosenfeld. 1994. A hybrid approach to
adaptive statistical language modeling. Proceed-
ings of the ARPA workshop on human language
technology, pages 76-81.
Frank Smadja. 1991. Macrocoding the lexicon with
co-occurrence knowledge. in (Zernik, 1991), pages
165-190.
ROhini K rihari and Charlotte M Baltus. 1993. Use
of language models in on-line recognition of hand-
written sentences. Proceedings of the Third Inter-
national Workshop on Frontiers in Handwriting
Recognition (IWFHR III).
SN rihari, JJ Hull, and R Chaudhari. 1983. In-
tegrating diverse knowledge sources in text recog-
nition. ACM Transactions on Office Information
Systems, 1:1:68-87.
RM Warren. 1970. Perceptual restoration of missing
speech sounds. Science, 167:392-393.
Uriel Weinreich. 1980. On Semantics. University of
Pennsylvania Press.
Un Zernik, editor. 1991. Lexical Acquisition: Ex-
ploiting On-line Resources to Build a Lexicon.
Lawrence Erlbaum.
</reference>
<page confidence="0.999039">
334
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.636174">
<title confidence="0.9897905">Quantifying lexical influence: Giving direction to context</title>
<author confidence="0.983747">V Kripasundar</author>
<email confidence="0.992496">kripacs.buffalo.edu</email>
<affiliation confidence="0.9636625">of Computer Science SUNY at Buffalo</affiliation>
<address confidence="0.996876">Buffalo NY 14260, USA</address>
<abstract confidence="0.999868846153846">The relevance of context in disambiguating natural language input has been widely acknowledged in the literature. However, most attempts at formalising the intuitive notion of context tend to treat the word and its context symmetrically. We demonstrate here that traditional measures such as mutual information score are likely to overlook a significant fraction of all co-occurrence phenomena in natural language. We also propose metrics for measuring directed lexical influence and compare performances.</abstract>
<keyword confidence="0.9783655">Keywords: contextual post-processing, defining context, lexical influence, direc-</keyword>
<intro confidence="0.734396">tionality of context</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<title>Attention and Performance XII: The Psychology of Reading. Lawrence Erlbaum.</title>
<date>1987</date>
<editor>Max Coltheart, editor.</editor>
<marker>1987</marker>
<rawString>Max Coltheart, editor. 1987. Attention and Performance XII: The Psychology of Reading. Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Dunning</author>
</authors>
<title>Accurate methods for the statistics of surprise and coincidence.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="7028" citStr="Dunning, 1993" startWordPosition="1081" endWordPosition="1082">e Wj tv2 maximum co-occurrence frequency in the corpus, and appears to be a better normalisation factor than the size of the corpus itself. The definition of MIS implicitly incorporates the size of the corpus, since it has two PO terms in the denominator, and only one in the numerator. The DIM&apos;s, on the other hand, have balanced fractions. Therefore, we have not included a log-term in the definitions of D1, D2, and D3 above. D1 is a straightforward estimation of the conditional probability of co-occurrence. It forms a baseline for performance evaluations, but is prone to sparse data problems (Dunning, 1993). The step() functions in D2 and D3 represent two attempts at minimising such errors. These functions are piecewise-linear mappings of the normalised cooccurrence frequency, and are used as scaling factors. Their effect is apparent in Table 1, especially in the bottom third of the table, where the low frequency of the primer pushes D3 down to insignificant levels. The metrics D2 and D3 can and should be normalised, perhaps to the 0-1 range, in order to facilitate integration with other metrics such as the recogniser&apos;s confidence value. Similarly, the lack of normalisation of MIS hampers direct</context>
</contexts>
<marker>Dunning, 1993</marker>
<rawString>Ted Dunning. 1993. Accurate methods for the statistics of surprise and coincidence. Computational Linguistics, 19:1:61-74.</rawString>
</citation>
<citation valid="true">
<authors>
<author>LJ Evett</author>
<author>CJ Wells</author>
<author>FG Keenan</author>
<author>T Rose</author>
<author>RJ Whitrow</author>
</authors>
<title>Using linguistic information to aid handwriting recognition.</title>
<date>1991</date>
<booktitle>Proceedings of the International Workshop on Frontiers in Handwriting Recognition,</booktitle>
<pages>303--311</pages>
<contexts>
<context position="2950" citStr="Evett et al., 1991" startWordPosition="430" endWordPosition="433">update these confidence values to reflect contextual knowledge. 2 Linguistic post-processing The language module can, in principle, perform several types of &amp;quot;post-processing&amp;quot; on the wordcandidate lists that the recogniser outputs for the different word-positions. The most promising possibilities are: • re-ranking the confusion set (and assigning new confidence-values to its entries), and, • deleting low-confidence entries from the confusion set (after applying contextual knowledge) Several researchers in NLP have acknowledged the relevance of context in disambiguating natural language input ((Evett et al., 1991); (Zernik, 1991); (Hindle &amp; Rooth, 1993); (Rosenfeld, 1994)). In fact, the recent revival of interest in statistical language processing is partly because of its (comparative) success in modelling context. However, a theoretically sound definition of context is needed to ensure that such re-ranking and deleting of word-choices helps and not hinders (Gale &amp; Church, 1990). Researchers in information theory have come up with many inter-related formalisations of the ideas of context and contextual influence, such as mutual information and joint entropy. However, to our knowledge, all attempts at a</context>
</contexts>
<marker>Evett, Wells, Keenan, Rose, Whitrow, 1991</marker>
<rawString>LJ Evett, CJ Wells, FG Keenan, T Rose, and RJ Whitrow. 1991. Using linguistic information to aid handwriting recognition. Proceedings of the International Workshop on Frontiers in Handwriting Recognition, pages 303-311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>Poor estimates of context are worse than none.</title>
<date>1990</date>
<booktitle>In Proceedings of the DARPA Speech and Natural Language Workshop,</booktitle>
<pages>283--287</pages>
<contexts>
<context position="3322" citStr="Gale &amp; Church, 1990" startWordPosition="486" endWordPosition="489">o its entries), and, • deleting low-confidence entries from the confusion set (after applying contextual knowledge) Several researchers in NLP have acknowledged the relevance of context in disambiguating natural language input ((Evett et al., 1991); (Zernik, 1991); (Hindle &amp; Rooth, 1993); (Rosenfeld, 1994)). In fact, the recent revival of interest in statistical language processing is partly because of its (comparative) success in modelling context. However, a theoretically sound definition of context is needed to ensure that such re-ranking and deleting of word-choices helps and not hinders (Gale &amp; Church, 1990). Researchers in information theory have come up with many inter-related formalisations of the ideas of context and contextual influence, such as mutual information and joint entropy. However, to our knowledge, all attempts at arriving at a theoretical basis for formalising the intuitive notion of context have treated the word and its context symmetrically. Many researchers ((Smadja, 1991); (rihari &amp; Baltus, 1993)) have suggested that the informationtheoretic notion of mutual information score (MIS) directly captures the idea of context. However, MIS 332 is deficient in its ability to detect o</context>
</contexts>
<marker>Gale, Church, 1990</marker>
<rawString>William A Gale and Kenneth W Church. 1990. Poor estimates of context are worse than none. In Proceedings of the DARPA Speech and Natural Language Workshop, pages 283-287.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul L Garvin</author>
</authors>
<title>On Machine Translation.</title>
<date>1972</date>
<publisher>Mouton.</publisher>
<contexts>
<context position="1252" citStr="Garvin (1972)" startWordPosition="174" endWordPosition="175">xical influence and compare performances. Keywords: contextual post-processing, defining context, lexical influence, directionality of context 1 Introduction It is widely accepted that context plays a significant role in shaping all aspects of language. Indeed, comprehension would be utterly impossible without the extensive application of contextual information. Evidence from psycholinguistic and cognitive psychological studies also demonstrates that contextual information affects the activation levels of lexical candidates during the process of perception (Weinreich, 1980; McClelland, 1987). Garvin (1972) describes the role of context as follows: [The meaning of] a particular text [is] not the system-derived meaning as a whole, but that part of it which is included in the contextually and situationally derived meaning proper to the text in question. (p. 69- 70) In effect, this means that the context of a word serves to restrict its sense. The problem addressed in this research is that of improving the performance of a natural-language recogniser (such as a recognition system for handwritten or spoken language). The recogniser output typically consists of an ordered set of candidate words (word</context>
</contexts>
<marker>Garvin, 1972</marker>
<rawString>Paul L Garvin. 1972. On Machine Translation. Mouton.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Hindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural ambiguity and lexical relations.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<pages>19--1</pages>
<contexts>
<context position="2990" citStr="Hindle &amp; Rooth, 1993" startWordPosition="436" endWordPosition="439">ect contextual knowledge. 2 Linguistic post-processing The language module can, in principle, perform several types of &amp;quot;post-processing&amp;quot; on the wordcandidate lists that the recogniser outputs for the different word-positions. The most promising possibilities are: • re-ranking the confusion set (and assigning new confidence-values to its entries), and, • deleting low-confidence entries from the confusion set (after applying contextual knowledge) Several researchers in NLP have acknowledged the relevance of context in disambiguating natural language input ((Evett et al., 1991); (Zernik, 1991); (Hindle &amp; Rooth, 1993); (Rosenfeld, 1994)). In fact, the recent revival of interest in statistical language processing is partly because of its (comparative) success in modelling context. However, a theoretically sound definition of context is needed to ensure that such re-ranking and deleting of word-choices helps and not hinders (Gale &amp; Church, 1990). Researchers in information theory have come up with many inter-related formalisations of the ideas of context and contextual influence, such as mutual information and joint entropy. However, to our knowledge, all attempts at arriving at a theoretical basis for forma</context>
</contexts>
<marker>Hindle, Rooth, 1993</marker>
<rawString>Donald Hindle and Mats Rooth. 1993. Structural ambiguity and lexical relations. Computational Linguistics, 19:1:103-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Kripisundar</author>
</authors>
<title>Drawing on Linguistic Context to Resolve Ambiguities OR How to imrove recongition in noisy domains.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer Science,</institution>
<marker>Kripisundar, 1994</marker>
<rawString>V Kripisundar. 1994. Drawing on Linguistic Context to Resolve Ambiguities OR How to imrove recongition in noisy domains. Ph.D. thesis, Computer Science, SUNY@Buffalo. (proposal).</rawString>
</citation>
<citation valid="true">
<authors>
<author>James L McClelland</author>
</authors>
<title>The case for interactionism in language processing.</title>
<date>1987</date>
<booktitle>In (Coltheart,</booktitle>
<institution>Lawrence Erlbaum.</institution>
<contexts>
<context position="1237" citStr="McClelland, 1987" startWordPosition="172" endWordPosition="173">asuring directed lexical influence and compare performances. Keywords: contextual post-processing, defining context, lexical influence, directionality of context 1 Introduction It is widely accepted that context plays a significant role in shaping all aspects of language. Indeed, comprehension would be utterly impossible without the extensive application of contextual information. Evidence from psycholinguistic and cognitive psychological studies also demonstrates that contextual information affects the activation levels of lexical candidates during the process of perception (Weinreich, 1980; McClelland, 1987). Garvin (1972) describes the role of context as follows: [The meaning of] a particular text [is] not the system-derived meaning as a whole, but that part of it which is included in the contextually and situationally derived meaning proper to the text in question. (p. 69- 70) In effect, this means that the context of a word serves to restrict its sense. The problem addressed in this research is that of improving the performance of a natural-language recogniser (such as a recognition system for handwritten or spoken language). The recogniser output typically consists of an ordered set of candid</context>
</contexts>
<marker>McClelland, 1987</marker>
<rawString>James L McClelland. 1987. The case for interactionism in language processing. In (Coltheart, 1987). Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Rosenfeld</author>
</authors>
<title>A hybrid approach to adaptive statistical language modeling.</title>
<date>1994</date>
<booktitle>Proceedings of the ARPA workshop on human language technology,</booktitle>
<pages>76--81</pages>
<contexts>
<context position="3009" citStr="Rosenfeld, 1994" startWordPosition="440" endWordPosition="441">. 2 Linguistic post-processing The language module can, in principle, perform several types of &amp;quot;post-processing&amp;quot; on the wordcandidate lists that the recogniser outputs for the different word-positions. The most promising possibilities are: • re-ranking the confusion set (and assigning new confidence-values to its entries), and, • deleting low-confidence entries from the confusion set (after applying contextual knowledge) Several researchers in NLP have acknowledged the relevance of context in disambiguating natural language input ((Evett et al., 1991); (Zernik, 1991); (Hindle &amp; Rooth, 1993); (Rosenfeld, 1994)). In fact, the recent revival of interest in statistical language processing is partly because of its (comparative) success in modelling context. However, a theoretically sound definition of context is needed to ensure that such re-ranking and deleting of word-choices helps and not hinders (Gale &amp; Church, 1990). Researchers in information theory have come up with many inter-related formalisations of the ideas of context and contextual influence, such as mutual information and joint entropy. However, to our knowledge, all attempts at arriving at a theoretical basis for formalising the intuitiv</context>
</contexts>
<marker>Rosenfeld, 1994</marker>
<rawString>Ronald Rosenfeld. 1994. A hybrid approach to adaptive statistical language modeling. Proceedings of the ARPA workshop on human language technology, pages 76-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frank Smadja</author>
</authors>
<title>Macrocoding the lexicon with co-occurrence knowledge. in (Zernik,</title>
<date>1991</date>
<pages>165--190</pages>
<contexts>
<context position="3714" citStr="Smadja, 1991" startWordPosition="547" endWordPosition="548">use of its (comparative) success in modelling context. However, a theoretically sound definition of context is needed to ensure that such re-ranking and deleting of word-choices helps and not hinders (Gale &amp; Church, 1990). Researchers in information theory have come up with many inter-related formalisations of the ideas of context and contextual influence, such as mutual information and joint entropy. However, to our knowledge, all attempts at arriving at a theoretical basis for formalising the intuitive notion of context have treated the word and its context symmetrically. Many researchers ((Smadja, 1991); (rihari &amp; Baltus, 1993)) have suggested that the informationtheoretic notion of mutual information score (MIS) directly captures the idea of context. However, MIS 332 is deficient in its ability to detect one-sided correlations (ef. Table 1), and our research indicates that asymmetric influence measures are required to properly handle them (Kripesundar, 1994). For example, it seems quite unlikely that any symmetric information measure can accurately capture the co-occurrence relationship between the two words &apos;Paleolithic&apos; and &apos;age&apos; in the phrase &apos;Paleolithic age&apos;. The suggestion that &apos;age&apos; </context>
</contexts>
<marker>Smadja, 1991</marker>
<rawString>Frank Smadja. 1991. Macrocoding the lexicon with co-occurrence knowledge. in (Zernik, 1991), pages 165-190.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ROhini K rihari</author>
<author>Charlotte M Baltus</author>
</authors>
<title>Use of language models in on-line recognition of handwritten sentences.</title>
<date>1993</date>
<booktitle>Proceedings of the Third International Workshop on Frontiers in Handwriting Recognition (IWFHR III).</booktitle>
<contexts>
<context position="3739" citStr="rihari &amp; Baltus, 1993" startWordPosition="549" endWordPosition="553">arative) success in modelling context. However, a theoretically sound definition of context is needed to ensure that such re-ranking and deleting of word-choices helps and not hinders (Gale &amp; Church, 1990). Researchers in information theory have come up with many inter-related formalisations of the ideas of context and contextual influence, such as mutual information and joint entropy. However, to our knowledge, all attempts at arriving at a theoretical basis for formalising the intuitive notion of context have treated the word and its context symmetrically. Many researchers ((Smadja, 1991); (rihari &amp; Baltus, 1993)) have suggested that the informationtheoretic notion of mutual information score (MIS) directly captures the idea of context. However, MIS 332 is deficient in its ability to detect one-sided correlations (ef. Table 1), and our research indicates that asymmetric influence measures are required to properly handle them (Kripesundar, 1994). For example, it seems quite unlikely that any symmetric information measure can accurately capture the co-occurrence relationship between the two words &apos;Paleolithic&apos; and &apos;age&apos; in the phrase &apos;Paleolithic age&apos;. The suggestion that &apos;age&apos; exerts as much influence </context>
</contexts>
<marker>rihari, Baltus, 1993</marker>
<rawString>ROhini K rihari and Charlotte M Baltus. 1993. Use of language models in on-line recognition of handwritten sentences. Proceedings of the Third International Workshop on Frontiers in Handwriting Recognition (IWFHR III).</rawString>
</citation>
<citation valid="true">
<authors>
<author>SN rihari</author>
<author>JJ Hull</author>
<author>R Chaudhari</author>
</authors>
<title>Integrating diverse knowledge sources in text recognition.</title>
<date>1983</date>
<journal>ACM Transactions on Office Information Systems,</journal>
<pages>1--1</pages>
<marker>rihari, Hull, Chaudhari, 1983</marker>
<rawString>SN rihari, JJ Hull, and R Chaudhari. 1983. Integrating diverse knowledge sources in text recognition. ACM Transactions on Office Information Systems, 1:1:68-87.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RM Warren</author>
</authors>
<title>Perceptual restoration of missing speech sounds.</title>
<date>1970</date>
<journal>Science,</journal>
<pages>167--392</pages>
<marker>Warren, 1970</marker>
<rawString>RM Warren. 1970. Perceptual restoration of missing speech sounds. Science, 167:392-393.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uriel Weinreich</author>
</authors>
<title>On Semantics.</title>
<date>1980</date>
<publisher>University of Pennsylvania Press.</publisher>
<contexts>
<context position="1218" citStr="Weinreich, 1980" startWordPosition="170" endWordPosition="171">se metrics for measuring directed lexical influence and compare performances. Keywords: contextual post-processing, defining context, lexical influence, directionality of context 1 Introduction It is widely accepted that context plays a significant role in shaping all aspects of language. Indeed, comprehension would be utterly impossible without the extensive application of contextual information. Evidence from psycholinguistic and cognitive psychological studies also demonstrates that contextual information affects the activation levels of lexical candidates during the process of perception (Weinreich, 1980; McClelland, 1987). Garvin (1972) describes the role of context as follows: [The meaning of] a particular text [is] not the system-derived meaning as a whole, but that part of it which is included in the contextually and situationally derived meaning proper to the text in question. (p. 69- 70) In effect, this means that the context of a word serves to restrict its sense. The problem addressed in this research is that of improving the performance of a natural-language recogniser (such as a recognition system for handwritten or spoken language). The recogniser output typically consists of an or</context>
</contexts>
<marker>Weinreich, 1980</marker>
<rawString>Uriel Weinreich. 1980. On Semantics. University of Pennsylvania Press.</rawString>
</citation>
<citation valid="true">
<title>Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon. Lawrence Erlbaum.</title>
<date>1991</date>
<editor>Un Zernik, editor.</editor>
<marker>1991</marker>
<rawString>Un Zernik, editor. 1991. Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon. Lawrence Erlbaum.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>