<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.079625">
<title confidence="0.99465">
Saarland: Vector-based models of semantic textual similarity
</title>
<author confidence="0.994907">
Georgiana Dinu Stefan Thater
</author>
<affiliation confidence="0.9990795">
Center of Mind/Brain Sciences Dept. of Computational Linguistics
University of Trento Universit¨at des Saarlandes
</affiliation>
<email confidence="0.988414">
georgiana.dinu@unitn.it stth@coli.uni-saarland.de
</email>
<sectionHeader confidence="0.997254" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999689714285714">
This paper describes our system for the Se-
meval 2012 Sentence Textual Similarity task.
The system is based on a combination of few
simple vector space-based methods for word
meaning similarity. Evaluation results show
that a simple combination of these unsuper-
vised data-driven methods can be quite suc-
cessful. The simple vector space components
achieve high performance on short sentences;
on longer, more complex sentences, they are
outperformed by a surprisingly competitive
word overlap baseline, but they still bring im-
provements over this baseline when incorpo-
rated into a mixture model.
</bodyText>
<sectionHeader confidence="0.999516" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956678571429">
Vector space models are widely-used methods for
word meaning similarity which exploit the so-called
distributional hypothesis, stating that semantically
similar words tend to occur in similar contexts. Word
meaning is represented by the contexts in which a
word occurs, and similarity is computed by compar-
ing these contexts in a high-dimensional vector space
(Turney and Pantel, 2010). Distributional models of
word meaning are attractive because they are sim-
ple, have wide coverage, and can be easily acquired
at virtually no cost in an unsupervised way. Fur-
thermore, recent research has shown that, at least
to some extent, these models can be generalized to
capture similarity beyond the (isolated) word level,
either as lexical meaning modulated by context, or
as vectorial meaning representations for phrases and
sentences. In this paper we evaluate the use of some
of these models for the Semantic Textual Similarity
(STS) task, which measures the degree of semantic
equivalence between two sentences.
In recent work Mitchell and Lapata (2008) has
drawn the attention to the question of building vecto-
rial meaning representations for sentences by combin-
ing individual word vectors. They propose a family of
simple “compositional” models that compute a vector
for a phrase or a sentence by combining vectors of
the constituent words, using different operations such
as vector addition or component-wise multiplication.
More refined models have been proposed recently by
Baroni and Zamparelli (2010) and Grefenstette and
Sadrzadeh (2011).
Thater et al. (2011) and others take a slightly dif-
ferent perspective on the problem: Instead of com-
puting a vector representation for a complete phrase
or sentence, they focus on the problem of “disam-
biguating” the vector representation of a target word
based on distributional information about the words
in the target’s context. While this approach is not
“compositional” in the sense described above, it still
captures some meaning of the complete phrase in
which a target word occurs.
In this paper, we report on the system we used in
the Semeval 2012 Sentence Textual Similarity shared
task and describe an approach that uses a combina-
tion of few simple vector-based components. We
extend the model of Thater et al. (2011), which has
been shown to perform well on a closely related para-
phrase ranking task, with an additive composition op-
eration along the lines of Mitchell and Lapata (2008),
and compare it with a simple alignment-based ap-
proach which in turn uses vector-based similarity
scores. Results show that in particular the alignment-
based approach can achieve good performance on
the Microsoft Research Video Description dataset.
On the other datasets, all vector-based components
are outperformed by a surprisingly competitive word
</bodyText>
<page confidence="0.98806">
603
</page>
<note confidence="0.528573">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 603–607,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999743142857143">
overlap baseline, but they still bring improvements
over this baseline when incorporated into a mixture
model. On the test dataset, the mixture model ranks
10th and 13th on the Microsoft Research Paraphrase
and Video Description datasets, respectively, which
we take this to be a quite promising result given that
we use only few relatively simple vector based com-
ponents to compute similarity scores for sentences.
The rest of the paper is structured as follows: Sec-
tion 2 presents the individual vector-based compo-
nents used by our system. In Section 3 we present
detailed evaluation results on the training set, as well
as results for our system on the test set, while Sec-
tion 4 concludes the paper.
</bodyText>
<sectionHeader confidence="0.997982" genericHeader="method">
2 Systems for Sentence Similarity
</sectionHeader>
<bodyText confidence="0.999946">
Our system is based on four different components:
We use two different vector space models to repre-
sent word meaning—a basic bag-of-words model
and a slightly simplified variant of the contextual-
ization model of Thater et al. (2011)—and two dif-
ferent methods to compute similarity scores for sen-
tences based on these two vector space models—one
“compositional” method that computes vectors for
sentences by summing over the vectors of the con-
stituent words, and one alignment-based method that
uses vector-based similarity scores for word pairs to
compute an alignment between the words in the two
sentences.
</bodyText>
<subsectionHeader confidence="0.967622">
2.1 Vector Space Models
</subsectionHeader>
<bodyText confidence="0.99996275">
For the basic vector-space model, we assume a set
W of words, and represent the meaning of a word
w E W by a vector in the vector space V spanned by
the set of basis vectors 1~ew&apos; 1 w&apos; E WI as follows:
</bodyText>
<equation confidence="0.9655125">
vbasic(w) _ Y, f (w,w&apos;)~ew&apos;
w&apos;EW
</equation>
<bodyText confidence="0.993860038461538">
where f is a function that assigns a co-occurrence
value to the word pair (w,w&apos;). In the experiments
reported below, we use pointwise mutual information
estimated on co-occurrence frequencies for words
within a 5-word window around the target word on
either side.1
1We use a 5-word window here as this setting has been shown
to give best results on a closely related task in the literature
(Mitchell and Lapata, 2008)
This basic “bag of words” vector space model rep-
resents word meaning by summing over all contexts
in which the target word occurs. Since words are of-
ten ambiguous, this means that context words pertain-
ing to different senses of the target word are mixed
within a single vector representation, which can lead
to “noisy” similarity scores. The vector for the noun
coach, for instance, contains context words like teach
and tell (person sense) as well as derail and crash
(vehicle sense).
To address this problem, Thater et al. (2011) pro-
pose a “contextualization” model in which the indi-
vidual components of the target word’s vector are re-
weighted, based on distributional information about
the words in the target’s context. Let us assume that
the context consist of a single word c. The vector for
a target w in context c is then defined as:
</bodyText>
<equation confidence="0.996609">
v(w,c) _ � a(c,w&apos;) f(w,w&apos;)~ew&apos;
w&apos;EW
</equation>
<bodyText confidence="0.99905975">
where a is some similarity score that quantifies to
what extent the vector dimension that corresponds
to w&apos; is compatible with the observed context c. In
the experiments reported below, we take a to be the
cosine similarity of c and w&apos;; see Section 3 for details.
In the experiments reported below, we use all
words in the syntactic context of the target word to
contextualize the target:
</bodyText>
<equation confidence="0.953968">
vctx(w) _ Y, v(w,c)
cEC(w)
</equation>
<bodyText confidence="0.999987846153846">
where C(w) is the context in which w occurs, i.e. all
words related to w by a dependency relation such as
subject or object, including inverse relations.
Remark. The contextualization model presented
above is a slightly simplified version of the original
model of Thater et al. (2011): it uses standard bag-of-
words vectors instead of syntax-based vectors. This
simplified version performs better on the training
dataset. Furthermore, the simplified model has been
shown to be equivalent to the models of Erk and
Pad´o (2008) and Thater et al. (2010) by Dinu and
Thater (2012), so the results reported below carry
over directly to these other models as well.
</bodyText>
<subsectionHeader confidence="0.995982">
2.2 Vector Composition and Alignment
</subsectionHeader>
<bodyText confidence="0.998376">
The two vector space models sketched above repre-
sent the meaning of words, and thus cannot be applied
</bodyText>
<page confidence="0.995672">
604
</page>
<bodyText confidence="0.999087545454545">
directly to model similarity of phrases or sentences.
One obvious and straightforward way to extend these
models to the sentence level is to follow Mitchell and
Lapata (2008) and represent sentences by vectors
obtained by summing over the individual vectors of
the constituent words. These “compositional” mod-
els can then be used to compute similarity scores
between sentence pairs in a straightforward way, sim-
ply by computing the cosine of the angle between
vectors (or some other similarity score) for the two
sentences:
</bodyText>
<equation confidence="0.997837">
simadd(S,S0) = cos( E v(w), E v(w0)) (1)
w∈S w0∈S0
</equation>
<bodyText confidence="0.999992222222222">
where v(w) can be instantiated either with basic or
with ctx vectors.
In addition to the compositional models, we also
experimented with an alignment-based approach: In-
stead of computing vectors for complete sentences,
we compute an alignment between the words in the
two sentences. To be more precise, we compute
cosine similarity scores between all possible pairs
of words (tokens) of the two sentences; based on
these similarity scores, we then compute a one-to-one
alignment between the words in the two sentences2,
using a greedy search strategy (see Fig. 1). We assign
a weight to each link in the alignment which is simply
the cosine similarity score of the corresponding word
pair and take the sum of the link weights, normalized
by the maximal length of the two sentences to be the
corresponding similarity score for the two sentences.
The final score is then:
</bodyText>
<equation confidence="0.9994075">
simalign(S,S0) = E(w,w0)∈ALIGN(S,S0) cos(v(w),v(w0))
max(|S|,|S0 |)
</equation>
<bodyText confidence="0.9994225">
where v(w) is the vector for w, which again can be
either the basic or the contextualized vector.
</bodyText>
<sectionHeader confidence="0.999369" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9998815">
In this section we present our experimental results.
In addition to the models described in Section 2, we
define a baseline model which simply computes the
word overlap between two sentences as:
</bodyText>
<equation confidence="0.891789142857143">
simoverlap(S,S0) = |S ∩ S0 |(2)
|S ∪ S0|
2Note that this can result in some words not being aligned
function ALIGN(S1,S2)
alignment ← 0/
marked ← 0/
pairs ← {hw,w0i  |w ∈ S1,w0 ∈ S2}
</equation>
<bodyText confidence="0.748316">
while pairs not empty do
hw,w0i ← highest cosine pair in pairs
if w ∈/ marked and w0 ∈/ marked then
</bodyText>
<figure confidence="0.953339571428571">
alignment ← hw,w0i ∪ alignment
marked ← {w,w0} ∪ marked
end if
pairs ← pairs \ {hw,w0i}
end while
return alignment
end function
</figure>
<figureCaption confidence="0.999995">
Figure 1: The alignment algorithm
</figureCaption>
<bodyText confidence="0.99994025">
The score assigned by this method is simply the num-
ber of words that the two sentences have in common
divided by their total number of words. Finally, we
also propose a straightforward mixture model which
combines all of the above methods. We use the train-
ing data to fit a degree two polynomial over these
individual predictors using least squares regression.
We report cross-validation scores.
</bodyText>
<subsectionHeader confidence="0.997452">
3.1 Evaluation setup
</subsectionHeader>
<bodyText confidence="0.999843428571428">
The vector space used in all experiments is a bag-of-
words space containing word co-occurrence counts.
We use the GigaWord (1.7 billion tokens) as input
corpus and extract word co-occurrences within a
symmetric 5-word context window. Co-occurrence
counts smaller than three are set to 0 and we further
apply (positive) pmi weighting.
</bodyText>
<subsectionHeader confidence="0.999743">
3.2 Training results
</subsectionHeader>
<bodyText confidence="0.99896275">
The training data results are shown in Figure 2. The
best performance on the video dataset is achieved
by the alignment method using a basic vector rep-
resentation to compute word-level similarity. All
vector-space methods perform considerably better
than the simple word overlap baseline on this dataset,
the alignment method achieving almost 20% gain
over this baseline. This indicates that information
about the meaning of the words is very beneficial for
this type of data, consisting of small, well-structured
sentences.
Using the alignment method with contextualized
</bodyText>
<page confidence="0.997806">
605
</page>
<table confidence="0.998318166666667">
Component MSRvid MSRpar SMTeur
basic/add 70.9 33.3 31.8
ctx/add 65.7 23.0 30.4
basic/align 74.6 40.5 32.1
overlap 56.8 59.5 50.0
mixture 78.1 61.8 54.1
</table>
<figureCaption confidence="0.994454">
Figure 2: Results on the training set.
</figureCaption>
<bodyText confidence="0.9999605">
vector representations (omitted in the table) does not
bring any improvement and it performs similarly to
the ctx/add method. This suggests that aligning sim-
ilar words in the two sentences does not benefit from
further meaning disambiguation through contextual-
ized vectors and that some level of disambiguation
may be implicitly performed.
On the paraphrase and europarl datasets, the over-
lap baseline outperforms, by a large margin, the vec-
tor space models. This is not surprising, as it is
known that word overlap baselines can be very com-
petitive on Recognizing Textual Entailment datasets,
to which these two datasets bare a large resemblance.
In particular this indicates that the methods proposed
for combining vector representations of words do
not provide, in the current state, accurate models for
modeling the meaning of larger sentences.
We also report 10-fold cross-validation scores ob-
tained with the mixture model. On all datasets, this
outperforms the individual methods, improving by
a margin of 2%-4% the best single methods. In par-
ticular, on the paraphrase and europarl datasets, this
shows that despite the considerably inferior perfor-
mance of the vector-based methods, these can still
help improve the overall performance.
This is also reflected in Table 3, where we evaluate
the performance of the mixture method when, in
turn, one of the individual components is excluded:
with few exceptions, all components contribute to the
performance of the mixtures.
</bodyText>
<subsectionHeader confidence="0.999982">
3.3 Test results
</subsectionHeader>
<bodyText confidence="0.9998698">
We have submitted as our official runs the best sin-
gle vector space model, performing alignment with
basic vector similarity, as well as the mixture meth-
ods. The mixture method uses weights individually
learned for each of the datasets made available during
</bodyText>
<table confidence="0.9973352">
Component MSRvid MSRpar SMTeur
basic/add −2.1 −0.1 −1.5
ctx/add −0.6 +1.3 +0.4
basic/align −4.1 −1.9 −2.6
overlap −0.1 −17.0 −23.0
</table>
<figureCaption confidence="0.9835535">
Figure 3: Results on the training set when removing indi-
vidual components from the mixture model.
</figureCaption>
<bodyText confidence="0.999690115384615">
training. For the two surprise datasets we carry over
the weights of what we have considered to be the
most similar training-available sets: video weights of
ontonotes and paraphrase weights for news.
The test data results are given in 4. We report
the results for the individual datasets as well as the
mean Pearson correlation, weighted by the sizes of
the datasets. The table also shows the performance
of the official task baseline as well as the top three
runs accoring to the overall weighted mean score.
As expected, the mixture method outperforms by
a large margin the alignment model, achieving rank
10 and rank 13 on the video and paraphrase datasets.
Overall the mixture method ranks 43 according to the
weighted mean measure (rank 22 if correcting our of-
ficial submission which contained the wrong output
file for the europarl dataset). The other more con-
troversial measures rank our official, not corrected,
submission at position 13 (RankNrm) and 71 (Rank),
overall. This is an encouraging result, as the individ-
ual components we have used are all unsupervised,
obtained solely from large amounts of unlabeled data,
and with no other additional resources. The training
data made available has only been used to learn a
set of weights for combining these individual compo-
nents.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9997762">
This paper describes an approach that combines few
simple vector space-based components to model sen-
tence similarity. We have extended the state-of-the-
art model for contextualized meaning representations
of Thater et al. (2011) with an additive composi-
tion operation along the lines of Mitchell and Lap-
ata (2008). We have combined this with a simple
alignment-based method and a word overlap baseline
into a mixture model.
Our system achieves promising results in particular
</bodyText>
<page confidence="0.996381">
606
</page>
<table confidence="0.99977875">
Dataset basic/align mixture baseline Run1 Run2 Run3
MSRvid 77.1 83.1 30.0 87.3 88.0 85.6
MSRpar 40.4 63.1 43.3 68.3 73.4 64.0
SMTeur 26.8 13.9 (37.1*) 45.4 52.8 47.7 51.5
OnWN 57.2 59.6 58.6 66.4 67.9 71.0
SMTnews 35.0 38.0 39.1 49.3 39.8 48.3
ALL 49.5 45.4 31.1 82.3 81.3 73.3
Rank 65 71 87 1 3 15
ALLNrm 78.7 82.5 67.3 85.7 86.3 85.2
RankNrm 50 13 85 2 1 5
Mean 50.6 56.6 (60.0*) 43.5 67.7 67.5 67.0
RankMean 60 43 (22*) 70 1 2 3
</table>
<figureCaption confidence="0.9932205">
Figure 4: Results on the test set. * – corrected score (official results score wrong prediction file we have submitted for
the europarl dataset). Official baseline and top three runs according to the weighted mean measure.
</figureCaption>
<bodyText confidence="0.999884772727273">
on the Microsoft Research Paraphrase and Video
Description datasets, on which it ranks 13th and 10th,
respectively. We take this to be a promising result,
given that our focus has not been the development
of a highly-competitive complex system, but rather
on investigating what performance can be achieved
when using only vector space methods.
An interesting observation is that the methods for
combining word vector representations (the vector
addition, or the meaning contextualization) can be
beneficial for modeling the similarity of the small,
well-structured sentences of the video dataset, how-
ever they do not perform well on comparing longer,
more complex sentences. In future work we plan to
further investigate methods for composition in vector
space models using the STS datasets, in addition to
the small, controlled datasets that have been typically
used in this line of research.
Acknowledgments. This work was supported by
the Cluster of Excellence “Multimodal Computing
and Interaction,” funded by the German Excellence
Initiative.
</bodyText>
<sectionHeader confidence="0.999635" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999824105263158">
Marco Baroni and Roberto Zamparelli. 2010. Nouns are
vectors, adjectives are matrices: Representing adjective-
noun constructions in semantic space. In Proceedings
of the 2010 Conference on Empirical Methods in Nat-
ural Language Processing, Cambridge, MA, October.
Association for Computational Linguistics.
Georgiana Dinu and Stefan Thater. 2012. A comparison
of models of word meaning in context. In Proceedings
of the 2012 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies. Short paper, to appear.
Katrin Erk and Sebastian Pad´o. 2008. A structured vector
space model for word meaning in context. In Proceed-
ings of the 2008 Conference on Empirical Methods in
Natural Language Processing, Honolulu, HI, USA.
Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011.
Experimental support for a categorical compositional
distributional model of meaning. In Proceedings of
the 2011 Conference on Empirical Methods in Natural
Language Processing, Edinburgh, Scotland, UK., July.
Association for Computational Linguistics.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL-08: HLT, Columbus, OH, USA.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2010. Contextualizing semantic representations using
syntactically enriched vector models. In Proceedings
of the 48th Annual Meeting of the Association for Com-
putational Linguistics, Uppsala, Sweden.
Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal.
2011. Word meaning in context: A simple and effective
vector model. In Proceedings of 5th International Joint
Conference on Natural Language Processing, pages
1134–1143, Chiang Mai, Thailand, November. Asian
Federation of Natural Language Processing.
Peter D. Turney and Patrick Pantel. 2010. From frequency
to meaning: Vector space modes of semantics. Journal
of Artificial Intelligence Research, 37:141–188.
</reference>
<page confidence="0.997767">
607
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.534425">
<title confidence="0.996469">Saarland: Vector-based models of semantic textual similarity</title>
<author confidence="0.99991">Georgiana Dinu Stefan Thater</author>
<affiliation confidence="0.987149">Center of Mind/Brain Sciences Dept. of Computational Linguistics University of Trento Universit¨at des Saarlandes</affiliation>
<email confidence="0.560956">georgiana.dinu@unitn.itstth@coli.uni-saarland.de</email>
<abstract confidence="0.997581133333333">This paper describes our system for the Semeval 2012 Sentence Textual Similarity task. The system is based on a combination of few simple vector space-based methods for word meaning similarity. Evaluation results show that a simple combination of these unsupervised data-driven methods can be quite successful. The simple vector space components achieve high performance on short sentences; on longer, more complex sentences, they are outperformed by a surprisingly competitive word overlap baseline, but they still bring improvements over this baseline when incorporated into a mixture model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Cambridge, MA,</location>
<contexts>
<context position="2376" citStr="Baroni and Zamparelli (2010)" startWordPosition="347" endWordPosition="350">hese models for the Semantic Textual Similarity (STS) task, which measures the degree of semantic equivalence between two sentences. In recent work Mitchell and Lapata (2008) has drawn the attention to the question of building vectorial meaning representations for sentences by combining individual word vectors. They propose a family of simple “compositional” models that compute a vector for a phrase or a sentence by combining vectors of the constituent words, using different operations such as vector addition or component-wise multiplication. More refined models have been proposed recently by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). Thater et al. (2011) and others take a slightly different perspective on the problem: Instead of computing a vector representation for a complete phrase or sentence, they focus on the problem of “disambiguating” the vector representation of a target word based on distributional information about the words in the target’s context. While this approach is not “compositional” in the sense described above, it still captures some meaning of the complete phrase in which a target word occurs. In this paper, we report on the system we used in the Semeval 2012 Sen</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjectivenoun constructions in semantic space. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, Cambridge, MA, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Stefan Thater</author>
</authors>
<title>A comparison of models of word meaning in context.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association</booktitle>
<note>to appear.</note>
<contexts>
<context position="7730" citStr="Dinu and Thater (2012)" startWordPosition="1235" endWordPosition="1238">tualize the target: vctx(w) _ Y, v(w,c) cEC(w) where C(w) is the context in which w occurs, i.e. all words related to w by a dependency relation such as subject or object, including inverse relations. Remark. The contextualization model presented above is a slightly simplified version of the original model of Thater et al. (2011): it uses standard bag-ofwords vectors instead of syntax-based vectors. This simplified version performs better on the training dataset. Furthermore, the simplified model has been shown to be equivalent to the models of Erk and Pad´o (2008) and Thater et al. (2010) by Dinu and Thater (2012), so the results reported below carry over directly to these other models as well. 2.2 Vector Composition and Alignment The two vector space models sketched above represent the meaning of words, and thus cannot be applied 604 directly to model similarity of phrases or sentences. One obvious and straightforward way to extend these models to the sentence level is to follow Mitchell and Lapata (2008) and represent sentences by vectors obtained by summing over the individual vectors of the constituent words. These “compositional” models can then be used to compute similarity scores between sentenc</context>
</contexts>
<marker>Dinu, Thater, 2012</marker>
<rawString>Georgiana Dinu and Stefan Thater. 2012. A comparison of models of word meaning in context. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Short paper, to appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
<author>Sebastian Pad´o</author>
</authors>
<title>A structured vector space model for word meaning in context.</title>
<date>2008</date>
<booktitle>In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Honolulu, HI, USA.</location>
<marker>Erk, Pad´o, 2008</marker>
<rawString>Katrin Erk and Sebastian Pad´o. 2008. A structured vector space model for word meaning in context. In Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, Honolulu, HI, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edward Grefenstette</author>
<author>Mehrnoosh Sadrzadeh</author>
</authors>
<title>Experimental support for a categorical compositional distributional model of meaning.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="2414" citStr="Grefenstette and Sadrzadeh (2011)" startWordPosition="352" endWordPosition="355">ual Similarity (STS) task, which measures the degree of semantic equivalence between two sentences. In recent work Mitchell and Lapata (2008) has drawn the attention to the question of building vectorial meaning representations for sentences by combining individual word vectors. They propose a family of simple “compositional” models that compute a vector for a phrase or a sentence by combining vectors of the constituent words, using different operations such as vector addition or component-wise multiplication. More refined models have been proposed recently by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). Thater et al. (2011) and others take a slightly different perspective on the problem: Instead of computing a vector representation for a complete phrase or sentence, they focus on the problem of “disambiguating” the vector representation of a target word based on distributional information about the words in the target’s context. While this approach is not “compositional” in the sense described above, it still captures some meaning of the complete phrase in which a target word occurs. In this paper, we report on the system we used in the Semeval 2012 Sentence Textual Similarity shared task a</context>
</contexts>
<marker>Grefenstette, Sadrzadeh, 2011</marker>
<rawString>Edward Grefenstette and Mehrnoosh Sadrzadeh. 2011. Experimental support for a categorical compositional distributional model of meaning. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-08: HLT,</booktitle>
<location>Columbus, OH, USA.</location>
<contexts>
<context position="1922" citStr="Mitchell and Lapata (2008)" startWordPosition="279" endWordPosition="282"> are attractive because they are simple, have wide coverage, and can be easily acquired at virtually no cost in an unsupervised way. Furthermore, recent research has shown that, at least to some extent, these models can be generalized to capture similarity beyond the (isolated) word level, either as lexical meaning modulated by context, or as vectorial meaning representations for phrases and sentences. In this paper we evaluate the use of some of these models for the Semantic Textual Similarity (STS) task, which measures the degree of semantic equivalence between two sentences. In recent work Mitchell and Lapata (2008) has drawn the attention to the question of building vectorial meaning representations for sentences by combining individual word vectors. They propose a family of simple “compositional” models that compute a vector for a phrase or a sentence by combining vectors of the constituent words, using different operations such as vector addition or component-wise multiplication. More refined models have been proposed recently by Baroni and Zamparelli (2010) and Grefenstette and Sadrzadeh (2011). Thater et al. (2011) and others take a slightly different perspective on the problem: Instead of computing</context>
<context position="3313" citStr="Mitchell and Lapata (2008)" startWordPosition="503" endWordPosition="506">utional information about the words in the target’s context. While this approach is not “compositional” in the sense described above, it still captures some meaning of the complete phrase in which a target word occurs. In this paper, we report on the system we used in the Semeval 2012 Sentence Textual Similarity shared task and describe an approach that uses a combination of few simple vector-based components. We extend the model of Thater et al. (2011), which has been shown to perform well on a closely related paraphrase ranking task, with an additive composition operation along the lines of Mitchell and Lapata (2008), and compare it with a simple alignment-based approach which in turn uses vector-based similarity scores. Results show that in particular the alignmentbased approach can achieve good performance on the Microsoft Research Video Description dataset. On the other datasets, all vector-based components are outperformed by a surprisingly competitive word 603 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 603–607, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics overlap baseline, but they still bring improvements over this baseline when</context>
<context position="5860" citStr="Mitchell and Lapata, 2008" startWordPosition="917" endWordPosition="920"> model, we assume a set W of words, and represent the meaning of a word w E W by a vector in the vector space V spanned by the set of basis vectors 1~ew&apos; 1 w&apos; E WI as follows: vbasic(w) _ Y, f (w,w&apos;)~ew&apos; w&apos;EW where f is a function that assigns a co-occurrence value to the word pair (w,w&apos;). In the experiments reported below, we use pointwise mutual information estimated on co-occurrence frequencies for words within a 5-word window around the target word on either side.1 1We use a 5-word window here as this setting has been shown to give best results on a closely related task in the literature (Mitchell and Lapata, 2008) This basic “bag of words” vector space model represents word meaning by summing over all contexts in which the target word occurs. Since words are often ambiguous, this means that context words pertaining to different senses of the target word are mixed within a single vector representation, which can lead to “noisy” similarity scores. The vector for the noun coach, for instance, contains context words like teach and tell (person sense) as well as derail and crash (vehicle sense). To address this problem, Thater et al. (2011) propose a “contextualization” model in which the individual compone</context>
<context position="8130" citStr="Mitchell and Lapata (2008)" startWordPosition="1301" endWordPosition="1304">ctors. This simplified version performs better on the training dataset. Furthermore, the simplified model has been shown to be equivalent to the models of Erk and Pad´o (2008) and Thater et al. (2010) by Dinu and Thater (2012), so the results reported below carry over directly to these other models as well. 2.2 Vector Composition and Alignment The two vector space models sketched above represent the meaning of words, and thus cannot be applied 604 directly to model similarity of phrases or sentences. One obvious and straightforward way to extend these models to the sentence level is to follow Mitchell and Lapata (2008) and represent sentences by vectors obtained by summing over the individual vectors of the constituent words. These “compositional” models can then be used to compute similarity scores between sentence pairs in a straightforward way, simply by computing the cosine of the angle between vectors (or some other similarity score) for the two sentences: simadd(S,S0) = cos( E v(w), E v(w0)) (1) w∈S w0∈S0 where v(w) can be instantiated either with basic or with ctx vectors. In addition to the compositional models, we also experimented with an alignment-based approach: Instead of computing vectors for </context>
<context position="15357" citStr="Mitchell and Lapata (2008)" startWordPosition="2468" endWordPosition="2472"> encouraging result, as the individual components we have used are all unsupervised, obtained solely from large amounts of unlabeled data, and with no other additional resources. The training data made available has only been used to learn a set of weights for combining these individual components. 4 Conclusions This paper describes an approach that combines few simple vector space-based components to model sentence similarity. We have extended the state-of-theart model for contextualized meaning representations of Thater et al. (2011) with an additive composition operation along the lines of Mitchell and Lapata (2008). We have combined this with a simple alignment-based method and a word overlap baseline into a mixture model. Our system achieves promising results in particular 606 Dataset basic/align mixture baseline Run1 Run2 Run3 MSRvid 77.1 83.1 30.0 87.3 88.0 85.6 MSRpar 40.4 63.1 43.3 68.3 73.4 64.0 SMTeur 26.8 13.9 (37.1*) 45.4 52.8 47.7 51.5 OnWN 57.2 59.6 58.6 66.4 67.9 71.0 SMTnews 35.0 38.0 39.1 49.3 39.8 48.3 ALL 49.5 45.4 31.1 82.3 81.3 73.3 Rank 65 71 87 1 3 15 ALLNrm 78.7 82.5 67.3 85.7 86.3 85.2 RankNrm 50 13 85 2 1 5 Mean 50.6 56.6 (60.0*) 43.5 67.7 67.5 67.0 RankMean 60 43 (22*) 70 1 2 3 F</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL-08: HLT, Columbus, OH, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Contextualizing semantic representations using syntactically enriched vector models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Uppsala,</location>
<marker>Thater, F¨urstenau, Pinkal, 2010</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2010. Contextualizing semantic representations using syntactically enriched vector models. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Thater</author>
<author>Hagen F¨urstenau</author>
<author>Manfred Pinkal</author>
</authors>
<title>Word meaning in context: A simple and effective vector model.</title>
<date>2011</date>
<journal>Asian Federation of Natural Language Processing.</journal>
<booktitle>In Proceedings of 5th International Joint Conference on Natural Language Processing,</booktitle>
<pages>1134--1143</pages>
<location>Chiang Mai, Thailand,</location>
<marker>Thater, F¨urstenau, Pinkal, 2011</marker>
<rawString>Stefan Thater, Hagen F¨urstenau, and Manfred Pinkal. 2011. Word meaning in context: A simple and effective vector model. In Proceedings of 5th International Joint Conference on Natural Language Processing, pages 1134–1143, Chiang Mai, Thailand, November. Asian Federation of Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space modes of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="1257" citStr="Turney and Pantel, 2010" startWordPosition="174" endWordPosition="177"> on short sentences; on longer, more complex sentences, they are outperformed by a surprisingly competitive word overlap baseline, but they still bring improvements over this baseline when incorporated into a mixture model. 1 Introduction Vector space models are widely-used methods for word meaning similarity which exploit the so-called distributional hypothesis, stating that semantically similar words tend to occur in similar contexts. Word meaning is represented by the contexts in which a word occurs, and similarity is computed by comparing these contexts in a high-dimensional vector space (Turney and Pantel, 2010). Distributional models of word meaning are attractive because they are simple, have wide coverage, and can be easily acquired at virtually no cost in an unsupervised way. Furthermore, recent research has shown that, at least to some extent, these models can be generalized to capture similarity beyond the (isolated) word level, either as lexical meaning modulated by context, or as vectorial meaning representations for phrases and sentences. In this paper we evaluate the use of some of these models for the Semantic Textual Similarity (STS) task, which measures the degree of semantic equivalence</context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter D. Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space modes of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>