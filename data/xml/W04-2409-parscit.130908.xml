<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000244">
<title confidence="0.9995085">
A Comparison of Manual and Automatic Constructions of Category
Hierarchy for Classifying Large Corpora
</title>
<author confidence="0.928787">
Fumiyo Fukumoto
</author>
<affiliation confidence="0.963384333333333">
Interdisciplinary Graduate
School of Medicine and Engineering
Univ. of Yamanashi
</affiliation>
<email confidence="0.995717">
fukumoto@skye.esb.yamanashi.ac.jp
</email>
<author confidence="0.976822">
Yoshimi Suzuki
</author>
<affiliation confidence="0.978622">
Interdisciplinary Graduate
School of Medicine and Engineering
Univ. of Yamanashi
</affiliation>
<email confidence="0.994962">
ysuzuki@ccn.yamanashi.ac.jp
</email>
<sectionHeader confidence="0.995574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951736842105">
We address the problem dealing with a large
collection of data, and investigate the use of
automatically constructing category hierarchy
from a given set of categories to improve clas-
sification of large corpora. We use two well-
known techniques, partitioning clustering, k-
means and a loss function to create category
hierarchy. k-means is to cluster the given cate-
gories in a hierarchy. To select the proper num-
ber of k, we use a loss function which mea-
sures the degree of our disappointment in any
differences between the true distribution over
inputs and the learner’s prediction. Once the
optimal number of k is selected, for each clus-
ter, the procedure is repeated. Our evaluation
using the 1996 Reuters corpus which consists
of 806,791 documents shows that automati-
cally constructing hierarchy improves classifi-
cation accuracy.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999881719298246">
Text classification has an important role to play, espe-
cially with the recent explosion of readily available on-
line documents. Much of the previous work on text clas-
sification use statistical and machine learning techniques.
However, the increasing number of documents and cate-
gories often hamper the development of practical classifi-
cation systems, mainly by statistical, computational, and
representational problems(Dietterich, 2000). One strat-
egy for solving these problems is to use category hierar-
chies. The idea behind this is that when humans organize
extensive data sets into fine-grained categories, category
hierarchies are often employed to make the large collec-
tion of categories more manageable.
McCallum et. al. presented a method called ‘shrink-
age’ to improve parameter estimates by taking advan-
tage of the hierarchy(McCallum,1999). They tested their
method using three different real-world datasets: 20,000
articles from the UseNet, 6,440 web pages from the In-
dustry Sector, and 14,831 pages from the Yahoo, and
showed improved performance. Dumais et. al. also de-
scribed a method for hierarchical classification of Web
content consisting of 50,078 Web pages for training, and
10,024 for testing, with promising results(Dumais and
Chen, 2000). Both of them use hierarchies which are
manually constructed. Such hierarchies are costly human
intervention, since the number of categories and the size
of the target corpora are usually very large. Further, man-
ually constructed hierarchies are very general in order to
meet the needs of a large number of forthcoming acces-
sible source of text data, and sometimes constructed by
relying on human intuition. Therefore, it is difficult to
keep consistency, and thus, problematic for classifying
text automatically.
In this paper, we address the problem dealing with a
large collection of data, and propose a method to gener-
ate category hierarchy for text classification. Our method
uses two well-known techniques, partitioning clustering
method called k-means and a loss function to create
hierarchical structure. k-means partitions a set of given
categories into k clusters, locally minimizing the average
squared distance between the data points and the clus-
ter centers. The algorithm involves iterating through the
data that the system is permitted to classify during each
iteration and constructs category hierarchy. To select the
proper number of k during each iteration, we use a loss
function which measures the degree of our disappoint-
ment in any differences between the true distribution over
inputs and the learner’s prediction. Another focus of this
paper is whether or not a large collection of data, the
1996 Reuters corpus helps to generate a category hier-
archy which is used to classify documents.
The rest of the paper is organized as follows. The next
section presents a brief review the earlier work. We then
explain the basic framework for constructing category hi-
erarchy, and describe hierarchical classification. Finally,
we report some experiments using the 1996 Reuters cor-
pus with a discussion of evaluation.
</bodyText>
<sectionHeader confidence="0.999298" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999942727272728">
Automatically generating hierarchies is not a new goal
for NLP and their application systems, and there have
been several attempts to create various types of hier-
archies(Koller and Sahami, 1997), (Nevill-Manning et
al., 1999), (Sanderson and Croft, 1999). One attempt
is Crouch(Crouch, 1988), which automatically generates
thesauri. Cutting et al. proposed a method called Scat-
ter/Gather in which clustering is used to create document
hierarchies(Cutting et al., 1992). Lawrie et al. proposed a
method to create domain specific hierarchies that can be
used for browsing a document set and locating relevant
documents(Lawrie and Croft, 2000).
At about the same time, several researchers have in-
vestigated the use of automatically generating hierarchies
for a particular application, text classification. Iwayama
et al. presented a probabilistic clustering algorithm called
Hierarchical Bayesian Clustering(HBC) to construct a set
of clusters for text classification(Iwayama and Tokunaga,
1995). The searching platform they focused on is the
probabilistic model of text categorisation that searches
the most likely clusters to which an unseen document is
classified. They tested their method using two data sets:
Japanese dictionary data called ‘Gendai yogo no kiso-
tisiki’ which contains 18,476 word entries, and a collec-
tion of English news stories from the Wall Street Jour-
nal which consists of 12,380 articles. The HBC model
showed 2-3% improvements in breakeven point over the
non-hierarchical model.
Weigend et al. proposed a method to generate hi-
erarchies using a probabilistic approach(Weigend et al.,
1999). They used an exploratory cluster analysis to create
hierarchies, and this was then verified by human assign-
ments. They used the Reuters-22173 and defined two-
level categories: 5 top-level categories (agriculture, en-
ergy, foreign exchange, metals and miscellaneous cate-
gory) called meta-topic, and other category groups as-
signed to its meta-topic. Their method is based on a prob-
abilistic approach that frames the learning problem as one
of function approximation for the posterior probability of
the topic vector given the input vector. They used a neu-
ral net architecture and explored several input represen-
tations. Information from each level of the hierarchy is
combined in a multiplicative fashion, so no hard decision
have to be made except at the leaf nodes. They found
a 5% advantage in average precision for the hierarchical
representation when using words.
All of these mentioned above perform well, while the
collection they tested is small compared with many real-
istic applications. In this paper, we investigate that a large
collection of data helps to generate a hierarchy, i.e. it is
statistically significant better than the results which uti-
lize hierarchical structure by hand, that has not previously
been explored in the context of hierarchical classification
except for the improvements of hierarchical model over
the flat model.
</bodyText>
<sectionHeader confidence="0.970192" genericHeader="method">
3 Generating Hierarchical Structure
</sectionHeader>
<subsectionHeader confidence="0.998915">
3.1 Document Representation
</subsectionHeader>
<bodyText confidence="0.99981280952381">
To generate hierarchies, we need to address the question
of how to represent texts(Cutting et al., 1992), (Lawrie
and Croft, 2000). The total number of words we focus on
is too large and it is computationally very expensive.
We use two statistical techniques to reduce the number
of inputs. The first is to use categorg vector instead of
document vector. The number of input vectors is not
the number of the training documents but equals to the
number of different categories. This allows to make the
large collection of data more manageable. The second is
a well-known technique, i.e. mutual information measure
between a word and a category. We use it as the value in
each dimension of the vector(Cover and Thomas, 1991).
More formally, each category in the training set is rep-
resented using a vector of weighted words. We call it
categorg vector. Category vectors are used for repre-
senting as points in Euclidean space in k-means cluster-
ing algorithm. Let cj be one of the categories cl, • • •, c,,,,,
and a vector assigned to cj be (clj, czj, • • •, cnj). The mu-
tual information MI(W, Cat) between a word W, and a
category Cat is defined as:
</bodyText>
<equation confidence="0.889497">
� P(W, Cat) log P(W, Cat) P(W)P(Cat) (1)
</equation>
<bodyText confidence="0.991278666666667">
Each c2j (1 G i G n) is the value of mutual information
between w2 and cj. We select the 1,000 words with the
largest mutual information for each category.
</bodyText>
<subsectionHeader confidence="0.999376">
3.2 Clustering
</subsectionHeader>
<bodyText confidence="0.999989272727273">
Clustering has long been used to group data with many
applications(Jain and Dubes, 1988). We use a simple
clustering technique, k-means to group categories and
construct a category hierarchy(Duda and Hart, 1973). k-
means is based on iterative relocation that partitions a
dataset into k clusters. The algorithm keeps track of the
centroids, i.e. seed points, of the subsets, and proceeds in
iterations. In each iteration, the following is performed:
(i) for each point x, find the seed point which is closest
to x. Associate x with this seed point, (ii) re-estimate
each seed point locations by taking the center of mass
</bodyText>
<equation confidence="0.937697666666667">
MI(W, Cat)
� WE{w, w}
CatE{c,c}
</equation>
<bodyText confidence="0.999815625">
of points associated with it. Before the first iteration the
seed points are initialized to random values. However, a
bad choice of initial centers can have a great impact on
performance, since k-means is fully deterministic, given
the starting seed points. We note that by utilizing hierar-
chical structure, the classification problem can be decom-
posed into a set of smaller problems corresponding to hi-
erarchical splits in the tree. This indicates that one first
learns rough distinctions among classes at the top level,
then lower level distinctions are learned only within the
appropriate top level of the tree, and lead to more special-
ized classifiers. We thus selected the top k frequent cate-
gories as initial seed points. Figure 1 illustrates a sample
hierarchy obtained by k-means. The input is a set of cat-
egory vectors. Seed points assigned to each cluster are
underlined in Figure 1.
</bodyText>
<equation confidence="0.966941928571429">
C8
C2
C5
k=3
C1, C2, C3 C6, C7, C8,
C4, C5 C9, C10
k=2
k=3
C1 C2
C6 C7 C9, C10
C6 C7 C9
C1 C3
C4
C10
</equation>
<figureCaption confidence="0.99909">
Figure 1: Hierarchical structure obtained by k-means
</figureCaption>
<bodyText confidence="0.999894333333333">
In general, the number of k is not given beforehand.
We thus use a loss function which is derived from Naive
Bayes(NB) classifiers to evaluate the goodness of k.
</bodyText>
<subsectionHeader confidence="0.930547">
3.3 NB
</subsectionHeader>
<bodyText confidence="0.999931615384616">
Naive Bayes(NB) probabilistic classifiers are commonly
studied in machine learning(Mitchell, 1996). The basic
idea in NB approaches is to use the joint probabilities of
words and categories to estimate the probabilities of cat-
egories given a document. The NB assumption is that all
the words in a text are conditionally independent given
the value of a classification variable. There are several
versions of the NB classifiers. Recent studies on a Naive
Bayes classifier which is proposed by McCallum et al. re-
ported high performance over some other commonly used
versions of NB on several data collections(McCallum,
1999). We use the model of NB by McCallum et al.
which is shown in formula (2).
</bodyText>
<equation confidence="0.972111357142857">
e) = P(c,
EI Id
r-1 P(cr I0)Hk zIP(wdz, I cr, �0)
P(c, I di,
�0)III�zI
k_1P (wdz, I ci, ~0~
where
�0tu = P(wt I cj, 0)
� �����
� N(wt, di) P(cj I di)
IV I +EJVh EI D1 N(w, di)P(cj I di)
IDI
�0ob = P(cj I 0) _ P(cj I di)IIDI (2)
2=1
</equation>
<bodyText confidence="0.99937944">
IV I refers to the size of vocabulary, IDI denotes the num-
ber of labeled training documents, and ICI shows the
number of categories. Id2I denotes document length. wdz,
is the word in position k of document d2, where the sub-
script of w, dik indicates an index into the vocabulary.
N(wt, d2) denotes the number of times word wt occurs
in document di, and P(cj I di) is defined by P(cj I di)
E {0,1}.
There are several strategies for assigning categories to
a documentbased on the probability P(cj I d2, B) such as
k-per-doc strategy (Field, 1975), probability threshold
and proportional assignment strategies(Lewis, 1992).
We use probability threshold(PT) strategy where each
document is assigned to the categories above a thresh-
old 01. The threshold 0 can be set to control preci-
sion and recall. Increasing 0, results in fewer test items
meeting the criterion, and this usually increases precision
but decreases recall. Conversely, decreasing 0 typically
decreases precision but increases recall. In a flat non-
hierarchical model, we chose 0 for each category, so as
to optimize performance on the F measure on a training
samples and development test samples. In a manual and
automatic construction of hierarchy, we chose 0 at each
level of a hierarchy using training samples and develop-
ment test samples.
</bodyText>
<subsectionHeader confidence="0.993007">
3.4 Estimating Error Reduction
</subsectionHeader>
<bodyText confidence="0.999784571428571">
Let P(y I x) be an unknown conditional distribution over
inputs, x, and output classes, y E {y1, yz, • • •, yn1, and
let P(x) be the marginal ‘input’ distribution. The learner
is given a labeled training set D, and estimates a classi-
fication function that, given an input x, produces an esti-
mated output distribution PD(y I x). The expected error
of the learner can be defined as follows:
</bodyText>
<equation confidence="0.8727465">
EPS _ �L(P(y I x),�PD(y I x))P(x) (3)
m
</equation>
<bodyText confidence="0.994185">
where L is some loss function that measures the degree
of our disappointment in any differences between the true
</bodyText>
<footnote confidence="0.685402333333333">
1We tested these three assignment strategies in the exper-
iment, and obtained a better result with probability threshold
than with other strategies.
</footnote>
<equation confidence="0.89761775">
distribution, P (g I x) and the learner’s prediction, PD(Y I
x). A log loss which is defined as follows:
# = E P(Y I X)109( PD(Y I X)) (4)
YEY
</equation>
<bodyText confidence="0.999789428571429">
Suppose that we chose the optimal number of k in
the k-means algorithm. Let Dk (2 &lt; k &lt; n) be one
of the result obtained by k-means algorithm, and be a
set of seed points(categories) labeled training samples.
The learner aims to select the result of D2, such that the
learner trained on the set D2 has lower error rate than any
other sets.
</bodyText>
<equation confidence="0.984182">
V &amp;quot; PD $ &amp;quot; PD� (5)
</equation>
<bodyText confidence="0.589516">
We defined a loss function as follows:
</bodyText>
<equation confidence="0.989533333333333">
1 1
E�PDi= lYkI IX xL
EX
</equation>
<bodyText confidence="0.999787142857143">
%k in formula (6) denotes a set of seed points(categories)
of Dk. We note that the true output distribution P(g I x)
in formula (6) is unknown for each sample x. Roy
et al.(Roy and McCallum, 2001) proposed a method of
active learning that directly optimizes expected future
error by log-loss, using the entropy of the posterior class
distribution on a sample of the unlabeled examples. We
applied their technique to estimate it using the current
learner. More precisely, from the development training
samples D, a different training set is created. The learner
then creates a new classifier from the set. This procedure
is repeated m times, and the final class posterior for an
instance is taken to be the average of the class posteriori
for each of the classifiers.
</bodyText>
<subsectionHeader confidence="0.912365">
3.5 Generating Category Hierarchy
</subsectionHeader>
<bodyText confidence="0.99509">
The algorithm for generating category hierarchy is as fol-
lows:
</bodyText>
<listItem confidence="0.9814129">
1. Create category vectors from the given training sam-
ples.
2. Apply k-means up to n-1 times (2 &lt; k &lt; n), where
n is the number of different categories.
3. Apply a loss function (6) to each result.
4. Select the i-th result using formula (5), i.e. the result
such that the learner trained on the i-th set has lower
error rate than any other results.
5. Assign every seed point(category) of the clusters in
the i-th result to each node of the tree.
</listItem>
<bodyText confidence="0.999823">
For each cluster of sub-branches, eliminates the seed
point, and the procedure 2 — 5 is repeated, i.e. run a local
k-means for each cluster of children, until the number of
categories in each cluster is less than two.
</bodyText>
<sectionHeader confidence="0.995407" genericHeader="method">
4 Hierarchical Classification
</sectionHeader>
<bodyText confidence="0.999793083333333">
Like Dumais’s approach(Dumais and Chen, 2000), we
classify test data using the hierarchy. We select the 1,000
features with the largest mutual information for each cat-
egory, and use them for testing. The selected features are
used as input to the NB classifiers.
We employ the hierarchy by learning separate classi-
fiers at each internal node of the tree. Then using these
classifiers, we assign categories to each test sample us-
ing probability threshold strategy where each sample is
assigned to categories above a threshold 0. The pro-
cess is repeated by greedily selecting sub-branches until
it reaches a leaf.
</bodyText>
<sectionHeader confidence="0.999728" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<subsectionHeader confidence="0.997199">
5.1 Data and Evaluation Methodology
</subsectionHeader>
<bodyText confidence="0.99804425">
We compare automatically created hierarchy with flat and
manually constructed hierarchy with respect to classifica-
tion accuracy. We further evaluate the generated category
hierarchy from two perspectives: we examine (i) whether
or not the number of categories effects to construct a cat-
egory hierarchy, and (ii) whether or not a large collection
of data helps to generate a category hierarchy.
The data we used is the 1996 Reuters corpus which
is available lately(Reuters, 2000). The corpus from 20th
Aug., 1996 to 19th Aug., 1997 consists of 806,791 doc-
uments. These documents are organized into 126 topi-
cal categories with a fifth level hierarchy. After eliminat-
ing unlabeled documents, we divide these documents into
four sets. Table 1 illustrates each data which we used in
each model, i.e. a flat non-hierarchical model, manually
constructed hierarchy, and automatically created hierar-
chy. The same notation of (X) in Table 1 denotes a pair of
training and test data. For example, ‘(F1) Training data’
shows that 145,919 samples are used for training NB clas-
sifiers, and ‘(F1) Test data’ illustrates that 290,665 sam-
ples are used for classification. We selected 102 cate-
gories which have at least one document in each data.
We obtained a vocabulary of 320,935 unique words
after eliminating words which occur only once, stem-
ming by a part-of-speech tagger(Schmid, 1995), and stop
word removal. The number of categories per document
is 3.21 on average. For both of the hierarchical and
non-hierarchical cases, we select the 1,000 features with
the largest MI for each of the 102 categories, and create
catcgoorg vcctor.
Like Roy et al’s method, we use !agging to reduce
variance of the true output distribution P(g I x). From
</bodyText>
<equation confidence="0.620225">
� P(y I x) log( PD,(y I x)) (6)
Y
</equation>
<tableCaption confidence="0.996723">
Table 1: Data sets used in each method
</tableCaption>
<table confidence="0.884474928571429">
Training samples Dev. training samples Dev. test samples Test samples
# of samples (145,919 samples) (300,000 samples) (60,021 samples) (290,665 samples)
Date ’96/08/20-’96/10/30 ’96/10/30-’97/03/19 ’97/03/19-’97/04/01 ’97/04/01-’97/08/19
Flat (F1) Training data (F2) Training data for (F2) Test data for esti- (F1) Test data
estimating PT mating PT (M1) Test data
Manual (M1) Training data (M2) Training data for (M2) Test data for es- (A1) Test data
estimating PT at each timating PT at each hi-
hierarchical level erarchical level
Automatic (A1) Training data (A2) Training data for (A2) Test data for esti-
estimating PT at each mating PT at each hi-
hierarchical level erarchical level
(A3) Training data (A3) Test data for esti-
for estimating the true mating the true output
output distribution distribution
</table>
<bodyText confidence="0.9805502">
our original development training set, 300,000 docu-
ments, a different training set which consists of 200,000
documents is created by random sampling. The learner
then creates a new NB classifier from this sample. This
procedure is repeated 10 times, and the final class poste-
rior for an instance is taken to be the average of the class
posteriors for each of the classifiers.
For evaluating the effectiveness of category assign-
ments, we use the standard recall, precision, and F-score.
Recall is defined to be the ratio of correct assignments
by the system divided by the total number of correct as-
signments. Precision is the ratio of correct assignments
by the system divided by the total number of the system’s
assignments. The F-score which combines recall (r) and
precision (p) with an equal weight is F(r, p) =
2,p
precision We
use micro-averaging F score where it computes globally
over n(all the number of categories) x m (the number of
total test documents) binary decisions.
</bodyText>
<subsectionHeader confidence="0.8372335">
5.2 Results and Discussion
5.2.1 Generating Category Hierarchy
</subsectionHeader>
<bodyText confidence="0.999988558823529">
Table 2 shows a top level of the hierarchy which is
manually constructed. Table 3 shows a portion of the au-
tomatically generating hierarchy which is associated with
the categories shown in Table 2. ‘x-g-• • •’ shows the ID
number which is assigned to each node of the tree. For
example, 3-5-1 shows that the ID number of the top, sec-
ond, and third level is 3, 5, and 1, respectively. 0 shows
a threshold value obtained by the training samples and
development test samples.
Tables 2 and 3 indicate that the automatically con-
structed hierarchy has different properties from manually
created hierarchies. When the top level categories of hi-
erarchical structure are equally discriminating proper-
ties, they are useful for text classification. In the man-
ual construction of hierarchy(Reuters, 2000), there are 25
categories in the top level, while the result of our method
based on corpus statistics shows that the top 4 frequent
categories are selected as a discriminative properties,
and other categories are sub-categorised into ‘Govern-
ment/social’ except for ‘Labour issues’ and ‘Weather’. In
the automatically generating hierarchy, ‘Economics’ —�
‘Expenditure’ —� ‘Welfare’, and ‘Economics’ —� ‘Labour
issues’ are created, while ‘Welfare’ and ‘Labour issues’
belong to the top level in the manually constructed hier-
archy.
Another interesting feature of our result is that some of
the related categories are merged into one cluster, while
in the manual hierarchy, they are different locations. Ta-
ble 4 illustrates the sample result of related categories in
the automatically created hierarchy. In Table 4, for exam-
ple, ‘Ec competition/subsidy’ is sub-categorised by ‘Mo-
nopolies/competition’. In a similar way, ‘E31’, ‘E311’,
‘E143’, and ‘E132’ are classified into ‘MCAT’, since
these categories are related to market news.
</bodyText>
<subsectionHeader confidence="0.845653">
5.2.2 Classification
</subsectionHeader>
<bodyText confidence="0.998651411764706">
As just described, thresholds for each level of a hier-
archy were established on the training samples and de-
velopment test samples. We then use these thresholds for
text classification, i.e. for each level of a hierarchy, if a
test sample exceeds the threshold, we assigned the cate-
gory to the test sample. A test sample can be in zero, one,
or more than one categories.
Table 5 shows the result of classification accuracy.
‘Flat’ and ‘Manual’ shows the baseline, i.e. the result
for all 102 categories are treated as a flat non-hierarchical
problem, and the result using manually constructed hi-
erarchy, respectively. ‘Automatic’ denotes the result of
our method. ‘miR’, ‘miP’, and ‘miF’ refers to the micro-
averaged recall, precision, and F-score, respectively.
Table 5 shows that the overall F values obtained by our
method was 3.9% better than the Flat model, and 3.3%
better than the Manual model. Both results are sta-
</bodyText>
<tableCaption confidence="0.959264">
Table 2: Manually constructed hierarchies
</tableCaption>
<table confidence="0.97136225">
Level 0 Category node
Top 0.400 1 Corporate/industrial 2 Economics 3 Government/social 4 Markets
5 Crime 6 Defence 7 International relations 8 Disasters
9 Entertainment 10 Environment 11 Fashion 12 Health
13 Labour issues 14 Obituaries 15 Human interest 16 Domestic politics
17 Biographies/people 18 Religion 19 Science 20 Sports
21 Tourism 22 War 23 Elections 24 Weather
25 Welfare
</table>
<tableCaption confidence="0.876285">
Table 3: Automatically constructed hierarchies
</tableCaption>
<table confidence="0.9998426">
Level 0 Category node
Top 0.422 1 Corporate/industrial 2 Economics 3 Government/social 4 Markets
Second 0.098 3-1 Domestic politics 3-2 International relations 3-3 War 3-4 Crime
3-5 Merchandise trade 3-6 Sports 3-7 Defence 3-8 Disasters
3-9 Elections 3-10 Biographies/people 4-10 Weather 2-5 Expenditure
2-6 Labour issues
Third 0.992 3-1-1 Health 3-5-1 Tourism 3-8-1 Environment 3-10-1 Entertainment
3-10-2 Religion 3-10-3 Science 3-10-4 Human interest 3-10-5 Obituaries
2-5-1 Welfare
Fourth 0.999 3-10-4-1 Fashion
</table>
<tableCaption confidence="0.960558">
Table 5: Classification accuracy
</tableCaption>
<table confidence="0.98148">
Method miR miP miF
Flat 0.753 0.647 0.695
Manual 0.685 0.708 0.701
Automatic 0.807 0.675 0.734
</table>
<bodyText confidence="0.999809848484848">
tistically significant using a micro sign test, P-value G
.01(Yang and Liu, 1999). Somewhat surprisingly, there
is no difference between Flat and Manual, since a mi-
cro sign test, P-value &gt; 0.05. This shows that manual
construction of hierarchy which depends on a corpus is
a difficult task. The overall F value of our method is
0.734. Classifying large data with similar categories is a
difficult task, so we did not expect to have exceptionally
high accuracy like Reuters-21578 (the performance over
0.85 F-score(Yang and Liu, 1999)). Performance on the
closed data, i.e. training samples and development test
samples in ‘Flat’, ‘Manual’, and ‘Automatic’ was 0.705,
0.720, and 0.782, respectively. Therefore, this is a diffi-
cult learning task and generalization to the test set is quite
reasonable.
Table 6 and Table 7 illustrates the results at each hi-
erarchical level of manually constructed hierarchies, and
our method, respectively. ‘Clusters’ denotes the number
of clusters, and ‘Categories’ refers to the number of cat-
egories at each level. The F-score of ‘Manual’ for the
top level categories is 0.744, and that of our method is
0.919. They outperform the flat model. However, the
performance by both ‘Manual’ and our method monoton-
ically decreases when the depth from the top level to each
node is large, and the overall F-score at the lower level
of hierarchies is very low. This is because at the lower
level more similar or the same features could be used as
features within the same top level category. This sug-
gests that we should be able to obtain further advantages
in efficiency in the hierarchical approach by reducing the
number of features which are not useful discriminators
within the lower-level of hierarchies(Koller and Sahami,
1997).
</bodyText>
<tableCaption confidence="0.991299">
Table 6: Accuracy by hierarchical level(Manual)
</tableCaption>
<table confidence="0.998448666666667">
Level Clusters Categories miR miP miF
Top 25 25 0.753 0.724 0.744
Second 16 63 0.524 0.553 0.543
Third 37 37 0.431 0.600 0.500
Fourth 43 43 0.276 0.103 0.146
Fifth 1 1 0 0 0
</table>
<tableCaption confidence="0.988735">
Table 7: Accuracy by hierarchical level(Automatic)
</tableCaption>
<table confidence="0.9831452">
Level Clusters Categories miR miP miF
Top 4 4 0.988 0.859 0.919
Second 59 59 0.813 0.697 0.750
Third 35 35 0.568 0.126 0.151
Fourth 4 4 0.246 0.102 0.103
</table>
<tableCaption confidence="0.999081">
Table 4: The sample result of related categories
</tableCaption>
<table confidence="0.999780181818182">
Node Category Node Category Node Category
1-22 Monopolies/competition(C34) 1-22-1 Ec competition/subsidy(G157)
2-7 Defence(GDEF) 2-7-1 Defence contracts(C331)
3 Markets(MCAT) 3-11 Output/capacity(E31)
3-12 Industrial production(E311)
3-13 Retail sales(E143) 3-13-1 Housing starts(E61)
3-14 Wholesale prices(E132)
4 Economics(ECAT) 4-2 Monetary/economic(E12)
4-8 Ec monetary/economic(G154)
4 Economics(ECAT) 4-6 Labour issues(GJOB)
4-7 Employment/labour(E41) 4-7-1 Unemployment(E411)
</table>
<subsubsectionHeader confidence="0.583848">
5.2.3 The Number of Categories v.s. Accuracy
</subsubsectionHeader>
<bodyText confidence="0.841151166666667">
Figure 2 shows the result of classification accuracy us-
ing different number of categories, i.e. 10, 50 and 102
categories. Each set of 10 and 50 categories from training
samples is created by random sampling. The sampling is
repeated 10 times2. Each point in Figure 2 is the average
performance over 10 sets.
</bodyText>
<subsectionHeader confidence="0.846173">
5.2.4 Efficiency of Large Corpora
</subsectionHeader>
<bodyText confidence="0.995852">
Figure 3 shows the result of classification accuracy us-
ing the different size of training samples, i.e. 10,000,
100,000 and 145,919 samples3. Each set of samples is
created by random sampling except for the set of 145,919
samples. The sampling process is repeated 10 times. The
average accuracy of each result across the 10 sets of sam-
ples is reported in Figure 3.
</bodyText>
<figure confidence="0.9973794">
0.74
0.72
0.7
0.68
0.66
0.64
0.62
0.6
10 20 30 40 50 60 70 80 90 100 110
# of categories
</figure>
<figureCaption confidence="0.991928">
Figure 2: Accuracy v.s. category size
</figureCaption>
<figure confidence="0.998226">
Flat
Manual
Automatic
0 20000 40000 60000 80000 100000 120000 140000 160000
# of training samples
</figure>
<figureCaption confidence="0.998905">
Figure 3: Accuracy v.s. training corpus size
</figureCaption>
<figure confidence="0.9994805">
Flat
Manual
Automatic
0.75
0.74
0.73
0.72
0.71
0.7
0.69
</figure>
<figureCaption confidence="0.654782">
Figure 2 shows that our method outperforms the flat
</figureCaption>
<bodyText confidence="0.95429135">
and manually constructed hierarchy at every point in the
graph. As can be seen, the grater the number of cate-
gories, the more likely it is that a test sample has been in-
correctly classified. Specifically, the performance of flat
model using 10 categories was 0.720 F-score and that of
50 categories was 0.695. This drop of accuracy indicates
that flat model is likely to be difficult to train when there
are a large number of classes with a large number of fea-
tures.
2In our method, 10 hierarchies are constructed for each set
of categories.
The performance can benefit significantly from much
larger training samples. At the number of training sam-
ples is 10,000, all methods have poor effectiveness, but it
learns rapidly, especially, the results of flat model shows
that it is extremely sensitive to the amount of training
data. At every point in the graph, the result of our method
with cluster-based generalisations outperforms other two
methods, especially, the method becomes more attractive
with less training samples.
</bodyText>
<footnote confidence="0.823532333333333">
3We used the same number of development, development
test, and test samples which are shown in Table1 in the experi-
ment.
</footnote>
<sectionHeader confidence="0.99741" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999998684210527">
We proposed a method for generating category hierar-
chy in order to improve text classification performance.
We used k-means and a loss function which is derived
from NB classifiers. We found small advantages in the
F-score for automatically generated hierarchy, compared
with a baseline flat non-hierarchy and that of manually
constructed hierarchy from large training samples. We
have also shown that our method can benefit significantly
from less training samples. Future work includes (i) ex-
tracting features which discriminate between categories
within the same cluster with low F-score, (ii) using other
machine learning techniques to obtain further advantages
in efficiency in dealing with a large collection of data, (iii)
comparing the method with other techniques such as hier-
archical agglomerative clustering and ‘X-means’(Pelleg
and Moore, 2000), and (iv) developing evaluation method
between manual and automatic construction of hierar-
chies to learn more about the strengths and weaknesses
of the two methods of classifying documents.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9999975">
We would like to thank anonymous reviewers for their
valuable comments. We also would like to express many
thanks to the Research and Standards Group of Reuters
who provided us the corpora.
</bodyText>
<sectionHeader confidence="0.999261" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999418417721519">
T. Cover and J. Thomas. 1991. Elements of Information
Theory. In Wiley.
C. Crouch. 1988. A Cluster-based Approach to The-
saurus Construction. In Proc. of the 11th Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 309–320.
D. Cutting, D. Karger, J. Pedersen, and J. Tukey. 1992.
Scatter/gather: A Cluster-based Approach to Brows-
ing Large Document Collections. In Proc. of the
15th Annual International ACM SIGIR Conference on
Research and Development in Information Retrieval,
pages 318–329.
T.G. Dietterich. 2000. Ensemble Methods in Machine
Learning. In Proc. of the 1st International Workshop
on Multiple Classifier Systems.
R. Duda and P. Hart. 1973. Pattern Classification and
Scene Analysis. Wiley.
S. Dumais and H. Chen. 2000. Hierarchical Classifica-
tion of Web Content. In Proc. of the 23rd Annual In-
ternational ACM SIGIR Conference on Research and
Development in Information Retrieval, pages 256–263.
B. Field. 1975. Towards Automatic Indexing: Auto-
matic Assignment of Controlled Language Indexing
and Classification from Free Indexing. Journal of
Documentation, pages 246–265.
M. Iwayama and T. Tokunaga. 1995. Cluster-Based Text
Categorization: A Comparison of Category Search
Strategies. In Proc. of the 18th Annual International
ACMSIGIR Conference on Research and Development
in Information Retrieval, pages 273–280.
A.K. Jain and R.C. Dubes. 1988. Algorithms for Clus-
tering Data. Englewood Cliffs N.J. Prentice Hall.
D. Koller and M. Sahami. 1997. Hierarchically Classify-
ing Documents using Very Few Words. In Proc. of the
14th International Conference on Machine Learning,
pages 170–178.
D. Lawrie and W.B. Croft. 2000. Discovering and Com-
paring Hierarchies. In Proc. ofRIAO 2000 Conference,
pages 314–330.
D.D. Lewis. 1992. An Evaluation of Phrasal and Clus-
tered Representations on a Text Categorization Task.
In Proc. of the 15th Annual International ACM SIGIR
Conference on Research and Development in Informa-
tion Retrieval, pages 37–50.
A.K. McCallum. 1999. Multi-Label Text Classification
with a Mixture Model Trained by EM. In Revised ver-
sion ofpaper appearing in AAAI’99 Workshop on Text
Learning.
T. Mitchell. 1996. Machine Learning. McGraw Hill.
Nevill-Manning, C.I. Witten, and G. Paynter. 1999.
Lexically-Generated Subject Hierarchies for Browsing
Large Collections. International Journal on digital Li-
braries, 2(3):111–123.
D. Pelleg and A. Moore. 2000. X-means: Extending
K-means with Efficient Estimation of the Number of
Clusters. In Proc. of the 17th International Conference
on Machine Learning, pages 725–734.
Reuters. 2000. Reuters Corpus Volume 1 En-
glish Language. 1996-08-20 to 1997-08-19,
release date 2000-11-03, Format version 1,
http://www.reuters.com/researchandstandards/corpus/.
N. Roy and A.K. McCallum. 2001. Toward Optimal
Active Learning through Sampling Estimation of Error
Reduction. In Proc. of the 18th International Confer-
ence on Machine Learning, pages 441–448.
M. Sanderson and B. Croft. 1999. Deriving Concept
Hierarchies from Text. In Proc. of the 22nd Annual
InternationalACMSIGIR Conference on Research and
Development in Information Retrieval, pages 206–213.
H. Schmid. 1995. Improvements in Part-of-Speech Tag-
ging with an Application to German. In Proc. of the
EACL SIGDAT Workshop.
A.S. Weigend, E.D. Wiener, and J.O. Pedersen. 1999.
Exploiting Hierarchy in Text Categorization. Informa-
tion Retrieval, 1(3):193–216.
Y. Yang and X. Liu. 1999. A Re-Examination of Text
Categorization Methods. In Proc. of the 22nd Annual
InternationalACMSIGIR Conference on Research and
Development in Information Retrieval, pages 42–49.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.234920">
<title confidence="0.9993325">A Comparison of Manual and Automatic Constructions of Hierarchy for Classifying Large Corpora</title>
<author confidence="0.384151">Fumiyo</author>
<affiliation confidence="0.834799666666667">Interdisciplinary School of Medicine and Univ. of Yamanashi</affiliation>
<email confidence="0.943273">fukumoto@skye.esb.yamanashi.ac.jp</email>
<affiliation confidence="0.93514575">Yoshimi Interdisciplinary School of Medicine and Univ. of Yamanashi</affiliation>
<email confidence="0.973773">ysuzuki@ccn.yamanashi.ac.jp</email>
<abstract confidence="0.99731155">We address the problem dealing with a large collection of data, and investigate the use of automatically constructing category hierarchy from a given set of categories to improve classification of large corpora. We use two welltechniques, partitioning clustering, and a function create category is to cluster the given categories in a hierarchy. To select the proper numof we use a function measures the degree of our disappointment in any differences between the true distribution over inputs and the learner’s prediction. Once the number of selected, for each cluster, the procedure is repeated. Our evaluation using the 1996 Reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classification accuracy.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>T Cover</author>
<author>J Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>In Wiley.</publisher>
<contexts>
<context position="8067" citStr="Cover and Thomas, 1991" startWordPosition="1229" endWordPosition="1232">1992), (Lawrie and Croft, 2000). The total number of words we focus on is too large and it is computationally very expensive. We use two statistical techniques to reduce the number of inputs. The first is to use categorg vector instead of document vector. The number of input vectors is not the number of the training documents but equals to the number of different categories. This allows to make the large collection of data more manageable. The second is a well-known technique, i.e. mutual information measure between a word and a category. We use it as the value in each dimension of the vector(Cover and Thomas, 1991). More formally, each category in the training set is represented using a vector of weighted words. We call it categorg vector. Category vectors are used for representing as points in Euclidean space in k-means clustering algorithm. Let cj be one of the categories cl, • • •, c,,,,, and a vector assigned to cj be (clj, czj, • • •, cnj). The mutual information MI(W, Cat) between a word W, and a category Cat is defined as: � P(W, Cat) log P(W, Cat) P(W)P(Cat) (1) Each c2j (1 G i G n) is the value of mutual information between w2 and cj. We select the 1,000 words with the largest mutual informatio</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>T. Cover and J. Thomas. 1991. Elements of Information Theory. In Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Crouch</author>
</authors>
<title>A Cluster-based Approach to Thesaurus Construction.</title>
<date>1988</date>
<booktitle>In Proc. of the 11th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>309--320</pages>
<contexts>
<context position="4610" citStr="Crouch, 1988" startWordPosition="695" endWordPosition="696">he paper is organized as follows. The next section presents a brief review the earlier work. We then explain the basic framework for constructing category hierarchy, and describe hierarchical classification. Finally, we report some experiments using the 1996 Reuters corpus with a discussion of evaluation. 2 Related Work Automatically generating hierarchies is not a new goal for NLP and their application systems, and there have been several attempts to create various types of hierarchies(Koller and Sahami, 1997), (Nevill-Manning et al., 1999), (Sanderson and Croft, 1999). One attempt is Crouch(Crouch, 1988), which automatically generates thesauri. Cutting et al. proposed a method called Scatter/Gather in which clustering is used to create document hierarchies(Cutting et al., 1992). Lawrie et al. proposed a method to create domain specific hierarchies that can be used for browsing a document set and locating relevant documents(Lawrie and Croft, 2000). At about the same time, several researchers have investigated the use of automatically generating hierarchies for a particular application, text classification. Iwayama et al. presented a probabilistic clustering algorithm called Hierarchical Bayesi</context>
</contexts>
<marker>Crouch, 1988</marker>
<rawString>C. Crouch. 1988. A Cluster-based Approach to Thesaurus Construction. In Proc. of the 11th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 309–320.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Cutting</author>
<author>D Karger</author>
<author>J Pedersen</author>
<author>J Tukey</author>
</authors>
<title>Scatter/gather: A Cluster-based Approach to Browsing Large Document Collections.</title>
<date>1992</date>
<booktitle>In Proc. of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>318--329</pages>
<contexts>
<context position="4787" citStr="Cutting et al., 1992" startWordPosition="718" endWordPosition="721"> describe hierarchical classification. Finally, we report some experiments using the 1996 Reuters corpus with a discussion of evaluation. 2 Related Work Automatically generating hierarchies is not a new goal for NLP and their application systems, and there have been several attempts to create various types of hierarchies(Koller and Sahami, 1997), (Nevill-Manning et al., 1999), (Sanderson and Croft, 1999). One attempt is Crouch(Crouch, 1988), which automatically generates thesauri. Cutting et al. proposed a method called Scatter/Gather in which clustering is used to create document hierarchies(Cutting et al., 1992). Lawrie et al. proposed a method to create domain specific hierarchies that can be used for browsing a document set and locating relevant documents(Lawrie and Croft, 2000). At about the same time, several researchers have investigated the use of automatically generating hierarchies for a particular application, text classification. Iwayama et al. presented a probabilistic clustering algorithm called Hierarchical Bayesian Clustering(HBC) to construct a set of clusters for text classification(Iwayama and Tokunaga, 1995). The searching platform they focused on is the probabilistic model of text </context>
<context position="7449" citStr="Cutting et al., 1992" startWordPosition="1123" endWordPosition="1126">erform well, while the collection they tested is small compared with many realistic applications. In this paper, we investigate that a large collection of data helps to generate a hierarchy, i.e. it is statistically significant better than the results which utilize hierarchical structure by hand, that has not previously been explored in the context of hierarchical classification except for the improvements of hierarchical model over the flat model. 3 Generating Hierarchical Structure 3.1 Document Representation To generate hierarchies, we need to address the question of how to represent texts(Cutting et al., 1992), (Lawrie and Croft, 2000). The total number of words we focus on is too large and it is computationally very expensive. We use two statistical techniques to reduce the number of inputs. The first is to use categorg vector instead of document vector. The number of input vectors is not the number of the training documents but equals to the number of different categories. This allows to make the large collection of data more manageable. The second is a well-known technique, i.e. mutual information measure between a word and a category. We use it as the value in each dimension of the vector(Cover</context>
</contexts>
<marker>Cutting, Karger, Pedersen, Tukey, 1992</marker>
<rawString>D. Cutting, D. Karger, J. Pedersen, and J. Tukey. 1992. Scatter/gather: A Cluster-based Approach to Browsing Large Document Collections. In Proc. of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 318–329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T G Dietterich</author>
</authors>
<title>Ensemble Methods in Machine Learning.</title>
<date>2000</date>
<booktitle>In Proc. of the 1st International Workshop on Multiple Classifier Systems.</booktitle>
<contexts>
<context position="1649" citStr="Dietterich, 2000" startWordPosition="236" endWordPosition="237">ed. Our evaluation using the 1996 Reuters corpus which consists of 806,791 documents shows that automatically constructing hierarchy improves classification accuracy. 1 Introduction Text classification has an important role to play, especially with the recent explosion of readily available online documents. Much of the previous work on text classification use statistical and machine learning techniques. However, the increasing number of documents and categories often hamper the development of practical classification systems, mainly by statistical, computational, and representational problems(Dietterich, 2000). One strategy for solving these problems is to use category hierarchies. The idea behind this is that when humans organize extensive data sets into fine-grained categories, category hierarchies are often employed to make the large collection of categories more manageable. McCallum et. al. presented a method called ‘shrinkage’ to improve parameter estimates by taking advantage of the hierarchy(McCallum,1999). They tested their method using three different real-world datasets: 20,000 articles from the UseNet, 6,440 web pages from the Industry Sector, and 14,831 pages from the Yahoo, and showed </context>
</contexts>
<marker>Dietterich, 2000</marker>
<rawString>T.G. Dietterich. 2000. Ensemble Methods in Machine Learning. In Proc. of the 1st International Workshop on Multiple Classifier Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Duda</author>
<author>P Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>Wiley.</publisher>
<contexts>
<context position="8914" citStr="Duda and Hart, 1973" startWordPosition="1382" endWordPosition="1385">ithm. Let cj be one of the categories cl, • • •, c,,,,, and a vector assigned to cj be (clj, czj, • • •, cnj). The mutual information MI(W, Cat) between a word W, and a category Cat is defined as: � P(W, Cat) log P(W, Cat) P(W)P(Cat) (1) Each c2j (1 G i G n) is the value of mutual information between w2 and cj. We select the 1,000 words with the largest mutual information for each category. 3.2 Clustering Clustering has long been used to group data with many applications(Jain and Dubes, 1988). We use a simple clustering technique, k-means to group categories and construct a category hierarchy(Duda and Hart, 1973). kmeans is based on iterative relocation that partitions a dataset into k clusters. The algorithm keeps track of the centroids, i.e. seed points, of the subsets, and proceeds in iterations. In each iteration, the following is performed: (i) for each point x, find the seed point which is closest to x. Associate x with this seed point, (ii) re-estimate each seed point locations by taking the center of mass MI(W, Cat) � WE{w, w} CatE{c,c} of points associated with it. Before the first iteration the seed points are initialized to random values. However, a bad choice of initial centers can have a </context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. Duda and P. Hart. 1973. Pattern Classification and Scene Analysis. Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Dumais</author>
<author>H Chen</author>
</authors>
<title>Hierarchical Classification of Web Content.</title>
<date>2000</date>
<booktitle>In Proc. of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>256--263</pages>
<contexts>
<context position="2471" citStr="Dumais and Chen, 2000" startWordPosition="360" endWordPosition="363">ften employed to make the large collection of categories more manageable. McCallum et. al. presented a method called ‘shrinkage’ to improve parameter estimates by taking advantage of the hierarchy(McCallum,1999). They tested their method using three different real-world datasets: 20,000 articles from the UseNet, 6,440 web pages from the Industry Sector, and 14,831 pages from the Yahoo, and showed improved performance. Dumais et. al. also described a method for hierarchical classification of Web content consisting of 50,078 Web pages for training, and 10,024 for testing, with promising results(Dumais and Chen, 2000). Both of them use hierarchies which are manually constructed. Such hierarchies are costly human intervention, since the number of categories and the size of the target corpora are usually very large. Further, manually constructed hierarchies are very general in order to meet the needs of a large number of forthcoming accessible source of text data, and sometimes constructed by relying on human intuition. Therefore, it is difficult to keep consistency, and thus, problematic for classifying text automatically. In this paper, we address the problem dealing with a large collection of data, and pr</context>
<context position="15697" citStr="Dumais and Chen, 2000" startWordPosition="2579" endWordPosition="2582">e number of different categories. 3. Apply a loss function (6) to each result. 4. Select the i-th result using formula (5), i.e. the result such that the learner trained on the i-th set has lower error rate than any other results. 5. Assign every seed point(category) of the clusters in the i-th result to each node of the tree. For each cluster of sub-branches, eliminates the seed point, and the procedure 2 — 5 is repeated, i.e. run a local k-means for each cluster of children, until the number of categories in each cluster is less than two. 4 Hierarchical Classification Like Dumais’s approach(Dumais and Chen, 2000), we classify test data using the hierarchy. We select the 1,000 features with the largest mutual information for each category, and use them for testing. The selected features are used as input to the NB classifiers. We employ the hierarchy by learning separate classifiers at each internal node of the tree. Then using these classifiers, we assign categories to each test sample using probability threshold strategy where each sample is assigned to categories above a threshold 0. The process is repeated by greedily selecting sub-branches until it reaches a leaf. 5 Evaluation 5.1 Data and Evaluat</context>
</contexts>
<marker>Dumais, Chen, 2000</marker>
<rawString>S. Dumais and H. Chen. 2000. Hierarchical Classification of Web Content. In Proc. of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Field</author>
</authors>
<title>Towards Automatic Indexing: Automatic Assignment of Controlled Language Indexing and Classification from Free Indexing.</title>
<date>1975</date>
<journal>Journal of Documentation,</journal>
<pages>246--265</pages>
<contexts>
<context position="12045" citStr="Field, 1975" startWordPosition="1944" endWordPosition="1945">D1 N(w, di)P(cj I di) IDI �0ob = P(cj I 0) _ P(cj I di)IIDI (2) 2=1 IV I refers to the size of vocabulary, IDI denotes the number of labeled training documents, and ICI shows the number of categories. Id2I denotes document length. wdz, is the word in position k of document d2, where the subscript of w, dik indicates an index into the vocabulary. N(wt, d2) denotes the number of times word wt occurs in document di, and P(cj I di) is defined by P(cj I di) E {0,1}. There are several strategies for assigning categories to a documentbased on the probability P(cj I d2, B) such as k-per-doc strategy (Field, 1975), probability threshold and proportional assignment strategies(Lewis, 1992). We use probability threshold(PT) strategy where each document is assigned to the categories above a threshold 01. The threshold 0 can be set to control precision and recall. Increasing 0, results in fewer test items meeting the criterion, and this usually increases precision but decreases recall. Conversely, decreasing 0 typically decreases precision but increases recall. In a flat nonhierarchical model, we chose 0 for each category, so as to optimize performance on the F measure on a training samples and development </context>
</contexts>
<marker>Field, 1975</marker>
<rawString>B. Field. 1975. Towards Automatic Indexing: Automatic Assignment of Controlled Language Indexing and Classification from Free Indexing. Journal of Documentation, pages 246–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Iwayama</author>
<author>T Tokunaga</author>
</authors>
<title>Cluster-Based Text Categorization: A Comparison of Category Search Strategies.</title>
<date>1995</date>
<booktitle>In Proc. of the 18th Annual International ACMSIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>273--280</pages>
<contexts>
<context position="5311" citStr="Iwayama and Tokunaga, 1995" startWordPosition="791" endWordPosition="794"> called Scatter/Gather in which clustering is used to create document hierarchies(Cutting et al., 1992). Lawrie et al. proposed a method to create domain specific hierarchies that can be used for browsing a document set and locating relevant documents(Lawrie and Croft, 2000). At about the same time, several researchers have investigated the use of automatically generating hierarchies for a particular application, text classification. Iwayama et al. presented a probabilistic clustering algorithm called Hierarchical Bayesian Clustering(HBC) to construct a set of clusters for text classification(Iwayama and Tokunaga, 1995). The searching platform they focused on is the probabilistic model of text categorisation that searches the most likely clusters to which an unseen document is classified. They tested their method using two data sets: Japanese dictionary data called ‘Gendai yogo no kisotisiki’ which contains 18,476 word entries, and a collection of English news stories from the Wall Street Journal which consists of 12,380 articles. The HBC model showed 2-3% improvements in breakeven point over the non-hierarchical model. Weigend et al. proposed a method to generate hierarchies using a probabilistic approach(W</context>
</contexts>
<marker>Iwayama, Tokunaga, 1995</marker>
<rawString>M. Iwayama and T. Tokunaga. 1995. Cluster-Based Text Categorization: A Comparison of Category Search Strategies. In Proc. of the 18th Annual International ACMSIGIR Conference on Research and Development in Information Retrieval, pages 273–280.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Jain</author>
<author>R C Dubes</author>
</authors>
<title>Algorithms for Clustering Data. Englewood Cliffs N.J.</title>
<date>1988</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="8791" citStr="Jain and Dubes, 1988" startWordPosition="1364" endWordPosition="1367">call it categorg vector. Category vectors are used for representing as points in Euclidean space in k-means clustering algorithm. Let cj be one of the categories cl, • • •, c,,,,, and a vector assigned to cj be (clj, czj, • • •, cnj). The mutual information MI(W, Cat) between a word W, and a category Cat is defined as: � P(W, Cat) log P(W, Cat) P(W)P(Cat) (1) Each c2j (1 G i G n) is the value of mutual information between w2 and cj. We select the 1,000 words with the largest mutual information for each category. 3.2 Clustering Clustering has long been used to group data with many applications(Jain and Dubes, 1988). We use a simple clustering technique, k-means to group categories and construct a category hierarchy(Duda and Hart, 1973). kmeans is based on iterative relocation that partitions a dataset into k clusters. The algorithm keeps track of the centroids, i.e. seed points, of the subsets, and proceeds in iterations. In each iteration, the following is performed: (i) for each point x, find the seed point which is closest to x. Associate x with this seed point, (ii) re-estimate each seed point locations by taking the center of mass MI(W, Cat) � WE{w, w} CatE{c,c} of points associated with it. Before</context>
</contexts>
<marker>Jain, Dubes, 1988</marker>
<rawString>A.K. Jain and R.C. Dubes. 1988. Algorithms for Clustering Data. Englewood Cliffs N.J. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>M Sahami</author>
</authors>
<title>Hierarchically Classifying Documents using Very Few Words.</title>
<date>1997</date>
<booktitle>In Proc. of the 14th International Conference on Machine Learning,</booktitle>
<pages>170--178</pages>
<contexts>
<context position="4513" citStr="Koller and Sahami, 1997" startWordPosition="679" endWordPosition="683">996 Reuters corpus helps to generate a category hierarchy which is used to classify documents. The rest of the paper is organized as follows. The next section presents a brief review the earlier work. We then explain the basic framework for constructing category hierarchy, and describe hierarchical classification. Finally, we report some experiments using the 1996 Reuters corpus with a discussion of evaluation. 2 Related Work Automatically generating hierarchies is not a new goal for NLP and their application systems, and there have been several attempts to create various types of hierarchies(Koller and Sahami, 1997), (Nevill-Manning et al., 1999), (Sanderson and Croft, 1999). One attempt is Crouch(Crouch, 1988), which automatically generates thesauri. Cutting et al. proposed a method called Scatter/Gather in which clustering is used to create document hierarchies(Cutting et al., 1992). Lawrie et al. proposed a method to create domain specific hierarchies that can be used for browsing a document set and locating relevant documents(Lawrie and Croft, 2000). At about the same time, several researchers have investigated the use of automatically generating hierarchies for a particular application, text classif</context>
<context position="25605" citStr="Koller and Sahami, 1997" startWordPosition="4164" endWordPosition="4167">is 0.919. They outperform the flat model. However, the performance by both ‘Manual’ and our method monotonically decreases when the depth from the top level to each node is large, and the overall F-score at the lower level of hierarchies is very low. This is because at the lower level more similar or the same features could be used as features within the same top level category. This suggests that we should be able to obtain further advantages in efficiency in the hierarchical approach by reducing the number of features which are not useful discriminators within the lower-level of hierarchies(Koller and Sahami, 1997). Table 6: Accuracy by hierarchical level(Manual) Level Clusters Categories miR miP miF Top 25 25 0.753 0.724 0.744 Second 16 63 0.524 0.553 0.543 Third 37 37 0.431 0.600 0.500 Fourth 43 43 0.276 0.103 0.146 Fifth 1 1 0 0 0 Table 7: Accuracy by hierarchical level(Automatic) Level Clusters Categories miR miP miF Top 4 4 0.988 0.859 0.919 Second 59 59 0.813 0.697 0.750 Third 35 35 0.568 0.126 0.151 Fourth 4 4 0.246 0.102 0.103 Table 4: The sample result of related categories Node Category Node Category Node Category 1-22 Monopolies/competition(C34) 1-22-1 Ec competition/subsidy(G157) 2-7 Defence</context>
</contexts>
<marker>Koller, Sahami, 1997</marker>
<rawString>D. Koller and M. Sahami. 1997. Hierarchically Classifying Documents using Very Few Words. In Proc. of the 14th International Conference on Machine Learning, pages 170–178.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lawrie</author>
<author>W B Croft</author>
</authors>
<title>Discovering and Comparing Hierarchies.</title>
<date>2000</date>
<booktitle>In Proc. ofRIAO 2000 Conference,</booktitle>
<pages>314--330</pages>
<contexts>
<context position="4959" citStr="Lawrie and Croft, 2000" startWordPosition="745" endWordPosition="748">erating hierarchies is not a new goal for NLP and their application systems, and there have been several attempts to create various types of hierarchies(Koller and Sahami, 1997), (Nevill-Manning et al., 1999), (Sanderson and Croft, 1999). One attempt is Crouch(Crouch, 1988), which automatically generates thesauri. Cutting et al. proposed a method called Scatter/Gather in which clustering is used to create document hierarchies(Cutting et al., 1992). Lawrie et al. proposed a method to create domain specific hierarchies that can be used for browsing a document set and locating relevant documents(Lawrie and Croft, 2000). At about the same time, several researchers have investigated the use of automatically generating hierarchies for a particular application, text classification. Iwayama et al. presented a probabilistic clustering algorithm called Hierarchical Bayesian Clustering(HBC) to construct a set of clusters for text classification(Iwayama and Tokunaga, 1995). The searching platform they focused on is the probabilistic model of text categorisation that searches the most likely clusters to which an unseen document is classified. They tested their method using two data sets: Japanese dictionary data call</context>
<context position="7475" citStr="Lawrie and Croft, 2000" startWordPosition="1127" endWordPosition="1130">ollection they tested is small compared with many realistic applications. In this paper, we investigate that a large collection of data helps to generate a hierarchy, i.e. it is statistically significant better than the results which utilize hierarchical structure by hand, that has not previously been explored in the context of hierarchical classification except for the improvements of hierarchical model over the flat model. 3 Generating Hierarchical Structure 3.1 Document Representation To generate hierarchies, we need to address the question of how to represent texts(Cutting et al., 1992), (Lawrie and Croft, 2000). The total number of words we focus on is too large and it is computationally very expensive. We use two statistical techniques to reduce the number of inputs. The first is to use categorg vector instead of document vector. The number of input vectors is not the number of the training documents but equals to the number of different categories. This allows to make the large collection of data more manageable. The second is a well-known technique, i.e. mutual information measure between a word and a category. We use it as the value in each dimension of the vector(Cover and Thomas, 1991). More f</context>
</contexts>
<marker>Lawrie, Croft, 2000</marker>
<rawString>D. Lawrie and W.B. Croft. 2000. Discovering and Comparing Hierarchies. In Proc. ofRIAO 2000 Conference, pages 314–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Lewis</author>
</authors>
<title>An Evaluation of Phrasal and Clustered Representations on a Text Categorization Task.</title>
<date>1992</date>
<booktitle>In Proc. of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>37--50</pages>
<contexts>
<context position="12120" citStr="Lewis, 1992" startWordPosition="1951" endWordPosition="1952">fers to the size of vocabulary, IDI denotes the number of labeled training documents, and ICI shows the number of categories. Id2I denotes document length. wdz, is the word in position k of document d2, where the subscript of w, dik indicates an index into the vocabulary. N(wt, d2) denotes the number of times word wt occurs in document di, and P(cj I di) is defined by P(cj I di) E {0,1}. There are several strategies for assigning categories to a documentbased on the probability P(cj I d2, B) such as k-per-doc strategy (Field, 1975), probability threshold and proportional assignment strategies(Lewis, 1992). We use probability threshold(PT) strategy where each document is assigned to the categories above a threshold 01. The threshold 0 can be set to control precision and recall. Increasing 0, results in fewer test items meeting the criterion, and this usually increases precision but decreases recall. Conversely, decreasing 0 typically decreases precision but increases recall. In a flat nonhierarchical model, we chose 0 for each category, so as to optimize performance on the F measure on a training samples and development test samples. In a manual and automatic construction of hierarchy, we chose</context>
</contexts>
<marker>Lewis, 1992</marker>
<rawString>D.D. Lewis. 1992. An Evaluation of Phrasal and Clustered Representations on a Text Categorization Task. In Proc. of the 15th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 37–50.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K McCallum</author>
</authors>
<title>Multi-Label Text Classification with a Mixture Model Trained by EM.</title>
<date>1999</date>
<booktitle>In Revised version ofpaper appearing in AAAI’99 Workshop on Text Learning.</booktitle>
<contexts>
<context position="11194" citStr="McCallum, 1999" startWordPosition="1772" endWordPosition="1773">ayes(NB) probabilistic classifiers are commonly studied in machine learning(Mitchell, 1996). The basic idea in NB approaches is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document. The NB assumption is that all the words in a text are conditionally independent given the value of a classification variable. There are several versions of the NB classifiers. Recent studies on a Naive Bayes classifier which is proposed by McCallum et al. reported high performance over some other commonly used versions of NB on several data collections(McCallum, 1999). We use the model of NB by McCallum et al. which is shown in formula (2). e) = P(c, EI Id r-1 P(cr I0)Hk zIP(wdz, I cr, �0) P(c, I di, �0)III�zI k_1P (wdz, I ci, ~0~ where �0tu = P(wt I cj, 0) � ����� � N(wt, di) P(cj I di) IV I +EJVh EI D1 N(w, di)P(cj I di) IDI �0ob = P(cj I 0) _ P(cj I di)IIDI (2) 2=1 IV I refers to the size of vocabulary, IDI denotes the number of labeled training documents, and ICI shows the number of categories. Id2I denotes document length. wdz, is the word in position k of document d2, where the subscript of w, dik indicates an index into the vocabulary. N(wt, d2) den</context>
</contexts>
<marker>McCallum, 1999</marker>
<rawString>A.K. McCallum. 1999. Multi-Label Text Classification with a Mixture Model Trained by EM. In Revised version ofpaper appearing in AAAI’99 Workshop on Text Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
</authors>
<title>Machine Learning.</title>
<date>1996</date>
<publisher>McGraw Hill.</publisher>
<contexts>
<context position="10670" citStr="Mitchell, 1996" startWordPosition="1686" endWordPosition="1687">ies as initial seed points. Figure 1 illustrates a sample hierarchy obtained by k-means. The input is a set of category vectors. Seed points assigned to each cluster are underlined in Figure 1. C8 C2 C5 k=3 C1, C2, C3 C6, C7, C8, C4, C5 C9, C10 k=2 k=3 C1 C2 C6 C7 C9, C10 C6 C7 C9 C1 C3 C4 C10 Figure 1: Hierarchical structure obtained by k-means In general, the number of k is not given beforehand. We thus use a loss function which is derived from Naive Bayes(NB) classifiers to evaluate the goodness of k. 3.3 NB Naive Bayes(NB) probabilistic classifiers are commonly studied in machine learning(Mitchell, 1996). The basic idea in NB approaches is to use the joint probabilities of words and categories to estimate the probabilities of categories given a document. The NB assumption is that all the words in a text are conditionally independent given the value of a classification variable. There are several versions of the NB classifiers. Recent studies on a Naive Bayes classifier which is proposed by McCallum et al. reported high performance over some other commonly used versions of NB on several data collections(McCallum, 1999). We use the model of NB by McCallum et al. which is shown in formula (2). e</context>
</contexts>
<marker>Mitchell, 1996</marker>
<rawString>T. Mitchell. 1996. Machine Learning. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C I Witten Nevill-Manning</author>
<author>G Paynter</author>
</authors>
<title>Lexically-Generated Subject Hierarchies for Browsing Large Collections.</title>
<date>1999</date>
<journal>International Journal on digital Libraries,</journal>
<volume>2</volume>
<issue>3</issue>
<marker>Nevill-Manning, Paynter, 1999</marker>
<rawString>Nevill-Manning, C.I. Witten, and G. Paynter. 1999. Lexically-Generated Subject Hierarchies for Browsing Large Collections. International Journal on digital Libraries, 2(3):111–123.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Pelleg</author>
<author>A Moore</author>
</authors>
<title>X-means: Extending K-means with Efficient Estimation of the Number of Clusters.</title>
<date>2000</date>
<booktitle>In Proc. of the 17th International Conference on Machine Learning,</booktitle>
<pages>725--734</pages>
<marker>Pelleg, Moore, 2000</marker>
<rawString>D. Pelleg and A. Moore. 2000. X-means: Extending K-means with Efficient Estimation of the Number of Clusters. In Proc. of the 17th International Conference on Machine Learning, pages 725–734.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reuters</author>
</authors>
<title>Reuters Corpus Volume 1 English Language. 1996-08-20 to 1997-08-19, release date 2000-11-03, Format version 1,</title>
<date>2000</date>
<location>http://www.reuters.com/researchandstandards/corpus/.</location>
<contexts>
<context position="16791" citStr="Reuters, 2000" startWordPosition="2756" endWordPosition="2757">0. The process is repeated by greedily selecting sub-branches until it reaches a leaf. 5 Evaluation 5.1 Data and Evaluation Methodology We compare automatically created hierarchy with flat and manually constructed hierarchy with respect to classification accuracy. We further evaluate the generated category hierarchy from two perspectives: we examine (i) whether or not the number of categories effects to construct a category hierarchy, and (ii) whether or not a large collection of data helps to generate a category hierarchy. The data we used is the 1996 Reuters corpus which is available lately(Reuters, 2000). The corpus from 20th Aug., 1996 to 19th Aug., 1997 consists of 806,791 documents. These documents are organized into 126 topical categories with a fifth level hierarchy. After eliminating unlabeled documents, we divide these documents into four sets. Table 1 illustrates each data which we used in each model, i.e. a flat non-hierarchical model, manually constructed hierarchy, and automatically created hierarchy. The same notation of (X) in Table 1 denotes a pair of training and test data. For example, ‘(F1) Training data’ shows that 145,919 samples are used for training NB classifiers, and ‘(</context>
<context position="20762" citStr="Reuters, 2000" startWordPosition="3408" endWordPosition="3409">s shown in Table 2. ‘x-g-• • •’ shows the ID number which is assigned to each node of the tree. For example, 3-5-1 shows that the ID number of the top, second, and third level is 3, 5, and 1, respectively. 0 shows a threshold value obtained by the training samples and development test samples. Tables 2 and 3 indicate that the automatically constructed hierarchy has different properties from manually created hierarchies. When the top level categories of hierarchical structure are equally discriminating properties, they are useful for text classification. In the manual construction of hierarchy(Reuters, 2000), there are 25 categories in the top level, while the result of our method based on corpus statistics shows that the top 4 frequent categories are selected as a discriminative properties, and other categories are sub-categorised into ‘Government/social’ except for ‘Labour issues’ and ‘Weather’. In the automatically generating hierarchy, ‘Economics’ —� ‘Expenditure’ —� ‘Welfare’, and ‘Economics’ —� ‘Labour issues’ are created, while ‘Welfare’ and ‘Labour issues’ belong to the top level in the manually constructed hierarchy. Another interesting feature of our result is that some of the related c</context>
</contexts>
<marker>Reuters, 2000</marker>
<rawString>Reuters. 2000. Reuters Corpus Volume 1 English Language. 1996-08-20 to 1997-08-19, release date 2000-11-03, Format version 1, http://www.reuters.com/researchandstandards/corpus/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Roy</author>
<author>A K McCallum</author>
</authors>
<title>Toward Optimal Active Learning through Sampling Estimation of Error Reduction.</title>
<date>2001</date>
<booktitle>In Proc. of the 18th International Conference on Machine Learning,</booktitle>
<pages>441--448</pages>
<contexts>
<context position="14288" citStr="Roy and McCallum, 2001" startWordPosition="2339" endWordPosition="2342">YEY Suppose that we chose the optimal number of k in the k-means algorithm. Let Dk (2 &lt; k &lt; n) be one of the result obtained by k-means algorithm, and be a set of seed points(categories) labeled training samples. The learner aims to select the result of D2, such that the learner trained on the set D2 has lower error rate than any other sets. V &amp;quot; PD $ &amp;quot; PD� (5) We defined a loss function as follows: 1 1 E�PDi= lYkI IX xL EX %k in formula (6) denotes a set of seed points(categories) of Dk. We note that the true output distribution P(g I x) in formula (6) is unknown for each sample x. Roy et al.(Roy and McCallum, 2001) proposed a method of active learning that directly optimizes expected future error by log-loss, using the entropy of the posterior class distribution on a sample of the unlabeled examples. We applied their technique to estimate it using the current learner. More precisely, from the development training samples D, a different training set is created. The learner then creates a new classifier from the set. This procedure is repeated m times, and the final class posterior for an instance is taken to be the average of the class posteriori for each of the classifiers. 3.5 Generating Category Hiera</context>
</contexts>
<marker>Roy, McCallum, 2001</marker>
<rawString>N. Roy and A.K. McCallum. 2001. Toward Optimal Active Learning through Sampling Estimation of Error Reduction. In Proc. of the 18th International Conference on Machine Learning, pages 441–448.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Sanderson</author>
<author>B Croft</author>
</authors>
<title>Deriving Concept Hierarchies from Text.</title>
<date>1999</date>
<booktitle>In Proc. of the 22nd Annual InternationalACMSIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>206--213</pages>
<contexts>
<context position="4573" citStr="Sanderson and Croft, 1999" startWordPosition="688" endWordPosition="691">which is used to classify documents. The rest of the paper is organized as follows. The next section presents a brief review the earlier work. We then explain the basic framework for constructing category hierarchy, and describe hierarchical classification. Finally, we report some experiments using the 1996 Reuters corpus with a discussion of evaluation. 2 Related Work Automatically generating hierarchies is not a new goal for NLP and their application systems, and there have been several attempts to create various types of hierarchies(Koller and Sahami, 1997), (Nevill-Manning et al., 1999), (Sanderson and Croft, 1999). One attempt is Crouch(Crouch, 1988), which automatically generates thesauri. Cutting et al. proposed a method called Scatter/Gather in which clustering is used to create document hierarchies(Cutting et al., 1992). Lawrie et al. proposed a method to create domain specific hierarchies that can be used for browsing a document set and locating relevant documents(Lawrie and Croft, 2000). At about the same time, several researchers have investigated the use of automatically generating hierarchies for a particular application, text classification. Iwayama et al. presented a probabilistic clustering</context>
</contexts>
<marker>Sanderson, Croft, 1999</marker>
<rawString>M. Sanderson and B. Croft. 1999. Deriving Concept Hierarchies from Text. In Proc. of the 22nd Annual InternationalACMSIGIR Conference on Research and Development in Information Retrieval, pages 206–213.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Improvements in Part-of-Speech Tagging with an Application to German.</title>
<date>1995</date>
<booktitle>In Proc. of the EACL SIGDAT Workshop.</booktitle>
<contexts>
<context position="17687" citStr="Schmid, 1995" startWordPosition="2902" endWordPosition="2903">which we used in each model, i.e. a flat non-hierarchical model, manually constructed hierarchy, and automatically created hierarchy. The same notation of (X) in Table 1 denotes a pair of training and test data. For example, ‘(F1) Training data’ shows that 145,919 samples are used for training NB classifiers, and ‘(F1) Test data’ illustrates that 290,665 samples are used for classification. We selected 102 categories which have at least one document in each data. We obtained a vocabulary of 320,935 unique words after eliminating words which occur only once, stemming by a part-of-speech tagger(Schmid, 1995), and stop word removal. The number of categories per document is 3.21 on average. For both of the hierarchical and non-hierarchical cases, we select the 1,000 features with the largest MI for each of the 102 categories, and create catcgoorg vcctor. Like Roy et al’s method, we use !agging to reduce variance of the true output distribution P(g I x). From � P(y I x) log( PD,(y I x)) (6) Y Table 1: Data sets used in each method Training samples Dev. training samples Dev. test samples Test samples # of samples (145,919 samples) (300,000 samples) (60,021 samples) (290,665 samples) Date ’96/08/20-’9</context>
</contexts>
<marker>Schmid, 1995</marker>
<rawString>H. Schmid. 1995. Improvements in Part-of-Speech Tagging with an Application to German. In Proc. of the EACL SIGDAT Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A S Weigend</author>
<author>E D Wiener</author>
<author>J O Pedersen</author>
</authors>
<title>Exploiting Hierarchy in Text Categorization.</title>
<date>1999</date>
<journal>Information Retrieval,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="5931" citStr="Weigend et al., 1999" startWordPosition="888" endWordPosition="891">). The searching platform they focused on is the probabilistic model of text categorisation that searches the most likely clusters to which an unseen document is classified. They tested their method using two data sets: Japanese dictionary data called ‘Gendai yogo no kisotisiki’ which contains 18,476 word entries, and a collection of English news stories from the Wall Street Journal which consists of 12,380 articles. The HBC model showed 2-3% improvements in breakeven point over the non-hierarchical model. Weigend et al. proposed a method to generate hierarchies using a probabilistic approach(Weigend et al., 1999). They used an exploratory cluster analysis to create hierarchies, and this was then verified by human assignments. They used the Reuters-22173 and defined twolevel categories: 5 top-level categories (agriculture, energy, foreign exchange, metals and miscellaneous category) called meta-topic, and other category groups assigned to its meta-topic. Their method is based on a probabilistic approach that frames the learning problem as one of function approximation for the posterior probability of the topic vector given the input vector. They used a neural net architecture and explored several input</context>
</contexts>
<marker>Weigend, Wiener, Pedersen, 1999</marker>
<rawString>A.S. Weigend, E.D. Wiener, and J.O. Pedersen. 1999. Exploiting Hierarchy in Text Categorization. Information Retrieval, 1(3):193–216.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Yang</author>
<author>X Liu</author>
</authors>
<title>A Re-Examination of Text Categorization Methods.</title>
<date>1999</date>
<booktitle>In Proc. of the 22nd Annual InternationalACMSIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<pages>42--49</pages>
<contexts>
<context position="23928" citStr="Yang and Liu, 1999" startWordPosition="3890" endWordPosition="3893">.098 3-1 Domestic politics 3-2 International relations 3-3 War 3-4 Crime 3-5 Merchandise trade 3-6 Sports 3-7 Defence 3-8 Disasters 3-9 Elections 3-10 Biographies/people 4-10 Weather 2-5 Expenditure 2-6 Labour issues Third 0.992 3-1-1 Health 3-5-1 Tourism 3-8-1 Environment 3-10-1 Entertainment 3-10-2 Religion 3-10-3 Science 3-10-4 Human interest 3-10-5 Obituaries 2-5-1 Welfare Fourth 0.999 3-10-4-1 Fashion Table 5: Classification accuracy Method miR miP miF Flat 0.753 0.647 0.695 Manual 0.685 0.708 0.701 Automatic 0.807 0.675 0.734 tistically significant using a micro sign test, P-value G .01(Yang and Liu, 1999). Somewhat surprisingly, there is no difference between Flat and Manual, since a micro sign test, P-value &gt; 0.05. This shows that manual construction of hierarchy which depends on a corpus is a difficult task. The overall F value of our method is 0.734. Classifying large data with similar categories is a difficult task, so we did not expect to have exceptionally high accuracy like Reuters-21578 (the performance over 0.85 F-score(Yang and Liu, 1999)). Performance on the closed data, i.e. training samples and development test samples in ‘Flat’, ‘Manual’, and ‘Automatic’ was 0.705, 0.720, and 0.7</context>
</contexts>
<marker>Yang, Liu, 1999</marker>
<rawString>Y. Yang and X. Liu. 1999. A Re-Examination of Text Categorization Methods. In Proc. of the 22nd Annual InternationalACMSIGIR Conference on Research and Development in Information Retrieval, pages 42–49.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>