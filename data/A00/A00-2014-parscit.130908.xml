<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.989338">
The Effectiveness of Corpus-Induced Dependency Grammars for
Post-processing Speech*
</title>
<author confidence="0.94778">
M. P. Harper, C. M. White, W. Wang, M. T. Johnson, and R. A. Helzerman
</author>
<affiliation confidence="0.9721435">
School of Electrical and Computer Engineering
Purdue University
</affiliation>
<address confidence="0.81991">
West Lafayette, IN 47907-1285
</address>
<email confidence="0.998443">
{harper,robot,wang28,mjohnson,helz}@ecn.purdue.edu
</email>
<sectionHeader confidence="0.99563" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999988285714286">
This paper investigates the impact of Constraint
Dependency Grammars (CDG) on the accuracy of
an integrated speech recognition and CDG pars-
ing system. We compare a conventional CDG with
CDGs that are induced from annotated sentences
and template-expanded sentences. The grammars
are evaluated on parsing speed, precision/coverage,
and improvement of word and sentence accuracy of
the integrated system. Sentence-derived CDGs sig-
nificantly improve recognition accuracy over the con-
ventional CDG but are less general. Expanding the
sentences with templates provides us with a mech-
anism for increasing the coverage of the grammar
with only minor reductions in recognition accuracy.
</bodyText>
<sectionHeader confidence="0.978079" genericHeader="keywords">
1 Background
</sectionHeader>
<bodyText confidence="0.999667625">
The question of when and how to integrate language
models with speech recognition systems is gaining in
importance as recognition tasks investigated by the
speech community become increasingly more chal-
lenging and as speech recognizers are used in hu-
man/computer interfaces and dialog systems (Block,
1997; Pieraccini and Levin, 1992; Schmid, 1994;
Wright et al., 1994; Zue et al., 1991). Many sys-
tems tightly integrate N-gram stochastic language
models, with a power limited to a regular grammar,
into the recognizer (Jeanrenaud et al., 1995; Ney et
al., 1994; Placeway et al., 1993) to build more ac-
curate speech recognizers. However, in order to act
based on the spoken interaction with the user, the
speech signal must be mapped to an internal repre-
sentation. Obtaining a syntactic representation for
the spoken utterance has a high degree of utility for
mapping to a semantic representation. Without a
structural analysis of an input, it is difficult to guar-
antee the correctness of the mapping from a sentence
to its interpretation (e.g., mathematical expressions
to internal calculations). We believe that significant
additional improvement in accuracy can be gained
in specific domains by using a more complex lan-
</bodyText>
<note confidence="0.693687333333333">
* This research was supported by grants from Intel, Purdue
Research Foundation, and National Science Foundation IRI
97-04358, CDA 96-17388, and #9980054-BCS.
</note>
<bodyText confidence="0.999001514285714">
guage model that combines syntactic, semantic, and
domain knowledge.
A language processing module that is more pow-
erful than a regular grammar can be loosely, mod-
erately, or tightly integrated with the spoken lan-
guage system, and there are advantages and dis-
advantages associated with each choice (Harper et
al., 1994). To tightly integrate a language model
with the power of a context-free grammar with the
acoustic module requires that the power of the two
modules be matched, making the integrated system
fairly intractable and difficult to train. By separat-
ing the language model from the acoustic model, it
becomes possible to use a more powerful language
model without increasing computational costs or the
amount of acoustic training data required by the rec-
ognizer. Furthermore, a loosely-integrated language
model can be developed independently of the speech
recognition component, which is clearly an advan-
tage. Decoupling the acoustic and language mod-
els also adds flexibility: a wide variety of language
models can be tried with a single acoustic model.
Systems that utilize a language model that operates
as a post-processor to a speech recognizer include
(Block, 1997; Seneff, 1992; Zue et al., 1991).
The goal of this research is to construct and ex-
perimentally evaluate a prototype of a spoken lan-
guage system that loosely integrates a speech recog-
nition component with an NLP component that uses
syntactic, semantic, and domain-specific knowledge
to more accurately select the sentence uttered by a
speaker. First we describe the system we have built.
Then we describe the mechanism used to rapidly de-
velop a domain-specific grammar that improves ac-
curacy of our speech recognizer.
</bodyText>
<sectionHeader confidence="0.988948" genericHeader="introduction">
2 Our System
</sectionHeader>
<bodyText confidence="0.999970142857143">
We have developed the prototype spoken language
system depicted in Figure 1 that integrates a speech
recognition component based on HMMs with a pow-
erful grammar model based on Constraint Depen-
dency Grammar (CDG). The speech recognizer is
implemented as a multiple-mixture triphone HMM
with a simple integrated word co-occurrence gram-
</bodyText>
<page confidence="0.997673">
102
</page>
<bodyText confidence="0.999857285714286">
mar (Ent, 1997; Young et al., 1997). Mel-scale cep-
stral coefficients, energy, and each of their their first
and second order differences are used as the under-
lying feature vector for each speech frame. Model
training is done using standard Baum-Welch Max-
imum Likelihood parameter re-estimation on diag-
onal covariance Gaussian Mixture Model (GMM)
feature distributions. The speech recognizer em-
ploys a token-passing version of the Viterbi algo-
rithm (Young et al., 1989) and pruning settings to
produce a pruned recognition lattice. This pruned
lattice contains the most likely alternative sentences
that account for the sounds present in an utterance
as well as their probabilities. Without any loss of in-
formation, this lattice is then compressed into a word
graph (Harper et al., 1999b; Johnson and Harper,
1999), which acts as the interface between the rec-
ognizer and the CDG parser. The word graph algo-
rithm begins with the recognition lattice and elim-
inates identical subgraphs by iteratively combining
word nodes that have exactly the same preceding
or following nodes (as well as edge probabilities),
pushing excess probability to adjacent nodes when-
ever possible. The resulting word graph represents
all possible word-level paths without eliminating or
adding any paths or modifying their probabilities.
Word graphs increase the bandwidth of useful acous-
tic information passed from the HMM to the CDG
parser compared to most current speech recognition
systems.
The CDG parser parses the word graph to identify
the best sentence consistent with both the acoustics
of the utterance and its own additional knowledge.
The loose coupling of the parser with the HMM
allows us to construct a more powerful combined
system without increasing the amount of training
data for the HMM or the computational complex-
ity of either of the component modules. Our NLP
component is implemented using a CDG parser
(Harper and Helzerman, 1995; Maruyama, 1990a;
Maruyama, 1990b) because of its power and flexibil-
ity, in particular:
</bodyText>
<listItem confidence="0.9996035625">
• It supports the use of syntactic, semantic, and
domain-specific knowledge in a uniform frame-
work.
• Our CDG parser supports efficient simultaneous
parsing of alternative sentence hypotheses in a
word graph (Harper and Helzerman, 1995; Helz-
erman and Harper, 1996).
• Because CDG is a dependency grammar, it can
better model free-order languages. Hence, CDG
can be used in processing a wider variety of human
languages than other grammar paradigms.
• It is capable of representing and using context-
dependent information unlike traditional gram-
mar approaches, thus providing a finer degree of
control over the syntactic analysis of a sentence.
• A CDG can be extracted directly from sentences
</listItem>
<bodyText confidence="0.9462871">
annotated with dependency information (i.e., fea-
ture and syntactic relationships).
We hypothesize that the accuracy of the combined
HMM/CDG system should benefit from the ability
to create a grammar that covers the domain as pre-
cisely as possible and that does not consider sen-
tences that would not make sense given the domain.
A corpus-based grammar is likely to have this degree
of control. In the next section we describe how we
construct a CDG from corpora.
</bodyText>
<figureCaption confidence="0.969438">
Figure 1: Block diagram of the loosely-coupled spo-
ken language system.
</figureCaption>
<sectionHeader confidence="0.970079" genericHeader="method">
3 Learning CDG Rules
</sectionHeader>
<bodyText confidence="0.992263666666667">
In this section, we introduce CDG and then describe
how CDG constraints can be learned from sentences
annotated with grammatical information.
</bodyText>
<subsectionHeader confidence="0.998764">
3.1 Introduction to CDG
</subsectionHeader>
<bodyText confidence="0.985064896551724">
Constraint Dependency Grammar (CDG), first
introduced by Maruyama (Maruyama, 1990a;
Maruyama, 1990b), uses constraints to determine
the grammatical dependencies for a sentence. The
parsing algorithm is framed as a constraint satis-
faction problem: the rules are the constraints and
the solutions are the parses. A CDG is defined as
a five-tuple, (E, R, L,C,T), where E = • • • , crcl
is a finite set of lexical categories (e.g., determiner),
R = {ri, , rp} is a finite set of uniquely named
roles or role ids (e.g., governor, needl, need2), L =
fli, ,1,71 is a finite set of labels (e.g., subject),
C is a constraint formula, and T is a table that
specifies allowable category-role-label combinations.
A sentence s = w1w2w3 ...wn has length n and
is an element of E*. For each word wi E E of a
sentence s, there are up to p different roles (with
most words needing only one or two (Harper et al.,
1999a)), yielding a maximum of n * p roles for the
entire sentence. A role is a variable that is assigned
a role value, an element of the set L X {1, 2,...,n}.
Role values are denoted as 1-rn, where 1 E L and
m E {1,2, ..., n} is called the modzfiee. Maruyama
originally used a modifiee of NIL to indicate that a
role value does not require a modifiee, but it is more
parsimonious to indicate that there is no dependent
by setting the modifiee to the position of its word.
Role values are assigned to roles to record the syn-
tactic dependencies between words in the sentence.
</bodyText>
<figure confidence="0.998184888888889">
Dictionary
HMM Speech
Recognizer
+ph CDG-based
NL Processor
Hypothesis
Selection
Top
Servence
</figure>
<page confidence="0.998566">
103
</page>
<bodyText confidence="0.999985347826087">
The governor role is assigned role values such that
the modifiee of the word indicates the position of the
word&apos;s governor or head (e.g., DET-3, when assigned
to the governor role of a determiner, indicates its
function and the position of its head). Every word
in a sentence has a governor role. Need roles are
used to ensure the requirements of a word are met.
For example, an object is required by a verb that
subcategorizes for one, unless it has passive voice.
The required object is accounted for by requiring
the verb&apos;s need role to be assigned a role value with
a modifiee that points at the object. Words can
have more than one need role, depending on the lex-
ical category of the word. The table T indicates the
roles that a word with a particular lexical category
must support.
A sentence s is said to be generated by the gram-
mar G if there exists an assignment A that maps a
role value to each of the roles for s such that C is
satisfied. There may be more than one assignment
of role values to the roles of a sentence that satisfies
C, in which case there is ambiguity. C is a first-
order predicate calculus formula over all roles that
requires that an assignment of role values to roles be
consistent with the formula; those role values incon-
sistent with C can be eliminated. A subformula
of C is a predicate involving =, &lt;, or &gt;, or predi-
cates joined by the logical connectives and, or, if,
or not. A subformula is a unary constraint if it con-
tains only a single variable (by convention, we use
x1) and a binary constraint if it contains two vari-
ables (by convention x1 and x2). An example of
a unary and binary constraint appears in Figure 2.
A CDG has an arity parameter a, which indicates
the maximum number of variables in the subformu-
las of C, and a degree parameter d, which is the
number of roles in the grammar. An arity of two
suffices to represent a grammar at least as power-
ful as a context-free grammar (Maruyama, 1990a;
Maruyama, 1990b). In (Harper et al., 1999a), we
developed a way to write constraints concerning the
category and feature values of a modifiee of a role
value (or role value pair). These constraints loosely
capture binary constraint information in unary con-
straints (or beyond binary for binary constraints)
and results in more efficient parsing.
</bodyText>
<figureCaption confidence="0.85536">
Figure 2: A Unary and binary constraint for CDG.
The white box in Figure 3 depicts a parse for
</figureCaption>
<bodyText confidence="0.99790062962963">
the sentence Clear the screen from the Resource
Management corpus (Price et al., 1988) (the ARV
and ARVP in the gray box will be discussed later),
which is a corpus we will use to evaluate our speech
processing system. We have constructed a conven-
tional CDG with around 1,500 unary and binary
constraints (i.e., its arity is 2) that were designed
to parse the sentences in the corpus. This CDG
covers a wide variety of grammar constructs (includ-
ing conjunctions and wh-movement) and has a fairly
rich semantics. It uses 16 lexical categories, 4 roles
(so its degree is 4), 24 labels, and 13 lexical fea-
ture types (subcat, agr, case, vtype (e.g., progres-
sive), mood, gap, inverted, voice, behavior (e.g.,
mass), type (e.g., interrogative, relative), semtype,
takesdet, and conjtype). The parse in Figure 3 is
an assignment of role values to roles that is consis-
tent with the unary and binary constraints. A role
value, when assigned to a role, has access to not only
the label and modifiee of its role value, but also the
role name of the role to which it is assigned, informa-
tion specific to the word (i.e., the word&apos;s position in
the sentence, its lexical category, and feature values
for each feature), and information about the lexical
class and feature values of its modffiee. Our unary
and binary constraints use this information to elim-
inate ungrammatical assignments.
</bodyText>
<figureCaption confidence="0.775555">
Figure 3: A CDG parse (see white box) is repre-
</figureCaption>
<bodyText confidence="0.737374">
sented by the assignment of role values to roles as-
sociated with a word with a specific lexical category
and one feature value per feature. ARVs and ARVPs
(see gray box) represent grammatical relations that
can be extracted from a sentence&apos;s parse.
</bodyText>
<subsectionHeader confidence="0.998636">
3.2 Learning CDG Constraints
</subsectionHeader>
<bodyText confidence="0.999805833333333">
The grammaticality of a sentence in a language de-
fined by a CDG was originally determined by apply-
ing the constraints of the grammar to the possible
A unary conetraint requiring that • role value
assigned to the governor role of a determiner
have the label DET end • modifise pointing to
</bodyText>
<listItem confidence="0.56811">
• subsequent word.
(if (and (= (category xi) determiner)
</listItem>
<equation confidence="0.664050333333333">
(= (rid ) G))
(and (= (label xi) DET)
(&gt; (mod x1) (pos Xt))))
</equation>
<bodyText confidence="0.962352833333333">
A binary constraint requiring that • role value
with the label $ assigned to • needl rola of one
word point at another word whoee governor
role is assigned • role value with the label SUBJ
and • modifiee thin points beck .0th. 1 irst ward.
(if (and (= (label xi) S)
</bodyText>
<equation confidence="0.5916612">
(= (rid xi) Ni)
(= (mod xi) (pos x2))
(= (rid x2) G))
(and (= (label x2) SUBJ)
(= (mod x2) (pos x1))))
</equation>
<bodyText confidence="0.278032">
Parse for &amp;quot;Clear the screen&amp;quot;
</bodyText>
<figure confidence="0.677669090909091">
1 2 3
Clear the screen
verb deteminar rIOUn
61.10CakObj type=definite CaSexomm011
vtypftinf aubC0t.aint38 behsx.zunt
voice--aalve type=nOne
Inverled.o a:Z.0410
gawione
mood=Carniand
seintyPe..Se
sermon*
</figure>
<table confidence="0.981741714285714">
G=rool-1 G.det.3 G.131-1
N14.1 1,13.1exxr-2
NUS.1
MW for det-3 assigned to G for the:
call.determiner, typel mdefintle, subcat1.eount3s,
lx (Pon oil (nb3,1
ARVP for det-3 assigned to 0 for the and
obj-1 assigned to G for screen:
Catl=deterrniner, typel=pefinite, subcatl=c000t3s, rid1=G,
iabeIl=det, (o (poS oil (mod x1)). Cat2=noun, ease2ommon,
behav2.sount, type2=nons, senlype2.display, apr2.3s,
dd2=G, labe12.:313(, (&lt; (mod 02) (POS 02)),
(pos xi) (pos 02)), (&lt; (mod 02) Prod 011),
lx (Sod x2) (Pos XI)), (= (mod xl) (pos 02))
</table>
<page confidence="0.993516">
104
</page>
<bodyText confidence="0.999066912087913">
role value assignments. If the set of all possible role
values assigned to the roles of a sentence of length n
is denoted S1=ExRxP08xLxMODxF1 x
x Fk, where k is the number of feature types,
Fi represents the set of feature values for that type,
POS = {1, 2, ... , n} is the set of possible positions,
MOD = {1, 2, ..., n} is the set of possible modi-
fiees, and n is sentence length (which can be any
arbitrary natural number), then unary constraints
partition Si into grammatical and ungrammatical
role values. Similarly, binary constraints partition
the set S2 = S x S1 = S? into compatible and in-
compatible pairs. Building upon this concept of role
value partitioning, it is possible to construct another
way of representing unary and binary constraints
because CDG constraints do not need to reference
the exact position of a word or a modifiee in the
sentence to parse sentences (Harper and Helzerman,
1995; Maruyama, 1990a; Maruyama, 1990b; Menzel,
1994; Menzel, 1995).
To represent the relative, rather than the abso-
lute, position information for the role values in a
grammatical sentence, it is only necessary to repre-
sent the positional relations between the modifiees
and the positions of the role values. To support an
arity of 2, these relations involve either equality or
less-than relations over the modifiees and positions
of role values assigned to the roles xi and s2. Since
unary constraints operate over role values assigned
to a single role, the only relative position relations
that can be tested are between the role value&apos;s posi-
tion (denoted as Psi) and its modifiee (denoted as
Mxi); one and only one of the following three re-
lations must be true: (Psi &lt; Mxi), (Mxi &lt; Psi);
or (Psi = Mxi). Since binary constraints operate
over role values assigned to pairs of roles, xi and x2,
the only possible relative position relations that can
be tested are between Pxi and Mxi, Px2 and Mx2,
Psi and Mx2, Px2 and Mxi, Pxi and Px2, Mxi and
Ms2. Note that each of the six has three positional
relations (as in the case of unary relations on Pxi
and Mxi) such that one and only one of them is
simultaneously true.
The unary and binary positional relations provide
the necessary mechanism to develop an alternative
view of the unary and binary constraints. First, we
develop the concept of an abstract role value (ARV),
which is a finite characterization of all possible role
values using relative, rather than absolute, position
relations. Formally, an ARV for a particular gram-
mar G = (E, R, L,C,T, F1, . , Fk) is an element of
the set: Ai = ExRxLxFi x...xFk xUC, where UC
encodes the three possible positional relations be-
tween Pxi and Mxi. The gray box of Figure 3 shows
an example of an ARV obtained from the parsed sen-
tence. Note that Ai is a finite set representing the
space of all possible ARVs for the grammarl; hence,
the set provides an alternative characterization of
the unary constraints for the grammar, which can
be partitioned into positive (grammatical) and neg-
ative (ungrammatical) ARVs. During parsing, if a
role value does not match one of the elements in the
positive ARV space, then it should be disallowed.
Positive ARVs can be obtained directly from the
parses of sentences: for each role value in a parse for
a sentence, simply extract its category, feature, role,
and label information, and then determine the po-
sitional relation that holds between the role value&apos;s
position and modifiee.
Similarly the set of legal abstract role value pairs
(ARV P s) , A2 = ExRxLxFix...xFkxExRxLx
x x Fk x BC, where BC encodes the positional
relations among Pxi, Mxi, Px2, and Mx2, provides
an alternative definition for the binary constraints2.
The gray box of Figure 3 shows an example of an
ARVP obtained from the parsed sentence. Positive
ARVPs can be obtained directly from the parses of
sentences. For each pair of role values assigned to
different roles, simply extract their category, feature,
role, and label information, and then determine the
positional relations that hold between the positions
and modifiees.
An enumeration of the positive ARV/ARVPs can
be used to represent the CDG constraints, C, and
ARWARVPs are PAC-learnable from positive ex-
amples, as can be shown using the techniques of
(Natarajan, 1989; Valiant, 1984). ARV/ARVP con-
straints can be enforced by using a fast table lookup
method to see if a role value (or role value pair) is
allowed (rather than propagating thousands of con-
straints), thus speeding up the parser.
</bodyText>
<sectionHeader confidence="0.982438" genericHeader="method">
4 Evaluation Using the Naval
Resource Management Domain
</sectionHeader>
<bodyText confidence="0.999982333333333">
An experiment was conducted to determine the
plausibility and the benefits of extracting CDG con-
straints from a domain-specific corpus of sentences.
For our speech application, the ideal CDG should be
general enough to cover sentences similar to those
that appear in the corpus while being restrictive
enough to eliminate sentences that are implausible
given the observed sentences. Hence, we investigate
whether a grammar extracted from annotated sen-
tences in a corpus achieves this precision of cover-
age. We also examine whether a learned grammar
has the ability to filter out incorrect sentence hy-
potheses produced by the HMM component of our
system in Figure 1. To investigate these issues, we
have performed an experiment using the standard
</bodyText>
<footnote confidence="0.99928425">
IA1 can also include information about the possible lexical
categories and feature values of the modifiee of x1.
2A2 can also include information about the possible lexical
categories and feature values of the modifiees of x1 and X2.
</footnote>
<page confidence="0.999058">
105
</page>
<bodyText confidence="0.999290875">
Resource Management (RM) (Price et al., 1988) and
Extended Resource Management (RM2) ((DARPA),
1990) corpora. These mid-size speech corpora have
a vocabulary of 991 words and contain utterances of
sentences derived from sentence templates based on
interviews with naval personnel familiar with naval
resource management tasks. They were chosen for
several reasons: they are two existing speech corpora
from the same domain; their manageable sizes make
them a good platform for the development of tech-
niques that require extensive experimentation; and
the sentences have both syntactic variety and rea-
sonably rich semantics. RM contains 5,190 separate
utterances (3,990 testing, 1,200 training) of 2,845
distinct sentences (2,245 training, 600 testing). We
have extracted several types of CDGs from annota-
tions of the RM sentences and tested their generality
using the 7,396 sentences in RM2 (out of the 8,173)
that are in the resource management domain but are
distinct from the RM sentences. We compare these
CDGs to each other and to the conventional CDG
described previously.
The corpus-based CDGs were created by extract-
ing the allowable grammar relationships from the
RM sentences that were annotated by language ex-
perts using the SENATOR annotation tool, a CGI
(Common Gateway Interace) HTML script written
in GNU C++ version 2.8.1 (White, 2000). We
tested two major CDG variations: those derived di-
rectly from the RM sentences (Sentence CDGs) and
those derived from simple template-expanded RM
sentences (Template CDGs). For example, &amp;quot;List
MIDPAC&apos;s deployments during (date)&amp;quot; is a sentence
containing a date template which allows any date
representations. For these experiments, we focused
on templates for dates, years, times, numbers, and
latitude and longitude coordinates. Each template
name identifies a sub-grammar which was produced
by annotating the appropriate strings. We then an-
notated sentences containing the template names as
if they were regular sentences. Approximately 25%
of the 2,845 RM sentences were expanded with one
or more templates.
Although annotating a corpus of sentences can be
a labor intensive task, we used an iterative approach
that is based on parsing using grammars with vary-
ing degrees of restrictiveness. A grammar can be
made less restrictive by ignoring:
</bodyText>
<listItem confidence="0.998140888888889">
• lexical information associated with a role value&apos;s
modifiee in the ARVPs,
• feature information of two role values in an ARVP
not directly related based on their modifiee rela-
tions,
• syntactic information provided by two role values
that are not directly related,
• specific feature information (e.g., semantics or
subcategorization) .
</listItem>
<bodyText confidence="0.999986322580645">
Initially, we bootstrapped the grammar by annotat-
ing a 200 sentence subset of the RM corpus and ex-
tracting a fairly general grammar from the annota-
tions. Then using increasingly restrictive grammars
at each iteration, we used the current grammar to
identify sentences that required annotation and ver-
ified the parse information for sentences that suc-
ceeded. This iterative technique reduced the time
required to build a CDG from about one year for the
conventional CDG to around two months (White,
2000).
Several methods of extracting an ARV/ARVP
grammar from sentences or template-extended sen-
tences were investigated. The ARVPs are extracted
differently for each method; whereas, the ARVs
are extracted in the same manner regardless of the
method. Recall that ARVs represent the set of ob-
served role value assignments. In our implementa-
tion, each ARV includes: the label of the role value,
the role to which the role value was assigned, the
lexical category and feature values of the word con-
taining the role, the relative position of the word and
the role value&apos;s modifiee, and the modifiee&apos;s lexical
category and feature values (modifiee constraints).
We use modifiee constraints for ARVs regardless of
extraction method because their use does not change
the coverage of the extracted grammar and not using
the information would significantly slow the parser
(Harper et al., 1999a). Because the ARVP space is
larger than the ARV space, we investigate six varia-
tions for extracting the pairs:
</bodyText>
<listItem confidence="0.971655461538461">
1. Full Mod: contains all grammar and feature
value information for all pairs of role values from
annotated sentences, as well as modifiee con-
straints. For a role value pair in a sentence to be
considered valid during parsing with this gram-
mar, it must match an ARVP extracted from the
annotated sentences.
2. Full: like Full Mod except it does not impose
modifiee constraints on a pair of role values during
parsing.
3. Feature Mod: contains all grammar relations
between all pairs of role values, but it consid-
ers feature and modifiee constraints only for pairs
</listItem>
<bodyText confidence="0.953239375">
that are directly related by a modifiee link. Dur-
ing parsing, if a role value pair is related by a
modifiee link, then a corresponding ARVP with
full feature and modifiee information must appear
in the grammar for it to be allowed. If the pair
is not directly related, then an ARVP must be
stored for the grammar relations, ignoring feature
and modifiee constraint information.
</bodyText>
<listItem confidence="0.9717768">
4. Feature: like Feature Mod except it does not
impose modifiee constraints on a pair of role val-
ues during parsing.
5. Direct Mod: stores only the grammar, feature,
and modifiee information for those pairs of role
</listItem>
<page confidence="0.998695">
106
</page>
<tableCaption confidence="0.797529">
Table 1: Number of ARVs and ARVPs extracted for
each RM grammar.
</tableCaption>
<table confidence="0.999070444444444">
ARVP Sentence Template Percent
Variation CDG CDG Increase
Full Mod 270,034 408,912 51.43%
Full 165,480 200,792 21.34%
Feature Mod 49,468 56,758 14.74%
Feature 36,558 40,308 10.26%
Direct Mod 41,124 47,004 14.30%
Direct 28,214 30,554 8.29%
ARVs 4,424 4,648 5.06%
</table>
<bodyText confidence="0.996336333333333">
values that are directly related by a modifiee link.
During parsing, if a role value pair is related by
such a link, then a corresponding ARVP must ap-
pear in the grammar for it to be allowed. Any
pair of role values not related by a modifiee link
is allowed (an open-world assumption).
</bodyText>
<listItem confidence="0.557550333333333">
6. Direct: like Direct Mod except it does not im-
pose modifiee constraints on a pair of role values
during parsing.
</listItem>
<bodyText confidence="0.9991126">
Grammar sizes for these six grammars, extracted
either directly from the 2,845 sentences or from the
2,845 sentences expanded with our sub-grammar
templates, appear in Table 1. The largest gram-
mars were derived using the Full Mod extrac-
tion method, with a fairly dramatic growth result-
ing from processing template-expanded sentences.
The Feature and Direct variations are more man-
ageable in size, even those derived from template-
expanded sentences.
Size is not the only important consideration for
a grammar. Other important issues are grammar
generality and the impact of the grammar on the
accuracy of selecting the correct sentence from the
recognition lattice of a spoken utterance. After
extracting the CDG grammars from the RM sen-
tences and template-expanded sentences, we tested
the generality of the extracted grammars by using
each grammar to parse the 7,396 RM2 sentences.
See the results in Table 2. The grammar with the
greatest generality was the conventional CDG for
the RM corpus; however, this grammar also has
the unfortunate attribute of being quite ambigu-
ous. The most generalizable of extracted grammars
uses the Direct method on template-expanded sen-
tences. In all cases, the template-expanded sen-
tence grammars gave better coverage than their cor-
responding sentence-only grammars.
We have also used the extracted grammars to
post-process word graphs created by the word graph
compression algorithm of (Johnson and Harper,
1999) for the test utterances in the RM corpus. As
was reported in (Johnson and Harper, 1999), the
word-error rate of our HMM recognizer with an em-
bedded word pair language model on the RM test set
</bodyText>
<tableCaption confidence="0.942489666666667">
Table 2: Number of successfully parsed sentences in
RM2 using the conventional CDG and CDGs derived
from sentences only or template-expanded sentences.
</tableCaption>
<table confidence="0.999305333333333">
ARVP # Parsed with # Parsed with
Variation Sentence CDG Template CDG
Full Mod 3,735 (50.50%) 4,461 (60.32%)
Full 4,509 (60.97%) 5,316 (71.88%)
Feature Mod 5,365 (72.54%) 5,927 (80.14%)
Feature 5,772 (78.04%) 6,208 (83.94%)
Direct Mod 5,464 (73.88%) 5,979 (80.84%)
Direct 5,931 (80.19%) 6,275 (84.82%)
Conventional 7,144 (96.59%) not applicable
</table>
<bodyText confidence="0.99974">
of 1200 utterances was 5.0%, the 1-best sentence ac-
curacy was 72.1%, and the word graph coverage ac-
curacy was 95.1%. Also, the average uncompressed
word graph size was 75.15 nodes, and our compres-
sion algorithm resulted in a average word graph size
of 28.62 word nodes. When parsing the word graph,
the probability associated with a word node can ei-
ther represent its acoustic score or a combination
of its acoustic and stochastic grammar score. We
use the acoustic score because (Johnson and Harper,
1999) showed that by using a word node&apos;s acoustic
score alone when extracting the top sentence candi-
date after parsing gave a 4% higher sentence accu-
racy.
For the parsing experiments, we processed the
1,080 word graphs produced for the RM test set
that contained 50 or fewer word nodes after com-
pression (out of 1,200 total) in order to efficiently
compare the 12 ARV/ARVP CDG grammars and
the conventional CDG (the larger word graphs re-
quire significant time and space to parse using the
conventional CDG). These 1,080 word graphs con-
tain 24.95 word nodes on average with a standard
deviation (SD) of 10.80, and result in 1-best sen-
tence accuracy was 75% before parsing. The num-
ber of role values prior to binary constraint propa-
gation differ across the grammars with an average
(and SD) for the conventional grammar of 504.99
(442.00), for the sentence-only grammars of 133.37
(119.48), and for the template-expanded grammars
of 157.87 (145.16). Table 3 shows the word graph
parsing speed and the path, node, and role value
(RV) ambiguity after parsing; Table 4 shows the
sentence accuracy and the accuracy and percent cor-
rect for words. Note that percent correct words is
calculated using iv-g-s and word accuracy using
, where N is the number of words, D is
the number of deletions, S is the number of substi-
tutions, and I is the number of insertions.
The most selective RM sentence grammar, Full
Mod, achieves the highest sentence accuracy, but
at a cost of a greater average parsing time than
the other RM sentence grammars. Higher accu-
</bodyText>
<page confidence="0.991405">
107
</page>
<table confidence="0.999720142857143">
ARVP Variation Parse Time (sec.) No. Paths No. Nodes No. RVs
Full Mod 33.89 (41.12) 2.21 (1.74) 10.59 (3.44) 19.51 (8.32)
Template Full Mod 41.85 (51.75) 2.78 (3.75) 10.76 (3.64) 19.93 (8.76)
Full 29.73 (36.68) 2.83 (2.92) 10.87 (3.54) 20.32 (8.86)
Template Full 36.80 (46.90) 3.40 (5.19) 11.03 (3.74) 20.77 (9.47)
Feature Mod 11.46 (14.46) 3.9 (5.97) 11.20 (3.94) 21.43 (10.49)
Template Feature Mod 13.80 (18.47) 4.22 (6.93) 11.28 (4.06) 21.81 (11.17)
Feature 11.60 (14.97) 5.19 (8.36) 11.72 (4.22) 23.41 (12.72)
Template Feature 14.24 (19.63) 6.86 (14.83) 11.94 (4.52) 24.47 (14.41)
Direct Mod 13.93 (19.73) 4.25 (6.49) 11.46 (4.27) 22.79 (13.44)
Template Direct Mod 17.28 (26.56) 4.62 (8.61) 11.45 (4.28) 22.95 (13.34)
Direct 19.95 (36.89) 8.08 (18.52) 12.81 (5.73) 32.85 (34.65)
Template Direct 28.02 (69.50) 9.98 (25.52) 12.95 (5.95) 33.36 (35.66)
Coventional 83.48 (167.51) 51.33 (132.43) 17.14 (8.02) 77.19 (76.26)
</table>
<tableCaption confidence="0.952592">
Table 3: Average parse times (SD), number of paths (SD), number of nodes (SD), and number of role values
(SD) remaining after parsing the 1,080 word graphs of 50 or fewer word nodes produced for the RM test set
using the 13 CDGs.
</tableCaption>
<table confidence="0.999928">
ARVP Variation Sentence Accuracy % Correct Words Word Accuracy
Full Mod 91.94% 98.55% 98.19%
Template Full Mod 91.57% 98.50% 98.14%
Full 91.57% 98.49% 98.11%
Template Full 91.20% 98.45% 98.05%
Feature Mod 90.56% 98.38% 97.95%
Template Feature Mod 90.19% 98.34% 97.90%
Feature 90.28% 98.35% 97.91%
Template Feature 89.91% 98.29% 97.85%
Direct Mod 90.46% 98.37% 97.91%
Template Direct Mod 90.09% 98.32% 97.86%
Direct 89.91% 98.30% 97.82%
Template Direct 89.44% 98.25% 97.75%
Conventional 81.20% 97.11% 96.10%
</table>
<tableCaption confidence="0.997236">
Table 4: The sentence accuracy, percent correct words, and word accuracy from parsing 1,080 word graphs
</tableCaption>
<bodyText confidence="0.991617117647059">
of 50 or fewer word nodes produced for the RM test set using the 13 CDGs.
racy appears to be correlated with the ability of the
constraints to eliminate word nodes from the word
graph during parsing. The least restrictive sentence
grammar, Direct, is less accurate than the other
sentence grammars and offers an intermediate speed
of parsing, most likely due to the increased ambigu-
ity in the parsing space. The fastest grammar was
the Feature-Mod grammar, which also offers an
intermediate level of accuracy. Its size (even with
templates), restrictiveness, and speed make it very
attractive. The template versions of each grammar
showed a slight increase in average parse times (from
processing a larger number of role values) and a
slight decrease in parsing accuracy. The conven-
tional grammar was the least competitive of the
grammars both in speed and in accuracy.
</bodyText>
<sectionHeader confidence="0.960989" genericHeader="conclusions">
5 Conclusion and Future Directions
</sectionHeader>
<bodyText confidence="0.999981368421053">
The ability to extract ARV/ARVP grammars with
varying degrees of specificity provides us with the
ability to rapidly develop a grammar with the abil-
ity to improve sentence accuracy of our speech sys-
tem. To achieve balance between precision and cov-
erage of our corpus-induced grammars, we have ex-
panded the RM sentences with templates for expres-
sions like dates and times. The grammars extracted
from these expanded sentences gave increased RM2
coverage without sacrificing even 1% of the sentence
accuracy. We are currently expanding the number of
templates in our grammar in an attempt to obtain
full coverage of the RM2 corpus using only template-
expanded RM sentences. We have recently added
ten semantic templates to the grammar and have
improved the coverage by 9.19% without losing any
sentence accuracy. We are also developing a stochas-
tic version of CDG that uses a statistical ARV, which
is similar to a supertag (Srinivas, 1996).
</bodyText>
<sectionHeader confidence="0.994902" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.860102">
H. U. Block. 1997. Language components in VERB-
MOBIL. In Proc. of the Int. Conf. of Acoustics,
Speech, and Signal Proc., pages 79-82.
</reference>
<page confidence="0.994898">
108
</page>
<reference confidence="0.99657080733945">
Defense Advanced Research Projects Agency
(DARPA). 1990. Extended resource manage-
ment: Continuous speech speaker-dependent
corpus (RM2). CD-ROM. NIST Speech Discs
3-1.2 and 3-2.2.
Entropic Cambridge Research Laboratory, Ltd.,
1997. HTK: Hidden Markov Model Toolkit V2.1.
M. P. Harper and R. A. Helzerman. 1995. Exten-
sions to constraint dependency parsing for spoken
language processing. Computer Speech and Lan-
guage, 9:187-234.
M. P. Harper, L. H. Jamieson, C. D. Mitchell,
G. Ying, S. Potisuk, P. N. Srinivasan, R. Chen,
C. B. Zoltowski, L. L. McPheters, B. Pellom,
and R. A. Helzerman. 1994. Integrating language
models with speech recognition. In Proc. of the
AAAI Workshop on the Integration of Natural
Language and Speech Processing, pages 139-146.
M. P. Harper, S. A. Hockema, and C. M. White.
1999a. Enhanced constraint dependency grammar
parsers. In Proc. of the IASTED Int. Conf. on
Artificial Intelligence and Soft Computing.
M. P. Harper, M. T. Johnson, L. H. Jamieson, and
C. M. White. 1999b. Interfacing a CDG parser
with an HMM word recognizer using word graphs.
In Proc. of the Int. Conf. of Acoustics, Speech, and
Signal Proc.
R. A. Helzerman and M. P. Harper. 1996. MUSE
CSP: An extension to the constraint satisfaction
problem. Journal of Artificial Intelligence Re-
search, 5:239-288.
P. Jeanrenaud, E. Eide, U. Chaudhari, J. Mc-
Donough, K. Ng, M. Siu, and H. Gish. 1995. Re-
ducing word error rate on conversational speech
from the Switchboard corpus. In Proc. of the
Int. Conf. of Acoustics, Speech, and Signal Proc.,
pages 53-56.
M. T. Johnson and M. P. Harper. 1999. Near min-
imal weighted word graphs for post-processing
speech. In 1999 Int. Workshop on Automatic
Speech Recognition and Understanding.
H. Maruyama. 1990a. Constraint Dependency
Grammar and its weak generative capacity. Com-
puter Software.
H. Maruyama. 1990b. Structural disambiguation
with constraint propagation. In Proc. of the An-
nual Meeting of Association for Computational
Linguistics, pages 31-38.
W. Menzel. 1994. Parsing of spoken language un-
der time constraints. In 11th European Conf. on
Artificial Intelligence, pages 560-564.
W. Menzel. 1995. Robust processing of natural lan-
guage. In Proc. of the 19th Annual German Conf.
on Artificial Intelligence.
B. Natarajan. 1989. On learning sets and functions.
Machine Learning, 4(1).
H. Ney, U. Essen, and R. Kneser. 1994. On struc-
turing probabilistic dependences in stochastic lan-
guage modelling. Computer Speech and Language,
8:1-38.
R. Pieraccini and E. Levin. 1992. Stochastic repre-
sentation of semantic structure for speech under-
standing. Speech Communication, 11:283-288.
P. Placeway, R. Schwartz, P. Fung, and L. Nguyen.
1993. The estimation of powerful language mod-
els from small and large corpora. In Proc. of the
Int. Conf. of Acoustics, Speech, and Signal Proc.,
pages 33-36.
P. J. Price, W. Fischer, J. Bernstein, and D. Pallett.
1988. A database for continuous speech recog-
nition in a 1000-word domain. In Proc. of the
Int. Conf. of Acoustics, Speech, and Signal Proc.,
pages 651-654.
L. A. Schmid. 1994. Parsing word graphs using a lin-
guistic grammar and a statistical language model.
In Proc. of the Int. Conf. of Acoustics, Speech,
and Signal Proc., pages 41-44.
S. Seneff. 1992. TINA: A natural language system
for spoken language applications. American Jour-
nal of Computational Linguistics, 18:61-86.
B. Srinivas. 1996. &apos;Almost parsing&apos; technique for
language modeling. In Proc. of the Int. Conf. on
Spoken Language Processing, pages 1173-1176.
L. G. Valiant. 1984. A theory of the learnable. Com-
munications of the ACM, 27(11):1134-1142.
C. M. White. 2000. Rapid Grammar Development
and Parsing Using Constraint Dependency Gram-
mars with Abstract Role Values. Ph.D. thesis,
Purdue University, School of Electrical and Com-
puter Engineering, West Lafayette, IN.
J. H. Wright, G. J. F. Jones, and H. Lloyd-Thomas.
1994. Robust language model incorporating a
substring parser and extended N-grams. In Proc.
of the Int. Conf. of Acoustics, Speech, and Signal
Proc., pages 361-364.
S. J. Young, N. H. Russell, and J. H. S. Thornton.
1989. Token passing : a simple conceptual model
for connected speech recognition systems. Tech-
nical Report TR38, Cambridge University, Cam-
bridge, England.
S. J. Young, J. Odell, D. 011ason, V. Valtchev, and
P. Woodland, 1997. The HTK Book. Entropic
Cambridge Research Laboratory Ltd., 2.1 edition.
V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips,
J. Polifroni, and S. Seneff. 1991. Integration of
speech recognition and natural language process-
ing in the MIT Voyager system. In Proc. of the
Int. Conf. of Acoustics, Speech, and Signal Proc.,
pages 713-716.
</reference>
<page confidence="0.998934">
109
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.818026">
<title confidence="0.998718">The Effectiveness of Corpus-Induced Dependency Grammars for Post-processing Speech*</title>
<author confidence="0.999989">M P Harper</author>
<author confidence="0.999989">C M White</author>
<author confidence="0.999989">W Wang</author>
<author confidence="0.999989">M T Johnson</author>
<author confidence="0.999989">R A Helzerman</author>
<affiliation confidence="0.9997405">School of Electrical and Computer Engineering Purdue University</affiliation>
<address confidence="0.998639">West Lafayette, IN 47907-1285</address>
<email confidence="0.998823">harper@ecn.purdue.edu</email>
<email confidence="0.998823">robot@ecn.purdue.edu</email>
<email confidence="0.998823">wang28@ecn.purdue.edu</email>
<email confidence="0.998823">mjohnson@ecn.purdue.edu</email>
<email confidence="0.998823">helz@ecn.purdue.edu</email>
<abstract confidence="0.998373757763975">This paper investigates the impact of Constraint Dependency Grammars (CDG) on the accuracy of an integrated speech recognition and CDG parsing system. We compare a conventional CDG with CDGs that are induced from annotated sentences and template-expanded sentences. The grammars are evaluated on parsing speed, precision/coverage, and improvement of word and sentence accuracy of the integrated system. Sentence-derived CDGs significantly improve recognition accuracy over the conventional CDG but are less general. Expanding the sentences with templates provides us with a mechanism for increasing the coverage of the grammar with only minor reductions in recognition accuracy. 1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more challenging and as speech recognizers are used in human/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many systems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more accurate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without a structural analysis of an input, it is difficult to guarantee the correctness of the mapping from a sentence to its interpretation (e.g., mathematical expressions to internal calculations). We believe that significant additional improvement in accuracy can be gained specific domains by using a more complex lan- * This research was supported by grants from Intel, Purdue Research Foundation, and National Science Foundation IRI 97-04358, CDA 96-17388, and #9980054-BCS. guage model that combines syntactic, semantic, and domain knowledge. A language processing module that is more powerful than a regular grammar can be loosely, moderately, or tightly integrated with the spoken language system, and there are advantages and disadvantages associated with each choice (Harper et al., 1994). To tightly integrate a language model with the power of a context-free grammar with the acoustic module requires that the power of the two modules be matched, making the integrated system fairly intractable and difficult to train. By separating the language model from the acoustic model, it becomes possible to use a more powerful language model without increasing computational costs or the amount of acoustic training data required by the recognizer. Furthermore, a loosely-integrated language model can be developed independently of the speech recognition component, which is clearly an advantage. Decoupling the acoustic and language models also adds flexibility: a wide variety of language models can be tried with a single acoustic model. Systems that utilize a language model that operates as a post-processor to a speech recognizer include (Block, 1997; Seneff, 1992; Zue et al., 1991). The goal of this research is to construct and experimentally evaluate a prototype of a spoken language system that loosely integrates a speech recognition component with an NLP component that uses syntactic, semantic, and domain-specific knowledge to more accurately select the sentence uttered by a speaker. First we describe the system we have built. Then we describe the mechanism used to rapidly develop a domain-specific grammar that improves accuracy of our speech recognizer. 2 Our System We have developed the prototype spoken language system depicted in Figure 1 that integrates a speech recognition component based on HMMs with a powerful grammar model based on Constraint Dependency Grammar (CDG). The speech recognizer is implemented as a multiple-mixture triphone HMM a simple integrated word co-occurrence gram- 102 mar (Ent, 1997; Young et al., 1997). Mel-scale cepstral coefficients, energy, and each of their their first and second order differences are used as the underlying feature vector for each speech frame. Model training is done using standard Baum-Welch Maximum Likelihood parameter re-estimation on diagonal covariance Gaussian Mixture Model (GMM) feature distributions. The speech recognizer employs a token-passing version of the Viterbi algorithm (Young et al., 1989) and pruning settings to produce a pruned recognition lattice. This pruned lattice contains the most likely alternative sentences that account for the sounds present in an utterance as well as their probabilities. Without any loss of information, this lattice is then compressed into a word graph (Harper et al., 1999b; Johnson and Harper, 1999), which acts as the interface between the recognizer and the CDG parser. The word graph algorithm begins with the recognition lattice and eliminates identical subgraphs by iteratively combining word nodes that have exactly the same preceding or following nodes (as well as edge probabilities), pushing excess probability to adjacent nodes whenever possible. The resulting word graph represents all possible word-level paths without eliminating or adding any paths or modifying their probabilities. Word graphs increase the bandwidth of useful acoustic information passed from the HMM to the CDG parser compared to most current speech recognition systems. The CDG parser parses the word graph to identify the best sentence consistent with both the acoustics of the utterance and its own additional knowledge. The loose coupling of the parser with the HMM allows us to construct a more powerful combined system without increasing the amount of training data for the HMM or the computational complexity of either of the component modules. Our NLP component is implemented using a CDG parser (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b) because of its power and flexibility, in particular: • It supports the use of syntactic, semantic, and domain-specific knowledge in a uniform framework. • Our CDG parser supports efficient simultaneous parsing of alternative sentence hypotheses in a word graph (Harper and Helzerman, 1995; Helzerman and Harper, 1996). • Because CDG is a dependency grammar, it can better model free-order languages. Hence, CDG can be used in processing a wider variety of human languages than other grammar paradigms. • It is capable of representing and using contextdependent information unlike traditional grammar approaches, thus providing a finer degree of control over the syntactic analysis of a sentence. • A CDG can be extracted directly from sentences annotated with dependency information (i.e., feature and syntactic relationships). We hypothesize that the accuracy of the combined HMM/CDG system should benefit from the ability to create a grammar that covers the domain as precisely as possible and that does not consider sentences that would not make sense given the domain. A corpus-based grammar is likely to have this degree of control. In the next section we describe how we construct a CDG from corpora. Figure 1: Block diagram of the loosely-coupled spoken language system. 3 Learning CDG Rules In this section, we introduce CDG and then describe how CDG constraints can be learned from sentences annotated with grammatical information.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>H U Block</author>
</authors>
<title>Language components in VERBMOBIL.</title>
<date>1997</date>
<booktitle>In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc.,</booktitle>
<pages>79--82</pages>
<contexts>
<context position="1306" citStr="Block, 1997" startWordPosition="183" endWordPosition="184">tegrated system. Sentence-derived CDGs significantly improve recognition accuracy over the conventional CDG but are less general. Expanding the sentences with templates provides us with a mechanism for increasing the coverage of the grammar with only minor reductions in recognition accuracy. 1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more challenging and as speech recognizers are used in human/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many systems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more accurate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without </context>
<context position="3561" citStr="Block, 1997" startWordPosition="538" endWordPosition="539">anguage model from the acoustic model, it becomes possible to use a more powerful language model without increasing computational costs or the amount of acoustic training data required by the recognizer. Furthermore, a loosely-integrated language model can be developed independently of the speech recognition component, which is clearly an advantage. Decoupling the acoustic and language models also adds flexibility: a wide variety of language models can be tried with a single acoustic model. Systems that utilize a language model that operates as a post-processor to a speech recognizer include (Block, 1997; Seneff, 1992; Zue et al., 1991). The goal of this research is to construct and experimentally evaluate a prototype of a spoken language system that loosely integrates a speech recognition component with an NLP component that uses syntactic, semantic, and domain-specific knowledge to more accurately select the sentence uttered by a speaker. First we describe the system we have built. Then we describe the mechanism used to rapidly develop a domain-specific grammar that improves accuracy of our speech recognizer. 2 Our System We have developed the prototype spoken language system depicted in Fi</context>
</contexts>
<marker>Block, 1997</marker>
<rawString>H. U. Block. 1997. Language components in VERBMOBIL. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 79-82.</rawString>
</citation>
<citation valid="true">
<title>Defense Advanced Research Projects Agency (DARPA).</title>
<date>1990</date>
<journal>CD-ROM. NIST Speech Discs</journal>
<volume>3</volume>
<pages>3--2</pages>
<marker>1990</marker>
<rawString>Defense Advanced Research Projects Agency (DARPA). 1990. Extended resource management: Continuous speech speaker-dependent corpus (RM2). CD-ROM. NIST Speech Discs 3-1.2 and 3-2.2.</rawString>
</citation>
<citation valid="true">
<date>1997</date>
<booktitle>HTK: Hidden Markov Model Toolkit V2.1.</booktitle>
<institution>Entropic Cambridge Research Laboratory, Ltd.,</institution>
<marker>1997</marker>
<rawString>Entropic Cambridge Research Laboratory, Ltd., 1997. HTK: Hidden Markov Model Toolkit V2.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>R A Helzerman</author>
</authors>
<title>Extensions to constraint dependency parsing for spoken language processing.</title>
<date>1995</date>
<journal>Computer Speech and Language,</journal>
<pages>9--187</pages>
<contexts>
<context position="6357" citStr="Harper and Helzerman, 1995" startWordPosition="980" endWordPosition="983">ilities. Word graphs increase the bandwidth of useful acoustic information passed from the HMM to the CDG parser compared to most current speech recognition systems. The CDG parser parses the word graph to identify the best sentence consistent with both the acoustics of the utterance and its own additional knowledge. The loose coupling of the parser with the HMM allows us to construct a more powerful combined system without increasing the amount of training data for the HMM or the computational complexity of either of the component modules. Our NLP component is implemented using a CDG parser (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b) because of its power and flexibility, in particular: • It supports the use of syntactic, semantic, and domain-specific knowledge in a uniform framework. • Our CDG parser supports efficient simultaneous parsing of alternative sentence hypotheses in a word graph (Harper and Helzerman, 1995; Helzerman and Harper, 1996). • Because CDG is a dependency grammar, it can better model free-order languages. Hence, CDG can be used in processing a wider variety of human languages than other grammar paradigms. • It is capable of representing and using contextdependent inf</context>
<context position="15965" citStr="Harper and Helzerman, 1995" startWordPosition="2650" endWordPosition="2653">ssible positions, MOD = {1, 2, ..., n} is the set of possible modifiees, and n is sentence length (which can be any arbitrary natural number), then unary constraints partition Si into grammatical and ungrammatical role values. Similarly, binary constraints partition the set S2 = S x S1 = S? into compatible and incompatible pairs. Building upon this concept of role value partitioning, it is possible to construct another way of representing unary and binary constraints because CDG constraints do not need to reference the exact position of a word or a modifiee in the sentence to parse sentences (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b; Menzel, 1994; Menzel, 1995). To represent the relative, rather than the absolute, position information for the role values in a grammatical sentence, it is only necessary to represent the positional relations between the modifiees and the positions of the role values. To support an arity of 2, these relations involve either equality or less-than relations over the modifiees and positions of role values assigned to the roles xi and s2. Since unary constraints operate over role values assigned to a single role, the only relative position relations that can be </context>
</contexts>
<marker>Harper, Helzerman, 1995</marker>
<rawString>M. P. Harper and R. A. Helzerman. 1995. Extensions to constraint dependency parsing for spoken language processing. Computer Speech and Language, 9:187-234.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>L H Jamieson</author>
<author>C D Mitchell</author>
<author>G Ying</author>
<author>S Potisuk</author>
<author>P N Srinivasan</author>
<author>R Chen</author>
<author>C B Zoltowski</author>
<author>L L McPheters</author>
<author>B Pellom</author>
<author>R A Helzerman</author>
</authors>
<title>Integrating language models with speech recognition.</title>
<date>1994</date>
<booktitle>In Proc. of the AAAI Workshop on the Integration of Natural Language and Speech Processing,</booktitle>
<pages>139--146</pages>
<contexts>
<context position="2698" citStr="Harper et al., 1994" startWordPosition="401" endWordPosition="404">rnal calculations). We believe that significant additional improvement in accuracy can be gained in specific domains by using a more complex lan* This research was supported by grants from Intel, Purdue Research Foundation, and National Science Foundation IRI 97-04358, CDA 96-17388, and #9980054-BCS. guage model that combines syntactic, semantic, and domain knowledge. A language processing module that is more powerful than a regular grammar can be loosely, moderately, or tightly integrated with the spoken language system, and there are advantages and disadvantages associated with each choice (Harper et al., 1994). To tightly integrate a language model with the power of a context-free grammar with the acoustic module requires that the power of the two modules be matched, making the integrated system fairly intractable and difficult to train. By separating the language model from the acoustic model, it becomes possible to use a more powerful language model without increasing computational costs or the amount of acoustic training data required by the recognizer. Furthermore, a loosely-integrated language model can be developed independently of the speech recognition component, which is clearly an advanta</context>
</contexts>
<marker>Harper, Jamieson, Mitchell, Ying, Potisuk, Srinivasan, Chen, Zoltowski, McPheters, Pellom, Helzerman, 1994</marker>
<rawString>M. P. Harper, L. H. Jamieson, C. D. Mitchell, G. Ying, S. Potisuk, P. N. Srinivasan, R. Chen, C. B. Zoltowski, L. L. McPheters, B. Pellom, and R. A. Helzerman. 1994. Integrating language models with speech recognition. In Proc. of the AAAI Workshop on the Integration of Natural Language and Speech Processing, pages 139-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Harper</author>
<author>S A Hockema</author>
<author>C M White</author>
</authors>
<title>Enhanced constraint dependency grammar parsers.</title>
<date>1999</date>
<booktitle>In Proc. of the IASTED Int. Conf. on Artificial Intelligence and Soft</booktitle>
<contexts>
<context position="5214" citStr="Harper et al., 1999" startWordPosition="799" endWordPosition="802">ature vector for each speech frame. Model training is done using standard Baum-Welch Maximum Likelihood parameter re-estimation on diagonal covariance Gaussian Mixture Model (GMM) feature distributions. The speech recognizer employs a token-passing version of the Viterbi algorithm (Young et al., 1989) and pruning settings to produce a pruned recognition lattice. This pruned lattice contains the most likely alternative sentences that account for the sounds present in an utterance as well as their probabilities. Without any loss of information, this lattice is then compressed into a word graph (Harper et al., 1999b; Johnson and Harper, 1999), which acts as the interface between the recognizer and the CDG parser. The word graph algorithm begins with the recognition lattice and eliminates identical subgraphs by iteratively combining word nodes that have exactly the same preceding or following nodes (as well as edge probabilities), pushing excess probability to adjacent nodes whenever possible. The resulting word graph represents all possible word-level paths without eliminating or adding any paths or modifying their probabilities. Word graphs increase the bandwidth of useful acoustic information passed f</context>
<context position="8758" citStr="Harper et al., 1999" startWordPosition="1382" endWordPosition="1385">the solutions are the parses. A CDG is defined as a five-tuple, (E, R, L,C,T), where E = • • • , crcl is a finite set of lexical categories (e.g., determiner), R = {ri, , rp} is a finite set of uniquely named roles or role ids (e.g., governor, needl, need2), L = fli, ,1,71 is a finite set of labels (e.g., subject), C is a constraint formula, and T is a table that specifies allowable category-role-label combinations. A sentence s = w1w2w3 ...wn has length n and is an element of E*. For each word wi E E of a sentence s, there are up to p different roles (with most words needing only one or two (Harper et al., 1999a)), yielding a maximum of n * p roles for the entire sentence. A role is a variable that is assigned a role value, an element of the set L X {1, 2,...,n}. Role values are denoted as 1-rn, where 1 E L and m E {1,2, ..., n} is called the modzfiee. Maruyama originally used a modifiee of NIL to indicate that a role value does not require a modifiee, but it is more parsimonious to indicate that there is no dependent by setting the modifiee to the position of its word. Role values are assigned to roles to record the syntactic dependencies between words in the sentence. Dictionary HMM Speech Recogni</context>
<context position="11414" citStr="Harper et al., 1999" startWordPosition="1875" endWordPosition="1878">he logical connectives and, or, if, or not. A subformula is a unary constraint if it contains only a single variable (by convention, we use x1) and a binary constraint if it contains two variables (by convention x1 and x2). An example of a unary and binary constraint appears in Figure 2. A CDG has an arity parameter a, which indicates the maximum number of variables in the subformulas of C, and a degree parameter d, which is the number of roles in the grammar. An arity of two suffices to represent a grammar at least as powerful as a context-free grammar (Maruyama, 1990a; Maruyama, 1990b). In (Harper et al., 1999a), we developed a way to write constraints concerning the category and feature values of a modifiee of a role value (or role value pair). These constraints loosely capture binary constraint information in unary constraints (or beyond binary for binary constraints) and results in more efficient parsing. Figure 2: A Unary and binary constraint for CDG. The white box in Figure 3 depicts a parse for the sentence Clear the screen from the Resource Management corpus (Price et al., 1988) (the ARV and ARVP in the gray box will be discussed later), which is a corpus we will use to evaluate our speech </context>
<context position="24590" citStr="Harper et al., 1999" startWordPosition="4048" endWordPosition="4051">Vs represent the set of observed role value assignments. In our implementation, each ARV includes: the label of the role value, the role to which the role value was assigned, the lexical category and feature values of the word containing the role, the relative position of the word and the role value&apos;s modifiee, and the modifiee&apos;s lexical category and feature values (modifiee constraints). We use modifiee constraints for ARVs regardless of extraction method because their use does not change the coverage of the extracted grammar and not using the information would significantly slow the parser (Harper et al., 1999a). Because the ARVP space is larger than the ARV space, we investigate six variations for extracting the pairs: 1. Full Mod: contains all grammar and feature value information for all pairs of role values from annotated sentences, as well as modifiee constraints. For a role value pair in a sentence to be considered valid during parsing with this grammar, it must match an ARVP extracted from the annotated sentences. 2. Full: like Full Mod except it does not impose modifiee constraints on a pair of role values during parsing. 3. Feature Mod: contains all grammar relations between all pairs of r</context>
</contexts>
<marker>Harper, Hockema, White, 1999</marker>
<rawString>M. P. Harper, S. A. Hockema, and C. M. White. 1999a. Enhanced constraint dependency grammar parsers. In Proc. of the IASTED Int. Conf. on Artificial Intelligence and Soft Computing. M. P. Harper, M. T. Johnson, L. H. Jamieson, and C. M. White. 1999b. Interfacing a CDG parser with an HMM word recognizer using word graphs. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R A Helzerman</author>
<author>M P Harper</author>
</authors>
<title>MUSE CSP: An extension to the constraint satisfaction problem.</title>
<date>1996</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>5--239</pages>
<contexts>
<context position="6710" citStr="Helzerman and Harper, 1996" startWordPosition="1033" endWordPosition="1037">with the HMM allows us to construct a more powerful combined system without increasing the amount of training data for the HMM or the computational complexity of either of the component modules. Our NLP component is implemented using a CDG parser (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b) because of its power and flexibility, in particular: • It supports the use of syntactic, semantic, and domain-specific knowledge in a uniform framework. • Our CDG parser supports efficient simultaneous parsing of alternative sentence hypotheses in a word graph (Harper and Helzerman, 1995; Helzerman and Harper, 1996). • Because CDG is a dependency grammar, it can better model free-order languages. Hence, CDG can be used in processing a wider variety of human languages than other grammar paradigms. • It is capable of representing and using contextdependent information unlike traditional grammar approaches, thus providing a finer degree of control over the syntactic analysis of a sentence. • A CDG can be extracted directly from sentences annotated with dependency information (i.e., feature and syntactic relationships). We hypothesize that the accuracy of the combined HMM/CDG system should benefit from the a</context>
</contexts>
<marker>Helzerman, Harper, 1996</marker>
<rawString>R. A. Helzerman and M. P. Harper. 1996. MUSE CSP: An extension to the constraint satisfaction problem. Journal of Artificial Intelligence Research, 5:239-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Jeanrenaud</author>
<author>E Eide</author>
<author>U Chaudhari</author>
<author>J McDonough</author>
<author>K Ng</author>
<author>M Siu</author>
<author>H Gish</author>
</authors>
<title>Reducing word error rate on conversational speech from the Switchboard corpus.</title>
<date>1995</date>
<booktitle>In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc.,</booktitle>
<pages>53--56</pages>
<contexts>
<context position="1543" citStr="Jeanrenaud et al., 1995" startWordPosition="219" endWordPosition="222"> of the grammar with only minor reductions in recognition accuracy. 1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more challenging and as speech recognizers are used in human/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many systems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more accurate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without a structural analysis of an input, it is difficult to guarantee the correctness of the mapping from a sentence to its interpretation (e.g., mathematical expressions to internal calculations). We believe that significant additional improv</context>
</contexts>
<marker>Jeanrenaud, Eide, Chaudhari, McDonough, Ng, Siu, Gish, 1995</marker>
<rawString>P. Jeanrenaud, E. Eide, U. Chaudhari, J. McDonough, K. Ng, M. Siu, and H. Gish. 1995. Reducing word error rate on conversational speech from the Switchboard corpus. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 53-56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M T Johnson</author>
<author>M P Harper</author>
</authors>
<title>Near minimal weighted word graphs for post-processing speech. In</title>
<date>1999</date>
<booktitle>Int. Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="5242" citStr="Johnson and Harper, 1999" startWordPosition="803" endWordPosition="806">speech frame. Model training is done using standard Baum-Welch Maximum Likelihood parameter re-estimation on diagonal covariance Gaussian Mixture Model (GMM) feature distributions. The speech recognizer employs a token-passing version of the Viterbi algorithm (Young et al., 1989) and pruning settings to produce a pruned recognition lattice. This pruned lattice contains the most likely alternative sentences that account for the sounds present in an utterance as well as their probabilities. Without any loss of information, this lattice is then compressed into a word graph (Harper et al., 1999b; Johnson and Harper, 1999), which acts as the interface between the recognizer and the CDG parser. The word graph algorithm begins with the recognition lattice and eliminates identical subgraphs by iteratively combining word nodes that have exactly the same preceding or following nodes (as well as edge probabilities), pushing excess probability to adjacent nodes whenever possible. The resulting word graph represents all possible word-level paths without eliminating or adding any paths or modifying their probabilities. Word graphs increase the bandwidth of useful acoustic information passed from the HMM to the CDG parse</context>
<context position="28041" citStr="Johnson and Harper, 1999" startWordPosition="4617" endWordPosition="4620">rs by using each grammar to parse the 7,396 RM2 sentences. See the results in Table 2. The grammar with the greatest generality was the conventional CDG for the RM corpus; however, this grammar also has the unfortunate attribute of being quite ambiguous. The most generalizable of extracted grammars uses the Direct method on template-expanded sentences. In all cases, the template-expanded sentence grammars gave better coverage than their corresponding sentence-only grammars. We have also used the extracted grammars to post-process word graphs created by the word graph compression algorithm of (Johnson and Harper, 1999) for the test utterances in the RM corpus. As was reported in (Johnson and Harper, 1999), the word-error rate of our HMM recognizer with an embedded word pair language model on the RM test set Table 2: Number of successfully parsed sentences in RM2 using the conventional CDG and CDGs derived from sentences only or template-expanded sentences. ARVP # Parsed with # Parsed with Variation Sentence CDG Template CDG Full Mod 3,735 (50.50%) 4,461 (60.32%) Full 4,509 (60.97%) 5,316 (71.88%) Feature Mod 5,365 (72.54%) 5,927 (80.14%) Feature 5,772 (78.04%) 6,208 (83.94%) Direct Mod 5,464 (73.88%) 5,979 </context>
</contexts>
<marker>Johnson, Harper, 1999</marker>
<rawString>M. T. Johnson and M. P. Harper. 1999. Near minimal weighted word graphs for post-processing speech. In 1999 Int. Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maruyama</author>
</authors>
<title>Constraint Dependency Grammar and its weak generative capacity.</title>
<date>1990</date>
<journal>Computer Software.</journal>
<contexts>
<context position="6373" citStr="Maruyama, 1990" startWordPosition="984" endWordPosition="985">e the bandwidth of useful acoustic information passed from the HMM to the CDG parser compared to most current speech recognition systems. The CDG parser parses the word graph to identify the best sentence consistent with both the acoustics of the utterance and its own additional knowledge. The loose coupling of the parser with the HMM allows us to construct a more powerful combined system without increasing the amount of training data for the HMM or the computational complexity of either of the component modules. Our NLP component is implemented using a CDG parser (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b) because of its power and flexibility, in particular: • It supports the use of syntactic, semantic, and domain-specific knowledge in a uniform framework. • Our CDG parser supports efficient simultaneous parsing of alternative sentence hypotheses in a word graph (Harper and Helzerman, 1995; Helzerman and Harper, 1996). • Because CDG is a dependency grammar, it can better model free-order languages. Hence, CDG can be used in processing a wider variety of human languages than other grammar paradigms. • It is capable of representing and using contextdependent information unlike </context>
<context position="7939" citStr="Maruyama, 1990" startWordPosition="1232" endWordPosition="1233">e a grammar that covers the domain as precisely as possible and that does not consider sentences that would not make sense given the domain. A corpus-based grammar is likely to have this degree of control. In the next section we describe how we construct a CDG from corpora. Figure 1: Block diagram of the loosely-coupled spoken language system. 3 Learning CDG Rules In this section, we introduce CDG and then describe how CDG constraints can be learned from sentences annotated with grammatical information. 3.1 Introduction to CDG Constraint Dependency Grammar (CDG), first introduced by Maruyama (Maruyama, 1990a; Maruyama, 1990b), uses constraints to determine the grammatical dependencies for a sentence. The parsing algorithm is framed as a constraint satisfaction problem: the rules are the constraints and the solutions are the parses. A CDG is defined as a five-tuple, (E, R, L,C,T), where E = • • • , crcl is a finite set of lexical categories (e.g., determiner), R = {ri, , rp} is a finite set of uniquely named roles or role ids (e.g., governor, needl, need2), L = fli, ,1,71 is a finite set of labels (e.g., subject), C is a constraint formula, and T is a table that specifies allowable category-role-</context>
<context position="11370" citStr="Maruyama, 1990" startWordPosition="1870" endWordPosition="1871">g =, &lt;, or &gt;, or predicates joined by the logical connectives and, or, if, or not. A subformula is a unary constraint if it contains only a single variable (by convention, we use x1) and a binary constraint if it contains two variables (by convention x1 and x2). An example of a unary and binary constraint appears in Figure 2. A CDG has an arity parameter a, which indicates the maximum number of variables in the subformulas of C, and a degree parameter d, which is the number of roles in the grammar. An arity of two suffices to represent a grammar at least as powerful as a context-free grammar (Maruyama, 1990a; Maruyama, 1990b). In (Harper et al., 1999a), we developed a way to write constraints concerning the category and feature values of a modifiee of a role value (or role value pair). These constraints loosely capture binary constraint information in unary constraints (or beyond binary for binary constraints) and results in more efficient parsing. Figure 2: A Unary and binary constraint for CDG. The white box in Figure 3 depicts a parse for the sentence Clear the screen from the Resource Management corpus (Price et al., 1988) (the ARV and ARVP in the gray box will be discussed later), which is </context>
<context position="15981" citStr="Maruyama, 1990" startWordPosition="2654" endWordPosition="2655">2, ..., n} is the set of possible modifiees, and n is sentence length (which can be any arbitrary natural number), then unary constraints partition Si into grammatical and ungrammatical role values. Similarly, binary constraints partition the set S2 = S x S1 = S? into compatible and incompatible pairs. Building upon this concept of role value partitioning, it is possible to construct another way of representing unary and binary constraints because CDG constraints do not need to reference the exact position of a word or a modifiee in the sentence to parse sentences (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b; Menzel, 1994; Menzel, 1995). To represent the relative, rather than the absolute, position information for the role values in a grammatical sentence, it is only necessary to represent the positional relations between the modifiees and the positions of the role values. To support an arity of 2, these relations involve either equality or less-than relations over the modifiees and positions of role values assigned to the roles xi and s2. Since unary constraints operate over role values assigned to a single role, the only relative position relations that can be tested are betwe</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>H. Maruyama. 1990a. Constraint Dependency Grammar and its weak generative capacity. Computer Software.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Maruyama</author>
</authors>
<title>Structural disambiguation with constraint propagation.</title>
<date>1990</date>
<booktitle>In Proc. of the Annual Meeting of Association for Computational Linguistics,</booktitle>
<pages>31--38</pages>
<contexts>
<context position="6373" citStr="Maruyama, 1990" startWordPosition="984" endWordPosition="985">e the bandwidth of useful acoustic information passed from the HMM to the CDG parser compared to most current speech recognition systems. The CDG parser parses the word graph to identify the best sentence consistent with both the acoustics of the utterance and its own additional knowledge. The loose coupling of the parser with the HMM allows us to construct a more powerful combined system without increasing the amount of training data for the HMM or the computational complexity of either of the component modules. Our NLP component is implemented using a CDG parser (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b) because of its power and flexibility, in particular: • It supports the use of syntactic, semantic, and domain-specific knowledge in a uniform framework. • Our CDG parser supports efficient simultaneous parsing of alternative sentence hypotheses in a word graph (Harper and Helzerman, 1995; Helzerman and Harper, 1996). • Because CDG is a dependency grammar, it can better model free-order languages. Hence, CDG can be used in processing a wider variety of human languages than other grammar paradigms. • It is capable of representing and using contextdependent information unlike </context>
<context position="7939" citStr="Maruyama, 1990" startWordPosition="1232" endWordPosition="1233">e a grammar that covers the domain as precisely as possible and that does not consider sentences that would not make sense given the domain. A corpus-based grammar is likely to have this degree of control. In the next section we describe how we construct a CDG from corpora. Figure 1: Block diagram of the loosely-coupled spoken language system. 3 Learning CDG Rules In this section, we introduce CDG and then describe how CDG constraints can be learned from sentences annotated with grammatical information. 3.1 Introduction to CDG Constraint Dependency Grammar (CDG), first introduced by Maruyama (Maruyama, 1990a; Maruyama, 1990b), uses constraints to determine the grammatical dependencies for a sentence. The parsing algorithm is framed as a constraint satisfaction problem: the rules are the constraints and the solutions are the parses. A CDG is defined as a five-tuple, (E, R, L,C,T), where E = • • • , crcl is a finite set of lexical categories (e.g., determiner), R = {ri, , rp} is a finite set of uniquely named roles or role ids (e.g., governor, needl, need2), L = fli, ,1,71 is a finite set of labels (e.g., subject), C is a constraint formula, and T is a table that specifies allowable category-role-</context>
<context position="11370" citStr="Maruyama, 1990" startWordPosition="1870" endWordPosition="1871">g =, &lt;, or &gt;, or predicates joined by the logical connectives and, or, if, or not. A subformula is a unary constraint if it contains only a single variable (by convention, we use x1) and a binary constraint if it contains two variables (by convention x1 and x2). An example of a unary and binary constraint appears in Figure 2. A CDG has an arity parameter a, which indicates the maximum number of variables in the subformulas of C, and a degree parameter d, which is the number of roles in the grammar. An arity of two suffices to represent a grammar at least as powerful as a context-free grammar (Maruyama, 1990a; Maruyama, 1990b). In (Harper et al., 1999a), we developed a way to write constraints concerning the category and feature values of a modifiee of a role value (or role value pair). These constraints loosely capture binary constraint information in unary constraints (or beyond binary for binary constraints) and results in more efficient parsing. Figure 2: A Unary and binary constraint for CDG. The white box in Figure 3 depicts a parse for the sentence Clear the screen from the Resource Management corpus (Price et al., 1988) (the ARV and ARVP in the gray box will be discussed later), which is </context>
<context position="15981" citStr="Maruyama, 1990" startWordPosition="2654" endWordPosition="2655">2, ..., n} is the set of possible modifiees, and n is sentence length (which can be any arbitrary natural number), then unary constraints partition Si into grammatical and ungrammatical role values. Similarly, binary constraints partition the set S2 = S x S1 = S? into compatible and incompatible pairs. Building upon this concept of role value partitioning, it is possible to construct another way of representing unary and binary constraints because CDG constraints do not need to reference the exact position of a word or a modifiee in the sentence to parse sentences (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b; Menzel, 1994; Menzel, 1995). To represent the relative, rather than the absolute, position information for the role values in a grammatical sentence, it is only necessary to represent the positional relations between the modifiees and the positions of the role values. To support an arity of 2, these relations involve either equality or less-than relations over the modifiees and positions of role values assigned to the roles xi and s2. Since unary constraints operate over role values assigned to a single role, the only relative position relations that can be tested are betwe</context>
</contexts>
<marker>Maruyama, 1990</marker>
<rawString>H. Maruyama. 1990b. Structural disambiguation with constraint propagation. In Proc. of the Annual Meeting of Association for Computational Linguistics, pages 31-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Menzel</author>
</authors>
<title>Parsing of spoken language under time constraints.</title>
<date>1994</date>
<booktitle>In 11th European Conf. on Artificial Intelligence,</booktitle>
<pages>560--564</pages>
<contexts>
<context position="16013" citStr="Menzel, 1994" startWordPosition="2658" endWordPosition="2659">modifiees, and n is sentence length (which can be any arbitrary natural number), then unary constraints partition Si into grammatical and ungrammatical role values. Similarly, binary constraints partition the set S2 = S x S1 = S? into compatible and incompatible pairs. Building upon this concept of role value partitioning, it is possible to construct another way of representing unary and binary constraints because CDG constraints do not need to reference the exact position of a word or a modifiee in the sentence to parse sentences (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b; Menzel, 1994; Menzel, 1995). To represent the relative, rather than the absolute, position information for the role values in a grammatical sentence, it is only necessary to represent the positional relations between the modifiees and the positions of the role values. To support an arity of 2, these relations involve either equality or less-than relations over the modifiees and positions of role values assigned to the roles xi and s2. Since unary constraints operate over role values assigned to a single role, the only relative position relations that can be tested are between the role value&apos;s position (de</context>
</contexts>
<marker>Menzel, 1994</marker>
<rawString>W. Menzel. 1994. Parsing of spoken language under time constraints. In 11th European Conf. on Artificial Intelligence, pages 560-564.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Menzel</author>
</authors>
<title>Robust processing of natural language.</title>
<date>1995</date>
<booktitle>In Proc. of the 19th Annual German Conf. on Artificial Intelligence.</booktitle>
<contexts>
<context position="16028" citStr="Menzel, 1995" startWordPosition="2660" endWordPosition="2661"> n is sentence length (which can be any arbitrary natural number), then unary constraints partition Si into grammatical and ungrammatical role values. Similarly, binary constraints partition the set S2 = S x S1 = S? into compatible and incompatible pairs. Building upon this concept of role value partitioning, it is possible to construct another way of representing unary and binary constraints because CDG constraints do not need to reference the exact position of a word or a modifiee in the sentence to parse sentences (Harper and Helzerman, 1995; Maruyama, 1990a; Maruyama, 1990b; Menzel, 1994; Menzel, 1995). To represent the relative, rather than the absolute, position information for the role values in a grammatical sentence, it is only necessary to represent the positional relations between the modifiees and the positions of the role values. To support an arity of 2, these relations involve either equality or less-than relations over the modifiees and positions of role values assigned to the roles xi and s2. Since unary constraints operate over role values assigned to a single role, the only relative position relations that can be tested are between the role value&apos;s position (denoted as Psi) a</context>
</contexts>
<marker>Menzel, 1995</marker>
<rawString>W. Menzel. 1995. Robust processing of natural language. In Proc. of the 19th Annual German Conf. on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Natarajan</author>
</authors>
<title>On learning sets and functions.</title>
<date>1989</date>
<booktitle>Machine Learning,</booktitle>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="19300" citStr="Natarajan, 1989" startWordPosition="3219" endWordPosition="3220">finition for the binary constraints2. The gray box of Figure 3 shows an example of an ARVP obtained from the parsed sentence. Positive ARVPs can be obtained directly from the parses of sentences. For each pair of role values assigned to different roles, simply extract their category, feature, role, and label information, and then determine the positional relations that hold between the positions and modifiees. An enumeration of the positive ARV/ARVPs can be used to represent the CDG constraints, C, and ARWARVPs are PAC-learnable from positive examples, as can be shown using the techniques of (Natarajan, 1989; Valiant, 1984). ARV/ARVP constraints can be enforced by using a fast table lookup method to see if a role value (or role value pair) is allowed (rather than propagating thousands of constraints), thus speeding up the parser. 4 Evaluation Using the Naval Resource Management Domain An experiment was conducted to determine the plausibility and the benefits of extracting CDG constraints from a domain-specific corpus of sentences. For our speech application, the ideal CDG should be general enough to cover sentences similar to those that appear in the corpus while being restrictive enough to elimi</context>
</contexts>
<marker>Natarajan, 1989</marker>
<rawString>B. Natarajan. 1989. On learning sets and functions. Machine Learning, 4(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>U Essen</author>
<author>R Kneser</author>
</authors>
<title>On structuring probabilistic dependences in stochastic language modelling.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<pages>8--1</pages>
<contexts>
<context position="1561" citStr="Ney et al., 1994" startWordPosition="223" endWordPosition="226"> minor reductions in recognition accuracy. 1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more challenging and as speech recognizers are used in human/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many systems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more accurate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without a structural analysis of an input, it is difficult to guarantee the correctness of the mapping from a sentence to its interpretation (e.g., mathematical expressions to internal calculations). We believe that significant additional improvement in accuracy </context>
</contexts>
<marker>Ney, Essen, Kneser, 1994</marker>
<rawString>H. Ney, U. Essen, and R. Kneser. 1994. On structuring probabilistic dependences in stochastic language modelling. Computer Speech and Language, 8:1-38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pieraccini</author>
<author>E Levin</author>
</authors>
<title>Stochastic representation of semantic structure for speech understanding.</title>
<date>1992</date>
<journal>Speech Communication,</journal>
<pages>11--283</pages>
<contexts>
<context position="1334" citStr="Pieraccini and Levin, 1992" startWordPosition="185" endWordPosition="188">em. Sentence-derived CDGs significantly improve recognition accuracy over the conventional CDG but are less general. Expanding the sentences with templates provides us with a mechanism for increasing the coverage of the grammar with only minor reductions in recognition accuracy. 1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more challenging and as speech recognizers are used in human/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many systems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more accurate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without a structural analysis of an </context>
</contexts>
<marker>Pieraccini, Levin, 1992</marker>
<rawString>R. Pieraccini and E. Levin. 1992. Stochastic representation of semantic structure for speech understanding. Speech Communication, 11:283-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Placeway</author>
<author>R Schwartz</author>
<author>P Fung</author>
<author>L Nguyen</author>
</authors>
<title>The estimation of powerful language models from small and large corpora.</title>
<date>1993</date>
<booktitle>In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc.,</booktitle>
<pages>33--36</pages>
<contexts>
<context position="1585" citStr="Placeway et al., 1993" startWordPosition="227" endWordPosition="230">in recognition accuracy. 1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more challenging and as speech recognizers are used in human/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many systems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more accurate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without a structural analysis of an input, it is difficult to guarantee the correctness of the mapping from a sentence to its interpretation (e.g., mathematical expressions to internal calculations). We believe that significant additional improvement in accuracy can be gained in specifi</context>
</contexts>
<marker>Placeway, Schwartz, Fung, Nguyen, 1993</marker>
<rawString>P. Placeway, R. Schwartz, P. Fung, and L. Nguyen. 1993. The estimation of powerful language models from small and large corpora. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 33-36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P J Price</author>
<author>W Fischer</author>
<author>J Bernstein</author>
<author>D Pallett</author>
</authors>
<title>A database for continuous speech recognition in a 1000-word domain.</title>
<date>1988</date>
<booktitle>In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc.,</booktitle>
<pages>651--654</pages>
<contexts>
<context position="11900" citStr="Price et al., 1988" startWordPosition="1955" endWordPosition="1958">ces to represent a grammar at least as powerful as a context-free grammar (Maruyama, 1990a; Maruyama, 1990b). In (Harper et al., 1999a), we developed a way to write constraints concerning the category and feature values of a modifiee of a role value (or role value pair). These constraints loosely capture binary constraint information in unary constraints (or beyond binary for binary constraints) and results in more efficient parsing. Figure 2: A Unary and binary constraint for CDG. The white box in Figure 3 depicts a parse for the sentence Clear the screen from the Resource Management corpus (Price et al., 1988) (the ARV and ARVP in the gray box will be discussed later), which is a corpus we will use to evaluate our speech processing system. We have constructed a conventional CDG with around 1,500 unary and binary constraints (i.e., its arity is 2) that were designed to parse the sentences in the corpus. This CDG covers a wide variety of grammar constructs (including conjunctions and wh-movement) and has a fairly rich semantics. It uses 16 lexical categories, 4 roles (so its degree is 4), 24 labels, and 13 lexical feature types (subcat, agr, case, vtype (e.g., progressive), mood, gap, inverted, voice</context>
<context position="20612" citStr="Price et al., 1988" startWordPosition="3428" endWordPosition="3431">er a grammar extracted from annotated sentences in a corpus achieves this precision of coverage. We also examine whether a learned grammar has the ability to filter out incorrect sentence hypotheses produced by the HMM component of our system in Figure 1. To investigate these issues, we have performed an experiment using the standard IA1 can also include information about the possible lexical categories and feature values of the modifiee of x1. 2A2 can also include information about the possible lexical categories and feature values of the modifiees of x1 and X2. 105 Resource Management (RM) (Price et al., 1988) and Extended Resource Management (RM2) ((DARPA), 1990) corpora. These mid-size speech corpora have a vocabulary of 991 words and contain utterances of sentences derived from sentence templates based on interviews with naval personnel familiar with naval resource management tasks. They were chosen for several reasons: they are two existing speech corpora from the same domain; their manageable sizes make them a good platform for the development of techniques that require extensive experimentation; and the sentences have both syntactic variety and reasonably rich semantics. RM contains 5,190 sep</context>
</contexts>
<marker>Price, Fischer, Bernstein, Pallett, 1988</marker>
<rawString>P. J. Price, W. Fischer, J. Bernstein, and D. Pallett. 1988. A database for continuous speech recognition in a 1000-word domain. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 651-654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L A Schmid</author>
</authors>
<title>Parsing word graphs using a linguistic grammar and a statistical language model.</title>
<date>1994</date>
<booktitle>In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc.,</booktitle>
<pages>41--44</pages>
<contexts>
<context position="1348" citStr="Schmid, 1994" startWordPosition="189" endWordPosition="190">gnificantly improve recognition accuracy over the conventional CDG but are less general. Expanding the sentences with templates provides us with a mechanism for increasing the coverage of the grammar with only minor reductions in recognition accuracy. 1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more challenging and as speech recognizers are used in human/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many systems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more accurate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without a structural analysis of an input, it is d</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>L. A. Schmid. 1994. Parsing word graphs using a linguistic grammar and a statistical language model. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 41-44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
</authors>
<title>TINA: A natural language system for spoken language applications.</title>
<date>1992</date>
<journal>American Journal of Computational Linguistics,</journal>
<pages>18--61</pages>
<contexts>
<context position="3575" citStr="Seneff, 1992" startWordPosition="540" endWordPosition="541"> from the acoustic model, it becomes possible to use a more powerful language model without increasing computational costs or the amount of acoustic training data required by the recognizer. Furthermore, a loosely-integrated language model can be developed independently of the speech recognition component, which is clearly an advantage. Decoupling the acoustic and language models also adds flexibility: a wide variety of language models can be tried with a single acoustic model. Systems that utilize a language model that operates as a post-processor to a speech recognizer include (Block, 1997; Seneff, 1992; Zue et al., 1991). The goal of this research is to construct and experimentally evaluate a prototype of a spoken language system that loosely integrates a speech recognition component with an NLP component that uses syntactic, semantic, and domain-specific knowledge to more accurately select the sentence uttered by a speaker. First we describe the system we have built. Then we describe the mechanism used to rapidly develop a domain-specific grammar that improves accuracy of our speech recognizer. 2 Our System We have developed the prototype spoken language system depicted in Figure 1 that in</context>
</contexts>
<marker>Seneff, 1992</marker>
<rawString>S. Seneff. 1992. TINA: A natural language system for spoken language applications. American Journal of Computational Linguistics, 18:61-86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Srinivas</author>
</authors>
<title>Almost parsing&apos; technique for language modeling.</title>
<date>1996</date>
<booktitle>In Proc. of the Int. Conf. on Spoken Language Processing,</booktitle>
<pages>1173--1176</pages>
<marker>Srinivas, 1996</marker>
<rawString>B. Srinivas. 1996. &apos;Almost parsing&apos; technique for language modeling. In Proc. of the Int. Conf. on Spoken Language Processing, pages 1173-1176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L G Valiant</author>
</authors>
<title>A theory of the learnable.</title>
<date>1984</date>
<journal>Communications of the ACM,</journal>
<pages>27--11</pages>
<contexts>
<context position="19316" citStr="Valiant, 1984" startWordPosition="3221" endWordPosition="3222">binary constraints2. The gray box of Figure 3 shows an example of an ARVP obtained from the parsed sentence. Positive ARVPs can be obtained directly from the parses of sentences. For each pair of role values assigned to different roles, simply extract their category, feature, role, and label information, and then determine the positional relations that hold between the positions and modifiees. An enumeration of the positive ARV/ARVPs can be used to represent the CDG constraints, C, and ARWARVPs are PAC-learnable from positive examples, as can be shown using the techniques of (Natarajan, 1989; Valiant, 1984). ARV/ARVP constraints can be enforced by using a fast table lookup method to see if a role value (or role value pair) is allowed (rather than propagating thousands of constraints), thus speeding up the parser. 4 Evaluation Using the Naval Resource Management Domain An experiment was conducted to determine the plausibility and the benefits of extracting CDG constraints from a domain-specific corpus of sentences. For our speech application, the ideal CDG should be general enough to cover sentences similar to those that appear in the corpus while being restrictive enough to eliminate sentences t</context>
</contexts>
<marker>Valiant, 1984</marker>
<rawString>L. G. Valiant. 1984. A theory of the learnable. Communications of the ACM, 27(11):1134-1142.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C M White</author>
</authors>
<title>Rapid Grammar Development and Parsing Using Constraint Dependency Grammars with Abstract Role Values.</title>
<date>2000</date>
<tech>Ph.D. thesis,</tech>
<institution>Purdue University, School of Electrical and Computer Engineering,</institution>
<location>West Lafayette, IN.</location>
<contexts>
<context position="21918" citStr="White, 2000" startWordPosition="3631" endWordPosition="3632">600 testing). We have extracted several types of CDGs from annotations of the RM sentences and tested their generality using the 7,396 sentences in RM2 (out of the 8,173) that are in the resource management domain but are distinct from the RM sentences. We compare these CDGs to each other and to the conventional CDG described previously. The corpus-based CDGs were created by extracting the allowable grammar relationships from the RM sentences that were annotated by language experts using the SENATOR annotation tool, a CGI (Common Gateway Interace) HTML script written in GNU C++ version 2.8.1 (White, 2000). We tested two major CDG variations: those derived directly from the RM sentences (Sentence CDGs) and those derived from simple template-expanded RM sentences (Template CDGs). For example, &amp;quot;List MIDPAC&apos;s deployments during (date)&amp;quot; is a sentence containing a date template which allows any date representations. For these experiments, we focused on templates for dates, years, times, numbers, and latitude and longitude coordinates. Each template name identifies a sub-grammar which was produced by annotating the appropriate strings. We then annotated sentences containing the template names as if t</context>
<context position="23710" citStr="White, 2000" startWordPosition="3910" endWordPosition="3911">alues that are not directly related, • specific feature information (e.g., semantics or subcategorization) . Initially, we bootstrapped the grammar by annotating a 200 sentence subset of the RM corpus and extracting a fairly general grammar from the annotations. Then using increasingly restrictive grammars at each iteration, we used the current grammar to identify sentences that required annotation and verified the parse information for sentences that succeeded. This iterative technique reduced the time required to build a CDG from about one year for the conventional CDG to around two months (White, 2000). Several methods of extracting an ARV/ARVP grammar from sentences or template-extended sentences were investigated. The ARVPs are extracted differently for each method; whereas, the ARVs are extracted in the same manner regardless of the method. Recall that ARVs represent the set of observed role value assignments. In our implementation, each ARV includes: the label of the role value, the role to which the role value was assigned, the lexical category and feature values of the word containing the role, the relative position of the word and the role value&apos;s modifiee, and the modifiee&apos;s lexical</context>
</contexts>
<marker>White, 2000</marker>
<rawString>C. M. White. 2000. Rapid Grammar Development and Parsing Using Constraint Dependency Grammars with Abstract Role Values. Ph.D. thesis, Purdue University, School of Electrical and Computer Engineering, West Lafayette, IN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Wright</author>
<author>G J F Jones</author>
<author>H Lloyd-Thomas</author>
</authors>
<title>Robust language model incorporating a substring parser and extended N-grams.</title>
<date>1994</date>
<booktitle>In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc.,</booktitle>
<pages>361--364</pages>
<contexts>
<context position="1369" citStr="Wright et al., 1994" startWordPosition="191" endWordPosition="194">prove recognition accuracy over the conventional CDG but are less general. Expanding the sentences with templates provides us with a mechanism for increasing the coverage of the grammar with only minor reductions in recognition accuracy. 1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more challenging and as speech recognizers are used in human/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many systems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more accurate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without a structural analysis of an input, it is difficult to guarantee</context>
</contexts>
<marker>Wright, Jones, Lloyd-Thomas, 1994</marker>
<rawString>J. H. Wright, G. J. F. Jones, and H. Lloyd-Thomas. 1994. Robust language model incorporating a substring parser and extended N-grams. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 361-364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Young</author>
<author>N H Russell</author>
<author>J H S Thornton</author>
</authors>
<title>Token passing : a simple conceptual model for connected speech recognition systems.</title>
<date>1989</date>
<tech>Technical Report TR38,</tech>
<institution>Cambridge University,</institution>
<location>Cambridge, England.</location>
<contexts>
<context position="4897" citStr="Young et al., 1989" startWordPosition="748" endWordPosition="751"> Dependency Grammar (CDG). The speech recognizer is implemented as a multiple-mixture triphone HMM with a simple integrated word co-occurrence gram102 mar (Ent, 1997; Young et al., 1997). Mel-scale cepstral coefficients, energy, and each of their their first and second order differences are used as the underlying feature vector for each speech frame. Model training is done using standard Baum-Welch Maximum Likelihood parameter re-estimation on diagonal covariance Gaussian Mixture Model (GMM) feature distributions. The speech recognizer employs a token-passing version of the Viterbi algorithm (Young et al., 1989) and pruning settings to produce a pruned recognition lattice. This pruned lattice contains the most likely alternative sentences that account for the sounds present in an utterance as well as their probabilities. Without any loss of information, this lattice is then compressed into a word graph (Harper et al., 1999b; Johnson and Harper, 1999), which acts as the interface between the recognizer and the CDG parser. The word graph algorithm begins with the recognition lattice and eliminates identical subgraphs by iteratively combining word nodes that have exactly the same preceding or following </context>
</contexts>
<marker>Young, Russell, Thornton, 1989</marker>
<rawString>S. J. Young, N. H. Russell, and J. H. S. Thornton. 1989. Token passing : a simple conceptual model for connected speech recognition systems. Technical Report TR38, Cambridge University, Cambridge, England.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S J Young</author>
<author>J Odell</author>
<author>D 011ason</author>
<author>V Valtchev</author>
<author>P Woodland</author>
</authors>
<date>1997</date>
<booktitle>The HTK Book. Entropic Cambridge Research Laboratory Ltd.,</booktitle>
<volume>2</volume>
<pages>edition.</pages>
<marker>Young, Odell, 011ason, Valtchev, Woodland, 1997</marker>
<rawString>S. J. Young, J. Odell, D. 011ason, V. Valtchev, and P. Woodland, 1997. The HTK Book. Entropic Cambridge Research Laboratory Ltd., 2.1 edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Zue</author>
<author>J Glass</author>
<author>D Goodine</author>
<author>H Leung</author>
<author>M Phillips</author>
<author>J Polifroni</author>
<author>S Seneff</author>
</authors>
<title>Integration of speech recognition and natural language processing in the MIT Voyager system.</title>
<date>1991</date>
<booktitle>In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc.,</booktitle>
<pages>713--716</pages>
<contexts>
<context position="1388" citStr="Zue et al., 1991" startWordPosition="195" endWordPosition="198">uracy over the conventional CDG but are less general. Expanding the sentences with templates provides us with a mechanism for increasing the coverage of the grammar with only minor reductions in recognition accuracy. 1 Background The question of when and how to integrate language models with speech recognition systems is gaining in importance as recognition tasks investigated by the speech community become increasingly more challenging and as speech recognizers are used in human/computer interfaces and dialog systems (Block, 1997; Pieraccini and Levin, 1992; Schmid, 1994; Wright et al., 1994; Zue et al., 1991). Many systems tightly integrate N-gram stochastic language models, with a power limited to a regular grammar, into the recognizer (Jeanrenaud et al., 1995; Ney et al., 1994; Placeway et al., 1993) to build more accurate speech recognizers. However, in order to act based on the spoken interaction with the user, the speech signal must be mapped to an internal representation. Obtaining a syntactic representation for the spoken utterance has a high degree of utility for mapping to a semantic representation. Without a structural analysis of an input, it is difficult to guarantee the correctness of</context>
<context position="3594" citStr="Zue et al., 1991" startWordPosition="542" endWordPosition="545">stic model, it becomes possible to use a more powerful language model without increasing computational costs or the amount of acoustic training data required by the recognizer. Furthermore, a loosely-integrated language model can be developed independently of the speech recognition component, which is clearly an advantage. Decoupling the acoustic and language models also adds flexibility: a wide variety of language models can be tried with a single acoustic model. Systems that utilize a language model that operates as a post-processor to a speech recognizer include (Block, 1997; Seneff, 1992; Zue et al., 1991). The goal of this research is to construct and experimentally evaluate a prototype of a spoken language system that loosely integrates a speech recognition component with an NLP component that uses syntactic, semantic, and domain-specific knowledge to more accurately select the sentence uttered by a speaker. First we describe the system we have built. Then we describe the mechanism used to rapidly develop a domain-specific grammar that improves accuracy of our speech recognizer. 2 Our System We have developed the prototype spoken language system depicted in Figure 1 that integrates a speech r</context>
</contexts>
<marker>Zue, Glass, Goodine, Leung, Phillips, Polifroni, Seneff, 1991</marker>
<rawString>V. Zue, J. Glass, D. Goodine, H. Leung, M. Phillips, J. Polifroni, and S. Seneff. 1991. Integration of speech recognition and natural language processing in the MIT Voyager system. In Proc. of the Int. Conf. of Acoustics, Speech, and Signal Proc., pages 713-716.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>