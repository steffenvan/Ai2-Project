<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000020">
<title confidence="0.7788375">
TALK&apos;N&apos;TRAVEL: A CONVERSATIONAL SYSTEM FOR AIR
TRAVEL PLANNING
</title>
<author confidence="0.655741">
David Stallard
</author>
<address confidence="0.835997333333333">
BBN Technologies, GTE
70 Fawcett St.
Cambridge, MA, USA, 02238
</address>
<email confidence="0.985367">
Stallard @bbn.c om
</email>
<sectionHeader confidence="0.992805" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999925538461538">
We describe Talk&apos;n&apos;Travel, a spoken
dialogue language system for making air
travel plans over the telephone.
Talk&apos;n&apos;Travel is a fully conversational,
mixed-initiative system that allows the
user to specify the constraints on his travel
plan in arbitrary order, ask questions, etc.,
in general spoken English. The system
operates according to a plan-based agenda
mechanism, rather than a finite state
network, and attempts to negotiate with
the user when not all of his constraints can
be met.
</bodyText>
<sectionHeader confidence="0.961434" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.99980788">
This paper describes Talk&apos;n&apos;Travel, a spoken
language dialogue system for making complex
air travel plans over the telephone.
Talk&apos;n&apos;Travel is a research prototype system
sponsored under the DARPA Communicator
program (MITRE, 1999). Some other systems
in the program are Ward and Pellom (1999),
Seneff and Polifroni (2000) and Rudnicky et al
(1999). The common task of this program is a
mixed-initiative dialogue over the telephone, in
which the user plans a multi-city trip by air,
including all flights, hotels, and rental cars, all in
conversational English over the telephone.
The Communicator common task presents
special challenges. It is a complex task with
many subtasks, including the booking of each
flight, hotel, and car reservation. Because the
number of legs of the trip may be arbitrary, the
number of such subtasks is not known in
advance. Furthermore, the user has complete
freedom to say anything at any time. His
utterances can affect just the current subtask, or
multiple subtasks at once (&amp;quot;I want to go from
Denver to Chicago and then to San Diego&amp;quot;). He
can go back and change the specifications for
completed subtasks. And there are important
constraints, such as temporal relationships
between flights, that must be maintained for the
solution to the whole task to be coherent.
In order to meet this challenge, we have sought
to develop dialogue techniques for
Talk&apos;n&apos;Travel that go beyond the rigid system-
directed style of familiar IVR systems.
Talk&apos;n&apos;Travel is instead a mixed initiative
system that allows the user to specify constraints
on his travel plan in arbitrary order. At any
point in the dialogue, the user can supply
information other than what the system is
currently prompting for, change his mind about
information he has previously given and even
ask questions himself. The system also tries to
be helpful, eliciting constraints from the user
when necessary. Furthermore, if at any point the
constraints the user has specified cannot all be
met, the system steps in and offers a relaxation
of them in an attempt to negotiate a partial
solution with the user.
The next section gives a brief overview of the
system. Relevant components are discussed in
subsequent sections.
</bodyText>
<sectionHeader confidence="0.97" genericHeader="method">
1 System Overview
</sectionHeader>
<bodyText confidence="0.99955575">
The system consists of the following modules:
speech recognizer, language understander,
dialogue manager, state manager, language
generator, and speech synthesizer. The modules
</bodyText>
<page confidence="0.99871">
68
</page>
<bodyText confidence="0.999860866666667">
interact with each other via the central hub
module of the Communicator Common
Architecture.
The speech recognizer is the Byblos system
(Nguyen, 1995). It uses an acoustic model
trained from the Macrophone telephone corpus,
and a bigram/trigram language model trained
from â€”40K utterances derived from various
sources, including data collected under the
previous ATIS program (Dahl et al, 1994).
The speech synthesizer is Lucent&apos;s commercial
system. Synthesizer and recognizer both
interface to the telephone via Dialogics
telephony board. The database is currently a
frozen snapshot of actual flights between 40
different US cities (we are currently engaged in
interfacing to a commercial air travel website).
The various language components are written in
Java. The complete system runs on Windows
NT, and is compliant with the DARPA
Communicator Common architecture.
The present paper is concerned with the dialogue
and discourse management, language generation
and language understanding components. In the
remainder of the paper, we present more detailed
discussion of these components, beginning with
the language understander in Section 2. Section
3 discusses the discourse and dialogue
components, and Section 4, the language
generator.
</bodyText>
<sectionHeader confidence="0.746879" genericHeader="method">
2 Language Understanding
</sectionHeader>
<subsectionHeader confidence="0.971168">
2.1 Meaning Representation
</subsectionHeader>
<bodyText confidence="0.999937695652174">
Semantic frames have proven useful as a
meaning representation for many applications.
Their simplicity and useful computational
properties have often been seen as more
important than their limitations in expressive
power, especially in simpler domains.
Even in such domains, however, frames still
have some shortcomings. While most naturally
representing equalities between slot and filler,
frames have a harder time with inequalities, such
as &apos;the departure time is before 10 AM&apos;, or &apos;the
airline is not Delta&apos;. These require the slot-filler
to be some sort of predicate, interval, or set
object, at a cost to simplicity uniformity. Other
problematic cases include n-ary relations ( &apos;3
miles from Denver&apos;), and disjunctions of
properties on different slots.
In our Talk&apos;n&apos;Travel work, we have developed
a meaning representation formalism called path
constraints, which overcomes these problems,
while retaining the computational advantages
that made frames attractive in the first place. A
path constraint is an expression of the form:
</bodyText>
<equation confidence="0.58703">
(&lt;path&gt; &lt;relation&gt; &lt;arguments&gt;*)
</equation>
<bodyText confidence="0.993267333333333">
The path is a compositional chain of one or more
attributes, and relations are 1-place or higher
predicates, whose first argument is implicitly the
path. The relation is followed by zero or more
other arguments. In the simplest case, path
constraints can be thought of as flattenings of a
tree of frames. The following represents the
constraint that the departure time of the first leg
of the itinerary is the city Boston:
</bodyText>
<sectionHeader confidence="0.536427" genericHeader="method">
LEGS.O.ORIG_CITY EQ BOSTON
</sectionHeader>
<bodyText confidence="0.99771325">
Because this syntax generalizes to any relation,
however, the constraint &amp;quot;departing before 10
AM&amp;quot; can be represented in a syntactically
equivalent way:
</bodyText>
<equation confidence="0.455621">
LEGS.O.DEPART_TIME LT 1000
</equation>
<bodyText confidence="0.999731">
Because the number of arguments is arbitrary, it
is equally straightforward to represent a one-
place property like &amp;quot;x is nonstop&amp;quot; and a three
place predicate like &amp;quot;x is 10 miles from Denver&amp;quot;.
Like frames, path constraints have a fixed
format that is indexed in a computationally
useful way, and are simpler than logical forms.
Unlike frames, however, path constraints can be
combined in arbitrary conjunctions, disjunctions,
and negations, even across different paths. Path
constraint meaning representations are also flat
lists of constraints rather than trees, making
matching rules, etc, easier to write for them.
</bodyText>
<page confidence="0.995772">
69
</page>
<subsectionHeader confidence="0.995658">
2.2 The GEM Understanding System
</subsectionHeader>
<bodyText confidence="0.9933465625">
Language understanding in Talk&apos;n&apos;Travel is
carried out using a system called GEM (for
Generative Extraction Model). GEM (Miller,
1998) is a probabilistic semantic grammar that is
an outgrowth of the work on the HUM system
(Miller, 1996), but uses hand-specified
knowledge in addition to probability. The hand-
specified knowledge is quite simple, and is
expressed by a two-level semantic dictionary. In
the first level, the entries map alternative word
strings to a single word class. For example, the
following entry maps several alternative forms
to the word class DEPART:
Leave, depart, get out of =&gt; DEPART
In the second level, entries map sequences of
word classes to constraints:
</bodyText>
<construct confidence="0.3064675">
Name: DepartCity 1
Head: DEPART
Classes: [DEPART FROM CITY]
Meaning: (DEST_CITY EQ &lt;CITY&gt;)
</construct>
<bodyText confidence="0.999881368421053">
The &amp;quot;head&amp;quot; feature allows the entry to pass one
of its constituent word classes up to a higher
level pattern, allowing the given pattern to be a
constituent of others.
The dictionary entries generate a probabilistic
recursive transition network (PRTN), whose
specific structure is determined by dictionary
entries. Paths through this network correspond
one-to-one with parse trees, so that given a path,
there is exactly one corresponding tree. The
probabilities for the arcs in this network can be
estimated from training data using the EM
(Expectation-Maximization) procedure.
GEM also includes a noise state to which
arbitrary input between patterns can be mapped,
making the system quite robust to ill-formed
input. There is no separate phase for handling
ungrammatical input, nor any distinction
between grammatical and ungrammatical input.
</bodyText>
<sectionHeader confidence="0.970344" genericHeader="method">
3 Discourse and Dialogue Processing
</sectionHeader>
<bodyText confidence="0.99776585">
A key feature of the Communicator task is that
the user can say anything at any time, adding or
changing information at will. He may add new
subtasks (e.g. trip legs) or modifying existing
ones. A conventional dialogue state network
approach would be therefore infeasible, as the
network would be almost unboundedly large and
complex.
A signifigant additional problem is that changes
need not be monotonic. In particular, when
changing his mind, or correcting the system&apos;s
misinterpretations, the user may delete subtask
structures altogether, as in the subdialog:
S: What day are you returning to Chicago?
U: No, I don&apos;t want a return flight.
Because they take information away rather than
add it, scenarios like this one make it
problematic to view discourse processing as
producing a contextualized, or &amp;quot;thick frame&amp;quot;,
version of the user&apos;s utterance. In our system,
therefore, we have chosen a somewhat different
approach.
The discourse processor, called the state
manager, computes the most likely new task
state, based on the user&apos;s input and the current
task state. It also computes a discourse event,
representing its interpretation of what happened
in the conversation as a result of the user&apos;s
utterance.
The dialogue manager is a separate module, as
has no state managing responsibilities at all.
Rather, it simply computes the next action to
take, based on its current goal agenda, the
discourse event returned by the state manager,
and the new state. This design has the advantage
of making the dialogue manager considerably
simpler. The discourse event also becomes
available to convey to the user as confirmation.
We discuss these two modules in more detail
below.
</bodyText>
<page confidence="0.987857">
70
</page>
<subsectionHeader confidence="0.998209">
3.1 State Manager
</subsectionHeader>
<bodyText confidence="0.999905769230769">
The state manager is responsible for computing
and maintaining the current task state. The task
state is simply the set of path constraints which
currently constrain the user&apos;s itinerary. Also
included in the task state are the history of user
and system utterances, and the current subtask
and object in focus, if any.
The state manager takes the N-best list of
recognition hypotheses as input. It invokes the
understanding module on a hypothesis to obtain
a semantic interpretation. The semantic
interpretation so obtained is subjected to the
following steps:
</bodyText>
<listItem confidence="0.9780065">
1. Resolve ellipses if any
2. Match input meaning to subtask(s)
3. Expand local ambiguities
4. Apply inference and coherency rules
5. Compute database satisfiers
6. Relax constraints if neccesary
7. Determine the most likely alternative and
compute the discourse event
</listItem>
<bodyText confidence="0.999856814814815">
At any of these steps, zero or more alternative
new states can result, and are fed to the next
step. If zero states result at any step, the new
meaning representation is rejected, and another
one requested from the understander. If no more
hypotheses are available, the entire utterance is
rejected, and a DONT_UNDERSTAND event is
returned to the dialogue manager.
Step 1 resolves ellipses. Ellipses include both
short responses like &amp;quot;Boston&amp;quot; and yes/no
responses. In this step, a complete meaning
representation such as `(ORIG_CITY EQ
BOSTON)&apos; is generated based on the system&apos;s
prompt and the input meaning. The hypothesis is
rejected if this cannot be done.
Step 2 matches the input meaning to one or more
of the subtasks of the problem. For the
Communicator problem, the subtasks are legs of
the user&apos;s itinerary, and matching is done based
on cities mentioned in the input meaning. The
default is the subtask currently in focus in the
dialogue.
A match to a subtask is represented by adding
the prefix for the subtask to the path of the
constraint. For example, &amp;quot;I want to arrive in
Denver by 4 PM&amp;quot; and then continue on to
Chicago would be:
</bodyText>
<sectionHeader confidence="0.602378" genericHeader="method">
LEGS.O.DEST_CITY EQ DENVER
LEGS.O.ARRIVE_TIME LE 1600
LEGS.1.0RIG_CITY EQ DENVER
LEGS.1.DEST_CITY EQ CHICAGO
</sectionHeader>
<bodyText confidence="0.999727909090909">
In Step 3, local ambiguities are expanded into
their different possibilities. These include
partially specified times such as &amp;quot;2 o&apos;clock &amp;quot;.
Step 4 applies inference and coherency rules.
These rules will vary from application to
application. They are written in the path
constraint formalism, augmented with variables
that can range over attributes and other values.
The following is an example, representing the
constraint a flight leg cannot be scheduled to
depart until after the preceding flight arrives:
</bodyText>
<equation confidence="0.764389666666667">
LEGS.$N.ARR1VE
LT
LEGS.$N-1-1.DEPART
</equation>
<bodyText confidence="0.99979105">
States that violate coherency constraints are
discarded.
Step 5 computes the set of objects in the
database that satisfy the constraints on the
current subtask. This set will be empty when the
constraints are not all satisfiable, in which case
the relaxation of Step 6 is invoked. This
relaxation is a best-first search for the satisfiable
subset of the constraints that are deemed closest
to what the user originally wanted. Alternative
relaxations are scored according to a sum of
penalty scores for each relaxed constraint,
derived from earlier work by Stallard (1995).
The penalty score is the sum of two terms: one
for the relative importance of the attribute
concerned (e.g. relaxations of DEPART_DATE
are penalised more than relaxations of
AIRLINE) and the other for the nearness of the
satisfiers to the original constraint (relevant for
number-like attributes like departure time).
</bodyText>
<page confidence="0.997071">
71
</page>
<bodyText confidence="0.999989931034483">
The latter allows the system to give credit to
solutions that are near fits to the user&apos;s goals,
even if they relax strongly desired constraints.
For example, suppose the user has expressed a
desire to fly on Delta and arrive by 3 PM, while
the system is only able to find a flight on Delta
that arrives at 3:15 PM. In this case, this flight,
which meets one constraint and almost meets the
other, may well satisfy the user more than a
flight on a different airline that happens to meet
the time constraint exactly.
In the final step, the alternative new states are
rank-ordered according to a pragmatic score, and
the highest-scoring alternative is chosen. The
pragmatic score is computed based on a number
of factors, including the plausibility of
disambiguated times and whether or not the state
interpreted the user as responding to the system
prompt.
The appropriate discourse event is then
deterministically computed and returned. There
are several types of discourse event. The most
common is UPDATE, which specifies the
constraints that have been added, removed, or
relaxed. Another type is REPEAT, which is
generated when the user has simply repeated
constraints the system already knows. Other
types include QUESTION, TIMEOUT, and
DONT_UNDERSTAND.
</bodyText>
<subsectionHeader confidence="0.99889">
3.1 Dialogue Manager
</subsectionHeader>
<bodyText confidence="0.963715245901639">
Upon receiving the new discourse event from
the state manager, the dialogue manager
determines what next action to take. Actions
can be external, such as speaking to the user or
asking him a question, or internal, such as
querying the database or other elements of the
system state. The current action is determined by
consulting a stack-based agenda of goals and
actions.
The agenda stack is in turn determined by an
application-dependent library of plans. Plans are
tree structures whose root is the name of the goal
the plan is designed to solve, and whose leaves
are either other goal names or actions. An
example of a plan is the following:
CompleteItinerary =&gt;
(Prompt &amp;quot;How can I help you?&amp;quot;)
(forall legs $n
GetRouteInfo
GetSpecificFlight
GetHotelAndCar
GetNextLeg))
This is a plan for achieving the goal
CompleteItinerary. It begins with a open-ended
prompt and then iterates over values of the
variable $N for which constraints on the prefix
LEGS.$N exist, working on high-level subgoals,
such as getting the route and booking a flight,
for each leg. The last goal determines whether
there is another leg to the itinerary, in which
case the itera
The system begins the interaction with the high-
level goal START on its stack. At each step, the
system examines the top of its goal stack and
either executes it if it is an action suitable for
execution, or replaces it on the stack with its
plan steps if it is a goal.
Actions are objects with success and relevancy
predicates and an execute method, somewhat
similar to the &amp;quot;handlers&amp;quot; of Rudnicky and Xu
(1999). An action has an underlying goal, such
as finding out the user&apos;s constraints on some
attribute. The action&apos;s success predicate will
return true if this underlying goal has been
achieved, and its relevancy predicate will return
true if it is still relevant to the current situation.
Before carrying out an action, the dialogue
manager first checks to see if its success
predicate returns false and its relevancy
predicate returns true. If either condition is not
met, the action is popped off the stack and
disposed of without being executed. Otherwise,
the action&apos;s execute method is invoked.
The system includes a set of actions that are
built in, and may be parameterized for each each
domain. For example, the action type ELICIT is
parameterized by an attribute A, a path prefix P,
and verbalization string S. Its success predicate
returns true if the path `P.A&apos; is constrained in the
current state. Its execute method generates a
meaning frame that is passed to the language
</bodyText>
<page confidence="0.995904">
72
</page>
<bodyText confidence="0.999940787234043">
generator, ultimately prompting the user with a
question such as &amp;quot;What city are you flying to?&amp;quot;
Once an action&apos;s execute method is invoked, it
remains on the stack for the next cycle, where it
is tested again for success and relevancy. In this
case, if the success condition is met â€” that is, if
the user did indeed reply with a specification of
his destination city â€” the action is popped off the
stack. If the system did not receive this
information, either because the user made a
stipulation about some different attribute, asked
a question, or simply was not understood, the
action remains on the stack to be executed again.
Of course, the user may have already specified
the destination city in a previous utterance. In
this case, the action is already satisfied, and is
not executed. In this way, the user has
flexibility in how he actually carries out the
dialogue.
In certain situations, other goals and actions may
be pushed onto the stack, temporarily
interrupting the execution of the current plan.
For example, the user himself may ask a
question. In this case, an action to answer the
question is created, and pushed onto the stack.
The dialogue manager then executes this action
to answer the user&apos;s question before continuing
on with the plan. Or the state manager may
generate a clarification question, which the
dialogue manager seeks to have the user answer.
Actions can also have a set of conditional
branchings that are tested after the action is
executed. If present, these determine the next
action to execute or goal to work on. For
example, the action that asks the user &amp;quot;Do you
want a return flight to X?&amp;quot; specifies the branch
to be taken when the user replies in the negative.
This branch includes an action that asks the user
&amp;quot;Is Y your final destination?&amp;quot;, an action that is
executed if the user did not specify an additional
destination along with his negative reply.
Unlike the approach taken by Ward and Pellom
(1999), which seeks to avoid scripting entirely
by driving the dialogue off the current status of
the itinerary, the Talk&apos;n&apos; Travel dialogue
manager thus seeks to allow partially scripted
dialogue where appropriate to the situation.
</bodyText>
<sectionHeader confidence="0.916302" genericHeader="method">
4 Language Generation
</sectionHeader>
<bodyText confidence="0.975841740740741">
The language generator takes a meaning frame
from the dialogue manager, and generates a text
string in English for it. It uses a set of pattern-
based rules that map constraints into alternative
syntactic realisations. For example, the
following rule allows a constraint on departure
time to be realized as &amp;quot;leave at 3 PM&amp;quot;
or &amp;quot;3 PM flight&amp;quot;:
LEG.$N.DEPART_TIME EQ $X
t* [leave at $X],
[nom-comp $X]
Different realization rules can be selected for
depending upon whether the constraint is to be
realized as an assertion or as a description. The
generation algorithm assembles the selected
realizations for each constraint into a simplified
syntax tree, selecting appropriate inflections of
verb and noun heads as it does so. Terminal
values in constraints are realized as type-specific
nominals, such as &amp;quot;3 PM&amp;quot; or &amp;quot;Delta&amp;quot;.
A crucial feature of the generation process is
that it adds to each prompt a paraphrase of the
most recent discourse event, corresponding to
what the system thinks the user just said. This
helps keep the conversation grounded in terms
of mutual understanding between the
participants.
</bodyText>
<sectionHeader confidence="0.995496" genericHeader="method">
5 Example Scenario
</sectionHeader>
<bodyText confidence="0.958507384615385">
The following is an example dialog with the
system:
S: Welcome to Talk&apos;n&apos;Travel. How can I help
you?
U: I need a flight from Boston to Denver
tomorrow.
S: On Thursday, November 18, flying from
Boston to Denver. What time do you want to
leave?
U: I want to leave before 10 AM.
S: Leaving before 10 AM. Do you have an
airline preference?
U: Continental.
</bodyText>
<page confidence="0.996905">
73
</page>
<bodyText confidence="0.8598425">
S: Continental. I&apos;m sorry, there are no flights on
Continental leaving before 10 AM. I can get
you a flight on Continental leaving at 10:10 AM
instead. Would that be OK?
U: Yes.
S: OK, I&apos;ve got you on Continental flight 429,
departing Boston at 10:10 AM, arriving at
Denver at 3:44 PM. Do you want to book a
return flight from Denver to Boston?
U: Yes, on Monday.
S: Alright, we&apos;ll add a flight on Monday,
November 22, from Boston to Denver to your
itinerary. I have a Continental flight at 7 AM.
Is that OK?
</bodyText>
<sectionHeader confidence="0.403838" genericHeader="method">
U. Anything later?
</sectionHeader>
<bodyText confidence="0.489178363636364">
S: Leaving after 7 AM. There are no flights on
Continental after 7 AM. I can get you a flight
after 7 AM on Frontier, Northwest, and United
instead. Would that be OK?
U: How about United.
S: United. I have a United flight at 8:01 AM. Is
that flight OK?
U: Yes.
S: OK, I&apos;ve got you on United flight 1762,
departing Denver at 8:01 AM, arriving at Boston
at 1:42 PM.
</bodyText>
<sectionHeader confidence="0.873913" genericHeader="conclusions">
6 Current Status and Conclusions
</sectionHeader>
<bodyText confidence="0.999964">
The Talk&apos;n&apos;Travel system described here was
successfully demonstrated at the DARPA
Communicator Compare and Contrast
Workshop in June 1999. We are currently
collecting data with test subjects and are using
the results to improve the system&apos;s performance
in all areas, in preparation for the forthcoming
common evaluation of Communicator systems
in June 2000.
To determine the performance of the system, we
ran an informal experiment in which 11 different
subjects called into the system and attempted to
use it to solve a travel problem. None of the
subjects were system developers. Each subject
had a single session in which he was given a
three-city trip to plan, including dates of travel,
constraints on departure and arrival times, airline
preferences.
8 of the subjects were successful. Of successful
sessions, the average duration was 387 seconds,
with a minimum of 272 and a maximum of 578.
The average number of user utterances was 25,
with a minimum of 18 and a maximum of 37.
The word error rate of the recognizer was
11.8%.
The primary cause of failure to complete the
scenario, as well as excessive time spent on
completing it, was corruption of the discourse
state due to recognition or interpretation errors.
While the system informs the user of the change
in state after every utterance, the user was not
always successful in correcting it when it made
errors, and sometimes the user did not even
notice when the system had made an error. If the
user is not attentive at the time, or happens not
to understand what the synthesizer said, there is
no implicit way for him to find out afterwards
what the system thinks his constraints are.
While preliminary, these results point to two
directions for future work. One is that the system
needs to be better able to recognize and deal
with problem situations in which the dialogue is
not advancing. The other is that the system
needs to be more communicative about its
current understanding of the user&apos;s goals, even
at points in the dialogue at which it might be
assumed that user and system were in
agreement.
</bodyText>
<sectionHeader confidence="0.996554" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9986922">
This work was sponsored by DARPA and
monitored by SPAWAR Systems Center under
Contract No. N66001-99-D-8615.
The author wishes to thank Scott Miller for the
use of his GEM system.
</bodyText>
<sectionHeader confidence="0.999447" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998108666666667">
MITRE (1999) DARPA Communicator homepage
http://fofoca.mitre.org/
Ward W., and Pellom, B. (1999) The CU
Communicator System. In 1999 IEEE Workshop
on Automatic Speech Recognition and
Understanding, Keystone, Colorado.
</reference>
<page confidence="0.971698">
711
</page>
<reference confidence="0.999458342105263">
Miller S. (1998) The Generative Extraction Model.
Unpublished manuscript.
Dahl D., Bates M., Brown M., Fisher, W. Hunicke-
Smith K., Pallet D., Pao C., Rudnicky A., and
Shriberg E. (1994) Expanding the scope of the
ATIS task. In Proceedings of the ARPA Spoken
Language Technology Workshop, Plainsboro, NJ.,
pp 3-8.
Constantinides P., Hansma S., Tchou C. and
Rudnicky, A. (1999) A schema-based approach to
dialog control. Proceedings of ICSLP, Paper 637.
Rudnicky A., Thayer, E., Constantinides P., Tchou
C., Shern, R., Lenzo K., Xu W., Oh A. (1999)
Creating natural dialogs in the Carnegie Mellon
Communicator system. Proceedings of
Eurospeech, 1999, Vol 4, pp. 1531-1534
Rudnicky A., and Xu W. (1999) An agenda-based
dialog management architecture for soken
language systems. In 1999 IEEE Workshop on
Automatic Speech Recognition and Understanding,
Keystone, Colorado.
Seneff S., and Polifroni, J. (2000) Dialogue
Management in the Mercury Flight Reservation
System. ANLP Conversational Systems Workshop.
Nguyen L., Anastasakos T., Kubala F., LaPre C.,
Makhoul J., Schwartz R., Yuan N., Zavaliagkos
G., and Zhao Y. (1995) The 1994 BBN/BYBLOS
Speech Recognition System, In Proc of ARPA
Spoken Language Systems Technology Workshop,
Austin, Texas, pp. 77-81.
Stallard D. (1995) The Initial Implementation of the
BBN ATIS4 Dialog System, In Proc of ARPA
Spoken Language Systems Technology Workshop,
Austin, Texas, pp. 208-211.
Miller S. and Stallard D. (1996) A Fully Statistical
Approach to Natural Language Interfaces, In Proc
of the 34th Annual Meeting of the Association for
Computational Linguistics, Santa Cruz, California.
</reference>
<page confidence="0.999135">
75
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.652541">
<title confidence="0.9994655">TALK&apos;N&apos;TRAVEL: A CONVERSATIONAL SYSTEM FOR AIR TRAVEL PLANNING</title>
<author confidence="0.999949">David Stallard</author>
<affiliation confidence="0.850087">BBN Technologies, GTE</affiliation>
<address confidence="0.995282">70 Fawcett St. Cambridge, MA, USA, 02238</address>
<email confidence="0.808857">Stallard@bbn.com</email>
<abstract confidence="0.996955857142857">We describe Talk&apos;n&apos;Travel, a spoken dialogue language system for making air travel plans over the telephone. Talk&apos;n&apos;Travel is a fully conversational, mixed-initiative system that allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English. The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>MITRE</author>
</authors>
<date>1999</date>
<note>DARPA Communicator homepage http://fofoca.mitre.org/</note>
<contexts>
<context position="906" citStr="MITRE, 1999" startWordPosition="131" endWordPosition="132"> a fully conversational, mixed-initiative system that allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English. The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met. Introduction This paper describes Talk&apos;n&apos;Travel, a spoken language dialogue system for making complex air travel plans over the telephone. Talk&apos;n&apos;Travel is a research prototype system sponsored under the DARPA Communicator program (MITRE, 1999). Some other systems in the program are Ward and Pellom (1999), Seneff and Polifroni (2000) and Rudnicky et al (1999). The common task of this program is a mixed-initiative dialogue over the telephone, in which the user plans a multi-city trip by air, including all flights, hotels, and rental cars, all in conversational English over the telephone. The Communicator common task presents special challenges. It is a complex task with many subtasks, including the booking of each flight, hotel, and car reservation. Because the number of legs of the trip may be arbitrary, the number of such subtasks </context>
</contexts>
<marker>MITRE, 1999</marker>
<rawString>MITRE (1999) DARPA Communicator homepage http://fofoca.mitre.org/</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Ward</author>
<author>B Pellom</author>
</authors>
<title>The CU Communicator System. In</title>
<date>1999</date>
<booktitle>IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<location>Keystone, Colorado.</location>
<contexts>
<context position="968" citStr="Ward and Pellom (1999)" startWordPosition="140" endWordPosition="143">t allows the user to specify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English. The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met. Introduction This paper describes Talk&apos;n&apos;Travel, a spoken language dialogue system for making complex air travel plans over the telephone. Talk&apos;n&apos;Travel is a research prototype system sponsored under the DARPA Communicator program (MITRE, 1999). Some other systems in the program are Ward and Pellom (1999), Seneff and Polifroni (2000) and Rudnicky et al (1999). The common task of this program is a mixed-initiative dialogue over the telephone, in which the user plans a multi-city trip by air, including all flights, hotels, and rental cars, all in conversational English over the telephone. The Communicator common task presents special challenges. It is a complex task with many subtasks, including the booking of each flight, hotel, and car reservation. Because the number of legs of the trip may be arbitrary, the number of such subtasks is not known in advance. Furthermore, the user has complete fr</context>
<context position="19392" citStr="Ward and Pellom (1999)" startWordPosition="3091" endWordPosition="3094">he dialogue manager seeks to have the user answer. Actions can also have a set of conditional branchings that are tested after the action is executed. If present, these determine the next action to execute or goal to work on. For example, the action that asks the user &amp;quot;Do you want a return flight to X?&amp;quot; specifies the branch to be taken when the user replies in the negative. This branch includes an action that asks the user &amp;quot;Is Y your final destination?&amp;quot;, an action that is executed if the user did not specify an additional destination along with his negative reply. Unlike the approach taken by Ward and Pellom (1999), which seeks to avoid scripting entirely by driving the dialogue off the current status of the itinerary, the Talk&apos;n&apos; Travel dialogue manager thus seeks to allow partially scripted dialogue where appropriate to the situation. 4 Language Generation The language generator takes a meaning frame from the dialogue manager, and generates a text string in English for it. It uses a set of patternbased rules that map constraints into alternative syntactic realisations. For example, the following rule allows a constraint on departure time to be realized as &amp;quot;leave at 3 PM&amp;quot; or &amp;quot;3 PM flight&amp;quot;: LEG.$N.DEPAR</context>
</contexts>
<marker>Ward, Pellom, 1999</marker>
<rawString>Ward W., and Pellom, B. (1999) The CU Communicator System. In 1999 IEEE Workshop on Automatic Speech Recognition and Understanding, Keystone, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
</authors>
<title>The Generative Extraction Model.</title>
<date>1998</date>
<note>Unpublished manuscript.</note>
<contexts>
<context position="6867" citStr="Miller, 1998" startWordPosition="1048" endWordPosition="1049">les from Denver&amp;quot;. Like frames, path constraints have a fixed format that is indexed in a computationally useful way, and are simpler than logical forms. Unlike frames, however, path constraints can be combined in arbitrary conjunctions, disjunctions, and negations, even across different paths. Path constraint meaning representations are also flat lists of constraints rather than trees, making matching rules, etc, easier to write for them. 69 2.2 The GEM Understanding System Language understanding in Talk&apos;n&apos;Travel is carried out using a system called GEM (for Generative Extraction Model). GEM (Miller, 1998) is a probabilistic semantic grammar that is an outgrowth of the work on the HUM system (Miller, 1996), but uses hand-specified knowledge in addition to probability. The handspecified knowledge is quite simple, and is expressed by a two-level semantic dictionary. In the first level, the entries map alternative word strings to a single word class. For example, the following entry maps several alternative forms to the word class DEPART: Leave, depart, get out of =&gt; DEPART In the second level, entries map sequences of word classes to constraints: Name: DepartCity 1 Head: DEPART Classes: [DEPART F</context>
</contexts>
<marker>Miller, 1998</marker>
<rawString>Miller S. (1998) The Generative Extraction Model. Unpublished manuscript.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Dahl</author>
<author>M Bates</author>
<author>M Brown</author>
<author>W HunickeSmith K Fisher</author>
<author>D Pallet</author>
<author>C Pao</author>
<author>A Rudnicky</author>
<author>E Shriberg</author>
</authors>
<title>Expanding the scope of the ATIS task.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Spoken Language Technology Workshop,</booktitle>
<pages>3--8</pages>
<location>Plainsboro, NJ.,</location>
<contexts>
<context position="3479" citStr="Dahl et al, 1994" startWordPosition="540" endWordPosition="543"> in subsequent sections. 1 System Overview The system consists of the following modules: speech recognizer, language understander, dialogue manager, state manager, language generator, and speech synthesizer. The modules 68 interact with each other via the central hub module of the Communicator Common Architecture. The speech recognizer is the Byblos system (Nguyen, 1995). It uses an acoustic model trained from the Macrophone telephone corpus, and a bigram/trigram language model trained from â€”40K utterances derived from various sources, including data collected under the previous ATIS program (Dahl et al, 1994). The speech synthesizer is Lucent&apos;s commercial system. Synthesizer and recognizer both interface to the telephone via Dialogics telephony board. The database is currently a frozen snapshot of actual flights between 40 different US cities (we are currently engaged in interfacing to a commercial air travel website). The various language components are written in Java. The complete system runs on Windows NT, and is compliant with the DARPA Communicator Common architecture. The present paper is concerned with the dialogue and discourse management, language generation and language understanding co</context>
</contexts>
<marker>Dahl, Bates, Brown, Fisher, Pallet, Pao, Rudnicky, Shriberg, 1994</marker>
<rawString>Dahl D., Bates M., Brown M., Fisher, W. HunickeSmith K., Pallet D., Pao C., Rudnicky A., and Shriberg E. (1994) Expanding the scope of the ATIS task. In Proceedings of the ARPA Spoken Language Technology Workshop, Plainsboro, NJ., pp 3-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Constantinides</author>
<author>S Hansma</author>
<author>C Tchou</author>
<author>A Rudnicky</author>
</authors>
<title>A schema-based approach to dialog control.</title>
<date>1999</date>
<booktitle>Proceedings of ICSLP, Paper 637.</booktitle>
<marker>Constantinides, Hansma, Tchou, Rudnicky, 1999</marker>
<rawString>Constantinides P., Hansma S., Tchou C. and Rudnicky, A. (1999) A schema-based approach to dialog control. Proceedings of ICSLP, Paper 637.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rudnicky</author>
<author>E Thayer</author>
<author>P Constantinides</author>
<author>C Tchou</author>
<author>R Shern</author>
<author>K Lenzo</author>
<author>W Xu</author>
<author>A Oh</author>
</authors>
<title>Creating natural dialogs in the Carnegie Mellon Communicator system.</title>
<date>1999</date>
<booktitle>Proceedings of Eurospeech,</booktitle>
<volume>4</volume>
<pages>1531--1534</pages>
<contexts>
<context position="1023" citStr="Rudnicky et al (1999)" startWordPosition="149" endWordPosition="152">el plan in arbitrary order, ask questions, etc., in general spoken English. The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met. Introduction This paper describes Talk&apos;n&apos;Travel, a spoken language dialogue system for making complex air travel plans over the telephone. Talk&apos;n&apos;Travel is a research prototype system sponsored under the DARPA Communicator program (MITRE, 1999). Some other systems in the program are Ward and Pellom (1999), Seneff and Polifroni (2000) and Rudnicky et al (1999). The common task of this program is a mixed-initiative dialogue over the telephone, in which the user plans a multi-city trip by air, including all flights, hotels, and rental cars, all in conversational English over the telephone. The Communicator common task presents special challenges. It is a complex task with many subtasks, including the booking of each flight, hotel, and car reservation. Because the number of legs of the trip may be arbitrary, the number of such subtasks is not known in advance. Furthermore, the user has complete freedom to say anything at any time. His utterances can a</context>
</contexts>
<marker>Rudnicky, Thayer, Constantinides, Tchou, Shern, Lenzo, Xu, Oh, 1999</marker>
<rawString>Rudnicky A., Thayer, E., Constantinides P., Tchou C., Shern, R., Lenzo K., Xu W., Oh A. (1999) Creating natural dialogs in the Carnegie Mellon Communicator system. Proceedings of Eurospeech, 1999, Vol 4, pp. 1531-1534</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rudnicky</author>
<author>W Xu</author>
</authors>
<title>An agenda-based dialog management architecture for soken language systems.</title>
<date>1999</date>
<booktitle>In 1999 IEEE Workshop on Automatic Speech Recognition and Understanding,</booktitle>
<location>Keystone, Colorado.</location>
<contexts>
<context position="16459" citStr="Rudnicky and Xu (1999)" startWordPosition="2589" endWordPosition="2592">prefix LEGS.$N exist, working on high-level subgoals, such as getting the route and booking a flight, for each leg. The last goal determines whether there is another leg to the itinerary, in which case the itera The system begins the interaction with the highlevel goal START on its stack. At each step, the system examines the top of its goal stack and either executes it if it is an action suitable for execution, or replaces it on the stack with its plan steps if it is a goal. Actions are objects with success and relevancy predicates and an execute method, somewhat similar to the &amp;quot;handlers&amp;quot; of Rudnicky and Xu (1999). An action has an underlying goal, such as finding out the user&apos;s constraints on some attribute. The action&apos;s success predicate will return true if this underlying goal has been achieved, and its relevancy predicate will return true if it is still relevant to the current situation. Before carrying out an action, the dialogue manager first checks to see if its success predicate returns false and its relevancy predicate returns true. If either condition is not met, the action is popped off the stack and disposed of without being executed. Otherwise, the action&apos;s execute method is invoked. The s</context>
</contexts>
<marker>Rudnicky, Xu, 1999</marker>
<rawString>Rudnicky A., and Xu W. (1999) An agenda-based dialog management architecture for soken language systems. In 1999 IEEE Workshop on Automatic Speech Recognition and Understanding, Keystone, Colorado.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>J Polifroni</author>
</authors>
<title>Dialogue Management in the Mercury Flight Reservation System. ANLP Conversational Systems Workshop.</title>
<date>2000</date>
<contexts>
<context position="997" citStr="Seneff and Polifroni (2000)" startWordPosition="144" endWordPosition="147">cify the constraints on his travel plan in arbitrary order, ask questions, etc., in general spoken English. The system operates according to a plan-based agenda mechanism, rather than a finite state network, and attempts to negotiate with the user when not all of his constraints can be met. Introduction This paper describes Talk&apos;n&apos;Travel, a spoken language dialogue system for making complex air travel plans over the telephone. Talk&apos;n&apos;Travel is a research prototype system sponsored under the DARPA Communicator program (MITRE, 1999). Some other systems in the program are Ward and Pellom (1999), Seneff and Polifroni (2000) and Rudnicky et al (1999). The common task of this program is a mixed-initiative dialogue over the telephone, in which the user plans a multi-city trip by air, including all flights, hotels, and rental cars, all in conversational English over the telephone. The Communicator common task presents special challenges. It is a complex task with many subtasks, including the booking of each flight, hotel, and car reservation. Because the number of legs of the trip may be arbitrary, the number of such subtasks is not known in advance. Furthermore, the user has complete freedom to say anything at any </context>
</contexts>
<marker>Seneff, Polifroni, 2000</marker>
<rawString>Seneff S., and Polifroni, J. (2000) Dialogue Management in the Mercury Flight Reservation System. ANLP Conversational Systems Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Nguyen</author>
<author>T Anastasakos</author>
<author>F Kubala</author>
<author>C LaPre</author>
<author>J Makhoul</author>
<author>R Schwartz</author>
<author>N Yuan</author>
<author>G Zavaliagkos</author>
<author>Y Zhao</author>
</authors>
<title>BBN/BYBLOS Speech Recognition System,</title>
<date>1995</date>
<booktitle>The</booktitle>
<pages>77--81</pages>
<location>Austin, Texas,</location>
<marker>Nguyen, Anastasakos, Kubala, LaPre, Makhoul, Schwartz, Yuan, Zavaliagkos, Zhao, 1995</marker>
<rawString>Nguyen L., Anastasakos T., Kubala F., LaPre C., Makhoul J., Schwartz R., Yuan N., Zavaliagkos G., and Zhao Y. (1995) The 1994 BBN/BYBLOS Speech Recognition System, In Proc of ARPA Spoken Language Systems Technology Workshop, Austin, Texas, pp. 77-81.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Stallard</author>
</authors>
<title>The Initial Implementation of the BBN ATIS4 Dialog System, In</title>
<date>1995</date>
<booktitle>Proc of ARPA Spoken Language Systems Technology Workshop,</booktitle>
<pages>208--211</pages>
<location>Austin, Texas,</location>
<contexts>
<context position="13292" citStr="Stallard (1995)" startWordPosition="2066" endWordPosition="2067">rives: LEGS.$N.ARR1VE LT LEGS.$N-1-1.DEPART States that violate coherency constraints are discarded. Step 5 computes the set of objects in the database that satisfy the constraints on the current subtask. This set will be empty when the constraints are not all satisfiable, in which case the relaxation of Step 6 is invoked. This relaxation is a best-first search for the satisfiable subset of the constraints that are deemed closest to what the user originally wanted. Alternative relaxations are scored according to a sum of penalty scores for each relaxed constraint, derived from earlier work by Stallard (1995). The penalty score is the sum of two terms: one for the relative importance of the attribute concerned (e.g. relaxations of DEPART_DATE are penalised more than relaxations of AIRLINE) and the other for the nearness of the satisfiers to the original constraint (relevant for number-like attributes like departure time). 71 The latter allows the system to give credit to solutions that are near fits to the user&apos;s goals, even if they relax strongly desired constraints. For example, suppose the user has expressed a desire to fly on Delta and arrive by 3 PM, while the system is only able to find a fl</context>
</contexts>
<marker>Stallard, 1995</marker>
<rawString>Stallard D. (1995) The Initial Implementation of the BBN ATIS4 Dialog System, In Proc of ARPA Spoken Language Systems Technology Workshop, Austin, Texas, pp. 208-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miller</author>
<author>D Stallard</author>
</authors>
<title>A Fully Statistical Approach to Natural Language Interfaces,</title>
<date>1996</date>
<booktitle>In Proc of the 34th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Santa Cruz, California.</location>
<marker>Miller, Stallard, 1996</marker>
<rawString>Miller S. and Stallard D. (1996) A Fully Statistical Approach to Natural Language Interfaces, In Proc of the 34th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, California.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>