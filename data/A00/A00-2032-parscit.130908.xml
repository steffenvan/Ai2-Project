<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000007">
<title confidence="0.949385">
Mostly-Unsupervised Statistical Segmentation of Japanese:
Applications to Kanji
</title>
<author confidence="0.946682">
Rie Kubota Ando and Lillian Lee
</author>
<affiliation confidence="0.9964505">
Department of Computer Science
Cornell University
</affiliation>
<address confidence="0.581769">
Ithaca, NY 14853-7501
</address>
<email confidence="0.992784">
Ocubotar,lleej @cs.cornell.edu
</email>
<sectionHeader confidence="0.99457" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999283">
Given the lack of word delimiters in written
Japanese, word segmentation is generally consid-
ered a crucial first step in processing Japanese texts.
Typical Japanese segmentation algorithms rely ei-
ther on a lexicon and grammar or on pre-segmented
data. In contrast, we introduce a novel statistical
method utilizing unsegmented training data, with
performance on kanji sequences comparable to and
sometimes surpassing that of morphological analyz-
ers over a variety of error metrics.
</bodyText>
<sectionHeader confidence="0.998418" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999814833333333">
Because Japanese is written without delimiters be-
tween words,&apos; accurate word segmentation to re-
cover the lexical items is a key step in Japanese text
processing. Proposed applications of segmentation
technology include extracting new technical terms,
indexing documents for information retrieval, and
correcting optical character recognition (OCR) er-
rors (Wu and Tseng, 1993; Nagao and Mori, 1994;
Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996;
Ring, 1998).
Typically, Japanese word segmentation is per-
formed by morphological analysis based on lexical
and grammatical knowledge. This analysis is aided
by the fact that there are three types of Japanese
characters, kanji, hiragana, and katakana: changes
in character type often indicate word boundaries, al-
though using this heuristic alone achieves less than
60% accuracy (Nagata, 1997).
Character sequences consisting solely of kanji
pose a challenge to morphologically-based seg-
menters for several reasons. First and most
importantly, kanji sequences often contain domain
terms and proper nouns: Fung (1998) notes that
50-85% of the terms in various technical dictio-
</bodyText>
<footnote confidence="0.9458195">
1The analogous situation in English would be if words were
written without spaces between them.
</footnote>
<figure confidence="0.9375468">
Sequence length # of characters % of corpus
1 - 3 kanji 20,405,486 25.6
4 - 6 kanji 12,743,177 16.1
more than 6 kanji 3,966,408 5.1
Total 37,115,071 46.8
</figure>
<figureCaption confidence="0.977101">
Figure 1: Statistics from 1993 Japanese newswire
(NIKKEI), 79,326,406 characters total.
</figureCaption>
<bodyText confidence="0.999796833333333">
naries are composed at least partly of kanji. Such
words tend to be missing from general-purpose
lexicons, causing an unknown word problem for
morphological analyzers; yet, these terms are quite
important for information retrieval, information
extraction, and text summarization, making correct
segmentation of these terms critical. Second, kanji
sequences often consist of compound nouns, so
grammatical constraints are not applicable. For
instance, the sequence sha-chohlkenigyoh-mulbu-
choh (president and business Igeneral manager
= &amp;quot;a president as well as a general manager of
business&amp;quot;) could be incorrectly segmented as: sha-
chohlken-gyohlmulbu-choh (president&apos; subsidiary
business ITsutomu [a name] jgeneral manager);
since both alternatives are four-noun sequences,
they cannot be distinguished by part-of-speech
information alone. Finally, heuristics based on
changes in character type obviously do not apply to
kanji-only sequences.
Although kanji sequences are difficult to seg-
ment, they can comprise a significant portion of
Japanese text, as shown in Figure 1. Since se-
quences of more than 3 kanji generally consist of
more than one word, at least 21.2% of 1993 Nikkei
newswire consists of kanji sequences requiring seg-
mentation. Thus, accuracy on kanji sequences is an
important aspect of the total segmentation process.
As an alternative to lexico-grammatical and su-
pervised approaches, we propose a simple, effi-
</bodyText>
<page confidence="0.99066">
241
</page>
<note confidence="0.6479185">
s1 ? S2
ABCD1WXYZ
</note>
<bodyText confidence="0.988262333333333">
dent segmentation method which learns mostly
from very large amounts of unsegmented training
data, thus avoiding the costs of building a lexicon
or grammar or hand-segmenting large amounts of
training data. Some key advantages of this method
are:
</bodyText>
<listItem confidence="0.998576">
• No Japanese-specific rules are employed, en-
hancing portability to other languages.
• A very small number of pre-segmented train-
</listItem>
<bodyText confidence="0.837569363636364">
ing examples (as few as 5 in our experiments)
are needed for good performance, as long as
large amounts of unsegmented data are avail-
able.
• For long kanji strings, the method produces re-
sults rivalling those produced by Juman 3.61
(Kurohashi and Nagao, 1998) and Chasen 1.0
(Matsumoto et al., 1997), two morphological
analyzers in widespread use. For instance, we
achieve 5% higher word precision and 6% bet-
ter morpheme recall.
</bodyText>
<sectionHeader confidence="0.978736" genericHeader="introduction">
2 Algorithm
</sectionHeader>
<bodyText confidence="0.987848">
Our algorithm employs counts of character n-grams
in an unsegmented corpus to make segmentation de-
cisions. We illustrate its use with an example (see
Figure 2).
Let &amp;quot;A BCDWXY Z&amp;quot; represent an eight-kanji
sequence. To decide whether there should be a word
boundary between D and W, we check whether n-
grams that are adjacent to the proposed boundary,
such as the 4-grams s1 =&amp;quot;A B C D&amp;quot; and s2
X Y Z&amp;quot;, tend to be more frequent than n-grams that
straddle it, such as the 4-gram t1 = &amp;quot;B C D W&amp;quot;. If
so, we have evidence of a word boundary between
D and W, since there seems to be relatively little
cohesion between the characters on opposite sides
of this gap.
The n-gram orders used as evidence in the seg-
mentation decision are specified by the set N. For
instance, if N = {4} in our example, then we pose
the six questions of the form, &amp;quot;Is #(sz) &gt; #(ti)?&amp;quot;,
where #(x) denotes the number of occurrences of
x in the (unsegmented) training corpus. If N =
{2,4}, then two more questions (Is &amp;quot;#(C D) &gt;
#(D W)?&amp;quot; and &amp;quot;Is #(W X) &gt; #(D W)?&amp;quot;) are
added.
More formally, let sy and s&apos;2i be the non-
straddling n-grams just to the left and right of lo-
cation k, respectively, and let t7 be the straddling
n-gram with j characters to the right of location k.
Figure 2: Collecting evidence for a word boundary
— are the non-straddling n-grams si and s2 more
frequent than the straddling n-grams t1, t2, and t3?
Let I&gt; (y, z) be an indicator function that is 1 when
y&gt; z, and 0 otherwise.2 In order to compensate for
the fact that there are more n-gram questions than
(n — 1)-gram questions, we calculate the fraction of
affirmative answers separately for each n in N:
</bodyText>
<equation confidence="0.99981475">
v(k) = 2(n— 1)
1
i=1 3=1
2 n-1
</equation>
<bodyText confidence="0.5139705">
Then, we average the contributions of each n-gram
order:
</bodyText>
<equation confidence="0.984734">
VN(k) = —k-v(k)
nEN
</equation>
<bodyText confidence="0.9541715">
After vN(k) is computed for every location, bound-
aries are placed at all locations such that either:
</bodyText>
<listItem confidence="0.999988666666667">
• VN(t) &gt; vN(t — 1) and VN() &gt; vN(il -I- 1)
(that is, e is a local maximum), or
• vN(f) &gt; t, a threshold parameter.
</listItem>
<bodyText confidence="0.999773416666667">
The second condition is necessary to allow for
single-character words (see Figure 3). Note that it
also controls the granularity of the segmentation:
low thresholds encourage shorter segments.
Both the count acquisition and the testing phase
are efficient. Computing n-gram statistics for all
possible values of n simultaneously can be done in
0(m log m) time using suffix arrays, where m is
the training corpus size (Manber and Myers, 1993;
Nagao and Mori, 1994). However, if the set N of
n-gram orders is known in advance, conceptually
simpler algorithms suffice. Memory allocation for
</bodyText>
<footnote confidence="0.738054666666667">
2Note that we do not take into account the magnitude of
the difference between the two frequencies; see section 5 for
discussion.
</footnote>
<page confidence="0.994116">
242
</page>
<figure confidence="0.979209">
vik)
A BIC DIW XZ
</figure>
<figureCaption confidence="0.744974">
Figure 3: Determining word boundaries. The X- Y
boundary is created by the threshold criterion, the
other three by the local maximum condition.
</figureCaption>
<bodyText confidence="0.997831">
count tables can be significantly reduced by omit-
ting n-grams occurring only once and assuming the
count of unseen n-grams to be one. In the applica-
tion phase, the algorithm is clearly linear in the test
corpus size if INI is treated as a constant.
Finally, we note that some pre-segmented data is
necessary in order to set the parameters N and t.
However, as described below, very little such data
was required to get good performance; we therefore
deem our algorithm to be &amp;quot;mostly unsupervised&amp;quot;.
</bodyText>
<sectionHeader confidence="0.997852" genericHeader="method">
3 Experimental Framework
</sectionHeader>
<bodyText confidence="0.999961076923077">
Our experimental data was drawn from 150
megabytes of 1993 Nikkei newswire (see Figure
1). Five 500-sequence held-out subsets were ob-
tained from this corpus, the rest of the data serv-
ing as the unsegmented corpus from which to derive
character n-gram counts. Each held-out subset was
hand-segmented and then split into a 50-sequence
parameter-training set and a 450-sequence test set.
Finally, any sequences occurring in both a test set
and its corresponding parameter-training set were
discarded from the parameter-training set, so that
these sets were disjoint. (Typically no more than
five sequences were removed.)
</bodyText>
<subsectionHeader confidence="0.999809">
3.1 Held-out set annotation
</subsectionHeader>
<bodyText confidence="0.9834605">
Each held-out set contained 500 randomly-extracted
kanji sequences at least ten characters long (about
twelve on average), lengthy sequences being the
most difficult to segment (Takeda and Fujisalci,
1987). To obtain the gold-standard annotations, we
segmented the sequences by hand, using an observa-
tion of Takeda and Fujisaki (1987) that many kanji
compound words consist of two-character stem
words together with one-character prefixes and suf-
fixes. Using this terminology, our two-level bracket-
ing annotation may be summarized as follows.3 At
3A complete description of the annotation policy, including
the treatment of numeric expressions, may be found in a tech-
nical report (Ando and Lee, 1999).
the word level, a stem and its affixes are bracketed
together as a single unit. At the morpheme level,
stems are divided from their affixes. For example,
although both naga-no (Nagano) and shi (city) can
appear as individual words, naga-no-shi (Nagano
city) is bracketed as anaga-no][shi]], since here shi
serves as a suffix. Loosely speaking, word-level
bracketing demarcates discourse entities, whereas
morpheme-level brackets enclose strings that cannot
be further segmented without loss of meaning.4 For
instance, if one segments naga-no in naga-no-shi
into naga (long) and no (field), the intended mean-
ing disappears. Here is an example sequence from
our datasets:
</bodyText>
<equation confidence="0.462882">
[I14451] [EN] {AIM Ai] [AN]
</equation>
<bodyText confidence="0.99971">
Three native Japanese speakers participated in
the annotation: one segmented all the held-out data
based on the above rules, and the other two reviewed
350 sequences in total. The percentage of agree-
ment with the first person&apos;s bracketing was 98.42%:
only 62 out of 3927 locations were contested by a
verifier. Interestingly, all disagreement was at the
morpheme level.
</bodyText>
<subsectionHeader confidence="0.999454">
3.2 Baseline algorithms
</subsectionHeader>
<bodyText confidence="0.999981904761905">
We evaluated our segmentation method by com-
paring its performance against Chasen 1.05 (Mat-
sumoto et al., 1997) and Juman 3.61,6 (Kurohashi
and Nagao, 1998), two state-of-the-art, publically-
available, user-extensible morphological analyzers.
In both cases, the grammars were used as distributed
without modification. The sizes of Chasen&apos;s and Ju-
man&apos;s default lexicons are approximately 115,000
and 231,000 words, respectively.
Comparison issues An important question that
arose in designing our experiments was how to en-
able morphological analyzers to make use of the
parameter-training data, since they do not have pa-
rameters to tune. The only significant way that they
can be updated is by changing their grammars or
lexicons, which is quite tedious (for instance, we
had to add part-of-speech information to new en-
tries by hand). We took what we felt to be a rea-
sonable, but not too time-consuming, course of cre-
ating new lexical entries for all the bracketed words
in the parameter-training data. Evidence that this
</bodyText>
<footnote confidence="0.99910975">
4This level of segmentation is consistent with Wu&apos;s (1998)
Monotonicity Principle for segmentation.
5http://cactus.aist-nara.acjp/labinItIchasen.html
6http://pine.kuee.kyoto-u.ac.jp/nl-resource/juman-e.html
</footnote>
<page confidence="0.997795">
243
</page>
<figure confidence="0.990204777777778">
Word accuracy
85
El precool
• wall
• Fineaure ,
75
70
CI-FASEN JUMAN oplinize optrraze recall optange F
Precision
</figure>
<figureCaption confidence="0.99751">
Figure 4: Word accuracy. The three rightmost
</figureCaption>
<bodyText confidence="0.990287181818182">
groups represent our algorithm with parameters
tuned for different optimization criteria.
was appropriate comes from the fact that these ad-
ditions never degraded test set performance, and in-
deed improved it by one percent in some cases (only
small improvements are to be expected because the
parameter-training sets were fairly small).
It is important to note that in the end, we are com-
paring algorithms with access to different sources
of knowledge. Juman and Chasen use lexicons and
grammars developed by human experts. Our al-
gorithm, not having access to such pre-compiled
knowledge bases, must of necessity draw on other
information sources (in this case, a very large un-
segmented corpus and a few pre-segmented exam-
ples) to compensate for this lack. Since we are in-
terested in whether using simple statistics can match
the performance of labor-intensive methods, we do
not view these information sources as conveying
an unfair advantage, especially since the annotated
training sets were small, available to the morpho-
logical analyzers, and disjoint from the test sets.
</bodyText>
<sectionHeader confidence="0.999954" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.991654241935484">
We report the average results over the five test sets
using the optimal parameter settings for the corre-
sponding training sets (we tried all nonempty sub-
sets of {2, 3,4, 5, 6} for the set of n-gram orders N
and all values in {.05, .1, .15, .. . , 1} for the thresh-
old 07. In all performance graphs, the &amp;quot;error bars&amp;quot;
represent one standard deviation. The results for
Chasen and Juman reflect the lexicon additions de-
&apos;For simplicity, ties were deterministically broken by pre-
ferring smaller sizes of N, shorter n-grams in N, and larger
threshold values, in that order.
scribed in section 3.2.
Word and morpheme accuracy The standard
metrics in word segmentation are word precision
and recall. Treating a proposed segmentation as a
non-nested bracketing (e. g. , &amp;quot; I AB IC I&amp;quot; corresponds
to the bracketing &amp;quot;[AB][C]&amp;quot;), word precision (P) is
defined as the percentage of proposed brackets that
exactly match word-level brackets in the annotation;
word recall (R) is the percentage of word-level an-
notation brackets that are proposed by the algorithm
in question; and word F combines precision and re-
call: F = 2P RI (P + R).
One problem with using word metrics is that
morphological analyzers are designed to produce
morpheme-level segments. To compensate, we al-
tered the segmentations produced by Juman and
Chasen by concatenating stems and affixes, as iden-
tified by the part-of-speech information the analyz-
ers provided. (We also measured morpheme accu-
racy, as described below.)
Figures 4 and 8 show word accuracy for Chasen,
Juman, and our algorithm for parameter settings
optimizing word precision, recall, and F-measure
rates. Our algorithm achieves 5.27% higher preci-
sion and 0.26% better F-measure accuracy than Ju-
man, and does even better (8.8% and 4.22%, respec-
tively) with respect to Chasen. The recall perfor-
mance falls (barely) between that of Juman and that
of Chasen.
As noted above, Juman and Chasen were de-
signed to produce morpheme-level segmentations.
We therefore also measured morpheme precision,
recall, and F measure, all defined analogously to
their word counterparts.
Figure 5 shows our morpheme accuracy results.
We see that our algorithm can achieve better recall
(by 6.51%) and F-measure (by 1.38%) than Juman,
and does better than Chasen by an even wider mar-
gin (11.18% and 5.39%, respectively). Precision
was generally worse than the morphological analyz-
ers.
Compatible Brackets Although word-level accu-
racy is a standard performance metric, it is clearly
very sensitive to the test annotation. Morpheme ac-
curacy suffers the same problem. Indeed, the au-
thors of Juman and Chasen may well have con-
structed their standard dictionaries using different
notions of word and morpheme than the definitions
we used in annotating the data. We therefore devel-
oped two new, more robust metrics to measure the
number of proposed brackets that would be incor-
</bodyText>
<page confidence="0.995246">
244
</page>
<figure confidence="0.408150571428571">
[ [data] [base]] [system] (annotation brackets)
Proposed segmentation word morpheme compatible-bracket errors
errors errors
crossing morpheme-dividing
[data] [base] [system] 2 0 0 0
[data] [basesystem] 2 1 1 0
[database] [sys] [tern] 2 3 0 2
</figure>
<figureCaption confidence="0.9648415">
Figure 6: Examples of word, morpheme, and compatible-bracket errors. The sequence &amp;quot;data base&amp;quot; has been
annotated as &amp;quot;[[data] base]]&amp;quot; because &amp;quot;data base&amp;quot; and &amp;quot;database&amp;quot; are interchangeable.
</figureCaption>
<figure confidence="0.967116">
Morpheme accuracy
05
</figure>
<figureCaption confidence="0.999988">
Figure 5: Morpheme accuracy.
</figureCaption>
<bodyText confidence="0.984138">
rect with respect to any reasonable annotation.
Our novel metrics account for two types of er-
rors. The first, a crossing bracket, is a proposed
bracket that overlaps but is not contained within an
annotation bracket (Grishman et al., 1992). Cross-
ing brackets cannot coexist with annotation brack-
ets, and it is unlikely that another human would
create such brackets. The second type of er-
ror, a morpheme-dividing bracket, subdivides a
morpheme-level annotation bracket; by definition,
such a bracket results in a loss of meaning. See Fig-
ure 6 for some examples.
We define a compatible bracket as a proposed
bracket that is neither crossing nor morpheme-
dividing. The compatible brackets rate is simply the
compatible brackets precision. Note that this met-
ric accounts for different levels of segmentation si-
multaneously, which is beneficial because the gran-
ularity of Chasen and Juman&apos;s segmentation varies
from morpheme level to compound word level (by
our definition). For instance, well-known university
names are treated as single segments by virtue of be-
ing in the default lexicon, whereas other university
names are divided into the name and the word &amp;quot;uni-
versity&amp;quot;. Using the compatible brackets rate, both
segmentations can be counted as correct.
We also use the all-compatible brackets rate,
which is the fraction of sequences for which all
the proposed brackets are compatible. Intuitively,
this function measures the ease with which a human
could correct the output of the segmentation algo-
rithm: if the all-compatible brackets rate is high,
then the errors are concentrated in relatively few
sequences; if it is low, then a human doing post-
processing would have to correct many sequences.
Figure 7 depicts the compatible brackets and all-
compatible brackets rates. Our algorithm does bet-
ter on both metrics (for instance, when F-measure
is optimized, by 2.16% and 1.9%, respectively, in
comparison to Chasen, and by 3.15% and 4.96%,
respectively, in comparison to Juman), regardless of
training optimization function (word precision, re-
call, or F — we cannot directly optimize the com-
patible brackets rate because &amp;quot;perfect&amp;quot; performance
is possible simply by making the entire sequence a
single segment).
</bodyText>
<subsectionHeader confidence="0.666235">
Compatible and all-compatible brackets rates
</subsectionHeader>
<footnote confidence="0.341542">
CHASFY juMAN conwe natc■W Opliftze.rocall, oirze F
7,-17 • a 071E. al as
</footnote>
<figureCaption confidence="0.9159">
Figure 7: Compatible brackets and all-compatible
bracket rates when word accuracy is optimized.
</figureCaption>
<figure confidence="0.998500125">
Elprec
Wrenn
a Vrrn:
.5?
175
70
CHAS EN JLIMAN wirrdze optimize recatl optima F
PrEcsion
</figure>
<page confidence="0.990101">
245
</page>
<table confidence="0.99076275">
Juman5 vs. Juman50 Our50 vs Juman50 Our5 vs. Juman5 Our5 vs. Juman50
precision -1.04 +5.27 +6.18 +5.14
recall -0.63 -4.39 -3.73 -4.36
F-measure -0.84 +0.26 +1.14 +0.30
</table>
<figureCaption confidence="0.8642485">
Figure 8: Relative word accuracy as a function of training set size. &amp;quot;5&amp;quot; and &amp;quot;50&apos; denote training set size
before discarding overlaps with the test sets.
</figureCaption>
<subsectionHeader confidence="0.927463">
4.1 Discussion
</subsectionHeader>
<bodyText confidence="0.999109782608695">
Minimal human effort is needed. In contrast
to our mostly-unsupervised method, morphological
analyzers need a lexicon and grammar rules built
using human expertise. The workload in creating
dictionaries on the order of hundreds of thousands
of words (the size of Chasen&apos;s and Juman&apos;s de-
fault lexicons) is clearly much larger than annotat-
ing the small parameter-training sets for our algo-
rithm. We also avoid the need to segment a large
amount of parameter-training data because our al-
gorithm draws almost all its information from an
unsegmented corpus. Indeed, the only human effort
involved in our algorithm is pre-segmenting the five
50-sequence parameter training sets, which took
only 42 minutes. In contrast, previously proposed
supervised approaches have used segmented train-
ing sets ranging from 1000-5000 sentences (Kash-
ioka et al., 1998) to 190,000 sentences (Nagata,
1996a).
To test how much annotated training data is actu-
ally necessary, we experimented with using minis-
cule parameter-training sets: five sets of only five
strings each (from which any sequences repeated in
the test data were discarded). It took only 4 minutes
to perform the hand segmentation in this case. As
shown in Figure 8, relative word performance was
not degraded and sometimes even slightly better. In
fact, from the last column of Figure 8 we see that
even if our algorithm has access to only five anno-
tated sequences when Juman has access to ten times
as many, we still achieve better precision and better
F measure.
Both the local maximum and threshold condi-
tions contribute. In our algorithm, a location k
is deemed a word boundary if vN(k) is either (1) a
local maximum or (2) at least as big as the thresh-
old t. It is natural to ask whether we really need two
conditions, or whether just one would suffice.
We therefore studied whether optimal perfor-
mance could be achieved using only one of the con-
ditions. Figure 9 shows that in fact both contribute
to producing good segmentations. Indeed, in some
cases, both are needed to achieve the best perfor-
mance; also, each condition when used in isolation
yields suboptimal performance with respect to some
performance metrics.
</bodyText>
<table confidence="0.78767075">
accuracy optimize optimize optimize
precision recall F-measure
word M M &amp; T M
morpheme M &amp; T T T
</table>
<figureCaption confidence="0.961113333333333">
Figure 9: Entries indicate whether best performance
is achieved using the local maximum condition (M),
the threshold condition (T), or both.
</figureCaption>
<sectionHeader confidence="0.999494" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999332269230769">
Japanese Many previously proposed segmenta-
tion methods for Japanese text make use of either
a pre-existing lexicon (Yamron et al., 1993; Mat-
sumoto and Nagao, 1994; Takeuchi and Matsumoto,
1995; Nagata, 1997; Fuchi and Takagi, 1998) or
pre-segmented training data (Nagata, 1994; Papa-
georgiou, 1994; Nagata, 1996a; Kashioka et al.,
1998; Mori and Nagao, 1998). Other approaches
bootstrap from an initial segmentation provided by
a baseline algorithm such as Juman (Matsukawa et
al., 1993; Yamamoto, 1996).
Unsupervised, non-lexicon-based methods for
Japanese segmentation do exist, but they often have
limited applicability. Both Tomokiyo and Ries
(1997) and Teller and Batchelder (1994) explicitly
avoid working with kanji charactes. Takeda and
Fujisald (1987) propose the short unit model, a
type of Hidden Markov Model with linguistically-
determined topology, to segment kanji compound
words. However, their method does not handle
three-character stem words or single-character stem
words with affixes, both of which often occur in
proper nouns. In our five test datasets, we found
that 13.56% of the kanji sequences contain words
that cannot be handled by the short unit model.
Nagao and Mori (1994) propose using the heuris-
</bodyText>
<page confidence="0.996016">
246
</page>
<bodyText confidence="0.999979063829788">
tic that high-frequency character n-grams may rep-
resent (portions of) new collocations and terms,
but the results are not experimentally evaluated,
nor is a general segmentation algorithm proposed.
The work of Ito and Kohda (1995) similarly relies
on high-frequency character n-grams, but again, is
more concerned with using these frequent n-grams
as pseudo-lexicon entries; a standard segmentation
algorithm is then used on the basis of the induced
lexicon. Our algorithm, on the hand, is fundamen-
tally different in that it incorporates no explicit no-
tion of word, but only &amp;quot;sees&amp;quot; locations between
characters.
Chinese According to Sproat et al. (1996), most
prior work in Chinese segmentation has exploited
lexical knowledge bases; indeed, the authors assert
that they were aware of only one previously pub-
lished instance (the mutual-information method of
Sproat and Shih (1990)) of a purely statistical ap-
proach. In a later paper, Palmer (1997) presents
a transformation-based algorithm, which requires
pre-segmented training data.
To our knowledge, the Chinese segmenter most
similar to ours is that of Sun et al. (1998). They
also avoid using a lexicon, determining whether a
given location constitutes a word boundary in part
by deciding whether the two characters on either
side tend to occur together; also, they use thresholds
and several types of local minima and maxima to
make segmentation decisions. However, the statis-
tics they use (mutual information and t-score) are
more complex than the simple n-gram counts that
we employ.
Our preliminary reimplementation of their
method shows that it does not perform as well as
the morphological analyzers on our datasets, al-
though we do not want to draw definite conclusions
because some aspects of Sun et al&apos;s method seem
incomparable to ours. We do note, however, that
their method incorporates numerical differences
between statistics, whereas we only use indicator
functions; for example, once we know that one
trigram is more common than another, we do not
take into account the difference between the two
frequencies. We conjecture that using absolute
differences may have an adverse effect on rare
sequences.
</bodyText>
<sectionHeader confidence="0.999166" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999328764705882">
In this paper, we have presented a simple, mostly-
unsupervised algorithm that segments Japanese se-
quences into words based on statistics drawn from
a large unsegmented corpus. We evaluated per-
formance on kanji with respect to several metrics,
including the novel compatible brackets and all-
compatible brackets rates, and found that our al-
gorithm could yield performances rivaling that of
lexicon-based morphological analyzers.
In future work, we plan to experiment on
Japanese sentences with mixtures of character
types, possibly in combination with morphologi-
cal analyzers in order to balance the strengths and
weaknesses of the two types of methods. Since
our method does not use any Japanese-dependent
heuristics, we also hope to test it on Chinese or other
languages as well.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99977425">
We thank Minoru Shindoh and Takashi Ando for
reviewing the annotations, and the anonymous re-
viewers for their comments. This material was sup-
ported in part by a grant from the GE Foundation.
</bodyText>
<sectionHeader confidence="0.977557" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.584199555555556">
Rie Ando and Lillian Lee. 1999. Unsupervised sta-
tistical segmentation of Japanese kanji strings.
Technical Report TR99-1756, Cornell University.
Takeshi Fuchi and Shinichiro Takagi. 1998.
Japanese morphological analyzer using word co-
occurrence - JTAG. In Proc. of COLING-ACL
&apos;98, pages 409-413.
Pascale Fung. 1998. Extracting key terms from
Chinese and Japanese texts. Computer Process-
ing of Oriental Languages, 12(1).
Ralph Grishman, Catherine Macleod, and John
Sterling. 1992. Evaluating parsing strategies us-
ing standardized parse files. In Proc. of the 3rd
ANLP, pages 156-161.
Akinori Ito and Kasaki Kohda. 1995. Language
modeling by string pattern N-gram for Japanese
speech recognition. In Proc. of ICASSP.
Hidelci Kashioka, Yasuhiro Kawata, Yumiko Kinjo,
Andrew Finch, and Ezra W. Black. 1998. Use
of mutual information based character clus-
ters in dictionary-less morphological analysis of
Japanese. In Proc. of COL1NG-ACL &apos;98, pages
658-662.
Sadao Kurohashi and Malcoto Nagao. 1998.
Japanese morphological analysis system JUMAN
version 3.6 manual. In Japanese.
Udi Manber and Gene Myers. 1993. Suffix arrays:
</bodyText>
<page confidence="0.99256">
247
</page>
<reference confidence="0.987321141414141">
A new method for on-line string searches. SIAM
Journal on Computing, 22(5):935-948.
T. Matsukawa, Scott Miller, and Ralph Weischedel.
1993. Example-based correction of word seg-
mentation and part of speech labelling. In Proc.
of the HLT Workshop, pages 227-32.
Yuji Matsumoto and Makoto Nagao. 1994. Im-
provements of Japanese morphological analyzer
JUMAN. In Proc. of the International Workshop
on Sharable Natural Language Resources, pages
22-28.
Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita,
Yoshitaka Hirano, Osamu Imaichi, and Tomoaki
Imamura. 1997. Japanese morphological anal-
ysis system ChaSen manual. Technical Report
NAIST-IS-TR97007, Nara Institute of Science
and Technology. In Japanese.
Shinsuke Mori and Makoto Nagao. 1998. Un-
known word extraction from corpora using n-
gram statistics. Journal of the Information Pro-
cessing Society of Japan, 39(7):2093-2100. In
Japanese.
Makoto Nagao and Shinsuke Mori. 1994. A new
method of N-gram statistics for large number of
n and automatic extraction of words and phrases
from large text data of Japanese. In Proc. of the
15th COLING, pages 611-615.
Masaaki Nagata. 1994. A stochastic Japanese
morphological analyzer using a forward-DP
backward-A* n-best search algorithm. In Proc.
of the 15th COLING, pages 201-207.
Masaaki Nagata. 1996a. Automatic extraction of
new words from Japanese texts using generalized
forward-backward search. In Proc. of the Confer-
ence on Empirical Methods in Natural Language
Processing, pages 48-59.
Masaaki Nagata. 1996b. Context-based spelling
correction for Japanese OCR. In Proc. of the 16th
COLING, pages 806-811.
Masaaki Nagata. 1997. A self-organizing Japanese
word segmenter using heuristic word identifica-
tion and re-estimation. In Proc. of the 5th Work-
shop on Very Large Corpora, pages 203-215.
David Palmer. 1997. A trainable rule-based algo-
rithm for word segmentation. In Proc. of the 35th
ACL/8th EACL, pages 321-328.
Constantine P. Papageorgiou. 1994. Japanese word
segmentation by hidden Markov model. In Proc.
of the HLT Workshop, pages 283-288.
Richard Sproat and ChilM Shih. 1990. A statistical
method for finding word boundaries in Chinese
text. Computer Processing of Chinese and Ori-
ental Languages, 4:336-351.
Richard Sproat, Chilin Shih, William Gale, and
Nancy Chang. 1996. A stochastic finite-sate
word-segmentation algorithm for Chinese. Com-
putational Linguistics, 22(3).
Maosong Sun, Dayang Shen, and Benjamin K.
Tsou. 1998. Chinese word segmentation without
using lexicon and hand-crafted training data. In
Proc. of COLING-ACL &apos;98, pages 1265-1271.
Koichi Takeda and Tetsunosuke Fujisaki. 1987.
Automatic decomposition of kanji compound
words using stochastic estimation. Journal of
the Information Processing Society of Japan,
28(9):952-961. In Japanese.
Kouichi Takeuchi and Yuji Matsumoto. 1995.
HMM parameter learning for Japanese morpho-
logical analyzer. In Proc. of the 10th Pacific Asia
Conference on Language, Information and Com-
putation (PA CLING), pages 163-172.
Virginia Teller and Eleanor Olds Batchelder. 1994.
A probabilistic algorithm for segmenting non-
kanji Japanese strings. In Proc. of the 12th AAAI,
pages 742-747.
Laura Mayfield Tomokiyo and Klaus Ries. 1997.
What makes a word: learning base units in
Japanese for speech recognition. In Proc. of the
ACL Special Interest Group in Natural Language
Learning (CoNLL97), pages 60-69.
Zimin Wu and Gwyneth Tseng. 1993. Chinese text
segmentation for text retrieval: Achievements
and problems. Journal of the American Society
for Information Science, 44(9):532-542.
Dekai Wu. 1998. A position statement on Chinese
segmentation. http://www.cs.ust.hki-dekait-
papers/segmentation.html. Presented at the
Chinese Language Processing Workshop,
University of Pennsylvania.
Mild° Yamamoto. 1996. A re-estimation method
for stochastic language modeling from ambigu-
ous observations. In Proc. of the 4th Workshop
on Very Large Corpora, pages 155-167.
J. Yamron, J. Baker, P. Bamberg, H. Chevalier,
T. Dietzel, J. Elder, F. Kampmann, M. Mandel,
L. Manganaro, T. Margolis, and E. Steele. 1993.
LINGSTAT: An interactive, machine-aided trans-
lation system. In Proc. of the HLT Workshop,
pages 191-195.
</reference>
<page confidence="0.997005">
248
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.889917">
<title confidence="0.9884585">Mostly-Unsupervised Statistical Segmentation of Japanese: Applications to Kanji</title>
<author confidence="0.99965">Rie Kubota Ando</author>
<author confidence="0.99965">Lillian Lee</author>
<affiliation confidence="0.9994505">Department of Computer Science Cornell University</affiliation>
<address confidence="0.999924">Ithaca, NY 14853-7501</address>
<email confidence="0.976642">Ocubotar,lleej@cs.cornell.edu</email>
<abstract confidence="0.993464818181818">Given the lack of word delimiters in written Japanese, word segmentation is generally considered a crucial first step in processing Japanese texts. Typical Japanese segmentation algorithms rely either on a lexicon and grammar or on pre-segmented data. In contrast, we introduce a novel statistical utilizing data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>A new method for on-line string searches.</title>
<journal>SIAM Journal on Computing,</journal>
<pages>22--5</pages>
<marker></marker>
<rawString>A new method for on-line string searches. SIAM Journal on Computing, 22(5):935-948.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Matsukawa</author>
<author>Scott Miller</author>
<author>Ralph Weischedel</author>
</authors>
<title>Example-based correction of word segmentation and part of speech labelling.</title>
<date>1993</date>
<booktitle>In Proc. of the HLT Workshop,</booktitle>
<pages>227--32</pages>
<contexts>
<context position="21849" citStr="Matsukawa et al., 1993" startWordPosition="3481" endWordPosition="3484">ndicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji compound words. However, their method does not handle three-character stem words or single-character stem words with affixes, both of which often occur in proper nouns. In our five test datasets, we found tha</context>
</contexts>
<marker>Matsukawa, Miller, Weischedel, 1993</marker>
<rawString>T. Matsukawa, Scott Miller, and Ralph Weischedel. 1993. Example-based correction of word segmentation and part of speech labelling. In Proc. of the HLT Workshop, pages 227-32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
<author>Makoto Nagao</author>
</authors>
<title>Improvements of Japanese morphological analyzer JUMAN.</title>
<date>1994</date>
<booktitle>In Proc. of the International Workshop on Sharable Natural Language Resources,</booktitle>
<pages>22--28</pages>
<contexts>
<context position="21526" citStr="Matsumoto and Nagao, 1994" startWordPosition="3432" endWordPosition="3436">g good segmentations. Indeed, in some cases, both are needed to achieve the best performance; also, each condition when used in isolation yields suboptimal performance with respect to some performance metrics. accuracy optimize optimize optimize precision recall F-measure word M M &amp; T M morpheme M &amp; T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) pr</context>
</contexts>
<marker>Matsumoto, Nagao, 1994</marker>
<rawString>Yuji Matsumoto and Makoto Nagao. 1994. Improvements of Japanese morphological analyzer JUMAN. In Proc. of the International Workshop on Sharable Natural Language Resources, pages 22-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuji Matsumoto</author>
</authors>
<title>Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imaichi, and Tomoaki Imamura.</title>
<date>1997</date>
<tech>Technical Report NAIST-IS-TR97007,</tech>
<institution>Nara Institute of Science and Technology. In Japanese.</institution>
<marker>Matsumoto, 1997</marker>
<rawString>Yuji Matsumoto, Akira Kitauchi, Tatsuo Yamashita, Yoshitaka Hirano, Osamu Imaichi, and Tomoaki Imamura. 1997. Japanese morphological analysis system ChaSen manual. Technical Report NAIST-IS-TR97007, Nara Institute of Science and Technology. In Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Makoto Nagao</author>
</authors>
<title>Unknown word extraction from corpora using ngram statistics.</title>
<date>1998</date>
<journal>Journal of the Information Processing Society of Japan,</journal>
<pages>39--7</pages>
<note>In Japanese.</note>
<contexts>
<context position="21721" citStr="Mori and Nagao, 1998" startWordPosition="3462" endWordPosition="3465">e metrics. accuracy optimize optimize optimize precision recall F-measure word M M &amp; T M morpheme M &amp; T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji compound words. However, their method does not handle three-character stem words</context>
</contexts>
<marker>Mori, Nagao, 1998</marker>
<rawString>Shinsuke Mori and Makoto Nagao. 1998. Unknown word extraction from corpora using ngram statistics. Journal of the Information Processing Society of Japan, 39(7):2093-2100. In Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Makoto Nagao</author>
<author>Shinsuke Mori</author>
</authors>
<title>A new method of N-gram statistics for large number of n and automatic extraction of words and phrases from large text data of Japanese.</title>
<date>1994</date>
<booktitle>In Proc. of the 15th COLING,</booktitle>
<pages>611--615</pages>
<contexts>
<context position="1117" citStr="Nagao and Mori, 1994" startWordPosition="153" endWordPosition="156">duce a novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics. 1 Introduction Because Japanese is written without delimiters between words,&apos; accurate word segmentation to recover the lexical items is a key step in Japanese text processing. Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) errors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Ring, 1998). Typically, Japanese word segmentation is performed by morphological analysis based on lexical and grammatical knowledge. This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, although using this heuristic alone achieves less than 60% accuracy (Nagata, 1997). Character sequences consisting solely of kanji pose a challenge to morphologically-based segmenters for several reasons. First and most importantly, kanji</context>
<context position="6877" citStr="Nagao and Mori, 1994" startWordPosition="1108" endWordPosition="1111">placed at all locations such that either: • VN(t) &gt; vN(t — 1) and VN() &gt; vN(il -I- 1) (that is, e is a local maximum), or • vN(f) &gt; t, a threshold parameter. The second condition is necessary to allow for single-character words (see Figure 3). Note that it also controls the granularity of the segmentation: low thresholds encourage shorter segments. Both the count acquisition and the testing phase are efficient. Computing n-gram statistics for all possible values of n simultaneously can be done in 0(m log m) time using suffix arrays, where m is the training corpus size (Manber and Myers, 1993; Nagao and Mori, 1994). However, if the set N of n-gram orders is known in advance, conceptually simpler algorithms suffice. Memory allocation for 2Note that we do not take into account the magnitude of the difference between the two frequencies; see section 5 for discussion. 242 vik) A BIC DIW XZ Figure 3: Determining word boundaries. The X- Y boundary is created by the threshold criterion, the other three by the local maximum condition. count tables can be significantly reduced by omitting n-grams occurring only once and assuming the count of unseen n-grams to be one. In the application phase, the algorithm is cl</context>
<context position="22564" citStr="Nagao and Mori (1994)" startWordPosition="3588" endWordPosition="3591"> but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji compound words. However, their method does not handle three-character stem words or single-character stem words with affixes, both of which often occur in proper nouns. In our five test datasets, we found that 13.56% of the kanji sequences contain words that cannot be handled by the short unit model. Nagao and Mori (1994) propose using the heuris246 tic that high-frequency character n-grams may represent (portions of) new collocations and terms, but the results are not experimentally evaluated, nor is a general segmentation algorithm proposed. The work of Ito and Kohda (1995) similarly relies on high-frequency character n-grams, but again, is more concerned with using these frequent n-grams as pseudo-lexicon entries; a standard segmentation algorithm is then used on the basis of the induced lexicon. Our algorithm, on the hand, is fundamentally different in that it incorporates no explicit notion of word, but o</context>
</contexts>
<marker>Nagao, Mori, 1994</marker>
<rawString>Makoto Nagao and Shinsuke Mori. 1994. A new method of N-gram statistics for large number of n and automatic extraction of words and phrases from large text data of Japanese. In Proc. of the 15th COLING, pages 611-615.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A stochastic Japanese morphological analyzer using a forward-DP backward-A* n-best search algorithm.</title>
<date>1994</date>
<booktitle>In Proc. of the 15th COLING,</booktitle>
<pages>201--207</pages>
<contexts>
<context position="21640" citStr="Nagata, 1994" startWordPosition="3451" endWordPosition="3452"> isolation yields suboptimal performance with respect to some performance metrics. accuracy optimize optimize optimize precision recall F-measure word M M &amp; T M morpheme M &amp; T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji</context>
</contexts>
<marker>Nagata, 1994</marker>
<rawString>Masaaki Nagata. 1994. A stochastic Japanese morphological analyzer using a forward-DP backward-A* n-best search algorithm. In Proc. of the 15th COLING, pages 201-207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>Automatic extraction of new words from Japanese texts using generalized forward-backward search.</title>
<date>1996</date>
<booktitle>In Proc. of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>48--59</pages>
<contexts>
<context position="1131" citStr="Nagata, 1996" startWordPosition="157" endWordPosition="158">al method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics. 1 Introduction Because Japanese is written without delimiters between words,&apos; accurate word segmentation to recover the lexical items is a key step in Japanese text processing. Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) errors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Ring, 1998). Typically, Japanese word segmentation is performed by morphological analysis based on lexical and grammatical knowledge. This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, although using this heuristic alone achieves less than 60% accuracy (Nagata, 1997). Character sequences consisting solely of kanji pose a challenge to morphologically-based segmenters for several reasons. First and most importantly, kanji sequences oft</context>
<context position="19825" citStr="Nagata, 1996" startWordPosition="3144" endWordPosition="3145">&apos;s and Juman&apos;s default lexicons) is clearly much larger than annotating the small parameter-training sets for our algorithm. We also avoid the need to segment a large amount of parameter-training data because our algorithm draws almost all its information from an unsegmented corpus. Indeed, the only human effort involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets, which took only 42 minutes. In contrast, previously proposed supervised approaches have used segmented training sets ranging from 1000-5000 sentences (Kashioka et al., 1998) to 190,000 sentences (Nagata, 1996a). To test how much annotated training data is actually necessary, we experimented with using miniscule parameter-training sets: five sets of only five strings each (from which any sequences repeated in the test data were discarded). It took only 4 minutes to perform the hand segmentation in this case. As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better. In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five annotated sequences when Juman has access to ten times as many, we still achieve better precis</context>
<context position="21674" citStr="Nagata, 1996" startWordPosition="3456" endWordPosition="3457">rmance with respect to some performance metrics. accuracy optimize optimize optimize precision recall F-measure word M M &amp; T M morpheme M &amp; T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji compound words. However, their me</context>
</contexts>
<marker>Nagata, 1996</marker>
<rawString>Masaaki Nagata. 1996a. Automatic extraction of new words from Japanese texts using generalized forward-backward search. In Proc. of the Conference on Empirical Methods in Natural Language Processing, pages 48-59.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>Context-based spelling correction for Japanese OCR.</title>
<date>1996</date>
<booktitle>In Proc. of the 16th COLING,</booktitle>
<pages>806--811</pages>
<contexts>
<context position="1131" citStr="Nagata, 1996" startWordPosition="157" endWordPosition="158">al method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics. 1 Introduction Because Japanese is written without delimiters between words,&apos; accurate word segmentation to recover the lexical items is a key step in Japanese text processing. Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) errors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Ring, 1998). Typically, Japanese word segmentation is performed by morphological analysis based on lexical and grammatical knowledge. This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, although using this heuristic alone achieves less than 60% accuracy (Nagata, 1997). Character sequences consisting solely of kanji pose a challenge to morphologically-based segmenters for several reasons. First and most importantly, kanji sequences oft</context>
<context position="19825" citStr="Nagata, 1996" startWordPosition="3144" endWordPosition="3145">&apos;s and Juman&apos;s default lexicons) is clearly much larger than annotating the small parameter-training sets for our algorithm. We also avoid the need to segment a large amount of parameter-training data because our algorithm draws almost all its information from an unsegmented corpus. Indeed, the only human effort involved in our algorithm is pre-segmenting the five 50-sequence parameter training sets, which took only 42 minutes. In contrast, previously proposed supervised approaches have used segmented training sets ranging from 1000-5000 sentences (Kashioka et al., 1998) to 190,000 sentences (Nagata, 1996a). To test how much annotated training data is actually necessary, we experimented with using miniscule parameter-training sets: five sets of only five strings each (from which any sequences repeated in the test data were discarded). It took only 4 minutes to perform the hand segmentation in this case. As shown in Figure 8, relative word performance was not degraded and sometimes even slightly better. In fact, from the last column of Figure 8 we see that even if our algorithm has access to only five annotated sequences when Juman has access to ten times as many, we still achieve better precis</context>
<context position="21674" citStr="Nagata, 1996" startWordPosition="3456" endWordPosition="3457">rmance with respect to some performance metrics. accuracy optimize optimize optimize precision recall F-measure word M M &amp; T M morpheme M &amp; T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji compound words. However, their me</context>
</contexts>
<marker>Nagata, 1996</marker>
<rawString>Masaaki Nagata. 1996b. Context-based spelling correction for Japanese OCR. In Proc. of the 16th COLING, pages 806-811.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>A self-organizing Japanese word segmenter using heuristic word identification and re-estimation.</title>
<date>1997</date>
<booktitle>In Proc. of the 5th Workshop on Very Large Corpora,</booktitle>
<pages>203--215</pages>
<contexts>
<context position="1561" citStr="Nagata, 1997" startWordPosition="221" endWordPosition="222">ng new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) errors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Ring, 1998). Typically, Japanese word segmentation is performed by morphological analysis based on lexical and grammatical knowledge. This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, although using this heuristic alone achieves less than 60% accuracy (Nagata, 1997). Character sequences consisting solely of kanji pose a challenge to morphologically-based segmenters for several reasons. First and most importantly, kanji sequences often contain domain terms and proper nouns: Fung (1998) notes that 50-85% of the terms in various technical dictio1The analogous situation in English would be if words were written without spaces between them. Sequence length # of characters % of corpus 1 - 3 kanji 20,405,486 25.6 4 - 6 kanji 12,743,177 16.1 more than 6 kanji 3,966,408 5.1 Total 37,115,071 46.8 Figure 1: Statistics from 1993 Japanese newswire (NIKKEI), 79,326,40</context>
<context position="21570" citStr="Nagata, 1997" startWordPosition="3441" endWordPosition="3442">ded to achieve the best performance; also, each condition when used in isolation yields suboptimal performance with respect to some performance metrics. accuracy optimize optimize optimize precision recall F-measure word M M &amp; T M morpheme M &amp; T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden</context>
</contexts>
<marker>Nagata, 1997</marker>
<rawString>Masaaki Nagata. 1997. A self-organizing Japanese word segmenter using heuristic word identification and re-estimation. In Proc. of the 5th Workshop on Very Large Corpora, pages 203-215.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Palmer</author>
</authors>
<title>A trainable rule-based algorithm for word segmentation.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th ACL/8th EACL,</booktitle>
<pages>321--328</pages>
<contexts>
<context position="23540" citStr="Palmer (1997)" startWordPosition="3741" endWordPosition="3742">quent n-grams as pseudo-lexicon entries; a standard segmentation algorithm is then used on the basis of the induced lexicon. Our algorithm, on the hand, is fundamentally different in that it incorporates no explicit notion of word, but only &amp;quot;sees&amp;quot; locations between characters. Chinese According to Sproat et al. (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously published instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical approach. In a later paper, Palmer (1997) presents a transformation-based algorithm, which requires pre-segmented training data. To our knowledge, the Chinese segmenter most similar to ours is that of Sun et al. (1998). They also avoid using a lexicon, determining whether a given location constitutes a word boundary in part by deciding whether the two characters on either side tend to occur together; also, they use thresholds and several types of local minima and maxima to make segmentation decisions. However, the statistics they use (mutual information and t-score) are more complex than the simple n-gram counts that we employ. Our p</context>
</contexts>
<marker>Palmer, 1997</marker>
<rawString>David Palmer. 1997. A trainable rule-based algorithm for word segmentation. In Proc. of the 35th ACL/8th EACL, pages 321-328.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantine P Papageorgiou</author>
</authors>
<title>Japanese word segmentation by hidden Markov model.</title>
<date>1994</date>
<booktitle>In Proc. of the HLT Workshop,</booktitle>
<pages>283--288</pages>
<contexts>
<context position="21660" citStr="Papageorgiou, 1994" startWordPosition="3453" endWordPosition="3455">lds suboptimal performance with respect to some performance metrics. accuracy optimize optimize optimize precision recall F-measure word M M &amp; T M morpheme M &amp; T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji compound words. How</context>
</contexts>
<marker>Papageorgiou, 1994</marker>
<rawString>Constantine P. Papageorgiou. 1994. Japanese word segmentation by hidden Markov model. In Proc. of the HLT Workshop, pages 283-288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>ChilM Shih</author>
</authors>
<title>A statistical method for finding word boundaries</title>
<date>1990</date>
<booktitle>in Chinese text. Computer Processing of Chinese and Oriental Languages,</booktitle>
<pages>4--336</pages>
<contexts>
<context position="23473" citStr="Sproat and Shih (1990)" startWordPosition="3727" endWordPosition="3730">equency character n-grams, but again, is more concerned with using these frequent n-grams as pseudo-lexicon entries; a standard segmentation algorithm is then used on the basis of the induced lexicon. Our algorithm, on the hand, is fundamentally different in that it incorporates no explicit notion of word, but only &amp;quot;sees&amp;quot; locations between characters. Chinese According to Sproat et al. (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously published instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical approach. In a later paper, Palmer (1997) presents a transformation-based algorithm, which requires pre-segmented training data. To our knowledge, the Chinese segmenter most similar to ours is that of Sun et al. (1998). They also avoid using a lexicon, determining whether a given location constitutes a word boundary in part by deciding whether the two characters on either side tend to occur together; also, they use thresholds and several types of local minima and maxima to make segmentation decisions. However, the statistics they use (mutual information and t-score) a</context>
</contexts>
<marker>Sproat, Shih, 1990</marker>
<rawString>Richard Sproat and ChilM Shih. 1990. A statistical method for finding word boundaries in Chinese text. Computer Processing of Chinese and Oriental Languages, 4:336-351.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Chilin Shih</author>
<author>William Gale</author>
<author>Nancy Chang</author>
</authors>
<title>A stochastic finite-sate word-segmentation algorithm for Chinese.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>3</issue>
<contexts>
<context position="1168" citStr="Sproat et al., 1996" startWordPosition="161" endWordPosition="164">d training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics. 1 Introduction Because Japanese is written without delimiters between words,&apos; accurate word segmentation to recover the lexical items is a key step in Japanese text processing. Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) errors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Ring, 1998). Typically, Japanese word segmentation is performed by morphological analysis based on lexical and grammatical knowledge. This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, although using this heuristic alone achieves less than 60% accuracy (Nagata, 1997). Character sequences consisting solely of kanji pose a challenge to morphologically-based segmenters for several reasons. First and most importantly, kanji sequences often contain domain terms and proper no</context>
<context position="23246" citStr="Sproat et al. (1996)" startWordPosition="3693" endWordPosition="3696"> n-grams may represent (portions of) new collocations and terms, but the results are not experimentally evaluated, nor is a general segmentation algorithm proposed. The work of Ito and Kohda (1995) similarly relies on high-frequency character n-grams, but again, is more concerned with using these frequent n-grams as pseudo-lexicon entries; a standard segmentation algorithm is then used on the basis of the induced lexicon. Our algorithm, on the hand, is fundamentally different in that it incorporates no explicit notion of word, but only &amp;quot;sees&amp;quot; locations between characters. Chinese According to Sproat et al. (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously published instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical approach. In a later paper, Palmer (1997) presents a transformation-based algorithm, which requires pre-segmented training data. To our knowledge, the Chinese segmenter most similar to ours is that of Sun et al. (1998). They also avoid using a lexicon, determining whether a given location constitutes a word boundary in part by deciding whether t</context>
</contexts>
<marker>Sproat, Shih, Gale, Chang, 1996</marker>
<rawString>Richard Sproat, Chilin Shih, William Gale, and Nancy Chang. 1996. A stochastic finite-sate word-segmentation algorithm for Chinese. Computational Linguistics, 22(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maosong Sun</author>
<author>Dayang Shen</author>
<author>Benjamin K Tsou</author>
</authors>
<title>Chinese word segmentation without using lexicon and hand-crafted training data.</title>
<date>1998</date>
<booktitle>In Proc. of COLING-ACL &apos;98,</booktitle>
<pages>1265--1271</pages>
<contexts>
<context position="23717" citStr="Sun et al. (1998)" startWordPosition="3765" endWordPosition="3768">ferent in that it incorporates no explicit notion of word, but only &amp;quot;sees&amp;quot; locations between characters. Chinese According to Sproat et al. (1996), most prior work in Chinese segmentation has exploited lexical knowledge bases; indeed, the authors assert that they were aware of only one previously published instance (the mutual-information method of Sproat and Shih (1990)) of a purely statistical approach. In a later paper, Palmer (1997) presents a transformation-based algorithm, which requires pre-segmented training data. To our knowledge, the Chinese segmenter most similar to ours is that of Sun et al. (1998). They also avoid using a lexicon, determining whether a given location constitutes a word boundary in part by deciding whether the two characters on either side tend to occur together; also, they use thresholds and several types of local minima and maxima to make segmentation decisions. However, the statistics they use (mutual information and t-score) are more complex than the simple n-gram counts that we employ. Our preliminary reimplementation of their method shows that it does not perform as well as the morphological analyzers on our datasets, although we do not want to draw definite concl</context>
</contexts>
<marker>Sun, Shen, Tsou, 1998</marker>
<rawString>Maosong Sun, Dayang Shen, and Benjamin K. Tsou. 1998. Chinese word segmentation without using lexicon and hand-crafted training data. In Proc. of COLING-ACL &apos;98, pages 1265-1271.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koichi Takeda</author>
<author>Tetsunosuke Fujisaki</author>
</authors>
<title>Automatic decomposition of kanji compound words using stochastic estimation.</title>
<date>1987</date>
<journal>Journal of the Information Processing Society of Japan,</journal>
<pages>28--9</pages>
<note>In Japanese.</note>
<contexts>
<context position="8801" citStr="Takeda and Fujisaki (1987)" startWordPosition="1415" endWordPosition="1418">t and a 450-sequence test set. Finally, any sequences occurring in both a test set and its corresponding parameter-training set were discarded from the parameter-training set, so that these sets were disjoint. (Typically no more than five sequences were removed.) 3.1 Held-out set annotation Each held-out set contained 500 randomly-extracted kanji sequences at least ten characters long (about twelve on average), lengthy sequences being the most difficult to segment (Takeda and Fujisalci, 1987). To obtain the gold-standard annotations, we segmented the sequences by hand, using an observation of Takeda and Fujisaki (1987) that many kanji compound words consist of two-character stem words together with one-character prefixes and suffixes. Using this terminology, our two-level bracketing annotation may be summarized as follows.3 At 3A complete description of the annotation policy, including the treatment of numeric expressions, may be found in a technical report (Ando and Lee, 1999). the word level, a stem and its affixes are bracketed together as a single unit. At the morpheme level, stems are divided from their affixes. For example, although both naga-no (Nagano) and shi (city) can appear as individual words, </context>
</contexts>
<marker>Takeda, Fujisaki, 1987</marker>
<rawString>Koichi Takeda and Tetsunosuke Fujisaki. 1987. Automatic decomposition of kanji compound words using stochastic estimation. Journal of the Information Processing Society of Japan, 28(9):952-961. In Japanese.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kouichi Takeuchi</author>
<author>Yuji Matsumoto</author>
</authors>
<title>HMM parameter learning for Japanese morphological analyzer.</title>
<date>1995</date>
<booktitle>In Proc. of the 10th Pacific Asia Conference on Language, Information and Computation (PA CLING),</booktitle>
<pages>163--172</pages>
<contexts>
<context position="21556" citStr="Takeuchi and Matsumoto, 1995" startWordPosition="3437" endWordPosition="3440">d, in some cases, both are needed to achieve the best performance; also, each condition when used in isolation yields suboptimal performance with respect to some performance metrics. accuracy optimize optimize optimize precision recall F-measure word M M &amp; T M morpheme M &amp; T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a </context>
</contexts>
<marker>Takeuchi, Matsumoto, 1995</marker>
<rawString>Kouichi Takeuchi and Yuji Matsumoto. 1995. HMM parameter learning for Japanese morphological analyzer. In Proc. of the 10th Pacific Asia Conference on Language, Information and Computation (PA CLING), pages 163-172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Virginia Teller</author>
<author>Eleanor Olds Batchelder</author>
</authors>
<title>A probabilistic algorithm for segmenting nonkanji Japanese strings.</title>
<date>1994</date>
<booktitle>In Proc. of the 12th AAAI,</booktitle>
<pages>742--747</pages>
<contexts>
<context position="22049" citStr="Teller and Batchelder (1994)" startWordPosition="3507" endWordPosition="3510">r Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji compound words. However, their method does not handle three-character stem words or single-character stem words with affixes, both of which often occur in proper nouns. In our five test datasets, we found that 13.56% of the kanji sequences contain words that cannot be handled by the short unit model. Nagao and Mori (1994) propose using the heuris246 tic that high-frequency character n-grams may represent </context>
</contexts>
<marker>Teller, Batchelder, 1994</marker>
<rawString>Virginia Teller and Eleanor Olds Batchelder. 1994. A probabilistic algorithm for segmenting nonkanji Japanese strings. In Proc. of the 12th AAAI, pages 742-747.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Mayfield Tomokiyo</author>
<author>Klaus Ries</author>
</authors>
<title>What makes a word: learning base units in Japanese for speech recognition.</title>
<date>1997</date>
<booktitle>In Proc. of the ACL Special Interest Group in Natural Language Learning (CoNLL97),</booktitle>
<pages>60--69</pages>
<contexts>
<context position="22016" citStr="Tomokiyo and Ries (1997)" startWordPosition="3502" endWordPosition="3505">posed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji compound words. However, their method does not handle three-character stem words or single-character stem words with affixes, both of which often occur in proper nouns. In our five test datasets, we found that 13.56% of the kanji sequences contain words that cannot be handled by the short unit model. Nagao and Mori (1994) propose using the heuris246 tic that high-frequency</context>
</contexts>
<marker>Tomokiyo, Ries, 1997</marker>
<rawString>Laura Mayfield Tomokiyo and Klaus Ries. 1997. What makes a word: learning base units in Japanese for speech recognition. In Proc. of the ACL Special Interest Group in Natural Language Learning (CoNLL97), pages 60-69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zimin Wu</author>
<author>Gwyneth Tseng</author>
</authors>
<title>Chinese text segmentation for text retrieval: Achievements and problems.</title>
<date>1993</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>44--9</pages>
<contexts>
<context position="1095" citStr="Wu and Tseng, 1993" startWordPosition="149" endWordPosition="152">n contrast, we introduce a novel statistical method utilizing unsegmented training data, with performance on kanji sequences comparable to and sometimes surpassing that of morphological analyzers over a variety of error metrics. 1 Introduction Because Japanese is written without delimiters between words,&apos; accurate word segmentation to recover the lexical items is a key step in Japanese text processing. Proposed applications of segmentation technology include extracting new technical terms, indexing documents for information retrieval, and correcting optical character recognition (OCR) errors (Wu and Tseng, 1993; Nagao and Mori, 1994; Nagata, 1996a; Nagata, 1996b; Sproat et al., 1996; Ring, 1998). Typically, Japanese word segmentation is performed by morphological analysis based on lexical and grammatical knowledge. This analysis is aided by the fact that there are three types of Japanese characters, kanji, hiragana, and katakana: changes in character type often indicate word boundaries, although using this heuristic alone achieves less than 60% accuracy (Nagata, 1997). Character sequences consisting solely of kanji pose a challenge to morphologically-based segmenters for several reasons. First and m</context>
</contexts>
<marker>Wu, Tseng, 1993</marker>
<rawString>Zimin Wu and Gwyneth Tseng. 1993. Chinese text segmentation for text retrieval: Achievements and problems. Journal of the American Society for Information Science, 44(9):532-542.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>A position statement on Chinese segmentation. http://www.cs.ust.hki-dekaitpapers/segmentation.html. Presented at the Chinese Language Processing Workshop,</title>
<date>1998</date>
<institution>University of Pennsylvania.</institution>
<marker>Wu, 1998</marker>
<rawString>Dekai Wu. 1998. A position statement on Chinese segmentation. http://www.cs.ust.hki-dekaitpapers/segmentation.html. Presented at the Chinese Language Processing Workshop, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mild° Yamamoto</author>
</authors>
<title>A re-estimation method for stochastic language modeling from ambiguous observations.</title>
<date>1996</date>
<booktitle>In Proc. of the 4th Workshop on Very Large Corpora,</booktitle>
<pages>155--167</pages>
<contexts>
<context position="21866" citStr="Yamamoto, 1996" startWordPosition="3485" endWordPosition="3486">formance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Takeda and Fujisald (1987) propose the short unit model, a type of Hidden Markov Model with linguisticallydetermined topology, to segment kanji compound words. However, their method does not handle three-character stem words or single-character stem words with affixes, both of which often occur in proper nouns. In our five test datasets, we found that 13.56% of the k</context>
</contexts>
<marker>Yamamoto, 1996</marker>
<rawString>Mild° Yamamoto. 1996. A re-estimation method for stochastic language modeling from ambiguous observations. In Proc. of the 4th Workshop on Very Large Corpora, pages 155-167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yamron</author>
<author>J Baker</author>
<author>P Bamberg</author>
<author>H Chevalier</author>
<author>T Dietzel</author>
<author>J Elder</author>
<author>F Kampmann</author>
<author>M Mandel</author>
<author>L Manganaro</author>
<author>T Margolis</author>
<author>E Steele</author>
</authors>
<title>LINGSTAT: An interactive, machine-aided translation system.</title>
<date>1993</date>
<booktitle>In Proc. of the HLT Workshop,</booktitle>
<pages>191--195</pages>
<contexts>
<context position="21499" citStr="Yamron et al., 1993" startWordPosition="3428" endWordPosition="3431">ontribute to producing good segmentations. Indeed, in some cases, both are needed to achieve the best performance; also, each condition when used in isolation yields suboptimal performance with respect to some performance metrics. accuracy optimize optimize optimize precision recall F-measure word M M &amp; T M morpheme M &amp; T T T Figure 9: Entries indicate whether best performance is achieved using the local maximum condition (M), the threshold condition (T), or both. 5 Related Work Japanese Many previously proposed segmentation methods for Japanese text make use of either a pre-existing lexicon (Yamron et al., 1993; Matsumoto and Nagao, 1994; Takeuchi and Matsumoto, 1995; Nagata, 1997; Fuchi and Takagi, 1998) or pre-segmented training data (Nagata, 1994; Papageorgiou, 1994; Nagata, 1996a; Kashioka et al., 1998; Mori and Nagao, 1998). Other approaches bootstrap from an initial segmentation provided by a baseline algorithm such as Juman (Matsukawa et al., 1993; Yamamoto, 1996). Unsupervised, non-lexicon-based methods for Japanese segmentation do exist, but they often have limited applicability. Both Tomokiyo and Ries (1997) and Teller and Batchelder (1994) explicitly avoid working with kanji charactes. Ta</context>
</contexts>
<marker>Yamron, Baker, Bamberg, Chevalier, Dietzel, Elder, Kampmann, Mandel, Manganaro, Margolis, Steele, 1993</marker>
<rawString>J. Yamron, J. Baker, P. Bamberg, H. Chevalier, T. Dietzel, J. Elder, F. Kampmann, M. Mandel, L. Manganaro, T. Margolis, and E. Steele. 1993. LINGSTAT: An interactive, machine-aided translation system. In Proc. of the HLT Workshop, pages 191-195.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>