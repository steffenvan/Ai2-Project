<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<title confidence="0.991826">
Ranking suspected answers to natural language questions using
predictive annotation
</title>
<author confidence="0.99134">
Dragomir R. Radev*
</author>
<affiliation confidence="0.997692">
School of Information
University of Michigan
</affiliation>
<address confidence="0.990589">
Ann Arbor, MI 48103
</address>
<email confidence="0.972217">
radevOumich.edu
</email>
<author confidence="0.986482">
John Prager
</author>
<affiliation confidence="0.9089665">
TJ Watson Research Center
IBM Research Division
</affiliation>
<address confidence="0.927243">
Hawthorne, NY 10532
</address>
<email confidence="0.996669">
jprager@us.ibm.com
</email>
<author confidence="0.983904">
Valerie Samn*
</author>
<affiliation confidence="0.987457">
Teachers College
Columbia University
</affiliation>
<address confidence="0.992989">
New York, NY 10027
</address>
<email confidence="0.998374">
vs115@columbia.edu
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998837307692308">
In this paper, we describe a system to rank sus-
pected answers to natural language questions.
We process both corpus and query using a new
technique, predictive annotation, which aug-
ments phrases in texts with labels anticipating
their being targets of certain kinds of questions.
Given a natural language question, our IR sys-
tem returns a set of matching passages, which
we then rank using a linear function of seven
predictor variables. We provide an evaluation of
the techniques based on results from the TREC
Q&amp;A evaluation in which our system partici-
pated.
</bodyText>
<sectionHeader confidence="0.999383" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98389585">
Question Answering is a task that calls for a
combination of techniques from Information Re-
trieval and Natural Language Processing. The
former has the advantage of years of develop-
ment of efficient techniques for indexing and
searching large collections of data, but lacks of
any meaningful treatment of the semantics of
the query or the texts indexed. NLP tackles
the semantics, but tends to be computationally
expensive.
We have attempted to carve out a middle
ground, whereby we use a modified IR system
augmented by shallow NL parsing. Our ap-
proach was motivated by the following problem
with traditional IR systems. Suppose the user
asks &amp;quot;Where did &lt;some event&gt; happen?&amp;quot;. If
the system does no pre-processing of the query,
then &amp;quot;where&amp;quot; will be included in the bag of
words submitted to the search engine, but this
will not be helpful since the target text will
be unlikely to contain the word &amp;quot;where&amp;quot;. If
the word is stripped out as a stop-word, then
• The work presented in this paper was performed while
the first and third authors were at IBM Research.
the search engine will have no idea that a lo-
cation is sought. Our approach, called predic-
tive annotation, is to augment the query with
semantic category markers (which we call QA-
Tokens), in this case with the PLACE$ to-
ken, and also to label with QA-Tokens all oc-
currences in text that are recognized entities,
(for example, places). Then traditional bag-of-
words matching proceeds successfully, and will
return matching passages. The answer-selection
process then looks for and ranks in these pas-
sages occurrences of phrases containing the par-
ticular QA-Token(s) from the augmented query.
This classification of questions is conceptually
similar to the query expansion in (Voorhees,
1994) but is expected to achieve much better
performance since potentially matching phrases
in text are classified in a similar and synergistic
way.
Our system participated in the official TREC
Q&amp;A evaluation. For 200 questions in the eval-
uation set, we were asked to provide a list of
50-byte and 250-byte extracts from a 2-GB cor-
pus. The results are shown in Section 7.
Some techniques used by other participants in
the TREC evaluation are paragraph indexing,
followed by abductive inference (Harabagiu and
Maiorano, 1999) and knowledge-representation
combined with information retrieval (Breck et
al., 1999). Some earlier systems related to our
work are FaqFinder (Kulyukin et al., 1998),
MURAX (Kupiec, 1993), which uses an encyclo-
pedia as a knowledge base from which to extract
answers, and PROFILE (Radev and McKeown,
1997) which identifies named entities and noun
phrases that describe them in text.
</bodyText>
<sectionHeader confidence="0.948778" genericHeader="introduction">
2 System description
</sectionHeader>
<bodyText confidence="0.958880333333333">
Our system (Figure 1) consists of two pieces:
an IR component (GuruQA) that which returns
matching texts, and an answer selection compo-
</bodyText>
<page confidence="0.996295">
150
</page>
<bodyText confidence="0.9982334">
nent (AnSel/Werlect) that extracts and ranks
potential answers from these texts.
This paper focuses on the process of rank-
ing potential answers selected by the IR engine,
which is itself described in (Prager et al., 1999).
</bodyText>
<figureCaption confidence="0.999039">
Figure 1: System Architecture.
</figureCaption>
<subsectionHeader confidence="0.957926">
2.1 The Information Retrieval
component
</subsectionHeader>
<bodyText confidence="0.998898">
In the context of fact-seeking questions, we
made the following observations:
</bodyText>
<listItem confidence="0.958769952380952">
• In documents that contain the answers, the
query terms tend to occur in close proxim-
ity to each other.
• The answers to fact-seeking questions are
usually phrases: &amp;quot;President Clinton&amp;quot;, &amp;quot;in
the Rocky Mountains&amp;quot;, and &amp;quot;today&amp;quot;).
• These phrases can be categorized by a set of
a dozen or so labels (Figure 2) correspond-
ing to question types.
• The phrases can be identified in text by
pattern matching techniques (without full
NLP).
As a result, we defined a set of about 20 cat-
egories, each labeled with its own QA-Token,
and built an IR system which deviates from the
traditional model in three important aspects.
• We process the query against a set of ap-
proximately 200 question templates which,
may replace some of the query words
with a set of QA-Tokens, called a SYN-
class. Thus &amp;quot;Where&amp;quot; gets mapped
</listItem>
<bodyText confidence="0.997655571428571">
to &amp;quot;PLACE$&amp;quot;, but &amp;quot;How long &amp;quot; goes
to &amp;quot;@SYN(LENGTHS, DURATIONS)&amp;quot;.
Some templates do not cause complete re-
placement of the matched string. For ex-
ample, the pattern &amp;quot;What is the popula-
tion&amp;quot; gets replaced by &amp;quot;NUMBER$ popu-
lation&amp;quot;.
</bodyText>
<listItem confidence="0.99107447368421">
• Before indexing the text, we process it
with Textract (Byrd and Ravin, 1998;
Wacholder et al., 1997), which performs
lemmatization, and discovers proper names
and technical terms. We added a new
module (Resporator) which annotates text
segments with QA-Tokens using pattern
matching. Thus the text &amp;quot;for 5 centuries&amp;quot;
matches the DURATION$ pattern &amp;quot;for
:CARDINAL _timeperiod&amp;quot;, where :CAR-
DINAL is the label for cardinal numbers,
and _timeperiod marks a time expression.
• GuruQA scores text passages instead of
documents. We use a simple document-
and collection-independent weighting
scheme: QA-Tokens get a weight of 400,
proper nouns get 200 and any other word
- 100 (stop words are removed in query
processing after the pattern template
</listItem>
<bodyText confidence="0.961408833333333">
matching operation). The density of
matching query tokens within a passage is
contributes a score of 1 to 99 (the highest
scores occur when all matched terms are
consecutive).
Predictive Annotation works best for Where,
When, What, Which and How+adjective ques-
tions than for How+verb and Why questions,
since the latter are typically not answered by
phrases. However, we observed that &amp;quot;by&amp;quot; +
the present participle would usually indicate
the description of a procedure, so we instan-
tiate a METHOD$ QA-Token for such occur-
rences. We have no such QA-Token for Why
questions, but we do replace the word &amp;quot;why&amp;quot;
with &amp;quot;OSYN(result, cause, because)&amp;quot;, since the
occurrence of any of these words usually beto-
kens an explanation.
</bodyText>
<sectionHeader confidence="0.933766" genericHeader="method">
3 Answer selection
</sectionHeader>
<bodyText confidence="0.969369">
So far, we have described how we retrieve rel-
evant passages that may contain the answer to
a query. The output of GuruQA is a list of
10 short passages containing altogether a large
</bodyText>
<figure confidence="0.996453">
Search
Index
GuruQA
Textract
Resporator
Query
Processing
Indexer
Hit List
Ranked
Hi tList
AnSel/
Werlect
Answer selection
documents
query
</figure>
<page confidence="0.976263">
151
</page>
<table confidence="0.999116952380952">
QA-Token Question type Example
PLACES Where In the Rocky Mountains
COUNTRY$ Where/What country United Kingdom
STATES Where/What state Massachusetts
PERSONS Who Albert Einstein
ROLES Who Doctor
NAMES Who/What/Which The Shakespeare Festival
ORG$ Who/What The US Post Office
DURATIONS How long For 5 centuries
AGES How old 30 years old
YEARS When/What year 1999
TIMES When In the afternoon
DATES When/What date July 4th, 1776
VOLUMES How big 3 gallons
AREAS How big 4 square inches
LENGTHS How big/long/high 3 miles
WEIGHTS How big/heavy 25 tons
NUMBERS How many 1,234.5
METHODS How By rubbing
RATES How much 50 per cent
MONEYS How much 4 million dollars
</table>
<figureCaption confidence="0.996277">
Figure 2: Sample QA-Tokens.
</figureCaption>
<bodyText confidence="0.999882666666667">
number (often more than 30 or 40) of potential
answers in the form of phrases annotated with
QA-Tokens.
</bodyText>
<subsectionHeader confidence="0.999812">
3.1 Answer ranking
</subsectionHeader>
<bodyText confidence="0.999994571428571">
We now describe two algorithms, AnSel and
Werlect, which rank the spans returned by Gu-
ruQA. AnSel and Werlectl use different ap-
proaches, which we describe, evaluate and com-
pare and contrast. The output of either system
consists of five text extracts per question that
contain the likeliest answers to the questions.
</bodyText>
<subsectionHeader confidence="0.99967">
3.2 Sample Input to AnSel/Werlect
</subsectionHeader>
<bodyText confidence="0.999031142857143">
The role of answer selection is to decide which
among the spans extracted by GuruQA are
most likely to contain the precise answer to the
questions. Figure 3 contains an example of the
data structure passed from GuruQA to our an-
swer selection module.
The input consists of four items:
</bodyText>
<listItem confidence="0.9872776">
• a query (marked with &lt;QUERY&gt; tokens
in the example),
• a list of 10 passages (one of which is shown
above),
• a list of annotated text spans within the
passages, annotated with QA-Tokens, and
&apos;from ANswer SELect and ansWER seLECT, respec-
tively
• the SYN-class corresponding to the type of
question (e.g., &amp;quot;PERSON$ NAME$&amp;quot;).
</listItem>
<bodyText confidence="0.999916375">
The text in Figure 3 contains five spans (po-
tential answers), of which three (&amp;quot;Biography of
Margaret Thatcher&amp;quot;, &amp;quot;Hugo Young&amp;quot;, and &amp;quot;Mar-
garet Thatcher&amp;quot;) are of types included in the
SYN-class for the question (PERSON NAME).
The full output of GuruQA for this question in-
cludes a total of 14 potential spans (5 PERSONs
and 9 NAMEs).
</bodyText>
<subsectionHeader confidence="0.999665">
3.3 Sample Output of AnSel/Werlect
</subsectionHeader>
<bodyText confidence="0.999789823529412">
The answer selection module has two outputs:
internal (phrase) and external (text passage).
Internal output: The internal output is a
ranked list of spans as shown in Table 1. It
represents a ranked list of the spans (potential
answers) sent by GuruQA.
External output: The external output is
a ranked list of 50-byte and 250-byte extracts.
These extracts are selected in a way to cover
the highest-ranked spans in the list of potential
answers. Examples are given later in the paper.
The external output was required for the
TREC evaluation while system&apos;s internal out-
put can be used in a variety of applications, e.g.,
to highlight the actual span that we believe is
the answer to the question within the context
of the passage in which it appears.
</bodyText>
<figure confidence="0.832738857142857">
1 52
&lt;p&gt; &lt;NUMBER&gt; 1&lt;/NUMBER&gt;&lt;/p&gt;
&lt;p&gt;&lt;QUERY&gt;Who is the author of the book, &amp;quot;The Iron Lady: A Biography of Margaret Thatcher&amp;quot;?
&lt;/QUERY&gt;&lt;/p&gt;
&lt;p&gt;&lt;PROCESSED_QUERY&gt;@excwin(*dynamic* ©weight( 200 *Iron_Lady) ©weight (200
Biography_of_Margaret_Thatcher) @weight(200 Margaret) @weight(100 author)
4weight(100 book) @weight(100 iron) @weight(100 lady) Oweight(100 :) @weight(100 biography)
aweight(100 thatcher) @weight(400 @syn(PERSON$ NAMES)))&lt;/PROCESSED_QUERY&gt;&lt;/p&gt;
&lt;p&gt;&lt;DOC&gt;LA000200-0118&lt;/DOC&gt;&lt;/p&gt; &lt;p&gt;&lt;SCORE&gt;1020.8114&lt;/SCORE&gt;&lt;/p&gt;
&lt;TEXT&gt;&lt;p&gt;THE IRON LADY; A &lt;span class=&amp;quot;NAME&amp;quot;&gt;Biography of Margaret Thatcher &lt;/span&gt;
by &lt;span class=&amp;quot;PERSON&amp;quot;&gt;Hugo Young&lt;/span&gt; (&lt;span class=&amp;quot;ORG&amp;quot;&gt;Farrar , Straus
Giroux&lt;/span&gt;) The central riddle revealed here is why, as a woman &lt;span class=&amp;quot;PLACEDEF&amp;quot;&gt;in a
man&lt;/span&gt;&apos;s world, &lt;span class=&amp;quot;PERSON&amp;quot;&gt;Margaret Thatcher&lt;/span&gt; evinces such an exclusionary
attitude toward women.&lt;/p&gt;&lt;/TEXT&gt;
</figure>
<figureCaption confidence="0.862025">
Figure 3: Input sent from GuruQA to AnSel/Werlect.
</figureCaption>
<table confidence="0.999530428571428">
Score Span
5.06 Hugo Young
-8.14 Biography of Margaret Thatcher
-13.60 David Williams
-18.00 Williams
-19.38 Sir Ronald Millar
-26.06 Santiago
-31.75 Oxford
-32.38 Maggie
-36.78 Seriously Rich
-42.68 FT
-198.34 Margaret Thatcher
-217.80 Thatcher
-234.55 Iron Lady
</table>
<tableCaption confidence="0.999769">
Table 1: Ranked potential answers to Quest. 1.
</tableCaption>
<sectionHeader confidence="0.363242" genericHeader="method">
4 Analysis of corpus and question
sets
</sectionHeader>
<bodyText confidence="0.99916225">
In this section we describe the corpora used for
training and evaluation as well as the questions
contained in the training and evaluation ques-
tion sets.
</bodyText>
<subsectionHeader confidence="0.999593">
4.1 Corpus analysis
</subsectionHeader>
<bodyText confidence="0.995292666666667">
For both training and evaluation, we used the
TREC corpus, consisting of approximately 2
GB of articles from four news agencies.
</bodyText>
<subsectionHeader confidence="0.958203">
4.2 Training set TR38
</subsectionHeader>
<bodyText confidence="0.987669">
To train our system, we used 38 questions (see
Figure 4) for which the answers were provided
by NIST.
</bodyText>
<subsectionHeader confidence="0.960561">
4.3 Test set T200
</subsectionHeader>
<bodyText confidence="0.972338">
The majority of the 200 questions (see Figure 5)
in the evaluation set (T200) were not substan-
</bodyText>
<figure confidence="0.971324111111111">
Question/Answer (TR38)
Q: Who was Johnny Mathis&apos; high school
track coach?
A: Lou Vasquez
Q: What year was the Magna Carta signed?
A: 1215
Q: What two companies produce bovine
somatotropin?
A: Monsanto and Eli Lilly
</figure>
<figureCaption confidence="0.999952">
Figure 4: Sample questions from TR38.
</figureCaption>
<bodyText confidence="0.9950825">
tially different from these in TR38, although the
introduction of &amp;quot;why&amp;quot; and &amp;quot;how&amp;quot; questions as
well as the wording of questions in the format
&amp;quot;Name X&amp;quot; made the task slightly harder.
</bodyText>
<figure confidence="0.786504888888889">
Question/Answer (T200)
Q: Why did David Koresh ask the FBI for a
word processor?
A: to record his revelations.
Q: How tall is the Matterhorn?
A: 14,776 feet 9 inches
Q: How tall is the replica of the Matterhorn
at Disneyland?
A: 147-foot
</figure>
<figureCaption confidence="0.999635">
Figure 5: Sample questions from T200.
</figureCaption>
<bodyText confidence="0.99531">
Some examples of problematic questions are
shown in Figure 6.
</bodyText>
<page confidence="0.995462">
153
</page>
<bodyText confidence="0.597154666666667">
Q: Why did David Koresh ask the FBI for
a word processor?
Q: Name the first private citizen to fly in
space.
Q: What is considered the costliest disaster
the insurance industry has ever faced?
Q: What did John Hinckley do to impress
Jodie Foster?
Q: How did Socrates die?
</bodyText>
<figureCaption confidence="0.990697">
Figure 6: Sample harder questions from T200.
</figureCaption>
<sectionHeader confidence="0.99233" genericHeader="method">
5 AnSel
</sectionHeader>
<bodyText confidence="0.997471888888889">
AnSel uses an optimization algorithm with 7
predictive variables to describe how likely a
given span is to be the correct answer to a
question. The variables are illustrated with ex-
amples related to the sample question number
10001 from TR38 &amp;quot;Who was Johnny Mathis&apos;
high school track coach?&amp;quot;. The potential an-
swers (extracted by GuruQA) are shown in Ta-
ble 2.
</bodyText>
<subsectionHeader confidence="0.994605">
5.1 Feature selection
</subsectionHeader>
<bodyText confidence="0.9839252">
The seven span features described below were
found to correlate with the correct answers.
Number: position of the span among all spans
returned from the hit-list.
Rspanno: position of the span among all spans
returned within the current passage.
Count: number of spans of any span class re-
trieved within the current passage.
Noting: the number of words in the span that
do not appear in the query.
Type: the position of the span type in the list
of potential span types. Example: Type
(&amp;quot;Lou Vasquez&amp;quot;) = 1, because the span
type of &amp;quot;Lou Vasquez&amp;quot;, namely &amp;quot;PER-
SON&amp;quot; appears first in the SYN-class &amp;quot;PER-
SON ORG NAME ROLE&amp;quot;.
Avgdst: the average distance in words between
the beginning of the span and query words
that also appear in the passage. Example:
given the passage &amp;quot;Tim O&apos;Donohue, Wood-
bridge High School&apos;s varsity baseball coach,
resigned Monday and will be replaced by
assistant Johnny Ceballos, Athletic Direc-
tor Dave Cowen said.&amp;quot; and the span &amp;quot;Tim
O&apos;Donohue&amp;quot; , the value of avgdst is equal
to 8.
Sscore: passage relevance as computed by Gu-
ruQA.
Number: the position of the span among all
retrieved spans.
</bodyText>
<subsectionHeader confidence="0.999579">
5.2 AnSel algorithm
</subsectionHeader>
<bodyText confidence="0.999483333333333">
The TOTAL score for a given potential answer
is computed as a linear combination of the fea-
tures described in the previous subsection:
</bodyText>
<sectionHeader confidence="0.286736" genericHeader="method">
TOT AL =&gt;w2*fj
</sectionHeader>
<bodyText confidence="0.9306142">
The algorithm that the training component
of AnSel uses to learn the weights used in the
formula is shown in Figure 7.
For each &lt;question,span&gt; tuple in training
set:
</bodyText>
<listItem confidence="0.997600888888889">
1. Compute features for each span
2. Compute TOTAL score for each span
using current set of weights
Repeat
3. Compute performance on training
set
4. Adjust weights wi through
logistic regression
Until performance &gt; threshold
</listItem>
<figureCaption confidence="0.9613945">
Figure 7: Training algorithm used by AnSel.
Training discovered the following weights:
</figureCaption>
<figure confidence="0.3789945">
3.0; wnoting
Wnumber = —0.3; Wrspanno = —0.5; Wcount =
2.0; wtypes = 15.0; Waegdst =
—1.0; Wsscore = 1.5
</figure>
<bodyText confidence="0.986140428571429">
At runtime, the weights are used to rank po-
tential answers. Each span is assigned a TO-
TAL score and the top 5 distinct extracts of
50 (or 250) bytes centered around the span are
output. The 50-byte extracts for question 10001
are shown in Figure 8. For lack of space, we are
omitting the 250-byte extracts.
</bodyText>
<sectionHeader confidence="0.997612" genericHeader="method">
6 Werlect
</sectionHeader>
<bodyText confidence="0.962755666666667">
The Werlect algorithm used many of the same
features of phrases used by AnSel, but employed
a different ranking scheme.
</bodyText>
<subsectionHeader confidence="0.997201">
6.1 Approach
</subsectionHeader>
<bodyText confidence="0.997307">
Unlike AnSel, Werlect is based on a two-step,
rule-based process approximating a function
with interaction between variables. In the first
stage of this algorithm, we assign a rank to
</bodyText>
<page confidence="0.999159">
154
</page>
<table confidence="0.9995315">
Span Type Number Rspanno Count Noting Type Avgdst Sscore TOTAL
011ie Matson 2 ma PV 222222mmomm &apos;Iv m .0 romrom 3 3 6 1 . o 1 12 0.02507 -7.53
0 &gt; rl 0 0 &gt; &gt; &gt; &gt; &gt; m o w o o moom o m m S8 0
Cl, 0 0 0 N , t,, ,,,, ... ,... N N ..., 01 ■—• S. N N N
&amp;quot; r 1 t &apos;1 0 0&apos; 0 CO&apos; 0) 0) Gi ,T) `, ; ,) CO r&gt;
z z z zzzzzzzzz
Lou Vasquez 1 1 6 1 16 0.02507 -9.93
Tim O&apos;Donohue 17 1 4 1 8 0.02257 -12.57
Athletic Director Dave Cowen 23 6 4 1 11 0.02257 -15.87
Johnny Ceballos 22 5 4 1 9 0.02257 -19.07
Civic Center Director Martin Durham 13 1 2 1 16 0.02505 -19.36
Johnny Hodges 25 2 4 1 15 0.02256 -25.22
Derric Evans 33 4 4 1 14 0.02256 -25.37
NEWSWIRE Johnny Majors 30 1 4 1 17 0.02256 -25.47
Woodbridge High School 18 2 4 2 6 0.02257 -28.37
Evan 37 6 4 1 14 0.02256 -29.57
Gary Edwards 38 7 4 1 17 0.02256 -30,87
O.J. Simpson 2 2 6 3 12 0.02507 -37.40
South Lake Tahoe 7 5 6 3 14 0.02507 -40.06
Washington High 10 6 6 3 18 0.02507 -49.80
Morgan 26 3 4 3 12 0.02256 -52.52
Tennesseefootball 31 2 4 3 15 0.02256 -56.27
Ellington 24 1 4 3 20 0.02256 -59.42
assistant 21 4 4 4 8 0.02257 -62.77
the Volunteers 34 5 4 4 14 0.02256 -71.17
Johnny Mathis 4 4 6 1 11 0.02507 -211.33
Mathis 14 2 2 3 10 0.02505 -254.16
coach 19 3 4 4 4 0.02257 -259.67
</table>
<tableCaption confidence="0.988458">
Table 2: Feature set and span rankings for training question 10001.
</tableCaption>
<note confidence="0.885699166666667">
Document ID Score Extract
LA053189-0069 892.5 of O.J. Simpson, 011ie Matson and Johnny Mathis
LA053189-0069 890.1 Lou Vasquez , track coach of O.J. Simpson , 011ie
LA060889-0181 887.4 Tim O&apos;Donohue , Woodbridge High School &apos;s varsity
LA060889-0181 884.1 nny Ceballos , Athletic Director Dave Cowen said.
LA060889-0181 880.9 aced by assistant Johnny Ceballos , Athletic Direc
</note>
<figureCaption confidence="0.999641">
Figure 8: Fifty-byte extracts.
</figureCaption>
<bodyText confidence="0.999928625">
every relevant phrase within each sentence ac-
cording to how likely it is to be the target an-
swer. Next, we generate and rank each N-byte
fragment based on the sentence score given by
GuruQA, measures of the fragment&apos;s relevance,
and the ranks of its component phrases. Unlike
AnSel, Werlect was optimized through manual
trial-and-error using the TR38 questions.
</bodyText>
<subsectionHeader confidence="0.999417">
6.2 Step One: Feature Selection
</subsectionHeader>
<bodyText confidence="0.999979757575758">
The features considered in Werlect that were
also used by AnSel, were Type, Avgdst and Ss-
core. Two additional features were also taken
into account:
NotinqW: a modified version of Noting. As
in AnSel, spans that are contained in the
query are given a rank of 0. However, par-
tial matches are weighted favorably in some
cases. For example, if the question asks,
&amp;quot;Who was Lincoln&apos;s Secretary of State?&amp;quot;
a noun phrase that contains &amp;quot;Secretary of
State&amp;quot; is more likely to be the answer than
one that does not. In this example, the
phrase, &amp;quot;Secretary of State William Se-
ward&amp;quot; is the most likely candidate. This
criterion also seems to play a role in the
event that Resporator fails to identify rel-
evant phrase types. For example, in the
training question, &amp;quot;What shape is a por-
poise&apos;s tooth?&amp;quot; the phrase &amp;quot;spade-shaped&amp;quot;
is correctly selected from among all nouns
and adjectives of the sentences returned by
Guru-QA.
Frequency: how often the span occurs across
different passages. For example, the test
question, &amp;quot;How many lives were lost in the
Pan Am crash in Lockerbie, Scotland?&amp;quot; re-
sulted in four potential answers in the first
two sentences returned by Guru-QA. Ta-
ble 3 shows the frequencies of each term,
and their eventual influence on the span
rank. The repeated occurrence of &amp;quot;270&amp;quot;,
helps promote it to first place.
</bodyText>
<subsectionHeader confidence="0.995738">
6.3 Step two: ranking the sentence
spans
</subsectionHeader>
<bodyText confidence="0.944892428571429">
After each relevant span is assigned a rank, we
rank all possible text segments of 50 (or 250)
bytes from the hit list based on the sum of the
phrase ranks plus additional points for other
words in the segment that match the query.
The algorithm used by Werlect is shown in
Figure 9.
</bodyText>
<page confidence="0.988502">
155
</page>
<table confidence="0.9996968">
Initial Sentence Rank Phrase Frequency Span Plank
1 Two 51 9
1 365 million 1 3
1 11 ii 4
2 270 7 I (ranked highest)
</table>
<tableCaption confidence="0.998671">
Table 3: Influence of frequency on span rank.
</tableCaption>
<reference confidence="0.9257136">
1. Let candidate_set = all potential
answers, ranked and sorted.
2. For each hit-list passage, extract
all spans of 50 (or 250) bytes, on
word boundaries.
3. Rank and sort all segments based
on phrase ranks, matching terms,
and sentence ranks.
4. For each candidate in sorted
candidate_set
- Let highest_ranked_span
= highest-ranked span
containing candidate
- Let answer_setLi++] =
highest_ranked_span
- Remove every candidate from
candidate_set that is found in
highest_ranked_span
- Exit if i &gt;
5. Output answer_set
</reference>
<figureCaption confidence="0.995172">
Figure 9: Algorithm used by Werlect.
</figureCaption>
<sectionHeader confidence="0.99655" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.999161">
In this section, we describe the performance of
our system using results from our four official
runs.
</bodyText>
<subsectionHeader confidence="0.991141">
7.1 Evaluation scheme
</subsectionHeader>
<bodyText confidence="0.997384777777778">
For each question, the performance is computed
as the reciprocal value of the rank (RAR) of
the highest-ranked correct answer given by the
system. For example, if the system has given
the correct answer in three positions: second,
third, and fifth, RAR for that question is
The Mean Reciprocal Answer Rank (MRAR)
is used to compute the overall performance of
systems participating in the TREC evaluation:
</bodyText>
<figure confidence="0.603755333333333">
1
RAR 1 n 1 \
ranki,MRAR n rank,)
</figure>
<footnote confidence="0.5667855">
7.2 Performance on the official
evaluation data
</footnote>
<bodyText confidence="0.98954935">
Overall, Ansel (runs A50 and A25) performed
marginally better than Werlect. However, we
noted that on the 14 questions we were unable
to classify with a QA-Token, Werlect (runs W50
and W250) achieved an MRAR of 3.5 to Ansel&apos;s
2.0.
The cumulative RAR of A50 on T200 (Ta-
ble 4) is 63.22 (i.e., we got 49 questions among
the 198 right from our first try and 39 others
within the first five answers).
The performance of A250 on T200 is shown
in Table 5. We were able to answer 71 questions
with our first answer and 38 others within our
first five answers (cumulative RAR = 85.17).
To better characterize the performance of our
system, we split the 198 questions into 20 groups
of 10 questions. Our performance on groups
of questions ranged from 0.87 to 5.50 MRAR
for A50 and from 1.98 to 7.5 MRAR for A250
(Table 6).
</bodyText>
<table confidence="0.999831666666667">
50 bytes 250 bytes
n 20 20
Avg 3.19 4.30
Min 0.87 1.98
Max 5.50 7.50
Std Dev 1.17 1.27
</table>
<tableCaption confidence="0.999433">
Table 6: Performance on groups of ten questions
</tableCaption>
<bodyText confidence="0.998458833333333">
Finally, Table 7 shows how our official runs
compare to the rest of the 25 official submis-
sions. Our performance using AnSel and 50-
byte output was 0.430. The performance of
Werlect was 0.395. On 250 bytes, AnSel scored
0.319 and Werlect - 0.280.
</bodyText>
<sectionHeader confidence="0.996711" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999952888888889">
We presented a new technique, predictive an-
notation, for finding answers to natural lan-
guage questions in text corpora. We showed
that a system based on predictive annotation
can deliver very good results compared to other
competing systems.
We described a set of features that correlate
with the plausibility of a given text span be-
ing a good answer to a question. We experi-
</bodyText>
<page confidence="0.996642">
156
</page>
<table confidence="0.977597333333333">
First Second Third Fourth Fifth TOTAL
nb of cases 49 15 11 9 4 88
Points 49.00 7.50 3.67 2.25 0.80 63.22
</table>
<tableCaption confidence="0.982544">
Table 4: Performance of A50 on T200
</tableCaption>
<table confidence="0.992968666666667">
First Second Third Fourth Fifth TOTAL
nb of cases 71 16 11 6 5 109
Points 71.00 8.00 3.67 1.50 1.00 85.17
</table>
<tableCaption confidence="0.982344">
Table 5: Performance of A250 on T200
</tableCaption>
<table confidence="0.995457666666667">
Run Median Average Our Average Nb Times Nb Times Nb Times
&gt; Median --= Median &lt; Median
W50 0.12 0.280 56 126 16
A50 0.12 0.319 72 112 14
W250 0.29 0.395 60 106 32
A250 0.29 0.430 66 110 22
</table>
<tableCaption confidence="0.999773">
Table 7: Comparison of our system with the other participants
</tableCaption>
<bodyText confidence="0.9997552">
mented with two algorithms for ranking poten-
tial answers based on these features. We discov-
ered that a linear combination of these features
performs better overall, while a non-linear algo-
rithm performs better on unclassified questions.
</bodyText>
<sectionHeader confidence="0.997102" genericHeader="acknowledgments">
9 Acknowledgments
</sectionHeader>
<bodyText confidence="0.999809833333333">
We would like to thank Eric Brown, Anni Co-
den, and Wlodek Zadrozny from IBM Research
for useful comments and collaboration. We
would also like to thank the organizers of the
TREC Q&amp;A evaluation for initiating such a
wonderful research initiative.
</bodyText>
<sectionHeader confidence="0.999378" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999277422222222">
Eric Breck, John Burger, David House, Marc
Light, and Inderjeet Mani. 1999. Ques-
tion answering from large document collec-
tions. In Proceedings of AAAI Fall Sympo-
sium on Question Answering Systems, North
Falmouth, Massachusetts.
Roy Byrd and Yael Ravin. 1998. Identifying
and extracting relations in text. In Proceed-
ings of NLDB, Klagenfurt, Austria.
Sanda Harabagiu and Steven J. Maiorano.
1999. Finding answers in large collections of
texts : Paragraph indexing + abductive in-
ference. In Proceedings of AAAI Fall Sympo-
sium on Question Answering Systems, North
Falmouth, Massachusetts.
Vladimir Kulyukin, Kristian Hammond, and
Robin Burke. 1998. Answering questions
for an organization online. In Proceedings of
AAAI, Madison, Wisconsin.
Julian M. Kupiec. 1993. MURAX: A robust
linguistic approach for question answering us-
ing an on-line encyclopedia. In Proceedings,
16th Annual International ACM SIGIR Con-
ference on Research and Development in In-
formation Retrieval.
John Prager, Dragomir R. Radev, Eric Brown,
Anni Coden, and Valerie Samn. 1999. The
use of predictive annotation for question an-
swering in TREC8. In Proceedings of TREC-
8, Gaithersburg, Maryland.
Dragomir R. Radev and Kathleen R. McKe-
own. 1997. Building a generation knowledge
source using internet-accessible newswire. In
Proceedings of the 5th Conference on Applied
Natural Language Processing, pages 221-228,
Washington, DC, April.
Ellen Voorhees. 1994. Query expansion using
lexical-semantic relations. In Proceedings of
ACM SIGIR, Dublin, Ireland.
Nina Wacholder, Yael Ravin, and Misook Choi.
1997. Disambiguation of proper names in
text. In Proceedings of the Fifth Applied Nat-
ural Language Processing Conference, Wash-
ington, D.C. Association for Computational
Linguistics.
</reference>
<page confidence="0.997717">
157
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.738491">
<title confidence="0.998014">Ranking suspected answers to natural language questions using predictive annotation</title>
<author confidence="0.999818">Dragomir R Radev</author>
<affiliation confidence="0.9999585">School of Information University of Michigan</affiliation>
<address confidence="0.999281">Ann Arbor, MI 48103</address>
<email confidence="0.999342">radevOumich.edu</email>
<author confidence="0.999741">John Prager</author>
<affiliation confidence="0.989401">TJ Watson Research Center Division</affiliation>
<address confidence="0.995023">Hawthorne, NY 10532</address>
<email confidence="0.998835">jprager@us.ibm.com</email>
<author confidence="0.966548">Valerie Samn</author>
<affiliation confidence="0.9990195">Teachers College Columbia University</affiliation>
<address confidence="0.998702">New York, NY 10027</address>
<email confidence="0.991011">vs115@columbia.edu</email>
<abstract confidence="0.9854025">In this paper, we describe a system to rank suspected answers to natural language questions. We process both corpus and query using a new technique, predictive annotation, which augments phrases in texts with labels anticipating their being targets of certain kinds of questions. Given a natural language question, our IR system returns a set of matching passages, which we then rank using a linear function of seven predictor variables. We provide an evaluation of the techniques based on results from the TREC Q&amp;A evaluation in which our system participated.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>Let candidate_set = all potential answers, ranked and sorted.</title>
<marker></marker>
<rawString>1. Let candidate_set = all potential answers, ranked and sorted.</rawString>
</citation>
<citation valid="false">
<title>For each hit-list passage, extract all spans of 50 (or 250) bytes, on word boundaries.</title>
<marker></marker>
<rawString>2. For each hit-list passage, extract all spans of 50 (or 250) bytes, on word boundaries.</rawString>
</citation>
<citation valid="false">
<title>Rank and sort all segments based on phrase ranks, matching terms, and sentence ranks.</title>
<marker></marker>
<rawString>3. Rank and sort all segments based on phrase ranks, matching terms, and sentence ranks.</rawString>
</citation>
<citation valid="false">
<title>For each candidate</title>
<note>in sorted candidate_set</note>
<marker></marker>
<rawString>4. For each candidate in sorted candidate_set</rawString>
</citation>
<citation valid="false">
<title>Let highest_ranked_span = highest-ranked span containing candidate</title>
<marker></marker>
<rawString>- Let highest_ranked_span = highest-ranked span containing candidate</rawString>
</citation>
<citation valid="false">
<journal>Let answer_setLi++] = highest_ranked_span</journal>
<marker></marker>
<rawString>- Let answer_setLi++] = highest_ranked_span</rawString>
</citation>
<citation valid="false">
<title>Remove every candidate from candidate_set that is found</title>
<note>in highest_ranked_span</note>
<marker></marker>
<rawString>- Remove every candidate from candidate_set that is found in highest_ranked_span</rawString>
</citation>
<citation valid="false">
<title>Exit if i &gt; 5. Output answer_set</title>
<marker></marker>
<rawString>- Exit if i &gt; 5. Output answer_set</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>John Burger</author>
<author>David House</author>
<author>Marc Light</author>
<author>Inderjeet Mani</author>
</authors>
<title>Question answering from large document collections.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI Fall Symposium on Question Answering Systems,</booktitle>
<location>North Falmouth, Massachusetts.</location>
<contexts>
<context position="3311" citStr="Breck et al., 1999" startWordPosition="524" endWordPosition="527">oorhees, 1994) but is expected to achieve much better performance since potentially matching phrases in text are classified in a similar and synergistic way. Our system participated in the official TREC Q&amp;A evaluation. For 200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus. The results are shown in Section 7. Some techniques used by other participants in the TREC evaluation are paragraph indexing, followed by abductive inference (Harabagiu and Maiorano, 1999) and knowledge-representation combined with information retrieval (Breck et al., 1999). Some earlier systems related to our work are FaqFinder (Kulyukin et al., 1998), MURAX (Kupiec, 1993), which uses an encyclopedia as a knowledge base from which to extract answers, and PROFILE (Radev and McKeown, 1997) which identifies named entities and noun phrases that describe them in text. 2 System description Our system (Figure 1) consists of two pieces: an IR component (GuruQA) that which returns matching texts, and an answer selection compo150 nent (AnSel/Werlect) that extracts and ranks potential answers from these texts. This paper focuses on the process of ranking potential answers</context>
</contexts>
<marker>Breck, Burger, House, Light, Mani, 1999</marker>
<rawString>Eric Breck, John Burger, David House, Marc Light, and Inderjeet Mani. 1999. Question answering from large document collections. In Proceedings of AAAI Fall Symposium on Question Answering Systems, North Falmouth, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Byrd</author>
<author>Yael Ravin</author>
</authors>
<title>Identifying and extracting relations in text.</title>
<date>1998</date>
<booktitle>In Proceedings of NLDB,</booktitle>
<location>Klagenfurt, Austria.</location>
<contexts>
<context position="5245" citStr="Byrd and Ravin, 1998" startWordPosition="848" endWordPosition="851">ach labeled with its own QA-Token, and built an IR system which deviates from the traditional model in three important aspects. • We process the query against a set of approximately 200 question templates which, may replace some of the query words with a set of QA-Tokens, called a SYNclass. Thus &amp;quot;Where&amp;quot; gets mapped to &amp;quot;PLACE$&amp;quot;, but &amp;quot;How long &amp;quot; goes to &amp;quot;@SYN(LENGTHS, DURATIONS)&amp;quot;. Some templates do not cause complete replacement of the matched string. For example, the pattern &amp;quot;What is the population&amp;quot; gets replaced by &amp;quot;NUMBER$ population&amp;quot;. • Before indexing the text, we process it with Textract (Byrd and Ravin, 1998; Wacholder et al., 1997), which performs lemmatization, and discovers proper names and technical terms. We added a new module (Resporator) which annotates text segments with QA-Tokens using pattern matching. Thus the text &amp;quot;for 5 centuries&amp;quot; matches the DURATION$ pattern &amp;quot;for :CARDINAL _timeperiod&amp;quot;, where :CARDINAL is the label for cardinal numbers, and _timeperiod marks a time expression. • GuruQA scores text passages instead of documents. We use a simple documentand collection-independent weighting scheme: QA-Tokens get a weight of 400, proper nouns get 200 and any other word - 100 (stop word</context>
</contexts>
<marker>Byrd, Ravin, 1998</marker>
<rawString>Roy Byrd and Yael Ravin. 1998. Identifying and extracting relations in text. In Proceedings of NLDB, Klagenfurt, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanda Harabagiu</author>
<author>Steven J Maiorano</author>
</authors>
<title>Finding answers in large collections of texts : Paragraph indexing + abductive inference.</title>
<date>1999</date>
<booktitle>In Proceedings of AAAI Fall Symposium on Question Answering Systems,</booktitle>
<location>North Falmouth, Massachusetts.</location>
<contexts>
<context position="3225" citStr="Harabagiu and Maiorano, 1999" startWordPosition="514" endWordPosition="517">ted query. This classification of questions is conceptually similar to the query expansion in (Voorhees, 1994) but is expected to achieve much better performance since potentially matching phrases in text are classified in a similar and synergistic way. Our system participated in the official TREC Q&amp;A evaluation. For 200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus. The results are shown in Section 7. Some techniques used by other participants in the TREC evaluation are paragraph indexing, followed by abductive inference (Harabagiu and Maiorano, 1999) and knowledge-representation combined with information retrieval (Breck et al., 1999). Some earlier systems related to our work are FaqFinder (Kulyukin et al., 1998), MURAX (Kupiec, 1993), which uses an encyclopedia as a knowledge base from which to extract answers, and PROFILE (Radev and McKeown, 1997) which identifies named entities and noun phrases that describe them in text. 2 System description Our system (Figure 1) consists of two pieces: an IR component (GuruQA) that which returns matching texts, and an answer selection compo150 nent (AnSel/Werlect) that extracts and ranks potential an</context>
</contexts>
<marker>Harabagiu, Maiorano, 1999</marker>
<rawString>Sanda Harabagiu and Steven J. Maiorano. 1999. Finding answers in large collections of texts : Paragraph indexing + abductive inference. In Proceedings of AAAI Fall Symposium on Question Answering Systems, North Falmouth, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir Kulyukin</author>
<author>Kristian Hammond</author>
<author>Robin Burke</author>
</authors>
<title>Answering questions for an organization online.</title>
<date>1998</date>
<booktitle>In Proceedings of AAAI,</booktitle>
<location>Madison, Wisconsin.</location>
<contexts>
<context position="3391" citStr="Kulyukin et al., 1998" startWordPosition="537" endWordPosition="540">tially matching phrases in text are classified in a similar and synergistic way. Our system participated in the official TREC Q&amp;A evaluation. For 200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus. The results are shown in Section 7. Some techniques used by other participants in the TREC evaluation are paragraph indexing, followed by abductive inference (Harabagiu and Maiorano, 1999) and knowledge-representation combined with information retrieval (Breck et al., 1999). Some earlier systems related to our work are FaqFinder (Kulyukin et al., 1998), MURAX (Kupiec, 1993), which uses an encyclopedia as a knowledge base from which to extract answers, and PROFILE (Radev and McKeown, 1997) which identifies named entities and noun phrases that describe them in text. 2 System description Our system (Figure 1) consists of two pieces: an IR component (GuruQA) that which returns matching texts, and an answer selection compo150 nent (AnSel/Werlect) that extracts and ranks potential answers from these texts. This paper focuses on the process of ranking potential answers selected by the IR engine, which is itself described in (Prager et al., 1999). </context>
</contexts>
<marker>Kulyukin, Hammond, Burke, 1998</marker>
<rawString>Vladimir Kulyukin, Kristian Hammond, and Robin Burke. 1998. Answering questions for an organization online. In Proceedings of AAAI, Madison, Wisconsin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian M Kupiec</author>
</authors>
<title>MURAX: A robust linguistic approach for question answering using an on-line encyclopedia.</title>
<date>1993</date>
<booktitle>In Proceedings, 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</booktitle>
<contexts>
<context position="3413" citStr="Kupiec, 1993" startWordPosition="542" endWordPosition="543"> are classified in a similar and synergistic way. Our system participated in the official TREC Q&amp;A evaluation. For 200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus. The results are shown in Section 7. Some techniques used by other participants in the TREC evaluation are paragraph indexing, followed by abductive inference (Harabagiu and Maiorano, 1999) and knowledge-representation combined with information retrieval (Breck et al., 1999). Some earlier systems related to our work are FaqFinder (Kulyukin et al., 1998), MURAX (Kupiec, 1993), which uses an encyclopedia as a knowledge base from which to extract answers, and PROFILE (Radev and McKeown, 1997) which identifies named entities and noun phrases that describe them in text. 2 System description Our system (Figure 1) consists of two pieces: an IR component (GuruQA) that which returns matching texts, and an answer selection compo150 nent (AnSel/Werlect) that extracts and ranks potential answers from these texts. This paper focuses on the process of ranking potential answers selected by the IR engine, which is itself described in (Prager et al., 1999). Figure 1: System Archi</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Julian M. Kupiec. 1993. MURAX: A robust linguistic approach for question answering using an on-line encyclopedia. In Proceedings, 16th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Prager</author>
<author>Dragomir R Radev</author>
<author>Eric Brown</author>
<author>Anni Coden</author>
<author>Valerie Samn</author>
</authors>
<title>The use of predictive annotation for question answering in TREC8.</title>
<date>1999</date>
<booktitle>In Proceedings of TREC8,</booktitle>
<location>Gaithersburg, Maryland.</location>
<contexts>
<context position="3989" citStr="Prager et al., 1999" startWordPosition="634" endWordPosition="637">Kulyukin et al., 1998), MURAX (Kupiec, 1993), which uses an encyclopedia as a knowledge base from which to extract answers, and PROFILE (Radev and McKeown, 1997) which identifies named entities and noun phrases that describe them in text. 2 System description Our system (Figure 1) consists of two pieces: an IR component (GuruQA) that which returns matching texts, and an answer selection compo150 nent (AnSel/Werlect) that extracts and ranks potential answers from these texts. This paper focuses on the process of ranking potential answers selected by the IR engine, which is itself described in (Prager et al., 1999). Figure 1: System Architecture. 2.1 The Information Retrieval component In the context of fact-seeking questions, we made the following observations: • In documents that contain the answers, the query terms tend to occur in close proximity to each other. • The answers to fact-seeking questions are usually phrases: &amp;quot;President Clinton&amp;quot;, &amp;quot;in the Rocky Mountains&amp;quot;, and &amp;quot;today&amp;quot;). • These phrases can be categorized by a set of a dozen or so labels (Figure 2) corresponding to question types. • The phrases can be identified in text by pattern matching techniques (without full NLP). As a result, we def</context>
</contexts>
<marker>Prager, Radev, Brown, Coden, Samn, 1999</marker>
<rawString>John Prager, Dragomir R. Radev, Eric Brown, Anni Coden, and Valerie Samn. 1999. The use of predictive annotation for question answering in TREC8. In Proceedings of TREC8, Gaithersburg, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Building a generation knowledge source using internet-accessible newswire.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>221--228</pages>
<location>Washington, DC,</location>
<contexts>
<context position="3530" citStr="Radev and McKeown, 1997" startWordPosition="560" endWordPosition="563">ion. For 200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus. The results are shown in Section 7. Some techniques used by other participants in the TREC evaluation are paragraph indexing, followed by abductive inference (Harabagiu and Maiorano, 1999) and knowledge-representation combined with information retrieval (Breck et al., 1999). Some earlier systems related to our work are FaqFinder (Kulyukin et al., 1998), MURAX (Kupiec, 1993), which uses an encyclopedia as a knowledge base from which to extract answers, and PROFILE (Radev and McKeown, 1997) which identifies named entities and noun phrases that describe them in text. 2 System description Our system (Figure 1) consists of two pieces: an IR component (GuruQA) that which returns matching texts, and an answer selection compo150 nent (AnSel/Werlect) that extracts and ranks potential answers from these texts. This paper focuses on the process of ranking potential answers selected by the IR engine, which is itself described in (Prager et al., 1999). Figure 1: System Architecture. 2.1 The Information Retrieval component In the context of fact-seeking questions, we made the following obse</context>
</contexts>
<marker>Radev, McKeown, 1997</marker>
<rawString>Dragomir R. Radev and Kathleen R. McKeown. 1997. Building a generation knowledge source using internet-accessible newswire. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 221-228, Washington, DC, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Voorhees</author>
</authors>
<title>Query expansion using lexical-semantic relations.</title>
<date>1994</date>
<booktitle>In Proceedings of ACM SIGIR,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2706" citStr="Voorhees, 1994" startWordPosition="432" endWordPosition="433">ch, called predictive annotation, is to augment the query with semantic category markers (which we call QATokens), in this case with the PLACE$ token, and also to label with QA-Tokens all occurrences in text that are recognized entities, (for example, places). Then traditional bag-ofwords matching proceeds successfully, and will return matching passages. The answer-selection process then looks for and ranks in these passages occurrences of phrases containing the particular QA-Token(s) from the augmented query. This classification of questions is conceptually similar to the query expansion in (Voorhees, 1994) but is expected to achieve much better performance since potentially matching phrases in text are classified in a similar and synergistic way. Our system participated in the official TREC Q&amp;A evaluation. For 200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus. The results are shown in Section 7. Some techniques used by other participants in the TREC evaluation are paragraph indexing, followed by abductive inference (Harabagiu and Maiorano, 1999) and knowledge-representation combined with information retrieval (Breck et al., </context>
</contexts>
<marker>Voorhees, 1994</marker>
<rawString>Ellen Voorhees. 1994. Query expansion using lexical-semantic relations. In Proceedings of ACM SIGIR, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nina Wacholder</author>
<author>Yael Ravin</author>
<author>Misook Choi</author>
</authors>
<title>Disambiguation of proper names in text.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth Applied Natural Language Processing Conference,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Washington, D.C.</location>
<contexts>
<context position="5270" citStr="Wacholder et al., 1997" startWordPosition="852" endWordPosition="855">wn QA-Token, and built an IR system which deviates from the traditional model in three important aspects. • We process the query against a set of approximately 200 question templates which, may replace some of the query words with a set of QA-Tokens, called a SYNclass. Thus &amp;quot;Where&amp;quot; gets mapped to &amp;quot;PLACE$&amp;quot;, but &amp;quot;How long &amp;quot; goes to &amp;quot;@SYN(LENGTHS, DURATIONS)&amp;quot;. Some templates do not cause complete replacement of the matched string. For example, the pattern &amp;quot;What is the population&amp;quot; gets replaced by &amp;quot;NUMBER$ population&amp;quot;. • Before indexing the text, we process it with Textract (Byrd and Ravin, 1998; Wacholder et al., 1997), which performs lemmatization, and discovers proper names and technical terms. We added a new module (Resporator) which annotates text segments with QA-Tokens using pattern matching. Thus the text &amp;quot;for 5 centuries&amp;quot; matches the DURATION$ pattern &amp;quot;for :CARDINAL _timeperiod&amp;quot;, where :CARDINAL is the label for cardinal numbers, and _timeperiod marks a time expression. • GuruQA scores text passages instead of documents. We use a simple documentand collection-independent weighting scheme: QA-Tokens get a weight of 400, proper nouns get 200 and any other word - 100 (stop words are removed in query pr</context>
</contexts>
<marker>Wacholder, Ravin, Choi, 1997</marker>
<rawString>Nina Wacholder, Yael Ravin, and Misook Choi. 1997. Disambiguation of proper names in text. In Proceedings of the Fifth Applied Natural Language Processing Conference, Washington, D.C. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>