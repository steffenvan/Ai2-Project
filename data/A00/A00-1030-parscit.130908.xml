<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001586">
<title confidence="0.704886">
Aggressive Morphology for Robust Lexical Coverage
William A. Woods
</title>
<note confidence="0.69281">
Sun Microsystems Laboratories
1 Network Drive
Burlington, MA 01803
</note>
<email confidence="0.626084">
William.Woods©east.sun.com
</email>
<sectionHeader confidence="0.992353" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999929333333333">
This paper describes an approach to providing lex-
ical information for natural language processing in
unrestricted domains. A system of approximately
1200 morphological rules is used to extend a core lex-
icon of 39,000 words to provide lexical coverage that
exceeds that of a lexicon of 80,000 words or 150,000
word forms. The morphological system is described,
and lexical coverage is evaluated for random words
chosen from a previously unanalyzed corpus.
</bodyText>
<sectionHeader confidence="0.991464" genericHeader="keywords">
1 Motivation
</sectionHeader>
<bodyText confidence="0.997892976744186">
Many applications of natural language processing
have a need for a large vocabulary lexicon. How-
ever, no matter how large a lexicon one starts with,
most applications will encounter terms that are not
covered. This paper describes an approach to the
lexicon problem that emphasizes recognition of mor-
phological structure in unknown words in order to
extend a relatively small core lexicon to allow ro-
bust natural language processing in unrestricted do-
mains. This technique, which extends functionality
originally developed for the Lunar system (Woods et
al., 1972), has been most recently applied in a con-
ceptual indexing and retrieval system (Woods, 1997;
Ambroziak and Woods, 1998; Woods et al., 2000).
The system described here uses a collection of
approximately 1200 knowledge-based morphologi-
cal rules to extend a core lexicon of approximately
39,000 words to give coverage that exceeds that of an
English lexicon of more than 80,000 base forms (or
150,000 base plus inflected forms). To illustrate the
need for a robust extensible lexicon, a random sam-
ple of 100 words from the vocabulary of the million-
word Brown corpus (Kucera and Francis, 1967), con-
tained 24 words that were not included in a 300,000-
word list of English word forms. This suggests that
approximately 25% of the words in the Brown cor-
pus would not be covered by an independent lexicon
of even 300,000 words.
In a recent experiment, 54% of approximately
34,000 word types (numbers and hyphenated words
excluded) from a 3.1-million-word corpus of techni-
cal literature would not be covered by our hypothet-
ical 300,000-word lexicon. Many of these are special
forms (e.g., 1Vb203 and Ti/tin), and some are ap-
parent misspellings (e.g., auniprocessor and sychro-
nized), but the following are a sampling of fairly nor-
mal words that were not in the 300,000-word list:
busmaster
copyline
hereabove
preprocessing
uniprocessors
unreacted
</bodyText>
<sectionHeader confidence="0.9948215" genericHeader="introduction">
2 Integrated, Preferential, Heuristic
Morphology
</sectionHeader>
<bodyText confidence="0.998456034482759">
There are a number of systems that have been used
to describe natural language morphology for compu-
tational use. The most popular of these is perhaps
the finite-state Kimmo system (Koskenniemi, 1983).
Other approaches are described in (Sproat, 1992).
The system described here differs from other systems
in a number of dimensions. First, it is integrated
with an extensive lexicon, a semantic ontology, and
a syntactic analysis system, which it both consults
and augments. For example, subsumption relation-
ships in the semantic ontology enable the system to
determine whether a proposed root is a container or
a mental attitude, so that cupful is interpreted as a
unit of measure (a kind of noun), while hopeful is
interpreted as an adjective.
Second, it uses ordered preferential rules that at-
tempt to choose a small number of correct analy-
ses of a word (usually 1-3) from the many potential
analyses that might be found. Finally, it uses rules
that are heuristic in that they are not guaranteed to
give correct analyses, but rather are designed to deal
with various states of lack of knowledge and to make
plausible inferences in the face of uncertainty. The
focus is to use what it knows (or can infer) to de-
termine a usable set of part-of-speech classifications
for the word and to determine any root-plus-affix
or internal compound structure that is apparent. If
possible, it also assigns a semantic categorization to
the word. It deals with unknown as well as known
</bodyText>
<page confidence="0.997264">
218
</page>
<bodyText confidence="0.999961894736842">
roots, and it indicates relative confidences in its clas-
sifications when its rules indicate uncertainty in the
result.
The role of the morphological analysis component
in this system is to construct lexical entries for words
that do not already have entries, so that subsequent
encounters with the same word will find an already
existing lexical entry. Thus, morphological analysis
happens only once for each encountered word type
that is not already in the core lexicon. The resulting
lexical entries can be saved in a supplementary lex-
icon that is constructed as a side-effect of analyzing
text. The rules of the morphological analysis sys-
tem can ask syntactic and semantic questions about
potential base forms. The system handles prefixes,
suffixes, and lexical compounds (e.g., bitmap and re-
plybuffer). It also handles multiword lexical items
and many special forms, including Roman numer-
als, dates, and apparent phone numbers.
</bodyText>
<subsectionHeader confidence="0.999749">
2.1 Morphological rules and the lexicon
</subsectionHeader>
<bodyText confidence="0.998066">
The morphological analysis system makes use of a
number of different kinds of morphological rules, ap-
plied in the following preferential order to words that
are not already in the lexicon:
</bodyText>
<listItem confidence="0.993341666666666">
1. Morph-precheck for special forms
2. Phase one pass with suffix rules (allow only
&amp;quot;known&amp;quot; roots in phase one)
3. Prefix rules
4. Lexical compound rules
5. Check of name lists and city lists for words not
yet recognized
6. Phase two pass with suffix rules (allow unknown
roots and default rules)
</listItem>
<bodyText confidence="0.999821358974359">
Generally, the rules are ordered in decreasing or-
der of specificity, confidence and likelihood. Very
specific tests are applied in Step 1 to identify and
deal with &amp;quot;words&amp;quot; that are not ordinary sequences
of alphabetic characters. These include numbers,
alphanumeric sequences, and expressions involving
special characters. Falling this, an ordered sequence
of suffix rules is applied in Step 2 in a first pass that
will allow a match only if the proposed root word is
&amp;quot;known.&amp;quot; The same list of rules will be applied later
in a second pass without this known-root condition
if an earlier analysis does not succeed. This issue of
&amp;quot;known&amp;quot; roots is a subtle one that can involve con-
sulting external lists of known words as well as words
already in the lexicon, and can also consider certain
derived forms of known roots to be &amp;quot;known,&amp;quot; even
when they have not been previously encountered.
For example, if fish is a known word, then fishing is
as good as known, so is considered a &amp;quot;known&amp;quot; root
for this purpose. In general, suffix rules applied to
&amp;quot;known&amp;quot; roots are more reliable than applications of
rules to unknown roots or to words with no identifi-
able root.
If no phase-one suffix rules apply, prefix rules are
tried in Step 3 to see if an interpretation of this word
as a prefix combined with some other &amp;quot;known&amp;quot; word
is possible. Failing this, a set of lexical compound
rules is tried, in Step 4, to see if the word is inter-
pretable as a compound of two or more words, and
failing that, lists of first and last names of people
and names of cities are checked in Step 5. All of
steps 3-5 are considered more reliable if they suc-
ceed than the phase-two pass of the suffix rules that
comes in Step 6. This ordering allows prefixes and
compounding to be tried before less confident suffix
analyses are attempted, and avoids applying weak
suffix analyses to known names. Various other ways
to order these rules have been tried, but this order
has been found to be the most effective.
</bodyText>
<subsectionHeader confidence="0.993968">
2.2 Special form tests
</subsectionHeader>
<bodyText confidence="0.980689">
Before trying pattern-based rules for suffixes, pre-
fixes, and lexical compounds, the morphological an-
alyzer makes a number of tests for special forms that
require idiosyncratic treatment. These tests include
the following:
</bodyText>
<listItem confidence="0.998232733333334">
• number (including integer, floating, and expo-
nential notations, including numbers too large
to be represented internally as numbers in the
machine),
• Roman numeral (vii, mcm),
• ordinal (1st, 2nd, twenty-third),
• alphanum (A1203, 79D),
• letter (b, x),
• initial (B.),
• phone number (123-4567),
• hyphenated adjective (all-volunteer),
• ratio (3/4, V/R),
• multiword lexical item (snake_in_the_grass),
• special proper nouns (gls@init.edu, /usr/bin,
http //www . sun. com, C++)
</listItem>
<subsectionHeader confidence="0.880515">
2.3 Pattern-action rules
</subsectionHeader>
<bodyText confidence="0.489717">
Suffix rules in this system are pattern-action rules
that specify:
</bodyText>
<listItem confidence="0.998824">
1. a pattern of characters to match at the end of
the word to be analyzed,
2. possibly a number of characters to remove
and/or a sequence of characters to add to form
a root (or base form),
3. a sequence of tests and action clauses indicating
possible interpretations of a word matching this
pattern.
</listItem>
<page confidence="0.997068">
219
</page>
<bodyText confidence="0.9958615">
These rules are organized into blocks that are typi-
cally indexed by a shared final letter, and are applied
in order within a block until a rule is encountered
that generates one or more interpretations. At that
point, no further rules are tried, and the interpreta-
tions generated by that rule are used to construct a
lexical entry for the analyzed word.
The following is an example of a fairly specific,
but productive, knowledge-rich morphological suffix
rule:
</bodyText>
<figure confidence="0.983268705882353">
((f i s h) (kill 4)
(test (plausible-root root))
(cat nmsp
(is-root-of-cat root &apos;(adj n))
eval (progn (mark-dict lex
&apos;false-root
root t t)
(mark-dict lex
&apos;kindof
&apos;fish t t)
(mark-dict lex
&apos;has-prefix
root t t)
(mark-dict lex
&apos;root
&apos;fish t t)
&apos;-es)))
</figure>
<bodyText confidence="0.999955731707317">
This rule matches a word that ends in fish and
removes four letters from the end (the fish part) to
produce a root word which it then tests to see if it
is a plausible root (e.g., does it at least have a vowel
in it?). If it gets this far, the rule will construct
a category nmsp interpretation (a kind of noun), if
the condition (is-root-of-cat root &apos; (adj n)) is
true (i.e., if the root is a known adjective or noun).
This rule deals with words like hagfish and goatfish
and comes before the rules that handle words with
ish as a suffix, like doltish and oafish. Incidentally,
this rule doesn&apos;t apply to oafish because the hypoth-
esized root oa, which would result from removing
four letters, is not known to be an adjective or noun.
When this rule succeeds, it specifies that the word
will be assigned the category nmsp, a category indi-
cating a word that has a mass sense, a singular count
sense, and can also be used as a plural (e.g., Goatfish
are funny-looking.). (The category nmsp comes from
a collection of 91 syntactic categories, organized in
a hierarchy based on generality, so that, for exam-
ple, nm subsumes nmsp.) The action part of this rule
specifies that (contrary to the usual case) the &amp;quot;root&amp;quot;
obtained by removing characters from the end of the
word (e.g., goat) is in this case a false root. The real
root is fish, and the false root (goat) is actually a
prefix. The rule also specifies that the word refers
to a kind of fish and that the inflectional paradigm
for this word is -es (thus allowing goatfishes as an
alternative plural).
The rules within a block are ordered in decreasing
order of confidence and specificity. Thus, rules with
conditions that check explicit inflectional paradigms
of known roots are ordered before rules that guess
the inflectional paradigm from the spelling of the
root, and rules with more specific conditions are or-
dered before rules with less specific conditions so
that the latter can assume that the former will al-
ready have been tested and rejected. The rules
within a block of suffix rules will typically try for
interpretations in roughly the following order:
</bodyText>
<listItem confidence="0.994702333333333">
1. inflected form of a known root satisfying a
named inflectional paradigm (paradigmatic)
2. inflected form of a known word in right category
with unknown inflectional paradigm
3. apparent inflected form of a known word of
some other category
4. apparent inflected form of an unknown word
5. apparent derived form of a known root of the
right category
6. apparent derived form of a known root regard-
less of category
7. apparent derived form of an unknown root
</listItem>
<bodyText confidence="0.939835296296296">
8. word with apparent syntactic category and per-
haps suffix, without identifiable root
9. guessed noun (and perhaps verb also, if core
vocabulary is not comprehensive)
The last rule in this sequence is a default guess-
ing rule that depends on a flag that tells it whether
it is running with a core lexicon that is believed to
contain most nonobvious verbs. If so, then only the
noun part-of-speech is assigned, but with a smaller
core lexicon, the guessing rules would also assign a
less likely interpretation as a verb, in order to pro-
vide a way for unknown verbs to be parsed correctly
in sentences.
Prefix rules are similar in structure to suffix rules,
except that the pattern is matched at the beginning
of the word, and the rule blocks are indexed by the
initial letter of the word. Lexical compound rules
have a slightly different format and are called by a
specialized interpreter that looks for places to divide
a word into two pieces of sufficient size. The points
of potential decomposition are searched from right
to left, and the first such point that has an interpre-
tation is taken, with the following exception: The
morph compound analyzer checks for special cases
where, for example, the first word is plural and ends
in an s, but there is an alternative segmentation in
which the singular of the first word is followed by a
</bodyText>
<page confidence="0.987968">
220
</page>
<bodyText confidence="0.999977285714286">
word starting with the s. In such cases, the decom-
position using the singular first word is preferred
over the one using the plural. For example, the
word minesweeper will be analyzed as mine+ sweeper
rather than mines+weeper. This preference heuris-
tic is specific to English and might be different for
other languages.
</bodyText>
<subsectionHeader confidence="0.996683">
2.4 Recursive application of rules
</subsectionHeader>
<bodyText confidence="0.999983222222223">
When attempting to apply a rule to a word, the
morphological analyzer can be applied recursively
to analyze the hypothesized root. A simple caching
technique is used to control the potential for corn-
binatoric explosion and to block looping. This is
sufficiently effective that the time required for mor-
phological analysis is a negligible part of the time
required for processing large amounts of natural lan-
guage text. Protection against looping is especially
important for a kind of morphological rule that de-
rives one word from another without either of them
being a root of the other in the usual sense (e.g., de-
riving communist from communism or external from
internal). Operating in a loop-safe environment al-
lows rules like these to identify the relationship be-
tween a new word and a known word in either di-
rection, whichever of the two forms is encountered
first.
</bodyText>
<sectionHeader confidence="0.995601" genericHeader="evaluation">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.999929952380953">
Since analyzing a word is done once per unknown
word type and consumes a negligible fraction of the
overall text-processing time, speed of operation is
not considered a factor for evaluation. The inter-
esting dimension of evaluation deals with the cov-
erage of the rules and the kinds of errors that are
made. This was tested by applying the system to
two word lists randomly selected from the Brown
corpus and provided to me by Philip Resnik, using
some sampling tools that he developed. The first of
these (the token sample) consists of 100 word tokens
selected randomly, without eliminating duplicates,
and the second (the type sample) consists of 100
distinct word types selected randomly from the vo-
cabulary of the Brown corpus. Prior to a single test
run on each of these samples, neither the lexicon nor
the morphological rule system had any exposure to
the Brown corpus, nor had either of these word lists
been looked at by the experimenter. Consequently,
the results are a fair evaluation of the expected per-
formance of this system on an unknown domain.
</bodyText>
<subsectionHeader confidence="0.999932">
3.1 Grading rule performance
</subsectionHeader>
<bodyText confidence="0.987529142857143">
Since different syntactic category errors have dif-
ferent consequences for parsing text, it is useful to
grade the syntactic category assignments of the ana-
lyzer on an A-B-C-D-F scale according to the sever-
ity of any mistakes. Grades are assigned to a lexical
entry as follows:
A if all appropriate syntactic categories are assigned
and no incorrect categories are assigned
B if all categories are correct, allowing for catego-
rizing an adjective or a name as a noun or a
noun as a name
C if an entry has at least one correct category and
is correct except for missing a noun category or
having a single extra category
D if there is more than one extra category or if there
is a missing category other than one of the above
cases, provided that there is at least one correct
category
F if there are no correct categories
Both A and B grades are considered acceptable
assignments for the sake of evaluation, since cate-
gory B errors would allow a reasonable parse to be
found. This is because the grammar used for pars-
ing sentences and phrases allows a noun to be used
as an adjective modifier and a proper noun to be
used in place of a noun. One parser/grammar that
uses this lexicon also allows any other category to be
used as a noun, at the expense of a penalty, so that
a C grade will still enable a parse, although with a
penalty and a substantial likelihood that other false
parses might score better. Similarly, a D grade in-
creases the likelihood that a false parse might score
better.
Separately, we measure whether count/mass dis-
tinctions are made correctly (for nouns only),
and whether roots of derived and inflected forms
are identified correctly. We are interested in
the count/mass distinction because, like the com-
mon/proper noun distinction, it affects the gram-
maticality and likelihood, of a noun phrase interpre-
tation for a singular noun in absence of an explicit
determiner.
</bodyText>
<subsectionHeader confidence="0.999913">
3.2 Sampling rule performance
</subsectionHeader>
<bodyText confidence="0.9999760625">
The morphological analyzer has been applied to the
words from the two sample word lists that were not
already in its core lexicon. There were 17 such
words from the token sample and 72 such words
from the type sample. Of the 17 unknown token-
sample words, 100% were graded B or better (88%
A and 12% B); 85% of the roots were identified cor-
rectly (all but one); 85% of the count noun senses
were found (all but one); and 100% of the mass noun
senses were found. Token-sample performance is not
a very challenging test for a morphological analyzer
because it is biased toward a relatively small number
of frequently occurring word types. Token-sample
performance is used to assess the per-token error rate
that one would expect in analyzing large amounts of
running text. In contrast, type-sample performance
</bodyText>
<page confidence="0.998814">
221
</page>
<tableCaption confidence="0.999633">
Table 1: Syntactic category performance of the analyzer.
</tableCaption>
<table confidence="0.999362666666667">
Category Grade A B C D F B or better
Number 62 8 1 0 1 70
Percent 86% 11% 1.5% 0% 1.5% 97%
</table>
<tableCaption confidence="0.870615">
Table 2: Count/mass distinction performance of the analyzer.
</tableCaption>
<table confidence="0.9995">
Count/mass Good count Extra count Good mass Missing mass
Number 39 1 14 1
Percent 100% 2.6% 93% 6.7%
</table>
<tableCaption confidence="0.981838">
Table 3: Root identification performance of the analyzer.
</tableCaption>
<table confidence="0.998569666666667">
Detect root Good Wrong Debatable Missing Extra
Number 57 1 1 0 1
Percent 95% 1.7% 1.7% 0 1.7%
</table>
<bodyText confidence="0.938689428571429">
gives a measure of the expected performance on new
words the analyzer is likely to encounter.
For the 72 words in the type sample that are not
covered by the lexicon, Tables 1-3 show the syntactic
category performance of the analyzer and its abilities
to make count/mass distinctions and identify roots.
Notes on incorrect or debatable analyses:
</bodyText>
<listItem confidence="0.696223190476191">
1. One N (noun) for a probable name (Tonio),
counted as B.
2. Two NPR(proper name) for abbreviations;
(A. V. may be ADJ, W.B. is correct), counted
as one B and one A.
3. One wrong root when suffix ism was identified
as root of hooliganism in a hypothesized com-
pound hooligan+ism (arguably justifiable as a
kind of ism, which is known in the lexicon, but
counted as an error anyway). Reanalyzing this
word after hooligan is a known word gets the
correct interpretation.
4. One debatable root in the hyphenated phrase
reference-points whose root was listed as points
rather than reference-point. This is due to a
bug that caused the hyphenated word rules to
incorrectly identify this as a verb, rather than
a noun (counted as F for syntax).
5. One extra root for embouchure from embouche
(but a correct form of the French root?).
6. One missing category N for bobbles, which was
</listItem>
<bodyText confidence="0.9642344">
given category V but not N because the core
lexicon incorrectly listed bobble only as a verb
(counted as C for syntax). This is corrected by
adding the missing category to the lexical entry
for bobble.
</bodyText>
<sectionHeader confidence="0.999051" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999989590909091">
We have described an approach to robust lexical cov-
erage for unrestricted text applications that makes
use of an aggressive set of morphological rules to
supplement a core lexicon of approximately 39,000
words to give lexical coverage that exceeds that of a
much larger lexicon. This morphological analyzer
is integrated with an extensive lexicon, an ontol-
ogy, and a syntactic analysis system, which it both
consults and augments. It uses ordered preferential
rules that attempt to choose a small number of cor-
rect analyses of a word and are designed to deal with
various states of lack of knowledge. When applied
to 72 unknown words from a random sample of 100
distinct word types from the Brown corpus, its syn-
tactic category assignments received a grade of B or
better (using a grading system explained herein) for
97% of the words, and it correctly identified 95%
of the root words. This performance demonstrates
that one can obtain robust lexical coverage for natu-
ral language processing applications in unrestricted
domains, using a relatively small core lexicon and an
aggressive collection of morphological rules.
</bodyText>
<sectionHeader confidence="0.998795" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998510666666667">
Jacek Ambroziak and William A. Woods. 1998.
Natural language technology in precision content
retrieval. In International Conference on Natural
Language Processing and Industrial Applications,
Moncton, New Brunswick, Canada, August.
www.sun.com/research/techrep / 1998/abstract-
69.html.
Kimmo Koskenniemi. 1983. Two-level model for
morphological analysis. In Proceedings of the In-
ternational Joint Conference on Artificial Intelli-
gence, pages 683-685, Los Angelos, CA. Morgan
Kauffmann.
H. Kucera and W. Francis. 1967. Computa-
tional Analysis of Present-Day American English.
Brown University Press.
</reference>
<page confidence="0.9979095">
222
223
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.038832">
<title confidence="0.999681">Aggressive Morphology for Robust Lexical Coverage</title>
<author confidence="0.999945">William A Woods</author>
<affiliation confidence="0.823073">Sun Microsystems Laboratories 1 Network Drive</affiliation>
<address confidence="0.999849">Burlington, MA 01803</address>
<email confidence="0.997803">William.Woods©east.sun.com</email>
<abstract confidence="0.99675503586498">This paper describes an approach to providing lexical information for natural language processing in unrestricted domains. A system of approximately 1200 morphological rules is used to extend a core lexicon of 39,000 words to provide lexical coverage that exceeds that of a lexicon of 80,000 words or 150,000 word forms. The morphological system is described, and lexical coverage is evaluated for random words chosen from a previously unanalyzed corpus. 1 Motivation Many applications of natural language processing have a need for a large vocabulary lexicon. However, no matter how large a lexicon one starts with, most applications will encounter terms that are not covered. This paper describes an approach to the lexicon problem that emphasizes recognition of morphological structure in unknown words in order to extend a relatively small core lexicon to allow robust natural language processing in unrestricted domains. This technique, which extends functionality originally developed for the Lunar system (Woods et al., 1972), has been most recently applied in a conceptual indexing and retrieval system (Woods, 1997; Ambroziak and Woods, 1998; Woods et al., 2000). The system described here uses a collection of approximately 1200 knowledge-based morphological rules to extend a core lexicon of approximately 39,000 words to give coverage that exceeds that of an English lexicon of more than 80,000 base forms (or 150,000 base plus inflected forms). To illustrate the need for a robust extensible lexicon, a random sample of 100 words from the vocabulary of the millionword Brown corpus (Kucera and Francis, 1967), contained 24 words that were not included in a 300,000word list of English word forms. This suggests that approximately 25% of the words in the Brown corpus would not be covered by an independent lexicon of even 300,000 words. In a recent experiment, 54% of approximately 34,000 word types (numbers and hyphenated words excluded) from a 3.1-million-word corpus of technical literature would not be covered by our hypothetical 300,000-word lexicon. Many of these are special (e.g., some are apmisspellings (e.g., sychrothe following are a sampling of fairly normal words that were not in the 300,000-word list: busmaster copyline hereabove preprocessing uniprocessors unreacted 2 Integrated, Preferential, Heuristic Morphology There are a number of systems that have been used to describe natural language morphology for computational use. The most popular of these is perhaps the finite-state Kimmo system (Koskenniemi, 1983). Other approaches are described in (Sproat, 1992). The system described here differs from other systems in a number of dimensions. First, it is integrated with an extensive lexicon, a semantic ontology, and a syntactic analysis system, which it both consults and augments. For example, subsumption relationships in the semantic ontology enable the system to determine whether a proposed root is a container or mental attitude, so that interpreted as a of measure (a kind of noun), while interpreted as an adjective. Second, it uses ordered preferential rules that attempt to choose a small number of correct analyses of a word (usually 1-3) from the many potential analyses that might be found. Finally, it uses rules that are heuristic in that they are not guaranteed to give correct analyses, but rather are designed to deal with various states of lack of knowledge and to make plausible inferences in the face of uncertainty. The focus is to use what it knows (or can infer) to determine a usable set of part-of-speech classifications for the word and to determine any root-plus-affix or internal compound structure that is apparent. If possible, it also assigns a semantic categorization to the word. It deals with unknown as well as known 218 roots, and it indicates relative confidences in its classifications when its rules indicate uncertainty in the result. The role of the morphological analysis component in this system is to construct lexical entries for words that do not already have entries, so that subsequent encounters with the same word will find an already existing lexical entry. Thus, morphological analysis happens only once for each encountered word type that is not already in the core lexicon. The resulting lexical entries can be saved in a supplementary lexicon that is constructed as a side-effect of analyzing text. The rules of the morphological analysis system can ask syntactic and semantic questions about potential base forms. The system handles prefixes, and lexical compounds (e.g., realso handles multiword lexical items and many special forms, including Roman numerals, dates, and apparent phone numbers. 2.1 Morphological rules and the lexicon The morphological analysis system makes use of a number of different kinds of morphological rules, applied in the following preferential order to words that are not already in the lexicon: 1. Morph-precheck for special forms 2. Phase one pass with suffix rules (allow only &amp;quot;known&amp;quot; roots in phase one) 3. Prefix rules 4. Lexical compound rules 5. Check of name lists and city lists for words not yet recognized 6. Phase two pass with suffix rules (allow unknown roots and default rules) Generally, the rules are ordered in decreasing order of specificity, confidence and likelihood. Very specific tests are applied in Step 1 to identify and deal with &amp;quot;words&amp;quot; that are not ordinary sequences of alphabetic characters. These include numbers, alphanumeric sequences, and expressions involving special characters. Falling this, an ordered sequence of suffix rules is applied in Step 2 in a first pass that will allow a match only if the proposed root word is &amp;quot;known.&amp;quot; The same list of rules will be applied later in a second pass without this known-root condition if an earlier analysis does not succeed. This issue of &amp;quot;known&amp;quot; roots is a subtle one that can involve consulting external lists of known words as well as words already in the lexicon, and can also consider certain derived forms of known roots to be &amp;quot;known,&amp;quot; even when they have not been previously encountered. example, if a known word, then is as good as known, so is considered a &amp;quot;known&amp;quot; root for this purpose. In general, suffix rules applied to &amp;quot;known&amp;quot; roots are more reliable than applications of rules to unknown roots or to words with no identifiable root. If no phase-one suffix rules apply, prefix rules are tried in Step 3 to see if an interpretation of this word as a prefix combined with some other &amp;quot;known&amp;quot; word is possible. Failing this, a set of lexical compound rules is tried, in Step 4, to see if the word is interpretable as a compound of two or more words, and failing that, lists of first and last names of people and names of cities are checked in Step 5. All of steps 3-5 are considered more reliable if they succeed than the phase-two pass of the suffix rules that comes in Step 6. This ordering allows prefixes and compounding to be tried before less confident suffix analyses are attempted, and avoids applying weak suffix analyses to known names. Various other ways to order these rules have been tried, but this order has been found to be the most effective. 2.2 Special form tests Before trying pattern-based rules for suffixes, prefixes, and lexical compounds, the morphological analyzer makes a number of tests for special forms that require idiosyncratic treatment. These tests include the following: • number (including integer, floating, and exponential notations, including numbers too large to be represented internally as numbers in the machine), Roman numeral (vii, ordinal 2nd, twenty-third), alphanum 79D), letter x), initial phone number hyphenated adjective ratio V/R), multiword lexical item special proper nouns /usr/bin, http //www . sun. com, C++) rules Suffix rules in this system are pattern-action rules that specify: 1. a pattern of characters to match at the end of the word to be analyzed, 2. possibly a number of characters to remove and/or a sequence of characters to add to form a root (or base form), 3. a sequence of tests and action clauses indicating possible interpretations of a word matching this pattern. 219 These rules are organized into blocks that are typically indexed by a shared final letter, and are applied in order within a block until a rule is encountered that generates one or more interpretations. At that point, no further rules are tried, and the interpretations generated by that rule are used to construct a lexical entry for the analyzed word. The following is an example of a fairly specific, but productive, knowledge-rich morphological suffix rule: ((f i s h) (kill 4) (test (plausible-root root)) (cat nmsp (is-root-of-cat root &apos;(adj n)) eval (progn (mark-dict lex &apos;false-root root t t) (mark-dict lex &apos;kindof &apos;fish t t) (mark-dict lex &apos;has-prefix root t t) (mark-dict lex &apos;root &apos;fish t t) &apos;-es))) rule matches a word that ends in four letters from the end (the to produce a root word which it then tests to see if it is a plausible root (e.g., does it at least have a vowel in it?). If it gets this far, the rule will construct a category nmsp interpretation (a kind of noun), if condition root &apos; (adj n)) true (i.e., if the root is a known adjective or noun). rule deals with words like and comes before the rules that handle words with a suffix, like rule doesn&apos;t apply to the hypothroot would result from removing four letters, is not known to be an adjective or noun. When this rule succeeds, it specifies that the word be assigned the category category indicating a word that has a mass sense, a singular count and can also be used as a plural (e.g., funny-looking.). category from a collection of 91 syntactic categories, organized in a hierarchy based on generality, so that, for example, nm subsumes nmsp.) The action part of this rule specifies that (contrary to the usual case) the &amp;quot;root&amp;quot; obtained by removing characters from the end of the (e.g., in this case a false root. The real is the false root actually a prefix. The rule also specifies that the word refers to a kind of fish and that the inflectional paradigm this word is allowing an alternative plural). The rules within a block are ordered in decreasing order of confidence and specificity. Thus, rules with conditions that check explicit inflectional paradigms of known roots are ordered before rules that guess the inflectional paradigm from the spelling of the root, and rules with more specific conditions are ordered before rules with less specific conditions so that the latter can assume that the former will already have been tested and rejected. The rules within a block of suffix rules will typically try for interpretations in roughly the following order: 1. inflected form of a known root satisfying a named inflectional paradigm (paradigmatic) 2. inflected form of a known word in right category with unknown inflectional paradigm 3. apparent inflected form of a known word of some other category 4. apparent inflected form of an unknown word 5. apparent derived form of a known root of the right category 6. apparent derived form of a known root regardless of category 7. apparent derived form of an unknown root 8. word with apparent syntactic category and perhaps suffix, without identifiable root 9. guessed noun (and perhaps verb also, if core vocabulary is not comprehensive) The last rule in this sequence is a default guessing rule that depends on a flag that tells it whether it is running with a core lexicon that is believed to contain most nonobvious verbs. If so, then only the noun part-of-speech is assigned, but with a smaller core lexicon, the guessing rules would also assign a less likely interpretation as a verb, in order to provide a way for unknown verbs to be parsed correctly in sentences. Prefix rules are similar in structure to suffix rules, except that the pattern is matched at the beginning of the word, and the rule blocks are indexed by the initial letter of the word. Lexical compound rules have a slightly different format and are called by a specialized interpreter that looks for places to divide a word into two pieces of sufficient size. The points of potential decomposition are searched from right to left, and the first such point that has an interpretation is taken, with the following exception: The morph compound analyzer checks for special cases where, for example, the first word is plural and ends an there is an alternative segmentation in which the singular of the first word is followed by a 220 starting with the such cases, the decomposition using the singular first word is preferred over the one using the plural. For example, the be analyzed as sweeper than preference heuristic is specific to English and might be different for other languages. 2.4 Recursive application of rules When attempting to apply a rule to a word, the morphological analyzer can be applied recursively to analyze the hypothesized root. A simple caching technique is used to control the potential for cornbinatoric explosion and to block looping. This is sufficiently effective that the time required for morphological analysis is a negligible part of the time required for processing large amounts of natural language text. Protection against looping is especially important for a kind of morphological rule that derives one word from another without either of them being a root of the other in the usual sense (e.g., dein a loop-safe environment allows rules like these to identify the relationship between a new word and a known word in either direction, whichever of the two forms is encountered first. 3 Evaluation Since analyzing a word is done once per unknown word type and consumes a negligible fraction of the overall text-processing time, speed of operation is not considered a factor for evaluation. The interesting dimension of evaluation deals with the coverage of the rules and the kinds of errors that are made. This was tested by applying the system to two word lists randomly selected from the Brown corpus and provided to me by Philip Resnik, using some sampling tools that he developed. The first of (the token sample) consists of tokens selected randomly, without eliminating duplicates, the second (the type sample) consists of distinct word types selected randomly from the vocabulary of the Brown corpus. Prior to a single test run on each of these samples, neither the lexicon nor the morphological rule system had any exposure to the Brown corpus, nor had either of these word lists been looked at by the experimenter. Consequently, the results are a fair evaluation of the expected performance of this system on an unknown domain. 3.1 Grading rule performance Since different syntactic category errors have different consequences for parsing text, it is useful to grade the syntactic category assignments of the analyzer on an A-B-C-D-F scale according to the severity of any mistakes. Grades are assigned to a lexical entry as follows: all appropriate syntactic categories are assigned and no incorrect categories are assigned all categories are correct, allowing for categorizing an adjective or a name as a noun or a noun as a name an entry has at least one correct category and is correct except for missing a noun category or having a single extra category there is more than one extra category or if there is a missing category other than one of the above cases, provided that there is at least one correct category there are no correct categories Both A and B grades are considered acceptable assignments for the sake of evaluation, since category B errors would allow a reasonable parse to be found. This is because the grammar used for parsing sentences and phrases allows a noun to be used as an adjective modifier and a proper noun to be used in place of a noun. One parser/grammar that uses this lexicon also allows any other category to be used as a noun, at the expense of a penalty, so that a C grade will still enable a parse, although with a penalty and a substantial likelihood that other false parses might score better. Similarly, a D grade increases the likelihood that a false parse might score better. Separately, we measure whether count/mass distinctions are made correctly (for nouns only), and whether roots of derived and inflected forms are identified correctly. We are interested in the count/mass distinction because, like the common/proper noun distinction, it affects the gramand of a noun phrase interpretation for a singular noun in absence of an explicit determiner. 3.2 Sampling rule performance The morphological analyzer has been applied to the words from the two sample word lists that were not already in its core lexicon. There were 17 such words from the token sample and 72 such words from the type sample. Of the 17 unknown tokensample words, 100% were graded B or better (88% and 12% of the roots were identified correctly (all but one); 85% of the count noun senses were found (all but one); and 100% of the mass noun senses were found. Token-sample performance is not a very challenging test for a morphological analyzer because it is biased toward a relatively small number of frequently occurring word types. Token-sample performance is used to assess the per-token error rate that one would expect in analyzing large amounts of running text. In contrast, type-sample performance 221 Table 1: Syntactic category performance of the analyzer. Category Grade A B C D F B or better Number 62 8 1 0 1 70 Percent 86% 11% 1.5% 0% 1.5% 97% Table 2: Count/mass distinction performance of the analyzer. Count/mass Good count Extra count Good mass Missing mass Number 39 1 14 1 Percent 100% 2.6% 93% 6.7% Table3: identificationperformance ofthe analyzer. Detect root Good Wrong Debatable Missing Extra Number 57 1 1 0 1 Percent 95% 1.7% 1.7% 0 1.7% gives a measure of the expected performance on new words the analyzer is likely to encounter. For the 72 words in the type sample that are not covered by the lexicon, Tables 1-3 show the syntactic category performance of the analyzer and its abilities to make count/mass distinctions and identify roots. Notes on incorrect or debatable analyses: One N (noun) for a probable name counted as B. 2. Two NPR(proper name) for abbreviations; V. be ADJ, correct), counted as one B and one A. One wrong root when suffix identified root of a hypothesized comjustifiable as a kind of ism, which is known in the lexicon, but counted as an error anyway). Reanalyzing this after a known word gets the correct interpretation. 4. One debatable root in the hyphenated phrase root was listed as than is due to a bug that caused the hyphenated word rules to incorrectly identify this as a verb, rather than a noun (counted as F for syntax). One extra root for (but a correct form of the French root?). One missing category N for was given category V but not N because the core incorrectly listed as a verb (counted as C for syntax). This is corrected by adding the missing category to the lexical entry 4 Conclusions We have described an approach to robust lexical coverage for unrestricted text applications that makes use of an aggressive set of morphological rules to supplement a core lexicon of approximately 39,000 words to give lexical coverage that exceeds that of a much larger lexicon. This morphological analyzer is integrated with an extensive lexicon, an ontology, and a syntactic analysis system, which it both consults and augments. It uses ordered preferential rules that attempt to choose a small number of correct analyses of a word and are designed to deal with various states of lack of knowledge. When applied to 72 unknown words from a random sample of 100 distinct word types from the Brown corpus, its syntactic category assignments received a grade of B or better (using a grading system explained herein) for 97% of the words, and it correctly identified 95% of the root words. This performance demonstrates that one can obtain robust lexical coverage for natural language processing applications in unrestricted domains, using a relatively small core lexicon and an aggressive collection of morphological rules.</abstract>
<note confidence="0.6755476">References Jacek Ambroziak and William A. Woods. 1998. Natural language technology in precision content In Conference on Natural Language Processing and Industrial Applications, Moncton, New Brunswick, Canada, August. www.sun.com/research/techrep / 1998/abstract- 69.html. Kimmo Koskenniemi. 1983. Two-level model for analysis. In of the International Joint Conference on Artificial Intelli- 683-685, Los Angelos, CA. Morgan Kauffmann. and W. Francis. 1967. Computational Analysis of Present-Day American English.</note>
<affiliation confidence="0.980413">Brown University Press.</affiliation>
<address confidence="0.863149">222 223</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jacek Ambroziak</author>
<author>William A Woods</author>
</authors>
<title>Natural language technology in precision content retrieval.</title>
<date>1998</date>
<booktitle>In International Conference on Natural Language Processing and Industrial Applications,</booktitle>
<location>Moncton, New Brunswick, Canada,</location>
<note>www.sun.com/research/techrep / 1998/abstract69.html.</note>
<contexts>
<context position="1320" citStr="Ambroziak and Woods, 1998" startWordPosition="196" endWordPosition="199">eed for a large vocabulary lexicon. However, no matter how large a lexicon one starts with, most applications will encounter terms that are not covered. This paper describes an approach to the lexicon problem that emphasizes recognition of morphological structure in unknown words in order to extend a relatively small core lexicon to allow robust natural language processing in unrestricted domains. This technique, which extends functionality originally developed for the Lunar system (Woods et al., 1972), has been most recently applied in a conceptual indexing and retrieval system (Woods, 1997; Ambroziak and Woods, 1998; Woods et al., 2000). The system described here uses a collection of approximately 1200 knowledge-based morphological rules to extend a core lexicon of approximately 39,000 words to give coverage that exceeds that of an English lexicon of more than 80,000 base forms (or 150,000 base plus inflected forms). To illustrate the need for a robust extensible lexicon, a random sample of 100 words from the vocabulary of the millionword Brown corpus (Kucera and Francis, 1967), contained 24 words that were not included in a 300,000- word list of English word forms. This suggests that approximately 25% o</context>
</contexts>
<marker>Ambroziak, Woods, 1998</marker>
<rawString>Jacek Ambroziak and William A. Woods. 1998. Natural language technology in precision content retrieval. In International Conference on Natural Language Processing and Industrial Applications, Moncton, New Brunswick, Canada, August. www.sun.com/research/techrep / 1998/abstract69.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimmo Koskenniemi</author>
</authors>
<title>Two-level model for morphological analysis.</title>
<date>1983</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<pages>683--685</pages>
<publisher>Morgan Kauffmann.</publisher>
<location>Los Angelos, CA.</location>
<contexts>
<context position="2787" citStr="Koskenniemi, 1983" startWordPosition="433" endWordPosition="434">literature would not be covered by our hypothetical 300,000-word lexicon. Many of these are special forms (e.g., 1Vb203 and Ti/tin), and some are apparent misspellings (e.g., auniprocessor and sychronized), but the following are a sampling of fairly normal words that were not in the 300,000-word list: busmaster copyline hereabove preprocessing uniprocessors unreacted 2 Integrated, Preferential, Heuristic Morphology There are a number of systems that have been used to describe natural language morphology for computational use. The most popular of these is perhaps the finite-state Kimmo system (Koskenniemi, 1983). Other approaches are described in (Sproat, 1992). The system described here differs from other systems in a number of dimensions. First, it is integrated with an extensive lexicon, a semantic ontology, and a syntactic analysis system, which it both consults and augments. For example, subsumption relationships in the semantic ontology enable the system to determine whether a proposed root is a container or a mental attitude, so that cupful is interpreted as a unit of measure (a kind of noun), while hopeful is interpreted as an adjective. Second, it uses ordered preferential rules that attempt</context>
</contexts>
<marker>Koskenniemi, 1983</marker>
<rawString>Kimmo Koskenniemi. 1983. Two-level model for morphological analysis. In Proceedings of the International Joint Conference on Artificial Intelligence, pages 683-685, Los Angelos, CA. Morgan Kauffmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Kucera</author>
<author>W Francis</author>
</authors>
<title>Computational Analysis of Present-Day American English.</title>
<date>1967</date>
<publisher>Brown University Press.</publisher>
<contexts>
<context position="1791" citStr="Kucera and Francis, 1967" startWordPosition="274" endWordPosition="277">he Lunar system (Woods et al., 1972), has been most recently applied in a conceptual indexing and retrieval system (Woods, 1997; Ambroziak and Woods, 1998; Woods et al., 2000). The system described here uses a collection of approximately 1200 knowledge-based morphological rules to extend a core lexicon of approximately 39,000 words to give coverage that exceeds that of an English lexicon of more than 80,000 base forms (or 150,000 base plus inflected forms). To illustrate the need for a robust extensible lexicon, a random sample of 100 words from the vocabulary of the millionword Brown corpus (Kucera and Francis, 1967), contained 24 words that were not included in a 300,000- word list of English word forms. This suggests that approximately 25% of the words in the Brown corpus would not be covered by an independent lexicon of even 300,000 words. In a recent experiment, 54% of approximately 34,000 word types (numbers and hyphenated words excluded) from a 3.1-million-word corpus of technical literature would not be covered by our hypothetical 300,000-word lexicon. Many of these are special forms (e.g., 1Vb203 and Ti/tin), and some are apparent misspellings (e.g., auniprocessor and sychronized), but the followi</context>
</contexts>
<marker>Kucera, Francis, 1967</marker>
<rawString>H. Kucera and W. Francis. 1967. Computational Analysis of Present-Day American English. Brown University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>