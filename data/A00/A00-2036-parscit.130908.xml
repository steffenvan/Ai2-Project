<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000015">
<title confidence="0.98836">
Left-To-Right Parsing
and Bilexical Context-Free Grammars
</title>
<author confidence="0.621625">
Mark-Jan Nederhof
</author>
<affiliation confidence="0.431274">
DFKI
</affiliation>
<address confidence="0.749819333333333">
Stuhlsatzenhausweg 3
D-66123 Saarbriicken
Germany
</address>
<email confidence="0.990301">
nederhof@dfki.de
</email>
<note confidence="0.472475333333333">
Giorgio Satta
Dipartimento di Elettronica e Informatica
Universita di Padova
</note>
<address confidence="0.943148">
via Gradenigo, 6/A
1-35131 Padova, Italy
</address>
<email confidence="0.998718">
satta@dei.unipd.it
</email>
<sectionHeader confidence="0.997383" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999586">
We compare the asymptotic time complexity of
left-to-right and bidirectional parsing techniques for
bilexical context-free grammars, a grammar formal-
ism that is an abstraction of language models used in
several state-of-the-art real-world parsers. We pro-
vide evidence that left-to-right parsing cannot be re-
alised within acceptable time-bounds if the so called
correct-prefix property is to be ensured. Our evi-
dence is based on complexity results for the repre-
sentation of regular languages.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999872552631579">
Traditionally, algorithms for natural language pars-
ing process the input string strictly from left to right.
In contrast, several algorithms have been proposed
in the literature that process the input in a bidi-
rectional fashion; see (van Noord, 1997; Satta and
Stock, 1994) and references therein. The issue of
parsing efficiency for left-to-right vs. bidirectional
methods has longly been debated. On the basis of
experimental results, it has been argued that the
choice of the most favourable strategy should depend
on the grammar at hand. With respect to grammar
formalisms based upon context-free grammars, and
when the rules of these formalisms strongly depend
on lexical information, (van Noord, 1997) shows that
bidirectional strategies are more efficient than left-
to-right strategies. This is because bidirectional
strategies are most effective in reducing the parsing
search space, by activating as early as possible the
maximum number of lexical constraints available in
the grammar.
In this paper we present mathematical arguments
in support of the above empirically motivated the-
sis. We investigate a class of lexicalized grammars
that, in their probabilistic versions, have been widely
adopted as language models in state-of-the-art real-
world parsers. The size of these grammars usually
grows with the square of the size of the working lex-
icon, and thus can be very large. In these cases, the
primary goal in the design of a parsing algorithm
is to achieve asymptotic time performance sublinear
in the size of the working grammar and indepen-
dent of the size of the lexicon. These desiderata are
met by existing bidirectional algorithms (Alshawi,
1996; Eisner, 1997; Eisner and Satta, 1999). In con-
trast, we show the following two main results for
the asymptotic time performance of left-to-right al-
gorithms satisfying the so called correct-prefix prop-
erty.
</bodyText>
<listItem confidence="0.990834">
• In case off-line compilation of the working gram-
mar is not allowed, left-to-right parsing cannot
be realised within time bounds independent of
the size of the lexicon.
• In case polynomial-time, off-line compilation of
</listItem>
<bodyText confidence="0.932803483870968">
the working grammar is allowed, left-to-right
parsing cannot be realised in polynomial time,
and independently of the size of the lexicon, un-
less a strong conjecture based on complexity re-
sults for the representation of regular languages
is falsified.
The first result implies that the well known Earley
algorithm and related standard parsing techniques
that do not require grammar precompilation can-
not be directly extended to process the above men-
tioned grammars (resp. language models) within an
acceptable time bound. The second result provides
evidence that well known parsing techniques as left-
corner parsing, requiring polynomial-time prepro-
cessing of the grammar, also cannot be directly ex-
tended to process these formalisms within an accept-
able time bound.
The grammar formalisms we investigate are based
upon context-free grammars and are called bilex-
ical context-free grammars. Bilexical context-free
grammars have been presented in (Eisner and Satta,
1999) as an abstraction of language models that have
been adopted in several recent real-world parsers,
improving state-of-the-art parsing accuracy (Al-
shawi, 1996; Eisner, 1996; Charniak, 1997; Collins,
1997). Our results directly transfer to all these lan-
guage models. In a bilexical context-free grammar,
possible arguments of a word are always specified
along with possible head words for those arguments.
Therefore a bilexical grammar requires the grammar
writer to make stipulations about the compatibil-
</bodyText>
<page confidence="0.994228">
272
</page>
<bodyText confidence="0.999934428571429">
ity of particular pairs of words in particular roles,
something that was not necessarily true of general
context-free grammars.
The remainder of this paper is organized as fol-
lows. We introduce bilexical context-free grammars
in Section 2, and discuss parsing with the correct-
prefix property in Section 3. Our results for parsing
with on-line and off-line grammar compilation are
presented in Sections 4 and 5, respectively. To com-
plete the presentation, Appendix A shows that left-
to-right parsing in time independent of the size of
the lexicon is indeed possible when an off-line com-
pilation of the working grammar is allowed that has
an exponential time complexity.
</bodyText>
<sectionHeader confidence="0.586239" genericHeader="method">
2 Bilexical context-free grammars
</sectionHeader>
<bodyText confidence="0.99963">
In this section we introduce the grammar formalism
we investigate in this paper. This formalism, origi-
nally presented in (Eisner and Satta, 1999), is an ab-
straction of the language models adopted by several
state-of-the-art real-world parsers (see Section 1).
We specify a non-stochastic version of the formal-
ism, noting that probabilities may be attached to
the rewrite rules exactly as in stochastic CFG (Gon-
zales and Thomason, 1978; Wetherell, 1980). We
assume that the reader is familiar with context-free
grammars. Here we follow the notation of (Harrison,
1978; Hoperoft and Ullman, 1979).
A context-free grammar (CFG) is a tuple G =
( VN VT P, S), where VN and VT are finite, disjoint
sets of nonterminal and terminal symbols, respec-
tively, S E VN is the start symbol, and P is a finite
set of productions having the form A -4 a, where
A E VN and a E (VN U VT)*. A &amp;quot;derives&amp;quot; relation,
written is associated with a CFG as usual. We
use the reflexive and transitive closure of writ-
ten and define L(G) accordingly. The size of a
CFG G is defined as IG 1 = E(Aa)EP jAa. If every
production in P has the form A BC or A -4 a,
for A, B,C E VN , a E VT, then G is said to be in
Chomsky Normal Form (CNF).
A CFG G = (VN, VT/P7 S[$]) in CNF is called a
bilexical context-free grammar if there exists a
set VD called the set of delexicalized nontermi-
nals, such that nonterminals from VN are of the form
A[a], consisting of A E VD and a E VT, and every
production in P has one of the following two forms:
</bodyText>
<listItem confidence="0.9378155">
(i) A[a] B [b) C [c] , a E {b, c};
(ii) A[a] -4 a.
</listItem>
<bodyText confidence="0.99875825">
A nonterminal A[a] is said to have terminal symbol
a as its lexical head. Note that in a parse tree for
G, the lexical head of a nonterminal is always &amp;quot;in-
herited&amp;quot; from some daughter symbol (i.e., from some
symbol in the right-hand side of a production). In
the sequel, we also refer to the set VT as the lexicon
of the grammar.
A bilexical CFG can encode lexically specific pref-
erences in the form of binary relations on lexi-
cal items. For instance, one might specify P as
to contain the production VP[solve] --4 V[solve]
NP[puzzles] but not the production VP[eat] -4
V[eat] NP[puzzles]. This will allow derivation of
some VP constituents such as &amp;quot;solve two puzzles&amp;quot;,
while forbidding &amp;quot;eat two puzzles&amp;quot;. See (Eisner and
Satta, 1999) for further discussion.
The cost of this expressiveness is a very large
grammar. Indeed, we have ClI = (.9(1171313 • IVTI2 )1
and in practical applications WTI &gt;&gt; IVD1 &gt; 1. Thus,
the grammar size is dominated in its growth by the
square of the size of the working lexicon. Even if we
conveniently group lexical items with distributional
similarities into the same category, in practical ap-
plications the resulting grammar might have several
thousand productions. Parsing strategies that can-
not work in sublinear time with respect to the size of
the lexicon and with respect to the size of the whole
input grammar are very inefficient in these cases.
</bodyText>
<sectionHeader confidence="0.98622" genericHeader="method">
3 Correct-prefix property
</sectionHeader>
<bodyText confidence="0.999753970588235">
So called left-to-right strategies are standaidly
adopted in algorithms for natural language pars-
ing. Although intuitive, the notion of left-to-right
parsing is a concept with no precise mathematical
meaning. Note that in fact, in a pathological way,
one could read the input string from left-to-right,
storing it into some data structure, and then per-
form syntactic analysis with a non-left-to-right strat-
egy. In this paper we focus on a precise definition
of left-to-right parsing, known in the literature as
correct-prefix property parsing (Sippu and Soisalon-
Soininen, 1990). Several algorithms commonly used
in natural language parsing satisfy this property, as
for instance Earley&apos;s algorithm (Earley, 1970), tab-
ular left-corner and PLR parsing (Nederhof, 1994)
and tabular LR parsing (Tomita, 1986).
Let VT be some alphabet. A generic string over VT
is denoted as w = al • • an, with n &gt; 0 and ai E VT
(1 &lt; i &lt; n); in case n = 0, w equals the empty
string e. For integers i and j with 1 &lt; i &lt; j &lt; n, we
write w[i, j] to denote string ajai+i • • • ai; if i &gt; j,
we define w[i, j] = e.
Let G = (VN, VT, P, S) be a CFG and let w
al • • • an with n &gt; 0 be some string over VT. A rec-
ognizer for the CFG class is an algorithm R that,
on input (G, w), decides whether w E L(G). We
say that R satisfies the correct-prefix property
(CPP) if the following condition holds. Algorithm
R processes the input string from left-to-right, &amp;quot;con-
suming&amp;quot; one symbol a, at a time. If for some i,
0 &lt; i &lt; n, the set of derivations in G having the
form S = w[1, G (VN U VT)*, is empty, then
R rejects and halts, and it does so before consuming
symbol at+i, if i &lt; n. In this case, we say that R
</bodyText>
<page confidence="0.98953">
273
</page>
<bodyText confidence="0.999979043478261">
has detected an error at position i in w. Note that
the above property forces the recognizer to do rele-
vant computation for each terminal symbol that is
consumed.
We say that w[1, i] is a correct-prefix for a lan-
guage L if there exists a string z such that w[1, i]z E
L. In the natural language parsing literature, the
CPP is sometimes defined with the following condi-
tion in place of the above. If for some i, 0 &lt;i &lt; n,
w[1, i] is not a correct prefix for L(G), then R rejects
and halts, and it does so before consuming symbol
ai+1, if i &lt; n. Note that the latter definition asks
for a stronger condition, and the two definitions are
equivalent only in case the input grammar G is re-
duced.1 While the above mentioned parsing algo-
rithms satisfy the former definition of CPP, they do
not satisfy the latter. Actually, we are not aware of
any practically used parsing algorithm that satisfies
the latter definition of CPP.
One needs to distinguish CPP parsing from some
well known parsing algorithms in the literature that
process symbols in the right-hand sides of each gram-
mar production from left to right, but that do not
exhibit any left-to-right dependency between differ-
ent productions. In particular, processing of the
right-hand side of some production may be initi-
ated at some input position without consultation of
productions or parts of productions that may have
been found to cover parts of the input to the left
of that position. These algorithms may also consult
input symbols from left to right, but the processing
that takes place to the right of some position i does
not strictly depend on the processing that has taken
place to the left of i. Examples are pure bottom-up
methods, such as left-corner parsing without top-
down filtering (Wiren, 1987).
Algorithms that do satisfy the CPP make use of
some form of top-down prediction. Top-down pre-
diction can be implemented at parse-time as in the
case of Earley&apos;s algorithm by means of the &amp;quot;predic-
tor&amp;quot; step, or can be precompiled, as in the case of
left-corner parsing (Rosenkrantz and Lewis, 1970),
by means of the left-corner relation, or as in the case
of LR parsers (Sippu and Soisalon-Soininen, 1990),
through the closure function used in the construc-
tion of LR states.
</bodyText>
<sectionHeader confidence="0.953377" genericHeader="method">
4 Recognition without
precompilation
</sectionHeader>
<bodyText confidence="0.92332046875">
In this section we consider recognition algorithms
that do not require off-line compilation of the input
grammar. Among algorithms that satisfy the CPP,
the most popular example of a recognizer that does
1A context-free grammar G is reduced if every nonterminal
of G can be part of at least one derivation that rewrites the
start symbol into some string of terminal symbols.
not require grammar precompilation is perhaps Ear-
ley&apos;s algorithm (Earley, 1970). We show here that
methods in this family cannot be extended to work
in time independent of the size of the lexicon, in
contrast with bidirectional recognition algorithms.
The result presented below rests on the follow-
ing, quite obvious, assumption. There exists a con-
stant c, depending on the underlying computation
model, such that in k &gt; 0 elementary computation
steps any recognizer can only read up to c • k pro-
ductions from set P. In what follows, and without
any loss of generality, we assume c = 1. Apart from
this assumption, no other restriction is imposed on
the representation of the input grammar or on the
access to the elements of sets VN, VT and P.
Theorem 1 Let f be any function of two variables
defined on natural numbers. No recognizer for bilexi-
cal context-free grammars that satisfies the CPP can
run on input (G, w) in an amount of time bounded
by f (IVD1,1w1), where VD is the set of delexicalized
nonterminals of G.
Proof. Assume the existence of a recognizer R sat-
isfying the CPP and running in f (IVDI , 1w1) steps or
less. We show how to derive a contradiction.
Let q &gt; 1 be an integer. Define a bilexical CFG
</bodyText>
<equation confidence="0.976230666666667">
Gq = (V , Pq , Atbil) where VI contains q + 2
distinct symbols {b1, , bq+2} and
Vi&amp; = {A[b2] I 1 i q 1} U {T[b] I b E Val},
</equation>
<bodyText confidence="0.9529925">
and where set Pq contains all and only the following
productions:
</bodyText>
<listItem confidence="0.953379333333333">
(i) A[b1] A[bi+i] T[bi], 1 5_ q;
(ii) A[b9+1] —÷ T[bq+21 Tfbg-Fri;
(iii) T[b] -4 b, b E
</listItem>
<bodyText confidence="0.9460811">
Productions in (i) are called bridging productions.
Note that there are q bridging productions in Gq.
Also, note that V = IA,T1 does not depend on
the choice of q. Thus, we will simply write VD.
Choose q &gt; max{ f (IVD , 2), 1}. On input
(Gq,bq+21)q+1), R does not detect any error at posi-
tion 1, that is after having read the first symbol bq+2
of the input string. This is because A[b1] bq+2^/
with 7 = T[bq+1]T[liq] T [bq_ 1] • T[bil is a valid
derivation in G. Since R executes no more than
f (IVDI , 2) steps, from our assumption that reading
a production takes unit time it follows that there
must be an integer k, 1 &lt; k &lt; q, such that bridging
production A[bk] —+ A[bk±i] T[bk] is not read from
Gq. Construct then a new grammar Gig by replacing
in Gq the production A[bk] -4 A[bk+1] T[bk] with
the new production Ab) T[bk] A[bk+i], leaving
everything else unchanged. It follows that, on in-
put (Gi1,bq+2N+1), R behaves exactly as before and
does not detect any error at position 1. But this is
</bodyText>
<page confidence="0.989561">
274
</page>
<bodyText confidence="0.9770758125">
a contradiction, since there is no derivation in Gig of
the form A[b1] bq+27, E (VN U VT)*, as can be
easily verified. •
We can use the above result in the comparison
of left-to-right and bidirectional recognizers. The
recognition of bilexical context-free languages can
be carried out by existing bidirectional algorithms
in time independent of the size of the lexicon and
without any precompilation of the input bilexical
grammar. For instance, the algorithms presented
in (Eisner and Satta, 1999) allow recognition in time
0(1VD131w14).2 Theorem 1 states that this time
bound cannot be met if we require the CPP and if
the input grammar is not precompiled. In the next
section, we will consider the possibility that the in-
put grammar is in a precompiled form.
</bodyText>
<sectionHeader confidence="0.967183" genericHeader="method">
5 Recognition with precompilation
</sectionHeader>
<bodyText confidence="0.999654935483871">
In this section we consider recognition algorithms
that satisfy the CPP and allow off-line, polynomial-
time compilation of the working grammar. We focus
on a class of bilexical context-free grammars where
recognition requires the stacking of a number of un-
resolved lexical dependencies that is proportional to
the length of the input string. We provide evidence
that the above class of recognizers perform much less
efficiently for these grammars than existing bidirec-
tional recognizers.
We assume that the reader is familiar with the
notions of deterministic and nondeterministic finite
automata. We follow here the notation in (Hoperoft
and Tillman, 1979). A nondeterministic finite au-
tomaton (FA) is a tuple M (Q,E,8,q0,F), where
Q and E are finite, disjoint sets of state and alphabet
symbols, respectively, qo E Q and F C Q are the ini-
tial state and the set of final states, respectively, and
is a total function mapping Q x E to 2, the power-
set of Q. Function 5 represents the transitions of the
automaton. Given a string w = al • • • an, n &gt; 0, an
accepting computation in M for w is a sequence
q0, ai , a2, q2, ,a,,,q„ such that qi E
for 1 &lt; i &lt; n, and qm E F. The language L(M) is
the set of all strings in E* that admit at least one
accepting computation in M. The size of M is de-
fined as 1M1 EqEQ,a€E15(q, a)1. The automaton
M is deterministic if, for every q E Q and a E E, we
have 1(5(q, a)1 = 1.
We call quasi-determinizer any algorithm A
that satisfies the following two conditions:
</bodyText>
<listItem confidence="0.55857425">
1. A takes as input a nondeterministic FA M =
(Q, E, go, F) and produces as output a device
Dm that, when given a string w as input, de-
cides whether w E L(M); and
</listItem>
<footnote confidence="0.994283">
2More precisely, the running time for these algorithms is
0(1 VD13 iwl3 min-0 VT!, lw ID. In cases of practical interest,
we always have iwi &lt;
</footnote>
<listItem confidence="0.916531333333333">
2. there exists a polynomial PA such that every
DM runs in an amount of time bounded by
PA Owl ).
</listItem>
<bodyText confidence="0.999901962264151">
We remark that, given a nondeterministic FA M
specified as above, known algorithms allow simula-
tion of M on an input string w in time 0(IMIlwl)
(see for instance (Aho et al., 1974, Thm. 9.5)
or (Sippu and Soisalon-Soininen, 1988, Thm. 3.38)).
In contrast, a quasi-determinizer produces a device
that simulates M in an amount of time independent
of the size of M itself.
A standard example of a quasi-determinizer is the
so called power-set construction, used to convert
a nondeterministic FA into a language-equivalent
deterministic FA (see for instance (Hoperoft and
Ullman, 1979, Thm. 2.1) or (Sippu and Soisalon-
Soininen, 1988, Thm. 3.30)). In fact, there exist
constants c and c&apos; such that any deterministic FA
can be simulated on input string w in an amount of
time bounded by c Iwl + c&apos;. This requires function (5
to be stored as a )Q! x 1E1, 2-dimensional array with
values in Q. This is a standard representation for
automata-like structures; see (Gusfield, 1997, Sect.
6.5) for discussion.
We now pose the question of the time efficiency
of a quasi-determinizer, and consider the amount of
time needed in the construction of Dm. In (Meyer
and Fisher, 1971; Stearns and Hunt, 1981) it is
shown that there exist (infinitely many) nonde-
terministic FAs with state set Q, such that any
language-equivalent deterministic FA must have at
least 21Q1 states. This means that the power-set con-
struction cannot work in polynomial time in the size
of the input FA. Despite of much effort, no algo-
rithm has been found, up to the authors&apos; knowledge,
that can simulate a nondeterministic FA on an input
string w in linear time in 1w1 and independently of
1M1, if only polynomial-time precompilation of M
is allowed. Even in case we relax the linear-time re-
striction and consider recognition of w in polynomial
time, for some fixed polynomial, it seems unlikely
that the problem can be solved if only polynomial-
time precompilation of M is allowed. Furthermore,
if we consider precompilation of nondeterministic
FAs into &amp;quot;partially determinized&amp;quot; FAs that would
allow recognition in polynomial (or even exponen-
tial) time in lwl, it seems unlikely that the analysis
required for this precompilation could consider less
than exponentially many combinations of states that
may be active at the same time for the original non-
deterministic FA. Finally, although more powerful
formalisms have been shown to represent some regu-
lar languages much more succinctly than FAs (Meyer
and Fisher, 1971), while allowing polynomial-time
parsing, it seem unlikely that this could hold for reg-
ular languages in general.
</bodyText>
<page confidence="0.995476">
275
</page>
<bodyText confidence="0.965513565217391">
Conjecture There is no quasi-determinizer that
works in polynomial time in the size of the input
automaton.
Before turning to our main result, we need to
develop some additional machinery. Let M =
(Q,E,6,q0,F) be a nondeterministic FA and let
w = al •••a, E L(M), where n &gt; 0. Let
go, a1, , an, qn be an accepting computation for
w in M, and choose some symbol $ E. We can
now encode the accepting computation as
($, go )(ai, qi) • • • (an, qn)
where we pair alphabet symbols to states, prepend-
ing $ to make up for the difference in the number
of alphabet symbols and states. We now provide
a construction that associates M with a bilexical
CFG Gm. Strings in L(Gm) are obtained by pair-
ing strings in L(M) with encodings of their accepting
computations (see below for an example).
Definition 1 Let M = (Q,E,(5,q0,F) be a nonde-
terministic FA. Choose two symbols $,# E, and
let A = {(a, q) I aEEU{$},gEQ}. A bilexi-
cal CFG GM = (VN, VT) P, QS, OM is specified as
follows:
</bodyText>
<listItem confidence="0.983345416666667">
(i) VN = {T[al cr E VT} U {C[a),Cla]
(ii) Vr =AUEU {#};
(iii) P contains all and only the following produc-
tions:
(a) for each a E VT,
T[a] -4 a;
(b) for each (a,q),(at,q1) E A such that q&apos; E
6(q, a&apos;),
CRa, q)] —+ CT&amp; , q1)1 q)];
(c) for each (a,q) E A,
-4 T[a] CRa, q)1;
(d) for each (a, q) E A such that q E F,
</listItem>
<bodyText confidence="0.9659318125">
C[(a, q)] T[#1 T [(a, q)].
We give an example of the above construc-
tion. Consider an automaton M and a string
w = a1a2a3 such that W E L(M). Let
($, go)(ai , gi)(a2, q2)(a3, g3) be the encoding of an
accepting computation in M for w. Then the
string ai a2a3#(a3, g3)(a2 , q2)(ai , )($, go) belongs
to L(Gm). The tree depicted in Figure 1 represents
a derivation in Gm of such a string.
The following fact will be used below.
Lemma 1 For each w E E*, w# is a correct-prefix
for L(GM) if and only if W E L(M).
Outline of the proof. We claim the following
fact. For each k &gt; 0, a1, a2,. .,ak E E and
go, qi, , qk EQ we have
q, E(q_1,at.), for all i (1 &lt; i &lt; k),
</bodyText>
<equation confidence="0.986104090909091">
C[($ 0)]
TR$,
Cu(ai qi)]
T[ai) CRai,qi)l ($,O)
ai C&apos;[(a2, g2)]
T[a2] C[(az 42)i (al,
a2 C&apos;[(a3, g3)] T[(a2, 42)1
TEa31 C[(a3, q3)1 (a2, g2)
T(#
1 1
(a3, g3 )
</equation>
<figureCaption confidence="0.976767">
Figure 1: A derivation in Gm for string
</figureCaption>
<equation confidence="0.463887">
aia2a3#(a3 , g3)(a2, g2)(ai , )($, go).
if and only if
CRS, 40)]
al • • akC[(ak, %)1(ak—i, qk—i) • • ($, 0).
</equation>
<bodyText confidence="0.9947032">
The claim can be proved by induction on k, using
productions (a) to (c) from Definition 1.
Let R denote the reverse operator on strings.3
From the above claim and using production (d) from
Definition 1, one can easily show that
</bodyText>
<equation confidence="0.786481">
L(Gm) = {w#u j W E L(M), uR encodes an
</equation>
<bodyText confidence="0.953021266666667">
accepting computation for w}.
The lemma directly follows from this relation. •
We can now provide the main result of this sec-
tion. To this end, we refine the definition of rec-
ognizer presented in Section 3. A recognizer for the
CFG class is an algorithm R that has random access
to some data structure C(G) obtained by means of
some off-line precompilation of a CFG G. On in-
put w, which is a string on the terminal symbols of
G, R decides whether w E L(G). The definition of
the CPP extends in the obvious way to recognizers
working with precompiled grammars.
Theorem 2 Let p be any polynomial in two vari-
ables. If the conjecture about quasi-determinizers
holds true, then no recognizer exists that
</bodyText>
<footnote confidence="0.8592555">
3Note that R does not affect individual symbols in a string.
Thus (a, q)R = (a , q)
</footnote>
<page confidence="0.993156">
276
</page>
<listItem confidence="0.99158175">
(i) has random access to data structure C(G) pre-
compiled from a bilexical CFG G in polynomial
time in IG1,
(ii) runs in an amount of time bounded by
P(IVDI ,Iwp, where VD is the set of delexicalized
nonterminals of G and w is the input string,
and
(iii) satisfies the CPP.
</listItem>
<bodyText confidence="0.999378857142857">
Proof. Assume there exists a recognizer R that sat-
isfies conditions (i) to (iii) in the statement of the
theorem. We show how this entails that the conjec-
ture about quasi-determinizers is false.
We use algorithm R to specify a quasi-
determinizer A. Given a nondeterministic FA M,
A goes through the following steps.
</bodyText>
<listItem confidence="0.99868275">
1. A constructs grammar GM as in Definition 1.
2. A precompiles Gm as required by R, producing
data structure C(Gm).
3. A returns a device Dm specified as follows.
Given a string w as input, Dm runs R on string
w#. If R detects an error at any position i,
0 &lt; i &lt; lw#1, then Dm rejects and halts, oth-
erwise Dm accepts and halts.
</listItem>
<bodyText confidence="0.995900806451613">
From Lemma 1 we have that Dm accepts w if and
only if w E L(M). Since R runs in time PaVDI ,1w()
and since Gm has a set of delexicalized nonterminals
independent of M, we have that there exists a poly-
nomial PA such that every Dm works in an amount
of time bounded by pA(14. We therefore conclude
that A is a quasi-determinizer.
It remains to be shown that A works in polyno-
mial time in (MI. Step 1 can be carried out in time
0((M. The compilation at Step 2 takes polynomial
time in &apos;Gm&apos;, following our hypotheses on R, and
hence polynomial time in &apos;MI, since IGmI = OaM().
Finally, the construction of Dm at Step 3 can easily
be carried out in time 0(1M() as well. IN
In addition to Theorem 1, Theorem 2 states that,
even in case the input grammar is compiled off-
line and in polynomial time, we cannot perform
CPP recognition for bilexical context-free grammars
in time polynomial in the grammar and the input
string but independent of the lexicon size. This
is true with at least the same evidence that sup-
ports the conjecture on quasi-determinizers. Again,
this should be contrasted with the time performance
of existing bidirectional algorithms, allowing recog-
nition for bilexical context-free grammars in time
0(IVD131w(4).
In order to complete our investigation of the above
problem, in Appendix A we show that, when we drop
the polynomial-time restriction on the grammar pre-
compilation, it is indeed possible to get rid of any
IVT1 factor from the running time of the recognizer.
</bodyText>
<sectionHeader confidence="0.99129" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9999686">
Empirical results presented in the literature show
that bidirectional parsing strategies can be more
time efficient in cases of grammar formalisms whose
rules are specialized for one or more lexical items.
In this paper we have provided an original mathe-
matical argument in favour of this thesis. Our re-
sults hold for bilexical context-free grammars and
directly transfer to several language models that can
be seen as stochastic versions of this formalism (see
Section 1). We perceive that these results can be ex-
tended to other language models that properly em-
bed bilexical context-free grammars, as for instance
the more general history-based models used in (Rat-
naparkhi, 1997) and (Chelba and Jelinek, 1998). We
leave this for future work.
</bodyText>
<sectionHeader confidence="0.997987" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.998598166666667">
We would like to thank Jason Eisner and Mehryar
Mohri for fruitful discussions. The first author is
supported by the German Federal Ministry of Edu-
cation, Science, Research and Technology (BMBF)
in the framework of the VERBMOBIL Project under
Grant 01 IV 701 VO, and was employed at AT&amp;T
Shannon Laboratory during a part of the period this
paper was written. The second author is supported
by MURST under project PRIN: BioInformatica e
Ricerca Genomica and by University of Padua, un-
der project Sviluppo di Sisterni ad Addestramento
Automatic° per l&apos;Analisi del Linguaggio Naturale.
</bodyText>
<sectionHeader confidence="0.998202" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.979063043478261">
A. V. Aho, J. E. Hoperoft, and J. D. Ullman. 1974.
The Design and Analysis of Computer Algorithms.
Addison-Wesley, Reading, MA.
H. Alshawi. 1996. Head automata and bilingual
tiling: Translation with minimal representations.
In Proc. of the 34th ACL, pages 167-176, Santa
Cruz, CA.
E. Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In
Proc. of AAAI-97, Menlo Park, CA.
C. Chelba and F. Jelinek. 1998. Exploiting syntactic
structure for language modeling. In Proc. of the
36th ACL, Montreal, Canada.
M. Collins. 1997. Three generative, lexicalised mod-
els for statistical parsing. In Proc. of the 35th
ACL, Madrid, Spain.
J. Earley. 1970. An efficient context-free parsing al-
gorithm. Communications of the Association for
Computing Machinery, 13(2):94-102.
J. Eisner and G. Satta. 1999. Efficient parsing for
bilexical context-free grammars and head automa-
ton grammars. In Proc. of the 37th ACL, pages
457-464, College Park, Maryland.
</reference>
<page confidence="0.984122">
277
</page>
<reference confidence="0.992798129032258">
J. Eisner. 1996. An empirical comparison of proba-
bility models for dependency grammar. Technical
Report IRCS-96-11, IRCS, Univ. of Pennsylvania.
J. Eisner. 1997. Bilexical grammars and a cubic-
time probabilistic parser. In Proceedings of the
4th Int. Workshop on Parsing Technologies, MIT,
Cambridge, MA, September.
R. C. Gonzales and M. G. Thomason. 1978. Syntac-
tic Pattern Recognition. Addison-Wesley, Read-
ing, MA.
D. Gusfield. 1997. Algorithms on Strings, Trees and
Sequences. Cambridge University Press, Cam-
bridge, UK.
M. A. Harrison. 1978. Introduction to Formal Lan-
guage Theory. Addison-Wesley, Reading, MA.
J. E. Hoperoft and J. D. Ullman. 1979. Introduc-
tion to Automata Theory, Languages and Compu-
tation. Addison-Wesley, Reading, MA.
A. R. Meyer and M. J. Fisher. 1971. Economy of de-
scription by automata, grammars and formal sys-
tems. In 12th Annual Symp. on Switching and Au-
tomata Theory, pages 188-190, New York. IEEE.
M.-J. Nederhof and G. Satta. 1996. Efficient tabular
LR, parsing. In Proc. of the 34th ACL, pages 239-
246, Santa Cruz, CA.
M.-J. Nederhof. 1994. An optimal tabular parsing
algorithm. In Proc. of the 32nd ACL, pages 117-
124, Las Cruces, New Mexico.
A. Ratnaparkhi. 1997. A linear observed time sta-
tistical parser based on maximum entropy mod-
els. In Second Conference on Empirical Methods
in Natural Language Processing, Brown Univer-
sity, Providence, Rhode Island.
D. J. Rosenkrantz and P. M. Lewis. 1970. Determin-
istic left corner parsing. In IEEE Conf. Record
nth Annual Symposium on Switching and Au-
tomata Theory, pages 139-152.
G. Satta and 0. Stock. 1994. Bidirectional context-
free grammar parsing for natural language pro-
cessing. Artificial Intelligence, 69:123-164.
S. Sippu and E. Soisalon-Soininen. 1988. Pars-
ing Theory: Languages and Parsing, volume 1.
Springer-Verlag, Berlin, Germany.
S. Sippu and E. Soisalon-Soininen. 1990, Parsing
Theory: LR(k) and LL(k) Parsing, volume 2.
Springer-Verlag, Berlin, Germany.
R. E. Stearns and H. B. Hunt. 1981. On the equiva-
lence and containment problem for unambiguous
regular expressions, grammars, and automata. In
22nd Annual Symp. on Foundations of Computer
Science, pages 74-81, New York. IEEE.
M. Tomita. 1986. Efficient Parsing for Natural Lan-
guage. Kluwer, Boston, Mass.
G. van Noord. 1997. An efficient implementation of
the head-corner parser. Computational Linguis-
tics, 23(3):425-456.
C. S. Wetherell. 1980. Probabilistic languages: A
review and some open questions. Computing Sur-
veys, 12(4):361-379.
M. Wiren. 1987. A comparison of rule-invocation
strategies in parsing. In Proc. of the 3rd EACL,
pages 226-233, Copenhagen, Denmark.
</reference>
<bodyText confidence="0.973187875">
A Recognition in time independent
of the lexicon
In Section 5 we have shown that it is unlikely that
correct-prefix property parsing for a bilexical CFG
can be carried out in polynomial time and indepen-
dently of the lexicon size, when only polynomial-
time off-line compilation of the grammar is allowed.
To complete our presentation, we show here that
correct-prefix property parsing in time independent
of the lexicon size is indeed possible if we spend ex-
ponential time on grammar precompilation.
We first consider tabular LR parsing (Tomita,
1986), a technique which satisfies the correct-prefix
property, and apply it to bilexical CFGs. Our pre-
sentation relies on definitions from (Nederhof and
Satta, 1996). Let w E Vp be some input string. A
property of LR parsing is that any state that can be
reached after reading prefix w[1, j], j &lt; lwl, must be
of the form
goto(goto(. (goto(q,n, . .), X
where qin is the initial LR state, and Xi, , Xn, are
terminals or nonterminals such that Xi • • • Xrii
w[1, j]. For a bilexical CFG, each Xi is of the form bi
or of the form Bi [bi], where bi, , 67, is some subse-
quence of w[1, j). This means that there are at most
(2+ (VD pn distinct states that can be reached by the
recognizer, apart from qt„.. In the algorithm, the tab-
ulation prevents repeated manipulation of states for
a triple of input positions, leading to a time complex-
ity of 0(n3 !Va.&amp;quot;), where n = lwl. Hence, when we
apply precompilation of the grammar, we can carry
out recognition in time exponential in the length of
the input string, yet independent of the lexicon size.
Note however that the precompilation for LR pars-
ing takes exponential time.
The second algorithm with the CPP we will con-
sider can be derived from Earley&apos;s algorithm (Ear-
ley, 1970). For this new recognizer, we achieve a
time complexity completely independent of the size
of the whole grammar, not merely independent of
the size of the lexicon as in the case of tabular LR
parsing. Furthermore, the input grammar can be
any general CFG, not necessarily a bilexical one. In
terms of the length of the input, the complexity is
polynomial rather than exponential.
Earley&apos;s algorithm is outlined in what follows,
with minor modifications with respect to its origi-
nal presentation. An item is an object of the form
</bodyText>
<page confidence="0.98861">
278
</page>
<bodyText confidence="0.999953808510638">
[A -4 a • )3], where A -4 ai3 is a production from
the grammar. The recognition algorithm consists in
an incremental construction of a (n + 1) x (n + 1),
2-dimensional table T, where n is the length of the
input string. At each stage, each entry T[i, j] in
the table contains a set of items, which is initially
the empty set. After an initial item is added to en-
try T[O, Olin the table, other items in other entries
are derived from it, directly or indirectly, using three
steps called predictor, scanner and completer. When
no more new items can be derived, the presence of
a final item in entry T[O, n] indicates whether the
input is recognized.
The recognition process can be precompiled,
based on the observation that for any grammar the
set of all possible items is finite, and thereby all po-
tential contents of T&apos;s entries can be enumerated.
Furthermore, the dependence of entries on one an-
other is not cyclic; one item in T[i, j] may be derived
from a second item in the same entry, but it is not
possible that, for example, an item in T[i,j] is de-
rived from an item in T[ii, j1], with (i, j) (ii, j&apos;),
which is in turn derived from an item in T[i, jj.
A consequence is that entries can be computed
in a strict order, and an operation that involves the
combination of, say, the items from two entries T[i, j]
and T[j,k] by means of the completer step can be
implemented by a simple table lookup. More pre-
cisely, each set of items is represented by an atomic
state, and combining two sets of items according
to the completer step is implemented by indexing
a 2-dimensional array by the two states representing
those two sets, yielding a third state representing
the resulting set of items. Similarly, the scanner
and predictor steps and the union operation on sets
of items can all be implemented by table lookup.
The time complexity of recognition can straight-
forwardly be shown to be 0(n3), independent of
the size of the grammar. However, massive pre-
compilation is involved in enumerating all possi-
ble sets of items and precomputing the operations
on them. The motivation for discussing this algo-
rithm is therefore purely theoretical: it illustrates
the unfavourable complexity properties that The-
orem 2, together with the conjecture about quasi-
determinizers, attributes to the recognition problem
if the correct-prefix property is to be ensured.
</bodyText>
<page confidence="0.997499">
279
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.482350">
<title confidence="0.9996545">Left-To-Right Parsing and Bilexical Context-Free Grammars</title>
<author confidence="0.909841">Mark-Jan Nederhof</author>
<affiliation confidence="0.785562">DFKI</affiliation>
<address confidence="0.772012666666667">Stuhlsatzenhausweg 3 D-66123 Saarbriicken Germany</address>
<email confidence="0.997036">nederhof@dfki.de</email>
<author confidence="0.998793">Giorgio Satta</author>
<affiliation confidence="0.999801">Dipartimento di Elettronica e Informatica Universita di Padova</affiliation>
<address confidence="0.991985">via Gradenigo, 6/A 1-35131 Padova, Italy</address>
<email confidence="0.998905">satta@dei.unipd.it</email>
<abstract confidence="0.998633272727273">We compare the asymptotic time complexity of left-to-right and bidirectional parsing techniques for bilexical context-free grammars, a grammar formalism that is an abstraction of language models used in several state-of-the-art real-world parsers. We provide evidence that left-to-right parsing cannot be realised within acceptable time-bounds if the so called correct-prefix property is to be ensured. Our evidence is based on complexity results for the representation of regular languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>J E Hoperoft</author>
<author>J D Ullman</author>
</authors>
<title>The Design and Analysis of Computer Algorithms.</title>
<date>1974</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="17736" citStr="Aho et al., 1974" startWordPosition="3083" endWordPosition="3086">es the following two conditions: 1. A takes as input a nondeterministic FA M = (Q, E, go, F) and produces as output a device Dm that, when given a string w as input, decides whether w E L(M); and 2More precisely, the running time for these algorithms is 0(1 VD13 iwl3 min-0 VT!, lw ID. In cases of practical interest, we always have iwi &lt; 2. there exists a polynomial PA such that every DM runs in an amount of time bounded by PA Owl ). We remark that, given a nondeterministic FA M specified as above, known algorithms allow simulation of M on an input string w in time 0(IMIlwl) (see for instance (Aho et al., 1974, Thm. 9.5) or (Sippu and Soisalon-Soininen, 1988, Thm. 3.38)). In contrast, a quasi-determinizer produces a device that simulates M in an amount of time independent of the size of M itself. A standard example of a quasi-determinizer is the so called power-set construction, used to convert a nondeterministic FA into a language-equivalent deterministic FA (see for instance (Hoperoft and Ullman, 1979, Thm. 2.1) or (Sippu and SoisalonSoininen, 1988, Thm. 3.30)). In fact, there exist constants c and c&apos; such that any deterministic FA can be simulated on input string w in an amount of time bounded b</context>
</contexts>
<marker>Aho, Hoperoft, Ullman, 1974</marker>
<rawString>A. V. Aho, J. E. Hoperoft, and J. D. Ullman. 1974. The Design and Analysis of Computer Algorithms. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Alshawi</author>
</authors>
<title>Head automata and bilingual tiling: Translation with minimal representations.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th ACL,</booktitle>
<pages>167--176</pages>
<location>Santa Cruz, CA.</location>
<contexts>
<context position="2464" citStr="Alshawi, 1996" startWordPosition="365" endWordPosition="366">rt of the above empirically motivated thesis. We investigate a class of lexicalized grammars that, in their probabilistic versions, have been widely adopted as language models in state-of-the-art realworld parsers. The size of these grammars usually grows with the square of the size of the working lexicon, and thus can be very large. In these cases, the primary goal in the design of a parsing algorithm is to achieve asymptotic time performance sublinear in the size of the working grammar and independent of the size of the lexicon. These desiderata are met by existing bidirectional algorithms (Alshawi, 1996; Eisner, 1997; Eisner and Satta, 1999). In contrast, we show the following two main results for the asymptotic time performance of left-to-right algorithms satisfying the so called correct-prefix property. • In case off-line compilation of the working grammar is not allowed, left-to-right parsing cannot be realised within time bounds independent of the size of the lexicon. • In case polynomial-time, off-line compilation of the working grammar is allowed, left-to-right parsing cannot be realised in polynomial time, and independently of the size of the lexicon, unless a strong conjecture based </context>
<context position="4017" citStr="Alshawi, 1996" startWordPosition="598" endWordPosition="600">me bound. The second result provides evidence that well known parsing techniques as leftcorner parsing, requiring polynomial-time preprocessing of the grammar, also cannot be directly extended to process these formalisms within an acceptable time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilexical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these language models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibil272 ity of particular pairs of words in particular roles, something that was not necessarily true of general context-free grammars. The remainder of this paper is organized as follows. We introduce bilexical context-free grammars in Section 2, and discus</context>
</contexts>
<marker>Alshawi, 1996</marker>
<rawString>H. Alshawi. 1996. Head automata and bilingual tiling: Translation with minimal representations. In Proc. of the 34th ACL, pages 167-176, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proc. of AAAI-97,</booktitle>
<location>Menlo Park, CA.</location>
<contexts>
<context position="4047" citStr="Charniak, 1997" startWordPosition="603" endWordPosition="604">rovides evidence that well known parsing techniques as leftcorner parsing, requiring polynomial-time preprocessing of the grammar, also cannot be directly extended to process these formalisms within an acceptable time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilexical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these language models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibil272 ity of particular pairs of words in particular roles, something that was not necessarily true of general context-free grammars. The remainder of this paper is organized as follows. We introduce bilexical context-free grammars in Section 2, and discuss parsing with the correctpref</context>
</contexts>
<marker>Charniak, 1997</marker>
<rawString>E. Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proc. of AAAI-97, Menlo Park, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Chelba</author>
<author>F Jelinek</author>
</authors>
<title>Exploiting syntactic structure for language modeling.</title>
<date>1998</date>
<booktitle>In Proc. of the 36th ACL,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="26581" citStr="Chelba and Jelinek, 1998" startWordPosition="4687" endWordPosition="4690">es can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items. In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998). We leave this for future work. Acknowledgements We would like to thank Jason Eisner and Mehryar Mohri for fruitful discussions. The first author is supported by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 VO, and was employed at AT&amp;T Shannon Laboratory during a part of the period this paper was written. The second author is supported by MURST under project PRIN: BioInformatica e Ricerca Genomica and by University of Padua, under project Sviluppo di Sisterni ad Addestramento Automatic° per l&apos;</context>
</contexts>
<marker>Chelba, Jelinek, 1998</marker>
<rawString>C. Chelba and F. Jelinek. 1998. Exploiting syntactic structure for language modeling. In Proc. of the 36th ACL, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proc. of the 35th ACL,</booktitle>
<location>Madrid,</location>
<contexts>
<context position="4063" citStr="Collins, 1997" startWordPosition="605" endWordPosition="606"> that well known parsing techniques as leftcorner parsing, requiring polynomial-time preprocessing of the grammar, also cannot be directly extended to process these formalisms within an acceptable time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilexical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these language models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibil272 ity of particular pairs of words in particular roles, something that was not necessarily true of general context-free grammars. The remainder of this paper is organized as follows. We introduce bilexical context-free grammars in Section 2, and discuss parsing with the correctprefix property in S</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>M. Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proc. of the 35th ACL, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the Association for Computing Machinery,</journal>
<pages>13--2</pages>
<contexts>
<context position="8741" citStr="Earley, 1970" startWordPosition="1408" endWordPosition="1409">g. Although intuitive, the notion of left-to-right parsing is a concept with no precise mathematical meaning. Note that in fact, in a pathological way, one could read the input string from left-to-right, storing it into some data structure, and then perform syntactic analysis with a non-left-to-right strategy. In this paper we focus on a precise definition of left-to-right parsing, known in the literature as correct-prefix property parsing (Sippu and SoisalonSoininen, 1990). Several algorithms commonly used in natural language parsing satisfy this property, as for instance Earley&apos;s algorithm (Earley, 1970), tabular left-corner and PLR parsing (Nederhof, 1994) and tabular LR parsing (Tomita, 1986). Let VT be some alphabet. A generic string over VT is denoted as w = al • • an, with n &gt; 0 and ai E VT (1 &lt; i &lt; n); in case n = 0, w equals the empty string e. For integers i and j with 1 &lt; i &lt; j &lt; n, we write w[i, j] to denote string ajai+i • • • ai; if i &gt; j, we define w[i, j] = e. Let G = (VN, VT, P, S) be a CFG and let w al • • • an with n &gt; 0 be some string over VT. A recognizer for the CFG class is an algorithm R that, on input (G, w), decides whether w E L(G). We say that R satisfies the correct</context>
<context position="12440" citStr="Earley, 1970" startWordPosition="2101" endWordPosition="2102">se of LR parsers (Sippu and Soisalon-Soininen, 1990), through the closure function used in the construction of LR states. 4 Recognition without precompilation In this section we consider recognition algorithms that do not require off-line compilation of the input grammar. Among algorithms that satisfy the CPP, the most popular example of a recognizer that does 1A context-free grammar G is reduced if every nonterminal of G can be part of at least one derivation that rewrites the start symbol into some string of terminal symbols. not require grammar precompilation is perhaps Earley&apos;s algorithm (Earley, 1970). We show here that methods in this family cannot be extended to work in time independent of the size of the lexicon, in contrast with bidirectional recognition algorithms. The result presented below rests on the following, quite obvious, assumption. There exists a constant c, depending on the underlying computation model, such that in k &gt; 0 elementary computation steps any recognizer can only read up to c • k productions from set P. In what follows, and without any loss of generality, we assume c = 1. Apart from this assumption, no other restriction is imposed on the representation of the inp</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An efficient context-free parsing algorithm. Communications of the Association for Computing Machinery, 13(2):94-102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
<author>G Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and head automaton grammars.</title>
<date>1999</date>
<booktitle>In Proc. of the 37th ACL,</booktitle>
<pages>457--464</pages>
<location>College Park, Maryland.</location>
<contexts>
<context position="2503" citStr="Eisner and Satta, 1999" startWordPosition="369" endWordPosition="372">otivated thesis. We investigate a class of lexicalized grammars that, in their probabilistic versions, have been widely adopted as language models in state-of-the-art realworld parsers. The size of these grammars usually grows with the square of the size of the working lexicon, and thus can be very large. In these cases, the primary goal in the design of a parsing algorithm is to achieve asymptotic time performance sublinear in the size of the working grammar and independent of the size of the lexicon. These desiderata are met by existing bidirectional algorithms (Alshawi, 1996; Eisner, 1997; Eisner and Satta, 1999). In contrast, we show the following two main results for the asymptotic time performance of left-to-right algorithms satisfying the so called correct-prefix property. • In case off-line compilation of the working grammar is not allowed, left-to-right parsing cannot be realised within time bounds independent of the size of the lexicon. • In case polynomial-time, off-line compilation of the working grammar is allowed, left-to-right parsing cannot be realised in polynomial time, and independently of the size of the lexicon, unless a strong conjecture based on complexity results for the represent</context>
<context position="3860" citStr="Eisner and Satta, 1999" startWordPosition="575" endWordPosition="578">chniques that do not require grammar precompilation cannot be directly extended to process the above mentioned grammars (resp. language models) within an acceptable time bound. The second result provides evidence that well known parsing techniques as leftcorner parsing, requiring polynomial-time preprocessing of the grammar, also cannot be directly extended to process these formalisms within an acceptable time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilexical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these language models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibil272 ity of particular pairs of words in particular roles, something that was not necessarily true</context>
<context position="5214" citStr="Eisner and Satta, 1999" startWordPosition="783" endWordPosition="786">s in Section 2, and discuss parsing with the correctprefix property in Section 3. Our results for parsing with on-line and off-line grammar compilation are presented in Sections 4 and 5, respectively. To complete the presentation, Appendix A shows that leftto-right parsing in time independent of the size of the lexicon is indeed possible when an off-line compilation of the working grammar is allowed that has an exponential time complexity. 2 Bilexical context-free grammars In this section we introduce the grammar formalism we investigate in this paper. This formalism, originally presented in (Eisner and Satta, 1999), is an abstraction of the language models adopted by several state-of-the-art real-world parsers (see Section 1). We specify a non-stochastic version of the formalism, noting that probabilities may be attached to the rewrite rules exactly as in stochastic CFG (Gonzales and Thomason, 1978; Wetherell, 1980). We assume that the reader is familiar with context-free grammars. Here we follow the notation of (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = ( VN VT P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectivel</context>
<context position="7352" citStr="Eisner and Satta, 1999" startWordPosition="1188" endWordPosition="1191">e lexical head of a nonterminal is always &amp;quot;inherited&amp;quot; from some daughter symbol (i.e., from some symbol in the right-hand side of a production). In the sequel, we also refer to the set VT as the lexicon of the grammar. A bilexical CFG can encode lexically specific preferences in the form of binary relations on lexical items. For instance, one might specify P as to contain the production VP[solve] --4 V[solve] NP[puzzles] but not the production VP[eat] -4 V[eat] NP[puzzles]. This will allow derivation of some VP constituents such as &amp;quot;solve two puzzles&amp;quot;, while forbidding &amp;quot;eat two puzzles&amp;quot;. See (Eisner and Satta, 1999) for further discussion. The cost of this expressiveness is a very large grammar. Indeed, we have ClI = (.9(1171313 • IVTI2 )1 and in practical applications WTI &gt;&gt; IVD1 &gt; 1. Thus, the grammar size is dominated in its growth by the square of the size of the working lexicon. Even if we conveniently group lexical items with distributional similarities into the same category, in practical applications the resulting grammar might have several thousand productions. Parsing strategies that cannot work in sublinear time with respect to the size of the lexicon and with respect to the size of the whole </context>
<context position="15358" citStr="Eisner and Satta, 1999" startWordPosition="2641" endWordPosition="2644">lows that, on input (Gi1,bq+2N+1), R behaves exactly as before and does not detect any error at position 1. But this is 274 a contradiction, since there is no derivation in Gig of the form A[b1] bq+27, E (VN U VT)*, as can be easily verified. • We can use the above result in the comparison of left-to-right and bidirectional recognizers. The recognition of bilexical context-free languages can be carried out by existing bidirectional algorithms in time independent of the size of the lexicon and without any precompilation of the input bilexical grammar. For instance, the algorithms presented in (Eisner and Satta, 1999) allow recognition in time 0(1VD131w14).2 Theorem 1 states that this time bound cannot be met if we require the CPP and if the input grammar is not precompiled. In the next section, we will consider the possibility that the input grammar is in a precompiled form. 5 Recognition with precompilation In this section we consider recognition algorithms that satisfy the CPP and allow off-line, polynomialtime compilation of the working grammar. We focus on a class of bilexical context-free grammars where recognition requires the stacking of a number of unresolved lexical dependencies that is proportio</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>J. Eisner and G. Satta. 1999. Efficient parsing for bilexical context-free grammars and head automaton grammars. In Proc. of the 37th ACL, pages 457-464, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>An empirical comparison of probability models for dependency grammar.</title>
<date>1996</date>
<tech>Technical Report IRCS-96-11,</tech>
<institution>IRCS, Univ. of Pennsylvania.</institution>
<contexts>
<context position="4031" citStr="Eisner, 1996" startWordPosition="601" endWordPosition="602">econd result provides evidence that well known parsing techniques as leftcorner parsing, requiring polynomial-time preprocessing of the grammar, also cannot be directly extended to process these formalisms within an acceptable time bound. The grammar formalisms we investigate are based upon context-free grammars and are called bilexical context-free grammars. Bilexical context-free grammars have been presented in (Eisner and Satta, 1999) as an abstraction of language models that have been adopted in several recent real-world parsers, improving state-of-the-art parsing accuracy (Alshawi, 1996; Eisner, 1996; Charniak, 1997; Collins, 1997). Our results directly transfer to all these language models. In a bilexical context-free grammar, possible arguments of a word are always specified along with possible head words for those arguments. Therefore a bilexical grammar requires the grammar writer to make stipulations about the compatibil272 ity of particular pairs of words in particular roles, something that was not necessarily true of general context-free grammars. The remainder of this paper is organized as follows. We introduce bilexical context-free grammars in Section 2, and discuss parsing with</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. An empirical comparison of probability models for dependency grammar. Technical Report IRCS-96-11, IRCS, Univ. of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Bilexical grammars and a cubictime probabilistic parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 4th Int. Workshop on Parsing Technologies, MIT,</booktitle>
<location>Cambridge, MA,</location>
<contexts>
<context position="2478" citStr="Eisner, 1997" startWordPosition="367" endWordPosition="368"> empirically motivated thesis. We investigate a class of lexicalized grammars that, in their probabilistic versions, have been widely adopted as language models in state-of-the-art realworld parsers. The size of these grammars usually grows with the square of the size of the working lexicon, and thus can be very large. In these cases, the primary goal in the design of a parsing algorithm is to achieve asymptotic time performance sublinear in the size of the working grammar and independent of the size of the lexicon. These desiderata are met by existing bidirectional algorithms (Alshawi, 1996; Eisner, 1997; Eisner and Satta, 1999). In contrast, we show the following two main results for the asymptotic time performance of left-to-right algorithms satisfying the so called correct-prefix property. • In case off-line compilation of the working grammar is not allowed, left-to-right parsing cannot be realised within time bounds independent of the size of the lexicon. • In case polynomial-time, off-line compilation of the working grammar is allowed, left-to-right parsing cannot be realised in polynomial time, and independently of the size of the lexicon, unless a strong conjecture based on complexity </context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>J. Eisner. 1997. Bilexical grammars and a cubictime probabilistic parser. In Proceedings of the 4th Int. Workshop on Parsing Technologies, MIT, Cambridge, MA, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R C Gonzales</author>
<author>M G Thomason</author>
</authors>
<title>Syntactic Pattern Recognition.</title>
<date>1978</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="5503" citStr="Gonzales and Thomason, 1978" startWordPosition="828" endWordPosition="832">ependent of the size of the lexicon is indeed possible when an off-line compilation of the working grammar is allowed that has an exponential time complexity. 2 Bilexical context-free grammars In this section we introduce the grammar formalism we investigate in this paper. This formalism, originally presented in (Eisner and Satta, 1999), is an abstraction of the language models adopted by several state-of-the-art real-world parsers (see Section 1). We specify a non-stochastic version of the formalism, noting that probabilities may be attached to the rewrite rules exactly as in stochastic CFG (Gonzales and Thomason, 1978; Wetherell, 1980). We assume that the reader is familiar with context-free grammars. Here we follow the notation of (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = ( VN VT P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, S E VN is the start symbol, and P is a finite set of productions having the form A -4 a, where A E VN and a E (VN U VT)*. A &amp;quot;derives&amp;quot; relation, written is associated with a CFG as usual. We use the reflexive and transitive closure of written and define L(G) accordingly. The size of a C</context>
</contexts>
<marker>Gonzales, Thomason, 1978</marker>
<rawString>R. C. Gonzales and M. G. Thomason. 1978. Syntactic Pattern Recognition. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees and Sequences.</title>
<date>1997</date>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, UK.</location>
<contexts>
<context position="18526" citStr="Gusfield, 1997" startWordPosition="3216" endWordPosition="3217">e of M itself. A standard example of a quasi-determinizer is the so called power-set construction, used to convert a nondeterministic FA into a language-equivalent deterministic FA (see for instance (Hoperoft and Ullman, 1979, Thm. 2.1) or (Sippu and SoisalonSoininen, 1988, Thm. 3.30)). In fact, there exist constants c and c&apos; such that any deterministic FA can be simulated on input string w in an amount of time bounded by c Iwl + c&apos;. This requires function (5 to be stored as a )Q! x 1E1, 2-dimensional array with values in Q. This is a standard representation for automata-like structures; see (Gusfield, 1997, Sect. 6.5) for discussion. We now pose the question of the time efficiency of a quasi-determinizer, and consider the amount of time needed in the construction of Dm. In (Meyer and Fisher, 1971; Stearns and Hunt, 1981) it is shown that there exist (infinitely many) nondeterministic FAs with state set Q, such that any language-equivalent deterministic FA must have at least 21Q1 states. This means that the power-set construction cannot work in polynomial time in the size of the input FA. Despite of much effort, no algorithm has been found, up to the authors&apos; knowledge, that can simulate a nonde</context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>D. Gusfield. 1997. Algorithms on Strings, Trees and Sequences. Cambridge University Press, Cambridge, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Harrison</author>
</authors>
<title>Introduction to Formal Language Theory.</title>
<date>1978</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="5635" citStr="Harrison, 1978" startWordPosition="851" endWordPosition="852">e complexity. 2 Bilexical context-free grammars In this section we introduce the grammar formalism we investigate in this paper. This formalism, originally presented in (Eisner and Satta, 1999), is an abstraction of the language models adopted by several state-of-the-art real-world parsers (see Section 1). We specify a non-stochastic version of the formalism, noting that probabilities may be attached to the rewrite rules exactly as in stochastic CFG (Gonzales and Thomason, 1978; Wetherell, 1980). We assume that the reader is familiar with context-free grammars. Here we follow the notation of (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = ( VN VT P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, S E VN is the start symbol, and P is a finite set of productions having the form A -4 a, where A E VN and a E (VN U VT)*. A &amp;quot;derives&amp;quot; relation, written is associated with a CFG as usual. We use the reflexive and transitive closure of written and define L(G) accordingly. The size of a CFG G is defined as IG 1 = E(Aa)EP jAa. If every production in P has the form A BC or A -4 a, for A, B,C E VN , a E VT, then G is sai</context>
</contexts>
<marker>Harrison, 1978</marker>
<rawString>M. A. Harrison. 1978. Introduction to Formal Language Theory. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hoperoft</author>
<author>J D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA.</location>
<contexts>
<context position="5663" citStr="Hoperoft and Ullman, 1979" startWordPosition="853" endWordPosition="856">Bilexical context-free grammars In this section we introduce the grammar formalism we investigate in this paper. This formalism, originally presented in (Eisner and Satta, 1999), is an abstraction of the language models adopted by several state-of-the-art real-world parsers (see Section 1). We specify a non-stochastic version of the formalism, noting that probabilities may be attached to the rewrite rules exactly as in stochastic CFG (Gonzales and Thomason, 1978; Wetherell, 1980). We assume that the reader is familiar with context-free grammars. Here we follow the notation of (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = ( VN VT P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, S E VN is the start symbol, and P is a finite set of productions having the form A -4 a, where A E VN and a E (VN U VT)*. A &amp;quot;derives&amp;quot; relation, written is associated with a CFG as usual. We use the reflexive and transitive closure of written and define L(G) accordingly. The size of a CFG G is defined as IG 1 = E(Aa)EP jAa. If every production in P has the form A BC or A -4 a, for A, B,C E VN , a E VT, then G is said to be in Chomsky Normal Fo</context>
<context position="18137" citStr="Hoperoft and Ullman, 1979" startWordPosition="3144" endWordPosition="3147">ns in an amount of time bounded by PA Owl ). We remark that, given a nondeterministic FA M specified as above, known algorithms allow simulation of M on an input string w in time 0(IMIlwl) (see for instance (Aho et al., 1974, Thm. 9.5) or (Sippu and Soisalon-Soininen, 1988, Thm. 3.38)). In contrast, a quasi-determinizer produces a device that simulates M in an amount of time independent of the size of M itself. A standard example of a quasi-determinizer is the so called power-set construction, used to convert a nondeterministic FA into a language-equivalent deterministic FA (see for instance (Hoperoft and Ullman, 1979, Thm. 2.1) or (Sippu and SoisalonSoininen, 1988, Thm. 3.30)). In fact, there exist constants c and c&apos; such that any deterministic FA can be simulated on input string w in an amount of time bounded by c Iwl + c&apos;. This requires function (5 to be stored as a )Q! x 1E1, 2-dimensional array with values in Q. This is a standard representation for automata-like structures; see (Gusfield, 1997, Sect. 6.5) for discussion. We now pose the question of the time efficiency of a quasi-determinizer, and consider the amount of time needed in the construction of Dm. In (Meyer and Fisher, 1971; Stearns and Hun</context>
</contexts>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>J. E. Hoperoft and J. D. Ullman. 1979. Introduction to Automata Theory, Languages and Computation. Addison-Wesley, Reading, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A R Meyer</author>
<author>M J Fisher</author>
</authors>
<title>Economy of description by automata, grammars and formal systems.</title>
<date>1971</date>
<booktitle>In 12th Annual Symp. on Switching and Automata Theory,</booktitle>
<pages>188--190</pages>
<publisher>IEEE.</publisher>
<location>New York.</location>
<contexts>
<context position="18720" citStr="Meyer and Fisher, 1971" startWordPosition="3247" endWordPosition="3250">for instance (Hoperoft and Ullman, 1979, Thm. 2.1) or (Sippu and SoisalonSoininen, 1988, Thm. 3.30)). In fact, there exist constants c and c&apos; such that any deterministic FA can be simulated on input string w in an amount of time bounded by c Iwl + c&apos;. This requires function (5 to be stored as a )Q! x 1E1, 2-dimensional array with values in Q. This is a standard representation for automata-like structures; see (Gusfield, 1997, Sect. 6.5) for discussion. We now pose the question of the time efficiency of a quasi-determinizer, and consider the amount of time needed in the construction of Dm. In (Meyer and Fisher, 1971; Stearns and Hunt, 1981) it is shown that there exist (infinitely many) nondeterministic FAs with state set Q, such that any language-equivalent deterministic FA must have at least 21Q1 states. This means that the power-set construction cannot work in polynomial time in the size of the input FA. Despite of much effort, no algorithm has been found, up to the authors&apos; knowledge, that can simulate a nondeterministic FA on an input string w in linear time in 1w1 and independently of 1M1, if only polynomial-time precompilation of M is allowed. Even in case we relax the linear-time restriction and </context>
<context position="20035" citStr="Meyer and Fisher, 1971" startWordPosition="3460" endWordPosition="3463">hat the problem can be solved if only polynomialtime precompilation of M is allowed. Furthermore, if we consider precompilation of nondeterministic FAs into &amp;quot;partially determinized&amp;quot; FAs that would allow recognition in polynomial (or even exponential) time in lwl, it seems unlikely that the analysis required for this precompilation could consider less than exponentially many combinations of states that may be active at the same time for the original nondeterministic FA. Finally, although more powerful formalisms have been shown to represent some regular languages much more succinctly than FAs (Meyer and Fisher, 1971), while allowing polynomial-time parsing, it seem unlikely that this could hold for regular languages in general. 275 Conjecture There is no quasi-determinizer that works in polynomial time in the size of the input automaton. Before turning to our main result, we need to develop some additional machinery. Let M = (Q,E,6,q0,F) be a nondeterministic FA and let w = al •••a, E L(M), where n &gt; 0. Let go, a1, , an, qn be an accepting computation for w in M, and choose some symbol $ E. We can now encode the accepting computation as ($, go )(ai, qi) • • • (an, qn) where we pair alphabet symbols to sta</context>
</contexts>
<marker>Meyer, Fisher, 1971</marker>
<rawString>A. R. Meyer and M. J. Fisher. 1971. Economy of description by automata, grammars and formal systems. In 12th Annual Symp. on Switching and Automata Theory, pages 188-190, New York. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
<author>G Satta</author>
</authors>
<title>Efficient tabular LR, parsing.</title>
<date>1996</date>
<booktitle>In Proc. of the 34th ACL,</booktitle>
<pages>239--246</pages>
<location>Santa Cruz, CA.</location>
<marker>Nederhof, Satta, 1996</marker>
<rawString>M.-J. Nederhof and G. Satta. 1996. Efficient tabular LR, parsing. In Proc. of the 34th ACL, pages 239-246, Santa Cruz, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M-J Nederhof</author>
</authors>
<title>An optimal tabular parsing algorithm.</title>
<date>1994</date>
<booktitle>In Proc. of the 32nd ACL,</booktitle>
<pages>117--124</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="8795" citStr="Nederhof, 1994" startWordPosition="1416" endWordPosition="1417">arsing is a concept with no precise mathematical meaning. Note that in fact, in a pathological way, one could read the input string from left-to-right, storing it into some data structure, and then perform syntactic analysis with a non-left-to-right strategy. In this paper we focus on a precise definition of left-to-right parsing, known in the literature as correct-prefix property parsing (Sippu and SoisalonSoininen, 1990). Several algorithms commonly used in natural language parsing satisfy this property, as for instance Earley&apos;s algorithm (Earley, 1970), tabular left-corner and PLR parsing (Nederhof, 1994) and tabular LR parsing (Tomita, 1986). Let VT be some alphabet. A generic string over VT is denoted as w = al • • an, with n &gt; 0 and ai E VT (1 &lt; i &lt; n); in case n = 0, w equals the empty string e. For integers i and j with 1 &lt; i &lt; j &lt; n, we write w[i, j] to denote string ajai+i • • • ai; if i &gt; j, we define w[i, j] = e. Let G = (VN, VT, P, S) be a CFG and let w al • • • an with n &gt; 0 be some string over VT. A recognizer for the CFG class is an algorithm R that, on input (G, w), decides whether w E L(G). We say that R satisfies the correct-prefix property (CPP) if the following condition hold</context>
</contexts>
<marker>Nederhof, 1994</marker>
<rawString>M.-J. Nederhof. 1994. An optimal tabular parsing algorithm. In Proc. of the 32nd ACL, pages 117-124, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A linear observed time statistical parser based on maximum entropy models.</title>
<date>1997</date>
<booktitle>In Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<institution>Brown University,</institution>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="26550" citStr="Ratnaparkhi, 1997" startWordPosition="4683" endWordPosition="4685">ctional parsing strategies can be more time efficient in cases of grammar formalisms whose rules are specialized for one or more lexical items. In this paper we have provided an original mathematical argument in favour of this thesis. Our results hold for bilexical context-free grammars and directly transfer to several language models that can be seen as stochastic versions of this formalism (see Section 1). We perceive that these results can be extended to other language models that properly embed bilexical context-free grammars, as for instance the more general history-based models used in (Ratnaparkhi, 1997) and (Chelba and Jelinek, 1998). We leave this for future work. Acknowledgements We would like to thank Jason Eisner and Mehryar Mohri for fruitful discussions. The first author is supported by the German Federal Ministry of Education, Science, Research and Technology (BMBF) in the framework of the VERBMOBIL Project under Grant 01 IV 701 VO, and was employed at AT&amp;T Shannon Laboratory during a part of the period this paper was written. The second author is supported by MURST under project PRIN: BioInformatica e Ricerca Genomica and by University of Padua, under project Sviluppo di Sisterni ad </context>
</contexts>
<marker>Ratnaparkhi, 1997</marker>
<rawString>A. Ratnaparkhi. 1997. A linear observed time statistical parser based on maximum entropy models. In Second Conference on Empirical Methods in Natural Language Processing, Brown University, Providence, Rhode Island.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D J Rosenkrantz</author>
<author>P M Lewis</author>
</authors>
<title>Deterministic left corner parsing.</title>
<date>1970</date>
<booktitle>In IEEE Conf. Record nth Annual Symposium on Switching and Automata Theory,</booktitle>
<pages>139--152</pages>
<contexts>
<context position="11772" citStr="Rosenkrantz and Lewis, 1970" startWordPosition="1992" endWordPosition="1995">osition. These algorithms may also consult input symbols from left to right, but the processing that takes place to the right of some position i does not strictly depend on the processing that has taken place to the left of i. Examples are pure bottom-up methods, such as left-corner parsing without topdown filtering (Wiren, 1987). Algorithms that do satisfy the CPP make use of some form of top-down prediction. Top-down prediction can be implemented at parse-time as in the case of Earley&apos;s algorithm by means of the &amp;quot;predictor&amp;quot; step, or can be precompiled, as in the case of left-corner parsing (Rosenkrantz and Lewis, 1970), by means of the left-corner relation, or as in the case of LR parsers (Sippu and Soisalon-Soininen, 1990), through the closure function used in the construction of LR states. 4 Recognition without precompilation In this section we consider recognition algorithms that do not require off-line compilation of the input grammar. Among algorithms that satisfy the CPP, the most popular example of a recognizer that does 1A context-free grammar G is reduced if every nonterminal of G can be part of at least one derivation that rewrites the start symbol into some string of terminal symbols. not require</context>
</contexts>
<marker>Rosenkrantz, Lewis, 1970</marker>
<rawString>D. J. Rosenkrantz and P. M. Lewis. 1970. Deterministic left corner parsing. In IEEE Conf. Record nth Annual Symposium on Switching and Automata Theory, pages 139-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Satta</author>
</authors>
<title>Bidirectional contextfree grammar parsing for natural language processing.</title>
<date>1994</date>
<journal>Artificial Intelligence,</journal>
<pages>69--123</pages>
<marker>Satta, 1994</marker>
<rawString>G. Satta and 0. Stock. 1994. Bidirectional contextfree grammar parsing for natural language processing. Artificial Intelligence, 69:123-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sippu</author>
<author>E Soisalon-Soininen</author>
</authors>
<title>Parsing Theory: Languages and Parsing,</title>
<date>1988</date>
<volume>1</volume>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="17785" citStr="Sippu and Soisalon-Soininen, 1988" startWordPosition="3090" endWordPosition="3093"> 1. A takes as input a nondeterministic FA M = (Q, E, go, F) and produces as output a device Dm that, when given a string w as input, decides whether w E L(M); and 2More precisely, the running time for these algorithms is 0(1 VD13 iwl3 min-0 VT!, lw ID. In cases of practical interest, we always have iwi &lt; 2. there exists a polynomial PA such that every DM runs in an amount of time bounded by PA Owl ). We remark that, given a nondeterministic FA M specified as above, known algorithms allow simulation of M on an input string w in time 0(IMIlwl) (see for instance (Aho et al., 1974, Thm. 9.5) or (Sippu and Soisalon-Soininen, 1988, Thm. 3.38)). In contrast, a quasi-determinizer produces a device that simulates M in an amount of time independent of the size of M itself. A standard example of a quasi-determinizer is the so called power-set construction, used to convert a nondeterministic FA into a language-equivalent deterministic FA (see for instance (Hoperoft and Ullman, 1979, Thm. 2.1) or (Sippu and SoisalonSoininen, 1988, Thm. 3.30)). In fact, there exist constants c and c&apos; such that any deterministic FA can be simulated on input string w in an amount of time bounded by c Iwl + c&apos;. This requires function (5 to be sto</context>
</contexts>
<marker>Sippu, Soisalon-Soininen, 1988</marker>
<rawString>S. Sippu and E. Soisalon-Soininen. 1988. Parsing Theory: Languages and Parsing, volume 1. Springer-Verlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Sippu</author>
<author>E Soisalon-Soininen</author>
</authors>
<title>Parsing Theory: LR(k) and LL(k) Parsing,</title>
<date>1990</date>
<volume>2</volume>
<publisher>Springer-Verlag,</publisher>
<location>Berlin, Germany.</location>
<contexts>
<context position="11879" citStr="Sippu and Soisalon-Soininen, 1990" startWordPosition="2010" endWordPosition="2013">takes place to the right of some position i does not strictly depend on the processing that has taken place to the left of i. Examples are pure bottom-up methods, such as left-corner parsing without topdown filtering (Wiren, 1987). Algorithms that do satisfy the CPP make use of some form of top-down prediction. Top-down prediction can be implemented at parse-time as in the case of Earley&apos;s algorithm by means of the &amp;quot;predictor&amp;quot; step, or can be precompiled, as in the case of left-corner parsing (Rosenkrantz and Lewis, 1970), by means of the left-corner relation, or as in the case of LR parsers (Sippu and Soisalon-Soininen, 1990), through the closure function used in the construction of LR states. 4 Recognition without precompilation In this section we consider recognition algorithms that do not require off-line compilation of the input grammar. Among algorithms that satisfy the CPP, the most popular example of a recognizer that does 1A context-free grammar G is reduced if every nonterminal of G can be part of at least one derivation that rewrites the start symbol into some string of terminal symbols. not require grammar precompilation is perhaps Earley&apos;s algorithm (Earley, 1970). We show here that methods in this fam</context>
</contexts>
<marker>Sippu, Soisalon-Soininen, 1990</marker>
<rawString>S. Sippu and E. Soisalon-Soininen. 1990, Parsing Theory: LR(k) and LL(k) Parsing, volume 2. Springer-Verlag, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R E Stearns</author>
<author>H B Hunt</author>
</authors>
<title>On the equivalence and containment problem for unambiguous regular expressions, grammars, and automata.</title>
<date>1981</date>
<booktitle>In 22nd Annual Symp. on Foundations of Computer Science,</booktitle>
<pages>74--81</pages>
<publisher>IEEE.</publisher>
<location>New York.</location>
<contexts>
<context position="18745" citStr="Stearns and Hunt, 1981" startWordPosition="3251" endWordPosition="3254">nd Ullman, 1979, Thm. 2.1) or (Sippu and SoisalonSoininen, 1988, Thm. 3.30)). In fact, there exist constants c and c&apos; such that any deterministic FA can be simulated on input string w in an amount of time bounded by c Iwl + c&apos;. This requires function (5 to be stored as a )Q! x 1E1, 2-dimensional array with values in Q. This is a standard representation for automata-like structures; see (Gusfield, 1997, Sect. 6.5) for discussion. We now pose the question of the time efficiency of a quasi-determinizer, and consider the amount of time needed in the construction of Dm. In (Meyer and Fisher, 1971; Stearns and Hunt, 1981) it is shown that there exist (infinitely many) nondeterministic FAs with state set Q, such that any language-equivalent deterministic FA must have at least 21Q1 states. This means that the power-set construction cannot work in polynomial time in the size of the input FA. Despite of much effort, no algorithm has been found, up to the authors&apos; knowledge, that can simulate a nondeterministic FA on an input string w in linear time in 1w1 and independently of 1M1, if only polynomial-time precompilation of M is allowed. Even in case we relax the linear-time restriction and consider recognition of w</context>
</contexts>
<marker>Stearns, Hunt, 1981</marker>
<rawString>R. E. Stearns and H. B. Hunt. 1981. On the equivalence and containment problem for unambiguous regular expressions, grammars, and automata. In 22nd Annual Symp. on Foundations of Computer Science, pages 74-81, New York. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Efficient Parsing for Natural Language.</title>
<date>1986</date>
<publisher>Kluwer,</publisher>
<location>Boston, Mass.</location>
<contexts>
<context position="8833" citStr="Tomita, 1986" startWordPosition="1422" endWordPosition="1423">ematical meaning. Note that in fact, in a pathological way, one could read the input string from left-to-right, storing it into some data structure, and then perform syntactic analysis with a non-left-to-right strategy. In this paper we focus on a precise definition of left-to-right parsing, known in the literature as correct-prefix property parsing (Sippu and SoisalonSoininen, 1990). Several algorithms commonly used in natural language parsing satisfy this property, as for instance Earley&apos;s algorithm (Earley, 1970), tabular left-corner and PLR parsing (Nederhof, 1994) and tabular LR parsing (Tomita, 1986). Let VT be some alphabet. A generic string over VT is denoted as w = al • • an, with n &gt; 0 and ai E VT (1 &lt; i &lt; n); in case n = 0, w equals the empty string e. For integers i and j with 1 &lt; i &lt; j &lt; n, we write w[i, j] to denote string ajai+i • • • ai; if i &gt; j, we define w[i, j] = e. Let G = (VN, VT, P, S) be a CFG and let w al • • • an with n &gt; 0 be some string over VT. A recognizer for the CFG class is an algorithm R that, on input (G, w), decides whether w E L(G). We say that R satisfies the correct-prefix property (CPP) if the following condition holds. Algorithm R processes the input str</context>
</contexts>
<marker>Tomita, 1986</marker>
<rawString>M. Tomita. 1986. Efficient Parsing for Natural Language. Kluwer, Boston, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>An efficient implementation of the head-corner parser.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--3</pages>
<marker>van Noord, 1997</marker>
<rawString>G. van Noord. 1997. An efficient implementation of the head-corner parser. Computational Linguistics, 23(3):425-456.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C S Wetherell</author>
</authors>
<title>Probabilistic languages: A review and some open questions. Computing Surveys,</title>
<date>1980</date>
<pages>12--4</pages>
<contexts>
<context position="5521" citStr="Wetherell, 1980" startWordPosition="833" endWordPosition="834">exicon is indeed possible when an off-line compilation of the working grammar is allowed that has an exponential time complexity. 2 Bilexical context-free grammars In this section we introduce the grammar formalism we investigate in this paper. This formalism, originally presented in (Eisner and Satta, 1999), is an abstraction of the language models adopted by several state-of-the-art real-world parsers (see Section 1). We specify a non-stochastic version of the formalism, noting that probabilities may be attached to the rewrite rules exactly as in stochastic CFG (Gonzales and Thomason, 1978; Wetherell, 1980). We assume that the reader is familiar with context-free grammars. Here we follow the notation of (Harrison, 1978; Hoperoft and Ullman, 1979). A context-free grammar (CFG) is a tuple G = ( VN VT P, S), where VN and VT are finite, disjoint sets of nonterminal and terminal symbols, respectively, S E VN is the start symbol, and P is a finite set of productions having the form A -4 a, where A E VN and a E (VN U VT)*. A &amp;quot;derives&amp;quot; relation, written is associated with a CFG as usual. We use the reflexive and transitive closure of written and define L(G) accordingly. The size of a CFG G is defined as</context>
</contexts>
<marker>Wetherell, 1980</marker>
<rawString>C. S. Wetherell. 1980. Probabilistic languages: A review and some open questions. Computing Surveys, 12(4):361-379.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Wiren</author>
</authors>
<title>A comparison of rule-invocation strategies in parsing.</title>
<date>1987</date>
<booktitle>In Proc. of the 3rd EACL,</booktitle>
<pages>226--233</pages>
<location>Copenhagen, Denmark.</location>
<contexts>
<context position="11475" citStr="Wiren, 1987" startWordPosition="1943" endWordPosition="1944">endency between different productions. In particular, processing of the right-hand side of some production may be initiated at some input position without consultation of productions or parts of productions that may have been found to cover parts of the input to the left of that position. These algorithms may also consult input symbols from left to right, but the processing that takes place to the right of some position i does not strictly depend on the processing that has taken place to the left of i. Examples are pure bottom-up methods, such as left-corner parsing without topdown filtering (Wiren, 1987). Algorithms that do satisfy the CPP make use of some form of top-down prediction. Top-down prediction can be implemented at parse-time as in the case of Earley&apos;s algorithm by means of the &amp;quot;predictor&amp;quot; step, or can be precompiled, as in the case of left-corner parsing (Rosenkrantz and Lewis, 1970), by means of the left-corner relation, or as in the case of LR parsers (Sippu and Soisalon-Soininen, 1990), through the closure function used in the construction of LR states. 4 Recognition without precompilation In this section we consider recognition algorithms that do not require off-line compilati</context>
</contexts>
<marker>Wiren, 1987</marker>
<rawString>M. Wiren. 1987. A comparison of rule-invocation strategies in parsing. In Proc. of the 3rd EACL, pages 226-233, Copenhagen, Denmark.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>