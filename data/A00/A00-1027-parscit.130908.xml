<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000014">
<title confidence="0.994937">
Compound Noun Segmentation Based on Lexical Data
Extracted from Corpus*
</title>
<author confidence="0.759396">
Juntae Yoon
</author>
<email confidence="0.797299">
jtyoon@linc.cis.upenn.edu
</email>
<affiliation confidence="0.834881333333333">
IRCS, University of Pennsylvania,
3401 Walnut St., Suite 400A,
Philadelphia, PA 19104-6228, USA
</affiliation>
<sectionHeader confidence="0.984193" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999830647058824">
Compound noun analysis is one of the crucial prob-
lems in Korean language processing because a series
of nouns in Korean may appear without white space
in real texts, which makes it difficult to identify the
morphological constituents. This paper presents an
effective method of Korean compound noun segmen-
tation based on lexical data extracted from corpus.
The segmentation is done by two steps: First, it is
based on manually constructed built-in dictionary
for segmentation whose data were extracted from 30
million word corpus. Second, a segmentation algo-
rithm using statistical data is proposed, where sim-
ple nouns and their frequencies are also extracted
from corpus. The analysis is executed based on CYK
tabular parsing and min-max operation. By exper-
iments, its accuracy is about 97.29%, which turns
out to be very effective.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999885">
Morphological analysis is crucial for processing the
agglutinative language like Korean since words in
such languages have lots of morphological variants.
A sentence is represented by a sequence of eojeols
which are the syntactic unit- delimited by spacing
characters in Korean. Unlike in English, an eojeol
is not one word but composed of a series of words
(content words and functional words). In particu-
lar, since an eojeol can often contain more than one
noun, we cannot get proper interpretation of the sen-
tence or phrase without its accurate segmentation.
The problem in compound noun segmentation is
that it is not possible to register all compound nouns
in the dictionary since nouns are in the open set
of words as well as the number of them is very
large. Thus, they must be treated as unseen words
without a segmentation process. Furthermore, ac-
curate compound noun segmentation plays an im-
portant role in the application system. Compound
noun segmentation is necessarily required for im-
proving recall and precision in Korean information
</bodyText>
<footnote confidence="0.545999">
* This work was supported by a KOSEF&apos;s postdoctoral fel-
lowship grant.
</footnote>
<bodyText confidence="0.999969613636363">
retrieval, and obtaining better translation in ma-
chine translation. For example, suppose that a
compound noun `seol&apos;agsan-gugrib-gongwon(Seol&apos;ag
Mountain National Park)&apos; appear in documents.
A user might want to retrieve documents about
`seoPagsan(Seol&apos;ag Mountain)&apos;, and then it is likely
that the documents with seol&apos;agsan-gugrib-gongwon&apos;
are also the ones in his interest. Therefore, it
should be exactly segmented before indexing in or-
der for the documents to be retrieved with the query
`seol&apos;agsan). Also, to translate &apos; seol&apos;agsan-gugrib-
gongwon&apos; to Seol&apos;ag Mountain National Park, the
constituents should be identified first through the
process of segmentation.
This paper presents two methods for segmentation
of compound nouns. First, we extract compound
nouns from a large size of corpus, manually divide
them into simple nouns and construct the hand built
segmentation dictionary with them. The dictionary
includes compound nouns which are frequently used
and need exceptional process. The number of data
are about 100,000.
Second, the segmentation algorithm is applied if
the compound noun does not exist in the built-in
dictionary. Basically, the segmenter is based on fre-
quency of individual nouns extracted from corpus.
However, the problem is that it is difficult to dis-
tinguish proper noun and common noun since there
is no clue like capital letters in Korean. Thus, just
a large amount of lexical knowledge does not make
good results if it contains incorrect data and also it is
not appropriate to use frequencies obtained by auto-
matically tagging large corpus. Moreover, sufficient
lexical data cannot be acquired from small amounts
of tagged corpus.
In this paper, we propose a method to get sim-
ple nouns and their frequencies from frequently oc-
curring eojeols using repetitiveness of natural lan-
guage. The amount of eojeols investigated is man-
ually tractable and frequently used nouns extracted
from them are crucial for compound noun segmen-
tation. Furthermore, we propose min-max compo-
sition to divide a sequence of syllables, which would
be proven to be an effective method by experiments.
</bodyText>
<page confidence="0.963964">
14;
</page>
<bodyText confidence="0.9999848">
To briefly show the reason that we select the oper-
ation, let us consider the following example. Sup-
pose that a compound noun be composed of four
syllables &apos;81828384&apos;. There are several possibilities of
segmentation in the sequence of syllables, where we
consider the following possibilities (si /828384) and
(8182/8384). Assume that &apos;.51&apos; is a frequently ap-
pearing word in texts whereas &apos;828354&apos; is a rarely
occurring sequence of syllables as a word. On the
other hand &apos;81.52&apos; and &apos;8384&apos; occurs frequently but
although they don&apos;t occur as frequently as In
this case, the more likely segmentation would be
(8182/8384). It means that a sequence of syllables
should not be divided into frequently occurring one
and rarely occurring one. In this sense, min-max is
the appropriate operation for the selection. In other
words, min value is selected between two sequences
of syllables, and then max is taken from min values
selected. To apply the operation repetitively, we use
the CYK tabular parsing style algorithm.
</bodyText>
<sectionHeader confidence="0.955425" genericHeader="method">
2 Lexical Data Acquisition
</sectionHeader>
<bodyText confidence="0.999916444444444">
Since the compound noun consists of a series of
nouns, the probability model using transition among
parts of speech is not helpful, and rather lexical in-
formation is required for the compound noun seg-
mentation. Our segmentation algorithm is based on
a large collection of lexical information that consists
of two kinds of data: One is the hand built seg-
mentation dictionary (HBSD) and the other is the
simple noun dictionary for segmentation (SND).
</bodyText>
<subsectionHeader confidence="0.996572">
2.1 Hand-Built Segmentation Dictionary
</subsectionHeader>
<bodyText confidence="0.98732004">
The first phase of compound noun segmentation uses
the built-in dictionary (HBSD). The advantage of
using the built-in dictionary is that the segmenta-
tion could (1) be very accurate by hand-made data
and (2) become more efficient. In Korean compound
noun, one syllable noun is sometimes highly ambigu-
ous between suffix and noun, but human can easily
identify them using semantic knowledge. For ex-
ample, one syllable noun `.s.se in Korean might be
used either as a suffix or as a noun which means
`Mr/Ms&apos; or &apos;seed&apos; respectively. Without any seman-
tic information, the best way to distinguish them
is to record all the compound noun examples con-
taining the meaning of seed in the dictionary since
the number of compound nouns containing a mean-
ing of `seed&apos; is even smaller. Besides, we can treat
general spacing errors using the dictionary. By the
spacing rule for Korean, there should be one content
word except noun in an eojeol, but it turns out that
one or more content words of short length sometimes
appear without space in real texts, which causes the
lexical ambiguities. It makes the system inefficient
to deal with all these words on the phase of basic
morphological analysis.
compound nouns analysis information
</bodyText>
<equation confidence="0.970061333333333">
gajuggudu(leather shoes) gajug(leather)-Fgudu(shoes)
gajugggeun(leather string)
gaguyong(used for furniture)
sagwassi(apple seed)
podossi(graph seed) gagu(furniture)+xyong(used for)
sagwa(apple)±nssi(seed)
gajug(leather)+ggeun(string)
podo(grape)±nssi(seed)
chuggutim(football team) chuggu(football)-1-tim(team)
</equation>
<tableCaption confidence="0.9131945">
Table 1: Examples of compound noun and analysis
information in built-in dictionary
</tableCaption>
<bodyText confidence="0.999819352941177">
To construct the dictionary, compound nouns are
extracted from corpus and manually elaborated.
First, the morphological analyzer analyzes 30 mil-
lion eojeol corpus using only simple noun dictionary,
and the failed results are candidates for compound
noun. After postpositions, if any, are removed from
the compound noun candidates of the failure eoje-
ols, the candidates are modified and analyzed by
hand. In addition, a collection of compound nouns
of KAIST (Korea Advanced Institute of Science 8z
Technology) is added to the dictionary in order to
supplement them. The number of entries contained
in the built-in dictionary is about 100,000. Table 1
shows some examples in the built-in dictionary. The
italic characters such as &apos;n&apos; or &apos;x&apos; in analysis infor-
mation (right column) of the table is used to make
distinction between noun and suffix.
</bodyText>
<subsectionHeader confidence="0.999342">
2.2 Extraction of Lexical Information for
Segmentation from Corpus
</subsectionHeader>
<bodyText confidence="0.997609518518519">
As we said earlier, it is impossible for all compound
nouns to be registered in the dictionary, and thus the
built-in dictionary cannot cover all compound nouns
even though it gives more accurate results. We need
some good segmentation model for compound noun,
therefore.
In compound noun segmentation, the thing that
we pay attention to was that lexical information is
crucial for segmenting noun compounds. Since a
compound noun consists only of a sequence of nouns
i.e. {noun}+, the transition probability of parts of
speech is no use. Namely, the frequency of each noun
plays highly important role in compound noun seg-
mentation. Besides, since the parameter space is
huge, we cannot extract enough lexical information
from hundreds of thousands of POS tagged corpus&apos;
even if accurate lexical information can be extracted
from annotated corpus. Thus, a large size of cor-
pus should be used to extract proper frequencies of
nouns. However, it is difficult to look at a large size
of corpus and to assign analyses to it, which makes
it difficult to estimate the frequency distribution of
words. Therefore, we need another approach for ob-
taining frequencies of nouns.
tit is the size of POS tagged corpus currently publicized
by ETRI (Electronics and Telecommunications Research In-
stitute) project.
</bodyText>
<page confidence="0.994898">
197
</page>
<figureCaption confidence="0.999982">
Figure 1: Distribution of eojeols in Korean corpus
</figureCaption>
<bodyText confidence="0.995848711340206">
It must be noted here that each noun in compound
nouns could be easily segmented by human in many
cases because it has a prominent figure in the sense
that it is a frequently used word and so familiar with
him. In other words, nouns prominent in documents
can be defined as frequently occurred ones, which we
call distinct nouns. Compound nouns contains these
distinct nouns in many cases, which makes it easier
to segment them and to identify their constituents.
Empirically, it is well-known that too many words
in the dictionary have a bad influence on morpho-
logical analysis in Korean. It is because rarely used
nouns result in oversegmentation if they are included
in compound noun segmentation dictionary. There-
fore, it is necessary to select distinct nouns, which
leads us to use a part of corpus instead of entire
corpus that consists of frequently used ones in the
corpus.
First, we examined distribution of eojeols in cor-
pus in order to make the subset of corpus to extract
lexical frequencies of nouns. The notable thing in
our experiment is that the number of eojeols in cor-
pus is increased in proportion to the size of corpus,
but a small portion of eojeols takes most parts of the
whole corpus. For instance, 70% of the corpus con-
sists of just 60 thousand types of eojeols which take
7.5 million of frequency from 10 million eojeol corpus
and 20.5 million from 30 million eojeols. The lowest
frequency of the 60,000 eojeols is 49 in 30 million eo-
jeol corpus. We decided to take 60,000 eojeols which
are manually tractable and compose most parts of
corpus (Figure 1).
Second, we made morphological analyses for the
60,000 eojeols by hand. Since Korean is an aggluti-
native language, an eojeol is represented by a se-
quence of content words and functional words as
mentioned before. Especially, content words and
functional words often have different distribution
of syllables. In addition, inflectional endings for
predicate and postpositions for nominals also have
quite different distribution for syllables. Hence we
can distinguish the constituents of eojeols in many
cases. Of course, there are also many cases in which
the result of morphological analysis has ambigui-
ties. For example, an eojeol `na-neun&apos; in Korean
has ambiguity of `na/N+ neun/P&apos;, `na/PN+ neun/P&apos;
and `nal/V+ neun/E&apos;. In this example, the parts
of speech N, PN, P, V and E mean noun, pro-
noun, postposition, verb and ending, respectively.
On the other hand, many eojeols which are ana-
lyzed as having ambiguities by a morphological an-
alyzer are actually not ambiguous. For instance,
`ga-geora&apos; (go/imperative) has ambiguities by most
morphological analyzer among `ga/V+geora/E&apos; and
`ga/N+i/C+geora/E&apos; (C is copula), but it is actu-
ally not ambiguous. Such morphological ambiguity
is caused by overgeneration of the morphological an-
alyzer since the analyzer uses less detailed rules for
robustness of the system. Therefore, if we examine
and correct the results scrupulously, many ambigui-
ties can be removed through the process.
As the result of the manual process, only 15% of
60,000 eojeols remain ambiguous at the mid-level of
part of speech classification2. Then, we extracted
simple nouns and their frequencies from the data.
Despite of manual correction, there must be ambigu-
ities left for the reason mentioned above. There may
be some methods to distribute frequencies in case
of ambiguous words, but we simply assign the equal
distribution to them. For instance, gage has two pos-
sibilities of analysis i.e. `gage/N&apos; and `ga/V+ge/E),
and its frequency is 2263, in which the noun &apos;gage&apos; is
assigned 1132 as its frequency. Table 2 shows exam-
ples of manually corrected morphological analyses of
eojeols containing a noun &apos;gage&apos; and their frequen-
cies. We call the nouns extracted in such a way a
set of distinct nouns.
In addition, we supplement the dictionary with
other nouns not appeased in the words obtained
by the method mentioned above. First, nouns of
more than three syllables are rare in real texts in
Korean, as shown in Lee and Ahn (1996). Their
experiments proved that syllable based bigram in-
dexing model makes much better result than other
n-gram model such as trigram and quadragram in
Korean IR. It follows that two syllable nouns take
an overwhelming majority in nouns. Thus, there are
not many such nouns in the simple nouns extracted
by the manually corrected nouns (a set of distinct
nouns). In particular, since many nouns of more
2At the mid-level of part of speech classification, for ex-
ample, endings and postpositions are represented just by one
tag e.g. E and P. To identify the sentential or clausal type
(subordinate or declarative) in Korean, the ending should be
subclassified for syntactic analysis more detail which can be
done by statistical process. It is beyond the subject of this
paper.
</bodyText>
<page confidence="0.981811">
198
</page>
<table confidence="0.999903363636364">
eojeols constituents meaning frequencies
gage gage/NOga/V+ge/E store@go 2263
gage-ga gage/N+ga/P store/SUBJ 165
gage-neun gage/N+neun/P@ga/V+geneun/E store/TOP@go 113
gage-ro gage/N+ro/P to the store 166
gage-reul gage/N+reul/P store/OBJ 535
gage-e gage/N+e/P in the store 312
gage-eseo gage/N+eseo/P in the store 299
gage-yi gage/N+yi/P of the store 132
extracted noun frequency
gage store 2797
</table>
<tableCaption confidence="0.995779">
Table 2: Example of extraction of distinct nouns. Here N, V, P and E mean tag for noun, verb, postposition
</tableCaption>
<bodyText confidence="0.950614727272727">
and ending and &apos;@&apos; is marked for representation of ambiguous analysis
than three syllables are derived by a word and suf-
fixes and have some syllable features, they are useful
for distinguishing the boundaries of constituents in
compound nouns. We select nouns of more than
three syllables from morphological dictionary which
is used for basic morphological analysis and consists
of 89,000 words (noun, verb, adverb etc). Second,
simple nouns are extracted from hand-built segmen-
tation dictionary. We selected nouns which do not
exist in a set of distinct nouns.
The frequency is assigned equally with some value
f q. Since the model is based on min-max composi-
tion and the nouns extracted in the first phase are
most important, the value does not take an effect on
the system performance.
The nouns extracted in this way are referred to
as a set of supplementary nouns. And the SND for
compound noun segmentation is composed of a set
of distinct nouns and a set of supplementary nouns.
The number of simple nouns for compound noun seg-
mentation is about 50,000.
</bodyText>
<sectionHeader confidence="0.999456" genericHeader="method">
3 Compound Word Segmentation
Algorithm
</sectionHeader>
<subsectionHeader confidence="0.99969">
3.1 Basic Idea
</subsectionHeader>
<bodyText confidence="0.99973216">
To simply describe the basic idea of our compound
noun segmentation, we first consider a compound
noun to be segmented into only two nouns. Given a
compound noun, it is segmented by the possibility
that a sequence of syllables inside it forms a word.
The possibility that a sequence of syllables forms a
word is measured by the following formula.
In the formula, fq(si,... sj) is the frequency of
the syllable s,...sj, which is obtained from SND
constructed on the stages of lexical data extraction.
And, f qN is the total sum of frequencies of simple
nouns. Colloquially, the equation (1) estimates how
much the given sequence of syllables are likely to be
word. If a sequence of syllables in the set of distinct
nouns is included in a compound noun, it is more
probable that it is divided around the syllables. If
a compound noun consists of, for any combination
of syllables, sequences of syllables in the set of sup-
plementary nouns, the boundary of segmentation is
somewhat fuzzy. Besides, if a given sequence of syl-
lables is not found in SND, it is not probable that it
is a noun.
Consider a compound noun &apos; hag-gyo-saeng-
hwal(school life)&apos;. In case that segmentation of
syllables is made into two, there would be four
possibilities of segmentation for the example as
follows:
hag gyo-saeng-hwal
hag-gyo saeng-hwal
hag-gyo-saeng hwal
hag-gyo-saeng-hwal
As we mentioned earlier, it is desirable that the eo-
jeol is segmented in the position where each sequence
of syllables to be divided occurs frequently enough
in training data. As the length of a sequence of sylla-
bles is shorter in Korean, it occurs more frequently.
That is, the shorter part usually have higher fre-
quency than the other (longer) part when we divide
syllables into two. Moreover, if the other part is
the syllables that we rarely see in texts, then the
part would not be a word. In the first of the above
example, hag is a sequence of syllable appearing fre-
quently, but gyo-saeng-hwal is not. Actually, gyo-
saeng-hwal is not a word. On the other hand, both
hag-gyo and saeng-hwal are frequently occurring syl-
lables, and actually they are all words. Put another
way, if it is unlikely that one sequence of syllables is
a word, then it is more likely that the entire syllables
are not segmented. The min-max composition is a
suitable operation for this case. Therefore, we first
</bodyText>
<equation confidence="0.92808325">
f q(si, s j)
W ord(si, s j) =
fqN
(1)
</equation>
<page confidence="0.989209">
199
</page>
<bodyText confidence="0.9998675">
take the minimum value from the function Word for
each possibility of segmentation, and then we choose
the maximum from the selected minimums. Also,
the argument taking the maximum is selected as the
most likely segmentation result.
Here, Ward(s,...si) is assigned the frequency of
the syllables s,...si from the dictionary SND. Be-
sides, if two minimums are equal, the entire sylla-
ble such as hag-gyo-saeng-hwal, if compared, is pre-
ferred, the values of the other sequence of syllables
are compared or the dominant pattern has the pri-
ority.
</bodyText>
<subsectionHeader confidence="0.999782">
3.2 Segmentation Algorithm
</subsectionHeader>
<bodyText confidence="0.998016257142857">
In this section, we generalize the word segmentation
algorithm based on data obtained by the training
method described in the previous section. The basic
idea is to apply min-max operation to each sylla-
ble in a compound noun by the bottom-up strat-
egy. That is, if the minimum between Words of
two sequences of syllables is greater than Word of
the combination of them, the syllables should be
segmented. For instance, let us suppose a com-
pound noun consist of two syllable si and s2. If
min(Word(si), Word(s2)) &gt; Word(sis2), then the
compound noun is segmented into s1 and s2. It is
not segmented, otherwise. That is, we take the max-
imum among minimums. For example, &apos;hag&apos; is a fre-
quently occurring word, but `gyo&apos; is not in Korean.
In this case, we can hardly regard the sequence of
syllable &apos;hag-gyo&apos; as the combination of two words
&apos;hag&apos; and `gyo&apos;. The algorithm can be applied recur-
sively from individual syllable to the entire syllable
of the compound noun.
The segmentation algorithm is effectively imple-
mented by borrowing the CYK parsing method.
Since we use the bottom-up strategy, the execu-
tion looks like composition rather than segmenta-
tion. After all possible segmentation of syllables be-
ing checked, the final result is put in the top of the
table. When a compound noun is composed of n
syllables, i.e. s1s2 Sn, the composition is started
from each si (i = 1 n). Thus, the possibility that
the individual syllable forms a word is recorded in
the cell of the first row.
Here, Ci,j is an element of CYK ta-
ble where the segment result of the sylla-
bles s3,...,j+i-1 is stored (Figure 2). For
instance, the segmentation result such that
</bodyText>
<equation confidence="0.473065">
argmax(min(Word(si), Word(s2)), Word(sis2))
</equation>
<bodyText confidence="0.9448925">
is stored in C1,2- What is interesting here is
that the procedure follows the dynamic pro-
gramming. Thus, each cell C,j has the most
probable segmentation result for a series of syl-
lables Namely, C1,2 and C2,3 have
the most likely segmentation of s1s2 and .52.53
respectively. When the segmentation of Si 8283 is
about to be checked, min(volue(C2,1), va/ue(C1,3)),
</bodyText>
<figureCaption confidence="0.999529">
Figure 2: Composition Table
</figureCaption>
<bodyText confidence="0.961300151515152">
min(vatue(CLI), va/ue(C2,2)) and Word(sis2s3)
are compared to determine the segmentation for
the syllables, because all Ci,j have the most likely
segmentation. Here, value(C,,i) represents the
possibility value of Ci,j.
Then, we can describe the segmentation algorithm
as follows:
When it is about to make the segmentation of syl-
lables si sj, the segmentation results of less length
of syllables like si , si and so forth
would be already stored in the table. In order to
make analysis of si sj, we combine two shorter
length of analyses and the word generation possibil-
ities are computed and checked.
To make it easy to explain the algorithm, let us
take an example compound noun &apos;hag-gyo-saeng-
hwar (school life) which is segmented with chaggyo&apos;
(school) and `saenghwar (life) (Figure 3). When it
comes up to cell C4,1, we have to make the most
probable segmentation for `hag-gyo-saeng-hwar i.e.
Si s2s3s4. There are three kinds of sequences of syl-
lables, i.e. s1 in C1,1, s152 in C2,1 and sis2.53 in C3,1
that can construct the word consisting of 8182 S3 S4
which would be put in C4,1. For instance, the word
81828384 (hag-gyo-saeng-hwal) is made with Si (hag)
combined with s25384 (gyo-saeng-hwal). Likewise,
it might be made by 8182 combined with 8354 and
.502.93 combined with s4. Since each cell has the
most probable result and its value, it is simple to
find the best segmentation for each syllables. In
addition, four cases, including the whole sequences
of syllables, are compared to make segmentation of
81828384 as follows:
</bodyText>
<listItem confidence="0.77815">
1. min(vcdue(C3,1), va/ue(C3,4))
2. min(value(C2,1), va/ue(C2,3))
3. min(va/ue(Ci,i), va/ue(C3,2))
4. Word(sis2s3s4) = Word(hag-gyo-saeng-hwal)
</listItem>
<bodyText confidence="0.999786666666667">
Again, the most probable segmentation result is
put in C4,1 with the likelihood value for its segmen-
tation. We call it MLS (Most Likely Segmentation)
</bodyText>
<figure confidence="0.992954863636364">
n-1 ii
1
composition result
for
n-1
1
200
hag
hwal
gyo
gyo-saeng
hag-gyo
saeng
hag-gyo-seeng gyo-seeng-hwal
....._...._________.......„..-,....-.-&apos;&apos;)
ag-gyo-saeng-hwal arg max(min(w(hag-gyo-imeng),w(bwitM.
mintwaiag-hyoMv(caeng-hwal)).
mintw(hag),w(gymmengdcwal)).
Word(hag•gyo-siteng-hwal))
heom
-----7-----..Hequenee of syllables in disainet
default immolation pointer
</figure>
<figureCaption confidence="0.942599">
Figure 5: Default segmentation pointer for (geon-
chug-sa-si-heone where (si-heom&apos; is a very frequently
used noun.
</figureCaption>
<figure confidence="0.9895485">
geon chug
arg max(min(wMag Lw(gyo)l,w(hag-gyo))
</figure>
<figureCaption confidence="0.678428333333333">
Figure 3: State of table when analyzing &apos;hag-gyo-
saeng-hwal&apos;. Here, w(si si) = value(Ci,j)
which is found in the following way:
</figureCaption>
<equation confidence="0.9996774">
MLS(C4,1) =
arg max(min(va/ue(C3,1), va/ue(C3,4)),
min(vaine(C2,1), vcdue(C2,3)),
min(vcdue(C1,1), va/ue(C3,2)),
Word(sis2 S3 S4))
</equation>
<bodyText confidence="0.999902580645162">
From the four cases, the maximum value and the
segmentation result are selected and recorded in
C4,1. To generalize it, the algorithm is described
as shown in Figure 4.
The algorithm is straightforward. Let Word and
MLS be the likelihood of being a noun and the most
likely segmentation for a sequence of syllables. In the
initialization step, each cell of the table is assigned
Word value for a sequence of syllables si sj+i+i
using its frequency if it is found in SND. In other
words, if the value of Word for the sequence in each
cell is greater than zero, the syllables might be as a
noun a part of a compound noun and so the value is
recorded as MLS. It could be substituted by more
likely one in the segmentation process.
In order to make it efficient, the segmentation re-
sult is put as MLS instead of the syllables in case
the sequence of syllables exists in the HBND. The
minimum of each Word for constituents of the result
as Word is recorded.
Then, the segmenter compares possible analyses
to make a larger one as shown in Figure 4. When-
ever Word of the entire syllables is less than that of
segmented one, the syllables and value are replaced
with the segmented result and its value. For in-
stance, s1 + s2 and its likelihood substitutes C2,1
if min(Word(si ), Word(s2)) &gt; Word(sis2). When
the entire syllables from the first to nth syllable are
processed, Cno. has the segmentation result.
The overall complexity of the algorithm follows
that of CYK parsing, 0(n3).
</bodyText>
<subsectionHeader confidence="0.999896">
3.3 Default Analysis and Tuning
</subsectionHeader>
<bodyText confidence="0.75968278">
For the final result, we should take into consideration
several issues which are related with the syllables
that left unsegmented. There are several reasons
that the given string remains unsegmented:
1. The first one is a case where the string consists
of several nouns but one of them is a unreg-
istered word. A compound noun `geon-chug-
sa-si-heom&apos; is composed of (geon-chug-s&amp; and
`si-heorn&apos;, which have the meanings of autho-
rized architect and examination. In this case,
the unknown noun is caused by the suffix such
as &apos;Nil because the suffix derives many words.
However, it is known that it is very difficult to
treat the kinds of suffixes since the suffix like
(sa&apos; is a very frequently used character in Ko-
rean and thus prone to make oversegmentation
if included in basic morphological analysis.
2. The string might consist of a proper noun ad a
noun representing a position or geometric infor-
mation. For instance, a compound noun (kirn-
dae-jung-dae-tong-ryeong&apos; is composed of lim-
dae-jung&apos; and `dae-tong-ryeong&apos; where the for-
mer is personal name and the latter means pres-
ident respectively.
3. The string might be a proper noun itself. For
example, `tviill&apos;ainseu&apos; is a transliterated word
for foreign name &apos;Williams&apos; and &apos;hong-gil-dong&apos;
is a personal name in Korean. Generally, since
it has a different sequence of syllables from in
a general Korean word, it often remains unseg-
mented.
If the basic segmentation is failed, three proce-
dures would be executed for solving three problems
above. For the first issue, we use the set of distinct
nouns. That is, the offset pointer is stored in the ini-
tialization step as well as frequency of each noun in
compound noun is recorded in the table. Attention
should be paid to non-frequent sequence of syllables
(ones in the set of supplementary nouns) in the de-
fault segmentation because it could be found in any
proper noun such as personal names, place names,
etc or transliterated words. It is known that the per-
formance drops if all nouns in the compound noun
segmentation dictionary are considered for default
segmentation. We save the pointer to the boundary
only when a noun in distinct set appears. For the
above example (geon-chug-sa-si-heone, the default
segmentation would be `geon-chug-sa&apos; and (.5i-hem&amp;
since &apos;si-heom&apos; is in the set of distinct nouns and the
pointer is set before (si-heorn&apos; (Figure 5).
</bodyText>
<page confidence="0.993089">
201
</page>
<bodyText confidence="0.915060666666667">
/* initialization step */
for i=1 to n do
for j=1 to n-i+1 do
</bodyText>
<equation confidence="0.859589153846154">
value(Ci,j) = Word(si
MLS(Ci,j)= si ...sj+i-1; if value(Ci,j) &gt; 0
4); otherwise
for i=2 to n do
for j=1 to i do
value(Ci,j) = max(min(vadue(Ci_i,j), vatue(Ci,j+i-1)),
min(va/ue(Ci-2,j), vattze(C2,i-2)),
min(vcdue(Ci ,i), vatue(Ci—i,j+1)),
Word(si ...si+j))
MLS(Ci,j)= arg max(min(vcaue(Ci—i,j), va/ue(Ci,j+i—i)),
min(uc1ue(Ci-2,i),va1ue(C2,j-2)),
min(valt4e(Ci,j),,vattie(Ci—i,j+i)),
Word(si
</equation>
<figureCaption confidence="0.999774">
Figure 4: The segmentation algorithm
</figureCaption>
<bodyText confidence="0.999982666666667">
If this procedure is failed, the sequence of syllables
is checked whether it might be proper noun or not.
Since proper noun in Korean could have a kind of
nominal suffix such as `daetongryeong(president)&apos; or
`ssi(Mr/Ms)&apos; as mentioned above, we can identify
it by detaching the nominal suffixes. If there does
not exist any nominal suffix, then the entire syllables
would be regarded just as the transliterated foreign
word or a proper noun like personal or place name.
</bodyText>
<sectionHeader confidence="0.999159" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.999953444444444">
For the test of compound noun segmentation, we
first extracted compound noun from ETRI POS
tagged corpus3. By the processing, 1774 types of
compound nouns were extracted, which was used as
a gold standard test set.
We evaluated our system by two methods: (1)
the precision and recall rate, and (2) segmentation
accuracy per compound noun which we refer to as
SA. They are defined respectively as follows:
</bodyText>
<figure confidence="0.324050285714286">
Precision =
number of correct constituents in proposed segment results
total number of constituents in proposed segment results
Recall =
number of correct constituents in proposed segment results
total number of constituents in compoundnouns
SA =
</figure>
<footnote confidence="0.7704475">
number of correctly segmented compound nouns
total number of cornpoundnouns
3The corpus was constructed by the ETRI (Electronics
and Telecommunications Research Institute) project for stan-
dardization of natural language processing technology and the
corpus presented consists of about 270,000 eojeols at present.
</footnote>
<bodyText confidence="0.999484514285714">
What influences on the Korean IR system is
whether words are appropriately segmented or not.
The precision and recall estimate how appropriate
the segmentation results are. They are 98.04% and
97.80% respectively, which shows that our algorithm
is very effective (Table 3).
SA reflects how accurate the segmentation is for a
compound noun at all. We compared two methods:
(1) using only the segmentation algorithm with de-
fault analysis which is a baseline of our system and
so is needed to estimate the accuracy of the algo-
rithm. (2) using both the built-in dictionary and the
segmentation algorithm which reflects system accu-
racy as a whole. As shown in Table 4, the baseline
performance using only distinct nouns and the al-
gorithm is about 94.3% and fairly good. From the
results, we can find that the distinct nouns has great
impact on compound noun segmentation. Also, the
overall segmentation accuracy for the gold standard
is about 97.29% which is a very good result for the
application system. In addition, it shows that the
built-in dictionary supplements the algorithm which
results in better segmentation.
Lastly, we compare our system with the previous
work by (Yun et al. , 1997). It is impossible that we
directly compare our result with theirs, since the test
set is different. It was reported that the accuracy
given in the paper is about 95.6%. When comparing
the performance only in terms of the accuracy, our
system outperforms theirs.
Embeded in the morphological analyzer, the com-
pound noun segmentater is currently being used for
some projects on MT and IE which are worked in
several institutes and it turns out that the system is
very effective.
</bodyText>
<page confidence="0.992414">
202
</page>
<table confidence="0.9805">
Precision Recall
3553/3637
97.80
Number of correct constituents 3553/3628
Rate 98.04
</table>
<tableCaption confidence="0.9999965">
Table 3: Result 1: Precision and recall rate
Table 4: Result 2: Segmentation accuracy for Compound Noun
</tableCaption>
<figure confidence="0.997527">
SA
Whole System
1726/1774
97.29
Baseline
1673/1774
94.30
Number of correct constituents
Rate
</figure>
<sectionHeader confidence="0.990226" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999972592592593">
In this paper, we presented the new method for
Korean compound noun segmentation. First, we
proposed the lexical acquisition for compound noun
analysis, which consists of the manually constructed
segmentation dictionary (HBSD) and the dictionary
for applying the segmentation algorithm (SND). The
hand-built segmentation dictionary was made manu-
ally for compound nouns extracted from corpus. The
simple noun dictionary is based on very frequently
occurring nouns which are called distinct nouns be-
cause they are clues for identifying constituents of
compound nouns. Second, the compound noun was
segmented based on the modification of CYK tab-
ular parsing and min-max composition, which was
proven to be the very effective method by exper-
iments. The bottom up approach using min-max
operation guarantees the most likely segmentation,
being applied in the same way as dynamic program-
ming.
With our new method, the result for segmenta-
tion is as accurate as 97.29%. Especially, the al-
gorithm made results good enough and the built-
in dictionary supplemented the algorithm. Conse-
quently, the methodology is promising and the seg-
mentation system would be helpful for the applica-
tion system such as machine translation and infor-
mation retrieval.
</bodyText>
<sectionHeader confidence="0.999141" genericHeader="acknowledgments">
6 Acknowledgement
</sectionHeader>
<bodyText confidence="0.997537666666667">
We thank Prof. Mansuk Song at Yonsei Univ. and
Prof. Key-Sun Choi at KAIST to provide data for
experiments.
</bodyText>
<sectionHeader confidence="0.999593" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977102">
Cha, J., Lee, G. and Lee, J. 1998. Generalized Un-
known Morpheme Guessing for Hybrid POS Tag-
ging of Korean. In Proceedings of the 6th Work-
shop on Very Large Corpora.
Choi, K. S., Han, Y. S., Han, Y. G., and Kwon, 0.
W. 1994. KAIST Tree Bank Project for Korean:
Present and Future Development. In Proceedings
of the International Workshop on Sharable Natu-
ral Language Resources.
Elmi, M. A. and Evens, M. 1998. Spelling Cor-
rection Using Context. In Proceedings of COL-
ING/ACL 98
Hoperoft, J. E. and Tillman, J. D. 1979. Introduc-
tion to Automata Theory, Languages, and Com-
putation.
Jin, W. and Chen, L. 1995. Identifying Unknown
Words in Chinese Corpora In Proceedings of NL-
PRS 95
Lee, J. H. and Ahn, J. S. 1996. Using n-grams
for Korean Text Retrieval. In Proceedings of 19th
Annual International ACM SIGIR Conference on
Research and Development in Information Re-
trieval
Li, J. and Wang, K. 1995. Study and Implementa-
tion of Nondictionary Chinese Segmentation. In
Proceedings of NLPRS 95
Nagao, M. and Mori, S. 1994. A New Method of
N-gram Statistics for Large Number of N and Au-
tomatic Extraction of Words and Phrases from
Large Text Data of Japanese. In Proceedings of
COLING
Park, B, R., Hwang, Y. S. and Rim, H. C. 1997.
Recognizing Korean Unknown Words by Compar-
atively Analyzing Example Words. In Proceedings
of ICCPOL 97
Sproat, R. W., Shih, W., Gale, W. and Chang,
N. 1994. A Stochastic Finite-State Word-
segmentation Algorithm for Chinese. In Proceed-
ings of the 32nd Annual Meeting of ACL
Yoon, J., Kang, B. and Choi, K. S. 1999. Informa-
tion Retrieval Based on Compound Noun Analysis
for Exact Term Extraction. Submitted in Journal
of Computer Processing of Orientla Language.
Yoon, J., Lee, W. and Choi, K. S. 1999. Word Seg-
mentation Based on Estimation of Words from
Examples. Technical Report.
Yun, B. H., Cho, M. C. and Rim, H. C. 1997. Seg-
menting Korean Compound Nouns Using Statis-
tical Information and a Preference Rules. In Pro-
ceedings of PA CLING.
</reference>
<page confidence="0.999205">
203
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.633234">
<title confidence="0.8221515">Compound Noun Segmentation Based on Lexical Data Extracted from Corpus*</title>
<author confidence="0.953648">Juntae Yoon</author>
<email confidence="0.998229">jtyoon@linc.cis.upenn.edu</email>
<affiliation confidence="0.999942">IRCS, University of Pennsylvania,</affiliation>
<address confidence="0.9984005">3401 Walnut St., Suite 400A, Philadelphia, PA 19104-6228, USA</address>
<abstract confidence="0.998867666666667">Compound noun analysis is one of the crucial problems in Korean language processing because a series of nouns in Korean may appear without white space in real texts, which makes it difficult to identify the morphological constituents. This paper presents an effective method of Korean compound noun segmentation based on lexical data extracted from corpus. The segmentation is done by two steps: First, it is based on manually constructed built-in dictionary for segmentation whose data were extracted from 30 million word corpus. Second, a segmentation algorithm using statistical data is proposed, where simple nouns and their frequencies are also extracted from corpus. The analysis is executed based on CYK tabular parsing and min-max operation. By experiments, its accuracy is about 97.29%, which turns out to be very effective.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Cha</author>
<author>G Lee</author>
<author>J Lee</author>
</authors>
<title>Generalized Unknown Morpheme Guessing for Hybrid POS Tagging of Korean.</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th Workshop on Very Large Corpora.</booktitle>
<marker>Cha, Lee, Lee, 1998</marker>
<rawString>Cha, J., Lee, G. and Lee, J. 1998. Generalized Unknown Morpheme Guessing for Hybrid POS Tagging of Korean. In Proceedings of the 6th Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K S Choi</author>
<author>Y S Han</author>
<author>Y G Han</author>
<author>Kwon</author>
</authors>
<title>KAIST Tree Bank Project for Korean: Present and Future Development.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Workshop on Sharable Natural Language Resources.</booktitle>
<marker>Choi, Han, Han, Kwon, 1994</marker>
<rawString>Choi, K. S., Han, Y. S., Han, Y. G., and Kwon, 0. W. 1994. KAIST Tree Bank Project for Korean: Present and Future Development. In Proceedings of the International Workshop on Sharable Natural Language Resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A Elmi</author>
<author>M Evens</author>
</authors>
<title>Spelling Correction Using Context.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL 98</booktitle>
<marker>Elmi, Evens, 1998</marker>
<rawString>Elmi, M. A. and Evens, M. 1998. Spelling Correction Using Context. In Proceedings of COLING/ACL 98</rawString>
</citation>
<citation valid="true">
<authors>
<author>J E Hoperoft</author>
<author>J D Tillman</author>
</authors>
<title>Introduction to Automata Theory, Languages, and Computation.</title>
<date>1979</date>
<marker>Hoperoft, Tillman, 1979</marker>
<rawString>Hoperoft, J. E. and Tillman, J. D. 1979. Introduction to Automata Theory, Languages, and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Jin</author>
<author>L Chen</author>
</authors>
<title>Identifying Unknown Words in Chinese Corpora</title>
<date>1995</date>
<booktitle>In Proceedings of NLPRS 95</booktitle>
<marker>Jin, Chen, 1995</marker>
<rawString>Jin, W. and Chen, L. 1995. Identifying Unknown Words in Chinese Corpora In Proceedings of NLPRS 95</rawString>
</citation>
<citation valid="true">
<authors>
<author>J H Lee</author>
<author>J S Ahn</author>
</authors>
<title>Using n-grams for Korean Text Retrieval.</title>
<date>1996</date>
<booktitle>In Proceedings of 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</booktitle>
<contexts>
<context position="13739" citStr="Lee and Ahn (1996)" startWordPosition="2182" endWordPosition="2185">distribution to them. For instance, gage has two possibilities of analysis i.e. `gage/N&apos; and `ga/V+ge/E), and its frequency is 2263, in which the noun &apos;gage&apos; is assigned 1132 as its frequency. Table 2 shows examples of manually corrected morphological analyses of eojeols containing a noun &apos;gage&apos; and their frequencies. We call the nouns extracted in such a way a set of distinct nouns. In addition, we supplement the dictionary with other nouns not appeased in the words obtained by the method mentioned above. First, nouns of more than three syllables are rare in real texts in Korean, as shown in Lee and Ahn (1996). Their experiments proved that syllable based bigram indexing model makes much better result than other n-gram model such as trigram and quadragram in Korean IR. It follows that two syllable nouns take an overwhelming majority in nouns. Thus, there are not many such nouns in the simple nouns extracted by the manually corrected nouns (a set of distinct nouns). In particular, since many nouns of more 2At the mid-level of part of speech classification, for example, endings and postpositions are represented just by one tag e.g. E and P. To identify the sentential or clausal type (subordinate or d</context>
</contexts>
<marker>Lee, Ahn, 1996</marker>
<rawString>Lee, J. H. and Ahn, J. S. 1996. Using n-grams for Korean Text Retrieval. In Proceedings of 19th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Li</author>
<author>K Wang</author>
</authors>
<title>Study and Implementation of Nondictionary Chinese Segmentation.</title>
<date>1995</date>
<booktitle>In Proceedings of NLPRS 95</booktitle>
<marker>Li, Wang, 1995</marker>
<rawString>Li, J. and Wang, K. 1995. Study and Implementation of Nondictionary Chinese Segmentation. In Proceedings of NLPRS 95</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Nagao</author>
<author>S Mori</author>
</authors>
<title>A New Method of N-gram Statistics for Large Number of N and Automatic Extraction of Words and Phrases from Large Text Data of Japanese.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING</booktitle>
<marker>Nagao, Mori, 1994</marker>
<rawString>Nagao, M. and Mori, S. 1994. A New Method of N-gram Statistics for Large Number of N and Automatic Extraction of Words and Phrases from Large Text Data of Japanese. In Proceedings of COLING</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Park</author>
<author>R Hwang</author>
<author>Y S</author>
<author>H C Rim</author>
</authors>
<title>Recognizing Korean Unknown Words by Comparatively Analyzing Example Words.</title>
<date>1997</date>
<booktitle>In Proceedings of ICCPOL 97</booktitle>
<marker>Park, Hwang, S, Rim, 1997</marker>
<rawString>Park, B, R., Hwang, Y. S. and Rim, H. C. 1997. Recognizing Korean Unknown Words by Comparatively Analyzing Example Words. In Proceedings of ICCPOL 97</rawString>
</citation>
<citation valid="true">
<authors>
<author>R W Sproat</author>
<author>W Shih</author>
<author>W Gale</author>
<author>N Chang</author>
</authors>
<title>A Stochastic Finite-State Wordsegmentation Algorithm for Chinese.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of ACL</booktitle>
<marker>Sproat, Shih, Gale, Chang, 1994</marker>
<rawString>Sproat, R. W., Shih, W., Gale, W. and Chang, N. 1994. A Stochastic Finite-State Wordsegmentation Algorithm for Chinese. In Proceedings of the 32nd Annual Meeting of ACL</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yoon</author>
<author>B Kang</author>
<author>K S Choi</author>
</authors>
<title>Information Retrieval Based on Compound Noun Analysis for Exact Term Extraction. Submitted in</title>
<date>1999</date>
<journal>Journal of Computer Processing of Orientla Language.</journal>
<marker>Yoon, Kang, Choi, 1999</marker>
<rawString>Yoon, J., Kang, B. and Choi, K. S. 1999. Information Retrieval Based on Compound Noun Analysis for Exact Term Extraction. Submitted in Journal of Computer Processing of Orientla Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Yoon</author>
<author>W Lee</author>
<author>K S Choi</author>
</authors>
<title>Word Segmentation Based on Estimation of Words from Examples.</title>
<date>1999</date>
<tech>Technical Report.</tech>
<marker>Yoon, Lee, Choi, 1999</marker>
<rawString>Yoon, J., Lee, W. and Choi, K. S. 1999. Word Segmentation Based on Estimation of Words from Examples. Technical Report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B H Yun</author>
<author>M C Cho</author>
<author>H C Rim</author>
</authors>
<title>Segmenting Korean Compound Nouns Using Statistical Information and a Preference Rules.</title>
<date>1997</date>
<booktitle>In Proceedings of PA CLING.</booktitle>
<marker>Yun, Cho, Rim, 1997</marker>
<rawString>Yun, B. H., Cho, M. C. and Rim, H. C. 1997. Segmenting Korean Compound Nouns Using Statistical Information and a Preference Rules. In Proceedings of PA CLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>