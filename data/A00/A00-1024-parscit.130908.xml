<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.102593">
<title confidence="0.9980355">
Categorizing Unknown Words: Using Decision Trees to Identify
Names and Misspellings
</title>
<author confidence="0.99459">
Janine Toole
</author>
<affiliation confidence="0.986465333333333">
Natural Language Laboratory
Department of Computing Science
Simon Fraser University
</affiliation>
<address confidence="0.720433">
Burnaby, BC, Canada V5A 1S6
</address>
<email confidence="0.862862">
toolegcs.sfu.ca
</email>
<sectionHeader confidence="0.990941" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999818181818182">
This paper introduces a system for categorizing un-
known words. The system is based on a multi-
component architecture where each component is re-
sponsible for identifying one class of unknown words.
The focus of this paper is the components that iden-
tify names and spelling errors. Each component
uses a decision tree architecture to combine multiple
types of evidence about the unknown word. The sys-
tem is evaluated using data from live closed captions
- a genre replete with a wide variety of unknown
words.
</bodyText>
<sectionHeader confidence="0.998429" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999840744680851">
In any real world use, a Natural Language Process-
ing (NLP) system will encounter words that are not
in its lexicon, what we term &apos;unknown words&apos;. Un-
known words are problematic because a NLP system
will perform well only if it recognizes the words that
it is meant to analyze or translate: the more words a
system does not recognize the more the system&apos;s per-
formance will degrade. Even when unknown words
are infrequent, they can have a disproportionate ef-
fect on system quality. For example, Min (1996)
found that while only 0.6% of words in 300 e-mails
were misspelled, this meant that 12% of the sen-
tences contained an error (discussed in (Min and
Wilson, 1998)).
Words may be unknown for many reasons: the
word may be a proper name, a misspelling, an ab-
breviation, a number, a morphological variant of a
known word (e.g. recleared), or missing from the
dictionary. The first step in dealing with unknown
words is to identify the class of the unknown word;
whether it is a misspelling, a proper name, an ab-
breviation etc. Once this is known, the proper ac-
tion can be taken, misspellings can be corrected, ab-
breviations can be expanded and so on, as deemed
necessary by the particular text processing applica-
tion. In this paper we introduce a system for cat-
egorizing unknown words. The system is based on
a multi- component architecture where each compo-
nent is responsible for identifying one category of
unknown words. The main focus of this paper is the
components that identify names and spelling errors.
Both components use a decision tree architecture to
combine multiple types of evidence about the un-
known word. Results from the two components are
combined using a weighted voting procedure. The
system is evaluated using data from live closed cap-
tions - a genre replete with a wide variety of un-
known words.
This paper is organized as follows. In section 2
we outline the overall architecture of the unknown
word categorizer. The name identifier and the mis-
spelling identifier are introduced in section 3. Perfor-
mance and evaluation issues are discussed in section
4. Section 5 considers portability issues. Section 6
compares the current system with relevant preced-
ing research. Concluding comments can be found in
section 6.
</bodyText>
<sectionHeader confidence="0.970269" genericHeader="method">
2 System Architecture
</sectionHeader>
<bodyText confidence="0.9999602">
The goal of our research is to develop a system that
automatically categorizes unknown words. Accord-
ing to our definition, an unknown word is a word
that is not contained in the lexicon of an NLP sys-
tem. As defined, &apos;unknown-ness&apos; is a relative con-
cept: a word that is known to one system may be
unknown to another system.
Our research is motivated by the problems that
we have experienced in translating live closed cap-
tions: live captions are produced under tight time
constraints and contain many unknown words. Typ-
ically, the caption transcriber has a five second win-
dow to transcribe the broadcast dialogue. Because
of the live nature of the broadcast, there is no op-
portunity to post-edit the transcript in any way. Al-
though motivated by our specific requirements, the
unknown word categorizer would benefit any NLP
system that encounters unknown words of differ-
ing categories. Some immediately obvious domains
where unknown words are frequent include e-mail
messages, internet chat rooms, data typed in by call
centre operators, etc.
To deal with these issues we propose a multi-
component architecture where individual compo-
nents specialize in identifying one particular type of
</bodyText>
<page confidence="0.998284">
173
</page>
<bodyText confidence="0.999982095238096">
unknown word. For example, the misspelling iden-
tifier will specialize in identifying misspellings, the
abbreviation component will specialize in identify-
ing abbreviations, etc. Each component will return
a confidence measure of the reliability of its predic-
tion, c.f. (Elworthy, 1998). The results from each
component are evaluated to determine the final cat-
egory of the word.
There are several advantages to this approach.
Firstly, the system can take advantage of existing
research. For example, the name recognition mod-
ule can make use of the considerable research that
exists on name recognition, e.g. (McDonald, 1996),
(Mani et al., 1996). Secondly, individual compo-
nents can be replaced when improved models are
available, without affecting other parts of the sys-
tem. Thirdly, this approach is compatible with in-
corporating multiple components of the same type to
improve performance (cf. (van HaIteren et al., 1998)
who found that combining the results of several part
of speech taggers increased performance).
</bodyText>
<sectionHeader confidence="0.992221" genericHeader="method">
3 The Current System
</sectionHeader>
<bodyText confidence="0.996183285714286">
In this paper we introduce a simplified version
of the unknown word categorizer: one that con-
tains just two components: misspelling identifica-
tion and name identification. In this section we in-
troduce these components and the &apos;decision&apos; compo-
nent which combines the results from the individual
modules.
</bodyText>
<subsectionHeader confidence="0.998038">
3.1 The Name Identifier
</subsectionHeader>
<bodyText confidence="0.99998924390244">
The goal of the name identifier is to differentiate be-
tween those unknown words which are proper names,
and those which are not. We define a name as word
identifying a person, place, or concept that would
typically require capitalization in English.
One of the motivations for the modular architec-
ture introduced above, was to be able to leverage
existing research. For example, ideally, we should
be able to plug in an existing proper name recog-
nizer and avoid the problem of creating our own.
However, the domain in which we are currently op-
erating - live closed captions - makes this approach
difficult. Closed captions do not contain any case
information, all captions are in upper case. Exist-
ing proper name recognizers rely heavily on case to
identify names, hence they perform poorly on our
data.
A second disadvantage of currently available name
recognizers is that they do not generally return a
confidence measure with their prediction. Some
indication of confidence is required in the multi-
component architecture we have implemented. How-
ever, while currently existing name recognizers are
inappropriate for the needs of our domain, future
name recognizers may well meet these requirements
and be able to be incorporated into the architecture
we propose.
For these reasons we develop our own name iden-
tifier. We utilize a decision tree to model the charac-
teristics of proper names. The advantage of decision
trees is that they are highly explainable: one can
readily understand the features that are affecting
the analysis (Weiss and Indurkhya, 1998). Further-
more, decision trees are well-suited for combining a
wide variety of information.
For this project, we made use of the decision tree
that is part of IBM&apos;s Intelligent Miner suite for data
mining. Since the point of this paper is to describe
an application of decision trees rather than to ar-
gue for a particular decision tree algorithm, we omit
further details of the decision tree software. Sim-
ilar results should be obtained by using other de-
cision tree software. Indeed, the results we obtain
could perhaps be improved by using more sophisti-
cated decision-tree approaches such as the adaptive-
resampling described in (Weiss et al, 1999).
The features that we use to train the decision tree
are intended to capture the characteristics of names.
We specify a total of ten features for each unknown
word. These identify two features of the unknown
word itself as well as two features for each of the two
preceding and two following words.
The first feature represents the part of speech of
the word. We use an in-house statistical tagger
(based on (Church, 1988)) to tag the text in which
the unknown word occurs. The tag set used is a
simplified version of the tags used in the machine-
readable version of the Oxford Advanced Learners
Dictionary (OALD). The tag set contains just one
tag to identify nouns.
The second feature provides more informative tag-
ging for specific parts of speech (these are referred
to as &apos;detailed tags&apos; (DETAG)). This tagset consists
of the nine tags listed in Table 1. All parts of speech
apart from noun and punctuation tags are assigned
the tag &apos;OTHER&apos;. All punctuation tags are assigned
the tag &apos;BOUNDARY&apos;. Words identified as nouns
are assigned one of the remaining tags depending on
the information provided in the OALD (although the
unknown word, by definition, will not appear in the
OALD, the preceding and following words may well
appear in the dictionary). If the word is identified in
the OALD as a common noun it is assigned the tag
&apos;COM&apos;. If it is identified in the OALD as a proper
name it is assigned the tag &apos;NAME&apos;. If the word is
specified as both a name and a common noun (e.g.
&apos;bill&apos;), then it is assigned the tag `NCOM&apos;. Pronouns
are assigned the tag PRON&apos;. If the word is in a list
of titles that we have compiled, then the tag &apos;TITLE&apos;
is assigned. Similarly, if the word is a member of the
class of words that can follow a name (e.g. jr&apos;), then
the tag &apos;POST&apos; is assigned. A simple rule-based sys-
</bodyText>
<page confidence="0.994125">
174
</page>
<table confidence="0.775348666666667">
COM common noun
NAME name
NCOM name and common noun
PRONOUN pronoun
TITLE title
POST post-name word
BOUNDARY boundary marker
OTHER not noun or boundary
UNKNOWN unknown noun
</table>
<tableCaption confidence="0.996592">
Table 1: List of Detailed Tags (DETAG)
</tableCaption>
<figure confidence="0.824009166666667">
Corpus frequency
Word length
Edit distance
Ispell information
Character sequence frequency
Non-English characters
</figure>
<tableCaption confidence="0.982081">
Table 2: Features used in misspelling decision tree
</tableCaption>
<bodyText confidence="0.998176428571429">
tern is used to assign these tags.
If we were dealing with data that contains case
information, we would also include fields represent-
ing the existence/non-existence of initial upper case
for the five words. However, since our current data
does not include case information we do not include
these features.
</bodyText>
<subsectionHeader confidence="0.999832">
3.2 The Misspelling Identifier
</subsectionHeader>
<bodyText confidence="0.998709932432433">
The goal of the misspelling identifier is to differenti-
ate between those unknown words which are spelling
errors and those which are not. We define a mis-
spelling as an unintended, orthographically incorrect
representation (with respect to the NLP system) of a
word. A misspelling differs from the intended known
word through one or more additions, deletions, sub-
stitutions, or reversals of letters, or the exclusion of
punctuation such as hyphenation or spacing. Like
the definition of &apos;unknown word&apos;, the definition of a
misspelling is also relative to a particular NLP sys-
tem.
Like the name identifier, we make use of a decision
tree to capture the characteristics of misspellings.
The features we use are derived from previous re-
search, including our own previous research on mis-
spelling identification. An abridged list of the fea-
tures that are used in the training data is listed in
Table 2 and discussed below.
Corpus frequency: (Vosse, 1992) differentiates
between misspellings and neologisms (new words)
in terms of their frequency. His algorithm classi-
fies unknown words that appear infrequently as mis-
spellings, and those that appear more frequently as
neologisms. Our corpus frequency variable specifies
the frequency of each unknown word in a 2.6 million
word corpus of business news closed captions.
Word Length: (Agirre et al., 1998) note that
their predictions for the correct spelling of mis-
spelled words are more accurate for words longer
than four characters, and much less accurate for
shorter words. This observation can also be found in
(Kukich, 1992). Our word length variables measures
the number of characters in each word.
Edit distance: Edit-distance is a metric for iden-
tifying the orthographic similarity of two words.
Typically, one edit-distance corresponds to one sub-
stitution, deletion, reversal or addition of a charac-
ter. (Damerau, 1964) observed that 80% of spelling
errors in his data were just one edit-distance from
the intended word. Similarly, (Mitton, 1987) found
that 70% of his data was within one edit-distance
from the intended word. Our edit distance feature
represents the edit distance from the unknown word
to the closest suggestion produced by the unix spell
checker, ispell. If ispell does not produce any sugges-
tions, an edit distance of thirty is assigned. In pre-
vious work we have experimented with more sophis-
ticated distance measures. However, simple edit dis-
tance proved to be the most effective (Toole, 1999).
Character sequence frequency: A characteris-
tic of some misspellings is that they contain charac-
ter sequences which are not typical of the language,
e.g.tited, wful. Exploiting this information is a stan-
dard way of identifying spelling errors when using a
dictionary is not desired or appropriate, e.g. (Hull
and Srihari, 1982), (Zamora et al., 1981).
To calculate our character sequence feature, we
firstly determine the frequencies of the two least fre-
quent character tri-gram sequences in the word in
each of a selection of corpora. In previous work we
included each of these values as individual features.
However, the resulting trees were quite unstable as
one feature would be relevant to one tree, whereas
a different character sequence feature would be rel-
evant to another tree. To avoid this problem, we
developed a composite feature that is the sum of all
individual character sequence frequencies.
Non-English characters: This binary feature
specifies whether a word contains a character that is
not typical of English words, such as accented char-
acters, etc. Such characters are indicative of foreign
names or transmission noise (in the case of captions)
rather than misspellings.
</bodyText>
<subsectionHeader confidence="0.992005">
3.3 Decision Making Component
</subsectionHeader>
<bodyText confidence="0.99998375">
The misspelling identifier and the name identifier
will each return a prediction for an unknown word.
In cases where the predictions are compatible, e.g.
where the name identifier predicts that it is a name
and the spelling identifier predicts that it is not
a misspelling, then the decision is straightforward.
Similarly, if both decision trees make negative pre-
dictions, then we can assume that the unknown word
</bodyText>
<page confidence="0.99418">
175
</page>
<bodyText confidence="0.989601266666667">
is neither a misspelling nor a name, but some other
category of unknown word.
However, it is also possible that both the spelling
identifier and the name identifier will make positive
predictions. In these cases we need a mechanism
to decide which assignment is upheld. For the pur-
poses of this paper, we make use of a simple heuris-
tic where in the case of two positive predictions the
one with the highest confidence measure is accepted.
The decision trees return a confidence measure for
each leaf of the tree. The confidence measure for a
particular leaf is calculated from the training data
and corresponds to the proportion of correct predic-
tions over the total number of predictions at this
leaf.
</bodyText>
<sectionHeader confidence="0.999049" genericHeader="method">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999928721649484">
In this section we evaluate the unknown word cat-
egorizer introduced above. We begin by describing
the training and test data. Following this, we eval-
uate the individual components and finally, we eval-
uate the decision making component.
The training and test data for the decision tree
consists of 7000 cases of unknown words extracted
from a 2.6 million word corpus of live business news
captions. Of the 7000 cases, 70.4% were manually
identified as names and 21.3% were identified as mis-
spellings.The remaining cases were other types of
unknown words such as abbreviations, morphologi-
cal variants, etc. Seventy percent of the data was
randomly selected to serve as the training corpus.
The remaining thirty percent, or 2100 records, was
reserved as the test corpus. The test data consists of
ten samples of 2100 records selected randomly with
replacement from the test corpus.
We now consider the results of training a decision
tree to identify misspellings using those features we
introduced in the section on the misspelling identi-
fier. The tree was trained on the training data de-
scribed above. The tree was evaluated using each of
the ten test data sets. The average precision and
recall data for the ten test sets are given in Ta-
ble 3, together with the base-line case of assuming
that we categorize all unknown words as names (the
most common category). With the baseline case we
achieve 70.4% precision but with 0% recall. In con-
trast, the decision tree approach obtains 77.1% pre-
cision and 73.8% recall.
We also trained a decision tree using not only the
features identified in our discussion on misspellings
but also those features that we introduced in our
discussion of name identification. The results for
this tree can be found in the second line of Table
3. The inclusion of the additional features has in-
creased precision by approximately 5%. However, it
has also decreased recall by about the same amount.
The overall F-score is quite similar. It appears that
the name features are not predictive for identifying
misspellings in this domain. This is not surprising
considering that eight of the ten features specified
for name identification are concerned with features
of the two preceding and two following words. Such
word-external information is of little use in identify-
ing a misspelling.
An analysis of the cases where the misspelling de-
cision tree failed to identify a misspelling revealed
two major classes of omissions. The first class con-
tains a collection of words which have typical char-
acteristics of English words, but differ from the in-
tended word by the addition or deletion of a syllable.
Words in this class include creditability for credi-
bility, coordinatored for coordinated, and represen-
tives for representatives. The second class contains
misspellings that differ from known words by the
deletion of a blank. Examples in this class include
webpage, crewlizeilibers, and rainshower. The second
class of misspellings can be addressed by adding a
feature that specifies whether the unknown word can
be split up into two component known words. Such
a feature should provide strong predictability for the
second class of words. The first class of words are
more of a challenge. These words have a close ho-
mophonic relationship with the intended word rather
than a close homographic relationship (as captured
by edit distance). Perhaps this class of words would
benefit from a feature representing phonetic distance
rather than edit distance.
Among those words which were incorrectly iden-
tified as misspellings, it is also possible to identify
common causes for the misidentification. Among
these words are many foreign words which have
character sequences which are not common in En-
glish. Examples include khanchanalak, phytoplank-
ton, bryceen.
The results for our name identifier are given in
Table 4. Again, the decision tree approach is a sig-
nificant improvement over the baseline case. If we
take the baseline approach and assume that all un-
known words are names, then we would achieve a
precision of 70.4%. However, using the decision tree
approach, we obtain 86.5% precision and 92.9% re-
call.
We also trained a tree using both the name and
misspelling features. The results can be found in
the second line of Table 4. Unlike the case when we
trained the misspelling identifier on all the features,
the extended tree for the name identifier provides
increased recall as well as increased precision. Un-
like the case with the misspelling decision-tree, the
misspelling-identification features do provide predic-
tive information for name identification. If we review
the features, this result seems quite reasonable: fea-
tures such as corpus frequency and non-English char-
acters can provide evidence for/against name iden-
</bodyText>
<page confidence="0.996555">
176
</page>
<table confidence="0.9998042">
Baseline Precision/Recall Precision Recall F-score
70.4%0% 73.8% 77.1% 75.4
82.8% 68.9% 75.2
Misspelling features only
All features
</table>
<tableCaption confidence="0.999874">
Table 3: Precision and recall for misspelling identification
</tableCaption>
<bodyText confidence="0.999990395061729">
tification as well as for/against misspelling identifi-
cation. For example, an unknown word that occurs
quite frequently (such as c/inton) is likely to be a
name, whereas an unknown word that occurs infre-
quently (such as wind) is likely to be a misspelling.
A review of the errors made by the name iden-
tifier again provides insight for future development.
Among those unknown words that are names but
which were not identified as such are predominantly
names that can (and did) appear with determiners.
Examples of this class include steelers in the steelers,
and pathfinder in the pathfinder. Hence, the name
identifier seems adept at finding the names of indi-
vidual people and places, which typically cannot be
combined with determiners. But, the name identi-
fier has more problems with names that have similar
distributions to common nouns.
The cases where the name identifier incorrectly
identifies unknown words as names also have identifi-
able characteristics. These examples mostly include
words with unusual character sequences such as the
misspellings szetion and fwlantg. No doubt these
have similar characteristics to foreign names. As
the misidentified words are also correctly identified
as misspellings by the misspelling identifier, these
are less problematic. It is the task of the decision-
making component to resolve issues such as these.
The final results we include are for the unknown
word categorizer itself using the voting procedure
outlined in previous discussion. As introduced pre-
viously, confidence measure is used as a tie-breaker
in cases where the two components make positive
decision. We evaluate the categorizer using preci-
sion and recall metrics. The precision metric identi-
fies the number of correct misspelling or name cat-
egorizations over the total number of times a word
was identified as a misspelling or a name. The re-
call metric identifies the number of times the system
correctly identifies a misspelling or name over the
number of misspellings and names existing in the
data. As illustrated in Table 5, the unknown word
categorizer achieves 86% precision and 89.9% recall
on the task of identifying names and misspellings.
An examination of the confusion matrix of the tie-
breaker decisions is also revealing. We include the
confusion matrix for one test data set in Table 6.
Firstly, in only about 5% of the cases was it nec-
essary to revert to confidence measure to determine
the category of the unknown word. In all other cases
the predictions were compatible. Secondly, in the
majority of cases the decision-maker rules in favour
of the name prediction. In hindsight this is not sur-
prising since the name decision tree has higher re-
sults and hence is likely to have higher confidence
measures.
A review of the largest error category in this con-
fusion matrix is also insightful. These are cases
where the decision-maker classifies the unknown
word as a name when it should be a misspelling (37
cases). The words in this category are typically ex-
amples where the misspelled word has a phonetic
relationship with the intended word. For example,
teint for tempt, floyda for florida, and dimotv part of
the intended word democrat. Not surprisingly, it was
these types of words which were identified as prob-
lematic for the current misspelling identifier. Aug-
menting the misspelling identifier with features to
identify these types of misspellings should also lead
to improvement in the decision-maker.
We find these results encouraging: they indicate
that the approach we are taking is productive. Our
future work will focus on three fronts. Firstly, we
will improve our existing components by developing
further features which are sensitive to the distinction
between names and misspellings. The discussion in
this section has indicated several promising direc-
tions. Secondly, we will develop components to iden-
tify the remaining types of unknown words, such as
abbreviations, morphological variants, etc. Thirdly,
we will experiment with alternative decision-making
processes.
</bodyText>
<sectionHeader confidence="0.93707" genericHeader="method">
5 Examining Portability
</sectionHeader>
<bodyText confidence="0.998355176470588">
In this paper we have introduced a means for iden-
tifying names and misspellings from among other
types of unknown words and have illustrated the pro-
cess using the domain of closed captions. Although
not explicitly specified, one of the goals of the re-
search has been to develop an approach that will be
portable to new domains and languages.
We are optimistic that the approach we have de-
veloped is portable. The system that we have de-
veloped requires very little in terms of linguistic re-
sources. Apart from a corpus of the new domain
and language, the only other requirements are some
means of generating spelling suggestions (ispell is
available for many languages) and a part-of-speech
tagger. For this reason, the unknown word cate-
gorizer should be portable to new languages, even
where extensive language resources do not exist. If
</bodyText>
<page confidence="0.991516">
177
</page>
<table confidence="0.9998428">
Baseline Precision Precision Recall F-score
86.5% 92.9% 89.6
91.8% 94.5% 93.1
Name features only 70.4%
All Features
</table>
<tableCaption confidence="0.993347">
Table 4: Precision and recall for name identification
</tableCaption>
<table confidence="0.9844935">
Precision Recall F-score
Predicting Names and Misspellings 86.6% 89.9% 88.2
</table>
<tableCaption confidence="0.999646">
Table 5: Precision and recall for decision-making component
</tableCaption>
<bodyText confidence="0.99995">
more information sources are available, then these
can be readily included in the information provided
to the decision tree training algorithm.
For many languages, the features used in the
unknown word categorizer may well be sufficient.
However, the features used do make some assump-
tions about the nature of the writing system used.
For example, the edit distance feature in the mis-
spelling identifier assumes that words consist of al-
phabetic characters which have undergone substitu-
tion/addition/deletion. However, this feature will be
less useful in a language such as Japanese or Chinese
which use ideographic characters. However, while
the exact features used in this paper may be inap-
propriate for a given language, we believe the general
approach is transferable. In the case of a language
such as Japanese, one would consider the means by
which misspellings differ from their intended word
and identify features to capture these differences.
</bodyText>
<sectionHeader confidence="0.999476" genericHeader="method">
6 Related Research
</sectionHeader>
<bodyText confidence="0.999981163265306">
There is little research that has focused on differen-
tiating the different types of unknown words. For
example, research on spelling error detection and
correction for the most part assumes that all un-
known words are misspellings and makes no attempt
to identify other types of unknown words, e.g. (Elmi
and Evens, 1998). Naturally, these are not appropri-
ate comparisons for the work reported here. How-
ever, as is evident from the discussion above, previ-
ous spelling research does provide an important role
in suggesting productive features to include in the
decision tree.
Research that is more similar in goal to that out-
lined in this paper is Vosse (Vosse, 1992). Vosse uses
a simple algorithm to identify three classes of un-
known words: misspellings, neologisms, and names.
Capitalization is his sole means of identifying names.
However, capitalization information is not available
in closed captions. Hence, his system would be inef-
fective on the closed caption domain with which we
are working. (Granger, 1983) uses expectations gen-
erated by scripts to analyze unknown words. The
drawback of his system is that it lacks portability
since it incorporates scripts that make use of world
knowledge of the situation being described; in this
case, naval ship-to-shore messages.
Research that is similar in technique to that re-
ported here is (Baluja et al., 1999). Baluja and his
colleagues use a decision tree classifier to identify
proper names in text. They incorporate three types
of features: word level (essentially utilizes case in-
formation), dictionary-level (comparable to our is-
pell feature), and POS information (comparable to
our POS tagging). Their highest F-score for name
identification is 95.2, slightly higher than our name
identifier. However, it is difficult to compare the
two sets of results since our tasks are slightly dif-
ferent. The goal of Baluja&apos;s research, and all other
proper name identification research, is to identify all
those words and phrases in the text which are proper
names. Our research, on the other hand, is not con-
cerned with all text, but only those words which are
unknown. Also preventing comparison is the type of
data that we deal with. Baluja&apos;s data contains case
information whereas ours does not- the lack of case
information makes name identification significantly
more difficult. Indeed, Baluja&apos;s results when they
exclude their word-level (case) features are signifi-
cantly lower: a maximum F-score of 79.7.
</bodyText>
<sectionHeader confidence="0.998824" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.998400888888889">
In this paper we have introduced an unknown word
categorizer that can identify misspellings and names.
The unknown word categorizer consists of individ-
ual components, each of which specialize in iden-
tifying a particular class of unknown word. The
two existing components are implemented as deci-
sion trees. The system provides encouraging results
when evaluated against a particularly challenging
domain: transcripts from live closed captions.
</bodyText>
<sectionHeader confidence="0.999222" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9938995">
E. Agirre, K. Gojenola, K. Sarasola„ and A. Vouti-
lainen. 1998. Towards a single proposal in spelling
correction. In Proceedings of the 36th Annual
Meeting of the ACL and the 17th International
</reference>
<page confidence="0.985359">
178
</page>
<figure confidence="0.785956285714286">
Predicted Spelling Predicted Name
0 6
10 37
4 43
Neither name nor misspelling
Misspelling
Name
</figure>
<tableCaption confidence="0.9292305">
Table 6: Confusion matrix for decision maker: includes only those examples where both components made
a positive prediction.
</tableCaption>
<reference confidence="0.997734350649351">
Conference on Computational Linguistics, pages
22-28.
S. Baluja, V. Mittal, and R.. Sukthankar. 1999.
Applying machine learning for high performance
named-entity extraction. In Proceedings of the
Conference of the Pacific Association for Com-
putational Linguistics, pages 365-378.
K. Church 1988. A stochastic parts program and
noun phrase parser for unrestricted text. In Pro-
ceedings of the Second Conference on Applied Nat-
ural Language Processing, pages 136-143.
F. Damerau. 1964. A technique for computer detec-
tion and correction of spelling errors. Communi-
cations of the ACM, 7:171-176.
M. Elmi and M. Evens. 1998. Spelling correction
using context. In Proceedings of the 36th Annual
Meeting of the ACL and the 17th International
Conference on Computational Linguistics, pages
360-364.
D. Elworthy. 1998. Language identification with
confidence limits. In Proceedings of the 6th Work-
shop on Very large Corpora.
R. Granger. 1983. The nomad system: expectation-
based detection and correction of errors during un-
derstanding of syntactically and semantically ill-
formed text. American Journal of Computational
Linguistics, 9:188-198.
J. Hull and S. Srihari. 1982. Experiments in text
recognition with binary n-gram and viterbi algo-
rithms. IEEE Trans. Patt. Anal. Machine Intell.
PAMI-4, 5:520-530.
K.. Kukich. 1992. Techniques for automatically cor-
recting words in text. ACM Computing Surveys,
24:377-439.
I. Mani, R. McMillan, S. Luperfoy, E. Lusher, and
S. Laskowski, 1996. Corpus Processing for Lexical
Acquisition, chapter Identifying unknown proper
names in newswire text. MIT Press, Cambridge.
D. McDonald, 1996. Corpus Processing for Lexi-
cal Acquisition, chapter Internal and external ev-
idence in the identification and semantic catego-
rization of proper names. MIT Press, Cambridge.
K. Min and W. Wilson. 1998. Integrated control of
chart items for error repair. In Proceedings of the
36th Annual Meeting of the Association for Com-
putational Linguistics and the 17th International
Conference on Computational Linguistics.
K. Min. 1996. Hierarchical Error Recovery Based
on Bidirectional Chart Parsing Techniques. Ph.D.
thesis, University of NSW, Sydney, Australia.
R. Mitton. 1987. Spelling checkers, spelling correc-
tors, and the misspellings of poor spellers. Inf.
Process. Manage, 23:495-505.
J. Toole 1999 Categorizing Unknown Words: A de-
cision tree-based misspelling identifier In Foo, N
(ed.) Advanced Topics in _Artificial Intelligence,
pages 122-133.
H. van Halteren, J. Zavrel, and W. Daelemans. 1998.
Improving data driven word class tagging by sys-
tem combination. In Proceedings of the 36th An-
nual Meeting of the ACL and the 17th Interna-
tional Conference on Computational Linguistics,
pages 491-497.
T. Vosse. 1992. Detecting and correcting morpho-
syntactic errors in real texts. In Proceedings of
the 3rd Conference on Applied Natural Language
Processing, pages 111-118.
S. Weiss and N. Indurkhya. 1998. Predictive Data
Mining. Morgan Kauffman Publishers.
S. Weiss, and C. Apte, and F. Damerau, and
D. Johnson, and F. Oles and T. Goetz, and
T. Hampp. 1999 Maximizing text-mining per-
formance. IEEE Intelligent Systems and their
Applications,14(4):63-69
E. Zamora, J. Pollock, and A. Zamora. 1981. The
use of tri-gram analysis for spelling error detec-
tion. Inf. Process. Manage., 17:305-316.
</reference>
<page confidence="0.998809">
179
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000558">
<title confidence="0.9989605">Categorizing Unknown Words: Using Decision Trees to Identify Names and Misspellings</title>
<author confidence="0.998551">Janine Toole</author>
<affiliation confidence="0.997962">Natural Language Laboratory Department of Computing Science Simon Fraser University</affiliation>
<address confidence="0.9986">Burnaby, BC, Canada V5A 1S6</address>
<email confidence="0.994203">toolegcs.sfu.ca</email>
<abstract confidence="0.995816828990229">This paper introduces a system for categorizing unknown words. The system is based on a multicomponent architecture where each component is responsible for identifying one class of unknown words. The focus of this paper is the components that identify names and spelling errors. Each component uses a decision tree architecture to combine multiple types of evidence about the unknown word. The system is evaluated using data from live closed captions a genre replete with a wide variety of unknown words. In any real world use, a Natural Language Processing (NLP) system will encounter words that are not in its lexicon, what we term &apos;unknown words&apos;. Unknown words are problematic because a NLP system will perform well only if it recognizes the words that it is meant to analyze or translate: the more words a system does not recognize the more the system&apos;s performance will degrade. Even when unknown words are infrequent, they can have a disproportionate effect on system quality. For example, Min (1996) found that while only 0.6% of words in 300 e-mails were misspelled, this meant that 12% of the sentences contained an error (discussed in (Min and Wilson, 1998)). Words may be unknown for many reasons: the word may be a proper name, a misspelling, an abbreviation, a number, a morphological variant of a word (e.g. missing from the dictionary. The first step in dealing with unknown words is to identify the class of the unknown word; whether it is a misspelling, a proper name, an abbreviation etc. Once this is known, the proper action can be taken, misspellings can be corrected, abbreviations can be expanded and so on, as deemed necessary by the particular text processing application. In this paper we introduce a system for categorizing unknown words. The system is based on a multicomponent architecture where each component is responsible for identifying one category of unknown words. The main focus of this paper is the components that identify names and spelling errors. Both components use a decision tree architecture to combine multiple types of evidence about the unknown word. Results from the two components are combined using a weighted voting procedure. The system is evaluated using data from live closed captions a genre replete with a wide variety of unknown words. This paper is organized as follows. In section 2 we outline the overall architecture of the unknown word categorizer. The name identifier and the misspelling identifier are introduced in section 3. Performance and evaluation issues are discussed in section 4. Section 5 considers portability issues. Section 6 compares the current system with relevant preceding research. Concluding comments can be found in section 6. 2 System Architecture The goal of our research is to develop a system that automatically categorizes unknown words. According to our definition, an unknown word is a word that is not contained in the lexicon of an NLP system. As defined, &apos;unknown-ness&apos; is a relative concept: a word that is known to one system may be unknown to another system. Our research is motivated by the problems that we have experienced in translating live closed captions: live captions are produced under tight time constraints and contain many unknown words. Typically, the caption transcriber has a five second window to transcribe the broadcast dialogue. Because of the live nature of the broadcast, there is no opportunity to post-edit the transcript in any way. Although motivated by our specific requirements, the unknown word categorizer would benefit any NLP system that encounters unknown words of differing categories. Some immediately obvious domains where unknown words are frequent include e-mail messages, internet chat rooms, data typed in by call centre operators, etc. To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of 173 unknown word. For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc. Each component will return a confidence measure of the reliability of its prediction, c.f. (Elworthy, 1998). The results from each component are evaluated to determine the final category of the word. There are several advantages to this approach. Firstly, the system can take advantage of existing research. For example, the name recognition module can make use of the considerable research that exists on name recognition, e.g. (McDonald, 1996), (Mani et al., 1996). Secondly, individual components can be replaced when improved models are available, without affecting other parts of the system. Thirdly, this approach is compatible with incorporating multiple components of the same type to improve performance (cf. (van HaIteren et al., 1998) who found that combining the results of several part of speech taggers increased performance). 3 The Current System In this paper we introduce a simplified version of the unknown word categorizer: one that contains just two components: misspelling identification and name identification. In this section we introduce these components and the &apos;decision&apos; component which combines the results from the individual modules. 3.1 The Name Identifier The goal of the name identifier is to differentiate between those unknown words which are proper names, and those which are not. We define a name as word identifying a person, place, or concept that would typically require capitalization in English. One of the motivations for the modular architecture introduced above, was to be able to leverage existing research. For example, ideally, we should be able to plug in an existing proper name recognizer and avoid the problem of creating our own. However, the domain in which we are currently operating live closed captions makes this approach difficult. Closed captions do not contain any case information, all captions are in upper case. Existing proper name recognizers rely heavily on case to identify names, hence they perform poorly on our data. A second disadvantage of currently available name recognizers is that they do not generally return a confidence measure with their prediction. Some indication of confidence is required in the multicomponent architecture we have implemented. However, while currently existing name recognizers are inappropriate for the needs of our domain, future name recognizers may well meet these requirements and be able to be incorporated into the architecture we propose. For these reasons we develop our own name identifier. We utilize a decision tree to model the characteristics of proper names. The advantage of decision trees is that they are highly explainable: one can readily understand the features that are affecting the analysis (Weiss and Indurkhya, 1998). Furthermore, decision trees are well-suited for combining a wide variety of information. For this project, we made use of the decision tree that is part of IBM&apos;s Intelligent Miner suite for data mining. Since the point of this paper is to describe an application of decision trees rather than to argue for a particular decision tree algorithm, we omit further details of the decision tree software. Similar results should be obtained by using other decision tree software. Indeed, the results we obtain could perhaps be improved by using more sophisticated decision-tree approaches such as the adaptiveresampling described in (Weiss et al, 1999). The features that we use to train the decision tree are intended to capture the characteristics of names. We specify a total of ten features for each unknown word. These identify two features of the unknown word itself as well as two features for each of the two preceding and two following words. The first feature represents the part of speech of the word. We use an in-house statistical tagger (based on (Church, 1988)) to tag the text in which the unknown word occurs. The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD). The tag set contains just one tag to identify nouns. The second feature provides more informative tagging for specific parts of speech (these are referred to as &apos;detailed tags&apos; (DETAG)). This tagset consists of the nine tags listed in Table 1. All parts of speech apart from noun and punctuation tags are assigned the tag &apos;OTHER&apos;. All punctuation tags are assigned the tag &apos;BOUNDARY&apos;. Words identified as nouns are assigned one of the remaining tags depending on the information provided in the OALD (although the unknown word, by definition, will not appear in the OALD, the preceding and following words may well appear in the dictionary). If the word is identified in the OALD as a common noun it is assigned the tag &apos;COM&apos;. If it is identified in the OALD as a proper name it is assigned the tag &apos;NAME&apos;. If the word is specified as both a name and a common noun (e.g. &apos;bill&apos;), then it is assigned the tag `NCOM&apos;. Pronouns are assigned the tag PRON&apos;. If the word is in a list of titles that we have compiled, then the tag &apos;TITLE&apos; is assigned. Similarly, if the word is a member of the class of words that can follow a name (e.g. jr&apos;), then tag &apos;POST&apos; is assigned. A simple rule-based sys- 174 COM common noun NAME name NCOM name and common noun PRONOUN pronoun TITLE title POST post-name word BOUNDARY boundary marker OTHER not noun or boundary UNKNOWN unknown noun Table 1: List of Detailed Tags (DETAG) Corpus frequency Word length Edit distance Ispell information Character sequence frequency Non-English characters Table 2: Features used in misspelling decision tree tern is used to assign these tags. If we were dealing with data that contains case information, we would also include fields representing the existence/non-existence of initial upper case for the five words. However, since our current data does not include case information we do not include these features. 3.2 The Misspelling Identifier The goal of the misspelling identifier is to differentiate between those unknown words which are spelling errors and those which are not. We define a misspelling as an unintended, orthographically incorrect representation (with respect to the NLP system) of a word. A misspelling differs from the intended known word through one or more additions, deletions, substitutions, or reversals of letters, or the exclusion of punctuation such as hyphenation or spacing. Like the definition of &apos;unknown word&apos;, the definition of a misspelling is also relative to a particular NLP system. Like the name identifier, we make use of a decision tree to capture the characteristics of misspellings. The features we use are derived from previous research, including our own previous research on misspelling identification. An abridged list of the features that are used in the training data is listed in Table 2 and discussed below. frequency: 1992) differentiates between misspellings and neologisms (new words) in terms of their frequency. His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms. Our corpus frequency variable specifies the frequency of each unknown word in a 2.6 million word corpus of business news closed captions. Length: et al., 1998) note that their predictions for the correct spelling of misspelled words are more accurate for words longer than four characters, and much less accurate for shorter words. This observation can also be found in (Kukich, 1992). Our word length variables measures the number of characters in each word. distance: is a metric for identifying the orthographic similarity of two words. Typically, one edit-distance corresponds to one substitution, deletion, reversal or addition of a character. (Damerau, 1964) observed that 80% of spelling errors in his data were just one edit-distance from the intended word. Similarly, (Mitton, 1987) found that 70% of his data was within one edit-distance from the intended word. Our edit distance feature represents the edit distance from the unknown word to the closest suggestion produced by the unix spell checker, ispell. If ispell does not produce any suggestions, an edit distance of thirty is assigned. In previous work we have experimented with more sophisticated distance measures. However, simple edit distance proved to be the most effective (Toole, 1999). sequence frequency: characteristic of some misspellings is that they contain character sequences which are not typical of the language, wful. this information is a standard way of identifying spelling errors when using a is not desired or appropriate, e.g. and Srihari, 1982), (Zamora et al., 1981). To calculate our character sequence feature, we firstly determine the frequencies of the two least frequent character tri-gram sequences in the word in each of a selection of corpora. In previous work we included each of these values as individual features. However, the resulting trees were quite unstable as one feature would be relevant to one tree, whereas a different character sequence feature would be relevant to another tree. To avoid this problem, we developed a composite feature that is the sum of all individual character sequence frequencies. characters: binary feature specifies whether a word contains a character that is not typical of English words, such as accented characters, etc. Such characters are indicative of foreign names or transmission noise (in the case of captions) rather than misspellings. 3.3 Decision Making Component The misspelling identifier and the name identifier will each return a prediction for an unknown word. In cases where the predictions are compatible, e.g. where the name identifier predicts that it is a name and the spelling identifier predicts that it is not a misspelling, then the decision is straightforward. Similarly, if both decision trees make negative predictions, then we can assume that the unknown word 175 is neither a misspelling nor a name, but some other category of unknown word. However, it is also possible that both the spelling identifier and the name identifier will make positive predictions. In these cases we need a mechanism to decide which assignment is upheld. For the purposes of this paper, we make use of a simple heuristic where in the case of two positive predictions the one with the highest confidence measure is accepted. The decision trees return a confidence measure for each leaf of the tree. The confidence measure for a particular leaf is calculated from the training data and corresponds to the proportion of correct predictions over the total number of predictions at this leaf. 4 Evaluation In this section we evaluate the unknown word categorizer introduced above. We begin by describing the training and test data. Following this, we evaluate the individual components and finally, we evaluate the decision making component. The training and test data for the decision tree consists of 7000 cases of unknown words extracted from a 2.6 million word corpus of live business news captions. Of the 7000 cases, 70.4% were manually identified as names and 21.3% were identified as misspellings.The remaining cases were other types of unknown words such as abbreviations, morphological variants, etc. Seventy percent of the data was randomly selected to serve as the training corpus. The remaining thirty percent, or 2100 records, was reserved as the test corpus. The test data consists of ten samples of 2100 records selected randomly with replacement from the test corpus. We now consider the results of training a decision tree to identify misspellings using those features we introduced in the section on the misspelling identifier. The tree was trained on the training data described above. The tree was evaluated using each of the ten test data sets. The average precision and recall data for the ten test sets are given in Table 3, together with the base-line case of assuming that we categorize all unknown words as names (the most common category). With the baseline case we achieve 70.4% precision but with 0% recall. In contrast, the decision tree approach obtains 77.1% precision and 73.8% recall. We also trained a decision tree using not only the features identified in our discussion on misspellings but also those features that we introduced in our discussion of name identification. The results for this tree can be found in the second line of Table 3. The inclusion of the additional features has increased precision by approximately 5%. However, it has also decreased recall by about the same amount. The overall F-score is quite similar. It appears that the name features are not predictive for identifying misspellings in this domain. This is not surprising considering that eight of the ten features specified for name identification are concerned with features of the two preceding and two following words. Such word-external information is of little use in identifying a misspelling. An analysis of the cases where the misspelling decision tree failed to identify a misspelling revealed two major classes of omissions. The first class contains a collection of words which have typical characteristics of English words, but differ from the intended word by the addition or deletion of a syllable. in this class include credicoordinatored represensecond class contains misspellings that differ from known words by the deletion of a blank. Examples in this class include crewlizeilibers, second class of misspellings can be addressed by adding a feature that specifies whether the unknown word can be split up into two component known words. Such a feature should provide strong predictability for the second class of words. The first class of words are more of a challenge. These words have a close homophonic relationship with the intended word rather than a close homographic relationship (as captured by edit distance). Perhaps this class of words would benefit from a feature representing phonetic distance rather than edit distance. Among those words which were incorrectly identified as misspellings, it is also possible to identify common causes for the misidentification. Among these words are many foreign words which have character sequences which are not common in En- Examples include phytoplank- The results for our name identifier are given in Table 4. Again, the decision tree approach is a significant improvement over the baseline case. If we take the baseline approach and assume that all unknown words are names, then we would achieve a precision of 70.4%. However, using the decision tree approach, we obtain 86.5% precision and 92.9% recall. We also trained a tree using both the name and misspelling features. The results can be found in the second line of Table 4. Unlike the case when we trained the misspelling identifier on all the features, the extended tree for the name identifier provides increased recall as well as increased precision. Unlike the case with the misspelling decision-tree, the misspelling-identification features do provide predictive information for name identification. If we review the features, this result seems quite reasonable: features such as corpus frequency and non-English charcan provide evidence for/against name iden- 176 Baseline Precision/Recall Precision Recall F-score 70.4%0% 73.8% 77.1% 75.4 82.8% 68.9% 75.2 Misspelling features only All features Table 3: Precision and recall for misspelling identification tification as well as for/against misspelling identification. For example, an unknown word that occurs frequently (such as likely to be a name, whereas an unknown word that occurs infre- (such as likely to be a misspelling. A review of the errors made by the name identifier again provides insight for future development. Among those unknown words that are names but which were not identified as such are predominantly names that can (and did) appear with determiners. of this class include steelers, pathfinder. the name identifier seems adept at finding the names of individual people and places, which typically cannot be combined with determiners. But, the name identifier has more problems with names that have similar distributions to common nouns. The cases where the name identifier incorrectly identifies unknown words as names also have identifiable characteristics. These examples mostly include words with unusual character sequences such as the doubt these have similar characteristics to foreign names. As the misidentified words are also correctly identified as misspellings by the misspelling identifier, these are less problematic. It is the task of the decisionmaking component to resolve issues such as these. The final results we include are for the unknown word categorizer itself using the voting procedure outlined in previous discussion. As introduced previously, confidence measure is used as a tie-breaker in cases where the two components make positive decision. We evaluate the categorizer using precision and recall metrics. The precision metric identifies the number of correct misspelling or name categorizations over the total number of times a word was identified as a misspelling or a name. The recall metric identifies the number of times the system correctly identifies a misspelling or name over the number of misspellings and names existing in the data. As illustrated in Table 5, the unknown word categorizer achieves 86% precision and 89.9% recall on the task of identifying names and misspellings. An examination of the confusion matrix of the tiebreaker decisions is also revealing. We include the confusion matrix for one test data set in Table 6. Firstly, in only about 5% of the cases was it necessary to revert to confidence measure to determine the category of the unknown word. In all other cases the predictions were compatible. Secondly, in the majority of cases the decision-maker rules in favour of the name prediction. In hindsight this is not surprising since the name decision tree has higher results and hence is likely to have higher confidence measures. A review of the largest error category in this confusion matrix is also insightful. These are cases where the decision-maker classifies the unknown word as a name when it should be a misspelling (37 cases). The words in this category are typically examples where the misspelled word has a phonetic relationship with the intended word. For example, floyda of intended word surprisingly, it was these types of words which were identified as problematic for the current misspelling identifier. Augmenting the misspelling identifier with features to identify these types of misspellings should also lead to improvement in the decision-maker. We find these results encouraging: they indicate that the approach we are taking is productive. Our future work will focus on three fronts. Firstly, we will improve our existing components by developing further features which are sensitive to the distinction between names and misspellings. The discussion in this section has indicated several promising directions. Secondly, we will develop components to identify the remaining types of unknown words, such as abbreviations, morphological variants, etc. Thirdly, we will experiment with alternative decision-making processes. 5 Examining Portability In this paper we have introduced a means for identifying names and misspellings from among other types of unknown words and have illustrated the process using the domain of closed captions. Although not explicitly specified, one of the goals of the research has been to develop an approach that will be portable to new domains and languages. We are optimistic that the approach we have developed is portable. The system that we have developed requires very little in terms of linguistic resources. Apart from a corpus of the new domain and language, the only other requirements are some means of generating spelling suggestions (ispell is available for many languages) and a part-of-speech tagger. For this reason, the unknown word categorizer should be portable to new languages, even where extensive language resources do not exist. If 177 Baseline Precision Precision Recall F-score 86.5% 92.9% 89.6 91.8% 94.5% 93.1 Name features only All Features 70.4% Table 4: Precision and recall for name identification Precision Recall F-score PredictingNames and 86.6% 89.9% 88.2 Table 5: Precision and recall for decision-making component more information sources are available, then these can be readily included in the information provided to the decision tree training algorithm. For many languages, the features used in the unknown word categorizer may well be sufficient. However, the features used do make some assumptions about the nature of the writing system used. For example, the edit distance feature in the misspelling identifier assumes that words consist of alphabetic characters which have undergone substitution/addition/deletion. However, this feature will be less useful in a language such as Japanese or Chinese which use ideographic characters. However, while the exact features used in this paper may be inappropriate for a given language, we believe the general approach is transferable. In the case of a language such as Japanese, one would consider the means by which misspellings differ from their intended word and identify features to capture these differences. Research There is little research that has focused on differentiating the different types of unknown words. For example, research on spelling error detection and correction for the most part assumes that all unknown words are misspellings and makes no attempt to identify other types of unknown words, e.g. (Elmi and Evens, 1998). Naturally, these are not appropriate comparisons for the work reported here. However, as is evident from the discussion above, previous spelling research does provide an important role in suggesting productive features to include in the decision tree. Research that is more similar in goal to that outlined in this paper is Vosse (Vosse, 1992). Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions. Hence, his system would be ineffective on the closed caption domain with which we are working. (Granger, 1983) uses expectations generated by scripts to analyze unknown words. The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages. Research that is similar in technique to that reported here is (Baluja et al., 1999). Baluja and his colleagues use a decision tree classifier to identify proper names in text. They incorporate three types of features: word level (essentially utilizes case information), dictionary-level (comparable to our ispell feature), and POS information (comparable to our POS tagging). Their highest F-score for name identification is 95.2, slightly higher than our name identifier. However, it is difficult to compare the two sets of results since our tasks are slightly different. The goal of Baluja&apos;s research, and all other proper name identification research, is to identify all those words and phrases in the text which are proper names. Our research, on the other hand, is not concerned with all text, but only those words which are unknown. Also preventing comparison is the type of data that we deal with. Baluja&apos;s data contains case information whereas ours does notthe lack of case information makes name identification significantly more difficult. Indeed, Baluja&apos;s results when they exclude their word-level (case) features are significantly lower: a maximum F-score of 79.7. 7 Conclusion In this paper we have introduced an unknown word categorizer that can identify misspellings and names. The unknown word categorizer consists of individual components, each of which specialize in identifying a particular class of unknown word. The two existing components are implemented as decision trees. The system provides encouraging results when evaluated against a particularly challenging domain: transcripts from live closed captions.</abstract>
<note confidence="0.4997425">References E. Agirre, K. Gojenola, K. Sarasola„ and A. Voutilainen. 1998. Towards a single proposal in spelling In of the 36th Annual Meeting of the ACL and the 17th International 178</note>
<phone confidence="0.562186">PredictedSpelling0 10 4 PredictedName6 37 43</phone>
<abstract confidence="0.7544754">Neither name nor misspelling Misspelling Name Table 6: Confusion matrix for decision maker: includes only those examples where both components made a positive prediction.</abstract>
<note confidence="0.708319272727273">on Computational Linguistics, 22-28. S. Baluja, V. Mittal, and R.. Sukthankar. 1999. Applying machine learning for high performance extraction. In of the Conference of the Pacific Association for Com- Linguistics, 365-378. K. Church 1988. A stochastic parts program and phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Nat- Language Processing, 136-143.</note>
<abstract confidence="0.8491254375">F. Damerau. 1964. A technique for computer detecand correction of spelling errors. Communiof the ACM, M. Elmi and M. Evens. 1998. Spelling correction context. In of the 36th Annual of the ACL 17th International on Computational Linguistics, 360-364. D. Elworthy. 1998. Language identification with limits. In of the 6th Workshop on Very large Corpora. R. Granger. 1983. The nomad system: expectationbased detection and correction of errors during understanding of syntactically and semantically illtext. Journal of Computational J. Hull and S. Srihari. 1982. Experiments in text recognition with binary n-gram and viterbi algo- Trans. Patt. Anal. Machine Intell. K.. Kukich. 1992. Techniques for automatically corwords in text. Computing Surveys, 24:377-439. I. Mani, R. McMillan, S. Luperfoy, E. Lusher, and Laskowski, 1996. Processing for Lexical Identifying unknown proper names in newswire text. MIT Press, Cambridge. McDonald, 1996. Processing for Lexi- Acquisition, Internal and external evidence in the identification and semantic categorization of proper names. MIT Press, Cambridge. K. Min and W. Wilson. 1998. Integrated control of items for error repair. In of the of the Association for Com-</abstract>
<note confidence="0.9417992">putational Linguistics and the 17th International Conference on Computational Linguistics. Min. 1996. Error Recovery Based Bidirectional Chart Parsing Techniques. thesis, University of NSW, Sydney, Australia. R. Mitton. 1987. Spelling checkers, spelling correcand the misspellings of poor spellers. Manage, J. Toole 1999 Categorizing Unknown Words: A decision tree-based misspelling identifier In Foo, N Topics in _Artificial Intelligence, pages 122-133. H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improving data driven word class tagging by syscombination. In of the 36th An- Meeting of the ACL 17th International Conference on Computational Linguistics, pages 491-497. T. Vosse. 1992. Detecting and correcting morphoerrors in real texts. In of the 3rd Conference on Applied Natural Language 111-118. Weiss and N. Indurkhya. 1998. Data Kauffman Publishers. S. Weiss, and C. Apte, and F. Damerau, and D. Johnson, and F. Oles and T. Goetz, and T. Hampp. 1999 Maximizing text-mining per- Intelligent Systems Applications,14(4):63-69 E. Zamora, J. Pollock, and A. Zamora. 1981. The</note>
<affiliation confidence="0.705503">use of tri-gram analysis for spelling error detec- Process. Manage.,</affiliation>
<address confidence="0.773983">179</address>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>K Gojenola</author>
<author>K Sarasola„</author>
<author>A Voutilainen</author>
</authors>
<title>Towards a single proposal in spelling correction.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>22--28</pages>
<marker>Agirre, Gojenola, Sarasola„, Voutilainen, 1998</marker>
<rawString>E. Agirre, K. Gojenola, K. Sarasola„ and A. Voutilainen. 1998. Towards a single proposal in spelling correction. In Proceedings of the 36th Annual Meeting of the ACL and the 17th International Conference on Computational Linguistics, pages 22-28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Baluja</author>
<author>V Mittal</author>
<author>R Sukthankar</author>
</authors>
<title>Applying machine learning for high performance named-entity extraction.</title>
<date>1999</date>
<booktitle>In Proceedings of the Conference of the Pacific Association for Computational Linguistics,</booktitle>
<pages>365--378</pages>
<contexts>
<context position="27601" citStr="Baluja et al., 1999" startWordPosition="4484" endWordPosition="4487">sspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions. Hence, his system would be ineffective on the closed caption domain with which we are working. (Granger, 1983) uses expectations generated by scripts to analyze unknown words. The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages. Research that is similar in technique to that reported here is (Baluja et al., 1999). Baluja and his colleagues use a decision tree classifier to identify proper names in text. They incorporate three types of features: word level (essentially utilizes case information), dictionary-level (comparable to our ispell feature), and POS information (comparable to our POS tagging). Their highest F-score for name identification is 95.2, slightly higher than our name identifier. However, it is difficult to compare the two sets of results since our tasks are slightly different. The goal of Baluja&apos;s research, and all other proper name identification research, is to identify all those wor</context>
</contexts>
<marker>Baluja, Mittal, Sukthankar, 1999</marker>
<rawString>S. Baluja, V. Mittal, and R.. Sukthankar. 1999. Applying machine learning for high performance named-entity extraction. In Proceedings of the Conference of the Pacific Association for Computational Linguistics, pages 365-378.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted text.</title>
<date>1988</date>
<booktitle>In Proceedings of the Second Conference on Applied Natural Language Processing,</booktitle>
<pages>136--143</pages>
<contexts>
<context position="8199" citStr="Church, 1988" startWordPosition="1344" endWordPosition="1345">on tree software. Indeed, the results we obtain could perhaps be improved by using more sophisticated decision-tree approaches such as the adaptiveresampling described in (Weiss et al, 1999). The features that we use to train the decision tree are intended to capture the characteristics of names. We specify a total of ten features for each unknown word. These identify two features of the unknown word itself as well as two features for each of the two preceding and two following words. The first feature represents the part of speech of the word. We use an in-house statistical tagger (based on (Church, 1988)) to tag the text in which the unknown word occurs. The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dictionary (OALD). The tag set contains just one tag to identify nouns. The second feature provides more informative tagging for specific parts of speech (these are referred to as &apos;detailed tags&apos; (DETAG)). This tagset consists of the nine tags listed in Table 1. All parts of speech apart from noun and punctuation tags are assigned the tag &apos;OTHER&apos;. All punctuation tags are assigned the tag &apos;BOUNDARY&apos;. Words identified as nou</context>
</contexts>
<marker>Church, 1988</marker>
<rawString>K. Church 1988. A stochastic parts program and noun phrase parser for unrestricted text. In Proceedings of the Second Conference on Applied Natural Language Processing, pages 136-143.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Damerau</author>
</authors>
<title>A technique for computer detection and correction of spelling errors.</title>
<date>1964</date>
<journal>Communications of the ACM,</journal>
<pages>7--171</pages>
<contexts>
<context position="12176" citStr="Damerau, 1964" startWordPosition="2004" endWordPosition="2005">6 million word corpus of business news closed captions. Word Length: (Agirre et al., 1998) note that their predictions for the correct spelling of misspelled words are more accurate for words longer than four characters, and much less accurate for shorter words. This observation can also be found in (Kukich, 1992). Our word length variables measures the number of characters in each word. Edit distance: Edit-distance is a metric for identifying the orthographic similarity of two words. Typically, one edit-distance corresponds to one substitution, deletion, reversal or addition of a character. (Damerau, 1964) observed that 80% of spelling errors in his data were just one edit-distance from the intended word. Similarly, (Mitton, 1987) found that 70% of his data was within one edit-distance from the intended word. Our edit distance feature represents the edit distance from the unknown word to the closest suggestion produced by the unix spell checker, ispell. If ispell does not produce any suggestions, an edit distance of thirty is assigned. In previous work we have experimented with more sophisticated distance measures. However, simple edit distance proved to be the most effective (Toole, 1999). Cha</context>
</contexts>
<marker>Damerau, 1964</marker>
<rawString>F. Damerau. 1964. A technique for computer detection and correction of spelling errors. Communications of the ACM, 7:171-176.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Elmi</author>
<author>M Evens</author>
</authors>
<title>Spelling correction using context.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>360--364</pages>
<contexts>
<context position="26558" citStr="Elmi and Evens, 1998" startWordPosition="4316" endWordPosition="4319">this paper may be inappropriate for a given language, we believe the general approach is transferable. In the case of a language such as Japanese, one would consider the means by which misspellings differ from their intended word and identify features to capture these differences. 6 Related Research There is little research that has focused on differentiating the different types of unknown words. For example, research on spelling error detection and correction for the most part assumes that all unknown words are misspellings and makes no attempt to identify other types of unknown words, e.g. (Elmi and Evens, 1998). Naturally, these are not appropriate comparisons for the work reported here. However, as is evident from the discussion above, previous spelling research does provide an important role in suggesting productive features to include in the decision tree. Research that is more similar in goal to that outlined in this paper is Vosse (Vosse, 1992). Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions. Hence, his sys</context>
</contexts>
<marker>Elmi, Evens, 1998</marker>
<rawString>M. Elmi and M. Evens. 1998. Spelling correction using context. In Proceedings of the 36th Annual Meeting of the ACL and the 17th International Conference on Computational Linguistics, pages 360-364.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Elworthy</author>
</authors>
<title>Language identification with confidence limits.</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th Workshop on Very large Corpora.</booktitle>
<contexts>
<context position="4487" citStr="Elworthy, 1998" startWordPosition="737" endWordPosition="738"> of differing categories. Some immediately obvious domains where unknown words are frequent include e-mail messages, internet chat rooms, data typed in by call centre operators, etc. To deal with these issues we propose a multicomponent architecture where individual components specialize in identifying one particular type of 173 unknown word. For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc. Each component will return a confidence measure of the reliability of its prediction, c.f. (Elworthy, 1998). The results from each component are evaluated to determine the final category of the word. There are several advantages to this approach. Firstly, the system can take advantage of existing research. For example, the name recognition module can make use of the considerable research that exists on name recognition, e.g. (McDonald, 1996), (Mani et al., 1996). Secondly, individual components can be replaced when improved models are available, without affecting other parts of the system. Thirdly, this approach is compatible with incorporating multiple components of the same type to improve perfor</context>
</contexts>
<marker>Elworthy, 1998</marker>
<rawString>D. Elworthy. 1998. Language identification with confidence limits. In Proceedings of the 6th Workshop on Very large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Granger</author>
</authors>
<title>The nomad system: expectationbased detection and correction of errors during understanding of syntactically and semantically illformed text.</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics,</journal>
<pages>9--188</pages>
<contexts>
<context position="27254" citStr="Granger, 1983" startWordPosition="4429" endWordPosition="4430">ever, as is evident from the discussion above, previous spelling research does provide an important role in suggesting productive features to include in the decision tree. Research that is more similar in goal to that outlined in this paper is Vosse (Vosse, 1992). Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions. Hence, his system would be ineffective on the closed caption domain with which we are working. (Granger, 1983) uses expectations generated by scripts to analyze unknown words. The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-shore messages. Research that is similar in technique to that reported here is (Baluja et al., 1999). Baluja and his colleagues use a decision tree classifier to identify proper names in text. They incorporate three types of features: word level (essentially utilizes case information), dictionary-level (comparable to our ispell feature), and POS infor</context>
</contexts>
<marker>Granger, 1983</marker>
<rawString>R. Granger. 1983. The nomad system: expectationbased detection and correction of errors during understanding of syntactically and semantically illformed text. American Journal of Computational Linguistics, 9:188-198.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hull</author>
<author>S Srihari</author>
</authors>
<title>Experiments in text recognition with binary n-gram and viterbi algorithms.</title>
<date>1982</date>
<journal>IEEE Trans. Patt. Anal. Machine Intell.</journal>
<volume>4</volume>
<pages>5--520</pages>
<contexts>
<context position="13099" citStr="Hull and Srihari, 1982" startWordPosition="2151" endWordPosition="2154">suggestion produced by the unix spell checker, ispell. If ispell does not produce any suggestions, an edit distance of thirty is assigned. In previous work we have experimented with more sophisticated distance measures. However, simple edit distance proved to be the most effective (Toole, 1999). Character sequence frequency: A characteristic of some misspellings is that they contain character sequences which are not typical of the language, e.g.tited, wful. Exploiting this information is a standard way of identifying spelling errors when using a dictionary is not desired or appropriate, e.g. (Hull and Srihari, 1982), (Zamora et al., 1981). To calculate our character sequence feature, we firstly determine the frequencies of the two least frequent character tri-gram sequences in the word in each of a selection of corpora. In previous work we included each of these values as individual features. However, the resulting trees were quite unstable as one feature would be relevant to one tree, whereas a different character sequence feature would be relevant to another tree. To avoid this problem, we developed a composite feature that is the sum of all individual character sequence frequencies. Non-English charac</context>
</contexts>
<marker>Hull, Srihari, 1982</marker>
<rawString>J. Hull and S. Srihari. 1982. Experiments in text recognition with binary n-gram and viterbi algorithms. IEEE Trans. Patt. Anal. Machine Intell. PAMI-4, 5:520-530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kukich</author>
</authors>
<title>Techniques for automatically correcting words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<pages>24--377</pages>
<contexts>
<context position="11877" citStr="Kukich, 1992" startWordPosition="1959" endWordPosition="1960">ates between misspellings and neologisms (new words) in terms of their frequency. His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms. Our corpus frequency variable specifies the frequency of each unknown word in a 2.6 million word corpus of business news closed captions. Word Length: (Agirre et al., 1998) note that their predictions for the correct spelling of misspelled words are more accurate for words longer than four characters, and much less accurate for shorter words. This observation can also be found in (Kukich, 1992). Our word length variables measures the number of characters in each word. Edit distance: Edit-distance is a metric for identifying the orthographic similarity of two words. Typically, one edit-distance corresponds to one substitution, deletion, reversal or addition of a character. (Damerau, 1964) observed that 80% of spelling errors in his data were just one edit-distance from the intended word. Similarly, (Mitton, 1987) found that 70% of his data was within one edit-distance from the intended word. Our edit distance feature represents the edit distance from the unknown word to the closest s</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>K.. Kukich. 1992. Techniques for automatically correcting words in text. ACM Computing Surveys, 24:377-439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Mani</author>
<author>R McMillan</author>
<author>S Luperfoy</author>
<author>E Lusher</author>
<author>S Laskowski</author>
</authors>
<title>Corpus Processing for Lexical Acquisition, chapter Identifying unknown proper names in newswire text.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4846" citStr="Mani et al., 1996" startWordPosition="793" endWordPosition="796">, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc. Each component will return a confidence measure of the reliability of its prediction, c.f. (Elworthy, 1998). The results from each component are evaluated to determine the final category of the word. There are several advantages to this approach. Firstly, the system can take advantage of existing research. For example, the name recognition module can make use of the considerable research that exists on name recognition, e.g. (McDonald, 1996), (Mani et al., 1996). Secondly, individual components can be replaced when improved models are available, without affecting other parts of the system. Thirdly, this approach is compatible with incorporating multiple components of the same type to improve performance (cf. (van HaIteren et al., 1998) who found that combining the results of several part of speech taggers increased performance). 3 The Current System In this paper we introduce a simplified version of the unknown word categorizer: one that contains just two components: misspelling identification and name identification. In this section we introduce the</context>
</contexts>
<marker>Mani, McMillan, Luperfoy, Lusher, Laskowski, 1996</marker>
<rawString>I. Mani, R. McMillan, S. Luperfoy, E. Lusher, and S. Laskowski, 1996. Corpus Processing for Lexical Acquisition, chapter Identifying unknown proper names in newswire text. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McDonald</author>
</authors>
<title>Corpus Processing for Lexical Acquisition, chapter Internal and external evidence in the identification and semantic categorization of proper names.</title>
<date>1996</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="4825" citStr="McDonald, 1996" startWordPosition="791" endWordPosition="792"> word. For example, the misspelling identifier will specialize in identifying misspellings, the abbreviation component will specialize in identifying abbreviations, etc. Each component will return a confidence measure of the reliability of its prediction, c.f. (Elworthy, 1998). The results from each component are evaluated to determine the final category of the word. There are several advantages to this approach. Firstly, the system can take advantage of existing research. For example, the name recognition module can make use of the considerable research that exists on name recognition, e.g. (McDonald, 1996), (Mani et al., 1996). Secondly, individual components can be replaced when improved models are available, without affecting other parts of the system. Thirdly, this approach is compatible with incorporating multiple components of the same type to improve performance (cf. (van HaIteren et al., 1998) who found that combining the results of several part of speech taggers increased performance). 3 The Current System In this paper we introduce a simplified version of the unknown word categorizer: one that contains just two components: misspelling identification and name identification. In this sec</context>
</contexts>
<marker>McDonald, 1996</marker>
<rawString>D. McDonald, 1996. Corpus Processing for Lexical Acquisition, chapter Internal and external evidence in the identification and semantic categorization of proper names. MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Min</author>
<author>W Wilson</author>
</authors>
<title>Integrated control of chart items for error repair.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics. K. Min.</booktitle>
<tech>Ph.D. thesis,</tech>
<institution>University of NSW,</institution>
<location>Sydney, Australia.</location>
<contexts>
<context position="1419" citStr="Min and Wilson, 1998" startWordPosition="231" endWordPosition="234">Processing (NLP) system will encounter words that are not in its lexicon, what we term &apos;unknown words&apos;. Unknown words are problematic because a NLP system will perform well only if it recognizes the words that it is meant to analyze or translate: the more words a system does not recognize the more the system&apos;s performance will degrade. Even when unknown words are infrequent, they can have a disproportionate effect on system quality. For example, Min (1996) found that while only 0.6% of words in 300 e-mails were misspelled, this meant that 12% of the sentences contained an error (discussed in (Min and Wilson, 1998)). Words may be unknown for many reasons: the word may be a proper name, a misspelling, an abbreviation, a number, a morphological variant of a known word (e.g. recleared), or missing from the dictionary. The first step in dealing with unknown words is to identify the class of the unknown word; whether it is a misspelling, a proper name, an abbreviation etc. Once this is known, the proper action can be taken, misspellings can be corrected, abbreviations can be expanded and so on, as deemed necessary by the particular text processing application. In this paper we introduce a system for categori</context>
</contexts>
<marker>Min, Wilson, 1998</marker>
<rawString>K. Min and W. Wilson. 1998. Integrated control of chart items for error repair. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and the 17th International Conference on Computational Linguistics. K. Min. 1996. Hierarchical Error Recovery Based on Bidirectional Chart Parsing Techniques. Ph.D. thesis, University of NSW, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Mitton</author>
</authors>
<title>Spelling checkers, spelling correctors, and the misspellings of poor spellers.</title>
<date>1987</date>
<journal>Inf. Process. Manage,</journal>
<pages>23--495</pages>
<contexts>
<context position="12303" citStr="Mitton, 1987" startWordPosition="2024" endWordPosition="2025">orrect spelling of misspelled words are more accurate for words longer than four characters, and much less accurate for shorter words. This observation can also be found in (Kukich, 1992). Our word length variables measures the number of characters in each word. Edit distance: Edit-distance is a metric for identifying the orthographic similarity of two words. Typically, one edit-distance corresponds to one substitution, deletion, reversal or addition of a character. (Damerau, 1964) observed that 80% of spelling errors in his data were just one edit-distance from the intended word. Similarly, (Mitton, 1987) found that 70% of his data was within one edit-distance from the intended word. Our edit distance feature represents the edit distance from the unknown word to the closest suggestion produced by the unix spell checker, ispell. If ispell does not produce any suggestions, an edit distance of thirty is assigned. In previous work we have experimented with more sophisticated distance measures. However, simple edit distance proved to be the most effective (Toole, 1999). Character sequence frequency: A characteristic of some misspellings is that they contain character sequences which are not typical</context>
</contexts>
<marker>Mitton, 1987</marker>
<rawString>R. Mitton. 1987. Spelling checkers, spelling correctors, and the misspellings of poor spellers. Inf. Process. Manage, 23:495-505.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Toole</author>
</authors>
<title>Categorizing Unknown Words: A decision tree-based misspelling identifier</title>
<date>1999</date>
<booktitle>Advanced Topics in _Artificial Intelligence,</booktitle>
<pages>122--133</pages>
<editor>In Foo, N (ed.)</editor>
<contexts>
<context position="12771" citStr="Toole, 1999" startWordPosition="2102" endWordPosition="2103">. (Damerau, 1964) observed that 80% of spelling errors in his data were just one edit-distance from the intended word. Similarly, (Mitton, 1987) found that 70% of his data was within one edit-distance from the intended word. Our edit distance feature represents the edit distance from the unknown word to the closest suggestion produced by the unix spell checker, ispell. If ispell does not produce any suggestions, an edit distance of thirty is assigned. In previous work we have experimented with more sophisticated distance measures. However, simple edit distance proved to be the most effective (Toole, 1999). Character sequence frequency: A characteristic of some misspellings is that they contain character sequences which are not typical of the language, e.g.tited, wful. Exploiting this information is a standard way of identifying spelling errors when using a dictionary is not desired or appropriate, e.g. (Hull and Srihari, 1982), (Zamora et al., 1981). To calculate our character sequence feature, we firstly determine the frequencies of the two least frequent character tri-gram sequences in the word in each of a selection of corpora. In previous work we included each of these values as individual</context>
</contexts>
<marker>Toole, 1999</marker>
<rawString>J. Toole 1999 Categorizing Unknown Words: A decision tree-based misspelling identifier In Foo, N (ed.) Advanced Topics in _Artificial Intelligence, pages 122-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H van Halteren</author>
<author>J Zavrel</author>
<author>W Daelemans</author>
</authors>
<title>Improving data driven word class tagging by system combination.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the ACL and the 17th International Conference on Computational Linguistics,</booktitle>
<pages>491--497</pages>
<marker>van Halteren, Zavrel, Daelemans, 1998</marker>
<rawString>H. van Halteren, J. Zavrel, and W. Daelemans. 1998. Improving data driven word class tagging by system combination. In Proceedings of the 36th Annual Meeting of the ACL and the 17th International Conference on Computational Linguistics, pages 491-497.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Vosse</author>
</authors>
<title>Detecting and correcting morphosyntactic errors in real texts.</title>
<date>1992</date>
<booktitle>In Proceedings of the 3rd Conference on Applied Natural Language Processing,</booktitle>
<pages>111--118</pages>
<contexts>
<context position="11253" citStr="Vosse, 1992" startWordPosition="1862" endWordPosition="1863"> additions, deletions, substitutions, or reversals of letters, or the exclusion of punctuation such as hyphenation or spacing. Like the definition of &apos;unknown word&apos;, the definition of a misspelling is also relative to a particular NLP system. Like the name identifier, we make use of a decision tree to capture the characteristics of misspellings. The features we use are derived from previous research, including our own previous research on misspelling identification. An abridged list of the features that are used in the training data is listed in Table 2 and discussed below. Corpus frequency: (Vosse, 1992) differentiates between misspellings and neologisms (new words) in terms of their frequency. His algorithm classifies unknown words that appear infrequently as misspellings, and those that appear more frequently as neologisms. Our corpus frequency variable specifies the frequency of each unknown word in a 2.6 million word corpus of business news closed captions. Word Length: (Agirre et al., 1998) note that their predictions for the correct spelling of misspelled words are more accurate for words longer than four characters, and much less accurate for shorter words. This observation can also be</context>
<context position="26903" citStr="Vosse, 1992" startWordPosition="4376" endWordPosition="4377">tiating the different types of unknown words. For example, research on spelling error detection and correction for the most part assumes that all unknown words are misspellings and makes no attempt to identify other types of unknown words, e.g. (Elmi and Evens, 1998). Naturally, these are not appropriate comparisons for the work reported here. However, as is evident from the discussion above, previous spelling research does provide an important role in suggesting productive features to include in the decision tree. Research that is more similar in goal to that outlined in this paper is Vosse (Vosse, 1992). Vosse uses a simple algorithm to identify three classes of unknown words: misspellings, neologisms, and names. Capitalization is his sole means of identifying names. However, capitalization information is not available in closed captions. Hence, his system would be ineffective on the closed caption domain with which we are working. (Granger, 1983) uses expectations generated by scripts to analyze unknown words. The drawback of his system is that it lacks portability since it incorporates scripts that make use of world knowledge of the situation being described; in this case, naval ship-to-sh</context>
</contexts>
<marker>Vosse, 1992</marker>
<rawString>T. Vosse. 1992. Detecting and correcting morphosyntactic errors in real texts. In Proceedings of the 3rd Conference on Applied Natural Language Processing, pages 111-118.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Weiss</author>
<author>N Indurkhya</author>
</authors>
<title>Predictive Data Mining.</title>
<date>1998</date>
<publisher>Morgan Kauffman Publishers.</publisher>
<contexts>
<context position="7129" citStr="Weiss and Indurkhya, 1998" startWordPosition="1160" endWordPosition="1163"> prediction. Some indication of confidence is required in the multicomponent architecture we have implemented. However, while currently existing name recognizers are inappropriate for the needs of our domain, future name recognizers may well meet these requirements and be able to be incorporated into the architecture we propose. For these reasons we develop our own name identifier. We utilize a decision tree to model the characteristics of proper names. The advantage of decision trees is that they are highly explainable: one can readily understand the features that are affecting the analysis (Weiss and Indurkhya, 1998). Furthermore, decision trees are well-suited for combining a wide variety of information. For this project, we made use of the decision tree that is part of IBM&apos;s Intelligent Miner suite for data mining. Since the point of this paper is to describe an application of decision trees rather than to argue for a particular decision tree algorithm, we omit further details of the decision tree software. Similar results should be obtained by using other decision tree software. Indeed, the results we obtain could perhaps be improved by using more sophisticated decision-tree approaches such as the adap</context>
</contexts>
<marker>Weiss, Indurkhya, 1998</marker>
<rawString>S. Weiss and N. Indurkhya. 1998. Predictive Data Mining. Morgan Kauffman Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Weiss</author>
<author>C Apte</author>
<author>F Damerau</author>
<author>D Johnson</author>
<author>F Oles</author>
<author>T Goetz</author>
<author>T Hampp</author>
</authors>
<title>Maximizing text-mining performance.</title>
<date>1999</date>
<journal>IEEE Intelligent Systems and their Applications,14(4):63-69</journal>
<contexts>
<context position="7776" citStr="Weiss et al, 1999" startWordPosition="1268" endWordPosition="1271"> are well-suited for combining a wide variety of information. For this project, we made use of the decision tree that is part of IBM&apos;s Intelligent Miner suite for data mining. Since the point of this paper is to describe an application of decision trees rather than to argue for a particular decision tree algorithm, we omit further details of the decision tree software. Similar results should be obtained by using other decision tree software. Indeed, the results we obtain could perhaps be improved by using more sophisticated decision-tree approaches such as the adaptiveresampling described in (Weiss et al, 1999). The features that we use to train the decision tree are intended to capture the characteristics of names. We specify a total of ten features for each unknown word. These identify two features of the unknown word itself as well as two features for each of the two preceding and two following words. The first feature represents the part of speech of the word. We use an in-house statistical tagger (based on (Church, 1988)) to tag the text in which the unknown word occurs. The tag set used is a simplified version of the tags used in the machinereadable version of the Oxford Advanced Learners Dict</context>
</contexts>
<marker>Weiss, Apte, Damerau, Johnson, Oles, Goetz, Hampp, 1999</marker>
<rawString>S. Weiss, and C. Apte, and F. Damerau, and D. Johnson, and F. Oles and T. Goetz, and T. Hampp. 1999 Maximizing text-mining performance. IEEE Intelligent Systems and their Applications,14(4):63-69</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Zamora</author>
<author>J Pollock</author>
<author>A Zamora</author>
</authors>
<title>The use of tri-gram analysis for spelling error detection.</title>
<date>1981</date>
<journal>Inf. Process. Manage.,</journal>
<pages>17--305</pages>
<contexts>
<context position="13122" citStr="Zamora et al., 1981" startWordPosition="2155" endWordPosition="2158"> unix spell checker, ispell. If ispell does not produce any suggestions, an edit distance of thirty is assigned. In previous work we have experimented with more sophisticated distance measures. However, simple edit distance proved to be the most effective (Toole, 1999). Character sequence frequency: A characteristic of some misspellings is that they contain character sequences which are not typical of the language, e.g.tited, wful. Exploiting this information is a standard way of identifying spelling errors when using a dictionary is not desired or appropriate, e.g. (Hull and Srihari, 1982), (Zamora et al., 1981). To calculate our character sequence feature, we firstly determine the frequencies of the two least frequent character tri-gram sequences in the word in each of a selection of corpora. In previous work we included each of these values as individual features. However, the resulting trees were quite unstable as one feature would be relevant to one tree, whereas a different character sequence feature would be relevant to another tree. To avoid this problem, we developed a composite feature that is the sum of all individual character sequence frequencies. Non-English characters: This binary featu</context>
</contexts>
<marker>Zamora, Pollock, Zamora, 1981</marker>
<rawString>E. Zamora, J. Pollock, and A. Zamora. 1981. The use of tri-gram analysis for spelling error detection. Inf. Process. Manage., 17:305-316.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>