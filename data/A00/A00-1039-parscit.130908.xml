<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998157">
Unsupervised Discovery of Scenario-Level Patterns for
Information Extraction
</title>
<author confidence="0.885323">
Roman Yangarber
</author>
<affiliation confidence="0.58717425">
romanOcs.nyu.edu
Courant Institute of
Mathematical Sciences
New York University
</affiliation>
<author confidence="0.698004">
Ralph Grishman
</author>
<affiliation confidence="0.56269775">
grishmanOcs.nyu.edu
Courant Institute of
Mathematical Sciences
New York University
</affiliation>
<note confidence="0.2365865">
Pasi Tapanainen t Silja Huttunen
tapanainOconexor.fi sihuttunOling.helsinki.fi
Conexor Oy University of Helsinki
Helsinki, Finland Finland
</note>
<sectionHeader confidence="0.972441" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999608923076923">
Information Extraction (1E) systems are com-
monly based on pattern matching. Adapting
an IE system to a new scenario entails the
construction of a new pattern base—a time-
consuming and expensive process. We have
implemented a system for finding patterns au-
tomatically from un-annotated text. Starting
with a small initial set of seed patterns proposed
by the user, the system applies an incremental
discovery procedure to identify new patterns.
We present experiments with evaluations which
show that the resulting patterns exhibit high
precision and recall.
</bodyText>
<sectionHeader confidence="0.997302" genericHeader="introduction">
0 Introduction
</sectionHeader>
<bodyText confidence="0.99953619047619">
The task of Information Extraction (1E) is
the selective extraction of meaning from free
natural language text.1 &amp;quot;Meaning&amp;quot; is under-
stood here in terms of a fixed set of semantic
objects—entities, relationships among entities,
and events in which entities participate. The
semantic objects belong to a small number of
types, all having fixed regular structure, within
a fixed and closely circumscribed subject do-
main. The extracted objects are then stored in
a relational database. In this paper, we use the
nomenclature accepted in current IE literature;
the term subject domain denotes a class of tex-
tual documents to be processed, e.g., &amp;quot;business
news,&amp;quot; and scenario denotes the specific topic
of interest within the domain, i.e., the set of
facts to be extracted. One example of a sce-
nario is &amp;quot;management succession,&amp;quot; the topic of
MUC-6 (the Sixth Message Understanding Con-
ference); in this scenario the system seeks to
identify events in which corporate managers left
</bodyText>
<footnote confidence="0.9212985">
1For general references on IE, cf., e.g., (Pazienza,
1997; muc, 1995; muc, 1993).
</footnote>
<bodyText confidence="0.999022318181818">
their posts or assumed new ones. We will con-
sider this scenario in detail in a later section
describing experiments.
IE systems today are commonly based on pat-
tern matching. The patterns are regular ex-
pressions, stored in a &amp;quot;pattern base&amp;quot; containing
a general-purpose component and a substantial
domain- and scenario-specific component.
Portability and performance are two major
problem areas which are recognized as imped-
ing widespread use of IE. This paper presents a
novel approach, which addresses both of these
problems by automatically discovering good
patterns for a new scenario. The viability of
our approach is tested and evaluated with an
actual IE system.
In the next section we describe the problem in
more detail in the context of our IE system; sec-
tions 2 and 3 describe our algorithm for pattern
discovery; section 4 describes our experimental
results, followed by comparison with prior work
and discussion, in section 5.
</bodyText>
<sectionHeader confidence="0.97393" genericHeader="method">
1 The IE System
</sectionHeader>
<bodyText confidence="0.998220125">
Our IE system, among others, contains a a back-
end core engine, at the heart of which is a
regular-expression pattern matcher. The engine
draws on attendant knowledge bases (KBs) of
varying degrees of domain-specificity. The KB
components are commonly factored out to make
the systems portable to new scenarios. There
are four customizable knowledge bases in our IE
system: the Lexicon contains general dictionar-
ies and scenario-specific terms; the concept base
groups terms into classes; the predicate base de-
scribes the logical structure of events to be ex-
tracted, and the pattern base contains patterns
that catch the events in text.
Each KB has a. substantial domain-specific
component, which must be modified when mov-
</bodyText>
<page confidence="0.994322">
282
</page>
<bodyText confidence="0.99979864516129">
ing to new domains and scenarios. The system
allows the user (i.e. scenario developer) to start
with example sentences in text which contain
events of interest, the candidates, and general-
ize them into patterns. However, the user is
ultimately responsible for finding all the can-
didates, which amounts to manually processing
example sentences in a very large training cor-
pus. Should s/he fail to provide an example
of a particular class of syntactic/semantic con-
struction, the system has no hope of recovering
the corresponding events. Our experience has
shown that (1) the process of discovering candi-
dates is highly expensive, and (2) gaps in pat-
terns directly translate into gaps in coverage.
How can the system help automate the pro-
cess of discovering new good candidates? The
system should find examples of all common lin-
guistic constructs relevant to a scenario. While
there has been prior research on identifying the
primary lexical patterns of a sub-language or
corpus (Grishman et al., 1986; Riloff, 1996), the
task here is more complex, since we are typi-
cally not provided in advance with a sub-corpus
of relevant passages; these passages must them-
selves be found as part of the discovery process.
The difficulty is that one of the best indications
of the relevance of the passages is precisely the
presence of these constructs. Because of this
circularity, we propose to acquire the constructs
and passages in tandem.
</bodyText>
<sectionHeader confidence="0.942883" genericHeader="method">
2 Solution
</sectionHeader>
<bodyText confidence="0.915424342857143">
We outline our procedure for automatic ac-
quisition of patterns; details are elaborated in
later sections. The procedure is unsupervised
in that it does not require the training corpus
to be manually annotated with events of inter-
est, nor a pre-classified corpus with relevance
judgements, nor any feedback or intervention
from the user2. The idea is to combine IR-style
document selection with an iterative relaxation
process; this is similar to techniques used else-
where in NLP, and is inspired in large part, if
remotely, by the work of (Kay and Roscheisen,
1993) on automatic alignment of sentences and
words in a bilingual corpus. There, the reason-
ing was: sentences that are translations of each
2however, it may be supervised after each iteration,
where the user can answer yes/no questions to improve
the quality of the results
other are good indicators that words they con-
tain are translation pairs; conversely, words that
are translation pairs indicate that the sentences
which contain them correspond to one another.
In our context, we observe that documents
that are relevant to the scenario will neces-
sarily contain good patterns; conversely, good
patterns are strong indicators of relevant docu-
ments. The outline of our approach is as follows.
0. Given: (1) a large corpus of un-annotated
and un-classified documents in the domain;
(2) an initial set of trusted scenario pat-
terns, as chosen ad hoc by the user—the
seed; as will be seen, the seed can be quite
small—two or three patterns seem to suf-
fice. (3) an initial (possibly empty) set of
concept classes
</bodyText>
<listItem confidence="0.995026619047619">
1. The pattern set induces a binary partition
(a split) on the corpus: on any document,
either zero or more than zero patterns will
match. Thus the universe of documents, U,
is partitioned into the relevant sub-corpus,
R, vs. the non-relevant sub-corpus, R =
U — R, with respect to the given pattern
set. Actually, the documents are assigned
weights which are 1 for documents matched
by the trusted seed, and 0 otherwise.3
2. Search for new candidate patterns:
(a) Automatically convert each sentence
in the corpus, into a set of candidate
patterns.4
(b) Generalize each pattern by replacing
each lexical item which is a member of
a concept class by the class name.
(c) Working from the relevant documents,
select those patterns whose distribu-
tion is strongly correlated with other
relevant documents (i.e., much more
</listItem>
<bodyText confidence="0.9627362">
3R represents the trusted truth through the discovery
iterations, since it was induced by the manually-selected
seed.
4Here, for each clause in the sentence we extract a
tuple of its major roles: the head of the subject, the
verb group, the object, object complement, as described
below. This tuple is considered to be a pattern for the
present purposes of discovery; it is a skeleton for the
rich, syntactically transformed patterns our system uses
in the extraction phase.
</bodyText>
<page confidence="0.989687">
283
</page>
<bodyText confidence="0.9998426">
densely distributed among the rele-
vant documents than among the non-
relevant ones). The idea is to consider
those candidate patterns, p, which
meet the density, criterion:
</bodyText>
<equation confidence="0.2935885">
IH n RI IRI
IH n Ul
</equation>
<bodyText confidence="0.9479322">
where H = H(p) is the set of docu-
ments where p hits.
(d) Based on co-occurrence with the cho-
sen patterns, extend the concept
classes.
</bodyText>
<listItem confidence="0.932522333333333">
3. Optional: Present the new candidates and
classes to the user for review, retaining
those relevant to the scenario.
</listItem>
<bodyText confidence="0.96306825">
4. The new pattern set induces a new parti-
tion on the corpus. With this pattern set,
return to step 1. Repeat the procedure un-
til no more patterns can be added.
</bodyText>
<sectionHeader confidence="0.998622" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.995472">
3.1 Pre-processing: Normalization
</subsectionHeader>
<bodyText confidence="0.999997">
Before applying the discovery procedure, we
subject the corpus to several stages of pre-
processing. First, we apply a name recognition
module, and replace each name with a token
describing its class, e.g. C-Person, C-Company,
etc. We collapse together all numeric expres-
sions, currency values, dates, etc., using a single
token to designate each of these classes.
</bodyText>
<subsectionHeader confidence="0.999709">
3.2 Syntactic Analysis
</subsectionHeader>
<bodyText confidence="0.9447585625">
We then apply a parser to perform syntactic
normalization to transform each clause into a
common predicate-argument structure. We use
the general-purpose dependency parser of En-
glish, based on the FDG formalism (Tapanainen
and Jarvinen, 1997) and developed by the Re-
search Unit for Multilingual Language Technol-
ogy at the University of Helsinki, and Conexor
Oy. The parser (modified to understand the
name labels attached in the previous step) is
used for reducing such variants as passive and
relative clauses to a tuple, consisting of several
elements.
1. For each claus, the first element is the sub-
ject, a &amp;quot;semantic&amp;quot; subject of a non-finite
sentence or agent of the passive.5
</bodyText>
<listItem confidence="0.877217125">
2. The second element is the verb.
3. The third element is the object, certain
object-like adverbs, subject of the passive
or subject complement6
4. The fourth element is a phrase which
refers to the object or the subject. A
typical example of such an argument is
an object complement, such as Com-
</listItem>
<bodyText confidence="0.927990076923077">
pany named John Smith president. An-
other instance is the so-called copredica-
tive (Nichols, 1978), in the parsing system
(Jarvinen and Tapanainen, 1997). A co-
predicative refers to a subject or an object,
though this distinction is typically difficult
to resolve automatically.7
Clausal tuples also contain a locative modifier,
and a temporal modifier. We used a corpus of
5,963 articles from the Wall Street Journal, ran-
domly chosen. The parsed articles yielded a to-
tal of 250,000 clausal tuples, of which 135,000
were distinct.
</bodyText>
<subsectionHeader confidence="0.995913">
3.3 Generalization and Concept Classes
</subsectionHeader>
<bodyText confidence="0.999927">
Because tuples may not repeat with sufficient
frequency to obtain reliable statistics, each tu-
ple is reduced to a set of pairs: e.g., a verb-
object pair, a subject-object pair, etc. Each
pair is used as a generalized pattern during
the candidate selection stage. Once we have
identified pairs which are relevant to the sce-
nario, we use them to construct or augment con-
cept classes, by grouping together the missing
roles, (for example, a class of verbs which oc-
cur with a relevant subject-object pair: &amp;quot;com-
pany {hire/fire/expel...} person&amp;quot;). This is sim-
ilar to work by several other groups which
aims to induce semantic classes through syn-
tactic co-occurrence analysis (Riloff and Jones,
1999; Pereira et al., 1993; Dagan et al., 1993;
Hirschman et al., 1975), although in our case
the contexts are limited to selected patterns,
relevant to the scenario.
</bodyText>
<footnote confidence="0.609183375">
5E.g., &amp;quot;John sleeps&amp;quot;, &amp;quot;John is appointed by
Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;, &amp;quot;She asked
John to buy a car&amp;quot;.
SE.g., &amp;quot;John is appointed by Company&amp;quot;, &amp;quot;John is the
president of Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;,
Th—e—a73-i-Which I saw sleeps.
7For example, &amp;quot;She gave us our coffee black&amp;quot;, &amp;quot;Com-
pany appointed John Smith as president&amp;quot;.
</footnote>
<page confidence="0.989952">
284
</page>
<subsectionHeader confidence="0.973817">
3.4 Pattern Discovery
</subsectionHeader>
<bodyText confidence="0.999615">
Here we present the results from experiments
we conducted on the MUC-6 scenario, &amp;quot;man-
agement succession&amp;quot;. The discovery procedure
was seeded with a small pattern set, namely:
</bodyText>
<table confidence="0.980535333333333">
Subject Verb Direct Object
C-Company C-Appoint C-Person
C-Person C-Resign —
</table>
<bodyText confidence="0.97154275">
Here C-Company and C-Person denote se-
mantic classes containing named entities of the
corresponding semantic types. C-Appoint de-
notes a class of verbs, containing four verbs
{ appoint, elect, promote, name}; C-Resign =
{ resign, depart, quit, step-down }.
During a single iteration, we compute the
score8, L(p), for each candidate pattern p:
</bodyText>
<equation confidence="0.989088">
L(p) Pc(p) • log 11/ n RI (1)
</equation>
<bodyText confidence="0.999093">
where R denotes the relevant subset, and H =
H(p) the documents matching p, as above, and
</bodyText>
<equation confidence="0.961539">
P(p) = Lurif I • s
</equation>
<bodyText confidence="0.9832077">
!Hi the conditional probability of
relevance. We further impose two support cri-
teria: we distrust such frequent patterns where
11-/- n tfl &gt; 411 as uninformative, and rare pat-
terns for which if n RI &lt; 13 as noise.9 At the
end of each iteration, the system selects the pat-
tern with the highest score, L(p), and adds it to
the seed set. The documents which the winning
pattern hits are added to the relevant set. The
pattern search is then restarted.
</bodyText>
<subsectionHeader confidence="0.6894905">
3.5 Re-computation of Document
Relevance
</subsectionHeader>
<bodyText confidence="0.999775166666667">
The above is a simplification of the actual pro-
cedure, in several important respects.
Only generalized patterns are considered for
candidacy, with one or more slots filled with
wild-cards. In computing the score of the gen-
eralized pattern, we do not take into considera-
tion all possible values of the wild-card role. We
instead constrain the wild-card to those values
which themselves in turn produce patterns with
high scores. These values then become members
of a new class, which is output in tandem with
the winning patternl°
</bodyText>
<footnote confidence="0.9771498">
8similarly to (Riloff, 1996)
9U denotes the universe of documents. We used a =
0.1 and )3 =2.
19The classes are currently unused by subsequent iter-
ations; this important issue is considered in future work.
</footnote>
<bodyText confidence="0.999008111111111">
Documents are assigned relevance scores on
a scale between 0 and 1. The seed patterns
are accepted as ground truth; thus the docu-
ments they match have relevance 1. On sub-
sequent iterations, the newly accepted patterns
are not trusted as absolutely. On iteration num-
ber i + 1, each pattern p is assigned a precision
measure, based on the relevance of the docu-
ments it matches:
</bodyText>
<equation confidence="0.958585333333333">
1 E i(d) (2)
preci Rel
(d) IH(p) I dElf(12)
</equation>
<bodyText confidence="0.999711571428571">
where Rel(d) is the relevance of the document
from the previous iteration, and H(p) is the set
of documents where p matched. More generally,
if K is a classifier consisting of a set of patterns,
we can define H(K) as the set of documents
where all of patterns p E K match, and the
&amp;quot;cumulative&amp;quot; precisionll of K as
</bodyText>
<equation confidence="0.998426666666667">
1
E Relz(d) (3)
Preci±l(K) 1H(K dEH(K)
</equation>
<bodyText confidence="0.999984125">
Once the new winning pattern is accepted,
the relevance scores of the documents are re-
adjusted as follows. For each document d which
is matched by some (non-empty) subset of the
currently accepted patterns, we can view that
subset of patterns as a classifier Kd =
These patterns determine the new relevance
score of the document is
</bodyText>
<equation confidence="0.956903">
Reli+1(d) = max (Reli (d), Preci+1(Kd)) (4)
</equation>
<bodyText confidence="0.999471416666667">
This ensures that the relevance score grows
monotonically, and only when there is sufficient
positive evidence, as the patterns in effect vote
&amp;quot;conjunctively&amp;quot; on the documents. The results
which follow use this measure.
Thus in the formulas above, R is not sim-
ply the count of the relevant documents, but
is rather their cumulative relevance. The two
formulas, (3) and (4), capture the mutual de-
pendency of patterns and documents; this re-
computation and growing of precision and rele-
vance scores is at the heart of the procedure.
</bodyText>
<footnote confidence="0.9801905">
110f course, this measure is defined only when
H(K) O.
</footnote>
<page confidence="0.995518">
285
</page>
<figure confidence="0.849764">
4 Results 1
0 10 20 30 40 SO 60 70 80
Generation #
</figure>
<bodyText confidence="0.999856434782609">
An objective measure of goodness of a pattern
is not trivial to establish since the patterns can-
not be used for extraction directly, without be-
ing properly incorporated into the knowledge
base. Thus, the discovery procedure does not
lend itself easily to MUC-style evaluations, since
a pattern lacks information about which events
it induces and which slots its arguments should
fill.
However, it is possible to apply some objec-
tive measures of performance. One way we eval-
uated the system is by noting that in addition
to growing the pattern set, the procedure also
grows the relevance of documents. The latter
can be objectively evaluated.
We used a test corpus of 100 MUC-6 formal-
training documents (which were included in the
main development corpus of about 6000 docu-
ments) plus another 150 documents picked at
random from the main corpus and judged by
hand. These judgements constituted the ground
truth and were used only for evaluation, (not in
the discovery procedure).
</bodyText>
<subsectionHeader confidence="0.991374">
4.1 Text Filtering
</subsectionHeader>
<bodyText confidence="0.997669043478261">
Figure 1 shows the recall/precision measures
with respect to the test corpus of 250 docu-
ments, over a span of 60 generations, starting
with the seed set in table 3.4. The seed pat-
terns matched 184 of the 5963 documents, yield-
ing an initial recall of .11 and precision of .93;
by the last generation it searched through 982
documents with non-zero relevance, and ended
with .80 precision and .78 recall. This facet of
the discovery procedure is closely related to the
MUC &amp;quot;text-filtering&amp;quot; sub-task, where the sys-
tems are judged at the level of documents rather
than event slots. It is interesting to compare the
results with other MUC-6 participants, shown
anonymously in figure 2. Considering recall and
precision separately, the discovery procedure at-
tains values comparable to those achieved by
some of the participants, all of which were ei-
ther heavily-supervised or manually coded sys-
tems. It is important to bear in mind that the
discovery procedure had no benefit of training
material, or any information beyond the seed
pattern set.
</bodyText>
<figureCaption confidence="0.9836055">
Figure 1: Recall/Precision curves for Manage-
ment Succession
</figureCaption>
<subsectionHeader confidence="0.992925">
4.2 Choice of Test Corpus
</subsectionHeader>
<bodyText confidence="0.999874272727273">
Figure 2 shows two evaluations of our discovery
procedure, tested against the original MUC-6
corpus of 100 documents, and against our test
corpus, which consists of an additional 150 doc-
uments judged manually. The two plots in the
figure show a slight difference in results, indi-
cating that in some sense, the MUC corpus was
more &amp;quot;random&amp;quot;, or that our expanded corpus
was somewhat skewed in favor of more common
patterns that the system is able to find more
easily.
</bodyText>
<subsectionHeader confidence="0.999911">
4.3 Choice of Evaluation Metric
</subsectionHeader>
<bodyText confidence="0.999960083333333">
The graphs shown in Figures 1 and 2 are based
on an &amp;quot;objective&amp;quot; measure we adopted during
the experiments. This is the same measure of
relevance used internally by the discovery proce-
dure on each iteration (relative to the &amp;quot;truth&amp;quot; of
relevance scores of the previous iteration), and
is not quite the standard measure used for text
filtering in IR. According to this measure, the
system gets a score for each document based on
the relevance which it assigned to the document.
Thus if the system -assigned relevance of X per-
cent to a relevant document, it only received X
</bodyText>
<figure confidence="0.984496833333333">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
286
0.4 0.4
0 0.10.20.30.40.50.60.70.80.9 1 0 0.10.20.30.40.50.60.70.80.9 1
O.9
O.8
O.6
O.7
O.5
1
1
0.9
0.8
0.6
0.5
Recall
</figure>
<figureCaption confidence="0.999963">
Figure 2: Precision vs. Recall
</figureCaption>
<bodyText confidence="0.999380733333333">
percent on the recall score for classifying that
document correctly. Similarly, if the system as-
signed relevance Y to an irrelevant document,
it was penalized only for the mis-classified Y
percent on the precision score. To make our re-
sults more comparable to those of other MUC
competitors, we chose a cut-off point and force
the system to make a binary relevance decision
on each document. The cut-off of 0.5 seemed
optimal from empirical observations. Figure 3
shows a noticeable improvement in scores, when
using our continuous, &amp;quot;objective&amp;quot; measure, vs.
the cut-off measure, with the entire graph essen-
tially translated to the right for a gain of almost
10 percentage points of recall.
</bodyText>
<subsectionHeader confidence="0.99952">
4.4 Evaluating Patterns
</subsectionHeader>
<bodyText confidence="0.9997505">
Another effective, if simple, measure of perfor-
mance is how many of the patterns the pro-
cedure found, and comparing them with those
used by an extraction engine which was manu-
ally constructed for the same task. Our MUC-6
system used approximately 75 clause level pat-
terns, with 30 distinct verbal heads. In one
conservative experiment, we observed that the
discovery procedure found 17 of these verbs, or
57%. However, it also found at least 8 verbs the
</bodyText>
<figure confidence="0.565023">
Recall
</figure>
<figureCaption confidence="0.99777">
Figure 3: Results on the MUC corpus
</figureCaption>
<bodyText confidence="0.6566675">
manual system lacked, which seemed relevant to
the scenario:
</bodyText>
<equation confidence="0.6332846">
company-bring-person- [as + officerp2
person-come- [to+ company]as + officer]
person-rejoin-company- [as + officer]
person_{ return, continue,remain,stay} -[as+ officer]
person-pursue-interest
</equation>
<bodyText confidence="0.999595529411765">
At the risk of igniting a philosophical de-
bate over what is or is not relevant to a sce-
nario, we note that the first four of these verbs
are evidently essential to the scenario in the
strictest definition, since they imply changes of
post. The next three are &amp;quot;staying&amp;quot; verbs, and
are actually also needed, since higher-level infer-
ences required in tracking events for long-range
merging over documents, require knowledge of
persons occupying posts, rather than only as-
suming or leaving them. The most curious one
is &amp;quot;person-pursue-interest&amp;quot;; surprisingly, it too
is useful, even in the strictest MUC sense, cf.,
(muc, 1995). Systems are judged on filling a
slot called &amp;quot;other-organization&amp;quot;, indicating from
or to which company the person came or went.
This pattern is consistently used in text to indi-
</bodyText>
<footnote confidence="0.750763">
12bracketed constituents are outside of the central
SVO triplet, included here for clarity.
</footnote>
<page confidence="0.993325">
287
</page>
<bodyText confidence="0.999939">
cate that the person left to pursue other, undis-
closed interests, the knowledge of which would
relieve the system from seeking other informa-
tion in order to fill this slot. This is to say that
here strict evaluation is elusive.
</bodyText>
<sectionHeader confidence="0.903447" genericHeader="conclusions">
5 Discussion and Current Work
</sectionHeader>
<figureCaption confidence="0.630409916666667">
Some of the prior research has emphasized in-
teractive tools to convert examples to extraction
patterns, cf. (Yangarber and Grishman, 1997),
while others have focused on methods for au-
tomatically converting a corpus annotated with•
extraction examples into such patterns (Lehn-
ert et al., 1992; Fisher et al., 1995; Miller et
al., 1998). These methods, however, do not re-
duce the burden of finding the examples to an-
notate. With either approach, the portability
bottleneck is shifted from the problem of build-
ing patterns to that of finding good candidates.
</figureCaption>
<bodyText confidence="0.999955777777778">
The prior work most closely related to this
study is (Riloff, 1996), which, along with (Riloff,
1993), seeks automatic methods for filling slots
in event templates. However, the prior work
differs from that presented here in several cru-
cial respects; firstly, the prior work does not at-
tempt to find entire events, after the fashion
of MUC&apos;s highest-level scenario-template task.
Rather the patterns produced by those systems
identify NPs that fill individual slots, without
specifying how these slots may be combined
at a later stage into complete event templates.
The present work focuses on directly discovering
event-level, multi-slot relational patterns. Sec-
ondly, the prior work either relies on a set of
documents with relevance judgements to find
slot fillers where they are relevant to events,
(Riloff, 1996), or utilizes an un-classified cor-
pus containing a very high proportion of rele-
vant documents to find all instances of a seman-
tic class, (Riloff and Jones, 1999). By contrast,
our procedure requires no relevance judgements,
and works on the assumption that the corpus is
balanced and the proportion of relevant docu-
ments is small. Classifying documents by hand,
although admittedly easier than tagging event
instances in text for automatic training, is still
a formidable task. When we prepared the test
corpus, it took 5 hours to mark 150 short doc-
uments.
The presented results indicate that our
method of corpus analysis can be used to rapidly
identify a large number of relevant patterns
without pre-classifying a large training corpus.
We are at the early stages of understanding
how to optimally tune these techniques, and
there are number of areas that need refinement.
We are working on capturing the rich informa-
tion about concept classes which is currently re-
turned as part of our pattern discovery proce-
dure, to build up a concept dictionary in tandem
with the pattern base. We are also consider-
ing the proper selection of weights and thresh-
olds for controlling the rankings of patterns and
documents, criteria for terminating the itera-
tion process, and for dynamic adjustments of
these weights. We feel that the generalization
technique in pattern discovery offers a great
opportunity for combating sparseness of data,
though this requires further research. Lastly,
we are studying these algorithms under several
unrelated scenarios to determine to what extent
scenario-specific phenomena affect their perfor-
mance.
</bodyText>
<sectionHeader confidence="0.996312" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998464115384615">
Ido Dagan, Shaul Marcus, and Shaul
Markovitch. 1993. Contextual word simi-
larity and estimation from sparse data. In
Proceedings of the 31st Annual Meeting of
the Assn. for Computational Linguistics,
pages 31-37, Columbus, OH, June.
David Fisher, Stephen Soderland, Joseph Mc-
Carthy, Fangfang Feng, and Wendy Lehnert.
1995. Description of the UMass system as
used for MUC-6. In Proc. Sixth Message Un-
derstanding Conf. (MUC-6), Columbia, MD,
November. Morgan Kaufmann.
R. Grishman, L. Hirschman, and N.T. Nhan.
1986. Discovery procedures for sublanguage
selectional patterns: Initial experiments.
Computational Linguistics, 12(3):205-16.
Lynette Hirschman, Ralph Grishman, and
Naomi Sager. 1975. Grammatically-based
automatic word class formation. Information
Processing and Management, 11(1/2):39-57.
Timo Jarvinen and Pasi Tapanainen. 1997. A
dependency parser for English. Technical Re-
port TR-1, Department of General Linguis-
tics, University of Helsinki, Finland, Febru-
ary.
Martin Kay and Martin Ri5scheisen. 1993.
</reference>
<page confidence="0.96839">
288
</page>
<reference confidence="0.999742064516129">
Text-translation alignment. Computational
Linguistics, 19(1).
W. Lehnert, C. Cardie, D. Fisher, J. McCarthy,
E. Riloff, and S. Soderland. 1992. Univer-
sity of massachusetts: MUC-4 test results
and analysis. In Proc. Fourth Message Un-
derstanding Conf., McLean, VA, June. Mor-
gan Kaufmann.
Scott Miller, Michael Crystal, Heidi Fox,
Lance Ramshaw, Richard Schwartz, Rebecca
Stone, Ralph Weischedel, and the Annota-
tion Group. 1998. Algorithms that learn to
extract information; BBN: Description of the
SIFT system as used for MUC-7. In Proc. of
the Seventh Message Understanding Confer-
ence, Fairfax, VA.
1993. Proceedings of the Fifth Message Un-
derstanding Conference (MUG- 5), Baltimore,
MD, August. Morgan Kaufmann.
1995. Proceedings of the Sixth Message Un-
derstanding Conference (MUC-6), Columbia,
MD, November. Morgan Kaufmann.
Johanna Nichols. 1978. Secondary predicates.
Proceedings of the 4th Annual Meeting of
Berkeley Linguistics Society, pages 114-127.
Maria Teresa Pazienza, editor. 1997. Infor-
mation Extraction. Springer-Verlag, Lecture
Notes in Artificial Intelligence, Rome.
Fernando Pereira, Naftali Tishby, and Lillian
Lee. 1993. Distributional clustering of En-
glish words. In Proceedings of the 31st An-
nual Meeting of the Assn. for Computational
Linguistics, pages 183-190, Columbus, OH,
June.
Ellen Riloff and Rosie Jones. 1999. Learn-
ing dictionaries for information extraction by
multi-level bootstrapping. In Proceedings of
Sixteenth National Conference on Artificial
Intelligence (AAAI-99), Orlando, Florida.
Ellen Riloff. 1993. Automatically construct-
ing a dictionary for information extraction
tasks. In Proceedings of Eleventh National
Conference on Artificial Intelligence (AAAI-
93), pages 811-816. The AAAI Press/MIT
Press.
Ellen Riloff. 1996. Automatically generating
extraction patterns from untagged text. In
Proceedings of Thirteenth National Confer-
ence on Artificial Intelligence (AAAI-96),
pages 1044-1049. The AAAI Press/MIT
Press.
Pasi Tapanainen and Timo Jarvinen. 1997. A
non-projective dependency parser. In Pro-
ceedings of the 5th Conference on Applied
Natural Language Processing, pages 64-71,
Washington, D.C., April. ACL.
Roman Yangarber and Ralph Grishman. 1997.
Customization of information extraction sys-
tems. In Paola Velardi, editor, International
Workshop on Lexically Driven Information
Extraction, pages 1-11, Frascati, Italy, July.
Universita di Roma.
</reference>
<page confidence="0.998642">
289
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.382582">
<title confidence="0.999639">Unsupervised Discovery of Scenario-Level Patterns for Information Extraction</title>
<author confidence="0.999976">Roman Yangarber</author>
<email confidence="0.998833">romanOcs.nyu.edu</email>
<affiliation confidence="0.963291666666667">Courant Institute of Mathematical Sciences New York University</affiliation>
<author confidence="0.999767">Ralph Grishman</author>
<email confidence="0.999167">grishmanOcs.nyu.edu</email>
<affiliation confidence="0.956065">Courant Institute of Mathematical Sciences New York University</affiliation>
<author confidence="0.8174985">Pasi Tapanainen t Silja Huttunen tapanainOconexor fi sihuttunOling helsinki fi</author>
<affiliation confidence="0.999729">Oy University Helsinki</affiliation>
<address confidence="0.714044">Helsinki, Finland Finland</address>
<abstract confidence="0.997405714285714">Information Extraction (1E) systems are commonly based on pattern matching. Adapting an IE system to a new scenario entails the construction of a new pattern base—a timeconsuming and expensive process. We have implemented a system for finding patterns aufrom Starting a small initial set of proposed by the user, the system applies an incremental discovery procedure to identify new patterns. We present experiments with evaluations which show that the resulting patterns exhibit high precision and recall.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Shaul Marcus</author>
<author>Shaul Markovitch</author>
</authors>
<title>Contextual word similarity and estimation from sparse data.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Assn. for Computational Linguistics,</booktitle>
<pages>31--37</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="11348" citStr="Dagan et al., 1993" startWordPosition="1829" endWordPosition="1832">et of pairs: e.g., a verbobject pair, a subject-object pair, etc. Each pair is used as a generalized pattern during the candidate selection stage. Once we have identified pairs which are relevant to the scenario, we use them to construct or augment concept classes, by grouping together the missing roles, (for example, a class of verbs which occur with a relevant subject-object pair: &amp;quot;company {hire/fire/expel...} person&amp;quot;). This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis (Riloff and Jones, 1999; Pereira et al., 1993; Dagan et al., 1993; Hirschman et al., 1975), although in our case the contexts are limited to selected patterns, relevant to the scenario. 5E.g., &amp;quot;John sleeps&amp;quot;, &amp;quot;John is appointed by Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;, &amp;quot;She asked John to buy a car&amp;quot;. SE.g., &amp;quot;John is appointed by Company&amp;quot;, &amp;quot;John is the president of Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;, Th—e—a73-i-Which I saw sleeps. 7For example, &amp;quot;She gave us our coffee black&amp;quot;, &amp;quot;Company appointed John Smith as president&amp;quot;. 284 3.4 Pattern Discovery Here we present the results from experiments we conducted on the MUC-6 scenario, &amp;quot;management succession&amp;quot;. The disco</context>
</contexts>
<marker>Dagan, Marcus, Markovitch, 1993</marker>
<rawString>Ido Dagan, Shaul Marcus, and Shaul Markovitch. 1993. Contextual word similarity and estimation from sparse data. In Proceedings of the 31st Annual Meeting of the Assn. for Computational Linguistics, pages 31-37, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Fisher</author>
<author>Stephen Soderland</author>
<author>Joseph McCarthy</author>
<author>Fangfang Feng</author>
<author>Wendy Lehnert</author>
</authors>
<title>Description of the UMass system as used for MUC-6.</title>
<date>1995</date>
<booktitle>In Proc. Sixth Message Understanding Conf. (MUC-6),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Columbia, MD,</location>
<contexts>
<context position="21893" citStr="Fisher et al., 1995" startWordPosition="3600" endWordPosition="3603">tral SVO triplet, included here for clarity. 287 cate that the person left to pursue other, undisclosed interests, the knowledge of which would relieve the system from seeking other information in order to fill this slot. This is to say that here strict evaluation is elusive. 5 Discussion and Current Work Some of the prior research has emphasized interactive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with• extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prior work differs from that presented here in several crucial respects; firstly, the prior work does not attempt to find entire events, after the fashion of MUC&apos;s highest-level </context>
</contexts>
<marker>Fisher, Soderland, McCarthy, Feng, Lehnert, 1995</marker>
<rawString>David Fisher, Stephen Soderland, Joseph McCarthy, Fangfang Feng, and Wendy Lehnert. 1995. Description of the UMass system as used for MUC-6. In Proc. Sixth Message Understanding Conf. (MUC-6), Columbia, MD, November. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Grishman</author>
<author>L Hirschman</author>
<author>N T Nhan</author>
</authors>
<title>Discovery procedures for sublanguage selectional patterns: Initial experiments.</title>
<date>1986</date>
<journal>Computational Linguistics,</journal>
<pages>12--3</pages>
<contexts>
<context position="4723" citStr="Grishman et al., 1986" startWordPosition="730" endWordPosition="733">ould s/he fail to provide an example of a particular class of syntactic/semantic construction, the system has no hope of recovering the corresponding events. Our experience has shown that (1) the process of discovering candidates is highly expensive, and (2) gaps in patterns directly translate into gaps in coverage. How can the system help automate the process of discovering new good candidates? The system should find examples of all common linguistic constructs relevant to a scenario. While there has been prior research on identifying the primary lexical patterns of a sub-language or corpus (Grishman et al., 1986; Riloff, 1996), the task here is more complex, since we are typically not provided in advance with a sub-corpus of relevant passages; these passages must themselves be found as part of the discovery process. The difficulty is that one of the best indications of the relevance of the passages is precisely the presence of these constructs. Because of this circularity, we propose to acquire the constructs and passages in tandem. 2 Solution We outline our procedure for automatic acquisition of patterns; details are elaborated in later sections. The procedure is unsupervised in that it does not req</context>
</contexts>
<marker>Grishman, Hirschman, Nhan, 1986</marker>
<rawString>R. Grishman, L. Hirschman, and N.T. Nhan. 1986. Discovery procedures for sublanguage selectional patterns: Initial experiments. Computational Linguistics, 12(3):205-16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Ralph Grishman</author>
<author>Naomi Sager</author>
</authors>
<title>Grammatically-based automatic word class formation.</title>
<date>1975</date>
<booktitle>Information Processing and Management,</booktitle>
<pages>11--1</pages>
<contexts>
<context position="11373" citStr="Hirschman et al., 1975" startWordPosition="1833" endWordPosition="1836"> verbobject pair, a subject-object pair, etc. Each pair is used as a generalized pattern during the candidate selection stage. Once we have identified pairs which are relevant to the scenario, we use them to construct or augment concept classes, by grouping together the missing roles, (for example, a class of verbs which occur with a relevant subject-object pair: &amp;quot;company {hire/fire/expel...} person&amp;quot;). This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis (Riloff and Jones, 1999; Pereira et al., 1993; Dagan et al., 1993; Hirschman et al., 1975), although in our case the contexts are limited to selected patterns, relevant to the scenario. 5E.g., &amp;quot;John sleeps&amp;quot;, &amp;quot;John is appointed by Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;, &amp;quot;She asked John to buy a car&amp;quot;. SE.g., &amp;quot;John is appointed by Company&amp;quot;, &amp;quot;John is the president of Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;, Th—e—a73-i-Which I saw sleeps. 7For example, &amp;quot;She gave us our coffee black&amp;quot;, &amp;quot;Company appointed John Smith as president&amp;quot;. 284 3.4 Pattern Discovery Here we present the results from experiments we conducted on the MUC-6 scenario, &amp;quot;management succession&amp;quot;. The discovery procedure was seeded</context>
</contexts>
<marker>Hirschman, Grishman, Sager, 1975</marker>
<rawString>Lynette Hirschman, Ralph Grishman, and Naomi Sager. 1975. Grammatically-based automatic word class formation. Information Processing and Management, 11(1/2):39-57.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timo Jarvinen</author>
<author>Pasi Tapanainen</author>
</authors>
<title>A dependency parser for English.</title>
<date>1997</date>
<tech>Technical Report TR-1,</tech>
<institution>Department of General Linguistics, University of Helsinki,</institution>
<location>Finland,</location>
<contexts>
<context position="10199" citStr="Jarvinen and Tapanainen, 1997" startWordPosition="1640" endWordPosition="1643">tive clauses to a tuple, consisting of several elements. 1. For each claus, the first element is the subject, a &amp;quot;semantic&amp;quot; subject of a non-finite sentence or agent of the passive.5 2. The second element is the verb. 3. The third element is the object, certain object-like adverbs, subject of the passive or subject complement6 4. The fourth element is a phrase which refers to the object or the subject. A typical example of such an argument is an object complement, such as Company named John Smith president. Another instance is the so-called copredicative (Nichols, 1978), in the parsing system (Jarvinen and Tapanainen, 1997). A copredicative refers to a subject or an object, though this distinction is typically difficult to resolve automatically.7 Clausal tuples also contain a locative modifier, and a temporal modifier. We used a corpus of 5,963 articles from the Wall Street Journal, randomly chosen. The parsed articles yielded a total of 250,000 clausal tuples, of which 135,000 were distinct. 3.3 Generalization and Concept Classes Because tuples may not repeat with sufficient frequency to obtain reliable statistics, each tuple is reduced to a set of pairs: e.g., a verbobject pair, a subject-object pair, etc. Eac</context>
</contexts>
<marker>Jarvinen, Tapanainen, 1997</marker>
<rawString>Timo Jarvinen and Pasi Tapanainen. 1997. A dependency parser for English. Technical Report TR-1, Department of General Linguistics, University of Helsinki, Finland, February.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
<author>Martin Ri5scheisen</author>
</authors>
<title>Text-translation alignment.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>1</issue>
<marker>Kay, Ri5scheisen, 1993</marker>
<rawString>Martin Kay and Martin Ri5scheisen. 1993. Text-translation alignment. Computational Linguistics, 19(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Lehnert</author>
<author>C Cardie</author>
<author>D Fisher</author>
<author>J McCarthy</author>
<author>E Riloff</author>
<author>S Soderland</author>
</authors>
<title>University of massachusetts: MUC-4 test results and analysis.</title>
<date>1992</date>
<booktitle>In Proc. Fourth Message Understanding Conf.,</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>McLean, VA,</location>
<contexts>
<context position="21872" citStr="Lehnert et al., 1992" startWordPosition="3595" endWordPosition="3599">are outside of the central SVO triplet, included here for clarity. 287 cate that the person left to pursue other, undisclosed interests, the knowledge of which would relieve the system from seeking other information in order to fill this slot. This is to say that here strict evaluation is elusive. 5 Discussion and Current Work Some of the prior research has emphasized interactive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with• extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prior work differs from that presented here in several crucial respects; firstly, the prior work does not attempt to find entire events, after the fashion of</context>
</contexts>
<marker>Lehnert, Cardie, Fisher, McCarthy, Riloff, Soderland, 1992</marker>
<rawString>W. Lehnert, C. Cardie, D. Fisher, J. McCarthy, E. Riloff, and S. Soderland. 1992. University of massachusetts: MUC-4 test results and analysis. In Proc. Fourth Message Understanding Conf., McLean, VA, June. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Scott Miller</author>
<author>Michael Crystal</author>
<author>Heidi Fox</author>
<author>Lance Ramshaw</author>
<author>Richard Schwartz</author>
<author>Rebecca Stone</author>
<author>Ralph Weischedel</author>
</authors>
<title>and the Annotation Group.</title>
<date>1998</date>
<booktitle>In Proc. of the Seventh Message Understanding Conference,</booktitle>
<location>Fairfax, VA.</location>
<contexts>
<context position="21915" citStr="Miller et al., 1998" startWordPosition="3604" endWordPosition="3607">luded here for clarity. 287 cate that the person left to pursue other, undisclosed interests, the knowledge of which would relieve the system from seeking other information in order to fill this slot. This is to say that here strict evaluation is elusive. 5 Discussion and Current Work Some of the prior research has emphasized interactive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with• extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prior work differs from that presented here in several crucial respects; firstly, the prior work does not attempt to find entire events, after the fashion of MUC&apos;s highest-level scenario-template task</context>
</contexts>
<marker>Miller, Crystal, Fox, Ramshaw, Schwartz, Stone, Weischedel, 1998</marker>
<rawString>Scott Miller, Michael Crystal, Heidi Fox, Lance Ramshaw, Richard Schwartz, Rebecca Stone, Ralph Weischedel, and the Annotation Group. 1998. Algorithms that learn to extract information; BBN: Description of the SIFT system as used for MUC-7. In Proc. of the Seventh Message Understanding Conference, Fairfax, VA.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<booktitle>Proceedings of the Fifth Message Understanding Conference (MUG- 5),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Baltimore, MD,</location>
<marker>1993</marker>
<rawString>1993. Proceedings of the Fifth Message Understanding Conference (MUG- 5), Baltimore, MD, August. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<date>1995</date>
<booktitle>Proceedings of the Sixth Message Understanding Conference (MUC-6),</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>Columbia, MD,</location>
<marker>1995</marker>
<rawString>1995. Proceedings of the Sixth Message Understanding Conference (MUC-6), Columbia, MD, November. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johanna Nichols</author>
</authors>
<title>Secondary predicates.</title>
<date>1978</date>
<booktitle>Proceedings of the 4th Annual Meeting of Berkeley Linguistics Society,</booktitle>
<pages>114--127</pages>
<contexts>
<context position="10144" citStr="Nichols, 1978" startWordPosition="1634" endWordPosition="1635">ucing such variants as passive and relative clauses to a tuple, consisting of several elements. 1. For each claus, the first element is the subject, a &amp;quot;semantic&amp;quot; subject of a non-finite sentence or agent of the passive.5 2. The second element is the verb. 3. The third element is the object, certain object-like adverbs, subject of the passive or subject complement6 4. The fourth element is a phrase which refers to the object or the subject. A typical example of such an argument is an object complement, such as Company named John Smith president. Another instance is the so-called copredicative (Nichols, 1978), in the parsing system (Jarvinen and Tapanainen, 1997). A copredicative refers to a subject or an object, though this distinction is typically difficult to resolve automatically.7 Clausal tuples also contain a locative modifier, and a temporal modifier. We used a corpus of 5,963 articles from the Wall Street Journal, randomly chosen. The parsed articles yielded a total of 250,000 clausal tuples, of which 135,000 were distinct. 3.3 Generalization and Concept Classes Because tuples may not repeat with sufficient frequency to obtain reliable statistics, each tuple is reduced to a set of pairs: e</context>
</contexts>
<marker>Nichols, 1978</marker>
<rawString>Johanna Nichols. 1978. Secondary predicates. Proceedings of the 4th Annual Meeting of Berkeley Linguistics Society, pages 114-127.</rawString>
</citation>
<citation valid="true">
<title>Information Extraction.</title>
<date>1997</date>
<booktitle>Lecture Notes in Artificial Intelligence,</booktitle>
<editor>Maria Teresa Pazienza, editor.</editor>
<publisher>Springer-Verlag,</publisher>
<location>Rome.</location>
<marker>1997</marker>
<rawString>Maria Teresa Pazienza, editor. 1997. Information Extraction. Springer-Verlag, Lecture Notes in Artificial Intelligence, Rome.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando Pereira</author>
<author>Naftali Tishby</author>
<author>Lillian Lee</author>
</authors>
<title>Distributional clustering of English words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Assn. for Computational Linguistics,</booktitle>
<pages>183--190</pages>
<location>Columbus, OH,</location>
<contexts>
<context position="11328" citStr="Pereira et al., 1993" startWordPosition="1825" endWordPosition="1828">uple is reduced to a set of pairs: e.g., a verbobject pair, a subject-object pair, etc. Each pair is used as a generalized pattern during the candidate selection stage. Once we have identified pairs which are relevant to the scenario, we use them to construct or augment concept classes, by grouping together the missing roles, (for example, a class of verbs which occur with a relevant subject-object pair: &amp;quot;company {hire/fire/expel...} person&amp;quot;). This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis (Riloff and Jones, 1999; Pereira et al., 1993; Dagan et al., 1993; Hirschman et al., 1975), although in our case the contexts are limited to selected patterns, relevant to the scenario. 5E.g., &amp;quot;John sleeps&amp;quot;, &amp;quot;John is appointed by Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;, &amp;quot;She asked John to buy a car&amp;quot;. SE.g., &amp;quot;John is appointed by Company&amp;quot;, &amp;quot;John is the president of Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;, Th—e—a73-i-Which I saw sleeps. 7For example, &amp;quot;She gave us our coffee black&amp;quot;, &amp;quot;Company appointed John Smith as president&amp;quot;. 284 3.4 Pattern Discovery Here we present the results from experiments we conducted on the MUC-6 scenario, &amp;quot;management su</context>
</contexts>
<marker>Pereira, Tishby, Lee, 1993</marker>
<rawString>Fernando Pereira, Naftali Tishby, and Lillian Lee. 1993. Distributional clustering of English words. In Proceedings of the 31st Annual Meeting of the Assn. for Computational Linguistics, pages 183-190, Columbus, OH, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
<author>Rosie Jones</author>
</authors>
<title>Learning dictionaries for information extraction by multi-level bootstrapping.</title>
<date>1999</date>
<booktitle>In Proceedings of Sixteenth National Conference on Artificial Intelligence (AAAI-99),</booktitle>
<location>Orlando, Florida.</location>
<contexts>
<context position="11306" citStr="Riloff and Jones, 1999" startWordPosition="1821" endWordPosition="1824">iable statistics, each tuple is reduced to a set of pairs: e.g., a verbobject pair, a subject-object pair, etc. Each pair is used as a generalized pattern during the candidate selection stage. Once we have identified pairs which are relevant to the scenario, we use them to construct or augment concept classes, by grouping together the missing roles, (for example, a class of verbs which occur with a relevant subject-object pair: &amp;quot;company {hire/fire/expel...} person&amp;quot;). This is similar to work by several other groups which aims to induce semantic classes through syntactic co-occurrence analysis (Riloff and Jones, 1999; Pereira et al., 1993; Dagan et al., 1993; Hirschman et al., 1975), although in our case the contexts are limited to selected patterns, relevant to the scenario. 5E.g., &amp;quot;John sleeps&amp;quot;, &amp;quot;John is appointed by Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;, &amp;quot;She asked John to buy a car&amp;quot;. SE.g., &amp;quot;John is appointed by Company&amp;quot;, &amp;quot;John is the president of Company&amp;quot;, &amp;quot;I saw a dog which sleeps&amp;quot;, Th—e—a73-i-Which I saw sleeps. 7For example, &amp;quot;She gave us our coffee black&amp;quot;, &amp;quot;Company appointed John Smith as president&amp;quot;. 284 3.4 Pattern Discovery Here we present the results from experiments we conducted on the MUC-6 sc</context>
<context position="23115" citStr="Riloff and Jones, 1999" startWordPosition="3796" endWordPosition="3799"> scenario-template task. Rather the patterns produced by those systems identify NPs that fill individual slots, without specifying how these slots may be combined at a later stage into complete event templates. The present work focuses on directly discovering event-level, multi-slot relational patterns. Secondly, the prior work either relies on a set of documents with relevance judgements to find slot fillers where they are relevant to events, (Riloff, 1996), or utilizes an un-classified corpus containing a very high proportion of relevant documents to find all instances of a semantic class, (Riloff and Jones, 1999). By contrast, our procedure requires no relevance judgements, and works on the assumption that the corpus is balanced and the proportion of relevant documents is small. Classifying documents by hand, although admittedly easier than tagging event instances in text for automatic training, is still a formidable task. When we prepared the test corpus, it took 5 hours to mark 150 short documents. The presented results indicate that our method of corpus analysis can be used to rapidly identify a large number of relevant patterns without pre-classifying a large training corpus. We are at the early s</context>
</contexts>
<marker>Riloff, Jones, 1999</marker>
<rawString>Ellen Riloff and Rosie Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In Proceedings of Sixteenth National Conference on Artificial Intelligence (AAAI-99), Orlando, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically constructing a dictionary for information extraction tasks.</title>
<date>1993</date>
<booktitle>In Proceedings of Eleventh National Conference on Artificial Intelligence (AAAI93),</booktitle>
<pages>811--816</pages>
<publisher>The AAAI Press/MIT Press.</publisher>
<contexts>
<context position="22238" citStr="Riloff, 1993" startWordPosition="3660" endWordPosition="3661">teractive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with• extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prior work differs from that presented here in several crucial respects; firstly, the prior work does not attempt to find entire events, after the fashion of MUC&apos;s highest-level scenario-template task. Rather the patterns produced by those systems identify NPs that fill individual slots, without specifying how these slots may be combined at a later stage into complete event templates. The present work focuses on directly discovering event-level, multi-slot relational patterns. Secondly, the prior work either relies on</context>
</contexts>
<marker>Riloff, 1993</marker>
<rawString>Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. In Proceedings of Eleventh National Conference on Artificial Intelligence (AAAI93), pages 811-816. The AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically generating extraction patterns from untagged text.</title>
<date>1996</date>
<booktitle>In Proceedings of Thirteenth National Conference on Artificial Intelligence (AAAI-96),</booktitle>
<pages>1044--1049</pages>
<publisher>The AAAI Press/MIT Press.</publisher>
<contexts>
<context position="4738" citStr="Riloff, 1996" startWordPosition="734" endWordPosition="735">de an example of a particular class of syntactic/semantic construction, the system has no hope of recovering the corresponding events. Our experience has shown that (1) the process of discovering candidates is highly expensive, and (2) gaps in patterns directly translate into gaps in coverage. How can the system help automate the process of discovering new good candidates? The system should find examples of all common linguistic constructs relevant to a scenario. While there has been prior research on identifying the primary lexical patterns of a sub-language or corpus (Grishman et al., 1986; Riloff, 1996), the task here is more complex, since we are typically not provided in advance with a sub-corpus of relevant passages; these passages must themselves be found as part of the discovery process. The difficulty is that one of the best indications of the relevance of the passages is precisely the presence of these constructs. Because of this circularity, we propose to acquire the constructs and passages in tandem. 2 Solution We outline our procedure for automatic acquisition of patterns; details are elaborated in later sections. The procedure is unsupervised in that it does not require the traini</context>
<context position="13612" citStr="Riloff, 1996" startWordPosition="2211" endWordPosition="2212">en restarted. 3.5 Re-computation of Document Relevance The above is a simplification of the actual procedure, in several important respects. Only generalized patterns are considered for candidacy, with one or more slots filled with wild-cards. In computing the score of the generalized pattern, we do not take into consideration all possible values of the wild-card role. We instead constrain the wild-card to those values which themselves in turn produce patterns with high scores. These values then become members of a new class, which is output in tandem with the winning patternl° 8similarly to (Riloff, 1996) 9U denotes the universe of documents. We used a = 0.1 and )3 =2. 19The classes are currently unused by subsequent iterations; this important issue is considered in future work. Documents are assigned relevance scores on a scale between 0 and 1. The seed patterns are accepted as ground truth; thus the documents they match have relevance 1. On subsequent iterations, the newly accepted patterns are not trusted as absolutely. On iteration number i + 1, each pattern p is assigned a precision measure, based on the relevance of the documents it matches: 1 E i(d) (2) preci Rel (d) IH(p) I dElf(12) wh</context>
<context position="22204" citStr="Riloff, 1996" startWordPosition="3655" endWordPosition="3656">e prior research has emphasized interactive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with• extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prior work differs from that presented here in several crucial respects; firstly, the prior work does not attempt to find entire events, after the fashion of MUC&apos;s highest-level scenario-template task. Rather the patterns produced by those systems identify NPs that fill individual slots, without specifying how these slots may be combined at a later stage into complete event templates. The present work focuses on directly discovering event-level, multi-slot relational patterns. Secondl</context>
</contexts>
<marker>Riloff, 1996</marker>
<rawString>Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In Proceedings of Thirteenth National Conference on Artificial Intelligence (AAAI-96), pages 1044-1049. The AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pasi Tapanainen</author>
<author>Timo Jarvinen</author>
</authors>
<title>A non-projective dependency parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th Conference on Applied Natural Language Processing,</booktitle>
<pages>64--71</pages>
<publisher>ACL.</publisher>
<location>Washington, D.C.,</location>
<contexts>
<context position="9313" citStr="Tapanainen and Jarvinen, 1997" startWordPosition="1490" endWordPosition="1493">n Before applying the discovery procedure, we subject the corpus to several stages of preprocessing. First, we apply a name recognition module, and replace each name with a token describing its class, e.g. C-Person, C-Company, etc. We collapse together all numeric expressions, currency values, dates, etc., using a single token to designate each of these classes. 3.2 Syntactic Analysis We then apply a parser to perform syntactic normalization to transform each clause into a common predicate-argument structure. We use the general-purpose dependency parser of English, based on the FDG formalism (Tapanainen and Jarvinen, 1997) and developed by the Research Unit for Multilingual Language Technology at the University of Helsinki, and Conexor Oy. The parser (modified to understand the name labels attached in the previous step) is used for reducing such variants as passive and relative clauses to a tuple, consisting of several elements. 1. For each claus, the first element is the subject, a &amp;quot;semantic&amp;quot; subject of a non-finite sentence or agent of the passive.5 2. The second element is the verb. 3. The third element is the object, certain object-like adverbs, subject of the passive or subject complement6 4. The fourth el</context>
</contexts>
<marker>Tapanainen, Jarvinen, 1997</marker>
<rawString>Pasi Tapanainen and Timo Jarvinen. 1997. A non-projective dependency parser. In Proceedings of the 5th Conference on Applied Natural Language Processing, pages 64-71, Washington, D.C., April. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
</authors>
<title>Customization of information extraction systems.</title>
<date>1997</date>
<booktitle>International Workshop on Lexically Driven Information Extraction,</booktitle>
<pages>1--11</pages>
<editor>In Paola Velardi, editor,</editor>
<publisher>Universita</publisher>
<location>Frascati, Italy,</location>
<contexts>
<context position="21719" citStr="Yangarber and Grishman, 1997" startWordPosition="3572" endWordPosition="3575">lled &amp;quot;other-organization&amp;quot;, indicating from or to which company the person came or went. This pattern is consistently used in text to indi12bracketed constituents are outside of the central SVO triplet, included here for clarity. 287 cate that the person left to pursue other, undisclosed interests, the knowledge of which would relieve the system from seeking other information in order to fill this slot. This is to say that here strict evaluation is elusive. 5 Discussion and Current Work Some of the prior research has emphasized interactive tools to convert examples to extraction patterns, cf. (Yangarber and Grishman, 1997), while others have focused on methods for automatically converting a corpus annotated with• extraction examples into such patterns (Lehnert et al., 1992; Fisher et al., 1995; Miller et al., 1998). These methods, however, do not reduce the burden of finding the examples to annotate. With either approach, the portability bottleneck is shifted from the problem of building patterns to that of finding good candidates. The prior work most closely related to this study is (Riloff, 1996), which, along with (Riloff, 1993), seeks automatic methods for filling slots in event templates. However, the prio</context>
</contexts>
<marker>Yangarber, Grishman, 1997</marker>
<rawString>Roman Yangarber and Ralph Grishman. 1997. Customization of information extraction systems. In Paola Velardi, editor, International Workshop on Lexically Driven Information Extraction, pages 1-11, Frascati, Italy, July. Universita di Roma.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>