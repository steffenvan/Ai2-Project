<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000229">
<title confidence="0.989864">
Domain-Specific Knowledge Acquisition from Text
</title>
<author confidence="0.999516">
Dan Moldovan, Roxana Girju and Vasile Rus
</author>
<affiliation confidence="0.9928965">
Department of Computer Science and Engineering
University of Southern Methodist University
</affiliation>
<address confidence="0.766943">
Dallas, Texas, 75275-0122
</address>
<email confidence="0.995593">
{moldovan, roxana, rus}@seas.smu.edu
</email>
<sectionHeader confidence="0.993823" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.975655111111111">
In many knowledge intensive applications, it is nec-
essary to have extensive domain-specific knowledge
in addition to general-purpose knowledge bases.
This paper presents a methodology for discovering
domain-specific concepts and relationships in an at-
tempt to extend WordNet. The method was tested
on five seed concepts selected from the financial
domain: interest rate, stock market, inflation, eco-
nomic growth, and employment.
</bodyText>
<sectionHeader confidence="0.91897" genericHeader="keywords">
1 Desiderata for Automated
Knowledge Acquisition
</sectionHeader>
<bodyText confidence="0.9865713">
The need for knowledge
The knowledge is infinite and no matter how large a
knowledge base is, it is not possible to store all the
concepts and procedures for all domains. Even if
that were possible, the knowledge is generative and
there are no guarantees that a system will have the
latest information all the time. And yet, if we are to
build common-sense knowledge processing systems
in the future, it is necessary to have general-purpose
and domain-specific knowledge that is up to date.
Our inability to build large knowledge bases without
much effort has impeded many ANLP developments.
The most successful current Information Extrac-
tion systems rely on hand coded linguistic rules rep-
resenting lexico-syntactic patterns capable of match-
ing natural language expressions of events. Since
the rules are hand-coded it is difficult to port sys-
tems across domains. Question answering, inference,
summarization, and other applications can benefit
from large linguistic knowledge bases.
</bodyText>
<subsectionHeader confidence="0.575565">
The basic idea
</subsectionHeader>
<bodyText confidence="0.999958111111111">
A possible solution to the problem of rapid develop-
ment of flexible knowledge bases is to design an au-
tomatic knowledge acquisition system that extracts
knowledge from texts for the purpose of merging it
with a core ontological knowledge base. The attempt
to create a knowledge base manually is time con-
suming and error prone, even for small application
domains, and we believe that automatic knowledge
acquisition and classification is the only viable solu-
tion to large-scale, knowledge intensive applications.
This paper presents an interactive method that ac-
quires new concepts and connections associated with
user-selected seed concepts, and adds them to the
WordNet linguistic knowledge structure (Fellbaum
1998). The sources of the new knowledge are texts
acquired from the Internet or other corpora. At the
present time, our system works in a semi-automatic
mode, in the sense that it acquires concepts and re-
lations automatically, but their validation is done by
the user.
We believe that domain knowledge should not be
acquired in a vacuum; it should expand an existent
ontology with a skeletal structure built on consistent
and acceptable principles. The method presented in
this paper is applicable to any Machine Readable
Dictionary. However, we chose WordNet because it
is freely available and widely used.
</bodyText>
<sectionHeader confidence="0.380249" genericHeader="related work">
Related work
</sectionHeader>
<bodyText confidence="0.9999262">
This work was inspired in part by Marti Hearst&apos;s
paper (Hearst 1998) where she discovers manually
le)dco-syntactic patterns for the HYPERNYMY rela-
tion in WordNet.
Much of the work in pattern extraction from texts
was done for improving the performance of Infor-
mation Extraction systems. Research in this area
was done by (Kim and Moldovan 1995) (Riloff 1996),
(Soderland 1997) and others.
The MindNet (Richardson 1998) project at Mi-
crosoft is an attempt to transform the Longman Dic-
tionary of Contemporary English (LDOCE) into a
form of knowledge base for text processing.
Woods studied knowledge representation and clas-
sification for long time (Woods 1991), and more re-
cently is trying to automate the construction of tax-
onomies by extracting concepts directly from texts
(Woods 1997).
The Knowledge Acquisition from Text (KAT) sys-
tem is presented next. It consists of four parts: (1)
discovery of new concepts, (2) discovery of new lex-
ical patterns, (3) discovery of new relationships re-
flected by the lexical patterns, and (4) the classifi-
cation and integration of the knowledge discovered
with a WordNet - like knowledge base.
</bodyText>
<page confidence="0.997216">
268
</page>
<sectionHeader confidence="0.958578" genericHeader="method">
2 KAT System
</sectionHeader>
<subsectionHeader confidence="0.969163">
2.1 Discover new concepts
</subsectionHeader>
<bodyText confidence="0.966615414634147">
Select seed concepts. New domain knowledge can
be acquired around some seed concepts that a user
considers important. In this paper we focus on the
financial domain, and use: interest rate, stock mar-
ket, inflation, economic growth, and employment as
seed concepts. The knowledge we seek to acquire re-
lates to one or more of these concepts, and consists
of new concepts not defined in WordNet and new re-
lations that link these concepts with other concepts,
some of which are in WordNet.
For example, from the sentence: When the US
economy enters a boom, mortgage interest rates rise,
the system discovers: (1) the new concept mortgage
interest rate not defined in WordNet but related to
the seed concept interest rate, and (2) the state of
the US economy and the value of mortgage interest
rate are in a DIRECT RELATIONSHIP.
In WordNet, a concept is represented as a synset
that contains words sharing the same meaning. In
our experiments, we extend the seed words to their
corresponding synset. For example, stock market is
synonym with stock exchange and securities market,
and we aim to learn concepts related to all these
terms, not only to stock market.
Extract sentences. Queries are formed with each
seed concept to ex-tract documents from the Internet
and other possible sources. The documents retrieved
are further processed such that only the sentences
that contain the seed concepts are retained. This
way, an arbitrarily large corpus A is formed of sen-
tences containing the seed concepts. We limit the
size of this corpus to 1000 &apos;sentences per seed con-
cept.
Parse sentences. Each sentence in this corpus is
first part-of-speech (POS) tagged then parsed. We
use Brill&apos;s POS tagger and our own parser. The out-
put of the POS tagger for the example above is:
When/wRB the/DT U.S./NNP economy/NN en-
ters/vBz a/DT boom/NN mortgage/NN inter-
est_ratesINNs rise/vBP ./.
The syntactic parser output is:
</bodyText>
<equation confidence="0.98692175">
TOP (S (SBAR (WHADVP (WRB When)) (S (NP (DT
the) (NNP U.S.) (NN economy)) (vP (vsz enters) (NP
(DT a) (NN boom) (â€ž))))) (NP (NN mortgage) (NNS
interest_rates)) (vP (vBP rise)))
</equation>
<bodyText confidence="0.99842225">
Extract new concepts. In this paper only noun
concepts are considered. Since, most likely, one-
word nouns are already defined in WordNet, the fo-
cus here is on compound nouns and nouns with mod-
ifiers that have meaning but are not in WordNet.
The new concepts directly related to the seeds are
extracted from the noun phrases (NPs) that contain
the seeds. In the example above, we see that the
seed belongs to the NP: mortgage interest rate.
This way, a list of NPs containing the seeds is
assembled automatically from the parsed texts. Ev-
ery such NP is considered a potential new concept.
This is only the &amp;quot;raw material&amp;quot; from which actual
concepts are discovered.
In some noun phrases the seed is the head noun,
i.e. [word, word,..seed], where word can be a noun or
an adjective. For example, [interest rate] is in Word-
Net, but [short term nominal interest rate] is not in
WordNet. Most of the new concepts related to a
seed are generated this way. In other cases the seed
is not the head noun i.e. [word, word,. .seed, word,
word]. For example [interest rate peg], or [interna-
tional interest rate differential].
The following procedures are used to discover con-
cepts, and are applicable in both cases:
Procedure 1.1. WordNet reduction. Search NP for
words collocations that are defined in WordNet as
concepts. Thus [long term interest rate] becomes
[long_term interest_rate], [prime interest rate] be-
comes [prime_interest_rate], as all hyphenated con-
cepts are in WordNet.
Procedure 1.2. Dictionary reduction. For each
NP, search further in other on-line dictionaries for
more compound concepts, and if found, hyphen-
ate the words. Many domain-specific dictionaries
are available on-line. For example, [mortgage inter-
est_rate] becomes imortgage_interest_ratej, since it is
defined in the on-line dictionary OneLook Dictionar-
ies (http://www.onelook.com).
Procedure 1.3. User validation. Since currently we
lack a formal definition of a concept, it is not possible
to completely automate the discovery of concepts.
The human inspects the list of noun phrases and
decides whether to accept or decline each concept.
</bodyText>
<subsectionHeader confidence="0.997673">
2.2 Discover lexico-syntactic patterns
</subsectionHeader>
<bodyText confidence="0.999943176470588">
Texts represent a rich source of information from
which in addition to concepts we can also discover
relations between concepts. We are interested in dis-
covering semantic relationships that link the con-
cepts extracted above with other concepts, some of
which may be in WordNet. The approach is to
search for lexico-syntactic patterns comprising the
concepts of interest. The semantic relations from
WordNet are the first we search for, as it is only
natural to add more of these relations to enhance
the WordNet knowledge base. However, since the
focus is on the acquisition of domain-specific knowl-
edge, there are semantic relations between concepts
other than the WordNet relations that are impor-
tant. These new relations can be discovered auto-
matically from the clauses and sentences in which
the seeds occur.
</bodyText>
<page confidence="0.996675">
269
</page>
<bodyText confidence="0.981995685714286">
Pick a semantic relation R. These can be Word-
Net semantic relations or any other relations defined
by the user. So far, we have experimented with the
WordNet HYPERNYMY (or so-called Is-A) relation,
and three other relations. By inspecting a few sen-
tences containing interest rate one can notice that
INFLUENCE is a frequently used relation. The two
other relations are CAUSE and EQUIVALENT.
Pick a pair of concepts C1, Ci among which
R holds. These may be any noun concepts. In the
context of finance domain, some examples of con-
cepts linked by the INFLUENCE relation are:
interest rate INFLUENCES earnings, or
credit worthiness INFLUENCES interest rate.
Extract lexico-syntactic patterns C2 P Ci.
Search any corpus B, different from A for all in-
stances where Ci and Ci occur in the same sentence.
Extract the lexico-syntactic patterns that link the
two concepts. For example, from the sentence: The
graph indicates the impact on earnings from several
different interest rate scenarios, the generally appli-
cable pattern extracted is:
impact on NP2 from NPI
This pattern corresponds unambiguously to the re-
lation R we started with, namely INFLUENCE. Thus
we conclude: INFLuENcE(NP1, NP2).
Another example is: As the credit worthiness de-
creases, the interest rate increases. From this sen-
tence we extract another lexical pattern that ex-
presses the INFLUENCE relation:
[as NP1 vbl, NP2 vb2] &amp; [vb1 and vb2 are antonyms]
This pattern is rather complex since it contains not
only the lexical part but also the verb condition that
needs to be satisfied.
This procedure repeats for all relations R.
</bodyText>
<subsectionHeader confidence="0.8596575">
2.3 Discover new relationships between
concepts
</subsectionHeader>
<bodyText confidence="0.998466291666667">
Let us denote with C, the seed-related concepts
found with Procedures 1.1 through 1.3. We search
now corpus A for the occurrence of patterns P dis-
covered above such that one of their two concepts is
a concept C3.
Search corpus A for a pattern P. Using a lexico-
syntactic pattern P, one at a time, search corpus A
for its occurrence. If found, search further whether
or not one of the NPs is a seed-related concept C,.
Identify new concepts Cn. Part of the pattern P
are two noun phrases, one of which is C8. The head
noun from the other noun phrase is a concept Cn we
are looking for. This may be a WordNet concept,
and if it is not it will be added to the list of concepts
discovered.
Form relation R(C. â€ž Ca). Since each pattern P is
a linguistic expression of its corresponding seman-
tic relation R, we conclude R(C8, Cn) (this is in-
terpreted &amp;quot;C, is relation R Câ€ž)&amp;quot;). These steps are
repeated for all patterns.
User intervention to accept or reject relationships
is necessary mainly due to our system inability of
handling coreference resolution and other complex
linguistic phenomena.
</bodyText>
<subsectionHeader confidence="0.8844">
2.4 Knowledge classification and
integration
</subsectionHeader>
<bodyText confidence="0.9967463125">
Next, a taxonomy needs to be created that is con-
sistent with WordNet. In addition to creating a
taxonomy, this step is also useful for validating the
concepts acquired above. The classification is based
on the subsumption principle (Schmolze and Lipids
1983), (Woods 1991).
This algorithm provides the overall steps for the
classification of concepts within the context of Word-
Net. Figure 1 shows the inputs of the Classification
Algorithm and suggests that the classification is an
iterative process. In addition to WordNet, the in-
puts consist of the corpus A, the sets of concepts C,
and Cn, and the relationships R.. Let&apos;s denote with
C = C, U Cn the union of the seed related concepts
with the new concepts. All these concepts need to
be classified.
</bodyText>
<figure confidence="0.92185">
WordNet Corpora A Concepts Cs, Co Relationships R
I 1 1
Knowledge Classification t=k
Algorithm
t=k+1
1 Knowledge Base
</figure>
<figureCaption confidence="0.999935">
Figure 1: The knowledge classification diagram
</figureCaption>
<bodyText confidence="0.9347755">
Step 1. From the set of relationships 7Z, discovered
in Part 3, pick all the HYPERNYMY relations. From
the way these relations were developed, there are
two possibilities:
</bodyText>
<listItem confidence="0.9959574">
(1) A HYPERNYMY relation links a WordNet concept
Cu, with another concept from the set C denoted
with qv, or
(2) A HYPERNYMY relation links a concept C, with
a concept Cn.
</listItem>
<bodyText confidence="0.96531825">
Concepts qv are immediately linked to Word-
Net and added to the knowledge base. The concepts
from case (2) are also added to the knowledge base
but they form at this point only some isolated islands
since are not yet linked to the rest of the knowledge
base.
Step 2. Search corpus A for all the patterns asso-
ciated with the HYPERNYMY relation that may link
</bodyText>
<page confidence="0.942053">
270
</page>
<figure confidence="0.999142125">
[Asian_country interest_rate
IIS-A IIS-A
[Japan discount_rate
a)
[country interest_rate
IS-A IS-A
[Japan discount_rate [Germany prime_interest_rate
b)
</figure>
<figureCaption confidence="0.999649">
Figure 2: Relative classification of two concepts
</figureCaption>
<bodyText confidence="0.99986916">
concepts in the set Cn with any WordNet concepts.
Altough concepts Cr, are not seed-based concepts,
they are related to at least one Cs concept via a re-
lationship (as found in Task 3). Here we seek to find
HYPERNYMY links between them and WordNet con-
cepts. If such Cm concepts exist, denote them with
CL. The union Cm&apos;, = Cj,L, U CL represents all
concepts from the set C that are linked to WordNet
without any further effort. We focus now on the rest
of concepts, C, = C n Ch, that are not yet linked
to any WordNet concepts.
Step 3. Classify all concepts in set Cc using Pro-
cedures 4.1 through 4.5 below.
Step 4. Repeat Step 3 for all the concepts in set
Ce several times till no more changes occur. This
reclassification is necessary since the insertion of a
concept into the knowledge base may perturb the
ordering of other surrounding concepts in the hier-
archy.
Step 5. Add the rest of relationships R. other
than the HYPERNYMY to the new knowledge base.
The HYPERNYMY relations have already been used
in the Classification Algorithm, but the other rela-
tions, i.e. INFLUENCE, CAUSE and EQUIVALENT need
to be added to the knowledge base.
</bodyText>
<subsectionHeader confidence="0.843101">
Concept classification procedures
</subsectionHeader>
<bodyText confidence="0.98730572">
Procedure 4.1. Classify a concept of the form [word,
head] with respect to concept [head].
It is assumed here that the [head] concept exists
in WordNet simply because in many instances the
&amp;quot;head&amp;quot; is the &amp;quot;seed&amp;quot; concept, and because frequently
the head is a single word common noun usually de-
fined in WordNet. In this procedure we consider only
those head nouns that do not have any hyponyms
since the other case when the head has other con-
cepts under it is more complex and is treated by
Procedure 4.4. Here &amp;quot;word&amp;quot; is a noun or an adjec-
tive.
The classification is based on the simple idea
that a compound concept [word, head] is onto-
logically subsumed by concept [head]. For exam-
ple, mortgage_interest_rate is a kind of interest_rate,
thus linked by a relation HYPERNYMY(interest_rate,
mortgage_interest_rate).
Procedure 4.2. Classify a concept [wordi, headd
with respect to another concept [word2, head2].
For a relative classification of two such concepts, the
ontological relations between headi and head2 and
between wordi and word2, if exist, are extended to
the two concepts. We distinguish here three possi-
bilities:
</bodyText>
<listItem confidence="0.889785380952381">
1. headi subsumes head2 and wordi subsumes
word2. In this case [wordi, headi] subsumes
[word2, head2]. The subsumption may not al-
ways be a direct connection; sometimes it may
consist of a chain of subsumption relations since
subsumption is (usually) a transitive relation
(Woods 1991). An example is shown in Fig-
ure 2a; in WordNet, Asian_country subsumes
Japan and interest_rate subsumes discount_rate.
A particular case of this is when head&apos; is iden-
tical with head2.
2. Another case is when there is no direct sub-
sumption relation in WordNet between wordi
and word2, and/or headi and head2, but there
are a common subsuming concepts, for each
pair. When such concepts are found, pick
the most specific common subsumer (MSCS)
concepts of word&apos; and word2, and of head1
and head2, respectively. Then form a concept
[MSCS(wordi , word2), MSCS (head&apos;, head2)]
and place [word&apos; headi] and [word2 head2] un-
der it. This is exemplified in Figure 2b. In
WordNet, country subsumes Japan and Ger-
many, and interest_rate subsumes discount_rate
and prirne_interest_rate.
3. In all other cases, no subsumption relation is es-
tablished between the two concepts. For exam-
ple, we cannot say whether Asian_country dis-
count_rate is more or less abstract then Japan
interest_rate.
Procedure 4.3. Classify concept [wordi word2 head].
Several possibilities exist:
1. When there is already a concept [word2 head]
in the knowledge base under the [head], then
place [word&apos; word2 head] under concept [word2
head].
2. When there is already a concept [wordi head]
in the knowledge base under the [head], then
place [wordi word2 head] under concept [wordi
head].
3. When both cases 1 and 2 are true then place
[wordi word2 head] under both concepts.
</listItem>
<page confidence="0.995982">
271
</page>
<bodyText confidence="0.996973529411765">
4. When neither [wordi head] nor [word2 head] are
in the knowledge base, then place [wordi word2
head] under the [head]. The example in Figure
3 corresponds to case 3.
components
are already identified and it is easy to map every
verb into such a semantic relation.
As far as the newly discovered concepts are con-
cerned, their defining relations need to be retrieved
from texts. Human assistance is required, at least
for now, to pinpoint the most characteristic relations
that define a concept.
Below is a two step algorithm that we envision for
the relative classification of two concepts A and B.
Let&apos;s us denote with Alia Cc, and BRbCb the rela-
tionships that define concepts A and B respectively.
These are similar to rolesets and values.
</bodyText>
<listItem confidence="0.955441">
1. Extract relations (denoted by verbs) be-
tween concept and other gloss concepts.
</listItem>
<figure confidence="0.981805777777778">
ARai Cal BRblCbl
AR02Ca2 BRb2Cb2
IS-A
radio components
IS-A
IS-A
automobile components
IS-A
automobile radio components
</figure>
<figureCaption confidence="0.996908">
Figure 3: Classification of a compound concept with respect to ARamCnm BRbnCbn
its HYPERNYM concepts
</figureCaption>
<bodyText confidence="0.999810189189189">
Since we do not deal here with the sentence seman-
tics, it is not possible to completely determine the
meaning of [word&apos; word2 head], as it may be either
[((wordi word2) head)] or [(wordi (word2 head))] of-
ten depending on the sentence context.
In the example of Figure 3 there is only one mean-
ing, i.e. [(automobile radio) components]. However,
in the case of [performance skiing equipment] there
are two valid interpretations, namely [(performance
skiing) equipment] and [performance (skiing equip-
ment)].
Procedure 4.4 Classify a concept [wordi, head] with
respect to a concept hierarchy under the [head].
The task here is to identify the most specific sub-
sumer (MSS) from all the concepts under the head
that subsumes [wordi, head]. By default, [wordi
head] is placed under [head], however, since it may
be more specific than other hyponyms of [head], a
more complex classification analysis needs to be im-
plemented.
In the previous work on knowledge classification
it was assumed that the concepts were accompanied
by rolesets and values (Schmolze and Lipids 1983),
(Woods 1991), and others. Knowledge classifiers are
part of almost any knowledge representation system.
However, the problem we face here is more diffi-
cult. While in build-by-hand knowledge representa-
tion systems, the relations and values defining con-
cepts are readily available, here we have to extract
them from text. Fortunately, one can take advantage
of the glossary definitions that are associated with
concepts in WordNet and other dictionaries. One
approach is to identify a set of semantic relations
into which the verbs used in the gloss definitions are
mapped into for the purpose of working with a man-
ageable set of relations that may describe the con-
cepts restrictions. In WordNet these basic relations
</bodyText>
<listItem confidence="0.9944926">
2. A subsumes B if and only if
(a) Relations Rai subsume Rbi, for 1 &lt;i &lt;m.
(b) Cai subsumes or is a meronym of Cbi-
(c) Concept B has more relations than concept
A, i.e. m &lt;n.
</listItem>
<bodyText confidence="0.99921040625">
Example: In Figure 4 it is shown the classification
of concept monetary policy that has been discovered.
By default this concept is placed under policy. How-
ever in WordNet there is a hierarchy fiscal policy -
IS-A - economic policy - IS-A - policy. The question
is where exactly to place monetary policy in this hi-
erarchy.
The gloss of economic policy indicates that it is
MADE BY Government, and that it CONTROLS eco-
nomic growth - (here we simplified the explanation
and used economy instead of economic growth). The
gloss of fiscal policy leads to relations MADE BY Gov-
ernment, CONTROLS budget, and CONTROLS taxa-
tion. The concept money supply was found by Pro-
cedure 1.2 in several dictionaries, and its dictionary
definition leads to relations MADE BY Federal Gov-
ernment, and CONTROLS money supply. In Word-
Net Government subsumes Federal Government, and
economy HAS PART money. All necessary conditions
are satisfied for economic policy to subsume mone-
tary policy. However, fiscal policy does not subsume
monetary policy since monetary policy does not con-
trol budget or taxation, or any of their hyponyms.
Procedure 4.5 Merge a structure of concepts with
the rest of the knowledge base.
It is possible that structures consisting of several
inter-connected concepts are formed in isolation of
the main knowledge base as a result of some proce-
dures. The task here is to merge such structures with
the main knowledge base such that the new knowl-
edge base will be consistent with both the struc-
ture and the main knowledge base. This is done by
</bodyText>
<page confidence="0.985458">
272
</page>
<figure confidence="0.971581117647059">
pocy made by 2- government
IS-A
economic policy ,
monetary policy made by -
..,
IS-A
- - federal government
IS-A
IS-A
PART OF
con rols
economy
controls )- money supply
fiscal policy , made by )- government
controls )- budget
s, &gt;â€¢ taxation
s-1. controls
</figure>
<figureCaption confidence="0.999642">
Figure 4: Classification of the new concept monetary policy
</figureCaption>
<figure confidence="0.994171545454545">
WordNet The new structure from text WordNet
before merger after merger
work place
t IS-A
exchange IS-A V N IS-A
indistry
I IS-A
maret
[ISA
financial market
work place industry fin arket
tIS-A f IS-A + IS-A IS-A
exchange market
IIS-A IS-A
stock market money market
capitarket money market
IS-A
stock market
money market
capital market
IS-A
stock market
</figure>
<figureCaption confidence="0.983898">
Figure 5: Merging a structure of concepts with WordNet
</figureCaption>
<bodyText confidence="0.986780862068966">
IS-A
bridging whenever possible the structure concepts
and the main knowledge base concepts. It is possi-
ble that as a result of this merging procedure, some
HYPERNYMY relations either from the structure or
the main knowledge base will be destroyed to keep
the consistency. An example is shown in Figure 5.
Example : The following HYPERNYMY relation-
ships were discovered in Part 3:
HYPERNYMY(financial market,capital market)
HYPERNYMY(financial market,money market)
HYPERNYMY(capital market,stock market)
The structure obtained from these relationships
along with a part of WordNet hierarchy is shown
in Figure 5. An attempt is made to merge the new
structure with WordNet. To these relations it cor-
responds a structure as shown in Figure 5. An at-
tempt is made to merge this structure with Word-
Net. Searching WordNet for all concepts in the
structure we find money market and stock market
in WordNet where as capital market and financial
market are not. Figure 5 shows how the structure
merges with WordNet and moreover how concepts
that were unrelated in WordNet (i.e. stock market
and money market) become connected through the
new structure. It is also interesting to notice that the
IS-A link in WordNet from money market to market
is interrupted by the insertion of financial market
in-between them.
</bodyText>
<sectionHeader confidence="0.9834" genericHeader="method">
3 Implementation and Results
</sectionHeader>
<bodyText confidence="0.999949272727273">
The KAT Algorithm has been implemented, and
when given some seed concepts, it produces new con-
cepts, patterns and relationships between concepts
in an interactive mode. Table 1 shows the number
of concepts extracted from a 5000 sentence corpus,
in which each sentence contains at least one of the
five seed concepts.
The NPs were automatically searched in Word-
Net and other on-line dictionaries. There were 3745
distinct noun phrases of interest extracted; the rest
contained only the seeds or repetitions. Most of the
</bodyText>
<page confidence="0.997363">
273
</page>
<table confidence="0.994177964285714">
I Relations I Lexico-syntactic Patterns Examples
WordNet Relations
HYPERNYMy NPI 1&lt;be&gt;1 a kind of NP2 Thus, LIBOR is a kind of interest rate, as it is charged
HYPEanNY(NP1,NP2) on deposits between banks in the Eurodolar market.
New Relations 11
CAUSE NP1 1&lt;be&gt;1 cause NP2 Phillips, a British economist, stated in 1958 that high inflation
causE(NP1,NP2) causes low unemployment rates.
INFLUENCE NP1 impact on NP2 The Bank of Israel governor said that the tight economic policy
INFLuENcE(NP1,NP2) would have an immediate impact on inflation this year.
As NP1 vb, so &lt;do&gt; NP2 As the economy picks up steam, so does inflation.
INFLuENce(NP1,NP2)
NP1 &lt;be&gt; associated with NP2 Higher interest rates are normally associated with weaker bond markets.
INFIAJENce(NP1,NP2)
INFLueNcE(NP2,NP1)
As/if/when NP1 vbl, NP2 vb2. -I- On the other hand, if interest rates go down, bonds go up,
vbl, vb2 = antonyms / go in and your bond becomes more valuable.
opposite directions
INFLuENCE(NP1,NP2)
the effect(s) of NP1 on/upon NP2 The effects of inflation on debtors and creditors varies as the
iNFLuENCE(NP1,NP2) actual inflation is compared to the expected one.
inverse relationship between There exists an inverse relationship between unemployment rates
NP1 and NP2 and inflation, best illustrated by the Phillips Curve.
INFLuENcE(NP1,NP2)
INFLUENcE(NP2,NP1)
NP2 &lt;be&gt; function of NP1 Irish employment is also largely a function of the past
INFLuENcE(NP1,NP2) high birth rate.
NP1 (and thus NP2) We believe that the Treasury bonds (and thus interest rates)
r. iNFinENcE(NP1,NP2) are in a downward cycle.
</table>
<tableCaption confidence="0.971131">
Table 2: Examples of lexico-syntactic patterns and semantic relations derived from the 5000 sentence corpus
a b I_ c e
</tableCaption>
<table confidence="0.999834666666667">
Total potential 773 382 833 921 836
concepts (NPs)
Total concepts extracted with Procedurel
Concepts found 2 o 1 o 2
in WordNet
Concepts Concepts 6 0 3 o 0
found in with seed
on-line head
dictionaries,
but not in
Concepts 7 0 1 1 1
with seed
WordNet not head
Concepts accepted 78 62 58 60 _ 37
by human
</table>
<tableCaption confidence="0.983573333333333">
Table 1: Results showing the number of new concepts learned
from the corpus related to (a) interest rate, (b) stock market, (c)
inflation, (d) economic growth, and (e) employment.
</tableCaption>
<bodyText confidence="0.999688739130435">
processing in Part 1 is taken by the parser. The hu-
man intervention to accept or decline concepts takes
about 4 min./seed.
The next step was to search for lexico-syntactic
patterns. We considered one WordNet semantic re-
lation, HYPERNYMY and three other relations that
we found relevant for the domain, namely INFLU-
ENCE, CAUSE and EQUIVALENT. For each relation,
a pair of related words was selected and searched
for on the Internet. The first 500 sentences/relation
were retained. A human selected and validated semi-
automatically the patterns for each sentence. A sam-
ple of the results is shown in Table 2. A total of 22
patterns were obtained and their selection and vali-
dation took approximately 35 minutes/relation.
Next, the patterns are searched for on the 5000
sentence corpus (Part 3). The procedure provided
a total of 43 new concepts and 166 relationships
in which at least one of the seeds occurred. From
these relationships, by inspection, we have accepted
63 and rejected 102, procedure which took about 7
minutes. Table 3 lists some of the 63 relationships
discovered.
</bodyText>
<table confidence="0.999747807692308">
Relationships Examples 11
HYPEariviviv(interest rate, LIBOR)
ftvemmvmv(leading stock market,
New York Stock Exchange)
HYPERNYMY(market risks, interest rate risk)
nvinftwfmv(capital markets, stock markets)
CAUSE(inflation, unemployment)
cAnsE(labour shortage, wage inflation)
CAUSE(exCessive demand, inflation)
INFLUENCE_DIRECT_PROPORTIONALY(economy, inflation)
INFLuENcE_DIREcT_PRoPoRTIoNALY(settlements, interest rate)
INFLUENCE_DIRECT_PROPORTIONALY(U.S. interest rates, dollars)
INFLUENCE_DIRECT_PROPORTIONALY(Oil prices, inflation)
INFLUENCE_DIRECT_PROPORTIONALY(inflatiOn, nominal interest rates)
INFLuExcE_DIREcr_PRoPoaTIoNALY(deflation, real interest rates)
INFLUENCE_DIRECT_PROPORTIONALY(currencies,inflation)
INFLuENcE_tNvEase_PRoPowrioNaLv(unemployment rates, inflation)
INFLuEncE_INvErtsE_PaoPortTiondux(monetary policies, inflation)
INFLUENCE_INVERSE_PROPORTIONALY(econorny, interest rates)
iNFLuENcE...n4vEasE_PaoPoR.TioNALv(inflation, unemployment rates)
INFLUENCE_INVERSE_PROPORTIONALY(credit worthiness, interest rate)
INFLuENcEJNvEasE_PrtoPoterioNALY(interest rates, bonds)
INFLuExce(Internal Revenue Service, interest rates)
INFLUENCE(economic growth, share prices)
EQUIVALENT(big mistakes, high inflation rates of 1970s)
EQUIVALENT(fixed interest rate, coupon)
</table>
<tableCaption confidence="0.993846">
Table 3: A part of the relationships derived from the 5000
sentence corpus
</tableCaption>
<page confidence="0.997265">
274
</page>
<sectionHeader confidence="0.997319" genericHeader="method">
4 Applications
</sectionHeader>
<bodyText confidence="0.999848857142857">
An application in need of domain-specific knowledge
is Question Answering. The concepts and the rela-
tionships acquired can be useful in answering dif-
ficult questions that normally cannot be easily an-
swered just by using the information from WordNet.
Consider the processing of the following questions af-
ter the new domain knowledge has been acquired:
</bodyText>
<listItem confidence="0.7873004">
Q1: What factors have an impact on the interest
rate?
Q2: What happens with the employment when the
economic growth rises?
Q3: How does deflation influence prices?
</listItem>
<figure confidence="0.964006333333333">
interest rate
â€¢
C;Cc growth onoml-c)
</figure>
<figureCaption confidence="0.94459">
Figure 6: A sample of concepts and relations acquired from the
5000 sentence corpus. Legend: continue lines represent influence
inverse proportionally, dashed lines represent influence direct
proportionally, and dotted lines represent influence (the direction
of the relationship was not specified in the text).
</figureCaption>
<bodyText confidence="0.999551095238095">
Figure 6 shows a portion of the new domain
knowledge that is relevant to these questions. The
first question can be easily answered by extracting
the relationships that point to the concept interest
rate. The factors that influence the interest rate are
Fed, inflation, economic growth, and employment.
The last two questions ask for more detailed infor-
mation about the complex relationship among these
concepts. Following the path from the deflation con-
cept up to prices, the system learns that deflation in-
fluences direct proportionally real interest rate, and
real interest rate has an inverse proportional impact
on prices. Both these relationships came from the
sentence: Thus, the deflation and the real interest
rate are positively correlated, and so a higher real
interest rate leads to falling prices.
This method may be adapted to acquire infor-
mation when the question concepts are not in the
knowledge base. Procedures may be invoked to dis-
cover these concepts and the relations in which they
may be used.
</bodyText>
<sectionHeader confidence="0.999636" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999979291666667">
The knowledge acquisition technology described
above is applicable to any domain, by simply select-
ing appropriate seed concepts. We started with five
concepts interest rate, stock market, inflation, eco-
nomic growth, and employment and from a corpus
of 5000 sentences we acquired a total of 362 con-
cepts of which 319 contain the seeds and 43 relate
to these via selected relations. There were 22 dis-
tinct lexico-syntactic patterns discovered used in 63
instances. Most importantly, the new concepts can
be integrated with an existing ontology.
The method works in an interactive mode where
the user accepts or declines concepts, patterns and
relationships. The manual operation took on aver-
age 40 minutes per seed for the 5000 sentence corpus.
KAT is useful considering that most of the knowl-
edge base construction today is done manually.
Complex linguistic phenomena such as corefer-
ence resolution, word sense disambiguation, and oth-
ers have to be dealt with in order to increase the
automation of the knowledge acquisition system.
Without a good handling of these problems the re-
sults are not always accurate and human interven-
tion is necessary.
</bodyText>
<sectionHeader confidence="0.998521" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999499378378379">
Christiane Fellbaum. WordNet - An Electronic Lexical
Database, MIT Press, Cambridge, MA, 1998.
Marti Hearst. Automated Discovery of WordNet Rela-
tions. In WordNet: An Electronic Lexical Database
and Some of its Applications, editor Fellbaum, C.,
MIT Press, Cambridge, MA, 1998.
J. Kim and D. Moldovan. Acquisition of Linguistic
Patterns for knowledge-based information extraction.
IEEE Transactions on Knowledge and Data Engineer-
ing 7(5): pages 713-724.
R. MacGregor. A Description Classifier for the Predicate
Calculus. Proceedings of the 12th National Conference
on Artificial Intelligence (AAAI94), pp. 213-220, 1994.
Stephen D. Richardson, William B. Dolan, Lucy Vander-
wende. MindNet: acquiring and structuring seman-
tic information from text. Proceedings of ACL-Coling
1998, pages 1098-1102.
Ellen Riloff. Automatically Generating Extraction Pat-
terns from Untagged Text. In Proceedings of the Thir-
teenth National Conference on Artificial Intelligence,
1044-1049. The AAAI Press/MIT Press.
J.G. Schmolze and T. Lipkis. Classification in the KL-
ONE knowledge representation system. Proceedings
of 8th Int&apos;l Joint Conference on Artificial Intelligence
(IJCAI83), 1983.
S. Soderland. Learning to extract text-based informa-
tion from the world wide web. In the Proceedings of
the Third International Conference on Knowledge Dis-
covery and Data Mining (KDD-97).
Text REtrieval Conference. http://trec.nist.gov 1999
W.A. Woods. Understanding Subsumption and Taxon-
omy: A Framework for Progress. In the Principles
of Semantic Networks: Explorations in the Represen-
tation of Knowledge, Morgan Kaufmann, San Mateo,
Calif. 1991, pages 45-94.
W.A. Woods. A Better way to Organize Knowledge.
Technical Report of Sun Microsystems Inc., 1997.
</reference>
<figure confidence="0.997558714285714">
stock
market
InterC_Ils_Dt rate
IS-A
(deflation
CITsflatiorD
Ceâ€”râ€”nployment)
</figure>
<page confidence="0.972097">
275
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000049">
<title confidence="0.999835">Domain-Specific Knowledge Acquisition from Text</title>
<author confidence="0.996657">Dan Moldovan</author>
<author confidence="0.996657">Roxana Girju</author>
<author confidence="0.996657">Vasile Rus</author>
<affiliation confidence="0.999872">Department of Computer Science and Engineering University of Southern Methodist University</affiliation>
<address confidence="0.999883">Dallas, Texas, 75275-0122</address>
<email confidence="0.999592">moldovan@seas.smu.edu</email>
<email confidence="0.999592">roxana@seas.smu.edu</email>
<email confidence="0.999592">rus@seas.smu.edu</email>
<abstract confidence="0.990086804459692">knowledge intensive applications, it is necessary to have extensive domain-specific knowledge in addition to general-purpose knowledge bases. This paper presents a methodology for discovering domain-specific concepts and relationships in an attempt to extend WordNet. The method was tested on five seed concepts selected from the financial rate, stock market, inflation, ecogrowth, 1 Desiderata for Automated Knowledge Acquisition The need for knowledge The knowledge is infinite and no matter how large a knowledge base is, it is not possible to store all the concepts and procedures for all domains. Even if that were possible, the knowledge is generative and there are no guarantees that a system will have the latest information all the time. And yet, if we are to build common-sense knowledge processing systems in the future, it is necessary to have general-purpose and domain-specific knowledge that is up to date. Our inability to build large knowledge bases without much effort has impeded many ANLP developments. The most successful current Information Extraction systems rely on hand coded linguistic rules representing lexico-syntactic patterns capable of matching natural language expressions of events. Since the rules are hand-coded it is difficult to port systems across domains. Question answering, inference, summarization, and other applications can benefit from large linguistic knowledge bases. The basic idea A possible solution to the problem of rapid development of flexible knowledge bases is to design an automatic knowledge acquisition system that extracts knowledge from texts for the purpose of merging it with a core ontological knowledge base. The attempt to create a knowledge base manually is time consuming and error prone, even for small application domains, and we believe that automatic knowledge acquisition and classification is the only viable solution to large-scale, knowledge intensive applications. This paper presents an interactive method that acquires new concepts and connections associated with and adds them to the WordNet linguistic knowledge structure (Fellbaum 1998). The sources of the new knowledge are texts acquired from the Internet or other corpora. At the present time, our system works in a semi-automatic mode, in the sense that it acquires concepts and relations automatically, but their validation is done by the user. We believe that domain knowledge should not be acquired in a vacuum; it should expand an existent ontology with a skeletal structure built on consistent and acceptable principles. The method presented in this paper is applicable to any Machine Readable Dictionary. However, we chose WordNet because it is freely available and widely used. Related work This work was inspired in part by Marti Hearst&apos;s paper (Hearst 1998) where she discovers manually patterns for the relation in WordNet. Much of the work in pattern extraction from texts was done for improving the performance of Information Extraction systems. Research in this area was done by (Kim and Moldovan 1995) (Riloff 1996), (Soderland 1997) and others. The MindNet (Richardson 1998) project at Microsoft is an attempt to transform the Longman Dictionary of Contemporary English (LDOCE) into a form of knowledge base for text processing. Woods studied knowledge representation and classification for long time (Woods 1991), and more recently is trying to automate the construction of taxonomies by extracting concepts directly from texts (Woods 1997). The Knowledge Acquisition from Text (KAT) system is presented next. It consists of four parts: (1) discovery of new concepts, (2) discovery of new lexical patterns, (3) discovery of new relationships reflected by the lexical patterns, and (4) the classification and integration of the knowledge discovered with a WordNet like knowledge base. 268 2 KAT System 2.1 Discover new concepts seed concepts. domain knowledge can be acquired around some seed concepts that a user considers important. In this paper we focus on the domain, and use: rate, stock marinflation, economic growth, as seed concepts. The knowledge we seek to acquire relates to one or more of these concepts, and consists of new concepts not defined in WordNet and new relations that link these concepts with other concepts, some of which are in WordNet. example, from the sentence: the US enters mortgage interest rates rise, system discovers: (1) the new concept rate defined in WordNet but related to seed concept and (2) the state of economy the value of interest in a RELATIONSHIP. In WordNet, a concept is represented as a synset that contains words sharing the same meaning. In our experiments, we extend the seed words to their synset. For example, market is with exchange market, and we aim to learn concepts related to all these not only to market. sentences. are formed with each concept to documents from the Internet and other possible sources. The documents retrieved are further processed such that only the sentences that contain the seed concepts are retained. This an arbitrarily large corpus formed of sentences containing the seed concepts. We limit the size of this corpus to 1000 &apos;sentences per seed concept. sentences. sentence in this corpus is first part-of-speech (POS) tagged then parsed. We use Brill&apos;s POS tagger and our own parser. The output of the POS tagger for the example above is: the/DT economy/NN ena/DT mortgage/NN ./. The syntactic parser output is: (S (SBAR (WHADVP (WRB (NP (DT (vP (vsz enters) (NN boom) (â€ž))))) (NN (NNS interest_rates)) (vP (vBP rise))) new concepts. this paper only noun concepts are considered. Since, most likely, oneword nouns are already defined in WordNet, the fohere is nouns with modhave meaning but are not in WordNet. The new concepts directly related to the seeds are extracted from the noun phrases (NPs) that contain the seeds. In the example above, we see that the belongs to the NP: interest rate. This way, a list of NPs containing the seeds is assembled automatically from the parsed texts. Every such NP is considered a potential new concept. This is only the &amp;quot;raw material&amp;quot; from which actual concepts are discovered. In some noun phrases the seed is the head noun, [word, word can be a noun or adjective. For example, rate] in Wordbut term nominal interest rate] not in WordNet. Most of the new concepts related to a seed are generated this way. In other cases the seed not the head noun i.e. [word, .seed, word, For example rate peg], [international interest rate differential]. The following procedures are used to discover concepts, and are applicable in both cases: 1.1.WordNet reduction. NP for words collocations that are defined in WordNet as Thus term interest becomes interest_rate], [prime interest beall hyphenated concepts are in WordNet. 1.2.Dictionary reduction. each NP, search further in other on-line dictionaries for more compound concepts, and if found, hyphenate the words. Many domain-specific dictionaries available on-line. For example, interit is in the on-line dictionary Dictionar- 1.3.User validation. currently we lack a formal definition of a concept, it is not possible to completely automate the discovery of concepts. The human inspects the list of noun phrases and decides whether to accept or decline each concept. Discover patterns Texts represent a rich source of information from which in addition to concepts we can also discover relations between concepts. We are interested in discovering semantic relationships that link the concepts extracted above with other concepts, some of which may be in WordNet. The approach is to search for lexico-syntactic patterns comprising the concepts of interest. The semantic relations from WordNet are the first we search for, as it is only natural to add more of these relations to enhance the WordNet knowledge base. However, since the focus is on the acquisition of domain-specific knowledge, there are semantic relations between concepts other than the WordNet relations that are important. These new relations can be discovered automatically from the clauses and sentences in which the seeds occur. 269 a semantic relation can be Word- Net semantic relations or any other relations defined by the user. So far, we have experimented with the so-called Is-A) relation, and three other relations. By inspecting a few sencontaining rate can notice that a frequently used relation. The two relations are EQUIVALENT. a pair of concepts among which holds. may be any noun concepts. In the context of finance domain, some examples of conlinked by the are: rate worthiness rate. lexico-syntactic patterns P any corpus from all inwhere Ci and in the same sentence. Extract the lexico-syntactic patterns that link the concepts. For example, from the sentence: indicates the earningsfrom several rate scenarios,the generally applicable pattern extracted is: NP2 from NPI This pattern corresponds unambiguously to the restarted with, namely we conclude: INFLuENcE(NP1, NP2). example is: the worthinessdethe rateincreases. this sentence we extract another lexical pattern that exthe [as NP1 vbl, NP2 vb2] &amp; [vb1 and vb2 are antonyms] This pattern is rather complex since it contains not only the lexical part but also the verb condition that needs to be satisfied. This procedure repeats for all relations R. 2.3 Discover new relationships between concepts Let us denote with C, the seed-related concepts found with Procedures 1.1 through 1.3. We search corpus the occurrence of patterns P discovered above such that one of their two concepts is concept corpus a pattern P. a lexicopattern P, one at a time, search corpus for its occurrence. If found, search further whether or not one of the NPs is a seed-related concept C,. new concepts Part of the pattern P two noun phrases, one of which is The head from the other noun phrase is a concept we are looking for. This may be a WordNet concept, and if it is not it will be added to the list of concepts discovered. relation â€ž each pattern P is a linguistic expression of its corresponding semanrelation conclude (this is in- &amp;quot;C, is relation Câ€ž)&amp;quot;). steps are repeated for all patterns. User intervention to accept or reject relationships is necessary mainly due to our system inability of handling coreference resolution and other complex linguistic phenomena. 2.4 Knowledge classification and integration Next, a taxonomy needs to be created that is consistent with WordNet. In addition to creating a taxonomy, this step is also useful for validating the concepts acquired above. The classification is based the (Schmolze and Lipids 1983), (Woods 1991). This algorithm provides the overall steps for the classification of concepts within the context of Word- Net. Figure 1 shows the inputs of the Classification Algorithm and suggests that the classification is an iterative process. In addition to WordNet, the inconsist of the corpus sets of concepts and the relationships R.. Let&apos;s denote with = C, U the union of the seed related concepts with the new concepts. All these concepts need to be classified. WordNet Corpora A Concepts Cs, Co Relationships R I 1 1 Knowledge Classification t=k Algorithm t=k+1 Base Figure 1: The knowledge classification diagram 1. the set of relationships 7Z, discovered Part 3, pick all the From the way these relations were developed, there are two possibilities: A links a WordNet concept with another concept from the set C denoted or A links a concept concept are immediately linked to Word- Net and added to the knowledge base. The concepts from case (2) are also added to the knowledge base but they form at this point only some isolated islands since are not yet linked to the rest of the knowledge base. 2. corpus all the patterns assowith the that may link 270 [Asian_country interest_rate IIS-A IIS-A [Japan discount_rate a) [country interest_rate IS-A [Japan discount_rate [Germany prime_interest_rate b) classification of two concepts in the set with any WordNet concepts. concepts not seed-based concepts, are related to at least one concept via a relationship (as found in Task 3). Here we seek to find between them and WordNet con- If such concepts exist, denote them with The union Cm&apos;, = CL represents all concepts from the set C that are linked to WordNet without any further effort. We focus now on the rest concepts, C, = are not yet linked to any WordNet concepts. 3. all concepts in set using Procedures 4.1 through 4.5 below. Step 3 for all the concepts in set several times till no more changes occur. This reclassification is necessary since the insertion of a concept into the knowledge base may perturb the ordering of other surrounding concepts in the hierarchy. 5. the rest of relationships R. other the the new knowledge base. have already been used in the Classification Algorithm, but the other relai.e. CAUSE to be added to the knowledge base. Concept classification procedures 4.1.Classify of the form [word, head] with respect to concept [head]. It is assumed here that the [head] concept exists in WordNet simply because in many instances the &amp;quot;head&amp;quot; is the &amp;quot;seed&amp;quot; concept, and because frequently the head is a single word common noun usually defined in WordNet. In this procedure we consider only those head nouns that do not have any hyponyms since the other case when the head has other concepts under it is more complex and is treated by Procedure 4.4. Here &amp;quot;word&amp;quot; is a noun or an adjective. The classification is based on the simple idea that a compound concept [word, head] is ontologically subsumed by concept [head]. For exama kind of linked by a relation mortgage_interest_rate). 4.2.Classify a concept [wordi, headd respect another concept [word2, For a relative classification of two such concepts, the ontological relations between headi and head2 and and word2, if exist, are extended to the two concepts. We distinguish here three possibilities: 1. headi subsumes head2 and wordi subsumes In this case [wordi, headi] subsumes subsumption may not always be a direct connection; sometimes it may consist of a chain of subsumption relations since subsumption is (usually) a transitive relation (Woods 1991). An example is shown in Fig- 2a; in WordNet, particular case of this is when is identical with head2. 2. Another case is when there is no direct subsumption relation in WordNet between wordi and/or headi and head2, but there are a common subsuming concepts, for each pair. When such concepts are found, pick the most specific common subsumer (MSCS) concepts of word&apos; and word2, and of head1 respectively. Then form a concept , word2), MSCS (head&apos;, head2)] place [word&apos; headi] and [word2 under it. This is exemplified in Figure 2b. In Ger- 3. In all other cases, no subsumption relation is established between the two concepts. For examwe cannot say whether dismore or less abstract then interest_rate. 4.3.Classify concept head]. Several possibilities exist: 1. When there is already a concept [word2 head] in the knowledge base under the [head], then place [word&apos; word2 head] under concept [word2 head]. When there is already a concept head] in the knowledge base under the [head], then place [wordi word2 head] under concept [wordi head]. 3. When both cases 1 and 2 are true then place word2 head] under both concepts. 271 When neither head] nor [word2 head] are in the knowledge base, then place [wordi word2 head] under the [head]. The example in Figure 3 corresponds to case 3. components are already identified and it is easy to map every verb into such a semantic relation. As far as the newly discovered concepts are concerned, their defining relations need to be retrieved from texts. Human assistance is required, at least for now, to pinpoint the most characteristic relations that define a concept. Below is a two step algorithm that we envision for relative classification of two concepts A and us denote with relathat define concepts A and These are similar to rolesets and values. 1. Extract relations (denoted by verbs) between concept and other gloss concepts. Cal AR02Ca2 BRb2Cb2 IS-A radio components IS-A IS-A automobile components IS-A automobile radio components 3: of a compound concept with respect to ARamCnm Since we do not deal here with the sentence semantics, it is not possible to completely determine the meaning of [word&apos; word2 head], as it may be either [((wordi word2) head)] or [(wordi (word2 head))] often depending on the sentence context. In the example of Figure 3 there is only one mean- [(automobile the case of skiing equipment] two valid interpretations, namely equipment] (skiing equipment)]. 4.4Classify a concept head] with to hierarchy under the [head]. The task here is to identify the most specific subsumer (MSS) from all the concepts under the head that subsumes [wordi, head]. By default, [wordi head] is placed under [head], however, since it may be more specific than other hyponyms of [head], a more complex classification analysis needs to be implemented. In the previous work on knowledge classification it was assumed that the concepts were accompanied by rolesets and values (Schmolze and Lipids 1983), (Woods 1991), and others. Knowledge classifiers are part of almost any knowledge representation system. However, the problem we face here is more difficult. While in build-by-hand knowledge representation systems, the relations and values defining concepts are readily available, here we have to extract them from text. Fortunately, one can take advantage of the glossary definitions that are associated with concepts in WordNet and other dictionaries. One approach is to identify a set of semantic relations into which the verbs used in the gloss definitions are mapped into for the purpose of working with a manageable set of relations that may describe the concepts restrictions. In WordNet these basic relations and only if Relations 1 &lt;i &lt;m. (b) or is a meronym of Cbi- Concept more relations than concept A, i.e. m &lt;n. Figure 4 it is shown the classification concept policy has been discovered. default this concept is placed under Howin WordNet there is a hierarchy policy policy - question where exactly to place policy this hierarchy. gloss of indicates that it is BY that it ecogrowth we simplified the explanation used of growth). of leads to relations BY Govtaxaconcept money found by Procedure 1.2 in several dictionaries, and its dictionary leads to relations BY Govsupply. Word- Government subsumes Government, PART necessary conditions satisfied for policy subsume monepolicy. policy not subsume policy monetary not conany of their hyponyms. 4.5Merge a structure concepts with the rest of the knowledge base. It is possible that structures consisting of several inter-connected concepts are formed in isolation of the main knowledge base as a result of some procedures. The task here is to merge such structures with the main knowledge base such that the new knowledge base will be consistent with both the structure and the main knowledge base. This is done by 272 pocy made by government economic policy , monetary policy made by - .., - federal government IS-A PART OF con rols economy controls )money supply fiscal policy , made by )government controls )budget &gt;â€¢ taxation s-1. controls 4: of the new concept policy WordNet The new structure from text WordNet before merger after merger work place t IS-A exchange IS-A V N IS-A indistry I IS-A maret [ISA financial market work place industry f IS-A + IS-A IS-A exchange market IS-A stock market money market capitarket money market IS-A stock market money market capital market IS-A stock market 5: a structure of concepts with WordNet IS-A bridging whenever possible the structure concepts and the main knowledge base concepts. It is possible that as a result of this merging procedure, some either from the structure or the main knowledge base will be destroyed to keep the consistency. An example is shown in Figure 5. : following relationships were discovered in Part 3: market) market) market) The structure obtained from these relationships along with a part of WordNet hierarchy is shown in Figure 5. An attempt is made to merge the new structure with WordNet. To these relations it corresponds a structure as shown in Figure 5. An attempt is made to merge this structure with Word- Net. Searching WordNet for all concepts in the we find market market WordNet where as market not. Figure 5 shows how the structure merges with WordNet and moreover how concepts were unrelated in WordNet (i.e. market market) connected through the new structure. It is also interesting to notice that the in WordNet from market interrupted by the insertion of market in-between them. 3 Implementation and Results The KAT Algorithm has been implemented, and when given some seed concepts, it produces new concepts, patterns and relationships between concepts in an interactive mode. Table 1 shows the number of concepts extracted from a 5000 sentence corpus, in which each sentence contains at least one of the five seed concepts. The NPs were automatically searched in Word- Net and other on-line dictionaries. There were 3745 distinct noun phrases of interest extracted; the rest contained only the seeds or repetitions. Most of the 273</abstract>
<title confidence="0.8279965">Relations I Patterns Examples WordNet Relations</title>
<abstract confidence="0.803052450980392">HYPERNYMy NPI 1&lt;be&gt;1 a kind of NP2 is kind of rate, as is charged on deposits between banks in the Eurodolar market. HYPEanNY(NP1,NP2) New Relations 11 CAUSE NP1 1&lt;be&gt;1 cause NP2 causE(NP1,NP2) Phillips, a British economist, stated in 1958 that high inflation low rates. INFLUENCE NP1 impact on NP2 INFLuENcE(NP1,NP2) Bank of Israel governor said that the economic would have an immediate impact on inflation this year. As NP1 vb, so &lt;do&gt; NP2 INFLuENce(NP1,NP2) the so does inflation. NP1 &lt;be&gt; associated with NP2 INFIAJENce(NP1,NP2) INFLueNcE(NP2,NP1) rates normally associated with weaker markets. NP1 vbl, NP2 -Ivbl, vb2 = antonyms / go in opposite directions INFLuENCE(NP1,NP2) the other hand, if rates down, up, and your bond becomes more valuable. the effect(s) of NP1 on/upon NP2 iNFLuENCE(NP1,NP2) effects of inflation debtors as the actual inflation is compared to the expected one. inverse relationship between exists an inverse relationship between rates inflation, illustrated by the Phillips Curve. NP1 and NP2 INFLuENcE(NP1,NP2) INFLUENcE(NP2,NP1) NP2 &lt;be&gt; function of NP1 INFLuENcE(NP1,NP2) employment also largely a function of the past rate. NP1 (and thus NP2) believe that the bonds thus rates) in downward cycle. 2: patterns and semantic relations derived from the 5000 sentence corpus a b I_ c e Total potential concepts (NPs) 773 382 833 921 836 Total concepts extracted with Procedurel Concepts found in WordNet 2 o 1 o 2 Concepts on-line dictionaries, in Concepts with seed head 6 0 3 o 0 Concepts with seed 7 0 1 1 1 WordNet not head Concepts accepted by human 78 62 58 60 _ 37 1: showing the number of new concepts learned the corpus related to (a) rate, market, (d) growth, (e) employment. processing in Part 1 is taken by the parser. The human intervention to accept or decline concepts takes about 4 min./seed. The next step was to search for lexico-syntactic patterns. We considered one WordNet semantic rethree other relations that found relevant for the domain, namely INFLU- CAUSE each relation, a pair of related words was selected and searched for on the Internet. The first 500 sentences/relation were retained. A human selected and validated semiautomatically the patterns for each sentence. A sample of the results is shown in Table 2. A total of 22 patterns were obtained and their selection and validation took approximately 35 minutes/relation. Next, the patterns are searched for on the 5000 sentence corpus (Part 3). The procedure provided a total of 43 new concepts and 166 relationships in which at least one of the seeds occurred. From these relationships, by inspection, we have accepted 63 and rejected 102, procedure which took about 7 minutes. Table 3 lists some of the 63 relationships discovered. Examples rate, ftvemmvmv(leading stock market,</abstract>
<author confidence="0.632414">York Stock</author>
<abstract confidence="0.9987382">interest rate risk) stock markets) INFLuENcE_DIREcT_PRoPoRTIoNALY(settlements, interest rate) rates, dollars) inflation) interest rates) INFLuExcE_DIREcr_PRoPoaTIoNALY(deflation, real interest rates) INFLUENCE_DIRECT_PROPORTIONALY(currencies,inflation) INFLuENcE_tNvEase_PRoPowrioNaLv(unemployment rates, inflation) inflation) rates) iNFLuENcE...n4vEasE_PaoPoR.TioNALv(inflation, unemployment rates) interest rate) INFLuENcEJNvEasE_PrtoPoterioNALY(interest rates, bonds) INFLuExce(Internal Revenue Service, interest rates) share prices) high inflation rates of 1970s) EQUIVALENT(fixed interest rate, coupon) 3: part of the relationships derived from the 5000 sentence corpus 274 4 Applications An application in need of domain-specific knowledge is Question Answering. The concepts and the relationships acquired can be useful in answering difficult questions that normally cannot be easily answered just by using the information from WordNet. Consider the processing of the following questions after the new domain knowledge has been acquired: What factors have an impact on the rate? What happens with the the growth How does interest rate â€¢ growthonoml-c) 6: sample of concepts and relations acquired from the 5000 sentence corpus. Legend: continue lines represent influence inverse proportionally, dashed lines represent influence direct and dotted lines represent direction of the relationship was not specified in the text). Figure 6 shows a portion of the new domain knowledge that is relevant to these questions. The first question can be easily answered by extracting relationships that point to the concept factors that influence the rate inflation, economic growth, The last two questions ask for more detailed information about the complex relationship among these Following the path from the conup to system learns that indirect proportionally interest rate, interest rate an inverse proportional impact these relationships came from the the deflation and the real interest rate are positively correlated, and so a higher real interest rate leads to falling prices. This method may be adapted to acquire information when the question concepts are not in the knowledge base. Procedures may be invoked to discover these concepts and the relations in which they may be used. 5 Conclusions The knowledge acquisition technology described above is applicable to any domain, by simply selecting appropriate seed concepts. We started with five rate, stock market, inflation, ecogrowth, from a corpus of 5000 sentences we acquired a total of 362 concepts of which 319 contain the seeds and 43 relate to these via selected relations. There were 22 distinct lexico-syntactic patterns discovered used in 63 instances. Most importantly, the new concepts can be integrated with an existing ontology. The method works in an interactive mode where the user accepts or declines concepts, patterns and relationships. The manual operation took on average 40 minutes per seed for the 5000 sentence corpus. KAT is useful considering that most of the knowledge base construction today is done manually. Complex linguistic phenomena such as coreference resolution, word sense disambiguation, and others have to be dealt with in order to increase the automation of the knowledge acquisition system. Without a good handling of these problems the results are not always accurate and human intervention is necessary.</abstract>
<note confidence="0.937880642857143">References Fellbaum. WordNet - An Lexical Cambridge, MA, 1998. Marti Hearst. Automated Discovery of WordNet Rela- In An Electronic Lexical Database of its Fellbaum, C., MIT Press, Cambridge, MA, 1998. J. Kim and D. Moldovan. Acquisition of Linguistic Patterns for knowledge-based information extraction. IEEE Transactions on Knowledge and Data Engineering 7(5): pages 713-724. R. MacGregor. A Description Classifier for the Predicate of the 12th Artificial Intelligence pp. 213-220, 1994.</note>
<author confidence="0.844477">Stephen D Richardson</author>
<author confidence="0.844477">William B Dolan</author>
<author confidence="0.844477">Lucy Vander-</author>
<abstract confidence="0.646525214285714">wende. MindNet: acquiring and structuring semantic information from text. Proceedings of ACL-Coling 1998, pages 1098-1102. Ellen Riloff. Automatically Generating Extraction Patfrom Untagged Text. In of the Thirteenth National Conference on Artificial Intelligence, 1044-1049. The AAAI Press/MIT Press. J.G. Schmolze and T. Lipkis. Classification in the KLknowledge representation system. of 8th Int&apos;l Joint Conference on Artificial Intelligence (IJCAI83), 1983. S. Soderland. Learning to extract text-based informafrom the world wide web. In the of the Third International Conference on Knowledge Disand Data Text REtrieval Conference. http://trec.nist.gov 1999 W.A. Woods. Understanding Subsumption and Taxon- A Framework for Progress. In the of Semantic Networks: Explorations in the Represenof Knowledge, Kaufmann, San Mateo, Calif. 1991, pages 45-94. Woods. Better way to Organize Knowledge. Technical Report of Sun Microsystems Inc., 1997. stock market InterC_Ils_Dt rate IS-A (deflation</abstract>
<intro confidence="0.484376">275</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet - An Electronic Lexical Database,</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="2448" citStr="Fellbaum 1998" startWordPosition="361" endWordPosition="362">omatic knowledge acquisition system that extracts knowledge from texts for the purpose of merging it with a core ontological knowledge base. The attempt to create a knowledge base manually is time consuming and error prone, even for small application domains, and we believe that automatic knowledge acquisition and classification is the only viable solution to large-scale, knowledge intensive applications. This paper presents an interactive method that acquires new concepts and connections associated with user-selected seed concepts, and adds them to the WordNet linguistic knowledge structure (Fellbaum 1998). The sources of the new knowledge are texts acquired from the Internet or other corpora. At the present time, our system works in a semi-automatic mode, in the sense that it acquires concepts and relations automatically, but their validation is done by the user. We believe that domain knowledge should not be acquired in a vacuum; it should expand an existent ontology with a skeletal structure built on consistent and acceptable principles. The method presented in this paper is applicable to any Machine Readable Dictionary. However, we chose WordNet because it is freely available and widely use</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. WordNet - An Electronic Lexical Database, MIT Press, Cambridge, MA, 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Automated Discovery of WordNet Relations.</title>
<date>1998</date>
<booktitle>In WordNet: An Electronic Lexical Database and Some of its Applications, editor Fellbaum, C.,</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="3132" citStr="Hearst 1998" startWordPosition="473" endWordPosition="474">or other corpora. At the present time, our system works in a semi-automatic mode, in the sense that it acquires concepts and relations automatically, but their validation is done by the user. We believe that domain knowledge should not be acquired in a vacuum; it should expand an existent ontology with a skeletal structure built on consistent and acceptable principles. The method presented in this paper is applicable to any Machine Readable Dictionary. However, we chose WordNet because it is freely available and widely used. Related work This work was inspired in part by Marti Hearst&apos;s paper (Hearst 1998) where she discovers manually le)dco-syntactic patterns for the HYPERNYMY relation in WordNet. Much of the work in pattern extraction from texts was done for improving the performance of Information Extraction systems. Research in this area was done by (Kim and Moldovan 1995) (Riloff 1996), (Soderland 1997) and others. The MindNet (Richardson 1998) project at Microsoft is an attempt to transform the Longman Dictionary of Contemporary English (LDOCE) into a form of knowledge base for text processing. Woods studied knowledge representation and classification for long time (Woods 1991), and more </context>
</contexts>
<marker>Hearst, 1998</marker>
<rawString>Marti Hearst. Automated Discovery of WordNet Relations. In WordNet: An Electronic Lexical Database and Some of its Applications, editor Fellbaum, C., MIT Press, Cambridge, MA, 1998.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J Kim</author>
<author>D Moldovan</author>
</authors>
<title>Acquisition of Linguistic Patterns for knowledge-based information extraction.</title>
<journal>IEEE Transactions on Knowledge and Data Engineering</journal>
<volume>7</volume>
<issue>5</issue>
<pages>713--724</pages>
<marker>Kim, Moldovan, </marker>
<rawString>J. Kim and D. Moldovan. Acquisition of Linguistic Patterns for knowledge-based information extraction. IEEE Transactions on Knowledge and Data Engineering 7(5): pages 713-724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R MacGregor</author>
</authors>
<title>A Description Classifier for the Predicate Calculus.</title>
<date>1994</date>
<booktitle>Proceedings of the 12th National Conference on Artificial Intelligence (AAAI94),</booktitle>
<pages>213--220</pages>
<marker>MacGregor, 1994</marker>
<rawString>R. MacGregor. A Description Classifier for the Predicate Calculus. Proceedings of the 12th National Conference on Artificial Intelligence (AAAI94), pp. 213-220, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen D Richardson</author>
<author>William B Dolan</author>
<author>Lucy Vanderwende</author>
</authors>
<title>MindNet: acquiring and structuring semantic information from text.</title>
<date>1998</date>
<booktitle>Proceedings of ACL-Coling</booktitle>
<pages>1098--1102</pages>
<marker>Richardson, Dolan, Vanderwende, 1998</marker>
<rawString>Stephen D. Richardson, William B. Dolan, Lucy Vanderwende. MindNet: acquiring and structuring semantic information from text. Proceedings of ACL-Coling 1998, pages 1098-1102.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ellen Riloff</author>
</authors>
<title>Automatically Generating Extraction Patterns from Untagged Text.</title>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1044--1049</pages>
<publisher>The AAAI Press/MIT Press.</publisher>
<marker>Riloff, </marker>
<rawString>Ellen Riloff. Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence, 1044-1049. The AAAI Press/MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Schmolze</author>
<author>T Lipkis</author>
</authors>
<title>Classification in the KLONE knowledge representation system.</title>
<date>1983</date>
<booktitle>Proceedings of 8th Int&apos;l Joint Conference on Artificial Intelligence (IJCAI83),</booktitle>
<marker>Schmolze, Lipkis, 1983</marker>
<rawString>J.G. Schmolze and T. Lipkis. Classification in the KLONE knowledge representation system. Proceedings of 8th Int&apos;l Joint Conference on Artificial Intelligence (IJCAI83), 1983.</rawString>
</citation>
<citation valid="false">
<authors>
<author>S Soderland</author>
</authors>
<title>Learning to extract text-based information from the world wide web.</title>
<booktitle>In the Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (KDD-97).</booktitle>
<marker>Soderland, </marker>
<rawString>S. Soderland. Learning to extract text-based information from the world wide web. In the Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (KDD-97).</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>Text REtrieval Conference. http://trec.nist.gov</booktitle>
<marker>1999</marker>
<rawString>Text REtrieval Conference. http://trec.nist.gov 1999</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>Understanding Subsumption and Taxonomy: A Framework for Progress.</title>
<date>1991</date>
<booktitle>In the Principles of Semantic Networks: Explorations in the Representation of Knowledge,</booktitle>
<pages>45--94</pages>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, Calif.</location>
<contexts>
<context position="3721" citStr="Woods 1991" startWordPosition="565" endWordPosition="566">&apos;s paper (Hearst 1998) where she discovers manually le)dco-syntactic patterns for the HYPERNYMY relation in WordNet. Much of the work in pattern extraction from texts was done for improving the performance of Information Extraction systems. Research in this area was done by (Kim and Moldovan 1995) (Riloff 1996), (Soderland 1997) and others. The MindNet (Richardson 1998) project at Microsoft is an attempt to transform the Longman Dictionary of Contemporary English (LDOCE) into a form of knowledge base for text processing. Woods studied knowledge representation and classification for long time (Woods 1991), and more recently is trying to automate the construction of taxonomies by extracting concepts directly from texts (Woods 1997). The Knowledge Acquisition from Text (KAT) system is presented next. It consists of four parts: (1) discovery of new concepts, (2) discovery of new lexical patterns, (3) discovery of new relationships reflected by the lexical patterns, and (4) the classification and integration of the knowledge discovered with a WordNet - like knowledge base. 268 2 KAT System 2.1 Discover new concepts Select seed concepts. New domain knowledge can be acquired around some seed concept</context>
<context position="12327" citStr="Woods 1991" startWordPosition="1988" endWordPosition="1989">tion R, we conclude R(C8, Cn) (this is interpreted &amp;quot;C, is relation R Câ€ž)&amp;quot;). These steps are repeated for all patterns. User intervention to accept or reject relationships is necessary mainly due to our system inability of handling coreference resolution and other complex linguistic phenomena. 2.4 Knowledge classification and integration Next, a taxonomy needs to be created that is consistent with WordNet. In addition to creating a taxonomy, this step is also useful for validating the concepts acquired above. The classification is based on the subsumption principle (Schmolze and Lipids 1983), (Woods 1991). This algorithm provides the overall steps for the classification of concepts within the context of WordNet. Figure 1 shows the inputs of the Classification Algorithm and suggests that the classification is an iterative process. In addition to WordNet, the inputs consist of the corpus A, the sets of concepts C, and Cn, and the relationships R.. Let&apos;s denote with C = C, U Cn the union of the seed related concepts with the new concepts. All these concepts need to be classified. WordNet Corpora A Concepts Cs, Co Relationships R I 1 1 Knowledge Classification t=k Algorithm t=k+1 1 Knowledge Base </context>
<context position="16461" citStr="Woods 1991" startWordPosition="2686" endWordPosition="2687">nterest_rate). Procedure 4.2. Classify a concept [wordi, headd with respect to another concept [word2, head2]. For a relative classification of two such concepts, the ontological relations between headi and head2 and between wordi and word2, if exist, are extended to the two concepts. We distinguish here three possibilities: 1. headi subsumes head2 and wordi subsumes word2. In this case [wordi, headi] subsumes [word2, head2]. The subsumption may not always be a direct connection; sometimes it may consist of a chain of subsumption relations since subsumption is (usually) a transitive relation (Woods 1991). An example is shown in Figure 2a; in WordNet, Asian_country subsumes Japan and interest_rate subsumes discount_rate. A particular case of this is when head&apos; is identical with head2. 2. Another case is when there is no direct subsumption relation in WordNet between wordi and word2, and/or headi and head2, but there are a common subsuming concepts, for each pair. When such concepts are found, pick the most specific common subsumer (MSCS) concepts of word&apos; and word2, and of head1 and head2, respectively. Then form a concept [MSCS(wordi , word2), MSCS (head&apos;, head2)] and place [word&apos; headi] and </context>
<context position="20025" citStr="Woods 1991" startWordPosition="3269" endWordPosition="3270">nd [performance (skiing equipment)]. Procedure 4.4 Classify a concept [wordi, head] with respect to a concept hierarchy under the [head]. The task here is to identify the most specific subsumer (MSS) from all the concepts under the head that subsumes [wordi, head]. By default, [wordi head] is placed under [head], however, since it may be more specific than other hyponyms of [head], a more complex classification analysis needs to be implemented. In the previous work on knowledge classification it was assumed that the concepts were accompanied by rolesets and values (Schmolze and Lipids 1983), (Woods 1991), and others. Knowledge classifiers are part of almost any knowledge representation system. However, the problem we face here is more difficult. While in build-by-hand knowledge representation systems, the relations and values defining concepts are readily available, here we have to extract them from text. Fortunately, one can take advantage of the glossary definitions that are associated with concepts in WordNet and other dictionaries. One approach is to identify a set of semantic relations into which the verbs used in the gloss definitions are mapped into for the purpose of working with a ma</context>
</contexts>
<marker>Woods, 1991</marker>
<rawString>W.A. Woods. Understanding Subsumption and Taxonomy: A Framework for Progress. In the Principles of Semantic Networks: Explorations in the Representation of Knowledge, Morgan Kaufmann, San Mateo, Calif. 1991, pages 45-94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Woods</author>
</authors>
<title>A Better way to Organize Knowledge.</title>
<date>1997</date>
<tech>Technical Report of</tech>
<institution>Sun Microsystems Inc.,</institution>
<contexts>
<context position="3849" citStr="Woods 1997" startWordPosition="586" endWordPosition="587"> work in pattern extraction from texts was done for improving the performance of Information Extraction systems. Research in this area was done by (Kim and Moldovan 1995) (Riloff 1996), (Soderland 1997) and others. The MindNet (Richardson 1998) project at Microsoft is an attempt to transform the Longman Dictionary of Contemporary English (LDOCE) into a form of knowledge base for text processing. Woods studied knowledge representation and classification for long time (Woods 1991), and more recently is trying to automate the construction of taxonomies by extracting concepts directly from texts (Woods 1997). The Knowledge Acquisition from Text (KAT) system is presented next. It consists of four parts: (1) discovery of new concepts, (2) discovery of new lexical patterns, (3) discovery of new relationships reflected by the lexical patterns, and (4) the classification and integration of the knowledge discovered with a WordNet - like knowledge base. 268 2 KAT System 2.1 Discover new concepts Select seed concepts. New domain knowledge can be acquired around some seed concepts that a user considers important. In this paper we focus on the financial domain, and use: interest rate, stock market, inflati</context>
</contexts>
<marker>Woods, 1997</marker>
<rawString>W.A. Woods. A Better way to Organize Knowledge. Technical Report of Sun Microsystems Inc., 1997.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>