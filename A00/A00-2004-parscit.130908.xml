<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000018">
<title confidence="0.970278">
Advances in domain independent linear text segmentation
</title>
<author confidence="0.99774">
Freddy Y. Y. Choi
</author>
<affiliation confidence="0.996520666666667">
Artificial Intelligence Group
Department of Computer Science
University of Manchester
</affiliation>
<address confidence="0.841571">
Manchester, England
</address>
<email confidence="0.929224">
choifths.man.ac.uk
</email>
<sectionHeader confidence="0.972652" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999044333333333">
This paper describes a method for linear text seg-
mentation which is twice as accurate and over seven
times as fast as the state-of-the-art (Reynar, 1998).
Inter-sentence similarity is replaced by rank in the
local context. Boundary locations are discovered by
divisive clustering.
</bodyText>
<sectionHeader confidence="0.996863" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999871714285714">
Even moderately long documents typically address
several topics or different aspects of the same topic.
The aim of linear text segmentation is to discover
the topic boundaries. The uses of this procedure
include information retrieval (Hearst and Plaunt,
1993; Hearst, 1994; Yaari, 1997; Reynar, 1999),
summarization (Reynar, 1998), text understanding,
anaphora resolution (Kozima, 1993), language mod-
elling (Morris and Hirst, 1991; Beeferman et al.,
1997b) and improving document navigation for the
visually disabled (Choi, 2000).
This paper focuses on domain independent meth-
ods for segmenting written text. We present a new
algorithm that builds on previous work by Reynar
(Reynar, 1998; Reynar, 1994). The primary distinc-
tion of our method is the use of a ranking scheme
and the cosine similarity measure (van Rijsbergen,
1979) in formulating the similarity matrix. We pro-
pose that the similarity values of short text segments
is statistically insignificant. Thus, one can only rely
on their order, or rank, for clustering.
</bodyText>
<sectionHeader confidence="0.944588" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999747833333334">
Existing work falls into one of two categories, lexical
cohesion methods and multi-source methods (Yaari,
1997). The former stem from the work of Halliday
and Hasan (Halliday and Hasan, 1976). They pro-
posed that text segments with similar vocabulary
are likely to be part of a coherent topic segment.
Implementations of this idea use word stem repe-
tition (Youmans, 1991; Reynar, 1994; Ponte and
Croft, 1997), context vectors (Hearst, 1994; Yaar-
i, 1997; Kaufmann, 1999; Eichmann et al., 1999),
entity repetition (Kan et al., 1998), semantic simi-
larity (Morris and Hirst, 1991; Kozima, 1993), word
distance model (Beeferman et al., 1997a) and word
frequency model (Reynar, 1999) to detect cohesion.
Methods for finding the topic boundaries include s-
liding window (Hearst, 1994), lexical chains (Mor-
ris, 1988; Kan et al., 1998), dynamic programming
(Ponte and Croft, 1997; Heinonen, 1998), agglomer-
ative clustering (Yaari, 1997) and divisive clustering
(Reynar, 1994). Lexical cohesion methods are typi-
cally used for segmenting written text in a collection
to improve information retrieval (Hearst, 1994; Rey-
nar, 1998).
Multi-source methods combine lexical cohesion
with other indicators of topic shift such as cue phras-
es, prosodic features, reference, syntax and lexical
attraction (Beeferman et al., 1997a) using decision
trees (Miike et al., 1994; Kurohashi and Nagao,
1994; Litman and Passonneau, 1995) and probabilis-
tic models (Beeferman et al., 1997b; Hajime et al.,
1998; Reynar, 1998). Work in this area is largely mo-
tivated by the topic detection and tracking (TDT)
initiative (Allan et al., 1998). The focus is on the
segmentation of transcribed spoken text and broad-
cast news stories where the presentation format and
regular cues can be exploited to improve accuracy.
</bodyText>
<sectionHeader confidence="0.997962" genericHeader="method">
3 Algorithm
</sectionHeader>
<bodyText confidence="0.999637714285714">
Our segmentation algorithm takes a list of tokenized
sentences as input. A tokenizer (Grefenstette and
Tapanainen, 1994) and a sentence boundary disam-
biguation algorithm (Palmer and Hearst, 1994; Rey-
nar and Ratnaparkhi, 1997) or EAGLE (Reynar et
al., 1997) may be used to convert a plain text docu-
ment into the acceptable input format.
</bodyText>
<subsectionHeader confidence="0.995553">
3.1 Similarity measure
</subsectionHeader>
<bodyText confidence="0.9930906">
Punctuation and uninformative words are removed
from each sentence using a simple regular expression
pattern matcher and a stopword list. A stemming
algorithm (Porter, 1980) is then applied to the re-
maining tokens to obtain the word stems. A dic-
tionary of word stem frequencies is constructed for
each sentence. This is represented as a vector of
frequency counts.
Let fi,i denote the frequency of word j in sentence
i. The similarity between a pair of sentences :1:, y
</bodyText>
<page confidence="0.99735">
26
</page>
<figureCaption confidence="0.900400727272727">
is computed using the cosine measure as shown in
equation 1. This is applied to all sentence pairs to
generate a similarity matrix.
Figure 1 shows an example of a similarity matrixl .
High similarity values are represented by bright pix-
els. The bottom-left and top-right pixel show the
self-similarity for the first and last sentence, respec-
tively. Notice the matrix is symmetric and contains
bright square regions along the diagonal. These re-
gions represent cohesive text segments.
Figure 1: An example similarity matrix.
</figureCaption>
<subsectionHeader confidence="0.984425">
3.2 Ranking
</subsectionHeader>
<bodyText confidence="0.988974303030303">
For short text segments, the absolute value of
sim(x, y) is unreliable. An additional occurrence of
a common word (reflected in the numerator) causes
a disproportionate increase in sim(x, y) unless the
denominator (related to segment length) is large.
Thus, in the context of text segmentation where a
segment has typically &lt; 100 informative tokens, one
can only use the metric to estimate the order of sim-
ilarity between sentences, e.g. a is more similar to b
than c.
Furthermore, language usage varies throughout a
document. For instance, the introduction section of
a document is less cohesive than a section which is
about a particular topic. Consequently, it is inap-
propriate to directly compare the similarity values
from different regions of the similarity matrix.
In non-parametric statistical analysis, one com-
pares the rank of data sets when the qualitative be-
haviour is similar but the absolute quantities are un-
reliable. We present a ranking scheme which is an
adaptation of that described in (O&apos;Neil and Denos,
1992).
&apos;The contrast of the image has been adjusted to highlight
the image features.
Each value in the similarity matrix is replaced by
its rank in the local region. The rank is the num-
ber of neighbouring elements with a lower similarity
value. Figure 2 shows an example of image ranking
using a 3 x 3 rank mask with output range {0, 8).
For segmentation, we used a 11 x 11 rank mask. The
output is expressed as a ratio r (equation 2) to cir-
cumvent normalisation problems (consider the cases
when the rank mask is not contained in the image).
</bodyText>
<figureCaption confidence="0.993928">
Figure 2: A working example of image ranking.
</figureCaption>
<bodyText confidence="0.869852">
# of elements with a lower value
</bodyText>
<equation confidence="0.881657">
r = (2)
# of elements examined
</equation>
<bodyText confidence="0.920888875">
To demonstrate the effect of image ranking, the
process was applied to the matrix shown in figure 1
to produce figure 32. Notice the contrast has been
improved significantly. Figure 4 illustrates the more
subtle effects of our ranking scheme. r(x) is the rank
(1 x 11 mask) of (x) which is a sine wave with
decaying mean, amplitude and frequency (equation
3).
</bodyText>
<figureCaption confidence="0.994999">
Figure 3: The matrix in figure 1 after ranking.
</figureCaption>
<footnote confidence="0.652869666666667">
2The process was applied to the original matrix, prior to
contrast enhancement. The output image has not been en-
hanced.
</footnote>
<figure confidence="0.922316181818182">
2 3 1 2 8
MEM
DUO MMEM
UE103 NOME
INIU6 ANN
Similarity matrix Rank matrix
Step
Similarity matrix
EIUM MEM
MOM11 MUM,
MUM3: MU
4 9
Step 3
flarilt matrix
1 6
1 5
7
Step 2
2 6
•
7 4
1
</figure>
<equation confidence="0.8792035">
E • fx,i x
sim(x, y)
(1)
\/Ei fx2, x
</equation>
<page confidence="0.843591">
27
</page>
<figure confidence="0.983566285714286">
f(:r) = g(x x 4) Rank matrix Step 2
g(z) = 1(e.&apos;/2 + le—z/2(1+ sin(10z&amp;quot;)))
(3)
09 r(x)
B 7
07
06
Scp 1 Step 3
os
04
03
02
01
50 100 160 200
</figure>
<figureCaption confidence="0.9937315">
Figure 4: An illustration of the more subtle effects
of our ranking scheme.
</figureCaption>
<subsectionHeader confidence="0.998344">
3.3 Clustering
</subsectionHeader>
<bodyText confidence="0.999967">
The final process determines the location of the topic
boundaries. The method is based on Reynar&apos;s max-
imisation algorithm (Reynar, 1998; Helfman, 1996;
Church, 1993; Church and Helfman, 1993). A text
segment is defined by two sentences i, j (inclusive).
This is represented as a square region along the di-
agonal of the rank matrix. Let si,j denote the sum of
the rank values in a segment and ai,j = (j —i +1)2
be the inside area. B = {b1, ...,197-4 is a list of in
coherent text segments. sk and ak refers to the sum
of rank and area of segment k in B. D is the inside
density of B (see equation 4).
</bodyText>
<equation confidence="0.989517666666667">
v,772
D = L&apos;k=1 sk
V-1771.
</equation>
<bodyText confidence="0.96415">
ak
To initialise the process, the entire document is
placed in B as one coherent text segment. Each step
of the process splits one of the segments in B. The
split point is a potential boundary which maximises
D. Figure 5 shows a working example.
The number of segments to generate, in, is deter-
mined automatically. Den) is the inside density of n
segments and SD(n) , Den) Den-1) is the gradient.
For a document with b potential boundaries, b step-
s of divisive clustering generates {D(1), ...,D(b+1)}
and {bD(2), oD(b+1)} (see figure 6 and 7). An
unusually large reduction in 6D suggests the opti-
inal clustering has been obtained3 (see n = 10 in
</bodyText>
<footnote confidence="0.8157245">
31n practice, convolution (mask {1, 2, 4, 8, 4, 2, 1}) is first
applied to 6D to smooth out sharp local changes
</footnote>
<figureCaption confidence="0.95492">
Figure 5: A working example of the divisive cluster-
ing algorithm.
figure 7). Let p and v be the mean and variance of
E {2, b + 1). m is obtained by applying
</figureCaption>
<bodyText confidence="0.456322">
the threshold, p+c x to dD (c= 1.2 works well
in practice)
</bodyText>
<figure confidence="0.998582181818182">
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
10 20 30 40 50 60 70 BO
Number of segments
</figure>
<figureCaption confidence="0.983094">
Figure 6: The inside density of all levels of segmen-
tation.
</figureCaption>
<subsectionHeader confidence="0.985647">
3.4 Speed optimisation
</subsectionHeader>
<bodyText confidence="0.9999288">
The running time of each step is dominated by the
computation of sk. Given si,j is constant, our algo-
rithm pre-computes all the values to improve speed
performance. The procedure computes the values a-
long diagonals, starting from the main diagonal and
</bodyText>
<figure confidence="0.889502">
(4)
28
I
70
30 40 50 so
Number of segments
</figure>
<figureCaption confidence="0.9952885">
Figure 7: Finding the optimal segmentation using
the gradient.
</figureCaption>
<bodyText confidence="0.998595714285714">
works towards the corner. The method has a com-
plexity of order 171-5.n2. Let ri,j refer to the rank value
in the rank matrix R and S to the sum of rank ma-
trix. Given R of size n X 77,, S is computed in three
steps (see equation 5). Figure 8 shows the result of
applying this procedure to the rank matrix in figure
5.
</bodyText>
<sectionHeader confidence="0.994413" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.999994555555556">
The definition of a topic segment ranges from com-
plete stories (Allan et al., 1998) to summaries (Ponte
and Croft, 1997). Given the quality of an algorithm
is task dependent, the following experiments focus
on the relative performance. Our evaluation strat-
egy is a variant of that described in (Reynar, 1998,
71-73) and the TDT segmentation task (Allan et al.,
1998). We assume a good algorithm is one that finds
the most prominent topic boundaries.
</bodyText>
<subsectionHeader confidence="0.988933">
4.1 Experiment procedure
</subsectionHeader>
<bodyText confidence="0.987095222222222">
An artificial test corpus of 700 samples is used to
assess the accuracy and speed performance of seg-
mentation algorithms. A sample is a concatenation
of ten text segments. A segment is the first n sen-
tences of a randomly selected document from the
Brown corpus&apos;. A sample is characterised by the
range of n. The corpus was generated by an auto-
matic procedure5. Table 1 presents the corpus s-
tatistics.
</bodyText>
<table confidence="0.986146">
Range of n 3— 11 3 — 5 6 — 8 9— 11
# samples 400 100 100 100
</table>
<tableCaption confidence="0.999571">
Table 1: Test corpus statistics.
</tableCaption>
<figure confidence="0.8056478">
0.05
0.04
3
3
0.02
0.01
(5)
Sum of rank matrix
91 67 46 33 ! 19 4
65 43 26 15 5 19 1
42 ; 28 13 4 15 33
30 18 5 13 i 26 46
-
15 5 18 28 1 43 67 1
4 IS 30 42 65 91
</figure>
<figureCaption confidence="0.9891365">
Figure 8: Improving speed performance by pre-
computing s.
</figureCaption>
<bodyText confidence="0.931194928571428">
p(erroriref, hyp, k) =
p(misslref, hyp, diff, k)p(diffl ref, k)+ (6)
p(fairef, hyp, same, k)p(samelref, k)
Speed performance is measured by the average
number of CPU seconds required to process a test
sample6. Segmentation accuracy is measured by the
error metric (equation 6, fa false alarms) proposed
in (Beeferman et al., 1999). Low error probability
indicates high accuracy. Other performance mea-
sures include the popular precision and recall metric
(PR) (Hearst, 1994), fuzzy PR (Reynar, 1998) and
edit distance (Ponte and Croft, 1997). The prob-
lems associated with these metrics are discussed in
(Beeferman et al., 1999).
</bodyText>
<subsectionHeader confidence="0.742222">
4.2 Experiment 1 - Baseline
</subsectionHeader>
<bodyText confidence="0.9009122">
Five degenerate algorithms define the baseline for
the experiments. B„ does not propose any bound-
aries. B„ reports all potential boundaries as real
boundaries. B„ partitions the sample into regular
segments. B(i.,?) randomly selects any number of
</bodyText>
<footnote confidence="0.8023445">
40nly the news articles ca**.pos and informative text
cj**.pos were used in the experiment.
5A11 experiment data, algorithms, scripts and detailed
re-
sults are available from the author.
6All experiments were conducted on a Pentium II 266MIlz
PC with 128Mb RAM running RedHat Linux 6.0 and the
Blackdown Linux port of .11)K1.1.7 v3.
</footnote>
<figure confidence="0.757640111111111">
1. si,i
for
2. si+1,i
si,i+i
for
si,i+j
for
Rank matrix
1 2 1 1 1 2 1 5 1 4 1
</figure>
<page confidence="0.994817">
29
</page>
<bodyText confidence="0.999532285714286">
boundaries as real boundaries. B(r,b) randomly se-
lects b boundaries as real boundaries.
The accuracy of the last two algorithms are com-
puted analytically. We consider the status of in po-
tential boundaries as a bit string (1 -4 topic bound-
ary). The terms p(iniss) awl p(fa) in equation 6 cor-
responds to p(samelk) and p(difflk) = 1 -p(samelk).
Equation 7, 8 and 9 gives the general form of
p(samelk), B(r,?) and Berm, respectively&apos;.
Table 2 presents the experimental results. The
values in row two and three, four and five are not
actually the same. However, their differences are
insignificant according to the Kolmogorov-Smirnov,
or KS-test (Press et al., 1992).
</bodyText>
<equation confidence="0.963209714285714">
# valid segmentations
p(samelk) =
# possible segmentations
2m-k
p(sarnelk, B(r,?)) = 27,0
P(samel k, m, B(r,b))
(9)
</equation>
<table confidence="0.986938666666667">
3-11 3-5 6-8 9-11
Be 45% 38% 39% 36%
B„ 47% 47% 47% 46%
B(7.,b) 47% 47% 47% 46%
B„ 53% 53% 53% 54%
/3(i.,?) 53% 53% 53% 54%
</table>
<tableCaption confidence="0.999637">
Table 2: The error rate of the baseline algorithms.
</tableCaption>
<subsectionHeader confidence="0.948271">
4.3 Experiment 2 - TextTiling
</subsectionHeader>
<bodyText confidence="0.999989444444444">
We compare three versions of the TextTiling algo-
rithm (Hearst, 1994). H94(c,d) is Hearst&apos;s C im-
plementation with default parameters. H94(e.7.) us-
es the recommended parameters k = 6, w = 20.
H94(3,,,) is my implementation of the algorithm.
Experimental result (table 3) shows H94(,,d) and
H94(,,) are more accurate than H94(j,,,). We sus-
pect this is due to the use of a different stopword list
and stemming algorithm.
</bodyText>
<subsectionHeader confidence="0.995539">
4.4 Experiment 3 - DotPlot
</subsectionHeader>
<bodyText confidence="0.999154285714286">
Five versions of Reynar&apos;s optimisation algorithm
(Reynar, 1998) were evaluated. R98 and R98(7-„rn)
are exact implementations of his maximisation and
minimisation algorithm. R98(8,,08) is my version of
the maximisation algorithm which uses the cosine
coefficient instead of dot density for measuring sim-
ilarity. It incorporates the optimisations described
</bodyText>
<footnote confidence="0.9575265">
7The full derivation of our method is available from the
author.
</footnote>
<table confidence="0.999340142857143">
3-11 3-5 6-8 9-11
H94(,,d) 46% 44% 43% 48%
H94(,,,,) 46% 44% 44% 49%
H94(3,,,) 54% 45% 52% 53%
H94(C(l) 0.67s 0.52s 0.66s 0.88s
H94(c,r) 0.68s 0.52s 0.67s 0.92s
H94(j,r) 3.77s 2.21s 3.69s 5.07s
</table>
<tableCaption confidence="0.952096">
Table 3: The error rate and speed performance of
TextTiling.
</tableCaption>
<bodyText confidence="0.999199133333333">
in section 3.4. R98(,„,d0t) is the modularised version
of R98 for experimenting with different similarity
measures.
R98(,,,,„) uses a variant of Kozima&apos;s semantic sim-
ilarity measure (Kozima, 1993) to compute block
similarity. Word similarity is a function of word co-
occurrence statistics in the given document. Word-
s that belong to the same sentence are considered
to be related. Given the co-occurrence frequen-
cies f (wi, wi), the transition probability matrix t is
computed by equation 10. Equation 11 defines our
spread activation scheme. s denotes the word sim-
ilarity matrix, x is the number of activation steps
and norm(y) converts a matrix y into a transition
matrix. x = 5 was used in the experiment.
</bodyText>
<equation confidence="0.9788192">
f
ti,j = p(wi I ) =
Ei f
ti)
)
</equation>
<bodyText confidence="0.99975575">
Experimental result (table 4) shows the cosine co-
efficient and our spread activation method improved
segmentation accuracy. The speed optimisations sig-
nificantly reduced the execution time.
</bodyText>
<table confidence="0.997488636363636">
3-11 3-5 6-8 9-11
R98(„,,,,a) 18% 20% 15% 12%
R98(8,05) 21% 18% 19% 18%
R98(01,d0t) 22% 21% 18% 16%
R98 22% 21% 18% 16%
R98(min) n/a 34% 37% 37%
R98(8,c03) 4.54s 2.24s 4.36s 6.99s
R98 29.58s 9.29s 28.09s 55.03s
R98(7,1,80) 41.02s 7.34s 40.05s 113.5s
R98(7-0,d08) 46.58s 9.24s 42.72s 115.4s
R98(0,n) n/a 19.62s 58.77s 122.6s
</table>
<tableCaption confidence="0.978388">
Table 4: The error rate and speed performance of
Reynar&apos;s optimisation algorithm.
</tableCaption>
<subsectionHeader confidence="0.92967">
4.5 Experiment 4 - Segmenter
</subsectionHeader>
<bodyText confidence="0.991431">
We compare three versions of Segmenter (Kan et at,
1998). K98(p) is the original Perl implementation of
</bodyText>
<figure confidence="0.826662285714286">
=2
— k) Cb
mCb
X!
yqx—y)!
(10)
S ,---- norm
</figure>
<page confidence="0.989097">
30
</page>
<bodyText confidence="0.999206714285714">
the algorithm (version 1.6). K98(i) is my imple-
mentation of the algorithm. K98(j,,i) is a version of
K98(i) which uses a document specific chain break-
ing strategy. The distribution of link distances are
used to identify unusually long links. The threshold
is a function p + c x VT, of the mean p and variance
We found c = 1 works well in practice.
Table 5 summarises the experimental results.
K98 performed performed significantly better than K98(J,).
This is due to the use of a different part-of-speech
tagger and shallow parser. The difference in speed is
largely due to the programming languages and term
clustering strategies. Our chain breaking strategy
improved accuracy (compare K98(i) with K98(j,a))•
</bodyText>
<table confidence="0.999570285714286">
3-11 3-5 6-8 9-11
K98 36% 36% 23% 33% 43%
K98(j,,i) n/a 41% 46% 50%
K98(3) n/a 44% 48% 51%
K98(p) 4.24s 2.57s 4.21s 6.00s
K98(3) n/a 21.43s 65.54s 129.3s
K98(3,a) n/a 21.44s 65.49s 129.7s
</table>
<tableCaption confidence="0.9637415">
Table 5: The error rate and speed performance of
Segmenter.
</tableCaption>
<subsectionHeader confidence="0.973272">
4.6 Experiment 5 - Our algorithm, C99
</subsectionHeader>
<bodyText confidence="0.999505416666667">
Two versions of our algorithm were developed, C99
and C99(b). The former is an exact implementation
of the algorithm described in this paper. The latter
is given the expected number of topic segments for
fair comparison with R98. Both algorithms used a
11 x 11 ranking mask.
The first experiment focuses on the impact of our
automatic termination strategy on C99(b) (table 6).
C99(b) is marginally more accurate than C99. This
indicates our automatic termination strategy is effec-
tive but not optimal. The minor reduction in speed
performance is acceptable.
</bodyText>
<table confidence="0.9984192">
3-11 3-5 6-8 9-11
C99(b) 12% 12% 9% 9%
C99 13% 18% 10% 10%
C99(b) 4.00s 1.91s 3.73s 5.99s
C99 4.04s 2.12s 4.04s 6.31s
</table>
<tableCaption confidence="0.880598">
Table 6: The error rate and speed performance of
our algorithm, C99.
</tableCaption>
<bodyText confidence="0.9983201">
The second experiment investigates the effect of
different ranking mask size on the performance of
C99 (table 7). Execution time increases with mask
size. A 1 x 1 ranking mask reduces all the elements in
the rank matrix to zero. Interestingly, the increase
in ranking mask size beyond 3 x 3 has insignificant
effect on segmentation accuracy. This suggests the
use of extrema for clustering has a greater impact on
accuracy than linearising the similarity scores (figure
4).
</bodyText>
<table confidence="0.999888947368421">
3-11 3-5 6-8 9-11
1 x 1 48% 48% 50% 49%
3 x 3 12% 11% 10% 8%
5 x 5 12% 11% 10% 8%
7 x 7 12% 11% 10% 8%
9 x 9 12% 11% 10% 9%
11 x 11 12% 11% 10% 9%
13 x 13 12% 11% 10% 9%
15 x 15 12% 11% 10% 9%
17 x 17 12% 10% 10% 8%
1 x 1 3.92s 2.06s 3.84s 5.91s
3 x 3 3.83s 2.03s 3.79s 5.85s
5 x 5 3.86s 2.04s 3.84s 5.92s
7 x 7 3.90s 2.06s 3.88s 6.00s
9 x 9 3.96s 2.07s 3.92s 6.12s
11 x 11 4.02s 2.09s 3.98s 6.26s
13 x 13 4.11s 2.11s 4.07s 6.41s
15 x 15 4.20s 2.14s 4.14s 6.60s
17 x 17 4.29s 2.17s 4.25s 6.79s
</table>
<tableCaption confidence="0.981682">
Table 7: The impact of mask size on the performance
of C99.
</tableCaption>
<subsectionHeader confidence="0.992337">
4.7 Summary
</subsectionHeader>
<bodyText confidence="0.995581">
Experimental result (table 8) shows our algorith-
m C99 is more accurate than existing algorithms.
A two-fold increase in accuracy and seven-fold in-
crease in speed was achieved (compare C99(b) with
R98). If one disregards segmentation accuracy, H94
has the best algorithmic performance (linear). C99,
K98 and R98 are all polynomial time algorithms.
The significance of our results has been confirmed
by both t-test and KS-test.
</bodyText>
<table confidence="0.999567272727273">
3-11 3-5 6-8 9-11
C99(b) 12% 12% 9% 9%
C99 13% 18% 10% 10%
R98 22% 21% 18% 16%
K98(,) 36% 23% 33% 43%
H94(,,d) 46% 44% 43% 48%
H94(3,0 3.77s 2.21s 3.69s 5.07s
C99(b) 4.00s 1.91s 3.73s 5.99s
C99 4.04s 2.12s 4.04s 6.31s
R98 29.58s 9.29s 28.09s 55.03s
K98(3) n/a 21.43s 65.54s 129.3s
</table>
<tableCaption confidence="0.998795">
Table 8: A summary of our experimental results.
</tableCaption>
<sectionHeader confidence="0.768033" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999781">
A segmentation algorithm has two key elements, a,
clustering strategy and a similarity measure. Our
</bodyText>
<page confidence="0.999581">
31
</page>
<bodyText confidence="0.999895833333333">
results show divisive clustering (R98) is more precise
than sliding window (H94) and lexical chains (K98)
for locating topic boundaries.
Four similarity measures were examined. The co-
sine coefficient (R98(s,„0) and dot density measure
(R98(m,doo ) yield similar results. Our spread activa-
tion based semantic measure (R98(„,sa)) improved
accuracy. This confirms that although Kozima&apos;s ap-
proach (Kozima, 1993) is computationally expen-
sive, it does produce more precise segmentation.
The most significant improvement was due to our
ranking scheme which linearises the cosine coefficien-
t,. Our experiments demonstrate that given insuffi-
cient data, the qualitative behaviour of the cosine
measure is indeed more reliable than the actual val-
ues.
Although our evaluation scheme is sufficient for
this comparative study, further research requires a
large scale, task independent benchmark. It would
be interesting to compare C99 with the multi-source
method described in (Beeferman et al., 1999) using
the TDT corpus. We would also like to develop a
linear time and multi-source version of the algorith-
m.
</bodyText>
<sectionHeader confidence="0.996897" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999826909090909">
This paper has benefitted from the comments of
Mary McGee Wood and the anonymous reviewer-
s. Thanks are due to my parents and department
for making this work possible; Jeffrey Reynar for
discussions and guidance on the segmentation prob-
lem; Hideki Kozima for help on the spread activation
measure; Min-Yen Kan and Marti Hearst for their
segmentation algorithms; Daniel Oram for references
to image processing techniques; Magnus Rattray and
Stephen Marsland for help on statistics and mathe-
matics.
</bodyText>
<sectionHeader confidence="0.998968" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997430945945946">
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic
detection and tracking pilot study final report. In
Proceedings of the DARPA Broadcast News Tran-
scription and Understanding Workshop.
Doug Beeferman, Adam Berger, and John Lafferty.
1997a. A model of lexical attraction and repul-
sion. In Proceedings of the 35th Annual Meeting
of the ACL.
Doug Beeferman, Adam Berger, and John Lafferty.
1997b. Text segmentation using exponential mod-
els. In Proceedings of EMNLP-2.
Doug Beeferman, Adam Berger, and John Laffer-
ty. 1999. Statistical models for text segmentation.
Machine learning, special issue on Natural Lan-
guage Processing, 34(1-3):177-210. C. Cardie and
R. Mooney (editors).
Freddy Y. Y. Choi. 2000. A speech interface for
rapid reading. In Proceedings of IEE colloquium:
Speech and Language Processing for Disabled and
Elderly People, London, England, April. IEE.
Kenneth W. Church and Jonathan I. Helfman. 1993.
Dotplot: A program for exploring self-similarity in
millions of lines of text and code. The Journal of
Computational and Graphical Statistics.
Kenneth W. Church. 1993. Char_align: A program
for aligning parallel texts at the character level.
In Proceedings of the 31st Annual Meeting of the
ACL.
David Eichmann, Miguel Ruiz, and Padmini S-
rinivasan. 1999. A cluster-based approach to
tracking, detection and segmentation of broadcast
news. In Proceedings of the 1999 DARPA Broad-
cast News Workshop (TDT-2).
Gregory Grefenstette and Pasi Tapanainen. 1994.
What is a word, what is a sentence? problems of
tokenization. In Proceedings of the 3rd Conference
on Computational Lexicography and Text Research
(COMPLEX&apos;94), Budapest, July.
Mochizuki Hajime, Honda Takeo, and Okumura
Manabu. 1998. Text segmentation with mul-
tiple surface linguistic cues. In Proceedings of
COLING-ACL&apos;98, pages 881-885.
Michael Halliday and Rucjaiya Hasan. 1976. Cohe-
sion in English. Longman Group, New York.
Marti Hearst and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In
Proceedings of the 16th Annual International
ACM/SIGIR Conference, Pittsburgh, PA.
Marti A. Hearst. 1994. Multi-paragraph segmenta-
tion of expository text. In Proceedings of the A-
CL&apos;94. Las Crces, NM.
Oskari Heinonen. 1998. Optimal multi-paragraph
text segmentation by dynamic programming. In
Proceedings of COLING- A CL &apos;98.
Jonathan I. Helfman. 1996. Dotplot patterns: A lit-
eral look at pattern languages. Theory and Prac-
tice of Object Systems, 2(1):31-41.
Min-Yen Kan, Judith L. Klavans, and Kathleen R.
McKeown. 1998. Linear segmentation and seg-
ment significance. In Proceedings of the 6th
International Workshop of Very Large Corpora
(WVLC-6), pages 197-205, Montreal, Quebec,
Canada, August.
Stefan Kaufmann. 1999. Cohesion and collocation:
Using context vectors in text segmentation. In
Proceedings of the 37th Annual Meeting of the As-
sociation of for Computational Linguistics (Stu-
dent Session), pages 591-595, College Park, USA,
June. ACL.
Hideki Kozima. 1993. Text segmentation based on
similarity between words. In Proceedings of A-
CL &apos;93, pages 286-288, Ohio.
Sadao Kurohashi and Makoto Nagao. 1994. Auto-
</reference>
<page confidence="0.984626">
32
</page>
<reference confidence="0.998503909090909">
matic detection of discourse structure by checking
surface information in sentences. In Processings
of COLING&apos;94, volume 2, pages 1123-1127.
Diane J. Litman and Rebecca J. Passonneau. 1995.
Combining multiple knowledge sources for dis-
course segmentation. In Proceedings of the 33rd
Annual Meeting of the ACL.
S. Miike, E. Itoh, K. Ono, and K. Sumita. 1994.
A full text retrieval system. In Proceedings of SI-
GIR&apos;94, Dublin, Ireland.
.J. Morris and G. Hirst. 1991. Lexical cohesion com-
puted by thesaural relations as an indicator of
the structure of text. Computational Linguistic-
s, (17):21-48.
Jane Morris. 1988. Lexical cohesion, the thesaurus,
and the structure of text. Technical Report CSRI
219, Computer Systems Research Institute, Uni-
versity of Toronto.
M. A. O&apos;Neil and M. I. Denos. 1992. Practical ap-
proach to the stereo-matching of urban imagery.
Image and Vision Computing, 10(2):89-98.
David D. Palmer and Marti A. Hearst. 1994. Adap-
tive sentence boundary disambiguation. In Pro-
ceedings of the 4th Conference on Applied Natural
Language Processing, Stuttgart, Germany, Octo-
ber. ACL.
Jay M. Ponte and Bruce W. Croft. 1997. Text seg-
mentation by topic. In Proceedings of the first Eu-
ropean Conference on research and advanced tech-
nology for digital libraries. U.Mass. Computer Sci-
ence Technical Report TR97-18.
M. Porter. 1980. An algorithm for suffix stripping.
Program, 14(3):130-137, July.
William H. Press, Saul A. Teukolsky, William T.
Vettering, and Brian P. Flannery, 1992. Numeri-
cal recipes in C: The Art of Scientific Computing,
chapter 14, pages 623-628. Cambridge University
Press, second edition.
Jeffrey Reynar and Adwait Ratnaparkhi. 1997. A
maximum entropy approach to identifying sen-
tence boundaries. In Proceedings of the fifth con-
ference on Applied NLP, Washington D.C.
Jeffrey Reynar, Breck Baldwin, Christine Doran,
Michael Niv, B. Srinivas, and Mark Wasson. 1997.
Eagle: An extensible architecture for general lin-
guistic engineering. In Proceedings of RIAO &apos;97,
Montreal, June.
Jeffrey C. Reynar. 1994. An automatic method of
finding topic boundaries. In Proceedings of A-
CL &apos;94 (Student session).
Jeffrey C. Reynar. 1998. Topic segmentation: Algo-
rithms and applications. Ph.D. thesis, Computer
and Information Science, University of Pennsylva-
nia.
Jeffrey C. Reynar. 1999. Statistical models for topic
segmentation. In Proceedings of the 37th Annual
Meeting of the A CL, pages 357-364. 20-26th June,
Maryland, USA.
C. J. van Rijsbergen. 1979. Information Retrieval.
Buttersworth.
Yaakov Yaari. 1997. Segmentation of expository
texts by hierarchical agglomerative clustering. In
Proceedings of RANLP&apos;97. Bulgaria.
Gilbert Youmans. 1991. A new tool for discourse
analysis: The vocabulary-management profile.
Language, pages 763-789.
</reference>
<page confidence="0.999376">
33
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884601">
<title confidence="0.99879">Advances in domain independent linear text segmentation</title>
<author confidence="0.999996">Freddy Y Y Choi</author>
<affiliation confidence="0.999964333333333">Artificial Intelligence Group Department of Computer Science University of Manchester</affiliation>
<address confidence="0.999313">Manchester, England</address>
<email confidence="0.998996">choifths.man.ac.uk</email>
<abstract confidence="0.973624">This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime Carbonell</author>
<author>George Doddington</author>
<author>Jonathan Yamron</author>
<author>Yiming Yang</author>
</authors>
<title>Topic detection and tracking pilot study final report.</title>
<date>1998</date>
<booktitle>In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop.</booktitle>
<contexts>
<context position="3148" citStr="Allan et al., 1998" startWordPosition="472" endWordPosition="475">thods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format. 3.1 Similarity measure Punctuation and uninformative words are removed from ea</context>
<context position="9892" citStr="Allan et al., 1998" startWordPosition="1693" endWordPosition="1696">performance. The procedure computes the values along diagonals, starting from the main diagonal and (4) 28 I 70 30 40 50 so Number of segments Figure 7: Finding the optimal segmentation using the gradient. works towards the corner. The method has a complexity of order 171-5.n2. Let ri,j refer to the rank value in the rank matrix R and S to the sum of rank matrix. Given R of size n X 77,, S is computed in three steps (see equation 5). Figure 8 shows the result of applying this procedure to the rank matrix in figure 5. 4 Evaluation The definition of a topic segment ranges from complete stories (Allan et al., 1998) to summaries (Ponte and Croft, 1997). Given the quality of an algorithm is task dependent, the following experiments focus on the relative performance. Our evaluation strategy is a variant of that described in (Reynar, 1998, 71-73) and the TDT segmentation task (Allan et al., 1998). We assume a good algorithm is one that finds the most prominent topic boundaries. 4.1 Experiment procedure An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms. A sample is a concatenation of ten text segments. A segment is the first n sentences o</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study final report. In Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>A model of lexical attraction and repulsion.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="958" citStr="Beeferman et al., 1997" startWordPosition="131" endWordPosition="134">of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work </context>
<context position="2848" citStr="Beeferman et al., 1997" startWordPosition="423" endWordPosition="426">tect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen,</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1997a. A model of lexical attraction and repulsion. In Proceedings of the 35th Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Text segmentation using exponential models.</title>
<date>1997</date>
<booktitle>In Proceedings of EMNLP-2.</booktitle>
<contexts>
<context position="958" citStr="Beeferman et al., 1997" startWordPosition="131" endWordPosition="134">of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work </context>
<context position="2848" citStr="Beeferman et al., 1997" startWordPosition="423" endWordPosition="426">tect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen,</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1997b. Text segmentation using exponential models. In Proceedings of EMNLP-2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine learning, special issue on Natural Language Processing,</booktitle>
<pages>34--1</pages>
<editor>C. Cardie and R. Mooney (editors).</editor>
<contexts>
<context position="11329" citStr="Beeferman et al., 1999" startWordPosition="1966" endWordPosition="1969">5 6 — 8 9— 11 # samples 400 100 100 100 Table 1: Test corpus statistics. 0.05 0.04 3 3 0.02 0.01 (5) Sum of rank matrix 91 67 46 33 ! 19 4 65 43 26 15 5 19 1 42 ; 28 13 4 15 33 30 18 5 13 i 26 46 - 15 5 18 28 1 43 67 1 4 IS 30 42 65 91 Figure 8: Improving speed performance by precomputing s. p(erroriref, hyp, k) = p(misslref, hyp, diff, k)p(diffl ref, k)+ (6) p(fairef, hyp, same, k)p(samelref, k) Speed performance is measured by the average number of CPU seconds required to process a test sample6. Segmentation accuracy is measured by the error metric (equation 6, fa false alarms) proposed in (Beeferman et al., 1999). Low error probability indicates high accuracy. Other performance measures include the popular precision and recall metric (PR) (Hearst, 1994), fuzzy PR (Reynar, 1998) and edit distance (Ponte and Croft, 1997). The problems associated with these metrics are discussed in (Beeferman et al., 1999). 4.2 Experiment 1 - Baseline Five degenerate algorithms define the baseline for the experiments. B„ does not propose any boundaries. B„ reports all potential boundaries as real boundaries. B„ partitions the sample into regular segments. B(i.,?) randomly selects any number of 40nly the news articles ca*</context>
<context position="20590" citStr="Beeferman et al., 1999" startWordPosition="3539" endWordPosition="3542">t although Kozima&apos;s approach (Kozima, 1993) is computationally expensive, it does produce more precise segmentation. The most significant improvement was due to our ranking scheme which linearises the cosine coefficient,. Our experiments demonstrate that given insufficient data, the qualitative behaviour of the cosine measure is indeed more reliable than the actual values. Although our evaluation scheme is sufficient for this comparative study, further research requires a large scale, task independent benchmark. It would be interesting to compare C99 with the multi-source method described in (Beeferman et al., 1999) using the TDT corpus. We would also like to develop a linear time and multi-source version of the algorithm. Acknowledgements This paper has benefitted from the comments of Mary McGee Wood and the anonymous reviewers. Thanks are due to my parents and department for making this work possible; Jeffrey Reynar for discussions and guidance on the segmentation problem; Hideki Kozima for help on the spread activation measure; Min-Yen Kan and Marti Hearst for their segmentation algorithms; Daniel Oram for references to image processing techniques; Magnus Rattray and Stephen Marsland for help on stati</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Machine learning, special issue on Natural Language Processing, 34(1-3):177-210. C. Cardie and R. Mooney (editors).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>A speech interface for rapid reading.</title>
<date>2000</date>
<booktitle>In Proceedings of IEE colloquium: Speech and Language Processing for Disabled and Elderly People,</booktitle>
<publisher>IEE.</publisher>
<location>London, England,</location>
<contexts>
<context position="1033" citStr="Choi, 2000" startWordPosition="143" endWordPosition="144">context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. A speech interface for rapid reading. In Proceedings of IEE colloquium: Speech and Language Processing for Disabled and Elderly People, London, England, April. IEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
<author>Jonathan I Helfman</author>
</authors>
<title>Dotplot: A program for exploring self-similarity in millions of lines of text and code.</title>
<date>1993</date>
<journal>The Journal of Computational and Graphical Statistics.</journal>
<contexts>
<context position="7549" citStr="Church and Helfman, 1993" startWordPosition="1240" endWordPosition="1243">MMEM UE103 NOME INIU6 ANN Similarity matrix Rank matrix Step Similarity matrix EIUM MEM MOM11 MUM, MUM3: MU 4 9 Step 3 flarilt matrix 1 6 1 5 7 Step 2 2 6 • 7 4 1 E • fx,i x sim(x, y) (1) \/Ei fx2, x 27 f(:r) = g(x x 4) Rank matrix Step 2 g(z) = 1(e.&apos;/2 + le—z/2(1+ sin(10z&amp;quot;))) (3) 09 r(x) B 7 07 06 Scp 1 Step 3 os 04 03 02 01 50 100 160 200 Figure 4: An illustration of the more subtle effects of our ranking scheme. 3.3 Clustering The final process determines the location of the topic boundaries. The method is based on Reynar&apos;s maximisation algorithm (Reynar, 1998; Helfman, 1996; Church, 1993; Church and Helfman, 1993). A text segment is defined by two sentences i, j (inclusive). This is represented as a square region along the diagonal of the rank matrix. Let si,j denote the sum of the rank values in a segment and ai,j = (j —i +1)2 be the inside area. B = {b1, ...,197-4 is a list of in coherent text segments. sk and ak refers to the sum of rank and area of segment k in B. D is the inside density of B (see equation 4). v,772 D = L&apos;k=1 sk V-1771. ak To initialise the process, the entire document is placed in B as one coherent text segment. Each step of the process splits one of the segments in B. The split p</context>
</contexts>
<marker>Church, Helfman, 1993</marker>
<rawString>Kenneth W. Church and Jonathan I. Helfman. 1993. Dotplot: A program for exploring self-similarity in millions of lines of text and code. The Journal of Computational and Graphical Statistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth W Church</author>
</authors>
<title>Char_align: A program for aligning parallel texts at the character level.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="7522" citStr="Church, 1993" startWordPosition="1238" endWordPosition="1239">1 2 8 MEM DUO MMEM UE103 NOME INIU6 ANN Similarity matrix Rank matrix Step Similarity matrix EIUM MEM MOM11 MUM, MUM3: MU 4 9 Step 3 flarilt matrix 1 6 1 5 7 Step 2 2 6 • 7 4 1 E • fx,i x sim(x, y) (1) \/Ei fx2, x 27 f(:r) = g(x x 4) Rank matrix Step 2 g(z) = 1(e.&apos;/2 + le—z/2(1+ sin(10z&amp;quot;))) (3) 09 r(x) B 7 07 06 Scp 1 Step 3 os 04 03 02 01 50 100 160 200 Figure 4: An illustration of the more subtle effects of our ranking scheme. 3.3 Clustering The final process determines the location of the topic boundaries. The method is based on Reynar&apos;s maximisation algorithm (Reynar, 1998; Helfman, 1996; Church, 1993; Church and Helfman, 1993). A text segment is defined by two sentences i, j (inclusive). This is represented as a square region along the diagonal of the rank matrix. Let si,j denote the sum of the rank values in a segment and ai,j = (j —i +1)2 be the inside area. B = {b1, ...,197-4 is a list of in coherent text segments. sk and ak refers to the sum of rank and area of segment k in B. D is the inside density of B (see equation 4). v,772 D = L&apos;k=1 sk V-1771. ak To initialise the process, the entire document is placed in B as one coherent text segment. Each step of the process splits one of the</context>
</contexts>
<marker>Church, 1993</marker>
<rawString>Kenneth W. Church. 1993. Char_align: A program for aligning parallel texts at the character level. In Proceedings of the 31st Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Eichmann</author>
<author>Miguel Ruiz</author>
<author>Padmini Srinivasan</author>
</authors>
<title>A cluster-based approach to tracking, detection and segmentation of broadcast news.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1999 DARPA Broadcast News Workshop (TDT-2).</booktitle>
<contexts>
<context position="2035" citStr="Eichmann et al., 1999" startWordPosition="303" endWordPosition="306"> short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hea</context>
</contexts>
<marker>Eichmann, Ruiz, Srinivasan, 1999</marker>
<rawString>David Eichmann, Miguel Ruiz, and Padmini Srinivasan. 1999. A cluster-based approach to tracking, detection and segmentation of broadcast news. In Proceedings of the 1999 DARPA Broadcast News Workshop (TDT-2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gregory Grefenstette</author>
<author>Pasi Tapanainen</author>
</authors>
<title>What is a word, what is a sentence? problems of tokenization.</title>
<date>1994</date>
<booktitle>In Proceedings of the 3rd Conference on Computational Lexicography and Text Research (COMPLEX&apos;94),</booktitle>
<location>Budapest,</location>
<contexts>
<context position="3454" citStr="Grefenstette and Tapanainen, 1994" startWordPosition="519" endWordPosition="522">tion (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format. 3.1 Similarity measure Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list. A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems. A dictionary of word stem frequencies is constructed for each sentence. This is represented as a vector of frequency coun</context>
</contexts>
<marker>Grefenstette, Tapanainen, 1994</marker>
<rawString>Gregory Grefenstette and Pasi Tapanainen. 1994. What is a word, what is a sentence? problems of tokenization. In Proceedings of the 3rd Conference on Computational Lexicography and Text Research (COMPLEX&apos;94), Budapest, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mochizuki Hajime</author>
<author>Honda Takeo</author>
<author>Okumura Manabu</author>
</authors>
<title>Text segmentation with multiple surface linguistic cues.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL&apos;98,</booktitle>
<pages>881--885</pages>
<contexts>
<context position="3019" citStr="Hajime et al., 1998" startWordPosition="450" endWordPosition="453"> Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a pla</context>
</contexts>
<marker>Hajime, Takeo, Manabu, 1998</marker>
<rawString>Mochizuki Hajime, Honda Takeo, and Okumura Manabu. 1998. Text segmentation with multiple surface linguistic cues. In Proceedings of COLING-ACL&apos;98, pages 881-885.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Halliday</author>
<author>Rucjaiya Hasan</author>
</authors>
<title>Cohesion in English.</title>
<date>1976</date>
<publisher>Longman Group,</publisher>
<location>New York.</location>
<contexts>
<context position="1735" citStr="Halliday and Hasan, 1976" startWordPosition="254" endWordPosition="257">t. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains</context>
</contexts>
<marker>Halliday, Hasan, 1976</marker>
<rawString>Michael Halliday and Rucjaiya Hasan. 1976. Cohesion in English. Longman Group, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
<author>Christian Plaunt</author>
</authors>
<title>Subtopic structuring for full-length document access.</title>
<date>1993</date>
<booktitle>In Proceedings of the 16th Annual International ACM/SIGIR Conference,</booktitle>
<location>Pittsburgh, PA.</location>
<contexts>
<context position="762" citStr="Hearst and Plaunt, 1993" startWordPosition="105" endWordPosition="108">ersity of Manchester Manchester, England choifths.man.ac.uk Abstract This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similari</context>
</contexts>
<marker>Hearst, Plaunt, 1993</marker>
<rawString>Marti Hearst and Christian Plaunt. 1993. Subtopic structuring for full-length document access. In Proceedings of the 16th Annual International ACM/SIGIR Conference, Pittsburgh, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>Multi-paragraph segmentation of expository text.</title>
<date>1994</date>
<booktitle>In Proceedings of the ACL&apos;94. Las Crces, NM.</booktitle>
<contexts>
<context position="776" citStr="Hearst, 1994" startWordPosition="109" endWordPosition="110">hester, England choifths.man.ac.uk Abstract This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We </context>
<context position="2319" citStr="Hearst, 1994" startWordPosition="348" endWordPosition="349">an (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994;</context>
<context position="11472" citStr="Hearst, 1994" startWordPosition="1988" endWordPosition="1989">19 1 42 ; 28 13 4 15 33 30 18 5 13 i 26 46 - 15 5 18 28 1 43 67 1 4 IS 30 42 65 91 Figure 8: Improving speed performance by precomputing s. p(erroriref, hyp, k) = p(misslref, hyp, diff, k)p(diffl ref, k)+ (6) p(fairef, hyp, same, k)p(samelref, k) Speed performance is measured by the average number of CPU seconds required to process a test sample6. Segmentation accuracy is measured by the error metric (equation 6, fa false alarms) proposed in (Beeferman et al., 1999). Low error probability indicates high accuracy. Other performance measures include the popular precision and recall metric (PR) (Hearst, 1994), fuzzy PR (Reynar, 1998) and edit distance (Ponte and Croft, 1997). The problems associated with these metrics are discussed in (Beeferman et al., 1999). 4.2 Experiment 1 - Baseline Five degenerate algorithms define the baseline for the experiments. B„ does not propose any boundaries. B„ reports all potential boundaries as real boundaries. B„ partitions the sample into regular segments. B(i.,?) randomly selects any number of 40nly the news articles ca**.pos and informative text cj**.pos were used in the experiment. 5A11 experiment data, algorithms, scripts and detailed results are available f</context>
<context position="13368" citStr="Hearst, 1994" startWordPosition="2313" endWordPosition="2314">resents the experimental results. The values in row two and three, four and five are not actually the same. However, their differences are insignificant according to the Kolmogorov-Smirnov, or KS-test (Press et al., 1992). # valid segmentations p(samelk) = # possible segmentations 2m-k p(sarnelk, B(r,?)) = 27,0 P(samel k, m, B(r,b)) (9) 3-11 3-5 6-8 9-11 Be 45% 38% 39% 36% B„ 47% 47% 47% 46% B(7.,b) 47% 47% 47% 46% B„ 53% 53% 53% 54% /3(i.,?) 53% 53% 53% 54% Table 2: The error rate of the baseline algorithms. 4.3 Experiment 2 - TextTiling We compare three versions of the TextTiling algorithm (Hearst, 1994). H94(c,d) is Hearst&apos;s C implementation with default parameters. H94(e.7.) uses the recommended parameters k = 6, w = 20. H94(3,,,) is my implementation of the algorithm. Experimental result (table 3) shows H94(,,d) and H94(,,) are more accurate than H94(j,,,). We suspect this is due to the use of a different stopword list and stemming algorithm. 4.4 Experiment 3 - DotPlot Five versions of Reynar&apos;s optimisation algorithm (Reynar, 1998) were evaluated. R98 and R98(7-„rn) are exact implementations of his maximisation and minimisation algorithm. R98(8,,08) is my version of the maximisation algori</context>
</contexts>
<marker>Hearst, 1994</marker>
<rawString>Marti A. Hearst. 1994. Multi-paragraph segmentation of expository text. In Proceedings of the ACL&apos;94. Las Crces, NM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oskari Heinonen</author>
</authors>
<title>Optimal multi-paragraph text segmentation by dynamic programming.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING- A CL &apos;98.</booktitle>
<contexts>
<context position="2429" citStr="Heinonen, 1998" startWordPosition="365" endWordPosition="366">t of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, </context>
</contexts>
<marker>Heinonen, 1998</marker>
<rawString>Oskari Heinonen. 1998. Optimal multi-paragraph text segmentation by dynamic programming. In Proceedings of COLING- A CL &apos;98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan I Helfman</author>
</authors>
<title>Dotplot patterns: A literal look at pattern languages. Theory and Practice of Object Systems,</title>
<date>1996</date>
<pages>2--1</pages>
<contexts>
<context position="7508" citStr="Helfman, 1996" startWordPosition="1236" endWordPosition="1237"> enhanced. 2 3 1 2 8 MEM DUO MMEM UE103 NOME INIU6 ANN Similarity matrix Rank matrix Step Similarity matrix EIUM MEM MOM11 MUM, MUM3: MU 4 9 Step 3 flarilt matrix 1 6 1 5 7 Step 2 2 6 • 7 4 1 E • fx,i x sim(x, y) (1) \/Ei fx2, x 27 f(:r) = g(x x 4) Rank matrix Step 2 g(z) = 1(e.&apos;/2 + le—z/2(1+ sin(10z&amp;quot;))) (3) 09 r(x) B 7 07 06 Scp 1 Step 3 os 04 03 02 01 50 100 160 200 Figure 4: An illustration of the more subtle effects of our ranking scheme. 3.3 Clustering The final process determines the location of the topic boundaries. The method is based on Reynar&apos;s maximisation algorithm (Reynar, 1998; Helfman, 1996; Church, 1993; Church and Helfman, 1993). A text segment is defined by two sentences i, j (inclusive). This is represented as a square region along the diagonal of the rank matrix. Let si,j denote the sum of the rank values in a segment and ai,j = (j —i +1)2 be the inside area. B = {b1, ...,197-4 is a list of in coherent text segments. sk and ak refers to the sum of rank and area of segment k in B. D is the inside density of B (see equation 4). v,772 D = L&apos;k=1 sk V-1771. ak To initialise the process, the entire document is placed in B as one coherent text segment. Each step of the process spl</context>
</contexts>
<marker>Helfman, 1996</marker>
<rawString>Jonathan I. Helfman. 1996. Dotplot patterns: A literal look at pattern languages. Theory and Practice of Object Systems, 2(1):31-41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Min-Yen Kan</author>
<author>Judith L Klavans</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Linear segmentation and segment significance.</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th International Workshop of Very Large Corpora (WVLC-6),</booktitle>
<pages>197--205</pages>
<location>Montreal, Quebec, Canada,</location>
<contexts>
<context position="2073" citStr="Kan et al., 1998" startWordPosition="309" endWordPosition="312">nificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source</context>
</contexts>
<marker>Kan, Klavans, McKeown, 1998</marker>
<rawString>Min-Yen Kan, Judith L. Klavans, and Kathleen R. McKeown. 1998. Linear segmentation and segment significance. In Proceedings of the 6th International Workshop of Very Large Corpora (WVLC-6), pages 197-205, Montreal, Quebec, Canada, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Kaufmann</author>
</authors>
<title>Cohesion and collocation: Using context vectors in text segmentation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association of for Computational Linguistics (Student Session),</booktitle>
<pages>591--595</pages>
<publisher>ACL.</publisher>
<location>College Park, USA,</location>
<contexts>
<context position="2011" citStr="Kaufmann, 1999" startWordPosition="301" endWordPosition="302">larity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve in</context>
</contexts>
<marker>Kaufmann, 1999</marker>
<rawString>Stefan Kaufmann. 1999. Cohesion and collocation: Using context vectors in text segmentation. In Proceedings of the 37th Annual Meeting of the Association of for Computational Linguistics (Student Session), pages 591-595, College Park, USA, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kozima</author>
</authors>
<title>Text segmentation based on similarity between words.</title>
<date>1993</date>
<booktitle>In Proceedings of ACL &apos;93,</booktitle>
<pages>286--288</pages>
<location>Ohio.</location>
<contexts>
<context position="890" citStr="Kozima, 1993" startWordPosition="122" endWordPosition="123">wice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely </context>
<context position="2133" citStr="Kozima, 1993" startWordPosition="320" endWordPosition="321">lustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of t</context>
<context position="14615" citStr="Kozima, 1993" startWordPosition="2507" endWordPosition="2508">ent instead of dot density for measuring similarity. It incorporates the optimisations described 7The full derivation of our method is available from the author. 3-11 3-5 6-8 9-11 H94(,,d) 46% 44% 43% 48% H94(,,,,) 46% 44% 44% 49% H94(3,,,) 54% 45% 52% 53% H94(C(l) 0.67s 0.52s 0.66s 0.88s H94(c,r) 0.68s 0.52s 0.67s 0.92s H94(j,r) 3.77s 2.21s 3.69s 5.07s Table 3: The error rate and speed performance of TextTiling. in section 3.4. R98(,„,d0t) is the modularised version of R98 for experimenting with different similarity measures. R98(,,,,„) uses a variant of Kozima&apos;s semantic similarity measure (Kozima, 1993) to compute block similarity. Word similarity is a function of word cooccurrence statistics in the given document. Words that belong to the same sentence are considered to be related. Given the co-occurrence frequencies f (wi, wi), the transition probability matrix t is computed by equation 10. Equation 11 defines our spread activation scheme. s denotes the word similarity matrix, x is the number of activation steps and norm(y) converts a matrix y into a transition matrix. x = 5 was used in the experiment. f ti,j = p(wi I ) = Ei f ti) ) Experimental result (table 4) shows the cosine coefficien</context>
<context position="20010" citStr="Kozima, 1993" startWordPosition="3455" endWordPosition="3456">n/a 21.43s 65.54s 129.3s Table 8: A summary of our experimental results. 5 Conclusions and future work A segmentation algorithm has two key elements, a, clustering strategy and a similarity measure. Our 31 results show divisive clustering (R98) is more precise than sliding window (H94) and lexical chains (K98) for locating topic boundaries. Four similarity measures were examined. The cosine coefficient (R98(s,„0) and dot density measure (R98(m,doo ) yield similar results. Our spread activation based semantic measure (R98(„,sa)) improved accuracy. This confirms that although Kozima&apos;s approach (Kozima, 1993) is computationally expensive, it does produce more precise segmentation. The most significant improvement was due to our ranking scheme which linearises the cosine coefficient,. Our experiments demonstrate that given insufficient data, the qualitative behaviour of the cosine measure is indeed more reliable than the actual values. Although our evaluation scheme is sufficient for this comparative study, further research requires a large scale, task independent benchmark. It would be interesting to compare C99 with the multi-source method described in (Beeferman et al., 1999) using the TDT corpu</context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>Hideki Kozima. 1993. Text segmentation based on similarity between words. In Proceedings of ACL &apos;93, pages 286-288, Ohio.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Automatic detection of discourse structure by checking surface information in sentences.</title>
<date>1994</date>
<booktitle>In Processings of COLING&apos;94,</booktitle>
<volume>2</volume>
<pages>1123--1127</pages>
<contexts>
<context position="2918" citStr="Kurohashi and Nagao, 1994" startWordPosition="434" endWordPosition="437">ding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and He</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. Automatic detection of discourse structure by checking surface information in sentences. In Processings of COLING&apos;94, volume 2, pages 1123-1127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diane J Litman</author>
<author>Rebecca J Passonneau</author>
</authors>
<title>Combining multiple knowledge sources for discourse segmentation.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="2948" citStr="Litman and Passonneau, 1995" startWordPosition="438" endWordPosition="441"> lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnapa</context>
</contexts>
<marker>Litman, Passonneau, 1995</marker>
<rawString>Diane J. Litman and Rebecca J. Passonneau. 1995. Combining multiple knowledge sources for discourse segmentation. In Proceedings of the 33rd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Miike</author>
<author>E Itoh</author>
<author>K Ono</author>
<author>K Sumita</author>
</authors>
<title>A full text retrieval system.</title>
<date>1994</date>
<booktitle>In Proceedings of SIGIR&apos;94,</booktitle>
<location>Dublin, Ireland.</location>
<contexts>
<context position="2891" citStr="Miike et al., 1994" startWordPosition="430" endWordPosition="433">undaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguati</context>
</contexts>
<marker>Miike, Itoh, Ono, Sumita, 1994</marker>
<rawString>S. Miike, E. Itoh, K. Ono, and K. Sumita. 1994. A full text retrieval system. In Proceedings of SIGIR&apos;94, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Morris</author>
<author>G Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<pages>17--21</pages>
<contexts>
<context position="934" citStr="Morris and Hirst, 1991" startWordPosition="127" endWordPosition="130">es as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 B</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>.J. Morris and G. Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, (17):21-48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
</authors>
<title>Lexical cohesion, the thesaurus, and the structure of text.</title>
<date>1988</date>
<tech>Technical Report CSRI 219,</tech>
<institution>Computer Systems Research Institute, University of Toronto.</institution>
<contexts>
<context position="2349" citStr="Morris, 1988" startWordPosition="352" endWordPosition="354">They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) </context>
</contexts>
<marker>Morris, 1988</marker>
<rawString>Jane Morris. 1988. Lexical cohesion, the thesaurus, and the structure of text. Technical Report CSRI 219, Computer Systems Research Institute, University of Toronto.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M A O&apos;Neil</author>
<author>M I Denos</author>
</authors>
<title>Practical approach to the stereo-matching of urban imagery.</title>
<date>1992</date>
<journal>Image and Vision Computing,</journal>
<pages>10--2</pages>
<contexts>
<context position="5730" citStr="O&apos;Neil and Denos, 1992" startWordPosition="892" endWordPosition="895">ilarity between sentences, e.g. a is more similar to b than c. Furthermore, language usage varies throughout a document. For instance, the introduction section of a document is less cohesive than a section which is about a particular topic. Consequently, it is inappropriate to directly compare the similarity values from different regions of the similarity matrix. In non-parametric statistical analysis, one compares the rank of data sets when the qualitative behaviour is similar but the absolute quantities are unreliable. We present a ranking scheme which is an adaptation of that described in (O&apos;Neil and Denos, 1992). &apos;The contrast of the image has been adjusted to highlight the image features. Each value in the similarity matrix is replaced by its rank in the local region. The rank is the number of neighbouring elements with a lower similarity value. Figure 2 shows an example of image ranking using a 3 x 3 rank mask with output range {0, 8). For segmentation, we used a 11 x 11 rank mask. The output is expressed as a ratio r (equation 2) to circumvent normalisation problems (consider the cases when the rank mask is not contained in the image). Figure 2: A working example of image ranking. # of elements wi</context>
</contexts>
<marker>O&apos;Neil, Denos, 1992</marker>
<rawString>M. A. O&apos;Neil and M. I. Denos. 1992. Practical approach to the stereo-matching of urban imagery. Image and Vision Computing, 10(2):89-98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>Marti A Hearst</author>
</authors>
<title>Adaptive sentence boundary disambiguation.</title>
<date>1994</date>
<booktitle>In Proceedings of the 4th Conference on Applied Natural Language Processing,</booktitle>
<publisher>ACL.</publisher>
<location>Stuttgart, Germany,</location>
<contexts>
<context position="3528" citStr="Palmer and Hearst, 1994" startWordPosition="530" endWordPosition="533">d Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format. 3.1 Similarity measure Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list. A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems. A dictionary of word stem frequencies is constructed for each sentence. This is represented as a vector of frequency counts. Let fi,i denote the frequency of word j in sentence i. The similarity </context>
</contexts>
<marker>Palmer, Hearst, 1994</marker>
<rawString>David D. Palmer and Marti A. Hearst. 1994. Adaptive sentence boundary disambiguation. In Proceedings of the 4th Conference on Applied Natural Language Processing, Stuttgart, Germany, October. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>Bruce W Croft</author>
</authors>
<title>Text segmentation by topic.</title>
<date>1997</date>
<booktitle>In Proceedings of the first European Conference on research and advanced technology for digital libraries. U.Mass. Computer Science</booktitle>
<tech>Technical Report TR97-18.</tech>
<contexts>
<context position="1951" citStr="Ponte and Croft, 1997" startWordPosition="290" endWordPosition="293">1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically us</context>
<context position="9929" citStr="Ponte and Croft, 1997" startWordPosition="1699" endWordPosition="1702">s the values along diagonals, starting from the main diagonal and (4) 28 I 70 30 40 50 so Number of segments Figure 7: Finding the optimal segmentation using the gradient. works towards the corner. The method has a complexity of order 171-5.n2. Let ri,j refer to the rank value in the rank matrix R and S to the sum of rank matrix. Given R of size n X 77,, S is computed in three steps (see equation 5). Figure 8 shows the result of applying this procedure to the rank matrix in figure 5. 4 Evaluation The definition of a topic segment ranges from complete stories (Allan et al., 1998) to summaries (Ponte and Croft, 1997). Given the quality of an algorithm is task dependent, the following experiments focus on the relative performance. Our evaluation strategy is a variant of that described in (Reynar, 1998, 71-73) and the TDT segmentation task (Allan et al., 1998). We assume a good algorithm is one that finds the most prominent topic boundaries. 4.1 Experiment procedure An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms. A sample is a concatenation of ten text segments. A segment is the first n sentences of a randomly selected document from t</context>
<context position="11539" citStr="Ponte and Croft, 1997" startWordPosition="1997" endWordPosition="2000">43 67 1 4 IS 30 42 65 91 Figure 8: Improving speed performance by precomputing s. p(erroriref, hyp, k) = p(misslref, hyp, diff, k)p(diffl ref, k)+ (6) p(fairef, hyp, same, k)p(samelref, k) Speed performance is measured by the average number of CPU seconds required to process a test sample6. Segmentation accuracy is measured by the error metric (equation 6, fa false alarms) proposed in (Beeferman et al., 1999). Low error probability indicates high accuracy. Other performance measures include the popular precision and recall metric (PR) (Hearst, 1994), fuzzy PR (Reynar, 1998) and edit distance (Ponte and Croft, 1997). The problems associated with these metrics are discussed in (Beeferman et al., 1999). 4.2 Experiment 1 - Baseline Five degenerate algorithms define the baseline for the experiments. B„ does not propose any boundaries. B„ reports all potential boundaries as real boundaries. B„ partitions the sample into regular segments. B(i.,?) randomly selects any number of 40nly the news articles ca**.pos and informative text cj**.pos were used in the experiment. 5A11 experiment data, algorithms, scripts and detailed results are available from the author. 6All experiments were conducted on a Pentium II 266</context>
</contexts>
<marker>Ponte, Croft, 1997</marker>
<rawString>Jay M. Ponte and Bruce W. Croft. 1997. Text segmentation by topic. In Proceedings of the first European Conference on research and advanced technology for digital libraries. U.Mass. Computer Science Technical Report TR97-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<journal>Program,</journal>
<pages>14--3</pages>
<contexts>
<context position="3866" citStr="Porter, 1980" startWordPosition="586" endWordPosition="587">tation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format. 3.1 Similarity measure Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list. A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems. A dictionary of word stem frequencies is constructed for each sentence. This is represented as a vector of frequency counts. Let fi,i denote the frequency of word j in sentence i. The similarity between a pair of sentences :1:, y 26 is computed using the cosine measure as shown in equation 1. This is applied to all sentence pairs to generate a similarity matrix. Figure 1 shows an example of a similarity matrixl . High similarity values are represented by bright pixels. The bottom-left and top-right pixel show the self-similarit</context>
</contexts>
<marker>Porter, 1980</marker>
<rawString>M. Porter. 1980. An algorithm for suffix stripping. Program, 14(3):130-137, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William H Press</author>
<author>Saul A Teukolsky</author>
<author>William T Vettering</author>
<author>Brian P Flannery</author>
</authors>
<date>1992</date>
<booktitle>Numerical recipes in C: The Art of Scientific Computing, chapter 14,</booktitle>
<pages>623--628</pages>
<publisher>Cambridge University Press,</publisher>
<note>second edition.</note>
<contexts>
<context position="12976" citStr="Press et al., 1992" startWordPosition="2239" endWordPosition="2242">omly selects b boundaries as real boundaries. The accuracy of the last two algorithms are computed analytically. We consider the status of in potential boundaries as a bit string (1 -4 topic boundary). The terms p(iniss) awl p(fa) in equation 6 corresponds to p(samelk) and p(difflk) = 1 -p(samelk). Equation 7, 8 and 9 gives the general form of p(samelk), B(r,?) and Berm, respectively&apos;. Table 2 presents the experimental results. The values in row two and three, four and five are not actually the same. However, their differences are insignificant according to the Kolmogorov-Smirnov, or KS-test (Press et al., 1992). # valid segmentations p(samelk) = # possible segmentations 2m-k p(sarnelk, B(r,?)) = 27,0 P(samel k, m, B(r,b)) (9) 3-11 3-5 6-8 9-11 Be 45% 38% 39% 36% B„ 47% 47% 47% 46% B(7.,b) 47% 47% 47% 46% B„ 53% 53% 53% 54% /3(i.,?) 53% 53% 53% 54% Table 2: The error rate of the baseline algorithms. 4.3 Experiment 2 - TextTiling We compare three versions of the TextTiling algorithm (Hearst, 1994). H94(c,d) is Hearst&apos;s C implementation with default parameters. H94(e.7.) uses the recommended parameters k = 6, w = 20. H94(3,,,) is my implementation of the algorithm. Experimental result (table 3) shows H</context>
</contexts>
<marker>Press, Teukolsky, Vettering, Flannery, 1992</marker>
<rawString>William H. Press, Saul A. Teukolsky, William T. Vettering, and Brian P. Flannery, 1992. Numerical recipes in C: The Art of Scientific Computing, chapter 14, pages 623-628. Cambridge University Press, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Reynar</author>
<author>Adwait Ratnaparkhi</author>
</authors>
<title>A maximum entropy approach to identifying sentence boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the fifth conference on Applied NLP,</booktitle>
<location>Washington D.C.</location>
<contexts>
<context position="3559" citStr="Reynar and Ratnaparkhi, 1997" startWordPosition="534" endWordPosition="538"> Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format. 3.1 Similarity measure Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list. A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems. A dictionary of word stem frequencies is constructed for each sentence. This is represented as a vector of frequency counts. Let fi,i denote the frequency of word j in sentence i. The similarity between a pair of sentences :1:</context>
</contexts>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>Jeffrey Reynar and Adwait Ratnaparkhi. 1997. A maximum entropy approach to identifying sentence boundaries. In Proceedings of the fifth conference on Applied NLP, Washington D.C.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Reynar</author>
<author>Breck Baldwin</author>
<author>Christine Doran</author>
<author>Michael Niv</author>
<author>B Srinivas</author>
<author>Mark Wasson</author>
</authors>
<title>Eagle: An extensible architecture for general linguistic engineering.</title>
<date>1997</date>
<booktitle>In Proceedings of RIAO &apos;97,</booktitle>
<location>Montreal,</location>
<contexts>
<context position="3590" citStr="Reynar et al., 1997" startWordPosition="541" endWordPosition="544">els (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation format and regular cues can be exploited to improve accuracy. 3 Algorithm Our segmentation algorithm takes a list of tokenized sentences as input. A tokenizer (Grefenstette and Tapanainen, 1994) and a sentence boundary disambiguation algorithm (Palmer and Hearst, 1994; Reynar and Ratnaparkhi, 1997) or EAGLE (Reynar et al., 1997) may be used to convert a plain text document into the acceptable input format. 3.1 Similarity measure Punctuation and uninformative words are removed from each sentence using a simple regular expression pattern matcher and a stopword list. A stemming algorithm (Porter, 1980) is then applied to the remaining tokens to obtain the word stems. A dictionary of word stem frequencies is constructed for each sentence. This is represented as a vector of frequency counts. Let fi,i denote the frequency of word j in sentence i. The similarity between a pair of sentences :1:, y 26 is computed using the co</context>
</contexts>
<marker>Reynar, Baldwin, Doran, Niv, Srinivas, Wasson, 1997</marker>
<rawString>Jeffrey Reynar, Breck Baldwin, Christine Doran, Michael Niv, B. Srinivas, and Mark Wasson. 1997. Eagle: An extensible architecture for general linguistic engineering. In Proceedings of RIAO &apos;97, Montreal, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>An automatic method of finding topic boundaries.</title>
<date>1994</date>
<booktitle>In Proceedings of ACL &apos;94</booktitle>
<location>(Student session).</location>
<contexts>
<context position="1207" citStr="Reynar, 1994" startWordPosition="171" endWordPosition="172">e same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to</context>
<context position="2508" citStr="Reynar, 1994" startWordPosition="375" endWordPosition="376">ion (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracki</context>
</contexts>
<marker>Reynar, 1994</marker>
<rawString>Jeffrey C. Reynar. 1994. An automatic method of finding topic boundaries. In Proceedings of ACL &apos;94 (Student session).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Topic segmentation: Algorithms and applications.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>Computer and Information Science, University of Pennsylvania.</institution>
<contexts>
<context position="834" citStr="Reynar, 1998" startWordPosition="116" endWordPosition="117">scribes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments </context>
<context position="2659" citStr="Reynar, 1998" startWordPosition="397" endWordPosition="399">petition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely motivated by the topic detection and tracking (TDT) initiative (Allan et al., 1998). The focus is on the segmentation of transcribed spoken text and broadcast news stories where the presentation</context>
<context position="7493" citStr="Reynar, 1998" startWordPosition="1234" endWordPosition="1235">e has not been enhanced. 2 3 1 2 8 MEM DUO MMEM UE103 NOME INIU6 ANN Similarity matrix Rank matrix Step Similarity matrix EIUM MEM MOM11 MUM, MUM3: MU 4 9 Step 3 flarilt matrix 1 6 1 5 7 Step 2 2 6 • 7 4 1 E • fx,i x sim(x, y) (1) \/Ei fx2, x 27 f(:r) = g(x x 4) Rank matrix Step 2 g(z) = 1(e.&apos;/2 + le—z/2(1+ sin(10z&amp;quot;))) (3) 09 r(x) B 7 07 06 Scp 1 Step 3 os 04 03 02 01 50 100 160 200 Figure 4: An illustration of the more subtle effects of our ranking scheme. 3.3 Clustering The final process determines the location of the topic boundaries. The method is based on Reynar&apos;s maximisation algorithm (Reynar, 1998; Helfman, 1996; Church, 1993; Church and Helfman, 1993). A text segment is defined by two sentences i, j (inclusive). This is represented as a square region along the diagonal of the rank matrix. Let si,j denote the sum of the rank values in a segment and ai,j = (j —i +1)2 be the inside area. B = {b1, ...,197-4 is a list of in coherent text segments. sk and ak refers to the sum of rank and area of segment k in B. D is the inside density of B (see equation 4). v,772 D = L&apos;k=1 sk V-1771. ak To initialise the process, the entire document is placed in B as one coherent text segment. Each step of </context>
<context position="10116" citStr="Reynar, 1998" startWordPosition="1731" endWordPosition="1732"> The method has a complexity of order 171-5.n2. Let ri,j refer to the rank value in the rank matrix R and S to the sum of rank matrix. Given R of size n X 77,, S is computed in three steps (see equation 5). Figure 8 shows the result of applying this procedure to the rank matrix in figure 5. 4 Evaluation The definition of a topic segment ranges from complete stories (Allan et al., 1998) to summaries (Ponte and Croft, 1997). Given the quality of an algorithm is task dependent, the following experiments focus on the relative performance. Our evaluation strategy is a variant of that described in (Reynar, 1998, 71-73) and the TDT segmentation task (Allan et al., 1998). We assume a good algorithm is one that finds the most prominent topic boundaries. 4.1 Experiment procedure An artificial test corpus of 700 samples is used to assess the accuracy and speed performance of segmentation algorithms. A sample is a concatenation of ten text segments. A segment is the first n sentences of a randomly selected document from the Brown corpus&apos;. A sample is characterised by the range of n. The corpus was generated by an automatic procedure5. Table 1 presents the corpus statistics. Range of n 3— 11 3 — 5 6 — 8 9—</context>
<context position="11497" citStr="Reynar, 1998" startWordPosition="1992" endWordPosition="1993">0 18 5 13 i 26 46 - 15 5 18 28 1 43 67 1 4 IS 30 42 65 91 Figure 8: Improving speed performance by precomputing s. p(erroriref, hyp, k) = p(misslref, hyp, diff, k)p(diffl ref, k)+ (6) p(fairef, hyp, same, k)p(samelref, k) Speed performance is measured by the average number of CPU seconds required to process a test sample6. Segmentation accuracy is measured by the error metric (equation 6, fa false alarms) proposed in (Beeferman et al., 1999). Low error probability indicates high accuracy. Other performance measures include the popular precision and recall metric (PR) (Hearst, 1994), fuzzy PR (Reynar, 1998) and edit distance (Ponte and Croft, 1997). The problems associated with these metrics are discussed in (Beeferman et al., 1999). 4.2 Experiment 1 - Baseline Five degenerate algorithms define the baseline for the experiments. B„ does not propose any boundaries. B„ reports all potential boundaries as real boundaries. B„ partitions the sample into regular segments. B(i.,?) randomly selects any number of 40nly the news articles ca**.pos and informative text cj**.pos were used in the experiment. 5A11 experiment data, algorithms, scripts and detailed results are available from the author. 6All expe</context>
<context position="13807" citStr="Reynar, 1998" startWordPosition="2384" endWordPosition="2385">3(i.,?) 53% 53% 53% 54% Table 2: The error rate of the baseline algorithms. 4.3 Experiment 2 - TextTiling We compare three versions of the TextTiling algorithm (Hearst, 1994). H94(c,d) is Hearst&apos;s C implementation with default parameters. H94(e.7.) uses the recommended parameters k = 6, w = 20. H94(3,,,) is my implementation of the algorithm. Experimental result (table 3) shows H94(,,d) and H94(,,) are more accurate than H94(j,,,). We suspect this is due to the use of a different stopword list and stemming algorithm. 4.4 Experiment 3 - DotPlot Five versions of Reynar&apos;s optimisation algorithm (Reynar, 1998) were evaluated. R98 and R98(7-„rn) are exact implementations of his maximisation and minimisation algorithm. R98(8,,08) is my version of the maximisation algorithm which uses the cosine coefficient instead of dot density for measuring similarity. It incorporates the optimisations described 7The full derivation of our method is available from the author. 3-11 3-5 6-8 9-11 H94(,,d) 46% 44% 43% 48% H94(,,,,) 46% 44% 44% 49% H94(3,,,) 54% 45% 52% 53% H94(C(l) 0.67s 0.52s 0.66s 0.88s H94(c,r) 0.68s 0.52s 0.67s 0.92s H94(j,r) 3.77s 2.21s 3.69s 5.07s Table 3: The error rate and speed performance of </context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Jeffrey C. Reynar. 1998. Topic segmentation: Algorithms and applications. Ph.D. thesis, Computer and Information Science, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Statistical models for topic segmentation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the A CL,</booktitle>
<pages>357--364</pages>
<location>Maryland, USA.</location>
<contexts>
<context position="804" citStr="Reynar, 1999" startWordPosition="113" endWordPosition="114">n.ac.uk Abstract This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity </context>
<context position="2220" citStr="Reynar, 1999" startWordPosition="333" endWordPosition="334">n methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attrac</context>
</contexts>
<marker>Reynar, 1999</marker>
<rawString>Jeffrey C. Reynar. 1999. Statistical models for topic segmentation. In Proceedings of the 37th Annual Meeting of the A CL, pages 357-364. 20-26th June, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J van Rijsbergen</author>
</authors>
<date>1979</date>
<journal>Information Retrieval. Buttersworth.</journal>
<marker>van Rijsbergen, 1979</marker>
<rawString>C. J. van Rijsbergen. 1979. Information Retrieval. Buttersworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yaakov Yaari</author>
</authors>
<title>Segmentation of expository texts by hierarchical agglomerative clustering.</title>
<date>1997</date>
<booktitle>In Proceedings of RANLP&apos;97.</booktitle>
<contexts>
<context position="789" citStr="Yaari, 1997" startWordPosition="111" endWordPosition="112">d choifths.man.ac.uk Abstract This paper describes a method for linear text segmentation which is twice as accurate and over seven times as fast as the state-of-the-art (Reynar, 1998). Inter-sentence similarity is replaced by rank in the local context. Boundary locations are discovered by divisive clustering. 1 Introduction Even moderately long documents typically address several topics or different aspects of the same topic. The aim of linear text segmentation is to discover the topic boundaries. The uses of this procedure include information retrieval (Hearst and Plaunt, 1993; Hearst, 1994; Yaari, 1997; Reynar, 1999), summarization (Reynar, 1998), text understanding, anaphora resolution (Kozima, 1993), language modelling (Morris and Hirst, 1991; Beeferman et al., 1997b) and improving document navigation for the visually disabled (Choi, 2000). This paper focuses on domain independent methods for segmenting written text. We present a new algorithm that builds on previous work by Reynar (Reynar, 1998; Reynar, 1994). The primary distinction of our method is the use of a ranking scheme and the cosine similarity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that </context>
<context position="2469" citStr="Yaari, 1997" startWordPosition="370" endWordPosition="371">ons of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lexical cohesion methods are typically used for segmenting written text in a collection to improve information retrieval (Hearst, 1994; Reynar, 1998). Multi-source methods combine lexical cohesion with other indicators of topic shift such as cue phrases, prosodic features, reference, syntax and lexical attraction (Beeferman et al., 1997a) using decision trees (Miike et al., 1994; Kurohashi and Nagao, 1994; Litman and Passonneau, 1995) and probabilistic models (Beeferman et al., 1997b; Hajime et al., 1998; Reynar, 1998). Work in this area is largely moti</context>
</contexts>
<marker>Yaari, 1997</marker>
<rawString>Yaakov Yaari. 1997. Segmentation of expository texts by hierarchical agglomerative clustering. In Proceedings of RANLP&apos;97. Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gilbert Youmans</author>
</authors>
<title>A new tool for discourse analysis: The vocabulary-management profile. Language,</title>
<date>1991</date>
<pages>763--789</pages>
<contexts>
<context position="1913" citStr="Youmans, 1991" startWordPosition="286" endWordPosition="287">ity measure (van Rijsbergen, 1979) in formulating the similarity matrix. We propose that the similarity values of short text segments is statistically insignificant. Thus, one can only rely on their order, or rank, for clustering. 2 Background Existing work falls into one of two categories, lexical cohesion methods and multi-source methods (Yaari, 1997). The former stem from the work of Halliday and Hasan (Halliday and Hasan, 1976). They proposed that text segments with similar vocabulary are likely to be part of a coherent topic segment. Implementations of this idea use word stem repetition (Youmans, 1991; Reynar, 1994; Ponte and Croft, 1997), context vectors (Hearst, 1994; Yaari, 1997; Kaufmann, 1999; Eichmann et al., 1999), entity repetition (Kan et al., 1998), semantic similarity (Morris and Hirst, 1991; Kozima, 1993), word distance model (Beeferman et al., 1997a) and word frequency model (Reynar, 1999) to detect cohesion. Methods for finding the topic boundaries include sliding window (Hearst, 1994), lexical chains (Morris, 1988; Kan et al., 1998), dynamic programming (Ponte and Croft, 1997; Heinonen, 1998), agglomerative clustering (Yaari, 1997) and divisive clustering (Reynar, 1994). Lex</context>
</contexts>
<marker>Youmans, 1991</marker>
<rawString>Gilbert Youmans. 1991. A new tool for discourse analysis: The vocabulary-management profile. Language, pages 763-789.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>