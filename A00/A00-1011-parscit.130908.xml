<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.660473">
REES: A Large-Scale Relation and Event Extraction System
</title>
<note confidence="0.8583825">
Chinatsu Aone M i la Ramos-Santacruz
SRA International, Inc. SRA International, Inc.
4300 Fair Lakes Court 4300 Fair Lakes Court
Fairfax, VA 22033 Fairfax, VA 22033
</note>
<email confidence="0.988686">
aonec@verdi.sra.com mila@verdi.sra.com
</email>
<sectionHeader confidence="0.994631" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966153846154">
This paper reports on a large-scale, end-to-
end relation and event extraction system. At
present, the system extracts a total of 100
types of relations and events, which
represents a much wider coverage than is
typical of extraction systems. The system
consists of three specialized pattern-based
tagging modules, a high-precision co-
reference resolution module, and a
configurable template generation module.
We report quantitative evaluation results,
analyze the results in detail, and discuss
future directions.
</bodyText>
<sectionHeader confidence="0.962469" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.99984744">
One major goal of information extraction (1E)
technology is to help users quickly identify a
variety of relations and events and their key
players in a large volume of documents. In
contrast with this goal, state-of-the-art
information extraction systems, as shown in the
various Message Understanding Conferences
(MUCs), extract a small number of relations and
events. For instance, the most recent MUC,
MUC-7, called for the extraction of 3 relations
(person-employer, maker-product, and
organization-location) and 1 event (spacecraft
launches). Our goal is to develop an IE system
which scales up to extract as many types of
relations and events as possible with a minimum
amount of porting effort combined with high
accuracy. Currently, REES handles 100 types of
relations and events, and it does so in a modular,
configurable, and scalable manner.
Below, Section 1 presents the ontologies of
relations and events that we have developed.
Section 2 describes REES&apos; system architecture.
Section 3 evaluates the system&apos;s performance,
and offers a qualitative analysis of system errors.
Section 4 discusses future directions.
</bodyText>
<sectionHeader confidence="0.983165" genericHeader="method">
1 Relation and Event Ontologies
</sectionHeader>
<bodyText confidence="0.999307615384615">
As the first step in building a large-scale relation
and event extraction system, we developed
ontologies of the relations and events to be
extracted. These ontologies represent a wide
variety of domains: political, financial, business,
military, and life-related events and relations.
&amp;quot;Relations&amp;quot; covers what in MUC-7 are called
Template Elements (TEs) and Template
Relations (TRs). There are 39 types of relations.
While MUC TE&apos;s only dealt with singular
entities, REES extracts both singular and plural
entities (e.g., &amp;quot;five executives&amp;quot;). The TR
relations are shown in italic in the table below.
</bodyText>
<table confidence="0.958637">
Relations
Place Relations Artifact Relations
Place-Name&amp;Aliases Artifact-Name&amp;Ahases
Place-Type Artifact-Type
Place-Subtype Artifact-Subtype
Place-Descriptor Artifact-Descriptor
Place-Country Artifact-Maker
Artifact-Owner
Organization Relations Person Relations
Org-Name&amp;Ahases Person-Name&amp;Ahases
Org-Descriptor Person-Type
Org-FoundationDate Person-Subtype
Org-Nationality Person-Descriptor
Org-TickerSymbol Person-Honorific
Org-Location Person-Age
Org-ParentOrg Person-PhoneNumber
Org-Owner Person-Nationality
Org-Founder Person-Affiliation
Org-StockMarket Person-Sibling
Person-Spouse
Person-Parent
Person-Grandparent
</table>
<page confidence="0.913681">
76
</page>
<tableCaption confidence="0.999772">
Table 1: Relation Ontology
</tableCaption>
<bodyText confidence="0.999822857142857">
&amp;quot;Events&amp;quot; are extracted along with their event
participants, e.g., &amp;quot;who did what to whom when
and where?&amp;quot; For example, for a BUYING
event, REES extracts the buyer, the artifact, the
seller, and the time and location of the BUYING
event. REES currently covers 61 types of
events, as shown below.
</bodyText>
<table confidence="0.997824780487805">
Events
Vehicle Transaction
Vehicle departs Buy artifact
Vehicle arrives Sell artifact
Spacecraft launch Import artifact
Vehicle crash Export artifact
Give money
Personnel Change Business
Hire Start business
Terminate contract Close business
Promote Make artifact
Succeed Acquire company
Start office Sell company
Sue organization
Merge company
Crime Financial
Sexual assault Currency moves up
Steal money Currency moves down
Seize drug Stock moves up
Indict Stock moves down
Arrest Stock market moves up
Try Stock market moves down
Convict Stock index moves up
Sentence Stock index moves down
Jail
Political Conflict
Nominate Kill
Appoint Injure
Elect Hijack vehicle
Expel person Hold hostages
Reach agreement Attack target
Hold meeting Fire weapon
Impose embargo Weapon hit
Topple Invade land
Move forces
Retreat
Surrender
Evacuate
Family
Die
Marry
</table>
<tableCaption confidence="0.999618">
Table 2: Event Ontology
</tableCaption>
<bodyText confidence="0.957152">
Figures 1 and 2 show sample relation and event
templates. Figure 1 shows a Person-Affiliation
relation template for &amp;quot;Frank Ashley, a
spokesman for Occidental Petroleum Corp.&amp;quot;
</bodyText>
<figure confidence="0.447323">
&lt;PERSON_AFFILIATION-AP8802230207-54&gt; :â€”
TYPE: PERSON AFFILIATION
PERSON: [TE for &amp;quot;Frank Ashley&amp;quot;]
ORG: [TE for &amp;quot;Occidental Petroleum&amp;quot;]
</figure>
<figureCaption confidence="0.909117666666667">
Figure 1: Example of Relation Template
Figure 2 shows an Attack Target event template
for the sentence &amp;quot;an Iraqi warplane attacked the
</figureCaption>
<figure confidence="0.9332488">
frigate Stark with missiles May 17, 1987.&amp;quot;
&lt;ATTACK_TARGET-AP8804160078-12&gt;:=
TYPE: CONFLICT
SUBTYPE: ATTACK_TARGET
ATTACKER: [TE for &amp;quot;an Iraqi warplane]
TARGET: [TE for &amp;quot;the frigate Stark&amp;quot;]
WEAPON: [TE for &amp;quot;missiles&amp;quot;]
TIME: &amp;quot;May 17, 1987&amp;quot;
PLACE: [TE for &amp;quot;the gulf]
COMMENT: &amp;quot;attacked&amp;quot;
</figure>
<figureCaption confidence="0.999678">
Figure 2: Example of Event Template
</figureCaption>
<sectionHeader confidence="0.458244" genericHeader="method">
2 System Architecture and Components
</sectionHeader>
<bodyText confidence="0.997169444444444">
Figure 3 illustrates the REES system
architecture. REES consists of three main
components: a tagging component (cf. Section
2.1), a co-reference resolution module (cf.
Section 2.2), and a template generation module
(cf. Section 2.3). Figure 3 also illustrates that
the user may run REES from a Graphical User
Interface (GUI) called TemplateTool (cf.
Section 2.4).
</bodyText>
<subsectionHeader confidence="0.992197">
2.1 Tagging Modules
</subsectionHeader>
<bodyText confidence="0.999940285714286">
The tagging component consists of three
modules as shown in Figure 3: NameTagger,
NPTagger and EventTagger. Each module relies
on the same pattern-based extraction engine, but
uses different sets of patterns. The NameTagger
recognizes names of people, organizations,
places, and artifacts (currently only vehicles).
</bodyText>
<figure confidence="0.944610363636364">
Person-OtherRelative
Person-BirthPlace
Person-BirthDate
77
REES
Name NP Event
Tagger Tagger Tagger
TemplateTool
GUI interaction
4--
...--........
</figure>
<figureCaption confidence="0.999952">
Figure 3: The REES System Architecture
</figureCaption>
<bodyText confidence="0.967347027777778">
The NPTagger then takes the XML-tagged
output of the NameTagger through two phases.
First, it recognizes non-recursive Base Noun
Phrase (BNP) (our specifications for BNP
resemble those in Ramshaw and Marcus 1995).
Second, it recognizes complex NPs for only
the four main semantic types of NPs, i.e.,
Person, Organization, Location, and Artifact
(vehicle, drug and weapon). It makes post-
modifier attachment decisions only for those
NPs that are crucial to the extraction at hand.
During this second phase, relations which can
be recognized locally (e.g., Age, Affiliation,
Maker) are also recognized and stored using
the XML attributes for the NPs. For instance,
the XML tag for &amp;quot;President of XYZ Corp.&amp;quot;
below holds an AFFILIATION attribute with
the ID for &amp;quot;XYZ Corp.&amp;quot;
&lt;PNP ID=&amp;quot;03&amp;quot; AFFILIATION=&amp;quot;04&amp;quot;&gt;President of
&lt;ENTITY ID=&amp;quot;04&amp;quot;&gt;XYZ Corp.&lt;ENTITY&gt;
&lt;/PNP&gt;
Building upon the XML output of the
NPTagger, the EventTagger recognizes
events applying its lexicon-driven,
syntactically-based generic patterns. These
patterns tag events in the presence of at
least one of the arguments specified in the
lexical entry for a predicate. Subsequent
patterns try to find additional arguments as
well as place and time adjunct information
for the tagged event. As an example of the
EventTagger&apos;s generic patterns, consider
the simplified pattern below. This pattern
matches on an event-denoting verb that
requires a direct object of type weapon
(e.g., &apos;fire a gun&amp;quot;)
</bodyText>
<equation confidence="0.96595225">
(8z.
{AND $VP {ARG2_SYN=DO}
{ARG2_SEM=WEAPON}}
{AND $ARTIFACT {SUBTYPE=WEAPON} } )1
</equation>
<bodyText confidence="0.986018142857143">
The important aspect of REES is its
declarative, lexicon-driven approach. This
approach requires a lexicon entry for each
event-denoting word, which is generally a
I &amp;=concatenation, AND=Boolean operator, $VP
and $ART1FACT are macro references for complex
phrases.
</bodyText>
<page confidence="0.758892">
7R
</page>
<bodyText confidence="0.999982066666667">
verb. The lexicon entry specifies the syntactic
and semantic restrictions on the verb&apos;s
arguments. For instance, the following lexicon
entry is for the verb &amp;quot;attack.&amp;quot; It indicates that
the verb &amp;quot;attack&amp;quot; belongs to the CONFLICT
ontology and to the ATTACK TARGET type.
The first argument for the verb &amp;quot;attack&amp;quot; is
semantically an organization, location, person,
or artifact (ARGl_SEM), and syntactically a
subject (ARG l_SYN). The second argument
is semantically an organization, location,
person or artifact, and syntactically a direct
object. The third argument is semantically a
weapon and syntactically a prepositional
phrase introduced by the preposition &amp;quot;with&amp;quot;.
</bodyText>
<sectionHeader confidence="0.49142525" genericHeader="method">
ATTACK ( { {CATEGORY VERB}
{ONTOLOGY CONFLICT}
{TYPE ATTACK TARGET}
{ARGl_SEM {ORGANIZATION LOCATION
</sectionHeader>
<equation confidence="0.927653428571429">
PERSON ARTIFACT} }
{ARGI_SYN {SUBJECT}}
{ARG2_SEM {ORGANIZATION LOCATION
PERSON ARTIFACT))
(ARG2_SYN (DO)}
{ARG3_SEM{ WEAPON))
{ARG3_SYN (WITH) } } }
</equation>
<bodyText confidence="0.9986238">
About 50 generic event extraction patterns,
supported by lexical information as shown
above, allow extraction of events and their
arguments in cases like:
An Iraqi warplane attacked the frigate Stark
with missiles May 17, 1987.
This generic, lexicon-driven event extraction
approach makes REES easily portable because
new types of events can be extracted by just
adding new verb entries to the lexicon. No
new patterns are required. Moreover, this
approach allows for easy customization
capability: a person with no knowledge of the
pattern language would be able to configure
the system to extract new events.
While the tagging component is similar to
other pattern-based IE systems (e.g., Appelt et
al. 1995; Aone et al. 1998, Yangarber and
Grishman 1998), our EventTagger is more
portable through a lexicon-driven approach.
</bodyText>
<subsectionHeader confidence="0.962836">
2.2 Co-reference Resolution
</subsectionHeader>
<bodyText confidence="0.999818333333333">
After the tagging phase, REES sends the XML
output through a rule-based co-reference
resolution module that resolves:
</bodyText>
<listItem confidence="0.996142666666667">
â€¢ definite noun phrases of Organization,
Person, and Location types, and
â€¢ singular person pronouns: he and she.
</listItem>
<bodyText confidence="0.992973923076923">
Only &amp;quot;high-precision&amp;quot; rules are currently
applied to selected types of anaphora. That is,
we resolve only those cases of anaphora whose
antecedents the module can identify with high
confidence. For example, the pronoun rules
look for the antecedents only within 3
sentences, and the definite NP rules rely
heavily on the head noun matches. Our high-
precision approach results from our
observation that unless the module is very
accurate (above 80% precision), the co-
reference module can hurt the overall
extraction results by over-merging templates.
</bodyText>
<subsectionHeader confidence="0.98999">
2.3 Template Generation Module
</subsectionHeader>
<bodyText confidence="0.999966428571429">
A typical template generation module is a
hard-coded post-processing module which has
to be written for each type of template. By
contrast, our Template Generation module is
unique as it uses declarative rules to generate
and merge templates automatically so as to
achieve portability.
</bodyText>
<subsubsectionHeader confidence="0.844071">
2.3.1 Declarative Template Generation
</subsubsectionHeader>
<bodyText confidence="0.999843785714286">
REES outputs the extracted information in the
form of either MUC-style templates, as
illustrated in Figure 1 and 2, or XML. A
crucial part of a portable, scalable system is to
be able to output different types of relations
and events without changing the template
generation code. REES maps XML-tagged
output of the co-reference module to templates
using declarative template definitions, which
specifies the template label (e.g.,
ATTACK TARGET), XML attribute names
(e.g., ARGâ€”UMENT1), corresponding template
slot names (e.g., ATTACKER), and the type
restrictions on slot values (e.g., string).
</bodyText>
<page confidence="0.992495">
79
</page>
<subsubsectionHeader confidence="0.987357">
2.3.2 Event Merging
</subsubsectionHeader>
<bodyText confidence="0.979268">
One of the challenges of event extraction is to
be able to recognize and merge those event
descriptions which refer to the same event.
The Template Generation module uses a set of
declarative, customizable rules to merge co-
referring events into a single event. Often, the
rules reflect pragmatic knowledge of the world.
For example, consider the rule below for the
DYING event type. This rule establishes that
if two die events have the same subject, then
they refer to the same event (i.e., a person
cannot die more than once).
{merge
</bodyText>
<equation confidence="0.96650775">
{EVENT 1 {AND {SUBTYPE DIE) {PERSON
$foo}}
{EVENT 2 {AND (SUBTYPE DIE) {PERSON
$foo}})
</equation>
<subsectionHeader confidence="0.977512">
2.4 Graphical User Interface (GUI)
</subsectionHeader>
<bodyText confidence="0.9960735">
For some applications such as database
population, the user may want to validate the
system output. REES is provided with a Java-
based Graphical User Interface that allows the
user to run REES and display, delete, or
modify the system output. As illustrated in
Figure 4, the tool displays the templates on the
bottom half of the screen, and the user can
choose which template to display. The top half
of the screen displays the input document with
extracted phrases in different colors. The user
can select any slot value, and the tool will
highlight the portion of the input text
responsible for the slot value. This feature is
very useful in efficiently verifying system
output. Once the system&apos;s output has been
verified, the resulting templates can be saved
and used to populate a database.
</bodyText>
<sectionHeader confidence="0.990732" genericHeader="method">
3 System Evaluation
</sectionHeader>
<bodyText confidence="0.998591">
The table below shows the system&apos;s recall,
precision, and F-Measure scores for the
training set (200 texts) and the blind set (208
texts) from about a dozen news sources. Each
set contains at least 3 examples of each type of
relations and events. As we mentioned earlier,
&amp;quot;relations&amp;quot; includes MUC-style TEs and TRs.
</bodyText>
<table confidence="0.9986553">
Text Task Templates R P F-M
Set in keys
Train Rel. 9955 76 74 75.35
Events 2525 57 74 64.57
Rel. &amp; 10707 74 74 73.95
Events
Blind Rel. 8938 74 74 73.74
Events 2020 42 75 53.75
Rel. &amp; 9526 69 74 71.39
Events
</table>
<tableCaption confidence="0.999839">
Table 3: Evaluation Results
</tableCaption>
<bodyText confidence="0.9991654">
The blind set F-Measure for 31 types of
relations (73.95%) exceeded our initial goal of
70%. While the blind set F-Measure for 61
types of events was 53.75%, it is significant to
note that 26 types of events achieved an F-
Measure over 70%, and 37 types over 60% (cf.
Table 4). For reference, though not exactly
comparable, the best-performing MUC-7
system achieved 87% in TE, 76% in TR, and
51% in event extraction.
</bodyText>
<table confidence="0.9998525">
F-M in Event types
blind set
90-100 2 : Buy artifact. Marry
80-89 9 : Succeed, Merge company, Kill,
Surrender, Arrest, Convict, Sentence,
Nominate, Expel.
70-79 15 : Die, Sell artifact, Export
Artifact, Hire, Start office, Make
artifact, Acquire company, Sue
organization, Stock Index moves
down, Steal money, Indict, Jail,
Vehicle crash, Elect, Hold meeting.
</table>
<tableCaption confidence="0.99955">
Table 4: Top-performing Event Types
</tableCaption>
<page confidence="0.91045">
80
</page>
<table confidence="0.998297266666667">
1 iho COnsio Surer o F.n...o..; F.refarance.E. I iel0 ,. .
â–  2
SI t X 1
p_ _........_............ f or Coorig ,â€¢ril rig
flit. A P800221 0002 AL I L.- &apos; &amp;quot;--â€¢ Tarnpi:A,.&apos;.... I.. ad.,
DOCUIMOM N umlaut. AP880221 001)2 â€¢â€¢â€¢ r. dor. 7.ogi t he
. - -,. - __......... rsport net. ..
.7.U&amp;quot;..;I:.
â€¢,â€¢ and decLar ad support
&apos; &apos;&apos;&apos;&apos;&apos; L&apos; -.. &apos; &apos; =NM : .&apos;â€¢ &apos;
. . â€¢ i...,dth ..-. â€¢ ...â€¢ s â€¢ ; -â€¢ . â€¢3 . - â€¢â€¢ . . â€¢
:. â€¢.LÂ±-hour clauh o_n e_L-To_ne h but no o_rpuringol were
naood,
IFERSAPA r,&apos;..h.) NAME AND E7.1
RIP115--F:Aq_nM E -4 . .Nabin Dori L- !
Iraman-Dackea He.zvOlah CocIfer,01,, em IC 1
Shiite 11111-111AnIth saarchulp a soulhe Fl
a Iti onapped V 5 Manna TYPE:
hit antludt rt .f.E.R.k.i.e.,
The mernSers ofJ ustic a AlFnisber Na SUB râ– TE
Nab in Bern PER :Iv
a pofich Spokesrnan _
Haqballan t owls fa C)EieRill.TOR
Im k esman Julice herr, 1.e..â€¢
o upo -10NoPiF.;
Four cr./ InanS
A. He7Ooltah leaner
//nlinarnR Higgins
lilt strugglers
Cteete Delete
</table>
<figureCaption confidence="0.989916">
Figure 4: TemplateTool
</figureCaption>
<bodyText confidence="0.999937655172414">
Regarding relation extraction, the difference in
the score between the training and blind sets
was very small. In fact, the total F-Measure on
the blind set is less than 2 points lower than
that of the training set. It is also interesting to
note that for 8 of the 12 relation types where
the F-Measure dropped more than 10 points,
the training set includes less than 20 instances.
In other words, there seems to be a natural
correlation between low number of instances in
the training set and low performance in the
blind set.
There was a significant drop between the
training and blind sets in event extraction: 11
points. We believe that the main reason is that
the total number of events in the training set is
fairly low: 801 instances of 61 types of events
(an average of 13/event), where 35 of the event
types had fewer than 10 instances. In fact, 9
out of the 14 event types which scored lower
than 40% F-Measure had fewer than 10
examples. In comparison, there were 34,000
instances of 39 types of relations in the training
set.
The contribution of the co-reference module is
illustrated in the table below. Co-reference
resolution consistently improves F-Measures
both in training and blind sets. Its impact is
larger in relation than event extraction.
</bodyText>
<table confidence="0.998032909090909">
Text set Task Co- No co-
reference reference
rules rules
Training Relations 75.35 72.54
Events 64.57 63.62
Relations 73.95 71.34
&amp; Events
Blind Relations 73.74 72.03
Events 53.75 53.22
Relations 71.39 69.86
&amp; Events
</table>
<tableCaption confidence="0.9892695">
Table 5: Comparative results with and without
co-reference rules
</tableCaption>
<bodyText confidence="0.984309">
In the next two sections, we analyze both false
positives and false negatives.
</bodyText>
<page confidence="0.99622">
81
</page>
<subsectionHeader confidence="0.978724">
3.1 False Positives (or Precision Errors)
</subsectionHeader>
<bodyText confidence="0.9599775">
REES produced precision errors in the
following cases:
</bodyText>
<listItem confidence="0.986699">
â€¢ Most of the errors were due to over-
generation of templates. These are mostly
</listItem>
<bodyText confidence="0.9889365">
cases of co-referring noun phrases that the
system failed to resolve. For example:
&amp;quot;Panama ... the nation ...this country... his
country&amp;quot;
Rules for the co-reference module are still
under development, and at present REES
handles only limited types of plural noun
phrase anaphora.
</bodyText>
<listItem confidence="0.892810777777778">
â€¢ Spurious events resulted from verbs in
conditional constructions (e.g., &amp;quot;if ...
then...&amp;quot;) or from ambiguous predicates.
For instance, &amp;quot;appoint&amp;quot; as a POLITICAL
event vs. a PERSONNEL CHANGE
event.
â€¢ The subject of a verb was misidentified.
This is particularly frequent in reduced
relative clauses.
</listItem>
<bodyText confidence="0.9941672">
Kabul radio said the latest deaths brought
to 38 the number of people killed in the
three car bomb explosions.
(Wrong subject: &amp;quot;the number of people&amp;quot; as
the KILLER instead of the victim)
</bodyText>
<subsectionHeader confidence="0.994065">
3.2 False Negatives (or Recall Errors)
</subsectionHeader>
<bodyText confidence="0.9995765">
Below, we list the most frequent recall errors
in the training set.
</bodyText>
<listItem confidence="0.9796435">
â€¢ Some event arguments are mentioned with
event nouns instead of event verbs. The
current system does not handle noun-based
event extraction.
</listItem>
<bodyText confidence="0.826027904761905">
India&apos;s acquisition last month of the
nuclear submarine from the Soviet
Union...
(SELLER=&amp;quot;Soviet Union&amp;quot; and
TIME=&amp;quot;/ast month&amp;quot; come with the noun-
based event &amp;quot;acquisition.&amp;quot;)
â€¢ Pronouns &amp;quot;it&amp;quot; and &amp;quot;they,&amp;quot; which carry
little semantic information, are currently
not resolved by the co-reference module.
It also has bought three late-1970s vintage
Kilo class Soviet submarines and two West
German HDW 209 subs
(Missed BUYER=India because of
unresolved it.)
â€¢ Verb arguments are a conjunction of noun
phrases. The current system does not
handle coordination of verb arguments.
Hezbollah killed 21 Israelis and 43 of
Lahad&apos;s soldiers
(The system gets only the first object: 21
Israelis.)
</bodyText>
<listItem confidence="0.889357">
â€¢ Ellipsis cases. The current system does not
handle ellipsis.
</listItem>
<bodyText confidence="0.884346205882353">
The two were sentenced to five-year prison
terms with hard labor by the state security
court...
(Missed PERSON_SENTENCED fill
because of unresolved the two.)
â€¢ The subject of the event is relatively far
from the event-denoting verb:
Vladislav Listyev, 38, who brought
television interview shows in the style of
Phil Donahue or Larry King to Russian
viewers and pioneered hard-hitting
television journalism in the 1980s, was
shot in the heart by unknown assailants
and died immediately...
(The system missed subject Vladislav
Listyev for attack event shot)
â€¢ Missed ORG_LOCATION relations for
locations that are part of the organization&apos;s
name.
Larnaca General Hospital
(Missed ORG LOCATION TR for this
and Larnaca.)
We asked a person who is not involved in the
development of REES to review the event
extraction output for the blind set. This person
reported that:
â€¢ In 35% of the cases where the REES
system completely missed an event, it was
because the lexicon was missing the
predicate. REES&apos;s event predicate lexicon
is rather small at present (a total of 140
verbs for 61 event types) and is mostly
based on the examples found in the
training set.
</bodyText>
<listItem confidence="0.991632">
â€¢ In 30% of the cases, the subject or object
was elliptical. The system does not
currently handle ellipsis.
</listItem>
<page confidence="0.985282">
82
</page>
<bodyText confidence="0.939652714285714">
â€¢ In 25% of the cases, syntactic/semantic
argument structures were missing from
existing lexical entries.
It is quite encouraging that simply adding
additional predicates and predicate argument
structures to the lexicon could significantly
increase the blind set performance.
</bodyText>
<sectionHeader confidence="0.999155" genericHeader="method">
4 Future Directions
</sectionHeader>
<bodyText confidence="0.999983">
We believe that improving co-reference
resolution and adding noun-based event
extraction capability are critical to achieving
our ultimate goal of at least 80% F-Measure
for relations and 70% for events.
</bodyText>
<subsectionHeader confidence="0.884368">
4.1 Co-reference Resolution
</subsectionHeader>
<bodyText confidence="0.991279833333333">
As discussed in Section 3.1 and 3.2, accurate
co-reference resolution is crucial to improving
the accuracy of extraction, both in terms of
recall and precision. In particular, we
identified two types of high-payoff co-
reference resolution:
</bodyText>
<listItem confidence="0.788849333333333">
â€¢ definite noun phrase resolution, especially
plural noun phrases
â€¢ -rd
</listItem>
<bodyText confidence="0.721047">
person neutral pronouns &amp;quot;it&amp;quot; and
&amp;quot;they.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.837363">
4.2 Noun-based Event Extraction
</subsectionHeader>
<bodyText confidence="0.980947">
REES currently handles only verb-based
events. Noun-based event extraction adds
more complexity because:
</bodyText>
<listItem confidence="0.99189275">
â€¢ Nouns are often used in a generic, non-
referential manner (e.g., &amp;quot;We see a merger
as being in the consumer&apos;s interest&amp;quot;), and
â€¢ When referential, nouns often refer to
</listItem>
<bodyText confidence="0.913059142857143">
verb-based events, thus requiring noun-
verb co-reference resolution (&amp;quot;An F-14
crashed shortly after takeoff... The crash&amp;quot;).
However, noun-based events are crucial
because they often introduce additional key
information, as the underlined phrases below
indicate:
While Bush&apos;s meetings with prominent anti-
apartheid leaders such as Archbishop
Desmond Tutu and Albertina Sisulu are
important...
We plan to develop a generic set of patterns for
noun-based event extraction to complement the
set of generic verb-based extraction patterns.
</bodyText>
<sectionHeader confidence="0.999549" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999814181818182">
In this paper, we reported on a fast, portable,
large-scale event and relation extraction system
REES. To the best of our knowledge, this is
the first attempt to develop an IE system which
can extract such a wide range of relations and
events with high accuracy. It performs
particularly well on relation extraction, and it
achieves 70% or higher F-Measure for 26 types
of events already. In addition, the design of
REES is highly portable for future addition of
new relations and events.
</bodyText>
<sectionHeader confidence="0.996537" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9983618">
This project would have not been possible
without the contributions of Arcel Castillo,
Lauren Halverson, and Sandy Shinn. Our
thanks also to Brandon Kennedy, who
prepared the hand-tagged data.
</bodyText>
<sectionHeader confidence="0.998901" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99919685">
Aone, Chinatsu, Lauren Halverson, Tom Hampton,
and Mila Ramos-Santacruz. 1998. &amp;quot;SRA:
Description of the 1E2 System Used for MUC- 7.&amp;quot;
In Proceedings of the 7th Message Understanding
Conference (MUC-7).
Appelt, Douglas E., Jerry R Hobbs, John Bear,
David Israel, Megumi Kameyama, Andy Kehler,
David Martin, Karen Myers, and Mabry Tyson.
1995. &amp;quot;SRI International FASTUS System: MUC-
6 Test Results and Analysis.&amp;quot; In Proceedings of
the 6th Message Understanding Conference
(MUC-6).
Ramshaw, Lance A., and Mitchell P. Marcus. 1995.
&amp;quot;Text Chunking Using Transformation-Based
Learning&amp;quot;. In Proceedings of the 3rd ACL
Workshop on Very Large Corpora (WVLC95).
Yangarber, Roman and Ralph Grishman. 1998.
&amp;quot;NYU: Description of the Proteus/PET System as
Used for MUC-7 ST&amp;quot; In Proceedings of the 6th
Message Understanding Conference (MUC-7).
</reference>
<page confidence="0.999309">
83
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.945854">
<title confidence="0.999129">REES: A Large-Scale Relation and Event Extraction System</title>
<author confidence="0.98431">Chinatsu Aone M i la Ramos-Santacruz</author>
<affiliation confidence="0.999543">SRA International, Inc. SRA International, Inc.</affiliation>
<address confidence="0.9993775">4300 Fair Lakes Court 4300 Fair Lakes Court Fairfax, VA 22033 Fairfax, VA 22033</address>
<email confidence="0.99899">aonec@verdi.sra.commila@verdi.sra.com</email>
<abstract confidence="0.997424357142857">This paper reports on a large-scale, end-toend relation and event extraction system. At present, the system extracts a total of 100 types of relations and events, which represents a much wider coverage than is typical of extraction systems. The system consists of three specialized pattern-based tagging modules, a high-precision coreference resolution module, and a configurable template generation module. We report quantitative evaluation results, analyze the results in detail, and discuss future directions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chinatsu Aone</author>
<author>Lauren Halverson</author>
<author>Tom Hampton</author>
<author>Mila Ramos-Santacruz</author>
</authors>
<title>SRA: Description of the 1E2 System Used for MUC- 7.&amp;quot;</title>
<date>1998</date>
<booktitle>In Proceedings of the 7th Message Understanding Conference (MUC-7).</booktitle>
<contexts>
<context position="9502" citStr="Aone et al. 1998" startWordPosition="1348" endWordPosition="1351">f events and their arguments in cases like: An Iraqi warplane attacked the frigate Stark with missiles May 17, 1987. This generic, lexicon-driven event extraction approach makes REES easily portable because new types of events can be extracted by just adding new verb entries to the lexicon. No new patterns are required. Moreover, this approach allows for easy customization capability: a person with no knowledge of the pattern language would be able to configure the system to extract new events. While the tagging component is similar to other pattern-based IE systems (e.g., Appelt et al. 1995; Aone et al. 1998, Yangarber and Grishman 1998), our EventTagger is more portable through a lexicon-driven approach. 2.2 Co-reference Resolution After the tagging phase, REES sends the XML output through a rule-based co-reference resolution module that resolves: â€¢ definite noun phrases of Organization, Person, and Location types, and â€¢ singular person pronouns: he and she. Only &amp;quot;high-precision&amp;quot; rules are currently applied to selected types of anaphora. That is, we resolve only those cases of anaphora whose antecedents the module can identify with high confidence. For example, the pronoun rules look for the ant</context>
</contexts>
<marker>Aone, Halverson, Hampton, Ramos-Santacruz, 1998</marker>
<rawString>Aone, Chinatsu, Lauren Halverson, Tom Hampton, and Mila Ramos-Santacruz. 1998. &amp;quot;SRA: Description of the 1E2 System Used for MUC- 7.&amp;quot; In Proceedings of the 7th Message Understanding Conference (MUC-7).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas E Appelt</author>
<author>Jerry R Hobbs</author>
<author>John Bear</author>
<author>David Israel</author>
<author>Megumi Kameyama</author>
<author>Andy Kehler</author>
<author>David Martin</author>
<author>Karen Myers</author>
<author>Mabry Tyson</author>
</authors>
<title>Results and Analysis.&amp;quot;</title>
<date>1995</date>
<booktitle>SRI International FASTUS System: MUC6 Test</booktitle>
<contexts>
<context position="9484" citStr="Appelt et al. 1995" startWordPosition="1344" endWordPosition="1347">, allow extraction of events and their arguments in cases like: An Iraqi warplane attacked the frigate Stark with missiles May 17, 1987. This generic, lexicon-driven event extraction approach makes REES easily portable because new types of events can be extracted by just adding new verb entries to the lexicon. No new patterns are required. Moreover, this approach allows for easy customization capability: a person with no knowledge of the pattern language would be able to configure the system to extract new events. While the tagging component is similar to other pattern-based IE systems (e.g., Appelt et al. 1995; Aone et al. 1998, Yangarber and Grishman 1998), our EventTagger is more portable through a lexicon-driven approach. 2.2 Co-reference Resolution After the tagging phase, REES sends the XML output through a rule-based co-reference resolution module that resolves: â€¢ definite noun phrases of Organization, Person, and Location types, and â€¢ singular person pronouns: he and she. Only &amp;quot;high-precision&amp;quot; rules are currently applied to selected types of anaphora. That is, we resolve only those cases of anaphora whose antecedents the module can identify with high confidence. For example, the pronoun rule</context>
</contexts>
<marker>Appelt, Hobbs, Bear, Israel, Kameyama, Kehler, Martin, Myers, Tyson, 1995</marker>
<rawString>Appelt, Douglas E., Jerry R Hobbs, John Bear, David Israel, Megumi Kameyama, Andy Kehler, David Martin, Karen Myers, and Mabry Tyson. 1995. &amp;quot;SRI International FASTUS System: MUC6 Test Results and Analysis.&amp;quot; In Proceedings of the 6th Message Understanding Conference (MUC-6).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lance A Ramshaw</author>
<author>Mitchell P Marcus</author>
</authors>
<title>Text Chunking Using Transformation-Based Learning&amp;quot;.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd ACL Workshop on Very Large Corpora (WVLC95).</booktitle>
<contexts>
<context position="6271" citStr="Ramshaw and Marcus 1995" startWordPosition="868" endWordPosition="871">r and EventTagger. Each module relies on the same pattern-based extraction engine, but uses different sets of patterns. The NameTagger recognizes names of people, organizations, places, and artifacts (currently only vehicles). Person-OtherRelative Person-BirthPlace Person-BirthDate 77 REES Name NP Event Tagger Tagger Tagger TemplateTool GUI interaction 4-- ...--........ Figure 3: The REES System Architecture The NPTagger then takes the XML-tagged output of the NameTagger through two phases. First, it recognizes non-recursive Base Noun Phrase (BNP) (our specifications for BNP resemble those in Ramshaw and Marcus 1995). Second, it recognizes complex NPs for only the four main semantic types of NPs, i.e., Person, Organization, Location, and Artifact (vehicle, drug and weapon). It makes postmodifier attachment decisions only for those NPs that are crucial to the extraction at hand. During this second phase, relations which can be recognized locally (e.g., Age, Affiliation, Maker) are also recognized and stored using the XML attributes for the NPs. For instance, the XML tag for &amp;quot;President of XYZ Corp.&amp;quot; below holds an AFFILIATION attribute with the ID for &amp;quot;XYZ Corp.&amp;quot; &lt;PNP ID=&amp;quot;03&amp;quot; AFFILIATION=&amp;quot;04&amp;quot;&gt;President of &lt;</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>Ramshaw, Lance A., and Mitchell P. Marcus. 1995. &amp;quot;Text Chunking Using Transformation-Based Learning&amp;quot;. In Proceedings of the 3rd ACL Workshop on Very Large Corpora (WVLC95).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roman Yangarber</author>
<author>Ralph Grishman</author>
</authors>
<title>NYU: Description of the Proteus/PET System as Used for MUC-7 ST&amp;quot;</title>
<date>1998</date>
<booktitle>In Proceedings of the 6th Message Understanding Conference (MUC-7).</booktitle>
<contexts>
<context position="9532" citStr="Yangarber and Grishman 1998" startWordPosition="1352" endWordPosition="1355"> arguments in cases like: An Iraqi warplane attacked the frigate Stark with missiles May 17, 1987. This generic, lexicon-driven event extraction approach makes REES easily portable because new types of events can be extracted by just adding new verb entries to the lexicon. No new patterns are required. Moreover, this approach allows for easy customization capability: a person with no knowledge of the pattern language would be able to configure the system to extract new events. While the tagging component is similar to other pattern-based IE systems (e.g., Appelt et al. 1995; Aone et al. 1998, Yangarber and Grishman 1998), our EventTagger is more portable through a lexicon-driven approach. 2.2 Co-reference Resolution After the tagging phase, REES sends the XML output through a rule-based co-reference resolution module that resolves: â€¢ definite noun phrases of Organization, Person, and Location types, and â€¢ singular person pronouns: he and she. Only &amp;quot;high-precision&amp;quot; rules are currently applied to selected types of anaphora. That is, we resolve only those cases of anaphora whose antecedents the module can identify with high confidence. For example, the pronoun rules look for the antecedents only within 3 sentenc</context>
</contexts>
<marker>Yangarber, Grishman, 1998</marker>
<rawString>Yangarber, Roman and Ralph Grishman. 1998. &amp;quot;NYU: Description of the Proteus/PET System as Used for MUC-7 ST&amp;quot; In Proceedings of the 6th Message Understanding Conference (MUC-7).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>