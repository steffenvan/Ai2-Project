<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000037">
<title confidence="0.9919195">
A Finite State and Data-Oriented Method for Grapheme to
Phoneme Conversion
</title>
<author confidence="0.894605">
Gosse Bouma
</author>
<affiliation confidence="0.767898">
Alfa-informatica
</affiliation>
<address confidence="0.76720475">
Rijksuniversiteit Groningen
Postbus 716
9700 AS Groningen
The Netherlands
</address>
<email confidence="0.985911">
gosse@let.rug.n1
</email>
<sectionHeader confidence="0.986569" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999201">
A finite-state method, based on leftmost longest-
match replacement, is presented for segmenting
words into graphemes, and for converting graphemes
into phonemes. A small set of hand-crafted conver-
sion rules for Dutch achieves a phoneme accuracy of
over 93%. The accuracy of the system is further im-
proved by using transformation-based learning. The
phoneme accuracy of the best system (using a large
rule and a &apos;lazy&apos; variant of Brill&apos;s algoritm), trained
on only 40K words, reaches 99%.
</bodyText>
<sectionHeader confidence="0.99378" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999927301886793">
Automatic grapheme to phoneme conversion (i.e.
the conversion of a string of characters into a string
of phonemes) is essential for applications of text
to speech synthesis dealing with unrestricted text,
where the input may contain words which do not
occur in the system dictionary. Furthermore, a
transducer for grapheme to phoneme conversion
can be used to generate candidate replacements in
a (pronunciation-sensitive) spelling correction sys-
tem. When given the pronunciation of a misspelled
word, the inverse of the grapheme to phoneme trans-
ducer will generate all identically pronounced words.
Below, we present a method for developing such
grapheme to phoneme transducers based on a com-
bination of hand-crafted conversion rules, imple-
mented using finite state calculus, and automatically
induced rules.
The hand-crafted system is defined as a two-
step procedure: segmentation of the input into a
sequence of graphemes (i.e. sequences of one or
more characters typically corresponding to a sin-
gle phoneme) and conversion of graphemes into (se-
quences of) phonemes. The composition of the
transducer which performs segmentation and the
transducer defined by the conversion rules, is a
transducer which converts sequences of characters
into sequences of phonemes.
Specifying the conversion rules is a difficult task.
Although segmentation of the input can in princi-
ple be dispensed with, we found that writing con-
version rules for segmented input substantially re-
duces the context-sensitivity and order-dependence
of such rules. We manually developed a grapheme to
phoneme transducer for Dutch data obtained from
CELEX (Baayen et al., 1993) and achieved a word ac-
curacy of 60.6% and a phoneme accuracy of 93.6%.
To improve the performance of our system, we
used transformation-based learning (TBL) (Brill,
1995). Training data are obtained by aligning the
output of the hand-crafted finite state transducer
with the correct phoneme strings. These data can
then be used as input for TBL, provided that suit-
able rule templates are available. We performed sev-
eral experiments, in which the amount of training
data, the algorithm (Brill&apos;s original formulation and
&apos;lazy&apos; variants (Samuel et al., 1998)), and the num-
ber of rule templates varied. The best experiment
(40K words, using a &apos;lazy&apos; strategy with a large set
of rule templates) induces over 2000 transformation
rules, leading to 92.6% word accuracy and 99.0%
phoneme accuracy. This result, obtained using a
relatively small set of training data, compares well
with that of other systems.
</bodyText>
<sectionHeader confidence="0.94686" genericHeader="method">
2 Finite State Calculus
</sectionHeader>
<bodyText confidence="0.999849736842105">
As argued in Kaplan and Kay (1994), Karttunen
(1995), Karttunen et, al. (1997), and elsewhere,
many of the rules used in phonology and morphol-
ogy can be analysed as special cases of regular ex-
pressions. By extending the language of regular ex-
pressions with operators which capture the interpre-
tation of linguistic rule systems, high-level linguis-
tic descriptions can be compiled into finite state au-
tomata directly. Furthermore, such automata can be
combined with other finite state automata perform-
ing low-level tasks such as tokenization or lexical-
lookup, or more advanced tasks such as shallow pars-
ing. Composition of the individual components into
a single transducer may lead to highly efficient pro-
cessing.
The system described below was implemented us-
ing FSA Utilities,&apos; a package for implementing and
manipulating finite state automata, which provides
possibilities for defining new regular expression oper-
</bodyText>
<footnote confidence="0.979635">
1www.let.rug.n1/-vannoord/fsa/
</footnote>
<page confidence="0.992177">
303
</page>
<note confidence="0.487893">
LI
</note>
<equation confidence="0.871501916666667">
. ,R]
IF11, , Riil
R&amp;quot;
ignore(A,B)
A x B
identity (A)
T o U
macro (Term ,R)
the empty string
concatenation
disjunction
optionality
</equation>
<listItem confidence="0.901084">
ignore: A interspersed with elements of B
cross-product: the transducer which maps
all strings in A to all strings in B.
identity: the transducer which maps each
element in A onto itself.
composition of the transducers T and U.
use Term as an abbreviation for R (where Term and R may contain variables).
</listItem>
<figureCaption confidence="0.902893">
Figure 1: A fragment of FSA regular expression syntax. A and B are regular expressions denoting recognizers,
T and U transducers, and R can be either.
</figureCaption>
<bodyText confidence="0.989878757575758">
ators. The part of FSA&apos;s built-in regular expression
syntax relevant to this paper, is listed in figure 1.
One particular useful extension of the basic syn-
tax of regular expressions is the replace-operator.
Karttunen (1995) argues that many phonological
and morphological rules can be interpreted as rules
which replace a certain portion of the input string.
Although several implementations of the replace-
operator are proposed, the most relevant case for
our purposes is so-called &apos;leftmost longest-match&apos; re-
placement. In case of overlapping rule targets in the
input, this operator will replace the leftmost target,
and in cases where a rule target contains a prefix
which is also a potential target, the longer sequence
will be replaced. Gerdemann and van Noord (1999)
implement leftmost longest-match replacement in
FSA as the operator
replace(Target, LeftContext,RightContext),
where Target is a transducer defining the actual re-
placement, and LeftContext and RightContext are
regular expressions defining the left- and rightcon-
text of the rule, respectively.
An example where leftmost replacement is use-
ful is hyphenation. Hyphenation of (non-compound)
words in Dutch amounts to segmenting a word
into syllables, separated by hyphens. In cases
where (the written form of) a word can in prin-
ciple be segmented in several ways (i.e. the se-
quence alf abet can be segmented as al-f a-bet ,
al-f ab-et, alf -a-bet , or alf -ab-et), the seg-
mentation which maximizes onsets is in general the
correct one (i.e. al-f a-bet). This property of hy-
phenation is captured by leftmost replacement:
</bodyText>
<equation confidence="0.522256">
macro (hyphenate
replace([)x -, syllable, syllable) ) .
</equation>
<bodyText confidence="0.992162">
Leftmost replacement ensures that hyphens are in-
troduced &apos;eagerly&apos;, i.e. as early as possible. Given
a suitable definition of syllable, this ensures that
wherever a consonant can be final in a coda or initial
in the next onset, it is in fact added to the onset.
The segmentation task discussed below makes cru-
cial use of longest match.
</bodyText>
<sectionHeader confidence="0.4721235" genericHeader="method">
3 A finite state method for
grapheme to phoneme conversion
</sectionHeader>
<bodyText confidence="0.934437">
Grapheme to phoneme conversion is implemented as
the composition of four transducers:
</bodyText>
<equation confidence="0.8992105">
macro(graph2phon,
segmentation % segment the input
</equation>
<listItem confidence="0.913975">
o mark_begin_end % add &apos;#&apos;
o conversion % apply rules
o clean_up ). % remove markers
</listItem>
<bodyText confidence="0.6082025">
An example of conversion including the in-
termediate steps is given below for the word
aanknopingspunt ( connection-point).
input: aanknopingspunt
</bodyText>
<equation confidence="0.83957425">
s: aa-n-k-n-o-p-i-ng-s-p-u-n-t-
m: #-aa-n-k-n-o-p-i-ng-s-p-u-n-t-#
co: #-a+N+k-n-o-p-I+N+s-p-}i-n-t-#
cl: aNknopINsp}nt
</equation>
<bodyText confidence="0.997265916666667">
The first transducer (segmentation) takes as
its input a sequence of characters and groups
these into segments. The second transducer
(mark_begin_end) adds a marker (&apos;#&apos;) to the be-
ginning and end of the sequence of segments. The
third transducer (conversion) performs the actual
conversion step. It converts each segment into a
sequence of (zero or more) phonemes. The final
step (clean_up) removes all markers. The output
is a list of phonemes in the notation used by CELEX
(which can be easily translated into the more com-
mon sAmPA-notation).
</bodyText>
<subsectionHeader confidence="0.998434">
3.1 Segmentation
</subsectionHeader>
<bodyText confidence="0.999687">
The goal of segmentation is to divide a word into a
sequence of graphemes, providing a convenient input
</bodyText>
<page confidence="0.996166">
304
</page>
<bodyText confidence="0.999947633333333">
level of representation for the actual grapheme to
phoneme conversion rules.
While there are many letter-combinations which
are realized as a single phoneme (ch, ng, aa, bb,
), it is only rarely the case that a single letter is
mapped onto more than one phoneme (x), or that a
letter receives no pronunciation at all (such as word-
final n in Dutch, which is elided if it is proceeded by
a schwa). As the number of cases where multiple
letters have to be mapped onto a single phoneme
is relatively high, it is natural to model a letter to
phoneme system as involving two subtasks: segmen-
tation and conversion. Segmentation splits an input
string into graphemes, where each grapheme typ-
ically, but not necessarily, corresponds to a single
phoneme.
Segmentation is defined as:
cases which are less natural, but which do not nec-
essarily lead to errors. The grapheme eu, for in-
stance, almost always goes to I &apos; , but translates
as &apos; e , j , } &apos; in (loan-) words such as museum and
petroleum. One might argue that a segmentation
e-u- is therefore required, but a special conver-
sion rule which covers these exceptional cases (i.e.
eu followed by m) can easily be formulated. Simi-
larly, ng almost always translates as N, but in some
cases actually represents the two graphemes n-g-,
as in aaneengesloten (connected), where it should
be translated as NG. This case is harder to detect,
and is a potential source of errors.
</bodyText>
<subsectionHeader confidence="0.999222">
3.2 The Conversion Rules
</subsectionHeader>
<bodyText confidence="0.999587">
The g2p operator is designed to facilitate the formu-
lation of conversion rules for segmented input:
</bodyText>
<equation confidence="0.764221">
macro(segmentation, 0 x - macro(g2p(Target,LtCont,RtCont),
replace( replace([Target, - x +3,
adentity(graphemes), [ignore(LtCont,f+,-1),
). ignore(RtCont,f+,-1)
</equation>
<bodyText confidence="0.998689076923077">
The macro graphemes defines the set of graphemes.
It contains 77 elements, some of which are:
a, aa, au, ai, aai, e, ee, ei, eu, eau,
eeu, i, ie, lee, ieu, ij, o, oe, oei,..
Segmentation attaches the marker &apos;-&apos; to each
grapheme. Segmentation, as it is defined here,
is not context-sensitive, and thus the second and
third arguments of replace are simply empty. As
the set of graphemes contains many elements which
are substrings of other graphemes (i.e. e is a
substring of ei, eau, etc.), longest-match is es-
sential: the segmentation of beiaardier (caril-
lon player) should be b-ei-aa-r-d-ie-r- and not
b-e-i-a-a-r-d-i-e-r-. This effect can be ob-
tained by making the segment itself part of the tar-
get of the replace statement. Targets are identi-
fied using leftmost longest-match, and thus at each
point in the input, only the longest valid segment is
marked.
The set of graphemes contains a number of ele-
ments which might seem superfluous. The grapheme
aai, for instance, translates as aj , a sequence which
could also be derived on the basis of two graphemes
aa and 1. However, if we leave out the segment
aai, segmentation (using leftmost longest match) of
words such as waaien (to blow) would lead to the
segmentation w-aa-ie-n, which is unnatural, as it
would require an extra conversion rule for ie. Us-
ing the grapheme aai allows for two conversion rules
which always map aai to aj and ie goes to I.
Segmentation as defined above provides the in-
tuitively correct result in almost all cases, given a
suitably defined set of graphemes. There are some
The g2p-operator implements a special purpose ver-
sion of the replace-operator. The replacement of the
marker &apos;-&apos; by &apos;+&apos; in the target ensures that g2p-
conversion rules cannot apply in sequence to the
same grapheme.2 Second, each target of the g2p-
operator must be a grapheme (and not some sub-
string of it). This is a consequence of the fact that
the final element of the left-context must be a marker
and the target itself ends in &apos;-&apos;. Finally, the ig-
nore statements in the left and right context imply
that the rule contexts can abstract over the potential
presence of markers.
An overview of the conversion rules we used for
Dutch is given in Figure 2. As the rules are ap-
plied in sequence, exceptional rules can be ordered
before the regular cases, thus allowing the regular
cases to be specified with little or no context. The
special_vowel_rules deal with exceptional trans-
lations of graphemes such as eu or cases where i or
ij goes to &apos;0&apos;. The short_vowel_rules treat sin-
gle vowels preceding two consonants, or a word final
consonant. One problematic case is e, which can
be translated either as &apos;E&apos; or &apos; &apos; . Here, an ap-
proximation is attempted which specifies the con-
text where e goes &apos;E&apos;, and subsumes the other
case under the general rule for short vowels. The
special_consonant_rules address devoicing and a
few other exceptional eases. The default_rules
supply a default mapping for a large number of
&apos;Note that the input and output alphabet are not disjoint,
and thus rules applying in sequence to the same part of the
input are not excluded in principle.
</bodyText>
<page confidence="0.996417">
305
</page>
<bodyText confidence="0.999866916666667">
graphemes. The target of this rule is a long disjunc-
tion of grapheme-phoneme mappings. As this rule-
set applies after all more specific cases have been
dealt with, no context restrictions need to be speci-
fied.
Depending somewhat on how one counts, the full
set of conversion rules for Dutch contains approxi-
mately 80 conversion rules, more than 40 of which
are default mappings requiring no context.3 Compi-
lation of the complete system results in a (minimal,
deterministic) transducer with 747 states and 20,123
transitions.
</bodyText>
<subsectionHeader confidence="0.999747">
3.3 Test results and discussion
</subsectionHeader>
<bodyText confidence="0.966669875">
The accuracy of the hand-crafted system was eval-
uated by testing it on all of the words wihtout di-
acritics in the CELEX lexical database which have a
phonetic transcription. After several development
cycles, we achieved a word accuracy of 60.6% and
a phoneme accuracy (measured as the edit distance
between the phoneme string produced by the sys-
tem and the correct string, divided by the number
of phonemes in the correct string) of 93.6%.
There have been relatively few attempts at devel-
oping grapheme to phoneme conversion systems us-
ing finite state technology alone. Williams (1994) re-
ports on a system for Welsh, which uses no less than
700 rules implemented in a rather restricted environ-
ment. The rules are also implemented in a two-level
system, PC-KIMMO, (Antworth, 1990), but this still
requires over 400 rules. Möbius et al. (1997) report
on full-fledged text-to-speech system for German,
containing around 200 rules (which are compiled into
a weighted finite state transducer) for the grapheme-
to-phoneme conversion step. These numbers suggest
that our implementation (which contains around 80
rules in total) benefits considerably from the flexibil-
ity and high-level of abstraction made available by
finite state calculus.
One might suspect that a two-level approach to
grapheme to phoneme conversion is more appropri-
ate than the sequential approach used here. Some-
what surprisingly, however, Williams concludes that
a sequential approach is preferable. The formulation
of rules in the latter approach is more intuitive, and
rule ordering provides a way of dealing with excep-
tional cases which is not easily available in a two-
level system.
While further improvements would definitely have
been possible at this point, it becomes increasingly
difficult to do this on the basis of linguistic knowl-
edge alone. That is, most of the rules which have
to be added deal with highly idiosyncratic cases (of-
ten related to loan-words) which can only be discov-
3It should be noted that we only considered words which
do not contain diacritics. Including those is unproblematic in
principle, but would lead to a slight increase of the number
of rules.
ered by browsing through the test results of previ-
ous runs. At this point, switching from a linguistics-
oriented to a data-oriented methodology, seemed ap-
propriate.
</bodyText>
<sectionHeader confidence="0.6314515" genericHeader="method">
4 Transformation-based grapheme
to phoneme conversion
</sectionHeader>
<bodyText confidence="0.999759304347826">
Brill (1995) demonstrates that accurate part-of-
speech tagging can be learned by using a two-step
process. First, a simple system is used which as-
signs the most probable tag to each word. The re-
sults of the system are aligned with the correct tags
for some corpus of training data. Next, (context-
sensitive) transformation rules are selected from a
pool of rule patterns, which replace erroneous tags
by correct tags. The rule with the largest benefit on
the training data (i.e. the rule for which the number
of corrections minus the number of newly introduced
mistakes, is the largest) is learned and applied to
the training data. This process continues until no
more rules can be found which lead to improvement
(above a certain threshold).
Transformation-based learning (TBL) can be ap-
plied to the present problem as wel1.4 In this case,
the base-line system is the finite state transducer de-
scribed above, which can be used to produce a set
of phonemic transcriptions for a word list. Next,
these results are aligned with the correct transcrip-
tions. In combination with suitable rule patterns,
these data can be used as input for a TBL process.
</bodyText>
<subsectionHeader confidence="0.963568">
4.1 Alignment
</subsectionHeader>
<bodyText confidence="0.999572227272727">
TBL requires aligned data for training and testing.
While alignment is mostly trivial for part-of-speech
tagging, this is not the case for the present task.
Aligning data for grapheme-to-phoneme conversion
amounts to aligning each part of the input (a se-
quence of characters) with a part of the output (a
sequence of phonemes). As the length of both se-
quences is not guaranteed to be equal, it must be
possible to align more than one character with a
single phoneme (the usual case) or a single character
with more than one phoneme (the exceptional case,
i.e. &apos;x&apos;). The alignment problem is often solved (Du-
toit, 1997; Daelemans and van den Bosch, 1996) by
allowing &apos;null&apos; symbols in the phoneme string, and
introducing &apos;compound&apos; phonemes, such as &apos;ks&apos; to
account for exceptional cases where a single charac-
ter must be aligned with two phonemes.
As our finite state system already segments the
input into graphemes, we have adopted a strategy
where graphemes instead of characters are aligned
with phoneme strings (see Lawrence and Kaye
(1986) for a similar approach). The correspondence
</bodyText>
<footnote confidence="0.938982666666667">
4Hoste et al. (2000b) compare TBL to C5.0 (Quinlan,
1993) on a similar task, i.e. the mapping of the pronunciation
of one regional variant of Dutch into another.
</footnote>
<page confidence="0.998048">
306
</page>
<bodyText confidence="0.856436">
macro(conversion, special_vowel_rules o short_vowel_rules
o special_consonant_rules o default_rules ).
macro(special_vowel_rules,
</bodyText>
<equation confidence="0.995023047619048">
g2p(re,u] x Ce,j,}], m) %% museum
o g2p(i x @, , g) %% moedig(st)
o g2p([i,j] x @, 1, k) %% mogelijkheid
macro(short_vowel_rules,
g2p(e x &apos;E&apos;, f[t,t],[k,k] ,x,
g2p({ a x &apos;A&apos; , e x @, i x &apos;I&apos; , o x &apos;0&apos;, u x
macro(special_consonant_rules,
g2p(b x p, [7, fs,t,#1)
o g2p([d,t] x t, D ,
o g2p f x v, s x n , fb,d1)
o g2p(g x &apos;G&apos;, vowel, vowel)
o g2p(n x &apos;N&apos;, [7, fk,q1)
o g2p (n x 0 , , C#1)
macro(default_rules ,
g2p({ [a,a] x a, [a,a,i] x (a,j] , [a,u] x &apos;M&apos;,
x b, [d,d] x d, , lc ,10 x &apos;x&apos;,
1 , , D )
) .
&apos; C], [cons, {cons , #}] )
[e,a,u] x o, ,
rs,c,h1 x [s,x] , [n,g] x
</equation>
<figureCaption confidence="0.994682">
Figure 2: Conversion Rules
</figureCaption>
<bodyText confidence="0.990243620689655">
between graphemes and phonemes is usually one to
one, but it is no problem to align a grapheme with
two or more phonemes. Null symbols are only intro-
duced in the output if a grapheme, such as word-final
n&apos;, is not realized phonologically.
For TBL, the input actually has to be aligned both
with the system output as well as with the correct
phoneme string. The first task can be solved triv-
ially: since our finite state system proceeds by first
segmenting the input into graphemes (sequences of
characters), and then transduces each grapheme into
a sequence of phonemes, we can obtain aligned data
by simply aligning each grapheme with its corre-
sponding phoneme string. The input is segmented
into graphemes by doing the segmentation step of
the finite state transducer only. The corresponding
phoneme strings can be identified by applying the
conversion transducer to the segmented input, while
keeping the boundary symbols &apos;-&apos; and &apos;+&apos;. As a con-
sequence of the design of the conversion-rules, the
resulting sequence of separated phonemes sequences
stands in a one-to-one relationship to the graphemes.
An example is shown in figure 3, where GR represents
the grapheme segmented string, and SP the (system)
phoneme strings produced by the finite state trans-
ducer. Note that the final SP cell contains only a
boundary marker, indicating that the grapheme &apos;n&apos;
is translated into the null phoneme.
For the alignment between graphemes (and, idi-
</bodyText>
<table confidence="0.90091025">
Word aalbessen (currants)
GR aa- 1- b- e - ss- e - n -
SP a + 1- b- ©-i- s + ©-i- +
CP a 1 b E s ©
</table>
<figureCaption confidence="0.995472">
Figure 3: Alignment
</figureCaption>
<bodyText confidence="0.9999807">
rectly, the system output) and the correct phoneme
strings (as found in Celex), we used the &apos;hand-
seeded&apos; probabilistic alignment procedure described
by Black et al. (1998). From the finite state conver-
sion rules, a set of possible grapheme -4 phoneme se-
quence mappings can be derived. This allowables-set
was extended with (exceptional) mappings present
in the correct data, but not in the hand-crafted sys-
tem. We computed all possible alignments between
(segmented) words and correct phoneme strings li-
cenced by the allowables-set. Next, probabilities for
all allowed mappings were estimated on the basis
of all possible alignments, and the data was parsed
again, now picking the most probable alignment for
each word. To minimize the number of words that
could not be aligned, a maximum of one unseen map-
ping (which was assigned a low probability) was al-
lowed per word. With this modification, only one
out of 1000 words on average could not be aligned.&apos;
These words were discarded.The aligned phoneme
</bodyText>
<footnote confidence="0.9844405">
5Typical cases are loan words (umpires) and letter words
(i.e. abbreviations) (abc).
</footnote>
<page confidence="0.988301">
307
</page>
<table confidence="0.999471230769231">
method training data phoneme word induced CPU time
(words) accuracy accuracy rules (in minutes)
Base-line 93.6 60.6
Brill 20K 98.0 86.1 447 162
Brill 40K 98.4 88.9 812 858
lazy(5) 20K 97.6 83.5 337 43
lazy(5) 40K 98.2 87.0 701 190
lazy(5) 60K 98.4 88.3 922 397
lazy(10) 20K 97.7 84.3 368 83
lazy(10) 40K 98.2 87.5 738 335
lazy(10) 60K 98.4 88.9 974 711
lazy(5)+ 20K 98.6 89.8 1225 186
lazy(5)+ 40K 99.0 92.6 2221 603
</table>
<figureCaption confidence="0.995143">
Figure 4: Experimental Results using training data produced by graph2phon
</figureCaption>
<bodyText confidence="0.999853">
string for the example in figure 3 is shown in the
bottom line. Note that the final cell is empty, rep-
resenting the null phoneme.
</bodyText>
<subsectionHeader confidence="0.97502">
4.2 The experiments
</subsectionHeader>
<bodyText confidence="0.999840375">
For the experiments with TBL we used the tt-TBL-
package (Lager, 1999). This Prolog implementation
of TBL is considerably more efficient (up to ten
times faster) than Brill&apos;s original (C) implementa-
tion. The speed-up results mainly from using Pro-
log&apos;s first-argument indexing to access large quanti-
ties of data efficiently.
We constructed a set of 22 rule templates which
replace a predicted phoneme with a (corrected)
phoneme on the basis of the underlying segment,
and a context consisting either of phoneme strings,
with a maximum length of two on either side, or
a context consisting of graphemes, with a maximal
length of 1 on either side. Using only 20K words
(which corresponds to almost 180K segments), and
Brill&apos;s algorithm, we achieved a phoneme accuracy
of 98.0% (see figure 4) on a test set of 20K words of
unseen data.6 Going to 40K words resulted in 98.4%
phoneme accuracy. Note, however, that in spite of
the relative efficiency of the implementation, CPU
time also goes up sharply.
The heavy computation costs of TBL are due to
the fact that for each error in the training data, all
possible instantiations of the rule templates which
correct this error are generated, and for each of these
instantiated rules the score on the whole training set
has to be computed. Samuel et al. (1998) there-
fore propose an efficient, &apos;lazy&apos;, alternative, based
on Monte Carlo sampling of the rules. For each er-
ror in the training set, only a sample of the rules
is considered which might correct it. As rules which
correct a high number of errors have a higher chance
</bodyText>
<footnote confidence="0.852683333333333">
6The statistics for less time consuming experiments were
obtained by 10-fold cross-validation and for the more expen-
sive experiments by 5-fold cross-validation.
</footnote>
<bodyText confidence="0.99998765">
of being sampled at some point, higher scoring rules
are more likely to be generated than lower scoring
rules, but no exhaustive search is required. We ex-
perimented with sampling sizes 5 and 10. As CPU
requirements are more modest, we managed to per-
form experiments on 60K words in this case, which
lead to results which are comparable with Brill&apos;s al-
goritm applied to 40K words.
Apart from being able to work with larger data
sets, the &apos;lazy&apos; strategy also has the advantage that
it can cope with larger sets of rule templates. Brill&apos;s
algorithm slows down quickly when the set of rule
templates is extended, but for an algorithm based on
rule sampling, this effect is much less severe. Thus,
we also constructed a set of 500 rule templates, con-
taining transformation rules which allowed up to
three graphemes or phoneme sequences as left or
right context, and also allowed for disjunctive con-
texts (i.e. the context must contain an &apos;a&apos; at the
first or second position to the right). We used this
rule set in combination with a &apos;lazy&apos; strategy with
sampling size 5 (lazy(5)+ in figure 4). This led to a
further improvement of phoneme accuracy to 99.0%,
and word accuracy of 92.6%, using only 40K words
of training material.
Finally, we investigated what the contribution was
of using a relatively accurate training set. To this
end, we constructed an alternative training set, in
which every segment was associated with its most
probable phoneme (where frequencies were obtained
from the aligned CELEX data). As shown in figure 5,
the initial accuracy for such as system is much lower
than that of the hand-crafted system. The exper-
imental results, for the &apos;lazy&apos; algorithm with sam-
pling size 5, show that the phoneme accuracy for
training on 20K words is 0.3% less than for the cor-
responding experiment in figure 4. For 40K words,
the difference is still 0.2%, which, in both cases, cor-
responds to a difference in error rate of around 10%.
As might be expected, the number of induced rules
</bodyText>
<page confidence="0.994682">
308
</page>
<table confidence="0.9960066">
method training data phoneme word induced CPU time
(words) accuracy accuracy rules (in minutes)
Base-line 72.9 10.8
lazy(5) 20K 97.3 81.6 691 133
lazy(5) 40K 98.0 86.0 1075 705
</table>
<figureCaption confidence="0.978553">
Figure 5: Experimental results using data based on frequency.
</figureCaption>
<bodyText confidence="0.967652">
is much higher now, and thus CPU-requirements also
increase substantially.
</bodyText>
<sectionHeader confidence="0.989539" genericHeader="conclusions">
5 Concluding remarks
</sectionHeader>
<bodyText confidence="0.999978136363637">
We have presented a method for grapheme to
phoneme conversion, which combines a hand-crafted
finite state transducer with rules induced by a
transformation-based learning. An advantage of this
method is that it is able to achieve a high level of
accuracy using relatively small training sets. Busser
(1998), for instance, uses a memory-based learning
strategy to achieve 90.1% word accuracy on the same
task, but used 90% of the CELEX data (over 300K
words) as training set and a (character/phoneme)
window size of 9. Hoste et al. (2000a) achieve a
word accuracy of 95.7% and a phoneme accuracy of
99.5% on the same task, using a combination of ma-
chine learning techniques, as well as additional data
obtained from a second dictionary.
Given the result of Roche and Schabes (1997), an
obvious next step is to compile the induced rules into
an actual transducer, and to compose this with the
hand-crafted transducer. It should be noted, how-
ever, that the number of induced rules is quite large
in some of the experiments, so that the compilation
procedure may require some attention.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999719">
Evan L. Antworth. 1990. PC-KIMMO : a two-level
processor for morphological analysis. Summer In-
stitute of Linguistics, Dallas, Tex.
R. H. Baayen, R. Piepenbrock, and H. van Rijn.
1993. The CELEX Lexical Database (CD-ROM).
Linguistic Data Consortium, University of Penn-
sylvania, Philadelphia, PA.
Alan Black, Kevin Lenzo, and Vincent Pagel. 1998.
Issues in building general letter to sound rules. In
Proceedings of the 3rd ESCA/COCSADA Work-
shop on Speech Synthesis, pages 77-81, Jenolan
Caves, Australia.
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational
Linguistics, 21:543-566.
Bertjan Busser. 1998. TreeTalk-D: a machine learn-
ing approach to Dutch word pronunciation. In
Proceedings TSD Conference, pages 3-8, Masaryk
University, Czech Republic.
W. Daelemans and A. van den Bosch. 1996.
Language-independent data-oriented grapheme-
to-phoneme conversion. In Progress in Speech
Synthesis, pages 77-90, New York. Springer Ver-
lag.
Thierry Dutoit. 1997. An Introduction to Text-to-
Speech Synthesis. Kluwer, Dordrecht.
Dale Gerdemann and Gertjan van Noord. 1999.
Transducers from rewrite rules with backrefer-
ences. In Proceedings of the Ninth Conference of
the European Chapter of the Association for Com-
putational Linguistics, pages 126-133, Bergen.
Veronique Hoste, Walter Daelemans, Erik
Tjong Kim Sang, and Steven Gillis. 2000a.
Meta-learning of phonemic annotation of corpora.
ms., University of Antwerp.
Veronique Hoste, Steven Gillis, and Walter Daele-
mans. 2000b. A rule induction approach to mod-
elling regional pronunciation variation. ms., Uni-
versity of Antwerp.
Ronald Kaplan and Martin Kay. 1994. Regular
models of phonological rule systems. Computa-
tional Linguistics, 20(3).
L. Karttunen, Chanod, G. Grefenstette, and
A. Schiller. 1997. Regular expressions for lan-
guage engineering. Natural Lanuage Engineering,
pages 1-24.
Lauri Karttunen. 1995. The replace operator. In
33th Annual Meeting of the Association for Com-
putational Linguistics, pages 16-23, Boston, Mas-
sachusetts.
Torbjorn Lager. 1999. The ic--TBL System:
Logic programming tools for transformation-
based learning. In Proceedings of the Third In-
ternational Workshop on Computational Natu-
ral Language Learning (CoNLL&apos;99), pages 33-42,
Bergen.
S. C. G. Lawrence and G. Kaye. 1986. Alignment of
phonemes with their corresponding orthography.
Computer Speech and Language, 1 (2) : 153 -165.
Bernd Möbius, Richard Sproat, Jan van Santen, arid
Joseph Olive. 1997. The Bell Labs German text-
to-speech system: An overview. In Proceedings of
the European Conference on Speech Communica-
tion and Technology, pages 2443-2446, Rhodes.
J. R. Quinlan. 1993. C4.5: programs for machine
learning. Morgan Kaufmann Publishers, San Ma-
teo.
</reference>
<page confidence="0.990679">
309
</page>
<reference confidence="0.979710923076923">
Emmanuel Roche and Yves Schabes. 1997. Deter-
ministic part-of-speech tagging with finite-state
transducers. In Emmanuel Roche and Yves Sch-
abes, editors, Finite state language processing,
pages 205-239. MIT Press, Cambridge, Mass.
Ken Samuel, Sandra Carberry, and K. Vijay-
Shanker. 1998. Dialogue act tagging with
transformation-based learning. In Proceedings of
the 17th International Conference on Computa-
tional Linguistics (COLING-ACL &apos;98), Montreal.
Briony Williams. 1994. Welsh letter-to-sound rules:
rewrite rules and two-level rules compared. Com-
puter Speech and Language, 8:261-277.
</reference>
<page confidence="0.998506">
310
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.283638">
<title confidence="0.9991125">A Finite State and Data-Oriented Method for Grapheme to Phoneme Conversion</title>
<author confidence="0.924754">Gosse Bouma</author>
<affiliation confidence="0.692822">Alfa-informatica Rijksuniversiteit Groningen</affiliation>
<address confidence="0.991633333333333">Postbus 716 9700 AS Groningen The Netherlands</address>
<email confidence="0.670166">gosse@let.rug.n1</email>
<abstract confidence="0.996267909090909">A finite-state method, based on leftmost longestmatch replacement, is presented for segmenting words into graphemes, and for converting graphemes into phonemes. A small set of hand-crafted conversion rules for Dutch achieves a phoneme accuracy of over 93%. The accuracy of the system is further improved by using transformation-based learning. The phoneme accuracy of the best system (using a large rule and a &apos;lazy&apos; variant of Brill&apos;s algoritm), trained on only 40K words, reaches 99%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Evan L Antworth</author>
</authors>
<title>PC-KIMMO : a two-level processor for morphological analysis. Summer Institute of Linguistics,</title>
<date>1990</date>
<location>Dallas, Tex.</location>
<contexts>
<context position="14165" citStr="Antworth, 1990" startWordPosition="2291" endWordPosition="2292">cription. After several development cycles, we achieved a word accuracy of 60.6% and a phoneme accuracy (measured as the edit distance between the phoneme string produced by the system and the correct string, divided by the number of phonemes in the correct string) of 93.6%. There have been relatively few attempts at developing grapheme to phoneme conversion systems using finite state technology alone. Williams (1994) reports on a system for Welsh, which uses no less than 700 rules implemented in a rather restricted environment. The rules are also implemented in a two-level system, PC-KIMMO, (Antworth, 1990), but this still requires over 400 rules. Möbius et al. (1997) report on full-fledged text-to-speech system for German, containing around 200 rules (which are compiled into a weighted finite state transducer) for the graphemeto-phoneme conversion step. These numbers suggest that our implementation (which contains around 80 rules in total) benefits considerably from the flexibility and high-level of abstraction made available by finite state calculus. One might suspect that a two-level approach to grapheme to phoneme conversion is more appropriate than the sequential approach used here. Somewha</context>
</contexts>
<marker>Antworth, 1990</marker>
<rawString>Evan L. Antworth. 1990. PC-KIMMO : a two-level processor for morphological analysis. Summer Institute of Linguistics, Dallas, Tex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Baayen</author>
<author>R Piepenbrock</author>
<author>H van Rijn</author>
</authors>
<date>1993</date>
<booktitle>The CELEX Lexical Database (CD-ROM). Linguistic Data</booktitle>
<institution>Consortium, University of Pennsylvania,</institution>
<location>Philadelphia, PA.</location>
<marker>Baayen, Piepenbrock, van Rijn, 1993</marker>
<rawString>R. H. Baayen, R. Piepenbrock, and H. van Rijn. 1993. The CELEX Lexical Database (CD-ROM). Linguistic Data Consortium, University of Pennsylvania, Philadelphia, PA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Black</author>
<author>Kevin Lenzo</author>
<author>Vincent Pagel</author>
</authors>
<title>Issues in building general letter to sound rules.</title>
<date>1998</date>
<booktitle>In Proceedings of the 3rd ESCA/COCSADA Workshop on Speech Synthesis,</booktitle>
<pages>77--81</pages>
<location>Jenolan Caves, Australia.</location>
<contexts>
<context position="20627" citStr="Black et al. (1998)" startWordPosition="3389" endWordPosition="3392">le is shown in figure 3, where GR represents the grapheme segmented string, and SP the (system) phoneme strings produced by the finite state transducer. Note that the final SP cell contains only a boundary marker, indicating that the grapheme &apos;n&apos; is translated into the null phoneme. For the alignment between graphemes (and, idiWord aalbessen (currants) GR aa- 1- b- e - ss- e - n - SP a + 1- b- ©-i- s + ©-i- + CP a 1 b E s © Figure 3: Alignment rectly, the system output) and the correct phoneme strings (as found in Celex), we used the &apos;handseeded&apos; probabilistic alignment procedure described by Black et al. (1998). From the finite state conversion rules, a set of possible grapheme -4 phoneme sequence mappings can be derived. This allowables-set was extended with (exceptional) mappings present in the correct data, but not in the hand-crafted system. We computed all possible alignments between (segmented) words and correct phoneme strings licenced by the allowables-set. Next, probabilities for all allowed mappings were estimated on the basis of all possible alignments, and the data was parsed again, now picking the most probable alignment for each word. To minimize the number of words that could not be a</context>
</contexts>
<marker>Black, Lenzo, Pagel, 1998</marker>
<rawString>Alan Black, Kevin Lenzo, and Vincent Pagel. 1998. Issues in building general letter to sound rules. In Proceedings of the 3rd ESCA/COCSADA Workshop on Speech Synthesis, pages 77-81, Jenolan Caves, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--543</pages>
<contexts>
<context position="2519" citStr="Brill, 1995" startWordPosition="379" endWordPosition="380">ich converts sequences of characters into sequences of phonemes. Specifying the conversion rules is a difficult task. Although segmentation of the input can in principle be dispensed with, we found that writing conversion rules for segmented input substantially reduces the context-sensitivity and order-dependence of such rules. We manually developed a grapheme to phoneme transducer for Dutch data obtained from CELEX (Baayen et al., 1993) and achieved a word accuracy of 60.6% and a phoneme accuracy of 93.6%. To improve the performance of our system, we used transformation-based learning (TBL) (Brill, 1995). Training data are obtained by aligning the output of the hand-crafted finite state transducer with the correct phoneme strings. These data can then be used as input for TBL, provided that suitable rule templates are available. We performed several experiments, in which the amount of training data, the algorithm (Brill&apos;s original formulation and &apos;lazy&apos; variants (Samuel et al., 1998)), and the number of rule templates varied. The best experiment (40K words, using a &apos;lazy&apos; strategy with a large set of rule templates) induces over 2000 transformation rules, leading to 92.6% word accuracy and 99.</context>
<context position="15765" citStr="Brill (1995)" startWordPosition="2542" endWordPosition="2543">o this on the basis of linguistic knowledge alone. That is, most of the rules which have to be added deal with highly idiosyncratic cases (often related to loan-words) which can only be discov3It should be noted that we only considered words which do not contain diacritics. Including those is unproblematic in principle, but would lead to a slight increase of the number of rules. ered by browsing through the test results of previous runs. At this point, switching from a linguisticsoriented to a data-oriented methodology, seemed appropriate. 4 Transformation-based grapheme to phoneme conversion Brill (1995) demonstrates that accurate part-ofspeech tagging can be learned by using a two-step process. First, a simple system is used which assigns the most probable tag to each word. The results of the system are aligned with the correct tags for some corpus of training data. Next, (contextsensitive) transformation rules are selected from a pool of rule patterns, which replace erroneous tags by correct tags. The rule with the largest benefit on the training data (i.e. the rule for which the number of corrections minus the number of newly introduced mistakes, is the largest) is learned and applied to t</context>
</contexts>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21:543-566.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bertjan Busser</author>
</authors>
<title>TreeTalk-D: a machine learning approach to Dutch word pronunciation.</title>
<date>1998</date>
<booktitle>In Proceedings TSD Conference,</booktitle>
<pages>3--8</pages>
<institution>Masaryk University, Czech Republic.</institution>
<contexts>
<context position="26555" citStr="Busser (1998)" startWordPosition="4390" endWordPosition="4391"> word induced CPU time (words) accuracy accuracy rules (in minutes) Base-line 72.9 10.8 lazy(5) 20K 97.3 81.6 691 133 lazy(5) 40K 98.0 86.0 1075 705 Figure 5: Experimental results using data based on frequency. is much higher now, and thus CPU-requirements also increase substantially. 5 Concluding remarks We have presented a method for grapheme to phoneme conversion, which combines a hand-crafted finite state transducer with rules induced by a transformation-based learning. An advantage of this method is that it is able to achieve a high level of accuracy using relatively small training sets. Busser (1998), for instance, uses a memory-based learning strategy to achieve 90.1% word accuracy on the same task, but used 90% of the CELEX data (over 300K words) as training set and a (character/phoneme) window size of 9. Hoste et al. (2000a) achieve a word accuracy of 95.7% and a phoneme accuracy of 99.5% on the same task, using a combination of machine learning techniques, as well as additional data obtained from a second dictionary. Given the result of Roche and Schabes (1997), an obvious next step is to compile the induced rules into an actual transducer, and to compose this with the hand-crafted tr</context>
</contexts>
<marker>Busser, 1998</marker>
<rawString>Bertjan Busser. 1998. TreeTalk-D: a machine learning approach to Dutch word pronunciation. In Proceedings TSD Conference, pages 3-8, Masaryk University, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>A van den Bosch</author>
</authors>
<title>Language-independent data-oriented graphemeto-phoneme conversion.</title>
<date>1996</date>
<booktitle>In Progress in Speech Synthesis,</booktitle>
<pages>77--90</pages>
<publisher>Springer Verlag.</publisher>
<location>New York.</location>
<marker>Daelemans, van den Bosch, 1996</marker>
<rawString>W. Daelemans and A. van den Bosch. 1996. Language-independent data-oriented graphemeto-phoneme conversion. In Progress in Speech Synthesis, pages 77-90, New York. Springer Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thierry Dutoit</author>
</authors>
<title>An Introduction to Text-toSpeech Synthesis.</title>
<date>1997</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="17531" citStr="Dutoit, 1997" startWordPosition="2841" endWordPosition="2843">equires aligned data for training and testing. While alignment is mostly trivial for part-of-speech tagging, this is not the case for the present task. Aligning data for grapheme-to-phoneme conversion amounts to aligning each part of the input (a sequence of characters) with a part of the output (a sequence of phonemes). As the length of both sequences is not guaranteed to be equal, it must be possible to align more than one character with a single phoneme (the usual case) or a single character with more than one phoneme (the exceptional case, i.e. &apos;x&apos;). The alignment problem is often solved (Dutoit, 1997; Daelemans and van den Bosch, 1996) by allowing &apos;null&apos; symbols in the phoneme string, and introducing &apos;compound&apos; phonemes, such as &apos;ks&apos; to account for exceptional cases where a single character must be aligned with two phonemes. As our finite state system already segments the input into graphemes, we have adopted a strategy where graphemes instead of characters are aligned with phoneme strings (see Lawrence and Kaye (1986) for a similar approach). The correspondence 4Hoste et al. (2000b) compare TBL to C5.0 (Quinlan, 1993) on a similar task, i.e. the mapping of the pronunciation of one region</context>
</contexts>
<marker>Dutoit, 1997</marker>
<rawString>Thierry Dutoit. 1997. An Introduction to Text-toSpeech Synthesis. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dale Gerdemann</author>
<author>Gertjan van Noord</author>
</authors>
<title>Transducers from rewrite rules with backreferences.</title>
<date>1999</date>
<booktitle>In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>126--133</pages>
<location>Bergen.</location>
<marker>Gerdemann, van Noord, 1999</marker>
<rawString>Dale Gerdemann and Gertjan van Noord. 1999. Transducers from rewrite rules with backreferences. In Proceedings of the Ninth Conference of the European Chapter of the Association for Computational Linguistics, pages 126-133, Bergen.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Veronique Hoste</author>
</authors>
<title>Walter Daelemans, Erik Tjong Kim Sang, and Steven Gillis. 2000a. Meta-learning of phonemic annotation of corpora. ms.,</title>
<institution>University of Antwerp.</institution>
<marker>Hoste, </marker>
<rawString>Veronique Hoste, Walter Daelemans, Erik Tjong Kim Sang, and Steven Gillis. 2000a. Meta-learning of phonemic annotation of corpora. ms., University of Antwerp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronique Hoste</author>
<author>Steven Gillis</author>
<author>Walter Daelemans</author>
</authors>
<title>A rule induction approach to modelling regional pronunciation variation. ms.,</title>
<date>2000</date>
<institution>University of Antwerp.</institution>
<contexts>
<context position="18022" citStr="Hoste et al. (2000" startWordPosition="2918" endWordPosition="2921">ngle character with more than one phoneme (the exceptional case, i.e. &apos;x&apos;). The alignment problem is often solved (Dutoit, 1997; Daelemans and van den Bosch, 1996) by allowing &apos;null&apos; symbols in the phoneme string, and introducing &apos;compound&apos; phonemes, such as &apos;ks&apos; to account for exceptional cases where a single character must be aligned with two phonemes. As our finite state system already segments the input into graphemes, we have adopted a strategy where graphemes instead of characters are aligned with phoneme strings (see Lawrence and Kaye (1986) for a similar approach). The correspondence 4Hoste et al. (2000b) compare TBL to C5.0 (Quinlan, 1993) on a similar task, i.e. the mapping of the pronunciation of one regional variant of Dutch into another. 306 macro(conversion, special_vowel_rules o short_vowel_rules o special_consonant_rules o default_rules ). macro(special_vowel_rules, g2p(re,u] x Ce,j,}], m) %% museum o g2p(i x @, , g) %% moedig(st) o g2p([i,j] x @, 1, k) %% mogelijkheid macro(short_vowel_rules, g2p(e x &apos;E&apos;, f[t,t],[k,k] ,x, g2p({ a x &apos;A&apos; , e x @, i x &apos;I&apos; , o x &apos;0&apos;, u x macro(special_consonant_rules, g2p(b x p, [7, fs,t,#1) o g2p([d,t] x t, D , o g2p f x v, s x n , fb,d1) o g2p(g x &apos;G&apos;</context>
</contexts>
<marker>Hoste, Gillis, Daelemans, 2000</marker>
<rawString>Veronique Hoste, Steven Gillis, and Walter Daelemans. 2000b. A rule induction approach to modelling regional pronunciation variation. ms., University of Antwerp.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronald Kaplan</author>
<author>Martin Kay</author>
</authors>
<title>Regular models of phonological rule systems.</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<issue>3</issue>
<contexts>
<context position="3309" citStr="Kaplan and Kay (1994)" startWordPosition="505" endWordPosition="508">r TBL, provided that suitable rule templates are available. We performed several experiments, in which the amount of training data, the algorithm (Brill&apos;s original formulation and &apos;lazy&apos; variants (Samuel et al., 1998)), and the number of rule templates varied. The best experiment (40K words, using a &apos;lazy&apos; strategy with a large set of rule templates) induces over 2000 transformation rules, leading to 92.6% word accuracy and 99.0% phoneme accuracy. This result, obtained using a relatively small set of training data, compares well with that of other systems. 2 Finite State Calculus As argued in Kaplan and Kay (1994), Karttunen (1995), Karttunen et, al. (1997), and elsewhere, many of the rules used in phonology and morphology can be analysed as special cases of regular expressions. By extending the language of regular expressions with operators which capture the interpretation of linguistic rule systems, high-level linguistic descriptions can be compiled into finite state automata directly. Furthermore, such automata can be combined with other finite state automata performing low-level tasks such as tokenization or lexicallookup, or more advanced tasks such as shallow parsing. Composition of the individua</context>
</contexts>
<marker>Kaplan, Kay, 1994</marker>
<rawString>Ronald Kaplan and Martin Kay. 1994. Regular models of phonological rule systems. Computational Linguistics, 20(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Karttunen</author>
<author>G Grefenstette Chanod</author>
<author>A Schiller</author>
</authors>
<title>Regular expressions for language engineering. Natural Lanuage Engineering,</title>
<date>1997</date>
<pages>1--24</pages>
<marker>Karttunen, Chanod, Schiller, 1997</marker>
<rawString>L. Karttunen, Chanod, G. Grefenstette, and A. Schiller. 1997. Regular expressions for language engineering. Natural Lanuage Engineering, pages 1-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lauri Karttunen</author>
</authors>
<title>The replace operator.</title>
<date>1995</date>
<booktitle>In 33th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<location>Boston, Massachusetts.</location>
<contexts>
<context position="3327" citStr="Karttunen (1995)" startWordPosition="509" endWordPosition="510">itable rule templates are available. We performed several experiments, in which the amount of training data, the algorithm (Brill&apos;s original formulation and &apos;lazy&apos; variants (Samuel et al., 1998)), and the number of rule templates varied. The best experiment (40K words, using a &apos;lazy&apos; strategy with a large set of rule templates) induces over 2000 transformation rules, leading to 92.6% word accuracy and 99.0% phoneme accuracy. This result, obtained using a relatively small set of training data, compares well with that of other systems. 2 Finite State Calculus As argued in Kaplan and Kay (1994), Karttunen (1995), Karttunen et, al. (1997), and elsewhere, many of the rules used in phonology and morphology can be analysed as special cases of regular expressions. By extending the language of regular expressions with operators which capture the interpretation of linguistic rule systems, high-level linguistic descriptions can be compiled into finite state automata directly. Furthermore, such automata can be combined with other finite state automata performing low-level tasks such as tokenization or lexicallookup, or more advanced tasks such as shallow parsing. Composition of the individual components into </context>
<context position="5035" citStr="Karttunen (1995)" startWordPosition="782" endWordPosition="783">ransducer which maps all strings in A to all strings in B. identity: the transducer which maps each element in A onto itself. composition of the transducers T and U. use Term as an abbreviation for R (where Term and R may contain variables). Figure 1: A fragment of FSA regular expression syntax. A and B are regular expressions denoting recognizers, T and U transducers, and R can be either. ators. The part of FSA&apos;s built-in regular expression syntax relevant to this paper, is listed in figure 1. One particular useful extension of the basic syntax of regular expressions is the replace-operator. Karttunen (1995) argues that many phonological and morphological rules can be interpreted as rules which replace a certain portion of the input string. Although several implementations of the replaceoperator are proposed, the most relevant case for our purposes is so-called &apos;leftmost longest-match&apos; replacement. In case of overlapping rule targets in the input, this operator will replace the leftmost target, and in cases where a rule target contains a prefix which is also a potential target, the longer sequence will be replaced. Gerdemann and van Noord (1999) implement leftmost longest-match replacement in FSA</context>
</contexts>
<marker>Karttunen, 1995</marker>
<rawString>Lauri Karttunen. 1995. The replace operator. In 33th Annual Meeting of the Association for Computational Linguistics, pages 16-23, Boston, Massachusetts.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Torbjorn Lager</author>
</authors>
<title>The ic--TBL System: Logic programming tools for transformationbased learning.</title>
<date>1999</date>
<booktitle>In Proceedings of the Third International Workshop on Computational Natural Language Learning (CoNLL&apos;99),</booktitle>
<pages>33--42</pages>
<location>Bergen.</location>
<contexts>
<context position="22258" citStr="Lager, 1999" startWordPosition="3667" endWordPosition="3668">ase-line 93.6 60.6 Brill 20K 98.0 86.1 447 162 Brill 40K 98.4 88.9 812 858 lazy(5) 20K 97.6 83.5 337 43 lazy(5) 40K 98.2 87.0 701 190 lazy(5) 60K 98.4 88.3 922 397 lazy(10) 20K 97.7 84.3 368 83 lazy(10) 40K 98.2 87.5 738 335 lazy(10) 60K 98.4 88.9 974 711 lazy(5)+ 20K 98.6 89.8 1225 186 lazy(5)+ 40K 99.0 92.6 2221 603 Figure 4: Experimental Results using training data produced by graph2phon string for the example in figure 3 is shown in the bottom line. Note that the final cell is empty, representing the null phoneme. 4.2 The experiments For the experiments with TBL we used the tt-TBLpackage (Lager, 1999). This Prolog implementation of TBL is considerably more efficient (up to ten times faster) than Brill&apos;s original (C) implementation. The speed-up results mainly from using Prolog&apos;s first-argument indexing to access large quantities of data efficiently. We constructed a set of 22 rule templates which replace a predicted phoneme with a (corrected) phoneme on the basis of the underlying segment, and a context consisting either of phoneme strings, with a maximum length of two on either side, or a context consisting of graphemes, with a maximal length of 1 on either side. Using only 20K words (whi</context>
</contexts>
<marker>Lager, 1999</marker>
<rawString>Torbjorn Lager. 1999. The ic--TBL System: Logic programming tools for transformationbased learning. In Proceedings of the Third International Workshop on Computational Natural Language Learning (CoNLL&apos;99), pages 33-42, Bergen.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C G Lawrence</author>
<author>G Kaye</author>
</authors>
<title>Alignment of phonemes with their corresponding orthography.</title>
<date>1986</date>
<journal>Computer Speech and Language,</journal>
<volume>1</volume>
<issue>2</issue>
<pages>153--165</pages>
<contexts>
<context position="17958" citStr="Lawrence and Kaye (1986)" startWordPosition="2908" endWordPosition="2911">more than one character with a single phoneme (the usual case) or a single character with more than one phoneme (the exceptional case, i.e. &apos;x&apos;). The alignment problem is often solved (Dutoit, 1997; Daelemans and van den Bosch, 1996) by allowing &apos;null&apos; symbols in the phoneme string, and introducing &apos;compound&apos; phonemes, such as &apos;ks&apos; to account for exceptional cases where a single character must be aligned with two phonemes. As our finite state system already segments the input into graphemes, we have adopted a strategy where graphemes instead of characters are aligned with phoneme strings (see Lawrence and Kaye (1986) for a similar approach). The correspondence 4Hoste et al. (2000b) compare TBL to C5.0 (Quinlan, 1993) on a similar task, i.e. the mapping of the pronunciation of one regional variant of Dutch into another. 306 macro(conversion, special_vowel_rules o short_vowel_rules o special_consonant_rules o default_rules ). macro(special_vowel_rules, g2p(re,u] x Ce,j,}], m) %% museum o g2p(i x @, , g) %% moedig(st) o g2p([i,j] x @, 1, k) %% mogelijkheid macro(short_vowel_rules, g2p(e x &apos;E&apos;, f[t,t],[k,k] ,x, g2p({ a x &apos;A&apos; , e x @, i x &apos;I&apos; , o x &apos;0&apos;, u x macro(special_consonant_rules, g2p(b x p, [7, fs,t,#1</context>
</contexts>
<marker>Lawrence, Kaye, 1986</marker>
<rawString>S. C. G. Lawrence and G. Kaye. 1986. Alignment of phonemes with their corresponding orthography. Computer Speech and Language, 1 (2) : 153 -165.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Möbius</author>
<author>Richard Sproat</author>
<author>Jan van Santen</author>
<author>arid Joseph Olive</author>
</authors>
<title>The Bell Labs German textto-speech system: An overview.</title>
<date>1997</date>
<booktitle>In Proceedings of the European Conference on Speech Communication and Technology,</booktitle>
<pages>2443--2446</pages>
<location>Rhodes.</location>
<marker>Möbius, Sproat, van Santen, Olive, 1997</marker>
<rawString>Bernd Möbius, Richard Sproat, Jan van Santen, arid Joseph Olive. 1997. The Bell Labs German textto-speech system: An overview. In Proceedings of the European Conference on Speech Communication and Technology, pages 2443-2446, Rhodes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Quinlan</author>
</authors>
<title>C4.5: programs for machine learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers,</publisher>
<location>San Mateo.</location>
<contexts>
<context position="18060" citStr="Quinlan, 1993" startWordPosition="2926" endWordPosition="2927">(the exceptional case, i.e. &apos;x&apos;). The alignment problem is often solved (Dutoit, 1997; Daelemans and van den Bosch, 1996) by allowing &apos;null&apos; symbols in the phoneme string, and introducing &apos;compound&apos; phonemes, such as &apos;ks&apos; to account for exceptional cases where a single character must be aligned with two phonemes. As our finite state system already segments the input into graphemes, we have adopted a strategy where graphemes instead of characters are aligned with phoneme strings (see Lawrence and Kaye (1986) for a similar approach). The correspondence 4Hoste et al. (2000b) compare TBL to C5.0 (Quinlan, 1993) on a similar task, i.e. the mapping of the pronunciation of one regional variant of Dutch into another. 306 macro(conversion, special_vowel_rules o short_vowel_rules o special_consonant_rules o default_rules ). macro(special_vowel_rules, g2p(re,u] x Ce,j,}], m) %% museum o g2p(i x @, , g) %% moedig(st) o g2p([i,j] x @, 1, k) %% mogelijkheid macro(short_vowel_rules, g2p(e x &apos;E&apos;, f[t,t],[k,k] ,x, g2p({ a x &apos;A&apos; , e x @, i x &apos;I&apos; , o x &apos;0&apos;, u x macro(special_consonant_rules, g2p(b x p, [7, fs,t,#1) o g2p([d,t] x t, D , o g2p f x v, s x n , fb,d1) o g2p(g x &apos;G&apos;, vowel, vowel) o g2p(n x &apos;N&apos;, [7, fk,</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. R. Quinlan. 1993. C4.5: programs for machine learning. Morgan Kaufmann Publishers, San Mateo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emmanuel Roche</author>
<author>Yves Schabes</author>
</authors>
<title>Deterministic part-of-speech tagging with finite-state transducers.</title>
<date>1997</date>
<booktitle>In Emmanuel Roche and Yves Schabes, editors, Finite state language processing,</booktitle>
<pages>205--239</pages>
<publisher>MIT Press,</publisher>
<location>Cambridge, Mass.</location>
<marker>Roche, Schabes, 1997</marker>
<rawString>Emmanuel Roche and Yves Schabes. 1997. Deterministic part-of-speech tagging with finite-state transducers. In Emmanuel Roche and Yves Schabes, editors, Finite state language processing, pages 205-239. MIT Press, Cambridge, Mass.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Samuel</author>
<author>Sandra Carberry</author>
<author>K VijayShanker</author>
</authors>
<title>Dialogue act tagging with transformation-based learning.</title>
<date>1998</date>
<booktitle>In Proceedings of the 17th International Conference on Computational Linguistics (COLING-ACL &apos;98),</booktitle>
<location>Montreal.</location>
<contexts>
<context position="2905" citStr="Samuel et al., 1998" startWordPosition="438" endWordPosition="441">sducer for Dutch data obtained from CELEX (Baayen et al., 1993) and achieved a word accuracy of 60.6% and a phoneme accuracy of 93.6%. To improve the performance of our system, we used transformation-based learning (TBL) (Brill, 1995). Training data are obtained by aligning the output of the hand-crafted finite state transducer with the correct phoneme strings. These data can then be used as input for TBL, provided that suitable rule templates are available. We performed several experiments, in which the amount of training data, the algorithm (Brill&apos;s original formulation and &apos;lazy&apos; variants (Samuel et al., 1998)), and the number of rule templates varied. The best experiment (40K words, using a &apos;lazy&apos; strategy with a large set of rule templates) induces over 2000 transformation rules, leading to 92.6% word accuracy and 99.0% phoneme accuracy. This result, obtained using a relatively small set of training data, compares well with that of other systems. 2 Finite State Calculus As argued in Kaplan and Kay (1994), Karttunen (1995), Karttunen et, al. (1997), and elsewhere, many of the rules used in phonology and morphology can be analysed as special cases of regular expressions. By extending the language o</context>
<context position="23490" citStr="Samuel et al. (1998)" startWordPosition="3873" endWordPosition="3876">ds to almost 180K segments), and Brill&apos;s algorithm, we achieved a phoneme accuracy of 98.0% (see figure 4) on a test set of 20K words of unseen data.6 Going to 40K words resulted in 98.4% phoneme accuracy. Note, however, that in spite of the relative efficiency of the implementation, CPU time also goes up sharply. The heavy computation costs of TBL are due to the fact that for each error in the training data, all possible instantiations of the rule templates which correct this error are generated, and for each of these instantiated rules the score on the whole training set has to be computed. Samuel et al. (1998) therefore propose an efficient, &apos;lazy&apos;, alternative, based on Monte Carlo sampling of the rules. For each error in the training set, only a sample of the rules is considered which might correct it. As rules which correct a high number of errors have a higher chance 6The statistics for less time consuming experiments were obtained by 10-fold cross-validation and for the more expensive experiments by 5-fold cross-validation. of being sampled at some point, higher scoring rules are more likely to be generated than lower scoring rules, but no exhaustive search is required. We experimented with sa</context>
</contexts>
<marker>Samuel, Carberry, VijayShanker, 1998</marker>
<rawString>Ken Samuel, Sandra Carberry, and K. VijayShanker. 1998. Dialogue act tagging with transformation-based learning. In Proceedings of the 17th International Conference on Computational Linguistics (COLING-ACL &apos;98), Montreal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Briony Williams</author>
</authors>
<title>Welsh letter-to-sound rules: rewrite rules and two-level rules compared.</title>
<date>1994</date>
<journal>Computer Speech and Language,</journal>
<pages>8--261</pages>
<contexts>
<context position="13971" citStr="Williams (1994)" startWordPosition="2258" endWordPosition="2259">3 Test results and discussion The accuracy of the hand-crafted system was evaluated by testing it on all of the words wihtout diacritics in the CELEX lexical database which have a phonetic transcription. After several development cycles, we achieved a word accuracy of 60.6% and a phoneme accuracy (measured as the edit distance between the phoneme string produced by the system and the correct string, divided by the number of phonemes in the correct string) of 93.6%. There have been relatively few attempts at developing grapheme to phoneme conversion systems using finite state technology alone. Williams (1994) reports on a system for Welsh, which uses no less than 700 rules implemented in a rather restricted environment. The rules are also implemented in a two-level system, PC-KIMMO, (Antworth, 1990), but this still requires over 400 rules. Möbius et al. (1997) report on full-fledged text-to-speech system for German, containing around 200 rules (which are compiled into a weighted finite state transducer) for the graphemeto-phoneme conversion step. These numbers suggest that our implementation (which contains around 80 rules in total) benefits considerably from the flexibility and high-level of abst</context>
</contexts>
<marker>Williams, 1994</marker>
<rawString>Briony Williams. 1994. Welsh letter-to-sound rules: rewrite rules and two-level rules compared. Computer Speech and Language, 8:261-277.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>