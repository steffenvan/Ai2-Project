<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008151">
<title confidence="0.972024">
A Maximum-Entropy-Inspired Parser *
</title>
<author confidence="0.981074">
Eugene Charniak
</author>
<affiliation confidence="0.824243666666667">
Brown Laboratory for Linguistic Information Processing
Department of Computer Science
Brown University, Box 1910, Providence, RI 02912
</affiliation>
<email confidence="0.877199">
ecOcs.brown.edu
</email>
<sectionHeader confidence="0.995999" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999892473684211">
We present a new parser for parsing down to
Penn tree-bank style parse trees that achieves
90.1% average precision/recall for sentences of
length 40 and less, and 89.5% for sentences of
length 100 and less when trained and tested on
the previously established [5,9,10,15,17] &amp;quot;stan-
dard&amp;quot; sections of the Wall Street Journal tree-
bank. This represents a 13% decrease in er-
ror rate over the best single-parser results on
this corpus [9]. The major technical innova-
tion is the use of a &amp;quot;maximum-entropy-inspired&amp;quot;
model for conditioning and smoothing that let
us successfully to test and combine many differ-
ent conditioning events. We also present some
partial results showing the effects of different
conditioning information, including a surpris-
ing 2% improvement due to guessing the lexical
head&apos;s pre-terminal before guessing the lexical
head.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.895273576923077">
We present a new parser for parsing down
to Penn tree-bank style parse trees [16] that
achieves 90.1% average precision/recall for sen-
tences of length &lt; 40, and 89.5% for sentences
of length &lt; 100, when trained and tested on the
previously established [5,9,10,15,17] &amp;quot;standard&amp;quot;
sections of the Wall Street Journal tree-bank.
This represents a 13% decrease in error rate over
the best single-parser results on this corpus [9].
Following [5,10], our parser is based upon a
probabilistic generative model. That is, for all
sentences s and all parses 7r, the parser assigns a
probability p(s , 7r) = p(r), the equality holding
when we restrict consideration to 7r whose yield
* This research was supported in part by NSF grant
LIS SBR 9720368. The author would like to thank Mark
Johnson and all the rest of the Brown Laboratory for
Linguistic Information Processing.
is s. Then for any s the parser returns the parse
ir that maximizes this probability. That is, the
parser implements the function
arg maxrp(7r s) = arg maxirp(7r, s)
= arg maxrp(w).
What fundamentally distinguishes probabilis-
tic generative parsers is how they compute p(r),
and it is to that topic we turn next.
</bodyText>
<sectionHeader confidence="0.993681" genericHeader="method">
2 The Generative Model
</sectionHeader>
<bodyText confidence="0.999984714285714">
The model assigns a probability to a parse by
a top-down process of considering each con-
stituent c in Ir and for each c first guessing the
pre-terminal of c, t(c) (t for &amp;quot;tag&amp;quot;), then the
lexical head of c, h(c), and then the expansion
of c into further constituents e(c). Thus the
probability of a parse is given by the equation
</bodyText>
<equation confidence="0.9990605">
P(r) = II p(t(c) ) 1(c), H (c))
cEir
.p(h(c) I t(c), 1(c) , H (c))
-p(e(c) 1(c), t(c), h(c) , H (c))
</equation>
<bodyText confidence="0.9993819">
where 1(c) is the label of c (e.g., whether it is a
noun phrase (np), verb-phrase, etc.) and H (c) is
the relevant history of c — information outside c
that our probability model deems important in
determining the probability in question. Much
of the interesting work is determining what goes
into H (c). Whenever it is clear to which con-
stituent we are referring we omit the (c) in, e.g.,
h(c). In this notation the above equation takes
the following form:
</bodyText>
<equation confidence="0.979989666666667">
p(r) = J p(t I 1, H).p(h It, 1, H).p(e I 1, t , Ii, H).
cE/1&amp;quot;
(1)
</equation>
<page confidence="0.990595">
132
</page>
<bodyText confidence="0.999973142857143">
Next we describe how we assign a probability
to the expansion e of a constituent. In Sec-
tion 5 we present some results in which the
possible expansions of a constituent are fixed
in advanced by extracting a tree-bank grammar
[3] from the training corpus. The method that
gives the best results, however, uses a Markov
grammar — a method for assigning probabil-
ities to any possible expansion using statistics
gathered from the training corpus [6,10,15]. The
method we use follows that of [10]. In this
scheme a traditional probabilistic context-free
grammar (PCFG) rule can be thought of as con-
sisting of a left-hand side with a label 1(c) drawn
from the non-terminal symbols of our grammar,
and a right-hand side that is a sequence of one or
more such symbols. (We assume that all termi-
nal symbols are generated by rules of the form
&amp;quot;preterm word&amp;quot; and we treat these as a spe-
cial case.) For us the non-terminal symbols are
those of the tree-bank, augmented by the sym-
bols aux and auxg, which have been assigned de-
terministically to certain auxiliary verbs such as
&amp;quot;have&amp;quot; or &amp;quot;having&amp;quot;. For each expansion we dis-
tinguish one of the right-hand side labels as the
&amp;quot;middle&amp;quot; or &amp;quot;head&amp;quot; symbol M(c). M(c) is the
constituent from which the head lexical item h
is obtained according to deterministic rules that
pick the head of a constituent from among the
heads of its children. To the left of M is a se-
quence of one or more left labels Li (c) including
the special termination symbol A, which indi-
cates that there are no more symbols to the left,
and similarly for the labels to the right, Ri(c).
Thus an expansion e(c) looks like:
</bodyText>
<equation confidence="0.838127">
1-4 AL,„...LiM Ri...RnA. (2)
</equation>
<bodyText confidence="0.99775648">
The expansion is generated by guessing first M,
then in order L1 through L,„.+1 (= A), and sim-
ilarly for RI through
In a pure Markov PCFG we are given the
left-hand side label 1 and then probabilistically
generate the right-hand side conditioning on no
information other than 1 and (possibly) previ-
ously generated pieces of the right-hand side
itself. In the simplest of such models, a zero-
order Markov grammar, each label on the right-
hand side is generated conditioned only on / —
that is, according to the distributions p(Li j1),
p(M I 1), and p(Ri I 1).
More generally, one can condition on the m
previously generated labels, thereby obtaining
an mth-order Markov grammar. So, for ex-
ample, in a second-order Markov PCFG, L2
would be conditioned on L1 and M. In our
complete model, of course, the probability of
each label in the expansions is also conditioned
on other material as specified in Equation 1,
e.g., p(e t, h, H). Thus we would use p(L2 I
L1, M, 1, t, h, H). Note that the As on both ends
of the expansion in Expression 2 are conditioned
just like any other label in the expansion.
</bodyText>
<sectionHeader confidence="0.752358" genericHeader="method">
3 Maximum-Entropy-Inspired
Parsing
</sectionHeader>
<bodyText confidence="0.999351756756757">
The major problem confronting the author of
a generative parser is what information to use
to condition the probabilities required in the
model, and how to smooth the empirically ob-
tained probabilities to take the sting out of the
sparse data problems that are inevitable with
even the most modest conditioning. For exam-
ple, in a second-order Markov grammar we con-
ditioned the L2 label according to the distribu-
tion p(L2 I L1, M,1, t, h, H). Also, remember
that H is a pla,ceholder for any other informa-
tion beyond the constituent c that may be useful
in assigning c a probability.
In the past few years the maximum entropy,
or log-linear, approach has recommended itself
to probabilistic model builders for its flexibility
and its novel approach to smoothing [1,17]. A
complete review of log-linear models is beyond
the scope of this paper. Rather, we concentrate
on the aspects of these models that most di-
rectly influenced the model presented here.
To compute a probability in a log-linear
model one first defines a set of &amp;quot;features&amp;quot;,
functions from the space of configurations over
which one is trying to compute probabilities to
integers that denote the number of times some
pattern occurs in the input. In our work we as-
sume that any feature can occur at most once,
so features are boolean-valued: 0 if the pattern
does not occur, 1 if it does.
In the parser we further assume that fea-
tures are chosen from certain feature schemata
and that every feature is a boolean conjunc-
tion of sub-features. For example, in computing
the probability of the head&apos;s pre-terminal t we
might want a feature schema f (t, 1) that returns
1 if the observed pre-terminal of c = t and the
</bodyText>
<page confidence="0.998515">
133
</page>
<bodyText confidence="0.971209444444444">
label of c = 1, and zero otherwise. This feature
is obviously composed of two sub-features, one
recognizing t, the other 1. If both return 1, then
the feature returns 1.
Now consider computing a conditional prob-
ability p(a H) with a set of features h
that connect a to the history H. In a log-linear
model the probability function takes the follow-
ing form:
</bodyText>
<equation confidence="0.92669125">
1 exi(a,H)fi(a,H)+...+Am(a,H)fm(a,H)
p(a I H) =
Z(H)
(3)
</equation>
<bodyText confidence="0.995233">
Here the Ai are weights between negative and
positive infinity that indicate the relative impor-
tance of a feature: the more relevant the feature
to the value of the probability, the higher the ab-
solute value of the associated A. The function
Z(H), called the partition function, is a normal-
izing constant (for fixed H), so the probabilities
over all a sum to one.
Now for our purposes it is useful to rewrite
this as a sequence of multiplicative functions
gi(a, H) for 0 &lt; i &lt; j:
</bodyText>
<equation confidence="0.972658">
p(a I H) = go(a, H)gi(a, H) . . .gi(a, H). (4)
</equation>
<bodyText confidence="0.999888157894737">
Here go(a, H) = 11Z (H) and gi(a, H) =
eAi(a,H) fi(°,11). The intuitive idea is that each
factor gi is larger than one if the feature in ques-
tion makes the probability more likely, one if the
feature has no effect, and smaller than one if it
makes the probability less likely.
Maximum-entropy models have two benefits
for a parser builder. First, as already implicit in
our discussion, factoring the probability compu-
tation into a sequence of values corresponding
to various &amp;quot;features&amp;quot; suggests that the proba-
bility model should be easily changeable — just
change the set of features used. This point is
emphasized by Ratnaparkhi in discussing his
parser [17). Second, and this is a point we have
not yet mentioned, the features used in these
models need have no particular independence of
one another. This is useful if one is using a log-
linear model for smoothing. That is, suppose
we want to compute a conditional probability
p(a b, c), but we are not sure that we have
enough examples of the conditioning event b, c
in the training corpus to ensure that the empiri-
cally obtained probability P (a I b, c) is accurate.
The traditional way to handle this is also to
compute P(a b), and perhaps P(a c) as well,
and take some combination of these values as
one&apos;s best estimate for p(a I b, c). This method
is known as &amp;quot;deleted interpolation&amp;quot; smoothing.
In max-entropy models one can simply include
features for all three events f1 (a, b, c), f2 (a, b),
and f3(a, c) and combine them in the model ac-
cording to Equation 3, or equivalently, Equation
4. The fact that the features are very far from
independent is not a concern.
Now let us note that we can get an equation
of exactly the same form as Equation 4 in the
following fashion:
</bodyText>
<equation confidence="0.974253333333333">
p(a b, c, d) = p(a b)p(a
p(a I b) p(a I b, c)
(5)
</equation>
<bodyText confidence="0.999932318181818">
Note that the first term of the equation gives a
probability based upon little conditioning infor-
mation and that each subsequent term is a num-
ber from zero to positive infinity that is greater
or smaller than one if the new information be-
ing considered makes the probability greater or
smaller than the previous estimate.
As it stands, this last equation is pretty much
content-free. But let us look at how it works for
a particular case in our parsing scheme. Con-
sider the probability distribution for choosing
the pre-terminal for the head of a constituent.
In Equation 1 we wrote this as p(t I 1, H). As
we discuss in more detail in Section 5, several
different features in the context surrounding c
are useful to include in H: the label, head
pre-terminal and head of the parent of c (de-
noted as lp, tp, hp), the label of c&apos;s left sibling
(lb for &amp;quot;before&amp;quot;), and the label of the grand-
parent of c (la). That is, we wish to compute
p(t 1, lp, tp, lb, lg, hp). We can now rewrite this
in the form of Equation 5 as follows:
</bodyText>
<equation confidence="0.995979">
p(t I 1, lp,tp, 16,1g, hp) =
p(t I 1)p(t I 1,1p) p(t I 1, lp, tp) p(t l
p(t I 1, lp, tp, lb) p(t 1, lp,tp, lb, lg) (6)
</equation>
<bodyText confidence="0.999946833333333">
Here we have sequentially conditioned on
steadily increasing portions of c&apos;s history. In
many cases this is clearly warranted. For ex-
ample, it does not seem to make much sense
to condition on, say, hp without first condition-
ing on ti,. In other cases, however, we seem
</bodyText>
<equation confidence="0.635464">
p(t II) p(t 1,1p) p(t I 1, lp, tp)
p(t I 1, lp, tp, lb,1g) p(t I 1, lp, tp, lb,19, hp)
</equation>
<page confidence="0.972809">
134
</page>
<bodyText confidence="0.999914">
to be conditioning on apples and oranges, so
to speak. For example, one can well imagine
that one might want to condition on the par-
ent&apos;s lexical head without conditioning on the
left sibling, or the grandparent label. One way
to do this is to modify the simple version shown
in Equation 6 to allow this:
</bodyText>
<equation confidence="0.9950485">
(7)
p(t I 1,1p, tp) p(t I 1,1p, tp) •
</equation>
<bodyText confidence="0.999683606557377">
Note the changes to the last three terms in
Equation 7. Rather than conditioning each
term on the previous ones, they are now condi-
tioned only on those aspects of the history that
seem most relevant. The hope is that by doing
this we will have less difficulty with the splitting
of conditioning events, and thus somewhat less
difficulty with sparse data.
We make one more point on the connec-
tion of Equation 7 to a maximum entropy for-
mulation. Suppose we were, in fact, going
to compute a true maximum entropy model
based upon the features used in Equation 7,
Ii (t,1), f2(t,1,1p), f3(t,1,lp) . ... This requires
finding the appropriate Ais for Equation 3,
which is accomplished using an algorithm such
as iterative scaling [II] in which values for the Ai
are initially &amp;quot;guessed&amp;quot; and then modified until
they converge on stable values. With no prior
knowledge of values for the Ai one traditionally
starts with Ai = 0, this being a neutral assump-
tion that the feature has neither a positive nor
negative impact on the probability in question.
With some prior knowledge, non-zero values can
greatly speed up this process because fewer it-
erations are required for convergence. We com-
ment on this because in our example we can sub-
stantially speed up the process by choosing val-
ues picked so that, when the maximum-entropy
equation is expressed in the form of Equation
4, the gi have as their initial values the values
of the corresponding terms in Equation 7. (Our
experience is that rather than requiring 50 or so
iterations, three suffice.) Now we observe that
if we were to use a maximum-entropy approach
but run iterative scaling zero times, we would,
in fact, just have Equation 7.
The major advantage of using Equation 7 is
that one can generally get away without com-
puting the partition function Z(H). In the sim-
ple (content-free) form (Equation 6), it is clear
that Z(H) = 1. In the more interesting version,
Equation 7, this is not true in general, but one
would not expect it to differ much from one,
and we assume that as long as we are not pub-
lishing the raw probabilities (as we would be
doing, for example, in publishing perplexity re-
sults) the difference from one should be unim-
portant. As partition-function calculation is
typically the major on-line computational prob-
lem for maximum-entropy models, this simpli-
fies the model significantly.
Naturally, the distributions required by
Equation 7 cannot be used without smooth-
ing. In a pure maximum-entropy model this is
done by feature selection, as in Ratnaparkhi&apos;s
maximum-entropy parser [17]. While we could
have smoothed in the same fashion, we choose
instead to use standard deleted interpolation.
(Actually, we use a minor variant described in
[4].)
</bodyText>
<sectionHeader confidence="0.996757" genericHeader="method">
4 The Experiment
</sectionHeader>
<bodyText confidence="0.99995128">
We created a parser based upon the maximum-
entropy-inspired model of the last section,
smoothed using standard deleted interpolation.
As the generative model is top-down and we
use a standard bottom-up best-first probabilis-
tic chart parser [2,7], we use the chart parser as
a first pass to generate candidate possible parses
to be evaluated in the second pass by our prob-
abilistic model. For runs with the generative
model based upon Markov grammar statistics,
the first pass uses the same statistics, but con-
ditioned only on standard PCFG information.
This allows the second pass to see expansions
not present in the training corpus.
We use the gathered statistics for all observed
words, even those with very low counts, though
obviously our deleted interpolation smoothing
gives less emphasis to observed probabilities for
rare words. We guess the preterminals of words
that are not observed in the training data using
statistics on capitalization, hyphenation, word
endings (the last two letters), and the probabil-
ity that a given pre-terminal is realized using a
previously unobserved word.
As noted above, the probability model uses
</bodyText>
<equation confidence="0.62825425">
p(t I 1, lp,tp, b, Ig, hp) =
p(t i\p(t I 1, p(t I 1, lp,tp) p(t I 1, i,,, tp, lb)
I 1 p(t 11) p(t I 1,lp) p(t I 1,1,t)
p(t I 1, lp, tp,19) p(t
</equation>
<page confidence="0.789242">
135
</page>
<table confidence="0.9813218">
Parser LR LP CB OCB 2CB
&lt;40 words (2245 sentences)
Char97 87.5 87.4 1.00 62.1 86.1
Co1199 88.5 88.7 0.92 66.7 87.1
Char00 90.1 90.1 0.74 70.1 89.6
&lt; 100 words (2416 sentences)
Char97 86.7 86.6 1.20 59.9 83.2
Co1199 88.1 88.3 1.06 64.0 85.1
Ratna99 86.3 87.5
Char00 89.6 89.5 0.88 67.6 87.7
</table>
<figureCaption confidence="0.958753">
Figure 1: Parsing results compared with previ-
ous work
</figureCaption>
<bodyText confidence="0.999970928571429">
five smoothed probability distributions, one
each for Li, M,Ri,t, and h. The equation for
the (unsmoothed) conditional probability distri-
bution for t is given in Equation 7. The other
four equations can be found in a longer version
of this paper available on the author&apos;s website
(www.cs.brown.eduhiec). L and R are condi-
tioned on three previous labels so we are using
a third-order Markov grammar. Also, the label
of the parent constituent lp is conditioned upon
even when it is not obviously related to the fur-
ther conditioning events. This is due to the im-
portance of this factor in parsing, as noted in,
e.g., [14].
In keeping with the standard methodology [5,
9,10,15,17], we used the Penn Wall Street Jour-
nal tree-bank [16] with sections 2-21 for train-
ing, section 23 for testing, and section 24 for
development (debugging and tuning).
Performance on the test corpus is measured
using the standard measures from [5,9,10,17].
In particular, we measure labeled precision
(LP) and recall (LR), average number of cross-
brackets per sentence (CB), percentage of sen-
tences with zero cross brackets (OCB), and per-
centage of sentences with &lt; 2 cross brackets
(2CB). Again as standard, we take separate
measurements for all sentences of length &lt; 40
and all sentences of length &lt; 100. Note that
the definitions of labeled precision and recall are
those given in [9] and used in all of the previous
work. As noted in [5], these definitions typically
give results about 0.4% higher than the more
obvious ones. The results for the new parser
as well as for the previous top-three individual
parsers on this corpus are given in Figure 1.
As is typical, all of the standard measures tell
pretty much the same story, with the new parser
outperforming the other three parsers. Looking
in particular at the precision and recall figures,
the new parser&apos;s give us a 13% error reduction
over the best of the previous work, Co1199 [9].
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="method">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999983837209302">
In the previous sections we have concentrated
on the relation of the parser to a maximum-
entropy approach, the aspect of the parser that
is most novel. However, we do not think this
aspect is the sole or even the most important
reason for its comparative success. Here we list
what we believe to be the most significant con-
tributions and give some experimental results
on how well the program behaves without them.
We take as our starting point the parser
labled Char97 in Figure 1 [5], as that is the
program from which our current parser derives.
That parser, as stated in Figure 1, achieves an
average precision/recall of 87.5. As noted in [5],
that system is based upon a &amp;quot;tree-bank gram-
mar&amp;quot; - a grammar read directly off the train-
ing corpus. This is as opposed to the &amp;quot;Markov-
grammar&amp;quot; approach used in the current parser.
Also, the earlier parser uses two techniques not
employed in the current parser. First, it uses
a clustering scheme on words to give the sys-
tem a &amp;quot;soft&amp;quot; clustering of heads and sub-heads.
(It is &amp;quot;soft&amp;quot; clustering in that a word can be-
long to more than one cluster with different
weights - the weights express the probability
of producing the word given that one is going
to produce a word from that cluster.) Second,
Char97 uses unsupervised learning in that the
original system was run on about thirty million
words of unparsed text, the output was taken
as &amp;quot;correct&amp;quot;, and statistics were collected on
the resulting parses. Without these enhance-
ments Char97 performs at the 86.6% level for
sentences of length &lt; 40.
In this section we evaluate the effects of the
various changes we have made by running var-
ious versions of our current program. To avoid
repeated evaluations based upon the testing cor-
pus, here our evaluation is based upon sen-
tences of length &lt; 40 from the development cor-
pus. We note here that this corpus is somewhat
more difficult than the &amp;quot;official&amp;quot; test corpus.
For example, the final version of our system
</bodyText>
<page confidence="0.996874">
136
</page>
<table confidence="0.99946">
System Precision Recall
Old 86.3 86.1
Explicit Pre-Term 88.0 88.1
Marked Coordination 88.6 88.7
Standard Interpolation 88.2 88.3
MaxEnt-Inspired 89.0 89.2
First-order Markov 88.6 87.4
Second-order Markov 89.5 89.3
Best 89.8 89.6
</table>
<figureCaption confidence="0.9736145">
Figure 2: Labeled precision/recall for length &lt;
40, development corpus
</figureCaption>
<bodyText confidence="0.999924772727273">
achieves an average precision/recall of 90.1% on
the test corpus but an average precision/recall
of only 89.7% on the development corpus. This
is indicated in Figure 2, where the model la-
beled &amp;quot;Best&amp;quot; has precision of 89.8% and recall of
89.6% for an average of 89.7%, 0.4% lower than
the results on the official test corpus. This is in
accord with our experience that development-
corpus results are from 0.3% to 0.5% lower than
those obtained on the test corpus.
The model labeled &amp;quot;Old&amp;quot; attempts to recreate
the Char97 system using the current program.
It makes no use of special maximum-entropy-
inspired features (though their presence made
it much easier to perform these experiments), it
does not guess the pre-terminal before guess-
ing the lexical head, and it uses a tree-bank
grammar rather than a Markov grammar. This
parser achieves an average precision/recall of
86.2%. This is consistent with the average pre-
cision/recall of 86.6% for [5] mentioned above,
as the latter was on the test corpus and the for-
mer on the development corpus.
Between the Old model and the Best model,
Figure 2 gives precision/recall measurements for
several different versions of our parser. One of
the first and without doubt the most signifi-
cant change we made in the current parser is to
move from two stages of probabilistic decisions
at each node to three. As already noted, Char97
first guesses the lexical head of a constituent
and then, given the head, guesses the PCFG
rule used to expand the constituent in question.
In contrast, the current parser first guesses the
head&apos;s pre-terminal, then the head, and then the
expansion. It turns out that usefulness of this
process had already been discovered by Collins
[10], who in turn notes (personal communica-
tion) that it was previously used by Eisner [12].
However, Collins in [10] does not stress the de-
cision to guess the head&apos;s pre-terminal first, and
it might be lost on the casual reader. Indeed,
it was lost on the present author until he went
back after the fact and found it there. In Figure
2 we show that this one factor improves perfor-
mance by nearly 2%.
It may not be obvious why this should make
so great a difference, since most words are ef-
fectively unambiguous. (For example, part-of-
speech tagging using the most probable pre-
terminal for each word is 90% accurate [8].) We
believe that two factors contribute to this per-
formance gain. The first is simply that if we first
guess the pre-terminal, when we go to guess the
head the first thing we can condition upon is
the pre-terminal, i.e., we compute p(h I t). This
quantity is a relatively intuitive one (as, for ex-
ample, it is the quantity used in a PCFG to re-
late words to their pre-terminals) and it seems
particularly good to condition upon here since
we use it, in effect, as the unsmoothed probabil-
ity upon which all smoothing of p(h) is based.
This one &amp;quot;fix&amp;quot; makes slightly over a percent dif-
ference in the results.
The second major reason why first guessing
the pre-terminal makes so much difference is
that it can be used when backing off the lexical
head in computing the probability of the rule
expansion. For example, when we first guess
the lexical head we can move from computing
p(r I 1,1p, h) to p(r I /, t, /p, h). So, e.g., even
if the word &amp;quot;conflating&amp;quot; does not appear in the
training corpus (and it does not), the &amp;quot;ng&amp;quot; end-
ing allows our program to guess with relative
security that the word has the vbg pre-terminal,
and thus the probability of various rule expan-
sions can be considerable sharpened. For exam-
ple, the tree-bank PCFG probability of the rule
&amp;quot;VP --+ vbg np&amp;quot; is 0.0145, whereas once we con-
dition on the fact that the lexical head is a vbg
we get a probability of 0.214.
The second modification is the explicit mark-
ing of noun and verb-phrase coordination. We
have already noted the importance of condition-
ing on the parent label /p. So, for example,
information about an np is conditioned on the
parent — e.g., an s, vp, pp, etc. Note that when
an np is part of an np coordinate structure the
</bodyText>
<page confidence="0.981767">
137
</page>
<figure confidence="0.975923333333333">
VP
aux vp
vbd np
</figure>
<figureCaption confidence="0.981731">
Figure 3: Verb phrase with both main and aux-
iliary verbs
</figureCaption>
<bodyText confidence="0.99993874">
parent will itself be an np, and similarly for a
vp. But nps and vps can occur with np and
vp parents in non-coordinate structures as well.
For example, in the Penn Treebank a vp with
both main and auxiliary verbs has the structure
shown in Figure 3. Note that the subordinate
vp has a vp parent.
Thus np and vp parents of constituents are
marked to indicate if the parents are a coor-
dinate structure. A vp coordinate structure
is defined here as a constituent with two or
more vp children, one or more of the con-
stituents comma, cc, conjp (conjunctive phrase),
and nothing else; coordinate np phrases are de-
fined similarly. Something very much like this is
done in [15]. As shown in Figure 2, condition-
ing on this information gives a 0.6% improve-
ment. We believe that this is mostly due to
improvements in guessing the sub-constituent&apos;s
pre-terminal and head. Given we are already
at the 88% level of accuracy, we judge a 0.6%
improvement to be very much worth while.
Next we add the less obvious conditioning
events noted in our previous discussion of the
final model — grandparent label lg and left
sibling label /b. When we do so using our
maximum-entropy-inspired conditioning, we get
another 0.45% improvement in average preci-
sion/recall, as indicated in Figure 2 on the line
labeled &amp;quot;MaxEnt-Inspired&apos;. Note that we also
tried including this information using a stan-
dard deleted-interpolation model. The results
here are shown in the line &amp;quot;Standard Interpola-
tion&amp;quot;. Including this information within a stan-
dard deleted-interpolation model causes a 0.6%
decrease from the results using the less conven-
tional model. Indeed, the resulting performance
is worse than not using this information at all.
Up to this point all the models considered
in this section are tree-bank grammar models.
That is, the PCFG grammar rules are read di-
rectly off the training corpus. As already noted,
our best model uses a Markov-grammar ap-
proach. As one can see in Figure 2, a first-
order Markov grammar (with all the aforemen-
tioned improvements) performs slightly worse
than the equivalent tree-bank-grammar parser.
However, a second-order grammar does slightly
better and a third-order grammar does signifi-
cantly better than the tree-bank parser.
</bodyText>
<sectionHeader confidence="0.999342" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999995536585366">
We have presented a lexicalized Markov gram-
mar parsing model that achieves (using the now
standard training/testing/development sections
of the Penn treebank) an average preci-
sion/recall of 91.1% on sentences of length &lt;
40 and 89.5% on sentences of length &lt; 100.
This corresponds to an error reduction of 13%
over the best previously published single parser
results on this test set, those of Collins [9].
That the previous three best parsers on this
test [5,9,17] all perform within a percentage
point of each other, despite quite different ba-
sic mechanisms, led some researchers to won-
der if there might be some maximum level of
parsing performance that could be obtained us-
ing the treebank for training, and to conjec-
ture that perhaps we were at it. The results
reported here disprove this conjecture. The re-
sults of [13] achieved by combining the afore-
mentioned three-best parsers also suggest that
the limit on tree-bank trained parsers is much
higher than previously thought. Indeed, it may
be that adding this new parser to the mix may
yield still higher results.
From our perspective, perhaps the two most
important numbers to come out of this re-
search are the overall error reduction of 13%
over the results in [9] and the intermediate-
result improvement of nearly 2% on labeled pre-
cision/recall due to the simple idea of guess-
ing the head&apos;s pre-terminal before guessing the
head. Neither of these results were anticipated
at the start of this research.
As noted above, the main methodological
innovation presented here is our &amp;quot;maximum-
entropy-inspired&amp;quot; model for conditioning and
smoothing. Two aspects of this model deserve
some comment. The first is the slight, but im-
portant, improvement achieved by using this
model over conventional deleted interpolation,
as indicated in Figure 2. We expect that as
</bodyText>
<page confidence="0.994624">
138
</page>
<bodyText confidence="0.999987368421053">
we experiment with other, more semantic con-
ditioning information, the importance of this as-
pect of the model will increase.
More important in our eyes, though, is
the flexibility of the maximum-entropy-inspired
model. Though in some respects not quite as
flexible as true maximum entropy, it is much
simpler and, in our estimation, has benefits
when it comes to smoothing. Ultimately it is
this flexibility that let us try the various condi-
tioning events, to move on to a Markov gram-
mar approach, and to try several Markov gram-
mars of different orders, without significant pro-
gramming. Indeed, we initiated this line of work
in an attempt to create a parser that would be
flexible enough to allow modifications for pars-
ing down to more semantic levels of detail. It is
to this project that our future parsing work will
be devoted.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99967677027027">
1. BERGER, A. L., PIETRA, S. A. D. AND
PIETRA, V. J. D. A maximum entropy ap-
proach to natural language processing. Com-
putational Linguistics 22 1 (1996), 39-71.
2. CARABALLO, S. AND CHARNIAK, E. New
figures of merit for best-first probabilistic
chart parsing. Computational Linguistics 24
(1998), 275-298.
3. CHARNIAK, E. Tree-bank grammars. In
Proceedings of the Thirteenth National
Conference on Artificial Intelligence. AAAI
Press/MIT Press, Menlo Park, 1996, 1031-
1036.
4. CHARNIAK, E. Expected-frequency interpo-
lation. Department of Computer Science,
Brown University, Technical Report CS96-37,
1996.
5. CHARNIAK, E. Statistical parsing with a
context-free grammar and word statistics.
In Proceedings of the Fourteenth National
Conference on Artificial Intelligence. AAAI
Press/MIT Press, Menlo Park, CA, 1997,
598-603.
6. CHARNIAK, E. Statistical techniques for
natural language parsing. Al Magazine 18 4
(1997), 33-43.
7. CHARNIAK, E., GOLDWATER, S. AND JOHN-
SON, M. Edge-based best-first chart pars-
ing. In Proceedings of the Sixth Workshop
on Very Large Corpora. 1998, 127-133.
8. CHARNIAK, E., HENDRICKSON, C., JACOB-
SON, N. AND PERKOWITZ, M. Equations
for part-of-speech tagging. In Proceedings of
the Eleventh National Conference on Arti-
ficial Intelligence. AAAI Press/MIT Press,
Menlo Park, 1993, 784-789.
9. COLLINS, M. Head-Driven Statistical Mod-
els for Natural Language Parsing. University
of Pennsylvania, Ph.D. Disseration, 1999.
10. COLLINS, M. J. Three generative lexicalised
models for statistical parsing. In Proceedings
of the 35th Annual Meeting of the ACL. 1997,
16-23.
11. DARROCH, J. N. AND RATCLIPF, D. Gener-
alized iterative scaling for log-linear models.
Annals of Mathematical Statistics 33 (1972),
1470-1480.
12. EISNER, J. M. An empirical comparison of
probability models for dependency grammar.
Institute for Research in Cognitive Science,
University of Pennsylvania, Technical Report
IRCS-96-11, 1996.
13. HENDERSON, J. C. AND BRILL, E. Exploit-
ing diversity in natural language process-
ing: combining parsers. In 1999 Joint Sigdat
Conference on Empirical Methods in Natu-
ral Language Processing and Very Large Cor-
pora. ACL, New Brunswick NJ, 1999, 187-
194.
14. JOHNSON, M. PCFG models of linguistic
tree representations. Computational Linguis-
tics 24 4(1998), 613-632.
15. MAGERMAN, D. M. Statistical decision-tree
models for parsing. In Proceedings of the 33rd
Annual Meeting of the Association for Com-
putational Linguistics. 1995, 276-283.
16. MARCUS, M. P., SANTORINI, B. AND
MARCINKIEWICZ, M. A. Building a large
annotated corpus of English: the Penn tree-
bank. Computational Linguistics 19 (1993),
313-330.
17. RATNAPARKHI, A. Learning to parse natu-
ral language with maximum entropy models.
Machine Learning 341/2/3 (1999), 151-176.
</reference>
<page confidence="0.998859">
139
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.506949">
<title confidence="0.999339">A Maximum-Entropy-Inspired Parser *</title>
<author confidence="0.997443">Eugene Charniak</author>
<affiliation confidence="0.838781666666667">Brown Laboratory for Linguistic Information Processing Department of Computer Science Brown University, Box 1910, Providence, RI 02912</affiliation>
<email confidence="0.999824">ecOcs.brown.edu</email>
<abstract confidence="0.9976608">We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of 40 and less, and for of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a &amp;quot;maximum-entropy-inspired&amp;quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&apos;s pre-terminal before guessing the lexical head.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A L BERGER</author>
<author>S A D PIETRA</author>
<author>V J D PIETRA</author>
</authors>
<title>A maximum entropy approach to natural language processing.</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<volume>22</volume>
<pages>39--71</pages>
<contexts>
<context position="6775" citStr="[1,17]" startWordPosition="1171" endWordPosition="1171">th the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning. For example, in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1, M,1, t, h, H). Also, remember that H is a pla,ceholder for any other information beyond the constituent c that may be useful in assigning c a probability. In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17]. A complete review of log-linear models is beyond the scope of this paper. Rather, we concentrate on the aspects of these models that most directly influenced the model presented here. To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input. In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. In the parser w</context>
</contexts>
<marker>1.</marker>
<rawString>BERGER, A. L., PIETRA, S. A. D. AND PIETRA, V. J. D. A maximum entropy approach to natural language processing. Computational Linguistics 22 1 (1996), 39-71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S CARABALLO</author>
<author>E CHARNIAK</author>
</authors>
<title>New figures of merit for best-first probabilistic chart parsing.</title>
<date>1998</date>
<journal>Computational Linguistics</journal>
<volume>24</volume>
<pages>275--298</pages>
<contexts>
<context position="15236" citStr="[2,7]" startWordPosition="2698" endWordPosition="2698">tributions required by Equation 7 cannot be used without smoothing. In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi&apos;s maximum-entropy parser [17]. While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation. (Actually, we use a minor variant described in [4].) 4 The Experiment We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information. This allows the second pass to see expansions not present in the training corpus. We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words. We gues</context>
</contexts>
<marker>2.</marker>
<rawString>CARABALLO, S. AND CHARNIAK, E. New figures of merit for best-first probabilistic chart parsing. Computational Linguistics 24 (1998), 275-298.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E CHARNIAK</author>
</authors>
<title>Tree-bank grammars.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth National Conference on Artificial Intelligence. AAAI</booktitle>
<pages>1031</pages>
<publisher>Press/MIT Press,</publisher>
<location>Menlo Park,</location>
<contexts>
<context position="3453" citStr="[3]" startWordPosition="587" endWordPosition="587">hat our probability model deems important in determining the probability in question. Much of the interesting work is determining what goes into H (c). Whenever it is clear to which constituent we are referring we omit the (c) in, e.g., h(c). In this notation the above equation takes the following form: p(r) = J p(t I 1, H).p(h It, 1, H).p(e I 1, t , Ii, H). cE/1&amp;quot; (1) 132 Next we describe how we assign a probability to the expansion e of a constituent. In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus. The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15]. The method we use follows that of [10]. In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols. (We assume that all terminal symbols are generated by rules of the </context>
</contexts>
<marker>3.</marker>
<rawString>CHARNIAK, E. Tree-bank grammars. In Proceedings of the Thirteenth National Conference on Artificial Intelligence. AAAI Press/MIT Press, Menlo Park, 1996, 1031-</rawString>
</citation>
<citation valid="false">
<authors>
<author>E CHARNIAK</author>
</authors>
<title>Expected-frequency interpolation.</title>
<tech>Technical Report CS96-37,</tech>
<institution>Department of Computer Science, Brown University,</institution>
<contexts>
<context position="14972" citStr="[4]" startWordPosition="2658" endWordPosition="2658">ample, in publishing perplexity results) the difference from one should be unimportant. As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly. Naturally, the distributions required by Equation 7 cannot be used without smoothing. In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi&apos;s maximum-entropy parser [17]. While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation. (Actually, we use a minor variant described in [4].) 4 The Experiment We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information. This allows the second pass </context>
</contexts>
<marker>4.</marker>
<rawString>CHARNIAK, E. Expected-frequency interpolation. Department of Computer Science, Brown University, Technical Report CS96-37,</rawString>
</citation>
<citation valid="true">
<authors>
<author>E CHARNIAK</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence. AAAI</booktitle>
<pages>598--603</pages>
<publisher>Press/MIT Press,</publisher>
<location>Menlo Park, CA,</location>
<contexts>
<context position="1334" citStr="[5,9,10,15,17]" startWordPosition="202" endWordPosition="202">odel for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&apos;s pre-terminal before guessing the lexical head. 1 Introduction We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. Following [5,10], our parser is based upon a probabilistic generative model. That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368. The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is</context>
<context position="17297" citStr="[5, 9,10,15,17]" startWordPosition="3056" endWordPosition="3057">i,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7. The other four equations can be found in a longer version of this paper available on the author&apos;s website (www.cs.brown.eduhiec). L and R are conditioned on three previous labels so we are using a third-order Markov grammar. Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events. This is due to the importance of this factor in parsing, as noted in, e.g., [14]. In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning). Performance on the test corpus is measured using the standard measures from [5,9,10,17]. In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with &lt; 2 cross brackets (2CB). Again as standard, we take separate measurements for all sentences of length &lt; 40 and all sentences of length &lt; 1</context>
<context position="19037" citStr="[5]" startWordPosition="3358" endWordPosition="3358">l figures, the new parser&apos;s give us a 13% error reduction over the best of the previous work, Co1199 [9]. 5 Discussion In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel. However, we do not think this aspect is the sole or even the most important reason for its comparative success. Here we list what we believe to be the most significant contributions and give some experimental results on how well the program behaves without them. We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives. That parser, as stated in Figure 1, achieves an average precision/recall of 87.5. As noted in [5], that system is based upon a &amp;quot;tree-bank grammar&amp;quot; - a grammar read directly off the training corpus. This is as opposed to the &amp;quot;Markovgrammar&amp;quot; approach used in the current parser. Also, the earlier parser uses two techniques not employed in the current parser. First, it uses a clustering scheme on words to give the system a &amp;quot;soft&amp;quot; clustering of heads and sub-heads. (It is &amp;quot;soft&amp;quot; clustering in that a word can belong to more than one clu</context>
<context position="21756" citStr="[5]" startWordPosition="3816" endWordPosition="3816"> with our experience that developmentcorpus results are from 0.3% to 0.5% lower than those obtained on the test corpus. The model labeled &amp;quot;Old&amp;quot; attempts to recreate the Char97 system using the current program. It makes no use of special maximum-entropyinspired features (though their presence made it much easier to perform these experiments), it does not guess the pre-terminal before guessing the lexical head, and it uses a tree-bank grammar rather than a Markov grammar. This parser achieves an average precision/recall of 86.2%. This is consistent with the average precision/recall of 86.6% for [5] mentioned above, as the latter was on the test corpus and the former on the development corpus. Between the Old model and the Best model, Figure 2 gives precision/recall measurements for several different versions of our parser. One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three. As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question. In contrast, the current parser fir</context>
<context position="27632" citStr="[5,9,17]" startWordPosition="4842" endWordPosition="4842">wever, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser. 6 Conclusion We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100. This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9]. That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it. The results reported here disprove this conjecture. The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. Indeed, it may be that adding this new parser to the mix may yield still higher re</context>
</contexts>
<marker>5.</marker>
<rawString>CHARNIAK, E. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the Fourteenth National Conference on Artificial Intelligence. AAAI Press/MIT Press, Menlo Park, CA, 1997, 598-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E CHARNIAK</author>
</authors>
<title>Statistical techniques for natural language parsing.</title>
<date>1997</date>
<journal>Al Magazine</journal>
<volume>18</volume>
<pages>33--43</pages>
<contexts>
<context position="3676" citStr="[6,10,15]" startWordPosition="622" endWordPosition="622">t the (c) in, e.g., h(c). In this notation the above equation takes the following form: p(r) = J p(t I 1, H).p(h It, 1, H).p(e I 1, t , Ii, H). cE/1&amp;quot; (1) 132 Next we describe how we assign a probability to the expansion e of a constituent. In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus. The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15]. The method we use follows that of [10]. In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols. (We assume that all terminal symbols are generated by rules of the form &amp;quot;preterm word&amp;quot; and we treat these as a special case.) For us the non-terminal symbols are those of the tree-bank, augmented by the symbols aux and auxg, which have been assigned deterministically to certain auxiliary v</context>
</contexts>
<marker>6.</marker>
<rawString>CHARNIAK, E. Statistical techniques for natural language parsing. Al Magazine 18 4 (1997), 33-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E CHARNIAK</author>
<author>S GOLDWATER</author>
<author>M JOHNSON</author>
</authors>
<title>Edge-based best-first chart parsing.</title>
<date>1998</date>
<booktitle>In Proceedings of the Sixth Workshop on Very Large Corpora.</booktitle>
<pages>127--133</pages>
<contexts>
<context position="15236" citStr="[2,7]" startWordPosition="2698" endWordPosition="2698">tributions required by Equation 7 cannot be used without smoothing. In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi&apos;s maximum-entropy parser [17]. While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation. (Actually, we use a minor variant described in [4].) 4 The Experiment We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. For runs with the generative model based upon Markov grammar statistics, the first pass uses the same statistics, but conditioned only on standard PCFG information. This allows the second pass to see expansions not present in the training corpus. We use the gathered statistics for all observed words, even those with very low counts, though obviously our deleted interpolation smoothing gives less emphasis to observed probabilities for rare words. We gues</context>
</contexts>
<marker>7.</marker>
<rawString>CHARNIAK, E., GOLDWATER, S. AND JOHNSON, M. Edge-based best-first chart parsing. In Proceedings of the Sixth Workshop on Very Large Corpora. 1998, 127-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E CHARNIAK</author>
<author>C HENDRICKSON</author>
<author>N JACOBSON</author>
<author>M PERKOWITZ</author>
</authors>
<title>Equations for part-of-speech tagging.</title>
<date>1993</date>
<booktitle>In Proceedings of the Eleventh National Conference on Artificial Intelligence. AAAI</booktitle>
<pages>784--789</pages>
<publisher>Press/MIT Press,</publisher>
<location>Menlo Park,</location>
<contexts>
<context position="23134" citStr="[8]" startWordPosition="4058" endWordPosition="4058">n notes (personal communication) that it was previously used by Eisner [12]. However, Collins in [10] does not stress the decision to guess the head&apos;s pre-terminal first, and it might be lost on the casual reader. Indeed, it was lost on the present author until he went back after the fact and found it there. In Figure 2 we show that this one factor improves performance by nearly 2%. It may not be obvious why this should make so great a difference, since most words are effectively unambiguous. (For example, part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].) We believe that two factors contribute to this performance gain. The first is simply that if we first guess the pre-terminal, when we go to guess the head the first thing we can condition upon is the pre-terminal, i.e., we compute p(h I t). This quantity is a relatively intuitive one (as, for example, it is the quantity used in a PCFG to relate words to their pre-terminals) and it seems particularly good to condition upon here since we use it, in effect, as the unsmoothed probability upon which all smoothing of p(h) is based. This one &amp;quot;fix&amp;quot; makes slightly over a percent difference in the re</context>
</contexts>
<marker>8.</marker>
<rawString>CHARNIAK, E., HENDRICKSON, C., JACOBSON, N. AND PERKOWITZ, M. Equations for part-of-speech tagging. In Proceedings of the Eleventh National Conference on Artificial Intelligence. AAAI Press/MIT Press, Menlo Park, 1993, 784-789.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M COLLINS</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<institution>University of Pennsylvania, Ph.D. Disseration,</institution>
<contexts>
<context position="643" citStr="[9]" startWordPosition="96" endWordPosition="96">ne Charniak Brown Laboratory for Linguistic Information Processing Department of Computer Science Brown University, Box 1910, Providence, RI 02912 ecOcs.brown.edu Abstract We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a &amp;quot;maximum-entropy-inspired&amp;quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&apos;s pre-terminal before guessing the lexical head. 1 Introduction We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sent</context>
<context position="17297" citStr="[5, 9,10,15,17]" startWordPosition="3056" endWordPosition="3057">i,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7. The other four equations can be found in a longer version of this paper available on the author&apos;s website (www.cs.brown.eduhiec). L and R are conditioned on three previous labels so we are using a third-order Markov grammar. Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events. This is due to the importance of this factor in parsing, as noted in, e.g., [14]. In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning). Performance on the test corpus is measured using the standard measures from [5,9,10,17]. In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with &lt; 2 cross brackets (2CB). Again as standard, we take separate measurements for all sentences of length &lt; 40 and all sentences of length &lt; 1</context>
<context position="18538" citStr="[9]" startWordPosition="3269" endWordPosition="3269">ed precision and recall are those given in [9] and used in all of the previous work. As noted in [5], these definitions typically give results about 0.4% higher than the more obvious ones. The results for the new parser as well as for the previous top-three individual parsers on this corpus are given in Figure 1. As is typical, all of the standard measures tell pretty much the same story, with the new parser outperforming the other three parsers. Looking in particular at the precision and recall figures, the new parser&apos;s give us a 13% error reduction over the best of the previous work, Co1199 [9]. 5 Discussion In the previous sections we have concentrated on the relation of the parser to a maximumentropy approach, the aspect of the parser that is most novel. However, we do not think this aspect is the sole or even the most important reason for its comparative success. Here we list what we believe to be the most significant contributions and give some experimental results on how well the program behaves without them. We take as our starting point the parser labled Char97 in Figure 1 [5], as that is the program from which our current parser derives. That parser, as stated in Figure 1, a</context>
<context position="27572" citStr="[9]" startWordPosition="4832" endWordPosition="4832"> worse than the equivalent tree-bank-grammar parser. However, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser. 6 Conclusion We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100. This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9]. That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it. The results reported here disprove this conjecture. The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. Indeed, it may be that</context>
</contexts>
<marker>9.</marker>
<rawString>COLLINS, M. Head-Driven Statistical Models for Natural Language Parsing. University of Pennsylvania, Ph.D. Disseration, 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J COLLINS</author>
</authors>
<title>Three generative lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL.</booktitle>
<pages>16--23</pages>
<contexts>
<context position="1334" citStr="[5,9,10,15,17]" startWordPosition="202" endWordPosition="202">odel for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&apos;s pre-terminal before guessing the lexical head. 1 Introduction We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. Following [5,10], our parser is based upon a probabilistic generative model. That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368. The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is</context>
<context position="3676" citStr="[6,10,15]" startWordPosition="622" endWordPosition="622">t the (c) in, e.g., h(c). In this notation the above equation takes the following form: p(r) = J p(t I 1, H).p(h It, 1, H).p(e I 1, t , Ii, H). cE/1&amp;quot; (1) 132 Next we describe how we assign a probability to the expansion e of a constituent. In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus. The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15]. The method we use follows that of [10]. In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols. (We assume that all terminal symbols are generated by rules of the form &amp;quot;preterm word&amp;quot; and we treat these as a special case.) For us the non-terminal symbols are those of the tree-bank, augmented by the symbols aux and auxg, which have been assigned deterministically to certain auxiliary v</context>
<context position="17297" citStr="[5, 9,10,15,17]" startWordPosition="3056" endWordPosition="3057">i,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7. The other four equations can be found in a longer version of this paper available on the author&apos;s website (www.cs.brown.eduhiec). L and R are conditioned on three previous labels so we are using a third-order Markov grammar. Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events. This is due to the importance of this factor in parsing, as noted in, e.g., [14]. In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning). Performance on the test corpus is measured using the standard measures from [5,9,10,17]. In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with &lt; 2 cross brackets (2CB). Again as standard, we take separate measurements for all sentences of length &lt; 40 and all sentences of length &lt; 1</context>
<context position="22519" citStr="[10]" startWordPosition="3946" endWordPosition="3946">sion/recall measurements for several different versions of our parser. One of the first and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three. As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question. In contrast, the current parser first guesses the head&apos;s pre-terminal, then the head, and then the expansion. It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12]. However, Collins in [10] does not stress the decision to guess the head&apos;s pre-terminal first, and it might be lost on the casual reader. Indeed, it was lost on the present author until he went back after the fact and found it there. In Figure 2 we show that this one factor improves performance by nearly 2%. It may not be obvious why this should make so great a difference, since most words are effectively unambiguous. (For example, part-ofspeech tagging using the most probable preterminal for each word is 9</context>
</contexts>
<marker>10.</marker>
<rawString>COLLINS, M. J. Three generative lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the ACL. 1997, 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N DARROCH</author>
<author>D RATCLIPF</author>
</authors>
<title>Generalized iterative scaling for log-linear models.</title>
<date>1972</date>
<journal>Annals of Mathematical Statistics</journal>
<volume>33</volume>
<pages>1470--1480</pages>
<marker>11.</marker>
<rawString>DARROCH, J. N. AND RATCLIPF, D. Generalized iterative scaling for log-linear models. Annals of Mathematical Statistics 33 (1972), 1470-1480.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M EISNER</author>
</authors>
<title>An empirical comparison of probability models for dependency grammar.</title>
<date>1996</date>
<tech>Technical Report IRCS-96-11,</tech>
<institution>Institute for Research in Cognitive Science, University of Pennsylvania,</institution>
<contexts>
<context position="22606" citStr="[12]" startWordPosition="3961" endWordPosition="3961"> and without doubt the most significant change we made in the current parser is to move from two stages of probabilistic decisions at each node to three. As already noted, Char97 first guesses the lexical head of a constituent and then, given the head, guesses the PCFG rule used to expand the constituent in question. In contrast, the current parser first guesses the head&apos;s pre-terminal, then the head, and then the expansion. It turns out that usefulness of this process had already been discovered by Collins [10], who in turn notes (personal communication) that it was previously used by Eisner [12]. However, Collins in [10] does not stress the decision to guess the head&apos;s pre-terminal first, and it might be lost on the casual reader. Indeed, it was lost on the present author until he went back after the fact and found it there. In Figure 2 we show that this one factor improves performance by nearly 2%. It may not be obvious why this should make so great a difference, since most words are effectively unambiguous. (For example, part-ofspeech tagging using the most probable preterminal for each word is 90% accurate [8].) We believe that two factors contribute to this performance gain. The </context>
</contexts>
<marker>12.</marker>
<rawString>EISNER, J. M. An empirical comparison of probability models for dependency grammar. Institute for Research in Cognitive Science, University of Pennsylvania, Technical Report IRCS-96-11, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C HENDERSON</author>
<author>E BRILL</author>
</authors>
<title>Exploiting diversity in natural language processing: combining parsers.</title>
<date>1999</date>
<booktitle>In 1999 Joint Sigdat Conference on Empirical Methods in Natural Language Processing and Very Large Corpora. ACL,</booktitle>
<pages>187</pages>
<location>New Brunswick NJ,</location>
<contexts>
<context position="27992" citStr="[13]" startWordPosition="4905" endWordPosition="4905">40 and 89.5% on sentences of length &lt; 100. This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9]. That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it. The results reported here disprove this conjecture. The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. Indeed, it may be that adding this new parser to the mix may yield still higher results. From our perspective, perhaps the two most important numbers to come out of this research are the overall error reduction of 13% over the results in [9] and the intermediateresult improvement of nearly 2% on labeled precision/recall due to the simple idea of guessing the head&apos;s pre-terminal before guessing the head. Neither of these results were antic</context>
</contexts>
<marker>13.</marker>
<rawString>HENDERSON, J. C. AND BRILL, E. Exploiting diversity in natural language processing: combining parsers. In 1999 Joint Sigdat Conference on Empirical Methods in Natural Language Processing and Very Large Corpora. ACL, New Brunswick NJ, 1999, 187-</rawString>
</citation>
<citation valid="false">
<authors>
<author>M JOHNSON</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<journal>Computational Linguistics</journal>
<volume>4</volume>
<issue>1998</issue>
<pages>613--632</pages>
<contexts>
<context position="17239" citStr="[14]" startWordPosition="3049" endWordPosition="3049">probability distributions, one each for Li, M,Ri,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7. The other four equations can be found in a longer version of this paper available on the author&apos;s website (www.cs.brown.eduhiec). L and R are conditioned on three previous labels so we are using a third-order Markov grammar. Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events. This is due to the importance of this factor in parsing, as noted in, e.g., [14]. In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning). Performance on the test corpus is measured using the standard measures from [5,9,10,17]. In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with &lt; 2 cross brackets (2CB). Again as standard, we take separate measurements for al</context>
</contexts>
<marker>14.</marker>
<rawString>JOHNSON, M. PCFG models of linguistic tree representations. Computational Linguistics 24 4(1998), 613-632.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D M MAGERMAN</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>276--283</pages>
<contexts>
<context position="1334" citStr="[5,9,10,15,17]" startWordPosition="202" endWordPosition="202">odel for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&apos;s pre-terminal before guessing the lexical head. 1 Introduction We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. Following [5,10], our parser is based upon a probabilistic generative model. That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368. The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is</context>
<context position="3676" citStr="[6,10,15]" startWordPosition="622" endWordPosition="622">t the (c) in, e.g., h(c). In this notation the above equation takes the following form: p(r) = J p(t I 1, H).p(h It, 1, H).p(e I 1, t , Ii, H). cE/1&amp;quot; (1) 132 Next we describe how we assign a probability to the expansion e of a constituent. In Section 5 we present some results in which the possible expansions of a constituent are fixed in advanced by extracting a tree-bank grammar [3] from the training corpus. The method that gives the best results, however, uses a Markov grammar — a method for assigning probabilities to any possible expansion using statistics gathered from the training corpus [6,10,15]. The method we use follows that of [10]. In this scheme a traditional probabilistic context-free grammar (PCFG) rule can be thought of as consisting of a left-hand side with a label 1(c) drawn from the non-terminal symbols of our grammar, and a right-hand side that is a sequence of one or more such symbols. (We assume that all terminal symbols are generated by rules of the form &amp;quot;preterm word&amp;quot; and we treat these as a special case.) For us the non-terminal symbols are those of the tree-bank, augmented by the symbols aux and auxg, which have been assigned deterministically to certain auxiliary v</context>
<context position="17297" citStr="[5, 9,10,15,17]" startWordPosition="3056" endWordPosition="3057">i,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7. The other four equations can be found in a longer version of this paper available on the author&apos;s website (www.cs.brown.eduhiec). L and R are conditioned on three previous labels so we are using a third-order Markov grammar. Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events. This is due to the importance of this factor in parsing, as noted in, e.g., [14]. In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning). Performance on the test corpus is measured using the standard measures from [5,9,10,17]. In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with &lt; 2 cross brackets (2CB). Again as standard, we take separate measurements for all sentences of length &lt; 40 and all sentences of length &lt; 1</context>
<context position="25592" citStr="[15]" startWordPosition="4514" endWordPosition="4514">ps can occur with np and vp parents in non-coordinate structures as well. For example, in the Penn Treebank a vp with both main and auxiliary verbs has the structure shown in Figure 3. Note that the subordinate vp has a vp parent. Thus np and vp parents of constituents are marked to indicate if the parents are a coordinate structure. A vp coordinate structure is defined here as a constituent with two or more vp children, one or more of the constituents comma, cc, conjp (conjunctive phrase), and nothing else; coordinate np phrases are defined similarly. Something very much like this is done in [15]. As shown in Figure 2, conditioning on this information gives a 0.6% improvement. We believe that this is mostly due to improvements in guessing the sub-constituent&apos;s pre-terminal and head. Given we are already at the 88% level of accuracy, we judge a 0.6% improvement to be very much worth while. Next we add the less obvious conditioning events noted in our previous discussion of the final model — grandparent label lg and left sibling label /b. When we do so using our maximum-entropy-inspired conditioning, we get another 0.45% improvement in average precision/recall, as indicated in Figure 2 </context>
</contexts>
<marker>15.</marker>
<rawString>MAGERMAN, D. M. Statistical decision-tree models for parsing. In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. 1995, 276-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P MARCUS</author>
<author>B SANTORINI</author>
<author>M A MARCINKIEWICZ</author>
</authors>
<title>Building a large annotated corpus of English: the Penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics</journal>
<volume>19</volume>
<pages>313--330</pages>
<contexts>
<context position="1149" citStr="[16]" startWordPosition="173" endWordPosition="173">his represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a &amp;quot;maximum-entropy-inspired&amp;quot; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&apos;s pre-terminal before guessing the lexical head. 1 Introduction We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. Following [5,10], our parser is based upon a probabilistic generative model. That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This re</context>
<context position="17350" citStr="[16]" startWordPosition="3067" endWordPosition="3067">ility distribution for t is given in Equation 7. The other four equations can be found in a longer version of this paper available on the author&apos;s website (www.cs.brown.eduhiec). L and R are conditioned on three previous labels so we are using a third-order Markov grammar. Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events. This is due to the importance of this factor in parsing, as noted in, e.g., [14]. In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning). Performance on the test corpus is measured using the standard measures from [5,9,10,17]. In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with &lt; 2 cross brackets (2CB). Again as standard, we take separate measurements for all sentences of length &lt; 40 and all sentences of length &lt; 100. Note that the definitions of labeled precision an</context>
</contexts>
<marker>16.</marker>
<rawString>MARCUS, M. P., SANTORINI, B. AND MARCINKIEWICZ, M. A. Building a large annotated corpus of English: the Penn treebank. Computational Linguistics 19 (1993), 313-330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A RATNAPARKHI</author>
</authors>
<title>Learning to parse natural language with maximum entropy models.</title>
<date>1999</date>
<journal>Machine Learning</journal>
<volume>341</volume>
<pages>151--176</pages>
<contexts>
<context position="1334" citStr="[5,9,10,15,17]" startWordPosition="202" endWordPosition="202">odel for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&apos;s pre-terminal before guessing the lexical head. 1 Introduction We present a new parser for parsing down to Penn tree-bank style parse trees [16] that achieves 90.1% average precision/recall for sentences of length &lt; 40, and 89.5% for sentences of length &lt; 100, when trained and tested on the previously established [5,9,10,15,17] &amp;quot;standard&amp;quot; sections of the Wall Street Journal tree-bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. Following [5,10], our parser is based upon a probabilistic generative model. That is, for all sentences s and all parses 7r, the parser assigns a probability p(s , 7r) = p(r), the equality holding when we restrict consideration to 7r whose yield * This research was supported in part by NSF grant LIS SBR 9720368. The author would like to thank Mark Johnson and all the rest of the Brown Laboratory for Linguistic Information Processing. is</context>
<context position="6775" citStr="[1,17]" startWordPosition="1171" endWordPosition="1171">th the empirically obtained probabilities to take the sting out of the sparse data problems that are inevitable with even the most modest conditioning. For example, in a second-order Markov grammar we conditioned the L2 label according to the distribution p(L2 I L1, M,1, t, h, H). Also, remember that H is a pla,ceholder for any other information beyond the constituent c that may be useful in assigning c a probability. In the past few years the maximum entropy, or log-linear, approach has recommended itself to probabilistic model builders for its flexibility and its novel approach to smoothing [1,17]. A complete review of log-linear models is beyond the scope of this paper. Rather, we concentrate on the aspects of these models that most directly influenced the model presented here. To compute a probability in a log-linear model one first defines a set of &amp;quot;features&amp;quot;, functions from the space of configurations over which one is trying to compute probabilities to integers that denote the number of times some pattern occurs in the input. In our work we assume that any feature can occur at most once, so features are boolean-valued: 0 if the pattern does not occur, 1 if it does. In the parser w</context>
<context position="14813" citStr="[17]" startWordPosition="2632" endWordPosition="2632">al, but one would not expect it to differ much from one, and we assume that as long as we are not publishing the raw probabilities (as we would be doing, for example, in publishing perplexity results) the difference from one should be unimportant. As partition-function calculation is typically the major on-line computational problem for maximum-entropy models, this simplifies the model significantly. Naturally, the distributions required by Equation 7 cannot be used without smoothing. In a pure maximum-entropy model this is done by feature selection, as in Ratnaparkhi&apos;s maximum-entropy parser [17]. While we could have smoothed in the same fashion, we choose instead to use standard deleted interpolation. (Actually, we use a minor variant described in [4].) 4 The Experiment We created a parser based upon the maximumentropy-inspired model of the last section, smoothed using standard deleted interpolation. As the generative model is top-down and we use a standard bottom-up best-first probabilistic chart parser [2,7], we use the chart parser as a first pass to generate candidate possible parses to be evaluated in the second pass by our probabilistic model. For runs with the generative model</context>
<context position="17297" citStr="[5, 9,10,15,17]" startWordPosition="3056" endWordPosition="3057">i,t, and h. The equation for the (unsmoothed) conditional probability distribution for t is given in Equation 7. The other four equations can be found in a longer version of this paper available on the author&apos;s website (www.cs.brown.eduhiec). L and R are conditioned on three previous labels so we are using a third-order Markov grammar. Also, the label of the parent constituent lp is conditioned upon even when it is not obviously related to the further conditioning events. This is due to the importance of this factor in parsing, as noted in, e.g., [14]. In keeping with the standard methodology [5, 9,10,15,17], we used the Penn Wall Street Journal tree-bank [16] with sections 2-21 for training, section 23 for testing, and section 24 for development (debugging and tuning). Performance on the test corpus is measured using the standard measures from [5,9,10,17]. In particular, we measure labeled precision (LP) and recall (LR), average number of crossbrackets per sentence (CB), percentage of sentences with zero cross brackets (OCB), and percentage of sentences with &lt; 2 cross brackets (2CB). Again as standard, we take separate measurements for all sentences of length &lt; 40 and all sentences of length &lt; 1</context>
<context position="27632" citStr="[5,9,17]" startWordPosition="4842" endWordPosition="4842">wever, a second-order grammar does slightly better and a third-order grammar does significantly better than the tree-bank parser. 6 Conclusion We have presented a lexicalized Markov grammar parsing model that achieves (using the now standard training/testing/development sections of the Penn treebank) an average precision/recall of 91.1% on sentences of length &lt; 40 and 89.5% on sentences of length &lt; 100. This corresponds to an error reduction of 13% over the best previously published single parser results on this test set, those of Collins [9]. That the previous three best parsers on this test [5,9,17] all perform within a percentage point of each other, despite quite different basic mechanisms, led some researchers to wonder if there might be some maximum level of parsing performance that could be obtained using the treebank for training, and to conjecture that perhaps we were at it. The results reported here disprove this conjecture. The results of [13] achieved by combining the aforementioned three-best parsers also suggest that the limit on tree-bank trained parsers is much higher than previously thought. Indeed, it may be that adding this new parser to the mix may yield still higher re</context>
</contexts>
<marker>17.</marker>
<rawString>RATNAPARKHI, A. Learning to parse natural language with maximum entropy models. Machine Learning 341/2/3 (1999), 151-176.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>