<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000124">
<title confidence="0.976812">
Unit Completion for a Computer-aided Translation Typing
System
</title>
<author confidence="0.557382">
Philippe Langlais, George Foster and Guy Lapalme
RALI / DIRO
</author>
<affiliation confidence="0.553456">
Universite de Montreal
</affiliation>
<address confidence="0.7078785">
C.P. 6128, succursale Centre-ville
Montral (Qubec), Canada, H3C 3J7
</address>
<email confidence="0.996679">
ffelipe,foster,lapalmel@iro.umontreal.ca
</email>
<sectionHeader confidence="0.997359" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999886625">
This work is in the context of TRANSTYPE, a sys-
tem that observes its user as he or she types a trans-
lation and repeatedly suggests completions for the
text already entered. The user may either accept,
modify, or ignore these suggestions. We describe the
design, implementation, and performance of a pro-
totype which suggests completions of units of texts
that are longer than one word.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955727272727">
TRANSTYPE is part of a project set up to explore
an appealing solution to Interactive Machine Trans-
lation (IMT). In constrast to classical IMT systems,
where the user&apos;s role consists mainly of assisting the
computer to analyse the source text (by answering
questions about word sense, ellipses, phrasal attach-
ments, etc), in TRANSTYPE the interaction is direct-
ly concerned with establishing the target text.
Our interactive translation system works as fol-
lows: a translator selects a sentence and begins typ-
ing its translation. After each character typed by
the translator, the system displays a proposed com-
pletion, which may either be accepted using a spe-
cial key or rejected by continuing to type. Thus
the translator remains in control of the translation
process and the machine must continually adapt it-
s suggestions in response to his or her input. We
are currently undertaking a study to measure the
extent to which our word-completion prototype can
improve translator productivity. The conclusions of
this study will be presented elsewhere.
The first version of TRANSTYPE (Foster et al.,
1997) only proposed completions for the current
word. This paper deals with predictions which ex-
tend to the next several words in the text. The po-
tential gain from multiple-word predictions can be
appreciated in the one-sentence translation task re-
ported in table 1, where a hypothetical user saves
over 60% of the keystrokes needed to produce a
translation in a word completion scenario, and about
85% in a &amp;quot;unit&amp;quot; completion scenario.
In all the figures that follow, we use different fonts
to differentiate the various input and output: italics
are used for the source text, sans-serif for characters
typed by the user and typewriter-like for charac-
ters completed by the system.
The first few lines of the table 1 give an idea of
how TransType functions. Let us assume the unit s-
cenario (see column 2 of the table) and suppose that
the user wants to produce the sentence &amp;quot;Ce projet
de loi est examine a la chambre des communes&amp;quot; as a
translation for the source sentence &amp;quot;This bill is ex-
amined in the house of commons&amp;quot;. The first hypoth-
esis that the system produces before the user enters
a character is loi (law). As this is not a good guess
from TRANSTYPE the user types the first character
(c) of the words he or she wants as a translation.
Taking this new input into account, TRANSTYPE
then modifies its proposal so that it is compatible
whith what the translator has typed. It suggests
the desired sequence ce projet de loi, which the user
can simply validate by typing a dedicated key. Con-
tinuing in this way, the user and TRANSTYPE alter-
nately contribute to the final translation. A screen
copy of this prototype is provided in figure 1.
</bodyText>
<sectionHeader confidence="0.990526" genericHeader="method">
2 The Core Engine
</sectionHeader>
<bodyText confidence="0.9999522">
The core of TRANSTYPE is a completion engine
which comprises two main parts: an evaluator which
assigns probabilistic scores to completion hypotheses
and a generator which uses the evaluation function
to select the best candidate for completion.
</bodyText>
<subsectionHeader confidence="0.993446">
2.1 The Evaluator
</subsectionHeader>
<bodyText confidence="0.998409727272727">
The evaluator is a function p(tIt&apos; , s) which assigns to
each target-text unit t an estimate of its probability
given a source text s and the tokens t&apos; which precede
t in the current translation of s.1 Our approach to
modeling this distribution is based to a large extent
on that of the IBM group (Brown et al., 1993), but
it differs in one significant aspect: whereas the IB-
M model involves a &amp;quot;noisy channel&amp;quot; decomposition,
we use a linear combination of separate prediction-
s from a language model p(tit&apos;) and a translation
model p(t1s). Although the noisy channel technique
</bodyText>
<footnote confidence="0.516841">
&apos;We assume the existence of a deterministic procedure for
tokenizing the target text.
</footnote>
<page confidence="0.993522">
135
</page>
<note confidence="0.554459">
This bill is examined in the house of commons
</note>
<table confidence="0.999092615384615">
word-completion task unit-completion task
pref. completions pref. completions
ce ce+ /loi. • c/&apos; c+ /loi • c/e projet de loi
projet p+ /est • p/rojet -
de d+ /trim • d/e ..
loi I+ /tres • IPA. ..
est e+ /de • e/st e+ /de • e/st
examine e+ /en • e/xamine ex+ /a la chambre des communes • e/n • ex/amine
a la a+ /par • a/ la + /a la chambre des communes
chambre + /chambre -
des de+ /communes • die • de/s -
communes + /communes -
54 char. 10 + 10 accept. -4 20 keystrokes 4 + 4 accept. -4 8 keystrokes
</table>
<tableCaption confidence="0.999372">
Table 1: A one-sentence session illustrating the word- and unit-completion tasks. The first column indicates
</tableCaption>
<bodyText confidence="0.9991965">
the target words the user is expected to produce. The next two columns indicate respectively the prefixes
typed by the user and the completions proposed by the system in a word-completion task. The last two
columns provide the same information for the unit-completion task. The total number of keystrokes for
both tasks is reported in the last line. + indicates the acceptance key typed by the user. A completion is
denoted by a/13 where a is the typed prefix and 0 the completed part. Completions for different prefixes
are separated by
is powerful, it has the disadvantage that p(sit&apos; ,t) is
more expensive to compute than p(tis) when using
IBM-style translation models. Since speed is cru-
cial for our application, we chose to forego the noisy
channel approach in the work described here. Our
linear combination model is described as follows:
</bodyText>
<equation confidence="0.634114">
p(tit&apos; , s) = p(tle) a(t&apos; , s) + p(t1s) [1 – a(t&apos; , s)] (1)
language translation
</equation>
<bodyText confidence="0.99841652">
where a(t&apos; , s) E [0,1] are context-dependent inter-
polation coefficients. For example, the translation
model could have a higher weight at the start of a
sentence but the contribution of the language mod-
el might become more important in the middle or
the end of the sentence. A study of the weightings
for these two models is described elsewhere. In the
work described here we did not use the contribution
of the language model (that is, a(e, s) = 0, V ts, s).
Techniques for weakening the independence as-
sumptions made by the IBM models 1 and 2 have
been proposed in recent work (Brown et al., 1993;
Berger et al., 1996; Och and Weber, 98; Wang and
Waibel, 98; Wu and Wong, 98). These studies report
improvements on some specific tasks (task-oriented
limited vocabulary) which by nature are very differ-
ent from the task TRANSTYPE is devoted to. Fur-
thermore, the underlying decoding strategies are too
time consuming for our application. We therefore
use a translation model based on the simple linear in-
terpolation given in equation 2 which combines pre-
dictions of two translation models — M, and Mu —
both based on IBM-like model 2(Brown et al., 1993).
M, was trained on single words and Ma, described
in section 3, was trained on both words and units.
</bodyText>
<equation confidence="0.971265">
p(tis) -= Ops(tis) + (1— 0)Pu(tig(s)) (2)
...--„,.--, &apos;.......—..,,...........&amp;quot;
word unit
</equation>
<bodyText confidence="0.999993">
where Ps and pu stand for the probabilities given re-
spectively by M„ and M. g(s) represents the new
sequence of tokens obtained after grouping the to-
kens of s into units. The grouping operator 0 is
illustrated in table 2 and is described in section 3.
</bodyText>
<subsectionHeader confidence="0.999953">
2.2 The Generator
</subsectionHeader>
<bodyText confidence="0.999962285714286">
The task of the generator is to identify units that
match the current prefix typed by the user, and pick
the best candidate according to the evaluator. Due
to time considerations, the generator introduces a
division of the target vocabulary into two parts: a
small active component whose contents are always
searched for a match to the current prefix, and a
much larger passive part over (380,000 word form-
s) which comes into play only when no candidates
are found in the active vocabulary. The active part
is computed dynamically when a new sentence is s-
elected by the translator. It is composed of a few
entities (tokens and units) that are likely to appear
in the translation. It is a union of the best can-
didates provided by each model M, and M. over
the set of all possible target tokens (resp. units)
that have a non-null translation probability of being
translated by any of the current source tokens (resp.
units). Table 2 shows the 10 most likely tokens and
units in the active vocabulary for an example source
sentence.
</bodyText>
<page confidence="0.994397">
136
</page>
<construct confidence="0.483819571428571">
that is • what the prime minister said
, • and i • have • outlined • what • has -
happened since • then • .
C&apos; est • ce que • le - premier ministre • a -
dit • , • et • j&apos; • ai • resume ce • qui • s&apos; est -
produit depuis • .
G(s) that is what • the prime minister said • , and i
</construct>
<listItem confidence="0.99687575">
• have outlined • what has happened • since
then • .
, . • est • ce • ministre que • et • a • premier
• le
</listItem>
<bodyText confidence="0.977621777777778">
Au ce qui s&apos; est produit • et je c&apos; est ce que • voila
ce que• qu&apos; est c&apos; est • , et • le premier ministre
disait
Table 2: Role of the generator for a sample pair of
sentences (t is the translation of s in our corpus).
g(s) is the sequence of source tokens recasted by
the grouping operator g. A. indicates the 10 best
tokens according to the word model, Au the 10 best
units according to the unit model.
</bodyText>
<sectionHeader confidence="0.970597" genericHeader="method">
3 Modeling Unit Associations
</sectionHeader>
<bodyText confidence="0.999932833333333">
Automatically identifying which source words or
groups of words will give rise to which target words
or groups of words is a fundamental problem which
remains open. In this work, we decided to proceed
in two steps: a) monolingually identifying groups of
words that would be better handled as units in a giv-
en context, and b) mapping the resulting source and
target units. To train our unit models, we used a
segment of the Hansard corpus consisting of 15,377
pairs of sentences, totaling 278,127 english token-
s (13,543 forms) and 292,865 french tokens (16,399
forms).
</bodyText>
<subsectionHeader confidence="0.999882">
3.1 Finding Monolingual Units
</subsectionHeader>
<bodyText confidence="0.999761052631579">
Finding relevant units in a text has been explored in
many areas of natural language processing. Our ap-
proach relies on distributional and frequency statis-
tics computed on each sequence of words found in a
training corpus. For sake of efficiency, we used the
suffix array technique to get a compact representa-
tion of our training corpus. This method allows the
efficient retrieval of arbitrary length n-grams (Nagao
and Mori, 94; Haruno et al., 96; Ikehaxa et al., 96;
Shimohata et al., 1997; Russell, 1998).
The literature abounds in measures that can help
to decide whether words that co-occur are linguisti-
cally significant or not. In this work, the strength of
association of a sequence of words wr = w1, wn
is computed by two measures: a likelihood-based one
p(wr) (where is the likelihood ratio given in (Dun-
ning, 93)) and an entropy-based one e(w) (Shimo-
hata et al., 1997). Letting T stand for the training
text and m a token:
</bodyText>
<equation confidence="0.975588111111111">
argmin t(w1,74+1)
iEjl,n[
e(w) = 0.5x
h m
Ernimw;&apos;
ET q((ww)
(fir::))
EmInqmET h ffr:qeq(7:174)
))
</equation>
<bodyText confidence="0.9950911">
Intuitively, the first measurement accounts for the
fact that parts of a sequence of words that should
be considered as a whole should not appear often by
themselves. The second one reflects the fact that a
salient unit should appear in various contexts (i.e.
should have a high entropy score).
We implemented a cascade filtering strategy based
on the likelihood score p, the frequency f, the length
1 and the entropy value e of the sequences. A
first filter (Fi (imin 5 fmin Pmin, emin)) removes any
sequence s for which /(s) &lt; /min or p(s) &lt; prnin
or e(s) &lt; emin or f (s) &lt; /min. A second filter
(F2) removes sequences that are included in pre-
ferred ones. In terms of sequence reduction, apply-
ing F1(2, 2, 5.0, 0.2) on the 81,974 English sequences
of at least two tokens seen at least twice in our train-
ing corpus, less than 50% of them (39,093) were fil-
tered: 17,063 (21%) were removed because of their
low entropy value, 25,818 (31%) because of their low
likelihood value.
</bodyText>
<subsectionHeader confidence="0.999195">
3.2 Mapping
</subsectionHeader>
<bodyText confidence="0.999950470588235">
Mapping the identified units (tokens or sequences) to
their equivalents in the other language was achieved
by training a new translation model (IBM 2) us-
ing the EM algorithm as described in (Brown et al.,
1993). This required grouping the tokens in our
training corpus into sequences, on the basis of the
unit lexicons identified in the previous step (we will
refer to the results of this grouping as the sequence-
based corpus). To deal with overlapping possibilities,
we used a dynamic programming scheme which opti-
mized a criterion C given by equation 4 over a set S
of all units collected for a given language plus all sin-
gle words. g(wr) is obtained by returning the path
that maximized B(n). We investigated several C-
criteria and we found Ci—a length-based measure—
to be the most satisfactory. Table 2 shows an output
of the grouping function.
</bodyText>
<figure confidence="0.975678884615384">
p(w)
(3)
0 i f j &lt;=
with: Ci(wh = + 1 else
1 0 if i = 0
(4)
B(i — I — 1) )
B(i) =
argmax
/OM ,
137
source unit (s)
we have
we must
this bill
people of canada
mr. speaker :
what is happening
of course ,
is it the pleasure of the house
adopt the
f(s) target units ((cv,p])
1748 nous,0.49] [avons,0.41] [,nous avons,0.071
720 nous devons,0.61] [ii faut,0.19] [nous,0.14]
640 ce projet de loi3O.35] [projet de loi .,0.21] [projet de loi3O.18]
282 les canadiens,0.26] [des canadiens,0.21] [la population,0.07]
269 m. le president :,0.80] [a,0.07] [a. la,0.06)
190 ce qui se passe,0.21] [ce qui se,0.16] [et,0.15]
178 evidemment „0.26] [naturellement,0.08] [bien sfir,0.08]
to 14 plait-il A la chambre d&apos; adopter,0.49] [la motion ?,0.421 [motion
?,0.04]
201 le raonde,0.46] [du monde,0.33] [le monde entier,0.19]
86 les garderies,0.591 [la garde d&apos; enfants,0.23] [des services de
garde d&apos; enfants,0.13]
75 l&apos; accord de libre-echange,0.96] [la decision du gatt,0.04]
66 l&apos; enseignement postsecondaire,0.75] [1&apos; education postsec-
ondaire,0.15] [des fonds,0.06]
62 la premiere fois,1.00]
36 [le bureau canadien de la securite aerienne,0.55] [du bureau cana-
dien de la securite arienne,0.311 [1&apos; un,0.14]
26 [au cours des cinq prochaines annees,0.53] [cinq prochaines an-
nees,0.27] [25 milliards de d ollars,0.10]
17 [le peuple chinois,0.38] [la population chinoise,0.25] [les chi-
nois,0.131
the world
child care
the free trade agreement
post-secondary education
the first time
the canadian aviation safety board
the next five years
the people of china
</figure>
<tableCaption confidence="0.793559">
Table 3: Bilingual associations. The first column indicates a source unit, the second one its frequency in the
</tableCaption>
<bodyText confidence="0.992196103448276">
training corpus. The third column reports its 3-best ranked target associations (a being a token or a unit,
p being the translation probability). The second half of the table reports NP-associations obtained after the
filter described in the text.
We investigated three ways of estimating the pa-
rameters of the unit model. In the first one, e1,
the translation parameters are estimated by apply-
ing the EM algorithm in a straightforward fashion
over all entities (tokens and units) present at least
twice in the sequence-based corpus 2 . The two next
methods filter the probabilities obtained with the £1.
method. In E2 all probabilities p(t1s) are set to 0
whenever s is a token (not a unit), thus forcing the
model to contain only associations between source
units and target entities (tokens or units). In E3
any parameter of the model that involves a token
is removed (that is, p(tis) = 0 if t or s is a token).
The resulting model will thus contain only unit as-
sociations. In both cases, the final probabilities are
renormalized. Table 3 shows a few entries from a
unit model (M.) obtained after 15 iterations of the
EM-algorithm on a sequence corpus resulting from
the application of the length-grouping criterion (C1)
over a lexicon of units whose likelihood score is above
5.0. The probabilities have been obtained by appli-
cation of the method E2
We found many partially correct associations
(over the years/au fits des, we have/nous, etc) that
illustrate the weakness of decoupling the unit iden-
tification from the mapping problem. In most cas-
</bodyText>
<footnote confidence="0.81027">
2The entities seen only once are mapped to a special &amp;quot;un-
known&amp;quot; word
</footnote>
<bodyText confidence="0.99877475">
es however, these associations have a lower proba-
bility than the good ones. We also found few er-
ratic associations (the first time/ c&apos;etait, some hon.
members/!, etc) due to distributional artifacts. It is
also interesting to note that the good associations
we found are not necessary compositional in nature
(we must/il faut, people of canada/les canadiens, of
course/evidemment, etc).
</bodyText>
<subsectionHeader confidence="0.998711">
3.3 Filtering
</subsectionHeader>
<bodyText confidence="0.999853473684211">
One way to increase the precision of the mapping
process is to impose some linguistic constraints on
the sequences such as simple noun-phrase contraints
(Gaussier, 1995; Kupiec, 1993; hua Chen and Chen,
94; Fung, 1995; Evans and Zhai, 1996). It is also
possible to focus on non-compositional compounds,
a key point in bilingual applications (Su et al., 1994;
Melamed, 1997; Lin, 99). Another interesting ap-
proach is to restrict sequences to those that do not
cross constituent boundary patterns (Wu, 1995; Fu-
ruse and Iida, 96). In this study, we filtered for po-
tential sequences that are likely to be noun phrases,
using simple regular expressions over the associated
part-of-speech tags. An excerpt of the association
probabilities of a unit model trained considering on-
ly the NP-sequences is given in table 3. Applying
this filter (referred to as .7.Np in the following) to the
39,093 english sequences still surviving after previ-
ous filters .7.1 and .F2 removes 35,939 of them (92%).
</bodyText>
<page confidence="0.980085">
138
</page>
<figure confidence="0.93952725">
model spared ok good nu
1 baseline — model 1 48.98 0 0 747 0
2 baseline — model 2 51.83 0 0 747 0
3 Si + (2, 2, 0, 0.2) 50.98 527 1702 5 626
4 El + F1(2,2,5,0.2) 51.61 596 2149 5 658
5 Si + (2, 2, 5, 0.2) + 51.72 633 2265 5 657
6 £2 + F1(2, 2, 0, 0.2) 51.39 514 1551 43 578
7 £2 + (2, 2, 5, 0.2) 51.99 470 1889 46 614
8 £2 + .T1(2, 2, 5, 0.2) + 7.2 52.12 493 1951 46 606
9 £3 ± 2:1(2, 2, 0, 0.2) 51.07 577 1699 43 588
10 £3 ± (2, 2, 5, 0.2) 51.47 629 2124 46 618
11 £3 ± (2, 2, 5, 0.2) + 7-2 51.68 665 2209 46 615
12 El + (2, 2, 5, 0.2) + F2 + FNP 52.83 416 1302 4 564
13 E3 + F1(2, 2, 5, 0.2) + FNP 53.12 439 1031 228 425
14 £3 ± F1(2, 2, 5, 0.2) + FNP 53.16 458 1052 199 439
15 £3 ± # = 0.4 + (2, 2, 5, 0.2) + FNP 53.22 495 1031 228 425
</figure>
<tableCaption confidence="0.5558358">
Table 4: Completion results of several translation models. spared: theoretical proportion of characters
saved; ok: number of target units accepted by the user; good: number of target units that matched the
expected whether they were proposed or not; nu: number of sentences for which no target unit was found
by the translation model; u: number of sentences for which at least one helpful unit has been found by the
model, but not necessarily proposed.
</tableCaption>
<bodyText confidence="0.986836">
More than half of the 3,154 remaining NP-sequences
contain only two words.
</bodyText>
<sectionHeader confidence="0.999425" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999954883333334">
We collected completion results on a test corpus
of 747 sentences (13,386 english tokens and 14,506
french ones) taken from the Hansard corpus. These
sentences have been selected randomly among sen-
tences that have not been used for the training.
Around 18% of the source and target words are not
known by the translation model.
The baseline models (line 1 and 2) are obtained
without any unit model (i.e. 13 = 1 in equation 2).
The first one is obtained with an IBM-like model 1
while the second is an IBM-like model 2. We observe
that for the pair of languages we considered, model
2 improves the amount of saved keystrokes of almost
3% compared to model 1. Therefore we made use of
alignment probabilities for the other models.
The three next blocks in table 4 show how the
parameter estimation method affects performance.
Training models under the Ei method gives the worst
results. This results from the fact that the word-
to-word probabilities trained on the sequence based
corpus (predicted by M. in equation 2) are less ac-
curate than the ones learned from the token based
corpus. The reason is simply that there are less oc-
currences of each token, especially if many units are
identified by the grouping operator.
In methods £2 and 53, the unit model of equation
2 only makes predictions pit(tis) when s is a source u-
nit, thus lowering the noise compared to method Si
We also observe in these three blocks the influence
of sequence filtering: the more we filter, the better
the results. This holds true for all estimation meth-
ods tried. In the fifth block of table 4 we observe
the positive influence of the NP-filtering, especially
when using the third estimation method.
The best combination we found is reported in line
15. It outperforms the baseline by around 1.5%.
This model has been obtained by retaining all se-
quences seen at least two times in the training cor-
pus for which the likelihood test value was above 5
and the entropy score above 0.2 (F1(2, 2, 5, 0.2)). In
terms of the coverage of this unit model, it is in-
teresting to note that among the 747 sentences of
the test session, there were 228 for which the model
did not propose any units at all. For 425 of the re-
maining sentences, the model proposed at least one
helpful (good or partially good) unit. The active vo-
cabulary for these sentences contained an average of
around 2.5 good units per sentence, of which only
half (495) were proposed during the session. The
fact that this model outperforms others despite it-
s relatively poor coverage (compared to the others)
may be explained by the fact that it also removes
part of the noise introduced by decoupling the i-
dentification of the salient units from the training
procedure. Furthermore, as we mentionned earlier,
the more we filter, the less the grouping scheeme
presented in equation 4 remains necessary, thus re-
ducing a possible source of noise.
The fact that this model outperforms others, de-
spite its relatively poor coverage, is due to the fact
</bodyText>
<page confidence="0.997263">
139
</page>
<bodyText confidence="0.753655">
trams typo Nre F1 Eli
</bodyText>
<subsectionHeader confidence="0.987975">
Ficnior Options
</subsectionHeader>
<bodyText confidence="0.997533166666667">
I at.. pleased to take part in this debate today .
Using today &apos;s technologies , it is possible for all Canadians to
register their votes on issues of public spending and public
borrowing.
COur proposed is a symbolical and responsible attempt to democratize &apos;-
and open the whole issue of public finance.
</bodyText>
<figure confidence="0.989165846153846">
J II me fait plalsIr de prendre la parole aujoi..ird&apos;hul dans le cadre de co
clObat
Grace A la technologle modern°. tous les Canadiens peuvent se
prononcer sur les questions de clOpenses et d&apos; emprunts de l&apos; Etat
Ti
I
Notre p ro.r.Vaon j
e.ys
TOjftt
Dalton
zo p o pi&amp;quot; Dna
pelthcs.ang phrase:3 /6
Stst.-I Sweet*: 3 so OR: 37 Key:59
</figure>
<figureCaption confidence="0.793861">
Figure 1: Example of an interaction in TRANSTYPE with the source text in the top half of the screen. The
target text is typed in the bottom half with suggestions given by the menu at the insertion point.
</figureCaption>
<bodyText confidence="0.997774714285714">
that it also removes part of the noise that is intro-
duced by dissociating the identification of the salient
units from the training procedure. Furthermore, as
we mentioned earlier, the more we filter, the less the
grouping scheme presented in equation 4 remains
necessary, thus further reducing an other possible
source of noise.
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999971923076923">
We have described a prototype system called
TRANSTYPE which embodies an innovative ap-
proach to interactive machine translation in which
the interaction is directly concerned with establish-
ing the target text. We proposed and tested a mech-
anism to enhance TRANSTYPE by having it predic-
t sequences of words rather than just completions
for the current word. The results show a modest
improvement in prediction performance which will
serve as a baseline for our future investigations. One
obvious direction for future research is to revise our
current strategy of decoupling the selection of units
from their bilingual context.
</bodyText>
<sectionHeader confidence="0.99518" genericHeader="acknowledgments">
Acknowlegments
</sectionHeader>
<bodyText confidence="0.999728">
TRANSTYPE is a project funded by the Natural Sci-
ences and Engineering Research Council of Canada.
We are undebted to Elliott Macklovitch and Pierre
Isabelle for the fruitful orientations they gave to this
work.
</bodyText>
<sectionHeader confidence="0.97293" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.979855678571428">
Adam L. Berger, Stephen A. Della Pietra, and Vin-
cent J. Della Pietra. 1996. A maximum entropy
approach to natural language processing. Compu-
tational Linguistics, 22(1):39-71.
Peter F. Brown, Stephen A. Della Pietra, Vincen-
t Della J. Pietra, and Robert L. Mercer. 1993.
The mathematics of machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263-312, June.
Ted Dunning. 93. Accurate methods for the statis-
tics of surprise and coincidence. Computational
Linguistics, 19(1):61-74.
David A. Evans and Chengxiang Zhai. 1996. Noun-
phrase analysis in unrestricted text for informa-
tion retrieval. In Proceedings of the 34th Annu-
al Meeting of the Association for Computational
Linguistics, pages 17-24, Santa Cruz, California.
George Foster, Pierre Isabelle, and Pierre Plamon-
don. 1997. Target-text Mediated Interactive Ma-
chine Translation. Machine Translation, 12:175-
194.
Pascale Fung. 1995. A pattern matching method for
finding noun and proper noun translations from
noisy parallel corpora. In Proceedings of the 33rd
Annual Meeting of the Association for Compu-
tational Linguistics, pages 236-243, Cambridge,
Massachusetts.
Osamu Furuse and Hitoshi Iida. 96. Incremen-
</bodyText>
<page confidence="0.995596">
140
</page>
<reference confidence="0.997699385542169">
tal translation utilizing constituent boundray pat-
terns. In Proceedings of the 16th International
Conference On Computational Linguistics, pages
412-417, Copenhagen, Denmark.
Eric Gaussier. 1995. Modles statistiques et patron-
s morphosyntaxiques pour l&apos;extraction de lexiques
bilingues. Ph.D. thesis, Universit de Paris 7, jan-
vier.
Masahiko Haruno, Satoru Ikehara, and Takefumi
Yamazaki. 96. Learning bilingual collocations by
word-level sorting. In Proceedings of the 16th In-
ternational Conference On Computational Lin-
guistics, pages 525-530, Copenhagen, Denmark.
Kuang hua Chen and Hsin-Hsi Chen. 94. Extract-
ing noun phrases from large-scale texts: A hybrid
approach and its automatic evaluation. In Pro-
ceedings of the 32nd Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 234-
241, Las Cruces, New Mexico.
Satoru Ikehara, Satoshi Shirai, and Hajine Uchino.
96. A statistical method for extracting uinterupt-
ed and interrupted collocations from very large
corpora. In Proceedings of the 16th International
Conference On Computational Linguistics, pages
574-579, Copenhagen, Denmark.
Julian Kupiec. 1993. An algorithm for finding noun
phrase correspondences in bilingual corpora. In
Proceedings of the 31st Annual Meeting of the
Association for Computational Linguistics, pages
17-22, Colombus, Ohio.
Dekang Lin. 99. Automatic identification of non-
compositional phrases. In Proceedings of the 37th
Annual Meeting of the Association for Computa-
tional Linguistics, pages 317-324, College Park,
Maryland.
I. Dan Melamed. 1997. Automatic discovery of non-
compositional coumpounds in parallel data. In
Proceedings of the 2nd Conference on Empirical
Methods in Natural Language Processing, pages
97-108, Providence, RI, August, lst-2nd.
Makoto Nagao and Shinsuke Mori. 94. A new
method of n-gram statistics for large number of
n and automatic extraction of words and phrases
from large text data of japanese. In Proceedings
of the 16th International Conference On Com-
putational Linguistics, volume 1, pages 611-615,
Copenhagen, Denmark.
Franz Josef Och and Hans Weber. 98. Improving
statistical natural language translation with cate-
gories and rules. In Proceedings of the 36th Annu-
al Meeting of the Association for Computational
Linguistics, pages 985-989, Montréal, Canada.
Graham Russell. 1998. Identification of salient to-
ken sequences. Internal report, RALI, University
of Montreal, Canada.
Sayori Shimohata, Toshiyuki Sugio, and Junji
Nagata. 1997. Retrieving collocations by co-
occurrences and word order constraints. In Pro-
ceedings of the 35th Annual Meeting of the Asso-
ciation for Computational Linguistics, pages 476-
481, Madrid Spain.
Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang.
1994. A corpus-based approach to automatic com-
pound extraction. In Proceedings of the 32nd An-
nual Meeting of the Association for Computation-
al Linguistics, pages 242-247, Las Cruces, New
Mexico.
Ye-Yi Wang and Alex Waibel. 98. Modeling with
structures in statistical machine translation. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics, vol-
ume 2, pages 1357-1363, Montréal, Canada.
Delcai Wu and Hongsing Wong. 98. Machine trans-
lation with a stochastic grammatical channel. In
Proceedings of the 36th Annual Meeting of the
Association for Computational Linguistics, pages
1408-1414, Montréal, Canada.
Dekai Wu. 1995. Stochastic inversion transduc-
tion grammars, with application to segmentation,
bracketing, and alignment of parallel corpora. In
Proceedings of the International Joint Conference
on Artificial Intelligence, volume 2, pages 1328-
1335, Montréal, Canada.
</reference>
<page confidence="0.998262">
141
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.469511">
<title confidence="0.998917">Unit Completion for a Computer-aided Translation Typing System</title>
<author confidence="0.999883">Philippe Langlais</author>
<author confidence="0.999883">George Foster</author>
<author confidence="0.999883">Guy Lapalme</author>
<affiliation confidence="0.995226">RALI / DIRO Universite de Montreal</affiliation>
<address confidence="0.681363">C.P. 6128, succursale Centre-ville Montral (Qubec), Canada, H3C 3J7</address>
<email confidence="0.994019">ffelipe,foster,lapalmel@iro.umontreal.ca</email>
<abstract confidence="0.998931888888889">This work is in the context of TRANSTYPE, a system that observes its user as he or she types a transand repeatedly suggests the text already entered. The user may either accept, modify, or ignore these suggestions. We describe the design, implementation, and performance of a prototype which suggests completions of units of texts that are longer than one word.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>tal translation utilizing constituent boundray patterns.</title>
<booktitle>In Proceedings of the 16th International Conference On Computational Linguistics,</booktitle>
<pages>412--417</pages>
<location>Copenhagen, Denmark.</location>
<marker></marker>
<rawString>tal translation utilizing constituent boundray patterns. In Proceedings of the 16th International Conference On Computational Linguistics, pages 412-417, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Gaussier</author>
</authors>
<title>Modles statistiques et patrons morphosyntaxiques pour l&apos;extraction de lexiques bilingues.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<volume>7</volume>
<pages>janvier.</pages>
<institution>Universit de Paris</institution>
<contexts>
<context position="16711" citStr="Gaussier, 1995" startWordPosition="2897" endWordPosition="2898"> once are mapped to a special &amp;quot;unknown&amp;quot; word es however, these associations have a lower probability than the good ones. We also found few erratic associations (the first time/ c&apos;etait, some hon. members/!, etc) due to distributional artifacts. It is also interesting to note that the good associations we found are not necessary compositional in nature (we must/il faut, people of canada/les canadiens, of course/evidemment, etc). 3.3 Filtering One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Gaussier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996). It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99). Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; Furuse and Iida, 96). In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags. An excerpt of the association probabilities of a unit model trained considering</context>
</contexts>
<marker>Gaussier, 1995</marker>
<rawString>Eric Gaussier. 1995. Modles statistiques et patrons morphosyntaxiques pour l&apos;extraction de lexiques bilingues. Ph.D. thesis, Universit de Paris 7, janvier.</rawString>
</citation>
<citation valid="false">
<title>Learning bilingual collocations by word-level sorting.</title>
<booktitle>In Proceedings of the 16th International Conference On Computational Linguistics,</booktitle>
<pages>525--530</pages>
<location>Copenhagen, Denmark.</location>
<marker></marker>
<rawString>Masahiko Haruno, Satoru Ikehara, and Takefumi Yamazaki. 96. Learning bilingual collocations by word-level sorting. In Proceedings of the 16th International Conference On Computational Linguistics, pages 525-530, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="false">
<title>Extracting noun phrases from large-scale texts: A hybrid approach and its automatic evaluation.</title>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>234--241</pages>
<location>Las Cruces, New Mexico.</location>
<marker></marker>
<rawString>Kuang hua Chen and Hsin-Hsi Chen. 94. Extracting noun phrases from large-scale texts: A hybrid approach and its automatic evaluation. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 234-241, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="false">
<title>A statistical method for extracting uinterupted and interrupted collocations from very large corpora.</title>
<booktitle>In Proceedings of the 16th International Conference On Computational Linguistics,</booktitle>
<pages>574--579</pages>
<location>Copenhagen, Denmark.</location>
<marker></marker>
<rawString>Satoru Ikehara, Satoshi Shirai, and Hajine Uchino. 96. A statistical method for extracting uinterupted and interrupted collocations from very large corpora. In Proceedings of the 16th International Conference On Computational Linguistics, pages 574-579, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julian Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>17--22</pages>
<location>Colombus, Ohio.</location>
<contexts>
<context position="16725" citStr="Kupiec, 1993" startWordPosition="2899" endWordPosition="2900"> to a special &amp;quot;unknown&amp;quot; word es however, these associations have a lower probability than the good ones. We also found few erratic associations (the first time/ c&apos;etait, some hon. members/!, etc) due to distributional artifacts. It is also interesting to note that the good associations we found are not necessary compositional in nature (we must/il faut, people of canada/les canadiens, of course/evidemment, etc). 3.3 Filtering One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Gaussier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996). It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99). Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; Furuse and Iida, 96). In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags. An excerpt of the association probabilities of a unit model trained considering only the NP-s</context>
</contexts>
<marker>Kupiec, 1993</marker>
<rawString>Julian Kupiec. 1993. An algorithm for finding noun phrase correspondences in bilingual corpora. In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, pages 17-22, Colombus, Ohio.</rawString>
</citation>
<citation valid="false">
<title>Automatic identification of noncompositional phrases.</title>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>317--324</pages>
<location>College Park, Maryland.</location>
<marker></marker>
<rawString>Dekang Lin. 99. Automatic identification of noncompositional phrases. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 317-324, College Park, Maryland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Dan Melamed</author>
</authors>
<title>Automatic discovery of noncompositional coumpounds in parallel data.</title>
<date>1997</date>
<booktitle>In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>97--108</pages>
<location>Providence, RI,</location>
<contexts>
<context position="16915" citStr="Melamed, 1997" startWordPosition="2931" endWordPosition="2932">, etc) due to distributional artifacts. It is also interesting to note that the good associations we found are not necessary compositional in nature (we must/il faut, people of canada/les canadiens, of course/evidemment, etc). 3.3 Filtering One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Gaussier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996). It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99). Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; Furuse and Iida, 96). In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags. An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as .7.Np in the following) to the 39,093 english sequences still surviving after previous filters .7.1 and .F2 removes 35,939 </context>
</contexts>
<marker>Melamed, 1997</marker>
<rawString>I. Dan Melamed. 1997. Automatic discovery of noncompositional coumpounds in parallel data. In Proceedings of the 2nd Conference on Empirical Methods in Natural Language Processing, pages 97-108, Providence, RI, August, lst-2nd.</rawString>
</citation>
<citation valid="false">
<title>A new method of n-gram statistics for large number of n and automatic extraction of words and phrases from large text data of japanese.</title>
<booktitle>In Proceedings of the 16th International Conference On Computational Linguistics,</booktitle>
<volume>1</volume>
<pages>611--615</pages>
<location>Copenhagen, Denmark.</location>
<marker></marker>
<rawString>Makoto Nagao and Shinsuke Mori. 94. A new method of n-gram statistics for large number of n and automatic extraction of words and phrases from large text data of japanese. In Proceedings of the 16th International Conference On Computational Linguistics, volume 1, pages 611-615, Copenhagen, Denmark.</rawString>
</citation>
<citation valid="false">
<title>Improving statistical natural language translation with categories and rules.</title>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>985--989</pages>
<location>Montréal, Canada.</location>
<marker></marker>
<rawString>Franz Josef Och and Hans Weber. 98. Improving statistical natural language translation with categories and rules. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 985-989, Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Russell</author>
</authors>
<title>Identification of salient token sequences.</title>
<date>1998</date>
<tech>Internal report,</tech>
<institution>RALI, University of Montreal, Canada.</institution>
<contexts>
<context position="10509" citStr="Russell, 1998" startWordPosition="1853" endWordPosition="1854">,127 english tokens (13,543 forms) and 292,865 french tokens (16,399 forms). 3.1 Finding Monolingual Units Finding relevant units in a text has been explored in many areas of natural language processing. Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus. For sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus. This method allows the efficient retrieval of arbitrary length n-grams (Nagao and Mori, 94; Haruno et al., 96; Ikehaxa et al., 96; Shimohata et al., 1997; Russell, 1998). The literature abounds in measures that can help to decide whether words that co-occur are linguistically significant or not. In this work, the strength of association of a sequence of words wr = w1, wn is computed by two measures: a likelihood-based one p(wr) (where is the likelihood ratio given in (Dunning, 93)) and an entropy-based one e(w) (Shimohata et al., 1997). Letting T stand for the training text and m a token: argmin t(w1,74+1) iEjl,n[ e(w) = 0.5x h m Ernimw;&apos; ET q((ww) (fir::)) EmInqmET h ffr:qeq(7:174) )) Intuitively, the first measurement accounts for the fact that parts of a s</context>
</contexts>
<marker>Russell, 1998</marker>
<rawString>Graham Russell. 1998. Identification of salient token sequences. Internal report, RALI, University of Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sayori Shimohata</author>
<author>Toshiyuki Sugio</author>
<author>Junji Nagata</author>
</authors>
<title>Retrieving collocations by cooccurrences and word order constraints.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>476--481</pages>
<location>Madrid</location>
<contexts>
<context position="10493" citStr="Shimohata et al., 1997" startWordPosition="1849" endWordPosition="1852"> sentences, totaling 278,127 english tokens (13,543 forms) and 292,865 french tokens (16,399 forms). 3.1 Finding Monolingual Units Finding relevant units in a text has been explored in many areas of natural language processing. Our approach relies on distributional and frequency statistics computed on each sequence of words found in a training corpus. For sake of efficiency, we used the suffix array technique to get a compact representation of our training corpus. This method allows the efficient retrieval of arbitrary length n-grams (Nagao and Mori, 94; Haruno et al., 96; Ikehaxa et al., 96; Shimohata et al., 1997; Russell, 1998). The literature abounds in measures that can help to decide whether words that co-occur are linguistically significant or not. In this work, the strength of association of a sequence of words wr = w1, wn is computed by two measures: a likelihood-based one p(wr) (where is the likelihood ratio given in (Dunning, 93)) and an entropy-based one e(w) (Shimohata et al., 1997). Letting T stand for the training text and m a token: argmin t(w1,74+1) iEjl,n[ e(w) = 0.5x h m Ernimw;&apos; ET q((ww) (fir::)) EmInqmET h ffr:qeq(7:174) )) Intuitively, the first measurement accounts for the fact t</context>
</contexts>
<marker>Shimohata, Sugio, Nagata, 1997</marker>
<rawString>Sayori Shimohata, Toshiyuki Sugio, and Junji Nagata. 1997. Retrieving collocations by cooccurrences and word order constraints. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 476-481, Madrid Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keh-Yih Su</author>
<author>Ming-Wen Wu</author>
<author>Jing-Shin Chang</author>
</authors>
<title>A corpus-based approach to automatic compound extraction.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>242--247</pages>
<location>Las Cruces, New Mexico.</location>
<contexts>
<context position="16900" citStr="Su et al., 1994" startWordPosition="2927" endWordPosition="2930">me hon. members/!, etc) due to distributional artifacts. It is also interesting to note that the good associations we found are not necessary compositional in nature (we must/il faut, people of canada/les canadiens, of course/evidemment, etc). 3.3 Filtering One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Gaussier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996). It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99). Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; Furuse and Iida, 96). In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags. An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as .7.Np in the following) to the 39,093 english sequences still surviving after previous filters .7.1 and .F2 </context>
</contexts>
<marker>Su, Wu, Chang, 1994</marker>
<rawString>Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang. 1994. A corpus-based approach to automatic compound extraction. In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, pages 242-247, Las Cruces, New Mexico.</rawString>
</citation>
<citation valid="false">
<title>Modeling with structures in statistical machine translation.</title>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<volume>2</volume>
<pages>1357--1363</pages>
<location>Montréal, Canada.</location>
<marker></marker>
<rawString>Ye-Yi Wang and Alex Waibel. 98. Modeling with structures in statistical machine translation. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, volume 2, pages 1357-1363, Montréal, Canada.</rawString>
</citation>
<citation valid="false">
<title>Machine translation with a stochastic grammatical channel.</title>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1408--1414</pages>
<location>Montréal, Canada.</location>
<marker></marker>
<rawString>Delcai Wu and Hongsing Wong. 98. Machine translation with a stochastic grammatical channel. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 1408-1414, Montréal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora.</title>
<date>1995</date>
<booktitle>In Proceedings of the International Joint Conference on Artificial Intelligence,</booktitle>
<volume>2</volume>
<pages>1328--1335</pages>
<location>Montréal, Canada.</location>
<contexts>
<context position="17047" citStr="Wu, 1995" startWordPosition="2952" endWordPosition="2953">l in nature (we must/il faut, people of canada/les canadiens, of course/evidemment, etc). 3.3 Filtering One way to increase the precision of the mapping process is to impose some linguistic constraints on the sequences such as simple noun-phrase contraints (Gaussier, 1995; Kupiec, 1993; hua Chen and Chen, 94; Fung, 1995; Evans and Zhai, 1996). It is also possible to focus on non-compositional compounds, a key point in bilingual applications (Su et al., 1994; Melamed, 1997; Lin, 99). Another interesting approach is to restrict sequences to those that do not cross constituent boundary patterns (Wu, 1995; Furuse and Iida, 96). In this study, we filtered for potential sequences that are likely to be noun phrases, using simple regular expressions over the associated part-of-speech tags. An excerpt of the association probabilities of a unit model trained considering only the NP-sequences is given in table 3. Applying this filter (referred to as .7.Np in the following) to the 39,093 english sequences still surviving after previous filters .7.1 and .F2 removes 35,939 of them (92%). 138 model spared ok good nu 1 baseline — model 1 48.98 0 0 747 0 2 baseline — model 2 51.83 0 0 747 0 3 Si + (2, 2, 0</context>
</contexts>
<marker>Wu, 1995</marker>
<rawString>Dekai Wu. 1995. Stochastic inversion transduction grammars, with application to segmentation, bracketing, and alignment of parallel corpora. In Proceedings of the International Joint Conference on Artificial Intelligence, volume 2, pages 1328-1335, Montréal, Canada.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>