<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.994862">
Word-for-Word Glossing with Contextually Similar Words
</title>
<author confidence="0.986018">
Patrick Pantel and Dekang Lin
</author>
<affiliation confidence="0.9983965">
Department of Computer Science
University of Manitoba
</affiliation>
<address confidence="0.665935">
Winnipeg, Manitoba R3T 2N2 Canada
</address>
<email confidence="0.972941">
{ppantel, lindek} @cs.umanitoba.ca
</email>
<sectionHeader confidence="0.997017" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999933818181818">
Many corpus-based machine translation
systems require parallel corpora. In this
paper, we present a word-for-word glossing
algorithm that requires only a source
language corpus. To gloss a word, we first
identify its similar words that occurred in
the same context in a large corpus. We then
determine the gloss by maximizing the
similarity between the set of contextually
similar words and the different translations
of the word in a bilingual thesaurus.
</bodyText>
<sectionHeader confidence="0.998772" genericHeader="keywords">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999932775">
Word-for-word glossing is the process of
directly translating each word or term in a
document without considering the word order.
Automating this process would benefit many
NLP applications. For example, in cross-
language information retrieval, glossing a
document often provides a sufficient translation
for humans to comprehend the key concepts.
Furthermore, a glossing algorithm can be used
for lexical selection in a full-fledged machine
translation (MT) system.
Many corpus-based MT systems require
parallel corpora (Brown et al., 1990; Brown et
al., 1991; Gale and Church, 1991; Resnik,
1999). Kikui (1999) used a word sense
disambiguation algorithm and a non-parallel
bilingual corpus to resolve translation
ambiguity.
In this paper, we present a word-for-word
glossing algorithm that requires only a source
language corpus. The intuitive idea behind our
algorithm is the following. Suppose w is a word
to be translated. We first identify a set of words
similar to w that occurred in the same context as
w in a large corpus. We then use this set (called
the contextually similar words of w) to select a
translation for w. For example, the contextually
similar words of duty in fiduciary duty include
responsibility, obligation, role, ... This list is
then used to select a translation for duty.
In the next section, we describe the resources
required by our algorithm. In Section 3, we
present an algorithm for constructing the
contextually similar words of a word in a
context. Section 4 presents the word-for-word
glossing algorithm and Section 5 describes the
group similarity metric used in our algorithm. In
Section 6, we present some experimental results
and finally, in Section 7, we conclude with a
discussion of future work.
</bodyText>
<sectionHeader confidence="0.999273" genericHeader="introduction">
2. Resources
</sectionHeader>
<bodyText confidence="0.9999035">
The input to our algorithm includes a collocation
database (Lin, 1998b) and a corpus-based
thesaurus (Lin, 1998a), which are both available
on the Internet&apos;. In addition, we require a
bilingual thesaurus. Below, we briefly describe
these resources.
</bodyText>
<subsectionHeader confidence="0.992939">
2.1. Collocation database
</subsectionHeader>
<bodyText confidence="0.999943666666667">
Given a word w in a dependency relationship
(such as subject or object), the collocation
database can be used to retrieve the words that
occurred in that relationship with w, in a large
corpus, along with their frequencies2. Figure 1
shows excerpts of the entries in the collocation
database for the words corporate, duty, and
fiduciary. The database contains a total of 11
million unique dependency relationships.
</bodyText>
<footnote confidence="0.9980238">
1 Available at www.cs.umanitoba.ca/-lindek/depdb.htm
and www.cs.umanitoba.ca/-lindek/simdb.htm
2 We use the term collocation to refer to a pair of
words that occur in a dependency relationship (rather
than the linear proximity of a pair of words).
</footnote>
<page confidence="0.996177">
78
</page>
<tableCaption confidence="0.9825195">
Table I. Clustered similar words of duty as given by (Lin,
1998a).
</tableCaption>
<table confidence="0.857727384615385">
CLUSTER CLUSTERED SIMILAR WORDS OF DUTY
(WITH SIMILARITY SCORE)
1 responsibility 0.16, obligation 0.109, task 0.101,
function 0.098, role 0.091, post 0.087, position
0.086, job 0.084, chore 0.08, mission 0.08,
assignment 0.079, liability 0.077, ...
2 tariff 0.091, restriction 0.089, tax 0.086,
regulation 0.085, requirement 0.081, procedure
0.079, penalty 0.079, quota 0.074, rule 0.07, levy
0.061, ...
3 fee 0.085, salary 0.081, pay 0.064, fine 0.058
4 personnel 0.073, staff 0.073
5 training 0.072, work 0.064, exercise 0.061
</table>
<sectionHeader confidence="0.448596" genericHeader="method">
6 privilege 0.069, right 0.057, license 0.056
</sectionHeader>
<subsectionHeader confidence="0.985638">
2.2. Corpus-based thesaurus
</subsectionHeader>
<bodyText confidence="0.999987888888889">
Using the collocation database, Lin used an
unsupervised method to construct a corpus-
based thesaurus (Lin, 1998a) consisting of
11839 nouns, 3639 verbs and 5658
adjectives/adverbs. Given a word w, the
thesaurus returns a clustered list of similar words
of w along with their similarity to w. For
example, the clustered similar words of duty are
shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.997623">
2.3. Bilingual thesaurus
</subsectionHeader>
<bodyText confidence="0.998225125">
Using the corpus-based thesaurus and a bilingual
dictionary, we manually constructed a bilingual
thesaurus. The entry for a source language word
w is constructed by manually associating one or
more clusters of similar words of w to each
candidate translation of w. We refer to the
assigned clusters as Words Associated with a
Translation (WAT). For example, Figure 2
shows an excerpt of our English/French
bilingual thesaurus for the words account and
duty.
Although the WAT assignment is a manual
process, it is a considerably easier task than
providing lexicographic definitions. Also, we
only require entries for source language words
that have multiple translations. In Section 7, we
</bodyText>
<construct confidence="0.914819">
corporate:
modifier-of: client 196, debt 236, development 179, fee 6,
function 16, headquarter 316, IOU 128, levy
3, liability 14, manager 203, market 195,
obligation 1, personnel 7, profit 595,
responsibility 27, rule 7, staff 113, tax 201,
training 2, vice president 231, ...
duty:
</construct>
<bodyText confidence="0.927086">
object-of: assume 177, breach Ill, carry out 71, do
114, have 257, impose 114, perform 151, ...
subject-of: affect 4, apply 6, include 42, involve 8, keep
5, officer 22, protect 8, require 13, ...
adj-modifier: active 202, additional 46, administrative 44,
</bodyText>
<construct confidence="0.816316333333333">
fiduciary 317, official 66, other 83, ...
fiduciary:
modifier-of: act 2, behavior 1, breach 2, claim 1,
company 2, duty 317, irresponsibility 2,
obligation 32, requirement 1, responsibility
89, role 2, ...
</construct>
<figureCaption confidence="0.995537">
Figure 1. Excepts of entries in the collocation database for
the words corporate, duty, and fiduciary.
Figure 1. Bilingual thesaurus entries for account and duty.
</figureCaption>
<bodyText confidence="0.98509">
discuss a method for automatically assigning the
WATs.
</bodyText>
<sectionHeader confidence="0.877531" genericHeader="method">
3. Contextually Similar Words
</sectionHeader>
<bodyText confidence="0.922059">
The contextually similar words of a word w are
words similar to the intended meaning of w in its
context. Figure 3 gives the data flow diagram for
our algorithm for identifying the contextually
similar words of w. Data are represented by
ovals, external resources by double ovals and
processes by rectangles.
By parsing a sentence with Minipar3, we
extract the dependency relationships involving
w. For each dependency relationship, we retrieve
</bodyText>
<listItem confidence="0.884395333333333">
3 Available at www.cs.umanitoba.ca/-lindek/minipar.htm
account:
1. compte:
</listItem>
<bodyText confidence="0.9199973125">
fund, deposit, loan, asset, portfolio,
investment, transaction, payment, saving,
money, contract, Budget, reserve, security,
contribution, debt, property, holding,
interest, bond, plan, business, ...
2. rapport: report, statement, testimony, card, story,
record, document, data, information, view,
check, figure, article, description, estimate,
assessment, number, statistic, comment,
letter, picture, note, ...
responsibility, obligation, task, function,
role, post, position, job, chore, mission,
assignment, liability, ...
tariff, restriction, tax, regulation,
requirement, procedure, penalty, quota, rule,
levy, ...
</bodyText>
<figure confidence="0.9656441">
duty:
I. devoir:
2. taxe:
\NAT for
Calltple
79
Input
)
(S
Conte tuallyimilar Words
</figure>
<figureCaption confidence="0.998724">
Figure 3. Data flow diagram for identifying the
contextually similar words of a word in context.
</figureCaption>
<bodyText confidence="0.818683772727273">
from the collocation database the words that
occurred in the same dependency relationship as
w. We refer to this set of words as the cohort of
w for that dependency relationship. Consider the
word duty in the contexts corporate duty and
fiduciary duty. The cohort of duty in corporate
duty consists of nouns modified by corporate in
Figure 1 (e.g. client, debt, development, ...) and
the cohort of duty in fiduciary duty consists of
nouns modified by fiduciary in Figure 1 (e.g.
act, behaviour, breach, ...).
Intersecting the set of similar words and the
cohort then forms the set of contextually similar
words of w. For example, Table 2 shows the
contextually similar words of duty in the
contexts corporate duty and fiduciary duty. The
words in the first row are retrieved by
intersecting the words in Table 1 with the nouns
modified by corporate in Figure 1. Similarly,
the second row represents the intersection of the
words in Table 1 and the nouns modified by
fiduciary in Figure 1.
</bodyText>
<tableCaption confidence="0.92726675">
The first set of contextually similar words in
Table 2 contains words that are similar to both
Table 2. The words similar to duty that occurred in the
contexts corporate duty and fiduciary duty.
</tableCaption>
<bodyText confidence="0.836774411764706">
CONTEXT CONTEXTUALLY SIMILAR WORDS OF DUTY
corporate duty
fee, function, levy, liability, obligation,
personnel, responsibility, rule, staff, tax,
training
Iduciary duty obligation, requirement, responsibility, role
the responsibility and tax senses of duty,
reflecting the fact that the meaning of duty is
indeed ambiguous if corporate duty is its sole
context. In contrast, the second row in Table 2
clearly indicates the responsibility sense of duty.
While previous word sense disambiguation
algorithms rely on a lexicon to provide sense
inventories of words, the contextually similar
words provide a way of distinguishing between
different senses of words without committing to
any particular sense inventory.
</bodyText>
<sectionHeader confidence="0.4998505" genericHeader="method">
4. Overview of the Word-for-Word
Glossing Algorithm
</sectionHeader>
<figureCaption confidence="0.970759">
Figure 4 illustrates the data flow of the word-
for-word glossing algorithm and Figure 5
describes it.
</figureCaption>
<bodyText confidence="0.998340090909091">
For example, suppose we wish to translate
into French the word duty in the context
corporate fiduciary duty. Step 1 retrieves the
candidate translations for duty and its WATs
from Figure 2. In Step 2, we construct two lists
of contextually similar words, one for the
dependency context corporate duty and one for
the dependency context fiduciary duty, shown in
Table 2. The proposed translation for the context
is obtained by maximizing the group similarities
between the lists of contextually similar words
and the WATs.
Using the group similarity measure from
Section 5, Table 3 lists the group similarity
scores between each list of contextually similar
words and each WAT as well as the final
combined score for each candidate translation.
The combined score for a candidate is the sum
of the logs of all group similarity scores
involving its WAT. The correct proposed
translation for duty in this context is devoir since
its WAT received the highest score.
</bodyText>
<page confidence="0.943944">
80
</page>
<figure confidence="0.920336">
Input
</figure>
<figureCaption confidence="0.8883149">
Figure 4. Data flow diagram for the word-for-word
glossing algorithm.
Input: A word w to be translated and a set of
dependency contexts involving w.
Step 1: Retrieve the candidate translations of w and
the corresponding WATs from the bilingual
thesaurus.
Step 2: Find the contextually similar words of w in
each dependency context using the algorithm
from Section 3.
</figureCaption>
<bodyText confidence="0.8309568">
Step 3: Compute the group similarity (see details in
Section 5) between each set of contextually
similar words and each WAT; the results are
stored in a matrix t, where t[i,j] is the group
similarity between the 1thlist of contextually
similar words and the jh WAT.
Step 4: Add the logs of the group similarity scores in
column of t to obtain a score for each WAT.
Output: The candidate translation corresponding to
the WAT with the highest score.
</bodyText>
<figureCaption confidence="0.984755">
Figure 5. The word-for-word glossing algorithm.
</figureCaption>
<sectionHeader confidence="0.98509" genericHeader="method">
5. Group Similarity
</sectionHeader>
<bodyText confidence="0.9970415">
The corpus-based thesaurus contains only the
similarities between individual pairs of words. In
our algorithm, we require the similarity between
groups of words. The group similarity measure
</bodyText>
<tableCaption confidence="0.9418415">
Table 3. Group similarity scores between the contextually
similar words of duty in corporate duty and fiduciary duty
</tableCaption>
<table confidence="0.953436833333333">
with the WATs for candidate translations devoir and taxe.
CANDIDATE CANDIDATE
DEVOIR TAXE
corporate duty 60.3704 16.569
fiduciary duty 51.2960 4.8325
Combined Score 8.0381 4.3829
</table>
<figureCaption confidence="0.9913634">
Figure 6. An example illustrating the difference between
the interconnectivity and closeness measures. The
interconnectivity in (a) and (b) remains constant while the
closeness in (a) is higher than in (b) since there are more
zero similarity pairs in (b).
</figureCaption>
<bodyText confidence="0.995296142857143">
we use is proposed by Karypis et al. (1999). It
takes as input two groups of elements, GI and
G2, and a similarity matrix, sim, which specifies
the similarity between individual elements. GI
and G2 are describable by graphs where the
vertices are the words and each weighted edge
between vertices -mil and w2 represents the
similarity, sim(wt, 1412), between the words w,
and w2.
Karypis et at. consider both the
interconnectivity and the closeness of the
groups. The absolute interconnectivity between
G, and G2, Al(Gt, G2), is defined as the aggregate
similarity between the two groups:
</bodyText>
<equation confidence="0.6402195">
AAGI , G2 )=-- si rn(x, y)
XE G1 ye
</equation>
<bodyText confidence="0.82160725">
The absolute closeness between G1 and G2,
A C(GI, G2), is defined as the average similarity
between a pair of elements, one from each
group:
</bodyText>
<equation confidence="0.388440666666667">
1
AC(GI, G2 ) AikGI 5 G2 I
IGI1G21
</equation>
<page confidence="0.994648">
81
</page>
<tableCaption confidence="0.996401">
Table 4. Candidate translations for each testing word along with their frequency of occurrence
in the test corpus.
</tableCaption>
<table confidence="0.9872515">
WORD CANDIDATE ENGLISH SENSE FREQUENCY OF
TRANSLATION OCCURRENCE
account compte bank account, business 245
duty rapport report, statement 55
race devoir responsibility, obligation 80
suit taxe tax 30
check course contest 87
record race racial group 23
proces lawsuit 281
costume garment 17
cheque draft, bank order 105
controle evaluation, verification 25
record unsurpassed statistic/performance 98
enregistrement recorded data or documentation 12
</table>
<bodyText confidence="0.998847666666667">
The difference between the absolute
interconnectivity and the absolute closeness is
that the latter takes zero similarity pairs into
account. In Figure 6, the interconnectivity in (a)
and (b) remains constant. However, the
closeness in (a) is higher than in (b) since there
are more zero similarity pairs in (b).
Karypis et al. normalized the absolute
interconnectivity and closeness by the internal
interconnectivity and closeness of the individual
groups. The normalized measures are referred to
as relative interconnectivity, RAG&apos;, G2), and
relative closeness, RC(Gt, G2). The internal
interconnectivity and closeness are obtained by
first computing a minimal edge bisection of
each group. An even-sized partition {G&apos;, G&amp;quot;} of
a group G is called a minimal edge bisection of
G if AI(G&apos;, G&amp;quot;) is minimal among all such
partitions. The internal interconnectivity of G,
II(G), is defined as II(G) = AI(G&apos;, G&amp;quot;) and the
internal closeness of G, IC(G), as IC(G) =
AC(G&apos;,G&amp;quot;).
Minimal edge bisection is performed for all
WATs and all sets of contextually similar words.
However, the minimal edge bisection problem is
NP-complete (Garey and Johnson, 1979).
Fortunately, state of the art graph partitioning
algorithms can approximate these bisections in
polynomial time (Goehring and Saad, 1994;
Karypis and Kumar, 1999; Kernighan and Lin,
1970). We used the same approximation
methods as in (Karypis et al., 1999).
The similarity between G1 and G2 is then
defined as follows:
groupSim(G„ G2 )=-- R/(GI , G2 )x RC(GI , G2)
where
</bodyText>
<equation confidence="0.9987095">
RAG„ G2 ) -7. 2AI(G1, G2)
11(Gi )1- II(G2 )
</equation>
<bodyText confidence="0.658239">
is the relative closeness.
</bodyText>
<sectionHeader confidence="0.996421" genericHeader="evaluation">
6. Experimental Results
</sectionHeader>
<bodyText confidence="0.9891243">
The design of our glossing algorithm is
applicable to any source/destination language
pair as long as a source language parser is
available. We considered English-to-French
translations in our experiments.
We experimented with six English nouns that
have multiple French translations: account, duty,
race, suit, check, and record. Using the 1987
Wall Street Journal files on the LDC/DCI CD-
is the relative interconnectivity and
</bodyText>
<equation confidence="0.98916175">
A C(G„ G2)
RC(G„ G2 ) —
IG&apos;1Ic(G, )+ IG21 Ic(G2 )
iGii ± IG21 IG, I + IG21
</equation>
<page confidence="0.984551">
82
</page>
<bodyText confidence="0.99994656097561">
ROM, we extracted a testing corpus4 consisting
of the first 100 to 300 sentences containing the
non-idiomatic usage of the six nounss. Then, we
manually tagged each sentence with one of the
candidate translations shown in Table 4.
Each noun in Table 4 translates more
frequently to one candidate translation than the
other. In fact, always choosing the candidate
proces as the translation for suit yields 94%
accuracy. A better measure for evaluating the
system&apos;s classifications considers both the
algorithm&apos;s precision and recall on each
candidate translation. Table 5 illustrates the
precision and recall of our glossing algorithm for
each candidate translation. Albeit precision and
recall are used to evaluate the quality of the
classifications, overall accuracy is sufficient for
comparing different approaches with our system.
In Section 3, we presented an algorithm for
identifying the contextually similar words of a
word in a context using a corpus-based thesaurus
and a collocation database. Each of the six nouns
has similar words in the corpus-based thesaurus.
However, in order to find contextually similar
words, at least one similar word for each noun
must occur in the collocation database in a given
context. Thus, the algorithm for constructing
contextually similar words is dependent on the
coverage of the collocation database. We
estimated this coverage by counting the number
of times each of the six nouns, in several
different contexts, has at least one contextually
similar word. The result is shown in Table 6.
In Section 5, we described a group similarity
metric, groupSim, which we use for comparing a
WAT with a set of contextually similar words.
In Figure 7, we compare the translation accuracy
of our algorithm using other group similarity
metrics. Suppose GI and G2 are two groups of
words and w is the word that we wish to
translate. The metrics used are:
</bodyText>
<listItem confidence="0.857992">
1. closest3:
</listItem>
<bodyText confidence="0.5183325">
sum of similarity of the three closest
pairs of words from each group.
</bodyText>
<footnote confidence="0.48956375">
4 Available at ftp.cs.umanitoba.ca/pub/ppantel/
download/wfwgtest.zip
5 Omitted idiomatic phrases include take into
account, keep in check, check out, ...
</footnote>
<tableCaption confidence="0.999522">
Table 5. Precision vs. Recall for each candidate translation.
</tableCaption>
<table confidence="0.999937846153846">
WORD CANDIDATE PRECISION RECALL
account compte 0.982 0.902
rapport 0.680 0.927
duty devoir 0.951 0.963
taxe 0.897 0.867
race course 0.945 0.989
race 0.947 0.783
Suit proces 0.996 0.993
costume 0.889 0.941
check cheque 0.951 0.924
controle 0.714 0.800
record record 0.968 0.918
enregistrement 0.529 0.750
</table>
<tableCaption confidence="0.984280666666667">
Table 6. The coverage of the collocation database, shown
by the frequency with which a word in a given context has
at least one contextually similar word.
</tableCaption>
<figure confidence="0.8860954">
WORD
account
duty
race
suit
check
record
2. gs:
Esim(x, .)+ si m(y, w)
AG, AG,
</figure>
<listItem confidence="0.8461495">
3. AC:
as defined in Section 5.
4. Al:
as defined in Section 5.
5. RC:
as defined in Section 5.
6. RI:
as defined in Section 5.
</listItem>
<figure confidence="0.61081159375">
NUMBER OF COVERAGE
CONTEXTS
1074 95.7%
343 93.3%
294 92.5%
332 91.9%
2519 87.5%
1655 92.8%
Esim(x, ox max si m(x, y)+ Isim(y, w)x max sim(y, x)
A.,
AG,
83
Group Similarity Comparison
1.0
0.9
0.8 1 I U 1 all till ill II Ii • Iii III 1 Ill. II 1111-1 I
0.7 ill • I i 1 I 1 I
0.6 III II 111 II
0.5 Iiiii Ii 1 Ii
0.4 111 i II
0.3 II ii
0.2 I I Ii
0.1
0.01
groupSim moslFreguent closest3 gs AC Al RC RI
D account 0.9067 0.8167 0.5900 0.8933 0.9000 0.9100 0.9067 0.9000
• duty 0.9364 0.7273 0.8818 0.9091 0.7091 0.7545 0.9364 0.9182
a race 0.9455 0.7909 0.8818 0.9364 0.9455 0.9000 0.9273 0.9091
1:Isuit 0.9899 0.9367 0.9664 0.9832 0.9765 0.9161 0.9799 0.9832
• check 0.9000 0.8077 0.8615 0.8923 0.8385 0.9077 0.9077 0.8615
a recorcl 0.9000 0.8909 0.1636 0.7545 0.7273 0.1091 0.6909 0.8818
• average 0.9297 0.8284 0.7242 0.8948 0.8495 0.7496 0.8915 0.9090
</figure>
<figureCaption confidence="0.999713">
Figure 7. Performance comparison of different group similarity metrics.
</figureCaption>
<bodyText confidence="0.999979555555556">
In mostFrequent, we include the results
obtained if we always choose the translation that
occurs most frequently in the testing corpus.
We also compared the accuracy of our
glossing algorithm with Systran&apos;s translation
system by feeding the testing sentences into
Systran&apos;s web interface6 and manually
examining the results. Figure 8 summarizes the
overall accuracy obtained by each system and
the baseline on the testing corpus. Systran
tended to prefer one candidate translation over
the other and committed the majority of its
errors on the non-preferred senses.
Consequently, Systran is very accurate if its
preferred sense is the frequent sense (as in
account and duty) but is very inaccurate if its
preferred sense is the infrequent one (as in race,
suit, and check).
</bodyText>
<sectionHeader confidence="0.962559" genericHeader="conclusions">
7. Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.9999354">
This paper presents a word-for-word glossing
algorithm. The gloss of a word is determined by
maximizing the similarity between the set of
contextually similar words and the different
translations of the word in a bilingual thesaurus.
</bodyText>
<page confidence="0.487193">
6 Available at babelfish.altavista.com/cgi-bin/translate
</page>
<bodyText confidence="0.999934071428571">
The algorithm presented in this paper can be
improved and extended in many ways. At
present, our glossing algorithm does not take the
prior probabilities of translations into account.
For example, in WSJ, the bank account sense of
account is much more common than the report
sense. We should thus tend to prefer this sense
of account. This is achievable by weighting the
translation scores by the prior probabilities of
the translations. We are investigating an
Expectation-Maximization (EM) (Dempster et
al., 1977) algorithm to learn these prior
probabilities. Initially, we assume that the
candidate translations for a word are uniformly
distributed. After glossing each word in a large
corpus, we refine the prior probabilities using
the frequency counts obtained. This process is
repeated several times until the empirical prior
probabilities closely approximate the true prior
probabilities.
Finally, as discussed in Section 2.3,
automatically constructing the bilingual
thesaurus is necessary to gloss whole
documents. This is attainable by adding a
corpus-based destination language thesaurus to
our system. The process of assigning a cluster of
similar words as a WAT to a candidate
translation c is as follows. First, we
</bodyText>
<page confidence="0.997802">
84
</page>
<figureCaption confidence="0.999389">
Figure 8. Performance comparison of the word-for-word glossing algorithm and Systran.
</figureCaption>
<figure confidence="0.978115545454546">
Word-for-Word Glossing vs. Systran
account duty race suit check record
0.8167 0.7273 0.7909 0.9367 0.8077 0.8909
0.8567 0.8818 0.0727 0.3167 0,4231 0.6636
0.9067 0.9364 0.9455 0.9899 0.9000 0.9000
0 Glossing
0 mostFrequent
• Systran
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
o mostFrequent
• Systran
o Glossing
</figure>
<bodyText confidence="0.999733777777778">
automatically obtain the candidate translations
for a word using a bilingual dictionary. With the
destination language thesaurus, we obtain a list S
of all words similar to c. With the bilingual
dictionary, replace each word in S by its source
language translations. Using the group similarity
metric from Section 5, assign as the WAT the
cluster of similar words (obtained from the
source language thesaurus) most similar to S.
</bodyText>
<sectionHeader confidence="0.996004" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9998044">
The authors wish to thank the reviewers for their
helpful comments. This research was partly
supported by Natural Sciences and Engineering
Research Council of Canada grants OGP121338
and PGSA207797.
</bodyText>
<sectionHeader confidence="0.999275" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999889591836735">
Peter F. Brown; John Cocke; Stephen A. Della Pietra;
Vincent J. Della Pietra; Fredrick Jelinek; John D.
Lafferty; Robert L. Mercer and Paul S. Roossin.
1990. A Statistical Approach to Machine
Translation. Computation Linguistics, 16(2).
Peter F. Brown; Jennifer C. Lai and Robert L.
Mercer. 1991. Aligning Sentences in Parallel
Corpora. In Proceedings of ACL91. Berkeley.
A. P. Dempster; N. M. Laird; &amp; D. B. Rubin. 1977.
Maximum likelihood from incomplete data via the
EM algorithm. Journal of the Royal Statistical
Society, Series B, 39(1).
W. A. Gale and K. W. Church. 1991. A Program for
Aligning Sentences in Bilingual Corpora. In
Proceedings of ACL91. Berkeley.
M. R. Garey and D. S. Johnson. 1979. Computers
and Intractability: A Guide to the Theory of NP-
Completeness.W W. H. Freeman.
T. Goehring and Y. Saad. 1994. Heuristic Algorithms
for Automatic Graph Partitioning. Technical
Report. Department of Computer Science,
University of Minnesota.
George Karypis and Vipin Kumar. 1999. A Fast and
High Quality Multilevel Scheme for Partitioning
Irregular Graphs. SIAM Journal on Scientific
Computing, 20(1).
George Karypis; Eui-Hong Han and Vipin Kumar.
1999. Chameleon: A Hierarchical Clustering
Algorithm Using Dynamic Modeling. IEEE
Computer: Special Issue on Data Analysis and
Mining, 32(8). http://www-users.cs.umn.edu/
-4carypis/publications/Papers/PDF/chameleon.pdf
B. W. Kernighan and S. Lin. 1970. An Efficient
Heuristic Procedure for Partitioning Graphs. The
Bell System Technical Journal.
Genichiro Kikui. 1999. Resolving Translation
ambiguity using Non-parallel Bilingual Corpora.
In Proceedings of ACL99 Workshop on
Unsupervised Learning in Natural Language
Processing.
Dekang Lin. 1998a. Automatic Retrieval and
Clustering of Similar Words. In Proceedings of
COLING-ACL98. Montreal, Canada.
Dekang Lin. 1998b. Extracting Collocations from
Text Corpora. Workshop on Computational
Terminology. Montreal, Canada.
Philip Resnik. 1999. Mining the Web for Bilingual
Text. In Proceedings of ACL99. College Park,
Maryland.
</reference>
<page confidence="0.9997">
85
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.981907">
<title confidence="0.99982">Word-for-Word Glossing with Contextually Similar Words</title>
<author confidence="0.999285">Patrick Pantel</author>
<author confidence="0.999285">Dekang Lin</author>
<affiliation confidence="0.9998795">Department of Computer Science University of Manitoba</affiliation>
<address confidence="0.992263">Winnipeg, Manitoba R3T 2N2 Canada</address>
<email confidence="0.993801">ppantel@cs.umanitoba.ca</email>
<email confidence="0.993801">lindek@cs.umanitoba.ca</email>
<abstract confidence="0.999721166666667">Many corpus-based machine translation systems require parallel corpora. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. To gloss a word, we first identify its similar words that occurred in the same context in a large corpus. We then determine the gloss by maximizing the similarity between the set of contextually similar words and the different translations of the word in a bilingual thesaurus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>John Cocke</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Fredrick Jelinek</author>
<author>John D Lafferty</author>
<author>Robert L Mercer</author>
<author>Paul S Roossin</author>
</authors>
<title>A Statistical Approach to Machine Translation.</title>
<date>1990</date>
<journal>Computation Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1228" citStr="Brown et al., 1990" startWordPosition="174" endWordPosition="177">e different translations of the word in a bilingual thesaurus. 1. Introduction Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-parallel bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example</context>
</contexts>
<marker>Brown, Cocke, Pietra, Pietra, Jelinek, Lafferty, Mercer, Roossin, 1990</marker>
<rawString>Peter F. Brown; John Cocke; Stephen A. Della Pietra; Vincent J. Della Pietra; Fredrick Jelinek; John D. Lafferty; Robert L. Mercer and Paul S. Roossin. 1990. A Statistical Approach to Machine Translation. Computation Linguistics, 16(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Aligning Sentences in Parallel Corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL91.</booktitle>
<location>Berkeley.</location>
<contexts>
<context position="1248" citStr="Brown et al., 1991" startWordPosition="178" endWordPosition="181">ions of the word in a bilingual thesaurus. 1. Introduction Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-parallel bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually s</context>
</contexts>
<marker>Brown, Lai, Mercer, 1991</marker>
<rawString>Peter F. Brown; Jennifer C. Lai and Robert L. Mercer. 1991. Aligning Sentences in Parallel Corpora. In Proceedings of ACL91. Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum likelihood from incomplete data via the EM algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="20970" citStr="Dempster et al., 1977" startWordPosition="3320" endWordPosition="3323">ranslations of the word in a bilingual thesaurus. 6 Available at babelfish.altavista.com/cgi-bin/translate The algorithm presented in this paper can be improved and extended in many ways. At present, our glossing algorithm does not take the prior probabilities of translations into account. For example, in WSJ, the bank account sense of account is much more common than the report sense. We should thus tend to prefer this sense of account. This is achievable by weighting the translation scores by the prior probabilities of the translations. We are investigating an Expectation-Maximization (EM) (Dempster et al., 1977) algorithm to learn these prior probabilities. Initially, we assume that the candidate translations for a word are uniformly distributed. After glossing each word in a large corpus, we refine the prior probabilities using the frequency counts obtained. This process is repeated several times until the empirical prior probabilities closely approximate the true prior probabilities. Finally, as discussed in Section 2.3, automatically constructing the bilingual thesaurus is necessary to gloss whole documents. This is attainable by adding a corpus-based destination language thesaurus to our system. </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster; N. M. Laird; &amp; D. B. Rubin. 1977. Maximum likelihood from incomplete data via the EM algorithm. Journal of the Royal Statistical Society, Series B, 39(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
</authors>
<title>A Program for Aligning Sentences in Bilingual Corpora.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL91.</booktitle>
<location>Berkeley.</location>
<contexts>
<context position="1271" citStr="Gale and Church, 1991" startWordPosition="182" endWordPosition="185">a bilingual thesaurus. 1. Introduction Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-parallel bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>W. A. Gale and K. W. Church. 1991. A Program for Aligning Sentences in Bilingual Corpora. In Proceedings of ACL91. Berkeley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M R Garey</author>
<author>D S Johnson</author>
</authors>
<date>1979</date>
<booktitle>Computers and Intractability: A Guide to the Theory of NPCompleteness.W</booktitle>
<contexts>
<context position="14608" citStr="Garey and Johnson, 1979" startWordPosition="2275" endWordPosition="2278">vity, RAG&apos;, G2), and relative closeness, RC(Gt, G2). The internal interconnectivity and closeness are obtained by first computing a minimal edge bisection of each group. An even-sized partition {G&apos;, G&amp;quot;} of a group G is called a minimal edge bisection of G if AI(G&apos;, G&amp;quot;) is minimal among all such partitions. The internal interconnectivity of G, II(G), is defined as II(G) = AI(G&apos;, G&amp;quot;) and the internal closeness of G, IC(G), as IC(G) = AC(G&apos;,G&amp;quot;). Minimal edge bisection is performed for all WATs and all sets of contextually similar words. However, the minimal edge bisection problem is NP-complete (Garey and Johnson, 1979). Fortunately, state of the art graph partitioning algorithms can approximate these bisections in polynomial time (Goehring and Saad, 1994; Karypis and Kumar, 1999; Kernighan and Lin, 1970). We used the same approximation methods as in (Karypis et al., 1999). The similarity between G1 and G2 is then defined as follows: groupSim(G„ G2 )=-- R/(GI , G2 )x RC(GI , G2) where RAG„ G2 ) -7. 2AI(G1, G2) 11(Gi )1- II(G2 ) is the relative closeness. 6. Experimental Results The design of our glossing algorithm is applicable to any source/destination language pair as long as a source language parser is av</context>
</contexts>
<marker>Garey, Johnson, 1979</marker>
<rawString>M. R. Garey and D. S. Johnson. 1979. Computers and Intractability: A Guide to the Theory of NPCompleteness.W W. H. Freeman.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Goehring</author>
<author>Y Saad</author>
</authors>
<title>Heuristic Algorithms for Automatic Graph Partitioning.</title>
<date>1994</date>
<tech>Technical Report.</tech>
<institution>Department of Computer Science, University of Minnesota.</institution>
<contexts>
<context position="14746" citStr="Goehring and Saad, 1994" startWordPosition="2294" endWordPosition="2297"> edge bisection of each group. An even-sized partition {G&apos;, G&amp;quot;} of a group G is called a minimal edge bisection of G if AI(G&apos;, G&amp;quot;) is minimal among all such partitions. The internal interconnectivity of G, II(G), is defined as II(G) = AI(G&apos;, G&amp;quot;) and the internal closeness of G, IC(G), as IC(G) = AC(G&apos;,G&amp;quot;). Minimal edge bisection is performed for all WATs and all sets of contextually similar words. However, the minimal edge bisection problem is NP-complete (Garey and Johnson, 1979). Fortunately, state of the art graph partitioning algorithms can approximate these bisections in polynomial time (Goehring and Saad, 1994; Karypis and Kumar, 1999; Kernighan and Lin, 1970). We used the same approximation methods as in (Karypis et al., 1999). The similarity between G1 and G2 is then defined as follows: groupSim(G„ G2 )=-- R/(GI , G2 )x RC(GI , G2) where RAG„ G2 ) -7. 2AI(G1, G2) 11(Gi )1- II(G2 ) is the relative closeness. 6. Experimental Results The design of our glossing algorithm is applicable to any source/destination language pair as long as a source language parser is available. We considered English-to-French translations in our experiments. We experimented with six English nouns that have multiple French</context>
</contexts>
<marker>Goehring, Saad, 1994</marker>
<rawString>T. Goehring and Y. Saad. 1994. Heuristic Algorithms for Automatic Graph Partitioning. Technical Report. Department of Computer Science, University of Minnesota.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Karypis</author>
<author>Vipin Kumar</author>
</authors>
<title>A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs.</title>
<date>1999</date>
<journal>SIAM Journal on Scientific Computing,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="14771" citStr="Karypis and Kumar, 1999" startWordPosition="2298" endWordPosition="2301">roup. An even-sized partition {G&apos;, G&amp;quot;} of a group G is called a minimal edge bisection of G if AI(G&apos;, G&amp;quot;) is minimal among all such partitions. The internal interconnectivity of G, II(G), is defined as II(G) = AI(G&apos;, G&amp;quot;) and the internal closeness of G, IC(G), as IC(G) = AC(G&apos;,G&amp;quot;). Minimal edge bisection is performed for all WATs and all sets of contextually similar words. However, the minimal edge bisection problem is NP-complete (Garey and Johnson, 1979). Fortunately, state of the art graph partitioning algorithms can approximate these bisections in polynomial time (Goehring and Saad, 1994; Karypis and Kumar, 1999; Kernighan and Lin, 1970). We used the same approximation methods as in (Karypis et al., 1999). The similarity between G1 and G2 is then defined as follows: groupSim(G„ G2 )=-- R/(GI , G2 )x RC(GI , G2) where RAG„ G2 ) -7. 2AI(G1, G2) 11(Gi )1- II(G2 ) is the relative closeness. 6. Experimental Results The design of our glossing algorithm is applicable to any source/destination language pair as long as a source language parser is available. We considered English-to-French translations in our experiments. We experimented with six English nouns that have multiple French translations: account, d</context>
</contexts>
<marker>Karypis, Kumar, 1999</marker>
<rawString>George Karypis and Vipin Kumar. 1999. A Fast and High Quality Multilevel Scheme for Partitioning Irregular Graphs. SIAM Journal on Scientific Computing, 20(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Karypis</author>
<author>Eui-Hong Han</author>
<author>Vipin Kumar</author>
</authors>
<title>Chameleon: A Hierarchical Clustering Algorithm Using Dynamic Modeling.</title>
<date>1999</date>
<journal>IEEE Computer: Special Issue on Data Analysis and Mining,</journal>
<volume>32</volume>
<issue>8</issue>
<note>http://www-users.cs.umn.edu/ -4carypis/publications/Papers/PDF/chameleon.pdf</note>
<contexts>
<context position="12136" citStr="Karypis et al. (1999)" startWordPosition="1883" endWordPosition="1886">group similarity measure Table 3. Group similarity scores between the contextually similar words of duty in corporate duty and fiduciary duty with the WATs for candidate translations devoir and taxe. CANDIDATE CANDIDATE DEVOIR TAXE corporate duty 60.3704 16.569 fiduciary duty 51.2960 4.8325 Combined Score 8.0381 4.3829 Figure 6. An example illustrating the difference between the interconnectivity and closeness measures. The interconnectivity in (a) and (b) remains constant while the closeness in (a) is higher than in (b) since there are more zero similarity pairs in (b). we use is proposed by Karypis et al. (1999). It takes as input two groups of elements, GI and G2, and a similarity matrix, sim, which specifies the similarity between individual elements. GI and G2 are describable by graphs where the vertices are the words and each weighted edge between vertices -mil and w2 represents the similarity, sim(wt, 1412), between the words w, and w2. Karypis et at. consider both the interconnectivity and the closeness of the groups. The absolute interconnectivity between G, and G2, Al(Gt, G2), is defined as the aggregate similarity between the two groups: AAGI , G2 )=-- si rn(x, y) XE G1 ye The absolute close</context>
<context position="14866" citStr="Karypis et al., 1999" startWordPosition="2314" endWordPosition="2317">(G&apos;, G&amp;quot;) is minimal among all such partitions. The internal interconnectivity of G, II(G), is defined as II(G) = AI(G&apos;, G&amp;quot;) and the internal closeness of G, IC(G), as IC(G) = AC(G&apos;,G&amp;quot;). Minimal edge bisection is performed for all WATs and all sets of contextually similar words. However, the minimal edge bisection problem is NP-complete (Garey and Johnson, 1979). Fortunately, state of the art graph partitioning algorithms can approximate these bisections in polynomial time (Goehring and Saad, 1994; Karypis and Kumar, 1999; Kernighan and Lin, 1970). We used the same approximation methods as in (Karypis et al., 1999). The similarity between G1 and G2 is then defined as follows: groupSim(G„ G2 )=-- R/(GI , G2 )x RC(GI , G2) where RAG„ G2 ) -7. 2AI(G1, G2) 11(Gi )1- II(G2 ) is the relative closeness. 6. Experimental Results The design of our glossing algorithm is applicable to any source/destination language pair as long as a source language parser is available. We considered English-to-French translations in our experiments. We experimented with six English nouns that have multiple French translations: account, duty, race, suit, check, and record. Using the 1987 Wall Street Journal files on the LDC/DCI CDi</context>
</contexts>
<marker>Karypis, Han, Kumar, 1999</marker>
<rawString>George Karypis; Eui-Hong Han and Vipin Kumar. 1999. Chameleon: A Hierarchical Clustering Algorithm Using Dynamic Modeling. IEEE Computer: Special Issue on Data Analysis and Mining, 32(8). http://www-users.cs.umn.edu/ -4carypis/publications/Papers/PDF/chameleon.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>B W Kernighan</author>
<author>S Lin</author>
</authors>
<title>An Efficient Heuristic Procedure for Partitioning Graphs. The Bell System</title>
<date>1970</date>
<tech>Technical Journal.</tech>
<contexts>
<context position="14797" citStr="Kernighan and Lin, 1970" startWordPosition="2302" endWordPosition="2305">tion {G&apos;, G&amp;quot;} of a group G is called a minimal edge bisection of G if AI(G&apos;, G&amp;quot;) is minimal among all such partitions. The internal interconnectivity of G, II(G), is defined as II(G) = AI(G&apos;, G&amp;quot;) and the internal closeness of G, IC(G), as IC(G) = AC(G&apos;,G&amp;quot;). Minimal edge bisection is performed for all WATs and all sets of contextually similar words. However, the minimal edge bisection problem is NP-complete (Garey and Johnson, 1979). Fortunately, state of the art graph partitioning algorithms can approximate these bisections in polynomial time (Goehring and Saad, 1994; Karypis and Kumar, 1999; Kernighan and Lin, 1970). We used the same approximation methods as in (Karypis et al., 1999). The similarity between G1 and G2 is then defined as follows: groupSim(G„ G2 )=-- R/(GI , G2 )x RC(GI , G2) where RAG„ G2 ) -7. 2AI(G1, G2) 11(Gi )1- II(G2 ) is the relative closeness. 6. Experimental Results The design of our glossing algorithm is applicable to any source/destination language pair as long as a source language parser is available. We considered English-to-French translations in our experiments. We experimented with six English nouns that have multiple French translations: account, duty, race, suit, check, an</context>
</contexts>
<marker>Kernighan, Lin, 1970</marker>
<rawString>B. W. Kernighan and S. Lin. 1970. An Efficient Heuristic Procedure for Partitioning Graphs. The Bell System Technical Journal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Genichiro Kikui</author>
</authors>
<title>Resolving Translation ambiguity using Non-parallel Bilingual Corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL99 Workshop on Unsupervised Learning in Natural Language Processing.</booktitle>
<contexts>
<context position="1300" citStr="Kikui (1999)" startWordPosition="188" endWordPosition="189"> Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-parallel bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in fiduciary duty include respo</context>
</contexts>
<marker>Kikui, 1999</marker>
<rawString>Genichiro Kikui. 1999. Resolving Translation ambiguity using Non-parallel Bilingual Corpora. In Proceedings of ACL99 Workshop on Unsupervised Learning in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING-ACL98.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2511" citStr="Lin, 1998" startWordPosition="387" endWordPosition="388">ibility, obligation, role, ... This list is then used to select a translation for duty. In the next section, we describe the resources required by our algorithm. In Section 3, we present an algorithm for constructing the contextually similar words of a word in a context. Section 4 presents the word-for-word glossing algorithm and Section 5 describes the group similarity metric used in our algorithm. In Section 6, we present some experimental results and finally, in Section 7, we conclude with a discussion of future work. 2. Resources The input to our algorithm includes a collocation database (Lin, 1998b) and a corpus-based thesaurus (Lin, 1998a), which are both available on the Internet&apos;. In addition, we require a bilingual thesaurus. Below, we briefly describe these resources. 2.1. Collocation database Given a word w in a dependency relationship (such as subject or object), the collocation database can be used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies2. Figure 1 shows excerpts of the entries in the collocation database for the words corporate, duty, and fiduciary. The database contains a total of 11 million unique depen</context>
<context position="4164" citStr="Lin, 1998" startWordPosition="636" endWordPosition="637">task 0.101, function 0.098, role 0.091, post 0.087, position 0.086, job 0.084, chore 0.08, mission 0.08, assignment 0.079, liability 0.077, ... 2 tariff 0.091, restriction 0.089, tax 0.086, regulation 0.085, requirement 0.081, procedure 0.079, penalty 0.079, quota 0.074, rule 0.07, levy 0.061, ... 3 fee 0.085, salary 0.081, pay 0.064, fine 0.058 4 personnel 0.073, staff 0.073 5 training 0.072, work 0.064, exercise 0.061 6 privilege 0.069, right 0.057, license 0.056 2.2. Corpus-based thesaurus Using the collocation database, Lin used an unsupervised method to construct a corpusbased thesaurus (Lin, 1998a) consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs. Given a word w, the thesaurus returns a clustered list of similar words of w along with their similarity to w. For example, the clustered similar words of duty are shown in Table 1. 2.3. Bilingual thesaurus Using the corpus-based thesaurus and a bilingual dictionary, we manually constructed a bilingual thesaurus. The entry for a source language word w is constructed by manually associating one or more clusters of similar words of w to each candidate translation of w. We refer to the assigned clusters as Words Associated with</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998a. Automatic Retrieval and Clustering of Similar Words. In Proceedings of COLING-ACL98. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<date>1998</date>
<booktitle>Extracting Collocations from Text Corpora. Workshop on Computational Terminology.</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2511" citStr="Lin, 1998" startWordPosition="387" endWordPosition="388">ibility, obligation, role, ... This list is then used to select a translation for duty. In the next section, we describe the resources required by our algorithm. In Section 3, we present an algorithm for constructing the contextually similar words of a word in a context. Section 4 presents the word-for-word glossing algorithm and Section 5 describes the group similarity metric used in our algorithm. In Section 6, we present some experimental results and finally, in Section 7, we conclude with a discussion of future work. 2. Resources The input to our algorithm includes a collocation database (Lin, 1998b) and a corpus-based thesaurus (Lin, 1998a), which are both available on the Internet&apos;. In addition, we require a bilingual thesaurus. Below, we briefly describe these resources. 2.1. Collocation database Given a word w in a dependency relationship (such as subject or object), the collocation database can be used to retrieve the words that occurred in that relationship with w, in a large corpus, along with their frequencies2. Figure 1 shows excerpts of the entries in the collocation database for the words corporate, duty, and fiduciary. The database contains a total of 11 million unique depen</context>
<context position="4164" citStr="Lin, 1998" startWordPosition="636" endWordPosition="637">task 0.101, function 0.098, role 0.091, post 0.087, position 0.086, job 0.084, chore 0.08, mission 0.08, assignment 0.079, liability 0.077, ... 2 tariff 0.091, restriction 0.089, tax 0.086, regulation 0.085, requirement 0.081, procedure 0.079, penalty 0.079, quota 0.074, rule 0.07, levy 0.061, ... 3 fee 0.085, salary 0.081, pay 0.064, fine 0.058 4 personnel 0.073, staff 0.073 5 training 0.072, work 0.064, exercise 0.061 6 privilege 0.069, right 0.057, license 0.056 2.2. Corpus-based thesaurus Using the collocation database, Lin used an unsupervised method to construct a corpusbased thesaurus (Lin, 1998a) consisting of 11839 nouns, 3639 verbs and 5658 adjectives/adverbs. Given a word w, the thesaurus returns a clustered list of similar words of w along with their similarity to w. For example, the clustered similar words of duty are shown in Table 1. 2.3. Bilingual thesaurus Using the corpus-based thesaurus and a bilingual dictionary, we manually constructed a bilingual thesaurus. The entry for a source language word w is constructed by manually associating one or more clusters of similar words of w to each candidate translation of w. We refer to the assigned clusters as Words Associated with</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998b. Extracting Collocations from Text Corpora. Workshop on Computational Terminology. Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Mining the Web for Bilingual Text.</title>
<date>1999</date>
<booktitle>In Proceedings of ACL99. College Park,</booktitle>
<location>Maryland.</location>
<contexts>
<context position="1286" citStr="Resnik, 1999" startWordPosition="186" endWordPosition="187">1. Introduction Word-for-word glossing is the process of directly translating each word or term in a document without considering the word order. Automating this process would benefit many NLP applications. For example, in crosslanguage information retrieval, glossing a document often provides a sufficient translation for humans to comprehend the key concepts. Furthermore, a glossing algorithm can be used for lexical selection in a full-fledged machine translation (MT) system. Many corpus-based MT systems require parallel corpora (Brown et al., 1990; Brown et al., 1991; Gale and Church, 1991; Resnik, 1999). Kikui (1999) used a word sense disambiguation algorithm and a non-parallel bilingual corpus to resolve translation ambiguity. In this paper, we present a word-for-word glossing algorithm that requires only a source language corpus. The intuitive idea behind our algorithm is the following. Suppose w is a word to be translated. We first identify a set of words similar to w that occurred in the same context as w in a large corpus. We then use this set (called the contextually similar words of w) to select a translation for w. For example, the contextually similar words of duty in fiduciary duty</context>
</contexts>
<marker>Resnik, 1999</marker>
<rawString>Philip Resnik. 1999. Mining the Web for Bilingual Text. In Proceedings of ACL99. College Park, Maryland.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>