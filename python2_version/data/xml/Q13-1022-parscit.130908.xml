<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.996663">
Efficient Parsing for Head-Split Dependency Trees
</title>
<author confidence="0.999459">
Giorgio Satta Marco Kuhlmann
</author>
<affiliation confidence="0.9994705">
Dept. of Information Engineering Dept. of Linguistics and Philology
University of Padua, Italy Uppsala University, Sweden
</affiliation>
<email confidence="0.993707">
satta@dei.unipd.it marco.kuhlmann@lingfil.uu.se
</email>
<sectionHeader confidence="0.994633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997556615384615">
Head splitting techniques have been success-
fully exploited to improve the asymptotic
runtime of parsing algorithms for project-
ive dependency trees, under the arc-factored
model. In this article we extend these tech-
niques to a class of non-projective dependency
trees, called well-nested dependency trees with
block-degree at most 2, which has been previ-
ously investigated in the literature. We define a
structural property that allows head splitting for
these trees, and present two algorithms that im-
prove over the runtime of existing algorithms
at no significant loss in coverage.
</bodyText>
<sectionHeader confidence="0.998775" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999955563636364">
Much of the recent work on dependency parsing has
been aimed at finding a good balance between ac-
curacy and efficiency. For one end of the spectrum,
Eisner (1997) showed that the highest-scoring pro-
jective dependency tree under an arc-factored model
can be computed in time O(n3), where n is the length
of the input string. Later work has focused on mak-
ing projective parsing viable under more expressive
models (Carreras, 2007; Koo and Collins, 2010).
At the same time, it has been observed that for
many standard data sets, the coverage of projective
trees is far from complete (Kuhlmann and Nivre,
2006), which has led to an interest in parsing al-
gorithms for non-projective trees. While non-project-
ive parsing under an arc-factored model can be done
in time O(n2) (McDonald et al., 2005), parsing with
more informed models is intractable (McDonald and
Satta, 2007). This has led several authors to investig-
ate ‘mildly non-projective’ classes of trees, with the
goal of achieving a balance between expressiveness
and complexity (Kuhlmann and Nivre, 2006).
In this article we focus on a class of mildly non-
projective dependency structures called well-nested
dependency trees with block-degree at most 2. This
class was first introduced by Bodirsky et al. (2005),
who showed that it corresponds, in a natural way, to
the class of derivation trees of lexicalized tree-adjoin-
ing grammars (Joshi and Schabes, 1997). While there
are linguistic arguments against the restriction to this
class (Maier and Lichte, 2011; Chen-Main and Joshi,
2010), Kuhlmann and Nivre (2006) found that it has
excellent coverage on standard data sets. Assum-
ing an arc-factored model, well-nested dependency
trees with block-degree &lt; 2 can be parsed in time
O(n7) using the algorithm of G´omez-Rodr´ıguez et
al. (2011). Recently, Pitler et al. (2012) have shown
that if an additional restriction called 1-inherit is im-
posed, parsing can be done in time O(n6), without
any additional loss in coverage on standard data sets.
Standard context-free parsing methods, when adap-
ted to the parsing of projective trees, provide O(n5)
time complexity. The O(n3) time result reported by
Eisner (1997) has been obtained by exploiting more
sophisticated dynamic programming techniques that
‘split’ dependency trees at the position of their heads,
in order to save bookkeeping. Splitting techniques
have also been exploited to speed up parsing time
for other lexicalized formalisms, such as bilexical
context-free grammars and head automata (Eisner
and Satta, 1999). However, to our knowledge no at-
tempt has been made in the literature to extend these
techniques to non-projective dependency parsing.
In this article we leverage the central idea from
Eisner’s algorithm and extend it to the class of well-
nested dependency trees with block-degree at most 2.
</bodyText>
<page confidence="0.982032">
267
</page>
<bodyText confidence="0.960821227272727">
Transactions of the Association for Computational Linguistics, 1 (2013) 267–278. Action Editor: Brian Roark.
Submitted 3/2013; Published 7/2013. c�2013 Association for Computational Linguistics.
We introduce a structural property, called head-split,
that allows us to split these trees at the positions of
their heads. The property is restrictive, meaning that
it reduces the class of trees that can be generated.
However, we show that the restriction to head-split
trees comes at no significant loss in coverage, and it
allows parsing in time O(n6), an asymptotic improve-
ment of one order of magnitude over the algorithm
by G´omez-Rodr´ıguez et al. (2011) for the unrestric-
ted class. We also show that restricting the class of
head-split trees by imposing the already mentioned
1-inherit property does not cause any additional loss
in coverage, and that parsing for the combined class
is possible in time O(n5), one order of magnitude
faster than the algorithm by Pitler et al. (2012) for
the 1-inherit class without the head-split condition.
The above results have consequences also for the
parsing of other related formalisms, such as the
already mentioned lexicalized tree-adjoining gram-
mars. This will be discussed in the final section.
</bodyText>
<sectionHeader confidence="0.938367" genericHeader="method">
2 Head Splitting
</sectionHeader>
<bodyText confidence="0.999668956521739">
To introduce the basic idea of this article, we briefly
discuss in this section two well-known algorithms for
computing the set of all projective dependency trees
for a given input sentence: the naive, CKY-style
algorithm, and the improved algorithm with head
splitting, in the version of Eisner and Satta (1999).1
CKY parsing The CKY-style algorithm works in
a pure bottom-up way, building dependency trees
by combining subtrees. Assuming an input string
w = a1 • • • an, n &gt; 1, each subtree t is represented
by means of a finite signature [i, j, h], called item,
where i, j are the boundary positions of t’s span over
w and h is the position of t’s root. This is the only
information we need in order to combine subtrees
under the arc-factored model. Note that the number
of possible signatures is O(n3).
The main step of the algorithm is displayed in
Figure 1(a). Here we introduce the graphical conven-
tion, used throughout this article, of representing a
subtree by a shaded area, with an horizontal line in-
dicating the spanned fragment of the input string, and
of marking the position of the head by a bullet. The
illustrated step attaches a tree with signature [k, j, d]
</bodyText>
<footnote confidence="0.919843">
1Eisner (1997) describes a slightly different algorithm.
</footnote>
<figureCaption confidence="0.998616">
Figure 1: Basic steps for (a) the CKY-style algorithm
and (b, c) the head splitting algorithm.
</figureCaption>
<bodyText confidence="0.999860242424242">
as a dependent of a tree with signature [i, k, h]. There
can be O(n5) instantiations of this step, and this is
also the running time of the algorithm.
Eisner’s algorithm Eisner and Satta (1999) im-
prove over the CKY algorithm by reducing the num-
ber of position records in an item. They do this by
‘splitting’ each tree into a left and a right fragment,
so that the head is always placed at one of the two
boundary positions of a fragment, as opposed to be-
ing placed at an internal position. In this way items
need only two indices. Left and right fragments can
be processed independently, and merged afterwards.
Let us consider a right fragment t with head ah.
Attachment at t of a right dependent tree with head
ad is now performed in two steps. The first step at-
taches a left fragment with head ad, as in Figure 1(b).
This results in a new type of fragment/item that has
both heads ah and ad placed at its boundaries. The
second step attaches a right fragment with head ad,
as in Figure 1(c). The number of possible instanti-
ations of these steps, and the asymptotic runtime of
the algorithm, is O(n3).
In this article we extend the splitting technique to
the class of well-nested dependency trees with block-
degree at most 2. This amounts to defining a fac-
torization for these trees into fragments, each with
its own head at one of its boundary positions, along
with some unfolding of the attachment operation into
intermediate steps. While for projective trees head
splitting can be done without any loss in coverage,
for the extended class head splitting turns out to be
a proper restriction. The empirical relevance of this
will be discussed in §7.
</bodyText>
<figure confidence="0.9978846">
ah ad
ah ad
k
ah ad
�
ah ad
(a)
i j
i k j
�
j
ah ad
�
ah ad
j
</figure>
<page confidence="0.994105">
268
</page>
<sectionHeader confidence="0.997733" genericHeader="method">
3 Head-Split Trees
</sectionHeader>
<bodyText confidence="0.999922">
In this section we introduce the class of well-nested
dependency trees with block-degree at most 2, and
define the subclass of head-split dependency trees.
</bodyText>
<subsectionHeader confidence="0.996415">
3.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.996432097560976">
For non-negative integers i, j we write [i, j] to de-
note the set {i, i +1, ... , j }; when i &gt; j, [i, j ] is the
empty set. For a string w = a1 • • • an, where n &gt; 1
and each ai is a lexical token, and for i, j E [0, n]
with i &lt; j, we write wi;j to denote the substring
ai+1 • • • aj of w; wi;i is the empty string.
A dependency tree t over w is a directed tree
whose nodes are a subset of the tokens ai in w and
whose arcs encode a dependency relation between
two nodes. We write ai --* aj to denote the arc
(ai, aj) in t; here, the node ai is the head, and the
node aj is the dependent. If each token ai, i E [1, n],
is a node of t, then t is called complete. Sometimes
we write tai to emphasize that tree t is rooted in node
ai. If ai is a node of t, we also write t[ai] to denote
the subtree of t composed by node ai as its root and
all of its descendant nodes.
The nodes of t uniquely identify a set of max-
imal substrings of w, that is, substrings separated
by tokens not in t. The sequence of such substrings,
ordered from left to right, is the yield of t, written
yd(t). Let ai be some node of t. The block-degree
of ai in t, written bd(ai, t), is defined as the number
of string components of yd(t[ai]). The block-degree
of t, written bd(t), is the maximal block-degree of
its nodes. Tree t is non-projective if bd(t) &gt; 1.
Tree t is well-nested if, for each node ai of t and for
every pair of outgoing dependencies ai --* ad1 and
ai --* ad2, the string components of yd(t[ad1]) and
yd(t[ad2]) do not ‘interleave’ in w. More precisely,
it is required that, if some component of yd(t[adi ]),
i E [1, 2], occurs in w in between two components
s1, s2 of yd(t[adj ]), j E [1, 2] and j # i, then all
components of yd(t[adi ]) occur in between s1, s2.
Throughout this article, whenever we consider a
dependency tree t we always implicitly assume that
t is over w, that t has block-degree at most 2, and
that t is well-nested. Let tai be such a tree, with
bd(ai, tai) = 2. We call the portion of w in between
the two substrings of yd(tai) the gap of tai, denoted
by gap(tai ).
</bodyText>
<figureCaption confidence="0.937477666666667">
Figure 2: Example of a node ah with block-degree 2 in a
non-projective, well-nested dependency tree tah. Integer
m(tah), defined in §3.2, is also marked.
</figureCaption>
<bodyText confidence="0.934959333333333">
Example 1 Figure 2 schematically depicts a well-
nested tree tah with block-degree 2; we have marked
the root node ah and its dependent nodes adi. For
each node adi, a shaded area highlights t[adi]. We
have bd(ah, tah) = bd(ad1, tah) = bd(ad4, tah) =
2 and bd(ad2, tah) = bd(ad3, tah) = 1. ❑
</bodyText>
<subsectionHeader confidence="0.999427">
3.2 The Head-Split Property
</subsectionHeader>
<bodyText confidence="0.997956571428572">
We say that a dependency tree t has the head-split
property if it satisfies the following condition. Let
ah --* ad be any dependency in t with bd(ah, t) =
bd(ad, t) = 2. Whenever gap(t[ad]) contains ah, it
must also contain gap(t[ah]). Intuitively, this means
that if yd(t[ad]) ‘crosses over’ the lexical token ah in
w, then yd(t[ad]) must also ‘cross over’ gap(t[ah]).
Example 2 Dependency ah --* ad1 in Figure 3 viol-
ates the head-split condition, since yd(t[ad1]) crosses
over the lexical token ah in w, but does not cross over
gap(t[ah]). The remaining outgoing dependencies of
ah trivially satisfy the head-split condition, since the
child nodes have block-degree 1. ❑
Let tah be a dependency tree satisfying the head-
split property and with bd(ah, tah) = 2. We specify
below a construction that ‘splits’ tah with respect to
the position of the head ah in yd(tah), resulting in
two dependency trees sharing the root ah and having
all of the remaining nodes forming two disjoint sets.
Furthermore, the resulting trees have block-degree at
most 2.
</bodyText>
<equation confidence="0.744547">
ad1 ad2 ah ad3
</equation>
<figureCaption confidence="0.669619">
Figure 3: Arc ah --* ad1 violates the head-split condition.
</figureCaption>
<equation confidence="0.968725">
ad1 ad2 ah ad3 ad4
m(tah)
</equation>
<page confidence="0.947796">
269
</page>
<figureCaption confidence="0.863696">
ad, ad2 ah m.tah/
Figure 4: Lower tree (a) and upper tree (b) fragments for
the dependency tree in Figure 2.
</figureCaption>
<bodyText confidence="0.995844769230769">
Let yd.tah/ = (wi,j; wp,q) and assume that ah
is placed within wi,j . (A symmetric construction
should be used in case ah is placed within wp,q.) The
mirror image of ah with respect to gap.tah/, written
m.tah/, is the largest integer in Œp; q• such that there
are no dependencies linking nodes in wi,h_1 and
nodes in wp,m.tah/ and there are no dependencies
linking nodes in wh,j and nodes in wm.tah/,q. It is
not hard to see that such an integer always exists,
since tah is well-nested.
We classify every dependent ad of ah as being
an ‘upper’ dependent or a ‘lower’ dependent of
ah, according to the following conditions: (i) If
d E Œi; h — 1• U Œm.tah/ + 1; q•, then ad is an upper
dependent of ah. (ii) If d E Œh + 1; j• U Œp; m.tah/•,
then ad is a lower dependent of ah.
The upper tree of tah is the dependency tree
rooted in ah and composed of all dependencies
ah --* ad in tah with ad an upper dependent of
ah, along with all subtrees tahŒad• rooted in those
dependents. Similarly, the lower tree of tah is the
dependency tree rooted in ah and composed of all
dependencies ah --* ad in tah with ad a lower de-
pendent of ah, along with all subtrees tahŒad• rooted
in those dependents. As a general convention, in this
article we write tU,ah and tL,ah to denote the upper
and the lower trees of tah, respectively. Note that, in
some degenerate cases, the set of lower or upper de-
pendents may be empty; then tU,ah or tL,ah consists
of the root node ah only.
Example 3 Consider the tree tah displayed in Fig-
ure 2. Integer m.tah/ denotes the boundary between
the right component of yd.tahŒado•/ and the right
component of yd.tahŒad,•/. Nodes ad3 and ado are
lower dependents, and nodes ad, and ad2 are upper
dependents. Trees tL,ah and tU,ah are displayed in
Figure 4 (a) and (b), respectively. ❑
The importance of the head-split property can be
informally explained as follows. Let ah --* ad be a
dependency in tah. When we take apart the upper and
the lower trees of tah, the entire subtree tahŒad• ends
up in either of these two fragments. This allows us to
represent upper and lower fragments for some head
independently of the other, and to freely recombine
them. More formally, our algorithms will make use
of the following three properties, stated here without
any formal proof:
P1 Trees tU,ah and tL,ah are well-nested, have block-
degree &lt; 2, and satisfy the head-split property.
P2 Trees tU,ah and tL,ah have their head ah always
placed at one of the boundaries in their yields.
P3 Let t/ and t// be the upper and lower trees
</bodyText>
<figure confidence="0.5943615">
U,ah L,ah
and tah, respectively. If m(tah)
of distinct trees t/ah =
m.t// ah/,
//
t/U ah and tL,ah = tL,ah .
</figure>
<sectionHeader confidence="0.864254" genericHeader="method">
4 Parsing Items
</sectionHeader>
<bodyText confidence="0.989238692307692">
Let w = a1 • • • an, n &gt; 1, be the input string. We
need to compactly represent trees that span substrings
of w by recording only the information that is needed
to combine these trees into larger trees during the
parsing process. We do this by associating each
tree with a signature, called item, which is a tuple
Œi; j; p; q; h•X, where h E Œ1; n• identifies the token
ah, i; j with 0 &lt; i &lt; j &lt; n identify a substring wi,j ,
and p; q with j &lt; p &lt; q &lt; n identify a substring
wp,q. We also use the special setting p = q = —.
The intended meaning is that each item repres-
ents some tree tah. If p; q # — then yd.tah/ =
(wi,j;wp,q). If p;q = —then
</bodyText>
<equation confidence="0.878529">
(wi,j) if h E Œi + 1; j•
(wh,h; wi,j) if h &lt; i
(wi,j; wh,h) if h &gt; j + 1
</equation>
<bodyText confidence="0.998083333333333">
The two cases h &lt; i and h &gt; j + 1 above will
be used when the root node ah of tah has not yet
collected all of its dependents.
Note that h E {i; j + 11 is not used in the
definition of item. This is meant to avoid differ-
ent items representing the same dependency tree,
</bodyText>
<figure confidence="0.994995875">
(a)
ah ad3 ado
(b)
then there exists a tree tah such that tU,ah =
8
&lt;ˆ
ˆ:
yd.tah/ =
</figure>
<page confidence="0.980847">
270
</page>
<bodyText confidence="0.999940666666667">
which is undesired for the specification of our al-
gorithm. As an example, items [i, j, —, —, i C 1]X
and [i C 1, j, —, —, i C 1]X both represent a depend-
ency tree taiC1 with yd(taiC1) D hwi,ji. This and
other similar cases are avoided by the ban against
h 2 fi, j C 1g, which amounts to imposing some
normal form for items. In our example, only item
[i, j, —, —, i C 1]X is a valid signature.
Finally, we distinguish among several item types,
indicated by the value of subscript X. These types
are specific to each parsing algorithm, and will be
defined in later sections.
</bodyText>
<sectionHeader confidence="0.665978" genericHeader="method">
5 Parsing of Head-Split Trees
</sectionHeader>
<bodyText confidence="0.99999575">
We present in this section our first tabular algorithm
for computing the set of all dependency trees for an
input sentence w that have the head-split property,
under the arc-factored model. Recall that tai denotes
a tree with root ai, and tL,ai and tU,ai are the lower
and upper trees of tai. The steps of the algorithm
are specified by means of deduction rules over items,
following the approach of Shieber et al. (1995).
</bodyText>
<subsectionHeader confidence="0.998075">
5.1 Basic Idea
</subsectionHeader>
<bodyText confidence="0.999984">
Our algorithm builds trees step by step, by attaching
a tree tah0 as a dependent of a tree tah and creating
the new dependency ah ! ah0. Computationally,
the worst case for this operation is when both tah
and tah0 have a gap; then, for each tree we need to
keep a record of the four boundaries, along with the
position of the head, as done by G´omez-Rodriguez et
al. (2011). However, if we are interested in parsing
trees that satisfy the head-split property, we can avoid
representing a tree with a gap by means of a single
item. We instead follow the general idea of §2 for
projective parsing, and use different items for the
upper and the lower trees of the source tree.
When we need to attach tah0 as an upper dependent
of tah, defined as in §3.2, we perform two consecutive
steps. First, we attach tL,ah0 to tU,ah, resulting in a
new intermediate tree t1. As a second step, we attach
tU,ah0 to t1, resulting in a new tree t2 which is tU,ah
with tah0 attached as an upper dependent, as desired.
Both steps are depicted in Figure 5; here we introduce
the convention of indicating tree grouping through
a dashed line. A symmetric procedure can be used
to attach tah0 as a lower dependent to tL,ah. The
</bodyText>
<figure confidence="0.515367">
ah0 ah
</figure>
<figureCaption confidence="0.9945935">
Figure 5: Two step attachment of tah0 at tU,ah: (a) attach-
ment of tL,ah0 ; (b) attachment of tU,ah0 .
</figureCaption>
<bodyText confidence="0.997549916666667">
correctness of the two step approach follows from
properties P1 and P3 in §3.2.
By property P2 in §3.2, in both steps above the
lexical heads ah and ah0 can be read from the bound-
aries of the involved trees. Then these steps can be
implemented more efficiently than the naive method
of attaching tah0 to tah in a single step. A more de-
tailed computational analysis will be provided in §5.7.
To simplify the presentation, we restrict the use of
head splitting to trees with a gap and parse trees with
no gap with the naive method; this does not affect
the computational complexity.
</bodyText>
<subsectionHeader confidence="0.995312">
5.2 Item Types
</subsectionHeader>
<bodyText confidence="0.999632333333333">
We distinguish five different types of items, indicated
by the subscript X 2 f0, L, U, /L, /Ug, as described
in what follows.
</bodyText>
<listItem confidence="0.998226142857143">
• If X D 0, we have p D q D — and yd(ah) is
specified as in §4.
• If X D L, we use the item to represent some
lower tree. We have therefore p, q ¤ — and
h 2 fi C 1, qg.
• If X D U, we use the item to represent some
upper tree. We have therefore p, q ¤ — and
h 2 fj,p C 1g.
• If X D /L or X D /U, we use the item to
represent some intermediate step in the parsing
process, in which only the lower or upper tree of
some dependent has been collected by the head
ah, and we are still missing the upper (/U) or
the lower (/L) tree.
</listItem>
<figure confidence="0.999136846153846">
(a)
ah0
+
tU,ah
t1
tL,ah0
ah0
+
(b)
t2
tU,ah0
t1
ah
</figure>
<page confidence="0.986684">
271
</page>
<bodyText confidence="0.99992425">
We further specialize symbol =U by writing =U&lt;
(=U&gt;) to indicate that the missing upper tree should
have its head to the left (right) of its gap. We also use
=L&lt; and =L&gt; with a similar meaning.
</bodyText>
<subsectionHeader confidence="0.977667">
5.3 Item Normal Form
</subsectionHeader>
<bodyText confidence="0.9994975">
It could happen that our algorithm produces items of
type 0 that do not satisfy the normal form condition
discussed in •4. To avoid this problem, we assume
that every item of type 0 that is produced by the
algorithm is converted into an equivalent normal form
item, by means of the following rules:
</bodyText>
<equation confidence="0.99961825">
Œi; j; —; —; i•0
Œi — 1;j;—;—;i•0 (1)
Œi; j;—;—;j + 1•0
Œi; j + 1; —;—; j + 1•0 (2)
</equation>
<subsectionHeader confidence="0.986739">
5.4 Items of Type 0
</subsectionHeader>
<bodyText confidence="0.995371714285714">
We start with deduction rules that produce items of
type 0. As already mentioned, we do not apply the
head splitting technique in this case.
The next rule creates trees with a single node, rep-
resenting the head, and no dependents. The rule is
actually an axiom (there is no antecedent) and the
statement i E Œ1; n• is a side condition.
</bodyText>
<equation confidence="0.947234">
Œi — 1;i;—;—;i•0 ˚i E Œ1; n• (3)
</equation>
<bodyText confidence="0.9999675">
The next rule takes a tree headed in ahs and makes
it a dependent of a new head ah. This rule imple-
ments what has been called the ‘hook trick’. The first
side condition enforces that the tree headed in ahs
has collected all of its dependents, as discussed in •4.
The second side condition enforces that no cycle is
created. We also write ah --* ahs to indicate that a
new dependency is created in the parse forest.
</bodyText>
<equation confidence="0.9312365">
Œi; j;—;—;h&apos;•0
Œi; j;—;—;h•0
</equation>
<bodyText confidence="0.9997935">
The next two rules combine gap-free dependents
of the same head ah.
</bodyText>
<equation confidence="0.99971925">
Œi; k; —; —; h•0 Œk; j; —; —; h•0
Œi; j;—;—;h•0 (5)
Œi; h;—;—;h•0 Œh — 1;j;—;—;h•0
Œi; j; —; —; h•0 (6)
</equation>
<bodyText confidence="0.99978625">
We need the special case in (6) to deal with the con-
catenation of two items that share the head ah at the
concatenation point. Observe the apparent mismatch
in step (6) between index h in the first antecedent
and index h — 1 in the second antecedent. This is
because in our normal form, both the first and the
second antecedent have already incorporated a copy
of the shared head ah.
The next two rules collect a dependent of ah that
wraps around the dependents that have already been
collected. As already discussed, this operation is
performed by two successive steps: We first collect
the lower tree and then the upper tree. We present
the case in which the shared head of the two trees is
placed at the left of the gap. The case in which the
head is placed at the right of the gap is symmetric.
</bodyText>
<equation confidence="0.999824857142857">
Œi&apos;; j&apos;; —; —; h•0
Œi; i&apos;; j&apos;; j ; i + 1•L(h V Œi + 1;i&apos;•
1 U Œj&apos; + 1; j• (7)
Œi; j ; —; —; h•=U&lt;
Œi&apos;; j&apos;; —; —; h•=U&lt;
Œi;i&apos; + 1;j&apos;;j;i&apos; + 1•U
Œi; j; —; —; h•0
</equation>
<bodyText confidence="0.999776666666667">
Again, there is an overlap in rule (8) between the
two antecedents, due to the fact that both items have
already incorporated copies of the same head.
</bodyText>
<subsectionHeader confidence="0.993693">
5.5 Items of Type U
</subsectionHeader>
<bodyText confidence="0.999973285714286">
We now consider the deduction rules that are needed
to process upper trees. Throughout this subsection
we assume that the head of the upper tree is placed at
the left of the gap. The other case is symmetric. The
next rule creates an upper tree with a single node, rep-
resenting its head, and no dependents. We construct
an item for all possible right gap boundaries j.
</bodyText>
<equation confidence="0.9988355">
�i E Œ1; n• (9)
Œi — 1;i; j; j;i•U j E Œi + 1;n•
</equation>
<bodyText confidence="0.9999695">
The next rule adds to an upper tree a group of new
dependents that do not have any gap. We present the
case in which the new dependents are placed at the
left of the gap of the upper tree.
</bodyText>
<equation confidence="0.823708875">
Œi;i&apos;;—;—;j•0 Œi&apos;;j;p;q;j•U
Œi; j; p; q;j•U (10)
h&apos; E Œi + 1;j•
h E6Œi + 1;j•
ah --* ah&apos;
8
&lt;
:
(4)
h E6Œi + 1;i&apos; + 1•
U Œj&apos; + 1;j•
ah --* ai,+1
(8)
8
&lt;
:
</equation>
<page confidence="0.991669">
272
</page>
<bodyText confidence="0.999954833333333">
The next two rules collect a new dependent that
wraps around the upper tree. Again, this operation is
performed by two successive steps: We first collect
the lower tree, then the upper tree. We present the
case in which the shared head of the two trees is
placed at the left of the gap.
</bodyText>
<equation confidence="0.964491333333333">
Œi0; j; p; q0;j•U Œi; i0; q0;q;i + 1•L
Œi; j; p; q; j•/U&lt; (11)
Œi;j;p;q;j•U
</equation>
<subsectionHeader confidence="0.995374">
5.6 Items of Type L
</subsectionHeader>
<bodyText confidence="0.999972909090909">
So far we have always expanded items (type 0 or U)
at their external boundaries. When dealing with lower
trees, we have to reverse this strategy and expand
items (type L) at their internal boundaries. Apart
from this difference, the deduction rules below are
entirely symmetric to those in •5.5. Again, we as-
sume that the head of the lower tree is placed at
the left of the gap, the other case being symmetric.
Our first rule creates a lower tree with a single node,
representing its head. We blindly guess the right
boundary of the gap of such a tree.
</bodyText>
<equation confidence="0.966471">
i E Œ1; n• (13)
Œi - 1; i; j; j;i•L Ij E Œi + 1; n•
</equation>
<bodyText confidence="0.9999735">
The next rule adds to a lower tree a group of new
dependents that do not have any gap. We present the
case in which the new dependents are placed at the
left of the gap of the lower tree.
</bodyText>
<equation confidence="0.9980385">
Œj0; j; -; -; i + 1•0 Œi; j0; p; q; i + 1•L
Œi; j; p; q; i + 1•L (14)
</equation>
<bodyText confidence="0.999977428571429">
The next two rules collect a new dependent with
a gap and embed it within the gap of our lower tree,
creating a new dependency. Again, this operation is
performed by two successive steps, and we present
the case in which the common head of the lower and
upper trees that are embedded is placed at the left of
the gap, the other case being symmetric.
</bodyText>
<equation confidence="0.996792625">
Œi; j0; p0; q; i + 1•L Œj0; j; p; p0; j•U
Œi; j; p; q; i + 1•/L&lt; (15)
Œi; j0; p0; q; i + 1•/L&lt;
Œj0 - 1; j; p; p0; j0•L
{aiC1 --* aj0 (16)
Œi; j; p;q;i + 1•L
tU,ah tLL,ah tLR,ah
ad1 ah ad2 ad3 ad4 ad5
</equation>
<figureCaption confidence="0.994301333333333">
Figure 6: Node ah satisfies both the 1-inherit and head-
split conditions. Accordingly, tree tah can be split into
three fragments tU,ah, tLL,ah and tLR,ah.
</figureCaption>
<subsectionHeader confidence="0.976156">
5.7 Runtime
</subsectionHeader>
<bodyText confidence="0.999963444444444">
The algorithm runs in time O.n6/, where n is the
length of the input sentence. The worst case is due
to deduction rules that combine two items, each of
which represents trees with one gap. For instance,
rule (11) involves six free indices ranging over Œ1; n•,
and thus could be instantiated O.n6/ many times. If
the head-split property does not hold, attachment of a
dependent in one step results in time O.n7/, as seen
for instance in G´omez-Rodr´ıguez et al. (2011).
</bodyText>
<sectionHeader confidence="0.727456" genericHeader="method">
6 Parsing of 1-Inherit Head-Split Trees
</sectionHeader>
<bodyText confidence="0.999490666666667">
In this section we specialize the parsing algorithm
of •5 to a new, more efficient algorithm for a restric-
ted class of trees.
</bodyText>
<subsectionHeader confidence="0.997686">
6.1 1-Inherit Head-Split Trees
</subsectionHeader>
<bodyText confidence="0.999555642857143">
Pitler et al. (2012) introduce a restriction on well-nes-
ted dependency trees with block-degree at most 2.
A tree t satisfies the 1-inherit property if, for every
node ah in t with bd.ah; t/ = 2, there is at most
one dependency ah --* ad* such that gap.tŒad*•/
contains gap.tŒah•/. Informally, this means that
yd.tŒad*•/ ‘crosses over’ gap.tŒah•/, and we say that
ad* ‘inherits’ the gap of ah. In this section we in-
vestigate the parsing of head-split trees that also have
the 1-inherit property.
Example 4 Figure 6 shows a head node ah along
with dependents adi, satisfying the head-split condi-
tion. Only tad1 has its yield crossing over gap.tah/.
Thus ah also satisfies the 1-inherit condition. ❑
</bodyText>
<subsectionHeader confidence="0.999508">
6.2 Basic Idea
</subsectionHeader>
<bodyText confidence="0.99983625">
Let tah be some tree satisfying both the head-split
property and the 1-inherit propery. Assume that the
dependent node ad* which inherits the gap of tah
is placed within tU,ah. This means that, for every
</bodyText>
<equation confidence="0.971594">
Œi0;j;p;q0;j•/U&lt;
Œi;i0 + 1;q0;q;i0 + 1•U
{aj --* ai0C1 (12)
</equation>
<page confidence="0.99516">
273
</page>
<bodyText confidence="0.999726684210526">
dependency ah ! ad in tL,ah, yd.tŒad•/ does not
cross over gap.tL,ah/. Then we can further split
tL,ah into two trees, both with root ah. We call these
two trees the lower-left tree, written tLL,ah, and the
lower-right tree, written tLR,ah; see again Figure 6.
The basic idea behind our algorithm is to split tah
into three dependency trees tU,ah, tLL,ah and tLR,ah,
all sharing the same root ah. This means that tah
can be attached to an existing tree through three suc-
cessive steps, each processing one of the three trees
above. The correctness of this procedure follows
from a straightforward extension of properties P1 and
P3 from •3.2, stating that the tree fragments tU,ah,
tLL,ah and tLR,ah can be represented and processed
one independently of the others, and freely combined
if certain conditions are satisfied by their yields.
In case ad* is placed within tL,ah, we introduce
the upper-left and the upper-right trees, written
tUL,ah and tUR,ah, and apply a similar idea.
</bodyText>
<subsectionHeader confidence="0.997234">
6.3 Item Types
</subsectionHeader>
<bodyText confidence="0.99996925">
When processing an attachment, the order in which
the algorithm assembles the three tree fragments of
tah defined in •6.2 is not always the same. Such an
order is chosen on the basis of where the head ah
and the dependent ad* inheriting the gap are placed
within the involved trees. As a consequence, in our
algorithm we need to represent several intermediate
parsing states. Besides the item types from •5.2, we
therefore need additional types. The specification
of these new item types is rather technical, and is
therefore delayed until we introduce the relevant de-
duction rules.
</bodyText>
<subsectionHeader confidence="0.998894">
6.4 Items of Type 0
</subsectionHeader>
<bodyText confidence="0.999864769230769">
We start with the deduction rules for parsing of
trees tLL,ah and tLR,ah; trees tUL,ah and tUR,ah can be
treated symmetrically. The yields of tLL,ah and tLR,ah
have the form specified in •4 for the case p D q D —.
We can therefore use items of type 0 to parse these
trees, adopting a strategy similar to the one in •5.4.
The main difference is that, when a tree tah0 with a
gap is attached as a dependent to the head ah, we
use three consecutive steps, each processing a single
fragment of tah0. We assume below that tah0 can be
split into trees tU,ah0, tLL,ah0 and tLR,ah0, the other
case can be treated in a similar way.
We use rules (3), (4) and (5) from •5.4. Since in
</bodyText>
<equation confidence="0.666496">
tad* tU,ad* tLL,ad* tLR,ad*
ad* ah
</equation>
<figureCaption confidence="0.98391675">
Figure 7: Tree tU,ah is decomposed into tad* and subtrees
covering substrings 6Z, i 2 Œ1; 4•. Tree tad* is in turn
decomposed into three fragments (trees tLL,ad* , tLR,ad* ,
and tU,ad* in this example).
</figureCaption>
<bodyText confidence="0.98728075">
the trees tLL,ah and tLR,ah the head is never placed in
the middle of the yield, rule (6) is not needed now
and it can safely be discarded. Rule (7), attaching
a lower tree, needs to be replaced by two new rules,
processing a lower-left and a lower-right tree. We
assume here that the common head of these trees is
placed at the left boundary of the lower-left tree; we
leave out the symmetric case.
</bodyText>
<equation confidence="0.942232">
Œi; j; —; —; h•/U&lt;
</equation>
<bodyText confidence="0.999987166666667">
The first antecedent in (17) encodes a lower-left tree
with its head at the left boundary. The consequent
item has then the new type =LR&lt;, meaning that a
lower-right tree is missing that must have its head
at the left. The first antecedent in (18) provides the
missing lower-right tree, having the same head as
the already incorporated lower-left tree. After these
rules are applied, rule (8) from •5.4 can be applied
to the consequent item of (18). This completes the
attachment of a ‘wrapping’ dependent of ah, with the
incorporation of the missing upper tree and with the
construction of the new dependency.
</bodyText>
<subsectionHeader confidence="0.999177">
6.5 Items of Type U
</subsectionHeader>
<bodyText confidence="0.999097428571429">
We now assume that node ad* is realized within
tU,ah, so that tah can be split into trees tU,ah, tLL,ah
and tLR,ah. We provide deduction rules to parse of
tU,ah; this is the most involved part of the algorithm.
In case ad* is realized within tL,ah, tah must be
split into tL,ah, tUL,ah and tUR,ah, and a symmetrical
strategy can be applied to parse tL,ah.
</bodyText>
<equation confidence="0.900930230769231">
�r
a1
����
a3
�r
a4
�r
a2
Œi; i0; —; —; i C 1•0
Œj 0; j ; �; �; iC 1•0
Œi0; j; �; h•0 Ih 26Œi C 1; i0• (17)
Œi; j; —; —; h•/LR&lt;
Œi; j0; ; h•/LR&lt; Ih 26 Œj0 C 1;j • (18)
</equation>
<page confidence="0.616329">
274
</page>
<equation confidence="0.915184727272727">
rule (20)
rule (19)
ad* tU;ad* tLL;ad* ah tLR;ad*
U1 U2 U3 U4
rule (22) rule (23)
tLL;ad*
ah
tU;ad*
tLR;ad*
ad*
U1 U2 U3 U4
</equation>
<figureCaption confidence="0.9925885">
Figure 8: Decomposition of tU;ah as in Figure 7, with
highlighted application of rules (19) and (20).
</figureCaption>
<bodyText confidence="0.943551444444445">
We start by observing that yd.tad*/ splits
yd.tU;ah/ into at most four substrings ai; see Fig-
ure 7.2 Because of the well-nested property, within
the tree tU;ah each dependent of ah other than ad*
has a yield that is entirely placed within one of the
ai’s substrings. This means that each substring ai
can be parsed independently of the other substrings.
As a first step in the process of parsing tU;ah, we
parse each substring ai. We do this following the
parsing strategy specified in •6.4. As a second step,
we assume that each of the three fragments resulting
from the decomposition of tree tad* has already been
parsed; see again Figure 7. We then ‘merge’ these
three fragments and the trees for segments ai’s into
a complete parse tree representing tU;ah. This is
described in detail in what follows.
We assume that ah is placed at the left of the gap
of tU;ah (the right case being symmetrical) and we
distinguish four cases, depending on the two ways in
which tad* can be split, and the two side positions of
the head ad* with respect to gap.tad* /.
Case 1 We assume that tad* can be split into trees
tU;ad*, tLL;ad*, tLR;ad*, and the head ad* is placed
at the left of gap.tad* /; see again Figure 7.
Rule (19) below combines tLL;ad* with a parse for
segment a2, which has its head ah placed at its right
boundary; see Figure 8 for a graphical representation
of rule (19). The result is an item of the new type HH.
This item is used to represent an intermediate tree
fragment with root of block-degree 1, where both the
left and the right boundaries are heads; a dependency
2According to our definition of m.tah/ in •3.2, a3 is always
the empty string. However, here we deal with the general formu-
lation of the problem in order to claim in •8 that our algorithm
can be directly adapted to parse some subclasses of lexicalized
tree-adjoining grammars.
</bodyText>
<figureCaption confidence="0.9932">
Figure 9: Decomposition of tU;ah as in Figure 7, with
highlighted application of rules (22) and (23).
</figureCaption>
<equation confidence="0.816603">
between these heads will be constructed later.
Œi; i&apos;; —; —; i + 1•0 Œi&apos;; j; —; —; j •0
Œi; j; —; —; j•HH (19)
</equation>
<bodyText confidence="0.990507">
Rule (20) combines tU;ad* with a type 0 item rep-
resenting tLR;ad*; see again Figure 8. Note that this
combination operation expands an upper tree at one
of its internal boundaries, something that was not
possible with the rules specified in •5.5.
</bodyText>
<equation confidence="0.997207">
Œi;j;p&apos;;q;j•U Œp;p&apos;;—;—;j•0
Œi; j; p; q; j •U (20)
</equation>
<bodyText confidence="0.999868">
Finally, we combine the consequents of (19)
and (20), and process the dependency that was left
pending in the item of type HH.
</bodyText>
<equation confidence="0.994782">
˚aj --* aj, (21)
Œi; j; p; q;j•U
</equation>
<bodyText confidence="0.9997205">
After the above steps, parsing of tU;ah can be com-
pleted by combining item Œi; j; p; q; j •U from (21)
with items of type 0 representing parses for the sub-
strings a1, 93 and a4.
Case 2 We assume that tad* can be split into trees
tU;ad*, tLL;ad*, tLR;ad*, and the head ad* is placed
at the right of gap.tad* /, as depicted in Figure 9.
Rule (22) below, graphically represented in Fig-
ure 9, combines tU;ad* with a type 0 item represent-
ing tLL;ad*. This can be viewed as the symmetric
version of rule (20) of Case 1, expanding an upper
tree at one of its internal boundaries.
</bodyText>
<equation confidence="0.97884975">
Œi; j&apos;; p; q; p + 1•U Œj&apos;; j; —; —; p + 1•0
Œi; j; p; q; p + 1•U (22)
Œi; j&apos;; p; q; j&apos;•U
Œj&apos; — 1; j ; —; —; j•HH
</equation>
<page confidence="0.993775">
275
</page>
<table confidence="0.9987325">
Arabic Czech Danish Dutch Portuguese Swedish
Number of trees 1,460 72,703 5,190 13,349 9,071 11,042
WN2 1,458 99.9% 72,321 99.5% 5,175 99.7% 12,896 96.6% 8,650 95.4% 10,955 99.2%
Classes considered in this paper
WN2 + HS O(n6) 1,457 99.8% 72,182 99.3% 5,174 99.7% 12,774 95.7% 8,648 95.3% 10,951 99.2%
WN2 + HS + 1I 1,457 99.8% 72,182 99.3% 5,174 99.7% 12,774 95.7% 8,648 95.3% 10,951 99.2%
Classes considered by Pitler et al. (2012)
WN2 + 1I 1,458 99.9% 72,321 99.5% 5,175 99.7% 12,896 96.6% 8,650 95.4% 10,955 99.2%
WN2 + 0I O(n5) 1,394 95.5% 70,695 97.2% 4,985 96.1% 12,068 90.4% 8,481 93.5% 10,787 97.7%
Projective O(n3) 1,297 88.8% 55,872 76.8% 4,379 84.4% 8,484 63.6% 7,353 81.1% 9,963 90.2%
</table>
<tableCaption confidence="0.9959115">
Table 1: Coverage of various classes of dependency trees on the training sets used in the CoNLL-X shared task (WN2 =
well-nested, block-degree &lt; 2; HS = head-split; 1I = 1-inherit; 0I = 0-inherit, ‘gap-minding’)
</tableCaption>
<bodyText confidence="0.999598571428571">
Next, we combine the result of (22) with a parse for
substring U2. The result is an item of the new type
/LR&gt;. This item is used to represent an intermediate
tree fragment that is missing a lower-right tree with
its head at the right. In this fragment, two heads
are left pending, and a dependency relation will be
eventually established between them.
</bodyText>
<construct confidence="0.5459445">
[i, j&apos;, p, q, p + 1]U [j&apos;, j , -, -, j]0
[i, j, p, q, j ]=LR&gt; (23)
</construct>
<bodyText confidence="0.997024352941176">
The next rule combines the consequent item of (23)
with a tree tLR,ad* having its head at the right bound-
ary, and processes the dependency that was left
pending in the /LR&gt; item.
[i, j, p&apos;, q, j ]=LR&gt;
After the above rules, parsing of tU,ah continues by
combining the consequent item [i, j, p, q, j ]U from
rule (24) with items representing parses for the sub-
strings U1, U3 and U4.
Cases 3 and 4 We informally discuss the cases in
which tad* can be split into trees tL,ad*, tUL,ad*,
tUR,ad*, for both positions of the head ad* with re-
spect to gap(tad*). In both cases we can adopt a
strategy similar to the one of Case 2.
We first expand tL,ad* externally, at the side op-
posite to the head ad*, with a tree fragment tUL,ad*
or tUR,ad*, similarly to rule (22) of Case 2. This
results in a new fragment t1. Next, we merge t1
with a parse for U2 containing the head ah, similarly
to rule (23) of Case 2. This results in a new frag-
ment t2 where a dependency relation involving the
heads ad* and ah is left pending. Finally, we merge
t2 with a missing tree tUL,ad* or tUR,ad*, and pro-
cess the pending dependency, similarly to rule (24).
One should contrast this strategy with the alternative
strategy adopted in Case 1, where the fragment of
tad* having block-degree 2 cannot be merged with a
parse for the segment containing the head ah (U2 in
Case 1), because of an intervening fragment of tad*
with block-degree 1 (tLL,ad* in Case 1).
Finally, if there is no node ad* in tU,ah that inherits
the gap of ah, we can split tU,ah into two dependency
trees, as we have done for tL,ah in §6.2, and parse
the two fragments using the strategy of §6.4.
</bodyText>
<subsectionHeader confidence="0.897818">
6.6 Runtime
</subsectionHeader>
<bodyText confidence="0.999763777777778">
Our algorithm runs in time 0(n5), where n is the
length of the input sentence. The reason of the im-
provement with respect to the 0(n6) result of §5 is
that we no longer have deduction rules where both
antecedents represent trees with a gap. In the new al-
gorithm, the worst case is due to rules where only one
antecedent has a gap. This leads to rules involving a
maximum of five indices, ranging over [1, n]. These
rules can be instantiated in 0(n5) ways.
</bodyText>
<sectionHeader confidence="0.996603" genericHeader="method">
7 Empirical Coverage
</sectionHeader>
<bodyText confidence="0.99999375">
We have seen that the restriction to head-split de-
pendency trees enables us to parse these trees one
order of magnitude faster than the class of well-nes-
ted dependency trees with block-degree at most 2.
</bodyText>
<equation confidence="0.950759666666667">
[p, p&apos; + 1,-,-,p&apos; + 1]0
˚aj --* ap,+1 (24)
[i, j, p, q,j]U
</equation>
<page confidence="0.990401">
276
</page>
<bodyText confidence="0.9999784">
In connection with the 1-inherit property, this even
increases to two orders of magnitude. However, as
already stated in §2, this improvement is paid for by
a loss in coverage; for instance, trees of the form
shown in Figure 3 cannot be parsed any longer.
</bodyText>
<subsectionHeader confidence="0.995152">
7.1 Quantitative Evaluation
</subsectionHeader>
<bodyText confidence="0.999931565217391">
In order to assess the empirical loss in coverage that
the restriction to head-split trees incurs, we evaluated
the coverage of several classes of dependency trees
on standard data sets. Following Pitler et al. (2012),
we report in Table 1 figures for the training sets of
six languages used in the CoNLL-X shared task on
dependency parsing (Buchholz and Marsi, 2006). As
we can see, the O(n6) class of head-split trees has
only slightly lower coverage on this data than the
baseline class of well-nested dependency trees with
block-degree at most 2. The losses are up to 0.2
percentage points on five of the six languages, and 0.9
points on the Dutch data. Our even more restricted
O(n5) class of 1-inherit head-split trees has the same
coverage as our O(n6) class, which is expected given
the results of Pitler et al. (2012): Their O(n6) class
of 1-inherit trees has exactly the same coverage as
the baseline (and thereby more coverage than our
O(n6) class). Interestingly though, their O(n5) class
of ‘gap-minding’ trees has a significantly smaller
coverage than our O(n5) class. We conclude that
our class seems to strike a good balance between
expressiveness and parsing complexity.
</bodyText>
<subsectionHeader confidence="0.994427">
7.2 Qualitative Evaluation
</subsectionHeader>
<bodyText confidence="0.9993295625">
While the original motivation behind introducing the
head-split property was to improve parsing complex-
ity, it is interesting to also discuss the linguistic relev-
ance of this property. A first inspection of the struc-
tures that violate the head-split property revealed that
many such violations disappear if one ignores gaps
caused by punctuation. Some decisions about what
nodes should function as the heads of punctuation
symbols lead to more gaps than others. In order to
quantify the implications of this, we recomputed the
coverage of the class of head-split trees on data sets
where we first removed all punctuation. The results
are given in Table 2. We restrict ourselves to the five
native dependency treebanks used in the CoNLL-X
shared task, ignoring treebanks that have been con-
verted from phrase structure representations.
</bodyText>
<table confidence="0.901458666666667">
Arabic Czech Danish Slovene Turkish
with 1 139 1 2 2
without 1 46 0 0 2
</table>
<tableCaption confidence="0.923511">
Table 2: Violations against the head-split property (relative
to the class of well-nested trees with block-degree &lt; 2)
with and without punctuation.
</tableCaption>
<bodyText confidence="0.999971933333333">
We see that when we remove punctuation from
the sentences, the number of violations against the
head-split property at most decreases. For Danish
and Slovene, removing punctuation even has the ef-
fect that all well-nested dependency trees with block-
degree at most 2 become head-split. Overall, the
absolute numbers of violations are extremely small—
except for Czech, where we have 139 violations with
and 46 without punctuation. A closer inspection of
the Czech sentences reveals that many of these fea-
ture rather complex coordinations. Indeed, out of
the 46 violations in the punctuation-free data, only 9
remain when one ignores those with coordination.
For the remaining ones, we have not been able to
identify any clear patterns.
</bodyText>
<sectionHeader confidence="0.980403" genericHeader="conclusions">
8 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999989590909091">
In this article we have extended head splitting tech-
niques, originally developed for parsing of projective
dependency trees, to two subclasses of well-nested
dependency trees with block-degree at most 2. We
have improved over the asymptotic runtime of two
existing algorithms, at no significant loss in coverage.
With the same goal of improving parsing efficiency
for subclasses of non-projective trees, in very recent
work Pitler et al. (2013) have proposed an O(n4)
time algorithm for a subclass of non-projective trees
that are not well-nested, using an approach that is
orthogonal to the one we have explored here.
Other than for dependency parsing, our results
have also implications for mildly context-sensitive
phrase structure formalisms. In particular, the al-
gorithm of §5 can be adapted to parse a subclass
of lexicalized tree-adjoining grammars, improving
the result by Eisner and Satta (2000) from O(n7) to
O(n6). Similarly, the algorithm of §6 can be adapted
to parse a lexicalized version of the tree-adjoining
grammars investigated by Satta and Schuler (1998),
improving a naive O(n7) algorithm to O(n5).
</bodyText>
<page confidence="0.993995">
277
</page>
<sectionHeader confidence="0.993829" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999946391304348">
Manuel Bodirsky, Marco Kuhlmann, and Mathias M¨ohl.
2005. Well-nested drawings as models of syntactic
structure. In Proceedings of the 10th Conference on
Formal Grammar (FG) and Ninth Meeting on Mathem-
atics of Language (MOL), pages 195–203, Edinburgh,
UK.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of the Tenth Conference on Computational
Natural Language Learning (CoNLL), pages 149–164,
New York, USA.
Xavier Carreras. 2007. Experiments with a higher-order
projective dependency parser. In Proceedings of the
CoNLL Shared Task Session of EMNLP-CoNLL 2007,
pages 957–961, Prague, Czech Republic.
Joan Chen-Main and Aravind K. Joshi. 2010. Unavoid-
able ill-nestedness in natural language and the adequacy
of tree local-MCTAG induced dependency structures.
In Proceedings of the Tenth International Conference
on Tree Adjoining Grammars and Related Formalisms
(TAG+), New Haven, USA.
Jason Eisner and Giorgio Satta. 1999. Efficient parsing
for bilexical context-free grammars and Head Auto-
maton Grammars. In Proceedings of the 37th Annual
Meeting of the Association for Computational Linguist-
ics (ACL), pages 457–464, College Park, MD, USA.
Jason Eisner and Giorgio Satta. 2000. A faster parsing
algorithm for lexicalized Tree-Adjoining Grammars. In
Proceedings of the Fifth Workshop on Tree Adjoining
Grammars and Related Formalisms (TAG+), pages 14–
19, Paris, France.
Jason Eisner. 1997. Bilexical grammars and a cubic-time
probabilistic parser. In Proceedings of the Fifth Inter-
national Workshop on Parsing Technologies (IWPT),
pages 54–65, Cambridge, MA, USA.
Carlos G´omez-Rodr´ıguez, John Carroll, and David J. Weir.
2011. Dependency parsing schemata and mildly non-
projective dependency parsing. Computational Lin-
guistics, 37(3):541–586.
Aravind K. Joshi and Yves Schabes. 1997. Tree-
Adjoining Grammars. In Grzegorz Rozenberg and
Arto Salomaa, editors, Handbook of Formal Languages,
volume 3, pages 69–123. Springer.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proceedings of the 48th
Annual Meeting of the Association for Computational
Linguistics (ACL), pages 1–11, Uppsala, Sweden.
Marco Kuhlmann and Joakim Nivre. 2006. Mildly non-
projective dependency structures. In Proceedings of
the 21st International Conference on Computational
Linguistics (COLING) and 44th Annual Meeting of the
Association for Computational Linguistics (ACL) Main
Conference Poster Sessions, pages 507–514, Sydney,
Australia.
Wolfgang Maier and Timm Lichte. 2011. Characteriz-
ing discontinuity in constituent treebanks. In Philippe
de Groote, Markus Egg, and Laura Kallmeyer, editors,
Formal Grammar. 14th International Conference, FG
2009, Bordeaux, France, July 25–26, 2009, Revised
Selected Papers, volume 5591 of Lecture Notes in Com-
puter Science, pages 167–182. Springer.
Ryan McDonald and Giorgio Satta. 2007. On the com-
plexity of non-projective data-driven dependency pars-
ing. In Proceedings of the Tenth International Confer-
ence on Parsing Technologies (IWPT), pages 121–132,
Prague, Czech Republic.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan
Hajiˇc. 2005. Non-projective dependency parsing using
spanning tree algorithms. In Human Language Techno-
logy Conference (HLT) and Conference on Empirical
Methods in Natural Language Processing (EMNLP),
pages 523–530, Vancouver, Canada.
Emily Pitler, Sampath Kannan, and Mitchell Marcus.
2012. Dynamic programming for higher order parsing
of gap-minding trees. In Proceedings of the 2012 Joint
Conference on Empirical Methods in Natural Language
Processing (EMNLP) and Computational Natural Lan-
guage Learning (CoNLL), pages 478–488, Jeju Island,
Republic of Korea.
Emily Pitler, Sampath Kannan, and Mitchell Marcus.
2013. Finding optimal 1-endpoint-crossing trees.
Transactions of the Association for Computational Lin-
guistics.
Giorgio Satta and William Schuler. 1998. Restrictions on
tree adjoining languages. In Proceedings of the 36th
Annual Meeting of the Association for Computational
Linguistics (ACL) and 17th International Conference
on Computational Linguistics (COLING), pages 1176–
1182, Montr´eal, Canada.
Stuart M. Shieber, Yves Schabes, and Fernando Pereira.
1995. Principles and implementation of deductive pars-
ing. Journal of Logic Programming, 24(1–2):3–36.
</reference>
<page confidence="0.996937">
278
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.795319">
<title confidence="0.999956">Efficient Parsing for Head-Split Dependency Trees</title>
<author confidence="0.999986">Giorgio Satta Marco Kuhlmann</author>
<affiliation confidence="0.996118">Dept. of Information Engineering Dept. of Linguistics and Philology University of Padua, Italy Uppsala University, Sweden</affiliation>
<email confidence="0.810104">satta@dei.unipd.itmarco.kuhlmann@lingfil.uu.se</email>
<abstract confidence="0.999245785714286">Head splitting techniques have been successfully exploited to improve the asymptotic runtime of parsing algorithms for projective dependency trees, under the arc-factored model. In this article we extend these techniques to a class of non-projective dependency trees, called well-nested dependency trees with block-degree at most 2, which has been previously investigated in the literature. We define a structural property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Manuel Bodirsky</author>
<author>Marco Kuhlmann</author>
<author>Mathias M¨ohl</author>
</authors>
<title>Well-nested drawings as models of syntactic structure.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Conference on Formal Grammar (FG) and Ninth Meeting on Mathematics of Language (MOL),</booktitle>
<pages>195--203</pages>
<location>Edinburgh, UK.</location>
<marker>Bodirsky, Kuhlmann, M¨ohl, 2005</marker>
<rawString>Manuel Bodirsky, Marco Kuhlmann, and Mathias M¨ohl. 2005. Well-nested drawings as models of syntactic structure. In Proceedings of the 10th Conference on Formal Grammar (FG) and Ninth Meeting on Mathematics of Language (MOL), pages 195–203, Edinburgh, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL),</booktitle>
<pages>149--164</pages>
<location>New York, USA.</location>
<contexts>
<context position="38396" citStr="Buchholz and Marsi, 2006" startWordPosition="7106" endWordPosition="7109">nherit property, this even increases to two orders of magnitude. However, as already stated in §2, this improvement is paid for by a loss in coverage; for instance, trees of the form shown in Figure 3 cannot be parsed any longer. 7.1 Quantitative Evaluation In order to assess the empirical loss in coverage that the restriction to head-split trees incurs, we evaluated the coverage of several classes of dependency trees on standard data sets. Following Pitler et al. (2012), we report in Table 1 figures for the training sets of six languages used in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006). As we can see, the O(n6) class of head-split trees has only slightly lower coverage on this data than the baseline class of well-nested dependency trees with block-degree at most 2. The losses are up to 0.2 percentage points on five of the six languages, and 0.9 points on the Dutch data. Our even more restricted O(n5) class of 1-inherit head-split trees has the same coverage as our O(n6) class, which is expected given the results of Pitler et al. (2012): Their O(n6) class of 1-inherit trees has exactly the same coverage as the baseline (and thereby more coverage than our O(n6) class). Intere</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of the Tenth Conference on Computational Natural Language Learning (CoNLL), pages 149–164, New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xavier Carreras</author>
</authors>
<title>Experiments with a higher-order projective dependency parser.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007,</booktitle>
<pages>957--961</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1283" citStr="Carreras, 2007" startWordPosition="190" endWordPosition="191">al property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O(n3), where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O(n2) (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complex</context>
</contexts>
<marker>Carreras, 2007</marker>
<rawString>Xavier Carreras. 2007. Experiments with a higher-order projective dependency parser. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 957–961, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joan Chen-Main</author>
<author>Aravind K Joshi</author>
</authors>
<title>Unavoidable ill-nestedness in natural language and the adequacy of tree local-MCTAG induced dependency structures.</title>
<date>2010</date>
<booktitle>In Proceedings of the Tenth International Conference on Tree Adjoining Grammars and Related Formalisms (TAG+),</booktitle>
<location>New Haven, USA.</location>
<contexts>
<context position="2396" citStr="Chen-Main and Joshi, 2010" startWordPosition="365" endWordPosition="368">‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2. This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restriction to this class (Maier and Lichte, 2011; Chen-Main and Joshi, 2010), Kuhlmann and Nivre (2006) found that it has excellent coverage on standard data sets. Assuming an arc-factored model, well-nested dependency trees with block-degree &lt; 2 can be parsed in time O(n7) using the algorithm of G´omez-Rodr´ıguez et al. (2011). Recently, Pitler et al. (2012) have shown that if an additional restriction called 1-inherit is imposed, parsing can be done in time O(n6), without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O(n5) time complexity. The O(n3) time result r</context>
</contexts>
<marker>Chen-Main, Joshi, 2010</marker>
<rawString>Joan Chen-Main and Aravind K. Joshi. 2010. Unavoidable ill-nestedness in natural language and the adequacy of tree local-MCTAG induced dependency structures. In Proceedings of the Tenth International Conference on Tree Adjoining Grammars and Related Formalisms (TAG+), New Haven, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>Efficient parsing for bilexical context-free grammars and Head Automaton Grammars.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>457--464</pages>
<location>College Park, MD, USA.</location>
<contexts>
<context position="3382" citStr="Eisner and Satta, 1999" startWordPosition="516" endWordPosition="519">sing can be done in time O(n6), without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O(n5) time complexity. The O(n3) time result reported by Eisner (1997) has been obtained by exploiting more sophisticated dynamic programming techniques that ‘split’ dependency trees at the position of their heads, in order to save bookkeeping. Splitting techniques have also been exploited to speed up parsing time for other lexicalized formalisms, such as bilexical context-free grammars and head automata (Eisner and Satta, 1999). However, to our knowledge no attempt has been made in the literature to extend these techniques to non-projective dependency parsing. In this article we leverage the central idea from Eisner’s algorithm and extend it to the class of wellnested dependency trees with block-degree at most 2. 267 Transactions of the Association for Computational Linguistics, 1 (2013) 267–278. Action Editor: Brian Roark. Submitted 3/2013; Published 7/2013. c�2013 Association for Computational Linguistics. We introduce a structural property, called head-split, that allows us to split these trees at the positions o</context>
<context position="5249" citStr="Eisner and Satta (1999)" startWordPosition="810" endWordPosition="813">the algorithm by Pitler et al. (2012) for the 1-inherit class without the head-split condition. The above results have consequences also for the parsing of other related formalisms, such as the already mentioned lexicalized tree-adjoining grammars. This will be discussed in the final section. 2 Head Splitting To introduce the basic idea of this article, we briefly discuss in this section two well-known algorithms for computing the set of all projective dependency trees for a given input sentence: the naive, CKY-style algorithm, and the improved algorithm with head splitting, in the version of Eisner and Satta (1999).1 CKY parsing The CKY-style algorithm works in a pure bottom-up way, building dependency trees by combining subtrees. Assuming an input string w = a1 • • • an, n &gt; 1, each subtree t is represented by means of a finite signature [i, j, h], called item, where i, j are the boundary positions of t’s span over w and h is the position of t’s root. This is the only information we need in order to combine subtrees under the arc-factored model. Note that the number of possible signatures is O(n3). The main step of the algorithm is displayed in Figure 1(a). Here we introduce the graphical convention, u</context>
</contexts>
<marker>Eisner, Satta, 1999</marker>
<rawString>Jason Eisner and Giorgio Satta. 1999. Efficient parsing for bilexical context-free grammars and Head Automaton Grammars. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics (ACL), pages 457–464, College Park, MD, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
<author>Giorgio Satta</author>
</authors>
<title>A faster parsing algorithm for lexicalized Tree-Adjoining Grammars.</title>
<date>2000</date>
<booktitle>In Proceedings of the Fifth Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+),</booktitle>
<pages>14--19</pages>
<location>Paris, France.</location>
<marker>Eisner, Satta, 2000</marker>
<rawString>Jason Eisner and Giorgio Satta. 2000. A faster parsing algorithm for lexicalized Tree-Adjoining Grammars. In Proceedings of the Fifth Workshop on Tree Adjoining Grammars and Related Formalisms (TAG+), pages 14– 19, Paris, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and a cubic-time probabilistic parser.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth International Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>54--65</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1018" citStr="Eisner (1997)" startWordPosition="147" endWordPosition="148"> under the arc-factored model. In this article we extend these techniques to a class of non-projective dependency trees, called well-nested dependency trees with block-degree at most 2, which has been previously investigated in the literature. We define a structural property that allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O(n3), where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time </context>
<context position="3020" citStr="Eisner (1997)" startWordPosition="466" endWordPosition="467">and Nivre (2006) found that it has excellent coverage on standard data sets. Assuming an arc-factored model, well-nested dependency trees with block-degree &lt; 2 can be parsed in time O(n7) using the algorithm of G´omez-Rodr´ıguez et al. (2011). Recently, Pitler et al. (2012) have shown that if an additional restriction called 1-inherit is imposed, parsing can be done in time O(n6), without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O(n5) time complexity. The O(n3) time result reported by Eisner (1997) has been obtained by exploiting more sophisticated dynamic programming techniques that ‘split’ dependency trees at the position of their heads, in order to save bookkeeping. Splitting techniques have also been exploited to speed up parsing time for other lexicalized formalisms, such as bilexical context-free grammars and head automata (Eisner and Satta, 1999). However, to our knowledge no attempt has been made in the literature to extend these techniques to non-projective dependency parsing. In this article we leverage the central idea from Eisner’s algorithm and extend it to the class of wel</context>
<context position="6128" citStr="Eisner (1997)" startWordPosition="971" endWordPosition="972">e the boundary positions of t’s span over w and h is the position of t’s root. This is the only information we need in order to combine subtrees under the arc-factored model. Note that the number of possible signatures is O(n3). The main step of the algorithm is displayed in Figure 1(a). Here we introduce the graphical convention, used throughout this article, of representing a subtree by a shaded area, with an horizontal line indicating the spanned fragment of the input string, and of marking the position of the head by a bullet. The illustrated step attaches a tree with signature [k, j, d] 1Eisner (1997) describes a slightly different algorithm. Figure 1: Basic steps for (a) the CKY-style algorithm and (b, c) the head splitting algorithm. as a dependent of a tree with signature [i, k, h]. There can be O(n5) instantiations of this step, and this is also the running time of the algorithm. Eisner’s algorithm Eisner and Satta (1999) improve over the CKY algorithm by reducing the number of position records in an item. They do this by ‘splitting’ each tree into a left and a right fragment, so that the head is always placed at one of the two boundary positions of a fragment, as opposed to being plac</context>
</contexts>
<marker>Eisner, 1997</marker>
<rawString>Jason Eisner. 1997. Bilexical grammars and a cubic-time probabilistic parser. In Proceedings of the Fifth International Workshop on Parsing Technologies (IWPT), pages 54–65, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos G´omez-Rodr´ıguez</author>
<author>John Carroll</author>
<author>David J Weir</author>
</authors>
<title>Dependency parsing schemata and mildly nonprojective dependency parsing.</title>
<date>2011</date>
<journal>Computational Linguistics,</journal>
<volume>37</volume>
<issue>3</issue>
<marker>G´omez-Rodr´ıguez, Carroll, Weir, 2011</marker>
<rawString>Carlos G´omez-Rodr´ıguez, John Carroll, and David J. Weir. 2011. Dependency parsing schemata and mildly nonprojective dependency parsing. Computational Linguistics, 37(3):541–586.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>TreeAdjoining Grammars.</title>
<date>1997</date>
<booktitle>In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--123</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="2268" citStr="Joshi and Schabes, 1997" startWordPosition="346" endWordPosition="349">05), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2. This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restriction to this class (Maier and Lichte, 2011; Chen-Main and Joshi, 2010), Kuhlmann and Nivre (2006) found that it has excellent coverage on standard data sets. Assuming an arc-factored model, well-nested dependency trees with block-degree &lt; 2 can be parsed in time O(n7) using the algorithm of G´omez-Rodr´ıguez et al. (2011). Recently, Pitler et al. (2012) have shown that if an additional restriction called 1-inherit is imposed, parsing can be done in time O(n6), without any additional loss in coverage on standard data sets. Standard conte</context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind K. Joshi and Yves Schabes. 1997. TreeAdjoining Grammars. In Grzegorz Rozenberg and Arto Salomaa, editors, Handbook of Formal Languages, volume 3, pages 69–123. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1--11</pages>
<location>Uppsala,</location>
<contexts>
<context position="1307" citStr="Koo and Collins, 2010" startWordPosition="192" endWordPosition="195"> allows head splitting for these trees, and present two algorithms that improve over the runtime of existing algorithms at no significant loss in coverage. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O(n3), where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O(n2) (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre,</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1–11, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Kuhlmann</author>
<author>Joakim Nivre</author>
</authors>
<title>Mildly nonprojective dependency structures.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association for Computational Linguistics (ACL) Main Conference Poster Sessions,</booktitle>
<pages>507--514</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="1462" citStr="Kuhlmann and Nivre, 2006" startWordPosition="219" endWordPosition="222">ge. 1 Introduction Much of the recent work on dependency parsing has been aimed at finding a good balance between accuracy and efficiency. For one end of the spectrum, Eisner (1997) showed that the highest-scoring projective dependency tree under an arc-factored model can be computed in time O(n3), where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O(n2) (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2. </context>
</contexts>
<marker>Kuhlmann, Nivre, 2006</marker>
<rawString>Marco Kuhlmann and Joakim Nivre. 2006. Mildly nonprojective dependency structures. In Proceedings of the 21st International Conference on Computational Linguistics (COLING) and 44th Annual Meeting of the Association for Computational Linguistics (ACL) Main Conference Poster Sessions, pages 507–514, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wolfgang Maier</author>
<author>Timm Lichte</author>
</authors>
<title>Characterizing discontinuity in constituent treebanks.</title>
<date>2011</date>
<booktitle>Formal Grammar. 14th International Conference, FG 2009,</booktitle>
<volume>5591</volume>
<pages>167--182</pages>
<editor>In Philippe de Groote, Markus Egg, and Laura Kallmeyer, editors,</editor>
<publisher>Springer.</publisher>
<location>Bordeaux, France,</location>
<contexts>
<context position="2368" citStr="Maier and Lichte, 2011" startWordPosition="361" endWordPosition="364"> authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2. This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restriction to this class (Maier and Lichte, 2011; Chen-Main and Joshi, 2010), Kuhlmann and Nivre (2006) found that it has excellent coverage on standard data sets. Assuming an arc-factored model, well-nested dependency trees with block-degree &lt; 2 can be parsed in time O(n7) using the algorithm of G´omez-Rodr´ıguez et al. (2011). Recently, Pitler et al. (2012) have shown that if an additional restriction called 1-inherit is imposed, parsing can be done in time O(n6), without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O(n5) time complex</context>
</contexts>
<marker>Maier, Lichte, 2011</marker>
<rawString>Wolfgang Maier and Timm Lichte. 2011. Characterizing discontinuity in constituent treebanks. In Philippe de Groote, Markus Egg, and Laura Kallmeyer, editors, Formal Grammar. 14th International Conference, FG 2009, Bordeaux, France, July 25–26, 2009, Revised Selected Papers, volume 5591 of Lecture Notes in Computer Science, pages 167–182. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Giorgio Satta</author>
</authors>
<title>On the complexity of non-projective data-driven dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the Tenth International Conference on Parsing Technologies (IWPT),</booktitle>
<pages>121--132</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="1724" citStr="McDonald and Satta, 2007" startWordPosition="261" endWordPosition="264"> model can be computed in time O(n3), where n is the length of the input string. Later work has focused on making projective parsing viable under more expressive models (Carreras, 2007; Koo and Collins, 2010). At the same time, it has been observed that for many standard data sets, the coverage of projective trees is far from complete (Kuhlmann and Nivre, 2006), which has led to an interest in parsing algorithms for non-projective trees. While non-projective parsing under an arc-factored model can be done in time O(n2) (McDonald et al., 2005), parsing with more informed models is intractable (McDonald and Satta, 2007). This has led several authors to investigate ‘mildly non-projective’ classes of trees, with the goal of achieving a balance between expressiveness and complexity (Kuhlmann and Nivre, 2006). In this article we focus on a class of mildly nonprojective dependency structures called well-nested dependency trees with block-degree at most 2. This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restr</context>
</contexts>
<marker>McDonald, Satta, 2007</marker>
<rawString>Ryan McDonald and Giorgio Satta. 2007. On the complexity of non-projective data-driven dependency parsing. In Proceedings of the Tenth International Conference on Parsing Technologies (IWPT), pages 121–132, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>523--530</pages>
<location>Vancouver, Canada.</location>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005. Non-projective dependency parsing using spanning tree algorithms. In Human Language Technology Conference (HLT) and Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 523–530, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Sampath Kannan</author>
<author>Mitchell Marcus</author>
</authors>
<title>Dynamic programming for higher order parsing of gap-minding trees.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing (EMNLP) and Computational Natural Language Learning (CoNLL),</booktitle>
<pages>478--488</pages>
<location>Jeju Island, Republic of</location>
<contexts>
<context position="2681" citStr="Pitler et al. (2012)" startWordPosition="410" endWordPosition="413"> This class was first introduced by Bodirsky et al. (2005), who showed that it corresponds, in a natural way, to the class of derivation trees of lexicalized tree-adjoining grammars (Joshi and Schabes, 1997). While there are linguistic arguments against the restriction to this class (Maier and Lichte, 2011; Chen-Main and Joshi, 2010), Kuhlmann and Nivre (2006) found that it has excellent coverage on standard data sets. Assuming an arc-factored model, well-nested dependency trees with block-degree &lt; 2 can be parsed in time O(n7) using the algorithm of G´omez-Rodr´ıguez et al. (2011). Recently, Pitler et al. (2012) have shown that if an additional restriction called 1-inherit is imposed, parsing can be done in time O(n6), without any additional loss in coverage on standard data sets. Standard context-free parsing methods, when adapted to the parsing of projective trees, provide O(n5) time complexity. The O(n3) time result reported by Eisner (1997) has been obtained by exploiting more sophisticated dynamic programming techniques that ‘split’ dependency trees at the position of their heads, in order to save bookkeeping. Splitting techniques have also been exploited to speed up parsing time for other lexic</context>
<context position="4663" citStr="Pitler et al. (2012)" startWordPosition="719" endWordPosition="722">educes the class of trees that can be generated. However, we show that the restriction to head-split trees comes at no significant loss in coverage, and it allows parsing in time O(n6), an asymptotic improvement of one order of magnitude over the algorithm by G´omez-Rodr´ıguez et al. (2011) for the unrestricted class. We also show that restricting the class of head-split trees by imposing the already mentioned 1-inherit property does not cause any additional loss in coverage, and that parsing for the combined class is possible in time O(n5), one order of magnitude faster than the algorithm by Pitler et al. (2012) for the 1-inherit class without the head-split condition. The above results have consequences also for the parsing of other related formalisms, such as the already mentioned lexicalized tree-adjoining grammars. This will be discussed in the final section. 2 Head Splitting To introduce the basic idea of this article, we briefly discuss in this section two well-known algorithms for computing the set of all projective dependency trees for a given input sentence: the naive, CKY-style algorithm, and the improved algorithm with head splitting, in the version of Eisner and Satta (1999).1 CKY parsing</context>
<context position="25527" citStr="Pitler et al. (2012)" startWordPosition="4751" endWordPosition="4754">ut sentence. The worst case is due to deduction rules that combine two items, each of which represents trees with one gap. For instance, rule (11) involves six free indices ranging over Œ1; n•, and thus could be instantiated O.n6/ many times. If the head-split property does not hold, attachment of a dependent in one step results in time O.n7/, as seen for instance in G´omez-Rodr´ıguez et al. (2011). 6 Parsing of 1-Inherit Head-Split Trees In this section we specialize the parsing algorithm of •5 to a new, more efficient algorithm for a restricted class of trees. 6.1 1-Inherit Head-Split Trees Pitler et al. (2012) introduce a restriction on well-nested dependency trees with block-degree at most 2. A tree t satisfies the 1-inherit property if, for every node ah in t with bd.ah; t/ = 2, there is at most one dependency ah --* ad* such that gap.tŒad*•/ contains gap.tŒah•/. Informally, this means that yd.tŒad*•/ ‘crosses over’ gap.tŒah•/, and we say that ad* ‘inherits’ the gap of ah. In this section we investigate the parsing of head-split trees that also have the 1-inherit property. Example 4 Figure 6 shows a head node ah along with dependents adi, satisfying the head-split condition. Only tad1 has its yie</context>
<context position="34453" citStr="Pitler et al. (2012)" startWordPosition="6383" endWordPosition="6386">of Case 1, expanding an upper tree at one of its internal boundaries. Œi; j&apos;; p; q; p + 1•U Œj&apos;; j; —; —; p + 1•0 Œi; j; p; q; p + 1•U (22) Œi; j&apos;; p; q; j&apos;•U Œj&apos; — 1; j ; —; —; j•HH 275 Arabic Czech Danish Dutch Portuguese Swedish Number of trees 1,460 72,703 5,190 13,349 9,071 11,042 WN2 1,458 99.9% 72,321 99.5% 5,175 99.7% 12,896 96.6% 8,650 95.4% 10,955 99.2% Classes considered in this paper WN2 + HS O(n6) 1,457 99.8% 72,182 99.3% 5,174 99.7% 12,774 95.7% 8,648 95.3% 10,951 99.2% WN2 + HS + 1I 1,457 99.8% 72,182 99.3% 5,174 99.7% 12,774 95.7% 8,648 95.3% 10,951 99.2% Classes considered by Pitler et al. (2012) WN2 + 1I 1,458 99.9% 72,321 99.5% 5,175 99.7% 12,896 96.6% 8,650 95.4% 10,955 99.2% WN2 + 0I O(n5) 1,394 95.5% 70,695 97.2% 4,985 96.1% 12,068 90.4% 8,481 93.5% 10,787 97.7% Projective O(n3) 1,297 88.8% 55,872 76.8% 4,379 84.4% 8,484 63.6% 7,353 81.1% 9,963 90.2% Table 1: Coverage of various classes of dependency trees on the training sets used in the CoNLL-X shared task (WN2 = well-nested, block-degree &lt; 2; HS = head-split; 1I = 1-inherit; 0I = 0-inherit, ‘gap-minding’) Next, we combine the result of (22) with a parse for substring U2. The result is an item of the new type /LR&gt;. This item is</context>
<context position="38246" citStr="Pitler et al. (2012)" startWordPosition="7080" endWordPosition="7083">ll-nested dependency trees with block-degree at most 2. [p, p&apos; + 1,-,-,p&apos; + 1]0 ˚aj --* ap,+1 (24) [i, j, p, q,j]U 276 In connection with the 1-inherit property, this even increases to two orders of magnitude. However, as already stated in §2, this improvement is paid for by a loss in coverage; for instance, trees of the form shown in Figure 3 cannot be parsed any longer. 7.1 Quantitative Evaluation In order to assess the empirical loss in coverage that the restriction to head-split trees incurs, we evaluated the coverage of several classes of dependency trees on standard data sets. Following Pitler et al. (2012), we report in Table 1 figures for the training sets of six languages used in the CoNLL-X shared task on dependency parsing (Buchholz and Marsi, 2006). As we can see, the O(n6) class of head-split trees has only slightly lower coverage on this data than the baseline class of well-nested dependency trees with block-degree at most 2. The losses are up to 0.2 percentage points on five of the six languages, and 0.9 points on the Dutch data. Our even more restricted O(n5) class of 1-inherit head-split trees has the same coverage as our O(n6) class, which is expected given the results of Pitler et a</context>
</contexts>
<marker>Pitler, Kannan, Marcus, 2012</marker>
<rawString>Emily Pitler, Sampath Kannan, and Mitchell Marcus. 2012. Dynamic programming for higher order parsing of gap-minding trees. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing (EMNLP) and Computational Natural Language Learning (CoNLL), pages 478–488, Jeju Island, Republic of Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily Pitler</author>
<author>Sampath Kannan</author>
<author>Mitchell Marcus</author>
</authors>
<title>Finding optimal 1-endpoint-crossing trees. Transactions of the Association for Computational Linguistics.</title>
<date>2013</date>
<contexts>
<context position="41498" citStr="Pitler et al. (2013)" startWordPosition="7604" endWordPosition="7607">nctuation-free data, only 9 remain when one ignores those with coordination. For the remaining ones, we have not been able to identify any clear patterns. 8 Concluding Remarks In this article we have extended head splitting techniques, originally developed for parsing of projective dependency trees, to two subclasses of well-nested dependency trees with block-degree at most 2. We have improved over the asymptotic runtime of two existing algorithms, at no significant loss in coverage. With the same goal of improving parsing efficiency for subclasses of non-projective trees, in very recent work Pitler et al. (2013) have proposed an O(n4) time algorithm for a subclass of non-projective trees that are not well-nested, using an approach that is orthogonal to the one we have explored here. Other than for dependency parsing, our results have also implications for mildly context-sensitive phrase structure formalisms. In particular, the algorithm of §5 can be adapted to parse a subclass of lexicalized tree-adjoining grammars, improving the result by Eisner and Satta (2000) from O(n7) to O(n6). Similarly, the algorithm of §6 can be adapted to parse a lexicalized version of the tree-adjoining grammars investigat</context>
</contexts>
<marker>Pitler, Kannan, Marcus, 2013</marker>
<rawString>Emily Pitler, Sampath Kannan, and Mitchell Marcus. 2013. Finding optimal 1-endpoint-crossing trees. Transactions of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giorgio Satta</author>
<author>William Schuler</author>
</authors>
<title>Restrictions on tree adjoining languages.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING),</booktitle>
<pages>1176--1182</pages>
<location>Montr´eal, Canada.</location>
<marker>Satta, Schuler, 1998</marker>
<rawString>Giorgio Satta and William Schuler. 1998. Restrictions on tree adjoining languages. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics (ACL) and 17th International Conference on Computational Linguistics (COLING), pages 1176– 1182, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Yves Schabes</author>
<author>Fernando Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<contexts>
<context position="16746" citStr="Shieber et al. (1995)" startWordPosition="3012" endWordPosition="3015">e distinguish among several item types, indicated by the value of subscript X. These types are specific to each parsing algorithm, and will be defined in later sections. 5 Parsing of Head-Split Trees We present in this section our first tabular algorithm for computing the set of all dependency trees for an input sentence w that have the head-split property, under the arc-factored model. Recall that tai denotes a tree with root ai, and tL,ai and tU,ai are the lower and upper trees of tai. The steps of the algorithm are specified by means of deduction rules over items, following the approach of Shieber et al. (1995). 5.1 Basic Idea Our algorithm builds trees step by step, by attaching a tree tah0 as a dependent of a tree tah and creating the new dependency ah ! ah0. Computationally, the worst case for this operation is when both tah and tah0 have a gap; then, for each tree we need to keep a record of the four boundaries, along with the position of the head, as done by G´omez-Rodriguez et al. (2011). However, if we are interested in parsing trees that satisfy the head-split property, we can avoid representing a tree with a gap by means of a single item. We instead follow the general idea of §2 for project</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>Stuart M. Shieber, Yves Schabes, and Fernando Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1–2):3–36.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>