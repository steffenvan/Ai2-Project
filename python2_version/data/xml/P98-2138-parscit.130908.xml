<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004350">
<title confidence="0.649973">
Combining Trigram and Winnow in Thai OCR Error Correction
</title>
<author confidence="0.852777">
Surapant Meknavin
</author>
<affiliation confidence="0.851057">
National Electronics and Computer Technology Center
</affiliation>
<address confidence="0.734052">
73/1 Rama VI Road, Rajthevi, Bangkok, Thailand
</address>
<email confidence="0.924529">
surapan@nectec.or.th
</email>
<author confidence="0.992864">
Boonserm Kijsirikul, Ananlada Chotimongkol and Cholwich Nuttee
</author>
<affiliation confidence="0.986044">
Department of Computer Engineering
Chulalongkorn University, Thailand
</affiliation>
<email confidence="0.989571">
fengbks@chulkn.chula.ac.th
</email>
<sectionHeader confidence="0.996565" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998476">
For languages that have no explicit word bound-
ary such as Thai, Chinese and Japanese, cor-
recting words in text is harder than in English
because of additional ambiguities in locating er-
ror words. The traditional method handles this
by hypothesizing that every substrings in the
input sentence could be error words and trying
to correct all of them. In this paper, we pro-
pose the idea of reducing the scope of spelling
correction by focusing only on dubious areas in
the input sentence. Boundaries of these dubious
areas could be obtained approximately by ap-
plying word segmentation algorithm and finding
word sequences with low probability. To gener-
ate the candidate correction words, we used a
modified edit distance which reflects the charac-
teristic of Thai OCR errors. Finally, a part-of-
speech trigram model and Winnow algorithm
are combined to determine the most probable
correction.
</bodyText>
<sectionHeader confidence="0.998883" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999946549019608">
Optical character recognition (OCR) is useful
in a wide range of applications, such as office
automation and information retrieval system.
However, OCR in Thailand is still not widely
used, partly because existing Thai OCRs are
not quite satisfactory in terms of accuracy. Re-
cently, several research projects have focused on
spelling correction for many types of errors in-
cluding those from OCR (Kukich, 1992). Nev-
ertheless, the strategy is slightly different from
language to language, since the characteristic of
each language is different.
Two characteristics of Thai which make the
task of error correction different from those of
other languages are: (1) there is no explicit
word boundary, and (2) characters are written
in three levels; i.e., the middle, the upper and
the lower levels. In order to solve the prob-
lem of OCR error correction, the first task is
usually to detect error strings in the input sen-
tence. For languages that have explicit word
boundary such as English in which each word
is separated from the others by white spaces,
this task is comparatively simple. If the tok-
enized string is not found in the dictionary, it
could be an error string or an unknown word.
However, for the languages that have no ex-
plicit word boundary such as Chinese, Japanese
and Thai, this task is much more complicated.
Even without errors from OCR, it is difficult to
determine word boundary in these languages.
The situation gets worse when noises are intro-
duced in the text. The existing approach for
correcting the spelling error in the languages
that have no word boundary assumes that all
substrings in input sentence are error strings,
and then tries to correct them (Nagata, 1996).
This is computationally expensive since a large
portion of the input sentence is correct. The
other characteristic of Thai writing system is
that we have many levels for placing Thai char-
acters and several characters can occupy more
than one level. These characters are easily con-
nected to other characters in the upper or lower
level. These connected characters cause diffi-
culties in the process of character segmentation
which then cause errors in Thai OCR.
Other than the above problems specific to
Thai, real-word error is another source of er-
rors that is difficult to correct. Several previous
works on spelling correction demonstrated that
</bodyText>
<page confidence="0.996564">
836
</page>
<figure confidence="0.990974285714286">
tone
vowel 41=4 g a) u upper level
e
Itoplin
middle level
baseline
Iconsonant lower level
</figure>
<figureCaption confidence="0.999946">
Figure 1: No explicit word delimiter in Thai
</figureCaption>
<bodyText confidence="0.999972433333333">
feature-based approaches are very effective for
solving this problem.
In this paper, a hybrid method for Thai OCR
error correction is proposed. The method com-
bines the part-of-speech (POS) trigram model
with a feature-based model. First, the POS tri-
gram model is employed to correct non-word as
well as real-word errors. In this step, the num-
ber of non-word errors are mostly reduced, but
some real-word errors still remain because the
POS trigram model cannot capture some use-
ful features in discriminating candidate words.
A feature-based approach using Winnow algo-
rithm is then applied to correct the remaining
errors. In order to overcome the expensive com-
putation cost of the existing approach, we pro-
pose the idea of reducing the scope of correc-
tion by using word segmentation algorithm to
find the approximate error strings from the in-
put sentence. Though the word segmentation
algorithm cannot give the accurate boundary of
an error string, many of them can give clues
of unknown strings which may be error strings.
We can use this information to reduce the scope
of correction from entire sentence to a more nar-
row scope. Next, to capture the characteristic
of Thai OCR errors, we have defined the modi-
fied edit distance and use it to enumerate plau-
sible candidates which deviate from the word in
question within k-edit distance.
</bodyText>
<sectionHeader confidence="0.661718" genericHeader="method">
2 Problems of Thai OCR
</sectionHeader>
<bodyText confidence="0.9693845">
The problem of OCR error correction can be
defined as : given the string of characters
S = cicz • • • CT/ produced by OCR, find the
word sequence W = w1w2 wi that maximizes
the probability P(WIS). Before describing the
methods used to model P(W1S), below we list
some main characteristics of Thai that poses dif-
ficulties for correcting Thai OCR error.
</bodyText>
<listItem confidence="0.99045425">
• Words are written consecutively without
word boundary delimiters such as white
space characters. For example, the phrase
&amp;quot;CluluilsliW (Japan at present) in Figure
</listItem>
<bodyText confidence="0.991809444444444">
1, actually consists of three words: &amp;quot;cku&amp;quot;
(Japan), lu&amp;quot; (at), and &amp;quot;i7sltiu&amp;quot; (present).
Therefore, Thai OCR error correction has
to overcome word boundary ambiguity as
well as select the most probable correction
candidate at the same time. This is similar
to the problem of Connected Speech Recog-
nition and is sometimes called Connected
Text Recognition (Ingels, 1996).
</bodyText>
<listItem confidence="0.93287225">
• There are 3 levels for placing Thai charac-
ters and some characters can occupy more
than one level. For example, in Figure 2
&amp;quot;le consists of characters in three levels,
</listItem>
<bodyText confidence="0.999671375">
i.e., &amp;quot;, and il are in the top, the bot-
tom, the middle and both the middle and
top levels, respectively. The character that
occupies more than one level like i usually
connects to other characters (A) and causes
error on the output of OCR, i.e., A may
be recognized as i1 or I. Therefore, to cor-
rect characters produced by OCR, not only
substitution errors but also deletion and in-
sertion errors must be considered. In addi-
tion, in such a case, the candidates ranked
by OCR output are unreliable and cannot
be used to reduce search space. This is
because the connected characters tend to
have very different features from the origi-
nal separated ones.
</bodyText>
<page confidence="0.994476">
837
</page>
<figureCaption confidence="0.9670825">
Figure 2: Three levels for placing Thai charac-
ters
</figureCaption>
<sectionHeader confidence="0.999015" genericHeader="method">
3 Our Methods
</sectionHeader>
<subsectionHeader confidence="0.954691">
3.1 Trigram Model
</subsectionHeader>
<bodyText confidence="0.999674">
To find W that maximizes P(WIS), we can use
the POS trigram model as follows.
</bodyText>
<equation confidence="0.871472333333333">
arg max P(WIS)
argm,P(W)P(SIW)/P(S) (1)
arg max P(W)P(SIW) (2)
</equation>
<bodyText confidence="0.999774333333333">
The probability P(W) is given by the lan-
guage model and can be estimated by the tri-
gram model as:
</bodyText>
<equation confidence="0.9961745">
P(W) = P(W,T)
(3)
</equation>
<bodyText confidence="0.9879664">
P(SiW) is the characteristics of specific
OCR, and can be estimated by collecting sta-
tistical information from original text and the
text produced by OCR. We assume that given
the original word sequence W composed of char-
acters v1v2 vm, OCR produces the sequence
as string S (-= c1c2... cn) by repeatedly apply-
ing the following operation: substitute a char-
acter with another; insert a character; or delete
a character. Let Si be the i-prefix of S that
is formed by first character to the i-character
of S (= ci), and similarly Wi is the j-
prefix of W (= v1v2 vi). Using dynamic pro-
gramming technique, we can calculate P(SIW)
(= P(S„ I Wm)) by the following equation:
</bodyText>
<equation confidence="0.999803666666667">
P(SilWi) = max(P(Si_ilWi)* P(ins(g)),
P(del(vi)),
P(Si_11Wi9...i)*P(cilvj)) (4)
</equation>
<bodyText confidence="0.999965210526316">
where P(ins(c)), P(del(v)) and P(c1v) are the
probabilities that letter c is inserted, letter v is
deleted and letter v is substituted with c, re-
spectively.
One method to do OCR error correction us-
ing the above model is to hypothesize all sub-
strings in the input sentence as words (Nagata,
1996). Both words in the dictionary that ex-
actly match with the substrings and those that
approximately match are retrieved. To cope
with unknown words, all other substrings not
matched must also be considered. The word
lattice is then scanned to find the N-best word
sequences as correction candidates. In general,
this method is perfectly good, except in one as-
pect: its time complexity. Because it generates
a large number of hypothesized words and has
to find the best combination among them, it is
very slow.
</bodyText>
<subsectionHeader confidence="0.99929">
3.2 Selective Trigram Model
</subsectionHeader>
<bodyText confidence="0.997263">
To alleviate the above problem, we try to reduce
the number of hypothesized words by generat-
ing them only when needed. Having analyzed
the OCR output, we found that a large por-
tion of input sentence are correctly recognized
and need no approximation. Therefore, instead
of hypothesizing blindly through the whole sen-
tence, if we limit our hypotheses to only dubious
areas, we can save considerable amount of time.
Following is our algorithm for correcting OCR
output.
</bodyText>
<listItem confidence="0.905305181818182">
1. Find dubious areas: Find all substrings
in the input sentence that exactly match
words in the dictionary. Each substring
may overlap with others. The remaining
parts of sentence which are not covered by
any of these substrings are considered as
dubious areas.
2. Make hypotheses for nonwords and
unknown words:
(a) For each dubious string obtained from
1., the surrounding words are also con-
</listItem>
<bodyText confidence="0.981368">
sidered to form candidates for correc-
tion by concatenating them with the
dubious string. For example, in &amp;quot;in-
form at j on&amp;quot;, j is an unknown string
representing a dubious area, and in-
form at and on are words. In this
</bodyText>
<figure confidence="0.997008875">
consonant
I upper level
i
topliie
Imiddle level
baseline
Ilower level
tone
</figure>
<page confidence="0.98695">
838
</page>
<bodyText confidence="0.987548">
case, the unknown word and its sur-
rounding known words are combined
together, resulting in &amp;quot;informatjon&amp;quot; as
a new unknown string.
(b) For each unknown string obtained
form 2(a), apply the candidate genera-
tion routine to generate approximately
matched words within k-edit distance.
The value of k is varied proportionally
to the length of candidate word.
</bodyText>
<listItem confidence="0.995372260869565">
(c) All substrings except for ones that
violate Thai spelling rules, i.e., lead
by non-leading character, are hypoth-
esized as unknown words.
3. Find good word sequences: Find
the N-best word sequences according
to equation (2). For unknown words,
P(wi &apos;Unknown word) is computed by us-
ing the unknown word model in (Nagata,
1996).
4. Make hypotheses for real-word er-
ror: For each word wi in N-best word
sequence where the local probabilities
P(wi_i, w,wi+i, ti_1, 4, 4+1) are below a
threshold, generate candidate words by ap-
plying the process similar to step 2 except
that the nonword in step 2 is replaced with
the word wi. Find the word sequences
whose probabilities computed by equation
(2) are better than original ones.
5. Find the N-best word sequences:
From all word sequences obtained from step
4, select the N-best ones.
</listItem>
<bodyText confidence="0.989645555555556">
The candidate generation routine uses a mod-
ification of the standard edit distance and em-
ploys the error-tolerant finite-state recognition
algorithm (Oflazer, 1996) to generate candidate
words. The modified edit distance allows ar-
bitrary number of insertion and/or deletion of
upper level and lower level characters, but al-
lows no insertion or deletion of the middle level
characters. In the middle level, it allows only k
substitution. This is to reflect the characteristic
of Thai OCR which, 1. tends to merge several
characters into one when the character which
spans two levels are adjacent to characters in
the upper and lower level, and 2. rarely causes
insertion and deletion errors in the middle level.
For example, applying the candidate generation
routine with 1 edit distance to the string &amp;quot;14&amp;quot;
gives the set of candidates {14. 94. 3^4. 114, Vi4,
q4. 44-
From our experiments, we found that the se-
lective trigram model can deal with nonword
errors fairly well. However, the model is not
enough to correct real-word errors as well as
words with the same part of speech. This is
because the POS trigram model considers only
coarse information of POS in a fixed restricted
range of context, some useful information such
as specific word collocation may be lost. Using
word N-gram could recover some word-level in-
formation but requires an extremely large cor-
pus to estimate all parameters accurately and
consumes vast space resources to store the huge
word N-gram table. In addition, the model
losses generalized information at the level of
POS.
For English, a number of methods have
been proposed to cope with real-word errors in
spelling correction (Golding, 1995; Golding and
Roth, 1996; Golding and Schabes, 1993; Tong
and Evans, 1996). Among them, the feature-
based methods were shown to be superior to
other approaches. This is because the methods
can combine several kinds of features to deter-
mine the appropriate word in a given context.
For our task, we adopt a feature-based algo-
rithm called Winnow. There are two reasons
why we select Winnow. First, it has been shown
to be the best performer in English context-
sensitive spelling correction (Golding and Roth,
1996). Second, it was shown to be able to han-
dle difficult disambiguation tasks in Thai (Mek-
navin et al., 1997).
Below we describe Winnow algorithm that is
used for correcting real-word error.
</bodyText>
<subsectionHeader confidence="0.9487725">
3.3 Winnow Algorithm
3.3.1 The algorithm
</subsectionHeader>
<bodyText confidence="0.998722636363636">
A Winnow algorithm used in our experiment is
the algorithm described in (Blum, 1997). Win-
now is a multiplicative weight updating and in-
cremental algorithm (Littlestone, 1988; Golding
and Roth, 1996). The algorithm is originally de-
signed for learning two-class (positive and neg-
ative class) problems, and can be extended to
multiple-class problems as shown in Figure 3.
Winnow can be viewed as a network of one
target node connected to n nodes, called spe-
cialists, each of which examines one feature and
</bodyText>
<page confidence="0.997101">
839
</page>
<bodyText confidence="0.9926885">
Let V1,... V vni be the values of the target concept to be learned, and xi be the prediction of the
i-specialist.
</bodyText>
<listItem confidence="0.9988866">
1. Initialize the weights w1, , wn of all the specialists to 1.
2. For Each example x = {x1,...,xn} Do
(a) Let V be the value of the target concept of the example.
(b) Output 1.3i = arg wi
(c) If the algorithm makes a mistake 0 V), then:
</listItem>
<bodyText confidence="0.84763425">
i. for each xi equal to V, wi is updated to wi a
ii. for each xi equal to Oj, wi is updated to wi
where, a&gt; 1 and &lt; 1 are promotion parameter and demotion parameter, and are set to 3/2 and
1/2, respectively.
</bodyText>
<figureCaption confidence="0.998452">
Figure 3: The Winnow algorithm for learning multiple-class concept.
</figureCaption>
<bodyText confidence="0.999971767441861">
predicts xi as the value of the target concept.
The basic idea of the algorithm is that to ex-
tract some useful unknown features, the algo-
rithm asks for opinions from all specialists, each
of whom has his own specialty on one feature,
and then makes a global prediction based on a
weighted majority vote over all those opinions
as described in Step 2-(a) of Figure 3. In our ex-
periment, we have each specialist examine one
or two attributes of an example. For example,
a specialist may predict the value of the target
concept by checking for the pairs &amp;quot;(attribute].
= valuel) and (attribute2 = value2)&amp;quot;. These
pairs are candidates of features we are trying to
extract.
A specialist only makes a prediction if its con-
dition &amp;quot;(attributel = valuel)&amp;quot; is true in case
of one attribute, or both of its conditions &amp;quot;(at-
tributel = valuel) and (attibute2 = value2)&amp;quot;
are true in case of two attributes, and in that
case it predicts the most popular outcome out of
the last k times it had the chance to predict. A
specialist may choose to abstain instead of giv-
ing a prediction on any given example in case
that it did not see the same value of an attribute
in the example. In fact, we may have each spe-
cialist examines more than two attributes, but
for the sake of simplification of preliminary ex-
periment, let us assume that two attributes for
each specialist are enough to learn the target
concept.
The global algorithm updates the weight wi
of any specialist based on the vote of that spe-
cialist. The weight of any specialist is initialized
to 1. In case that the global algorithm predicts
incorrectly, the weight of the specialist that pre-
dicts incorrectly is halved and the weight of the
specialist that predicts correctly is multiplied by
3/2. This weight updating method is the same
as the one used in (Blum, 1997). The advan-
tage of Winnow, which made us decide to use
for our task, is that it is not sensitive to extra
irrelevant features (Littlestone, 1988).
</bodyText>
<subsectionHeader confidence="0.991549">
3.3.2 Constructing Confusion Set and
Defining Features
</subsectionHeader>
<bodyText confidence="0.998982954545455">
To employ Winnow in correcting OCR er-
rors, we first define k-edit distance confusion
set. A k-edit distance confusion set S =
{c, w1, , wn} is composed of one centroid
word c and words w1, w2, • • • , wn generated by
applying the candidate generation routine with
maximum k modified edit distance to the cen-
troid word. If a word c is produced by OCR
output or by the previous step, then it may be
corrected as wi, w2, ...,wn or c itself. For ex-
ample, suppose that the centroid word is know,
then all possible words in 1-edit distance con-
fusion set are {know, knob, knop, knot, knew,
enow, snow, known, not* Furthermore, words
with probability lower than a threshold are ex-
cluded from the set. For example, if a specific
OCR has low probability of substituting t with
w, &amp;quot;knot&amp;quot; should be excluded from the set.
Following previous works (Golding, 1995;
Meknavin et al., 1997), we have tried two types
of features: context words and collocations.
Context-word features is used to test for the
</bodyText>
<page confidence="0.98688">
840
</page>
<bodyText confidence="0.999667285714286">
presence of a particular word within +/- M
words of the target word, and collocations test
for a pattern of up to L contiguous words and/or
part-of-speech tags around the target word. In
our experiment M and L is set to 10 and 2,
respectively. Examples of features for discrimi-
nating between snow and know include:
</bodyText>
<listItem confidence="0.9015875">
(1) I {know, snow}
(2) winter within +10 words
</listItem>
<bodyText confidence="0.999988285714286">
where (1) is a collocation that tends to imply
know, and (2) is a context-word that tends to
imply snow. Then the algorithm should extract
the features (&amp;quot;word within +10 words of the
target word&amp;quot; = &amp;quot;winter&amp;quot;) as well as (&amp;quot;one word
before the target word&amp;quot; = &amp;quot;I&amp;quot;) as useful features
by assigning them with high weights.
</bodyText>
<subsectionHeader confidence="0.907575">
3.3.3 Using the Network to Rank
Sentences
</subsectionHeader>
<bodyText confidence="0.999976190476191">
After networks of k-edit distance confusion sets
are learned by Winnow, the networks are used
to correct the N-best sentences received from
POS trigram model. For each sentence, every
real word is evaluated by the network whose the
centroid word is that real word. The network
will then output the centroid word or any word
in the confusion set according to the context.
After the most probable word is determined, the
confidence level of that word will be calculated.
Since every specialist has weight voting for the
target word, we can consider the weight as con-
fidence level of that specialist for the word. We
define the confidence level of any word as all
weights that vote for that word divided by all
weights in the network. Based on the confidence
levels of all words in the sentence, the average
of them is taken as the confidence level of the
sentence. The N-best sentences are then re-
ranked according to the confidence level of the
sentences.
</bodyText>
<sectionHeader confidence="0.999849" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999887111111111">
We have prepared the corpus containing about
9,000 sentences (140,000 words, 1,300,000 char-
acters) for evaluating our methods. The corpus
is separated into two parts; the first part con-
taining about 80 % of the whole corpus is used
as a training set for both the trigram model
and Winnow, and the rest is used as a test set.
Based on the prepared corpus, experiments were
conducted to compare our methods. The results
</bodyText>
<table confidence="0.99871">
Type Error
Non-word Error 18.37%
Real-word Error 3.60%
Total 21.97%
</table>
<tableCaption confidence="0.8986595">
Table 1: The percentage of word error from
OCR
</tableCaption>
<table confidence="0.9998318">
Type Trigram Trigram +
Winnow
Non-word Error 82.16% 90.27%
Real-word Error 75.71% 87.60%
Introduced Error 1.42% 1.56%
</table>
<tableCaption confidence="0.9898425">
Table 2: The percentage of corrected word er-
rors after applying Trigram and Winnow
</tableCaption>
<bodyText confidence="0.992716035714286">
are shown in Table 1, and Table 2.
Table 1 shows the percentage of word errors
from the entire text. Table 2 shows the percent-
age of corrected word errors after applying Tri-
gram and Winnow. The result reveals that the
trigram model can correct non-word and real-
word, but introduced some new errors. By the
trigram model, real-word errors are more diffi-
cult to correct than non-word. Combining Win-
now to the trigram model, both types of errors
are further reduced, and improvement of real-
word error correction is more acute.
The reason for better performance of Tri-
gram+Winnow over Trigram alone is that the
former can exploit more useful features, i.e.,
context words and collocation features, in cor-
rection. For example, the word &amp;quot;lb&amp;quot; (to bring)
is frequently recognized as &amp;quot;lb&amp;quot; (water) because
the characters &amp;quot;r-1&amp;quot; is misreplaced with a sin-
gle character &amp;quot; *-1&amp;quot; by OCR. In this case, Tri-
gram cannot effectively recover the real-word
error &amp;quot;th&amp;quot; to the correct word &amp;quot;th&amp;quot;. The word
&amp;quot;th&amp;quot; is effectively corrected by Winnow as the
algorithm found the context words that indicate
the occurence of &amp;quot;Iii&amp;quot; such as the words &amp;quot;7:311D&amp;quot;
(evaporate) and &amp;quot;Air&amp;quot; (plant). Note that these
context words cannot be used by Trigram to
correct the real-word errors.
</bodyText>
<page confidence="0.996522">
841
</page>
<sectionHeader confidence="0.998997" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.99998225">
We have examined the application of the modi-
fied edit distance, POS trigram model and Win-
now algorithm to the task of Thai OCR error
correction. The experimental result shows that
our proposed method reduces both non-word er-
rors and real-word errors effectively. In future
work, we plan to test the method with much
more data and to incorporate other sources of
information to improve the quality of correc-
tion. It is also interesting to examine how
the method performs when applied to human-
generated misspellings.
</bodyText>
<sectionHeader confidence="0.984034" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.997675">
We would like to thank Paisarn Charoenporn-
sawat who helps us run experiment with Win-
now. This work was partly supported by the
Thai Government Research Fund.
</bodyText>
<sectionHeader confidence="0.997959" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995114227272727">
Avrim Blum. 1997. Empirical support for win-
now and weighted-majority algorithm: Re-
sults on a calendar scheduling domain. Ma-
chine Learning, 26.
Andrew R. Golding and Dan Roth. 1996. Ap-
plying winnow to context-sensitive spelling
correction. In Proceedings of the Thirteenth
International Conference on Machine Learn-
ing.
Andrew R. Golding and Yves Schabes. 1993.
Combining trigram-based and featured-based
methods for context-sensitive spelling cor-
rection. Technical Report TR-93-03a, Mit-
subishi Electric Research Laboratory.
Andrew R. Golding. 1995. A bayesian hybrid
method for context-sensitive spelling correc-
tion. In Proceedings of the Third Workshop
on Very Large Corpora.
Peter Ingels. 1996. Connected text recognition
using layered HMMs and token passing. In
Proceedings of the Second Conference on New
Methods in Language Processing.
Karen Kukich. 1992. Techniques for automati-
cally correction words in text. ACM Comput-
ing Surveys, 24(4).
Nick Littlestone. 1988. Learning quickly when
irrelevant attributes abound: A new linear-
threshold algorithm. Machine Learning, 2.
Surapant Meknavin, Paisarn Charoenporn-
sawat, and Boonserm Kijsirikul. 1997.
Feature-based Thai word segmentation. In
Proceedings of Natural Language Processing
Pacific Rim Symposium &apos;97.
Masaaki Nagata. 1996. Context-base spelling
correction for Japanese OCR. In Proceedings
of COLING &apos;96.
Kemal Oflazer. 1996. Error-tolerant finite-state
recognition with applications to morphologi-
cal analysis and spelling correction. Compu-
tational Linguistics, 22(1).
Xiang Tong and David A. Evans. 1996. A
statistical approach to automatic OCR error
correction in context. In Proceedings of the
Fourth Workshop on Very Large Corpora.
</reference>
<page confidence="0.997988">
842
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.486884">
<title confidence="0.99838">Combining Trigram and Winnow in Thai OCR Error Correction</title>
<author confidence="0.803407">Surapant Meknavin</author>
<affiliation confidence="0.998348">National Electronics and Computer Technology Center</affiliation>
<address confidence="0.999871">73/1 Rama VI Road, Rajthevi, Bangkok, Thailand</address>
<email confidence="0.965003">surapan@nectec.or.th</email>
<author confidence="0.7671">Ananlada Chotimongkol Nuttee Kijsirikul</author>
<affiliation confidence="0.9619705">Department of Computer Engineering Chulalongkorn University, Thailand</affiliation>
<email confidence="0.979173">fengbks@chulkn.chula.ac.th</email>
<abstract confidence="0.995269190476191">For languages that have no explicit word boundary such as Thai, Chinese and Japanese, correcting words in text is harder than in English because of additional ambiguities in locating error words. The traditional method handles this by hypothesizing that every substrings in the input sentence could be error words and trying to correct all of them. In this paper, we propose the idea of reducing the scope of spelling correction by focusing only on dubious areas in the input sentence. Boundaries of these dubious areas could be obtained approximately by applying word segmentation algorithm and finding word sequences with low probability. To generate the candidate correction words, we used a modified edit distance which reflects the characteristic of Thai OCR errors. Finally, a part-ofspeech trigram model and Winnow algorithm are combined to determine the most probable correction.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Avrim Blum</author>
</authors>
<title>Empirical support for winnow and weighted-majority algorithm: Results on a calendar scheduling domain.</title>
<date>1997</date>
<booktitle>Machine Learning,</booktitle>
<volume>26</volume>
<contexts>
<context position="13573" citStr="Blum, 1997" startWordPosition="2253" endWordPosition="2254">s of features to determine the appropriate word in a given context. For our task, we adopt a feature-based algorithm called Winnow. There are two reasons why we select Winnow. First, it has been shown to be the best performer in English contextsensitive spelling correction (Golding and Roth, 1996). Second, it was shown to be able to handle difficult disambiguation tasks in Thai (Meknavin et al., 1997). Below we describe Winnow algorithm that is used for correcting real-word error. 3.3 Winnow Algorithm 3.3.1 The algorithm A Winnow algorithm used in our experiment is the algorithm described in (Blum, 1997). Winnow is a multiplicative weight updating and incremental algorithm (Littlestone, 1988; Golding and Roth, 1996). The algorithm is originally designed for learning two-class (positive and negative class) problems, and can be extended to multiple-class problems as shown in Figure 3. Winnow can be viewed as a network of one target node connected to n nodes, called specialists, each of which examines one feature and 839 Let V1,... V vni be the values of the target concept to be learned, and xi be the prediction of the i-specialist. 1. Initialize the weights w1, , wn of all the specialists to 1.</context>
<context position="16433" citStr="Blum, 1997" startWordPosition="2773" endWordPosition="2774">examines more than two attributes, but for the sake of simplification of preliminary experiment, let us assume that two attributes for each specialist are enough to learn the target concept. The global algorithm updates the weight wi of any specialist based on the vote of that specialist. The weight of any specialist is initialized to 1. In case that the global algorithm predicts incorrectly, the weight of the specialist that predicts incorrectly is halved and the weight of the specialist that predicts correctly is multiplied by 3/2. This weight updating method is the same as the one used in (Blum, 1997). The advantage of Winnow, which made us decide to use for our task, is that it is not sensitive to extra irrelevant features (Littlestone, 1988). 3.3.2 Constructing Confusion Set and Defining Features To employ Winnow in correcting OCR errors, we first define k-edit distance confusion set. A k-edit distance confusion set S = {c, w1, , wn} is composed of one centroid word c and words w1, w2, • • • , wn generated by applying the candidate generation routine with maximum k modified edit distance to the centroid word. If a word c is produced by OCR output or by the previous step, then it may be c</context>
</contexts>
<marker>Blum, 1997</marker>
<rawString>Avrim Blum. 1997. Empirical support for winnow and weighted-majority algorithm: Results on a calendar scheduling domain. Machine Learning, 26.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Dan Roth</author>
</authors>
<title>Applying winnow to context-sensitive spelling correction.</title>
<date>1996</date>
<booktitle>In Proceedings of the Thirteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="12774" citStr="Golding and Roth, 1996" startWordPosition="2117" endWordPosition="2120">eech. This is because the POS trigram model considers only coarse information of POS in a fixed restricted range of context, some useful information such as specific word collocation may be lost. Using word N-gram could recover some word-level information but requires an extremely large corpus to estimate all parameters accurately and consumes vast space resources to store the huge word N-gram table. In addition, the model losses generalized information at the level of POS. For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996). Among them, the featurebased methods were shown to be superior to other approaches. This is because the methods can combine several kinds of features to determine the appropriate word in a given context. For our task, we adopt a feature-based algorithm called Winnow. There are two reasons why we select Winnow. First, it has been shown to be the best performer in English contextsensitive spelling correction (Golding and Roth, 1996). Second, it was shown to be able to handle difficult disambiguation tasks in Thai (Meknavin et al., 1997). Below </context>
</contexts>
<marker>Golding, Roth, 1996</marker>
<rawString>Andrew R. Golding and Dan Roth. 1996. Applying winnow to context-sensitive spelling correction. In Proceedings of the Thirteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
<author>Yves Schabes</author>
</authors>
<title>Combining trigram-based and featured-based methods for context-sensitive spelling correction.</title>
<date>1993</date>
<tech>Technical Report TR-93-03a,</tech>
<institution>Mitsubishi Electric Research Laboratory.</institution>
<contexts>
<context position="12801" citStr="Golding and Schabes, 1993" startWordPosition="2121" endWordPosition="2124">e POS trigram model considers only coarse information of POS in a fixed restricted range of context, some useful information such as specific word collocation may be lost. Using word N-gram could recover some word-level information but requires an extremely large corpus to estimate all parameters accurately and consumes vast space resources to store the huge word N-gram table. In addition, the model losses generalized information at the level of POS. For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996). Among them, the featurebased methods were shown to be superior to other approaches. This is because the methods can combine several kinds of features to determine the appropriate word in a given context. For our task, we adopt a feature-based algorithm called Winnow. There are two reasons why we select Winnow. First, it has been shown to be the best performer in English contextsensitive spelling correction (Golding and Roth, 1996). Second, it was shown to be able to handle difficult disambiguation tasks in Thai (Meknavin et al., 1997). Below we describe Winnow algorith</context>
</contexts>
<marker>Golding, Schabes, 1993</marker>
<rawString>Andrew R. Golding and Yves Schabes. 1993. Combining trigram-based and featured-based methods for context-sensitive spelling correction. Technical Report TR-93-03a, Mitsubishi Electric Research Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew R Golding</author>
</authors>
<title>A bayesian hybrid method for context-sensitive spelling correction.</title>
<date>1995</date>
<booktitle>In Proceedings of the Third Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="12750" citStr="Golding, 1995" startWordPosition="2115" endWordPosition="2116">same part of speech. This is because the POS trigram model considers only coarse information of POS in a fixed restricted range of context, some useful information such as specific word collocation may be lost. Using word N-gram could recover some word-level information but requires an extremely large corpus to estimate all parameters accurately and consumes vast space resources to store the huge word N-gram table. In addition, the model losses generalized information at the level of POS. For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996). Among them, the featurebased methods were shown to be superior to other approaches. This is because the methods can combine several kinds of features to determine the appropriate word in a given context. For our task, we adopt a feature-based algorithm called Winnow. There are two reasons why we select Winnow. First, it has been shown to be the best performer in English contextsensitive spelling correction (Golding and Roth, 1996). Second, it was shown to be able to handle difficult disambiguation tasks in Thai (Meknav</context>
<context position="17484" citStr="Golding, 1995" startWordPosition="2962" endWordPosition="2963">ate generation routine with maximum k modified edit distance to the centroid word. If a word c is produced by OCR output or by the previous step, then it may be corrected as wi, w2, ...,wn or c itself. For example, suppose that the centroid word is know, then all possible words in 1-edit distance confusion set are {know, knob, knop, knot, knew, enow, snow, known, not* Furthermore, words with probability lower than a threshold are excluded from the set. For example, if a specific OCR has low probability of substituting t with w, &amp;quot;knot&amp;quot; should be excluded from the set. Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations. Context-word features is used to test for the 840 presence of a particular word within +/- M words of the target word, and collocations test for a pattern of up to L contiguous words and/or part-of-speech tags around the target word. In our experiment M and L is set to 10 and 2, respectively. Examples of features for discriminating between snow and know include: (1) I {know, snow} (2) winter within +10 words where (1) is a collocation that tends to imply know, and (2) is a context-word that tends to </context>
</contexts>
<marker>Golding, 1995</marker>
<rawString>Andrew R. Golding. 1995. A bayesian hybrid method for context-sensitive spelling correction. In Proceedings of the Third Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Ingels</author>
</authors>
<title>Connected text recognition using layered HMMs and token passing.</title>
<date>1996</date>
<booktitle>In Proceedings of the Second Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="5993" citStr="Ingels, 1996" startWordPosition="970" endWordPosition="971">n characteristics of Thai that poses difficulties for correcting Thai OCR error. • Words are written consecutively without word boundary delimiters such as white space characters. For example, the phrase &amp;quot;CluluilsliW (Japan at present) in Figure 1, actually consists of three words: &amp;quot;cku&amp;quot; (Japan), lu&amp;quot; (at), and &amp;quot;i7sltiu&amp;quot; (present). Therefore, Thai OCR error correction has to overcome word boundary ambiguity as well as select the most probable correction candidate at the same time. This is similar to the problem of Connected Speech Recognition and is sometimes called Connected Text Recognition (Ingels, 1996). • There are 3 levels for placing Thai characters and some characters can occupy more than one level. For example, in Figure 2 &amp;quot;le consists of characters in three levels, i.e., &amp;quot;, and il are in the top, the bottom, the middle and both the middle and top levels, respectively. The character that occupies more than one level like i usually connects to other characters (A) and causes error on the output of OCR, i.e., A may be recognized as i1 or I. Therefore, to correct characters produced by OCR, not only substitution errors but also deletion and insertion errors must be considered. In addition,</context>
</contexts>
<marker>Ingels, 1996</marker>
<rawString>Peter Ingels. 1996. Connected text recognition using layered HMMs and token passing. In Proceedings of the Second Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karen Kukich</author>
</authors>
<title>Techniques for automatically correction words in text.</title>
<date>1992</date>
<journal>ACM Computing Surveys,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="1677" citStr="Kukich, 1992" startWordPosition="250" endWordPosition="251">fied edit distance which reflects the characteristic of Thai OCR errors. Finally, a part-ofspeech trigram model and Winnow algorithm are combined to determine the most probable correction. 1 Introduction Optical character recognition (OCR) is useful in a wide range of applications, such as office automation and information retrieval system. However, OCR in Thailand is still not widely used, partly because existing Thai OCRs are not quite satisfactory in terms of accuracy. Recently, several research projects have focused on spelling correction for many types of errors including those from OCR (Kukich, 1992). Nevertheless, the strategy is slightly different from language to language, since the characteristic of each language is different. Two characteristics of Thai which make the task of error correction different from those of other languages are: (1) there is no explicit word boundary, and (2) characters are written in three levels; i.e., the middle, the upper and the lower levels. In order to solve the problem of OCR error correction, the first task is usually to detect error strings in the input sentence. For languages that have explicit word boundary such as English in which each word is se</context>
</contexts>
<marker>Kukich, 1992</marker>
<rawString>Karen Kukich. 1992. Techniques for automatically correction words in text. ACM Computing Surveys, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nick Littlestone</author>
</authors>
<title>Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm.</title>
<date>1988</date>
<booktitle>Machine Learning,</booktitle>
<volume>2</volume>
<contexts>
<context position="13662" citStr="Littlestone, 1988" startWordPosition="2266" endWordPosition="2267">e adopt a feature-based algorithm called Winnow. There are two reasons why we select Winnow. First, it has been shown to be the best performer in English contextsensitive spelling correction (Golding and Roth, 1996). Second, it was shown to be able to handle difficult disambiguation tasks in Thai (Meknavin et al., 1997). Below we describe Winnow algorithm that is used for correcting real-word error. 3.3 Winnow Algorithm 3.3.1 The algorithm A Winnow algorithm used in our experiment is the algorithm described in (Blum, 1997). Winnow is a multiplicative weight updating and incremental algorithm (Littlestone, 1988; Golding and Roth, 1996). The algorithm is originally designed for learning two-class (positive and negative class) problems, and can be extended to multiple-class problems as shown in Figure 3. Winnow can be viewed as a network of one target node connected to n nodes, called specialists, each of which examines one feature and 839 Let V1,... V vni be the values of the target concept to be learned, and xi be the prediction of the i-specialist. 1. Initialize the weights w1, , wn of all the specialists to 1. 2. For Each example x = {x1,...,xn} Do (a) Let V be the value of the target concept of t</context>
<context position="16578" citStr="Littlestone, 1988" startWordPosition="2799" endWordPosition="2800">h specialist are enough to learn the target concept. The global algorithm updates the weight wi of any specialist based on the vote of that specialist. The weight of any specialist is initialized to 1. In case that the global algorithm predicts incorrectly, the weight of the specialist that predicts incorrectly is halved and the weight of the specialist that predicts correctly is multiplied by 3/2. This weight updating method is the same as the one used in (Blum, 1997). The advantage of Winnow, which made us decide to use for our task, is that it is not sensitive to extra irrelevant features (Littlestone, 1988). 3.3.2 Constructing Confusion Set and Defining Features To employ Winnow in correcting OCR errors, we first define k-edit distance confusion set. A k-edit distance confusion set S = {c, w1, , wn} is composed of one centroid word c and words w1, w2, • • • , wn generated by applying the candidate generation routine with maximum k modified edit distance to the centroid word. If a word c is produced by OCR output or by the previous step, then it may be corrected as wi, w2, ...,wn or c itself. For example, suppose that the centroid word is know, then all possible words in 1-edit distance confusion</context>
</contexts>
<marker>Littlestone, 1988</marker>
<rawString>Nick Littlestone. 1988. Learning quickly when irrelevant attributes abound: A new linearthreshold algorithm. Machine Learning, 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Surapant Meknavin</author>
<author>Paisarn Charoenpornsawat</author>
<author>Boonserm Kijsirikul</author>
</authors>
<date>1997</date>
<contexts>
<context position="13366" citStr="Meknavin et al., 1997" startWordPosition="2218" endWordPosition="2222">, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996). Among them, the featurebased methods were shown to be superior to other approaches. This is because the methods can combine several kinds of features to determine the appropriate word in a given context. For our task, we adopt a feature-based algorithm called Winnow. There are two reasons why we select Winnow. First, it has been shown to be the best performer in English contextsensitive spelling correction (Golding and Roth, 1996). Second, it was shown to be able to handle difficult disambiguation tasks in Thai (Meknavin et al., 1997). Below we describe Winnow algorithm that is used for correcting real-word error. 3.3 Winnow Algorithm 3.3.1 The algorithm A Winnow algorithm used in our experiment is the algorithm described in (Blum, 1997). Winnow is a multiplicative weight updating and incremental algorithm (Littlestone, 1988; Golding and Roth, 1996). The algorithm is originally designed for learning two-class (positive and negative class) problems, and can be extended to multiple-class problems as shown in Figure 3. Winnow can be viewed as a network of one target node connected to n nodes, called specialists, each of which</context>
<context position="17508" citStr="Meknavin et al., 1997" startWordPosition="2964" endWordPosition="2967">routine with maximum k modified edit distance to the centroid word. If a word c is produced by OCR output or by the previous step, then it may be corrected as wi, w2, ...,wn or c itself. For example, suppose that the centroid word is know, then all possible words in 1-edit distance confusion set are {know, knob, knop, knot, knew, enow, snow, known, not* Furthermore, words with probability lower than a threshold are excluded from the set. For example, if a specific OCR has low probability of substituting t with w, &amp;quot;knot&amp;quot; should be excluded from the set. Following previous works (Golding, 1995; Meknavin et al., 1997), we have tried two types of features: context words and collocations. Context-word features is used to test for the 840 presence of a particular word within +/- M words of the target word, and collocations test for a pattern of up to L contiguous words and/or part-of-speech tags around the target word. In our experiment M and L is set to 10 and 2, respectively. Examples of features for discriminating between snow and know include: (1) I {know, snow} (2) winter within +10 words where (1) is a collocation that tends to imply know, and (2) is a context-word that tends to imply snow. Then the alg</context>
</contexts>
<marker>Meknavin, Charoenpornsawat, Kijsirikul, 1997</marker>
<rawString>Surapant Meknavin, Paisarn Charoenpornsawat, and Boonserm Kijsirikul. 1997.</rawString>
</citation>
<citation valid="false">
<title>Feature-based Thai word segmentation.</title>
<booktitle>In Proceedings of Natural Language Processing Pacific Rim Symposium &apos;97.</booktitle>
<marker></marker>
<rawString>Feature-based Thai word segmentation. In Proceedings of Natural Language Processing Pacific Rim Symposium &apos;97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Nagata</author>
</authors>
<title>Context-base spelling correction for Japanese OCR.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING &apos;96.</booktitle>
<contexts>
<context position="2960" citStr="Nagata, 1996" startWordPosition="468" endWordPosition="469">mple. If the tokenized string is not found in the dictionary, it could be an error string or an unknown word. However, for the languages that have no explicit word boundary such as Chinese, Japanese and Thai, this task is much more complicated. Even without errors from OCR, it is difficult to determine word boundary in these languages. The situation gets worse when noises are introduced in the text. The existing approach for correcting the spelling error in the languages that have no word boundary assumes that all substrings in input sentence are error strings, and then tries to correct them (Nagata, 1996). This is computationally expensive since a large portion of the input sentence is correct. The other characteristic of Thai writing system is that we have many levels for placing Thai characters and several characters can occupy more than one level. These characters are easily connected to other characters in the upper or lower level. These connected characters cause difficulties in the process of character segmentation which then cause errors in Thai OCR. Other than the above problems specific to Thai, real-word error is another source of errors that is difficult to correct. Several previous</context>
<context position="8201" citStr="Nagata, 1996" startWordPosition="1360" endWordPosition="1361"> the i-prefix of S that is formed by first character to the i-character of S (= ci), and similarly Wi is the jprefix of W (= v1v2 vi). Using dynamic programming technique, we can calculate P(SIW) (= P(S„ I Wm)) by the following equation: P(SilWi) = max(P(Si_ilWi)* P(ins(g)), P(del(vi)), P(Si_11Wi9...i)*P(cilvj)) (4) where P(ins(c)), P(del(v)) and P(c1v) are the probabilities that letter c is inserted, letter v is deleted and letter v is substituted with c, respectively. One method to do OCR error correction using the above model is to hypothesize all substrings in the input sentence as words (Nagata, 1996). Both words in the dictionary that exactly match with the substrings and those that approximately match are retrieved. To cope with unknown words, all other substrings not matched must also be considered. The word lattice is then scanned to find the N-best word sequences as correction candidates. In general, this method is perfectly good, except in one aspect: its time complexity. Because it generates a large number of hypothesized words and has to find the best combination among them, it is very slow. 3.2 Selective Trigram Model To alleviate the above problem, we try to reduce the number of </context>
<context position="10576" citStr="Nagata, 1996" startWordPosition="1755" endWordPosition="1756">ogether, resulting in &amp;quot;informatjon&amp;quot; as a new unknown string. (b) For each unknown string obtained form 2(a), apply the candidate generation routine to generate approximately matched words within k-edit distance. The value of k is varied proportionally to the length of candidate word. (c) All substrings except for ones that violate Thai spelling rules, i.e., lead by non-leading character, are hypothesized as unknown words. 3. Find good word sequences: Find the N-best word sequences according to equation (2). For unknown words, P(wi &apos;Unknown word) is computed by using the unknown word model in (Nagata, 1996). 4. Make hypotheses for real-word error: For each word wi in N-best word sequence where the local probabilities P(wi_i, w,wi+i, ti_1, 4, 4+1) are below a threshold, generate candidate words by applying the process similar to step 2 except that the nonword in step 2 is replaced with the word wi. Find the word sequences whose probabilities computed by equation (2) are better than original ones. 5. Find the N-best word sequences: From all word sequences obtained from step 4, select the N-best ones. The candidate generation routine uses a modification of the standard edit distance and employs the</context>
</contexts>
<marker>Nagata, 1996</marker>
<rawString>Masaaki Nagata. 1996. Context-base spelling correction for Japanese OCR. In Proceedings of COLING &apos;96.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kemal Oflazer</author>
</authors>
<title>Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="11242" citStr="Oflazer, 1996" startWordPosition="1864" endWordPosition="1865">ord wi in N-best word sequence where the local probabilities P(wi_i, w,wi+i, ti_1, 4, 4+1) are below a threshold, generate candidate words by applying the process similar to step 2 except that the nonword in step 2 is replaced with the word wi. Find the word sequences whose probabilities computed by equation (2) are better than original ones. 5. Find the N-best word sequences: From all word sequences obtained from step 4, select the N-best ones. The candidate generation routine uses a modification of the standard edit distance and employs the error-tolerant finite-state recognition algorithm (Oflazer, 1996) to generate candidate words. The modified edit distance allows arbitrary number of insertion and/or deletion of upper level and lower level characters, but allows no insertion or deletion of the middle level characters. In the middle level, it allows only k substitution. This is to reflect the characteristic of Thai OCR which, 1. tends to merge several characters into one when the character which spans two levels are adjacent to characters in the upper and lower level, and 2. rarely causes insertion and deletion errors in the middle level. For example, applying the candidate generation routin</context>
</contexts>
<marker>Oflazer, 1996</marker>
<rawString>Kemal Oflazer. 1996. Error-tolerant finite-state recognition with applications to morphological analysis and spelling correction. Computational Linguistics, 22(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiang Tong</author>
<author>David A Evans</author>
</authors>
<title>A statistical approach to automatic OCR error correction in context.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="12824" citStr="Tong and Evans, 1996" startWordPosition="2125" endWordPosition="2128">rs only coarse information of POS in a fixed restricted range of context, some useful information such as specific word collocation may be lost. Using word N-gram could recover some word-level information but requires an extremely large corpus to estimate all parameters accurately and consumes vast space resources to store the huge word N-gram table. In addition, the model losses generalized information at the level of POS. For English, a number of methods have been proposed to cope with real-word errors in spelling correction (Golding, 1995; Golding and Roth, 1996; Golding and Schabes, 1993; Tong and Evans, 1996). Among them, the featurebased methods were shown to be superior to other approaches. This is because the methods can combine several kinds of features to determine the appropriate word in a given context. For our task, we adopt a feature-based algorithm called Winnow. There are two reasons why we select Winnow. First, it has been shown to be the best performer in English contextsensitive spelling correction (Golding and Roth, 1996). Second, it was shown to be able to handle difficult disambiguation tasks in Thai (Meknavin et al., 1997). Below we describe Winnow algorithm that is used for corr</context>
</contexts>
<marker>Tong, Evans, 1996</marker>
<rawString>Xiang Tong and David A. Evans. 1996. A statistical approach to automatic OCR error correction in context. In Proceedings of the Fourth Workshop on Very Large Corpora.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>