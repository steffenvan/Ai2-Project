<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001764">
<title confidence="0.997529">
Adaptive Chinese Word Segmentation with
Online Passive-Aggressive Algorithm
</title>
<author confidence="0.996571">
Wenjun Gao
</author>
<affiliation confidence="0.89979">
School of Computer Science
Fudan University
Shanghai, China
</affiliation>
<email confidence="0.991856">
wjgao616@gmail.com
</email>
<author confidence="0.996311">
Xipeng Qiu
</author>
<affiliation confidence="0.899613333333333">
School of Computer Science
Fudan University
Shanghai, China
</affiliation>
<email confidence="0.995581">
xpqiu@fudan.edu.cn
</email>
<author confidence="0.997612">
Xuanjing Huang
</author>
<affiliation confidence="0.899557">
School of Computer Science
Fudan University
Shanghai, China
</affiliation>
<email confidence="0.996667">
xjhuang@fudan.edu.cn
</email>
<sectionHeader confidence="0.993839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999777555555556">
In this paper, we describe our systems
for CIPS-SIGHAN-2010 bake-off task of
Chinese word segmentation, which fo-
cused on the cross-domain performance
of Chinese word segmentation algorithms.
We use the online passive-aggressive al-
gorithm with domain invariant informa-
tion for cross-domain Chinese word seg-
mentation.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999915954545455">
In recent years, Chinese word segmentation
(CWS) has undergone great development (Xue,
2003; Peng et al., 2004). The popular method is
to regard word segmentation as a sequence label-
ing problems. The goal of sequence labeling is to
assign labels to all elements of a sequence.
Due to the exponential size of the output
space, sequence labeling problems tend to be
more challenging than the conventional classifi-
cation problems. Many algorithms have been
proposed and the progress has been encourag-
ing, such as SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al.,
2004), conditional random fields (CRF) (Lafferty
et al., 2001), maximum margin Markov networks
(M3N) (Taskar et al., 2003) and so on. After years
of intensive researches, Chinese word segmenta-
tion achieves a quite high precision. However, the
performance of segmentation is not so satisfying
for out-of-domain text.
There are two domains in domain adaption
problem, a source domain and a target domain.
When we use the machine learning methods for
</bodyText>
<footnote confidence="0.9559665">
&apos;Available at http://code.google.com/p/
fudannlp/
</footnote>
<bodyText confidence="0.997293434782609">
Chinese word segmentation, we assume that train-
ing and test data are drawn from the same distri-
bution. This assumption underlies both theoreti-
cal analysis and experimental evaluations of learn-
ing algorithms. However, the assumption does
not hold for domain adaptation(Ben-David et al.,
2007; Blitzer et al., 2006). The challenge is the
difference of distribution between the source and
target domains.
In this paper, we use online margin max-
imization algorithm and domain invariant fea-
tures for domain adaptive CWS. The online learn-
ing algorithm is Passive-Aggressive (PA) algo-
rithm(Crammer et al., 2006), which passively ac-
cepts a solution whose loss is zero, while it ag-
gressively forces the new prototype vector to stay
as close as possible to the one previously learned.
The rest of the paper is organized as follows.
Section 2 introduces the related works. Then we
describe our algorithm in section 3 and 4. The
feature templates are described in section 5. Sec-
tion 6 gives the experimental analysis. Section 7
concludes the paper.
</bodyText>
<sectionHeader confidence="0.999144" genericHeader="introduction">
2 Related Works
</sectionHeader>
<bodyText confidence="0.99975528125">
There are several approaches to deal with the do-
main adaption problem.
The first approach is to use semi-supervised
learning (Zhu, 2005).
The second approach is to incorporate super-
vised learning with domain invariant information.
The third approach is to improve the present
model with a few labeled domain data.
Altun et al. (2006) investigated structured clas-
sification in a semi-supervised setting. They pre-
sented a discriminative approach that utilizes the
intrinsic geometry of inputs revealed by unlabeled
data points and we derive a maximum-margin for-
mulation of semi-supervised learning for struc-
tured variables.
Self-training (Zhu, 2005) is also a popular tech-
nology. In self-training a classifier is first trained
with the small amount of labeled data. The clas-
sifier is then used to classify the unlabeled data.
Typically the most confident unlabeled points, to-
gether with their predicted labels, are added to the
training set. The classifier is re-trained and the
procedure repeated. Note the classifier uses its
own predictions to teach itself. Yarowsky (1995)
uses self-training for word sense disambiguation,
e.g. deciding whether the word plant means a liv-
ing organism or a factory in a given context.
Zhao and Kit (2008) integrated unsupervised
segmentation and CRF learning for Chinese word
segmentation and named entity recognition. They
found word accessory variance (Feng et al., 2004)
is useful to CWS.
</bodyText>
<sectionHeader confidence="0.988054" genericHeader="method">
3 Online Passive-Aggressive Algorithm
</sectionHeader>
<bodyText confidence="0.998746166666667">
Sequence labeling, the task of assigning labels
y = y1, ... , yL to an input sequence x =
x1, . . . , xL.
Give a sample (x, y), we define the feature is
b(x, y). Thus, we can label x with a score func-
tion,
</bodyText>
<equation confidence="0.9693495">
yˆ = arg max F(w, b(x, z)), (1)
z
</equation>
<bodyText confidence="0.9984494">
where w is the parameter of function F(·).
The score function of our algorithm is linear
function.
Given an example (x, y), yˆ is denoted as the
incorrect label with the highest score,
</bodyText>
<equation confidence="0.9023174">
γ(w; (x, y)) = wTb(x, y) − wTb(x, ˆy). (3)
Thus, we calculate the hinge loss.
ℓ(w; (x, y) = { 1 ))
—&apos;y(w; (X,Y)), otherwise 1
(4)
</equation>
<bodyText confidence="0.999974666666667">
We use the online PA learning algorithm to
learn the weights of features. In round t, we find
new weight vector wt+1 by
</bodyText>
<equation confidence="0.952768">
1
wt+1 = arg min   ||w − wt  ||2 + C · ξ,
w∈R- 2
s.t. ℓ(w; (xt, yt)) &lt;= ξ and ξ &gt;= 0 (5)
</equation>
<bodyText confidence="0.999432071428571">
where C is a positive parameter which controls
the influence of the slack term on the objective
function.
The algorithms goal is to achieve a margin at
least 1 as often as possible, thus the Hamming loss
is also reduced indirectly. On rounds where the
algorithm attains a margin less than 1 it suffers an
instantaneous loss.
We abbreviate ℓ(wt; (x, y)) to ℓt. If ℓt = 0
then wt itself satisfies the constraint in Eq. (5)
and is clearly the optimal solution. We therefore
concentrate on the case where ℓt &gt; 0.
First, we define the Lagrangian of the optimiza-
tion problem in Eq. (5) to be
</bodyText>
<equation confidence="0.9002925">
L(w, ξ, α,β) = 2||w − wt||2 + C · ξ
1
+ α(ℓt − ξ) − βξ
s.t. α &gt;= 0, β &gt;= 0. (6)
</equation>
<bodyText confidence="0.999255666666667">
where α, β is a Lagrange multiplier.
Setting the partial derivatives of L with respect
to the elements of ξ to zero gives
</bodyText>
<equation confidence="0.9548828">
α + β = C. (7)
The gradient of w should be zero,
w − wt − α(b(x, y) − b(x, ˆy)) = 0, (8)
we get
w = wt + α(b(x, y) − b(x, ˆy)). (9)
</equation>
<bodyText confidence="0.607744">
Substitute Eq. (7) and Eq. (9) to dual objective
function Eq. (6), we get
</bodyText>
<equation confidence="0.9784175">
L(α) = −2||α(b(x,y) − b(x, ˆy))||2
1
− α(wtT(b(x, y) − b(x, ˆy)) + α (10)
yˆ = arg max wTb(x, z). (2)
z̸=y
The margin γ(w; (x, y)) is defined as
Differentiate with α, and set it to zero, we get
α||(x,y) − (x, ˆy)||2
+ wtT ((x, y) − (x, ˆy)) − 1 = 0. (11)
So,
1 − wtT ((x, y) − (x, ˆy))(12)
 ||(x, y) − (x, ˆy)  ||2
From α + β = C, we know that α &lt; C, so
¯α* = min(C, ¯α). (13)
Finally, we get update strategy,
wt+1 = wt + ¯α*((x, y) − (x, ˆy)). (14)
</equation>
<bodyText confidence="0.734583333333333">
Our final algorithm is shown in Algorithm 1. In
order to avoiding overfitting, the averaging tech-
nology is employed.
</bodyText>
<equation confidence="0.941672111111111">
input : training data set:
(xn, yn), n = 1, · · · , N, and
parameters: C, K
output: w
Initialize: cw +-- 0,;
fork = 0···K − 1 do
w0 +-- 0 ;
fort = 0···T − 1 do
receive an example (xt, yt);
predict:
ˆyt = arg max (wt, (xt, z));
z̸=yt
calculate e(w; (x, y));
update wt+1 with Eq.(14);
end
cw = cw + wT ;
end
w = cw/K ;
</equation>
<construct confidence="0.5080755">
Algorithm 1: Labelwise Margin Maxi-
mization Algorithm
</construct>
<sectionHeader confidence="0.999038" genericHeader="method">
4 Inference
</sectionHeader>
<bodyText confidence="0.99857975">
The PA algorithm is used to learn the weights of
features in training procedure. In inference pro-
cedure, we use Viterbi algorithm to calculate the
maximum score label.
Let w(n) be the best score of the partial label
sequence ending with yn. The idea of the Viterbi
algorithm is to use dynamic programming to com-
pute w(n):
</bodyText>
<equation confidence="0.993647666666667">
w(n) = max (w(n − 1) + wTϕ(x, yn, yn−1)) (15)
n−1
+wtϕ(x, yn)
</equation>
<bodyText confidence="0.999888333333333">
Using this recursive definition, we can evalu-
ate w(N) for all yN, where N is the input length.
This results in the identification of the best label
sequence.
The computational cost of the Viterbi algorithm
is O(NL2), where L is the number of labels.
</bodyText>
<sectionHeader confidence="0.9965" genericHeader="method">
5 Feature Templates
</sectionHeader>
<bodyText confidence="0.999879411764706">
All feature templates used in this paper are shown
in Table 1. C represents a Chinese character while
the subscript of C indicates its position in the sen-
tence relative to the current character, whose sub-
script is 0. T represents the character-based tag:
“B”, “B2”, “B3”, “M”, “E” and “S”, which repre-
sent the beginning, second, third, middle, end or
single character of a word respectively.
The type of character includes: digital, letter,
punctuation and other.
We also use the word accessor variance for do-
main adaption. Word accessor variance (AV) was
proposed by (Feng et al., 2004) and was used to
evaluate how independently a string is used, and
thus how likely it is that the string can be a word.
The accessor variety of a string s of more than one
character is defined as
</bodyText>
<equation confidence="0.913101">
AV (s) = min{L,,,,,(s), R,,,,,(s)} (16)
</equation>
<bodyText confidence="0.989606230769231">
L,,,,,(s) is called the left accessor variety and is
defined as the number of distinct characters (pre-
decessors) except “S” that precede s plus the num-
ber of distinct sentences of which s appears at
the beginning. Similarly, the right accessor va-
riety R,,,,,(s) is defined as the number of distinct
characters (successors) except “E” that succeed s
plus the number of distinct sentences in which s
appears at the end. The characters “S” and “E”
are defined as the begin and end of a sentence.
The word accessor variance was found effective
for CWS with unsegmented text (Zhao and Kit,
2008).
</bodyText>
<equation confidence="0.790501">
α¯ =
</equation>
<tableCaption confidence="0.991242">
Table 1: Feature templates
</tableCaption>
<table confidence="0.9859984">
CZ, T0, (i = −1, 0, 1, 2)
CZ, CZ+1, T0, (i = −2, −1, 0,1)
T−1,0
Tc: Type of Character
AV : word accessor variance
</table>
<sectionHeader confidence="0.924668" genericHeader="method">
6 CIPS-SIGHAN-2010 Bakeoff
</sectionHeader>
<bodyText confidence="0.86738675">
CIPS-SIGHAN-2010 bake-off task of Chinese
word segmentation focused on the cross-domain
performance of Chinese word segmentation algo-
rithms. There are two subtasks for this evaluation:
</bodyText>
<listItem confidence="0.976035">
(1) Word Segmentation for Simplified Chinese
Text;
(2) Word Segmentation for Traditional Chinese
Text.
</listItem>
<bodyText confidence="0.999879181818182">
The test corpus of each subtask covers four do-
mains: literature, computer science, medicine and
finance.
We participate in closed training evaluation of
both subtasks.
Firstly, we calculate the word accessor variance
AVL(s)of the continuous string s from labeled
corpus. Here, we set the largest length of string
stobe4.
Secondly, we train our model with feature tem-
ples and AVL(s).
Thirdly, when we process the different domain
unlabeled corpus, we recalculate the word ac-
cessory variance AVU(s) from the corresponding
corpus.
Fourthly, we segment the domain corpus with
new word accessory variance AVU(s) instead of
AVL(s).
The results are shown in Table 2 and 3. The
results show our method has a poor performance
in OOV ( Out-Of-Vocabulary) word.
The running environment is shown in Table 4.
</bodyText>
<tableCaption confidence="0.999592">
Table 4: Experimental environment
</tableCaption>
<table confidence="0.818682333333333">
OS Win 2003
CPU Intel Xeon 2.0G
Memory 4G
</table>
<bodyText confidence="0.999927857142857">
We set the max iterative number is 20. Our run-
ning time is shown in Table 5. “s” represents sec-
ond, “chars” is the number of Chinese character,
and “MB” is the megabyte. In practice, we found
the system can achieve the same performance af-
ter 7 loops. Therefore, we just need less half the
time in Table 5 actually.
</bodyText>
<sectionHeader confidence="0.984407" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999919230769231">
In this paper, we describe our system in CIPS-
SIGHAN-2010 bake-off task of Chinese word
segmentation. Although our method just achieve
a consequence of being average and not outstand-
ing, it has an advantage of faster training than
other batch learning algorithm, such as CRF and
M3N.
In the future, we wish to improve our method
in the following aspects. Firstly, we will investi-
gate more effective domain invariant feature rep-
resentation. Secondly, we will integrate our algo-
rithm with self-training and other semi-supervised
learning methods.
</bodyText>
<sectionHeader confidence="0.997488" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.98834725">
This work was (partially) funded by 863 Pro-
gram (No. 2009AA01A346), 973 Program (No.
2010CB327906), and Shanghai Science and Tech-
nology Development Funds (No. 08511500302).
</bodyText>
<sectionHeader confidence="0.99839" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997586777777778">
Altun, Y., D. McAllester, and M. Belkin. 2006. Max-
imum margin semi-supervised learning for struc-
tured variables. Advances in neural information
processing systems, 18:33.
Ben-David, S., J. Blitzer, K. Crammer, and F. Pereira.
2007. Analysis of representations for domain adap-
tation. Advances in Neural Information Processing
Systems, 19:137.
Blitzer, J., R. McDonald, and F. Pereira. 2006.
Domain adaptation with structural correspondence
learning. In Proceedings of the 2006 Conference on
Empirical Methods in Natural Language Process-
ing, pages 120–128. Association for Computational
Linguistics.
Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai
Shalev-Shwartz, and Yoram Singer. 2006. Online
passive-aggressive algorithms. Journal of Machine
Learning Research, 7:551–585.
</reference>
<tableCaption confidence="0.992035">
Table 2: Evaluation results on simplified corpus
</tableCaption>
<table confidence="0.999964888888889">
R P F1 OOV RR IV RR
Literature Best 0.945 0.946 0.946 0.816 0.954
Our 0.915 0.925 0.92 0.577 0.94
Computer Best 0.953 0.95 0.951 0.827 0.975
Our 0.934 0.919 0.926 0.739 0.969
Medicine Best 0.942 0.936 0.939 0.75 0.965
Our 0.927 0.924 0.925 0.714 0.953
Finance Best 0.959 0.96 0.959 0.827 0.972
Our 0.94 0.942 0.941 0.719 0.961
</table>
<tableCaption confidence="0.994867">
Table 3: Evaluation results on traditional corpus
</tableCaption>
<table confidence="0.999915777777778">
R P F1 OOV RR IV RR
Literature Best 0.942 0.942 0.942 0.788 0.958
Our 0.869 0.91 0.889 0.698 0.887
Computer Best 0.948 0.957 0.952 0.666 0.977
Our 0.933 0.949 0.941 0.791 0.948
Medicine Best 0.953 0.957 0.955 0.798 0.966
Our 0.908 0.932 0.92 0.771 0.919
Finance Best 0.964 0.962 0.963 0.812 0.975
Our 00.925 0.939 0.932 0.793 0.935
</table>
<tableCaption confidence="0.998028">
Table 5: Execution time of training and test phase.
</tableCaption>
<table confidence="0.92049">
Training Task A B C D
Simp 817.2s 795.6s 774.0s 792.0s
Trad 903.6s 889.2s 885.6s 874.8s
Test 20327 chars/s, or 17.97 s/MB
</table>
<reference confidence="0.998810552631579">
Feng, H., K. Chen, X. Deng, and W. Zheng. 2004. Ac-
cessor variety criteria for chinese word extraction.
Computational Linguistics, 30(1):75–93.
Lafferty, John D., Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling
sequence data. In ICML ’01: Proceedings of the
Eighteenth International Conference on Machine
Learning.
Peng, F., F. Feng, and A. McCallum. 2004. Chinese
segmentation and new word detection using condi-
tional random fields. Proceedings of the 20th inter-
national conference on Computational Linguistics.
Taskar, Ben, Carlos Guestrin, and Daphne Koller.
2003. Max-margin markov networks. In Proceed-
ings of Neural Information Processing Systems.
Tsochantaridis, I., T. Hofmann, T. Joachims, and Y Al-
tun. 2004. Support vector machine learning for in-
terdependent and structured output spaces. In Pro-
ceedings of the International Conference on Ma-
chine Learning(ICML).
Xue, N. 2003. Chinese word segmentation as charac-
ter tagging. Computational Linguistics and Chinese
Language Processing, 8(1):29–48.
Yarowsky, D. 1995. Unsupervised word sense disam-
biguation rivaling supervised methods. In Proceed-
ings of the 33rd annual meeting on Association for
Computational Linguistics, pages 189–196. Associ-
ation for Computational Linguistics.
Zhao, H. and C. Kit. 2008. Unsupervised segmenta-
tion helps supervised learning of character tagging
for word segmentation and named entity recogni-
tion. In The Sixth SIGHAN Workshop on Chinese
Language Processing, pages 106–111. Citeseer.
Zhu, Xiaojin. 2005. Semi-supervised learning
literature survey. Technical Report 1530, Com-
puter Sciences, University of Wisconsin-Madison.
http://www.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.051458">
<title confidence="0.9982365">Adaptive Chinese Word Segmentation Online Passive-Aggressive Algorithm</title>
<author confidence="0.957907">Wenjun</author>
<affiliation confidence="0.9218885">School of Computer Fudan</affiliation>
<address confidence="0.533205">Shanghai,</address>
<email confidence="0.999022">wjgao616@gmail.com</email>
<author confidence="0.611545">Xipeng</author>
<affiliation confidence="0.9150485">School of Computer Fudan</affiliation>
<address confidence="0.927413">Shanghai,</address>
<email confidence="0.982629">xpqiu@fudan.edu.cn</email>
<author confidence="0.400726">Xuanjing</author>
<affiliation confidence="0.9048445">School of Computer Fudan</affiliation>
<address confidence="0.925639">Shanghai,</address>
<email confidence="0.993044">xjhuang@fudan.edu.cn</email>
<abstract confidence="0.9789399">this paper, we describe our for CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which focused on the cross-domain performance of Chinese word segmentation algorithms. We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word segmentation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y Altun</author>
<author>D McAllester</author>
<author>M Belkin</author>
</authors>
<title>Maximum margin semi-supervised learning for structured variables. Advances in neural information processing systems,</title>
<date>2006</date>
<pages>18--33</pages>
<contexts>
<context position="3117" citStr="Altun et al. (2006)" startWordPosition="471" endWordPosition="474">earned. The rest of the paper is organized as follows. Section 2 introduces the related works. Then we describe our algorithm in section 3 and 4. The feature templates are described in section 5. Section 6 gives the experimental analysis. Section 7 concludes the paper. 2 Related Works There are several approaches to deal with the domain adaption problem. The first approach is to use semi-supervised learning (Zhu, 2005). The second approach is to incorporate supervised learning with domain invariant information. The third approach is to improve the present model with a few labeled domain data. Altun et al. (2006) investigated structured classification in a semi-supervised setting. They presented a discriminative approach that utilizes the intrinsic geometry of inputs revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Self-training (Zhu, 2005) is also a popular technology. In self-training a classifier is first trained with the small amount of labeled data. The classifier is then used to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the trainin</context>
</contexts>
<marker>Altun, McAllester, Belkin, 2006</marker>
<rawString>Altun, Y., D. McAllester, and M. Belkin. 2006. Maximum margin semi-supervised learning for structured variables. Advances in neural information processing systems, 18:33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Ben-David</author>
<author>J Blitzer</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Analysis of representations for domain adaptation.</title>
<date>2007</date>
<booktitle>Advances in Neural Information Processing Systems,</booktitle>
<pages>19--137</pages>
<contexts>
<context position="2021" citStr="Ben-David et al., 2007" startWordPosition="290" endWordPosition="293">hes, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for out-of-domain text. There are two domains in domain adaption problem, a source domain and a target domain. When we use the machine learning methods for &apos;Available at http://code.google.com/p/ fudannlp/ Chinese word segmentation, we assume that training and test data are drawn from the same distribution. This assumption underlies both theoretical analysis and experimental evaluations of learning algorithms. However, the assumption does not hold for domain adaptation(Ben-David et al., 2007; Blitzer et al., 2006). The challenge is the difference of distribution between the source and target domains. In this paper, we use online margin maximization algorithm and domain invariant features for domain adaptive CWS. The online learning algorithm is Passive-Aggressive (PA) algorithm(Crammer et al., 2006), which passively accepts a solution whose loss is zero, while it aggressively forces the new prototype vector to stay as close as possible to the one previously learned. The rest of the paper is organized as follows. Section 2 introduces the related works. Then we describe our algorit</context>
</contexts>
<marker>Ben-David, Blitzer, Crammer, Pereira, 2007</marker>
<rawString>Ben-David, S., J. Blitzer, K. Crammer, and F. Pereira. 2007. Analysis of representations for domain adaptation. Advances in Neural Information Processing Systems, 19:137.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Blitzer</author>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>120--128</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2044" citStr="Blitzer et al., 2006" startWordPosition="294" endWordPosition="297">tation achieves a quite high precision. However, the performance of segmentation is not so satisfying for out-of-domain text. There are two domains in domain adaption problem, a source domain and a target domain. When we use the machine learning methods for &apos;Available at http://code.google.com/p/ fudannlp/ Chinese word segmentation, we assume that training and test data are drawn from the same distribution. This assumption underlies both theoretical analysis and experimental evaluations of learning algorithms. However, the assumption does not hold for domain adaptation(Ben-David et al., 2007; Blitzer et al., 2006). The challenge is the difference of distribution between the source and target domains. In this paper, we use online margin maximization algorithm and domain invariant features for domain adaptive CWS. The online learning algorithm is Passive-Aggressive (PA) algorithm(Crammer et al., 2006), which passively accepts a solution whose loss is zero, while it aggressively forces the new prototype vector to stay as close as possible to the one previously learned. The rest of the paper is organized as follows. Section 2 introduces the related works. Then we describe our algorithm in section 3 and 4. </context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>Blitzer, J., R. McDonald, and F. Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, pages 120–128. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai Shalev-Shwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passive-aggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="2335" citStr="Crammer et al., 2006" startWordPosition="338" endWordPosition="342">com/p/ fudannlp/ Chinese word segmentation, we assume that training and test data are drawn from the same distribution. This assumption underlies both theoretical analysis and experimental evaluations of learning algorithms. However, the assumption does not hold for domain adaptation(Ben-David et al., 2007; Blitzer et al., 2006). The challenge is the difference of distribution between the source and target domains. In this paper, we use online margin maximization algorithm and domain invariant features for domain adaptive CWS. The online learning algorithm is Passive-Aggressive (PA) algorithm(Crammer et al., 2006), which passively accepts a solution whose loss is zero, while it aggressively forces the new prototype vector to stay as close as possible to the one previously learned. The rest of the paper is organized as follows. Section 2 introduces the related works. Then we describe our algorithm in section 3 and 4. The feature templates are described in section 5. Section 6 gives the experimental analysis. Section 7 concludes the paper. 2 Related Works There are several approaches to deal with the domain adaption problem. The first approach is to use semi-supervised learning (Zhu, 2005). The second ap</context>
</contexts>
<marker>Crammer, Dekel, Keshet, Shalev-Shwartz, Singer, 2006</marker>
<rawString>Crammer, Koby, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer. 2006. Online passive-aggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Feng</author>
<author>K Chen</author>
<author>X Deng</author>
<author>W Zheng</author>
</authors>
<title>Accessor variety criteria for chinese word extraction.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<contexts>
<context position="4191" citStr="Feng et al., 2004" startWordPosition="634" endWordPosition="637">ed to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. The classifier is re-trained and the procedure repeated. Note the classifier uses its own predictions to teach itself. Yarowsky (1995) uses self-training for word sense disambiguation, e.g. deciding whether the word plant means a living organism or a factory in a given context. Zhao and Kit (2008) integrated unsupervised segmentation and CRF learning for Chinese word segmentation and named entity recognition. They found word accessory variance (Feng et al., 2004) is useful to CWS. 3 Online Passive-Aggressive Algorithm Sequence labeling, the task of assigning labels y = y1, ... , yL to an input sequence x = x1, . . . , xL. Give a sample (x, y), we define the feature is b(x, y). Thus, we can label x with a score function, yˆ = arg max F(w, b(x, z)), (1) z where w is the parameter of function F(·). The score function of our algorithm is linear function. Given an example (x, y), yˆ is denoted as the incorrect label with the highest score, γ(w; (x, y)) = wTb(x, y) − wTb(x, ˆy). (3) Thus, we calculate the hinge loss. ℓ(w; (x, y) = { 1 )) —&apos;y(w; (X,Y)), othe</context>
<context position="8202" citStr="Feng et al., 2004" startWordPosition="1442" endWordPosition="1445">of labels. 5 Feature Templates All feature templates used in this paper are shown in Table 1. C represents a Chinese character while the subscript of C indicates its position in the sentence relative to the current character, whose subscript is 0. T represents the character-based tag: “B”, “B2”, “B3”, “M”, “E” and “S”, which represent the beginning, second, third, middle, end or single character of a word respectively. The type of character includes: digital, letter, punctuation and other. We also use the word accessor variance for domain adaption. Word accessor variance (AV) was proposed by (Feng et al., 2004) and was used to evaluate how independently a string is used, and thus how likely it is that the string can be a word. The accessor variety of a string s of more than one character is defined as AV (s) = min{L,,,,,(s), R,,,,,(s)} (16) L,,,,,(s) is called the left accessor variety and is defined as the number of distinct characters (predecessors) except “S” that precede s plus the number of distinct sentences of which s appears at the beginning. Similarly, the right accessor variety R,,,,,(s) is defined as the number of distinct characters (successors) except “E” that succeed s plus the number </context>
</contexts>
<marker>Feng, Chen, Deng, Zheng, 2004</marker>
<rawString>Feng, H., K. Chen, X. Deng, and W. Zheng. 2004. Accessor variety criteria for chinese word extraction. Computational Linguistics, 30(1):75–93.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML ’01: Proceedings of the Eighteenth International Conference on Machine Learning.</booktitle>
<contexts>
<context position="1295" citStr="Lafferty et al., 2001" startWordPosition="181" endWordPosition="184">on. 1 Introduction In recent years, Chinese word segmentation (CWS) has undergone great development (Xue, 2003; Peng et al., 2004). The popular method is to regard word segmentation as a sequence labeling problems. The goal of sequence labeling is to assign labels to all elements of a sequence. Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems. Many algorithms have been proposed and the progress has been encouraging, such as SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on. After years of intensive researches, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for out-of-domain text. There are two domains in domain adaption problem, a source domain and a target domain. When we use the machine learning methods for &apos;Available at http://code.google.com/p/ fudannlp/ Chinese word segmentation, we assume that training and test data are drawn from the same distribution. This assumption underlies both theoretical analysis and exper</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, John D., Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML ’01: Proceedings of the Eighteenth International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Peng</author>
<author>F Feng</author>
<author>A McCallum</author>
</authors>
<title>Chinese segmentation and new word detection using conditional random fields.</title>
<date>2004</date>
<booktitle>Proceedings of the 20th international conference on Computational Linguistics.</booktitle>
<contexts>
<context position="803" citStr="Peng et al., 2004" startWordPosition="102" endWordPosition="105">f Computer Science Fudan University Shanghai, China xpqiu@fudan.edu.cn Xuanjing Huang School of Computer Science Fudan University Shanghai, China xjhuang@fudan.edu.cn Abstract In this paper, we describe our systems for CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which focused on the cross-domain performance of Chinese word segmentation algorithms. We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word segmentation. 1 Introduction In recent years, Chinese word segmentation (CWS) has undergone great development (Xue, 2003; Peng et al., 2004). The popular method is to regard word segmentation as a sequence labeling problems. The goal of sequence labeling is to assign labels to all elements of a sequence. Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems. Many algorithms have been proposed and the progress has been encouraging, such as SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on. After years of intensive researches,</context>
</contexts>
<marker>Peng, Feng, McCallum, 2004</marker>
<rawString>Peng, F., F. Feng, and A. McCallum. 2004. Chinese segmentation and new word detection using conditional random fields. Proceedings of the 20th international conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Carlos Guestrin</author>
<author>Daphne Koller</author>
</authors>
<title>Max-margin markov networks.</title>
<date>2003</date>
<booktitle>In Proceedings of Neural Information Processing Systems.</booktitle>
<contexts>
<context position="1355" citStr="Taskar et al., 2003" startWordPosition="190" endWordPosition="193">(CWS) has undergone great development (Xue, 2003; Peng et al., 2004). The popular method is to regard word segmentation as a sequence labeling problems. The goal of sequence labeling is to assign labels to all elements of a sequence. Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems. Many algorithms have been proposed and the progress has been encouraging, such as SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on. After years of intensive researches, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for out-of-domain text. There are two domains in domain adaption problem, a source domain and a target domain. When we use the machine learning methods for &apos;Available at http://code.google.com/p/ fudannlp/ Chinese word segmentation, we assume that training and test data are drawn from the same distribution. This assumption underlies both theoretical analysis and experimental evaluations of learning algorithms. However, the ass</context>
</contexts>
<marker>Taskar, Guestrin, Koller, 2003</marker>
<rawString>Taskar, Ben, Carlos Guestrin, and Daphne Koller. 2003. Max-margin markov networks. In Proceedings of Neural Information Processing Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Tsochantaridis</author>
<author>T Hofmann</author>
<author>T Joachims</author>
<author>Y Altun</author>
</authors>
<title>Support vector machine learning for interdependent and structured output spaces.</title>
<date>2004</date>
<booktitle>In Proceedings of the International Conference on Machine Learning(ICML).</booktitle>
<contexts>
<context position="1238" citStr="Tsochantaridis et al., 2004" startWordPosition="173" endWordPosition="176"> invariant information for cross-domain Chinese word segmentation. 1 Introduction In recent years, Chinese word segmentation (CWS) has undergone great development (Xue, 2003; Peng et al., 2004). The popular method is to regard word segmentation as a sequence labeling problems. The goal of sequence labeling is to assign labels to all elements of a sequence. Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems. Many algorithms have been proposed and the progress has been encouraging, such as SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on. After years of intensive researches, Chinese word segmentation achieves a quite high precision. However, the performance of segmentation is not so satisfying for out-of-domain text. There are two domains in domain adaption problem, a source domain and a target domain. When we use the machine learning methods for &apos;Available at http://code.google.com/p/ fudannlp/ Chinese word segmentation, we assume that training and test data are drawn from the same distribution. This</context>
</contexts>
<marker>Tsochantaridis, Hofmann, Joachims, Altun, 2004</marker>
<rawString>Tsochantaridis, I., T. Hofmann, T. Joachims, and Y Altun. 2004. Support vector machine learning for interdependent and structured output spaces. In Proceedings of the International Conference on Machine Learning(ICML).</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Xue</author>
</authors>
<title>Chinese word segmentation as character tagging.</title>
<date>2003</date>
<booktitle>Computational Linguistics and Chinese Language Processing,</booktitle>
<volume>8</volume>
<issue>1</issue>
<contexts>
<context position="783" citStr="Xue, 2003" startWordPosition="100" endWordPosition="101">iu School of Computer Science Fudan University Shanghai, China xpqiu@fudan.edu.cn Xuanjing Huang School of Computer Science Fudan University Shanghai, China xjhuang@fudan.edu.cn Abstract In this paper, we describe our systems for CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation, which focused on the cross-domain performance of Chinese word segmentation algorithms. We use the online passive-aggressive algorithm with domain invariant information for cross-domain Chinese word segmentation. 1 Introduction In recent years, Chinese word segmentation (CWS) has undergone great development (Xue, 2003; Peng et al., 2004). The popular method is to regard word segmentation as a sequence labeling problems. The goal of sequence labeling is to assign labels to all elements of a sequence. Due to the exponential size of the output space, sequence labeling problems tend to be more challenging than the conventional classification problems. Many algorithms have been proposed and the progress has been encouraging, such as SVM&amp;quot;&amp;quot;&apos; (Tsochantaridis et al., 2004), conditional random fields (CRF) (Lafferty et al., 2001), maximum margin Markov networks (M3N) (Taskar et al., 2003) and so on. After years of i</context>
</contexts>
<marker>Xue, 2003</marker>
<rawString>Xue, N. 2003. Chinese word segmentation as character tagging. Computational Linguistics and Chinese Language Processing, 8(1):29–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised word sense disambiguation rivaling supervised methods.</title>
<date>1995</date>
<booktitle>In Proceedings of the 33rd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>189--196</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3858" citStr="Yarowsky (1995)" startWordPosition="585" endWordPosition="586">he intrinsic geometry of inputs revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Self-training (Zhu, 2005) is also a popular technology. In self-training a classifier is first trained with the small amount of labeled data. The classifier is then used to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. The classifier is re-trained and the procedure repeated. Note the classifier uses its own predictions to teach itself. Yarowsky (1995) uses self-training for word sense disambiguation, e.g. deciding whether the word plant means a living organism or a factory in a given context. Zhao and Kit (2008) integrated unsupervised segmentation and CRF learning for Chinese word segmentation and named entity recognition. They found word accessory variance (Feng et al., 2004) is useful to CWS. 3 Online Passive-Aggressive Algorithm Sequence labeling, the task of assigning labels y = y1, ... , yL to an input sequence x = x1, . . . , xL. Give a sample (x, y), we define the feature is b(x, y). Thus, we can label x with a score function, yˆ =</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>Yarowsky, D. 1995. Unsupervised word sense disambiguation rivaling supervised methods. In Proceedings of the 33rd annual meeting on Association for Computational Linguistics, pages 189–196. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Zhao</author>
<author>C Kit</author>
</authors>
<title>Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition.</title>
<date>2008</date>
<booktitle>In The Sixth SIGHAN Workshop on Chinese Language Processing,</booktitle>
<pages>106--111</pages>
<publisher>Citeseer.</publisher>
<contexts>
<context position="4022" citStr="Zhao and Kit (2008)" startWordPosition="611" endWordPosition="614">s. Self-training (Zhu, 2005) is also a popular technology. In self-training a classifier is first trained with the small amount of labeled data. The classifier is then used to classify the unlabeled data. Typically the most confident unlabeled points, together with their predicted labels, are added to the training set. The classifier is re-trained and the procedure repeated. Note the classifier uses its own predictions to teach itself. Yarowsky (1995) uses self-training for word sense disambiguation, e.g. deciding whether the word plant means a living organism or a factory in a given context. Zhao and Kit (2008) integrated unsupervised segmentation and CRF learning for Chinese word segmentation and named entity recognition. They found word accessory variance (Feng et al., 2004) is useful to CWS. 3 Online Passive-Aggressive Algorithm Sequence labeling, the task of assigning labels y = y1, ... , yL to an input sequence x = x1, . . . , xL. Give a sample (x, y), we define the feature is b(x, y). Thus, we can label x with a score function, yˆ = arg max F(w, b(x, z)), (1) z where w is the parameter of function F(·). The score function of our algorithm is linear function. Given an example (x, y), yˆ is deno</context>
<context position="9027" citStr="Zhao and Kit, 2008" startWordPosition="1589" endWordPosition="1592">n{L,,,,,(s), R,,,,,(s)} (16) L,,,,,(s) is called the left accessor variety and is defined as the number of distinct characters (predecessors) except “S” that precede s plus the number of distinct sentences of which s appears at the beginning. Similarly, the right accessor variety R,,,,,(s) is defined as the number of distinct characters (successors) except “E” that succeed s plus the number of distinct sentences in which s appears at the end. The characters “S” and “E” are defined as the begin and end of a sentence. The word accessor variance was found effective for CWS with unsegmented text (Zhao and Kit, 2008). α¯ = Table 1: Feature templates CZ, T0, (i = −1, 0, 1, 2) CZ, CZ+1, T0, (i = −2, −1, 0,1) T−1,0 Tc: Type of Character AV : word accessor variance 6 CIPS-SIGHAN-2010 Bakeoff CIPS-SIGHAN-2010 bake-off task of Chinese word segmentation focused on the cross-domain performance of Chinese word segmentation algorithms. There are two subtasks for this evaluation: (1) Word Segmentation for Simplified Chinese Text; (2) Word Segmentation for Traditional Chinese Text. The test corpus of each subtask covers four domains: literature, computer science, medicine and finance. We participate in closed trainin</context>
</contexts>
<marker>Zhao, Kit, 2008</marker>
<rawString>Zhao, H. and C. Kit. 2008. Unsupervised segmentation helps supervised learning of character tagging for word segmentation and named entity recognition. In The Sixth SIGHAN Workshop on Chinese Language Processing, pages 106–111. Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojin Zhu</author>
</authors>
<title>Semi-supervised learning literature survey.</title>
<date>2005</date>
<tech>Technical Report 1530,</tech>
<institution>Computer Sciences, University of Wisconsin-Madison.</institution>
<note>http://www.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf.</note>
<contexts>
<context position="2920" citStr="Zhu, 2005" startWordPosition="441" endWordPosition="442">thm(Crammer et al., 2006), which passively accepts a solution whose loss is zero, while it aggressively forces the new prototype vector to stay as close as possible to the one previously learned. The rest of the paper is organized as follows. Section 2 introduces the related works. Then we describe our algorithm in section 3 and 4. The feature templates are described in section 5. Section 6 gives the experimental analysis. Section 7 concludes the paper. 2 Related Works There are several approaches to deal with the domain adaption problem. The first approach is to use semi-supervised learning (Zhu, 2005). The second approach is to incorporate supervised learning with domain invariant information. The third approach is to improve the present model with a few labeled domain data. Altun et al. (2006) investigated structured classification in a semi-supervised setting. They presented a discriminative approach that utilizes the intrinsic geometry of inputs revealed by unlabeled data points and we derive a maximum-margin formulation of semi-supervised learning for structured variables. Self-training (Zhu, 2005) is also a popular technology. In self-training a classifier is first trained with the sm</context>
</contexts>
<marker>Zhu, 2005</marker>
<rawString>Zhu, Xiaojin. 2005. Semi-supervised learning literature survey. Technical Report 1530, Computer Sciences, University of Wisconsin-Madison. http://www.cs.wisc.edu/∼jerryzhu/pub/ssl survey.pdf.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>