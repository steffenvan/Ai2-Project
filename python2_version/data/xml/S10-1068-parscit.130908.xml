<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.068705">
<title confidence="0.997754">
372:Comparing the Benefit of Different Dependency Parsers for Textu-
al Entailment Using Syntactic Constraints Only
</title>
<author confidence="0.908337">
Alexander Volokh Günter Neumann
</author>
<email confidence="0.848846">
alexander.volokh@dfki.de neumann@dfki.de
</email>
<note confidence="0.413465">
DFKI DFKI
</note>
<address confidence="0.726072">
Stuhlsatzenhausweg 3 Stuhlsatzenhausweg 3
66123 Saarbrücken, Germany 66123 Saarbrücken, Germany
</address>
<sectionHeader confidence="0.969543" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9983825">
We compare several state of the art dependency
parsers with our own parser based on a linear
classification technique. Our primary goal is
therefore to use syntactic information only, in or-
der to keep the comparison of the parsers as fair
as possible. We demonstrate, that despite the in-
ferior result using the standard evaluation metrics
for parsers like UAS or LAS on standard test
data, our system achieves comparable results
when used in an application, such as the SemEv-
al-2 #12 evaluation exercise PETE. Our submis-
sion achieved the 4th position out of 19 participat-
ing systems. However, since it only uses a linear
classifier it works 17-20 times faster than other
state of the parsers, as for instance MaltParser or
Stanford Parser.
</bodyText>
<sectionHeader confidence="0.999121" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999954282608696">
Parsing is the process of mapping sentences to
their syntactic representations. These representa-
tions can be used by computers for performing
many interesting natural language processing
tasks, such as question answering or information
extraction. In recent years a lot of parsers have
been developed for this purpose.
A very interesting and important issue is the
comparison between a large number of such pars-
ing systems. The most widespread method is to
evaluate the number of correctly recognized units
according to a certain gold standard. For depend-
ency-based units unlabeled or labeled attachment
scores (percentage of correctly classified depend-
ency relations, either with or without the depend-
ency relation type) are usually used (cf. Buchholz
and Marsi, 2006).
However, parsing is very rarely a goal in itself.
In most cases it is a necessary preprocessing step
for a certain application. Therefore it is usually
not the best option to decide which parser suits
one&apos;s goals best by purely looking on its perform-
ance on some standard test data set. It is rather
more sensible to analyse whether the parser is
able to recognise those syntactic units or rela-
tions, which are most relevant for one&apos;s applica-
tion.
The shared task #12 PETE in the SemEval-
2010 Evaluation Exercises on Semantic Evalu-
ation (Yuret, Han and Turgut, 2010) involved re-
cognizing textual entailments (RTE). RTE is a
binary classification task, whose goal is to de-
termine, whether for a pair of texts T and H the
meaning of H is contained in T (Dagan et al.,
2006). This task can be very complex depending
on the properties of these texts. However, for the
data, released by the organisers of PETE, only the
syntactic information should be sufficient to reli-
ably perform this task. Thus it offers an ideal set-
ting for evaluating the performance of different
parsers.
To our mind evaluation of parsers via RTE is a
very good additional possibility, besides the usual
evaluation metrics, since in most cases the main
thing in real-word applications is to recognize the
primary units, such as the subject, the predicate,
</bodyText>
<page confidence="0.982993">
308
</page>
<bodyText confidence="0.963658913043478">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 308–312,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
the objects, as well as their modifiers, rather than
the other subordinate relations.
We have been developing our own a multilin-
gual dependency parser (called MDParser), which
is based on linear classification1. Whereas the
system is quite fast because the classification is
linear, it usually achieves inferior results (using
UAS/LAS evaluation metrics) in comparison to
other parsers, which for example use kernel-based
classification or other more sophisticated meth-
ods.
Therefore the PETE shared task was a perfect
opportunity for us to investigate whether the in-
ferior result of our parser is also relevant for its
applicability in a concrete task. We have com-
pared our system with three state of the art pars-
ers made available on the PETE web page: Malt-
Parser, MiniPar and StandfordParser. We have
achieved the total score of 0.6545 (200/301 cor-
rect answers on the test data), which is the 4th
rank out of 19 submissions.
</bodyText>
<sectionHeader confidence="0.968934" genericHeader="introduction">
2 NMParser
</sectionHeader>
<bodyText confidence="0.999971047619048">
MDParser stands for multilingual dependency
parser and is a data-driven system, which can be
used to parse text of an arbitrary language for
which training data is available. It is a transition-
based parser and uses a deterministic version of
the Covington&apos;s algorithm (Covington, 2000).
The models of the system are based on various
features, which are extracted from the words of
the sentence, including word forms and part of
speech tags. No additional morphological features
or lemmas are currently used in our models, even
if they are available in the training data, since the
system is especially designed for processing plain
text in different languages, and such components
are not available for every language.
The preprocessing components of MDParser
include a.) a sentence splitter2, since the parser
constructs a dependency structure for individual
sentences, b.) a tokenizer, in order to recognise
the elements between which the dependency rela-
tions will be built3, and c.) a part of speech tagger,
</bodyText>
<footnote confidence="0.9645284">
1http://www.dfki.de/~avolokh/mdparser.pdf
2http://morphadorner.northwestern.edu/morphadorner/sen-
tencesplitter/
3http://morphadorner.northwestern.edu/morphadorner/word-
tokenizer/
</footnote>
<bodyText confidence="0.999858947368421">
in order to determine the part of speech tags,
which are intensively used in the feature models4.
MDParser is an especially fast system because
it uses a linear classification algorithm L1R-
LR(L1 regularised logistic regression) from the
machine learning package LibLinear (Lin et al.,
2008) for constructing its dependency structures
and therefore it is particularly suitable for pro-
cessing very large amounts of data. Thus it can be
used as a part of larger applications in which de-
pendency structures are desired.
Additionally, significant efforts were made in
order to make the gap between our linear classi-
fication and more advanced methods as small as
possible, e.g. by introducing features conjunc-
tions, which are complex features built out of or-
dinary features, as well as methods for automatic-
ally measuring feature usefulness in order to auto-
mate and optimise feature engineering.
</bodyText>
<sectionHeader confidence="0.991248" genericHeader="method">
3 Triple Representation
</sectionHeader>
<bodyText confidence="0.999963375">
Every parser usually produces its own some-
how special representation of the sentence. We
have created such a representation, which we will
call triple representation and have implemented
an automatic transformation of the results of
Minipar, MaltParser, Stanford Parser and of
course MDParser into it (cf. Wang and Neumann,
2007).
The triple representation of a sentence is a set
of triple elements of the form &lt;parent, label,
child&gt;, where child and parent elements stand for
the head and the modifier words and their parts of
speech, and label stands for the relation between
them. E.g. &lt;have:VBZ, SBJ, Somebody:NN&gt;.
This information is extractable from the results of
any dependency parser.
</bodyText>
<sectionHeader confidence="0.984928" genericHeader="method">
4 Predicting Entailment
</sectionHeader>
<bodyText confidence="0.999971285714286">
Whereas the first part of the PETE shared task
was to construct syntactic representations for all
T-H-pairs, the second important subtask was to
determine whether the structure of H is entailed
by the structure of T. The PETE guide5 states that
the following three phenomena were particularly
important to recognise the entailment relation:
</bodyText>
<footnote confidence="0.999864666666667">
4The part of speech tagger was trained with the SVMTool
http://www.lsi.upc.edu/~nlp/SVMTool/
5http://pete.yuret.com/guide
</footnote>
<page confidence="0.994783">
309
</page>
<listItem confidence="0.995406">
1. subject-verb dependency (John kissed
Mary. → John kissed somebody.)
2. verb-object dependency (John kissed
Mary → Mary was kissed.)
3. noun-modifier dependency (The big red
boat sank. → The boat was big.)
</listItem>
<bodyText confidence="0.999688666666667">
Thus we have manually formulated the follow-
ing generic decision rule for determining the en-
tailment relation between T and H:
</bodyText>
<listItem confidence="0.979567823529412">
1. identify the root triple of H &lt;null:null,
ROOT, x&gt;
2. check whether the subject and the com-
plements(objects, verb complements) of the root
word in H are present in T. Formally: all triples of
H of the form &lt;x, z, y&gt; should be contained in
T(x in 1 and 2 is thus the same word).
3. if 2 returns false we have to check wheth-
er H is a structure in passive and T contains the
same content in active voice(a) or the other way
around(b). Formally:
3a. For triples of the form &lt;be:VBZ, SBJ, s&gt;
and &lt;be:VBZ, VC, t&gt; in H check whether there is
a triple of the form &lt;s, NMOD, t&gt; in T.
3b. For triples of the form &lt;u, OBJ,v&gt; in H
check whether there is a triple of the form &lt;v,
NMOD, u&gt; in T.
</listItem>
<bodyText confidence="0.999967956521739">
It turned out that few additional modifications
to the base rule were necessary for some sen-
tences: 1.) For sentences containing conjunctions:
If we were looking for a subject of a certain verb
and could not find it, we investigated whether this
verb is connected via a conjunction with another
one. If true, we compared the subject in H with
the subject of the conjunct verb. 2.) For sentences
containing special verbs, e.g. modal verbs may or
can or auxiliary verbs like to have it turned out to
be important to go one level deeper into the de-
pendency structure and to check whether all of
their arguments in H are also present in T, the
same way as in 3.
A triple &lt;x,z,y&gt; is contained in a set of triples
S, when there exists at least one of the triples in S
&lt;u,w,v&gt;, such that x=u, w=z and y=v. This is also
true if the words somebody, someone or some-
thing are used on one of the equation sides.
Moreover, we use an English lemmatizer for all
word forms, so when checking the equality of two
words we actually check their lemmas, e.g., is and
are are also treated equally.
</bodyText>
<sectionHeader confidence="0.999842" genericHeader="evaluation">
5 Results
</sectionHeader>
<bodyText confidence="0.999989722222222">
We have parsed the 66 pairs of the develop-
ment data with 4 parsers:6 MiniPar, Stanford
Parser, MaltParser and MDParser. After applying
our rule we have achieved the following result:
We used the latest versions of MiniPar7 and
Stanford Parser8. We did not re-test the perform-
ance of these parsers on standard data, since we
were sure that these versions provide the best pos-
sible results of these systems.
As far as the MaltParser is concerned we had to
train our own model. We have trained the model
with the following LibSVM options: “-s_0_-t_1_-
d_2_-g_0.18_-c_0.4_-r_0.4_-e_1.0”. We were
able to achieve a result of 83.86% LAS and
87.25% UAS on the standard CoNLL English test
data, a result which is only slightly worse than
those reported in the literature, where the options
are probably better tuned for the data. The train-
ing data used for training was the same as for
MDParser.
The application of our rule for MDParser and
MaltParser was fully automated, since both use
the same training data and thus work over the
same tag sets. For MiniPar and Stanford Parser,
which construct different dependency structures
with different relation types, we had to go
through all pairs manually in order to investigate
how the rule should be adopted to their tag sets
and structures. However, since we have already
counted the number of structures, for which an
adoptation of the rule would work during this in-
vestigation, we did not implement it in the end.
Therefore these results might be taken with a
pinch of salt, despite the fact that we have tried to
stay as fair as possible and treated some pairs as
correct, even if a quite large modification of the
</bodyText>
<footnote confidence="0.992599">
6For all results reported in this section a desktop PC with
an Intel Core 2 Duo E8400 3.00 GHz processor and 4.00 GB
RAM was used.
7http://webdocs.cs.ualberta.ca/~lindek/minipar
8http://nlp.stanford.edu/downloads/lex-parser.shtml
</footnote>
<table confidence="0.9989486">
Accuracy Parsing Speed
MiniPar 45/66 1233 ms
Stanford Parser 50/66 32889 ms
MaltParser 51/66 37149 ms
MDParser 50/66 1785 ms
</table>
<page confidence="0.997372">
310
</page>
<bodyText confidence="0.997625923076923">
rule was necessary in order to adopt it to the dif-
ferent tag set and/or dependency structure.
For test data we were only able to apply our
rule for the results of MDParser and MaltParser,
since for such a large number of pairs (301) only
the fully automated version of our mechanism for
predicting entailment could be applied. For Mini-
Par and Stanford Parser it was too tedious to ap-
ply it to them manually or to develop a mapping
between their dependency annotations and the
ones used in MDParser or MaltParser. Here are
the official results of our submissions for Malt-
Parser and MDParser:
</bodyText>
<table confidence="0.982694666666667">
Accuracy Parsing Speed
MDParser 197/301 8704 ms
MaltParser 196/301 147938 ms
</table>
<sectionHeader confidence="0.997865" genericHeader="conclusions">
6 Discussion
</sectionHeader>
<bodyText confidence="0.997840961038961">
We were able to show that our parser based on
a linear classification technique is especially fast
compared to other state of the art parsers. Further-
more, despite the fact, that it achieves an inferior
result, when using usual evaluation metrics like
UAS or LAS, it is absolutely suitable for being
used in applications, since the most important de-
pendency relations are recognized correctly even
with a less sophisticated linear classifier as the
one being used in MDParser.
As far as the overall score is concerned we
think a much better result could be achieved, if
we would put more effort into our mechanism for
recognizing entailment using triple representa-
tions. However, many of the pairs required more
than only syntactical information. In many cases
one would need to extend one&apos;s mechanism with
logic, semantics and the possibility to resolve
anaphoric expressions, which to our mind goes
beyond the idea behind the PETE task. Since we
were primarly interested in the comparison
between MaltParser and MDParser, we have not
tried to include solutions for such cases. Here are
some of the pairs we think require more than only
syntax:
(4069 entailment--&amp;quot;YES&amp;quot;) &lt;t&gt;Mr. Sherwood
speculated that the leeway that Sea Containers
has means that Temple would have to &amp;quot;substan-
tially increase their bid if they&apos;re going to top
us.&amp;quot;&lt;/t&gt;
&lt;h&gt;Someone would have to increase the
bid.&lt;/h&gt;
(7003 entailment--&amp;quot;YES&amp;quot;) &lt;t&gt;After all, if you
were going to set up a workshop you had to have
the proper equipment and that was that.&lt;/t&gt;
&lt;h&gt;Somebody had to have the equip-
ment.&lt;/h&gt;
(3132.N entailment--&amp;quot;YES&amp;quot;) &lt;t&gt;The first was
that America had become -- or was in danger of
becoming -- a second-rate military power.&lt;/t&gt;
&lt;h&gt;America was in danger.&lt;/h&gt;
→ 4069, 7003 and 3132.N are examples for
sentences were beyond syntactical information lo-
gic is required. Moreover we are surprised that
sentences of the form “if A, then B” entail B and
a sentence of the form “A or B” entails B, since
“or” in this case means uncertainty.
(4071.N entailment--&amp;quot;NO&amp;quot;) &lt;t&gt;Interpublic
Group said its television programming operations
-- which it expanded earlier this year -- agreed to
supply more than 4,000 hours of original pro-
gramming across Europe in 1990.&lt;/t&gt;
&lt;h&gt;Interpublic Group expanded.&lt;/h&gt;
(6034 entailment--&amp;quot;YES&amp;quot;) &lt;t&gt;&amp;quot;Oh,&amp;quot; said the
woman, &amp;quot;I&apos;ve seen that picture already.&amp;quot;&lt;/t&gt;
&lt;h&gt;The woman has seen something.&lt;/h&gt;
→ In 4071.N one has to resolve “it” in “it ex-
panded” to Interpublic Group. In 6034 one has to
resolve “I” in “I&apos;ve seen” to “the woman”. Both
cases are examples for the necessity of anaphora
resolution, which goes beyond syntax as well.
(2055) &lt;t&gt;The Big Board also added computer
capacity to handle huge surges in trading
volume.&lt;/t&gt;
&lt;h&gt;Surges were handled.&lt;/h&gt;
→ If something is added in order to do some-
thing it does not entail that this something is thus
automatically done. Anyways pure syntax is not
sufficient, since the entailment depends on the
verb used in such a construction.
(3151.N) &lt;t&gt;Most of them are Democrats and
nearly all consider themselves, and are viewed as,
liberals.&lt;/t&gt;
&lt;h&gt;Some consider themselves liberal.&lt;/h&gt;
→ One has to know that the semantics of “con-
sider themselves as liberals” and “consider them-
selves liberal” is the same.
</bodyText>
<sectionHeader confidence="0.998056" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<page confidence="0.996651">
311
</page>
<bodyText confidence="0.9999858">
The work presented here was partially suppor-
ted by a research grant from the German Federal
Ministry of Economics and Technology (BMWi)
to the DFKI project Theseus Ordo TechWatch
(FKZ: 01MQ07016). We thank Joakim Nivre and
Johan Hall for their support and tips when train-
ing models with MaltParser. Additionally, we are
very grateful to Sven Schmeier for providing us
with a trained part of speech tagger for English
and for his support when using this tool.
</bodyText>
<sectionHeader confidence="0.999194" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999346465116279">
Michael A. Covington, 2000. A Fundamental Al-
gorithm for Dependency Parsing. In Proceedings of
the 39th Annual ACM Southeast Conference.
Dan Klein and Christopher D. Manning, 2003. Accur-
ate Unlexicalized Parsing. Proceedings of the 41st
Meeting of the Association for Computational Lin-
guistics, pp. 423-430.
Lin D, 2003. Dependency-Based Evaluation Of Mini-
par. In Building and using Parsed Corpora Edited by:
Abeillé A. Dordrecht: Kluwer; 2003.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X
shared task on multilingual dependency parsing. In
Proceedings of CONLL-X, pages 149–164, New York.
Ido Dagan, Oren Glickman and Bernardo Magnini.
The PASCAL Recognising Textual Entailment Chal-
lenge. In Quinonero-Candela, J.; Dagan, I.; Magnini,
B.; d&apos;Alche-Buc, F. (Eds.), Machine Learning Chal-
lenges. Lecture Notes in Computer Science, Vol. 3944,
pp. 177-190, Springer, 2006.
Nivre, J., J. Hall and J. Nilsson, 2006. MaltParser: A
Data-Driven Parser-Generator for Dependency Pars-
ing. In Proceedings of the fifth international confer-
ence on Language Resources and Evaluation
(LREC2006), May 24-26, 2006, Genoa, Italy, pp.
2216-2219.
Rui Wang and Günter Neumann, 2007. Recognizing
Textual Entailment Using a Subsequence Kernel
Method. In Proceedings of AAAI 2007.
R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin,
2008. LIBLINEAR: A Library for Large Linear Classi-
fication. Journal of Machine Learning Research, 9(4):
1871–1874.
Deniz Yuret, Aydın Han and Zehra Turgut, 2010. Se-
mEval-2010 Task 12: Parser Evaluation using Textual
Entailments. In Proceedings of the SemEval-2010
Evaluation Exercises on Semantic Evaluation.
The Stanford Parser: A Statistical Parser.
http://nlp.stanford.edu/downloads/lex-parser.shtml
Maltparser. http://maltparser.org/
Minipar. http://webdocs.cs.ualberta.ca/~lindek/mini-
par.htm
MDParser: Multilingual Dependency Parser.
http://mdparser.sb.dfki.de/
</reference>
<page confidence="0.998554">
312
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.170580">
<title confidence="0.9567245">372:Comparing the Benefit of Different Dependency Parsers for Textual Entailment Using Syntactic Constraints Only</title>
<author confidence="0.996468">Alexander Volokh Günter Neumann</author>
<email confidence="0.565675">alexander.volokh@dfki.deneumann@dfki.de</email>
<affiliation confidence="0.93409">DFKI DFKI</affiliation>
<address confidence="0.97296">Stuhlsatzenhausweg 3 Stuhlsatzenhausweg 3 66123 Saarbrücken, Germany 66123 Saarbrücken, Germany</address>
<abstract confidence="0.995567125">We compare several state of the art dependency parsers with our own parser based on a linear classification technique. Our primary goal is therefore to use syntactic information only, in order to keep the comparison of the parsers as fair as possible. We demonstrate, that despite the inferior result using the standard evaluation metrics for parsers like UAS or LAS on standard test data, our system achieves comparable results when used in an application, such as the SemEval-2 #12 evaluation exercise PETE. Our submisachieved the position out of 19 participating systems. However, since it only uses a linear classifier it works 17-20 times faster than other state of the parsers, as for instance MaltParser or</abstract>
<intro confidence="0.361746">Stanford Parser.</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael A Covington</author>
</authors>
<title>A Fundamental Algorithm for Dependency Parsing.</title>
<date>2000</date>
<booktitle>In Proceedings of the 39th Annual ACM Southeast Conference.</booktitle>
<contexts>
<context position="4558" citStr="Covington, 2000" startWordPosition="726" endWordPosition="727">ant for its applicability in a concrete task. We have compared our system with three state of the art parsers made available on the PETE web page: MaltParser, MiniPar and StandfordParser. We have achieved the total score of 0.6545 (200/301 correct answers on the test data), which is the 4th rank out of 19 submissions. 2 NMParser MDParser stands for multilingual dependency parser and is a data-driven system, which can be used to parse text of an arbitrary language for which training data is available. It is a transitionbased parser and uses a deterministic version of the Covington&apos;s algorithm (Covington, 2000). The models of the system are based on various features, which are extracted from the words of the sentence, including word forms and part of speech tags. No additional morphological features or lemmas are currently used in our models, even if they are available in the training data, since the system is especially designed for processing plain text in different languages, and such components are not available for every language. The preprocessing components of MDParser include a.) a sentence splitter2, since the parser constructs a dependency structure for individual sentences, b.) a tokenize</context>
</contexts>
<marker>Covington, 2000</marker>
<rawString>Michael A. Covington, 2000. A Fundamental Algorithm for Dependency Parsing. In Proceedings of the 39th Annual ACM Southeast Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate Unlexicalized Parsing.</title>
<date>2003</date>
<booktitle>Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning, 2003. Accurate Unlexicalized Parsing. Proceedings of the 41st Meeting of the Association for Computational Linguistics, pp. 423-430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>Dependency-Based Evaluation Of Minipar. In Building and using Parsed Corpora Edited by: Abeillé A.</title>
<date>2003</date>
<publisher>Kluwer;</publisher>
<location>Dordrecht:</location>
<marker>Lin, 2003</marker>
<rawString>Lin D, 2003. Dependency-Based Evaluation Of Minipar. In Building and using Parsed Corpora Edited by: Abeillé A. Dordrecht: Kluwer; 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLL-X shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of CONLL-X,</booktitle>
<pages>149--164</pages>
<location>New York.</location>
<contexts>
<context position="1826" citStr="Buchholz and Marsi, 2006" startWordPosition="273" endWordPosition="276">ny interesting natural language processing tasks, such as question answering or information extraction. In recent years a lot of parsers have been developed for this purpose. A very interesting and important issue is the comparison between a large number of such parsing systems. The most widespread method is to evaluate the number of correctly recognized units according to a certain gold standard. For dependency-based units unlabeled or labeled attachment scores (percentage of correctly classified dependency relations, either with or without the dependency relation type) are usually used (cf. Buchholz and Marsi, 2006). However, parsing is very rarely a goal in itself. In most cases it is a necessary preprocessing step for a certain application. Therefore it is usually not the best option to decide which parser suits one&apos;s goals best by purely looking on its performance on some standard test data set. It is rather more sensible to analyse whether the parser is able to recognise those syntactic units or relations, which are most relevant for one&apos;s application. The shared task #12 PETE in the SemEval2010 Evaluation Exercises on Semantic Evaluation (Yuret, Han and Turgut, 2010) involved recognizing textual ent</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLL-X shared task on multilingual dependency parsing. In Proceedings of CONLL-X, pages 149–164, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge. In</title>
<date>2006</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>3944</volume>
<pages>177--190</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="2598" citStr="Dagan et al., 2006" startWordPosition="411" endWordPosition="414"> not the best option to decide which parser suits one&apos;s goals best by purely looking on its performance on some standard test data set. It is rather more sensible to analyse whether the parser is able to recognise those syntactic units or relations, which are most relevant for one&apos;s application. The shared task #12 PETE in the SemEval2010 Evaluation Exercises on Semantic Evaluation (Yuret, Han and Turgut, 2010) involved recognizing textual entailments (RTE). RTE is a binary classification task, whose goal is to determine, whether for a pair of texts T and H the meaning of H is contained in T (Dagan et al., 2006). This task can be very complex depending on the properties of these texts. However, for the data, released by the organisers of PETE, only the syntactic information should be sufficient to reliably perform this task. Thus it offers an ideal setting for evaluating the performance of different parsers. To our mind evaluation of parsers via RTE is a very good additional possibility, besides the usual evaluation metrics, since in most cases the main thing in real-word applications is to recognize the primary units, such as the subject, the predicate, 308 Proceedings of the 5th International Works</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman and Bernardo Magnini. The PASCAL Recognising Textual Entailment Challenge. In Quinonero-Candela, J.; Dagan, I.; Magnini, B.; d&apos;Alche-Buc, F. (Eds.), Machine Learning Challenges. Lecture Notes in Computer Science, Vol. 3944, pp. 177-190, Springer, 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>MaltParser: A Data-Driven Parser-Generator for Dependency Parsing.</title>
<date>2006</date>
<booktitle>In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC2006),</booktitle>
<pages>2216--2219</pages>
<location>Genoa, Italy,</location>
<marker>Nivre, Hall, Nilsson, 2006</marker>
<rawString>Nivre, J., J. Hall and J. Nilsson, 2006. MaltParser: A Data-Driven Parser-Generator for Dependency Parsing. In Proceedings of the fifth international conference on Language Resources and Evaluation (LREC2006), May 24-26, 2006, Genoa, Italy, pp. 2216-2219.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rui Wang</author>
<author>Günter Neumann</author>
</authors>
<title>Recognizing Textual Entailment Using a Subsequence Kernel Method.</title>
<date>2007</date>
<booktitle>In Proceedings of AAAI</booktitle>
<contexts>
<context position="6704" citStr="Wang and Neumann, 2007" startWordPosition="1038" endWordPosition="1041">nd more advanced methods as small as possible, e.g. by introducing features conjunctions, which are complex features built out of ordinary features, as well as methods for automatically measuring feature usefulness in order to automate and optimise feature engineering. 3 Triple Representation Every parser usually produces its own somehow special representation of the sentence. We have created such a representation, which we will call triple representation and have implemented an automatic transformation of the results of Minipar, MaltParser, Stanford Parser and of course MDParser into it (cf. Wang and Neumann, 2007). The triple representation of a sentence is a set of triple elements of the form &lt;parent, label, child&gt;, where child and parent elements stand for the head and the modifier words and their parts of speech, and label stands for the relation between them. E.g. &lt;have:VBZ, SBJ, Somebody:NN&gt;. This information is extractable from the results of any dependency parser. 4 Predicting Entailment Whereas the first part of the PETE shared task was to construct syntactic representations for all T-H-pairs, the second important subtask was to determine whether the structure of H is entailed by the structure </context>
</contexts>
<marker>Wang, Neumann, 2007</marker>
<rawString>Rui Wang and Günter Neumann, 2007. Recognizing Textual Entailment Using a Subsequence Kernel Method. In Proceedings of AAAI 2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Fan</author>
<author>K Chang</author>
<author>C Hsieh</author>
<author>X Wang</author>
<author>C Lin</author>
</authors>
<title>LIBLINEAR: A Library for Large Linear Classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<issue>4</issue>
<pages>1871--1874</pages>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>R. Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin, 2008. LIBLINEAR: A Library for Large Linear Classification. Journal of Machine Learning Research, 9(4): 1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deniz Yuret</author>
<author>Aydın Han</author>
<author>Zehra Turgut</author>
</authors>
<title>SemEval-2010 Task 12: Parser Evaluation using Textual Entailments.</title>
<date>2010</date>
<booktitle>In Proceedings of the SemEval-2010 Evaluation Exercises on Semantic Evaluation.</booktitle>
<marker>Yuret, Han, Turgut, 2010</marker>
<rawString>Deniz Yuret, Aydın Han and Zehra Turgut, 2010. SemEval-2010 Task 12: Parser Evaluation using Textual Entailments. In Proceedings of the SemEval-2010 Evaluation Exercises on Semantic Evaluation.</rawString>
</citation>
<citation valid="false">
<title>The Stanford Parser: A Statistical Parser.</title>
<note>http://nlp.stanford.edu/downloads/lex-parser.shtml Maltparser. http://maltparser.org/ Minipar. http://webdocs.cs.ualberta.ca/~lindek/minipar.htm</note>
<marker></marker>
<rawString>The Stanford Parser: A Statistical Parser. http://nlp.stanford.edu/downloads/lex-parser.shtml Maltparser. http://maltparser.org/ Minipar. http://webdocs.cs.ualberta.ca/~lindek/minipar.htm</rawString>
</citation>
<citation valid="false">
<title>MDParser: Multilingual Dependency Parser.</title>
<note>http://mdparser.sb.dfki.de/</note>
<marker></marker>
<rawString>MDParser: Multilingual Dependency Parser. http://mdparser.sb.dfki.de/</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>