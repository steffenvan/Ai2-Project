<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.017247">
<title confidence="0.9972335">
Ngram-based statistical machine translation enhanced with multiple
weighted reordering hypotheses
</title>
<author confidence="0.6313825">
Marta R. Costa-juss`a, Josep M. Crego, Patrik Lambert, Maxim Khalilov
Jos´e A. R. Fonollosa, Jos´e B. Mari˜no and Rafael E. Banchs
</author>
<affiliation confidence="0.5099035">
Department of Signal Theory and Communications
TALP Research Center (UPC)
</affiliation>
<address confidence="0.659272">
Barcelona 08034, Spain
</address>
<email confidence="0.998438">
(mruiz,jmcrego,lambert,khalilov,adrian,canton,rbanchs)@gps.tsc.upc.edu
</email>
<sectionHeader confidence="0.995909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996327857142857">
This paper describes the 2007 Ngram-based sta-
tistical machine translation system developed at
the TALP Research Center of the UPC (Uni-
versitat Polit`ecnica de Catalunya) in Barcelona.
Emphasis is put on improvements and extensions
of the previous years system, being highlighted
and empirically compared. Mainly, these include
a novel word ordering strategy based on: (1) sta-
tistically monotonizing the training source cor-
pus and (2) a novel reordering approach based
on weighted reordering graphs. In addition, this
system introduces a target language model based
on statistical classes, a feature for out-of-domain
units and an improved optimization procedure.
The paper provides details of this system par-
ticipation in the ACL 2007 SECOND WORK-
SHOP ON STATISTICAL MACHINE TRANSLA-
TION. Results on three pairs of languages are
reported, namely from Spanish, French and Ger-
man into English (and the other way round) for
both the in-domain and out-of-domain tasks.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959827586207">
Based on estimating a joint-probability model between
the source and the target languages, Ngram-based SMT
has proved to be a very competitive alternatively to
phrase-based and other state-of-the-art systems in previ-
ous evaluation campaigns, as shown in (Koehn and Monz,
2005; Koehn and Monz, 2006).
Given the challenge of domain adaptation, efforts have
been focused on improving strategies for Ngram-based
SMT which could generalize better. Specifically, a novel
reordering strategy is explored. It is based on extending
the search by using precomputed statistical information.
Results are promising while keeping computational ex-
penses at a similar level as monotonic search. Addition-
ally, a bonus for tuples from the out-of-domain corpus is
introduced, as well as a target language model based on
statistical classes. One of the advantages of working with
statistical classes is that they can easily be used for any
pair of languages.
This paper is organized as follows. Section 2 briefly
reviews last year’s system, including tuple definition and
extraction, translation model and feature functions, de-
coding tool and optimization criterion. Section 3 delves
into the word ordering problem, by contrasting last year
strategy with the novel weighted reordering input graph.
Section 4 focuses on new features: both tuple-domain
bonus and target language model based on classes. Later
on, Section 5 reports on all experiments carried out for
WMT 2007. Finally, Section 6 sums up the main conclu-
sions from the paper and discusses future research lines.
</bodyText>
<sectionHeader confidence="0.929891" genericHeader="method">
2 Baseline N-gram-based SMT System
</sectionHeader>
<bodyText confidence="0.999887333333333">
The translation model is based on bilingual n-grams. It
actually constitutes a language model of bilingual units,
referred to as tuples, which approximates the joint proba-
bility between source and target languages by using bilin-
gual n-grams.
Tuples are extracted from a word-to-word aligned cor-
pus according to the following two constraints: first, tu-
ple extraction should produce a monotonic segmentation
of bilingual sentence pairs; and second, no smaller tuples
can be extracted without violating the previous constraint.
For all experiments presented here, the translation
model consisted of a 4-gram language model of tuples.
In addition to this bilingual n-gram translation model, the
baseline system implements a log linear combination of
four feature functions. These four additional models are:
a target language model (a 5-gram model of words);
a word bonus; a source-to-target lexicon model and a
target-to-source lexicon model, both features provide a
complementary probability for each tuple in the transla-
tion table.
The decoder (called MARIE) for this translation sys-
</bodyText>
<page confidence="0.983426">
167
</page>
<bodyText confidence="0.9377094">
Proceedings of the Second Workshop on Statistical Machine Translation, pages 167–170,
Prague, June 2007. c�2007 Association for Computational Linguistics
tem is based on a beam search 1.
This baseline system is actually the same system used
for the first shared task “Exploiting Parallel Texts for Sta-
tistical Machine Translation” of the ACL 2005 Work-
shop on Building and Using Parallel Texts: Data-Driven
Machine Translation and Beyond. A more detailed de-
scription of the system can be found in (Mari˜no et al.,
2006).
</bodyText>
<sectionHeader confidence="0.7527105" genericHeader="method">
3 Baseline System Enhanced with a
Weighted Reordering Input Graph
</sectionHeader>
<bodyText confidence="0.997159">
This section briefly describes the statistical machine re-
ordering (SMR) technique. Further details on the archi-
tecture of SMR system can be found on (Costa-juss`a and
Fonollosa, 2006).
</bodyText>
<subsectionHeader confidence="0.992295">
3.1 Concept
</subsectionHeader>
<bodyText confidence="0.999974833333333">
The SMR system can be seen as a SMT system which
translates from an original source language (S) to a re-
ordered source language (S’), given a target language
(T). The SMR technique works with statistical word
classes (Och, 1999) instead of words themselves (partic-
ularly, we have used 200 classes in all experiments).
</bodyText>
<figureCaption confidence="0.98515">
Figure 1: SMR approach in the (A) training step (B) in
the test step (the weight of each arch is in brackets).
</figureCaption>
<subsectionHeader confidence="0.999171">
3.2 Using SMR technique to improve SMT training
</subsectionHeader>
<bodyText confidence="0.9997755">
The original source corpus S is translated into the re-
ordered source corpus S’ with the SMR system. Fig-
ure 1 (A) shows the corresponding block diagram. The
reordered training source corpus and the original training
target corpus are used to build the SMT system.
The main difference here is that the training is com-
puted with the S’2T task instead of the S2T original task.
Figure 2 (A) shows an example of the alignment com-
puted on the original training corpus. Figure 2 (B) shows
the same links but with the source training corpus in a
different order (this training corpus comes from the SMR
output). Although, the quality in alignment is the same,
the tuples that can be extracted change (notice that the
tuple extraction is monotonic). We are able to extract
</bodyText>
<footnote confidence="0.834976">
1http://gps-tsc.upc.es/veu/soft/soft/marie/
</footnote>
<bodyText confidence="0.994538666666667">
smaller tuples which reduces the translation vocabulary
sparseness. These new tuples are used to build the SMT
system.
</bodyText>
<figureCaption confidence="0.987713666666667">
Figure 2: Alignment and tuple extraction (A) original
training source corpus (B) reordered training source cor-
pus.
</figureCaption>
<subsectionHeader confidence="0.92724">
3.3 Using SMR technique to generate multiple
weighted reordering hypotheses
</subsectionHeader>
<bodyText confidence="0.99998825">
The SMR system, having its own search, can generate ei-
ther an output 1-best or an output graph. In decoding, the
SMR technique generates an output graph which is used
as an input graph by the SMT system. Figure 1 (B) shows
the corresponding block diagram in decoding: the SMR
output graph is given as an input graph to the SMT sys-
tem. Hereinafter, this either SMR output graph or SMT
input graph will be referred to as (weighted) reordering
graph. The monotonic search in the SMT system is ex-
tended with reorderings following this reordering graph.
This reordering graph has multiple paths and each path
has its own weight. This weight is added as a feature
function in the log-linear framework. Figure 3 shows the
weighted reordering graph.
The main difference with the reordering technique for
WMT06 (Crego et al., 2006) lies in (1) the tuples are ex-
tracted from the word alignment between the reordered
source training corpus and the given target training cor-
pus and (2) the graph structure: the SMR graph provides
weights for each reordering path.
</bodyText>
<sectionHeader confidence="0.980126" genericHeader="method">
4 Other features and functionalities
</sectionHeader>
<bodyText confidence="0.99963">
In addition to the novel reordering strategy, we consider
two new features functions.
</bodyText>
<subsectionHeader confidence="0.8939575">
4.1 Target Language Model based on Statistical
Classes
</subsectionHeader>
<bodyText confidence="0.999952666666667">
This feature implements a 5-gram language model of tar-
get statistical classes (Och, 1999). This model is trained
by considering statistical classes, instead of words, for
</bodyText>
<page confidence="0.997301">
168
</page>
<figureCaption confidence="0.9932805">
Figure 3: Weighted reordering input graph for SMT sys-
tem.
</figureCaption>
<bodyText confidence="0.999181333333333">
the target side of the training corpus. Accordingly, the tu-
ple translation unit is redefined in terms of a triplet which
includes: a source string containing the source side of
the tuple, a target string containing the target side of the
tuple, and a class string containing the statistical classes
corresponding to the words in the target strings.
</bodyText>
<subsectionHeader confidence="0.939803">
4.2 Bonus for out-of-domain tuples
</subsectionHeader>
<bodyText confidence="0.99938925">
This feature adds a bonus to those tuples which comes
from the training of the out-of-domain task. This feature
is added when optimizing with the development of the
out-of-domain task.
</bodyText>
<subsectionHeader confidence="0.993836">
4.3 Optimization
</subsectionHeader>
<bodyText confidence="0.99998575">
Finally, a n-best re-ranking strategy is implemented
which is used for optimization purposes just as pro-
posed in http://www.statmt.org/jhuws/. This procedure
allows for a faster and more efficient adjustment of model
weights by means of a double-loop optimization, which
provides significant reduction of the number of transla-
tions that should be carried out. The current optimization
procedure uses the Simplex algorithm.
</bodyText>
<sectionHeader confidence="0.991337" genericHeader="method">
5 Shared Task Framework
</sectionHeader>
<subsectionHeader confidence="0.892334">
5.1 Data
</subsectionHeader>
<bodyText confidence="0.999751833333333">
The data provided for this shared task corresponds to a
subset of the official transcriptions of the European Par-
liament Plenary Sessions 2. Additionally, there was avail-
able a smaller corpus called News-Commentary. For all
tasks and domains, our training corpus was the catenation
of both.
</bodyText>
<footnote confidence="0.775332">
2http://www.statmt.org/wmt07/shared-task/
</footnote>
<subsectionHeader confidence="0.998335">
5.2 Processing details
</subsectionHeader>
<bodyText confidence="0.999733857142857">
Word Alignment. The word alignment is automati-
cally computed by using GIZA++ 3 in both directions,
which are symmetrized by using the union operation. In-
stead of aligning words themselves, stems are used for
aligning. Afterwards case sensitive words are recovered.
Spanish Morphology Reduction. We implemented a
morphology reduction of the Spanish language as a pre-
processing step. As a consequence, training data sparse-
ness due to Spanish morphology was reduced improving
the performance of the overall translation system. In par-
ticular, the pronouns attached to the verb were separated
and contractions as del or al are splited into de el or a
el. As a post-processing, in the En2Es direction we used
a POS target language model as a feature (instead of the
target language model based on classes) that allowed to
recover the segmentations (de Gispert, 2006).
Language Model Interpolation. In other to better
adapt the system to the out-of-domain condition, the
target language model feature was built by combining
two 5-gram target language models (using SRILM 4).
One was trained from the EuroParl training data set, and
the other from the available, but much smaller, news-
commentary data set. The combination weights for the
EuroParl and news-commentary language models were
empirically adjusted by following a minimum perplexity
criterion. A relative perplexity reduction around 10-15%
respect to original EuroParl language model was achieved
in all the tasks.
</bodyText>
<subsectionHeader confidence="0.992348">
5.3 Experiments and Results
</subsectionHeader>
<bodyText confidence="0.999825611111111">
The main difference between this year’s and last year’s
systems are: the amount of data provided; the word align-
ment; the Spanish morphology reduction; the reordering
technique; the extra target language model based on sta-
tistical classes (except for the En2Es); and the bonus for
the out-of-domain task (only for the En2Es task).
Among them, the most important is the reordering
technique. That is why we provide a fair comparison be-
tween the reordering patterns (Crego and Mari˜no, 2006)
technique and the SMR reordering technique. Table 1
shows the system described above using either reorder-
ing patterns or the SMR technique. The BLEU calcula-
tion was case insensitive and sensitive to tokenization.
Table 2 presents the BLEU score obtained for the 2006
test data set comparing last year’s and this year’s systems.
The computed BLEU scores are case insensitive, sensi-
tive to tokenization and uses one translation reference.
The improvement in BLEU results shown from UPC-jm
</bodyText>
<footnote confidence="0.9994175">
3http://www.fjoch.com/GIZA++.html
4http://www.speech.sri.com/projects/srilm/
</footnote>
<page confidence="0.988384">
169
</page>
<table confidence="0.982176666666667">
Task Reordering patterns SMR technique
es2en 31.21 33.34
en2es 31.67 32.33
</table>
<tableCaption confidence="0.982212">
Table 1: BLEU comparison: reordering patterns vs. SMR
technique.
</tableCaption>
<table confidence="0.999708625">
Task UPC-jm 2006 UPC 2007
in-d out-d in-d out-d
es2en 31.01 27.92 33.34 32.85
en2es 30.44 25.59 32.33 33.07
fr2en 30.42 21.79 32.44 26.93
en2fr 31.75 23.30 32.30 27.03
de2en 24.43 17.57 26.54 21.63
en2de 17.73 10.96 19.74 15.06
</table>
<tableCaption confidence="0.9698195">
Table 2: BLEU scores for each of the six translation di-
rections considered (computed over 2006 test set) com-
paring last year’s and this year’s system results (in-
domain and out-domain).
</tableCaption>
<bodyText confidence="0.992784833333333">
2006 Table 2 and reordering patterns Table 1 in the En-
glish/Spanish in-domain task comes from the combina-
tion of: the additional corpora, the word alignment, the
Spanish morphology reduction and the extra target lan-
guage model based on classes (only in the Es2En direc-
tion).
</bodyText>
<sectionHeader confidence="0.996713" genericHeader="conclusions">
6 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999922">
This paper describes the UPC system for the WMT07
Evaluation. In the framework of Ngram-based system, a
novel reordering strategy which can be used for any pair
of languages has been presented and it has been showed
to significantly improve translation performance. Ad-
ditionally two features has been added to the log-lineal
scheme: the target language model based on classes and
the bonus for out-of-domain translation units.
</bodyText>
<sectionHeader confidence="0.999295" genericHeader="acknowledgments">
7 Acknowledgments
</sectionHeader>
<bodyText confidence="0.8953375">
This work has been funded by the European Union un-
der the TC-STAR project (IST-2002-FP6-506738) and
the Spanish Government under grant TEC2006-13964-
C03 (AVIVAVOZ project).
</bodyText>
<sectionHeader confidence="0.994815" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999915333333333">
M.R. Costa-juss`a and J.A.R. Fonollosa. 2006. Statistical
machine reordering. In EMNLP, pages 71–77, Sydney,
July. ACL.
J.M. Crego and J.B. Mari˜no. 2006. Reordering experi-
ments for n-gram-based smt. In SLT, pages 242–245,
Aruba.
Josep M. Crego, Adri`a de Gispert, Patrik Lambert,
Marta R. Costa-juss`a, Maxim Khalilov, Rafael Banchs,
Jos´e B. Mari˜no, and Jos´e A. R. Fonollosa. 2006. N-
gram-based smt system enhanced with reordering pat-
terns. In WMT, pages 162–165, New York City, June.
ACL.
Adri`a de Gispert. 2006. Introducing Linguistic Knowl-
edge in Statistical Machine Translation. Ph.D. thesis,
Universitat Polit`ecnica de Catalunya, December.
Philipp Koehn and Christof Monz. 2005. Shared task:
Statistical machine translation between european lan-
guages. In WMT, pages 119–124, Michigan, June.
ACL.
Philipp Koehn and Christof Monz. 2006. Manual and
automatic evaluation of machine translation between
european languages. In WMT, pages 102–121, New
York City, June. ACL.
J.B. Mari˜no, R.E. Banchs, J.M. Crego, A. de Gispert,
P. Lambert, J.A.R. Fonollosa, and M.R. Costa-juss`a.
2006. N-gram based machine translation. Computa-
tional Linguistics, 32(4):527–549, December.
F.J. Och. 1999. An efficient method for determin-
ing bilingual word classes. In EACL, pages 71–76,
Bergen, Norway, June.
</reference>
<page confidence="0.997426">
170
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925254">
<title confidence="0.9834165">Ngram-based statistical machine translation enhanced with weighted reordering hypotheses</title>
<author confidence="0.996174">Marta R Costa-juss`a</author>
<author confidence="0.996174">Josep M Crego</author>
<author confidence="0.996174">Patrik Lambert</author>
<author confidence="0.996174">Maxim A R Fonollosa</author>
<author confidence="0.996174">B Jos´e</author>
<author confidence="0.996174">E Rafael</author>
<affiliation confidence="0.999426">Department of Signal Theory and TALP Research Center</affiliation>
<address confidence="0.970517">Barcelona 08034,</address>
<email confidence="0.999774">(mruiz,jmcrego,lambert,khalilov,adrian,canton,rbanchs)@gps.tsc.upc.edu</email>
<abstract confidence="0.999230636363636">This paper describes the 2007 Ngram-based statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polit`ecnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the previous years system, being highlighted and empirically compared. Mainly, these include a novel word ordering strategy based on: (1) statistically monotonizing the training source corpus and (2) a novel reordering approach based on weighted reordering graphs. In addition, this system introduces a target language model based on statistical classes, a feature for out-of-domain units and an improved optimization procedure. The paper provides details of this system parin the ON STATISTICAL MACHINE Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M R Costa-juss`a</author>
<author>J A R Fonollosa</author>
</authors>
<title>Statistical machine reordering.</title>
<date>2006</date>
<booktitle>In EMNLP,</booktitle>
<pages>71--77</pages>
<publisher>ACL.</publisher>
<location>Sydney,</location>
<marker>Costa-juss`a, Fonollosa, 2006</marker>
<rawString>M.R. Costa-juss`a and J.A.R. Fonollosa. 2006. Statistical machine reordering. In EMNLP, pages 71–77, Sydney, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J M Crego</author>
<author>J B Mari˜no</author>
</authors>
<title>Reordering experiments for n-gram-based smt. In</title>
<date>2006</date>
<booktitle>SLT,</booktitle>
<pages>242--245</pages>
<location>Aruba.</location>
<marker>Crego, Mari˜no, 2006</marker>
<rawString>J.M. Crego and J.B. Mari˜no. 2006. Reordering experiments for n-gram-based smt. In SLT, pages 242–245, Aruba.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Josep M Crego</author>
<author>Adri`a de Gispert</author>
<author>Patrik Lambert</author>
<author>Marta R Costa-juss`a</author>
<author>Maxim Khalilov</author>
<author>Rafael Banchs</author>
<author>Jos´e B Mari˜no</author>
<author>Jos´e A R Fonollosa</author>
</authors>
<title>Ngram-based smt system enhanced with reordering patterns.</title>
<date>2006</date>
<booktitle>In WMT,</booktitle>
<pages>162--165</pages>
<publisher>ACL.</publisher>
<location>New York City,</location>
<marker>Crego, de Gispert, Lambert, Costa-juss`a, Khalilov, Banchs, Mari˜no, Fonollosa, 2006</marker>
<rawString>Josep M. Crego, Adri`a de Gispert, Patrik Lambert, Marta R. Costa-juss`a, Maxim Khalilov, Rafael Banchs, Jos´e B. Mari˜no, and Jos´e A. R. Fonollosa. 2006. Ngram-based smt system enhanced with reordering patterns. In WMT, pages 162–165, New York City, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Adri`a de Gispert</author>
</authors>
<title>Introducing Linguistic Knowledge in Statistical Machine Translation.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>Universitat Polit`ecnica de Catalunya,</institution>
<marker>de Gispert, 2006</marker>
<rawString>Adri`a de Gispert. 2006. Introducing Linguistic Knowledge in Statistical Machine Translation. Ph.D. thesis, Universitat Polit`ecnica de Catalunya, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Shared task: Statistical machine translation between european languages. In</title>
<date>2005</date>
<booktitle>WMT,</booktitle>
<pages>119--124</pages>
<publisher>ACL.</publisher>
<location>Michigan,</location>
<contexts>
<context position="1659" citStr="Koehn and Monz, 2005" startWordPosition="232" endWordPosition="235">roved optimization procedure. The paper provides details of this system participation in the ACL 2007 SECOND WORKSHOP ON STATISTICAL MACHINE TRANSLATION. Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks. 1 Introduction Based on estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternatively to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in (Koehn and Monz, 2005; Koehn and Monz, 2006). Given the challenge of domain adaptation, efforts have been focused on improving strategies for Ngram-based SMT which could generalize better. Specifically, a novel reordering strategy is explored. It is based on extending the search by using precomputed statistical information. Results are promising while keeping computational expenses at a similar level as monotonic search. Additionally, a bonus for tuples from the out-of-domain corpus is introduced, as well as a target language model based on statistical classes. One of the advantages of working with statistical cla</context>
</contexts>
<marker>Koehn, Monz, 2005</marker>
<rawString>Philipp Koehn and Christof Monz. 2005. Shared task: Statistical machine translation between european languages. In WMT, pages 119–124, Michigan, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
</authors>
<title>Manual and automatic evaluation of machine translation between european languages. In</title>
<date>2006</date>
<booktitle>WMT,</booktitle>
<pages>102--121</pages>
<publisher>ACL.</publisher>
<location>New York City,</location>
<contexts>
<context position="1682" citStr="Koehn and Monz, 2006" startWordPosition="236" endWordPosition="239">cedure. The paper provides details of this system participation in the ACL 2007 SECOND WORKSHOP ON STATISTICAL MACHINE TRANSLATION. Results on three pairs of languages are reported, namely from Spanish, French and German into English (and the other way round) for both the in-domain and out-of-domain tasks. 1 Introduction Based on estimating a joint-probability model between the source and the target languages, Ngram-based SMT has proved to be a very competitive alternatively to phrase-based and other state-of-the-art systems in previous evaluation campaigns, as shown in (Koehn and Monz, 2005; Koehn and Monz, 2006). Given the challenge of domain adaptation, efforts have been focused on improving strategies for Ngram-based SMT which could generalize better. Specifically, a novel reordering strategy is explored. It is based on extending the search by using precomputed statistical information. Results are promising while keeping computational expenses at a similar level as monotonic search. Additionally, a bonus for tuples from the out-of-domain corpus is introduced, as well as a target language model based on statistical classes. One of the advantages of working with statistical classes is that they can e</context>
</contexts>
<marker>Koehn, Monz, 2006</marker>
<rawString>Philipp Koehn and Christof Monz. 2006. Manual and automatic evaluation of machine translation between european languages. In WMT, pages 102–121, New York City, June. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J B Mari˜no</author>
<author>R E Banchs</author>
<author>J M Crego</author>
<author>A de Gispert</author>
<author>P Lambert</author>
<author>J A R Fonollosa</author>
<author>M R Costa-juss`a</author>
</authors>
<title>N-gram based machine translation.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>4</issue>
<marker>Mari˜no, Banchs, Crego, de Gispert, Lambert, Fonollosa, Costa-juss`a, 2006</marker>
<rawString>J.B. Mari˜no, R.E. Banchs, J.M. Crego, A. de Gispert, P. Lambert, J.A.R. Fonollosa, and M.R. Costa-juss`a. 2006. N-gram based machine translation. Computational Linguistics, 32(4):527–549, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
</authors>
<title>An efficient method for determining bilingual word classes.</title>
<date>1999</date>
<booktitle>In EACL,</booktitle>
<pages>71--76</pages>
<location>Bergen, Norway,</location>
<contexts>
<context position="5070" citStr="Och, 1999" startWordPosition="761" endWordPosition="762">iven Machine Translation and Beyond. A more detailed description of the system can be found in (Mari˜no et al., 2006). 3 Baseline System Enhanced with a Weighted Reordering Input Graph This section briefly describes the statistical machine reordering (SMR) technique. Further details on the architecture of SMR system can be found on (Costa-juss`a and Fonollosa, 2006). 3.1 Concept The SMR system can be seen as a SMT system which translates from an original source language (S) to a reordered source language (S’), given a target language (T). The SMR technique works with statistical word classes (Och, 1999) instead of words themselves (particularly, we have used 200 classes in all experiments). Figure 1: SMR approach in the (A) training step (B) in the test step (the weight of each arch is in brackets). 3.2 Using SMR technique to improve SMT training The original source corpus S is translated into the reordered source corpus S’ with the SMR system. Figure 1 (A) shows the corresponding block diagram. The reordered training source corpus and the original training target corpus are used to build the SMT system. The main difference here is that the training is computed with the S’2T task instead of </context>
<context position="7756" citStr="Och, 1999" startWordPosition="1206" endWordPosition="1207">s the weighted reordering graph. The main difference with the reordering technique for WMT06 (Crego et al., 2006) lies in (1) the tuples are extracted from the word alignment between the reordered source training corpus and the given target training corpus and (2) the graph structure: the SMR graph provides weights for each reordering path. 4 Other features and functionalities In addition to the novel reordering strategy, we consider two new features functions. 4.1 Target Language Model based on Statistical Classes This feature implements a 5-gram language model of target statistical classes (Och, 1999). This model is trained by considering statistical classes, instead of words, for 168 Figure 3: Weighted reordering input graph for SMT system. the target side of the training corpus. Accordingly, the tuple translation unit is redefined in terms of a triplet which includes: a source string containing the source side of the tuple, a target string containing the target side of the tuple, and a class string containing the statistical classes corresponding to the words in the target strings. 4.2 Bonus for out-of-domain tuples This feature adds a bonus to those tuples which comes from the training </context>
</contexts>
<marker>Och, 1999</marker>
<rawString>F.J. Och. 1999. An efficient method for determining bilingual word classes. In EACL, pages 71–76, Bergen, Norway, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>