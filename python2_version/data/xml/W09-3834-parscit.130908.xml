<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022747">
<title confidence="0.988361">
Wide-coverage parsing of speech transcripts
</title>
<author confidence="0.985757">
Jeroen Geertzen
</author>
<affiliation confidence="0.9877235">
Research Centre for English &amp; Applied Linguistics
University of Cambridge, UK
</affiliation>
<email confidence="0.988579">
jg532@cam.ac.uk
</email>
<sectionHeader confidence="0.993695" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998928">
This paper discusses the performance
difference of wide-coverage parsers on
small-domain speech transcripts. Two
parsers (C&amp;C CCG and RASP) are tested
on the speech transcripts of two different
domains (parent-child language, and pic-
ture descriptions).
The performance difference between
the domain-independent parsers and
two domain-trained parsers (MSTParser
and MEGRASP) is substantial, with a
difference of at least 30 percent point
in accuracy. Despite this gap, some of
the grammatical relations can still be
recovered reliably.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999984157894737">
Even though wide-coverage, domain-
independent1 parser systems may perform
sufficiently well for the task at hand, obtaining
highly accurate parses of sentences in a par-
ticular domain usually requires the parser to
be domain-trained. Training a parser requires
a sufficient amount of labelled data (a gold
standard), something that is only available for
very few domains. When accurate parses of
sentences in a new domain are desired, there
are several ways to proceed. Hand labelling all
data in the new domain is a consideration, but is
usually unfeasible as manual annotation is a costly
activity. Another possibility is to minimise the
amount of annotation effort required to achieve
good performance by resorting to semi-automatic
annotation or domain adaptation methods. In
any case, dedicated effort is still required to
obtain highly accurate parses, even with recent
</bodyText>
<footnote confidence="0.9633455">
1In this paper, the terms ‘wide-coverage’ and ’domain-
independent’ are used synonymously.
</footnote>
<bodyText confidence="0.999595583333333">
automated domain adaptation methods (Dredze
et al., 2007).
Work that requires parsing in a new domain as
basis of further study or as part of a larger nat-
ural language processing system usually involves
a domain-independent parser with the expectation
that parses are sufficiently accurate for the specific
purpose.2 For instance, Bos and Markert (2005)
use a wide-coverage CCG-parser (Clark and Cur-
ran, 2007) to generate semantic representations for
recognising textual entailment. Geertzen (2009)
uses a HPSG-based dependency parser (Bouma
et al., 2001) to obtain the semantic content of utter-
ances. And in the study of child language acqui-
sition, Buttery and Korhonen (2007) use RASP, a
wide-coverage dependency parser (Briscoe et al.,
2006), to look at lexical acquisition.
The goal of this paper is to give an indication
of wide-coverage, domain-independent parser per-
formance on specific domains. Additionally, the
study gives insight into RASP’s performance on
CHILDES, allowing to factor in parsing perfor-
mance in the syntax-based study of Buttery and
Korhonen (2007).
</bodyText>
<sectionHeader confidence="0.694248" genericHeader="method">
2 Parsing speech transcripts
</sectionHeader>
<bodyText confidence="0.996438">
Parsing performance of two domain-independent
parsers, C&amp;C CCG en RASP, is evaluated on two
speech domains. The first domain, CHILDES, in-
volves parent-child interactions; the second do-
main, CCC, involves a picture description task.
</bodyText>
<subsectionHeader confidence="0.998815">
2.1 Parsing systems
</subsectionHeader>
<bodyText confidence="0.876112">
Two wide-coverage parser systems are used.
RASP (Briscoe et al., 2006) is a parsing system for
</bodyText>
<footnote confidence="0.999774142857143">
2Without gold standard there is no way of knowing how
well the parser component performs with respect to a de-
sired outcome of syntactic structure. This may not neces-
sarily be a problem, as parsing in such cases is paramount,
and application-based evaluation is preferable. Moreover, it
may be that using linguistically most desired parses does not
result in best application performance.
</footnote>
<page confidence="0.931342">
218
</page>
<bodyText confidence="0.978433083333333">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 218–221,
Paris, October 2009. c�2009 Association for Computational Linguistics
English that utilises a manually-developed gram-
mar and outputs grammatical dependency rela-
tions. The C&amp;C CCG parser (Clark and Cur-
ran, 2007) is a parsing system that is based on
an automatically extracted grammar from CCG-
Bank and uses discriminative training. Both sys-
tems are able to output the exact set of dependency
relations, and in a comparison on a 560-sentence
test set used by Briscoe and Carroll (2006), Clark
and Curran (2007) report a micro-averaged F-
score of 81.14 for the CCG parser, and 76.29 for
RASP. 3 Both parsing systems utilise the Gram-
matical Relations (GR) annotation scheme pro-
posed by Carroll et al. (1998). This scheme is in-
tended to cater for parser evaluation, and extends
the dependency structure based method of eval-
uation proposed by Lin (1998). For the parent-
child interaction domain both parsing systems are
compared with two syntactic dependency parsers
that were specifically trained for CHILDES tran-
scripts: MEGRASP (Sagae et al., 2007) and MST-
parser (McDonald et al., 2005).
</bodyText>
<subsectionHeader confidence="0.999605">
2.2 Speech phenomena
</subsectionHeader>
<bodyText confidence="0.999777769230769">
As CCC and CHILDES transcripts are describ-
ing spoken language, they contain various markers
that encode speech phenomena, particularly dis-
fluencies (e.g. filled pauses, partial words, false
starts, repetitions) and speech repairs (e.g. re-
tractions and corrections). Prior to parser eval-
uation, such disfluencies have been deleted from
the transcripts, which slightly improves parser per-
formance for all systems mentioned. Similar per-
formance improvements are also reported in stud-
ies that address the effect of deletion of repairs
and fillers on parsing (e.g. Charniak and Johnson
(2001); Lease and Johnson (2006)).
</bodyText>
<subsectionHeader confidence="0.996439">
2.3 CHILDES data
</subsectionHeader>
<bodyText confidence="0.999824555555556">
The major part of the evaluation is based on
the parsing of parent-child interactions from the
CHILDES database (MacWhinney, 2000). A
large portion of CHILDES transcripts was recently
parsed with a domain-specific parser (Sagae et al.,
2007), allowing more reliable systematic studies
of syntactic development in child language acqui-
sition. Sagae et al. also released their gold stan-
dard data, allowing others to train and evaluate
</bodyText>
<footnote confidence="0.96885075">
3It should be remarked that such cross-formalism compar-
isons are difficult in nature. In this case, training data were
different (RASP is not tuned to newspaper text), and CCG
utilises a lexicalised parsing model where RASP does not.
</footnote>
<bodyText confidence="0.994800153846154">
other parser systems.
The gold standard data uses a GR scheme that
is based on that of Carroll et al. (1998) but that
differs in two respects: the scheme is extended
to suit the specific need of the child language re-
search community (cf. (Sagae et al., 2004)), and
the scheme does not extensively and explicitly use
the GR hierarchy.
To compare parsing performance, a mapping
from RASP GRs to CHILDES GRs was manu-
ally constructed, containing 75 rules that involve
the label and optional restrictions on the word or
POS-tag of the head or dependent.
</bodyText>
<sectionHeader confidence="0.994274" genericHeader="method">
3 Parser evaluation
</sectionHeader>
<subsectionHeader confidence="0.919111">
3.1 Measures
</subsectionHeader>
<bodyText confidence="0.999911833333333">
System performance is reported with accuracy
measures for labelled and unlabelled dependen-
cies resulting from 15-fold cross-validation.4 The
performance on each grammatical relation is ex-
pressed by precision, recall, and F1-score. Punc-
tuation has been excluded.
</bodyText>
<subsectionHeader confidence="0.951634">
3.2 CHILDES
</subsectionHeader>
<bodyText confidence="0.975557962962963">
The gold-standard used for evaluation is based on
15 (out of 20) files in the Eve section of the Brown
corpus. The annotations that are available were
made with the CHILDES GR scheme, for which
an inter-annotator percentage agreement of 96.5%
(N = 2) has been reported by Sagae et al. (2004).
From all manually annotated utterances initially
available, duplicates, those with less than three to-
kens (about 30% of all), and those with missing
or incomplete parses (1% of all) were removed,
resulting in a set of 14.137 sentences, comprising
93,594 tokens with 4.5 tokens per utterance on av-
erage.
The performance scores that are obtained when
the parsing systems are compared against the gold-
standard are listed in the upper part of Table 1.
As can be seen from the accuracy scores,
MEGRASP and the MSTParser perform with
more than 30 percent point accuracy considerably
better than the domain-independent parsers. How-
ever, the list of performance scores for each of
the grammatical relations in Table 2 shows that
some relations can be recovered with acceptable
4The exception being the MEGRASP, for which because
of computation problems the full gold standard was used (7%
larger than the other training sets), resulting in somewhat
higher scores than expected with cross-validation.
</bodyText>
<page confidence="0.99966">
219
</page>
<tableCaption confidence="0.999743">
Table 1: Parsing accuracy scores.
</tableCaption>
<table confidence="0.9484545">
CHILDES labelled unlabelled
RASP 60.1 69.2
CCG parser 39.1 66.5
MSTParser 93.8 95.4
MEGRASP 90.7 93.5
CCC labelled unlabelled
RASP 66.7 72.3
CCG parser 60.2 68.5
</table>
<bodyText confidence="0.801092">
Fl-scores, such as auxiliaries, determiners, sub-
jects, and objects of prepositions.5
</bodyText>
<subsectionHeader confidence="0.961998">
3.3 CCC
</subsectionHeader>
<bodyText confidence="0.999980823529412">
The Cambridge Cookie-theft Corpus (CCC, TO
APPEAR, 2010) contains audio-recorded mono-
logues of 196 subjects that were asked to fully de-
scribe a scene in a picture. As a result, the domain
is small, but at the same time, sentence bound-
aries are difficult to indicate. From this corpus
of 5,628 intonational phrases, a small evaluation
set of 80 phrases has been manually annotated6
with GRs. The performance scores for each of the
parsers is listed in the lower part of Table 1. Accu-
racy scores are higher than those for CHILDES,
and the difference in labelled accuracy between
the domain-independent parsers is less than with
CHILDES. Due to space restrictions it is not pos-
sible to present performance on individual GRs,
but the GRs that are most reliably recovered are
similar to those mentioned in Section 3.2.
</bodyText>
<sectionHeader confidence="0.996966" genericHeader="method">
4 Considerations
</sectionHeader>
<bodyText confidence="0.999975090909091">
In the work reported here, performance of domain-
independent parsers on narrow domains was cal-
culated for two domains. The availability of
more domain-specific datasets with manually su-
pervised GR annotations would allow a better gen-
eralisation of parser performance. Unfortunately,
datasets with manually verified annotations that
use the same set of syntactic dependencies are
rare.
The CHILDES figures show that the perfor-
mance difference between domain-independent
</bodyText>
<footnote confidence="0.9966106">
5MSTParser scores did not fit in the table, but largely
correspond in distributional characteristics, and are available
upon request.
6Not with multiple coders yet, but percentage agreement
for dependency annotation typically varies from 93-98%.
</footnote>
<bodyText confidence="0.999934588235294">
and domain-trained parsers is big. It should be
noted that these results are obtained from speech,
which is usually less syntactically well-formed
than written language. For the speech data anal-
ysed, RASP performs better than the CCG parser,
whereas Clark and Curran (2007) have shown that
the CCG parser outperforms RASP on written text.
To better explain this difference, it would be in-
sightful to compare the confusion matrices of GR
assignments. This would allow assessment on how
the domain-independent parser errors compare to
the domain-trained parser errors.
The mapping from RASP GRs to CHILDES
GRs that was constructed is exhaustive, but there
is still room for fine-tuning and more refined map-
pings, gaining up to about 2% accuracy by esti-
mate.
</bodyText>
<sectionHeader confidence="0.981247" genericHeader="conclusions">
5 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999984833333333">
This paper has provided performance scores of
wide-coverage parsers applied to narrow domain
spoken language transcripts to assess the perfor-
mance gap with domain-trained parsers. This gap
appears to be considerable (more than 30 percent
point for CHILDES), but a subset of GRs can still
be recovered with fair accuracy.
We have not yet dealt with comparing
domain-independent and domain-trained parser
errors, which may provide additional insight into
the strengths and weaknesses of wide-coverage
parsers for narrow use.
</bodyText>
<sectionHeader confidence="0.974596" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.996159">
This work is supported by UK EPSRC Grant
EP/F030061/1.
</bodyText>
<sectionHeader confidence="0.968803" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.987883307692308">
Bos, J. and Markert, K. (2005). Recognising tex-
tual entailment with logical inference. In Pro-
ceedings of the HLT and EMNLP conference,
pages 628–635.
Bouma, G., van Noord, G., and Malouf, R. (2001).
Alpino: Wide-coverage computational analysis
of dutch. In Proceedings of the CLIN 2000,
pages 45–59.
Briscoe, T. and Carroll, J. (2006). Evaluating the
accuracy of an unlexicalized statistical parser on
the PARC depbank. In Proceedings of the COL-
ING/ACL on Main conference poster sessions,
pages 41–48.
</bodyText>
<page confidence="0.998038">
220
</page>
<tableCaption confidence="0.8939565">
Table 2: Performance scores of the parsing systems for major GRs. Some of the relations could not be
reliably be mapped, and are absent for the CCG parser.
</tableCaption>
<table confidence="0.997229722222222">
relation Prec RASP Fi CCG parser MEGRASP
Rec Prec Rec Fi Prec Rec Fi
aux 89.13 69.87 78.33 90.81 62.21 73.84 98.13 96.21 97.16
com 67.80 6.12 11.23 - - - 93.15 88.52 90.78
comp 22.73 64.18 33.57 24.53 53.66 33.67 80.00 84.72 82.29
coord 70.42 64.31 67.23 82.50 30.62 44.66 75.07 83.93 79.26
cpzr 74.67 20.97 32.75 - - - 90.16 85.77 87.91
det 90.34 89.38 89.86 60.88 82.54 70.07 96.38 97.27 96.82
jct 57.85 56.68 57.26 54.71 5.16 9.42 85.14 83.05 84.08
mod 63.04 76.93 69.29 16.89 47.43 24.91 90.00 90.63 90.32
obj 73.34 75.50 74.40 46.09 69.25 55.34 91.93 91.10 91.52
obj2 32.81 55.13 41.13 53.37 39.16 45.18 83.33 74.14 78.47
pobj 88.11 75.51 81.33 - - - 91.94 93.05 92.49
pred 54.77 48.94 51.69 64.60 15.55 25.07 90.21 91.08 90.65
quant 55.87 68.87 61.69 - - - 83.10 91.46 87.08
subj 74.53 67.58 70.89 66.94 66.11 66.52 94.68 95.01 94.84
xcomp 52.17 64.97 57.87 1.62 3.35 2.19 92.11 87.13 89.55
xmod 12.93 15.32 14.02 2.60 24.19 4.69 56.64 65.32 60.67
</table>
<bodyText confidence="0.8363465">
Briscoe, T., Carroll, J., and Watson, R. (2006). The
second release of the RASP system. In Proceed-
ings of the COLING/ACL on Interactive presen-
tation sessions, pages 77–80.
Buttery, P. and Korhonen, A. (2007). I will
shoot your shopping down and you can shoot
all my tins—automatic lexical acquisition from
the CHILDES database. In Proceedings of the
</bodyText>
<reference confidence="0.980357681818182">
Workshop on Cognitive Aspects of Computa-
tional Language Acquisition, pages 33–40.
Carroll, J., Briscoe, T., and Sanfilippo, A. (1998).
Parser evaluation: a survey and a new proposal.
In Proceedings ofthe 1stLREC, pages 447–454.
Charniak, E. and Johnson, M. (2001). Edit de-
tection and parsing for transcribed speech. In
Proceedings ofNAACL, pages 118–126.
Clark, S. and Curran, J. R. (2007). Wide-
coverage efficient statistical parsing with CCG
and log-linear models. Computational Linguis-
tics, 33(4):493–552.
Dredze, M., Blitzer, J., Pratim Talukdar, P.,
Ganchev, K., Graca, J. a., and Pereira, F. (2007).
Frustratingly hard domain adaptation for depen-
dency parsing. In Proceedings of the CoNLL
Shared Task Session of EMNLP-CoNLL 2007,
pages 1051–1055.
Geertzen, J. (2009). Semantic interpretation of
Dutch spoken dialogue. In Proceedings of the
Eight IWCS, pages 286–290.
Lease, M. and Johnson, M. (2006). Early deletion
of fillers in processing conversational speech. In
Proceedings of the HLT-NAACL, pages 73–76.
Lin, D. (1998). A dependency-based method
for evaluating broad-coverage parsers. Natural
Language Engineering, 4(2):97–114.
MacWhinney, B. (2000). The CHILDES project:
Tools for analyzing talk. Lawrence Erlbaum As-
sociates, Mahwah, NJ, USA, third edition.
McDonald, R., Crammer, K., and Pereira, F.
(2005). Online large-margin training of depen-
dency parsers. In Proceedings of the 43rd An-
nual Meeting on ACL, pages 91–98.
Sagae, K., Davis, E., Lavie, A., MacWhinney,
B., and Wintner, S. (2007). High-accuracy an-
notation and parsing of CHILDES transcripts.
In Proceedings of the ACL-2007 workshop on
Cognitive Aspects of Computational Language
Acquisition.
Sagae, K., MacWhinney, B., and Lavie, A. (2004).
Adding syntactic annotations to transcripts of
parent-child dialogs. In In Proceedings of the
Fourth LREC, pages 1815–1818.
</reference>
<page confidence="0.998326">
221
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.938819">
<title confidence="0.998314">Wide-coverage parsing of speech transcripts</title>
<author confidence="0.985981">Jeroen</author>
<affiliation confidence="0.991483">Research Centre for English &amp; Applied University of Cambridge,</affiliation>
<email confidence="0.996227">jg532@cam.ac.uk</email>
<abstract confidence="0.9981915625">This paper discusses the performance difference of wide-coverage parsers on small-domain speech transcripts. Two parsers (C&amp;C CCG and RASP) are tested on the speech transcripts of two different domains (parent-child language, and picture descriptions). The performance difference between the domain-independent parsers and two domain-trained parsers (MSTParser and MEGRASP) is substantial, with a difference of at least 30 percent point in accuracy. Despite this gap, some of the grammatical relations can still be recovered reliably.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<booktitle>Workshop on Cognitive Aspects of Computational Language Acquisition,</booktitle>
<pages>33--40</pages>
<marker></marker>
<rawString>Workshop on Cognitive Aspects of Computational Language Acquisition, pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>T Briscoe</author>
<author>A Sanfilippo</author>
</authors>
<title>Parser evaluation: a survey and a new proposal.</title>
<date>1998</date>
<booktitle>In Proceedings ofthe 1stLREC,</booktitle>
<pages>447--454</pages>
<contexts>
<context position="4318" citStr="Carroll et al. (1998)" startWordPosition="651" endWordPosition="654">at utilises a manually-developed grammar and outputs grammatical dependency relations. The C&amp;C CCG parser (Clark and Curran, 2007) is a parsing system that is based on an automatically extracted grammar from CCGBank and uses discriminative training. Both systems are able to output the exact set of dependency relations, and in a comparison on a 560-sentence test set used by Briscoe and Carroll (2006), Clark and Curran (2007) report a micro-averaged Fscore of 81.14 for the CCG parser, and 76.29 for RASP. 3 Both parsing systems utilise the Grammatical Relations (GR) annotation scheme proposed by Carroll et al. (1998). This scheme is intended to cater for parser evaluation, and extends the dependency structure based method of evaluation proposed by Lin (1998). For the parentchild interaction domain both parsing systems are compared with two syntactic dependency parsers that were specifically trained for CHILDES transcripts: MEGRASP (Sagae et al., 2007) and MSTparser (McDonald et al., 2005). 2.2 Speech phenomena As CCC and CHILDES transcripts are describing spoken language, they contain various markers that encode speech phenomena, particularly disfluencies (e.g. filled pauses, partial words, false starts, </context>
<context position="6127" citStr="Carroll et al. (1998)" startWordPosition="933" endWordPosition="936">ILDES transcripts was recently parsed with a domain-specific parser (Sagae et al., 2007), allowing more reliable systematic studies of syntactic development in child language acquisition. Sagae et al. also released their gold standard data, allowing others to train and evaluate 3It should be remarked that such cross-formalism comparisons are difficult in nature. In this case, training data were different (RASP is not tuned to newspaper text), and CCG utilises a lexicalised parsing model where RASP does not. other parser systems. The gold standard data uses a GR scheme that is based on that of Carroll et al. (1998) but that differs in two respects: the scheme is extended to suit the specific need of the child language research community (cf. (Sagae et al., 2004)), and the scheme does not extensively and explicitly use the GR hierarchy. To compare parsing performance, a mapping from RASP GRs to CHILDES GRs was manually constructed, containing 75 rules that involve the label and optional restrictions on the word or POS-tag of the head or dependent. 3 Parser evaluation 3.1 Measures System performance is reported with accuracy measures for labelled and unlabelled dependencies resulting from 15-fold cross-va</context>
</contexts>
<marker>Carroll, Briscoe, Sanfilippo, 1998</marker>
<rawString>Carroll, J., Briscoe, T., and Sanfilippo, A. (1998). Parser evaluation: a survey and a new proposal. In Proceedings ofthe 1stLREC, pages 447–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Edit detection and parsing for transcribed speech.</title>
<date>2001</date>
<booktitle>In Proceedings ofNAACL,</booktitle>
<pages>118--126</pages>
<contexts>
<context position="5307" citStr="Charniak and Johnson (2001)" startWordPosition="801" endWordPosition="804">ald et al., 2005). 2.2 Speech phenomena As CCC and CHILDES transcripts are describing spoken language, they contain various markers that encode speech phenomena, particularly disfluencies (e.g. filled pauses, partial words, false starts, repetitions) and speech repairs (e.g. retractions and corrections). Prior to parser evaluation, such disfluencies have been deleted from the transcripts, which slightly improves parser performance for all systems mentioned. Similar performance improvements are also reported in studies that address the effect of deletion of repairs and fillers on parsing (e.g. Charniak and Johnson (2001); Lease and Johnson (2006)). 2.3 CHILDES data The major part of the evaluation is based on the parsing of parent-child interactions from the CHILDES database (MacWhinney, 2000). A large portion of CHILDES transcripts was recently parsed with a domain-specific parser (Sagae et al., 2007), allowing more reliable systematic studies of syntactic development in child language acquisition. Sagae et al. also released their gold standard data, allowing others to train and evaluate 3It should be remarked that such cross-formalism comparisons are difficult in nature. In this case, training data were dif</context>
</contexts>
<marker>Charniak, Johnson, 2001</marker>
<rawString>Charniak, E. and Johnson, M. (2001). Edit detection and parsing for transcribed speech. In Proceedings ofNAACL, pages 118–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>Widecoverage efficient statistical parsing with CCG and log-linear models.</title>
<date>2007</date>
<journal>Computational Linguistics,</journal>
<volume>33</volume>
<issue>4</issue>
<contexts>
<context position="2085" citStr="Clark and Curran, 2007" startWordPosition="303" endWordPosition="307">omain adaptation methods. In any case, dedicated effort is still required to obtain highly accurate parses, even with recent 1In this paper, the terms ‘wide-coverage’ and ’domainindependent’ are used synonymously. automated domain adaptation methods (Dredze et al., 2007). Work that requires parsing in a new domain as basis of further study or as part of a larger natural language processing system usually involves a domain-independent parser with the expectation that parses are sufficiently accurate for the specific purpose.2 For instance, Bos and Markert (2005) use a wide-coverage CCG-parser (Clark and Curran, 2007) to generate semantic representations for recognising textual entailment. Geertzen (2009) uses a HPSG-based dependency parser (Bouma et al., 2001) to obtain the semantic content of utterances. And in the study of child language acquisition, Buttery and Korhonen (2007) use RASP, a wide-coverage dependency parser (Briscoe et al., 2006), to look at lexical acquisition. The goal of this paper is to give an indication of wide-coverage, domain-independent parser performance on specific domains. Additionally, the study gives insight into RASP’s performance on CHILDES, allowing to factor in parsing pe</context>
<context position="3827" citStr="Clark and Curran, 2007" startWordPosition="565" endWordPosition="569">nent performs with respect to a desired outcome of syntactic structure. This may not necessarily be a problem, as parsing in such cases is paramount, and application-based evaluation is preferable. Moreover, it may be that using linguistically most desired parses does not result in best application performance. 218 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 218–221, Paris, October 2009. c�2009 Association for Computational Linguistics English that utilises a manually-developed grammar and outputs grammatical dependency relations. The C&amp;C CCG parser (Clark and Curran, 2007) is a parsing system that is based on an automatically extracted grammar from CCGBank and uses discriminative training. Both systems are able to output the exact set of dependency relations, and in a comparison on a 560-sentence test set used by Briscoe and Carroll (2006), Clark and Curran (2007) report a micro-averaged Fscore of 81.14 for the CCG parser, and 76.29 for RASP. 3 Both parsing systems utilise the Grammatical Relations (GR) annotation scheme proposed by Carroll et al. (1998). This scheme is intended to cater for parser evaluation, and extends the dependency structure based method o</context>
<context position="10271" citStr="Clark and Curran (2007)" startWordPosition="1590" endWordPosition="1593">endencies are rare. The CHILDES figures show that the performance difference between domain-independent 5MSTParser scores did not fit in the table, but largely correspond in distributional characteristics, and are available upon request. 6Not with multiple coders yet, but percentage agreement for dependency annotation typically varies from 93-98%. and domain-trained parsers is big. It should be noted that these results are obtained from speech, which is usually less syntactically well-formed than written language. For the speech data analysed, RASP performs better than the CCG parser, whereas Clark and Curran (2007) have shown that the CCG parser outperforms RASP on written text. To better explain this difference, it would be insightful to compare the confusion matrices of GR assignments. This would allow assessment on how the domain-independent parser errors compare to the domain-trained parser errors. The mapping from RASP GRs to CHILDES GRs that was constructed is exhaustive, but there is still room for fine-tuning and more refined mappings, gaining up to about 2% accuracy by estimate. 5 Conclusions and future work This paper has provided performance scores of wide-coverage parsers applied to narrow d</context>
</contexts>
<marker>Clark, Curran, 2007</marker>
<rawString>Clark, S. and Curran, J. R. (2007). Widecoverage efficient statistical parsing with CCG and log-linear models. Computational Linguistics, 33(4):493–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dredze</author>
<author>J Blitzer</author>
<author>Pratim Talukdar</author>
<author>P Ganchev</author>
<author>K Graca</author>
<author>J a</author>
<author>F Pereira</author>
</authors>
<title>Frustratingly hard domain adaptation for dependency parsing.</title>
<date>2007</date>
<booktitle>In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL</booktitle>
<pages>1051--1055</pages>
<contexts>
<context position="1733" citStr="Dredze et al., 2007" startWordPosition="248" endWordPosition="251">ces in a new domain are desired, there are several ways to proceed. Hand labelling all data in the new domain is a consideration, but is usually unfeasible as manual annotation is a costly activity. Another possibility is to minimise the amount of annotation effort required to achieve good performance by resorting to semi-automatic annotation or domain adaptation methods. In any case, dedicated effort is still required to obtain highly accurate parses, even with recent 1In this paper, the terms ‘wide-coverage’ and ’domainindependent’ are used synonymously. automated domain adaptation methods (Dredze et al., 2007). Work that requires parsing in a new domain as basis of further study or as part of a larger natural language processing system usually involves a domain-independent parser with the expectation that parses are sufficiently accurate for the specific purpose.2 For instance, Bos and Markert (2005) use a wide-coverage CCG-parser (Clark and Curran, 2007) to generate semantic representations for recognising textual entailment. Geertzen (2009) uses a HPSG-based dependency parser (Bouma et al., 2001) to obtain the semantic content of utterances. And in the study of child language acquisition, Buttery</context>
</contexts>
<marker>Dredze, Blitzer, Talukdar, Ganchev, Graca, a, Pereira, 2007</marker>
<rawString>Dredze, M., Blitzer, J., Pratim Talukdar, P., Ganchev, K., Graca, J. a., and Pereira, F. (2007). Frustratingly hard domain adaptation for dependency parsing. In Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pages 1051–1055.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Geertzen</author>
</authors>
<title>Semantic interpretation of Dutch spoken dialogue.</title>
<date>2009</date>
<booktitle>In Proceedings of the Eight IWCS,</booktitle>
<pages>286--290</pages>
<contexts>
<context position="2174" citStr="Geertzen (2009)" startWordPosition="316" endWordPosition="317">te parses, even with recent 1In this paper, the terms ‘wide-coverage’ and ’domainindependent’ are used synonymously. automated domain adaptation methods (Dredze et al., 2007). Work that requires parsing in a new domain as basis of further study or as part of a larger natural language processing system usually involves a domain-independent parser with the expectation that parses are sufficiently accurate for the specific purpose.2 For instance, Bos and Markert (2005) use a wide-coverage CCG-parser (Clark and Curran, 2007) to generate semantic representations for recognising textual entailment. Geertzen (2009) uses a HPSG-based dependency parser (Bouma et al., 2001) to obtain the semantic content of utterances. And in the study of child language acquisition, Buttery and Korhonen (2007) use RASP, a wide-coverage dependency parser (Briscoe et al., 2006), to look at lexical acquisition. The goal of this paper is to give an indication of wide-coverage, domain-independent parser performance on specific domains. Additionally, the study gives insight into RASP’s performance on CHILDES, allowing to factor in parsing performance in the syntax-based study of Buttery and Korhonen (2007). 2 Parsing speech tran</context>
</contexts>
<marker>Geertzen, 2009</marker>
<rawString>Geertzen, J. (2009). Semantic interpretation of Dutch spoken dialogue. In Proceedings of the Eight IWCS, pages 286–290.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lease</author>
<author>M Johnson</author>
</authors>
<title>Early deletion of fillers in processing conversational speech.</title>
<date>2006</date>
<booktitle>In Proceedings of the HLT-NAACL,</booktitle>
<pages>73--76</pages>
<contexts>
<context position="5333" citStr="Lease and Johnson (2006)" startWordPosition="805" endWordPosition="808"> phenomena As CCC and CHILDES transcripts are describing spoken language, they contain various markers that encode speech phenomena, particularly disfluencies (e.g. filled pauses, partial words, false starts, repetitions) and speech repairs (e.g. retractions and corrections). Prior to parser evaluation, such disfluencies have been deleted from the transcripts, which slightly improves parser performance for all systems mentioned. Similar performance improvements are also reported in studies that address the effect of deletion of repairs and fillers on parsing (e.g. Charniak and Johnson (2001); Lease and Johnson (2006)). 2.3 CHILDES data The major part of the evaluation is based on the parsing of parent-child interactions from the CHILDES database (MacWhinney, 2000). A large portion of CHILDES transcripts was recently parsed with a domain-specific parser (Sagae et al., 2007), allowing more reliable systematic studies of syntactic development in child language acquisition. Sagae et al. also released their gold standard data, allowing others to train and evaluate 3It should be remarked that such cross-formalism comparisons are difficult in nature. In this case, training data were different (RASP is not tuned </context>
</contexts>
<marker>Lease, Johnson, 2006</marker>
<rawString>Lease, M. and Johnson, M. (2006). Early deletion of fillers in processing conversational speech. In Proceedings of the HLT-NAACL, pages 73–76.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lin</author>
</authors>
<title>A dependency-based method for evaluating broad-coverage parsers.</title>
<date>1998</date>
<journal>Natural Language Engineering,</journal>
<volume>4</volume>
<issue>2</issue>
<contexts>
<context position="4462" citStr="Lin (1998)" startWordPosition="677" endWordPosition="678"> is based on an automatically extracted grammar from CCGBank and uses discriminative training. Both systems are able to output the exact set of dependency relations, and in a comparison on a 560-sentence test set used by Briscoe and Carroll (2006), Clark and Curran (2007) report a micro-averaged Fscore of 81.14 for the CCG parser, and 76.29 for RASP. 3 Both parsing systems utilise the Grammatical Relations (GR) annotation scheme proposed by Carroll et al. (1998). This scheme is intended to cater for parser evaluation, and extends the dependency structure based method of evaluation proposed by Lin (1998). For the parentchild interaction domain both parsing systems are compared with two syntactic dependency parsers that were specifically trained for CHILDES transcripts: MEGRASP (Sagae et al., 2007) and MSTparser (McDonald et al., 2005). 2.2 Speech phenomena As CCC and CHILDES transcripts are describing spoken language, they contain various markers that encode speech phenomena, particularly disfluencies (e.g. filled pauses, partial words, false starts, repetitions) and speech repairs (e.g. retractions and corrections). Prior to parser evaluation, such disfluencies have been deleted from the tra</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Lin, D. (1998). A dependency-based method for evaluating broad-coverage parsers. Natural Language Engineering, 4(2):97–114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B MacWhinney</author>
</authors>
<title>The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum Associates,</title>
<date>2000</date>
<location>Mahwah, NJ, USA,</location>
<note>third edition.</note>
<contexts>
<context position="5483" citStr="MacWhinney, 2000" startWordPosition="830" endWordPosition="831">s (e.g. filled pauses, partial words, false starts, repetitions) and speech repairs (e.g. retractions and corrections). Prior to parser evaluation, such disfluencies have been deleted from the transcripts, which slightly improves parser performance for all systems mentioned. Similar performance improvements are also reported in studies that address the effect of deletion of repairs and fillers on parsing (e.g. Charniak and Johnson (2001); Lease and Johnson (2006)). 2.3 CHILDES data The major part of the evaluation is based on the parsing of parent-child interactions from the CHILDES database (MacWhinney, 2000). A large portion of CHILDES transcripts was recently parsed with a domain-specific parser (Sagae et al., 2007), allowing more reliable systematic studies of syntactic development in child language acquisition. Sagae et al. also released their gold standard data, allowing others to train and evaluate 3It should be remarked that such cross-formalism comparisons are difficult in nature. In this case, training data were different (RASP is not tuned to newspaper text), and CCG utilises a lexicalised parsing model where RASP does not. other parser systems. The gold standard data uses a GR scheme th</context>
</contexts>
<marker>MacWhinney, 2000</marker>
<rawString>MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk. Lawrence Erlbaum Associates, Mahwah, NJ, USA, third edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on ACL,</booktitle>
<pages>91--98</pages>
<contexts>
<context position="4697" citStr="McDonald et al., 2005" startWordPosition="711" endWordPosition="714">iscoe and Carroll (2006), Clark and Curran (2007) report a micro-averaged Fscore of 81.14 for the CCG parser, and 76.29 for RASP. 3 Both parsing systems utilise the Grammatical Relations (GR) annotation scheme proposed by Carroll et al. (1998). This scheme is intended to cater for parser evaluation, and extends the dependency structure based method of evaluation proposed by Lin (1998). For the parentchild interaction domain both parsing systems are compared with two syntactic dependency parsers that were specifically trained for CHILDES transcripts: MEGRASP (Sagae et al., 2007) and MSTparser (McDonald et al., 2005). 2.2 Speech phenomena As CCC and CHILDES transcripts are describing spoken language, they contain various markers that encode speech phenomena, particularly disfluencies (e.g. filled pauses, partial words, false starts, repetitions) and speech repairs (e.g. retractions and corrections). Prior to parser evaluation, such disfluencies have been deleted from the transcripts, which slightly improves parser performance for all systems mentioned. Similar performance improvements are also reported in studies that address the effect of deletion of repairs and fillers on parsing (e.g. Charniak and John</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, R., Crammer, K., and Pereira, F. (2005). Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on ACL, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>E Davis</author>
<author>A Lavie</author>
<author>B MacWhinney</author>
<author>S Wintner</author>
</authors>
<title>High-accuracy annotation and parsing of CHILDES transcripts.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL-2007 workshop on Cognitive Aspects of Computational Language Acquisition.</booktitle>
<contexts>
<context position="4659" citStr="Sagae et al., 2007" startWordPosition="704" endWordPosition="707"> a 560-sentence test set used by Briscoe and Carroll (2006), Clark and Curran (2007) report a micro-averaged Fscore of 81.14 for the CCG parser, and 76.29 for RASP. 3 Both parsing systems utilise the Grammatical Relations (GR) annotation scheme proposed by Carroll et al. (1998). This scheme is intended to cater for parser evaluation, and extends the dependency structure based method of evaluation proposed by Lin (1998). For the parentchild interaction domain both parsing systems are compared with two syntactic dependency parsers that were specifically trained for CHILDES transcripts: MEGRASP (Sagae et al., 2007) and MSTparser (McDonald et al., 2005). 2.2 Speech phenomena As CCC and CHILDES transcripts are describing spoken language, they contain various markers that encode speech phenomena, particularly disfluencies (e.g. filled pauses, partial words, false starts, repetitions) and speech repairs (e.g. retractions and corrections). Prior to parser evaluation, such disfluencies have been deleted from the transcripts, which slightly improves parser performance for all systems mentioned. Similar performance improvements are also reported in studies that address the effect of deletion of repairs and fill</context>
</contexts>
<marker>Sagae, Davis, Lavie, MacWhinney, Wintner, 2007</marker>
<rawString>Sagae, K., Davis, E., Lavie, A., MacWhinney, B., and Wintner, S. (2007). High-accuracy annotation and parsing of CHILDES transcripts. In Proceedings of the ACL-2007 workshop on Cognitive Aspects of Computational Language Acquisition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sagae</author>
<author>B MacWhinney</author>
<author>A Lavie</author>
</authors>
<title>Adding syntactic annotations to transcripts of parent-child dialogs. In</title>
<date>2004</date>
<booktitle>In Proceedings of the Fourth LREC,</booktitle>
<pages>1815--1818</pages>
<contexts>
<context position="6277" citStr="Sagae et al., 2004" startWordPosition="960" endWordPosition="963">ment in child language acquisition. Sagae et al. also released their gold standard data, allowing others to train and evaluate 3It should be remarked that such cross-formalism comparisons are difficult in nature. In this case, training data were different (RASP is not tuned to newspaper text), and CCG utilises a lexicalised parsing model where RASP does not. other parser systems. The gold standard data uses a GR scheme that is based on that of Carroll et al. (1998) but that differs in two respects: the scheme is extended to suit the specific need of the child language research community (cf. (Sagae et al., 2004)), and the scheme does not extensively and explicitly use the GR hierarchy. To compare parsing performance, a mapping from RASP GRs to CHILDES GRs was manually constructed, containing 75 rules that involve the label and optional restrictions on the word or POS-tag of the head or dependent. 3 Parser evaluation 3.1 Measures System performance is reported with accuracy measures for labelled and unlabelled dependencies resulting from 15-fold cross-validation.4 The performance on each grammatical relation is expressed by precision, recall, and F1-score. Punctuation has been excluded. 3.2 CHILDES Th</context>
</contexts>
<marker>Sagae, MacWhinney, Lavie, 2004</marker>
<rawString>Sagae, K., MacWhinney, B., and Lavie, A. (2004). Adding syntactic annotations to transcripts of parent-child dialogs. In In Proceedings of the Fourth LREC, pages 1815–1818.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>