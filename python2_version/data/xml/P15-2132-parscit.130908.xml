<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000019">
<title confidence="0.991421">
Compact Lexicon Selection with Spectral Methods
</title>
<author confidence="0.997086">
Young-Bum Kim† Karl Stratos‡ Xiaohu Liu† Ruhi Sarikaya†
</author>
<affiliation confidence="0.9730245">
†Microsoft Corporation, Redmond, WA
‡Columbia University, New York, NY
</affiliation>
<email confidence="0.963966">
{ybkim, derekliu, ruhi.sarikaya}@microsoft.com
stratos@cs.columbia.edu
</email>
<sectionHeader confidence="0.993819" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999962111111111">
In this paper, we introduce the task of se-
lecting compact lexicon from large, noisy
gazetteers. This scenario arises often in
practice, in particular spoken language un-
derstanding (SLU). We propose a simple
and effective solution based on matrix de-
composition techniques: canonical corre-
lation analysis (CCA) and rank-revealing
QR (RRQR) factorization. CCA is first
used to derive low-dimensional gazetteer
embeddings from domain-specific search
logs. Then RRQR is used to find a sub-
set of these embeddings whose span ap-
proximates the entire lexicon space. Ex-
periments on slot tagging show that our
method yields a small set of lexicon en-
tities with average relative error reduction
of &gt; 50% over randomly selected lexicon.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99992555319149">
Discriminative models trained with large quanti-
ties of arbitrary features are a dominant paradigm
in spoken language understanding (SLU) (Li et
al., 2009; Hillard et al., 2011; Celikyilmaz et al.,
2013; Liu and Sarikaya, 2014; Sarikaya et al.,
2014; Anastasakos et al., 2014; Xu and Sarikaya,
2014; Celikyilmaz et al., 2015; Kim et al., 2015a;
Kim et al., 2015c; Kim et al., 2015b). An impor-
tant category of these features comes from entity
dictionaries or gazetteers—lists of phrases whose
labels are given. For instance, they can be lists
of movies, music titles, actors, restaurants, and
cities. These features enable SLU models to ro-
bustly handle unseen entities at test time.
However, these lists are often massive and very
noisy. This is because they are typically obtained
automatically by mining the web for recent en-
tries (such as newly launched movie names). Ide-
ally, we would like an SLU model to have access
to this vast source of information at deployment.
But this is difficult in practice because an SLU
model needs to be light-weight to support fast user
interaction. It becomes more challenging when
we consider multiple domains, languages, and lo-
cales.
In this paper, we introduce the task of selecting
a small, representative subset of noisy gazetteers
that will nevertheless improve model performance
nearly as much as the original lexicon. This will
allow an SLU model to take full advantage of
gazetteer resources at test time without being over-
whelmed by their scale.
Our selection method is two steps. First, we
gather relevant information for each gazetteer ele-
ment using domain-specific search logs. Then we
perform CCA using this information to derive low-
dimensional gazetteer embeddings (Hotelling,
1936). Second, we use a subset selection method
based on RRQR to locate gazetteer embeddings
whose span approximates the the entire lexicon
space (Boutsidis et al., 2009; Kim and Snyder,
2013). We show in slot tagging experiments that
the gazetteer elements selected by our method not
only preserve the performance of using full lexi-
con but even improve it in some cases. Compared
to random selection, our method achieves average
relative error reduction of &gt; 50%.
</bodyText>
<sectionHeader confidence="0.975918" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999738727272727">
We motivate our task by describing the process
of lexicon construction. Entity dictionaries are
usually automatically mined from the web us-
ing resources that provide typed entities. On
a regular basis, these dictionaries are automati-
cally updated and accumulated based on local data
feeds and knowledge graphs. Local data feeds
are generated from various origins (e.g., yellow
pages, Yelp). Knowledge graphs such as www.
freebase.com are resources that define a se-
mantic space of entities (e.g., movie names, per-
</bodyText>
<page confidence="0.937132">
806
</page>
<bodyText confidence="0.927932722222222">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 806–811,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
sons, places and organizations) and their relations.
Because of the need to keep dictionaries up-
dated to handle newly emerging entities, lexicon
construction is designed to aim for high recall at
the expense of precision. Consequently, the result-
ing gazetteers are noisy. For example, a movie dic-
tionary may contain hundreds of thousands movie
names, but many of them are false positives.
While this large base of entities is useful as a
whole, it is challenging to take advantage of at test
time. This is because we normally cannot afford
to consume so much memory when we deploy an
SLU model in practice. In the next section, we
will describe a way to filter these entities while
retaining their overall benefit.
</bodyText>
<sectionHeader confidence="0.998698" genericHeader="method">
3 Method
</sectionHeader>
<subsectionHeader confidence="0.999952">
3.1 Row subset selection problem
</subsectionHeader>
<bodyText confidence="0.999479875">
We frame gazetteer element selection as the row
subset selection problem. In this framework, we
organize n gazetteer elements as matrix A E Rnxd
whose rows Ai E Rd are some representations
of the gazetteer members. Given m &lt; n, let
S(A, m) := 1B E Rmxd : Bi = Aπ(i)} be a set
of matrices whose rows are a subset of the rows
of A. Note that IS(A, m)I = (n ). Our goal is to
</bodyText>
<equation confidence="0.9956406">
m
select 1
B* = arg min
BES(A,m) ��
��A − AB+BIIF
</equation>
<bodyText confidence="0.99845">
That is, we want B to satisfy range(BT) �
range(AT). We can solve for B* exactly with
exhaustive search in O(nm), but this brute-force
approach is clearly not scalable. Instead, we turn
to the O(nd2) algorithm of Boutsidis et al. (2009)
which we review below.
</bodyText>
<subsectionHeader confidence="0.761688">
3.1.1 RRQR factorization
</subsectionHeader>
<bodyText confidence="0.9988172">
A key ingredient in the algorithm of Boutsidis et
al. (2009) is the use of RRQR factorization. Recall
that a (thin) QR factorization of A expresses A =
QR where Q E Rnxd has orthonormal columns
and R E Rdxd is an upper triangular matrix. A
limitation of QR factorization is that it does not
assign a score to each of the d components. This is
in contrast to singular value decomposition (SVD)
which assigns a score (singular value) indicating
the importance of these components.
</bodyText>
<footnote confidence="0.90883">
1The Frobenius norm ||M||F is defined as the entry-wise
�EL2 norm: i,j m2ij. B+ is the Moore-Penrose pseudo-
inverse of B
</footnote>
<bodyText confidence="0.945950666666667">
Input: d-dimensional gazetteer representations A E Rnxd,
number of gazetteer elements to select m &lt; n
Output: m rows of A, call B E Rmxd, such that
</bodyText>
<equation confidence="0.779191">
����
��A − AB+B ��F is small
</equation>
<listItem confidence="0.997525">
• Perform SVD on A and let U E Rdxm be a ma-
trix whose columns are the left singular vectors cor-
responding to the largest m singular values.
• Associate a probability pi with the i-th row of A as
follows:
</listItem>
<equation confidence="0.7610425">
2
pi := min � 1, Lm log ml  ||UiI  ||1
</equation>
<listItem confidence="0.996445571428571">
• Discard the i-th row of A with probability 1 − pi.
If kept, the row is multiplied by 1/,,Ipi. Let these
O(m log m) rows form the columns of a new matrix
A¯ E RdxO(m log m).
• Perform RRQR on A¯ to obtain ¯All = QR.
• Return the m rows of the original A corresponding to
the top m columns of ¯All.
</listItem>
<figureCaption confidence="0.7551205">
Figure 1: Gazetteer selection based on the algo-
rithm of Boutsidis et al. (2009).
</figureCaption>
<bodyText confidence="0.9432632">
RRQR factorization is a less well-known vari-
ant of QR that addresses this limitation. Let
σi(M) denote the i-th largest singular value of
matrix M. Given A, RRQR jointly finds a
permutation matrix ll E 10,1}dxd, orthonor-
</bodyText>
<equation confidence="0.8474636">
mal Q E Rnxd, and upper triangular R =
[R11 R12; 0 R22] E Rdxd such that
fR11 R121
All = Q
R22
</equation>
<bodyText confidence="0.79963925">
satisfying
=
and
for k = 1... d. Because of this rank-
ing property, RRQR
the numerical rank
of A. Furthermore, the columns of All are sorted
in the order of decreasing importance.
</bodyText>
<subsubsectionHeader confidence="0.839534">
3.1.2 Gazetteer selection algorithm
</subsubsectionHeader>
<bodyText confidence="0.90143975">
The algorithm is atwo-stage procedure. In the first
step, we randomly sample
log m) rows of A
with carefully chosen probabilities an
</bodyText>
<equation confidence="0.938386">
σk(R11)
O(σk(A))
σ1(R22) =Ω(σk+1(A))
“reveals”
O(m
</equation>
<bodyText confidence="0.845666">
d scale them
</bodyText>
<listItem confidence="0.64996">
x A¯ E RdxO(m log m).
to form columns of matri
</listItem>
<figureCaption confidence="0.6458085">
In the second step, we perform RRQR factoriza-
tion on A¯ and collect the gazetteer elements cor-
responding to the top components given by the
RRQR permutation. The algorithm is shown in
Figure 1. The first stage involves random sam-
pling and scaling of rows, but it is shown that A¯
</figureCaption>
<page confidence="0.978681">
807
</page>
<bodyText confidence="0.972713428571429">
has O(m log m) columns with constant probabil-
ity.
This algorithm has the following optimality
guarantee:
Theorem 3.1 (Boutsidis et al. (2009)). Let Bˆ E
Rmxd be the matrix returned by the algorithm in
Figure 1. Then with probability at least 0.7,
</bodyText>
<equation confidence="0.736547">
i A − A ˆB+ Bˆ � �F c O(m log m)x
�����
</equation>
<bodyText confidence="0.95060775">
A − A˜ L
In other words, the selected rows are not arbi-
trarily worse than the best rank-m approximation
of A (given by SVD) with high probability.
</bodyText>
<subsectionHeader confidence="0.998321">
3.2 Gazetteer embeddings via CCA
</subsectionHeader>
<bodyText confidence="0.9999015">
In order to perform the selection algorithm in Fig-
ure 1, we need a d-dimensional representation for
each of n gazetteer elements. We use CCA for its
simplicity and generality.
</bodyText>
<subsectionHeader confidence="0.599005">
3.2.1 Canonical Correlation Analysis (CCA)
</subsectionHeader>
<bodyText confidence="0.999694111111111">
CCA is a general statistical technique that char-
acterizes the linear relationship between a pair of
multi-dimensional variables. CCA seeks to find k
dimensions (k is a parameter to be specified) in
which these variables are maximally correlated.
Let x1 ... xn E Rd and y1 ... yn E Rd0 be n
samples of the two variables. For simplicity, as-
sume that these variables have zero mean. Then
CCA computes the following for i = 1... k:
</bodyText>
<table confidence="0.962642333333333">
arg max �n l=1(u� i xl)(v�i yl)
uiERd, viERd0:
u&gt;i ui0=0 ∀i0&lt;i
v&gt;i vi0=0 ∀i0&lt;i
��n ��n
l=1(u� i xl)2 l=1(v�i yl)2
</table>
<bodyText confidence="0.999415384615385">
In other words, each (ui, vi) is a pair of projec-
tion vectors such that the correlation between the
projected variables uz xl and vz yl is maximized,
under the constraint that this projection is uncor-
related with the previous i − 1 projections.
This is a non-convex problem due to the inter-
action between ui and vi. However, a method
based on singular value decomposition (SVD) pro-
vides an efficient and exact solution to this prob-
lem (Hotelling, 1936). The resulting solution
u1 ... uk E Rd and v1 ... vk E Rd0 can be used
to project the variables from the original d- and
d&apos;-dimensional spaces to a k-dimensional space:
</bodyText>
<equation confidence="0.9668025">
x E Rd −� x¯E Rk : ¯xi = u� i x
y E Rd0 −� y¯E Rk : ¯yi = vZ y
</equation>
<bodyText confidence="0.999345">
The new k-dimensional representation of each
variable now contains information about the other
variable. The value of k is usually selected to be
much smaller than d or d&apos;, so the representation is
typically also low-dimensional.
</bodyText>
<subsectionHeader confidence="0.918534">
3.2.2 Inducing gazetteer embeddings
</subsectionHeader>
<bodyText confidence="0.999983272727273">
We now describe how to use CCA to induce vec-
tor representations for gazetteer elements. Using
the same notation, let n be the number of elements
in the entire gazetteers. Let x1 ... xn be the orig-
inal representations of the element samples and
y1 ... yn be the original representations of the as-
sociated features in the element.
We employ the following definition for the orig-
inal representations. Let d be the number of dis-
tinct element types and d&apos; be the number of distinct
feature types.
</bodyText>
<listItem confidence="0.848720166666667">
• xl E Rd is a zero vector in which the entry
corresponding to the element type of the l-th
instance is set to 1.
• yl E Rd0 is a zero vector in which the en-
tries corresponding to features generated by
the element are set to 1.
</listItem>
<bodyText confidence="0.9998036">
In our case, we want to induce gazetteer (ele-
ment) embeddings that correlate with the relevant
features about gazetteers. For this purpose, we use
three types of features: context features, search
click log features, and knowledge graph features.
Context features: For each gazetteer element g
of domain l, we take sentences from search logs
on domain l containing g and extract five words
each to the left and the right of the element g in
the sentences. For instance, if g = “The Matrix”
is a gazetteer element of domain l = “Movie”,
we collect sentences from movie-specific search
logs involving the phrase “The Matrix”. Such
domain-specific search logs are collected using a
pre-trained domain classifier.
Search click log features: Large-scale search
engines such as Bing and Google process mil-
lions of queries on a daily basis. Together with
the search queries, user clicked URLs are also
logged anonymously. These click logs have been
</bodyText>
<figure confidence="0.915504">
min
˜AERn×d:
rank(˜A)=m
���
</figure>
<page confidence="0.991988">
808
</page>
<bodyText confidence="0.9976606">
used for extracting semantic information for var-
ious NLP tasks (Kim et al., 2015a; Tseng et al.,
2009; Hakkani-T¨ur et al., 2011). We used the
clicked URLs as features to determine the likeli-
hood of an entity being a member of a dictionary.
These features are useful because common URLs
are shared across different names such as movie,
business and music. Table 1 shows the top five
most frequently clicked URLs for movies “Furi-
ous 7” and “The age of adaline”.
</bodyText>
<tableCaption confidence="0.504538">
Furious 7 The age of adaline
imdb.com imdb.com
en.wikipedia.org en.wikipedia.org
furious7.com youtube.com
rottentomatoes.com rottentomatoes.com
www.msn.com movieinsider.com
Table 1: Top clicked URLs of two movies.
</tableCaption>
<bodyText confidence="0.99984064516129">
One issue with using only click logs is that some
entities may not be covered in the query logs since
logs are extracted from a limited time frame (e.g.
six months). Even the big search engines employ
a moving time window for processing and stor-
ing search logs. Consequently, click logs are not
necessarily good evidence. For example, “apollo
thirteen” is a movie name appearing in the movie
training data, but it does not appear in search logs.
One way to solve the issue of missing logs for en-
tities is to search bing.com at real time. Given
that the search engine is updated on a daily ba-
sis, real-time search can make sure we capture the
newest entities. We run live search for all entities
no matter if they appear in search logs or not. Each
URL returned from the live search is considered to
have an additional click.
Knowledge graph features: The graph in
www.freebase.com contains a large set of tu-
ples in a resource description framework (RDF)
defined by W3C. A tuple typically consists of two
entities: a subject and an object linked by some
relation.
An interesting part of this resource is the entity
type defined in the graph for each entity. In the
knowledge graph, the “type” relation represents
the entity type. Table 2 shows some examples of
entities and their relations in the knowledge graph.
From the graph, we learn that “Romeo &amp; Juliet”
could be a film name or a music album since it has
two types: “film.film” and “music.album”.
</bodyText>
<table confidence="0.997002833333333">
Subject Relation Object
Jason Statham type film.actor
Jason Statham type tv.actor
Jason Statham type film.producer
Romeo &amp; Juliet type film.film
Romeo &amp; Juliet type music.album
</table>
<tableCaption confidence="0.99898">
Table 2: Entities &amp; relation in the knowledge graph.
</tableCaption>
<sectionHeader confidence="0.997586" genericHeader="evaluation">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999980677419355">
To test the effectiveness of the proposed gazetteer
selection method, we conduct slot tagging experi-
ments across a test suite of three domains: Movies,
Music and Places, which are very sensitive do-
mains to gazetteer features. The task of slot tag-
ging is to find the correct sequence of tags of
words given a user utterance. For example, in
Places domain, a user could say “search for home
depot in kingsport” and the phrase “home depot”
and “kingsport” are tagged with Place Name
and Location respectively. The data statistics
are shown in Table 3. One domain can have var-
ious kinds of gazetteers. For example, Places do-
main has business name, restaurant name, school
name and etc. Candidate dictionaries are mined
from the web and search logs automatically using
basic pattern matching approaches (e.g. entities
sharing the same or similar context in queries or
documents) and consequently contain significant
amount of noise. As the table indicates, the num-
ber of elements in total across all the gazetteers
(#total gazet elements) in each domain are too
large for models to consume.
In all our experiments, we trained conditional
random fields (CRFs) (Lafferty et al., 2001) with
the following features: (1) n-gram features up to
n = 3, (2) regular expression features, and (3)
Brown clusters (Brown et al., 1992) induced from
search logs. With these features, we compare the
following methods to demonstrate the importance
of adding appropriate gazetteers:
</bodyText>
<listItem confidence="0.999981571428571">
• NoG: train without gazetteer features.
• AllG: train with all gazetteers.
• RandG: train with randomly selected
gazetteers.
• RRQRG: train with gazetteers selected from
RRQR.
• RankAllG: train with all ranked gazetteers.
</listItem>
<page confidence="0.992809">
809
</page>
<table confidence="0.9985925">
Domains #labels #kinds of gazets #total gazet elements #training queries #test queries
Movies 25 21 14,188,527 43,784 12,179
Music 7 13 62,231,869 31,853 8,615
Places 32 31 34,227,612 22,345 6,143
</table>
<tableCaption confidence="0.999388">
Table 3: Data statistics
</tableCaption>
<bodyText confidence="0.999744111111111">
Here gazetteer features are activated when a
phrase contains an entity in a dictionary. For
RandG, we first sample a category of gazetteers
uniformly and then choose a lexicon from
gazetteers in that category. The results when we
use selected gazetteer randomly in whole cate-
gories are very low and did not include them here.
For selecting gazetteer methods (NoG, RnadG and
RRQRG), we select 500,000 elements in total.
</bodyText>
<table confidence="0.9991452">
Places Music Movies AVG.
NoG 89.10 81.53 84.78 85.14
AllG 92.11 84.24 88.56 88.30
RRQRG 91.80 83.83 87.41 87.68
RandG 86.20 76.53 77.23 79.99
</table>
<tableCaption confidence="0.996278">
Table 4: Comparison of models evaluated on three do-
mains. The numbers are F1-scores.
</tableCaption>
<subsectionHeader confidence="0.907684">
4.1 Results across Domains
</subsectionHeader>
<bodyText confidence="0.999988769230769">
First, we evaluate all models across three do-
mains. Note that the both training and test data
are collected from the United States. The results
are shown in Table 4. Not surprisingly, using
all gazetteer features (AllG) boosts the F1 score
from 85.14 % to 88.30%, confirming the power
of gazetteer features. However, with a random
selection of gazetteers, the model does not per-
form well, only achieving 79.99% F1-score. In-
terestingly, we see that across all domains our
method (RRQRG) fares better than both RandG
and NoG, almost reaching the AllG performance
with gazetteer size dramatically reduced.
</bodyText>
<subsectionHeader confidence="0.772376">
4.2 Results across Locales
</subsectionHeader>
<bodyText confidence="0.9999274">
In the next experiments, we run experiments
across three different locales in Places domain:
United Kingdom (GB), Australia (AU), and In-
dia (IN). The Places is a very sensitive domain to
locales2. For example, restaurant names in India
are very different from Australia. Here we assume
that unlike the previous experiments, the training
data is collected from the United States and test
data is collected from different locales. We used
same training data in the previous experiments and
</bodyText>
<footnote confidence="0.7933155">
2Since it is very difficult to create all locale specific train-
ing data, gazetteer features are very crucial.
</footnote>
<bodyText confidence="0.999616625">
the size of test data is about 5k for each locale.
The results are shown in Table 5. Interestingly, the
RRQR even outperforms the AllG. This is because
some noisy entities are filtered.
Finally, we show that the proposed method is
useful even in all gazetteer scenario (AllG). Us-
ing RRQR, we can order entities according to
their importance and transform a gazetteer fea-
ture into a few ones by binning the entities with
their rankings. For example, instead of having
one single big business names gazetteer, we can
divide them into lexicon with first 1000 entities,
10000 entities and so on. Results using ranked
gazetteers are shown in Table 6. We see that the
Ranked gazetteers approach (RankAllG) has con-
sistent gains across domains over AllG.
</bodyText>
<table confidence="0.9970066">
GB AU IN
NoG 87.70 82.20 80.30
AllG 90.12 86.98 89.77
RRQRG 90.18 87.48 90.28
RandG 86.20 65.34 64.20
</table>
<tableCaption confidence="0.99892">
Table 5: Comparison of models across different locales.
</tableCaption>
<table confidence="0.999472666666667">
Places Music Movies AVG.
AllG 92.11 84.24 88.56 88.30
RankAllG 92.78 86.30 89.1 89.40
</table>
<tableCaption confidence="0.950927333333333">
Table 6: Comparison of models with or without ranked
gazetteers. These are evaluated on three domains collected
in the United States.
</tableCaption>
<sectionHeader confidence="0.997898" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.9998142">
We proposed the task of selecting compact lexi-
cons from large and noisy gazetteers. This sce-
nario arises often in practice. We introduced a sim-
ple and effective solution based on matrix decom-
position techniques: CCA is used to derive low-
dimensional gazetteer embeddings and RRQR is
used to find a subset of these embeddings. Experi-
ments on slot tagging show that our method yields
relative error reduction of &gt; 50% on average over
the random selection method.
</bodyText>
<page confidence="0.994181">
810
</page>
<sectionHeader confidence="0.989854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999835139534884">
Tasos Anastasakos, Young-Bum Kim, and Anoop Deo-
ras. 2014. Task specific continuous word represen-
tations for mono and multi-lingual spoken language
understanding. In ICASSP, pages 3246–3250. IEEE.
Christos Boutsidis, Michael W Mahoney, and Petros
Drineas. 2009. An improved approximation al-
gorithm for the column subset selection problem.
In Proceedings of the twentieth Annual ACM-SIAM
Symposium on Discrete Algorithms, pages 968–977.
Society for Industrial and Applied Mathematics.
Peter F Brown, Peter V Desouza, Robert L Mercer,
Vincent J Della Pietra, and Jenifer C Lai. 1992.
Class-based n-gram models of natural language.
Computational linguistics, 18(4):467–479.
Asli Celikyilmaz, Dilek Z Hakkani-T¨ur, G¨okhan T¨ur,
and Ruhi Sarikaya. 2013. Semi-supervised seman-
tic tagging of conversational understanding using
markov topic regression. In ACL (1), pages 914–
923.
Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasu-
pat, and Ruhi Sarikaya. 2015. Enriching word em-
beddings using knowledge graph for semantic tag-
ging in conversational dialog systems. AAAI - As-
sociation for the Advancement of Artificial Intelli-
gence, January.
Dilek Hakkani-T¨ur, Gokhan Tur, Larry Heck, Asli Ce-
likyilmaz, Ashley Fidler, Dustin Hillard, Rukmini
Iyer, and S. Parthasarathy. 2011. Employing web
search query click logs for multi-domain spoken
language understanding. IEEE Automatic Speech
Recognition and Understanding Workshop, Decem-
ber.
Dustin Hillard, Asli Celikyilmaz, Dilek Z Hakkani-
T¨ur, and G¨okhan T¨ur. 2011. Learning weighted
entity lists from web click logs for spoken language
understanding. In INTERSPEECH, pages 705–708.
Harold Hotelling. 1936. Relations between two sets of
variates. Biometrika, 28(3/4):321–377.
Young-Bum Kim and Benjamin Snyder. 2013. Opti-
mal data set selection: An application to grapheme-
to-phoneme conversion. In HLT-NAACL, pages
1196–1205. Association for Computational Linguis-
tics.
Young-Bum Kim, Jeong Minwoo, Karl Startos, and
Ruhi Sarikaya. 2015a. Weakly supervised slot
tagging with partially labeled sequences from web
search click logs. In HLT-NAACL, pages 84–92. As-
sociation for Computational Linguistics.
Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya.
2015b. Pre-training of hidden-unit crfs. In ACL.
Association for Computational Linguistics.
Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and
Minwoo Jeong. 2015c. New transfer learning tech-
niques for disparate label sets. In ACL. Association
for Computational Linguistics.
John Lafferty, Andrew McCallum, and Fernando CN
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In ICML, pages 282–289.
Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extract-
ing structured information from user queries with
semi-supervised conditional random fields. In Pro-
ceedings of the 32nd international ACM SIGIR con-
ference on Research and development in information
retrieval.
Xiaohu Liu and Ruhi Sarikaya. 2014. A discriminative
model based entity dictionary weighting approach
for spoken language understanding. In Spoken Lan-
guage Technology Workshop (SLT), pages 195–199.
IEEE.
Ruhi Sarikaya, Asli C, Anoop Deoras, and Minwoo
Jeong. 2014. Shrinkage based features for slot tag-
ging with conditional random fields. In Proceeding
of ISCA - International Speech Communication As-
sociation, September.
Huihsin Tseng, Longbin Chen, Fan Li, Ziming Zhuang,
Lei Duan, and Belle Tseng. 2009. Mining search
engine clickthrough log for matching n-gram fea-
tures. In Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Process-
ing: Volume 2-Volume 2, pages 524–533. Associa-
tion for Computational Linguistics.
Puyang Xu and Ruhi Sarikaya. 2014. Targeted feature
dropout for robust slot filling in natural language un-
derstanding. In ISCA - International Speech Com-
munication Association, September.
</reference>
<page confidence="0.998369">
811
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.440173">
<title confidence="0.878976">Compact Lexicon Selection with Spectral Methods</title>
<affiliation confidence="0.58096">Corporation, Redmond,</affiliation>
<address confidence="0.943695">University, New York,</address>
<email confidence="0.9950345">derekliu,stratos@cs.columbia.edu</email>
<abstract confidence="0.999378578947369">In this paper, we introduce the task of selecting compact lexicon from large, noisy gazetteers. This scenario arises often in practice, in particular spoken language understanding (SLU). We propose a simple and effective solution based on matrix decomposition techniques: canonical correlation analysis (CCA) and rank-revealing QR (RRQR) factorization. CCA is first used to derive low-dimensional gazetteer embeddings from domain-specific search logs. Then RRQR is used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction randomly selected lexicon.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Tasos Anastasakos</author>
<author>Young-Bum Kim</author>
<author>Anoop Deoras</author>
</authors>
<title>Task specific continuous word representations for mono and multi-lingual spoken language understanding.</title>
<date>2014</date>
<booktitle>In ICASSP,</booktitle>
<pages>3246--3250</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1268" citStr="Anastasakos et al., 2014" startWordPosition="183" endWordPosition="186">nsional gazetteer embeddings from domain-specific search logs. Then RRQR is used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of &gt; 50% over randomly selected lexicon. 1 Introduction Discriminative models trained with large quantities of arbitrary features are a dominant paradigm in spoken language understanding (SLU) (Li et al., 2009; Hillard et al., 2011; Celikyilmaz et al., 2013; Liu and Sarikaya, 2014; Sarikaya et al., 2014; Anastasakos et al., 2014; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). An important category of these features comes from entity dictionaries or gazetteers—lists of phrases whose labels are given. For instance, they can be lists of movies, music titles, actors, restaurants, and cities. These features enable SLU models to robustly handle unseen entities at test time. However, these lists are often massive and very noisy. This is because they are typically obtained automatically by mining the web for recent entries (such as newly launched movie names). Ideal</context>
</contexts>
<marker>Anastasakos, Kim, Deoras, 2014</marker>
<rawString>Tasos Anastasakos, Young-Bum Kim, and Anoop Deoras. 2014. Task specific continuous word representations for mono and multi-lingual spoken language understanding. In ICASSP, pages 3246–3250. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christos Boutsidis</author>
<author>Michael W Mahoney</author>
<author>Petros Drineas</author>
</authors>
<title>An improved approximation algorithm for the column subset selection problem.</title>
<date>2009</date>
<journal>Society for Industrial and Applied Mathematics.</journal>
<booktitle>In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms,</booktitle>
<pages>968--977</pages>
<contexts>
<context position="2890" citStr="Boutsidis et al., 2009" startWordPosition="445" endWordPosition="448">at will nevertheless improve model performance nearly as much as the original lexicon. This will allow an SLU model to take full advantage of gazetteer resources at test time without being overwhelmed by their scale. Our selection method is two steps. First, we gather relevant information for each gazetteer element using domain-specific search logs. Then we perform CCA using this information to derive lowdimensional gazetteer embeddings (Hotelling, 1936). Second, we use a subset selection method based on RRQR to locate gazetteer embeddings whose span approximates the the entire lexicon space (Boutsidis et al., 2009; Kim and Snyder, 2013). We show in slot tagging experiments that the gazetteer elements selected by our method not only preserve the performance of using full lexicon but even improve it in some cases. Compared to random selection, our method achieves average relative error reduction of &gt; 50%. 2 Motivation We motivate our task by describing the process of lexicon construction. Entity dictionaries are usually automatically mined from the web using resources that provide typed entities. On a regular basis, these dictionaries are automatically updated and accumulated based on local data feeds an</context>
<context position="5407" citStr="Boutsidis et al. (2009)" startWordPosition="870" endWordPosition="873"> as the row subset selection problem. In this framework, we organize n gazetteer elements as matrix A E Rnxd whose rows Ai E Rd are some representations of the gazetteer members. Given m &lt; n, let S(A, m) := 1B E Rmxd : Bi = Aπ(i)} be a set of matrices whose rows are a subset of the rows of A. Note that IS(A, m)I = (n ). Our goal is to m select 1 B* = arg min BES(A,m) �� ��A − AB+BIIF That is, we want B to satisfy range(BT) � range(AT). We can solve for B* exactly with exhaustive search in O(nm), but this brute-force approach is clearly not scalable. Instead, we turn to the O(nd2) algorithm of Boutsidis et al. (2009) which we review below. 3.1.1 RRQR factorization A key ingredient in the algorithm of Boutsidis et al. (2009) is the use of RRQR factorization. Recall that a (thin) QR factorization of A expresses A = QR where Q E Rnxd has orthonormal columns and R E Rdxd is an upper triangular matrix. A limitation of QR factorization is that it does not assign a score to each of the d components. This is in contrast to singular value decomposition (SVD) which assigns a score (singular value) indicating the importance of these components. 1The Frobenius norm ||M||F is defined as the entry-wise �EL2 norm: i,j m</context>
<context position="6851" citStr="Boutsidis et al. (2009)" startWordPosition="1149" endWordPosition="1152">all • Perform SVD on A and let U E Rdxm be a matrix whose columns are the left singular vectors corresponding to the largest m singular values. • Associate a probability pi with the i-th row of A as follows: 2 pi := min � 1, Lm log ml ||UiI ||1 • Discard the i-th row of A with probability 1 − pi. If kept, the row is multiplied by 1/,,Ipi. Let these O(m log m) rows form the columns of a new matrix A¯ E RdxO(m log m). • Perform RRQR on A¯ to obtain ¯All = QR. • Return the m rows of the original A corresponding to the top m columns of ¯All. Figure 1: Gazetteer selection based on the algorithm of Boutsidis et al. (2009). RRQR factorization is a less well-known variant of QR that addresses this limitation. Let σi(M) denote the i-th largest singular value of matrix M. Given A, RRQR jointly finds a permutation matrix ll E 10,1}dxd, orthonormal Q E Rnxd, and upper triangular R = [R11 R12; 0 R22] E Rdxd such that fR11 R121 All = Q R22 satisfying = and for k = 1... d. Because of this ranking property, RRQR the numerical rank of A. Furthermore, the columns of All are sorted in the order of decreasing importance. 3.1.2 Gazetteer selection algorithm The algorithm is atwo-stage procedure. In the first step, we randoml</context>
</contexts>
<marker>Boutsidis, Mahoney, Drineas, 2009</marker>
<rawString>Christos Boutsidis, Michael W Mahoney, and Petros Drineas. 2009. An improved approximation algorithm for the column subset selection problem. In Proceedings of the twentieth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 968–977. Society for Industrial and Applied Mathematics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Peter V Desouza</author>
<author>Robert L Mercer</author>
<author>Vincent J Della Pietra</author>
<author>Jenifer C Lai</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational linguistics,</journal>
<volume>18</volume>
<issue>4</issue>
<contexts>
<context position="15472" citStr="Brown et al., 1992" startWordPosition="2649" endWordPosition="2652">onaries are mined from the web and search logs automatically using basic pattern matching approaches (e.g. entities sharing the same or similar context in queries or documents) and consequently contain significant amount of noise. As the table indicates, the number of elements in total across all the gazetteers (#total gazet elements) in each domain are too large for models to consume. In all our experiments, we trained conditional random fields (CRFs) (Lafferty et al., 2001) with the following features: (1) n-gram features up to n = 3, (2) regular expression features, and (3) Brown clusters (Brown et al., 1992) induced from search logs. With these features, we compare the following methods to demonstrate the importance of adding appropriate gazetteers: • NoG: train without gazetteer features. • AllG: train with all gazetteers. • RandG: train with randomly selected gazetteers. • RRQRG: train with gazetteers selected from RRQR. • RankAllG: train with all ranked gazetteers. 809 Domains #labels #kinds of gazets #total gazet elements #training queries #test queries Movies 25 21 14,188,527 43,784 12,179 Music 7 13 62,231,869 31,853 8,615 Places 32 31 34,227,612 22,345 6,143 Table 3: Data statistics Here g</context>
</contexts>
<marker>Brown, Desouza, Mercer, Pietra, Lai, 1992</marker>
<rawString>Peter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Z Hakkani-T¨ur</author>
<author>G¨okhan T¨ur</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Semi-supervised semantic tagging of conversational understanding using markov topic regression.</title>
<date>2013</date>
<booktitle>In ACL (1),</booktitle>
<pages>914--923</pages>
<marker>Celikyilmaz, Hakkani-T¨ur, T¨ur, Sarikaya, 2013</marker>
<rawString>Asli Celikyilmaz, Dilek Z Hakkani-T¨ur, G¨okhan T¨ur, and Ruhi Sarikaya. 2013. Semi-supervised semantic tagging of conversational understanding using markov topic regression. In ACL (1), pages 914– 923.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asli Celikyilmaz</author>
<author>Dilek Hakkani-Tur</author>
<author>Panupong Pasupat</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems. AAAI - Association for the Advancement of Artificial Intelligence,</title>
<date>2015</date>
<contexts>
<context position="1317" citStr="Celikyilmaz et al., 2015" startWordPosition="191" endWordPosition="194"> search logs. Then RRQR is used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of &gt; 50% over randomly selected lexicon. 1 Introduction Discriminative models trained with large quantities of arbitrary features are a dominant paradigm in spoken language understanding (SLU) (Li et al., 2009; Hillard et al., 2011; Celikyilmaz et al., 2013; Liu and Sarikaya, 2014; Sarikaya et al., 2014; Anastasakos et al., 2014; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). An important category of these features comes from entity dictionaries or gazetteers—lists of phrases whose labels are given. For instance, they can be lists of movies, music titles, actors, restaurants, and cities. These features enable SLU models to robustly handle unseen entities at test time. However, these lists are often massive and very noisy. This is because they are typically obtained automatically by mining the web for recent entries (such as newly launched movie names). Ideally, we would like an SLU model to have access to </context>
</contexts>
<marker>Celikyilmaz, Hakkani-Tur, Pasupat, Sarikaya, 2015</marker>
<rawString>Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasupat, and Ruhi Sarikaya. 2015. Enriching word embeddings using knowledge graph for semantic tagging in conversational dialog systems. AAAI - Association for the Advancement of Artificial Intelligence, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dilek Hakkani-T¨ur</author>
<author>Gokhan Tur</author>
<author>Larry Heck</author>
<author>Asli Celikyilmaz</author>
<author>Ashley Fidler</author>
<author>Dustin Hillard</author>
<author>Rukmini Iyer</author>
<author>S Parthasarathy</author>
</authors>
<title>Employing web search query click logs for multi-domain spoken language understanding. IEEE Automatic Speech Recognition and Understanding Workshop,</title>
<date>2011</date>
<marker>Hakkani-T¨ur, Tur, Heck, Celikyilmaz, Fidler, Hillard, Iyer, Parthasarathy, 2011</marker>
<rawString>Dilek Hakkani-T¨ur, Gokhan Tur, Larry Heck, Asli Celikyilmaz, Ashley Fidler, Dustin Hillard, Rukmini Iyer, and S. Parthasarathy. 2011. Employing web search query click logs for multi-domain spoken language understanding. IEEE Automatic Speech Recognition and Understanding Workshop, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dustin Hillard</author>
<author>Asli Celikyilmaz</author>
<author>Dilek Z HakkaniT¨ur</author>
<author>G¨okhan T¨ur</author>
</authors>
<title>Learning weighted entity lists from web click logs for spoken language understanding.</title>
<date>2011</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>705--708</pages>
<marker>Hillard, Celikyilmaz, HakkaniT¨ur, T¨ur, 2011</marker>
<rawString>Dustin Hillard, Asli Celikyilmaz, Dilek Z HakkaniT¨ur, and G¨okhan T¨ur. 2011. Learning weighted entity lists from web click logs for spoken language understanding. In INTERSPEECH, pages 705–708.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Harold Hotelling</author>
</authors>
<title>Relations between two sets of variates.</title>
<date>1936</date>
<journal>Biometrika,</journal>
<pages>28--3</pages>
<contexts>
<context position="2726" citStr="Hotelling, 1936" startWordPosition="421" endWordPosition="422"> we consider multiple domains, languages, and locales. In this paper, we introduce the task of selecting a small, representative subset of noisy gazetteers that will nevertheless improve model performance nearly as much as the original lexicon. This will allow an SLU model to take full advantage of gazetteer resources at test time without being overwhelmed by their scale. Our selection method is two steps. First, we gather relevant information for each gazetteer element using domain-specific search logs. Then we perform CCA using this information to derive lowdimensional gazetteer embeddings (Hotelling, 1936). Second, we use a subset selection method based on RRQR to locate gazetteer embeddings whose span approximates the the entire lexicon space (Boutsidis et al., 2009; Kim and Snyder, 2013). We show in slot tagging experiments that the gazetteer elements selected by our method not only preserve the performance of using full lexicon but even improve it in some cases. Compared to random selection, our method achieves average relative error reduction of &gt; 50%. 2 Motivation We motivate our task by describing the process of lexicon construction. Entity dictionaries are usually automatically mined fro</context>
<context position="9591" citStr="Hotelling, 1936" startWordPosition="1644" endWordPosition="1645">. Then CCA computes the following for i = 1... k: arg max �n l=1(u� i xl)(v�i yl) uiERd, viERd0: u&gt;i ui0=0 ∀i0&lt;i v&gt;i vi0=0 ∀i0&lt;i ��n ��n l=1(u� i xl)2 l=1(v�i yl)2 In other words, each (ui, vi) is a pair of projection vectors such that the correlation between the projected variables uz xl and vz yl is maximized, under the constraint that this projection is uncorrelated with the previous i − 1 projections. This is a non-convex problem due to the interaction between ui and vi. However, a method based on singular value decomposition (SVD) provides an efficient and exact solution to this problem (Hotelling, 1936). The resulting solution u1 ... uk E Rd and v1 ... vk E Rd0 can be used to project the variables from the original d- and d&apos;-dimensional spaces to a k-dimensional space: x E Rd −� x¯E Rk : ¯xi = u� i x y E Rd0 −� y¯E Rk : ¯yi = vZ y The new k-dimensional representation of each variable now contains information about the other variable. The value of k is usually selected to be much smaller than d or d&apos;, so the representation is typically also low-dimensional. 3.2.2 Inducing gazetteer embeddings We now describe how to use CCA to induce vector representations for gazetteer elements. Using the sam</context>
</contexts>
<marker>Hotelling, 1936</marker>
<rawString>Harold Hotelling. 1936. Relations between two sets of variates. Biometrika, 28(3/4):321–377.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Benjamin Snyder</author>
</authors>
<title>Optimal data set selection: An application to graphemeto-phoneme conversion.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>1196--1205</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="2913" citStr="Kim and Snyder, 2013" startWordPosition="449" endWordPosition="452">rove model performance nearly as much as the original lexicon. This will allow an SLU model to take full advantage of gazetteer resources at test time without being overwhelmed by their scale. Our selection method is two steps. First, we gather relevant information for each gazetteer element using domain-specific search logs. Then we perform CCA using this information to derive lowdimensional gazetteer embeddings (Hotelling, 1936). Second, we use a subset selection method based on RRQR to locate gazetteer embeddings whose span approximates the the entire lexicon space (Boutsidis et al., 2009; Kim and Snyder, 2013). We show in slot tagging experiments that the gazetteer elements selected by our method not only preserve the performance of using full lexicon but even improve it in some cases. Compared to random selection, our method achieves average relative error reduction of &gt; 50%. 2 Motivation We motivate our task by describing the process of lexicon construction. Entity dictionaries are usually automatically mined from the web using resources that provide typed entities. On a regular basis, these dictionaries are automatically updated and accumulated based on local data feeds and knowledge graphs. Loc</context>
</contexts>
<marker>Kim, Snyder, 2013</marker>
<rawString>Young-Bum Kim and Benjamin Snyder. 2013. Optimal data set selection: An application to graphemeto-phoneme conversion. In HLT-NAACL, pages 1196–1205. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Jeong Minwoo</author>
<author>Karl Startos</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Weakly supervised slot tagging with partially labeled sequences from web search click logs.</title>
<date>2015</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>84--92</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1335" citStr="Kim et al., 2015" startWordPosition="195" endWordPosition="198"> used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of &gt; 50% over randomly selected lexicon. 1 Introduction Discriminative models trained with large quantities of arbitrary features are a dominant paradigm in spoken language understanding (SLU) (Li et al., 2009; Hillard et al., 2011; Celikyilmaz et al., 2013; Liu and Sarikaya, 2014; Sarikaya et al., 2014; Anastasakos et al., 2014; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). An important category of these features comes from entity dictionaries or gazetteers—lists of phrases whose labels are given. For instance, they can be lists of movies, music titles, actors, restaurants, and cities. These features enable SLU models to robustly handle unseen entities at test time. However, these lists are often massive and very noisy. This is because they are typically obtained automatically by mining the web for recent entries (such as newly launched movie names). Ideally, we would like an SLU model to have access to this vast source o</context>
<context position="11864" citStr="Kim et al., 2015" startWordPosition="2047" endWordPosition="2050">in the sentences. For instance, if g = “The Matrix” is a gazetteer element of domain l = “Movie”, we collect sentences from movie-specific search logs involving the phrase “The Matrix”. Such domain-specific search logs are collected using a pre-trained domain classifier. Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been min ˜AERn×d: rank(˜A)=m ��� 808 used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 The age of adaline imdb.com imdb.com en.wikipedia.org en.wikipedia.org furious7.com youtube.com rottentomatoes.com rottentomatoes.com www.msn.com movieinsider.com Table 1: Top clicked URLs of two movies. One i</context>
</contexts>
<marker>Kim, Minwoo, Startos, Sarikaya, 2015</marker>
<rawString>Young-Bum Kim, Jeong Minwoo, Karl Startos, and Ruhi Sarikaya. 2015a. Weakly supervised slot tagging with partially labeled sequences from web search click logs. In HLT-NAACL, pages 84–92. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Karl Stratos</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Pre-training of hidden-unit crfs.</title>
<date>2015</date>
<booktitle>In ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1335" citStr="Kim et al., 2015" startWordPosition="195" endWordPosition="198"> used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of &gt; 50% over randomly selected lexicon. 1 Introduction Discriminative models trained with large quantities of arbitrary features are a dominant paradigm in spoken language understanding (SLU) (Li et al., 2009; Hillard et al., 2011; Celikyilmaz et al., 2013; Liu and Sarikaya, 2014; Sarikaya et al., 2014; Anastasakos et al., 2014; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). An important category of these features comes from entity dictionaries or gazetteers—lists of phrases whose labels are given. For instance, they can be lists of movies, music titles, actors, restaurants, and cities. These features enable SLU models to robustly handle unseen entities at test time. However, these lists are often massive and very noisy. This is because they are typically obtained automatically by mining the web for recent entries (such as newly launched movie names). Ideally, we would like an SLU model to have access to this vast source o</context>
<context position="11864" citStr="Kim et al., 2015" startWordPosition="2047" endWordPosition="2050">in the sentences. For instance, if g = “The Matrix” is a gazetteer element of domain l = “Movie”, we collect sentences from movie-specific search logs involving the phrase “The Matrix”. Such domain-specific search logs are collected using a pre-trained domain classifier. Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been min ˜AERn×d: rank(˜A)=m ��� 808 used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 The age of adaline imdb.com imdb.com en.wikipedia.org en.wikipedia.org furious7.com youtube.com rottentomatoes.com rottentomatoes.com www.msn.com movieinsider.com Table 1: Top clicked URLs of two movies. One i</context>
</contexts>
<marker>Kim, Stratos, Sarikaya, 2015</marker>
<rawString>Young-Bum Kim, Karl Stratos, and Ruhi Sarikaya. 2015b. Pre-training of hidden-unit crfs. In ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Young-Bum Kim</author>
<author>Karl Stratos</author>
<author>Ruhi Sarikaya</author>
<author>Minwoo Jeong</author>
</authors>
<title>New transfer learning techniques for disparate label sets.</title>
<date>2015</date>
<booktitle>In ACL. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1335" citStr="Kim et al., 2015" startWordPosition="195" endWordPosition="198"> used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of &gt; 50% over randomly selected lexicon. 1 Introduction Discriminative models trained with large quantities of arbitrary features are a dominant paradigm in spoken language understanding (SLU) (Li et al., 2009; Hillard et al., 2011; Celikyilmaz et al., 2013; Liu and Sarikaya, 2014; Sarikaya et al., 2014; Anastasakos et al., 2014; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). An important category of these features comes from entity dictionaries or gazetteers—lists of phrases whose labels are given. For instance, they can be lists of movies, music titles, actors, restaurants, and cities. These features enable SLU models to robustly handle unseen entities at test time. However, these lists are often massive and very noisy. This is because they are typically obtained automatically by mining the web for recent entries (such as newly launched movie names). Ideally, we would like an SLU model to have access to this vast source o</context>
<context position="11864" citStr="Kim et al., 2015" startWordPosition="2047" endWordPosition="2050">in the sentences. For instance, if g = “The Matrix” is a gazetteer element of domain l = “Movie”, we collect sentences from movie-specific search logs involving the phrase “The Matrix”. Such domain-specific search logs are collected using a pre-trained domain classifier. Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been min ˜AERn×d: rank(˜A)=m ��� 808 used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 The age of adaline imdb.com imdb.com en.wikipedia.org en.wikipedia.org furious7.com youtube.com rottentomatoes.com rottentomatoes.com www.msn.com movieinsider.com Table 1: Top clicked URLs of two movies. One i</context>
</contexts>
<marker>Kim, Stratos, Sarikaya, Jeong, 2015</marker>
<rawString>Young-Bum Kim, Karl Stratos, Ruhi Sarikaya, and Minwoo Jeong. 2015c. New transfer learning techniques for disparate label sets. In ACL. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando CN Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="15333" citStr="Lafferty et al., 2001" startWordPosition="2625" endWordPosition="2628">main can have various kinds of gazetteers. For example, Places domain has business name, restaurant name, school name and etc. Candidate dictionaries are mined from the web and search logs automatically using basic pattern matching approaches (e.g. entities sharing the same or similar context in queries or documents) and consequently contain significant amount of noise. As the table indicates, the number of elements in total across all the gazetteers (#total gazet elements) in each domain are too large for models to consume. In all our experiments, we trained conditional random fields (CRFs) (Lafferty et al., 2001) with the following features: (1) n-gram features up to n = 3, (2) regular expression features, and (3) Brown clusters (Brown et al., 1992) induced from search logs. With these features, we compare the following methods to demonstrate the importance of adding appropriate gazetteers: • NoG: train without gazetteer features. • AllG: train with all gazetteers. • RandG: train with randomly selected gazetteers. • RRQRG: train with gazetteers selected from RRQR. • RankAllG: train with all ranked gazetteers. 809 Domains #labels #kinds of gazets #total gazet elements #training queries #test queries Mo</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando CN Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiao Li</author>
<author>Ye-Yi Wang</author>
<author>Alex Acero</author>
</authors>
<title>Extracting structured information from user queries with semi-supervised conditional random fields.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval.</booktitle>
<contexts>
<context position="1147" citStr="Li et al., 2009" startWordPosition="163" endWordPosition="166">ical correlation analysis (CCA) and rank-revealing QR (RRQR) factorization. CCA is first used to derive low-dimensional gazetteer embeddings from domain-specific search logs. Then RRQR is used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of &gt; 50% over randomly selected lexicon. 1 Introduction Discriminative models trained with large quantities of arbitrary features are a dominant paradigm in spoken language understanding (SLU) (Li et al., 2009; Hillard et al., 2011; Celikyilmaz et al., 2013; Liu and Sarikaya, 2014; Sarikaya et al., 2014; Anastasakos et al., 2014; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). An important category of these features comes from entity dictionaries or gazetteers—lists of phrases whose labels are given. For instance, they can be lists of movies, music titles, actors, restaurants, and cities. These features enable SLU models to robustly handle unseen entities at test time. However, these lists are often massive and very noisy. This is because t</context>
</contexts>
<marker>Li, Wang, Acero, 2009</marker>
<rawString>Xiao Li, Ye-Yi Wang, and Alex Acero. 2009. Extracting structured information from user queries with semi-supervised conditional random fields. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaohu Liu</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>A discriminative model based entity dictionary weighting approach for spoken language understanding.</title>
<date>2014</date>
<booktitle>In Spoken Language Technology Workshop (SLT),</booktitle>
<pages>195--199</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="1219" citStr="Liu and Sarikaya, 2014" startWordPosition="175" endWordPosition="178">orization. CCA is first used to derive low-dimensional gazetteer embeddings from domain-specific search logs. Then RRQR is used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of &gt; 50% over randomly selected lexicon. 1 Introduction Discriminative models trained with large quantities of arbitrary features are a dominant paradigm in spoken language understanding (SLU) (Li et al., 2009; Hillard et al., 2011; Celikyilmaz et al., 2013; Liu and Sarikaya, 2014; Sarikaya et al., 2014; Anastasakos et al., 2014; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). An important category of these features comes from entity dictionaries or gazetteers—lists of phrases whose labels are given. For instance, they can be lists of movies, music titles, actors, restaurants, and cities. These features enable SLU models to robustly handle unseen entities at test time. However, these lists are often massive and very noisy. This is because they are typically obtained automatically by mining the web for recent en</context>
</contexts>
<marker>Liu, Sarikaya, 2014</marker>
<rawString>Xiaohu Liu and Ruhi Sarikaya. 2014. A discriminative model based entity dictionary weighting approach for spoken language understanding. In Spoken Language Technology Workshop (SLT), pages 195–199. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruhi Sarikaya</author>
<author>C Asli</author>
<author>Anoop Deoras</author>
<author>Minwoo Jeong</author>
</authors>
<title>Shrinkage based features for slot tagging with conditional random fields.</title>
<date>2014</date>
<booktitle>In Proceeding of ISCA - International Speech Communication Association,</booktitle>
<contexts>
<context position="1242" citStr="Sarikaya et al., 2014" startWordPosition="179" endWordPosition="182">used to derive low-dimensional gazetteer embeddings from domain-specific search logs. Then RRQR is used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of &gt; 50% over randomly selected lexicon. 1 Introduction Discriminative models trained with large quantities of arbitrary features are a dominant paradigm in spoken language understanding (SLU) (Li et al., 2009; Hillard et al., 2011; Celikyilmaz et al., 2013; Liu and Sarikaya, 2014; Sarikaya et al., 2014; Anastasakos et al., 2014; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). An important category of these features comes from entity dictionaries or gazetteers—lists of phrases whose labels are given. For instance, they can be lists of movies, music titles, actors, restaurants, and cities. These features enable SLU models to robustly handle unseen entities at test time. However, these lists are often massive and very noisy. This is because they are typically obtained automatically by mining the web for recent entries (such as newly la</context>
</contexts>
<marker>Sarikaya, Asli, Deoras, Jeong, 2014</marker>
<rawString>Ruhi Sarikaya, Asli C, Anoop Deoras, and Minwoo Jeong. 2014. Shrinkage based features for slot tagging with conditional random fields. In Proceeding of ISCA - International Speech Communication Association, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huihsin Tseng</author>
<author>Longbin Chen</author>
<author>Fan Li</author>
<author>Ziming Zhuang</author>
<author>Lei Duan</author>
<author>Belle Tseng</author>
</authors>
<title>Mining search engine clickthrough log for matching n-gram features.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume</booktitle>
<volume>2</volume>
<pages>524--533</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="11885" citStr="Tseng et al., 2009" startWordPosition="2051" endWordPosition="2054">or instance, if g = “The Matrix” is a gazetteer element of domain l = “Movie”, we collect sentences from movie-specific search logs involving the phrase “The Matrix”. Such domain-specific search logs are collected using a pre-trained domain classifier. Search click log features: Large-scale search engines such as Bing and Google process millions of queries on a daily basis. Together with the search queries, user clicked URLs are also logged anonymously. These click logs have been min ˜AERn×d: rank(˜A)=m ��� 808 used for extracting semantic information for various NLP tasks (Kim et al., 2015a; Tseng et al., 2009; Hakkani-T¨ur et al., 2011). We used the clicked URLs as features to determine the likelihood of an entity being a member of a dictionary. These features are useful because common URLs are shared across different names such as movie, business and music. Table 1 shows the top five most frequently clicked URLs for movies “Furious 7” and “The age of adaline”. Furious 7 The age of adaline imdb.com imdb.com en.wikipedia.org en.wikipedia.org furious7.com youtube.com rottentomatoes.com rottentomatoes.com www.msn.com movieinsider.com Table 1: Top clicked URLs of two movies. One issue with using only </context>
</contexts>
<marker>Tseng, Chen, Li, Zhuang, Duan, Tseng, 2009</marker>
<rawString>Huihsin Tseng, Longbin Chen, Fan Li, Ziming Zhuang, Lei Duan, and Belle Tseng. 2009. Mining search engine clickthrough log for matching n-gram features. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2-Volume 2, pages 524–533. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Puyang Xu</author>
<author>Ruhi Sarikaya</author>
</authors>
<title>Targeted feature dropout for robust slot filling in natural language understanding.</title>
<date>2014</date>
<booktitle>In ISCA - International Speech Communication Association,</booktitle>
<contexts>
<context position="1291" citStr="Xu and Sarikaya, 2014" startWordPosition="187" endWordPosition="190">gs from domain-specific search logs. Then RRQR is used to find a subset of these embeddings whose span approximates the entire lexicon space. Experiments on slot tagging show that our method yields a small set of lexicon entities with average relative error reduction of &gt; 50% over randomly selected lexicon. 1 Introduction Discriminative models trained with large quantities of arbitrary features are a dominant paradigm in spoken language understanding (SLU) (Li et al., 2009; Hillard et al., 2011; Celikyilmaz et al., 2013; Liu and Sarikaya, 2014; Sarikaya et al., 2014; Anastasakos et al., 2014; Xu and Sarikaya, 2014; Celikyilmaz et al., 2015; Kim et al., 2015a; Kim et al., 2015c; Kim et al., 2015b). An important category of these features comes from entity dictionaries or gazetteers—lists of phrases whose labels are given. For instance, they can be lists of movies, music titles, actors, restaurants, and cities. These features enable SLU models to robustly handle unseen entities at test time. However, these lists are often massive and very noisy. This is because they are typically obtained automatically by mining the web for recent entries (such as newly launched movie names). Ideally, we would like an SL</context>
</contexts>
<marker>Xu, Sarikaya, 2014</marker>
<rawString>Puyang Xu and Ruhi Sarikaya. 2014. Targeted feature dropout for robust slot filling in natural language understanding. In ISCA - International Speech Communication Association, September.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>