<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000347">
<title confidence="0.986905">
Going beyond sentences when applying tree kernels
</title>
<author confidence="0.988726">
Dmitry Ilvovsky
</author>
<affiliation confidence="0.844407666666667">
School of Applied Mathematics and Information Science
National Research University Higher School of Economics
Moscow, Russia
</affiliation>
<email confidence="0.993461">
dilvovsky@hse.ru
</email>
<sectionHeader confidence="0.993747" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999956434782609">
We go beyond the level of individual
sentences applying parse tree kernels to
paragraphs. We build a set of extended
trees for a paragraph of text from the in-
dividual parse trees for sentences and
learn short texts such as search results
and social profile postings to take ad-
vantage of additional discourse-related
information. Extension is based on coref-
erences and rhetoric structure relations
between the phrases in different sentenc-
es. We evaluate our approach, tracking
relevance classification improvement for
multi-sentence search task. The search
problem is formulated as classification of
search results into the classes of relevant
and irrelevant, learning from the Bing
search results. We compare performances
of individual sentence kernels with the
ones for extended parse trees and show
that adding discourse information to
learning data helps to improve classifica-
tion results.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977250000001">
In spite of substantial efforts to formulate a com-
plete linking theory between syntax and seman-
tics, it is not available yet. Hence the design of
syntactic features for automated learning of syn-
tactic structures is still an art. One of the solu-
tions to systematically treat these syntactic fea-
tures ‒ tree kernels built over syntactic parse
trees. Convolution tree kernel (Collins and
Duffy, 2002) defines a feature space consisting
of all subtree types of parse trees and counts the
number of common subtrees as the syntactic sim-
ilarity between two parse trees. They have found
a number of applications in several natural lan-
guage tasks, e.g. syntactic parsing re-ranking,
relation extraction (Zelenko et al., 2003; Zhang
et al 2006), named entity recognition (Cumby
and Roth, 2003) and Semantic Role Labeling
(Moschitti, 2004), pronoun resolution (Yang et
al., 2006), question classification (Zhang and
Lee, 2003) and machine translation (Zhang and
Li, 2009).
The kernel ability to generate large feature sets
is useful to quickly model new and not well un-
derstood linguistic phenomena in learning ma-
chines. However, it is often possible to manually
design features for linear kernels that produce
high accuracy and fast computation time whereas
the complexity of tree kernels may prevent their
application in real scenarios.
Many learning algorithms, such as SVM
(Vapnik, 1998) can work directly with kernels by
replacing the dot product with a particular kernel
function. This useful property of kernel methods,
that implicitly calculates the dot product in a
high-dimensional space over the original repre-
sentations of objects such as sentences, has made
kernel methods an effective solution to modeling
structured objects in NLP. A number of NL tasks
require computing of semantic features over par-
agraphs of text containing multiple sentences.
Doing it in a sentence pair-wise manner is not
always accurate, since it is strongly dependent on
how information (phrases) is distributed through
sentences.
An approach to build a kernel based on more
than a single parse tree has been proposed
(Severyn et.al., 2012), however without any rela-
tions between parse trees or for a different pur-
pose than treating multi-sentence portions of
text. To compensate for parsing errors (Zhang et
al., 2008), a convolution kernel over packed
parse forest (Severyn and Moschitti, 2012; Aioli
et.al, 2007) is used to mine syntactic features
from it directly. A packed forest compactly en-
codes exponential number of n-best parse trees,
and thus containing much more rich structured
features than a single parse tree. This advantage
enables the forest kernel not only to be more ro-
bust against parsing errors, but also to be able to
learn more reliable feature values and help to
</bodyText>
<page confidence="0.991482">
56
</page>
<note confidence="0.839058">
Proceedings of the ACL 2014 Student Research Workshop, pages 56–63,
</note>
<bodyText confidence="0.9409395">
Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics
solve the data sparseness issue that exists in the
traditional tree kernel.
On the contrary, in this study we form a tree for-
est of sequence of sentences in a paragraph of
text. Currently, kernel methods tackle individual
sentences. However, in learning settings where
texts include multiple sentences, structures which
include paragraph-level information need to be
employed. We demonstrate that in certain do-
mains and certain cases discourse structure is
essential for proper classification of texts.
</bodyText>
<sectionHeader confidence="0.497774" genericHeader="introduction">
2 Necessity to extend parse trees
</sectionHeader>
<bodyText confidence="0.9962553">
We introduce a domain where a pair-wise com-
parison of sentences is insufficient to properly
learn certain semantic features of texts. This is
due to the variability of ways information can be
communicated in multiple sentences, and varia-
tions in possible discourse structures of text
which needs to be taken into account.
We consider an example of text classification
problem, where short portions of text belong to
two classes:
</bodyText>
<listItem confidence="0.99701375">
• Tax liability of a landlord renting office
to a business.
• Tax liability of a business owner renting
an office from landlord.
</listItem>
<bodyText confidence="0.994579220779221">
I rent an office space. This office is for my busi-
ness. I can deduct office rental expense from my
business profit to calculate net income.
To run my business, I have to rent an office. The
net business profit is calculated as follows. Rental
expense needs to be subtracted from revenue.
To store goods for my retail business I rent some
space. When I calculate the net income, I take revenue
and subtract business expenses such as office rent.
I rent out a first floor unit of my house to a travel
business. I need to add the rental income to my profit.
However, when I repair my house, I can deduct the
repair expense from my rental income.
I receive rental income from my office. I have to
claim it as a profit in my tax forms. I need to add my
rental income to my profits, but subtract rental ex-
penses such as repair from it.
I advertised my property as a business rental. Ad-
vertisement and repair expenses can be subtracted
from the rental income. Remaining rental income
needs to be added to my profit and be reported as tax-
able profit.
Firstly, note that keyword-based analysis does
not help to separate the first three paragraphs and
the second three paragraphs. They all share the
same keywords rent-
al/office/income/profit/add/subtract. Phrase-
based analysis does not help, since both sets of
paragraphs share similar phrases. Secondly, pair-
wise sentence comparison does not solve the
problem either. Anaphora resolution is helpful
but insufficient. All these sentences include ‘I’
and its mention, but other links between words or
phrases in different sentences need to be used.
Rhetoric structures need to come into play to
provide additional links between sentences. The
structure to distinguish between
renting for yourself and deducting from total in-
come
and
renting to someone and adding to income
embraces multiple sentences. The second clause
about adding/subtracting incomes is linked by
means of the rhetoric relation of elaboration with
the first clause for landlord/tenant. This rhetoric
relation may link discourse units within a sen-
tence, between consecutive sentences and even
between first and third sentence in a paragraph.
Other rhetoric relations can play similar role for
forming essential links for text classification.
Which representations for these paragraphs of
text would produce such common sub-structure
between the structures of these paragraphs? We
believe that extended trees, which include the
first, second, and third sentence for each para-
graph together can serve as a structure to differ-
entiate the two above classes.
The dependency parse trees for the first text in
our set and its coreferences are shown in Fig. 1.
There are multiple ways the nodes from parse
trees of different sentences can be connected: we
choose the rhetoric relation of elaboration which
links the same entity office and helps us to form
the structure rent-office-space – for-my-business
– deduct-rental-expense which is the base for our
classification. We used Stanford Core NLP, co-
references resolution (Lee et al., 2012) and its
visualization to form Figs. 1 and 2.
Fig. 2 shows the resultant extended tree with
the root ‘I’ from the first sentence. It includes the
whole first sentence, a verb phrase from the sec-
ond sentence and a verb phrase from the third
sentence according to rhetoric relation of elabo-
ration. Notice that this extended tree can be intui-
tively viewed as representing the ‘main idea’ of
this text compared to other texts in our set. All
extended trees need to be formed for a text and
</bodyText>
<page confidence="0.990946">
57
</page>
<bodyText confidence="0.989864">
then compared with that of the other texts, since
we don’t know in advance which extended tree is
essential. From the standpoint of tree kernel
learning, extended trees are learned the same
way as regular parse trees.
R12: P1i →P2j between the nodes P1i and P2j, we
form the pair of extended trees P1*P2:
•.., P1i-2, P1i-1, P1i, P2j, P2j+1, P2j+2,•..
•.., P2j-2, P2j-1, P2j, P1i, P1i+1, P2i+2,•..,
which would form the feature set for tree kernel
learning in addition to the original trees P1 and
P2. Notice that the original order of nodes of
parse trees is retained under operation ‘*’ (Fig.
3).
</bodyText>
<equation confidence="0.897425">
P11 P21
P1i P2j
P2j+
</equation>
<figureCaption confidence="0.941005">
Fig.1: Coreferences and the set of dependency trees
for the first text.
Fig. 2: Extended tree which includes 3 sentences
</figureCaption>
<sectionHeader confidence="0.743844" genericHeader="method">
3 Building extended trees
</sectionHeader>
<bodyText confidence="0.999828357142857">
For every arc which connects two parse trees, we
derive the extension of these trees, extending
branches according to the arc (Fig. 3).
In this approach, for a given parse tree, we
will obtain a set of its extension, so the elements
of kernel will be computed for many extensions,
instead of just a single tree. The problem here is
that we need to find common sub-trees for a
much higher number of trees than the number of
sentences in text, however by subsumption (sub-
tree relation) the number of common sub-trees
will be substantially reduced.
If we have two parse trees P1 and P2 for two
sentences in a paragraph, and a relation
</bodyText>
<figureCaption confidence="0.997262666666667">
Fig. 3: An arc which connects two parse trees for two
sentences in a text (on the top) and the derived set of
extended trees (on the bottom).
</figureCaption>
<bodyText confidence="0.795174363636364">
The algorithm for building an extended tree for a
set of parse trees T is presented below:
Notice that the resultant trees are not the prop-
er parse trees for a sentence, but nevertheless
form an adequate feature space for tree kernel
learning.
Input:
1) Set of parse trees T.
2) Set of relations R, which includes relations Rijk be-
tween the nodes of Ti and Tj: Ti eT, Tj eT, Rijk eR.
We use index k to range over multiple relations be-
tween the nodes of parse tree for a pair of sentences.
Output: the exhaustive set of extended trees E.
Set E = ;
For each tree i=1:|T|
For each relation Rijk, k= 1: |R|
Obtain Tj
Form the pair of extended trees Ti * Tj;
Verify that each of the extended trees do not have
a super-tree in E
If verified, add to E;
Return E.
</bodyText>
<page confidence="0.965293">
58
</page>
<bodyText confidence="0.998326">
To obtain the inter-sentence links, we em-
ployed the following sources:
</bodyText>
<listItem confidence="0.998281">
• Coreferences from Stanford NLP (Re-
casens et al., 2013, Lee et al., 2013).
• Rhetoric relation extractor based on the
</listItem>
<bodyText confidence="0.711576166666667">
rule-based approach to finding relations
between elementary discourse units
(Galitsky et al., 2013). We combined
manual rules with automatically learned
derived from the available discourse cor-
pus by means of syntactic generalization.
</bodyText>
<sectionHeader confidence="0.675888" genericHeader="method">
4 Assessment of classification improve-
ment
</sectionHeader>
<bodyText confidence="0.9984634">
To confirm that using a set of extended parse
trees for paragraphs leverages additional seman-
tic information compared to a set of parse trees
for all sentences in a paragraph, we perform an
evaluation of relevance in search domain:
</bodyText>
<listItem confidence="0.993709">
• As a baseline, we take all trees for sen-
tences in paragraphs
• As an expected improvement, we take all
extended trees in a paragraph.
</listItem>
<bodyText confidence="0.999715142857143">
Since a benchmarking database for answering
complex multi-sentence questions is not availa-
ble, we form our own dataset for product-related
opinions. The question answering problem is
formulated as finding information on the web,
relevant to a user posting / opinion expression in
a blog, forum or social network.
For the purpose of this evaluation it is not es-
sential to provide the best possible set of an-
swers. Instead, we are concerned with the com-
parison of relevance improvement by using ex-
tended parse tree, as long as the evaluation set-
tings of question answering are identical. The
details of the evaluation are given in Section 7.
</bodyText>
<sectionHeader confidence="0.7730925" genericHeader="method">
5 Implementation of kernel learning for
extended trees
</sectionHeader>
<bodyText confidence="0.999314">
The evaluation framework described here is im-
plemented as an OpenNLP contribution. It relies
on the following systems:
</bodyText>
<listItem confidence="0.979805933333333">
• OpenNLP/Stanford NLP parser;
• Stanford NLP Coreference;
• Bing search;
• Wrapper of TK-Light kernel learner
(Moschitti, 2006).
Framework includes the following compo-
nents of Apache OpenNLP.similarity project:
• Rhetoric parser
• Parse thicket builder and generalizer
(Galitsky et al., 2012). Not used in this
evaluation.
• A number of applications based on the
above component, including search (re-
quest handler for SOLR), speech recog-
nition, content generation and others.
</listItem>
<bodyText confidence="0.999502533333333">
One of the use cases of this
OpenNLP.similarity component is a Java wrap-
per for tree kernel algorithms implemented in
C++. It allows seamless integration of tree kernel
algorithms into other open source systems avail-
able in Java for search, information retrieval and
machine learning. Moreover, tree kernel algo-
rithms can be embedded into Hadoop framework
in the domains where offline performance is es-
sential. Libraries and evaluation results described
in this paper are also available at
http://code.google.com/p/relevance-based-on-
parse-trees and
http://svn.apache.org/repos/asf/opennlp/sandbox/
opennlp-similarity/.
</bodyText>
<sectionHeader confidence="0.989631" genericHeader="method">
6 Complexity estimation
</sectionHeader>
<bodyText confidence="0.999945142857143">
To estimate the complexity of building extended
trees, let us consider an average case with 5 sen-
tences in each paragraph and 15 words in each
sentence. We have on average 10 inter-sentence
arcs, which give us up to 20 extended trees
formed from two sentences, and 60 extended
trees formed from 3 sentences. Hence we have to
apply tree learning to up to 100 trees (of a bigger
size) instead of just 5 original trees. We observe
that kernel learning of extended trees has to han-
dle at least 20 times bigger input set.
However, most of the smaller subtrees are re-
petitive and will be reduced in the course of di-
mensionality reduction.
</bodyText>
<sectionHeader confidence="0.998622" genericHeader="evaluation">
7 Evaluation
</sectionHeader>
<bodyText confidence="0.99854425">
To estimate whether additional high-level se-
mantic and discourse information contributes to
classical kernel based approach, we compare two
sources for trees:
</bodyText>
<listItem confidence="0.999933">
• Regular parse trees
• Extended parse trees
</listItem>
<page confidence="0.99595">
59
</page>
<bodyText confidence="0.99979727027027">
To perform this estimation, we need a corpus
including a high number of short texts similar to
our example in Introduction. These texts should
have high similarity (otherwise keyword ap-
proach would do well), certain discourse struc-
ture, and describe some objects (products) in a
meaningful application domain. Unfortunately,
to the best of our knowledge such corpus is not
available. Therefore, for comparison of tree ker-
nel performances we decided to use search re-
sults, given the query which is a short text. We
rely on search engine APIs following the evalua-
tion settings in the studies on answering complex
questions (Galitsky et al., 2013).
Search results typically include texts of fairly
high similarity, which is leveraged in our evalua-
tion. To formulate classification problem on the
set of texts obtained as search results, we need to
form positive and negative sets. To do that, we
select the first n search results as relevant (posi-
tive) and also n results towards to tail of search
results lists as irrelevant (negative). In this case
each search session yields an individual training
(and evaluation) dataset. The same nature of such
data allows averaging of precision and recall,
having individual training dataset of a limited
size. Hence reliability of our results is achieved
not via the size of individual dataset, but instead
by the increased number of search sessions. To
assure an abrupt change in relevance proceeding
from the head to the tail of search results lists,
we use complicated queries including multiple
sentences, which are not handled by modern
search engines well.
The preparation of search queries (which in-
clude multiple sentences) is based on the follow-
ing steps:
</bodyText>
<listItem confidence="0.901116538461538">
1. Forming the names of products and their
short descriptions
2. Given (1), find a text including an ex-
tended review or opinion about this
product.
3. Texts (2) cannot be used as queries as
they are. To form the queries from (2),
we need to extract most significant
phrases from them; otherwise, search
engines are confused which keywords to
choose and give either duplicate, or irrel-
evant results. These were the longest
noun and selected verb phrases from (2).
</listItem>
<bodyText confidence="0.999894745454546">
The analogous steps were conducted for Ya-
hoo Answers data. We manually select a 100
most interesting search queries for each domain.
The training/evaluation datasets is formed
from search results in the following way. We
obtain a first hundred search results (or less if
hundred is not available). We select 1..20 (or
first 20%) of search results as a positive set, and
81..100 as a negative set. Search results 21..80
form the basis of evaluation dataset, from which
we randomly select 10 texts to be classified into
the classes of positive or negative. Hence we
have the ratio 4:1 between the training and eval-
uation datasets.
To motivate our evaluation setting, we rely on
the following observations. In case of searching
for complex multi-sentence queries, relevance
indeed drops abruptly with proceeding from the
first 10-20 search results, as search evaluation
results demonstrated (Galitsky et al., 2013). The
order of search results in first 20% and last 20%
does not affect our evaluation. Although the last
20% of search results is not really a “gold stand-
ard”, it is nevertheless a set that can be reasona-
bly separated from the positive set. If such sepa-
ration is too easy or too difficult, it would be
hard to adequately evaluate the difference be-
tween regular parse trees and extended trees for
text classification. Search-based approach to col-
lect texts for evaluation of classification allows
reaching maximum degree of experiment auto-
mation.
It turned out that the use of tail search results
as negative set helps to leverage the high level
semantic and discourse information. Negative
examples, as well as positive ones, include most
keywords from the queries. However, the main
difference between the positive and negative
search results is that the former include much
more coreferences and rhetoric structures similar
to the query, than the latter set. The use of the
extended trees was beneficial in the cases where
phrases from queries are distributed through mul-
tiple sentences in search results.
We conducted two independent experiments
for each search session, classifying search result
snippets and also original texts, extracted from
webpages. For the snippets, we split them into
sentence fragments and built extended trees for
these fragments of sentences. For original texts,
we extracted all sentences related to the snippet
fragments and built extended trees for these sen-
tences.
Training and classification occurs in the auto-
mated mode, and the classification assessment is
</bodyText>
<page confidence="0.996503">
60
</page>
<bodyText confidence="0.999410777777778">
conducted by the members of research group
guided by the authors. The assessors only con-
sulted the query and answer snippets.
We used the standard parameters of tree se-
quence kernels from
http://disi.unitn.it/moschitti/Tree-Kernel.htm
(Moschitti, 2006). Tree kernel is applied to all
tree pairs from two forests. The latest version of
tree kernel learner was obtained from the author.
</bodyText>
<table confidence="0.999505090909091">
Products Basic Extended ker-
kernels nels (co-
refs+RST)
Texts Precision 0,5679 0,5868
from the
pages
Recall 0,7516 0,8458
F-measure 0,6485 0,6752
Snippets Precision 0,5625 0,6319
Recall 0,7840 0,8313
F-measure 0,6169 0,6695
</table>
<tableCaption confidence="0.997636">
Table 1: Evaluation results for products domain
</tableCaption>
<table confidence="0.99990625">
Answers Basic Extended Extended
kernels kernels kernels
(corefs) (corefs+
RST)
Texts P 0,5167 0,5083 0,5437
from the
pages
R 0,7361 0,7917 0,8333
F 0,6008 0,5458 0,6278
Snippets P 0,5950 0,6264 0,6794
R 0,7329 0,7492 0,7900
F 0,6249 0,6429 0,7067
</table>
<tableCaption confidence="0.9056855">
Table 2: Evaluation results for popular answers do-
main
</tableCaption>
<bodyText confidence="0.999948818181818">
Evaluation results show visible improvement of
classification accuracy achieved by extended
trees. For Yahoo Answers one can observe that
coreferences only provide a slight improvement
of accuracy, whereas RST added to coreferences
gives a stronger improvement. Stronger increase
of recall in comparison to precision can be ex-
plained by the following. It is due to the acquired
capability of extended trees to match phrases
from the search results distributed through multi-
ple sentences, with questions.
</bodyText>
<sectionHeader confidence="0.958062" genericHeader="conclusions">
8 Conclusions and future work
</sectionHeader>
<bodyText confidence="0.999849">
In this study we focused on how discourse in-
formation can help with text relevance tasks irre-
spectively of learning mechanisms. We com-
pared two sets of linguistic features:
</bodyText>
<listItem confidence="0.997461333333333">
• The baseline, parse trees for individual
sentences,
• Parse trees and discourse information,
</listItem>
<bodyText confidence="0.99997575">
and demonstrated that the enriched set of fea-
tures indeed improves the classification accura-
cy, having the learning framework fixed. This
improvement varies from 2 to 8 % in different
domains with different structure of texts. To
tackle such enriched set of linguistic features, an
adjustment of tree kernel algorithm itself was not
necessary.
The approach developed in this paper can also
be applied to parse tree querying and manipula-
tion problem (Levy and Galen, 2006). A system
such as Tregex is an expressive and flexible way
for single sentence parse tree querying and ma-
nipulation. Extending parse trees of individual
sentences towards paragraph of text, the recall of
a tree querying system would dramatically in-
crease, and dependence on how phrases are dis-
tributed through sentences would decrease.
There are a few possible directions of future
development. One interesting continuation of this
study is to applying standard ranking mecha-
nisms such as NDCG. We can draw the compari-
son between the standard and extended kernels in
terms of standard Bing ranking, as well as spe-
cial ranking based on syntactic similarity be-
tween the query and search results (Galitsky et
al., 2013).
We also plan to generalize extended tree ker-
nels towards graphs (DAGs) (Suzuki et al.,
2003). In this case we can perform learning on
Parse thickets (Galitsky et al., 2013) ‒ the struc-
tures which are the sets of parse trees for a para-
graph. It will be fruitful to compare performanc-
es of various ways of kernel computation and
estimate the contribution of a particular way of
paragraph representation to the quality of classi-
fication.
It is possible to apply the outlined approach to
perform question answering in the case where
the latter are extensive portions of paragraph-
sized text and the former include multiple sen-
tences.
Another obvious direction is applying tree
kernels to classify short texts based on standard
corpus data. However, a corpus of short texts,
where advantages of kernel methods over alter-
natives would become visible, does not exist.
One of our next tasks is to form such a corpus.
</bodyText>
<sectionHeader confidence="0.998295" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9988405">
We would like to thank Baidu for travel and con-
ference support for this paper.
</bodyText>
<page confidence="0.997456">
61
</page>
<note confidence="0.747758">
References Haussler, D. 1999. Convolution kernels on discrete
structures.
</note>
<reference confidence="0.999542058252427">
Cumby, C., Roth, D. 2003. Kernel methods for rela-
tional learning. In: ICML.
Kim, Jung-Jae, Pezik, P. and Rebholz-Schuhmann, D.
2008. MedEvi: Retrieving textual evidence of rela-
tions between biomedical concepts from Medline.
Bioinformatics. Volume 24, Issue 11 pp. 1410-
1412.
Zelenko, D., Aone, C., Richardella, A. 2003. Kernel
methods for relation extraction. JMLR (2003).
Suzuki, J., Hirao, H., Sasaki, Y and Maeda, E., Hier-
archical Directed Acyclic Graph Kernel: Methods
for Structured Natural Language Data. In Proceed-
ings of the 41th Annual Meeting of Association for
Computational Linguistics (ACL). 2003.
Galitsky, B. Natural Language Question Answering
System: Technique of Semantic Headers. Advanced
Knowledge International, Australia (2003).
Galitsky, B., de la Rosa, J., Dobrocsi, G. 2012. Infer-
ring the semantic properties of sentences by mining
syntactic parse trees. Data &amp; Knowledge Engineer-
ing. Volume 81-82, November (2012) 21-45.
Galitsky, B., Usikov, D. Kuznetsov, S. 2013. Parse
Thicket Representations for Answering Multi-
sentence questions. 20th International Conference
on Conceptual Structures, ICCS 2013.
Galitsky, B., Kuznetsov, S. 2008. Learning communi-
cative actions of conflicting human agents. J. Exp.
Theor. Artif. Intell. 20(4): 277-317.
Galitsky, B. 2012. Machine Learning of Syntactic
Parse Trees for Search and Classification of Text.
Engineering Application of AI,
http://dx.doi.org/10.1016/j.engappai.2012.09.017.
Galitsky, B., Ilvovsky, D., Kuznetsov, S., Strok, F.
2013. Improving Text Retrieval Efficiency with
Pattern Structures on Parse Thickets, Workshop
&amp;quot;Formal Concept Analysis meets Information Re-
trieval&amp;quot; at ECIR 2013, Moscow, Russia.
Ehrlich H.-C., Rarey M. 2011. Maximum common
subgraph isomorphism algorithms and their appli-
cations in molecular science: review. Wiley Inter-
disciplinary Reviews: Computational Molecular
Science, 2011, vol. 1 (1), pp. 68-79.
Yan, X., Han, J. 2002. gSpan: Graph-Based Substruc-
ture Pattern Mining. In: Proc. IEEE Int. Conf. on
Data Mining, ICDM’02, IEEE Computer Society
(2002), pp 721–724.
Jiangning Wu, Zhaoguo Xuan and Donghua Pan, En-
hancing text representation for classification tasks
with semantic graph structures, International Jour-
nal of Innovative Computing, Information and
Control (ICIC), Volume 7, Number 5(B).
Moschitti, A. 2006. Efficient Convolution Kernels for
Dependency and Constituent Syntactic Trees. In
Proceedings of the 17th European Conference on
Machine Learning, Berlin, Germany.
Severyn, A., Moschitti, A. 2012. Structural relation-
ships for large-scale learning of answer re-
ranking. SIGIR 2012: 741-750.
Severyn, A., Moschitti, A. 2012. Fast Support Vector
Machines for Convolution Tree Kernels. Data Min-
ing Knowledge Discovery 25: 325-357.
Aiolli, F., Da San Martino, G., Sperduti, A. and Mos-
chitti, A. 2007. Efficient Kernel-based Learning for
Trees, Proceeding of the IEEE Symposium on
Computational Intelligence and Data Mining
(CIDM), Honolulu, Hawaii.
Punyakanok, V., Roth, D., &amp; Yih, W. 2004. Mapping
dependencies trees: an application to question an-
swering. In: Proceedings of AI &amp; Math, Florida,
USA.
Mann, William C., Christian M. I. M. Matthiessen
and Sandra A. Thompson. 1992. Rhetorical Struc-
ture Theory and Text Analysis. Discourse Descrip-
tion: Diverse linguistic analyses of a fund-raising
text. ed. by W. C. Mann and S. A. Thompson. Am-
sterdam, John Benjamins: 39-78.
Sun, J., Min Zhang, Chew Lim Tan. 2011. Tree Se-
quence Kernel for Natural Language. AAAI-25,
2011.
Zhang, M.; Che, W.; Zhou, G.; Aw, A.; Tan, C.; Liu,
T.; and Li, S. 2008. Semantic role labeling using a
grammar-driven convolution tree kernel. IEEE
transactions on audio, speech, and language pro-
cessing 16(7):1315–1329.
Montaner, M.; Lopez, B.; de la Rosa, J. L. (June
2003). A Taxonomy of Recommender Agents on the
Internet. Artificial Intelligence Review 19 (4):
285–330.
Collins, M., and Duffy, N. 2002. Convolution kernels
for natural language. In Proceedings of NIPS,
625–632, 2002.
Heeyoung Lee, Angel Chang, Yves Peirsman, Na-
thanael Chambers, Mihai Surdeanu and Dan Juraf-
sky. 2013. Deterministic coreference resolution
based on entity-centric, precision-ranked rules.
Computational Linguistics 39(4).
Daniel Jurafsky, James H. Martin. 2008. Speech and
Language Processing. An Introduction to Natural
Language Processing, Computational Linguistics,
and Speech Recognition.
Robinson J.A. 1965. A machine-oriented logic based
on the resolution principle. Journal of the Associa-
tion for Computing Machinery, 12:23-41.
</reference>
<page confidence="0.980802">
62
</page>
<reference confidence="0.999903323076923">
Mill, J.S. 1843. A system of logic, ratiocinative and
inductive. London.
Fukunaga, K. Introduction to statistical pattern
recognition (2nd ed.), Academic Press Profession-
al, Inc., San Diego, CA, 1990.
Mitchell, T. 1997. Machine Learning. McGraw Hill.
Furukawa, K. 1998. From Deduction to Induction:
Logical Perspective. The Logic Programming Par-
adigm. In Apt, K.R., Marek V.W., Truszczynski,
M., Warren, D.S., Eds. Springer.
Bharat Bhasker; K. Srikumar. 2010. Recommender
Systems in E-Commerce. CUP. ISBN 978-0-07-
068067-8.
Trias i Mansilla, A., JL de la Rosa i Esteva. 2012.
Asknext: An Agent Protocol for Social Search. In-
formation Sciences 190, 144–161.
Punyakanok, V.,Roth, D. and Yih, W. 2005. The Ne-
cessity of Syntactic Parsing for Semantic Role La-
beling. IJCAI-05.
Domingos P. and Poon, H. 2009. Unsupervised Se-
mantic Parsing, In: Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, Singapore: ACL.
Marcu, D. 1997. From Discourse Structures to Text
Summaries, in I. Mani and M.Maybury (eds) Pro-
ceedings of ACL Workshop on Intelligent Scalable
Text Summarization, pp. 82–8, Madrid, Spain.
Abney, S. 1991. Parsing by Chunks, Principle-Based
Parsing, Kluwer Academic Publishers, 1991, pp.
257-278.
Hyeran Byun, Seong-Whan Lee. 2002. Applications
of Support Vector Machines for Pattern Recogni-
tion: A Survey. In Proceedings of the First Interna-
tional Workshop on Pattern Recognition with Sup-
port Vector Machines (SVM &apos;02), Seong-Whan
Lee and Alessandro Verri (Eds.). Springer-Verlag,
London, UK, UK, 213-236.
Chris Manning and Hinrich Schütze, Foundations of
Statistical Natural Language Processing, MIT
Press. Cambridge, MA: May 1999.
Sun, J.; Zhang, M.; and Tan, C. 2010. Exploring syn-
tactic structural features for sub-tree alignment us-
ing bilingual tree kernels. In Proceedings of ACL,
306–315.
Kohavi, Ron. 1995. A Study of Cross-Validation and
Bootstrap for Accuracy Estimation and Model Se-
lection. International Joint Conference on Artificial
Intelligence IJCAI 1995.
Kalervo Jarvelin, Jaana Kekalainen. 2002. Cumulated
gain-based evaluation of IR techniques. ACM
Transactions on Information Systems 20(4), 422–
446.
Roger Levy and Galen Andrew, Tregex and Tsur-
geon: tools for querying and manipulating tree data
structures. 5th International Conference on Lan-
guage Resources and Evaluation (LREC 2006),
2006.
Heeyoung Lee, Marta Recasens, Angel Chang, Mihai
Surdeanu, and Dan Jurafsky. 2012. Joint Entity and
Event Coreference Resolution across Documents.
In EMNLP-CoNLL 2012.
Marta Recasens, Marie-Catherine de Marneffe, and
Christopher Potts. 2013. The Life and Death of
Discourse Entities: Identifying Singleton Mentions.
In Proceedings of NAACL 2013.
</reference>
<page confidence="0.999462">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911822">
<title confidence="0.998905">Going beyond sentences when applying tree kernels</title>
<author confidence="0.947794">Dmitry</author>
<affiliation confidence="0.9960745">School of Applied Mathematics and Information National Research University Higher School of</affiliation>
<address confidence="0.98705">Moscow, Russia</address>
<email confidence="0.998051">dilvovsky@hse.ru</email>
<abstract confidence="0.999249083333334">We go beyond the level of individual sentences applying parse tree kernels to paragraphs. We build a set of extended trees for a paragraph of text from the individual parse trees for sentences and learn short texts such as search results and social profile postings to take advantage of additional discourse-related information. Extension is based on coreferences and rhetoric structure relations between the phrases in different sentences. We evaluate our approach, tracking relevance classification improvement for multi-sentence search task. The search problem is formulated as classification of search results into the classes of relevant and irrelevant, learning from the Bing search results. We compare performances of individual sentence kernels with the ones for extended parse trees and show that adding discourse information to learning data helps to improve classification results.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C Cumby</author>
<author>D Roth</author>
</authors>
<title>Kernel methods for relational learning. In:</title>
<date>2003</date>
<publisher>ICML.</publisher>
<contexts>
<context position="1908" citStr="Cumby and Roth, 2003" startWordPosition="288" endWordPosition="291">es for automated learning of syntactic structures is still an art. One of the solutions to systematically treat these syntactic features ‒ tree kernels built over syntactic parse trees. Convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. They have found a number of applications in several natural language tasks, e.g. syntactic parsing re-ranking, relation extraction (Zelenko et al., 2003; Zhang et al 2006), named entity recognition (Cumby and Roth, 2003) and Semantic Role Labeling (Moschitti, 2004), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009). The kernel ability to generate large feature sets is useful to quickly model new and not well understood linguistic phenomena in learning machines. However, it is often possible to manually design features for linear kernels that produce high accuracy and fast computation time whereas the complexity of tree kernels may prevent their application in real scenarios. Many learning algorithms, such as SVM (Vapnik, 1998) ca</context>
</contexts>
<marker>Cumby, Roth, 2003</marker>
<rawString>Cumby, C., Roth, D. 2003. Kernel methods for relational learning. In: ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jung-Jae Kim</author>
<author>P Pezik</author>
<author>D Rebholz-Schuhmann</author>
</authors>
<title>MedEvi: Retrieving textual evidence of relations between biomedical concepts from Medline.</title>
<date>2008</date>
<journal>Bioinformatics. Volume 24, Issue</journal>
<volume>11</volume>
<pages>1410--1412</pages>
<marker>Kim, Pezik, Rebholz-Schuhmann, 2008</marker>
<rawString>Kim, Jung-Jae, Pezik, P. and Rebholz-Schuhmann, D. 2008. MedEvi: Retrieving textual evidence of relations between biomedical concepts from Medline. Bioinformatics. Volume 24, Issue 11 pp. 1410-1412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zelenko</author>
<author>C Aone</author>
<author>A Richardella</author>
</authors>
<title>Kernel methods for relation extraction. JMLR</title>
<date>2003</date>
<contexts>
<context position="1840" citStr="Zelenko et al., 2003" startWordPosition="277" endWordPosition="280">tics, it is not available yet. Hence the design of syntactic features for automated learning of syntactic structures is still an art. One of the solutions to systematically treat these syntactic features ‒ tree kernels built over syntactic parse trees. Convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. They have found a number of applications in several natural language tasks, e.g. syntactic parsing re-ranking, relation extraction (Zelenko et al., 2003; Zhang et al 2006), named entity recognition (Cumby and Roth, 2003) and Semantic Role Labeling (Moschitti, 2004), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009). The kernel ability to generate large feature sets is useful to quickly model new and not well understood linguistic phenomena in learning machines. However, it is often possible to manually design features for linear kernels that produce high accuracy and fast computation time whereas the complexity of tree kernels may prevent their application in rea</context>
</contexts>
<marker>Zelenko, Aone, Richardella, 2003</marker>
<rawString>Zelenko, D., Aone, C., Richardella, A. 2003. Kernel methods for relation extraction. JMLR (2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Hirao</author>
<author>Y Sasaki</author>
<author>E Maeda</author>
</authors>
<title>Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41th Annual Meeting of Association for Computational Linguistics (ACL).</booktitle>
<contexts>
<context position="22317" citStr="Suzuki et al., 2003" startWordPosition="3620" endWordPosition="3623">the recall of a tree querying system would dramatically increase, and dependence on how phrases are distributed through sentences would decrease. There are a few possible directions of future development. One interesting continuation of this study is to applying standard ranking mechanisms such as NDCG. We can draw the comparison between the standard and extended kernels in terms of standard Bing ranking, as well as special ranking based on syntactic similarity between the query and search results (Galitsky et al., 2013). We also plan to generalize extended tree kernels towards graphs (DAGs) (Suzuki et al., 2003). In this case we can perform learning on Parse thickets (Galitsky et al., 2013) ‒ the structures which are the sets of parse trees for a paragraph. It will be fruitful to compare performances of various ways of kernel computation and estimate the contribution of a particular way of paragraph representation to the quality of classification. It is possible to apply the outlined approach to perform question answering in the case where the latter are extensive portions of paragraphsized text and the former include multiple sentences. Another obvious direction is applying tree kernels to classify </context>
</contexts>
<marker>Suzuki, Hirao, Sasaki, Maeda, 2003</marker>
<rawString>Suzuki, J., Hirao, H., Sasaki, Y and Maeda, E., Hierarchical Directed Acyclic Graph Kernel: Methods for Structured Natural Language Data. In Proceedings of the 41th Annual Meeting of Association for Computational Linguistics (ACL). 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
</authors>
<title>Natural Language Question Answering System: Technique of Semantic Headers. Advanced Knowledge International,</title>
<date>2003</date>
<marker>Galitsky, 2003</marker>
<rawString>Galitsky, B. Natural Language Question Answering System: Technique of Semantic Headers. Advanced Knowledge International, Australia (2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
<author>de la Rosa</author>
<author>J Dobrocsi</author>
<author>G</author>
</authors>
<title>Inferring the semantic properties of sentences by mining syntactic parse trees.</title>
<date>2012</date>
<journal>Data &amp; Knowledge Engineering.</journal>
<volume>Volume</volume>
<pages>81--82</pages>
<contexts>
<context position="12920" citStr="Galitsky et al., 2012" startWordPosition="2123" endWordPosition="2126">y using extended parse tree, as long as the evaluation settings of question answering are identical. The details of the evaluation are given in Section 7. 5 Implementation of kernel learning for extended trees The evaluation framework described here is implemented as an OpenNLP contribution. It relies on the following systems: • OpenNLP/Stanford NLP parser; • Stanford NLP Coreference; • Bing search; • Wrapper of TK-Light kernel learner (Moschitti, 2006). Framework includes the following components of Apache OpenNLP.similarity project: • Rhetoric parser • Parse thicket builder and generalizer (Galitsky et al., 2012). Not used in this evaluation. • A number of applications based on the above component, including search (request handler for SOLR), speech recognition, content generation and others. One of the use cases of this OpenNLP.similarity component is a Java wrapper for tree kernel algorithms implemented in C++. It allows seamless integration of tree kernel algorithms into other open source systems available in Java for search, information retrieval and machine learning. Moreover, tree kernel algorithms can be embedded into Hadoop framework in the domains where offline performance is essential. Libra</context>
</contexts>
<marker>Galitsky, Rosa, Dobrocsi, G, 2012</marker>
<rawString>Galitsky, B., de la Rosa, J., Dobrocsi, G. 2012. Inferring the semantic properties of sentences by mining syntactic parse trees. Data &amp; Knowledge Engineering. Volume 81-82, November (2012) 21-45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
<author>D Kuznetsov Usikov</author>
<author>S</author>
</authors>
<title>Parse Thicket Representations for Answering Multisentence questions.</title>
<date>2013</date>
<booktitle>20th International Conference on Conceptual Structures, ICCS</booktitle>
<contexts>
<context position="11266" citStr="Galitsky et al., 2013" startWordPosition="1858" endWordPosition="1861">ions between the nodes of parse tree for a pair of sentences. Output: the exhaustive set of extended trees E. Set E = ; For each tree i=1:|T| For each relation Rijk, k= 1: |R| Obtain Tj Form the pair of extended trees Ti * Tj; Verify that each of the extended trees do not have a super-tree in E If verified, add to E; Return E. 58 To obtain the inter-sentence links, we employed the following sources: • Coreferences from Stanford NLP (Recasens et al., 2013, Lee et al., 2013). • Rhetoric relation extractor based on the rule-based approach to finding relations between elementary discourse units (Galitsky et al., 2013). We combined manual rules with automatically learned derived from the available discourse corpus by means of syntactic generalization. 4 Assessment of classification improvement To confirm that using a set of extended parse trees for paragraphs leverages additional semantic information compared to a set of parse trees for all sentences in a paragraph, we perform an evaluation of relevance in search domain: • As a baseline, we take all trees for sentences in paragraphs • As an expected improvement, we take all extended trees in a paragraph. Since a benchmarking database for answering complex m</context>
<context position="15244" citStr="Galitsky et al., 2013" startWordPosition="2490" endWordPosition="2493">tion, we need a corpus including a high number of short texts similar to our example in Introduction. These texts should have high similarity (otherwise keyword approach would do well), certain discourse structure, and describe some objects (products) in a meaningful application domain. Unfortunately, to the best of our knowledge such corpus is not available. Therefore, for comparison of tree kernel performances we decided to use search results, given the query which is a short text. We rely on search engine APIs following the evaluation settings in the studies on answering complex questions (Galitsky et al., 2013). Search results typically include texts of fairly high similarity, which is leveraged in our evaluation. To formulate classification problem on the set of texts obtained as search results, we need to form positive and negative sets. To do that, we select the first n search results as relevant (positive) and also n results towards to tail of search results lists as irrelevant (negative). In this case each search session yields an individual training (and evaluation) dataset. The same nature of such data allows averaging of precision and recall, having individual training dataset of a limited s</context>
<context position="17678" citStr="Galitsky et al., 2013" startWordPosition="2888" endWordPosition="2891">ilable). We select 1..20 (or first 20%) of search results as a positive set, and 81..100 as a negative set. Search results 21..80 form the basis of evaluation dataset, from which we randomly select 10 texts to be classified into the classes of positive or negative. Hence we have the ratio 4:1 between the training and evaluation datasets. To motivate our evaluation setting, we rely on the following observations. In case of searching for complex multi-sentence queries, relevance indeed drops abruptly with proceeding from the first 10-20 search results, as search evaluation results demonstrated (Galitsky et al., 2013). The order of search results in first 20% and last 20% does not affect our evaluation. Although the last 20% of search results is not really a “gold standard”, it is nevertheless a set that can be reasonably separated from the positive set. If such separation is too easy or too difficult, it would be hard to adequately evaluate the difference between regular parse trees and extended trees for text classification. Search-based approach to collect texts for evaluation of classification allows reaching maximum degree of experiment automation. It turned out that the use of tail search results as </context>
<context position="22223" citStr="Galitsky et al., 2013" startWordPosition="3604" endWordPosition="3607">ying and manipulation. Extending parse trees of individual sentences towards paragraph of text, the recall of a tree querying system would dramatically increase, and dependence on how phrases are distributed through sentences would decrease. There are a few possible directions of future development. One interesting continuation of this study is to applying standard ranking mechanisms such as NDCG. We can draw the comparison between the standard and extended kernels in terms of standard Bing ranking, as well as special ranking based on syntactic similarity between the query and search results (Galitsky et al., 2013). We also plan to generalize extended tree kernels towards graphs (DAGs) (Suzuki et al., 2003). In this case we can perform learning on Parse thickets (Galitsky et al., 2013) ‒ the structures which are the sets of parse trees for a paragraph. It will be fruitful to compare performances of various ways of kernel computation and estimate the contribution of a particular way of paragraph representation to the quality of classification. It is possible to apply the outlined approach to perform question answering in the case where the latter are extensive portions of paragraphsized text and the form</context>
</contexts>
<marker>Galitsky, Usikov, S, 2013</marker>
<rawString>Galitsky, B., Usikov, D. Kuznetsov, S. 2013. Parse Thicket Representations for Answering Multisentence questions. 20th International Conference on Conceptual Structures, ICCS 2013.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
<author>S Kuznetsov</author>
</authors>
<title>Learning communicative actions of conflicting human agents.</title>
<date>2008</date>
<journal>J. Exp. Theor. Artif. Intell.</journal>
<volume>20</volume>
<issue>4</issue>
<pages>277--317</pages>
<marker>Galitsky, Kuznetsov, 2008</marker>
<rawString>Galitsky, B., Kuznetsov, S. 2008. Learning communicative actions of conflicting human agents. J. Exp. Theor. Artif. Intell. 20(4): 277-317.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
</authors>
<title>Machine Learning of Syntactic Parse Trees for Search and Classification of Text. Engineering Application of AI,</title>
<date>2012</date>
<location>http://dx.doi.org/10.1016/j.engappai.2012.09.017.</location>
<marker>Galitsky, 2012</marker>
<rawString>Galitsky, B. 2012. Machine Learning of Syntactic Parse Trees for Search and Classification of Text. Engineering Application of AI, http://dx.doi.org/10.1016/j.engappai.2012.09.017.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Galitsky</author>
<author>D Ilvovsky</author>
<author>S Kuznetsov</author>
<author>F Strok</author>
</authors>
<title>Improving Text Retrieval Efficiency with Pattern Structures on Parse Thickets, Workshop &amp;quot;Formal Concept Analysis meets Information Retrieval&amp;quot; at ECIR</title>
<date>2013</date>
<location>Moscow, Russia.</location>
<contexts>
<context position="11266" citStr="Galitsky et al., 2013" startWordPosition="1858" endWordPosition="1861">ions between the nodes of parse tree for a pair of sentences. Output: the exhaustive set of extended trees E. Set E = ; For each tree i=1:|T| For each relation Rijk, k= 1: |R| Obtain Tj Form the pair of extended trees Ti * Tj; Verify that each of the extended trees do not have a super-tree in E If verified, add to E; Return E. 58 To obtain the inter-sentence links, we employed the following sources: • Coreferences from Stanford NLP (Recasens et al., 2013, Lee et al., 2013). • Rhetoric relation extractor based on the rule-based approach to finding relations between elementary discourse units (Galitsky et al., 2013). We combined manual rules with automatically learned derived from the available discourse corpus by means of syntactic generalization. 4 Assessment of classification improvement To confirm that using a set of extended parse trees for paragraphs leverages additional semantic information compared to a set of parse trees for all sentences in a paragraph, we perform an evaluation of relevance in search domain: • As a baseline, we take all trees for sentences in paragraphs • As an expected improvement, we take all extended trees in a paragraph. Since a benchmarking database for answering complex m</context>
<context position="15244" citStr="Galitsky et al., 2013" startWordPosition="2490" endWordPosition="2493">tion, we need a corpus including a high number of short texts similar to our example in Introduction. These texts should have high similarity (otherwise keyword approach would do well), certain discourse structure, and describe some objects (products) in a meaningful application domain. Unfortunately, to the best of our knowledge such corpus is not available. Therefore, for comparison of tree kernel performances we decided to use search results, given the query which is a short text. We rely on search engine APIs following the evaluation settings in the studies on answering complex questions (Galitsky et al., 2013). Search results typically include texts of fairly high similarity, which is leveraged in our evaluation. To formulate classification problem on the set of texts obtained as search results, we need to form positive and negative sets. To do that, we select the first n search results as relevant (positive) and also n results towards to tail of search results lists as irrelevant (negative). In this case each search session yields an individual training (and evaluation) dataset. The same nature of such data allows averaging of precision and recall, having individual training dataset of a limited s</context>
<context position="17678" citStr="Galitsky et al., 2013" startWordPosition="2888" endWordPosition="2891">ilable). We select 1..20 (or first 20%) of search results as a positive set, and 81..100 as a negative set. Search results 21..80 form the basis of evaluation dataset, from which we randomly select 10 texts to be classified into the classes of positive or negative. Hence we have the ratio 4:1 between the training and evaluation datasets. To motivate our evaluation setting, we rely on the following observations. In case of searching for complex multi-sentence queries, relevance indeed drops abruptly with proceeding from the first 10-20 search results, as search evaluation results demonstrated (Galitsky et al., 2013). The order of search results in first 20% and last 20% does not affect our evaluation. Although the last 20% of search results is not really a “gold standard”, it is nevertheless a set that can be reasonably separated from the positive set. If such separation is too easy or too difficult, it would be hard to adequately evaluate the difference between regular parse trees and extended trees for text classification. Search-based approach to collect texts for evaluation of classification allows reaching maximum degree of experiment automation. It turned out that the use of tail search results as </context>
<context position="22223" citStr="Galitsky et al., 2013" startWordPosition="3604" endWordPosition="3607">ying and manipulation. Extending parse trees of individual sentences towards paragraph of text, the recall of a tree querying system would dramatically increase, and dependence on how phrases are distributed through sentences would decrease. There are a few possible directions of future development. One interesting continuation of this study is to applying standard ranking mechanisms such as NDCG. We can draw the comparison between the standard and extended kernels in terms of standard Bing ranking, as well as special ranking based on syntactic similarity between the query and search results (Galitsky et al., 2013). We also plan to generalize extended tree kernels towards graphs (DAGs) (Suzuki et al., 2003). In this case we can perform learning on Parse thickets (Galitsky et al., 2013) ‒ the structures which are the sets of parse trees for a paragraph. It will be fruitful to compare performances of various ways of kernel computation and estimate the contribution of a particular way of paragraph representation to the quality of classification. It is possible to apply the outlined approach to perform question answering in the case where the latter are extensive portions of paragraphsized text and the form</context>
</contexts>
<marker>Galitsky, Ilvovsky, Kuznetsov, Strok, 2013</marker>
<rawString>Galitsky, B., Ilvovsky, D., Kuznetsov, S., Strok, F. 2013. Improving Text Retrieval Efficiency with Pattern Structures on Parse Thickets, Workshop &amp;quot;Formal Concept Analysis meets Information Retrieval&amp;quot; at ECIR 2013, Moscow, Russia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H-C Ehrlich</author>
<author>M Rarey</author>
</authors>
<title>Maximum common subgraph isomorphism algorithms and their applications in molecular science: review.</title>
<date>2011</date>
<journal>Wiley Interdisciplinary Reviews: Computational Molecular Science,</journal>
<volume>1</volume>
<issue>1</issue>
<pages>68--79</pages>
<marker>Ehrlich, Rarey, 2011</marker>
<rawString>Ehrlich H.-C., Rarey M. 2011. Maximum common subgraph isomorphism algorithms and their applications in molecular science: review. Wiley Interdisciplinary Reviews: Computational Molecular Science, 2011, vol. 1 (1), pp. 68-79.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yan</author>
<author>J Han</author>
</authors>
<title>gSpan: Graph-Based Substructure Pattern Mining. In:</title>
<date>2002</date>
<booktitle>Proc. IEEE Int. Conf. on Data Mining, ICDM’02, IEEE Computer Society</booktitle>
<pages>721--724</pages>
<marker>Yan, Han, 2002</marker>
<rawString>Yan, X., Han, J. 2002. gSpan: Graph-Based Substructure Pattern Mining. In: Proc. IEEE Int. Conf. on Data Mining, ICDM’02, IEEE Computer Society (2002), pp 721–724.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jiangning Wu</author>
</authors>
<title>Zhaoguo Xuan and Donghua Pan, Enhancing text representation for classification tasks with semantic graph structures,</title>
<date></date>
<journal>International Journal of Innovative Computing, Information and Control (ICIC),</journal>
<volume>7</volume>
<location>Number</location>
<marker>Wu, </marker>
<rawString>Jiangning Wu, Zhaoguo Xuan and Donghua Pan, Enhancing text representation for classification tasks with semantic graph structures, International Journal of Innovative Computing, Information and Control (ICIC), Volume 7, Number 5(B).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Moschitti</author>
</authors>
<title>Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees.</title>
<date>2006</date>
<booktitle>In Proceedings of the 17th European Conference on Machine Learning,</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="12755" citStr="Moschitti, 2006" startWordPosition="2102" endWordPosition="2103">se of this evaluation it is not essential to provide the best possible set of answers. Instead, we are concerned with the comparison of relevance improvement by using extended parse tree, as long as the evaluation settings of question answering are identical. The details of the evaluation are given in Section 7. 5 Implementation of kernel learning for extended trees The evaluation framework described here is implemented as an OpenNLP contribution. It relies on the following systems: • OpenNLP/Stanford NLP parser; • Stanford NLP Coreference; • Bing search; • Wrapper of TK-Light kernel learner (Moschitti, 2006). Framework includes the following components of Apache OpenNLP.similarity project: • Rhetoric parser • Parse thicket builder and generalizer (Galitsky et al., 2012). Not used in this evaluation. • A number of applications based on the above component, including search (request handler for SOLR), speech recognition, content generation and others. One of the use cases of this OpenNLP.similarity component is a Java wrapper for tree kernel algorithms implemented in C++. It allows seamless integration of tree kernel algorithms into other open source systems available in Java for search, informatio</context>
<context position="19529" citStr="Moschitti, 2006" startWordPosition="3181" endWordPosition="3182">extracted from webpages. For the snippets, we split them into sentence fragments and built extended trees for these fragments of sentences. For original texts, we extracted all sentences related to the snippet fragments and built extended trees for these sentences. Training and classification occurs in the automated mode, and the classification assessment is 60 conducted by the members of research group guided by the authors. The assessors only consulted the query and answer snippets. We used the standard parameters of tree sequence kernels from http://disi.unitn.it/moschitti/Tree-Kernel.htm (Moschitti, 2006). Tree kernel is applied to all tree pairs from two forests. The latest version of tree kernel learner was obtained from the author. Products Basic Extended kerkernels nels (corefs+RST) Texts Precision 0,5679 0,5868 from the pages Recall 0,7516 0,8458 F-measure 0,6485 0,6752 Snippets Precision 0,5625 0,6319 Recall 0,7840 0,8313 F-measure 0,6169 0,6695 Table 1: Evaluation results for products domain Answers Basic Extended Extended kernels kernels kernels (corefs) (corefs+ RST) Texts P 0,5167 0,5083 0,5437 from the pages R 0,7361 0,7917 0,8333 F 0,6008 0,5458 0,6278 Snippets P 0,5950 0,6264 0,67</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Moschitti, A. 2006. Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees. In Proceedings of the 17th European Conference on Machine Learning, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Severyn</author>
<author>A Moschitti</author>
</authors>
<title>Structural relationships for large-scale learning of answer reranking. SIGIR</title>
<date>2012</date>
<pages>741--750</pages>
<contexts>
<context position="3498" citStr="Severyn and Moschitti, 2012" startWordPosition="535" endWordPosition="538">f NL tasks require computing of semantic features over paragraphs of text containing multiple sentences. Doing it in a sentence pair-wise manner is not always accurate, since it is strongly dependent on how information (phrases) is distributed through sentences. An approach to build a kernel based on more than a single parse tree has been proposed (Severyn et.al., 2012), however without any relations between parse trees or for a different purpose than treating multi-sentence portions of text. To compensate for parsing errors (Zhang et al., 2008), a convolution kernel over packed parse forest (Severyn and Moschitti, 2012; Aioli et.al, 2007) is used to mine syntactic features from it directly. A packed forest compactly encodes exponential number of n-best parse trees, and thus containing much more rich structured features than a single parse tree. This advantage enables the forest kernel not only to be more robust against parsing errors, but also to be able to learn more reliable feature values and help to 56 Proceedings of the ACL 2014 Student Research Workshop, pages 56–63, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics solve the data sparseness issue that exists i</context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Severyn, A., Moschitti, A. 2012. Structural relationships for large-scale learning of answer reranking. SIGIR 2012: 741-750.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Severyn</author>
<author>A Moschitti</author>
</authors>
<title>Fast Support Vector Machines for Convolution Tree Kernels.</title>
<date>2012</date>
<journal>Data Mining Knowledge Discovery</journal>
<volume>25</volume>
<pages>325--357</pages>
<contexts>
<context position="3498" citStr="Severyn and Moschitti, 2012" startWordPosition="535" endWordPosition="538">f NL tasks require computing of semantic features over paragraphs of text containing multiple sentences. Doing it in a sentence pair-wise manner is not always accurate, since it is strongly dependent on how information (phrases) is distributed through sentences. An approach to build a kernel based on more than a single parse tree has been proposed (Severyn et.al., 2012), however without any relations between parse trees or for a different purpose than treating multi-sentence portions of text. To compensate for parsing errors (Zhang et al., 2008), a convolution kernel over packed parse forest (Severyn and Moschitti, 2012; Aioli et.al, 2007) is used to mine syntactic features from it directly. A packed forest compactly encodes exponential number of n-best parse trees, and thus containing much more rich structured features than a single parse tree. This advantage enables the forest kernel not only to be more robust against parsing errors, but also to be able to learn more reliable feature values and help to 56 Proceedings of the ACL 2014 Student Research Workshop, pages 56–63, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association for Computational Linguistics solve the data sparseness issue that exists i</context>
</contexts>
<marker>Severyn, Moschitti, 2012</marker>
<rawString>Severyn, A., Moschitti, A. 2012. Fast Support Vector Machines for Convolution Tree Kernels. Data Mining Knowledge Discovery 25: 325-357.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Aiolli</author>
<author>Da San Martino</author>
<author>G Sperduti</author>
<author>A</author>
<author>A Moschitti</author>
</authors>
<title>Efficient Kernel-based Learning for Trees,</title>
<date>2007</date>
<booktitle>Proceeding of the IEEE Symposium on Computational Intelligence and Data Mining (CIDM),</booktitle>
<location>Honolulu, Hawaii.</location>
<marker>Aiolli, Martino, Sperduti, A, Moschitti, 2007</marker>
<rawString>Aiolli, F., Da San Martino, G., Sperduti, A. and Moschitti, A. 2007. Efficient Kernel-based Learning for Trees, Proceeding of the IEEE Symposium on Computational Intelligence and Data Mining (CIDM), Honolulu, Hawaii.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>Mapping dependencies trees: an application to question answering. In:</title>
<date>2004</date>
<booktitle>Proceedings of AI &amp; Math,</booktitle>
<location>Florida, USA.</location>
<marker>Punyakanok, Roth, Yih, 2004</marker>
<rawString>Punyakanok, V., Roth, D., &amp; Yih, W. 2004. Mapping dependencies trees: an application to question answering. In: Proceedings of AI &amp; Math, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Christian M I M Matthiessen</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical Structure Theory and Text Analysis. Discourse Description: Diverse linguistic analyses of a fund-raising text.</title>
<date>1992</date>
<pages>39--78</pages>
<editor>ed. by W. C. Mann and S. A. Thompson.</editor>
<location>Amsterdam, John Benjamins:</location>
<marker>Mann, Matthiessen, Thompson, 1992</marker>
<rawString>Mann, William C., Christian M. I. M. Matthiessen and Sandra A. Thompson. 1992. Rhetorical Structure Theory and Text Analysis. Discourse Description: Diverse linguistic analyses of a fund-raising text. ed. by W. C. Mann and S. A. Thompson. Amsterdam, John Benjamins: 39-78.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sun</author>
<author>Min Zhang</author>
<author>Chew Lim Tan</author>
</authors>
<title>Tree Sequence Kernel for Natural Language.</title>
<date>2011</date>
<marker>Sun, Zhang, Tan, 2011</marker>
<rawString>Sun, J., Min Zhang, Chew Lim Tan. 2011. Tree Sequence Kernel for Natural Language. AAAI-25, 2011.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Zhang</author>
<author>W Che</author>
<author>G Zhou</author>
<author>A Aw</author>
<author>C Tan</author>
<author>T Liu</author>
<author>S Li</author>
</authors>
<title>Semantic role labeling using a grammar-driven convolution tree kernel. IEEE transactions on audio, speech, and language processing 16(7):1315–1329.</title>
<date>2008</date>
<contexts>
<context position="3422" citStr="Zhang et al., 2008" startWordPosition="524" endWordPosition="527">effective solution to modeling structured objects in NLP. A number of NL tasks require computing of semantic features over paragraphs of text containing multiple sentences. Doing it in a sentence pair-wise manner is not always accurate, since it is strongly dependent on how information (phrases) is distributed through sentences. An approach to build a kernel based on more than a single parse tree has been proposed (Severyn et.al., 2012), however without any relations between parse trees or for a different purpose than treating multi-sentence portions of text. To compensate for parsing errors (Zhang et al., 2008), a convolution kernel over packed parse forest (Severyn and Moschitti, 2012; Aioli et.al, 2007) is used to mine syntactic features from it directly. A packed forest compactly encodes exponential number of n-best parse trees, and thus containing much more rich structured features than a single parse tree. This advantage enables the forest kernel not only to be more robust against parsing errors, but also to be able to learn more reliable feature values and help to 56 Proceedings of the ACL 2014 Student Research Workshop, pages 56–63, Baltimore, Maryland USA, June 22-27 2014. c�2014 Association</context>
</contexts>
<marker>Zhang, Che, Zhou, Aw, Tan, Liu, Li, 2008</marker>
<rawString>Zhang, M.; Che, W.; Zhou, G.; Aw, A.; Tan, C.; Liu, T.; and Li, S. 2008. Semantic role labeling using a grammar-driven convolution tree kernel. IEEE transactions on audio, speech, and language processing 16(7):1315–1329.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Montaner</author>
<author>B Lopez</author>
<author>de la Rosa</author>
<author>J L</author>
</authors>
<date>2003</date>
<journal>A Taxonomy of Recommender Agents on the Internet. Artificial Intelligence Review</journal>
<volume>19</volume>
<issue>4</issue>
<pages>285--330</pages>
<marker>Montaner, Lopez, Rosa, L, 2003</marker>
<rawString>Montaner, M.; Lopez, B.; de la Rosa, J. L. (June 2003). A Taxonomy of Recommender Agents on the Internet. Artificial Intelligence Review 19 (4): 285–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Collins</author>
<author>N Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2002</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>625--632</pages>
<contexts>
<context position="1522" citStr="Collins and Duffy, 2002" startWordPosition="226" endWordPosition="229"> search results. We compare performances of individual sentence kernels with the ones for extended parse trees and show that adding discourse information to learning data helps to improve classification results. 1 Introduction In spite of substantial efforts to formulate a complete linking theory between syntax and semantics, it is not available yet. Hence the design of syntactic features for automated learning of syntactic structures is still an art. One of the solutions to systematically treat these syntactic features ‒ tree kernels built over syntactic parse trees. Convolution tree kernel (Collins and Duffy, 2002) defines a feature space consisting of all subtree types of parse trees and counts the number of common subtrees as the syntactic similarity between two parse trees. They have found a number of applications in several natural language tasks, e.g. syntactic parsing re-ranking, relation extraction (Zelenko et al., 2003; Zhang et al 2006), named entity recognition (Cumby and Roth, 2003) and Semantic Role Labeling (Moschitti, 2004), pronoun resolution (Yang et al., 2006), question classification (Zhang and Lee, 2003) and machine translation (Zhang and Li, 2009). The kernel ability to generate larg</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Collins, M., and Duffy, N. 2002. Convolution kernels for natural language. In Proceedings of NIPS, 625–632, 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Angel Chang</author>
<author>Yves Peirsman</author>
<author>Nathanael Chambers</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Deterministic coreference resolution based on entity-centric, precision-ranked rules.</title>
<date>2013</date>
<journal>Computational Linguistics</journal>
<volume>39</volume>
<issue>4</issue>
<contexts>
<context position="11122" citStr="Lee et al., 2013" startWordPosition="1838" endWordPosition="1841">lations R, which includes relations Rijk between the nodes of Ti and Tj: Ti eT, Tj eT, Rijk eR. We use index k to range over multiple relations between the nodes of parse tree for a pair of sentences. Output: the exhaustive set of extended trees E. Set E = ; For each tree i=1:|T| For each relation Rijk, k= 1: |R| Obtain Tj Form the pair of extended trees Ti * Tj; Verify that each of the extended trees do not have a super-tree in E If verified, add to E; Return E. 58 To obtain the inter-sentence links, we employed the following sources: • Coreferences from Stanford NLP (Recasens et al., 2013, Lee et al., 2013). • Rhetoric relation extractor based on the rule-based approach to finding relations between elementary discourse units (Galitsky et al., 2013). We combined manual rules with automatically learned derived from the available discourse corpus by means of syntactic generalization. 4 Assessment of classification improvement To confirm that using a set of extended parse trees for paragraphs leverages additional semantic information compared to a set of parse trees for all sentences in a paragraph, we perform an evaluation of relevance in search domain: • As a baseline, we take all trees for senten</context>
</contexts>
<marker>Lee, Chang, Peirsman, Chambers, Surdeanu, Jurafsky, 2013</marker>
<rawString>Heeyoung Lee, Angel Chang, Yves Peirsman, Nathanael Chambers, Mihai Surdeanu and Dan Jurafsky. 2013. Deterministic coreference resolution based on entity-centric, precision-ranked rules. Computational Linguistics 39(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Jurafsky</author>
<author>James H Martin</author>
</authors>
<title>Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.</title>
<date>2008</date>
<marker>Jurafsky, Martin, 2008</marker>
<rawString>Daniel Jurafsky, James H. Martin. 2008. Speech and Language Processing. An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Robinson</author>
</authors>
<title>A machine-oriented logic based on the resolution principle.</title>
<date>1965</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<pages>12--23</pages>
<marker>Robinson, 1965</marker>
<rawString>Robinson J.A. 1965. A machine-oriented logic based on the resolution principle. Journal of the Association for Computing Machinery, 12:23-41.</rawString>
</citation>
<citation valid="false">
<authors>
<author>J S Mill</author>
</authors>
<title>A system of logic, ratiocinative and inductive.</title>
<location>London.</location>
<marker>Mill, </marker>
<rawString>Mill, J.S. 1843. A system of logic, ratiocinative and inductive. London.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Fukunaga</author>
</authors>
<title>Introduction to statistical pattern recognition (2nd ed.),</title>
<date>1990</date>
<publisher>Academic Press Professional, Inc.,</publisher>
<location>San Diego, CA,</location>
<marker>Fukunaga, 1990</marker>
<rawString>Fukunaga, K. Introduction to statistical pattern recognition (2nd ed.), Academic Press Professional, Inc., San Diego, CA, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Mitchell</author>
</authors>
<title>Machine Learning.</title>
<date>1997</date>
<publisher>McGraw Hill.</publisher>
<marker>Mitchell, 1997</marker>
<rawString>Mitchell, T. 1997. Machine Learning. McGraw Hill.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Furukawa</author>
</authors>
<title>From Deduction to Induction: Logical Perspective. The Logic Programming Paradigm. In</title>
<date>1998</date>
<publisher>Eds. Springer.</publisher>
<location>Apt, K.R., Marek</location>
<marker>Furukawa, 1998</marker>
<rawString>Furukawa, K. 1998. From Deduction to Induction: Logical Perspective. The Logic Programming Paradigm. In Apt, K.R., Marek V.W., Truszczynski, M., Warren, D.S., Eds. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bharat Bhasker</author>
<author>K Srikumar</author>
</authors>
<date>2010</date>
<booktitle>Recommender Systems in E-Commerce. CUP. ISBN</booktitle>
<pages>978--0</pages>
<marker>Bhasker, Srikumar, 2010</marker>
<rawString>Bharat Bhasker; K. Srikumar. 2010. Recommender Systems in E-Commerce. CUP. ISBN 978-0-07-068067-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Trias i Mansilla</author>
<author>A</author>
</authors>
<title>JL de la Rosa i Esteva.</title>
<date>2012</date>
<journal>Information Sciences</journal>
<volume>190</volume>
<pages>144--161</pages>
<marker>Mansilla, A, 2012</marker>
<rawString>Trias i Mansilla, A., JL de la Rosa i Esteva. 2012. Asknext: An Agent Protocol for Social Search. Information Sciences 190, 144–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Punyakanok</author>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>The Necessity of Syntactic Parsing for Semantic Role Labeling.</title>
<date>2005</date>
<marker>Punyakanok, Roth, Yih, 2005</marker>
<rawString>Punyakanok, V.,Roth, D. and Yih, W. 2005. The Necessity of Syntactic Parsing for Semantic Role Labeling. IJCAI-05.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Domingos</author>
<author>H Poon</author>
</authors>
<title>Unsupervised Semantic Parsing, In:</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>ACL.</publisher>
<location>Singapore:</location>
<marker>Domingos, Poon, 2009</marker>
<rawString>Domingos P. and Poon, H. 2009. Unsupervised Semantic Parsing, In: Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, Singapore: ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
</authors>
<title>From Discourse Structures to Text Summaries, in I. Mani and M.Maybury (eds)</title>
<date>1997</date>
<booktitle>Proceedings of ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>82--8</pages>
<location>Madrid,</location>
<marker>Marcu, 1997</marker>
<rawString>Marcu, D. 1997. From Discourse Structures to Text Summaries, in I. Mani and M.Maybury (eds) Proceedings of ACL Workshop on Intelligent Scalable Text Summarization, pp. 82–8, Madrid, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by Chunks, Principle-Based Parsing,</title>
<date>1991</date>
<pages>257--278</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<marker>Abney, 1991</marker>
<rawString>Abney, S. 1991. Parsing by Chunks, Principle-Based Parsing, Kluwer Academic Publishers, 1991, pp. 257-278.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hyeran Byun</author>
<author>Seong-Whan Lee</author>
</authors>
<title>Applications of Support Vector Machines for Pattern Recognition: A Survey.</title>
<date>2002</date>
<booktitle>In Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines (SVM &apos;02), Seong-Whan Lee and Alessandro Verri (Eds.).</booktitle>
<pages>213--236</pages>
<publisher>Springer-Verlag,</publisher>
<location>London, UK, UK,</location>
<marker>Byun, Lee, 2002</marker>
<rawString>Hyeran Byun, Seong-Whan Lee. 2002. Applications of Support Vector Machines for Pattern Recognition: A Survey. In Proceedings of the First International Workshop on Pattern Recognition with Support Vector Machines (SVM &apos;02), Seong-Whan Lee and Alessandro Verri (Eds.). Springer-Verlag, London, UK, UK, 213-236.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Manning</author>
<author>Hinrich Schütze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing,</title>
<date>1999</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<marker>Manning, Schütze, 1999</marker>
<rawString>Chris Manning and Hinrich Schütze, Foundations of Statistical Natural Language Processing, MIT Press. Cambridge, MA: May 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sun</author>
<author>M Zhang</author>
<author>C Tan</author>
</authors>
<title>Exploring syntactic structural features for sub-tree alignment using bilingual tree kernels.</title>
<date>2010</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>306--315</pages>
<marker>Sun, Zhang, Tan, 2010</marker>
<rawString>Sun, J.; Zhang, M.; and Tan, C. 2010. Exploring syntactic structural features for sub-tree alignment using bilingual tree kernels. In Proceedings of ACL, 306–315.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Kohavi</author>
</authors>
<title>A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection.</title>
<date>1995</date>
<booktitle>International Joint Conference on Artificial Intelligence IJCAI</booktitle>
<marker>Kohavi, 1995</marker>
<rawString>Kohavi, Ron. 1995. A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. International Joint Conference on Artificial Intelligence IJCAI 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalervo Jarvelin</author>
<author>Jaana Kekalainen</author>
</authors>
<title>Cumulated gain-based evaluation of IR techniques.</title>
<date>2002</date>
<journal>ACM Transactions on Information Systems</journal>
<volume>20</volume>
<issue>4</issue>
<pages>422--446</pages>
<marker>Jarvelin, Kekalainen, 2002</marker>
<rawString>Kalervo Jarvelin, Jaana Kekalainen. 2002. Cumulated gain-based evaluation of IR techniques. ACM Transactions on Information Systems 20(4), 422– 446.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Levy</author>
<author>Galen Andrew</author>
</authors>
<title>Tregex and Tsurgeon: tools for querying and manipulating tree data structures.</title>
<date>2006</date>
<booktitle>5th International Conference on Language Resources and Evaluation (LREC</booktitle>
<marker>Levy, Andrew, 2006</marker>
<rawString>Roger Levy and Galen Andrew, Tregex and Tsurgeon: tools for querying and manipulating tree data structures. 5th International Conference on Language Resources and Evaluation (LREC 2006), 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heeyoung Lee</author>
<author>Marta Recasens</author>
<author>Angel Chang</author>
<author>Mihai Surdeanu</author>
<author>Dan Jurafsky</author>
</authors>
<title>Joint Entity and Event Coreference Resolution across Documents. In EMNLP-CoNLL</title>
<date>2012</date>
<contexts>
<context position="8188" citStr="Lee et al., 2012" startWordPosition="1298" endWordPosition="1301">nclude the first, second, and third sentence for each paragraph together can serve as a structure to differentiate the two above classes. The dependency parse trees for the first text in our set and its coreferences are shown in Fig. 1. There are multiple ways the nodes from parse trees of different sentences can be connected: we choose the rhetoric relation of elaboration which links the same entity office and helps us to form the structure rent-office-space – for-my-business – deduct-rental-expense which is the base for our classification. We used Stanford Core NLP, coreferences resolution (Lee et al., 2012) and its visualization to form Figs. 1 and 2. Fig. 2 shows the resultant extended tree with the root ‘I’ from the first sentence. It includes the whole first sentence, a verb phrase from the second sentence and a verb phrase from the third sentence according to rhetoric relation of elaboration. Notice that this extended tree can be intuitively viewed as representing the ‘main idea’ of this text compared to other texts in our set. All extended trees need to be formed for a text and 57 then compared with that of the other texts, since we don’t know in advance which extended tree is essential. Fr</context>
</contexts>
<marker>Lee, Recasens, Chang, Surdeanu, Jurafsky, 2012</marker>
<rawString>Heeyoung Lee, Marta Recasens, Angel Chang, Mihai Surdeanu, and Dan Jurafsky. 2012. Joint Entity and Event Coreference Resolution across Documents. In EMNLP-CoNLL 2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marta Recasens</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher Potts</author>
</authors>
<title>The Life and Death of Discourse Entities: Identifying Singleton Mentions.</title>
<date>2013</date>
<booktitle>In Proceedings of NAACL</booktitle>
<marker>Recasens, de Marneffe, Potts, 2013</marker>
<rawString>Marta Recasens, Marie-Catherine de Marneffe, and Christopher Potts. 2013. The Life and Death of Discourse Entities: Identifying Singleton Mentions. In Proceedings of NAACL 2013.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>