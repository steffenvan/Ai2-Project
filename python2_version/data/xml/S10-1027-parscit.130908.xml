<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.054890">
<title confidence="0.883824">
UHD: Cross-Lingual Word Sense Disambiguation Using
Multilingual Co-occurrence Graphs
</title>
<author confidence="0.981929">
Carina Silberer and Simone Paolo Ponzetto
</author>
<affiliation confidence="0.987102">
Department of Computational Linguistics
Heidelberg University
</affiliation>
<email confidence="0.997967">
{silberer,ponzetto}@cl.uni-heidelberg.de
</email>
<sectionHeader confidence="0.994775" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999880538461538">
We describe the University of Heidelberg
(UHD) system for the Cross-Lingual Word
Sense Disambiguation SemEval-2010 task
(CL-WSD). The system performs CL-
WSD by applying graph algorithms pre-
viously developed for monolingual Word
Sense Disambiguation to multilingual co-
occurrence graphs. UHD has participated
in the BEST and out-of-five (OOF) eval-
uations and ranked among the most com-
petitive systems for this task, thus indicat-
ing that graph-based approaches represent
a powerful alternative for this task.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999393125">
This paper describes a graph-based system for
Cross-Lingual Word Sense Disambiguation, i.e.
the task of disambiguating a word in context by
providing its most appropriate translations in dif-
ferent languages (Lefever and Hoste, 2010, CL-
WSD henceforth). Our goal at SemEval-2010 was
to assess whether graph-based approaches, which
have been successfully developed for monolingual
Word Sense Disambiguation, represent a valid
framework for CL-WSD. These typically trans-
form a knowledge resource such as WordNet (Fell-
baum, 1998) into a graph and apply graph algo-
rithms to perform WSD. In our work, we follow
this line of research and apply graph-based meth-
ods to multilingual co-occurrence graphs which
are automatically created from parallel corpora.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999974130434783">
Our method is heavily inspired by previous pro-
posals from V´eronis (2004, Hyperlex) and Agirre
et al. (2006). Hyperlex performs graph-based
WSD based on co-occurrence graphs: given a
monolingual corpus, for each target word a graph
is built where nodes represent content words co-
occurring with the target word in context, and
edges connect the words which co-occur in these
contexts. The second step iteratively selects the
node with highest degree in the graph (root hub)
and removes it along with its adjacent nodes. Each
such selection corresponds to isolating a high-
density component of the graph, in order to select
a sense of the target word. In the last step the root
hubs are linked to the target word and the Mini-
mum Spanning Tree (MST) of the graph is com-
puted to disambiguate the target word in context.
Agirre et al. (2006) compare Hyperlex with an al-
ternative method to detect the root hubs based on
PageRank (Brin and Page, 1998). PageRank has
the advantage of requiring less parameters than
Hyperlex, whereas the authors ascertain equal per-
formance of the two methods.
</bodyText>
<sectionHeader confidence="0.994609" genericHeader="method">
3 Graph-based Cross-Lingual WSD
</sectionHeader>
<bodyText confidence="0.999937357142857">
We start by building for each target word a mul-
tilingual co-occurrence graph based on the target
word’s aligned contexts found in parallel corpora
(Sections 3.1 and 3.2). Multilingual nodes are
linked by translation edges, labeled with the target
word’s translations observed in the corresponding
contexts. We then use an adapted PageRank al-
gorithm to select the nodes which represent the
target word’s different senses (Section 3.3) and,
given these nodes, we compute the MST, which
is used to select the most relevant words in con-
text to disambiguate a given test instance (Section
3.4). Translations are finally given by the incom-
ing translation edges of the selected context words.
</bodyText>
<page confidence="0.980924">
134
</page>
<bodyText confidence="0.450957">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 134–137,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.999488">
3.1 Monolingual Graph
</subsectionHeader>
<bodyText confidence="0.999968583333333">
Let Cs be all contexts of a target word w in
a source language s, i.e. English in our case,
within a (PoS-tagged and lemmatized) monolin-
gual corpus. We first construct a monolingual co-
occurrence graph Gs = (Vs, Es). We collect all
pairs (cwi, cwj) of co-occurring nouns or adjec-
tives in Cs (excluding the target word itself) and
add each word as a node into the initially empty
graph. Each co-occurring word pair is connected
with an edge (vi, vj) E Es, which is assigned a
weight w(vi, vj) based on the strength of associa-
tion between the respective words cwi and cwj:
</bodyText>
<equation confidence="0.993199">
w(vi,vj) = 1—max [p(cwi|cwj),p(cwj|cwi)].
</equation>
<bodyText confidence="0.9993985">
The conditional probability of word cwi given
word cwj is estimated by the number of contexts
in which cwi and cwj co-occur divided by the
number of contexts containing cwj.
</bodyText>
<subsectionHeader confidence="0.999552">
3.2 Multilingual Graph
</subsectionHeader>
<bodyText confidence="0.993147333333333">
Given a set of target languages L, we then ex-
tend Gs to a labeled multilingual graph GML =
(VML, EML) where:
</bodyText>
<listItem confidence="0.988928">
1. VML = VsUUl∈L Vl is a set of nodes represent-
ing content words from either the source (Vs) or
the target (Vl) languages;
2. EML = Es U Ul∈L{El U Es,l1 is a set of
edges. These include (a) co-occurrence edges
El C Vl xVl between nodes representing words
in a target language (Vl), weighted in the same
way as the edges in the monolingual graph;
(b) labeled translation edges Es,l which repre-
sent translations of words from the source lan-
guage into a target language. These edges are
assigned a complex label t E Tw,l compris-
ing a translation of the word w in the target
language l and its frequency of translation, i.e.
Es,l C Vs x Tw,l x Vl.
</listItem>
<bodyText confidence="0.973963357142857">
The multilingual graph is built based on a word-
aligned multilingual parallel corpus and a multi-
lingual dictionary. The pseudocode is presented in
Algorithm 1. We start with the monolingual graph
from the source language (line 1) and then for each
target language l E L in turn, we add the transla-
tion edges (vs, t, vl) E Es,l of each word in the
source language (lines 5-15). In order to include
the information about the translations of w in the
different target languages, each translation edge
Algorithm 1 Multilingual co-occurrence graph.
Input: target word w and its contexts Cs
monolingual graph Gs = (Vs, Es)
set of target languages L
</bodyText>
<listItem confidence="0.978154523809524">
Output: a multilingual graph GML
1: GML = (VML, EML) — Gs = (Vs, Es)
2: for each l E L
3: Vl — 0
4: Cl := aligned sentences of Cs in lang. l
5: for each vs E Vs
6: Tvs,l := translations of vs found in Cl
7: Cvs C Cs := contexts containing w and vs
8: for each translation vl E Tvs,l
9: Cvl := aligned sentences of Cvs in lang. l
10: Tw,Cvl — translation labels of w from Cvl
11: if vl E/ VML then
12: VML — VML U vl
13: Vl — Vl U vl
14: for each t E Tw,Cvl
15: EML — EML U (vs, t, vl)
16: for each vi E Vl
17: for each vj E Vl, i =� j
18: if vi and vj co-occur in Cl then
19: EML — EML U (vi, vj)
20: return GML
</listItem>
<bodyText confidence="0.999220705882353">
(vs, t, vl) receives a translation label t. Formally,
let Cvs C Cs be the contexts where vs and w co-
occur, and Cvl the word-aligned contexts in lan-
guage l of Cvs, where vs is translated as vl. Then
each edge between nodes vs and vl is labeled with
a translation label t (lines 14-15): this includes a
translation of w in Cvl, its frequency of transla-
tion and the information of whether the transla-
tion is monosemous, as found in a multilingual
dictionary, i.e. EuroWordNet (Vossen, 1998) and
PanDictionary (Mausam et al., 2009). Finally, the
multilingual graph is further extended by inserting
all possible co-occurrence edges (vi, vj) E El be-
tween the nodes for the target language l (lines 16-
19, i.e. we apply the step from Section 3.1 to l and
Cl). As a result of the algorithm, the multilingual
graph is returned (line 20).
</bodyText>
<subsectionHeader confidence="0.998702">
3.3 Computing Root Hubs
</subsectionHeader>
<bodyText confidence="0.999213">
We compute the root hubs in the multilingual
graph to discriminate the senses of the target word
in the source language. Hubs are found using the
adapted PageRank from Agirre et al. (2006):
</bodyText>
<page confidence="0.918974">
135
</page>
<equation confidence="0.968128333333333">
�
PR(vi) = (1 − d) + d
j∈deg(vi)
</equation>
<bodyText confidence="0.998955689655172">
where d is the so-called damping factor (typically
set to 0.85), deg(vi) is the number of adjacent
nodes of node vi and wij is the weight of the co-
occurrence edge between nodes vi and vj.
Since this step aims to induce the senses for
the target word, only nodes referring to words
in English can become root hubs. However, in
order to use additional evidence from other lan-
guages, we furthermore include in the computa-
tion of PageRank co-occurrence edges from the
target languages, as long as these occur in con-
texts with ‘safe’, i.e. monosemous, translations of
the target word. Given an English co-occurrence
edge (vs,i, vs,j) and translation edges (vs,i, vl,i)
and (vs,j, vl,j) to nodes in the target language
l, labeled with monosemous translations, we in-
clude the co-occurrence edge (vl,i, vl,j) in the
PageRank computation. For instance, animal and
biotechnology are translated in German as Tier
and Biotechnologie, both with edges labeled with
the monosemous Pflanze: accordingly, we in-
clude the edge (Tier, Biotechnologie) in the com-
putation of PR(vi), where vi is either animal or
biotechnology.
Finally, following V´eronis (2004), a MST is
built with the target word as its root and the root
hubs of GML forming its first level. By using a
multilingual graph, we are able to obtain MSTs
which contain translation nodes and edges.
</bodyText>
<subsectionHeader confidence="0.949857">
3.4 Multilingual Disambiguation
</subsectionHeader>
<bodyText confidence="0.9999973">
Given a context W for the target word w in the
source language, we use the MST to find the most
relevant words in W for disambiguating w. We
first map each content word cw ∈ W to nodes
in the MST. Since each word is dominated by ex-
actly one hub, we can find the relevant nodes by
computing the correct hub disHub (i.e. sense) and
then only retain those nodes linked to disHub. Let
Wh be the set of mapped content words dominated
by hub h. Then, disHub can be found as:
</bodyText>
<equation confidence="0.9615545">
�disHub = argmax
h cw∈Wh
</equation>
<bodyText confidence="0.999984625">
where d(cw) is a function which assigns a weight
to cw according to its distance to w, i.e. the more
words occur between w and cw within W, the
smaller the weight, and dist(cw, h) is given by
the number of edges between cw and h in the
MST. Finally, we collect the translation edges of
the retained context nodes WdisHub and we sum
the translation counts to rank each translation.
</bodyText>
<sectionHeader confidence="0.997533" genericHeader="evaluation">
4 Results and Analysis
</sectionHeader>
<bodyText confidence="0.999832761904762">
Experimental Setting. We submitted two runs
for the task (UHD-1 and UHD-2 henceforth).
Since we were interested in assessing the impact
of using different resources with our methodology,
we automatically built multilingual graphs from
different sentence-aligned corpora, i.e. Europarl
(Koehn, 2005) for UHD-1, augmented with the
JRC-Acquis corpus (Steinberger et al., 2006) for
UHD-21. Both corpora were tagged and lemma-
tized with TreeTagger (Schmid, 1994) and word
aligned using GIZA++ (Och and Ney, 2003). For
German, in order to avoid the sparseness deriving
from the high productivity of compounds, we per-
formed a morphological analysis using Morphisto
(Zielinski et al., 2009).
To build the multilingual graph (Section 3.2),
we used a minimum frequency threshold of 2 oc-
currences for a word to be inserted as a node,
and retained only those edges with a weight less
or equal to 0.7. After constructing the multilin-
gual graph, we additionally removed those trans-
lations with a frequency count lower than 10 (7
in the case of German, due to the large amount
of compounds). Finally, the translations gener-
ated for the BEST evaluation setting were ob-
tained by applying the following rule onto the
ranked answer translations: add translation tri
while count(tri) ≥ count(tri−1)/3, where i is
the i-th ranked translation.
Results and discussion. The results for the
BEST and out-of-five (OOF) evaluations are pre-
sented in Tables 1 and 2 respectively. Results are
computed using the official scorer (Lefever and
Hoste, 2010) and no post-processing is applied to
the system’s output, i.e. we do not back-off to the
baseline most frequent translation in case the sys-
tem fails to provide an answer for a test instance.
For the sake of brevity, we present the results for
UHD-1, since we found no statistically significant
difference in the performance of the two systems
(e.g. UHD-2 outperforms UHD-1 only by +0.7%
on the BEST evaluation for French).
</bodyText>
<footnote confidence="0.9678995">
1As in the case of Europarl, only 1-to-1-aligned sentences
were extracted.
</footnote>
<equation confidence="0.993823">
E
k∈deg(vj) wjk
wij
PR(vj)
d(cw)
dist(cw, h) + 1
</equation>
<page confidence="0.996972">
136
</page>
<table confidence="0.9976006">
Language P R Mode P Mode R
FRENCH 20.22 16.21 17.59 14.56
GERMAN 12.20 9.32 11.05 7.78
ITALIAN 15.94 12.78 12.34 8.48
SPANISH 20.48 16.33 28.48 22.19
</table>
<tableCaption confidence="0.803375">
Table 1: BEST results (UHD-1).
</tableCaption>
<table confidence="0.9999002">
Language P R Mode P Mode R
FRENCH 39.06 32.00 37.00 26.79
GERMAN 27.62 22.82 25.68 21.16
ITALIAN 33.72 27.49 27.54 21.81
SPANISH 38.78 31.81 40.68 32.38
</table>
<tableCaption confidence="0.9966">
Table 2: OOF results (UHD-1).
</tableCaption>
<bodyText confidence="0.999882">
Overall, in the BEST evaluation our system
ranked in the middle for those languages where
the majority of systems participated – i.e. sec-
ond and fourth out of 7 submissions for FRENCH
and SPANISH. When compared against the base-
line, i.e. the most frequent translation found in
Europarl, our method was able to achieve in the
BEST evaluation a higher precision for ITALIAN
and SPANISH (+1.9% and +2.1%, respectively),
whereas FRENCH and GERMAN lie near below the
baseline scores (−0.5% and −1.0%, respectively).
The trade-off is a recall always below the base-
line. In contrast, we beat the Mode precision base-
line for all languages, i.e. up to +5.1% for SPAN-
ISH. The fact that our system is strongly precision-
oriented is additionally proved by a low perfor-
mance in the OOF evaluation, where we always
perform below the baseline (i.e. the five most fre-
quent translations in Europarl).
</bodyText>
<sectionHeader confidence="0.999237" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9997445">
We presented in this paper a graph-based system
to perform CL-WSD. Key to our approach is the
use of a co-occurrence graph built from multilin-
gual parallel corpora, and the application of well-
studied graph algorithms for monolingual WSD
(V´eronis, 2004; Agirre et al., 2006). Future work
will concentrate on extensions of the algorithms,
e.g. computing hubs in each language indepen-
dently and combining them as a joint problem, as
well as developing robust techniques for unsuper-
vised tuning of the graph weights, given the obser-
vation that the most frequent translations tend to
receive too much weight and accordingly crowd
out more appropriate translations. Finally, we
plan to investigate the application of our approach
directly to multilingual lexical resources such as
PanDictionary (Mausam et al., 2009) and Babel-
Net (Navigli and Ponzetto, 2010).
</bodyText>
<sectionHeader confidence="0.98119" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999694041666667">
Eneko Agirre, David Martinez, Oier L´opez de Lacalle,
and Aitor Soroa. 2006. Two graph-based algorithms
for state-of-the-art WSD. In Proc. of EMNLP-06,
pages 585–593.
Sergey Brin and Lawrence Page. 1998. The
anatomy of a large-scale hypertextual web search
engine. Computer Networks and ISDN Systems,
30(1–7):107–117.
Christiane Fellbaum, editor. 1998. WordNet: An Elec-
tronic Database. MIT Press, Cambridge, MA.
Philipp Koehn. 2005. Europarl: A parallel corpus
for statistical machine translation. In Proceedings
of Machine Translation Summit X.
Els Lefever and Veronique Hoste. 2010. SemEval-
2010 Task 3: Cross-lingual Word Sense Disam-
biguation. In Proc. of SemEval-2010.
Mausam, Stephen Soderland, Oren Etzioni, Daniel
Weld, Michael Skinner, and Jeff Bilmes. 2009.
Compiling a massive, multilingual dictionary via
probabilistic inference. In Proc. of ACL-IJCNLP-
09, pages 262–270.
Roberto Navigli and Simone Paolo Ponzetto. 2010.
BabelNet: Building a very large multilingual seman-
tic network. In Proc. of ACL-10.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Helmut Schmid. 1994. Probabilistic part-of-speech
tagging using decision trees. In Proceedings of the
International Conference on New Methods in Lan-
guage Processing (NeMLaP ’94), pages 44–49.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaˇz Erjavec, Dan Tufis¸, and
D´aniel Varga. 2006. The JRC-Acquis: A multilin-
gual aligned parallel corpus with 20+ languages. In
Proc. of LREC ’06.
Jean V´eronis. 2004. Hyperlex: lexical cartography
for information retrieval. Computer Speech &amp; Lan-
guage, 18(3):223–252.
Piek Vossen, editor. 1998. EuroWordNet: A Multi-
lingual Database with Lexical Semantic Networks.
Kluwer, Dordrecht, The Netherlands.
Andrea Zielinski, Christian Simon, and Tilman Wittl.
2009. Morphisto: Service-oriented open source
morphology for German. In State of the Art in Com-
putational Morphology, volume 41 of Communica-
tions in Computer and Information Science, pages
64–75. Springer.
</reference>
<page confidence="0.997808">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.975384">
<title confidence="0.9987315">UHD: Cross-Lingual Word Sense Disambiguation Using Multilingual Co-occurrence Graphs</title>
<author confidence="0.994049">Silberer Paolo Ponzetto</author>
<affiliation confidence="0.99982">Department of Computational Linguistics Heidelberg University</affiliation>
<abstract confidence="0.998642785714286">We describe the University of Heidelberg (UHD) system for the Cross-Lingual Word Sense Disambiguation SemEval-2010 task (CL-WSD). The system performs CL- WSD by applying graph algorithms previously developed for monolingual Word Sense Disambiguation to multilingual cooccurrence graphs. UHD has participated the out-of-five (OOF) evaluations and ranked among the most competitive systems for this task, thus indicating that graph-based approaches represent a powerful alternative for this task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>David Martinez</author>
<author>Oier L´opez de Lacalle</author>
<author>Aitor Soroa</author>
</authors>
<title>Two graph-based algorithms for state-of-the-art WSD.</title>
<date>2006</date>
<booktitle>In Proc. of EMNLP-06,</booktitle>
<pages>585--593</pages>
<marker>Agirre, Martinez, de Lacalle, Soroa, 2006</marker>
<rawString>Eneko Agirre, David Martinez, Oier L´opez de Lacalle, and Aitor Soroa. 2006. Two graph-based algorithms for state-of-the-art WSD. In Proc. of EMNLP-06, pages 585–593.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
<author>Lawrence Page</author>
</authors>
<title>The anatomy of a large-scale hypertextual web search engine.</title>
<date>1998</date>
<journal>Computer Networks and ISDN Systems,</journal>
<pages>30--1</pages>
<contexts>
<context position="2465" citStr="Brin and Page, 1998" startWordPosition="373" endWordPosition="376">nd edges connect the words which co-occur in these contexts. The second step iteratively selects the node with highest degree in the graph (root hub) and removes it along with its adjacent nodes. Each such selection corresponds to isolating a highdensity component of the graph, in order to select a sense of the target word. In the last step the root hubs are linked to the target word and the Minimum Spanning Tree (MST) of the graph is computed to disambiguate the target word in context. Agirre et al. (2006) compare Hyperlex with an alternative method to detect the root hubs based on PageRank (Brin and Page, 1998). PageRank has the advantage of requiring less parameters than Hyperlex, whereas the authors ascertain equal performance of the two methods. 3 Graph-based Cross-Lingual WSD We start by building for each target word a multilingual co-occurrence graph based on the target word’s aligned contexts found in parallel corpora (Sections 3.1 and 3.2). Multilingual nodes are linked by translation edges, labeled with the target word’s translations observed in the corresponding contexts. We then use an adapted PageRank algorithm to select the nodes which represent the target word’s different senses (Sectio</context>
</contexts>
<marker>Brin, Page, 1998</marker>
<rawString>Sergey Brin and Lawrence Page. 1998. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30(1–7):107–117.</rawString>
</citation>
<citation valid="true">
<title>WordNet: An Electronic Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet: An Electronic Database. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of Machine Translation Summit X.</booktitle>
<contexts>
<context position="9993" citStr="Koehn, 2005" startWordPosition="1725" endWordPosition="1726">. the more words occur between w and cw within W, the smaller the weight, and dist(cw, h) is given by the number of edges between cw and h in the MST. Finally, we collect the translation edges of the retained context nodes WdisHub and we sum the translation counts to rank each translation. 4 Results and Analysis Experimental Setting. We submitted two runs for the task (UHD-1 and UHD-2 henceforth). Since we were interested in assessing the impact of using different resources with our methodology, we automatically built multilingual graphs from different sentence-aligned corpora, i.e. Europarl (Koehn, 2005) for UHD-1, augmented with the JRC-Acquis corpus (Steinberger et al., 2006) for UHD-21. Both corpora were tagged and lemmatized with TreeTagger (Schmid, 1994) and word aligned using GIZA++ (Och and Ney, 2003). For German, in order to avoid the sparseness deriving from the high productivity of compounds, we performed a morphological analysis using Morphisto (Zielinski et al., 2009). To build the multilingual graph (Section 3.2), we used a minimum frequency threshold of 2 occurrences for a word to be inserted as a node, and retained only those edges with a weight less or equal to 0.7. After cons</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Els Lefever</author>
<author>Veronique Hoste</author>
</authors>
<date>2010</date>
<booktitle>SemEval2010 Task 3: Cross-lingual Word Sense Disambiguation. In Proc. of SemEval-2010.</booktitle>
<contexts>
<context position="990" citStr="Lefever and Hoste, 2010" startWordPosition="129" endWordPosition="132">. The system performs CLWSD by applying graph algorithms previously developed for monolingual Word Sense Disambiguation to multilingual cooccurrence graphs. UHD has participated in the BEST and out-of-five (OOF) evaluations and ranked among the most competitive systems for this task, thus indicating that graph-based approaches represent a powerful alternative for this task. 1 Introduction This paper describes a graph-based system for Cross-Lingual Word Sense Disambiguation, i.e. the task of disambiguating a word in context by providing its most appropriate translations in different languages (Lefever and Hoste, 2010, CLWSD henceforth). Our goal at SemEval-2010 was to assess whether graph-based approaches, which have been successfully developed for monolingual Word Sense Disambiguation, represent a valid framework for CL-WSD. These typically transform a knowledge resource such as WordNet (Fellbaum, 1998) into a graph and apply graph algorithms to perform WSD. In our work, we follow this line of research and apply graph-based methods to multilingual co-occurrence graphs which are automatically created from parallel corpora. 2 Related Work Our method is heavily inspired by previous proposals from V´eronis (</context>
<context position="11217" citStr="Lefever and Hoste, 2010" startWordPosition="1923" endWordPosition="1926">tructing the multilingual graph, we additionally removed those translations with a frequency count lower than 10 (7 in the case of German, due to the large amount of compounds). Finally, the translations generated for the BEST evaluation setting were obtained by applying the following rule onto the ranked answer translations: add translation tri while count(tri) ≥ count(tri−1)/3, where i is the i-th ranked translation. Results and discussion. The results for the BEST and out-of-five (OOF) evaluations are presented in Tables 1 and 2 respectively. Results are computed using the official scorer (Lefever and Hoste, 2010) and no post-processing is applied to the system’s output, i.e. we do not back-off to the baseline most frequent translation in case the system fails to provide an answer for a test instance. For the sake of brevity, we present the results for UHD-1, since we found no statistically significant difference in the performance of the two systems (e.g. UHD-2 outperforms UHD-1 only by +0.7% on the BEST evaluation for French). 1As in the case of Europarl, only 1-to-1-aligned sentences were extracted. E k∈deg(vj) wjk wij PR(vj) d(cw) dist(cw, h) + 1 136 Language P R Mode P Mode R FRENCH 20.22 16.21 17</context>
</contexts>
<marker>Lefever, Hoste, 2010</marker>
<rawString>Els Lefever and Veronique Hoste. 2010. SemEval2010 Task 3: Cross-lingual Word Sense Disambiguation. In Proc. of SemEval-2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephen Soderland Mausam</author>
<author>Oren Etzioni</author>
<author>Daniel Weld</author>
<author>Michael Skinner</author>
<author>Jeff Bilmes</author>
</authors>
<title>Compiling a massive, multilingual dictionary via probabilistic inference.</title>
<date>2009</date>
<booktitle>In Proc. of ACL-IJCNLP09,</booktitle>
<pages>262--270</pages>
<contexts>
<context position="6871" citStr="Mausam et al., 2009" startWordPosition="1180" endWordPosition="1183">vj E Vl, i =� j 18: if vi and vj co-occur in Cl then 19: EML — EML U (vi, vj) 20: return GML (vs, t, vl) receives a translation label t. Formally, let Cvs C Cs be the contexts where vs and w cooccur, and Cvl the word-aligned contexts in language l of Cvs, where vs is translated as vl. Then each edge between nodes vs and vl is labeled with a translation label t (lines 14-15): this includes a translation of w in Cvl, its frequency of translation and the information of whether the translation is monosemous, as found in a multilingual dictionary, i.e. EuroWordNet (Vossen, 1998) and PanDictionary (Mausam et al., 2009). Finally, the multilingual graph is further extended by inserting all possible co-occurrence edges (vi, vj) E El between the nodes for the target language l (lines 16- 19, i.e. we apply the step from Section 3.1 to l and Cl). As a result of the algorithm, the multilingual graph is returned (line 20). 3.3 Computing Root Hubs We compute the root hubs in the multilingual graph to discriminate the senses of the target word in the source language. Hubs are found using the adapted PageRank from Agirre et al. (2006): 135 � PR(vi) = (1 − d) + d j∈deg(vi) where d is the so-called damping factor (typic</context>
</contexts>
<marker>Mausam, Etzioni, Weld, Skinner, Bilmes, 2009</marker>
<rawString>Mausam, Stephen Soderland, Oren Etzioni, Daniel Weld, Michael Skinner, and Jeff Bilmes. 2009. Compiling a massive, multilingual dictionary via probabilistic inference. In Proc. of ACL-IJCNLP09, pages 262–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: Building a very large multilingual semantic network. In</title>
<date>2010</date>
<booktitle>Proc. of ACL-10.</booktitle>
<marker>Navigli, Ponzetto, 2010</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2010. BabelNet: Building a very large multilingual semantic network. In Proc. of ACL-10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10201" citStr="Och and Ney, 2003" startWordPosition="1756" endWordPosition="1759">tained context nodes WdisHub and we sum the translation counts to rank each translation. 4 Results and Analysis Experimental Setting. We submitted two runs for the task (UHD-1 and UHD-2 henceforth). Since we were interested in assessing the impact of using different resources with our methodology, we automatically built multilingual graphs from different sentence-aligned corpora, i.e. Europarl (Koehn, 2005) for UHD-1, augmented with the JRC-Acquis corpus (Steinberger et al., 2006) for UHD-21. Both corpora were tagged and lemmatized with TreeTagger (Schmid, 1994) and word aligned using GIZA++ (Och and Ney, 2003). For German, in order to avoid the sparseness deriving from the high productivity of compounds, we performed a morphological analysis using Morphisto (Zielinski et al., 2009). To build the multilingual graph (Section 3.2), we used a minimum frequency threshold of 2 occurrences for a word to be inserted as a node, and retained only those edges with a weight less or equal to 0.7. After constructing the multilingual graph, we additionally removed those translations with a frequency count lower than 10 (7 in the case of German, due to the large amount of compounds). Finally, the translations gene</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing (NeMLaP ’94),</booktitle>
<pages>44--49</pages>
<contexts>
<context position="10151" citStr="Schmid, 1994" startWordPosition="1749" endWordPosition="1750">y, we collect the translation edges of the retained context nodes WdisHub and we sum the translation counts to rank each translation. 4 Results and Analysis Experimental Setting. We submitted two runs for the task (UHD-1 and UHD-2 henceforth). Since we were interested in assessing the impact of using different resources with our methodology, we automatically built multilingual graphs from different sentence-aligned corpora, i.e. Europarl (Koehn, 2005) for UHD-1, augmented with the JRC-Acquis corpus (Steinberger et al., 2006) for UHD-21. Both corpora were tagged and lemmatized with TreeTagger (Schmid, 1994) and word aligned using GIZA++ (Och and Ney, 2003). For German, in order to avoid the sparseness deriving from the high productivity of compounds, we performed a morphological analysis using Morphisto (Zielinski et al., 2009). To build the multilingual graph (Section 3.2), we used a minimum frequency threshold of 2 occurrences for a word to be inserted as a node, and retained only those edges with a weight less or equal to 0.7. After constructing the multilingual graph, we additionally removed those translations with a frequency count lower than 10 (7 in the case of German, due to the large am</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing (NeMLaP ’94), pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
<author>Bruno Pouliquen</author>
<author>Anna Widiger</author>
<author>Camelia Ignat</author>
<author>Tomaˇz Erjavec</author>
<author>Dan Tufis¸</author>
<author>D´aniel Varga</author>
</authors>
<title>The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages.</title>
<date>2006</date>
<booktitle>In Proc. of LREC ’06.</booktitle>
<marker>Steinberger, Pouliquen, Widiger, Ignat, Erjavec, Tufis¸, Varga, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaˇz Erjavec, Dan Tufis¸, and D´aniel Varga. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proc. of LREC ’06.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean V´eronis</author>
</authors>
<title>Hyperlex: lexical cartography for information retrieval.</title>
<date>2004</date>
<journal>Computer Speech &amp; Language,</journal>
<volume>18</volume>
<issue>3</issue>
<marker>V´eronis, 2004</marker>
<rawString>Jean V´eronis. 2004. Hyperlex: lexical cartography for information retrieval. Computer Speech &amp; Language, 18(3):223–252.</rawString>
</citation>
<citation valid="true">
<date>1998</date>
<booktitle>EuroWordNet: A Multilingual Database with Lexical Semantic Networks.</booktitle>
<editor>Piek Vossen, editor.</editor>
<publisher>Kluwer,</publisher>
<location>Dordrecht, The Netherlands.</location>
<marker>1998</marker>
<rawString>Piek Vossen, editor. 1998. EuroWordNet: A Multilingual Database with Lexical Semantic Networks. Kluwer, Dordrecht, The Netherlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Zielinski</author>
<author>Christian Simon</author>
<author>Tilman Wittl</author>
</authors>
<title>Morphisto: Service-oriented open source morphology for German.</title>
<date>2009</date>
<booktitle>In State of the Art in Computational Morphology,</booktitle>
<volume>41</volume>
<pages>64--75</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="10376" citStr="Zielinski et al., 2009" startWordPosition="1783" endWordPosition="1786">UHD-1 and UHD-2 henceforth). Since we were interested in assessing the impact of using different resources with our methodology, we automatically built multilingual graphs from different sentence-aligned corpora, i.e. Europarl (Koehn, 2005) for UHD-1, augmented with the JRC-Acquis corpus (Steinberger et al., 2006) for UHD-21. Both corpora were tagged and lemmatized with TreeTagger (Schmid, 1994) and word aligned using GIZA++ (Och and Ney, 2003). For German, in order to avoid the sparseness deriving from the high productivity of compounds, we performed a morphological analysis using Morphisto (Zielinski et al., 2009). To build the multilingual graph (Section 3.2), we used a minimum frequency threshold of 2 occurrences for a word to be inserted as a node, and retained only those edges with a weight less or equal to 0.7. After constructing the multilingual graph, we additionally removed those translations with a frequency count lower than 10 (7 in the case of German, due to the large amount of compounds). Finally, the translations generated for the BEST evaluation setting were obtained by applying the following rule onto the ranked answer translations: add translation tri while count(tri) ≥ count(tri−1)/3, </context>
</contexts>
<marker>Zielinski, Simon, Wittl, 2009</marker>
<rawString>Andrea Zielinski, Christian Simon, and Tilman Wittl. 2009. Morphisto: Service-oriented open source morphology for German. In State of the Art in Computational Morphology, volume 41 of Communications in Computer and Information Science, pages 64–75. Springer.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>