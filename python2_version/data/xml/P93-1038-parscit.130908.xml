<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.024924">
<sectionHeader confidence="0.9268115" genericHeader="abstract">
IDENTIFYING RELEVANT PRIOR
EXPLANATIONS
</sectionHeader>
<author confidence="0.650458">
James A. Rosenblum
</author>
<affiliation confidence="0.896217666666667">
Department of Computer Science
University of Pittsburgh
Pittsburgh, PA 15260, USA
</affiliation>
<email confidence="0.738952">
Internet: jr@cs.pitt.edu
</email>
<sectionHeader confidence="0.996119" genericHeader="introduction">
Abstract
</sectionHeader>
<bodyText confidence="0.9995852">
When human tutors engage in dialogue, they freely
exploit all aspects of the mutually known context,
including the previous discourse. Utterances that
do not draw on previous discourse seem awkward,
unnatural, or even incoherent. Previous discourse
must be taken into account in order to relate new
information effectively to recently conveyed mate-
rial, and to avoid repeating old material that would
distract the student from what is new.
Producing a system that displays such behavior
involves finding an efficient way to identify which
previous explanations (if any) are relevant to the
current explanation task. Thus, we are implement-
ing a system that uses a case-based reasoning ap-
proach to identify previous situations and expla-
nations that could potentially affect the explana-
tion being constructed. We have identified heuris-
tics for constructing explanations that exploit this
information in ways similar to what we have ob-
served in human-human tutorial dialogues.
</bodyText>
<sectionHeader confidence="0.99202" genericHeader="method">
Introduction and Motivation
</sectionHeader>
<bodyText confidence="0.999781041666667">
We are building an explanation component for
an existing intelligent training system, SHERLOCK
(Lesgold et al., 1992), which trains avionics tech-
nicians to troubleshoot electronic equipment. Us-
ing SHERLOCK, trainees solve problems with min-
imal tutor interaction and then review their trou-
bleshooting in a post-problem reflective follow-up
(RFU) session where the tutor replays each stu-
dent action and assesses it as &amp;quot;good&amp;quot; (&lt;+&gt;) or
as &amp;quot;could be improved&amp;quot; (&lt;-&gt;). After a step is re-
played, the student can ask the tutor to justify its
assessment.
As an example of the way in which human tutors
exploit previous discourse, consider the dialogue
in Figure 1, taken from our data. Even though
the student has made the same mistake twice, the
second explanation looks quite different from the
first. Yet the two explanations are related to one
another in an important way. In the second expla-
nation the tutor simply reminds the student that
she has not determined the status of the main con-
trol data signals and that she should do so before
testing the secondary control data signals. The
tutor expects the student to be able to make use
of the previous explanation once he has indicated
that it is relevant to the current situation (&amp;quot;for the
same reasons given ...&amp;quot; serves this purpose). Ac-
cordingly, the tutor does not repeat the detailed
explanation of why the main control data signals
should be tested first. By generating the second
explanation in such a way that it &apos;meshes&apos; with the
first, not only has the tutor corrected the testing
mistake of the student, but has forced the student
to consider how the two situations are similar. In
pointing out this similarity, he has given the stu-
dent a better understanding of the domain. We
call an explanation that is later referred to (explic-
itly or implicitly) or is integrated into a subsequent
explanation the anchor.
Clearly it is desirable for a system to produce
text that is sensitive to what has been said previ-
ously. In order to do this, however, a system must
first be able to decide what previous explanation
(or part thereof) to use as an anchor. This in-
volves deciding, in an efficient way, whether there
exist suitable candidates to act as anchor, and if
so, which amongst them would be best to use. This
paper concentrates on this task.
</bodyText>
<subsectionHeader confidence="0.990093">
The Text Planner
</subsectionHeader>
<bodyText confidence="0.999628272727273">
For this work, we are extending the text planner
built by Moore and Paris (1989). Briefly, it works
in the following way. A communicative goal (e.g.,
&amp;quot;achieve the state where the hearer believes that
an action could be improved&amp;quot;) is formed based
upon the student&apos;s question. Using its library of
plan operators that encode knowledge about tu-
torial explanations, the system employs a linear
planning mechanism to synthesise a response to
achieve this goal. The result is a test plan for the
explanation.
The system then presents the explanation to the
user, retaining the plan that produced it in a di-
alogue history. The dialogue history is a record
of the conversation that has occurred thus far and
includes the user&apos;s utterances as well as the text
plans that led to the system&apos;s responses. In this
system, a text plan represents the effect that each
part of the text is intended to have on the hearer&apos;s
mental state, the linguistic strategies that were
used to achieve these effects, and how the complete
text achieves the overall communicative goal.
</bodyText>
<page confidence="0.978847">
277
</page>
<bodyText confidence="0.943042375">
TUTOR &lt;-&gt; You tested pin 28 of the A1A3A15.
STUDENT Why is testing pin 28 not a good step?
TUTOR As explained before, the main data inputs to relay B24 are highly suspect at this time.
You have tested the low input to B24 but not the high one. Since it is more likely that
there may be a problem with the main data signals to relay B24 than with the secondary
data signals to relays B25-B30, the main data signal should be tested first. Testing pin
28 is not a bad step, it is just more likely (on the average) that the main data signals
are causing the problem.
</bodyText>
<tableCaption confidence="0.679360666666667">
: One exchange and one action later.
TUTOR &lt;-&gt;You tested pin 36 of the A1A3A15.
STUDENT Don&apos;t I need to test pin 36?
TUTOR You might have to, but for the same reasons given when you tested pin 28, it is generally
more efficient to test the main control data signals first, and then test the secondary
control data signals if necessary.
</tableCaption>
<figureCaption confidence="0.985751">
Figure 1: Human-Human Advisory Interaction Displaying Contextual Effects
</figureCaption>
<sectionHeader confidence="0.964651" genericHeader="method">
Knowledge Sources for Finding
Relevant Prior Explanations
</sectionHeader>
<bodyText confidence="0.999992724137931">
The most straightforward way to find relevant
prior explanations is to exhaustively search the
system&apos;s dialogue history looking for explanations
that have certain features. For example, when ex-
plaining why a step was assessed as &amp;quot;could be im-
proved,&amp;quot; the system could look for previous expla-
nations that justified this type of assessment, and
in which the two actions being assessed were sim-
ilar (i.e., had the same features).
However, this approach is problematic. Expla-
nation plans are large complex structures, and
they will accumulate rapidly as the dialogue pro-
gresses. Exhaustively searching the discourse his-
tory for relevant prior explanations is computa-
tionally prohibitive. Thus, we require an indexing
strategy that allows the system to find possibly
relevant prior explanations in an efficient manner.
To satisfy this requirement, we use case-based
reasoning (csit) to provide a framework in which
previous student actions can be efficiently exam-
ined to determine which, if any, are relevant when
producing an explanation. This approach has the
additional advantage of allowing the system to
consider what was said as well as what was not
said when planning an explanation. For example,
the student may have previously performed an ac-
tion that displayed some characteristic that the tu-
tor decided not to mention at the time and which
would now be appropriate to discuss.
</bodyText>
<sectionHeader confidence="0.795081" genericHeader="method">
A Case-Based Algorithm
</sectionHeader>
<bodyText confidence="0.999983170212766">
The following aspect of SHERLOCK&apos;S reasoning is
extremely important to our work. SHERLOCK eval-
uates each student action by determining which
facets apply to that action. The facets repre-
sent factors that expert avionics tutors use in as-
sessing student&apos;s troubleshooting actions (Pokorny
and Gott, 1990). To evaluate an action, SEER-
LOCK finds each facet that applies to it and de-
termines whether that facet should be considered
good (g), bad (b), or neutral (n) given the current
problem-solving context. For example, the facet
&amp;quot;Making a measurement that is off the active cir-
cuit path&amp;quot; is considered a b-facet. The representa-
tion of a student action includes the list of facets
characterizing the action and an assessment (g, b,
or n) for each of those facets.
Case-based reasoning generalizes from cases to
support indexing and relevance assessment, and
can be used to evaluate a case by comparing it to
past cases (Ashley, 1992). This seems to describe
our task when we treat each student action as a
&amp;quot;case&amp;quot;. Influenced by the work of Aleven and Ash-
ley (1992), we noted certain similarities between
their domain and ours that led us to believe that
we could use CBR techniques to identify similar ac-
tions as described below.
Our algorithm builds a data structure called a
similarity DAG (Directed Acyclic Graph) which
indicates the previous student actions that are sim-
ilar to a given action. By similar, we mean simi-
lar with respect to a certain class of facets (some
combination of g, b, or n). For example, when an-
swering a question about why the current action
was assessed as &amp;quot;could be improved,&amp;quot; the similar-
ity DAG is built so that it indicates which previ-
ous actions were similar to the current action with
respect to the b-facets. The root of the DAG rep-
resents the current action and the facets of interest
(b-facets in our example) that apply to it. Each
node in the DAG, including the root, represents a
set of student actions that share the same set of
interesting facets. The more facets that a node has
in common with the current action (in the root),
the closer it will be to the root node. Proximity
in the DAG corresponds to similarity in facet sets.
Basically, the similarity DAG is a partial ordering
of the student&apos;s actions based on their facet lists.
</bodyText>
<page confidence="0.994191">
278
</page>
<table confidence="0.934740954545455">
FACETS
F100: Allowed main data signal
relay to remain partially tested (b)
F101: Tested secondary data signal
before main data signal (b)
CURRENT ACTION
Action 12: VDC test, pin 36 to
ground on AlA3A15 (b)
PREVIOUS ACTIONS
Action 9: VDC test, pin 28 to
ground on A1A3A15 (b)
FACETS
F100: Allowed a maia data signal
relay to remain prow)/ tested (b)
PREVIOUS ACTIONS
Action 8: VDC test, pin 38 to
_ground on A1A3A15 (b)
Similarity DAG
Discourse
History
... TEXT PLAN 1 ... TEXT PLAN 2 ...
■
</table>
<figureCaption confidence="0.963797">
Figure 2: Data structures when considering how to answer turn 5, Figure 1
</figureCaption>
<bodyText confidence="0.99849406">
Figure 2 shows the similarity DAG that is con-
structed when the system considers how to answer
the question, &amp;quot;Don&apos;t I need to test pin 36?&amp;quot; (turn
5 of Figure 1). The facets relevant to the action
in question are F100 and F101. The structure
indicates that two previous actions — 9 and to a
lesser degree 8, are similar to the current situa-
tion. Pointers index the dialogue history&apos;s record
of what was said at those times. At this point,
the system has identified candidate situations that
are relevant for planning the current explanation.
It can now consider these retrieved situations more
closely to determine any other facets that they may
possess, and can examine the related explanations
in the dialogue history to determine what was said
about each of the two previous situations. The fact
that there are no other nodes in the DAG indicates
that there are no other suitable prior situations.
Initial results using this algorithm seem promis-
ing. In an analysis of 8 student-tutor protocols
involving 154 actions and 22 opportunities for in-
tegrating a previous explanation into an answer,
the algorithm correctly identified the same previ-
ous situations that were used by the human tutor
in the actual interactions. In all but 3 cases, when
the human tutor did not make a reference to a pre-
vious explanation, our algorithm reported no sim-
ilar prior situation. In the 3 situations where our
algorithm identified a similarity not exploited by
the tutor, our expert agreed that they would have
been useful to incorporate into his explanations.
Lastly, this technique will be useful in answering
students&apos; direct questions about the similarities of
situations, e.g., &amp;quot;Why is testing 30 good? Isn&apos;t it
like 36 and 28?&amp;quot; By constructing and consulting
a similarity DAG, the system is able to plan re-
sponses such as: &amp;quot;Yes, but now you know the main
control data signals on pins 33 and 22 are good so
you need to test the secondary data signals.&amp;quot;
It is important to note that this approach is suc-
cessful, in part, because the facets are based on a
tutor&apos;s evaluation of a student&apos;s actions, and we
are currently addressing only questions that jus-
tify these evaluations. We focused on this type of
question because 48% of student&apos;s queries during
RFU are of this type. To answer additional ques-
tions in a context-sensitive fashion, we will need to
extend our indexing scheme to take the intentions
behind an explanation into account as well as the
domain content discussed.
</bodyText>
<sectionHeader confidence="0.705792" genericHeader="conclusions">
Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999925333333333">
We have indicated that in order to produce text
that is sensitive to the previous discourse, a sys-
tem must first be able to identify relevant previous
explanations and situations. To achieve this first
step, a CBE. algorithm was introduced that indexes
the dialogue history and supplies the explanations
with a context in which to be considered. We are
devising techniques that use this information to
plan subsequent explanations.
</bodyText>
<sectionHeader confidence="0.999205" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999380304347826">
Aleven, V. and Ashley, K. 1992. Auto-
mated generation of examples for a tutorial in
case-based argumentation. In Proc. of the 2nd
Intl Conference on Intelligent Tutoring Sys-
tems, Montreal, Canada.
Ashley, K. 1992. Case-based reasoning
and its implications for legal expert systems. Ar-
tificial Intelligence and Law 2(1).
Lesgold, A.; Lajoie, S.; Bunzo, M.; and
Eggan, G. 1992. Sherlock: A coached practice
environment for an electronics troubleshooting
job. In Computer Assisted Instruction and In-
telligent Tutoring Systems: Shared Goals and
Complementary Approaches. Lawrence Erlbaum
Assoc., NJ.
Moore, J. D. and Paris, C. L. 1989. Plan-
ning text for advisory dialogues. In Proc. of the
27th Annual Meeting of the A CL, Vancouver,
B.C., Canada. 203-211.
Pokorny, R. and Gott, S. 1990. The eval-
uation of a real-world instructional system: Us-
ing technical experts as raters. Technical report,
Armstrong Laboratories, &apos;Brooks AFB.
</reference>
<page confidence="0.998567">
279
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.912239">
<title confidence="0.9938175">IDENTIFYING RELEVANT PRIOR EXPLANATIONS</title>
<author confidence="0.99999">James A Rosenblum</author>
<affiliation confidence="0.999987">Department of Computer Science University of Pittsburgh</affiliation>
<address confidence="0.999668">Pittsburgh, PA 15260, USA</address>
<email confidence="0.998687">Internet:jr@cs.pitt.edu</email>
<abstract confidence="0.996391619047619">When human tutors engage in dialogue, they freely exploit all aspects of the mutually known context, including the previous discourse. Utterances that do not draw on previous discourse seem awkward, unnatural, or even incoherent. Previous discourse must be taken into account in order to relate new information effectively to recently conveyed material, and to avoid repeating old material that would distract the student from what is new. Producing a system that displays such behavior involves finding an efficient way to identify which previous explanations (if any) are relevant to the current explanation task. Thus, we are implementing a system that uses a case-based reasoning approach to identify previous situations and explanations that could potentially affect the explanation being constructed. We have identified heuristics for constructing explanations that exploit this information in ways similar to what we have observed in human-human tutorial dialogues.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V Aleven</author>
<author>K Ashley</author>
</authors>
<title>Automated generation of examples for a tutorial in case-based argumentation.</title>
<date>1992</date>
<booktitle>In Proc. of the 2nd Intl Conference on Intelligent Tutoring Systems,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="8046" citStr="Aleven and Ashley (1992)" startWordPosition="1331" endWordPosition="1335">bad (b), or neutral (n) given the current problem-solving context. For example, the facet &amp;quot;Making a measurement that is off the active circuit path&amp;quot; is considered a b-facet. The representation of a student action includes the list of facets characterizing the action and an assessment (g, b, or n) for each of those facets. Case-based reasoning generalizes from cases to support indexing and relevance assessment, and can be used to evaluate a case by comparing it to past cases (Ashley, 1992). This seems to describe our task when we treat each student action as a &amp;quot;case&amp;quot;. Influenced by the work of Aleven and Ashley (1992), we noted certain similarities between their domain and ours that led us to believe that we could use CBR techniques to identify similar actions as described below. Our algorithm builds a data structure called a similarity DAG (Directed Acyclic Graph) which indicates the previous student actions that are similar to a given action. By similar, we mean similar with respect to a certain class of facets (some combination of g, b, or n). For example, when answering a question about why the current action was assessed as &amp;quot;could be improved,&amp;quot; the similarity DAG is built so that it indicates which pr</context>
</contexts>
<marker>Aleven, Ashley, 1992</marker>
<rawString>Aleven, V. and Ashley, K. 1992. Automated generation of examples for a tutorial in case-based argumentation. In Proc. of the 2nd Intl Conference on Intelligent Tutoring Systems, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ashley</author>
</authors>
<title>Case-based reasoning and its implications for legal expert systems.</title>
<date>1992</date>
<journal>Artificial Intelligence and Law</journal>
<volume>2</volume>
<issue>1</issue>
<contexts>
<context position="7915" citStr="Ashley, 1992" startWordPosition="1309" endWordPosition="1310">n action, SEERLOCK finds each facet that applies to it and determines whether that facet should be considered good (g), bad (b), or neutral (n) given the current problem-solving context. For example, the facet &amp;quot;Making a measurement that is off the active circuit path&amp;quot; is considered a b-facet. The representation of a student action includes the list of facets characterizing the action and an assessment (g, b, or n) for each of those facets. Case-based reasoning generalizes from cases to support indexing and relevance assessment, and can be used to evaluate a case by comparing it to past cases (Ashley, 1992). This seems to describe our task when we treat each student action as a &amp;quot;case&amp;quot;. Influenced by the work of Aleven and Ashley (1992), we noted certain similarities between their domain and ours that led us to believe that we could use CBR techniques to identify similar actions as described below. Our algorithm builds a data structure called a similarity DAG (Directed Acyclic Graph) which indicates the previous student actions that are similar to a given action. By similar, we mean similar with respect to a certain class of facets (some combination of g, b, or n). For example, when answering a q</context>
</contexts>
<marker>Ashley, 1992</marker>
<rawString>Ashley, K. 1992. Case-based reasoning and its implications for legal expert systems. Artificial Intelligence and Law 2(1).</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lesgold</author>
<author>S Lajoie</author>
<author>M Bunzo</author>
<author>G Eggan</author>
</authors>
<title>Sherlock: A coached practice environment for an electronics troubleshooting job. In Computer Assisted Instruction and Intelligent Tutoring Systems: Shared Goals and Complementary Approaches. Lawrence Erlbaum Assoc.,</title>
<date>1992</date>
<location>NJ.</location>
<contexts>
<context position="1293" citStr="Lesgold et al., 1992" startWordPosition="186" endWordPosition="189">inding an efficient way to identify which previous explanations (if any) are relevant to the current explanation task. Thus, we are implementing a system that uses a case-based reasoning approach to identify previous situations and explanations that could potentially affect the explanation being constructed. We have identified heuristics for constructing explanations that exploit this information in ways similar to what we have observed in human-human tutorial dialogues. Introduction and Motivation We are building an explanation component for an existing intelligent training system, SHERLOCK (Lesgold et al., 1992), which trains avionics technicians to troubleshoot electronic equipment. Using SHERLOCK, trainees solve problems with minimal tutor interaction and then review their troubleshooting in a post-problem reflective follow-up (RFU) session where the tutor replays each student action and assesses it as &amp;quot;good&amp;quot; (&lt;+&gt;) or as &amp;quot;could be improved&amp;quot; (&lt;-&gt;). After a step is replayed, the student can ask the tutor to justify its assessment. As an example of the way in which human tutors exploit previous discourse, consider the dialogue in Figure 1, taken from our data. Even though the student has made the same</context>
</contexts>
<marker>Lesgold, Lajoie, Bunzo, Eggan, 1992</marker>
<rawString>Lesgold, A.; Lajoie, S.; Bunzo, M.; and Eggan, G. 1992. Sherlock: A coached practice environment for an electronics troubleshooting job. In Computer Assisted Instruction and Intelligent Tutoring Systems: Shared Goals and Complementary Approaches. Lawrence Erlbaum Assoc., NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Moore</author>
<author>C L Paris</author>
</authors>
<title>Planning text for advisory dialogues.</title>
<date>1989</date>
<booktitle>In Proc. of the 27th Annual Meeting of the A CL,</booktitle>
<location>Vancouver, B.C.,</location>
<contexts>
<context position="3571" citStr="Moore and Paris (1989)" startWordPosition="581" endWordPosition="584">rred to (explicitly or implicitly) or is integrated into a subsequent explanation the anchor. Clearly it is desirable for a system to produce text that is sensitive to what has been said previously. In order to do this, however, a system must first be able to decide what previous explanation (or part thereof) to use as an anchor. This involves deciding, in an efficient way, whether there exist suitable candidates to act as anchor, and if so, which amongst them would be best to use. This paper concentrates on this task. The Text Planner For this work, we are extending the text planner built by Moore and Paris (1989). Briefly, it works in the following way. A communicative goal (e.g., &amp;quot;achieve the state where the hearer believes that an action could be improved&amp;quot;) is formed based upon the student&apos;s question. Using its library of plan operators that encode knowledge about tutorial explanations, the system employs a linear planning mechanism to synthesise a response to achieve this goal. The result is a test plan for the explanation. The system then presents the explanation to the user, retaining the plan that produced it in a dialogue history. The dialogue history is a record of the conversation that has oc</context>
</contexts>
<marker>Moore, Paris, 1989</marker>
<rawString>Moore, J. D. and Paris, C. L. 1989. Planning text for advisory dialogues. In Proc. of the 27th Annual Meeting of the A CL, Vancouver, B.C., Canada. 203-211.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Pokorny</author>
<author>S Gott</author>
</authors>
<title>The evaluation of a real-world instructional system: Using technical experts as raters.</title>
<date>1990</date>
<tech>Technical report,</tech>
<location>Armstrong Laboratories, &apos;Brooks AFB.</location>
<contexts>
<context position="7287" citStr="Pokorny and Gott, 1990" startWordPosition="1200" endWordPosition="1203">wing the system to consider what was said as well as what was not said when planning an explanation. For example, the student may have previously performed an action that displayed some characteristic that the tutor decided not to mention at the time and which would now be appropriate to discuss. A Case-Based Algorithm The following aspect of SHERLOCK&apos;S reasoning is extremely important to our work. SHERLOCK evaluates each student action by determining which facets apply to that action. The facets represent factors that expert avionics tutors use in assessing student&apos;s troubleshooting actions (Pokorny and Gott, 1990). To evaluate an action, SEERLOCK finds each facet that applies to it and determines whether that facet should be considered good (g), bad (b), or neutral (n) given the current problem-solving context. For example, the facet &amp;quot;Making a measurement that is off the active circuit path&amp;quot; is considered a b-facet. The representation of a student action includes the list of facets characterizing the action and an assessment (g, b, or n) for each of those facets. Case-based reasoning generalizes from cases to support indexing and relevance assessment, and can be used to evaluate a case by comparing it </context>
</contexts>
<marker>Pokorny, Gott, 1990</marker>
<rawString>Pokorny, R. and Gott, S. 1990. The evaluation of a real-world instructional system: Using technical experts as raters. Technical report, Armstrong Laboratories, &apos;Brooks AFB.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>