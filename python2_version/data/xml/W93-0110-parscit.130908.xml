<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016686">
<title confidence="0.998356">
Acquiring Predicate-Argument Mapping
Information from Multilingual Texts
</title>
<author confidence="0.803856">
Chinatsu Aone, Douglas McKee
</author>
<affiliation confidence="0.656762">
Systems Research and Applications (SRA)
</affiliation>
<address confidence="0.7203275">
2000 15th Street North
Arlington, VA 22201
</address>
<email confidence="0.996048">
aonec@sra.com, mckeed@sra.com
</email>
<sectionHeader confidence="0.969768" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999324555555556">
This paper discusses automatic acquisition of predicate-argument mapping in-
formation from multilingual texts. The lexicon of our NLP system abstracts the
language-dependent portion of predicate-argument mapping information from the
core meaning of verb senses (i.e. semantic concepts as defined in the knowledge base).
We represent this mapping information in terms of cross-linguistically generalized
mapping types called situation types and word sense-specific idiosyncrasies. This
representation has enabled us to automatically acquire predicate-argument map-
ping information, specifically situation types and idiosyncrasies, for verbs in English,
Spanish, and Japanese texts.
</bodyText>
<sectionHeader confidence="0.998982" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999870105263158">
Lexicons for a natural language processing (NLP) system that perform syntactic and
semantic analysis require more than purely syntactic (e.g. part-of-speech information)
and semantic information (e.g. a concept hierarchy). Language understanding requires
mapping from syntactic structures into conceptual representation (henceforth predicate-
argument mapping), while language generation requires the inverse mapping. That is,
grammatical functions in the syntactic structures (e.g. subject, object, etc.) should be
mapped to thematic roles in the semantic structures (e.g. agent, theme, etc.).
In this paper, we discuss how we acquire such predicate-argument mapping information
from multilingual texts automatically (cf. Zernik and Jacobs work on collecting thematic
roles [20]). As discussed in Aone and Mckee [1], the lexicon of our NLP system abstracts
the language-dependent portion of predicate-argument mapping information from the core
meaning of verb senses (i.e. semantic concepts as defined in the knowledge base). We
represent this mapping information in terms of cross-linguistically generalized mapping
types called situation types and word sense-specific idiosyncrasies. This representation has
enabled us to automatically acquire predicate-argument mapping information, specifically
situation types and idiosyncrasies, for verbs in English, Spanish, and Japanese texts.
In the following sections, we first describe how we represent the predicate-mapping
information. Then, we discuss how we acquire situation type and idiosyncrasy information
automatically from multilingual texts and show some results.
</bodyText>
<sectionHeader confidence="0.971045" genericHeader="method">
2 Predicate-Argument Mapping Representation
</sectionHeader>
<bodyText confidence="0.988229">
Each lexical sense of a verb in our lexicon encodes its default predicate-argument mapping
type (i.e. situation type), any word-specific mapping exceptions (i.e. idiosyncrasies), and
</bodyText>
<page confidence="0.996925">
107
</page>
<note confidence="0.8597044">
# of required NP or S arguments default thematic roles prohibited thematic roles
CAUSED-PROCESS 2 Agent Theme -
PROCESS-OR-STATE 1 Theme Agent
AGENTIVE-ACTION 1 Agent
INVERSE-STATE 2 Goal Theme Agent
</note>
<tableCaption confidence="0.998967">
Table 1: Definitions of Situation Types
</tableCaption>
<table confidence="0.999026">
Enilish Spanish Japanese
CAUSED-PROCESS lull matar, mirar korosu, miru
PROCESS-OR-STATE die morir shibousuru
AGENTIVE-ACTION look ballet. odoru
INVERSE-STATE see ver mieru
</table>
<tableCaption confidence="0.998875">
Table 2: Situation Types and Verbs in Three Languages
</tableCaption>
<bodyText confidence="0.99205">
its semantic meaning (i.e. semantic concept) in addition to its morphological and syntactic
information. In the following, we discuss these three levels in detail.
</bodyText>
<subsectionHeader confidence="0.992065">
2.1 Situation Types
</subsectionHeader>
<bodyText confidence="0.999947434782609">
Each of a verb&apos;s lexical senses is classified into one of the four default predicate-argument
mapping types called situation types. As shown in Table 1, situation types of verbs are
defined by two kinds of information: 1) the number of subcategorized NP or S arguments
and 2) the types of thematic roles which these arguments should or should not map to.
Since this kind of information is applicable to verbs of any language, situation types are
language-independent predicate-argument mapping types. Thus, in any language, a verb
of type CAUSED-PROCESS has two arguments which map to AGENT and THEME in
the default case (e.g. &amp;quot;kill&amp;quot;). A verb of type PROCESS-OR-STATE has one argument
whose thematic role is THEME, and it does not allow AGENT as one of its thematic roles
(e.g. &amp;quot;die&amp;quot;). An AGENTIVE-ACTION verb also has one argument but the argument maps
to AGENT (e.g. &amp;quot;look&amp;quot;). Finally, an INVERSE-STATE verb has two arguments which
map to THEME and GOAL; it does not allow AGENT for its thematic role (e.g. &amp;quot;see&amp;quot;).
Examples from three languages are shown in Table 2.
Although verbs in different languages are classified into the same four situation types
using the same definition, mapping rules which map grammatical functions (i.e. subject,
object, etc.) in the syntactic structures1 to thematic roles in the semantic structures may
differ from one language to another. This is because languages do not necessarily express
the same thematic roles with the same grammatical functions. This mapping information
is language-specific (cf. Nirenburg and Levin [16]).
The default mapping rules for the four situation types are shown in Table 3. They are
nearly identical for the three languages (English, Spanish, and Japanese) we have analyzed
so far. The only difference is that in Japanese the THEME of an INVERSE-STATE verb
is expressed by marking the object NP with a particle &amp;quot;-ga&amp;quot; , which is usually a subject
</bodyText>
<footnote confidence="0.825291">
1We use structures similar to LFG&apos;s f-structures.
</footnote>
<page confidence="0.99035">
108
</page>
<table confidence="0.999415571428571">
English/Spanish Mapping Japanese Mapping
CAUSED-PROCESS AGENT (SURFACE SUBJECT) (SURFACE SUBJECT)
THEME SURFACE OBJECT) (SURFACE OBJECT)
PROCESS-OR-STATE THEME SURFACE SUBJECT SURFACE SUBJECT
AGENTIVE-ACTION AGENT SURFACE SUBJECT SURFACE SUBJECT
INVERSE-STATE GOAL SURFACE SUBJECT SURFACE SUBJECT
THEME (SURFACE OBJECT) (SURFACE OBJECT) (PARTICLE &amp;quot;GA&amp;quot;)
</table>
<tableCaption confidence="0.999674">
Table 3: Default Mapping Rules for Three Languages
</tableCaption>
<bodyText confidence="0.999614538461538">
marker (cf. Kuno [12]).2 3 So we add such information to the INVERSE-STATE mapping
rule for Japanese. Generalization expressed in situation types has saved us from defining
semantic mapping rules for each verb sense in each language, and also made it possible to
acquire them from large corpora automatically.
This classification system has been partially derived from Vendler and Dowty&apos;s as-
pectual classifications [19, 9] and Talmy&apos;s lexicalization patterns [18]. For example, all
AGENTIVE-ACTION verbs are so-called activity verbs, and so-called stative verbs fall
under either INVERSE-STATE (if transitive) or PROCESS-OR-STATE (if intransitive).
However, the situation types are not for specifying the semantics of aspect, which is ac-
tually a property of the whole sentence rather than a verb itself (cf. Kriflca [11], Dorr [8],
Moens and Steedman [15]). For instance, as shown below, the same verb can be classified
into two different aspectual classes (i.e. activity and accomplishment) depending on the
types of object NP&apos;s or existence of certain PP&apos;s.
</bodyText>
<listItem confidence="0.99971925">
(1) a. Sue drank wine for/*in an hour.
b. Sue drank a bottle of wine *for/in an hour.
(2) a. Harry climbed for/*in an hour.
b. Harry climbed to the top *for/in an hour.
</listItem>
<bodyText confidence="0.998842">
Situation types are intended to address the issue of cross-linguistic predicate-argument
mapping generalization, rather than the semantics of aspect.
</bodyText>
<subsectionHeader confidence="0.986316">
2.2 Idiosyncrasies
</subsectionHeader>
<bodyText confidence="0.999781428571428">
Idiosyncrasies slots in the lexicon specify word sense-specific idiosyncratic phenomena
which cannot be captured by semantic concepts or situation types. In particular, subcat-
egorized pre/postpositions of verbs are specified here. For example, the fact that &amp;quot;look&amp;quot;
denotes its THEME argument by the preposition &amp;quot;at&amp;quot; is captured by specifying idiosyn-
crasies. Examples of lexical entries with idiosyncrasies in English, Spanish and Japanese
are shown in Figure 1. As discussed in the next section, we derive this kind of word-specific
information automatically from corpora.
</bodyText>
<footnote confidence="0.7195068">
2There is a debate over whether the NP with &amp;quot;ga&amp;quot; is a subject or object. However, our approach can
accommodate either analysis.
3The GOAL of some INVERSE-STATE verbs in Japanese can be expressed by a &amp;quot;ni&amp;quot; postpositional
phrase. However, as Kuno [12] points out, since this is an idiosyncratic phenomenon, such information
does not go to the default mapping rule.
</footnote>
<page confidence="0.985123">
109
</page>
<figure confidence="0.6506118125">
(LOOK (CATEGORY . V)
(SENSE-NAME. LOOK-1)
(SEMANTIC-CONCEPT #LOOK#)
(IDIOSYNCRASIES (THEME (MAPPING (LITERAL &amp;quot;AT&amp;quot;))))
(SITUATION-TYPE AGENTIVE-ACTION))
(INFECTAR (CATEGORY. V)
(SENSE-NAME. INFECTAR-1)
(SEMANTIC-CONCEPT #INFECT#)
(IDIOSYNCRASIES (THEME (MAPPING (LITERAL &amp;quot;CON&amp;quot; &amp;quot;DE&amp;quot;)))
(GOAL (MAPPING (SURFACE 013.1ECT))))
(SITUATION-TYPE CAUSED-PROCESS))
(NARU (CATEGORY . V)
(SENSE-NAME . NARU-1)
(SEMANTIC-CONCEPT #BECOME#)
(IDIOSYNCRASIES (GOAL (MAPPING (LITERAL &amp;quot;TO&amp;quot; &amp;quot;NI&amp;quot;))))
(SITUATION-TYPE PROCESS-OR-STATE))
</figure>
<figureCaption confidence="0.998633">
Figure 1: Lexical entries for &amp;quot;look&amp;quot;, &amp;quot;infectar&amp;quot;, and &amp;quot;naru&amp;quot;
</figureCaption>
<subsectionHeader confidence="0.997881">
2.3 Semantic Concepts
</subsectionHeader>
<bodyText confidence="0.999943888888889">
Each lexical meaning of a verb is represented by a semantic concept (or frame) in our
language-independent knowledge base, which is similar to the one described in Onyshkevych
and Nirenburg [17]. Each verb frame has thematic role slots, which have two facets,
TYPE and MAPPING. A TYPE facet value of a given slot provides a constraint on the
type of objects which can be the value of the slot. In the MAPPING facets, we have
encoded some cross-linguistically general predicate-argument mapping information. For
example, we have defined that all the subclasses of #COMMUNICATION-EVENT# (e.g.
#REPORT#, #CONFIRM#, etc.) map their sentential complements (SENT-COMP)
to THEME, as shown below.
</bodyText>
<table confidence="0.724559142857143">
(#COMMUNICATION-EVENT#
(AKO #DYNAMIC-SITUATION#)
(AGENT (TYPE #PERSON# #ORGANIZATIONC)
(THEME (TYPE #SITUATION# #ENTITY#)
(MAPPING (SENT-COMP T)))
(GOAL (TYPE #PERSON# #ORGANIZATION#)
(MAPPING (P-ARG GOAL))))
</table>
<subsectionHeader confidence="0.998318">
2.4 Merging Predicate-Argument Mapping Information
</subsectionHeader>
<bodyText confidence="0.999833857142857">
For each verb, the information stored in the three levels discussed above is merged to form
a complete set of mapping rules. During this merging process, the idiosyncrasies take
precedence over the situation types and the semantic concepts, and the situation types
over the semantic concepts. For example, the two derived mapping rules for &amp;quot;break&amp;quot; (i.e.
one for &amp;quot;break&amp;quot; as in &amp;quot;John broke the window&amp;quot; and the other for &amp;quot;break&amp;quot; as in &amp;quot;The
window broke&amp;quot;) are shown in Figure 2. Notice that the semantic TYPE restriction and
INSTRUMENT role stored in the knowledge base are also inherited at this time.
</bodyText>
<page confidence="0.994844">
110
</page>
<table confidence="0.993634857142857">
eBREAKa
(AGENT (TYPE *CREATURE. GORGANIZATIONO)
(THEME (TYPE tENTITY.))
(INSTRUMENT (TYPE ePHYSICAL -OBJECT.) 103
(MAPPING (P-ARG INSTRUMENT)))
CAUSED-PROCESS PROCESS-OR-STATE Situation Typos
V
&amp;quot;break&amp;quot; &amp;quot;ALTA. con
(AGENT (TYPE (iCREATUREe hDRGANIZATIONI)) (THEME (TYPE 41ENTITY#)
(MAPPING (SURFACE SUBJECT)) (MAPPING (SURFACE SUBJECT)))
(THEME (TYPE (PENTITYe) (INSTRUMENT (TYPE (4PHYSICAL-OBJECTe))
(MAPPING (SURFACE OBJECT))) (MAPPING (P-ARG INSTRUMENT)))
(INSTRUMENT (TYPE #PHYSICAL-OBJECTe))
(MAPPING (P-ARG INSTRUMENT)))
</table>
<figureCaption confidence="0.8739295">
Figure 2: Information from the KB, the situation type, and the lexicon all combine to
form two predicate-argument mappings for the verb &amp;quot;break.&amp;quot;
</figureCaption>
<sectionHeader confidence="0.978161" genericHeader="method">
3 Automatic Acquisition from Corpora
</sectionHeader>
<bodyText confidence="0.999902333333333">
In order to expand our lexicon to the size needed for broad coverage and to be able to tune
the system to specific domains quickly, we have implemented algorithms to automatically
build multilingual lexicons from corpora. In this section, we discuss how the situation
types and lexical idiosyncrasies are determined for verbs.
Our overall approach is to use simple robust parsing techniques that depend on a
few language-dependent syntactic heuristics (e.g. in English and Spanish, a verb&apos;s object
usually directly follows the verb), and a dictionary for part of speech information. We
have used these techniques to acquire information from English, Spanish, and Japanese
corpora varying in length from about 25000 words to 2.7 million words.
</bodyText>
<subsectionHeader confidence="0.999514">
3.1 Acquiring Situation Type Information
</subsectionHeader>
<bodyText confidence="0.9998168">
We use two surface features to restrict the possible situation types of a verb: the verb&apos;s
transitivity rating and its subject animacy.
The transitivity rating of a verb is defined to be the number of transitive occurrences
in the corpus divided by the total occurrences of the verb. In English, a verb appears in
the transitive when either:
</bodyText>
<listItem confidence="0.989324857142857">
• The verb is directly followed by a noun, determiner, personal pronoun, adjective, or
wh-pronoun (e.g. &amp;quot;John owns a cow.&amp;quot;)
• The verb is directly followed by a &amp;quot;THAT&amp;quot; as a subordinate conjunction (e.g. &amp;quot;John
said that he liked llamas.&amp;quot;)
• The verb is directly followed by an infinitive (e.g. &amp;quot;John promised to walk the dog.&amp;quot;)
• The verb past participle is preceded by &amp;quot;BE,&amp;quot; as would occur in a passive construc-
tion (e.g. &amp;quot;The apple was eaten by the pig.&amp;quot;)
</listItem>
<page confidence="0.975046">
111
</page>
<table confidence="0.999929980392157">
verb occs TR SA Pred. ST Correct ST Prepositional Idio
SUFFICE 8 0.6250 0.0000 S) IS)
TIME 15 0.8333 1.0000 CP IS CP
TRAIN 20 1.0000 1.0000 CP IS CP PS) at
WRAP 22 0.7222 0.6667 CP IS CP) up over in with
SORT 25 0.4211 1.0000 CP IS AA PS) CP AA) out
UNITE 27 0.5833 1.0000 CP IS AA PS) CP AA)
TRANSPORT 28 0.8571 0.6667 CP IS) CP)
SUSTAIN 32 0.9062 0.6842 CP IS) CP)
SUBSTITUTE 33 0.7500 0.5000 IS) CP PS) for
TARGET 36 0.7778 0.8000 CP IS Cl&apos;&apos;
STORE 36 0.9091 1.0000 CP IS CP on
STEAL 36 0.9167 0.6667 CP IS CP from
SHUT 36 0.2400 0.5000 IS PS CP PS) up for
STRETCH 53 0.5278 0.5000 IS PS CP PS) over into out from
STRIP 57 0.7609 0.8571 CP IS CP) from into of
THREATEN 58 0.8793 0.4419 IS) CP IS) over
WEAR 61 0.8033 0.6667 CP IS IS) over
TREAT 77 0.8052 0.8000 CP IS CP) as
TERMINATE 79 0.9726 1.0000 CP IS CP PS)
WEIGH 81 0.2069 0.5294 IS PS CP PS) on with into
TEACH 82 0.7794 0.6875 CP IS CP) at
SURROUND 85 0.8000 0.6667 CP IS CP)
TOTAL 97 0.0515 0.2759 PS) CP PS) at
VARY 112 0.1354 0.0294 IS PS) CP PS) from over
WAIT 130 0.1923 1.0000 CP IS AA PS) AA) for up
SPEAK 139 0.1667 0.7500 CP IS AA PS) AA CP) out at up
SURVIVE 146 0.4754 0.3846 IS PS) IS PS)
UNDERSTAND 180 0.6946 0.8684 CP IS) S)
SURGE 188 0.0182 0.3125 PS) PS)
SUPPLY 188 0.7176 0.8571 CP IS) CP) with
SIT 199 0.0625 0.7027 AA PS) AA PS) on with at out in up
TEND 200 0.8594 0.4340 IS) CP IS)
BREAK 219 0.4771 0.5000 IS PS) CP PS) up into out
WRITE 243 0.4637 0.9123 CP IS AA PS) CP AA) off
WATCH 268 0.7069 0.8462 CP IS) CP) out over
SUCCEED 277 0.5379 0.8899 CP IS AA PS CP PS)
STAY 300 0.2156 0.6604 CP IS AA PS PS) out up on with at
STAND 310 0.2841 0.7237 CP IS AA PS PS CP AA) up at as out on
TELL 368 0.8054 0.8101 CP IS) CP)
SPEND 445 0.3823 0.8125 CP IS AA PS) CP) on over
SUPPORT 454 0.8486 0.5370 IS) CP IS
SUGGEST 570 0.7782 0.5918 IS) CP IS
TURN 852 0.3418 0.5891 IS PS) CP PS out into up over
START 890 0.3474 0.6221 CP IS AA PS) CP PS with off out
LOOK 1084 0.1718 0.6520 CP IS AA PS) AA PS at into for up
THINK 1227 0.7602 0.9237 CP IS CP)
TRY 1272 0.7904 0.8743 CP IS CP)
WANT 1659 0.8559 0.8787 CP IS IS)
USE 2211 0.8416 0.7725 CP IS CP)
TAKE 2525 0.7447 0.5933 IS) CP IS) over off out into up
</table>
<tableCaption confidence="0.999643">
Table 4: Automatically Derived Situation Type and Idiosyncrasy Data
</tableCaption>
<page confidence="0.974618">
112
</page>
<figure confidence="0.999596486486487">
Transitivity:
CP/IS
0.0 0.1 0.2 6.3 0.4
Ambig • • • • • .
0.0 6.1 0.2 0.3
AA/PS
0.0 0.1 .07 0.3
2
Subject Animacy:
CP/A
0.0 0.1 0.2 0.3
Ambig. 6 . .1 0.2 0.3
0.0
IS/PS •
0.0 0.1 0.2 0.3
0.5 d.; 0.7 .079
•• 0.8
09 •
0.5 6.6 6.7. 6.8 0.9
- 6.6 0.7 6.8 6.9
0.5 00
0.5 0.6 0.7 0.8 0.9
&amp;quot; . 0..7 6.8 a..
0.5 076
0.5 0.6 0.7 0.8 0.9
•
6.4
0.4
1.0
1.0
1.0
1.0
0.4
&amp;quot;
0.4
•
0.4
</figure>
<figureCaption confidence="0.999891">
Figure 3: This graph shows the accuracy of the Transitivity and Subject Animacy metrics.
</figureCaption>
<bodyText confidence="0.9329488">
For Spanish, we use a very similar algorithm, and for Japanese, we look for noun
phrases with an object marker &amp;quot;-wo&amp;quot; near and to the left of the verb. A high transitivity
is correlated with CAUSED-PROCESS and INVERSE-STATE while a low transitivity
correlates with AGENTIVE-ACTION and PROCESS-OR-STATE. Table 4 shows 50 verbs
and their calculated transitivity rating. Figure 3 shows that for all but one of the verbs
that are unambiguously transitive the transitivity rating is above 0.6. The verb &amp;quot;spend&amp;quot;
has a transitivity rating of 0.38 because most of its direct objects are numeric dollar
amounts. Phrases which begin with a number are not recognized as direct objects, since
most numeric amounts following verbs are adjuncts as in &amp;quot;John ran 3 miles.&amp;quot;
We define a verb&apos;s subject animacy to be the number of times the verb appears with
an animate subject over the total occurrences of the verb where we identified the subject.
Any noun or pronoun directly preceding a verb is considered to be its subject. This
heuristic fails in cases where the subject NP is modified by a PP or relative clause as in
&amp;quot;The man under the car wore a red shirt.&amp;quot; We have only implemented this metric for
English. The verb&apos;s subject is considered to be animate if it is any one of the following:
</bodyText>
<listItem confidence="0.998383666666667">
• A personal pronoun (&amp;quot;it&amp;quot; and &amp;quot;they&amp;quot; were excluded, since they may refer back to
inanimate objects.)
• A proper name
• A word under &amp;quot;agent&amp;quot; or &amp;quot;people&amp;quot; in WordNet (cf. [14])
• A word that appears in a MUC-4 template slot that can be filled only with humans
(cf. [7])
</listItem>
<bodyText confidence="0.9945595">
Verbs that have a low subject animacy cannot be either CAUSED-PROCESS or
AGENTIVE-ACTION, since the syntactic subject must map to the AGENT thematic
</bodyText>
<equation confidence="0.724933">
1 1 3
</equation>
<bodyText confidence="0.99437075">
role. A high subject animacy does not correlate with any particular situation type, since
several stative verbs take only animate subjects (e.g. perception verbs).
The predicted situation types shown in Figure 3 were calculated with the following
algorithm:
</bodyText>
<listItem confidence="0.998760857142857">
1. Assume that the verb can occur with every situation type.
2. If the transitivity rating is greater than 0.6, then discard the AGENTIVE-ACTION
and PROCESS-OR-STATE possibilities.
3. If the transitivity rating is below 0.1, then discard the CAUSED-PROCESS and
INVERSE-STATE possibilities.
4. If the subject animacy is below 0.6, then discard the CAUSED-PROCESS and
AGENTIVE-ACTION possibilities.
</listItem>
<bodyText confidence="0.9966745">
We are planning several improvements to our situation type determination algorithms.
First, because some stative verbs can take animate subjects (e.g. perception verbs like
&amp;quot;see&amp;quot;, &amp;quot;know&amp;quot;, etc.), we sometimes cannot distinguish between INVERSE-STATE or
PROCESS-OR-STATE and CAUSED-PROCESS or AGENTIVE-ACTION verbs. This
problem, however, can be solved by using algorithms by Brent [3] or Dorr [8] for identifying
stative verbs.
Second, verbs ambiguous between CAUSED-PROCESS and PROCESS-OR-STATE
(e.g. &amp;quot;break&amp;quot;, &amp;quot;vary&amp;quot;) often get inconclusive results because they appear transitively about
50% of the time. When these verbs are transitive, the subjects are almost always animate
and when they are intransitive, the subjects are nearly always inanimate. We plan to
recognize these situations by calculating animacy separately for transitive and intransitive
cases.
</bodyText>
<subsectionHeader confidence="0.99982">
3.2 Acquiring Idiosyncratic Information
</subsectionHeader>
<bodyText confidence="0.999955833333333">
We automatically identify likely pre/postpositional argument structures for a given verb
by looking for pre/postpositions in places where they are likely to attach to the verb (i.e.
within a few words to the right for Spanish and English, and to the left for Japanese).
When a particular pre/postposition appears here much more often than chance (based
on either Mutual Information or a chi-squared test [5, 4]), we assume that it is a likely
argument. A very similar strategy works well at identifying verbs that take sentential
complements by looking for complementizers (e.g. &amp;quot;that&amp;quot;, &amp;quot;to&amp;quot;) in positions of likely
attachment. Some English examples are shown in Tables 4 and 5, and Spanish examples
are shown in Tables 6 and 7. The details of the exact algorithms used for English are
contained in McKee and Maloney [13]. Areas for improvement include distinguishing
between cases where a verb takes a prepositional arguments, a prepositional particle, or
a common adjunct.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999772333333333">
We have automatically built lexicons with predicate-argument mapping information from
English, Spanish and Japanese corpora. These lexicons have been used for several multi-
lingual data extraction applications (cf. Aone et al. [2]) and a prototype Japanese-English
</bodyText>
<page confidence="0.994441">
114
</page>
<table confidence="0.999789166666667">
word possible clausal complements
know THATCOMP
vow THATCOMP, TOCOMP
eat —
want TOCOMP
resume INGCOMP
</table>
<tableCaption confidence="0.979202">
Table 5: English Verbs which Take Complementizers
</tableCaption>
<table confidence="0.997640571428571">
verb MI with &amp;quot;que&amp;quot;
indicar 9.3
sefi al ar 8.7
estimar 8.6
calcular 7.7
precisar 7.7
anunciar 7.7
</table>
<tableCaption confidence="0.853753">
Table 6: Spanish Verbs which Take Complementizers
</tableCaption>
<table confidence="0.999596769230769">
verb preposition MI between verb and preposition
luchar contra 12.4
unir contra 8.9
vacunar contra 8.9
cifrar sobre 9.6
consult ar sobre 9.6
pasar sobre 8.6
acordar con 10.8
contar con 10.3
relacionar con 9.7
notificar en 8.7
ocurrir en 8.0
encontrar en 7.8
</table>
<tableCaption confidence="0.997844">
Table 7: Spanish Verbs that Take Prepositional Arguments
</tableCaption>
<page confidence="0.987407">
5
</page>
<bodyText confidence="0.999771333333333">
machine translation system. The algorithms presented here have minimized our lexical
acquisition effort considerably.
Currently we are investigating ways in which thematic role slots of verb frames and
semantic type restrictions on these slots can be derived automatically from corpora (cf.
Dagan and Itai [6], Hindle and Rooth [10], Zernik and Jacobs [20]) so that knowledge
acquisition at all three levels of predicate-argument mapping can be automated.
</bodyText>
<sectionHeader confidence="0.999191" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999324784313726">
[1] Chinatsu Aone and Doug McKee. Three-Level Knowledge Representation of
Predicate-Argument Mapping for Multilingual Lexicons. In AAAI Spring Sympo-
sium Working Notes on Building Lexicons for Machine Translation, 1993.
[2] Chinatsu Aone, Doug McKee, Sandy Shinn, and Hatte Blejer. SRA: Description
of the SOLOMON System as Used for MUC-4. In Proceedings of Fourth Message
Understanding Conference (MUG-4), 1992.
[3] Michael Brent. Automatic Semantic Classification of Verbs from Their Syntactic
Contexts: An Implemented Classifier for Stativity. In Proceedings of the 5th European
ACL Conference, 1991.
[4] Kenneth Church and William Gale. Concordances for Parallel Text. In Proceedings
of the Seventh Annual Conference of the University of Waterloo Centre for the New
OED and Text Research: Using Corpora, 1991.
[5] Kenneth Church and Patrick Hanks. Word Association Norms, Mutual Information,
and Lexicography. Computational Linguistics, 16(1), 1990.
[6] Ido Dagan and Alon Itai. Automatic Acquisition of Constraints for the Resolution
of Anaphora References and Syntactic Ambiguities. In Proceedings of the 13th Inter-
national Conference on Computational Linguistics, 1990.
[7] Defense Advanced Research Projects Agency. Proceedings of Fourth Message Under-
standing Conference (MUC-4). Morgan Kaufmann Publishers, 1992.
[8] Bonnie Dorr. A Parameterized Approach to Integrating Aspect with Lexical-
Semantics for Machine Translation. In Proceedings of 30th Annual Meeting of the
ACL, 1992.
[9] David Dowty. Word Meaning and Montague Grammar. D. Reidel, 1979.
[10] Donald Ifindle and Mats Rooth. Structural Ambiguity and Lexical Relations. In
Proceedings of 29th Annual Meeting of the ACL, 1991.
[11] Manfred Krifka. Nominal Reference, Temporal Construction, and Quantification in
Event Semantics. In R. Bartsch et al., editors, Semantics and Contextual Expressions.
Foris, Dordrecht, 1989.
[12] Susumu Kuno. The Structure of the Japanese Language. MIT Press, 1973.
1 1 6
[13] Doug McKee and John Maloney. Using Statistics Gained from Corpora in
a Knowledge-Based NLP System. In Proceedings of The AAAI Workshop on
Statistically-Based NLP Techniques, 1992.
[14] George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine
Miller. Five papers on WordNet. Technical Report CSL Report 43, Cognitive Science
Laboratory, Princeton University, 1990.
[15] Marc Moens and Mark Steedman. Temporal ontology and temporal reference. Com-
putational Linguistics, 14(2), 1988.
[16] Sergei Nirenburg and Lori Levin. Syntax-Driven and Ontology-Driven Lexical Se-
mantics. In Proceedings of ACL Lexical Semantics and Knowledge Representation
Workshop, 1991.
[17] Boyan Onyshkevych and Sergei Nirenburg. Lexicon, Ontology and Text Meaning.
In Proceedings of ACL Lexical Semantics and Knowledge Representation Workshop,
1991.
[18] Leonard Talmy. Lexicalization Patterns: Semantic Structure in Lexical Forms. In
Timothy Shopen, editor, Language Typology and Syntactic Descriptions. Cambridge
University Press, 1985.
[19] Zeno Vendler. Linguistics in Philosophy. Cornell University Press, 1967.
[20] Uri Zernik and Paul Jacobs. Tagging for Learning: Collecting Thematic Relations
from Corpus. In Proceedings of the 13th International Conference on Computational
Linguistics, 1990.
</reference>
<page confidence="0.685225">
116 a
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.503683">
<title confidence="0.997871">Acquiring Predicate-Argument Information from Multilingual Texts</title>
<author confidence="0.990225">Chinatsu Aone</author>
<author confidence="0.990225">Douglas McKee</author>
<affiliation confidence="0.9805">Systems Research and Applications</affiliation>
<address confidence="0.9844545">2000 15th Street Arlington, VA</address>
<email confidence="0.998498">aonec@sra.com,mckeed@sra.com</email>
<abstract confidence="0.9489922">This paper discusses automatic acquisition of predicate-argument mapping information from multilingual texts. The lexicon of our NLP system abstracts the language-dependent portion of predicate-argument mapping information from the meaning of verb senses semantic concepts defined in the knowledge base). We represent this mapping information in terms of cross-linguistically generalized types called types word sense-specific representation has enabled us to automatically acquire predicate-argument mapping information, specifically situation types and idiosyncrasies, for verbs in English, Spanish, and Japanese texts.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chinatsu Aone</author>
<author>Doug McKee</author>
</authors>
<title>Three-Level Knowledge Representation of Predicate-Argument Mapping for Multilingual Lexicons.</title>
<date>1993</date>
<booktitle>In AAAI Spring Symposium Working Notes on Building Lexicons for Machine Translation,</booktitle>
<contexts>
<context position="1732" citStr="[1]" startWordPosition="223" endWordPosition="223">hierarchy). Language understanding requires mapping from syntactic structures into conceptual representation (henceforth predicateargument mapping), while language generation requires the inverse mapping. That is, grammatical functions in the syntactic structures (e.g. subject, object, etc.) should be mapped to thematic roles in the semantic structures (e.g. agent, theme, etc.). In this paper, we discuss how we acquire such predicate-argument mapping information from multilingual texts automatically (cf. Zernik and Jacobs work on collecting thematic roles [20]). As discussed in Aone and Mckee [1], the lexicon of our NLP system abstracts the language-dependent portion of predicate-argument mapping information from the core meaning of verb senses (i.e. semantic concepts as defined in the knowledge base). We represent this mapping information in terms of cross-linguistically generalized mapping types called situation types and word sense-specific idiosyncrasies. This representation has enabled us to automatically acquire predicate-argument mapping information, specifically situation types and idiosyncrasies, for verbs in English, Spanish, and Japanese texts. In the following sections, we</context>
</contexts>
<marker>[1]</marker>
<rawString>Chinatsu Aone and Doug McKee. Three-Level Knowledge Representation of Predicate-Argument Mapping for Multilingual Lexicons. In AAAI Spring Symposium Working Notes on Building Lexicons for Machine Translation, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chinatsu Aone</author>
<author>Doug McKee</author>
<author>Sandy Shinn</author>
<author>Hatte Blejer</author>
</authors>
<title>SRA: Description of the SOLOMON System as Used for MUC-4.</title>
<date>1992</date>
<booktitle>In Proceedings of Fourth Message Understanding Conference (MUG-4),</booktitle>
<contexts>
<context position="19761" citStr="[2]" startWordPosition="3114" endWordPosition="3114">y attachment. Some English examples are shown in Tables 4 and 5, and Spanish examples are shown in Tables 6 and 7. The details of the exact algorithms used for English are contained in McKee and Maloney [13]. Areas for improvement include distinguishing between cases where a verb takes a prepositional arguments, a prepositional particle, or a common adjunct. 4 Conclusion We have automatically built lexicons with predicate-argument mapping information from English, Spanish and Japanese corpora. These lexicons have been used for several multilingual data extraction applications (cf. Aone et al. [2]) and a prototype Japanese-English 114 word possible clausal complements know THATCOMP vow THATCOMP, TOCOMP eat — want TOCOMP resume INGCOMP Table 5: English Verbs which Take Complementizers verb MI with &amp;quot;que&amp;quot; indicar 9.3 sefi al ar 8.7 estimar 8.6 calcular 7.7 precisar 7.7 anunciar 7.7 Table 6: Spanish Verbs which Take Complementizers verb preposition MI between verb and preposition luchar contra 12.4 unir contra 8.9 vacunar contra 8.9 cifrar sobre 9.6 consult ar sobre 9.6 pasar sobre 8.6 acordar con 10.8 contar con 10.3 relacionar con 9.7 notificar en 8.7 ocurrir en 8.0 encontrar en 7.8 Tabl</context>
</contexts>
<marker>[2]</marker>
<rawString>Chinatsu Aone, Doug McKee, Sandy Shinn, and Hatte Blejer. SRA: Description of the SOLOMON System as Used for MUC-4. In Proceedings of Fourth Message Understanding Conference (MUG-4), 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Brent</author>
</authors>
<title>Automatic Semantic Classification of Verbs from Their Syntactic Contexts: An Implemented Classifier for Stativity.</title>
<date>1991</date>
<booktitle>In Proceedings of the 5th European ACL Conference,</booktitle>
<contexts>
<context position="18025" citStr="[3]" startWordPosition="2853" endWordPosition="2853">es. 3. If the transitivity rating is below 0.1, then discard the CAUSED-PROCESS and INVERSE-STATE possibilities. 4. If the subject animacy is below 0.6, then discard the CAUSED-PROCESS and AGENTIVE-ACTION possibilities. We are planning several improvements to our situation type determination algorithms. First, because some stative verbs can take animate subjects (e.g. perception verbs like &amp;quot;see&amp;quot;, &amp;quot;know&amp;quot;, etc.), we sometimes cannot distinguish between INVERSE-STATE or PROCESS-OR-STATE and CAUSED-PROCESS or AGENTIVE-ACTION verbs. This problem, however, can be solved by using algorithms by Brent [3] or Dorr [8] for identifying stative verbs. Second, verbs ambiguous between CAUSED-PROCESS and PROCESS-OR-STATE (e.g. &amp;quot;break&amp;quot;, &amp;quot;vary&amp;quot;) often get inconclusive results because they appear transitively about 50% of the time. When these verbs are transitive, the subjects are almost always animate and when they are intransitive, the subjects are nearly always inanimate. We plan to recognize these situations by calculating animacy separately for transitive and intransitive cases. 3.2 Acquiring Idiosyncratic Information We automatically identify likely pre/postpositional argument structures for a giv</context>
</contexts>
<marker>[3]</marker>
<rawString>Michael Brent. Automatic Semantic Classification of Verbs from Their Syntactic Contexts: An Implemented Classifier for Stativity. In Proceedings of the 5th European ACL Conference, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>William Gale</author>
</authors>
<title>Concordances for Parallel Text.</title>
<date>1991</date>
<booktitle>In Proceedings of the Seventh Annual Conference of the University of Waterloo Centre for the New OED and Text Research: Using Corpora,</booktitle>
<contexts>
<context position="18954" citStr="[5, 4]" startWordPosition="2989" endWordPosition="2990">e, the subjects are nearly always inanimate. We plan to recognize these situations by calculating animacy separately for transitive and intransitive cases. 3.2 Acquiring Idiosyncratic Information We automatically identify likely pre/postpositional argument structures for a given verb by looking for pre/postpositions in places where they are likely to attach to the verb (i.e. within a few words to the right for Spanish and English, and to the left for Japanese). When a particular pre/postposition appears here much more often than chance (based on either Mutual Information or a chi-squared test [5, 4]), we assume that it is a likely argument. A very similar strategy works well at identifying verbs that take sentential complements by looking for complementizers (e.g. &amp;quot;that&amp;quot;, &amp;quot;to&amp;quot;) in positions of likely attachment. Some English examples are shown in Tables 4 and 5, and Spanish examples are shown in Tables 6 and 7. The details of the exact algorithms used for English are contained in McKee and Maloney [13]. Areas for improvement include distinguishing between cases where a verb takes a prepositional arguments, a prepositional particle, or a common adjunct. 4 Conclusion We have automatically </context>
</contexts>
<marker>[4]</marker>
<rawString>Kenneth Church and William Gale. Concordances for Parallel Text. In Proceedings of the Seventh Annual Conference of the University of Waterloo Centre for the New OED and Text Research: Using Corpora, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Church</author>
<author>Patrick Hanks</author>
</authors>
<title>Word Association Norms, Mutual Information, and Lexicography.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<contexts>
<context position="18954" citStr="[5, 4]" startWordPosition="2989" endWordPosition="2990">e, the subjects are nearly always inanimate. We plan to recognize these situations by calculating animacy separately for transitive and intransitive cases. 3.2 Acquiring Idiosyncratic Information We automatically identify likely pre/postpositional argument structures for a given verb by looking for pre/postpositions in places where they are likely to attach to the verb (i.e. within a few words to the right for Spanish and English, and to the left for Japanese). When a particular pre/postposition appears here much more often than chance (based on either Mutual Information or a chi-squared test [5, 4]), we assume that it is a likely argument. A very similar strategy works well at identifying verbs that take sentential complements by looking for complementizers (e.g. &amp;quot;that&amp;quot;, &amp;quot;to&amp;quot;) in positions of likely attachment. Some English examples are shown in Tables 4 and 5, and Spanish examples are shown in Tables 6 and 7. The details of the exact algorithms used for English are contained in McKee and Maloney [13]. Areas for improvement include distinguishing between cases where a verb takes a prepositional arguments, a prepositional particle, or a common adjunct. 4 Conclusion We have automatically </context>
</contexts>
<marker>[5]</marker>
<rawString>Kenneth Church and Patrick Hanks. Word Association Norms, Mutual Information, and Lexicography. Computational Linguistics, 16(1), 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Automatic Acquisition of Constraints for the Resolution of Anaphora References and Syntactic Ambiguities.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<marker>[6]</marker>
<rawString>Ido Dagan and Alon Itai. Automatic Acquisition of Constraints for the Resolution of Anaphora References and Syntactic Ambiguities. In Proceedings of the 13th International Conference on Computational Linguistics, 1990.</rawString>
</citation>
<citation valid="true">
<title>Defense Advanced Research Projects Agency.</title>
<date>1992</date>
<booktitle>Proceedings of Fourth Message Understanding Conference (MUC-4).</booktitle>
<publisher>Morgan Kaufmann Publishers,</publisher>
<contexts>
<context position="16830" citStr="[7]" startWordPosition="2680" endWordPosition="2680">pronoun directly preceding a verb is considered to be its subject. This heuristic fails in cases where the subject NP is modified by a PP or relative clause as in &amp;quot;The man under the car wore a red shirt.&amp;quot; We have only implemented this metric for English. The verb&apos;s subject is considered to be animate if it is any one of the following: • A personal pronoun (&amp;quot;it&amp;quot; and &amp;quot;they&amp;quot; were excluded, since they may refer back to inanimate objects.) • A proper name • A word under &amp;quot;agent&amp;quot; or &amp;quot;people&amp;quot; in WordNet (cf. [14]) • A word that appears in a MUC-4 template slot that can be filled only with humans (cf. [7]) Verbs that have a low subject animacy cannot be either CAUSED-PROCESS or AGENTIVE-ACTION, since the syntactic subject must map to the AGENT thematic 1 1 3 role. A high subject animacy does not correlate with any particular situation type, since several stative verbs take only animate subjects (e.g. perception verbs). The predicted situation types shown in Figure 3 were calculated with the following algorithm: 1. Assume that the verb can occur with every situation type. 2. If the transitivity rating is greater than 0.6, then discard the AGENTIVE-ACTION and PROCESS-OR-STATE possibilities. 3. I</context>
</contexts>
<marker>[7]</marker>
<rawString>Defense Advanced Research Projects Agency. Proceedings of Fourth Message Understanding Conference (MUC-4). Morgan Kaufmann Publishers, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bonnie Dorr</author>
</authors>
<title>A Parameterized Approach to Integrating Aspect with LexicalSemantics for Machine Translation.</title>
<date>1992</date>
<booktitle>In Proceedings of 30th Annual Meeting of the ACL,</booktitle>
<contexts>
<context position="6620" citStr="[8]" startWordPosition="944" endWordPosition="944">guage, and also made it possible to acquire them from large corpora automatically. This classification system has been partially derived from Vendler and Dowty&apos;s aspectual classifications [19, 9] and Talmy&apos;s lexicalization patterns [18]. For example, all AGENTIVE-ACTION verbs are so-called activity verbs, and so-called stative verbs fall under either INVERSE-STATE (if transitive) or PROCESS-OR-STATE (if intransitive). However, the situation types are not for specifying the semantics of aspect, which is actually a property of the whole sentence rather than a verb itself (cf. Kriflca [11], Dorr [8], Moens and Steedman [15]). For instance, as shown below, the same verb can be classified into two different aspectual classes (i.e. activity and accomplishment) depending on the types of object NP&apos;s or existence of certain PP&apos;s. (1) a. Sue drank wine for/*in an hour. b. Sue drank a bottle of wine *for/in an hour. (2) a. Harry climbed for/*in an hour. b. Harry climbed to the top *for/in an hour. Situation types are intended to address the issue of cross-linguistic predicate-argument mapping generalization, rather than the semantics of aspect. 2.2 Idiosyncrasies Idiosyncrasies slots in the lexi</context>
<context position="18037" citStr="[8]" startWordPosition="2856" endWordPosition="2856">e transitivity rating is below 0.1, then discard the CAUSED-PROCESS and INVERSE-STATE possibilities. 4. If the subject animacy is below 0.6, then discard the CAUSED-PROCESS and AGENTIVE-ACTION possibilities. We are planning several improvements to our situation type determination algorithms. First, because some stative verbs can take animate subjects (e.g. perception verbs like &amp;quot;see&amp;quot;, &amp;quot;know&amp;quot;, etc.), we sometimes cannot distinguish between INVERSE-STATE or PROCESS-OR-STATE and CAUSED-PROCESS or AGENTIVE-ACTION verbs. This problem, however, can be solved by using algorithms by Brent [3] or Dorr [8] for identifying stative verbs. Second, verbs ambiguous between CAUSED-PROCESS and PROCESS-OR-STATE (e.g. &amp;quot;break&amp;quot;, &amp;quot;vary&amp;quot;) often get inconclusive results because they appear transitively about 50% of the time. When these verbs are transitive, the subjects are almost always animate and when they are intransitive, the subjects are nearly always inanimate. We plan to recognize these situations by calculating animacy separately for transitive and intransitive cases. 3.2 Acquiring Idiosyncratic Information We automatically identify likely pre/postpositional argument structures for a given verb by l</context>
</contexts>
<marker>[8]</marker>
<rawString>Bonnie Dorr. A Parameterized Approach to Integrating Aspect with LexicalSemantics for Machine Translation. In Proceedings of 30th Annual Meeting of the ACL, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Word Meaning and</title>
<date>1979</date>
<contexts>
<context position="6212" citStr="[19, 9]" startWordPosition="883" endWordPosition="884">CE SUBJECT SURFACE SUBJECT INVERSE-STATE GOAL SURFACE SUBJECT SURFACE SUBJECT THEME (SURFACE OBJECT) (SURFACE OBJECT) (PARTICLE &amp;quot;GA&amp;quot;) Table 3: Default Mapping Rules for Three Languages marker (cf. Kuno [12]).2 3 So we add such information to the INVERSE-STATE mapping rule for Japanese. Generalization expressed in situation types has saved us from defining semantic mapping rules for each verb sense in each language, and also made it possible to acquire them from large corpora automatically. This classification system has been partially derived from Vendler and Dowty&apos;s aspectual classifications [19, 9] and Talmy&apos;s lexicalization patterns [18]. For example, all AGENTIVE-ACTION verbs are so-called activity verbs, and so-called stative verbs fall under either INVERSE-STATE (if transitive) or PROCESS-OR-STATE (if intransitive). However, the situation types are not for specifying the semantics of aspect, which is actually a property of the whole sentence rather than a verb itself (cf. Kriflca [11], Dorr [8], Moens and Steedman [15]). For instance, as shown below, the same verb can be classified into two different aspectual classes (i.e. activity and accomplishment) depending on the types of obje</context>
</contexts>
<marker>[9]</marker>
<rawString>David Dowty. Word Meaning and Montague Grammar. D. Reidel, 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Donald Ifindle</author>
<author>Mats Rooth</author>
</authors>
<title>Structural Ambiguity and Lexical Relations.</title>
<date>1991</date>
<booktitle>In Proceedings of 29th Annual Meeting of the ACL,</booktitle>
<marker>[10]</marker>
<rawString>Donald Ifindle and Mats Rooth. Structural Ambiguity and Lexical Relations. In Proceedings of 29th Annual Meeting of the ACL, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manfred Krifka</author>
</authors>
<title>Nominal Reference, Temporal Construction, and Quantification in Event Semantics. In</title>
<date>1989</date>
<booktitle>Semantics and Contextual Expressions.</booktitle>
<editor>R. Bartsch et al., editors,</editor>
<location>Foris, Dordrecht,</location>
<contexts>
<context position="6610" citStr="[11]" startWordPosition="942" endWordPosition="942">in each language, and also made it possible to acquire them from large corpora automatically. This classification system has been partially derived from Vendler and Dowty&apos;s aspectual classifications [19, 9] and Talmy&apos;s lexicalization patterns [18]. For example, all AGENTIVE-ACTION verbs are so-called activity verbs, and so-called stative verbs fall under either INVERSE-STATE (if transitive) or PROCESS-OR-STATE (if intransitive). However, the situation types are not for specifying the semantics of aspect, which is actually a property of the whole sentence rather than a verb itself (cf. Kriflca [11], Dorr [8], Moens and Steedman [15]). For instance, as shown below, the same verb can be classified into two different aspectual classes (i.e. activity and accomplishment) depending on the types of object NP&apos;s or existence of certain PP&apos;s. (1) a. Sue drank wine for/*in an hour. b. Sue drank a bottle of wine *for/in an hour. (2) a. Harry climbed for/*in an hour. b. Harry climbed to the top *for/in an hour. Situation types are intended to address the issue of cross-linguistic predicate-argument mapping generalization, rather than the semantics of aspect. 2.2 Idiosyncrasies Idiosyncrasies slots i</context>
</contexts>
<marker>[11]</marker>
<rawString>Manfred Krifka. Nominal Reference, Temporal Construction, and Quantification in Event Semantics. In R. Bartsch et al., editors, Semantics and Contextual Expressions. Foris, Dordrecht, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susumu Kuno</author>
</authors>
<title>The Structure of the Japanese Language.</title>
<date>1973</date>
<volume>1</volume>
<publisher>MIT Press,</publisher>
<contexts>
<context position="5811" citStr="[12]" startWordPosition="823" endWordPosition="823"> the THEME of an INVERSE-STATE verb is expressed by marking the object NP with a particle &amp;quot;-ga&amp;quot; , which is usually a subject 1We use structures similar to LFG&apos;s f-structures. 108 English/Spanish Mapping Japanese Mapping CAUSED-PROCESS AGENT (SURFACE SUBJECT) (SURFACE SUBJECT) THEME SURFACE OBJECT) (SURFACE OBJECT) PROCESS-OR-STATE THEME SURFACE SUBJECT SURFACE SUBJECT AGENTIVE-ACTION AGENT SURFACE SUBJECT SURFACE SUBJECT INVERSE-STATE GOAL SURFACE SUBJECT SURFACE SUBJECT THEME (SURFACE OBJECT) (SURFACE OBJECT) (PARTICLE &amp;quot;GA&amp;quot;) Table 3: Default Mapping Rules for Three Languages marker (cf. Kuno [12]).2 3 So we add such information to the INVERSE-STATE mapping rule for Japanese. Generalization expressed in situation types has saved us from defining semantic mapping rules for each verb sense in each language, and also made it possible to acquire them from large corpora automatically. This classification system has been partially derived from Vendler and Dowty&apos;s aspectual classifications [19, 9] and Talmy&apos;s lexicalization patterns [18]. For example, all AGENTIVE-ACTION verbs are so-called activity verbs, and so-called stative verbs fall under either INVERSE-STATE (if transitive) or PROCESS-</context>
<context position="8010" citStr="[12]" startWordPosition="1159" endWordPosition="1159">cified here. For example, the fact that &amp;quot;look&amp;quot; denotes its THEME argument by the preposition &amp;quot;at&amp;quot; is captured by specifying idiosyncrasies. Examples of lexical entries with idiosyncrasies in English, Spanish and Japanese are shown in Figure 1. As discussed in the next section, we derive this kind of word-specific information automatically from corpora. 2There is a debate over whether the NP with &amp;quot;ga&amp;quot; is a subject or object. However, our approach can accommodate either analysis. 3The GOAL of some INVERSE-STATE verbs in Japanese can be expressed by a &amp;quot;ni&amp;quot; postpositional phrase. However, as Kuno [12] points out, since this is an idiosyncratic phenomenon, such information does not go to the default mapping rule. 109 (LOOK (CATEGORY . V) (SENSE-NAME. LOOK-1) (SEMANTIC-CONCEPT #LOOK#) (IDIOSYNCRASIES (THEME (MAPPING (LITERAL &amp;quot;AT&amp;quot;)))) (SITUATION-TYPE AGENTIVE-ACTION)) (INFECTAR (CATEGORY. V) (SENSE-NAME. INFECTAR-1) (SEMANTIC-CONCEPT #INFECT#) (IDIOSYNCRASIES (THEME (MAPPING (LITERAL &amp;quot;CON&amp;quot; &amp;quot;DE&amp;quot;))) (GOAL (MAPPING (SURFACE 013.1ECT)))) (SITUATION-TYPE CAUSED-PROCESS)) (NARU (CATEGORY . V) (SENSE-NAME . NARU-1) (SEMANTIC-CONCEPT #BECOME#) (IDIOSYNCRASIES (GOAL (MAPPING (LITERAL &amp;quot;TO&amp;quot; &amp;quot;NI&amp;quot;)))) (SI</context>
</contexts>
<marker>[12]</marker>
<rawString>Susumu Kuno. The Structure of the Japanese Language. MIT Press, 1973. 1 1 6</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug McKee</author>
<author>John Maloney</author>
</authors>
<title>Using Statistics Gained from Corpora in a Knowledge-Based NLP System. In</title>
<date>1992</date>
<booktitle>Proceedings of The AAAI Workshop on Statistically-Based NLP Techniques,</booktitle>
<contexts>
<context position="19365" citStr="[13]" startWordPosition="3059" endWordPosition="3059"> Spanish and English, and to the left for Japanese). When a particular pre/postposition appears here much more often than chance (based on either Mutual Information or a chi-squared test [5, 4]), we assume that it is a likely argument. A very similar strategy works well at identifying verbs that take sentential complements by looking for complementizers (e.g. &amp;quot;that&amp;quot;, &amp;quot;to&amp;quot;) in positions of likely attachment. Some English examples are shown in Tables 4 and 5, and Spanish examples are shown in Tables 6 and 7. The details of the exact algorithms used for English are contained in McKee and Maloney [13]. Areas for improvement include distinguishing between cases where a verb takes a prepositional arguments, a prepositional particle, or a common adjunct. 4 Conclusion We have automatically built lexicons with predicate-argument mapping information from English, Spanish and Japanese corpora. These lexicons have been used for several multilingual data extraction applications (cf. Aone et al. [2]) and a prototype Japanese-English 114 word possible clausal complements know THATCOMP vow THATCOMP, TOCOMP eat — want TOCOMP resume INGCOMP Table 5: English Verbs which Take Complementizers verb MI with </context>
</contexts>
<marker>[13]</marker>
<rawString>Doug McKee and John Maloney. Using Statistics Gained from Corpora in a Knowledge-Based NLP System. In Proceedings of The AAAI Workshop on Statistically-Based NLP Techniques, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
<author>Richard Beckwith</author>
<author>Christiane Fellbaum</author>
<author>Derek Gross</author>
<author>Katherine Miller</author>
</authors>
<title>Five papers on WordNet.</title>
<date>1990</date>
<tech>Technical Report CSL Report 43,</tech>
<institution>Cognitive Science Laboratory, Princeton University,</institution>
<contexts>
<context position="16737" citStr="[14]" startWordPosition="2661" endWordPosition="2661">e subject over the total occurrences of the verb where we identified the subject. Any noun or pronoun directly preceding a verb is considered to be its subject. This heuristic fails in cases where the subject NP is modified by a PP or relative clause as in &amp;quot;The man under the car wore a red shirt.&amp;quot; We have only implemented this metric for English. The verb&apos;s subject is considered to be animate if it is any one of the following: • A personal pronoun (&amp;quot;it&amp;quot; and &amp;quot;they&amp;quot; were excluded, since they may refer back to inanimate objects.) • A proper name • A word under &amp;quot;agent&amp;quot; or &amp;quot;people&amp;quot; in WordNet (cf. [14]) • A word that appears in a MUC-4 template slot that can be filled only with humans (cf. [7]) Verbs that have a low subject animacy cannot be either CAUSED-PROCESS or AGENTIVE-ACTION, since the syntactic subject must map to the AGENT thematic 1 1 3 role. A high subject animacy does not correlate with any particular situation type, since several stative verbs take only animate subjects (e.g. perception verbs). The predicted situation types shown in Figure 3 were calculated with the following algorithm: 1. Assume that the verb can occur with every situation type. 2. If the transitivity rating i</context>
</contexts>
<marker>[14]</marker>
<rawString>George Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine Miller. Five papers on WordNet. Technical Report CSL Report 43, Cognitive Science Laboratory, Princeton University, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Moens</author>
<author>Mark Steedman</author>
</authors>
<title>Temporal ontology and temporal reference.</title>
<date>1988</date>
<journal>Computational Linguistics,</journal>
<volume>14</volume>
<issue>2</issue>
<contexts>
<context position="6645" citStr="[15]" startWordPosition="948" endWordPosition="948">possible to acquire them from large corpora automatically. This classification system has been partially derived from Vendler and Dowty&apos;s aspectual classifications [19, 9] and Talmy&apos;s lexicalization patterns [18]. For example, all AGENTIVE-ACTION verbs are so-called activity verbs, and so-called stative verbs fall under either INVERSE-STATE (if transitive) or PROCESS-OR-STATE (if intransitive). However, the situation types are not for specifying the semantics of aspect, which is actually a property of the whole sentence rather than a verb itself (cf. Kriflca [11], Dorr [8], Moens and Steedman [15]). For instance, as shown below, the same verb can be classified into two different aspectual classes (i.e. activity and accomplishment) depending on the types of object NP&apos;s or existence of certain PP&apos;s. (1) a. Sue drank wine for/*in an hour. b. Sue drank a bottle of wine *for/in an hour. (2) a. Harry climbed for/*in an hour. b. Harry climbed to the top *for/in an hour. Situation types are intended to address the issue of cross-linguistic predicate-argument mapping generalization, rather than the semantics of aspect. 2.2 Idiosyncrasies Idiosyncrasies slots in the lexicon specify word sense-sp</context>
</contexts>
<marker>[15]</marker>
<rawString>Marc Moens and Mark Steedman. Temporal ontology and temporal reference. Computational Linguistics, 14(2), 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergei Nirenburg</author>
<author>Lori Levin</author>
</authors>
<title>Syntax-Driven and Ontology-Driven Lexical Semantics.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL Lexical Semantics and Knowledge Representation Workshop,</booktitle>
<contexts>
<context position="4980" citStr="[16]" startWordPosition="702" endWordPosition="702"> does not allow AGENT for its thematic role (e.g. &amp;quot;see&amp;quot;). Examples from three languages are shown in Table 2. Although verbs in different languages are classified into the same four situation types using the same definition, mapping rules which map grammatical functions (i.e. subject, object, etc.) in the syntactic structures1 to thematic roles in the semantic structures may differ from one language to another. This is because languages do not necessarily express the same thematic roles with the same grammatical functions. This mapping information is language-specific (cf. Nirenburg and Levin [16]). The default mapping rules for the four situation types are shown in Table 3. They are nearly identical for the three languages (English, Spanish, and Japanese) we have analyzed so far. The only difference is that in Japanese the THEME of an INVERSE-STATE verb is expressed by marking the object NP with a particle &amp;quot;-ga&amp;quot; , which is usually a subject 1We use structures similar to LFG&apos;s f-structures. 108 English/Spanish Mapping Japanese Mapping CAUSED-PROCESS AGENT (SURFACE SUBJECT) (SURFACE SUBJECT) THEME SURFACE OBJECT) (SURFACE OBJECT) PROCESS-OR-STATE THEME SURFACE SUBJECT SURFACE SUBJECT AG</context>
</contexts>
<marker>[16]</marker>
<rawString>Sergei Nirenburg and Lori Levin. Syntax-Driven and Ontology-Driven Lexical Semantics. In Proceedings of ACL Lexical Semantics and Knowledge Representation Workshop, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lexicon</author>
</authors>
<title>Ontology and Text Meaning.</title>
<date>1991</date>
<booktitle>In Proceedings of ACL Lexical Semantics and Knowledge Representation Workshop,</booktitle>
<contexts>
<context position="8919" citStr="[17]" startWordPosition="1272" endWordPosition="1272">. INFECTAR-1) (SEMANTIC-CONCEPT #INFECT#) (IDIOSYNCRASIES (THEME (MAPPING (LITERAL &amp;quot;CON&amp;quot; &amp;quot;DE&amp;quot;))) (GOAL (MAPPING (SURFACE 013.1ECT)))) (SITUATION-TYPE CAUSED-PROCESS)) (NARU (CATEGORY . V) (SENSE-NAME . NARU-1) (SEMANTIC-CONCEPT #BECOME#) (IDIOSYNCRASIES (GOAL (MAPPING (LITERAL &amp;quot;TO&amp;quot; &amp;quot;NI&amp;quot;)))) (SITUATION-TYPE PROCESS-OR-STATE)) Figure 1: Lexical entries for &amp;quot;look&amp;quot;, &amp;quot;infectar&amp;quot;, and &amp;quot;naru&amp;quot; 2.3 Semantic Concepts Each lexical meaning of a verb is represented by a semantic concept (or frame) in our language-independent knowledge base, which is similar to the one described in Onyshkevych and Nirenburg [17]. Each verb frame has thematic role slots, which have two facets, TYPE and MAPPING. A TYPE facet value of a given slot provides a constraint on the type of objects which can be the value of the slot. In the MAPPING facets, we have encoded some cross-linguistically general predicate-argument mapping information. For example, we have defined that all the subclasses of #COMMUNICATION-EVENT# (e.g. #REPORT#, #CONFIRM#, etc.) map their sentential complements (SENT-COMP) to THEME, as shown below. (#COMMUNICATION-EVENT# (AKO #DYNAMIC-SITUATION#) (AGENT (TYPE #PERSON# #ORGANIZATIONC) (THEME (TYPE #SITU</context>
</contexts>
<marker>[17]</marker>
<rawString>Boyan Onyshkevych and Sergei Nirenburg. Lexicon, Ontology and Text Meaning. In Proceedings of ACL Lexical Semantics and Knowledge Representation Workshop, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leonard Talmy</author>
</authors>
<title>Lexicalization Patterns: Semantic Structure in Lexical Forms.</title>
<date>1985</date>
<booktitle>Language Typology and Syntactic Descriptions.</booktitle>
<editor>In Timothy Shopen, editor,</editor>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="6253" citStr="[18]" startWordPosition="889" endWordPosition="889">L SURFACE SUBJECT SURFACE SUBJECT THEME (SURFACE OBJECT) (SURFACE OBJECT) (PARTICLE &amp;quot;GA&amp;quot;) Table 3: Default Mapping Rules for Three Languages marker (cf. Kuno [12]).2 3 So we add such information to the INVERSE-STATE mapping rule for Japanese. Generalization expressed in situation types has saved us from defining semantic mapping rules for each verb sense in each language, and also made it possible to acquire them from large corpora automatically. This classification system has been partially derived from Vendler and Dowty&apos;s aspectual classifications [19, 9] and Talmy&apos;s lexicalization patterns [18]. For example, all AGENTIVE-ACTION verbs are so-called activity verbs, and so-called stative verbs fall under either INVERSE-STATE (if transitive) or PROCESS-OR-STATE (if intransitive). However, the situation types are not for specifying the semantics of aspect, which is actually a property of the whole sentence rather than a verb itself (cf. Kriflca [11], Dorr [8], Moens and Steedman [15]). For instance, as shown below, the same verb can be classified into two different aspectual classes (i.e. activity and accomplishment) depending on the types of object NP&apos;s or existence of certain PP&apos;s. (1)</context>
</contexts>
<marker>[18]</marker>
<rawString>Leonard Talmy. Lexicalization Patterns: Semantic Structure in Lexical Forms. In Timothy Shopen, editor, Language Typology and Syntactic Descriptions. Cambridge University Press, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zeno Vendler</author>
</authors>
<title>Linguistics in Philosophy.</title>
<date>1967</date>
<publisher>Cornell University Press,</publisher>
<contexts>
<context position="6212" citStr="[19, 9]" startWordPosition="883" endWordPosition="884">CE SUBJECT SURFACE SUBJECT INVERSE-STATE GOAL SURFACE SUBJECT SURFACE SUBJECT THEME (SURFACE OBJECT) (SURFACE OBJECT) (PARTICLE &amp;quot;GA&amp;quot;) Table 3: Default Mapping Rules for Three Languages marker (cf. Kuno [12]).2 3 So we add such information to the INVERSE-STATE mapping rule for Japanese. Generalization expressed in situation types has saved us from defining semantic mapping rules for each verb sense in each language, and also made it possible to acquire them from large corpora automatically. This classification system has been partially derived from Vendler and Dowty&apos;s aspectual classifications [19, 9] and Talmy&apos;s lexicalization patterns [18]. For example, all AGENTIVE-ACTION verbs are so-called activity verbs, and so-called stative verbs fall under either INVERSE-STATE (if transitive) or PROCESS-OR-STATE (if intransitive). However, the situation types are not for specifying the semantics of aspect, which is actually a property of the whole sentence rather than a verb itself (cf. Kriflca [11], Dorr [8], Moens and Steedman [15]). For instance, as shown below, the same verb can be classified into two different aspectual classes (i.e. activity and accomplishment) depending on the types of obje</context>
</contexts>
<marker>[19]</marker>
<rawString>Zeno Vendler. Linguistics in Philosophy. Cornell University Press, 1967.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Uri Zernik</author>
<author>Paul Jacobs</author>
</authors>
<title>Tagging for Learning: Collecting Thematic Relations from Corpus.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics,</booktitle>
<contexts>
<context position="1695" citStr="[20]" startWordPosition="216" endWordPosition="216"> semantic information (e.g. a concept hierarchy). Language understanding requires mapping from syntactic structures into conceptual representation (henceforth predicateargument mapping), while language generation requires the inverse mapping. That is, grammatical functions in the syntactic structures (e.g. subject, object, etc.) should be mapped to thematic roles in the semantic structures (e.g. agent, theme, etc.). In this paper, we discuss how we acquire such predicate-argument mapping information from multilingual texts automatically (cf. Zernik and Jacobs work on collecting thematic roles [20]). As discussed in Aone and Mckee [1], the lexicon of our NLP system abstracts the language-dependent portion of predicate-argument mapping information from the core meaning of verb senses (i.e. semantic concepts as defined in the knowledge base). We represent this mapping information in terms of cross-linguistically generalized mapping types called situation types and word sense-specific idiosyncrasies. This representation has enabled us to automatically acquire predicate-argument mapping information, specifically situation types and idiosyncrasies, for verbs in English, Spanish, and Japanese</context>
</contexts>
<marker>[20]</marker>
<rawString>Uri Zernik and Paul Jacobs. Tagging for Learning: Collecting Thematic Relations from Corpus. In Proceedings of the 13th International Conference on Computational Linguistics, 1990.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>