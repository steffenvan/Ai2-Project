<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000016">
<title confidence="0.950896">
Recurrent Neural Networks for Word Alignment Model
</title>
<author confidence="0.975365">
Akihiro Tamura,* Taro Watanabe, Eiichiro Sumita
</author>
<affiliation confidence="0.968178">
National Institute of Information and Communications Technology
</affiliation>
<address confidence="0.881435">
3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN
</address>
<email confidence="0.950373">
a-tamura@ah.jp.nec.com,
{taro.watanabe, eiichiro.sumita}@nict.go.jp
</email>
<sectionHeader confidence="0.994729" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999839538461539">
This study proposes a word alignment
model based on a recurrent neural net-
work (RNN), in which an unlimited
alignment history is represented by re-
currently connected hidden layers. We
perform unsupervised learning using
noise-contrastive estimation (Gutmann
and Hyv¨arinen, 2010; Mnih and Teh,
2012), which utilizes artificially generated
negative samples. Our alignment model is
directional, similar to the generative IBM
models (Brown et al., 1993). To overcome
this limitation, we encourage agreement
between the two directional models by
introducing a penalty function that en-
sures word embedding consistency across
two directional models during training.
The RNN-based model outperforms
the feed-forward neural network-based
model (Yang et al., 2013) as well as the
IBM Model 4 under Japanese-English
and French-English word alignment
tasks, and achieves comparable transla-
tion performance to those baselines for
Japanese-English and Chinese-English
translation tasks.
</bodyText>
<sectionHeader confidence="0.998881" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999016777777778">
Automatic word alignment is an important task for
statistical machine translation. The most classical
approaches are the probabilistic IBM models 1-5
(Brown et al., 1993) and the HMM model (Vogel
et al., 1996). Various studies have extended those
models. Yang et al. (2013) adapted the Context-
Dependent Deep Neural Network for HMM (CD-
DNN-HMM) (Dahl et al., 2012), a type of feed-
forward neural network (FFNN)-based model, to
</bodyText>
<footnote confidence="0.465143">
* The first author is now affiliated with Knowledge
Discovery Research Laboratories, NEC Corporation, Nara,
Japan.
</footnote>
<bodyText confidence="0.999782414634146">
the HMM alignment model and achieved state-of-
the-art performance. However, the FFNN-based
model assumes a first-order Markov dependence
for alignments.
Recurrent neural network (RNN)-based models
have recently demonstrated state-of-the-art per-
formance that outperformed FFNN-based models
for various tasks (Mikolov et al., 2010; Mikolov
and Zweig, 2012; Auli et al., 2013; Kalchbrenner
and Blunsom, 2013; Sundermeyer et al., 2013).
An RNN has a hidden layer with recurrent con-
nections that propagates its own previous signals.
Through the recurrent architecture, RNN-based
models have the inherent property of modeling
long-span dependencies, e.g., long contexts, in in-
put data. We assume that this property would fit
with a word alignment task, and we propose an
RNN-based word alignment model. Our model
can maintain and arbitrarily integrate an alignment
history, e.g., bilingual context, which is longer
than the FFNN-based model.
The NN-based alignment models are super-
vised models. Unfortunately, it is usually dif-
ficult to prepare word-by-word aligned bilingual
data. Yang et al. (2013) trained their model from
word alignments produced by traditional unsuper-
vised probabilistic models. However, with this
approach, errors induced by probabilistic mod-
els are learned as correct alignments; thus, gen-
eralization capabilities are limited. To solve this
problem, we apply noise-contrastive estimation
(NCE) (Gutmann and Hyv¨arinen, 2010; Mnih
and Teh, 2012) for unsupervised training of our
RNN-based model without gold standard align-
ments or pseudo-oracle alignments. NCE artifi-
cially generates bilingual sentences through sam-
plings as pseudo-negative samples, and then trains
the model such that the scores of the original bilin-
gual sentences are higher than those of the sam-
pled bilingual sentences.
Our RNN-based alignment model has a direc-
</bodyText>
<page confidence="0.91645">
1470
</page>
<note confidence="0.8294695">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470–1480,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999926366666667">
tion, such as other alignment models, i.e., from f
(source language) to e (target language) and from
e to f. It has been proven that the limitation may
be overcome by encouraging two directional mod-
els to agree by training them concurrently (Ma-
tusov et al., 2004; Liang et al., 2006; Grac¸a et al.,
2008; Ganchev et al., 2008). The motivation for
this stems from the fact that model and generaliza-
tion errors by the two models differ, and the mod-
els must complement each other. Based on this
motivation, our directional models are also simul-
taneously trained. Specifically, our training en-
courages word embeddings to be consistent across
alignment directions by introducing a penalty term
that expresses the difference between embedding
of words into an objective function. This con-
straint prevents each model from overfitting to a
particular direction and leads to global optimiza-
tion across alignment directions.
This paper presents evaluations of Japanese-
English and French-English word alignment tasks
and Japanese-to-English and Chinese-to-English
translation tasks. The results illustrate that our
RNN-based model outperforms the FFNN-based
model (up to +0.0792 F1-measure) and the IBM
Model 4 (up to +0.0703 F1-measure) for the word
alignment tasks. For the translation tasks, our
model achieves up to 0.74% gain in BLEU as com-
pared to the FFNN-based model, which matches
the translation qualities of the IBM Model 4.
</bodyText>
<sectionHeader confidence="0.999796" genericHeader="introduction">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999438857142857">
Various word alignment models have been pro-
posed. These models are roughly clustered into
two groups: generative models, such as those pro-
posed by Brown et al. (1993), Vogel et al. (1996),
and Och and Ney (2003), and discriminative mod-
els, such as those proposed by Taskar et al. (2005),
Moore (2005), and Blunsom and Cohn (2006).
</bodyText>
<sectionHeader confidence="0.798732" genericHeader="method">
2.1 Generative Alignment Model
</sectionHeader>
<bodyText confidence="0.999966">
Given a source language sentence fJ1 = f1, ..., fJ
and a target language sentence eI1 = e1, ..., eI,
fJ1 is generated by eI1 via the alignment aJ1 =
a1,..., aJ. Each aj is a hidden variable indicat-
ing that the source word fj is aligned to the target
word eaj. Usually, a “null” word e0 is added to
the target language sentence and aJ1 may contain
aj = 0, which indicates that fj is not aligned to
any target word. The probability of generating the
</bodyText>
<equation confidence="0.962648333333333">
sentence fJ1 from eI1 is defined as
�p(fJ1 |eI1) = p(fJ1 , aJ1 |eI1). (1)
aJ 1
</equation>
<bodyText confidence="0.995439666666667">
The IBM Models 1 and 2 and the HMM model
decompose it into an alignment probability pa and
a lexical translation probability pt as
</bodyText>
<equation confidence="0.99494">
J
p(fJ1 , aJ1 |eI1) = H pa(aj|aj−1,j)pt(fj|eaj). (2)
j=1
</equation>
<bodyText confidence="0.999222538461539">
The three models differ in their definition of align-
ment probability. For example, the HMM model
uses an alignment probability with a first-order
Markov property: pa(aj|aj − aj−1). In addition,
the IBM models 3-5 are extensions of these, which
consider the fertility and distortion of each trans-
lated word.
These models are trained using the expectation-
maximization algorithm (Dempster et al., 1977)
from bilingual sentences without word-level align-
ments (unlabeled training data). Given a specific
model, the best alignment (Viterbi alignment) of
the sentence pair (fJ1 , eI1) can be found as
</bodyText>
<equation confidence="0.998044">
aJ1 = argmax p(fJ1 , aJ1 |eI1). (3)
aJ 1
</equation>
<bodyText confidence="0.995765">
For example, the HMM model identifies the
Viterbi alignment using the Viterbi algorithm.
</bodyText>
<subsectionHeader confidence="0.991676">
2.2 FFNN-based Alignment Model
</subsectionHeader>
<bodyText confidence="0.999879764705882">
As an instance of discriminative models, we de-
scribe an FFNN-based word alignment model
(Yang et al., 2013), which is our baseline. An
FFNN learns a hierarchy of nonlinear features
that can automatically capture complex statisti-
cal patterns in input data. Recently, FFNNs have
been applied successfully to several tasks, such as
speech recognition (Dahl et al., 2012), statistical
machine translation (Le et al., 2012; Vaswani et
al., 2013), and other popular natural language pro-
cessing tasks (Collobert and Weston, 2008; Col-
lobert et al., 2011).
Yang et al. (2013) have adapted a type of FFNN,
i.e., CD-DNN-HMM (Dahl et al., 2012), to the
HMM alignment model. Specifically, the lexical
translation and alignment probability in Eq. 2 are
computed using FFNNs as
</bodyText>
<equation confidence="0.8521035">
J
sNN(aJ1 |fJ1 , eI1) = H ta(aj − aj−1|c(eaj−1))
j=1
&apos;tlex(fj,eaj|c(fj),c(eaj)), (4)
1471
t (fj , eaj  |f j-1 , e )
j+1 aj+1
lex aj-1
The computations in the hidden and output layer
are as follows2:
Output Layer O× +BO
z1
z1 = f(H x z0 + BH), (5)
tlex = O x z1 + BO, (6)
</equation>
<figure confidence="0.73908225">
Hidden Layer z1
htanh(H× +BH)
z0
z0
</figure>
<figureCaption confidence="0.987601">
Figure 1: FFNN-based model for computing a lex-
ical translation score of (fj, eaj)
</figureCaption>
<bodyText confidence="0.9998534">
where ta and tlex are an alignment score and a lex-
ical translation score, respectively, sNN is a score
of alignments aJ1, and “c(a word w)” denotes a
context of word w. Note that the model uses non-
probabilistic scores rather than probabilities be-
cause normalization over all words is computa-
tionally expensive. The model finds the Viterbi
alignment using the Viterbi algorithm, similar to
the classic HMM model. Note that alignments
in the FFNN-based model are also governed by
first-order Markov dynamics because an align-
ment score depends on the previous alignment
aj−1.
Figure 1 shows the network structure with one
hidden layer for computing a lexical translation
probability tlex(fj, eaj|c(fj), c(eaj)). The model
consists of a lookup layer, a hidden layer, and an
output layer, which have weight matrices. The
model receives a source and target word with their
contexts as inputs, which are words in a prede-
fined window (the window size is three in Fig-
ure 1). First, the lookup layer converts each in-
put word into its word embedding by looking up
its corresponding column in the embedding ma-
trix (L), and then concatenates them. Let Vf (or
Ve) be a set of source words (or target words) and
M be a predetermined embedding length. L is a
M x (|Vf |+ |Ve|) matrix1. Word embeddings are
dense, low dimensional, and real-valued vectors
that can capture syntactic and semantic properties
of the words (Bengio et al., 2003). The concate-
nation (z0) is then fed to the hidden layer to cap-
ture nonlinear relations. Finally, the output layer
receives the output of the hidden layer (z1) and
computes a lexical translation score.
</bodyText>
<footnote confidence="0.854299">
1We add a special token (unk) to handle unknown words
and (null) to handle null alignments to Vf and Ve
</footnote>
<bodyText confidence="0.999214666666667">
where H, BH, O, and BO are |z1 |x |z0|, |z1 |x 1,
1x|z1|, and 1x1 matrices, respectively, and f(x)
is an activation function. Following Yang et al.
(2013), a “hard” version of the hyperbolic tangent,
htanh(x)3, is used as f(x) in this study.
The alignment model based on an FFNN is
formed in the same manner as the lexical trans-
lation model. Each model is optimized by mini-
mizing the following ranking loss with a margin
using stochastic gradient descent (SGD)4, where
gradients are computed by the back-propagation
algorithm (Rumelhart et al., 1986):
</bodyText>
<equation confidence="0.999653333333333">
loss(0) = � max{0,1 − sθ(a+|f, e)
(f,e)ET
+sθ(a−|f, e)}, (7)
</equation>
<bodyText confidence="0.999930166666667">
where 0 denotes the weights of layers in the
model, T is a set of training data, a+ is the gold
standard alignment, a− is the incorrect alignment
with the highest score under 0, and sθ denotes the
score defined by Eq. 4 as computed by the model
under 0.
</bodyText>
<sectionHeader confidence="0.996995" genericHeader="method">
3 RNN-based Alignment Model
</sectionHeader>
<bodyText confidence="0.999381">
This section proposes an RNN-based alignment
model, which computes a score for alignments aJ1
using an RNN:
</bodyText>
<equation confidence="0.9917455">
tRNN(aj|aj−1
1 , fj, eaj), (8)
</equation>
<bodyText confidence="0.989679111111111">
where tRNN is the score of an alignment aj. The
prediction of the j-th alignment aj depends on all
preceding alignments aj−1
1 . Note that the pro-
posed model also uses nonprobabilistic scores,
similar to the FFNN-based model.
The RNN-based model is illustrated in Figure
2. The model consists of a lookup layer, a hid-
den layer, and an output layer, which have weight
</bodyText>
<footnote confidence="0.842818857142857">
2Consecutive l hidden layers can be used: zl = f(Hl x
zl_1 + BH,). For simplicity, this paper describes the model
with 1 hidden layer.
3htanh(x) = −1 for x &lt; −1, htanh(x) = 1 for x &gt; 1,
and htanh(x) = x for others.
4In our experiments, we used a mini-batch SGD instead
of a plain SGD.
</footnote>
<figure confidence="0.975790333333333">
Lookup
Layer
L L L
Input fj-1 fj fj+1 eaj-1 eaj eaj+1
L L L
J
sNN(aJ1 |fJ1 , eI1) =
j=1
1472
j-1
JJ
tRNN(aj  |al &apos; , eaj
O× +BO
yj
yj
yj-1
Lookup Layer L L
Input fj eaj
</figure>
<figureCaption confidence="0.8599395">
Figure 2: RNN-based alignment model
matrices L, {Hd, Rd, Bd�}, and {O, BO}, respec-
</figureCaption>
<bodyText confidence="0.930775727272727">
tively. Each matrix in the hidden layer (Hd, Rd,
and Bd�) depends on alignment, where d denotes
the jump distance from aj−1 to aj: d = aj −
aj−1. In our experiments, we merge distances
that are greater than 8 and less than -8 into the
special “&gt;8” and “&lt;-8” distances, respectively.
Specifically, the hidden layer has weight matrices
{H≤−8, H−7, ··· ,H7, H≥8, R≤−8, R−7, ··· ,
R7, R≥8, B≤−8
� , B−7
� , ··· ,B7�, B≥8
� } and com-
putes yj using the corresponding matrices of the
jump distance d.
The Viterbi alignment is determined using the
Viterbi algorithm, similar to the FFNN-based
model, where the model is sequentially applied
from f1 to fJ5. When computing the score of the
alignment between fj and eaj, the two words are
input to the lookup layer. In the lookup layer, each
of these words is converted to its word embedding,
and then the concatenation of the two embeddings
(xj) is fed to the hidden layer in the same manner
as the FFNN-based model. Next, the hidden layer
receives the output of the lookup layer (xj) and
that of the previous hidden layer (yj−1). The hid-
den layer then computes and outputs the nonlinear
relations between them. Note that the weight ma-
trices used in this computation are embodied by
the specific jump distance d. The output of the hid-
den layer (yj) is copied and fed to the output layer
and the next hidden layer. Finally, the output layer
computes the score of aj (tRNN(aj|aj−1
</bodyText>
<equation confidence="0.794589">
1 , fj, eaj))
</equation>
<bodyText confidence="0.988428928571429">
from the output of the hidden layer (yj). Note that
the FFNN-based model consists of two compo-
5Strictly speaking, we cannot apply the dynamic pro-
gramming forward-backward algorithm (i.e., the Viterbi al-
gorithm) due to the long alignment history of yz. Thus, the
Viterbi alignment is computed approximately using heuristic
beam search.
nents: one is for lexical translation and the other
is for alignment. The proposed RNN produces a
single score that is constructed in the hidden layer
by employing the distance-dependent weight ma-
trices.
Specifically, the computations in the hidden and
output layer are as follows:
</bodyText>
<equation confidence="0.9999165">
yj = f(Hd x xj + Rd x yj−1 + Bd�), (9)
tRNN = O x yj + BO, (10)
</equation>
<bodyText confidence="0.990664071428571">
where Hd, Rd, Bd�, O, and BO are |yj |x |xj|,
|yj |x |yj−1|, |yj |x 1, 1 x |yj|, and 1 x 1 matri-
ces, respectively. Note that |yj−1 |= |yj|. f(x) is
an activation function, which is a hard hyperbolic
tangent, i.e., htanh(x), in this study.
As described above, the RNN-based model has
a hidden layer with recurrent connections. Under
the recurrence, the proposed model compactly en-
codes the entire history of previous alignments in
the hidden layer configuration yi. Therefore, the
proposed model can find alignments by taking ad-
vantage of the long alignment history, while the
FFNN-based model considers only the last align-
ment.
</bodyText>
<sectionHeader confidence="0.99182" genericHeader="method">
4 Training
</sectionHeader>
<bodyText confidence="0.999976894736842">
During training, we optimize the weight matrices
of each layer (i.e., L, Hd, Rd, Bd�, O, and BO)
following a given objective using a mini-batch
SGD with batch size D, which converges faster
than a plain SGD (D = 1). Gradients are com-
puted by the back-propagation through time algo-
rithm (Rumelhart et al., 1986), which unfolds the
network in time (j) and computes gradients over
time steps. In addition, an l2 regularization term
is added to the objective to prevent the model from
overfitting the training data.
The RNN-based model can be trained by a
supervised approach, similar to the FFNN-based
model, where training proceeds based on the rank-
ing loss defined by Eq. 7 (Section 2.2). However,
this approach requires gold standard alignments.
To overcome this drawback, we propose an un-
supervised method using NCE, which learns from
unlabeled training data.
</bodyText>
<subsectionHeader confidence="0.997107">
4.1 Unsupervised Learning
</subsectionHeader>
<bodyText confidence="0.99934525">
Dyer et al. (2011) presented an unsupervised
alignment model based on contrastive estimation
(CE) (Smith and Eisner, 2005). CE seeks to dis-
criminate observed data from its neighborhood,
</bodyText>
<figure confidence="0.989071666666667">
Hidden
Layer
d
htanh(H× xj+Rd× yj-1+BH )
xj
Output Layer
</figure>
<page confidence="0.970063">
1473
</page>
<bodyText confidence="0.999757">
which can be viewed as pseudo-negative samples.
Dyer et al. (2011) regarded all possible align-
ments of the bilingual sentences, which are given
as training data (T), and those of the full transla-
tion search space (Ω) as the observed data and its
neighborhood, respectively.
We introduce this idea to a ranking loss with
margin as
</bodyText>
<equation confidence="0.99807875">
loss(θ) = max{ 0,1 − ∑ ED [so (a |f+, e+)]
l (f+,e+)ET
ED[so(a|f+,e−)] 11
)]}, (11)
</equation>
<bodyText confidence="0.999909375">
where Φ is a set of all possible alignments given
(f, e), ED[so] is the expected value of the scores
so on Φ, e+ denotes a target language sentence in
the training data, and e− denotes a pseudo-target
language sentence. The first expectation term is
for the observed data, and the second is for the
neighborhood.
However, the computation for Ω is prohibitively
expensive. To reduce computation, we employ
NCE, which uses randomly sampled sentences
from all target language sentences in Ω as e−, and
calculate the expected values by a beam search
with beam width W to truncate alignments with
low scores. In our experiments, we set W to 100.
In addition, the above criterion is converted to an
online fashion as
</bodyText>
<equation confidence="0.9943205">
{max 0,1 − EGEN[so(a|f+, e+)]
EGEN[so(a|f+,e−)] }, (12)
</equation>
<bodyText confidence="0.99959385">
where e+ is a target language sentence aligned to
f+ in the training data, i.e., (f+, e+) ∈ T, e− is
a randomly sampled pseudo-target language sen-
tence with length |e+|, and N denotes the num-
ber of pseudo-target language sentences per source
sentence f+. Note that |e+ |= |e−|. GEN is a
subset of all possible word alignments Φ, which is
generated by beam search.
In a simple implementation, each e− is gener-
ated by repeating a random sampling from a set of
target words (Ve) |e+ |times and lining them up
sequentially. To employ more discriminative neg-
ative samples, our implementation samples each
word of e− from a set of the target words that co-
occur with fz ∈ f+ whose probability is above a
threshold C under the IBM Model 1 incorporating
l0 prior (Vaswani et al., 2012). The IBM Model
1 with l0 prior is convenient for reducing transla-
tion candidates because it generates more sparse
alignments than the standard IBM Model 1.
</bodyText>
<subsectionHeader confidence="0.993587">
4.2 Agreement Constraints
</subsectionHeader>
<bodyText confidence="0.996926052631579">
Both of the FFNN-based and RNN-based models
are based on the HMM alignment model, and they
are therefore asymmetric, i.e., they can represent
one-to-many relations from the target side. Asym-
metric models are usually trained in each align-
ment direction. The model proposed by Yang et
al. (2013) is no exception. However, it has been
demonstrated that encouraging directional mod-
els to agree improves alignment performance (Ma-
tusov et al., 2004; Liang et al., 2006; Grac¸a et al.,
2008; Ganchev et al., 2008).
Inspired by their work, we introduce an agree-
ment constraint to our learning. The constraint
concretely enforces agreement in word embed-
dings of both directions. The proposed method
trains two directional models concurrently based
on the following objective by incorporating a
penalty term that expresses the difference between
word embeddings:
</bodyText>
<equation confidence="0.955178">
argmin
oFE {loss(θFE) + α∥θLEF − θLFE∥}, (13)
oEF {loss(θEF) + α∥θLFE − θLEF ∥}, (14)
argmin
</equation>
<bodyText confidence="0.999954190476191">
where θFE (or θEF) denotes the weights of lay-
ers in a source-to-target (or target-to-source) align-
ment model, θL denotes weights of a lookup layer,
i.e., word embeddings, and α is a parameter that
controls the strength of the agreement constraint.
∥θ∥ indicates the norm of θ. 2-norm is used in our
experiments. Equations 13 and 14 can be applied
to both supervised and unsupervised approaches.
Equations 7 and 12 are substituted into loss(θ)
in supervised and unsupervised learning, respec-
tively. The proposed constraint penalizes overfit-
ting to a particular direction and enables two di-
rectional models to optimize across alignment di-
rections globally.
Our unsupervised learning procedure is summa-
rized in Algorithm 1. In Algorithm 1, line 2 ran-
domly samples D bilingual sentences (f+, e+)D
from training data T. Lines 3-1 and 3-2 gener-
ate N pseudo-negative samples for each f+ and
e+ based on the translation candidates of f+ and
e+ found by the IBM Model 1 with l0 prior,
</bodyText>
<equation confidence="0.974083375">
∑
+
(f+,e−)EQ
loss(θ) = ∑
f+ET
1 ∑
+ N
e−
</equation>
<page confidence="0.952638">
1474
</page>
<table confidence="0.997264428571429">
Algorithm 1 Training Algorithm
Input: O1FE, O1EF, training data T, MaxIter,
batch size D, N, C, IBM1, W, α
1: for all t such that 1 ≤ t ≤ MaxIter do
2: {(f+,e+)D}←sample(D,T)
3-1: {(f+, {e−}N)D}←nege({(f+, e+)D}, N, C, IBM1)
3-2: {(e+, {f−}N)D}←negf({(f+, e+)D}, N, C, IBM1)
4-1: Ot+1
F E ←update((f+, e+, {e−}N)D, OtF E, OtEF , W, α)
4-2: Ot+1
EF ←update((e+, f+, {f−}N)D, OtEF, OtFE, W, α)
5: end for
Output: OMaxIter+1 OMaxIter+1
EF,FE
Train Dev Test
BTEC 9 K 0 960
Hansards 1.1 M 37 447
FBI5 NI5T 03 240 K 878 919
NI5T04 1,597
IW5LT 40 K 2,501 489
NTCIR 3.2 M 2,000 2,000
</table>
<tableCaption confidence="0.999885">
Table 1: Size of experimental datasets
</tableCaption>
<bodyText confidence="0.996156833333333">
IBM1 (Section 4.1). Lines 4-1 and 4-2 update the
weights in each layer following a given objective
(Sections 4.1 and 4.2). Note that Ot F Eand OtEF are
concurrently updated in each iteration, and OtEF
(or OtFE) is employed to enforce agreement be-
tween word embeddings when updating Ot (or
</bodyText>
<equation confidence="0.2596915">
FE
OtEF).
</equation>
<sectionHeader confidence="0.995641" genericHeader="method">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.994336">
5.1 Experimental Data
</subsectionHeader>
<bodyText confidence="0.999782470588236">
We evaluated the alignment performance of the
proposed models with two tasks: Japanese-
English word alignment with the Basic Travel
Expression Corpus (BTEC) (Takezawa et al.,
2002) and French-English word alignment with
the Hansard dataset (Hansards) from the 2003
NAACL shared task (Mihalcea and Pedersen,
2003). In addition, we evaluated the end-to-end
translation performance of three tasks: a Chinese-
to-English translation task with the FBIS corpus
(FBI5), the IWSLT 2007 Japanese-to-English
translation task (IW5LT) (Fordyce, 2007), and
the NTCIR-9 Japanese-to-English patent transla-
tion task (NTCIR) (Goto et al., 2011)6.
Table 1 shows the sizes of our experimental
datasets. Note that the development data was
not used in the alignment tasks, i.e., BTEC
</bodyText>
<footnote confidence="0.986202666666667">
6We did not evaluate the translation performance on the
Hansards data because the development data is very small
and performance is unreliable.
</footnote>
<bodyText confidence="0.999410923076923">
and Hansards, because the hyperparameters of
the alignment models were set by preliminary
small-scale experiments. The BTEC data is
the first 9,960 sentence pairs in the training data
for IW5LT, which were annotated with word
alignment (Goh et al., 2010). We split these
pairs into the first 9,000 for training data and
the remaining 960 as test data. All the data in
BTEC is word-aligned, and the training data in
Hansards is unlabeled data. In FBI5, we used
the NIST02 evaluation data as the development
data, and the NIST03 and 04 evaluation data as
test data (NI5T03 and NI5T04).
</bodyText>
<subsectionHeader confidence="0.943967">
5.2 Comparing Methods
</subsectionHeader>
<bodyText confidence="0.999983057142857">
We evaluated the proposed RNN-based alignment
models against two baselines: the IBM Model
4 and the FFNN-based model with one hidden
layer. The IBM Model 4 was trained by pre-
viously presented model sequence schemes (Och
and Ney, 2003): 15H53545, i.e., five iterations of
the IBM Model 1 followed by five iterations of the
HMM Model, etc., which is the default setting for
GIZA++ (IBM4). For the FFNN-based model,
we set the word embedding length M to 30, the
number of units of a hidden layer |z1 |to 100, and
the window size of contexts to 5. Hence, |z0 |is
300 (30×5×2). Following Yang et al. (2013), the
FFNN-based model was trained by the supervised
approach described in Section 2.2 (FFNNs).
For the RNN-based models, we set M to 30
and the number of units of each recurrent hid-
den layer |yj |to 100. Thus, |xj |is 60 (30 × 2).
The number of units of each layer of the FFNN-
based and RNN-based models and M were set
through preliminary experiments. To demonstrate
the effectiveness of the proposed learning meth-
ods, we evaluated four types of RNN-based mod-
els: RNNs, RNNs+c, RNNu, and RNNu+c,
where “s/u” denotes a supervised/unsupervised
model and “+c” indicates that the agreement con-
straint was used.
In training all the models except IBM4, the
weights of each layer were initialized first. For
the weights of a lookup layer L, we preliminarily
trained word embeddings for the source and target
language from each side of the training data. We
then set the word embeddings to L to avoid falling
into local minima. Other weights were randomly
initialized to [−0.1, 0.1]. For the pretraining, we
</bodyText>
<page confidence="0.975527">
1475
</page>
<table confidence="0.9981504">
Alignment BTEC Hansards
IBM4 0.4859 0.9029
FFNN3(I) 0.4770 0.9020
RNN3(I) 0.5053+ 0.9068
RNN3+,(I) 0.5174+ 0.9202+
RNN,, 0.5307+ 0.9037
RNN,,+, 0.5562+ 0.9275+
FFNN3(R) 0.8224 -
RNN3(R) 0.8798+ -
RNN3+,(R) 0.8921+ -
</table>
<tableCaption confidence="0.919507">
Table 2: Word alignment performance (F1-
measure)
</tableCaption>
<bodyText confidence="0.99919188">
used the RNNLM Toolkit 7 (Mikolov et al., 2010)
with the default options. We mapped all words
that occurred less than five times to the special to-
ken ⟨unk⟩. Next, each weight was optimized us-
ing the mini-batch SGD, where batch size D was
100, learning rate was 0.01, and an l2 regulariza-
tion parameter was 0.1. The training stopped after
50 epochs. The other parameters were set as fol-
lows: W, N and C in the unsupervised learning
were 100, 50, and 0.001, respectively, and α for
the agreement constraint was 0.1.
In the translation tasks, we used the Moses
phrase-based SMT systems (Koehn et al., 2007).
All Japanese and Chinese sentences were seg-
mented by ChaSen8 and the Stanford Chinese seg-
menter9, respectively. In the training, long sen-
tences with over 40 words were filtered out. Using
the SRILM Toolkits (Stolcke, 2002) with modified
Kneser-Ney smoothing, we trained a 5-gram lan-
guage model on the English side of each training
data for IW 5LT and NTCIR, and a 5-gram lan-
guage model on the Xinhua portion of the English
Gigaword corpus for FBI5. The SMT weighting
parameters were tuned by MERT (Och, 2003) in
the development data.
</bodyText>
<subsectionHeader confidence="0.995891">
5.3 Word Alignment Results
</subsectionHeader>
<bodyText confidence="0.999924">
Table 2 shows the alignment performance by
the F1-measure. Hereafter, MODEL(R) and
MODEL(I) denote the MODEL trained from
gold standard alignments and word alignments
found by the IBM Model 4, respectively. In
Hansards, all models were trained from ran-
</bodyText>
<footnote confidence="0.9982875">
7http://www.fit.vutbr.cz/˜imikolov/
rnnlm/
8http://chasen-legacy.sourceforge.jp/
9http://nlp.stanford.edu/software/
</footnote>
<page confidence="0.551591">
segmenter.shtml
</page>
<bodyText confidence="0.993925538461539">
domly sampled 100 K data10. We evaluated
the word alignments produced by first applying
each model in both directions and then combin-
ing the alignments using the “grow-diag-final-
and” heuristic (Koehn et al., 2003). The signif-
icance test on word alignment performance was
performed by the sign test with a 5% significance
level. “+” in Table 2 indicates that the compar-
isons are significant over corresponding baselines,
IBM4 and FFNN3(R/I).
In Table 2, RNN,,+,, which includes all our
proposals, i.e., the RNN-based model, the unsu-
pervised learning, and the agreement constraint,
achieves the best performance for both BTEC
and Hansards. The differences from the base-
lines are statistically significant.
Table 2 shows that RNN3(R/I) outperforms
FFNN3(R/I), which is statistically significant
in BTEC. These results demonstrate that captur-
ing the long alignment history in the RNN-based
model improves the alignment performance. We
discuss the difference of the RNN-based model’s
effectiveness between language pairs in Section
6.1. Table 2 also shows that RNN3+,(R/I) and
RNN,,+, achieve significantly better performance
than RNN3(R/I) and RNN,, in both tasks, re-
spectively. This indicates that the proposed agree-
ment constraint is effective in training better mod-
els in both the supervised and unsupervised ap-
proaches.
In BTEC, RNN,, and RNN,,+, significantly
outperform RNN3(I) and RNN3+,(I), respec-
tively. The performance of these models is com-
parable with Hansards. This indicates that our
unsupervised learning benefits our models because
the supervised models are adversely affected by
errors in the automatically generated training data.
This is especially true when the quality of training
data, i.e., the performance of IBM4, is low.
</bodyText>
<subsectionHeader confidence="0.967052">
5.4 Machine Translation Results
</subsectionHeader>
<bodyText confidence="0.9999386">
Table 3 shows the translation performance by the
case sensitive BLEU4 metric11 (Papineni et al.,
2002). Table 3 presents the average BLEU of three
different MERT runs. In NTCIR and FBI5,
each alignment model was trained from the ran-
</bodyText>
<footnote confidence="0.919538">
10Due to high computational cost, we did not use all the
training data. Scaling up to larger datasets will be addressed
in future work.
11We used mteval-v13a.pl as the evaluation tool
(http://www.itl.nist.gov/iad/mig/tests/
mt/2009/).
</footnote>
<page confidence="0.944453">
1476
</page>
<table confidence="0.999857666666667">
Alignment IWSLT NTCIR FBIS
NIST03 NIST04
IBM4all 27.91 25.90 28.34
IBM4 46.47 27.25 25.41 27.65
FFNNs(I) 46.38 27.05 25.45 27.61
RNNs(I) 46.43 27.24 25.47 27.56
RNNs+c(I) 46.51 27.12 25.55 27.73
RNNu 47.05* 27.79* 25.76* 27.91*
RNNu+c 46.97* 27.76* 25.84* 28.20*
</table>
<tableCaption confidence="0.999945">
Table 3: Translation performance (BLEU4(%))
</tableCaption>
<bodyText confidence="0.999888136363636">
domly sampled 100 K data, and then a translation
model was trained from all the training data that
was word-aligned by the alignment model. In ad-
dition, for a detailed comparison, we evaluated the
SMT system where the IBM Model 4 was trained
from all the training data (IBM4all). The sig-
nificance test on translation performance was per-
formed by the bootstrap method (Koehn, 2004)
with a 5% significance level. “*” in Table 3 in-
dicates that the comparisons are significant over
both baselines, i.e., IBM4 and FFNNs(I).
Table 3 also shows that better word align-
ment does not always result in better translation,
which has been discussed previously (Yang et al.,
2013). However, RNNu and RNNu+c outper-
form FFNNs(I) and IBM4 in all tasks. These
results indicate that our proposals contribute to im-
proving translation performance12. In addition,
Table 3 shows that these proposed models are
comparable to IBM4all in NTCIR and FBIS
even though the proposed models are trained from
only a small part of the training data.
</bodyText>
<sectionHeader confidence="0.999679" genericHeader="method">
6 Discussion
</sectionHeader>
<subsectionHeader confidence="0.995489">
6.1 Effectiveness of RNN-based Alignment
Model
</subsectionHeader>
<bodyText confidence="0.999358166666667">
Figure 3 shows word alignment examples from
FFNNs and RNNs, where solid squares indi-
cate the gold standard alignments. Figure 3 (a)
shows that RRNs adequately identifies compli-
cated alignments with long distances compared
to FFNNs (e.g., jaggy alignments of “have you
been learning” in Fig 3 (a)) because RNNs cap-
tures alignment paths based on long alignment his-
tory, which can be viewed as phrase-level align-
ments, while FFNNs employs only the last align-
ment.
In French-English word alignment, the most
</bodyText>
<footnote confidence="0.98695">
12We also confirmed the effectiveness of our models on the
NIST05 and NTCIR-10 evaluation data.
</footnote>
<figureCaption confidence="0.96521">
Figure 3: Word alignment examples
</figureCaption>
<table confidence="0.99864075">
Alignment 40 K 9 K 1 K
IBM4 0.5467 0.4859 0.4128
RNNu+c 0.6004 0.5562 0.4842
RNNs+c(R) - 0.8921 0.6063
</table>
<tableCaption confidence="0.971879">
Table 4: Word alignment performance on BTEC
with various sized training data
</tableCaption>
<bodyText confidence="0.980251888888889">
valuable clues are located locally because English
and French have similar word orders and their
alignment has more one-to-one mappings than
Japanese-English word alignment (Figure 3). Fig-
ure 3 (b) shows that both RRNs and FFNNs
work for such simpler alignments. Therefore,
the RNN-based model has less effect on French-
English word alignment than Japanese-English
word alignment, as indicated in Table 2.
</bodyText>
<subsectionHeader confidence="0.999861">
6.2 Impact of Training Data Size
</subsectionHeader>
<bodyText confidence="0.999482">
Table 4 shows the alignment performance on
BTEC with various training data sizes, i.e., train-
ing data for IWSLT (40 K), training data for
BTEC (9 K), and the randomly sampled 1 K
data from the BTEC training data. Note that
RNNs+c(R) cannot be trained from the 40 K data
because the 40 K data does not have gold standard
</bodyText>
<figure confidence="0.991187217391304">
How
long
have
you
been
learning
English
?
△ △
△
△
△
△
△ △
△
△
(a) Japanese-English Alignment
: RNN S (R)
△
△
(b) French-English Alignment
△ : FFNN S (R)
they
also
have
a
role
to
play
in
the
food
chain
: RNN S (I)
△ : FFNN S (I)
△
△
.
△
△
△
△
△
△
△
△
</figure>
<page confidence="0.971772">
1477
</page>
<table confidence="0.999230285714286">
Alignment BTEC Hansards
FFNN3(I) 0.4770 0.9020
FFNN3+,(I) 0.4854+ 0.9085+
FFNN,, 0.5105+ 0.9026
FFNN,,+, 0.5313+ 0.9144+
FFNN3(R) 0.8224 -
FFNN3+,(R) 0.8367+ -
</table>
<tableCaption confidence="0.933176">
Table 5: Word alignment performance of various
FFNN-based models (F1-measure)
word alignments.
Table 4 demonstrates that the proposed RNN-
</tableCaption>
<bodyText confidence="0.989445">
based model outperforms IBM4 trained from the
unlabeled 40 K data by employing either the 1
K labeled data or the 9 K unlabeled data, which
is less than 25% of the training data for IBM4.
Consequently, the SMT system using RNN,,+,
trained from a small part of training data can
achieve comparable performance to that using
IBM4 trained from all training data, which is
shown in Table 3.
</bodyText>
<subsectionHeader confidence="0.9983415">
6.3 Effectiveness of Unsupervised
Learning/Agreement Constraints
</subsectionHeader>
<bodyText confidence="0.999979272727273">
The proposed unsupervised learning and agree-
ment constraints can be applied to any NN-based
alignment model. Table 5 shows the alignment
performance of the FFNN-based models trained
by our supervised/unsupervised approaches (s/u)
with and without our agreement constraints. In
Table 5, “+c” denotes that the agreement con-
straint was used, and “+” indicates that the
comparison with its corresponding baseline, i.e.,
FFNN3(I/R), is significant in the sign test with a
5% significance level.
Table 5 shows that FFNN3+,(R/I) and
FFNN,,+, achieve significantly better perfor-
mance than FFNN3(R/I) and FFNN,,, respec-
tively, in both BTEC and Hansards. In addi-
tion, FFNN,, and FFNN,,+, significantly out-
perform FFNN3(I) and FFNN3+,(I), respec-
tively, in BTEC. The performance of these mod-
els is comparable in Hansards. These results
indicate that the proposed unsupervised learning
and agreement constraint benefit the FFNN-based
model, similar to the RNN-based model.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999986913043478">
We have proposed a word alignment model based
on an RNN, which captures long alignment his-
tory through recurrent architectures. Furthermore,
we proposed an unsupervised method for training
our model using NCE and introduced an agree-
ment constraint that encourages word embeddings
to be consistent across alignment directions. Our
experiments have shown that the proposed model
outperforms the FFNN-based model (Yang et al.,
2013) for word alignment and machine translation,
and that the agreement constraint improves align-
ment performance.
In future, we plan to employ contexts composed
of surrounding words (e.g., c(fj) or c(eaj) in the
FFNN-based model) in our model, even though
our model implicitly encodes such contexts in the
alignment history. We also plan to enrich each
hidden layer in our model with multiple layers
following the success of Yang et al. (2013), in
which multiple hidden layers improved the perfor-
mance of the FFNN-based model. In addition, we
would like to prove the effectiveness of the pro-
posed method for other datasets.
</bodyText>
<sectionHeader confidence="0.997237" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999859666666667">
We thank the anonymous reviewers for their help-
ful suggestions and valuable comments on the first
version of this paper.
</bodyText>
<sectionHeader confidence="0.997801" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.989801375">
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint Language and Translation
Modeling with Recurrent Neural Networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044–
1054.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Janvin. 2003. A Neural Probabilistic Lan-
guage Model. Journal of Machine Learning Re-
search, 3:1137–1155.
Phil Blunsom and Trevor Cohn. 2006. Discriminative
Word Alignment with Conditional Random Fields.
In Proceedings of the 21st International Confer-
ence on Computational Linguistics and 44th Annual
Meeting of the Association for Computational Lin-
guistics, pages 65–72.
Peter F. Brown, Stephen A. Della Pietra, Vincent
J. Della Pietra, and Robert L. Mercer. 1993. The
Mathematics of Statistical Machine Translation: Pa-
rameter Estimation. Computational Linguistics,
19(2):263–311.
Ronan Collobert and Jason Weston. 2008. A Uni-
fied Architecture for Natural Language Processing:
Deep Neural Networks with Multitask Learning. In
</reference>
<page confidence="0.956981">
1478
</page>
<reference confidence="0.994573409090909">
Proceedings of the 25th International Conference on
Machine Learning, pages 160–167.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural Language Processing (Almost) from
Scratch. Journal of Machine Learning Research,
12:2493–2537.
George E. Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-Dependent Pre-trained Deep Neu-
ral Networks for Large Vocabulary Speech Recog-
nition. Audio, Speech, and Language Processing,
IEEE Transactions on, 20(1):30–42.
A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.
Maximum Likelihood from Incomplete Data via the
EM Algorithm. Journal of the Royal Statistical So-
ciety, Series B, 39(1):1–38.
Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A.
Smith. 2011. Unsupervised Word Alignment with
Arbitrary Features. In Proceedings of the 49th An-
nual Meeting of the Association for Computational
Linguistics: Human Language Technologies - Vol-
ume 1, pages 409–419.
Cameron S. Fordyce. 2007. Overview of the IWSLT
2007 Evaluation Campaign. In Proceedings of the
4th International Workshop on Spoken Languaeg
Translation, pages 1–12.
Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar.
2008. Better Alignments = Better Translations? In
Proceedings of the 46th Annual Conference of the
Association for Computational Linguistics: Human
Language Technologies, pages 986–993.
Chooi-Ling Goh, Taro Watanabe, Hirofumi Yamamoto,
and Eiichiro Sumita. 2010. Constraining a Gen-
erative Word Alignment Model with Discriminative
Output. IEICE Transactions, 93-D(7):1976–1983.
Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and
Benjamin K. Tsou. 2011. Overview of the Patent
Machine Translation Task at the NTCIR-9 Work-
shop. In Proceedings of the 9th NTCIR Workshop,
pages 559–578.
Jo˜ao V. Grac¸a, Kuzman Ganchev, and Ben Taskar.
2008. Expectation Maximization and Posterior
Constraints. In Advances in Neural Information
Processing Systems 20, pages 569–576.
Michael Gutmann and Aapo Hyv¨arinen. 2010. Noise-
Contrastive Estimation: A New Estimation Principle
for Unnormalized Statistical Models. In Proceed-
ings of the 13st International Conference on Artifi-
cial Intelligence and Statistics, pages 297–304.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
Continuous Translation Models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical Phrase-Based Translation. In Pro-
ceedings of the 2003 Human Language Technology
Conference: North American Chapter of the Associ-
ation for Computational Linguistics, pages 48–54.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-
dra Constrantin, and Evan Herbst. 2007. Moses:
Open Source Toolkit for Statistical Machine Trans-
lation. In Proceedings of the 45th Annual Meeting of
the Association for Computational Linguistics on In-
teractive Poster and Demonstration Sessions, pages
177–180.
Philipp Koehn. 2004. Statistical Significance Tests for
Machine Translation Evaluation. In Proceedings of
the 2004 Conference on Empirical Methods in Nat-
ural Language Processing, pages 388–395.
Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon.
2012. Continuous Space Translation Models with
Neural Networks. In Proceedings of the 2012 Con-
ference of the North American Chapter of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies, pages 39–48.
Percy Liang, Ben Taskar, and Dan Klein. 2006. Align-
ment by Agreement. In Proceedings of the Main
Conference on Human Language Technology Con-
ference of the North American Chapter of the As-
sociation of Computational Linguistics, pages 104–
111.
Evgeny Matusov, Richard Zens, and Hermann Ney.
2004. Symmetric Word Alignments for Statistical
Machine Translation. In Proceedings of the 20th In-
ternational Conference on Computational Linguis-
tics, pages 219–225.
Rada Mihalcea and Ted Pedersen. 2003. An Evalua-
tion Exercise for Word Alignment. In Proceedings
of the HLT-NAACL 2003 Workshop on Building and
Using Parallel Texts: Data Driven Machine Trans-
lation and Beyond, pages 1–10.
Tomas Mikolov and Geoffrey Zweig. 2012. Con-
text Dependent Recurrent Neural Network Lan-
guage Model. In Proceedings of the 4th IEEE Work-
shop on Spoken Language Technology, pages 234–
239.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock´y, and Sanjeev Khudanpur. 2010. Recur-
rent Neural Network based Language Model. In
Proceedings of 11th Annual Conference of the Inter-
national Speech Communication Association, pages
1045–1048.
Andriy Mnih and Yee Whye Teh. 2012. A Fast and
Simple Algorithm for Training Neural Probabilistic
Language Models. In Proceedings of the 29th In-
ternational Conference on Machine Learning, pages
1751–1758.
</reference>
<page confidence="0.895739">
1479
</page>
<reference confidence="0.999919273972602">
Robert C. Moore. 2005. A Discriminative Framework
for Bilingual Word Alignment. In Proceedings of
Human Language Technology Conference and Con-
ference on Empirical Methods in Natural Language
Processing, pages 81–88.
Franz Josef Och and Hermann Ney. 2003. A System-
atic Comparison of Various Statistical Alignment
Models. Computational Linguistics, 29:19–51.
Franz Josef Och. 2003. Minimum Error Rate Training
in Statistical Machine Translation. In Proceedings
of the 41st Annual Meeting of the Association for
Computational Linguistics, pages 160–167.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a Method for Automatic
Evaluation of Machine Translation. In Proceedings
of 40th Annual Meeting of the Association for Com-
putational Linguistics, pages 311–318.
D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
1986. Learning Internal Representations by Error
Propagation. In D. E. Rumelhart and J. L. McClel-
land, editors, Parallel Distributed Processing, pages
318–362. MIT Press.
Noah A. Smith and Jason Eisner. 2005. Contrastive
Estimation: Training Log-Linear Models on Unla-
beled Data. In Proceedings of the 43rd Annual
Meeting of the Association for Computational Lin-
guistics, pages 354–362.
Andreas Stolcke. 2002. SRILM - An Extensible Lan-
guage Modeling Toolkit. In Proceedings of Interna-
tional Conference on Spoken Language Processing,
pages 901–904.
Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain,
Ben Freiberg, Ralf Schl¨uter, and Hermann Ney.
2013. Comparison of Feedforward and Recurrent
Neural Network Language Models. In IEEE Inter-
national Conference on Acoustics, Speech, and Sig-
nal Processing, pages 8430–8434.
Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sug-
aya, Hirofumi Yamamoto, and Seiichi Yamamoto.
2002. Toward a Broad-coverage Bilingual Corpus
for Speech Translation of Travel Conversations in
the Real World. In Proceedings of the 3rd Interna-
tional Conference on Language Resources and Eval-
uation, pages 147–152.
Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005. A Discriminative Matching Approach to
Word Alignment. In Proceedings of Human Lan-
guage Technology Conference and Conference on
Empirical Methods in Natural Language Process-
ing, pages 73–80.
Ashish Vaswani, Liang Huang, and David Chiang.
2012. Smaller Alignment Models for Better Trans-
lations: Unsupervised Word Alignment with the lo-
norm. In Proceedings of the 50th Annual Meeting of
the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pages 311–319.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and
David Chiang. 2013. Decoding with Large-Scale
Neural Language Models Improves Translation. In
Proceedings of the 2013 Conference on Empirical
Methods in Natural Language Processing, pages
1387–1392.
Stephan Vogel, Hermann Ney, and Christoph Tillmann.
1996. Hmm-based Word Alignment in Statisti-
cal Translation. In Proceedings of the 16th Inter-
national Conference on Computational Linguistics,
pages 836–841.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word Alignment Modeling with Con-
text Dependent Deep Neural Network. In Proceed-
ings of the 51st Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 166–175.
</reference>
<page confidence="0.990642">
1480
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.851573">
<title confidence="0.997795">Recurrent Neural Networks for Word Alignment Model</title>
<author confidence="0.974339">Taro Watanabe</author>
<author confidence="0.974339">Eiichiro</author>
<affiliation confidence="0.980384">National Institute of Information and Communications</affiliation>
<address confidence="0.880287">3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto,</address>
<abstract confidence="0.999273777777778">This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint Language and Translation Modeling with Recurrent Neural Networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<contexts>
<context position="2190" citStr="Auli et al., 2013" startWordPosition="298" endWordPosition="301">Dependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to * The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks (Mikolov et al., 2010; Mikolov and Zweig, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Sundermeyer et al., 2013). An RNN has a hidden layer with recurrent connections that propagates its own previous signals. Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in input data. We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model. Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model. The NN-based alignment models are su</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint Language and Translation Modeling with Recurrent Neural Networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044– 1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Janvin</author>
</authors>
<title>A Neural Probabilistic Language Model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="9716" citStr="Bengio et al., 2003" startWordPosition="1534" endWordPosition="1537">atrices. The model receives a source and target word with their contexts as inputs, which are words in a predefined window (the window size is three in Figure 1). First, the lookup layer converts each input word into its word embedding by looking up its corresponding column in the embedding matrix (L), and then concatenates them. Let Vf (or Ve) be a set of source words (or target words) and M be a predetermined embedding length. L is a M x (|Vf |+ |Ve|) matrix1. Word embeddings are dense, low dimensional, and real-valued vectors that can capture syntactic and semantic properties of the words (Bengio et al., 2003). The concatenation (z0) is then fed to the hidden layer to capture nonlinear relations. Finally, the output layer receives the output of the hidden layer (z1) and computes a lexical translation score. 1We add a special token (unk) to handle unknown words and (null) to handle null alignments to Vf and Ve where H, BH, O, and BO are |z1 |x |z0|, |z1 |x 1, 1x|z1|, and 1x1 matrices, respectively, and f(x) is an activation function. Following Yang et al. (2013), a “hard” version of the hyperbolic tangent, htanh(x)3, is used as f(x) in this study. The alignment model based on an FFNN is formed in th</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Janvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Janvin. 2003. A Neural Probabilistic Language Model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Phil Blunsom</author>
<author>Trevor Cohn</author>
</authors>
<title>Discriminative Word Alignment with Conditional Random Fields.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>65--72</pages>
<contexts>
<context position="5639" citStr="Blunsom and Cohn (2006)" startWordPosition="829" endWordPosition="832"> model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks. For the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4. 2 Related Work Various word alignment models have been proposed. These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al. (1993), Vogel et al. (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al. (2005), Moore (2005), and Blunsom and Cohn (2006). 2.1 Generative Alignment Model Given a source language sentence fJ1 = f1, ..., fJ and a target language sentence eI1 = e1, ..., eI, fJ1 is generated by eI1 via the alignment aJ1 = a1,..., aJ. Each aj is a hidden variable indicating that the source word fj is aligned to the target word eaj. Usually, a “null” word e0 is added to the target language sentence and aJ1 may contain aj = 0, which indicates that fj is not aligned to any target word. The probability of generating the sentence fJ1 from eI1 is defined as �p(fJ1 |eI1) = p(fJ1 , aJ1 |eI1). (1) aJ 1 The IBM Models 1 and 2 and the HMM model</context>
</contexts>
<marker>Blunsom, Cohn, 2006</marker>
<rawString>Phil Blunsom and Trevor Cohn. 2006. Discriminative Word Alignment with Conditional Random Fields. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, pages 65–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The Mathematics of Statistical Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="741" citStr="Brown et al., 1993" startWordPosition="91" endWordPosition="94">nformation and Communications Technology 3-5 Hikaridai, Seika-cho, Soraku-gun, Kyoto, JAPAN a-tamura@ah.jp.nec.com, {taro.watanabe, eiichiro.sumita}@nict.go.jp Abstract This study proposes a word alignment model based on a recurrent neural network (RNN), in which an unlimited alignment history is represented by recurrently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical</context>
<context position="5476" citStr="Brown et al. (1993)" startWordPosition="800" endWordPosition="803">rd alignment tasks and Japanese-to-English and Chinese-to-English translation tasks. The results illustrate that our RNN-based model outperforms the FFNN-based model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks. For the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4. 2 Related Work Various word alignment models have been proposed. These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al. (1993), Vogel et al. (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al. (2005), Moore (2005), and Blunsom and Cohn (2006). 2.1 Generative Alignment Model Given a source language sentence fJ1 = f1, ..., fJ and a target language sentence eI1 = e1, ..., eI, fJ1 is generated by eI1 via the alignment aJ1 = a1,..., aJ. Each aj is a hidden variable indicating that the source word fj is aligned to the target word eaj. Usually, a “null” word e0 is added to the target language sentence and aJ1 may contain aj = 0, which indicates that fj is not aligned to any tar</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The Mathematics of Statistical Machine Translation: Parameter Estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th International Conference on Machine Learning,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="7663" citStr="Collobert and Weston, 2008" startWordPosition="1171" endWordPosition="1174">e HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as J sNN(aJ1 |fJ1 , eI1) = H ta(aj − aj−1|c(eaj−1)) j=1 &apos;tlex(fj,eaj|c(fj),c(eaj)), (4) 1471 t (fj , eaj |f j-1 , e ) j+1 aj+1 lex aj-1 The computations in the hidden and output layer are as follows2: Output Layer O× +BO z1 z1 = f(H x z0 + BH), (5) tlex = O x z1 + BO, (6) Hidden Layer z1 htanh(H× +BH) z0 z0 Figure 1: FFNN-based model for computing a lexical t</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning. In Proceedings of the 25th International Conference on Machine Learning, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural Language Processing (Almost) from Scratch.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="7688" citStr="Collobert et al., 2011" startWordPosition="1175" endWordPosition="1179">iterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as J sNN(aJ1 |fJ1 , eI1) = H ta(aj − aj−1|c(eaj−1)) j=1 &apos;tlex(fj,eaj|c(fj),c(eaj)), (4) 1471 t (fj , eaj |f j-1 , e ) j+1 aj+1 lex aj-1 The computations in the hidden and output layer are as follows2: Output Layer O× +BO z1 z1 = f(H x z0 + BH), (5) tlex = O x z1 + BO, (6) Hidden Layer z1 htanh(H× +BH) z0 z0 Figure 1: FFNN-based model for computing a lexical translation score of (fj, </context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural Language Processing (Almost) from Scratch. Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Dong Yu</author>
<author>Li Deng</author>
<author>Alex Acero</author>
</authors>
<title>Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition. Audio, Speech, and Language Processing,</title>
<date>2012</date>
<journal>IEEE Transactions on,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1642" citStr="Dahl et al., 2012" startWordPosition="222" endWordPosition="225">el (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical machine translation. The most classical approaches are the probabilistic IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Various studies have extended those models. Yang et al. (2013) adapted the ContextDependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to * The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks (Mikolov et al., 2010; Mikolov and Zweig, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Sundermeyer et al.</context>
<context position="7509" citStr="Dahl et al., 2012" startWordPosition="1148" endWordPosition="1151">, the best alignment (Viterbi alignment) of the sentence pair (fJ1 , eI1) can be found as aJ1 = argmax p(fJ1 , aJ1 |eI1). (3) aJ 1 For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as J sNN(aJ1 |fJ1 , eI1) = H ta(aj − aj−1|c(eaj−1)) j=1 &apos;tlex(fj,eaj|c(fj),c(eaj)), (4) 1471 t (fj , eaj |f j-1 , e ) j+1 aj+1 lex aj-1 The computations in the hidden and output layer are as follows2: Output</context>
</contexts>
<marker>Dahl, Yu, Deng, Acero, 2012</marker>
<rawString>George E. Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-Dependent Pre-trained Deep Neural Networks for Large Vocabulary Speech Recognition. Audio, Speech, and Language Processing, IEEE Transactions on, 20(1):30–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood from Incomplete Data via the EM Algorithm.</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, Series B,</journal>
<volume>39</volume>
<issue>1</issue>
<contexts>
<context position="6786" citStr="Dempster et al., 1977" startWordPosition="1033" endWordPosition="1036">eI1) = p(fJ1 , aJ1 |eI1). (1) aJ 1 The IBM Models 1 and 2 and the HMM model decompose it into an alignment probability pa and a lexical translation probability pt as J p(fJ1 , aJ1 |eI1) = H pa(aj|aj−1,j)pt(fj|eaj). (2) j=1 The three models differ in their definition of alignment probability. For example, the HMM model uses an alignment probability with a first-order Markov property: pa(aj|aj − aj−1). In addition, the IBM models 3-5 are extensions of these, which consider the fertility and distortion of each translated word. These models are trained using the expectationmaximization algorithm (Dempster et al., 1977) from bilingual sentences without word-level alignments (unlabeled training data). Given a specific model, the best alignment (Viterbi alignment) of the sentence pair (fJ1 , eI1) can be found as aJ1 = argmax p(fJ1 , aJ1 |eI1). (3) aJ 1 For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in </context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>A. P. Dempster, N. M. Laird, and D. B. Rubin. 1977. Maximum Likelihood from Incomplete Data via the EM Algorithm. Journal of the Royal Statistical Society, Series B, 39(1):1–38.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Dyer</author>
<author>Jonathan Clark</author>
<author>Alon Lavie</author>
<author>Noah A Smith</author>
</authors>
<title>Unsupervised Word Alignment with Arbitrary Features.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies -</booktitle>
<volume>1</volume>
<pages>409--419</pages>
<contexts>
<context position="15616" citStr="Dyer et al. (2011)" startWordPosition="2587" endWordPosition="2590">Rumelhart et al., 1986), which unfolds the network in time (j) and computes gradients over time steps. In addition, an l2 regularization term is added to the objective to prevent the model from overfitting the training data. The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq. 7 (Section 2.2). However, this approach requires gold standard alignments. To overcome this drawback, we propose an unsupervised method using NCE, which learns from unlabeled training data. 4.1 Unsupervised Learning Dyer et al. (2011) presented an unsupervised alignment model based on contrastive estimation (CE) (Smith and Eisner, 2005). CE seeks to discriminate observed data from its neighborhood, Hidden Layer d htanh(H× xj+Rd× yj-1+BH ) xj Output Layer 1473 which can be viewed as pseudo-negative samples. Dyer et al. (2011) regarded all possible alignments of the bilingual sentences, which are given as training data (T), and those of the full translation search space (Ω) as the observed data and its neighborhood, respectively. We introduce this idea to a ranking loss with margin as loss(θ) = max{ 0,1 − ∑ ED [so (a |f+, e+</context>
</contexts>
<marker>Dyer, Clark, Lavie, Smith, 2011</marker>
<rawString>Chris Dyer, Jonathan Clark, Alon Lavie, and Noah A. Smith. 2011. Unsupervised Word Alignment with Arbitrary Features. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies - Volume 1, pages 409–419.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cameron S Fordyce</author>
</authors>
<title>Overview of the IWSLT</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Spoken Languaeg Translation,</booktitle>
<pages>1--12</pages>
<contexts>
<context position="21439" citStr="Fordyce, 2007" startWordPosition="3572" endWordPosition="3573">d embeddings when updating Ot (or FE OtEF). 5 Experiment 5.1 Experimental Data We evaluated the alignment performance of the proposed models with two tasks: JapaneseEnglish word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). In addition, we evaluated the end-to-end translation performance of three tasks: a Chineseto-English translation task with the FBIS corpus (FBI5), the IWSLT 2007 Japanese-to-English translation task (IW5LT) (Fordyce, 2007), and the NTCIR-9 Japanese-to-English patent translation task (NTCIR) (Goto et al., 2011)6. Table 1 shows the sizes of our experimental datasets. Note that the development data was not used in the alignment tasks, i.e., BTEC 6We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable. and Hansards, because the hyperparameters of the alignment models were set by preliminary small-scale experiments. The BTEC data is the first 9,960 sentence pairs in the training data for IW5LT, which were annotated with word align</context>
</contexts>
<marker>Fordyce, 2007</marker>
<rawString>Cameron S. Fordyce. 2007. Overview of the IWSLT 2007 Evaluation Campaign. In Proceedings of the 4th International Workshop on Spoken Languaeg Translation, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kuzman Ganchev</author>
<author>Jo˜ao V Grac¸a</author>
<author>Ben Taskar</author>
</authors>
<title>Better Alignments = Better Translations?</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>986--993</pages>
<marker>Ganchev, Grac¸a, Taskar, 2008</marker>
<rawString>Kuzman Ganchev, Jo˜ao V. Grac¸a, and Ben Taskar. 2008. Better Alignments = Better Translations? In Proceedings of the 46th Annual Conference of the Association for Computational Linguistics: Human Language Technologies, pages 986–993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chooi-Ling Goh</author>
<author>Taro Watanabe</author>
<author>Hirofumi Yamamoto</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Constraining a Generative Word Alignment Model with Discriminative Output.</title>
<date>2010</date>
<journal>IEICE Transactions,</journal>
<pages>93--7</pages>
<contexts>
<context position="22062" citStr="Goh et al., 2010" startWordPosition="3668" endWordPosition="3671">the NTCIR-9 Japanese-to-English patent translation task (NTCIR) (Goto et al., 2011)6. Table 1 shows the sizes of our experimental datasets. Note that the development data was not used in the alignment tasks, i.e., BTEC 6We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable. and Hansards, because the hyperparameters of the alignment models were set by preliminary small-scale experiments. The BTEC data is the first 9,960 sentence pairs in the training data for IW5LT, which were annotated with word alignment (Goh et al., 2010). We split these pairs into the first 9,000 for training data and the remaining 960 as test data. All the data in BTEC is word-aligned, and the training data in Hansards is unlabeled data. In FBI5, we used the NIST02 evaluation data as the development data, and the NIST03 and 04 evaluation data as test data (NI5T03 and NI5T04). 5.2 Comparing Methods We evaluated the proposed RNN-based alignment models against two baselines: the IBM Model 4 and the FFNN-based model with one hidden layer. The IBM Model 4 was trained by previously presented model sequence schemes (Och and Ney, 2003): 15H53545, i.</context>
</contexts>
<marker>Goh, Watanabe, Yamamoto, Sumita, 2010</marker>
<rawString>Chooi-Ling Goh, Taro Watanabe, Hirofumi Yamamoto, and Eiichiro Sumita. 2010. Constraining a Generative Word Alignment Model with Discriminative Output. IEICE Transactions, 93-D(7):1976–1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Isao Goto</author>
<author>Bin Lu</author>
<author>Ka Po Chow</author>
<author>Eiichiro Sumita</author>
<author>Benjamin K Tsou</author>
</authors>
<date>2011</date>
<booktitle>Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop. In Proceedings of the 9th NTCIR Workshop,</booktitle>
<pages>559--578</pages>
<contexts>
<context position="21528" citStr="Goto et al., 2011" startWordPosition="3583" endWordPosition="3586">luated the alignment performance of the proposed models with two tasks: JapaneseEnglish word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). In addition, we evaluated the end-to-end translation performance of three tasks: a Chineseto-English translation task with the FBIS corpus (FBI5), the IWSLT 2007 Japanese-to-English translation task (IW5LT) (Fordyce, 2007), and the NTCIR-9 Japanese-to-English patent translation task (NTCIR) (Goto et al., 2011)6. Table 1 shows the sizes of our experimental datasets. Note that the development data was not used in the alignment tasks, i.e., BTEC 6We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable. and Hansards, because the hyperparameters of the alignment models were set by preliminary small-scale experiments. The BTEC data is the first 9,960 sentence pairs in the training data for IW5LT, which were annotated with word alignment (Goh et al., 2010). We split these pairs into the first 9,000 for training data and </context>
</contexts>
<marker>Goto, Lu, Chow, Sumita, Tsou, 2011</marker>
<rawString>Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, and Benjamin K. Tsou. 2011. Overview of the Patent Machine Translation Task at the NTCIR-9 Workshop. In Proceedings of the 9th NTCIR Workshop, pages 559–578.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jo˜ao V Grac¸a</author>
<author>Kuzman Ganchev</author>
<author>Ben Taskar</author>
</authors>
<title>Expectation Maximization and Posterior Constraints.</title>
<date>2008</date>
<booktitle>In Advances in Neural Information Processing Systems 20,</booktitle>
<pages>569--576</pages>
<marker>Grac¸a, Ganchev, Taskar, 2008</marker>
<rawString>Jo˜ao V. Grac¸a, Kuzman Ganchev, and Ben Taskar. 2008. Expectation Maximization and Posterior Constraints. In Advances in Neural Information Processing Systems 20, pages 569–576.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Gutmann</author>
<author>Aapo Hyv¨arinen</author>
</authors>
<title>NoiseContrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models.</title>
<date>2010</date>
<booktitle>In Proceedings of the 13st International Conference on Artificial Intelligence and Statistics,</booktitle>
<pages>297--304</pages>
<marker>Gutmann, Hyv¨arinen, 2010</marker>
<rawString>Michael Gutmann and Aapo Hyv¨arinen. 2010. NoiseContrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models. In Proceedings of the 13st International Conference on Artificial Intelligence and Statistics, pages 297–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent Continuous Translation Models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="2222" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="302" endWordPosition="305">al Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to * The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks (Mikolov et al., 2010; Mikolov and Zweig, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Sundermeyer et al., 2013). An RNN has a hidden layer with recurrent connections that propagates its own previous signals. Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in input data. We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model. Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model. The NN-based alignment models are supervised models. Unfortunately, </context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent Continuous Translation Models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical Phrase-Based Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Human Language Technology Conference: North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>48--54</pages>
<contexts>
<context position="26049" citStr="Koehn et al., 2003" startWordPosition="4327" endWordPosition="4330">ord Alignment Results Table 2 shows the alignment performance by the F1-measure. Hereafter, MODEL(R) and MODEL(I) denote the MODEL trained from gold standard alignments and word alignments found by the IBM Model 4, respectively. In Hansards, all models were trained from ran7http://www.fit.vutbr.cz/˜imikolov/ rnnlm/ 8http://chasen-legacy.sourceforge.jp/ 9http://nlp.stanford.edu/software/ segmenter.shtml domly sampled 100 K data10. We evaluated the word alignments produced by first applying each model in both directions and then combining the alignments using the “grow-diag-finaland” heuristic (Koehn et al., 2003). The significance test on word alignment performance was performed by the sign test with a 5% significance level. “+” in Table 2 indicates that the comparisons are significant over corresponding baselines, IBM4 and FFNN3(R/I). In Table 2, RNN,,+,, which includes all our proposals, i.e., the RNN-based model, the unsupervised learning, and the agreement constraint, achieves the best performance for both BTEC and Hansards. The differences from the baselines are statistically significant. Table 2 shows that RNN3(R/I) outperforms FFNN3(R/I), which is statistically significant in BTEC. These result</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical Phrase-Based Translation. In Proceedings of the 2003 Human Language Technology Conference: North American Chapter of the Association for Computational Linguistics, pages 48–54.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and Evan Herbst.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<contexts>
<context position="24889" citStr="Koehn et al., 2007" startWordPosition="4154" endWordPosition="4157">e (F1- measure) used the RNNLM Toolkit 7 (Mikolov et al., 2010) with the default options. We mapped all words that occurred less than five times to the special token ⟨unk⟩. Next, each weight was optimized using the mini-batch SGD, where batch size D was 100, learning rate was 0.01, and an l2 regularization parameter was 0.1. The training stopped after 50 epochs. The other parameters were set as follows: W, N and C in the unsupervised learning were 100, 50, and 0.001, respectively, and α for the agreement constraint was 0.1. In the translation tasks, we used the Moses phrase-based SMT systems (Koehn et al., 2007). All Japanese and Chinese sentences were segmented by ChaSen8 and the Stanford Chinese segmenter9, respectively. In the training, long sentences with over 40 words were filtered out. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we trained a 5-gram language model on the English side of each training data for IW 5LT and NTCIR, and a 5-gram language model on the Xinhua portion of the English Gigaword corpus for FBI5. The SMT weighting parameters were tuned by MERT (Och, 2003) in the development data. 5.3 Word Alignment Results Table 2 shows the alignment performan</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constrantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics on Interactive Poster and Demonstration Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical Significance Tests for Machine Translation Evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="28770" citStr="Koehn, 2004" startWordPosition="4744" endWordPosition="4745">47 27.25 25.41 27.65 FFNNs(I) 46.38 27.05 25.45 27.61 RNNs(I) 46.43 27.24 25.47 27.56 RNNs+c(I) 46.51 27.12 25.55 27.73 RNNu 47.05* 27.79* 25.76* 27.91* RNNu+c 46.97* 27.76* 25.84* 28.20* Table 3: Translation performance (BLEU4(%)) domly sampled 100 K data, and then a translation model was trained from all the training data that was word-aligned by the alignment model. In addition, for a detailed comparison, we evaluated the SMT system where the IBM Model 4 was trained from all the training data (IBM4all). The significance test on translation performance was performed by the bootstrap method (Koehn, 2004) with a 5% significance level. “*” in Table 3 indicates that the comparisons are significant over both baselines, i.e., IBM4 and FFNNs(I). Table 3 also shows that better word alignment does not always result in better translation, which has been discussed previously (Yang et al., 2013). However, RNNu and RNNu+c outperform FFNNs(I) and IBM4 in all tasks. These results indicate that our proposals contribute to improving translation performance12. In addition, Table 3 shows that these proposed models are comparable to IBM4all in NTCIR and FBIS even though the proposed models are trained from only</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical Significance Tests for Machine Translation Evaluation. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai-Son Le</author>
<author>Alexandre Allauzen</author>
<author>Franc¸ois Yvon</author>
</authors>
<title>Continuous Space Translation Models with Neural Networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,</booktitle>
<pages>39--48</pages>
<contexts>
<context position="7559" citStr="Le et al., 2012" startWordPosition="1155" endWordPosition="1158">nce pair (fJ1 , eI1) can be found as aJ1 = argmax p(fJ1 , aJ1 |eI1). (3) aJ 1 For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as J sNN(aJ1 |fJ1 , eI1) = H ta(aj − aj−1|c(eaj−1)) j=1 &apos;tlex(fj,eaj|c(fj),c(eaj)), (4) 1471 t (fj , eaj |f j-1 , e ) j+1 aj+1 lex aj-1 The computations in the hidden and output layer are as follows2: Output Layer O× +BO z1 z1 = f(H x z0 + BH), (5) tlex = O</context>
</contexts>
<marker>Le, Allauzen, Yvon, 2012</marker>
<rawString>Hai-Son Le, Alexandre Allauzen, and Franc¸ois Yvon. 2012. Continuous Space Translation Models with Neural Networks. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 39–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Percy Liang</author>
<author>Ben Taskar</author>
<author>Dan Klein</author>
</authors>
<title>Alignment by Agreement.</title>
<date>2006</date>
<booktitle>In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>104--111</pages>
<contexts>
<context position="4152" citStr="Liang et al., 2006" startWordPosition="594" endWordPosition="597"> the original bilingual sentences are higher than those of the sampled bilingual sentences. Our RNN-based alignment model has a direc1470 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470–1480, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tion, such as other alignment models, i.e., from f (source language) to e (target language) and from e to f. It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently (Matusov et al., 2004; Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2008). The motivation for this stems from the fact that model and generalization errors by the two models differ, and the models must complement each other. Based on this motivation, our directional models are also simultaneously trained. Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function. This constraint prevents each model from overfitting to a particular direction and leads to global optimizat</context>
<context position="18448" citStr="Liang et al., 2006" startWordPosition="3071" endWordPosition="3074">rior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1. 4.2 Agreement Constraints Both of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e., they can represent one-to-many relations from the target side. Asymmetric models are usually trained in each alignment direction. The model proposed by Yang et al. (2013) is no exception. However, it has been demonstrated that encouraging directional models to agree improves alignment performance (Matusov et al., 2004; Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2008). Inspired by their work, we introduce an agreement constraint to our learning. The constraint concretely enforces agreement in word embeddings of both directions. The proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings: argmin oFE {loss(θFE) + α∥θLEF − θLFE∥}, (13) oEF {loss(θEF) + α∥θLFE − θLEF ∥}, (14) argmin where θFE (or θEF) denotes the weights of layers in a source-to-target (or target-to-source) alignment model, θL den</context>
</contexts>
<marker>Liang, Taskar, Klein, 2006</marker>
<rawString>Percy Liang, Ben Taskar, and Dan Klein. 2006. Alignment by Agreement. In Proceedings of the Main Conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 104– 111.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeny Matusov</author>
<author>Richard Zens</author>
<author>Hermann Ney</author>
</authors>
<title>Symmetric Word Alignments for Statistical Machine Translation.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>219--225</pages>
<contexts>
<context position="4132" citStr="Matusov et al., 2004" startWordPosition="589" endWordPosition="593">uch that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences. Our RNN-based alignment model has a direc1470 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470–1480, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tion, such as other alignment models, i.e., from f (source language) to e (target language) and from e to f. It has been proven that the limitation may be overcome by encouraging two directional models to agree by training them concurrently (Matusov et al., 2004; Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2008). The motivation for this stems from the fact that model and generalization errors by the two models differ, and the models must complement each other. Based on this motivation, our directional models are also simultaneously trained. Specifically, our training encourages word embeddings to be consistent across alignment directions by introducing a penalty term that expresses the difference between embedding of words into an objective function. This constraint prevents each model from overfitting to a particular direction and leads</context>
<context position="18428" citStr="Matusov et al., 2004" startWordPosition="3066" endWordPosition="3070"> IBM Model 1 with l0 prior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1. 4.2 Agreement Constraints Both of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e., they can represent one-to-many relations from the target side. Asymmetric models are usually trained in each alignment direction. The model proposed by Yang et al. (2013) is no exception. However, it has been demonstrated that encouraging directional models to agree improves alignment performance (Matusov et al., 2004; Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2008). Inspired by their work, we introduce an agreement constraint to our learning. The constraint concretely enforces agreement in word embeddings of both directions. The proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings: argmin oFE {loss(θFE) + α∥θLEF − θLFE∥}, (13) oEF {loss(θEF) + α∥θLFE − θLEF ∥}, (14) argmin where θFE (or θEF) denotes the weights of layers in a source-to-target (or target-to-source) ali</context>
</contexts>
<marker>Matusov, Zens, Ney, 2004</marker>
<rawString>Evgeny Matusov, Richard Zens, and Hermann Ney. 2004. Symmetric Word Alignments for Statistical Machine Translation. In Proceedings of the 20th International Conference on Computational Linguistics, pages 219–225.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Ted Pedersen</author>
</authors>
<title>An Evaluation Exercise for Word Alignment.</title>
<date>2003</date>
<booktitle>In Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond,</booktitle>
<pages>1--10</pages>
<contexts>
<context position="21215" citStr="Mihalcea and Pedersen, 2003" startWordPosition="3540" endWordPosition="3543">). Lines 4-1 and 4-2 update the weights in each layer following a given objective (Sections 4.1 and 4.2). Note that Ot F Eand OtEF are concurrently updated in each iteration, and OtEF (or OtFE) is employed to enforce agreement between word embeddings when updating Ot (or FE OtEF). 5 Experiment 5.1 Experimental Data We evaluated the alignment performance of the proposed models with two tasks: JapaneseEnglish word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). In addition, we evaluated the end-to-end translation performance of three tasks: a Chineseto-English translation task with the FBIS corpus (FBI5), the IWSLT 2007 Japanese-to-English translation task (IW5LT) (Fordyce, 2007), and the NTCIR-9 Japanese-to-English patent translation task (NTCIR) (Goto et al., 2011)6. Table 1 shows the sizes of our experimental datasets. Note that the development data was not used in the alignment tasks, i.e., BTEC 6We did not evaluate the translation performance on the Hansards data because the development data is very small and performance is unreliable. and Han</context>
</contexts>
<marker>Mihalcea, Pedersen, 2003</marker>
<rawString>Rada Mihalcea and Ted Pedersen. 2003. An Evaluation Exercise for Word Alignment. In Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pages 1–10.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Context Dependent Recurrent Neural Network Language Model.</title>
<date>2012</date>
<booktitle>In Proceedings of the 4th IEEE Workshop on Spoken Language Technology,</booktitle>
<pages>234--239</pages>
<contexts>
<context position="2171" citStr="Mikolov and Zweig, 2012" startWordPosition="294" endWordPosition="297">2013) adapted the ContextDependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to * The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed FFNN-based models for various tasks (Mikolov et al., 2010; Mikolov and Zweig, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Sundermeyer et al., 2013). An RNN has a hidden layer with recurrent connections that propagates its own previous signals. Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in input data. We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model. Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model. The NN-based alig</context>
</contexts>
<marker>Mikolov, Zweig, 2012</marker>
<rawString>Tomas Mikolov and Geoffrey Zweig. 2012. Context Dependent Recurrent Neural Network Language Model. In Proceedings of the 4th IEEE Workshop on Spoken Language Technology, pages 234– 239.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock´y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent Neural Network based Language Model.</title>
<date>2010</date>
<booktitle>In Proceedings of 11th Annual Conference of the International Speech Communication Association,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock´y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock´y, and Sanjeev Khudanpur. 2010. Recurrent Neural Network based Language Model. In Proceedings of 11th Annual Conference of the International Speech Communication Association, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andriy Mnih</author>
<author>Yee Whye Teh</author>
</authors>
<title>A Fast and Simple Algorithm for Training Neural Probabilistic Language Models.</title>
<date>2012</date>
<booktitle>In Proceedings of the 29th International Conference on Machine Learning,</booktitle>
<pages>1751--1758</pages>
<contexts>
<context position="3280" citStr="Mnih and Teh, 2012" startWordPosition="458" endWordPosition="461">te an alignment history, e.g., bilingual context, which is longer than the FFNN-based model. The NN-based alignment models are supervised models. Unfortunately, it is usually difficult to prepare word-by-word aligned bilingual data. Yang et al. (2013) trained their model from word alignments produced by traditional unsupervised probabilistic models. However, with this approach, errors induced by probabilistic models are learned as correct alignments; thus, generalization capabilities are limited. To solve this problem, we apply noise-contrastive estimation (NCE) (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012) for unsupervised training of our RNN-based model without gold standard alignments or pseudo-oracle alignments. NCE artificially generates bilingual sentences through samplings as pseudo-negative samples, and then trains the model such that the scores of the original bilingual sentences are higher than those of the sampled bilingual sentences. Our RNN-based alignment model has a direc1470 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1470–1480, Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics tion, such</context>
</contexts>
<marker>Mnih, Teh, 2012</marker>
<rawString>Andriy Mnih and Yee Whye Teh. 2012. A Fast and Simple Algorithm for Training Neural Probabilistic Language Models. In Proceedings of the 29th International Conference on Machine Learning, pages 1751–1758.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
</authors>
<title>A Discriminative Framework for Bilingual Word Alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>81--88</pages>
<contexts>
<context position="5610" citStr="Moore (2005)" startWordPosition="826" endWordPosition="827">rms the FFNN-based model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks. For the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4. 2 Related Work Various word alignment models have been proposed. These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al. (1993), Vogel et al. (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al. (2005), Moore (2005), and Blunsom and Cohn (2006). 2.1 Generative Alignment Model Given a source language sentence fJ1 = f1, ..., fJ and a target language sentence eI1 = e1, ..., eI, fJ1 is generated by eI1 via the alignment aJ1 = a1,..., aJ. Each aj is a hidden variable indicating that the source word fj is aligned to the target word eaj. Usually, a “null” word e0 is added to the target language sentence and aJ1 may contain aj = 0, which indicates that fj is not aligned to any target word. The probability of generating the sentence fJ1 from eI1 is defined as �p(fJ1 |eI1) = p(fJ1 , aJ1 |eI1). (1) aJ 1 The IBM Mod</context>
</contexts>
<marker>Moore, 2005</marker>
<rawString>Robert C. Moore. 2005. A Discriminative Framework for Bilingual Word Alignment. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics,</title>
<date>2003</date>
<pages>29--19</pages>
<contexts>
<context position="5521" citStr="Och and Ney (2003)" startWordPosition="809" endWordPosition="812"> Chinese-to-English translation tasks. The results illustrate that our RNN-based model outperforms the FFNN-based model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks. For the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4. 2 Related Work Various word alignment models have been proposed. These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al. (1993), Vogel et al. (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al. (2005), Moore (2005), and Blunsom and Cohn (2006). 2.1 Generative Alignment Model Given a source language sentence fJ1 = f1, ..., fJ and a target language sentence eI1 = e1, ..., eI, fJ1 is generated by eI1 via the alignment aJ1 = a1,..., aJ. Each aj is a hidden variable indicating that the source word fj is aligned to the target word eaj. Usually, a “null” word e0 is added to the target language sentence and aJ1 may contain aj = 0, which indicates that fj is not aligned to any target word. The probability of generating the s</context>
<context position="22648" citStr="Och and Ney, 2003" startWordPosition="3769" endWordPosition="3772">ord alignment (Goh et al., 2010). We split these pairs into the first 9,000 for training data and the remaining 960 as test data. All the data in BTEC is word-aligned, and the training data in Hansards is unlabeled data. In FBI5, we used the NIST02 evaluation data as the development data, and the NIST03 and 04 evaluation data as test data (NI5T03 and NI5T04). 5.2 Comparing Methods We evaluated the proposed RNN-based alignment models against two baselines: the IBM Model 4 and the FFNN-based model with one hidden layer. The IBM Model 4 was trained by previously presented model sequence schemes (Och and Ney, 2003): 15H53545, i.e., five iterations of the IBM Model 1 followed by five iterations of the HMM Model, etc., which is the default setting for GIZA++ (IBM4). For the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer |z1 |to 100, and the window size of contexts to 5. Hence, |z0 |is 300 (30×5×2). Following Yang et al. (2013), the FFNN-based model was trained by the supervised approach described in Section 2.2 (FFNNs). For the RNN-based models, we set M to 30 and the number of units of each recurrent hidden layer |yj |to 100. Thus, |xj |is 60 (30 × 2). T</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A Systematic Comparison of Various Statistical Alignment Models. Computational Linguistics, 29:19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="25399" citStr="Och, 2003" startWordPosition="4243" endWordPosition="4244">t was 0.1. In the translation tasks, we used the Moses phrase-based SMT systems (Koehn et al., 2007). All Japanese and Chinese sentences were segmented by ChaSen8 and the Stanford Chinese segmenter9, respectively. In the training, long sentences with over 40 words were filtered out. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we trained a 5-gram language model on the English side of each training data for IW 5LT and NTCIR, and a 5-gram language model on the Xinhua portion of the English Gigaword corpus for FBI5. The SMT weighting parameters were tuned by MERT (Och, 2003) in the development data. 5.3 Word Alignment Results Table 2 shows the alignment performance by the F1-measure. Hereafter, MODEL(R) and MODEL(I) denote the MODEL trained from gold standard alignments and word alignments found by the IBM Model 4, respectively. In Hansards, all models were trained from ran7http://www.fit.vutbr.cz/˜imikolov/ rnnlm/ 8http://chasen-legacy.sourceforge.jp/ 9http://nlp.stanford.edu/software/ segmenter.shtml domly sampled 100 K data10. We evaluated the word alignments produced by first applying each model in both directions and then combining the alignments using the “</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Josef Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>311--318</pages>
<contexts>
<context position="27713" citStr="Papineni et al., 2002" startWordPosition="4575" endWordPosition="4578">etter models in both the supervised and unsupervised approaches. In BTEC, RNN,, and RNN,,+, significantly outperform RNN3(I) and RNN3+,(I), respectively. The performance of these models is comparable with Hansards. This indicates that our unsupervised learning benefits our models because the supervised models are adversely affected by errors in the automatically generated training data. This is especially true when the quality of training data, i.e., the performance of IBM4, is low. 5.4 Machine Translation Results Table 3 shows the translation performance by the case sensitive BLEU4 metric11 (Papineni et al., 2002). Table 3 presents the average BLEU of three different MERT runs. In NTCIR and FBI5, each alignment model was trained from the ran10Due to high computational cost, we did not use all the training data. Scaling up to larger datasets will be addressed in future work. 11We used mteval-v13a.pl as the evaluation tool (http://www.itl.nist.gov/iad/mig/tests/ mt/2009/). 1476 Alignment IWSLT NTCIR FBIS NIST03 NIST04 IBM4all 27.91 25.90 28.34 IBM4 46.47 27.25 25.41 27.65 FFNNs(I) 46.38 27.05 25.45 27.61 RNNs(I) 46.43 27.24 25.47 27.56 RNNs+c(I) 46.51 27.12 25.55 27.73 RNNu 47.05* 27.79* 25.76* 27.91* RN</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 311–318.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D E Rumelhart</author>
<author>G E Hinton</author>
<author>R J Williams</author>
</authors>
<title>Learning Internal Representations by Error Propagation.</title>
<date>1986</date>
<booktitle>Parallel Distributed Processing,</booktitle>
<pages>318--362</pages>
<editor>In D. E. Rumelhart and J. L. McClelland, editors,</editor>
<publisher>MIT Press.</publisher>
<contexts>
<context position="10572" citStr="Rumelhart et al., 1986" startWordPosition="1682" endWordPosition="1685">o handle unknown words and (null) to handle null alignments to Vf and Ve where H, BH, O, and BO are |z1 |x |z0|, |z1 |x 1, 1x|z1|, and 1x1 matrices, respectively, and f(x) is an activation function. Following Yang et al. (2013), a “hard” version of the hyperbolic tangent, htanh(x)3, is used as f(x) in this study. The alignment model based on an FFNN is formed in the same manner as the lexical translation model. Each model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent (SGD)4, where gradients are computed by the back-propagation algorithm (Rumelhart et al., 1986): loss(0) = � max{0,1 − sθ(a+|f, e) (f,e)ET +sθ(a−|f, e)}, (7) where 0 denotes the weights of layers in the model, T is a set of training data, a+ is the gold standard alignment, a− is the incorrect alignment with the highest score under 0, and sθ denotes the score defined by Eq. 4 as computed by the model under 0. 3 RNN-based Alignment Model This section proposes an RNN-based alignment model, which computes a score for alignments aJ1 using an RNN: tRNN(aj|aj−1 1 , fj, eaj), (8) where tRNN is the score of an alignment aj. The prediction of the j-th alignment aj depends on all preceding alignme</context>
<context position="15021" citStr="Rumelhart et al., 1986" startWordPosition="2492" endWordPosition="2495">Under the recurrence, the proposed model compactly encodes the entire history of previous alignments in the hidden layer configuration yi. Therefore, the proposed model can find alignments by taking advantage of the long alignment history, while the FFNN-based model considers only the last alignment. 4 Training During training, we optimize the weight matrices of each layer (i.e., L, Hd, Rd, Bd�, O, and BO) following a given objective using a mini-batch SGD with batch size D, which converges faster than a plain SGD (D = 1). Gradients are computed by the back-propagation through time algorithm (Rumelhart et al., 1986), which unfolds the network in time (j) and computes gradients over time steps. In addition, an l2 regularization term is added to the objective to prevent the model from overfitting the training data. The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq. 7 (Section 2.2). However, this approach requires gold standard alignments. To overcome this drawback, we propose an unsupervised method using NCE, which learns from unlabeled training data. 4.1 Unsupervised Learning Dyer et al. (2011) pres</context>
</contexts>
<marker>Rumelhart, Hinton, Williams, 1986</marker>
<rawString>D. E. Rumelhart, G. E. Hinton, and R. J. Williams. 1986. Learning Internal Representations by Error Propagation. In D. E. Rumelhart and J. L. McClelland, editors, Parallel Distributed Processing, pages 318–362. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noah A Smith</author>
<author>Jason Eisner</author>
</authors>
<title>Contrastive Estimation: Training Log-Linear Models on Unlabeled Data.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>354--362</pages>
<contexts>
<context position="15720" citStr="Smith and Eisner, 2005" startWordPosition="2601" endWordPosition="2604">ps. In addition, an l2 regularization term is added to the objective to prevent the model from overfitting the training data. The RNN-based model can be trained by a supervised approach, similar to the FFNN-based model, where training proceeds based on the ranking loss defined by Eq. 7 (Section 2.2). However, this approach requires gold standard alignments. To overcome this drawback, we propose an unsupervised method using NCE, which learns from unlabeled training data. 4.1 Unsupervised Learning Dyer et al. (2011) presented an unsupervised alignment model based on contrastive estimation (CE) (Smith and Eisner, 2005). CE seeks to discriminate observed data from its neighborhood, Hidden Layer d htanh(H× xj+Rd× yj-1+BH ) xj Output Layer 1473 which can be viewed as pseudo-negative samples. Dyer et al. (2011) regarded all possible alignments of the bilingual sentences, which are given as training data (T), and those of the full translation search space (Ω) as the observed data and its neighborhood, respectively. We introduce this idea to a ranking loss with margin as loss(θ) = max{ 0,1 − ∑ ED [so (a |f+, e+)] l (f+,e+)ET ED[so(a|f+,e−)] 11 )]}, (11) where Φ is a set of all possible alignments given (f, e), ED</context>
</contexts>
<marker>Smith, Eisner, 2005</marker>
<rawString>Noah A. Smith and Jason Eisner. 2005. Contrastive Estimation: Training Log-Linear Models on Unlabeled Data. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 354–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - An Extensible Language Modeling Toolkit. In</title>
<date>2002</date>
<booktitle>Proceedings of International Conference on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<contexts>
<context position="25113" citStr="Stolcke, 2002" startWordPosition="4192" endWordPosition="4193">, where batch size D was 100, learning rate was 0.01, and an l2 regularization parameter was 0.1. The training stopped after 50 epochs. The other parameters were set as follows: W, N and C in the unsupervised learning were 100, 50, and 0.001, respectively, and α for the agreement constraint was 0.1. In the translation tasks, we used the Moses phrase-based SMT systems (Koehn et al., 2007). All Japanese and Chinese sentences were segmented by ChaSen8 and the Stanford Chinese segmenter9, respectively. In the training, long sentences with over 40 words were filtered out. Using the SRILM Toolkits (Stolcke, 2002) with modified Kneser-Ney smoothing, we trained a 5-gram language model on the English side of each training data for IW 5LT and NTCIR, and a 5-gram language model on the Xinhua portion of the English Gigaword corpus for FBI5. The SMT weighting parameters were tuned by MERT (Och, 2003) in the development data. 5.3 Word Alignment Results Table 2 shows the alignment performance by the F1-measure. Hereafter, MODEL(R) and MODEL(I) denote the MODEL trained from gold standard alignments and word alignments found by the IBM Model 4, respectively. In Hansards, all models were trained from ran7http://w</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - An Extensible Language Modeling Toolkit. In Proceedings of International Conference on Spoken Language Processing, pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Sundermeyer</author>
<author>Ilya Oparin</author>
<author>Jean-Luc Gauvain</author>
<author>Ben Freiberg</author>
<author>Ralf Schl¨uter</author>
<author>Hermann Ney</author>
</authors>
<title>Comparison of Feedforward and Recurrent Neural Network Language Models.</title>
<date>2013</date>
<booktitle>In IEEE International Conference on Acoustics, Speech, and Signal Processing,</booktitle>
<pages>8430--8434</pages>
<marker>Sundermeyer, Oparin, Gauvain, Freiberg, Schl¨uter, Ney, 2013</marker>
<rawString>Martin Sundermeyer, Ilya Oparin, Jean-Luc Gauvain, Ben Freiberg, Ralf Schl¨uter, and Hermann Ney. 2013. Comparison of Feedforward and Recurrent Neural Network Language Models. In IEEE International Conference on Acoustics, Speech, and Signal Processing, pages 8430–8434.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Toshiyuki Takezawa</author>
<author>Eiichiro Sumita</author>
<author>Fumiaki Sugaya</author>
<author>Hirofumi Yamamoto</author>
<author>Seiichi Yamamoto</author>
</authors>
<title>Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World.</title>
<date>2002</date>
<booktitle>In Proceedings of the 3rd International Conference on Language Resources and Evaluation,</booktitle>
<pages>147--152</pages>
<contexts>
<context position="21083" citStr="Takezawa et al., 2002" startWordPosition="3521" endWordPosition="3524">0 K 878 919 NI5T04 1,597 IW5LT 40 K 2,501 489 NTCIR 3.2 M 2,000 2,000 Table 1: Size of experimental datasets IBM1 (Section 4.1). Lines 4-1 and 4-2 update the weights in each layer following a given objective (Sections 4.1 and 4.2). Note that Ot F Eand OtEF are concurrently updated in each iteration, and OtEF (or OtFE) is employed to enforce agreement between word embeddings when updating Ot (or FE OtEF). 5 Experiment 5.1 Experimental Data We evaluated the alignment performance of the proposed models with two tasks: JapaneseEnglish word alignment with the Basic Travel Expression Corpus (BTEC) (Takezawa et al., 2002) and French-English word alignment with the Hansard dataset (Hansards) from the 2003 NAACL shared task (Mihalcea and Pedersen, 2003). In addition, we evaluated the end-to-end translation performance of three tasks: a Chineseto-English translation task with the FBIS corpus (FBI5), the IWSLT 2007 Japanese-to-English translation task (IW5LT) (Fordyce, 2007), and the NTCIR-9 Japanese-to-English patent translation task (NTCIR) (Goto et al., 2011)6. Table 1 shows the sizes of our experimental datasets. Note that the development data was not used in the alignment tasks, i.e., BTEC 6We did not evaluat</context>
</contexts>
<marker>Takezawa, Sumita, Sugaya, Yamamoto, Yamamoto, 2002</marker>
<rawString>Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya, Hirofumi Yamamoto, and Seiichi Yamamoto. 2002. Toward a Broad-coverage Bilingual Corpus for Speech Translation of Travel Conversations in the Real World. In Proceedings of the 3rd International Conference on Language Resources and Evaluation, pages 147–152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Taskar</author>
<author>Simon Lacoste-Julien</author>
<author>Dan Klein</author>
</authors>
<title>A Discriminative Matching Approach to Word Alignment.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>73--80</pages>
<contexts>
<context position="5596" citStr="Taskar et al. (2005)" startWordPosition="822" endWordPosition="825">N-based model outperforms the FFNN-based model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks. For the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4. 2 Related Work Various word alignment models have been proposed. These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al. (1993), Vogel et al. (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al. (2005), Moore (2005), and Blunsom and Cohn (2006). 2.1 Generative Alignment Model Given a source language sentence fJ1 = f1, ..., fJ and a target language sentence eI1 = e1, ..., eI, fJ1 is generated by eI1 via the alignment aJ1 = a1,..., aJ. Each aj is a hidden variable indicating that the source word fj is aligned to the target word eaj. Usually, a “null” word e0 is added to the target language sentence and aJ1 may contain aj = 0, which indicates that fj is not aligned to any target word. The probability of generating the sentence fJ1 from eI1 is defined as �p(fJ1 |eI1) = p(fJ1 , aJ1 |eI1). (1) aJ</context>
</contexts>
<marker>Taskar, Lacoste-Julien, Klein, 2005</marker>
<rawString>Ben Taskar, Simon Lacoste-Julien, and Dan Klein. 2005. A Discriminative Matching Approach to Word Alignment. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 73–80.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the lonorm.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>311--319</pages>
<contexts>
<context position="17803" citStr="Vaswani et al., 2012" startWordPosition="2967" endWordPosition="2970">h length |e+|, and N denotes the number of pseudo-target language sentences per source sentence f+. Note that |e+ |= |e−|. GEN is a subset of all possible word alignments Φ, which is generated by beam search. In a simple implementation, each e− is generated by repeating a random sampling from a set of target words (Ve) |e+ |times and lining them up sequentially. To employ more discriminative negative samples, our implementation samples each word of e− from a set of the target words that cooccur with fz ∈ f+ whose probability is above a threshold C under the IBM Model 1 incorporating l0 prior (Vaswani et al., 2012). The IBM Model 1 with l0 prior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1. 4.2 Agreement Constraints Both of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e., they can represent one-to-many relations from the target side. Asymmetric models are usually trained in each alignment direction. The model proposed by Yang et al. (2013) is no exception. However, it has been demonstrated that encouraging directional models to agree improves alignment performa</context>
</contexts>
<marker>Vaswani, Huang, Chiang, 2012</marker>
<rawString>Ashish Vaswani, Liang Huang, and David Chiang. 2012. Smaller Alignment Models for Better Translations: Unsupervised Word Alignment with the lonorm. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 311–319.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with Large-Scale Neural Language Models Improves Translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1387--1392</pages>
<contexts>
<context position="7582" citStr="Vaswani et al., 2013" startWordPosition="1159" endWordPosition="1162">I1) can be found as aJ1 = argmax p(fJ1 , aJ1 |eI1). (3) aJ 1 For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and alignment probability in Eq. 2 are computed using FFNNs as J sNN(aJ1 |fJ1 , eI1) = H ta(aj − aj−1|c(eaj−1)) j=1 &apos;tlex(fj,eaj|c(fj),c(eaj)), (4) 1471 t (fj , eaj |f j-1 , e ) j+1 aj+1 lex aj-1 The computations in the hidden and output layer are as follows2: Output Layer O× +BO z1 z1 = f(H x z0 + BH), (5) tlex = O x z1 + BO, (6) Hidden </context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with Large-Scale Neural Language Models Improves Translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Vogel</author>
<author>Hermann Ney</author>
<author>Christoph Tillmann</author>
</authors>
<title>Hmm-based Word Alignment in Statistical Translation.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<contexts>
<context position="1489" citStr="Vogel et al., 1996" startWordPosition="197" endWordPosition="200">nsures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical machine translation. The most classical approaches are the probabilistic IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Various studies have extended those models. Yang et al. (2013) adapted the ContextDependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a type of feedforward neural network (FFNN)-based model, to * The first author is now affiliated with Knowledge Discovery Research Laboratories, NEC Corporation, Nara, Japan. the HMM alignment model and achieved state-ofthe-art performance. However, the FFNN-based model assumes a first-order Markov dependence for alignments. Recurrent neural network (RNN)-based models have recently demonstrated state-of-the-art performance that outperformed </context>
<context position="5497" citStr="Vogel et al. (1996)" startWordPosition="804" endWordPosition="807">d Japanese-to-English and Chinese-to-English translation tasks. The results illustrate that our RNN-based model outperforms the FFNN-based model (up to +0.0792 F1-measure) and the IBM Model 4 (up to +0.0703 F1-measure) for the word alignment tasks. For the translation tasks, our model achieves up to 0.74% gain in BLEU as compared to the FFNN-based model, which matches the translation qualities of the IBM Model 4. 2 Related Work Various word alignment models have been proposed. These models are roughly clustered into two groups: generative models, such as those proposed by Brown et al. (1993), Vogel et al. (1996), and Och and Ney (2003), and discriminative models, such as those proposed by Taskar et al. (2005), Moore (2005), and Blunsom and Cohn (2006). 2.1 Generative Alignment Model Given a source language sentence fJ1 = f1, ..., fJ and a target language sentence eI1 = e1, ..., eI, fJ1 is generated by eI1 via the alignment aJ1 = a1,..., aJ. Each aj is a hidden variable indicating that the source word fj is aligned to the target word eaj. Usually, a “null” word e0 is added to the target language sentence and aJ1 may contain aj = 0, which indicates that fj is not aligned to any target word. The probabi</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>Stephan Vogel, Hermann Ney, and Christoph Tillmann. 1996. Hmm-based Word Alignment in Statistical Translation. In Proceedings of the 16th International Conference on Computational Linguistics, pages 836–841.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Nenghai Yu</author>
</authors>
<title>Word Alignment Modeling with Context Dependent Deep Neural Network.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>166--175</pages>
<contexts>
<context position="1046" citStr="Yang et al., 2013" startWordPosition="133" endWordPosition="136">ted by recurrently connected hidden layers. We perform unsupervised learning using noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which utilizes artificially generated negative samples. Our alignment model is directional, similar to the generative IBM models (Brown et al., 1993). To overcome this limitation, we encourage agreement between the two directional models by introducing a penalty function that ensures word embedding consistency across two directional models during training. The RNN-based model outperforms the feed-forward neural network-based model (Yang et al., 2013) as well as the IBM Model 4 under Japanese-English and French-English word alignment tasks, and achieves comparable translation performance to those baselines for Japanese-English and Chinese-English translation tasks. 1 Introduction Automatic word alignment is an important task for statistical machine translation. The most classical approaches are the probabilistic IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996). Various studies have extended those models. Yang et al. (2013) adapted the ContextDependent Deep Neural Network for HMM (CDDNN-HMM) (Dahl et al., 2012), a </context>
<context position="2912" citStr="Yang et al. (2013)" startWordPosition="407" endWordPosition="410">nnections that propagates its own previous signals. Through the recurrent architecture, RNN-based models have the inherent property of modeling long-span dependencies, e.g., long contexts, in input data. We assume that this property would fit with a word alignment task, and we propose an RNN-based word alignment model. Our model can maintain and arbitrarily integrate an alignment history, e.g., bilingual context, which is longer than the FFNN-based model. The NN-based alignment models are supervised models. Unfortunately, it is usually difficult to prepare word-by-word aligned bilingual data. Yang et al. (2013) trained their model from word alignments produced by traditional unsupervised probabilistic models. However, with this approach, errors induced by probabilistic models are learned as correct alignments; thus, generalization capabilities are limited. To solve this problem, we apply noise-contrastive estimation (NCE) (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012) for unsupervised training of our RNN-based model without gold standard alignments or pseudo-oracle alignments. NCE artificially generates bilingual sentences through samplings as pseudo-negative samples, and then trains the model s</context>
<context position="7249" citStr="Yang et al., 2013" startWordPosition="1108" endWordPosition="1111">consider the fertility and distortion of each translated word. These models are trained using the expectationmaximization algorithm (Dempster et al., 1977) from bilingual sentences without word-level alignments (unlabeled training data). Given a specific model, the best alignment (Viterbi alignment) of the sentence pair (fJ1 , eI1) can be found as aJ1 = argmax p(fJ1 , aJ1 |eI1). (3) aJ 1 For example, the HMM model identifies the Viterbi alignment using the Viterbi algorithm. 2.2 FFNN-based Alignment Model As an instance of discriminative models, we describe an FFNN-based word alignment model (Yang et al., 2013), which is our baseline. An FFNN learns a hierarchy of nonlinear features that can automatically capture complex statistical patterns in input data. Recently, FFNNs have been applied successfully to several tasks, such as speech recognition (Dahl et al., 2012), statistical machine translation (Le et al., 2012; Vaswani et al., 2013), and other popular natural language processing tasks (Collobert and Weston, 2008; Collobert et al., 2011). Yang et al. (2013) have adapted a type of FFNN, i.e., CD-DNN-HMM (Dahl et al., 2012), to the HMM alignment model. Specifically, the lexical translation and ali</context>
<context position="10176" citStr="Yang et al. (2013)" startWordPosition="1617" endWordPosition="1620">ix1. Word embeddings are dense, low dimensional, and real-valued vectors that can capture syntactic and semantic properties of the words (Bengio et al., 2003). The concatenation (z0) is then fed to the hidden layer to capture nonlinear relations. Finally, the output layer receives the output of the hidden layer (z1) and computes a lexical translation score. 1We add a special token (unk) to handle unknown words and (null) to handle null alignments to Vf and Ve where H, BH, O, and BO are |z1 |x |z0|, |z1 |x 1, 1x|z1|, and 1x1 matrices, respectively, and f(x) is an activation function. Following Yang et al. (2013), a “hard” version of the hyperbolic tangent, htanh(x)3, is used as f(x) in this study. The alignment model based on an FFNN is formed in the same manner as the lexical translation model. Each model is optimized by minimizing the following ranking loss with a margin using stochastic gradient descent (SGD)4, where gradients are computed by the back-propagation algorithm (Rumelhart et al., 1986): loss(0) = � max{0,1 − sθ(a+|f, e) (f,e)ET +sθ(a−|f, e)}, (7) where 0 denotes the weights of layers in the model, T is a set of training data, a+ is the gold standard alignment, a− is the incorrect align</context>
<context position="18279" citStr="Yang et al. (2013)" startWordPosition="3044" endWordPosition="3047">t words that cooccur with fz ∈ f+ whose probability is above a threshold C under the IBM Model 1 incorporating l0 prior (Vaswani et al., 2012). The IBM Model 1 with l0 prior is convenient for reducing translation candidates because it generates more sparse alignments than the standard IBM Model 1. 4.2 Agreement Constraints Both of the FFNN-based and RNN-based models are based on the HMM alignment model, and they are therefore asymmetric, i.e., they can represent one-to-many relations from the target side. Asymmetric models are usually trained in each alignment direction. The model proposed by Yang et al. (2013) is no exception. However, it has been demonstrated that encouraging directional models to agree improves alignment performance (Matusov et al., 2004; Liang et al., 2006; Grac¸a et al., 2008; Ganchev et al., 2008). Inspired by their work, we introduce an agreement constraint to our learning. The constraint concretely enforces agreement in word embeddings of both directions. The proposed method trains two directional models concurrently based on the following objective by incorporating a penalty term that expresses the difference between word embeddings: argmin oFE {loss(θFE) + α∥θLEF − θLFE∥},</context>
<context position="23015" citStr="Yang et al. (2013)" startWordPosition="3837" endWordPosition="3840">omparing Methods We evaluated the proposed RNN-based alignment models against two baselines: the IBM Model 4 and the FFNN-based model with one hidden layer. The IBM Model 4 was trained by previously presented model sequence schemes (Och and Ney, 2003): 15H53545, i.e., five iterations of the IBM Model 1 followed by five iterations of the HMM Model, etc., which is the default setting for GIZA++ (IBM4). For the FFNN-based model, we set the word embedding length M to 30, the number of units of a hidden layer |z1 |to 100, and the window size of contexts to 5. Hence, |z0 |is 300 (30×5×2). Following Yang et al. (2013), the FFNN-based model was trained by the supervised approach described in Section 2.2 (FFNNs). For the RNN-based models, we set M to 30 and the number of units of each recurrent hidden layer |yj |to 100. Thus, |xj |is 60 (30 × 2). The number of units of each layer of the FFNNbased and RNN-based models and M were set through preliminary experiments. To demonstrate the effectiveness of the proposed learning methods, we evaluated four types of RNN-based models: RNNs, RNNs+c, RNNu, and RNNu+c, where “s/u” denotes a supervised/unsupervised model and “+c” indicates that the agreement constraint was</context>
<context position="29056" citStr="Yang et al., 2013" startWordPosition="4790" endWordPosition="4793"> model was trained from all the training data that was word-aligned by the alignment model. In addition, for a detailed comparison, we evaluated the SMT system where the IBM Model 4 was trained from all the training data (IBM4all). The significance test on translation performance was performed by the bootstrap method (Koehn, 2004) with a 5% significance level. “*” in Table 3 indicates that the comparisons are significant over both baselines, i.e., IBM4 and FFNNs(I). Table 3 also shows that better word alignment does not always result in better translation, which has been discussed previously (Yang et al., 2013). However, RNNu and RNNu+c outperform FFNNs(I) and IBM4 in all tasks. These results indicate that our proposals contribute to improving translation performance12. In addition, Table 3 shows that these proposed models are comparable to IBM4all in NTCIR and FBIS even though the proposed models are trained from only a small part of the training data. 6 Discussion 6.1 Effectiveness of RNN-based Alignment Model Figure 3 shows word alignment examples from FFNNs and RNNs, where solid squares indicate the gold standard alignments. Figure 3 (a) shows that RRNs adequately identifies complicated alignmen</context>
<context position="33444" citStr="Yang et al., 2013" startWordPosition="5506" endWordPosition="5509">s comparable in Hansards. These results indicate that the proposed unsupervised learning and agreement constraint benefit the FFNN-based model, similar to the RNN-based model. 7 Conclusion We have proposed a word alignment model based on an RNN, which captures long alignment history through recurrent architectures. Furthermore, we proposed an unsupervised method for training our model using NCE and introduced an agreement constraint that encourages word embeddings to be consistent across alignment directions. Our experiments have shown that the proposed model outperforms the FFNN-based model (Yang et al., 2013) for word alignment and machine translation, and that the agreement constraint improves alignment performance. In future, we plan to employ contexts composed of surrounding words (e.g., c(fj) or c(eaj) in the FFNN-based model) in our model, even though our model implicitly encodes such contexts in the alignment history. We also plan to enrich each hidden layer in our model with multiple layers following the success of Yang et al. (2013), in which multiple hidden layers improved the performance of the FFNN-based model. In addition, we would like to prove the effectiveness of the proposed method</context>
</contexts>
<marker>Yang, Liu, Li, Zhou, Yu, 2013</marker>
<rawString>Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu. 2013. Word Alignment Modeling with Context Dependent Deep Neural Network. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 166–175.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>