<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003333">
<title confidence="0.986591">
A Corpus of Fine-Grained Entailment Relations
</title>
<author confidence="0.981853">
Rodney D. Nielsen and Wayne Ward
</author>
<affiliation confidence="0.99759525">
Center for Spoken Language Research
Institute of Cognitive Science
Department of Computer Science
University of Colorado, Boulder
</affiliation>
<email confidence="0.996991">
Rodney.Nielsen, Wayne.Ward@Colorado.edu
</email>
<sectionHeader confidence="0.995592" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999973466666667">
This paper describes on-going efforts to an-
notate a corpus of almost 16000 answer
pairs with an estimated 69000 fine-grained
entailment relationships. We illustrate the
need for more detailed classification than
currently exists and describe our corpus
and annotation scheme. We discuss early
statistical analysis showing substantial in-
ter-annotator agreement even at the fine-
grained level. The corpus described here,
which is the only one providing such de-
tailed annotations, will be made available
as a public resource later this year (2007).
This is expected to enable application de-
velopment that is currently not practical.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999880705882353">
Determining whether the propositions in one text
fragment are entailed by those in another fragment
is important to numerous NLP applications. Con-
sider an intelligent tutoring system (ITS), where it
is critical for the tutor to assess which specific
facets of the desired or reference answer are en-
tailed by the student’s answer. Truly effective in-
teraction and pedagogy is only possible if the auto-
mated tutor can assess this entailment at a relative-
ly fine level of detail (c.f. Jordan et al., 2004).
The PASCAL Recognizing Textual Entailment
(RTE) challenge (Dagan et al., 2005) has brought
the issue of textual entailment before a broad com-
munity of researchers in a task independent fash-
ion. This task requires systems to make simple yes-
no judgments as to whether a human reading a text
t of one or more full sentences would typically
</bodyText>
<page confidence="0.977137">
28
</page>
<bodyText confidence="0.999957210526316">
consider a second, hypothesis, text h (usually one
full sentence) to most likely be true. This paper
discusses some of the extensions necessary to this
scheme in order to satisfy the requirements of an
ITS and provides a preliminary report on our ef-
forts to produce an annotated corpus applying
some of these additions to children’s answers to
science questions.
We first provide a brief overview of the RTE
challenge task and a synopsis of answer assess-
ment technology within existing ITSs and large
scale assessment applications. We then detail
some of the types of changes required in order to
facilitate more effective pedagogy. We provide a
report on our work in this direction and describe a
corpus we are annotating with fine-grained entail-
ment information. Finally, we discuss future direc-
tion and the relevance of this annotation scheme to
other applications such as question answering.
</bodyText>
<sectionHeader confidence="0.99879" genericHeader="introduction">
2 Prior Work
</sectionHeader>
<subsectionHeader confidence="0.980765">
2.1 RTE Challenge Task
</subsectionHeader>
<bodyText confidence="0.999353181818182">
Example 1 shows a typical t-h pair from the RTE
challenge. The task is to determine whether typi-
cally a reader would say that h is most likely true
having read t. The system output is a simple yes or
no decision about this entailment – in this example,
the decision is no – and that is similarly the extent
to which training data is annotated. There is no in-
dication of whether some facets of, the potentially
quite long, h are addressed (as they are in this case)
in t or conversely, which facets are not discussed
or are explicitly contradicted.
</bodyText>
<footnote confidence="0.518112">
(1) &lt;t&gt;At an international disas-
ter conference in Kobe, Japan, the
</footnote>
<note confidence="0.893904">
Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 28–35,
Prague, June 2007. @2007 Association for Computational Linguistics
</note>
<bodyText confidence="0.999309421052632">
U.N. humanitarian chief said the
United Nations should take the lead
in creating a tsunami early-warning
system in the Indian Ocean.&lt;/t&gt;
&lt;h&gt;Nations affected by the Asian
tsunami disaster have agreed the UN
should begin work on an early warning
system in the Indian Ocean.&lt;/h&gt;
However, in the third RTE challenge, there is an
optional pilot task1 that begins to address some of
these issues. Specifically, they have extended the
task by including an unknown label, where h is
neither entailed nor contradicted, and have request-
ed justification for decisions. The form that these
justifications will take has been left up to the
groups participating, but could conceivably pro-
vide some of the information about which specific
facets of the hypothesis are entailed, contradicted
and unaddressed.
</bodyText>
<subsectionHeader confidence="0.998954">
2.2 Existing Answer Assessment Technology
</subsectionHeader>
<bodyText confidence="0.999845448275862">
Effective ITSs exist in the laboratory producing
learning gains in high-school, college, and adult
subjects through text-based dialog interaction (e.g.,
Graesser et al., 2001; Koedinger et al., 1997; Peters
et al., 2004, VanLehn et al., 2005). However, most
ITSs today provide only a shallow assessment of
the learner’s comprehension (e.g., a correct versus
incorrect decision). Many ITS researchers are stri-
ving to provide more refined learner feedback
(Aleven et al., 2001; Graesser et al., 2001; Jordan
et al., 2004; Peters et al., 2004; Roll et al., 2005;
Rosé et al., 2003). However, they are developing
very domain-dependent approaches, requiring a
significant investment in hand-crafted logic repre-
sentations, parsers, knowledge-based ontologies,
and or dialog control mechanisms. Simply put,
these domain-dependent techniques will not scale
to the task of developing general purpose ITSs and
will never enable the long-term goal of effective
unconstrained interaction with learners or the peda-
gogy that requires it.
There is also a small, but growing, body of re-
search in the area of scoring free-text responses to
short answer questions (e.g., Callear et al., 2001;
Leacock, 2004; Mitchell et al., 2003; Pullman,
2005; Sukkarieh, 2005). Shaw (2004) and Whit-
tington (1999) provide reviews of some of these
approaches. Most of the systems that have been
implemented and tested are based on Information
</bodyText>
<footnote confidence="0.733436">
1 http://nlp.stanford.edu/RTE3-pilot/
</footnote>
<bodyText confidence="0.999657684210526">
Extraction (IE) techniques (Cowie &amp; Lehnert,
1996). They hand-craft a large number of pattern
rules, directed at detecting the propositions in com-
mon correct and incorrect answers. In general,
short-answer free-text response scoring systems
are designed for large scale assessment tasks, such
as those associated with the tests administered by
ETS. Therefore, they are not designed with the
goal of accommodating dynamically generated,
previously unseen questions. Similarly, these sys-
tems do not provide feedback regarding the specif-
ic aspects of answers that are correct or incorrect;
they merely provide a raw score for each question.
As with the related work directed specifically at
ITSs, these approaches all require in the range of
100-500 example student answers for each planned
test question to assist in the creation of IE patterns
or to train a machine learning algorithm used with-
in some component of their solution.
</bodyText>
<sectionHeader confidence="0.988812" genericHeader="method">
3 The Necessity of Finer-grained Analysis
</sectionHeader>
<bodyText confidence="0.999992620689655">
Imagine that you are an elementary school science
tutor and that rather than having access to the stu-
dent’s full response to your questions, you are sim-
ply given the information that their answer was
correct or incorrect, a yes or no entailment deci-
sion. Assuming the student’s answer was not cor-
rect, what question do you ask next? What follow
up question or action is most likely to lead to better
understanding on the part of the child? Clearly,
this is a far from ideal scenario, but it is roughly
the situation within which many ITSs exist today.
In order to optimize learning gains in the tutor-
ing environment, there are myriad issues the tutor
must understand regarding the semantics of the
student’s response. Here, we focus strictly on
drawing inferences regarding the student’s under-
standing of the low-level concepts and relation-
ships or facets of the reference answer. I use the
word facet throughout this paper to generically re-
fer to some part of a text’s meaning. The most
common type of answer facet discussed is the
meaning associated with a pair of related words
and the relation that connects them.
Rather than have a single yes or no entailment
decision for the reference answer as a whole, (i.e.,
does the student understand the reference answer
in its entirety or is there some unspecified part of it
that we are unsure whether the student under-
stands), we instead break the reference answer
</bodyText>
<page confidence="0.997589">
29
</page>
<bodyText confidence="0.999725368421053">
down into what we consider to be its lowest level
compositional facets. This roughly translates to
the set of triples composed of labeled dependencies
in a dependency parse of the reference answer.2
The following illustrates how a simple reference
answer (2) is decomposed into the answer facets
(2a-d) derived from its dependency parse and
(2a’-d’) provide a gloss of each facet’s meaning.
As can be seen in 2b and 2c, the dependencies are
augmented by thematic roles (Kipper et al., 2000)
(e.g., Agent, Theme, Cause, Instrument...) pro-
duced by a semantic role labeling system (c.f.,
Gildea and Jurafsky, 2002). The facets also in-
clude those semantic role relations that are not
derivable from a typical dependency tree. For ex-
ample, in the sentence “As it freezes the water will
expand and crack the glass”, water is not a modifi-
er of crack in the dependency tree, but it does play
the role of Agent in a shallow semantic parse.
</bodyText>
<listItem confidence="0.9730051">
(2) A long string produces a low pitch.
(2a) NMod(string, long)
(2b) Agent(produces, string)
(2c) Product(produces, pitch)
(2d) NMod(pitch, low)
(2a’) There is a long string.
(2b’) The string is producing some-
thing.
(2c’) A pitch is being produced.
(2d’) The pitch is low.
</listItem>
<bodyText confidence="0.999907125">
Breaking the reference answer down into low-
level facets provides the tutor’s dialog manager
with a much finer-grained assessment of the stu-
dent’s response, but a simple yes or no entailment
at the facet level still lacks semantic expressive-
ness with regard to the relation between the studen-
t’s answer and the facet in question. Did the stu-
dent contradict the facet? Did they express a relat-
ed concept that indicates a misconception? Did
they leave the facet unaddressed? Can you assume
that they understand the facet even though they did
not express it, since it was part of the information
given in the question? It is clear that, in addition to
2 The goal of most English dependency parsers is to pro-
duce a single projective tree structure for each sentence,
where each node represents a word in the sentence, each
link represents a functional category relation, usually la-
beled, between a governor (head) and a subordinate
(modifier), and each node has a single governor (c.f.,
Nivre and Scholz, 2004).
breaking the reference answer into fine-grained
facets, it is also necessary to break the annotation
into finer levels in order to specify more clearly the
relationship between the student’s answer and the
reference answer aspect.
There are many other issues that the system
must know to achieve near optimal tutoring, some
of which are mentioned later in the discussion sec-
tion, but these two – breaking the reference answer
into fine-grained facets and utilizing more expres-
sive annotation labels – are the emphasis of this ef-
fort.
</bodyText>
<sectionHeader confidence="0.97187" genericHeader="method">
4 Current Annotation Efforts
</sectionHeader>
<bodyText confidence="0.999904666666667">
This section describes our current efforts in anno-
tating a corpus of answers to science questions
from elementary school students.
</bodyText>
<subsectionHeader confidence="0.986436">
4.1 Corpus
</subsectionHeader>
<bodyText confidence="0.99592146875">
Lacking data from a real tutoring situation, we ac-
quired data gathered from 3rd-6th grade students in
schools utilizing the Full Option Science System
(FOSS). Assessment is a major FOSS research fo-
cus, of which the Assessing Science Knowledge
project is a key component.3 The FOSS project
has developed sixteen science teaching and learn-
ing modules targeted at grades 3-6, as shown in
Table 1. The ASK project created assessments for
each of these modules, including multiple choice,
fill in the blank, free response, and somewhat
lengthy experimental design questions. We re-
viewed these questions and selected about 290 free
response questions that were in line with the objec-
tives of this research project, specifically we se-
lected questions whose expected responses ranged
in length from moderately short verb phrases to a
few sentences, that could be assessed objectively,
and that were not too open ended. Table 2 shows a
3 “FOSS is a research-based science program for grades
K–8 developed at the Lawrence Hall of Science, Uni-
versity of California at Berkeley with support from the
National Science Foundation and published by Delta
Education. FOSS is also an ongoing research project
dedicated to improving the learning and teaching of sci-
ence.”
Assessing Science Knowledge (ASK) is “designed to
define, field test, and validate effective assessment tools
and techniques to be used by grade 3–6 classroom
teachers to assess, guide, and confirm student learning
in science.”
http://www.lawrencehallofscience.org/foss/
</bodyText>
<page confidence="0.994421">
30
</page>
<table confidence="0.930004923076924">
Grade Life Science Physical Science and Earth and Space Scientific Reasoning
Technology Science and Technology
3-4 HB: Human Body ME: Magnetism &amp; Electricity WA: Water II: Ideas &amp; Inventions
ST: Structure of Life PS: Physics of Sound EM: Earth Materials MS: Measurement
5-6 FN: Food &amp; Nutrition LP: Levers &amp; Pulleys SE: Solar Energy MD: Models &amp; Designs
EV: Environments MX: Mixtures &amp; Solutions LF: Landforms VB: Variables
1Table 1 FOSS / ASK Learning and Assessment Modules by Area and Grade
HB A: Dancers need to be able to point their feet. The tibialis is the major muscle on the front of the leg
and the gastrocnemius is the major muscle on the back of the leg. Describe how the muscles in the
front and back of the leg work together to make the dancer’s foot point.
The muscle in the back of the leg (the gastrocnemius) contracts and the muscle in the front of the
leg (the tibialis) relaxes to make the foot point.
The back muscle and the front muscle stretch to help each other pull up the foot.
ST A: Why is it important to have more than one shelter in a crayfish habitat with several crayfish?
Crayfish are territorial and will protect their territory. The shelters give them places to hide from
other crayfish. [Crayfish prefer the dark and the shelters provide darkness.]
So all the crayfish have room to hide and so they do not fight over them.
ME A: Lee has an object he wants to test to see if it is an insulator or a conductor. He is going to use the
circuit you see in the picture. Explain how he can use the circuit to test the object.
He should put one of the loose wires on one part of the object and the other loose wire on another
part of the object (and see if it completes the circuit).
You can touch one wire on one end and the other on the other side to see if it will run or not.
PS A: Kate said: “An object has to move to produce sound.” Do you agree with her? Why or why not?
Agree. Vibrations are movements and vibrations produce sound.
I agree with Kate because if you talk in a tube it produce sound in a long tone. And it vibrations
and make sound.
WA A: Anna spilled half of her cup of water on the kitchen floor. The other half was still in the cup. When
she came back hours later, all of the water on the floor had evaporated but most of the water in the
cup was still there. (Anna knew that no one had wiped up the water on the floor.) Explain to Anna
why the water on the floor had all evaporated but most of the water in the cup had not.
The water on the floor had a much larger surface area than the water in the cup.
Well Anna, in science, I learned that when water is in a more open are, then water evaporates faster.
So, since tile and floor don&apos;t have any boundaries or wall covering the outside, the water on the
floor evaporated faster, but since the water in the cup has boundaries, the water in the cup didn&apos;t
evaporate as fast.
EM A: You can tell if a rock contains calcite by putting it into a cold acid (like vinegar).
Describe what you would observe if you did the acid test on a rock that contains this substance.
Many tiny bubbles will rise from the calcite when it comes into contact with cold acid.
You would observe if it was fizzing because calcite has a strong reaction to vinegar.
</table>
<tableCaption confidence="0.99515">
Table 2 Sample Qs from FOSS-ASK with their reference (R) and an example student answer (A).
</tableCaption>
<bodyText confidence="0.798018">
few questions that are representative of those se-
lected for inclusion in the corpus, along with their
reference answers and an example student answer
for each. Questions without at least one verb
phrase were rejected because they were assumed to
be more trivial and less interesting from the re-
search perspective. Examples of such questions
along with their reference answers and an example
student response include: Q: Besides air, what (if
anything) can sound travel through? Reference
Answer: Sound can also travel through liquids and
solids. (Also other gases.) Student Answer: A
screen door. Q: Name a property of the sound of a
fire engine’s siren. Reference Answer: The sound
is very loud. OR The sound changes in pitch. Stu-
dent Answer: Annoying. An example of a free re-
sponse item that was dropped because it was too
open ended is: Design an investigation to find out
a plant’s range of tolerance for number of hours of
sunlight per day. You can use drawings to help ex-
plain your design.
We generated a corpus from a random sample of
the kids’ handwritten responses to these questions.
The only special transcription instructions were to
</bodyText>
<page confidence="0.999443">
31
</page>
<bodyText confidence="0.999987638888889">
fix spelling errors (since these would be irrelevant
in a spoken dialog environment), but not grammat-
ical errors (which would still be relevant), and to
skip blank answers and non-answers similar in na-
ture to I don’t know (since these are not particular-
ly interesting from the research perspective).
Three modules were designated as the test set
(Environments, Human Body, and Water) and the
remaining 13 modules will be used for develop-
ment and training of classification systems. We
judged the three test set modules to be representa-
tive of the entire corpus in terms of difficulty and
appropriateness for the types of questions that met
our research interests. We transcribed the respons-
es of approximately 40 randomly selected students
for each question in the training set and 100 ran-
domly selected students for each question in the
test set. In order to maximize the diversity of lan-
guage and knowledge represented by the training
and test datasets, random selection of students was
performed at the question level rather than using
the same students’ answers for all of the questions
in a given module. However, in total there were
only about 200 children that participated in any in-
dividual science module assessment, so there is
still moderate overlap in the students from one
question to another within a given module. On the
other hand, each assessment module was given to a
different group of kids, so there is no overlap in
students between modules. There are almost 60
questions and 5700 student answers in the test set,
comprising approximately 20% of all of the ques-
tions utilized and 36% of the total number of tran-
scribed student responses. In total, including test
and training datasets, there are nearly 16000 stu-
dent responses.
</bodyText>
<subsectionHeader confidence="0.986506">
4.2 Annotation
</subsectionHeader>
<bodyText confidence="0.968135242424242">
The answer assessment annotation described in this
paper is intended to be a step toward specifying the
detailed semantic understanding of a student’s an-
swer that is required for an ITS to interact effec-
tively with a learner. With that goal in mind, anno-
tators were asked to consider and annotate accord-
ing to what they would want to know about the stu-
dent’s answer if they were the tutor (but a tutor that
for some reason could not understand the unstruc-
tured text of the student’s answer). The key excep-
tion here is that we are only annotating a student’s
answer in terms of whether or not it accurately and
completely addresses the facets of the reference
(desired or correct) answer. So, if the student also
discusses concepts not addressed in the reference
answer, we will not annotate those points regard-
less of their quality or accuracy.
Each reference answer in the corpus is decom-
posed into its constituent facets. Then each student
answer is annotated relative to the facets in the cor-
responding reference answer. As described earlier,
the reference answer facets are roughly extracted
from the relations in a syntactic dependency parse
(c.f., Nivre and Scholz, 2004) and a shallow se-
mantic parse (Gildea and Jurafsky, 2002). These
are modified slightly to either eliminate most func-
tion words or incorporate them into the relation la-
bels (c.f., Lin and Pantel, 2001). Example 3 illus-
trates the decomposition of one of the reference an-
swers into its constituent parts along with their
glosses.
(3) The string is tighter, so the
pitch is higher.
</bodyText>
<equation confidence="0.904287166666667">
(3a) Is(string, tighter)
(3a’) The string is tighter.
(3b) Is(pitch, higher)
(3b’) The pitch is higher.
(3c) Cause(3b, 3a)
(3c’) 3b is caused by 3a
</equation>
<bodyText confidence="0.999973583333333">
The annotation tool lists the reference answer
facets that students are expected to address. Both a
formal relational representation and an English-
like gloss of the facet are displayed in a table, one
row per facet. The annotator’s job is to label each
of those facets to indicate the extent to which the
student addressed it. We settled on the eight anno-
tation labels noted in Table 3. Descriptions of
where each annotation label applies and some of
the most common annotation issues were detailed
with several examples in the guidelines and are
only very briefly summarized in the remainder of
this subsection.
Example 4 shows a student answer correspond-
ing to the reference answer in example 3, along
with its initial annotation in 4a-c and its final anno-
tation in 4a’-c’. It is assumed that the student un-
derstands that the pitch is higher (facet 4b), since
this is given in the question (... Write a note to
David to tell him why the pitch gets higher rather
than lower) and similarly it is assumed that the stu-
dent will be explaining what has the causal effect
of producing this higher pitch (facet 4c). There-
fore, these facets are initialized to Assumed by the
</bodyText>
<page confidence="0.998346">
32
</page>
<bodyText confidence="0.999915096774193">
system. Since the student does not contradict the
fact that the string is tighter (the string can be both
longer and tighter), we do not label this facet as
Contradicted. If the student’s response did not
mention anything about either the string or tight-
ness, we would annotate facet 4a as Unad-
dressed. However, the student did discuss a
property of the string, the string is long, producing
the facet Is(string, long). This parallels the
reference answer facet Is(string, tighter) with
the exception of a different argument to the Is re-
lation, resulting in the annotation Diff-Arg. This
indicates to the tutor that the student expressed a
related concept, but one which neither implies that
they understand the reference answer facet nor that
they explicitly hold a contradictory belief. Often,
this indicates that the student has a misconception.
For example, when asked about an effect on pitch,
many students say things like the pitch gets louder,
rather than higher or lower, which implies a mis-
conception involving their understanding of pitch
and volume. In this case, the Diff-Arg label can
help focus the tutor on correcting this misconcep-
tion. Facet 4c expressing the causal relation be-
tween 4a and 4b is labeled Expressed, since the
student did express a causal relation between the
concepts aligned with 4a and 4c. The tutor then
knows that the student was on track in regard to at-
tempting to express the desired causal relation and
the tutor need only deal with the fact that the cause
given was incorrect.
</bodyText>
<table confidence="0.999782625">
Expressed: Any facet directly expressed or inferred
by simple reasoning
Inferred: Facets inferred by pragmatics or nontrivial
logical reasoning
Contra-Expr: Facets directly contradicted by nega-
tion, antonymous expressions and their paraphrases
Contra-Infr: Facets contradicted by pragmatics or
complex reasoning
Self-Contra: Facets that are both contradicted and
implied (self contradictions)
Diff-Arg: The core relation is expressed, but it has a
different modifier or argument
Assumed: The system assigns this label, which is
changed if any of the above labels apply
Unaddressed: Facets that are not addressed at all by
the student’s answer
</table>
<tableCaption confidence="0.982371">
Table 3 Facet Annotation Labels
</tableCaption>
<listItem confidence="0.81104">
(4) David this is why because you
don&apos;t listen to your teacher. If the
string is long, the pitch will be
high.
(4a) Is(string, tighter), ---
(4b) Is(pitch, higher), Assumed
(4c) Cause(4b, 4a), Assumed
(4a’) Is(string, tighter), Diff-Arg
(4b’) Is(pitch, higher), Expressed
(4c’) Cause(4b, 4a), Expressed
</listItem>
<bodyText confidence="0.994831857142857">
The Self-Contra annotation is used in cases
like the response in example 5, where the student
simultaneously expresses the contradictory notions
that the string is tighter and that there is less ten-
sion.
(5) The string is tighter, so there is
less tension so the pitch gets higher.
</bodyText>
<equation confidence="0.547435">
(5a) Is(string, tighter), Self-Contra
</equation>
<bodyText confidence="0.99964865625">
There is no compelling reason from the perspec-
tive of the automated tutoring system to differenti-
ate between Expressed and Inferred facets,
since in either case the tutor can assume that the
student understands the concepts involved. How-
ever, from the systems development perspective
there are three primary reasons for differentiating
between these facets and similarly between facets
that are contradicted by inference versus more ex-
plicit expression. The first reason is that most sta-
tistical machine learning systems today cannot
hope to detect very many pragmatic inferences and
including these in the training data is likely to con-
fuse the algorithm resulting in worse performance.
Having separate labels allows one to remove the
more difficult inferences from the training data,
thus eliminating this problem. The second ratio-
nale is that systems hoping to handle both types of
inference might more easily learn to discriminate
between these opposing classifications if the class-
es are distinguished (for algorithms where this is
not the case, the classes can easily be combined au-
tomatically). Similarly, this allows the possibility
of training separate classifiers to handle the differ-
ent forms of inference. The third reason for sepa-
rate labels is that it facilitates system evaluation,
including the comparison of various techniques
and the effect of individual features.
Example 6 illustrates an example of a student
answer with the label Inferred. In this case, the
decision requires pragmatic inferences, applying
the Gricean maxims of Relation, be relevant – why
</bodyText>
<page confidence="0.998">
33
</page>
<bodyText confidence="0.9999585">
would the student mention vibrations if they did
not know they were a form of movement – and
Quantity, do not make your contribution more in-
formative than is required (Grice, 1975).
</bodyText>
<listItem confidence="0.984294875">
(6) Q: Kate said: “An object has to
move to produce sound.” Do you agree
with her? Why or why not?
Ref Ans: “Agree. Vibrations are move-
ments and vibrations produce sound.”
Student Answer: Yes because it has to
vibrate to make sounds.
(6b) Is(vibration, movement), Inferred
</listItem>
<bodyText confidence="0.997914166666667">
Annotators are primarily students of Education
and Linguistics and require moderate training on
the annotation task. The annotated reference an-
swers are stored in a stand-off markup in xml files,
including an annotated element for each reference
answer facet.
</bodyText>
<subsectionHeader confidence="0.960738">
4.3 Inter-Annotator Agreement Results
</subsectionHeader>
<bodyText confidence="0.999692571428571">
The results reported here are preliminary, based on
the first two annotators, and must be viewed under
the light that we have not yet completed annotator
training. We report results under three label
groupings: (1) All-Labels, where all labels are
left separate, (2) Tutor-Labels, where Ex-
pressed, Inferred and Assumed are combined
as are Contra-Expr and Contra-Infr, and (3)
Yes-No, which is a two-way division, Expressed,
Inferred and Assumed versus all other labels.
Agreement on Tutor-Labels indicates the
benefit to the tutor, since it is relatively unimpor-
tant to differentiate between the types of inference
required in determining that the student under-
stands a reference answer facet (or has contradict-
ed it). We evaluated mid-training inter-annotator
agreement on a random selection of 15 answers
from each of 14 Physics of Sound questions, total-
ing 210 answers and 915 total facet annotations.
Mid-training agreement on the Tutor-Labels is
87.4%, with a Kappa statistic of 0.717 correspond-
ing with substantial agreement (Cohen, 1960). In-
ter-annotator agreement at mid-training is 81.1%
on All-Labels and 90.1% on the binary Yes-No
decision. These also have Kappa statistics in the
range of substantial agreement.
The distribution of the 915 annotations is shown
in Table 4. It is somewhat surprising that this sci-
ence module had so few contradictions, just 2.7%
of all annotations, particularly given that many of
the questions seem more likely to draw contradic-
tions than unaddressed facets (e.g., many ask about
the effect on pitch and volume, typically eliciting
one of two possible responses). An analysis of the
inter-annotator confusion matrix indicates that the
most probable disagreement is between Inferred
and Unaddressed. The second most likely dis-
agreement is between Assumed and Expressed.
In discussing disagreements, the annotators almost
always agree quickly, reinforcing our belief that
we will increase agreement significantly with addi-
tional training.
</bodyText>
<table confidence="0.999704">
Label Count % Count %
Expressed 348 38.0 657 71.8
Inferred 51 5.6
Assumed 258 28.2
Contra-Expr 21 2.3 25 2.7
Contra-Infr 4 0.4
Self-Contra 1 0.1 1 0.1
Diff-Arg 33 3.6 33 3.6
Unaddressed 199 21.7 199 21.7
</table>
<tableCaption confidence="0.995523">
Table 4 Distribution of classifications (915 facets)
</tableCaption>
<sectionHeader confidence="0.994655" genericHeader="evaluation">
5 Discussion and Future Work
</sectionHeader>
<bodyText confidence="0.999990576923077">
The goal of our fine-grained classification is to en-
able more effective tutoring dialog management.
The additional labels facilitate understanding the
type of mismatch between the reference answer
and the student’s answer. Breaking the reference
answer down into low-level facets enables the tutor
to provide feedback relevant specifically to the ap-
propriate facet of the reference answer. In the
question answering domain, this facet-based classi-
fication would allow systems to accumulate entail-
ing evidence from a variety of corroborating
sources and incorporate answer details that might
not be found in any single sentence. In other appli-
cations outside of the tutoring domain, this fine-
grained classification can also facilitate more di-
rected user feedback. For example, both the addi-
tional classifications and the break down of facets
can be used to justify system decisions, which is
the stated goal of the pilot task at the third RTE
challenge.
The corpus described in this paper, which will
be released later this year (2007), represents a sub-
stantial contribution to the entailment community,
including an estimated 69000 facet entailment an-
notations. By contrast, three years of RTE chal-
lenge data comprise fewer than 4600 entailment
</bodyText>
<page confidence="0.99654">
34
</page>
<bodyText confidence="0.999971642857143">
annotations. More importantly, this is the only
corpus that provides entailment information at the
fine-grained level described in this paper. This
will enable application development that was not
practical previously.
Future work includes training machine learning
algorithms to perform the classifications described
in this paper. We also plan to annotate other as-
pects of the students’ understanding that are not di-
rect inferences of reference answer knowledge.
Consider example (4), in addition to the issues al-
ready annotated, the student contradicts a law of
physics that they have surely encountered else-
where in the text, specifically that longer strings
produce lower, not higher, pitches. Under the cur-
rent annotation scheme this is not annotated, since
it does not pertain directly to the reference answer
which has to do with the effect of string tension.
In other annotation plans, it would be very useful
for training learning algorithms if we provide an
indication of which student answer facets played a
role in making the inferences classified.
Initial inter-annotator agreement results look
promising, obtaining substantial agreement accord-
ing to the Kappa statistic. We will continue to re-
fine our annotation guidelines and provide further
training in order to push the agreement higher on
all classifications.
</bodyText>
<sectionHeader confidence="0.979916" genericHeader="conclusions">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.9999665">
We would like to thank Martha Palmer for valu-
able advice on this annotation effort.
</bodyText>
<sectionHeader confidence="0.999144" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999256545454546">
Aleven V, Popescu O, &amp; Koedinger K. (2001) A tutorial
dialogue system with knowledge-based understanding
and classification of student explanations. IJCAI WS
knowledge &amp; reasoning in practical dialogue systems
Callear, D., Jerrams-Smith, J., and Soh, V. (2001). CAA
of short non-MCQ answers. In 5th Intl CAA.
Cohen J. (1960). A coefficient of agreement for nominal
scales. Educational &amp; Psych Measurement. 20:37-46.
Cowie, J., Lehnert, W.G. (1996). Information Extrac-
tion. In Communications of the ACM, 39(1), 80-91.
Dagan, Ido, Glickman, Oren, and Magnini, Bernardo.
(2005). The PASCAL Recognizing Textual Entail-
ment Challenge. In 1st RTE Challenge Workshop.
Gildea, D. &amp; Jurafsky, D. (2002). Automatic labeling of
semantic roles. Computational Linguistics, 28:3, 245–288.
Graesser, A.C., Hu, X., Susarla, S., Harter, D., Person,
N.K., Louwerse, M., and Olde, B. (2001). AutoTutor:
An Intelligent Tutor and Conversational Tutoring
Scaffold. In 10th ICAI in Education, 47-49.
Grice, H. Paul. (1975). Logic and conversation. In P
Cole and J Morgan, editors, Syntax and Semantics,
Vol 3, Speech Acts, 43–58. Academic Press.
Jordan, P. W., Makatchev, M., &amp; VanLehn, K. (2004).
Combining competing language understanding ap-
proaches in an intelligent tutoring system. In 7th ITS.
Kipper, K, Dang, H, &amp; Palmer, M. (2000). Class-Based
Construction of a Verb Lexicon. AAAI 17th NCAI
Koedinger, K.R., Anderson, J.R., Hadley, W.H. &amp;
Mark, M.A. (1997). Intelligent tutoring goes to
school in the big city. Intl Jrnl of AI in Ed, 8, 30-43.
Leacock, Claudia. (2004). Scoring free-response auto-
matically: A case study of a large-scale Assessment.
Examens, 1(3).
Lin, Dekang and Pantel, Patrick. (2001). Discovery of
inference rules for Question Answering. In Natural
Language Engineering, 7(4):343-360.
Mitchell, T. Aldridge, N., and Broomhead, P. (2003).
Computerized marking of short-answer free-text re-
sponses. In 29th IAEA.
Nivre, J. and Scholz, M. (2004). Deterministic Depen-
dency Parsing of English Text. In Proc COLING.
Peters, S, Bratt, E.O., Clark, B., Pon-Barry, H. and
Schultz, K. (2004). Intelligent Systems for Training
Damage Control Assistants. In Proc. of ITSE.
Pulman S.G. &amp; Sukkarieh J.Z. (2005). Automatic Short
Answer Marking. ACL WS Bldg Ed Apps using NLP.
Roll, I, Baker, R, Aleven, V, McLaren, B, &amp; Koedinger,
K. (2005). Modeling Students’ Metacognitive Errors
in Two Intelligent Tutoring Systems. In UM 379–388
Rosé, P. Roque, A., Bhembe, D. &amp; VanLehn, K. (2003).
A hybrid text classification approach for analysis of
student essays. In Bldg Ed Apps using NLP
Shaw, Stuart. (2004). Automated writing assessment: a
review of four conceptual models. In Research Notes,
Cambridge ESOL. Downloaded Aug 10, 2005 from
http://www.cambridgeesol.org/rs_notes/rs_nts17.pdf
Sukkarieh, J. &amp; Pulman, S. (2005). Information extrac-
tion and machine learning: Auto-marking short free
text responses to science questions. In Proc of AIED.
VanLehn, K., Lynch, C., Schulze, K. Shapiro, J. A.,
Shelby, R., Taylor, L., Treacy, D., Weinstein, A., &amp;
Wintersgill, M. (2005). The Andes physics tutoring
system: Five years of evaluations. In 12th ICAI in Ed
Whittington, D., Hunt, H. (1999). Approaches to the
Computerised Assessment of Free-Text Responses.
Third ICAA.
</reference>
<page confidence="0.99932">
35
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.447920">
<title confidence="0.99985">A Corpus of Fine-Grained Entailment Relations</title>
<author confidence="0.999157">Rodney D Nielsen</author>
<author confidence="0.999157">Wayne</author>
<affiliation confidence="0.9923715">Center for Spoken Language Institute of Cognitive Department of Computer University of Colorado, Boulder</affiliation>
<email confidence="0.831321">Rodney.Nielsen,Wayne.Ward@Colorado.edu</email>
<abstract confidence="0.9462935625">This paper describes on-going efforts to annotate a corpus of almost 16000 answer pairs with an estimated 69000 fine-grained entailment relationships. We illustrate the need for more detailed classification than currently exists and describe our corpus and annotation scheme. We discuss early statistical analysis showing substantial inter-annotator agreement even at the finegrained level. The corpus described here, which is the only one providing such detailed annotations, will be made available as a public resource later this year (2007). This is expected to enable application development that is currently not practical.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>V Aleven</author>
<author>O Popescu</author>
<author>K Koedinger</author>
</authors>
<title>A tutorial dialogue system with knowledge-based understanding and classification of student explanations. IJCAI WS knowledge &amp; reasoning in practical dialogue systems</title>
<date>2001</date>
<contexts>
<context position="4749" citStr="Aleven et al., 2001" startWordPosition="754" endWordPosition="757"> information about which specific facets of the hypothesis are entailed, contradicted and unaddressed. 2.2 Existing Answer Assessment Technology Effective ITSs exist in the laboratory producing learning gains in high-school, college, and adult subjects through text-based dialog interaction (e.g., Graesser et al., 2001; Koedinger et al., 1997; Peters et al., 2004, VanLehn et al., 2005). However, most ITSs today provide only a shallow assessment of the learner’s comprehension (e.g., a correct versus incorrect decision). Many ITS researchers are striving to provide more refined learner feedback (Aleven et al., 2001; Graesser et al., 2001; Jordan et al., 2004; Peters et al., 2004; Roll et al., 2005; Rosé et al., 2003). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-dependent techniques will not scale to the task of developing general purpose ITSs and will never enable the long-term goal of effective unconstrained interaction with learners or the pedagogy that requires it. There is also a small, but growing, body of resear</context>
</contexts>
<marker>Aleven, Popescu, Koedinger, 2001</marker>
<rawString>Aleven V, Popescu O, &amp; Koedinger K. (2001) A tutorial dialogue system with knowledge-based understanding and classification of student explanations. IJCAI WS knowledge &amp; reasoning in practical dialogue systems</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Callear</author>
<author>J Jerrams-Smith</author>
<author>V Soh</author>
</authors>
<title>CAA of short non-MCQ answers.</title>
<date>2001</date>
<booktitle>In 5th Intl CAA.</booktitle>
<contexts>
<context position="5448" citStr="Callear et al., 2001" startWordPosition="861" endWordPosition="864"> 2005; Rosé et al., 2003). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-dependent techniques will not scale to the task of developing general purpose ITSs and will never enable the long-term goal of effective unconstrained interaction with learners or the pedagogy that requires it. There is also a small, but growing, body of research in the area of scoring free-text responses to short answer questions (e.g., Callear et al., 2001; Leacock, 2004; Mitchell et al., 2003; Pullman, 2005; Sukkarieh, 2005). Shaw (2004) and Whittington (1999) provide reviews of some of these approaches. Most of the systems that have been implemented and tested are based on Information 1 http://nlp.stanford.edu/RTE3-pilot/ Extraction (IE) techniques (Cowie &amp; Lehnert, 1996). They hand-craft a large number of pattern rules, directed at detecting the propositions in common correct and incorrect answers. In general, short-answer free-text response scoring systems are designed for large scale assessment tasks, such as those associated with the test</context>
</contexts>
<marker>Callear, Jerrams-Smith, Soh, 2001</marker>
<rawString>Callear, D., Jerrams-Smith, J., and Soh, V. (2001). CAA of short non-MCQ answers. In 5th Intl CAA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coefficient of agreement for nominal scales.</title>
<date>1960</date>
<journal>Educational &amp; Psych Measurement.</journal>
<pages>20--37</pages>
<contexts>
<context position="27872" citStr="Cohen, 1960" startWordPosition="4619" endWordPosition="4620"> and Assumed versus all other labels. Agreement on Tutor-Labels indicates the benefit to the tutor, since it is relatively unimportant to differentiate between the types of inference required in determining that the student understands a reference answer facet (or has contradicted it). We evaluated mid-training inter-annotator agreement on a random selection of 15 answers from each of 14 Physics of Sound questions, totaling 210 answers and 915 total facet annotations. Mid-training agreement on the Tutor-Labels is 87.4%, with a Kappa statistic of 0.717 corresponding with substantial agreement (Cohen, 1960). Inter-annotator agreement at mid-training is 81.1% on All-Labels and 90.1% on the binary Yes-No decision. These also have Kappa statistics in the range of substantial agreement. The distribution of the 915 annotations is shown in Table 4. It is somewhat surprising that this science module had so few contradictions, just 2.7% of all annotations, particularly given that many of the questions seem more likely to draw contradictions than unaddressed facets (e.g., many ask about the effect on pitch and volume, typically eliciting one of two possible responses). An analysis of the inter-annotator </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>Cohen J. (1960). A coefficient of agreement for nominal scales. Educational &amp; Psych Measurement. 20:37-46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cowie</author>
<author>W G Lehnert</author>
</authors>
<title>Information Extraction. In</title>
<date>1996</date>
<journal>Communications of the ACM,</journal>
<volume>39</volume>
<issue>1</issue>
<pages>80--91</pages>
<contexts>
<context position="5772" citStr="Cowie &amp; Lehnert, 1996" startWordPosition="907" endWordPosition="910">general purpose ITSs and will never enable the long-term goal of effective unconstrained interaction with learners or the pedagogy that requires it. There is also a small, but growing, body of research in the area of scoring free-text responses to short answer questions (e.g., Callear et al., 2001; Leacock, 2004; Mitchell et al., 2003; Pullman, 2005; Sukkarieh, 2005). Shaw (2004) and Whittington (1999) provide reviews of some of these approaches. Most of the systems that have been implemented and tested are based on Information 1 http://nlp.stanford.edu/RTE3-pilot/ Extraction (IE) techniques (Cowie &amp; Lehnert, 1996). They hand-craft a large number of pattern rules, directed at detecting the propositions in common correct and incorrect answers. In general, short-answer free-text response scoring systems are designed for large scale assessment tasks, such as those associated with the tests administered by ETS. Therefore, they are not designed with the goal of accommodating dynamically generated, previously unseen questions. Similarly, these systems do not provide feedback regarding the specific aspects of answers that are correct or incorrect; they merely provide a raw score for each question. As with the </context>
</contexts>
<marker>Cowie, Lehnert, 1996</marker>
<rawString>Cowie, J., Lehnert, W.G. (1996). Information Extraction. In Communications of the ACM, 39(1), 80-91.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognizing Textual Entailment Challenge.</title>
<date>2005</date>
<booktitle>In 1st RTE Challenge Workshop.</booktitle>
<contexts>
<context position="1483" citStr="Dagan et al., 2005" startWordPosition="220" endWordPosition="223">rrently not practical. 1 Introduction Determining whether the propositions in one text fragment are entailed by those in another fragment is important to numerous NLP applications. Consider an intelligent tutoring system (ITS), where it is critical for the tutor to assess which specific facets of the desired or reference answer are entailed by the student’s answer. Truly effective interaction and pedagogy is only possible if the automated tutor can assess this entailment at a relatively fine level of detail (c.f. Jordan et al., 2004). The PASCAL Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2005) has brought the issue of textual entailment before a broad community of researchers in a task independent fashion. This task requires systems to make simple yesno judgments as to whether a human reading a text t of one or more full sentences would typically 28 consider a second, hypothesis, text h (usually one full sentence) to most likely be true. This paper discusses some of the extensions necessary to this scheme in order to satisfy the requirements of an ITS and provides a preliminary report on our efforts to produce an annotated corpus applying some of these additions to children’s answe</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2005</marker>
<rawString>Dagan, Ido, Glickman, Oren, and Magnini, Bernardo. (2005). The PASCAL Recognizing Textual Entailment Challenge. In 1st RTE Challenge Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Gildea</author>
<author>D Jurafsky</author>
</authors>
<title>Automatic labeling of semantic roles.</title>
<date>2002</date>
<journal>Computational Linguistics,</journal>
<volume>28</volume>
<pages>245--288</pages>
<contexts>
<context position="8722" citStr="Gildea and Jurafsky, 2002" startWordPosition="1394" endWordPosition="1397">ence answer 29 down into what we consider to be its lowest level compositional facets. This roughly translates to the set of triples composed of labeled dependencies in a dependency parse of the reference answer.2 The following illustrates how a simple reference answer (2) is decomposed into the answer facets (2a-d) derived from its dependency parse and (2a’-d’) provide a gloss of each facet’s meaning. As can be seen in 2b and 2c, the dependencies are augmented by thematic roles (Kipper et al., 2000) (e.g., Agent, Theme, Cause, Instrument...) produced by a semantic role labeling system (c.f., Gildea and Jurafsky, 2002). The facets also include those semantic role relations that are not derivable from a typical dependency tree. For example, in the sentence “As it freezes the water will expand and crack the glass”, water is not a modifier of crack in the dependency tree, but it does play the role of Agent in a shallow semantic parse. (2) A long string produces a low pitch. (2a) NMod(string, long) (2b) Agent(produces, string) (2c) Product(produces, pitch) (2d) NMod(pitch, low) (2a’) There is a long string. (2b’) The string is producing something. (2c’) A pitch is being produced. (2d’) The pitch is low. Breakin</context>
<context position="20036" citStr="Gildea and Jurafsky, 2002" startWordPosition="3354" endWordPosition="3357">d completely addresses the facets of the reference (desired or correct) answer. So, if the student also discusses concepts not addressed in the reference answer, we will not annotate those points regardless of their quality or accuracy. Each reference answer in the corpus is decomposed into its constituent facets. Then each student answer is annotated relative to the facets in the corresponding reference answer. As described earlier, the reference answer facets are roughly extracted from the relations in a syntactic dependency parse (c.f., Nivre and Scholz, 2004) and a shallow semantic parse (Gildea and Jurafsky, 2002). These are modified slightly to either eliminate most function words or incorporate them into the relation labels (c.f., Lin and Pantel, 2001). Example 3 illustrates the decomposition of one of the reference answers into its constituent parts along with their glosses. (3) The string is tighter, so the pitch is higher. (3a) Is(string, tighter) (3a’) The string is tighter. (3b) Is(pitch, higher) (3b’) The pitch is higher. (3c) Cause(3b, 3a) (3c’) 3b is caused by 3a The annotation tool lists the reference answer facets that students are expected to address. Both a formal relational representatio</context>
</contexts>
<marker>Gildea, Jurafsky, 2002</marker>
<rawString>Gildea, D. &amp; Jurafsky, D. (2002). Automatic labeling of semantic roles. Computational Linguistics, 28:3, 245–288.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A C Graesser</author>
<author>X Hu</author>
<author>S Susarla</author>
<author>D Harter</author>
<author>N K Person</author>
<author>M Louwerse</author>
<author>B Olde</author>
</authors>
<title>AutoTutor: An Intelligent Tutor and Conversational Tutoring Scaffold.</title>
<date>2001</date>
<booktitle>In 10th ICAI in Education,</booktitle>
<pages>47--49</pages>
<contexts>
<context position="4449" citStr="Graesser et al., 2001" startWordPosition="707" endWordPosition="710">s. Specifically, they have extended the task by including an unknown label, where h is neither entailed nor contradicted, and have requested justification for decisions. The form that these justifications will take has been left up to the groups participating, but could conceivably provide some of the information about which specific facets of the hypothesis are entailed, contradicted and unaddressed. 2.2 Existing Answer Assessment Technology Effective ITSs exist in the laboratory producing learning gains in high-school, college, and adult subjects through text-based dialog interaction (e.g., Graesser et al., 2001; Koedinger et al., 1997; Peters et al., 2004, VanLehn et al., 2005). However, most ITSs today provide only a shallow assessment of the learner’s comprehension (e.g., a correct versus incorrect decision). Many ITS researchers are striving to provide more refined learner feedback (Aleven et al., 2001; Graesser et al., 2001; Jordan et al., 2004; Peters et al., 2004; Roll et al., 2005; Rosé et al., 2003). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog contro</context>
</contexts>
<marker>Graesser, Hu, Susarla, Harter, Person, Louwerse, Olde, 2001</marker>
<rawString>Graesser, A.C., Hu, X., Susarla, S., Harter, D., Person, N.K., Louwerse, M., and Olde, B. (2001). AutoTutor: An Intelligent Tutor and Conversational Tutoring Scaffold. In 10th ICAI in Education, 47-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Paul Grice</author>
</authors>
<title>Logic and conversation.</title>
<date>1975</date>
<booktitle>Syntax and Semantics, Vol 3, Speech Acts,</booktitle>
<pages>43--58</pages>
<editor>In P Cole and J Morgan, editors,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="26255" citStr="Grice, 1975" startWordPosition="4368" endWordPosition="4369">ing separate classifiers to handle the different forms of inference. The third reason for separate labels is that it facilitates system evaluation, including the comparison of various techniques and the effect of individual features. Example 6 illustrates an example of a student answer with the label Inferred. In this case, the decision requires pragmatic inferences, applying the Gricean maxims of Relation, be relevant – why 33 would the student mention vibrations if they did not know they were a form of movement – and Quantity, do not make your contribution more informative than is required (Grice, 1975). (6) Q: Kate said: “An object has to move to produce sound.” Do you agree with her? Why or why not? Ref Ans: “Agree. Vibrations are movements and vibrations produce sound.” Student Answer: Yes because it has to vibrate to make sounds. (6b) Is(vibration, movement), Inferred Annotators are primarily students of Education and Linguistics and require moderate training on the annotation task. The annotated reference answers are stored in a stand-off markup in xml files, including an annotated element for each reference answer facet. 4.3 Inter-Annotator Agreement Results The results reported here a</context>
</contexts>
<marker>Grice, 1975</marker>
<rawString>Grice, H. Paul. (1975). Logic and conversation. In P Cole and J Morgan, editors, Syntax and Semantics, Vol 3, Speech Acts, 43–58. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P W Jordan</author>
<author>M Makatchev</author>
<author>K VanLehn</author>
</authors>
<title>Combining competing language understanding approaches in an intelligent tutoring system.</title>
<date>2004</date>
<booktitle>In 7th ITS.</booktitle>
<contexts>
<context position="1403" citStr="Jordan et al., 2004" startWordPosition="209" endWordPosition="212">r this year (2007). This is expected to enable application development that is currently not practical. 1 Introduction Determining whether the propositions in one text fragment are entailed by those in another fragment is important to numerous NLP applications. Consider an intelligent tutoring system (ITS), where it is critical for the tutor to assess which specific facets of the desired or reference answer are entailed by the student’s answer. Truly effective interaction and pedagogy is only possible if the automated tutor can assess this entailment at a relatively fine level of detail (c.f. Jordan et al., 2004). The PASCAL Recognizing Textual Entailment (RTE) challenge (Dagan et al., 2005) has brought the issue of textual entailment before a broad community of researchers in a task independent fashion. This task requires systems to make simple yesno judgments as to whether a human reading a text t of one or more full sentences would typically 28 consider a second, hypothesis, text h (usually one full sentence) to most likely be true. This paper discusses some of the extensions necessary to this scheme in order to satisfy the requirements of an ITS and provides a preliminary report on our efforts to </context>
<context position="4793" citStr="Jordan et al., 2004" startWordPosition="762" endWordPosition="765">the hypothesis are entailed, contradicted and unaddressed. 2.2 Existing Answer Assessment Technology Effective ITSs exist in the laboratory producing learning gains in high-school, college, and adult subjects through text-based dialog interaction (e.g., Graesser et al., 2001; Koedinger et al., 1997; Peters et al., 2004, VanLehn et al., 2005). However, most ITSs today provide only a shallow assessment of the learner’s comprehension (e.g., a correct versus incorrect decision). Many ITS researchers are striving to provide more refined learner feedback (Aleven et al., 2001; Graesser et al., 2001; Jordan et al., 2004; Peters et al., 2004; Roll et al., 2005; Rosé et al., 2003). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-dependent techniques will not scale to the task of developing general purpose ITSs and will never enable the long-term goal of effective unconstrained interaction with learners or the pedagogy that requires it. There is also a small, but growing, body of research in the area of scoring free-text response</context>
</contexts>
<marker>Jordan, Makatchev, VanLehn, 2004</marker>
<rawString>Jordan, P. W., Makatchev, M., &amp; VanLehn, K. (2004). Combining competing language understanding approaches in an intelligent tutoring system. In 7th ITS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Kipper</author>
<author>H Dang</author>
<author>M Palmer</author>
</authors>
<title>Class-Based Construction of a Verb Lexicon.</title>
<date>2000</date>
<booktitle>AAAI 17th NCAI</booktitle>
<contexts>
<context position="8601" citStr="Kipper et al., 2000" startWordPosition="1376" endWordPosition="1379">s there some unspecified part of it that we are unsure whether the student understands), we instead break the reference answer 29 down into what we consider to be its lowest level compositional facets. This roughly translates to the set of triples composed of labeled dependencies in a dependency parse of the reference answer.2 The following illustrates how a simple reference answer (2) is decomposed into the answer facets (2a-d) derived from its dependency parse and (2a’-d’) provide a gloss of each facet’s meaning. As can be seen in 2b and 2c, the dependencies are augmented by thematic roles (Kipper et al., 2000) (e.g., Agent, Theme, Cause, Instrument...) produced by a semantic role labeling system (c.f., Gildea and Jurafsky, 2002). The facets also include those semantic role relations that are not derivable from a typical dependency tree. For example, in the sentence “As it freezes the water will expand and crack the glass”, water is not a modifier of crack in the dependency tree, but it does play the role of Agent in a shallow semantic parse. (2) A long string produces a low pitch. (2a) NMod(string, long) (2b) Agent(produces, string) (2c) Product(produces, pitch) (2d) NMod(pitch, low) (2a’) There is</context>
</contexts>
<marker>Kipper, Dang, Palmer, 2000</marker>
<rawString>Kipper, K, Dang, H, &amp; Palmer, M. (2000). Class-Based Construction of a Verb Lexicon. AAAI 17th NCAI</rawString>
</citation>
<citation valid="true">
<authors>
<author>K R Koedinger</author>
<author>J R Anderson</author>
<author>W H Hadley</author>
<author>M A Mark</author>
</authors>
<title>Intelligent tutoring goes to school in the big city.</title>
<date>1997</date>
<journal>Intl Jrnl of AI in Ed,</journal>
<volume>8</volume>
<pages>30--43</pages>
<contexts>
<context position="4473" citStr="Koedinger et al., 1997" startWordPosition="711" endWordPosition="714">ave extended the task by including an unknown label, where h is neither entailed nor contradicted, and have requested justification for decisions. The form that these justifications will take has been left up to the groups participating, but could conceivably provide some of the information about which specific facets of the hypothesis are entailed, contradicted and unaddressed. 2.2 Existing Answer Assessment Technology Effective ITSs exist in the laboratory producing learning gains in high-school, college, and adult subjects through text-based dialog interaction (e.g., Graesser et al., 2001; Koedinger et al., 1997; Peters et al., 2004, VanLehn et al., 2005). However, most ITSs today provide only a shallow assessment of the learner’s comprehension (e.g., a correct versus incorrect decision). Many ITS researchers are striving to provide more refined learner feedback (Aleven et al., 2001; Graesser et al., 2001; Jordan et al., 2004; Peters et al., 2004; Roll et al., 2005; Rosé et al., 2003). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put</context>
</contexts>
<marker>Koedinger, Anderson, Hadley, Mark, 1997</marker>
<rawString>Koedinger, K.R., Anderson, J.R., Hadley, W.H. &amp; Mark, M.A. (1997). Intelligent tutoring goes to school in the big city. Intl Jrnl of AI in Ed, 8, 30-43.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
</authors>
<title>Scoring free-response automatically: A case study of a large-scale Assessment.</title>
<date>2004</date>
<journal>Examens,</journal>
<volume>1</volume>
<issue>3</issue>
<contexts>
<context position="5463" citStr="Leacock, 2004" startWordPosition="865" endWordPosition="866">03). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-dependent techniques will not scale to the task of developing general purpose ITSs and will never enable the long-term goal of effective unconstrained interaction with learners or the pedagogy that requires it. There is also a small, but growing, body of research in the area of scoring free-text responses to short answer questions (e.g., Callear et al., 2001; Leacock, 2004; Mitchell et al., 2003; Pullman, 2005; Sukkarieh, 2005). Shaw (2004) and Whittington (1999) provide reviews of some of these approaches. Most of the systems that have been implemented and tested are based on Information 1 http://nlp.stanford.edu/RTE3-pilot/ Extraction (IE) techniques (Cowie &amp; Lehnert, 1996). They hand-craft a large number of pattern rules, directed at detecting the propositions in common correct and incorrect answers. In general, short-answer free-text response scoring systems are designed for large scale assessment tasks, such as those associated with the tests administered </context>
</contexts>
<marker>Leacock, 2004</marker>
<rawString>Leacock, Claudia. (2004). Scoring free-response automatically: A case study of a large-scale Assessment. Examens, 1(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of inference rules for Question Answering.</title>
<date>2001</date>
<booktitle>In Natural Language Engineering,</booktitle>
<pages>7--4</pages>
<contexts>
<context position="20179" citStr="Lin and Pantel, 2001" startWordPosition="3378" endWordPosition="3381">rence answer, we will not annotate those points regardless of their quality or accuracy. Each reference answer in the corpus is decomposed into its constituent facets. Then each student answer is annotated relative to the facets in the corresponding reference answer. As described earlier, the reference answer facets are roughly extracted from the relations in a syntactic dependency parse (c.f., Nivre and Scholz, 2004) and a shallow semantic parse (Gildea and Jurafsky, 2002). These are modified slightly to either eliminate most function words or incorporate them into the relation labels (c.f., Lin and Pantel, 2001). Example 3 illustrates the decomposition of one of the reference answers into its constituent parts along with their glosses. (3) The string is tighter, so the pitch is higher. (3a) Is(string, tighter) (3a’) The string is tighter. (3b) Is(pitch, higher) (3b’) The pitch is higher. (3c) Cause(3b, 3a) (3c’) 3b is caused by 3a The annotation tool lists the reference answer facets that students are expected to address. Both a formal relational representation and an Englishlike gloss of the facet are displayed in a table, one row per facet. The annotator’s job is to label each of those facets to in</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Lin, Dekang and Pantel, Patrick. (2001). Discovery of inference rules for Question Answering. In Natural Language Engineering, 7(4):343-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Aldridge Mitchell</author>
<author>N</author>
<author>P Broomhead</author>
</authors>
<title>Computerized marking of short-answer free-text responses.</title>
<date>2003</date>
<booktitle>In 29th IAEA.</booktitle>
<contexts>
<context position="5486" citStr="Mitchell et al., 2003" startWordPosition="867" endWordPosition="870">hey are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-dependent techniques will not scale to the task of developing general purpose ITSs and will never enable the long-term goal of effective unconstrained interaction with learners or the pedagogy that requires it. There is also a small, but growing, body of research in the area of scoring free-text responses to short answer questions (e.g., Callear et al., 2001; Leacock, 2004; Mitchell et al., 2003; Pullman, 2005; Sukkarieh, 2005). Shaw (2004) and Whittington (1999) provide reviews of some of these approaches. Most of the systems that have been implemented and tested are based on Information 1 http://nlp.stanford.edu/RTE3-pilot/ Extraction (IE) techniques (Cowie &amp; Lehnert, 1996). They hand-craft a large number of pattern rules, directed at detecting the propositions in common correct and incorrect answers. In general, short-answer free-text response scoring systems are designed for large scale assessment tasks, such as those associated with the tests administered by ETS. Therefore, they</context>
</contexts>
<marker>Mitchell, N, Broomhead, 2003</marker>
<rawString>Mitchell, T. Aldridge, N., and Broomhead, P. (2003). Computerized marking of short-answer free-text responses. In 29th IAEA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>M Scholz</author>
</authors>
<title>Deterministic Dependency Parsing of English Text.</title>
<date>2004</date>
<booktitle>In Proc COLING.</booktitle>
<contexts>
<context position="10320" citStr="Nivre and Scholz, 2004" startWordPosition="1669" endWordPosition="1672">s a related concept that indicates a misconception? Did they leave the facet unaddressed? Can you assume that they understand the facet even though they did not express it, since it was part of the information given in the question? It is clear that, in addition to 2 The goal of most English dependency parsers is to produce a single projective tree structure for each sentence, where each node represents a word in the sentence, each link represents a functional category relation, usually labeled, between a governor (head) and a subordinate (modifier), and each node has a single governor (c.f., Nivre and Scholz, 2004). breaking the reference answer into fine-grained facets, it is also necessary to break the annotation into finer levels in order to specify more clearly the relationship between the student’s answer and the reference answer aspect. There are many other issues that the system must know to achieve near optimal tutoring, some of which are mentioned later in the discussion section, but these two – breaking the reference answer into fine-grained facets and utilizing more expressive annotation labels – are the emphasis of this effort. 4 Current Annotation Efforts This section describes our current </context>
<context position="19979" citStr="Nivre and Scholz, 2004" startWordPosition="3344" endWordPosition="3347">t’s answer in terms of whether or not it accurately and completely addresses the facets of the reference (desired or correct) answer. So, if the student also discusses concepts not addressed in the reference answer, we will not annotate those points regardless of their quality or accuracy. Each reference answer in the corpus is decomposed into its constituent facets. Then each student answer is annotated relative to the facets in the corresponding reference answer. As described earlier, the reference answer facets are roughly extracted from the relations in a syntactic dependency parse (c.f., Nivre and Scholz, 2004) and a shallow semantic parse (Gildea and Jurafsky, 2002). These are modified slightly to either eliminate most function words or incorporate them into the relation labels (c.f., Lin and Pantel, 2001). Example 3 illustrates the decomposition of one of the reference answers into its constituent parts along with their glosses. (3) The string is tighter, so the pitch is higher. (3a) Is(string, tighter) (3a’) The string is tighter. (3b) Is(pitch, higher) (3b’) The pitch is higher. (3c) Cause(3b, 3a) (3c’) 3b is caused by 3a The annotation tool lists the reference answer facets that students are ex</context>
</contexts>
<marker>Nivre, Scholz, 2004</marker>
<rawString>Nivre, J. and Scholz, M. (2004). Deterministic Dependency Parsing of English Text. In Proc COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Peters</author>
<author>E O Bratt</author>
<author>B Clark</author>
<author>H Pon-Barry</author>
<author>K Schultz</author>
</authors>
<title>Intelligent Systems for Training Damage Control Assistants.</title>
<date>2004</date>
<booktitle>In Proc. of ITSE.</booktitle>
<contexts>
<context position="4494" citStr="Peters et al., 2004" startWordPosition="715" endWordPosition="718"> including an unknown label, where h is neither entailed nor contradicted, and have requested justification for decisions. The form that these justifications will take has been left up to the groups participating, but could conceivably provide some of the information about which specific facets of the hypothesis are entailed, contradicted and unaddressed. 2.2 Existing Answer Assessment Technology Effective ITSs exist in the laboratory producing learning gains in high-school, college, and adult subjects through text-based dialog interaction (e.g., Graesser et al., 2001; Koedinger et al., 1997; Peters et al., 2004, VanLehn et al., 2005). However, most ITSs today provide only a shallow assessment of the learner’s comprehension (e.g., a correct versus incorrect decision). Many ITS researchers are striving to provide more refined learner feedback (Aleven et al., 2001; Graesser et al., 2001; Jordan et al., 2004; Peters et al., 2004; Roll et al., 2005; Rosé et al., 2003). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-depend</context>
</contexts>
<marker>Peters, Bratt, Clark, Pon-Barry, Schultz, 2004</marker>
<rawString>Peters, S, Bratt, E.O., Clark, B., Pon-Barry, H. and Schultz, K. (2004). Intelligent Systems for Training Damage Control Assistants. In Proc. of ITSE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S G Pulman</author>
<author>J Z Sukkarieh</author>
</authors>
<title>Automatic Short Answer Marking. ACL WS Bldg Ed Apps using NLP.</title>
<date>2005</date>
<marker>Pulman, Sukkarieh, 2005</marker>
<rawString>Pulman S.G. &amp; Sukkarieh J.Z. (2005). Automatic Short Answer Marking. ACL WS Bldg Ed Apps using NLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Roll</author>
<author>R Baker</author>
<author>V Aleven</author>
<author>B McLaren</author>
<author>K Koedinger</author>
</authors>
<title>Modeling Students’ Metacognitive Errors in Two Intelligent Tutoring Systems.</title>
<date>2005</date>
<booktitle>In UM</booktitle>
<pages>379--388</pages>
<contexts>
<context position="4833" citStr="Roll et al., 2005" startWordPosition="770" endWordPosition="773">and unaddressed. 2.2 Existing Answer Assessment Technology Effective ITSs exist in the laboratory producing learning gains in high-school, college, and adult subjects through text-based dialog interaction (e.g., Graesser et al., 2001; Koedinger et al., 1997; Peters et al., 2004, VanLehn et al., 2005). However, most ITSs today provide only a shallow assessment of the learner’s comprehension (e.g., a correct versus incorrect decision). Many ITS researchers are striving to provide more refined learner feedback (Aleven et al., 2001; Graesser et al., 2001; Jordan et al., 2004; Peters et al., 2004; Roll et al., 2005; Rosé et al., 2003). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-dependent techniques will not scale to the task of developing general purpose ITSs and will never enable the long-term goal of effective unconstrained interaction with learners or the pedagogy that requires it. There is also a small, but growing, body of research in the area of scoring free-text responses to short answer questions (e.g., Calle</context>
</contexts>
<marker>Roll, Baker, Aleven, McLaren, Koedinger, 2005</marker>
<rawString>Roll, I, Baker, R, Aleven, V, McLaren, B, &amp; Koedinger, K. (2005). Modeling Students’ Metacognitive Errors in Two Intelligent Tutoring Systems. In UM 379–388</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Roque Rosé</author>
<author>A Bhembe</author>
<author>D</author>
<author>K VanLehn</author>
</authors>
<title>A hybrid text classification approach for analysis of student essays.</title>
<date>2003</date>
<booktitle>In Bldg Ed Apps using NLP</booktitle>
<contexts>
<context position="4853" citStr="Rosé et al., 2003" startWordPosition="774" endWordPosition="777">2 Existing Answer Assessment Technology Effective ITSs exist in the laboratory producing learning gains in high-school, college, and adult subjects through text-based dialog interaction (e.g., Graesser et al., 2001; Koedinger et al., 1997; Peters et al., 2004, VanLehn et al., 2005). However, most ITSs today provide only a shallow assessment of the learner’s comprehension (e.g., a correct versus incorrect decision). Many ITS researchers are striving to provide more refined learner feedback (Aleven et al., 2001; Graesser et al., 2001; Jordan et al., 2004; Peters et al., 2004; Roll et al., 2005; Rosé et al., 2003). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-dependent techniques will not scale to the task of developing general purpose ITSs and will never enable the long-term goal of effective unconstrained interaction with learners or the pedagogy that requires it. There is also a small, but growing, body of research in the area of scoring free-text responses to short answer questions (e.g., Callear et al., 2001; Lea</context>
</contexts>
<marker>Rosé, Bhembe, D, VanLehn, 2003</marker>
<rawString>Rosé, P. Roque, A., Bhembe, D. &amp; VanLehn, K. (2003). A hybrid text classification approach for analysis of student essays. In Bldg Ed Apps using NLP</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shaw</author>
</authors>
<title>Automated writing assessment: a review of four conceptual models.</title>
<date>2004</date>
<booktitle>In Research Notes, Cambridge ESOL. Downloaded</booktitle>
<note>from http://www.cambridgeesol.org/rs_notes/rs_nts17.pdf</note>
<contexts>
<context position="5532" citStr="Shaw (2004)" startWordPosition="875" endWordPosition="876">uiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-dependent techniques will not scale to the task of developing general purpose ITSs and will never enable the long-term goal of effective unconstrained interaction with learners or the pedagogy that requires it. There is also a small, but growing, body of research in the area of scoring free-text responses to short answer questions (e.g., Callear et al., 2001; Leacock, 2004; Mitchell et al., 2003; Pullman, 2005; Sukkarieh, 2005). Shaw (2004) and Whittington (1999) provide reviews of some of these approaches. Most of the systems that have been implemented and tested are based on Information 1 http://nlp.stanford.edu/RTE3-pilot/ Extraction (IE) techniques (Cowie &amp; Lehnert, 1996). They hand-craft a large number of pattern rules, directed at detecting the propositions in common correct and incorrect answers. In general, short-answer free-text response scoring systems are designed for large scale assessment tasks, such as those associated with the tests administered by ETS. Therefore, they are not designed with the goal of accommodati</context>
</contexts>
<marker>Shaw, 2004</marker>
<rawString>Shaw, Stuart. (2004). Automated writing assessment: a review of four conceptual models. In Research Notes, Cambridge ESOL. Downloaded Aug 10, 2005 from http://www.cambridgeesol.org/rs_notes/rs_nts17.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Sukkarieh</author>
<author>S Pulman</author>
</authors>
<title>Information extraction and machine learning: Auto-marking short free text responses to science questions.</title>
<date>2005</date>
<booktitle>In Proc of AIED.</booktitle>
<marker>Sukkarieh, Pulman, 2005</marker>
<rawString>Sukkarieh, J. &amp; Pulman, S. (2005). Information extraction and machine learning: Auto-marking short free text responses to science questions. In Proc of AIED.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K VanLehn</author>
<author>C Lynch</author>
<author>K Shapiro Schulze</author>
<author>J A</author>
<author>R Shelby</author>
<author>L Taylor</author>
<author>D Treacy</author>
<author>A Weinstein</author>
<author>M Wintersgill</author>
</authors>
<title>The Andes physics tutoring system: Five years of evaluations.</title>
<date>2005</date>
<booktitle>In 12th ICAI in Ed</booktitle>
<contexts>
<context position="4517" citStr="VanLehn et al., 2005" startWordPosition="719" endWordPosition="722"> label, where h is neither entailed nor contradicted, and have requested justification for decisions. The form that these justifications will take has been left up to the groups participating, but could conceivably provide some of the information about which specific facets of the hypothesis are entailed, contradicted and unaddressed. 2.2 Existing Answer Assessment Technology Effective ITSs exist in the laboratory producing learning gains in high-school, college, and adult subjects through text-based dialog interaction (e.g., Graesser et al., 2001; Koedinger et al., 1997; Peters et al., 2004, VanLehn et al., 2005). However, most ITSs today provide only a shallow assessment of the learner’s comprehension (e.g., a correct versus incorrect decision). Many ITS researchers are striving to provide more refined learner feedback (Aleven et al., 2001; Graesser et al., 2001; Jordan et al., 2004; Peters et al., 2004; Roll et al., 2005; Rosé et al., 2003). However, they are developing very domain-dependent approaches, requiring a significant investment in hand-crafted logic representations, parsers, knowledge-based ontologies, and or dialog control mechanisms. Simply put, these domain-dependent techniques will not</context>
</contexts>
<marker>VanLehn, Lynch, Schulze, A, Shelby, Taylor, Treacy, Weinstein, Wintersgill, 2005</marker>
<rawString>VanLehn, K., Lynch, C., Schulze, K. Shapiro, J. A., Shelby, R., Taylor, L., Treacy, D., Weinstein, A., &amp; Wintersgill, M. (2005). The Andes physics tutoring system: Five years of evaluations. In 12th ICAI in Ed</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Whittington</author>
<author>H Hunt</author>
</authors>
<title>Approaches to the Computerised Assessment of Free-Text Responses. Third ICAA.</title>
<date>1999</date>
<marker>Whittington, Hunt, 1999</marker>
<rawString>Whittington, D., Hunt, H. (1999). Approaches to the Computerised Assessment of Free-Text Responses. Third ICAA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>