<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003456">
<title confidence="0.995982">
Genre-Based Paragraph Classification for Sentiment Analysis
</title>
<author confidence="0.983896">
Maite Taboada Julian Brooke Manfred Stede
</author>
<affiliation confidence="0.927731666666667">
Department of Linguistics Department of Computer Science Institute of Linguistics
Simon Fraser University University of Toronto University of Potsdam
Burnaby, BC, Canada Toronto, ON, Canada Potsdam, Germany
</affiliation>
<email confidence="0.6549315">
mtaboada@sfu.ca jbrooke@cs.toronto.edu stede@ling.uni-
potsdam.de
</email>
<sectionHeader confidence="0.985206" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999843444444444">
We present a taxonomy and classification
system for distinguishing between differ-
ent types of paragraphs in movie reviews:
formal vs. functional paragraphs and,
within the latter, between description and
comment. The classification is used for
sentiment extraction, achieving im-
provement over a baseline without para-
graph classification.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999937158730159">
Much of the recent explosion in sentiment-
related research has focused on finding low-level
features that will help predict the polarity of a
phrase, sentence or text. Features, widely unders-
tood, may be individual words that tend to ex-
press sentiment, or other features that indicate
not only sentiment, but also polarity. The two
main approaches to sentiment extraction, the se-
mantic or lexicon-based, and the machine learn-
ing or corpus-based approach, both attempt to
identify low-level features that convey opinion.
In the semantic approach, the features are lists of
words and their prior polarity, (e.g., the adjective
terrible will have a negative polarity, and maybe
intensity, represented as -4; the noun masterpiece
may be a 5). Our approach is lexicon-based, but
we make use of information derived from ma-
chine learning classifiers.
Beyond the prior polarity of a word, its local
context obviously plays an important role in
conveying sentiment. Polanyi and Zaenen (2006)
use the term ‘contextual valence shifters’ to refer
to expressions in the local context that may
change a word’s polarity, such as intensifiers,
modal verbs, connectives, and of course negation.
Further beyond the local context, the overall
structure and organization of the text, influenced
by its genre, can help the reader determine how
the evaluation is expressed, and where it lies.
Polanyi and Zaenen (2006) also cite genre con-
straints as relevant factors in calculating senti-
ment.
Among the many definitions of genre, we take
the view of Systemic Functional Linguistics that
genres are purposeful activities that develop in
stages, or parts (Eggins and Martin, 1997), which
can be identified by lexicogrammatical proper-
ties (Eggins and Slade, 1997). Our proposal is
that, once we have identified different stages in a
text, the stages can be factored in the calculation
of sentiment, by weighing more heavily those
that are more likely to contain evaluation, an ap-
proach also pursued in automatic summarization
(Seki et al., 2006).
To test this hypothesis, we created a taxonomy
of stages specific to the genre of movie reviews,
and annotated a set of texts. We then trained
various classifiers to differentiate the stages.
Having identified the stages, we lowered the
weight of those that contained mostly description.
Our results show that we can achieve improve-
ment over a baseline when classifying the polar-
ity of texts, even with a classifier that can stand
to improve (at 71.1% accuracy). The best per-
formance comes from weights derived from the
output of a linear regression classifier.
We first describe our inventory of stages and
the manual annotation (Section 2), and in Sec-
tion 3 turn to automatic stage classification. After
describing our approach to sentiment classifica-
tion of texts in Section 4, we describe experi-
ments to improve its performance with the in-
formation on stages in Section 5. Section 6 dis-
</bodyText>
<note confidence="0.709897">
Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 62–70,
</note>
<affiliation confidence="0.662811">
Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics
</affiliation>
<page confidence="0.999295">
62
</page>
<bodyText confidence="0.965388">
cusses related work, and Section 7 provides con-
clusions.
</bodyText>
<sectionHeader confidence="0.77237" genericHeader="introduction">
2 Stages in movie reviews
</sectionHeader>
<bodyText confidence="0.999954315789474">
Within the larger review genre, we focus on
movie reviews. Movie reviews are particularly
difficult to classify (Turney, 2002), because large
portions of the review contain description of the
plot, the characters, actors, director, etc., or
background information about the film.
Our approach is based on the work of Bieler et
al. (2007), who identify formal and functional
zones (stages) within German movie reviews.
Formal zones are parts of the text that contribute
factual information about the cast and the credits,
and also about the review itself (author, date of
publication and the reviewer’s rating of the mov-
ie). Functional zones contain the main gist of the
review, and can be divided roughly into descrip-
tion and comment. Bieler et al. showed that func-
tional zones could be identified using 5-gram
SVM classifiers built from an annotated German
corpus.
</bodyText>
<subsectionHeader confidence="0.963755">
2.1 Taxonomy
</subsectionHeader>
<bodyText confidence="0.999996346153846">
In addition to the basic Describe/Comment dis-
tinction in Bieler et al., we use a De-
scribe+Comment label, as in our data it is often
the case that both description and comment are
present in the same paragraph. We decided that a
paragraph could be labeled as De-
scribe+Comment when it contained at least a
clause of each, and when the comment part could
be assigned a polarity (i.e., it was not only sub-
jective, but also clearly positive or negative).
Each of the three high-level tags has a subtag,
a feature also present in Bieler et al.’s manual
annotation. The five subtags are: overall, plot,
actors/characters, specific and general. ‘Specific’
refers to one particular aspect of the movie (not
plot or characters), whereas ‘general’ refers to
multiple topics in the same stage (special effects
and cinematography at the same time). Outside
the Comment/Describe scale, we also include
tags such as Background (discussion of other
movies or events outside the movie being
reviewed), Interpretation (subjective but not
opinionated or polar), and Quotes. Altogether,
the annotation system includes 40 tags, with 22
formal and 18 functional zones. Full lists of
zone/stage labels are provided in Appendix A.
</bodyText>
<subsectionHeader confidence="0.999493">
2.2 Manual annotation
</subsectionHeader>
<bodyText confidence="0.99998285">
We collected 100 texts from rottentomatoes.com,
trying to include one positive and one negative
review for the same movie. The reviews are part
of the “Top Critics” section of the site, all of
them published in newspapers or on-line maga-
zines. We restricted the texts to “Top Critics”
because we wanted well-structured, polished
texts, unlike those found in some on-line review
sites. Future work will address those more in-
formal reviews.
The 100 reviews contain 83,275 words and
1,542 paragraphs. The annotation was performed
at the paragraph level. Although stages may span
across paragraphs, and paragraphs may contain
more than one stage, there is a close relationship
between paragraphs and stages. The restriction
also resulted in a more reliable annotation, per-
formed with the PALinkA annotation tool (Ora-
san, 2003).
The annotation was performed by one of the
authors, and we carried out reliability tests with
two other annotators, one another one of the au-
thors, who helped develop the taxonomy, and the
third one a project member who read the annota-
tion guidelines1, and received a few hours’ train-
ing in the labels and software. We used Fleiss’
kappa (Fleiss, 1971), which extends easily to the
case of multiple raters (Di Eugenio and Glass,
2004). We all annotated four texts. The results of
the reliability tests show a reasonable agreement
level for the distinction between formal and
functional zones (.84 for the 3-rater kappa). The
lowest reliability was for the 3-way distinction in
the functional zones (.68 for the first two raters,
and .54 for the three raters). The full kappa val-
ues for all the distinctions are provided in Ap-
pendix B. After the reliability test, one of the
authors performed the full annotation for all 100
texts. Table 1 shows the breakdown of high-level
stages for the 100 texts.
</bodyText>
<table confidence="0.652931125">
Stage Count
Describe 347
Comment 237
Describe+Comment 237
Background 51
Interpretation 22
Quote 2
Formal 646
</table>
<tableCaption confidence="0.999202">
Table 1. Stages in 100 text RT corpus
</tableCaption>
<footnote confidence="0.9863375">
1Available from http://www.sfu.ca/~mtaboada/nserc-
project.html
</footnote>
<page confidence="0.999661">
63
</page>
<sectionHeader confidence="0.981712" genericHeader="method">
3 Classifying stages
</sectionHeader>
<bodyText confidence="0.999551666666667">
Our first classification task aims at distinguishing
the two main types of functional zones, Com-
ment and Describe, vs. Formal zones.
</bodyText>
<subsectionHeader confidence="0.977204">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.99992855">
We test two different sets of features. The first,
following Bieler et al. (2007), consists of 5-
grams (including unigrams, bigrams, 3-grams
and 4-grams), although we note in our case that
there was essentially no performance benefit
beyond 3-grams. We limited the size of our fea-
ture set to n-grams that appeared at least 4 times
in our training corpus. For the 2 class task (no
formal zones), this resulted in 8,092 binary fea-
tures, and for the 3 and 4 class task there were
9,357 binary n-gram features.
The second set of features captures different
aspects of genre and evaluation, and can in turn
be divided into four different types, according to
source. With two exceptions (features indicating
whether a paragraph was the first or last para-
graph in text), the features were numerical (fre-
quency) and normalized to the length of the pa-
ragraph.
The first group of genre features comes from
Biber (1988), who attempted to characterize di-
mensions of genre. The features here include fre-
quency of first, second and third person pro-
nouns; demonstrative pronouns; place and time
adverbials; intensifiers; and modals, among a
number of others.
The second category of genre features in-
cludes discourse markers, primarily from Knott
(1996), that indicate contrast, comparison, causa-
tion, evidence, condition, and similar relations.
The third type of genre features was a list of
500 adjectives classified in terms of Appraisal
(Martin and White, 2005) as indicating Apprec-
iation, Judgment or Affect. Appraisal categories
have been shown to be useful in improving the
performance of polarity classifiers (Whitelaw et
al., 2005).
Finally, we also include text statistics as fea-
tures, such as average length of words and sen-
tences and position of paragraphs in the text.
</bodyText>
<subsectionHeader confidence="0.997958">
3.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.999875769230769">
To classify paragraphs in the text, we use the
WEKA suite (Witten and Frank, 2005), testing
three popular machine learning algorithms:
Naïve Bayes, Support Vector Machine, and Li-
near Regression (preliminary testing with Deci-
sion Trees suggests that it is not appropriate for
this task). Training parameters were set to default
values.
In order to use Linear Regression, which pro-
vides a numerical output based on feature values
and derived feature weights, we have to conceive
of Comment/Describe/Describe+Comment not as
nominal (or ordinal) classes, but rather as corres-
ponding to a Comment/Describe ratio, with
“pure” Describe at one end and “pure” Comment
at the other. For training, we assign a 0 value (a
Comment ratio) to all paragraphs tagged De-
scribe and a 1 to all Comment paragraphs; for
Describe+Comment, various options (including
omission of this data) were tested. The time re-
quired to train a linear regression classifier on a
large feature set proved to be prohibitive, and
performance with smaller sets of features gener-
ally quite poor, so for the linear regression clas-
sifier we present results only for our compact set
of genre features.
</bodyText>
<subsectionHeader confidence="0.991208">
3.3 Performance
</subsectionHeader>
<bodyText confidence="0.999719903225807">
Table 2 shows the performance of classifi-
er/feature-set combinations for the 2-, 3-, and 4-
class tasks on the 100-text training set, with 10-
fold cross-validation, in terms of precision (P),
recall (R) and F-measure 2 . SVM and Naïve
Bayes provide comparable performance, al-
though there is considerable variation, particular-
ly with respect to the feature set; the SVM is a
significantly (p&lt;0.05) better choice for our genre
features 3 , while for the n-gram features the
Bayes classification is generally preferred. The
SVM-genre classifier significantly outperforms
the other classifiers in the 2-class task; these ge-
nre features, however, are not as useful as 5-
grams at identifying Formal zones (the n-gram
classifier, by contrast, can make use of words
such as cast). In general, formal zone classifica-
tion is fairly straightforward, whereas identifica-
tion of Describe+Comment is quite difficult, and
the SVM-genre classifier, which is more sensi-
tive to frequency bias, elects to (essentially) ig-
nore this category in order to boost overall accu-
racy.
To evaluate a linear regression (LR) classifier,
we calculate correlation coefficient ρ, which re-
flects the goodness of fit of the line to the da-
ta. Table 3 shows values for the classifiers built
from the corpus, with various Comment ratios
2 For the 2- and 3-way classifiers, Describe+Comment pa-
ragraphs are treated as Comment. This balances the num-
bers of each class, ultimately improving performance.
</bodyText>
<footnote confidence="0.700732">
3 All significance tests use chi-square (χ2).
</footnote>
<page confidence="0.996672">
64
</page>
<table confidence="0.999793533333333">
Classifier Comment Describe Formal Desc+Comm Overall
Accuracy
P R F P R F P R F P R F
2-class-5-gram-Bayes .66 .79 .72 .70 .55 .62 - - - - - - 68.0
2-class-5-gram-SVM .53 .63 .64 .68 .69 .69 - - - - - - 66.8
2-class-genre-Bayes .66 .75 .70 .67 .57 .61 - - - - - - 66.2
2-class-genre-SVM .71 .76 .74 .71 .65 .68 - - - - - - 71.1
3-class-5-gram-Bayes .69 .49 .57 .66 .78 .71 .92 .97 .95 - - - 78.1
3-class-5-gram-SVM .64 .63 .63 .68 .65 .65 .91 .97 .94 - - - 77.2
3-class-genre-Bayes .68 .68 .66 .67 .46 .55 .84 .96 .90 - - - 74.0
3-class-genre-SVM .66 .71 .68 .67 .56 .61 .90 .94 .92 - - - 76.8
4-class-5-gram-Bayes .46 .35 .38 .69 .47 .56 .92 .97 .95 .42 .64 .51 69.0
4-class-5-gram-SVM .43 .41 .44 .59 .62 .60 .91 .97 .94 .45 .41 .42 69.6
4-class-genre-Bayes .38 .31 .34 .66 .30 .41 .86 .97 .90 .33 .60 .42 62.3
4-class-genre-SVM .46 .32 .38 .53 .82 .65 .87 .94 .90 .26 .03 .06 67.4
</table>
<tableCaption confidence="0.999765">
Table 2. Stage identification performance of various categorical classifiers
</tableCaption>
<bodyText confidence="0.749405">
(C) assigned to paragraphs with the De-
scribe+Comment tag, and with De-
scribe+Comment paragraphs removed from con-
sideration.
</bodyText>
<table confidence="0.998901857142857">
Classifier ρ
LR, Des+Com C = 0 .37
LR, Des+Com C = 0.25 .44
LR, Des+Com C = 0.5 .47
LR, Des+Com C = 0.75 .46
LR, Des+Com C = 1 .43
LR, No Des+Com .50
</table>
<tableCaption confidence="0.9804625">
Table 3. Correlation coefficients for LR
classifiers
</tableCaption>
<bodyText confidence="0.999801785714286">
The drop in correlation when more extreme
values are assigned to Describe+Comment sug-
gests that Describe+Comment paragraphs do in-
deed belong in the middle of the Comment spec-
trum. Since there is a good deal of variation in
the amount of comment across De-
scribe+Comment paragraphs, the best correlation
comes with complete removal of these somewhat
unreliable paragraphs. Overall, these numbers
indicate that variations in relevant features are
able to predict roughly 50% of the variation in
Comment ratio, which is fairly good considering
the small number and simplistic nature of the
features involved.
</bodyText>
<sectionHeader confidence="0.980568" genericHeader="method">
4 Sentiment detection: SO-CAL
</sectionHeader>
<bodyText confidence="0.999980688888889">
In this section, we outline our semantic orienta-
tion calculator, SO-CAL. SO-CAL extracts
words from a text, and aggregates their semantic
orientation value, which is in turn extracted from
a set of dictionaries. SO-CAL uses five dictionar-
ies: four lexical dictionaries with 2,257 adjec-
tives, 1,142 nouns, 903 verbs, and 745 adverbs,
and a fifth dictionary containing 177 intensifying
expressions. Although the majority of the entries
are single words, the calculator also allows for
multiword entries written in regular expression-
like language.
The SO-carrying words in these dictionaries
were taken from a variety of sources, the three
largest a corpus of 400 reviews from Epin-
ions.com, first used by Taboada and Grieve
(2004), a 100 text subset of the 2,000 movie re-
views in the Polarity Dataset (Pang and Lee,
2004), and words from the General Inquirer dic-
tionary (Stone, 1997). Each of the open-class
words were given a hand-ranked SO value be-
tween 5 and -5 (neutral or zero-value words are
not included in the dictionary) by a native Eng-
lish speaker. The numerical values were chosen
to reflect both the prior polarity and strength of
the word, averaged across likely interpretations.
For example, the word phenomenal is a 5, nicely
a 2, disgust a -3, and monstrosity a -5. The dic-
tionary was later reviewed by a committee of
three other researchers in order to minimize the
subjectivity of ranking SO by hand.
Our calculator moves beyond simple averag-
ing of each word’s semantic orientation value,
and implements and expands on the insights of
Polanyi and Zaenen (2006) with respect to con-
textual valence shifters. We implement negation
by shifting the SO value of a word towards the
opposite polarity (not terrible, for instance, is
calculated as -5+4 = -1). Intensification is mod-
eled using percentage modifiers (very engaging:
4x125% = 5). We also ignore words appearing
within the scope of irrealis markers such as cer-
tain verbs, modals, and punctuation, and de-
crease the weight of words which appear often in
the text. In order to counter positive linguistic
</bodyText>
<page confidence="0.998229">
65
</page>
<bodyText confidence="0.9997698125">
bias (Boucher and Osgood, 1969), a problem for
lexicon-based sentiment classifiers (Kennedy and
Inkpen, 2006), we increase the final SO of any
negative expression appearing in the text.
The performance of SO-CAL tends to be in
the 76-81% range. We have tested on informal
movie, book and product reviews and on the Po-
larity Dataset (Pang and Lee, 2004). The perfor-
mance on movie reviews tends to be on the lower
end of the scale. Our baseline for movies, de-
scribed in Section 5, is 77.7%. We believe that
we have reached a ceiling in terms of word- and
phrase-level performance, and most future im-
provements need to come from discourse fea-
tures. The stage classification described in this
paper is one of them.
</bodyText>
<sectionHeader confidence="0.99996" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999954333333333">
The final goal of a stage classifier is to use the
information about different stages in sentiment
classification. Our assumption is that descriptive
paragraphs contain less evaluative content about
the movie being reviewed, and they may include
noise, such as evaluative words describing the
plot or the characters. Once the paragraph clas-
sifier had assigned labels we used those labels to
weigh paragraphs.
</bodyText>
<subsectionHeader confidence="0.996095">
5.1 Classification with manual tags
</subsectionHeader>
<bodyText confidence="0.999975142857143">
Before moving on to automatic paragraph classi-
fication, we used the 100 annotated texts to see
the general effect of weighting paragraphs with
the “perfect” human annotated tags on sentiment
detection, in order to show the potential im-
provements that can be gained from this ap-
proach.
Our baseline polarity detection performance
on the 100 annotated texts is 65%, which is very
low, even for movie reviews. We posit that for-
mal movie reviews might be particularly difficult
because full plot descriptions are more common
and the language used to express opinion less
straightforward (metaphors are common). How-
ever, if we lower the weight on non-Comment
and mixed Comment paragraphs (to 0, except for
Describe+Comment, which is maximized by a
0.1 weight), we are able to boost performance to
77%, an improvement which is significant at the
p&lt;0.05 level. Most of the improvement (7%) is
due to disregarding Describe paragraphs, but 2%
comes from Describe+Comment, and 1% each
from Background, Interpretation, and (all) For-
mal tags. There is no performance gain, however,
from the use of aspect tags (e.g., by increasing
the weight on Overall paragraphs), justifying our
decision to ignore subtags for text-level polarity
classification.
</bodyText>
<subsectionHeader confidence="0.998257">
5.2 Categorical classification
</subsectionHeader>
<bodyText confidence="0.9998612">
We evaluated all the classifiers from Table 2, but
we omit discussion of the worst performing. The
evaluation was performed on the Polarity Dataset
(Pang and Lee, 2004), a collection of 2,000 on-
line movie reviews, balanced for polarity. The
SO performance for the categorical classifiers is
given in Figure 1. When applicable, we always
gave Formal Zones (which Table 2 indicates are
fairly easy to identify) a weight of 0, however for
Describe paragraphs we tested at 0.1 intervals
between 0 and 1. Testing all possible values of
Describe+Comment was not feasible, so we set
the weights of those to a value halfway between
the weight of Comment paragraphs (1) and the
weight of the Describe paragraph.
Most of the classifiers were able to improve
performance beyond the 77.7% (unweighted)
baseline. The best performing model (the 2-
class-genre-SVM) reached a polarity identifica-
tion accuracy of 79.05%, while the second best
(the 3-class 5-gram-SVM) topped out at 78.9%.
Many of the classifiers showed a similar pattern
with respect to the weight on Describe, increas-
ing linearly as weight on Describe was decreased
before hitting a maximum in the 0.4-0.1 range,
and then dropping afterwards (often precipitous-
ly). Only the classifiers which were more con-
servative with respect to Describe, such as the 4-
class-5-gram-Bayes, avoided the drop, which can
be attributed to low precision Describe identifi-
cation: At some point, the cost associated with
disregarding paragraphs which have been mis-
tagged as Describe becomes greater that the ben-
efit of disregarding correctly-labeled ones. In-
deed, the best performing classifier for each class
option is exactly the one that has the highest pre-
cision for identification of Describe, regardless
of other factors. This suggests that improving
precision is key, and, in lieu of that, weighting is
a better strategy than simply removing parts of
the text.
In general, increasing the complexity of the
task (increasing the number of classes) decreases
performance. One clear problem is that the iden-
tification of Formal zones, which are much more
common in our training corpus than our test cor-
pus, does not add important information, since
most Formal zones have no SO valued words.
The delineation of an independent De-
scribe+Comment class is mostly ineffective,
</bodyText>
<page confidence="0.653827">
66
</page>
<figure confidence="0.998285722222222">
80
1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0
Weight on Describe Paragraph
SO Calculator Accuracy
79
78
77
No tagging Baseline
2-Class-5-gram-SVM
2-Class-5-gram-Bayes
2-Class-genre-Bayes
2-Class-genre-SVM
3-Class-5-gram-Bayes
3-Class-5-gram-SVM
3-Class-genre-Bayes
4-Class-5-gram-Bayes
4-Class-5-gram-SVM
4-Class-genre-Bayes
</figure>
<figureCaption confidence="0.99998">
Figure 1. SO Performance with various paragraph tagging classifiers, by weight on Describe
</figureCaption>
<bodyText confidence="0.996612">
probably because this class is not easily distin-
guishable from Describe and Comment (nor in
fact should it be).
We can further confirm that our classifier is
properly distinguishing Describe and Comment
by discounting Comment paragraphs rather than
Describe paragraphs (following Pang and Lee
2004). When Comment paragraphs tagged by the
best performing classifier are ignored, SO-CAL’s
accuracy drops to 56.65%, just barely above
chance.
</bodyText>
<subsectionHeader confidence="0.998131">
5.3 Continuous classification
</subsectionHeader>
<bodyText confidence="0.987700666666667">
Table 4 gives the results for the linear regression
classifier, which assigns a Comment ratio to each
paragraph used for weighting.
</bodyText>
<table confidence="0.997757571428571">
Model Accuracy
LR, Des+Com C = 0 78.75
LR, Des+Com C = 0.25 79.35
LR, Des+Com C = 0.5 79.00
LR, Des+Com C = 0.75 78.90
LR, Des+Com C = 1 78.95
LR, No Des+Com 79.05
</table>
<tableCaption confidence="0.999719">
Table 4. SO Performance with linear regression
</tableCaption>
<bodyText confidence="0.99996865625">
The linear regression model trained with a
0.25 comment ratio on Describe+Comment para-
graphs provides the best performance of all clas-
sifiers we tested (an improvement of 1.65% from
baseline). The correlation coefficients noted
in Table 4 are reflected in these results, but the
spike at C = 0.25 is most likely related to a gen-
eral preference for low (but non-zero) weights on
Describe+Comment paragraphs also noted when
weights were applied using the manual tags;
these paragraphs are unreliable (as compared to
pure Comment), but cannot be completely dis-
counted. There were some texts which had only
Describe+Comment paragraphs.
Almost a third of the tags assigned by the 2-
class genre feature classifier were different than
the corresponding n-gram classifier, suggesting
the two classifiers might have different strengths.
However, initial attempts to integrate the various
high performing classifiers—including collaps-
ing of feature sets, metaclassifiers, and double
tagging of paragraphs—resulted in similar or
worse performance. We have not tested all poss-
ible options (there are simply too many), but we
think it unlikely that additional gains will be
made with these simple, surface feature sets. Al-
though our testing with human annotated texts
and the large performance gap between movie
reviews and other consumer reviews both sug-
gest there is more potential for improvement, it
will probably require more sophisticated and
precise models.
</bodyText>
<sectionHeader confidence="0.99991" genericHeader="method">
6 Related work
</sectionHeader>
<bodyText confidence="0.999936">
The bulk of the work in sentiment analysis has
focused on classification at either the sentence
level, e.g., the subjectivity/polarity detection of
Wiebe and Riloff (2005), or alternatively at the
level of the entire text. With regards to the latter,
two major approaches have emerged: the use of
machine learning classifiers trained on n-grams
</bodyText>
<page confidence="0.998773">
67
</page>
<bodyText confidence="0.999900058823529">
or similar features (Pang et al., 2002), and the
use of sentiment dictionaries (Esuli and Sebas-
tiani, 2006; Taboada et al., 2006). Support Vec-
tor Machine (SVM) classifiers have been shown
to out-perform lexicon-based models within a
single domain (Kennedy and Inkpen, 2006);
however they have trouble with cross-domain
tasks (Aue and Gamon, 2005), and some re-
searchers have argued for hybrid classifiers (An-
dreevskaia and Bergler, 2008).
Pang and Lee (2004) attempted to improve the
performance of an SVM classifier by identifying
and removing objective sentences from the texts.
Results were mixed: The improvement was mi-
nimal for the SVM classifier (though the perfor-
mance of a naïve Bayes classifier was signifi-
cantly boosted), however testing with parts of the
text classified as subjective showed that the elim-
inated parts were indeed irrelevant. In contrast to
our findings, they reported a drop in performance
when paragraphs were taken as the only possible
boundary between subjective and objective text
spans.
Other research that has dealt with identifying
more or less relevant parts of the text for the pur-
poses of sentiment analysis include Taboada and
Grieve (2004), who improved the performance of
a lexicon-based model by weighing words to-
wards the end of the text; Nigam and Hurst
(2006), who detect polar expressions in topic
sentences; and Voll and Taboada (2007), who
used a topic classifier and discourse parser to
eliminate potentially off-topic or less important
sentences.
</bodyText>
<sectionHeader confidence="0.995619" genericHeader="method">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999992921568628">
We have described a genre-based taxonomy for
classifying paragraphs in movie reviews, with
the main classification being a distinction be-
tween formal and functional stages, and, within
those, between mainly descriptive vs. comment
stages. The taxonomy was used to annotate 100
movie reviews, as the basis for building classifi-
ers.
We tested a number of different classifiers.
Our results suggest that a simple, two-way or
continuous classification using a small set of lin-
guistically-motivated features is the best for our
purposes; a more complex system is feasible, but
comes at the cost of precision, which seems to be
the key variable in improving sentiment analysis.
Ultimately, the goal of the classification was
to improve the accuracy of SO-CAL, our seman-
tic orientation calculator. Using the manual an-
notations, we manage to boost performance by
12% over the baseline. With the best automatic
classifier, we still show consistent improvement
over the baseline. Given the relatively low accu-
racy of the classifiers, the crucial factor involves
using fine-grained weights on paragraphs, rather
than simply ignoring Describe-labeled para-
graphs, as Pang and Lee (2004) did for objective
sentences.
An obvious expansion to this work would in-
volve a larger dataset on which to train, to im-
prove the performance of the classifier(s). We
would also like to focus on the syntactic patterns
and verb class properties of narration, aspects
that are not captured with simply using words
and POS labels. Connectives in particular are
good indicators of the difference between narra-
tion (temporal connectives) and opinion (contras-
tive connectives). There may also be benefit to
combining paragraph- and sentence-based ap-
proaches. Finally, we would like to identify
common sequences of stages, such as plot and
character descriptions appearing together, and
before evaluation stages. This generic structure
has been extensively studied for many genres
(Eggins and Slade, 1997).
Beyond sentiment extraction, our taxonomy
and classifiers can be used for searching and in-
formation retrieval. One could, for instance, ex-
tract paragraphs that include mostly comment or
description. Using the more fine-grained labels,
searches for comment/description on actors, di-
rectors, or other aspects of the movie are possible.
</bodyText>
<sectionHeader confidence="0.995004" genericHeader="method">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.984903666666667">
This work was supported by SSHRC (410-2006-
1009) and NSERC (261104-2008) grants to
Maite Taboada.
</bodyText>
<sectionHeader confidence="0.998353" genericHeader="method">
References
</sectionHeader>
<reference confidence="0.994627384615385">
Andreevskaia, Alina &amp; Sabine Bergler. 2008. When
specialists and generalists work together: Domain
dependence in sentiment tagging. Proceedings of
46th Annual Meeting of the Association for Com-
putational Linguistics (pp. 290-298). Columbus,
OH.
Aue, Anthony &amp; Michael Gamon. 2005. Customizing
sentiment classifiers to new domains: A case study.
Proceedings of the International Conference on
Recent Advances in Natural Language Processing.
Borovets, Bulgaria.
Biber, Douglas. 1988. Variation across Speech and
Writing. Cambridge: Cambridge University Press.
</reference>
<page confidence="0.995325">
68
</page>
<reference confidence="0.999267825242719">
Bieler, Heike, Stefanie Dipper &amp; Manfred Stede.
2007. Identifying formal and functional zones in
film reviews. Proceedings of the 8th SIGdial
Workshop on Discourse and Dialogue (pp. 75-78).
Antwerp, Belgium.
Boucher, Jerry D. &amp; Charles E. Osgood. 1969. The
Pollyanna hypothesis. Journal of Verbal Learning
and Verbal Behaviour, 8: 1-8.
Di Eugenio, Barbara &amp; Michael Glass. 2004. The
kappa statistic: A second look. Computational Lin-
guistics, 30(1): 95-101.
Eggins, Suzanne &amp; James R. Martin. 1997. Genres
and registers of discourse. In Teun A. van Dijk
(ed.), Discourse as Structure and Process. Dis-
course Studies: A Multidisciplinary Introduction
(pp. 230-256). London: Sage.
Eggins, Suzanne &amp; Diana Slade. 1997. Analysing
Casual Conversation. London: Cassell.
Esuli, Andrea &amp; Fabrizio Sebastiani. 2006. Senti-
WordNet: A publicly available lexical resource for
opinion mining. Proceedings of 5th International
Conference on Language Resources and Evaluation
(LREC) (pp. 417-422). Genoa, Italy.
Fleiss, Joseph L. 1971. Measuring nominal scale
agreement among many raters. Psychological Bul-
letin, 76: 378-382.
Kennedy, Alistair &amp; Diana Inkpen. 2006. Sentiment
classification of movie and product reviews using
contextual valence shifters. Computational Intelli-
gence, 22(2): 110-125.
Knott, Alistair. 1996. A Data-Driven Methodology for
Motivating a Set of Coherence Relations. Edin-
burgh, UK: University of EdinburghThesis Type.
Martin, James R. &amp; Peter White. 2005. The Language
of Evaluation. New York: Palgrave.
Nigam, Kamal &amp; Matthew Hurst. 2006. Towards a
robust metric of polarity. In Janyce Wiebe (ed.),
Computing Attitude and Affect in Text: Theory
and Applications (pp. 265-279). Dordrecht: Sprin-
ger.
Orasan, Constantin. 2003. PALinkA: A highly custo-
mizable tool for discourse annotation. Proceedings
of 4th SIGdial Workshop on Discourse and Dialog
(pp. 39 – 43). Sapporo, Japan.
Pang, Bo &amp; Lillian Lee. 2004. A sentimental educa-
tion: Sentiment analysis using subjectivity summa-
rization based on minimum cuts. Proceedings of
42nd Meeting of the Association for Computation-
al Linguistics (pp. 271-278). Barcelona, Spain.
Pang, Bo, Lillian Lee &amp; Shivakumar Vaithyanathan.
2002. Thumbs up? Sentiment classification using
Machine Learning techniques. Proceedings of Con-
ference on Empirical Methods in NLP (pp. 79-86).
Polanyi, Livia &amp; Annie Zaenen. 2006. Contextual
valence shifters. In James G. Shanahan, Yan Qu &amp;
Janyce Wiebe (eds.), Computing Attitude and Af-
fect in Text: Theory and Applications (pp. 1-10).
Dordrecht: Springer.
Seki, Yohei, Koji Eguchi &amp; Noriko Kando. 2006.
Multi-document viewpoint summarization focused
on facts, opinion and knowledge. In Janyce Wiebe
(ed.), Computing Attitude and Affect in Text:
Theory and Applications (pp. 317-336). Dordrecht:
Springer.
Stone, Philip J. 1997. Thematic text analysis: New
agendas for analyzing text content. In Carl Roberts
(ed.), Text Analysis for the Social Sciences. Mah-
wah, NJ: Lawrence Erlbaum.
Taboada, Maite, Caroline Anthony &amp; Kimberly Voll.
2006. Creating semantic orientation dictionaries.
Proceedings of 5th International Conference on
Language Resources and Evaluation (LREC) (pp.
427-432). Genoa, Italy.
Taboada, Maite &amp; Jack Grieve. 2004. Analyzing ap-
praisal automatically. Proceedings of AAAI Spring
Symposium on Exploring Attitude and Affect in
Text (AAAI Technical Report SS-04-07) (pp. 158-
161). Stanford University, CA.
Turney, Peter. 2002. Thumbs up or thumbs down?
Semantic orientation applied to unsupervised clas-
sification of reviews. Proceedings of 40th Meeting
of the Association for Computational Linguistics
(pp. 417-424).
Voll, Kimberly &amp; Maite Taboada. 2007. Not all
words are created equal: Extracting semantic orien-
tation as a function of adjective relevance. Pro-
ceedings of the 20th Australian Joint Conference
on Artificial Intelligence (pp. 337-346). Gold
Coast, Australia.
Whitelaw, Casey, Navendu Garg &amp; Shlomo Arga-
mon. 2005. Using Appraisal groups for sentiment
analysis. Proceedings of ACM SIGIR Conference
on Information and Knowledge Management
(CIKM 2005) (pp. 625-631). Bremen, Germany.
Wiebe, Janyce &amp; Ellen Riloff. 2005. Creating subjec-
tive and objective sentence classifiers from unan-
notated texts. Proceedings of Sixth International
Conference on Intelligent Text Processing and
Computational Linguistics (CICLing-2005). Mex-
ico City, Mexico.
Witten, Ian H. &amp; Eibe Frank. 2005. Data Mining:
Practical Machine Learning Tools and Techniques
(2nd edn.). San Francisco: Morgan Kaufmann.
</reference>
<page confidence="0.998014">
69
</page>
<sectionHeader confidence="0.643385" genericHeader="method">
Appendix A: Full lists of formal and functional zones
</sectionHeader>
<figure confidence="0.999457">
Plot
Character
Specific
General
Content
Plot
Actors+characters
Specific
General
Overall
Plot
Actors+characters
Specific
General
Content
Describe
Comment
Tagline
Structure
Off-topic
Title, Title+year, Runtime,
Country+year, Director,
Genre, Audience-restriction,
Cast, Credits, Show-Loc+date,
Misc-Movie-Info
Source, Author, Author-Bio,
Place, Date, Legal-Notice,
Misc-Review-Info, Rating
</figure>
<figureCaption confidence="0.860956">
Figure A2. Formal zones
</figureCaption>
<figure confidence="0.9886194">
Structural
elements
Information
about the
film
</figure>
<figureCaption confidence="0.997552">
Figure A1. Functional zones
</figureCaption>
<table confidence="0.907140428571429">
Appendix B: Kappa values for annotation task
Classes 2-rater 3-rater
kappa kappa
Describe/Comment/Describe+Comment/Formal .82 .73
Describe/Comment/Formal .92 .84
Describe/Comment/Describe+Comment .68 .54
Describe/Comment .84 .69
</table>
<tableCaption confidence="0.992468">
Table B1. Kappa values for stage annotations
</tableCaption>
<page confidence="0.99677">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.515055">
<title confidence="0.996507">Genre-Based Paragraph Classification for Sentiment Analysis</title>
<author confidence="0.99625">Maite Julian Manfred</author>
<affiliation confidence="0.841536">Department of Department of Computer Institute of Simon Fraser University of University of</affiliation>
<address confidence="0.855781">Burnaby, BC, Canada Toronto, ON, Canada Potsdam, Germany</address>
<email confidence="0.995777">mtaboada@sfu.cajbrooke@cs.toronto.edupotsdam.de</email>
<abstract confidence="0.9697416">We present a taxonomy and classification system for distinguishing between different types of paragraphs in movie reviews: formal vs. functional paragraphs and, within the latter, between description and comment. The classification is used for sentiment extraction, achieving improvement over a baseline without paragraph classification.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alina Andreevskaia</author>
<author>Sabine Bergler</author>
</authors>
<title>When specialists and generalists work together: Domain dependence in sentiment tagging.</title>
<date>2008</date>
<booktitle>Proceedings of 46th Annual Meeting of the Association for Computational Linguistics</booktitle>
<pages>290--298</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="24944" citStr="Andreevskaia and Bergler, 2008" startWordPosition="4012" endWordPosition="4016"> and Riloff (2005), or alternatively at the level of the entire text. With regards to the latter, two major approaches have emerged: the use of machine learning classifiers trained on n-grams 67 or similar features (Pang et al., 2002), and the use of sentiment dictionaries (Esuli and Sebastiani, 2006; Taboada et al., 2006). Support Vector Machine (SVM) classifiers have been shown to out-perform lexicon-based models within a single domain (Kennedy and Inkpen, 2006); however they have trouble with cross-domain tasks (Aue and Gamon, 2005), and some researchers have argued for hybrid classifiers (Andreevskaia and Bergler, 2008). Pang and Lee (2004) attempted to improve the performance of an SVM classifier by identifying and removing objective sentences from the texts. Results were mixed: The improvement was minimal for the SVM classifier (though the performance of a naïve Bayes classifier was significantly boosted), however testing with parts of the text classified as subjective showed that the eliminated parts were indeed irrelevant. In contrast to our findings, they reported a drop in performance when paragraphs were taken as the only possible boundary between subjective and objective text spans. Other research th</context>
</contexts>
<marker>Andreevskaia, Bergler, 2008</marker>
<rawString>Andreevskaia, Alina &amp; Sabine Bergler. 2008. When specialists and generalists work together: Domain dependence in sentiment tagging. Proceedings of 46th Annual Meeting of the Association for Computational Linguistics (pp. 290-298). Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anthony Aue</author>
<author>Michael Gamon</author>
</authors>
<title>Customizing sentiment classifiers to new domains: A case study.</title>
<date>2005</date>
<booktitle>Proceedings of the International Conference on Recent Advances in Natural Language Processing. Borovets,</booktitle>
<contexts>
<context position="24854" citStr="Aue and Gamon, 2005" startWordPosition="3999" endWordPosition="4002">t either the sentence level, e.g., the subjectivity/polarity detection of Wiebe and Riloff (2005), or alternatively at the level of the entire text. With regards to the latter, two major approaches have emerged: the use of machine learning classifiers trained on n-grams 67 or similar features (Pang et al., 2002), and the use of sentiment dictionaries (Esuli and Sebastiani, 2006; Taboada et al., 2006). Support Vector Machine (SVM) classifiers have been shown to out-perform lexicon-based models within a single domain (Kennedy and Inkpen, 2006); however they have trouble with cross-domain tasks (Aue and Gamon, 2005), and some researchers have argued for hybrid classifiers (Andreevskaia and Bergler, 2008). Pang and Lee (2004) attempted to improve the performance of an SVM classifier by identifying and removing objective sentences from the texts. Results were mixed: The improvement was minimal for the SVM classifier (though the performance of a naïve Bayes classifier was significantly boosted), however testing with parts of the text classified as subjective showed that the eliminated parts were indeed irrelevant. In contrast to our findings, they reported a drop in performance when paragraphs were taken as</context>
</contexts>
<marker>Aue, Gamon, 2005</marker>
<rawString>Aue, Anthony &amp; Michael Gamon. 2005. Customizing sentiment classifiers to new domains: A case study. Proceedings of the International Conference on Recent Advances in Natural Language Processing. Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Douglas Biber</author>
</authors>
<title>Variation across Speech and Writing. Cambridge:</title>
<date>1988</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="9172" citStr="Biber (1988)" startWordPosition="1459" endWordPosition="1460">ams that appeared at least 4 times in our training corpus. For the 2 class task (no formal zones), this resulted in 8,092 binary features, and for the 3 and 4 class task there were 9,357 binary n-gram features. The second set of features captures different aspects of genre and evaluation, and can in turn be divided into four different types, according to source. With two exceptions (features indicating whether a paragraph was the first or last paragraph in text), the features were numerical (frequency) and normalized to the length of the paragraph. The first group of genre features comes from Biber (1988), who attempted to characterize dimensions of genre. The features here include frequency of first, second and third person pronouns; demonstrative pronouns; place and time adverbials; intensifiers; and modals, among a number of others. The second category of genre features includes discourse markers, primarily from Knott (1996), that indicate contrast, comparison, causation, evidence, condition, and similar relations. The third type of genre features was a list of 500 adjectives classified in terms of Appraisal (Martin and White, 2005) as indicating Appreciation, Judgment or Affect. Appraisal </context>
</contexts>
<marker>Biber, 1988</marker>
<rawString>Biber, Douglas. 1988. Variation across Speech and Writing. Cambridge: Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heike Bieler</author>
<author>Stefanie Dipper</author>
<author>Manfred Stede</author>
</authors>
<title>Identifying formal and functional zones in film reviews.</title>
<date>2007</date>
<booktitle>Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue</booktitle>
<pages>75--78</pages>
<location>Antwerp, Belgium.</location>
<contexts>
<context position="4290" citStr="Bieler et al. (2007)" startWordPosition="657" endWordPosition="660">009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 62–70, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 62 cusses related work, and Section 7 provides conclusions. 2 Stages in movie reviews Within the larger review genre, we focus on movie reviews. Movie reviews are particularly difficult to classify (Turney, 2002), because large portions of the review contain description of the plot, the characters, actors, director, etc., or background information about the film. Our approach is based on the work of Bieler et al. (2007), who identify formal and functional zones (stages) within German movie reviews. Formal zones are parts of the text that contribute factual information about the cast and the credits, and also about the review itself (author, date of publication and the reviewer’s rating of the movie). Functional zones contain the main gist of the review, and can be divided roughly into description and comment. Bieler et al. showed that functional zones could be identified using 5-gram SVM classifiers built from an annotated German corpus. 2.1 Taxonomy In addition to the basic Describe/Comment distinction in B</context>
<context position="8344" citStr="Bieler et al. (2007)" startWordPosition="1315" endWordPosition="1318">r the reliability test, one of the authors performed the full annotation for all 100 texts. Table 1 shows the breakdown of high-level stages for the 100 texts. Stage Count Describe 347 Comment 237 Describe+Comment 237 Background 51 Interpretation 22 Quote 2 Formal 646 Table 1. Stages in 100 text RT corpus 1Available from http://www.sfu.ca/~mtaboada/nsercproject.html 63 3 Classifying stages Our first classification task aims at distinguishing the two main types of functional zones, Comment and Describe, vs. Formal zones. 3.1 Features We test two different sets of features. The first, following Bieler et al. (2007), consists of 5- grams (including unigrams, bigrams, 3-grams and 4-grams), although we note in our case that there was essentially no performance benefit beyond 3-grams. We limited the size of our feature set to n-grams that appeared at least 4 times in our training corpus. For the 2 class task (no formal zones), this resulted in 8,092 binary features, and for the 3 and 4 class task there were 9,357 binary n-gram features. The second set of features captures different aspects of genre and evaluation, and can in turn be divided into four different types, according to source. With two exceptions</context>
</contexts>
<marker>Bieler, Dipper, Stede, 2007</marker>
<rawString>Bieler, Heike, Stefanie Dipper &amp; Manfred Stede. 2007. Identifying formal and functional zones in film reviews. Proceedings of the 8th SIGdial Workshop on Discourse and Dialogue (pp. 75-78). Antwerp, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jerry D Boucher</author>
<author>Charles E Osgood</author>
</authors>
<title>The Pollyanna hypothesis.</title>
<date>1969</date>
<journal>Journal of Verbal Learning and Verbal Behaviour,</journal>
<volume>8</volume>
<pages>1--8</pages>
<contexts>
<context position="16756" citStr="Boucher and Osgood, 1969" startWordPosition="2725" endWordPosition="2728">antic orientation value, and implements and expands on the insights of Polanyi and Zaenen (2006) with respect to contextual valence shifters. We implement negation by shifting the SO value of a word towards the opposite polarity (not terrible, for instance, is calculated as -5+4 = -1). Intensification is modeled using percentage modifiers (very engaging: 4x125% = 5). We also ignore words appearing within the scope of irrealis markers such as certain verbs, modals, and punctuation, and decrease the weight of words which appear often in the text. In order to counter positive linguistic 65 bias (Boucher and Osgood, 1969), a problem for lexicon-based sentiment classifiers (Kennedy and Inkpen, 2006), we increase the final SO of any negative expression appearing in the text. The performance of SO-CAL tends to be in the 76-81% range. We have tested on informal movie, book and product reviews and on the Polarity Dataset (Pang and Lee, 2004). The performance on movie reviews tends to be on the lower end of the scale. Our baseline for movies, described in Section 5, is 77.7%. We believe that we have reached a ceiling in terms of word- and phrase-level performance, and most future improvements need to come from disco</context>
</contexts>
<marker>Boucher, Osgood, 1969</marker>
<rawString>Boucher, Jerry D. &amp; Charles E. Osgood. 1969. The Pollyanna hypothesis. Journal of Verbal Learning and Verbal Behaviour, 8: 1-8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Barbara Di Eugenio</author>
<author>Michael Glass</author>
</authors>
<title>The kappa statistic: A second look.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<volume>30</volume>
<issue>1</issue>
<pages>95--101</pages>
<marker>Di Eugenio, Glass, 2004</marker>
<rawString>Di Eugenio, Barbara &amp; Michael Glass. 2004. The kappa statistic: A second look. Computational Linguistics, 30(1): 95-101.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Eggins</author>
<author>James R Martin</author>
</authors>
<title>Genres and registers of discourse.</title>
<date>1997</date>
<booktitle>Discourse as Structure and Process. Discourse Studies: A Multidisciplinary Introduction</booktitle>
<pages>230--256</pages>
<editor>In Teun A. van Dijk (ed.),</editor>
<publisher>Sage.</publisher>
<location>London:</location>
<contexts>
<context position="2390" citStr="Eggins and Martin, 1997" startWordPosition="350" endWordPosition="353">er to expressions in the local context that may change a word’s polarity, such as intensifiers, modal verbs, connectives, and of course negation. Further beyond the local context, the overall structure and organization of the text, influenced by its genre, can help the reader determine how the evaluation is expressed, and where it lies. Polanyi and Zaenen (2006) also cite genre constraints as relevant factors in calculating sentiment. Among the many definitions of genre, we take the view of Systemic Functional Linguistics that genres are purposeful activities that develop in stages, or parts (Eggins and Martin, 1997), which can be identified by lexicogrammatical properties (Eggins and Slade, 1997). Our proposal is that, once we have identified different stages in a text, the stages can be factored in the calculation of sentiment, by weighing more heavily those that are more likely to contain evaluation, an approach also pursued in automatic summarization (Seki et al., 2006). To test this hypothesis, we created a taxonomy of stages specific to the genre of movie reviews, and annotated a set of texts. We then trained various classifiers to differentiate the stages. Having identified the stages, we lowered t</context>
</contexts>
<marker>Eggins, Martin, 1997</marker>
<rawString>Eggins, Suzanne &amp; James R. Martin. 1997. Genres and registers of discourse. In Teun A. van Dijk (ed.), Discourse as Structure and Process. Discourse Studies: A Multidisciplinary Introduction (pp. 230-256). London: Sage.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Suzanne Eggins</author>
<author>Diana Slade</author>
</authors>
<title>Analysing Casual Conversation.</title>
<date>1997</date>
<publisher>Cassell.</publisher>
<location>London:</location>
<contexts>
<context position="2472" citStr="Eggins and Slade, 1997" startWordPosition="362" endWordPosition="365">ntensifiers, modal verbs, connectives, and of course negation. Further beyond the local context, the overall structure and organization of the text, influenced by its genre, can help the reader determine how the evaluation is expressed, and where it lies. Polanyi and Zaenen (2006) also cite genre constraints as relevant factors in calculating sentiment. Among the many definitions of genre, we take the view of Systemic Functional Linguistics that genres are purposeful activities that develop in stages, or parts (Eggins and Martin, 1997), which can be identified by lexicogrammatical properties (Eggins and Slade, 1997). Our proposal is that, once we have identified different stages in a text, the stages can be factored in the calculation of sentiment, by weighing more heavily those that are more likely to contain evaluation, an approach also pursued in automatic summarization (Seki et al., 2006). To test this hypothesis, we created a taxonomy of stages specific to the genre of movie reviews, and annotated a set of texts. We then trained various classifiers to differentiate the stages. Having identified the stages, we lowered the weight of those that contained mostly description. Our results show that we can</context>
</contexts>
<marker>Eggins, Slade, 1997</marker>
<rawString>Eggins, Suzanne &amp; Diana Slade. 1997. Analysing Casual Conversation. London: Cassell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Esuli</author>
<author>Fabrizio Sebastiani</author>
</authors>
<title>SentiWordNet: A publicly available lexical resource for opinion mining.</title>
<date>2006</date>
<booktitle>Proceedings of 5th International Conference on Language Resources and Evaluation (LREC)</booktitle>
<pages>417--422</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="24614" citStr="Esuli and Sebastiani, 2006" startWordPosition="3962" endWordPosition="3966">vie reviews and other consumer reviews both suggest there is more potential for improvement, it will probably require more sophisticated and precise models. 6 Related work The bulk of the work in sentiment analysis has focused on classification at either the sentence level, e.g., the subjectivity/polarity detection of Wiebe and Riloff (2005), or alternatively at the level of the entire text. With regards to the latter, two major approaches have emerged: the use of machine learning classifiers trained on n-grams 67 or similar features (Pang et al., 2002), and the use of sentiment dictionaries (Esuli and Sebastiani, 2006; Taboada et al., 2006). Support Vector Machine (SVM) classifiers have been shown to out-perform lexicon-based models within a single domain (Kennedy and Inkpen, 2006); however they have trouble with cross-domain tasks (Aue and Gamon, 2005), and some researchers have argued for hybrid classifiers (Andreevskaia and Bergler, 2008). Pang and Lee (2004) attempted to improve the performance of an SVM classifier by identifying and removing objective sentences from the texts. Results were mixed: The improvement was minimal for the SVM classifier (though the performance of a naïve Bayes classifier was</context>
</contexts>
<marker>Esuli, Sebastiani, 2006</marker>
<rawString>Esuli, Andrea &amp; Fabrizio Sebastiani. 2006. SentiWordNet: A publicly available lexical resource for opinion mining. Proceedings of 5th International Conference on Language Resources and Evaluation (LREC) (pp. 417-422). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph L Fleiss</author>
</authors>
<title>Measuring nominal scale agreement among many raters.</title>
<date>1971</date>
<journal>Psychological Bulletin,</journal>
<volume>76</volume>
<pages>378--382</pages>
<contexts>
<context position="7237" citStr="Fleiss, 1971" startWordPosition="1138" endWordPosition="1139">ges may span across paragraphs, and paragraphs may contain more than one stage, there is a close relationship between paragraphs and stages. The restriction also resulted in a more reliable annotation, performed with the PALinkA annotation tool (Orasan, 2003). The annotation was performed by one of the authors, and we carried out reliability tests with two other annotators, one another one of the authors, who helped develop the taxonomy, and the third one a project member who read the annotation guidelines1, and received a few hours’ training in the labels and software. We used Fleiss’ kappa (Fleiss, 1971), which extends easily to the case of multiple raters (Di Eugenio and Glass, 2004). We all annotated four texts. The results of the reliability tests show a reasonable agreement level for the distinction between formal and functional zones (.84 for the 3-rater kappa). The lowest reliability was for the 3-way distinction in the functional zones (.68 for the first two raters, and .54 for the three raters). The full kappa values for all the distinctions are provided in Appendix B. After the reliability test, one of the authors performed the full annotation for all 100 texts. Table 1 shows the bre</context>
</contexts>
<marker>Fleiss, 1971</marker>
<rawString>Fleiss, Joseph L. 1971. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76: 378-382.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Kennedy</author>
<author>Diana Inkpen</author>
</authors>
<title>Sentiment classification of movie and product reviews using contextual valence shifters.</title>
<date>2006</date>
<journal>Computational Intelligence,</journal>
<volume>22</volume>
<issue>2</issue>
<pages>110--125</pages>
<contexts>
<context position="16834" citStr="Kennedy and Inkpen, 2006" startWordPosition="2735" endWordPosition="2738"> and Zaenen (2006) with respect to contextual valence shifters. We implement negation by shifting the SO value of a word towards the opposite polarity (not terrible, for instance, is calculated as -5+4 = -1). Intensification is modeled using percentage modifiers (very engaging: 4x125% = 5). We also ignore words appearing within the scope of irrealis markers such as certain verbs, modals, and punctuation, and decrease the weight of words which appear often in the text. In order to counter positive linguistic 65 bias (Boucher and Osgood, 1969), a problem for lexicon-based sentiment classifiers (Kennedy and Inkpen, 2006), we increase the final SO of any negative expression appearing in the text. The performance of SO-CAL tends to be in the 76-81% range. We have tested on informal movie, book and product reviews and on the Polarity Dataset (Pang and Lee, 2004). The performance on movie reviews tends to be on the lower end of the scale. Our baseline for movies, described in Section 5, is 77.7%. We believe that we have reached a ceiling in terms of word- and phrase-level performance, and most future improvements need to come from discourse features. The stage classification described in this paper is one of them</context>
<context position="24781" citStr="Kennedy and Inkpen, 2006" startWordPosition="3988" endWordPosition="3991">ork The bulk of the work in sentiment analysis has focused on classification at either the sentence level, e.g., the subjectivity/polarity detection of Wiebe and Riloff (2005), or alternatively at the level of the entire text. With regards to the latter, two major approaches have emerged: the use of machine learning classifiers trained on n-grams 67 or similar features (Pang et al., 2002), and the use of sentiment dictionaries (Esuli and Sebastiani, 2006; Taboada et al., 2006). Support Vector Machine (SVM) classifiers have been shown to out-perform lexicon-based models within a single domain (Kennedy and Inkpen, 2006); however they have trouble with cross-domain tasks (Aue and Gamon, 2005), and some researchers have argued for hybrid classifiers (Andreevskaia and Bergler, 2008). Pang and Lee (2004) attempted to improve the performance of an SVM classifier by identifying and removing objective sentences from the texts. Results were mixed: The improvement was minimal for the SVM classifier (though the performance of a naïve Bayes classifier was significantly boosted), however testing with parts of the text classified as subjective showed that the eliminated parts were indeed irrelevant. In contrast to our fi</context>
</contexts>
<marker>Kennedy, Inkpen, 2006</marker>
<rawString>Kennedy, Alistair &amp; Diana Inkpen. 2006. Sentiment classification of movie and product reviews using contextual valence shifters. Computational Intelligence, 22(2): 110-125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alistair Knott</author>
</authors>
<title>A Data-Driven Methodology for Motivating a Set of Coherence Relations.</title>
<date>1996</date>
<institution>University of EdinburghThesis Type.</institution>
<location>Edinburgh, UK:</location>
<contexts>
<context position="9501" citStr="Knott (1996)" startWordPosition="1509" endWordPosition="1510">fferent types, according to source. With two exceptions (features indicating whether a paragraph was the first or last paragraph in text), the features were numerical (frequency) and normalized to the length of the paragraph. The first group of genre features comes from Biber (1988), who attempted to characterize dimensions of genre. The features here include frequency of first, second and third person pronouns; demonstrative pronouns; place and time adverbials; intensifiers; and modals, among a number of others. The second category of genre features includes discourse markers, primarily from Knott (1996), that indicate contrast, comparison, causation, evidence, condition, and similar relations. The third type of genre features was a list of 500 adjectives classified in terms of Appraisal (Martin and White, 2005) as indicating Appreciation, Judgment or Affect. Appraisal categories have been shown to be useful in improving the performance of polarity classifiers (Whitelaw et al., 2005). Finally, we also include text statistics as features, such as average length of words and sentences and position of paragraphs in the text. 3.2 Classifiers To classify paragraphs in the text, we use the WEKA sui</context>
</contexts>
<marker>Knott, 1996</marker>
<rawString>Knott, Alistair. 1996. A Data-Driven Methodology for Motivating a Set of Coherence Relations. Edinburgh, UK: University of EdinburghThesis Type.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James R Martin</author>
<author>Peter White</author>
</authors>
<title>The Language of Evaluation.</title>
<date>2005</date>
<publisher>Palgrave.</publisher>
<location>New York:</location>
<contexts>
<context position="9713" citStr="Martin and White, 2005" startWordPosition="1539" endWordPosition="1542">length of the paragraph. The first group of genre features comes from Biber (1988), who attempted to characterize dimensions of genre. The features here include frequency of first, second and third person pronouns; demonstrative pronouns; place and time adverbials; intensifiers; and modals, among a number of others. The second category of genre features includes discourse markers, primarily from Knott (1996), that indicate contrast, comparison, causation, evidence, condition, and similar relations. The third type of genre features was a list of 500 adjectives classified in terms of Appraisal (Martin and White, 2005) as indicating Appreciation, Judgment or Affect. Appraisal categories have been shown to be useful in improving the performance of polarity classifiers (Whitelaw et al., 2005). Finally, we also include text statistics as features, such as average length of words and sentences and position of paragraphs in the text. 3.2 Classifiers To classify paragraphs in the text, we use the WEKA suite (Witten and Frank, 2005), testing three popular machine learning algorithms: Naïve Bayes, Support Vector Machine, and Linear Regression (preliminary testing with Decision Trees suggests that it is not appropri</context>
</contexts>
<marker>Martin, White, 2005</marker>
<rawString>Martin, James R. &amp; Peter White. 2005. The Language of Evaluation. New York: Palgrave.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>Matthew Hurst</author>
</authors>
<title>Towards a robust metric of polarity.</title>
<date>2006</date>
<booktitle>In Janyce Wiebe (ed.), Computing Attitude and Affect in Text: Theory and Applications</booktitle>
<pages>265--279</pages>
<publisher>Springer.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="25811" citStr="Nigam and Hurst (2006)" startWordPosition="4154" endWordPosition="4157">aïve Bayes classifier was significantly boosted), however testing with parts of the text classified as subjective showed that the eliminated parts were indeed irrelevant. In contrast to our findings, they reported a drop in performance when paragraphs were taken as the only possible boundary between subjective and objective text spans. Other research that has dealt with identifying more or less relevant parts of the text for the purposes of sentiment analysis include Taboada and Grieve (2004), who improved the performance of a lexicon-based model by weighing words towards the end of the text; Nigam and Hurst (2006), who detect polar expressions in topic sentences; and Voll and Taboada (2007), who used a topic classifier and discourse parser to eliminate potentially off-topic or less important sentences. 7 Conclusions We have described a genre-based taxonomy for classifying paragraphs in movie reviews, with the main classification being a distinction between formal and functional stages, and, within those, between mainly descriptive vs. comment stages. The taxonomy was used to annotate 100 movie reviews, as the basis for building classifiers. We tested a number of different classifiers. Our results sugge</context>
</contexts>
<marker>Nigam, Hurst, 2006</marker>
<rawString>Nigam, Kamal &amp; Matthew Hurst. 2006. Towards a robust metric of polarity. In Janyce Wiebe (ed.), Computing Attitude and Affect in Text: Theory and Applications (pp. 265-279). Dordrecht: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Constantin Orasan</author>
</authors>
<title>PALinkA: A highly customizable tool for discourse annotation.</title>
<date>2003</date>
<booktitle>Proceedings of 4th SIGdial Workshop on Discourse and Dialog (pp. 39 – 43).</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="6883" citStr="Orasan, 2003" startWordPosition="1075" endWordPosition="1077">n newspapers or on-line magazines. We restricted the texts to “Top Critics” because we wanted well-structured, polished texts, unlike those found in some on-line review sites. Future work will address those more informal reviews. The 100 reviews contain 83,275 words and 1,542 paragraphs. The annotation was performed at the paragraph level. Although stages may span across paragraphs, and paragraphs may contain more than one stage, there is a close relationship between paragraphs and stages. The restriction also resulted in a more reliable annotation, performed with the PALinkA annotation tool (Orasan, 2003). The annotation was performed by one of the authors, and we carried out reliability tests with two other annotators, one another one of the authors, who helped develop the taxonomy, and the third one a project member who read the annotation guidelines1, and received a few hours’ training in the labels and software. We used Fleiss’ kappa (Fleiss, 1971), which extends easily to the case of multiple raters (Di Eugenio and Glass, 2004). We all annotated four texts. The results of the reliability tests show a reasonable agreement level for the distinction between formal and functional zones (.84 f</context>
</contexts>
<marker>Orasan, 2003</marker>
<rawString>Orasan, Constantin. 2003. PALinkA: A highly customizable tool for discourse annotation. Proceedings of 4th SIGdial Workshop on Discourse and Dialog (pp. 39 – 43). Sapporo, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>Proceedings of 42nd Meeting of the Association for Computational Linguistics</booktitle>
<pages>271--278</pages>
<location>Barcelona,</location>
<contexts>
<context position="15470" citStr="Pang and Lee, 2004" startWordPosition="2508" endWordPosition="2511">ictionaries. SO-CAL uses five dictionaries: four lexical dictionaries with 2,257 adjectives, 1,142 nouns, 903 verbs, and 745 adverbs, and a fifth dictionary containing 177 intensifying expressions. Although the majority of the entries are single words, the calculator also allows for multiword entries written in regular expressionlike language. The SO-carrying words in these dictionaries were taken from a variety of sources, the three largest a corpus of 400 reviews from Epinions.com, first used by Taboada and Grieve (2004), a 100 text subset of the 2,000 movie reviews in the Polarity Dataset (Pang and Lee, 2004), and words from the General Inquirer dictionary (Stone, 1997). Each of the open-class words were given a hand-ranked SO value between 5 and -5 (neutral or zero-value words are not included in the dictionary) by a native English speaker. The numerical values were chosen to reflect both the prior polarity and strength of the word, averaged across likely interpretations. For example, the word phenomenal is a 5, nicely a 2, disgust a -3, and monstrosity a -5. The dictionary was later reviewed by a committee of three other researchers in order to minimize the subjectivity of ranking SO by hand. Ou</context>
<context position="17077" citStr="Pang and Lee, 2004" startWordPosition="2779" endWordPosition="2782">age modifiers (very engaging: 4x125% = 5). We also ignore words appearing within the scope of irrealis markers such as certain verbs, modals, and punctuation, and decrease the weight of words which appear often in the text. In order to counter positive linguistic 65 bias (Boucher and Osgood, 1969), a problem for lexicon-based sentiment classifiers (Kennedy and Inkpen, 2006), we increase the final SO of any negative expression appearing in the text. The performance of SO-CAL tends to be in the 76-81% range. We have tested on informal movie, book and product reviews and on the Polarity Dataset (Pang and Lee, 2004). The performance on movie reviews tends to be on the lower end of the scale. Our baseline for movies, described in Section 5, is 77.7%. We believe that we have reached a ceiling in terms of word- and phrase-level performance, and most future improvements need to come from discourse features. The stage classification described in this paper is one of them. 5 Results The final goal of a stage classifier is to use the information about different stages in sentiment classification. Our assumption is that descriptive paragraphs contain less evaluative content about the movie being reviewed, and th</context>
<context position="19326" citStr="Pang and Lee, 2004" startWordPosition="3141" endWordPosition="3144">h is significant at the p&lt;0.05 level. Most of the improvement (7%) is due to disregarding Describe paragraphs, but 2% comes from Describe+Comment, and 1% each from Background, Interpretation, and (all) Formal tags. There is no performance gain, however, from the use of aspect tags (e.g., by increasing the weight on Overall paragraphs), justifying our decision to ignore subtags for text-level polarity classification. 5.2 Categorical classification We evaluated all the classifiers from Table 2, but we omit discussion of the worst performing. The evaluation was performed on the Polarity Dataset (Pang and Lee, 2004), a collection of 2,000 online movie reviews, balanced for polarity. The SO performance for the categorical classifiers is given in Figure 1. When applicable, we always gave Formal Zones (which Table 2 indicates are fairly easy to identify) a weight of 0, however for Describe paragraphs we tested at 0.1 intervals between 0 and 1. Testing all possible values of Describe+Comment was not feasible, so we set the weights of those to a value halfway between the weight of Comment paragraphs (1) and the weight of the Describe paragraph. Most of the classifiers were able to improve performance beyond t</context>
<context position="22178" citStr="Pang and Lee 2004" startWordPosition="3577" endWordPosition="3580">y 79 78 77 No tagging Baseline 2-Class-5-gram-SVM 2-Class-5-gram-Bayes 2-Class-genre-Bayes 2-Class-genre-SVM 3-Class-5-gram-Bayes 3-Class-5-gram-SVM 3-Class-genre-Bayes 4-Class-5-gram-Bayes 4-Class-5-gram-SVM 4-Class-genre-Bayes Figure 1. SO Performance with various paragraph tagging classifiers, by weight on Describe probably because this class is not easily distinguishable from Describe and Comment (nor in fact should it be). We can further confirm that our classifier is properly distinguishing Describe and Comment by discounting Comment paragraphs rather than Describe paragraphs (following Pang and Lee 2004). When Comment paragraphs tagged by the best performing classifier are ignored, SO-CAL’s accuracy drops to 56.65%, just barely above chance. 5.3 Continuous classification Table 4 gives the results for the linear regression classifier, which assigns a Comment ratio to each paragraph used for weighting. Model Accuracy LR, Des+Com C = 0 78.75 LR, Des+Com C = 0.25 79.35 LR, Des+Com C = 0.5 79.00 LR, Des+Com C = 0.75 78.90 LR, Des+Com C = 1 78.95 LR, No Des+Com 79.05 Table 4. SO Performance with linear regression The linear regression model trained with a 0.25 comment ratio on Describe+Comment para</context>
<context position="24965" citStr="Pang and Lee (2004)" startWordPosition="4017" endWordPosition="4020">ely at the level of the entire text. With regards to the latter, two major approaches have emerged: the use of machine learning classifiers trained on n-grams 67 or similar features (Pang et al., 2002), and the use of sentiment dictionaries (Esuli and Sebastiani, 2006; Taboada et al., 2006). Support Vector Machine (SVM) classifiers have been shown to out-perform lexicon-based models within a single domain (Kennedy and Inkpen, 2006); however they have trouble with cross-domain tasks (Aue and Gamon, 2005), and some researchers have argued for hybrid classifiers (Andreevskaia and Bergler, 2008). Pang and Lee (2004) attempted to improve the performance of an SVM classifier by identifying and removing objective sentences from the texts. Results were mixed: The improvement was minimal for the SVM classifier (though the performance of a naïve Bayes classifier was significantly boosted), however testing with parts of the text classified as subjective showed that the eliminated parts were indeed irrelevant. In contrast to our findings, they reported a drop in performance when paragraphs were taken as the only possible boundary between subjective and objective text spans. Other research that has dealt with ide</context>
<context position="27191" citStr="Pang and Lee (2004)" startWordPosition="4365" endWordPosition="4368"> is feasible, but comes at the cost of precision, which seems to be the key variable in improving sentiment analysis. Ultimately, the goal of the classification was to improve the accuracy of SO-CAL, our semantic orientation calculator. Using the manual annotations, we manage to boost performance by 12% over the baseline. With the best automatic classifier, we still show consistent improvement over the baseline. Given the relatively low accuracy of the classifiers, the crucial factor involves using fine-grained weights on paragraphs, rather than simply ignoring Describe-labeled paragraphs, as Pang and Lee (2004) did for objective sentences. An obvious expansion to this work would involve a larger dataset on which to train, to improve the performance of the classifier(s). We would also like to focus on the syntactic patterns and verb class properties of narration, aspects that are not captured with simply using words and POS labels. Connectives in particular are good indicators of the difference between narration (temporal connectives) and opinion (contrastive connectives). There may also be benefit to combining paragraph- and sentence-based approaches. Finally, we would like to identify common sequen</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Pang, Bo &amp; Lillian Lee. 2004. A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts. Proceedings of 42nd Meeting of the Association for Computational Linguistics (pp. 271-278). Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up? Sentiment classification using Machine Learning techniques.</title>
<date>2002</date>
<booktitle>Proceedings of Conference on Empirical Methods in NLP</booktitle>
<pages>79--86</pages>
<contexts>
<context position="24547" citStr="Pang et al., 2002" startWordPosition="3952" endWordPosition="3955">an annotated texts and the large performance gap between movie reviews and other consumer reviews both suggest there is more potential for improvement, it will probably require more sophisticated and precise models. 6 Related work The bulk of the work in sentiment analysis has focused on classification at either the sentence level, e.g., the subjectivity/polarity detection of Wiebe and Riloff (2005), or alternatively at the level of the entire text. With regards to the latter, two major approaches have emerged: the use of machine learning classifiers trained on n-grams 67 or similar features (Pang et al., 2002), and the use of sentiment dictionaries (Esuli and Sebastiani, 2006; Taboada et al., 2006). Support Vector Machine (SVM) classifiers have been shown to out-perform lexicon-based models within a single domain (Kennedy and Inkpen, 2006); however they have trouble with cross-domain tasks (Aue and Gamon, 2005), and some researchers have argued for hybrid classifiers (Andreevskaia and Bergler, 2008). Pang and Lee (2004) attempted to improve the performance of an SVM classifier by identifying and removing objective sentences from the texts. Results were mixed: The improvement was minimal for the SVM</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Pang, Bo, Lillian Lee &amp; Shivakumar Vaithyanathan. 2002. Thumbs up? Sentiment classification using Machine Learning techniques. Proceedings of Conference on Empirical Methods in NLP (pp. 79-86).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Livia Polanyi</author>
<author>Annie Zaenen</author>
</authors>
<title>Contextual valence shifters.</title>
<date>2006</date>
<booktitle>Computing Attitude and Affect in Text: Theory and Applications</booktitle>
<pages>1--10</pages>
<editor>In James G. Shanahan, Yan Qu &amp; Janyce Wiebe (eds.),</editor>
<publisher>Springer.</publisher>
<location>Dordrecht:</location>
<contexts>
<context position="1716" citStr="Polanyi and Zaenen (2006)" startWordPosition="244" endWordPosition="247">ent extraction, the semantic or lexicon-based, and the machine learning or corpus-based approach, both attempt to identify low-level features that convey opinion. In the semantic approach, the features are lists of words and their prior polarity, (e.g., the adjective terrible will have a negative polarity, and maybe intensity, represented as -4; the noun masterpiece may be a 5). Our approach is lexicon-based, but we make use of information derived from machine learning classifiers. Beyond the prior polarity of a word, its local context obviously plays an important role in conveying sentiment. Polanyi and Zaenen (2006) use the term ‘contextual valence shifters’ to refer to expressions in the local context that may change a word’s polarity, such as intensifiers, modal verbs, connectives, and of course negation. Further beyond the local context, the overall structure and organization of the text, influenced by its genre, can help the reader determine how the evaluation is expressed, and where it lies. Polanyi and Zaenen (2006) also cite genre constraints as relevant factors in calculating sentiment. Among the many definitions of genre, we take the view of Systemic Functional Linguistics that genres are purpos</context>
<context position="16227" citStr="Polanyi and Zaenen (2006)" startWordPosition="2637" endWordPosition="2640">en 5 and -5 (neutral or zero-value words are not included in the dictionary) by a native English speaker. The numerical values were chosen to reflect both the prior polarity and strength of the word, averaged across likely interpretations. For example, the word phenomenal is a 5, nicely a 2, disgust a -3, and monstrosity a -5. The dictionary was later reviewed by a committee of three other researchers in order to minimize the subjectivity of ranking SO by hand. Our calculator moves beyond simple averaging of each word’s semantic orientation value, and implements and expands on the insights of Polanyi and Zaenen (2006) with respect to contextual valence shifters. We implement negation by shifting the SO value of a word towards the opposite polarity (not terrible, for instance, is calculated as -5+4 = -1). Intensification is modeled using percentage modifiers (very engaging: 4x125% = 5). We also ignore words appearing within the scope of irrealis markers such as certain verbs, modals, and punctuation, and decrease the weight of words which appear often in the text. In order to counter positive linguistic 65 bias (Boucher and Osgood, 1969), a problem for lexicon-based sentiment classifiers (Kennedy and Inkpen</context>
</contexts>
<marker>Polanyi, Zaenen, 2006</marker>
<rawString>Polanyi, Livia &amp; Annie Zaenen. 2006. Contextual valence shifters. In James G. Shanahan, Yan Qu &amp; Janyce Wiebe (eds.), Computing Attitude and Affect in Text: Theory and Applications (pp. 1-10). Dordrecht: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yohei Seki</author>
</authors>
<title>Koji Eguchi &amp; Noriko Kando.</title>
<date>2006</date>
<booktitle>In Janyce Wiebe (ed.), Computing Attitude and Affect in Text: Theory and Applications</booktitle>
<pages>317--336</pages>
<publisher>Springer.</publisher>
<location>Dordrecht:</location>
<marker>Seki, 2006</marker>
<rawString>Seki, Yohei, Koji Eguchi &amp; Noriko Kando. 2006. Multi-document viewpoint summarization focused on facts, opinion and knowledge. In Janyce Wiebe (ed.), Computing Attitude and Affect in Text: Theory and Applications (pp. 317-336). Dordrecht: Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
</authors>
<title>Thematic text analysis: New agendas for analyzing text content.</title>
<date>1997</date>
<editor>In Carl Roberts (ed.),</editor>
<contexts>
<context position="15532" citStr="Stone, 1997" startWordPosition="2520" endWordPosition="2521"> with 2,257 adjectives, 1,142 nouns, 903 verbs, and 745 adverbs, and a fifth dictionary containing 177 intensifying expressions. Although the majority of the entries are single words, the calculator also allows for multiword entries written in regular expressionlike language. The SO-carrying words in these dictionaries were taken from a variety of sources, the three largest a corpus of 400 reviews from Epinions.com, first used by Taboada and Grieve (2004), a 100 text subset of the 2,000 movie reviews in the Polarity Dataset (Pang and Lee, 2004), and words from the General Inquirer dictionary (Stone, 1997). Each of the open-class words were given a hand-ranked SO value between 5 and -5 (neutral or zero-value words are not included in the dictionary) by a native English speaker. The numerical values were chosen to reflect both the prior polarity and strength of the word, averaged across likely interpretations. For example, the word phenomenal is a 5, nicely a 2, disgust a -3, and monstrosity a -5. The dictionary was later reviewed by a committee of three other researchers in order to minimize the subjectivity of ranking SO by hand. Our calculator moves beyond simple averaging of each word’s sema</context>
</contexts>
<marker>Stone, 1997</marker>
<rawString>Stone, Philip J. 1997. Thematic text analysis: New agendas for analyzing text content. In Carl Roberts (ed.), Text Analysis for the Social Sciences. Mahwah, NJ: Lawrence Erlbaum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Caroline Anthony</author>
<author>Kimberly Voll</author>
</authors>
<title>Creating semantic orientation dictionaries.</title>
<date>2006</date>
<booktitle>Proceedings of 5th International Conference on Language Resources and Evaluation (LREC)</booktitle>
<pages>427--432</pages>
<location>Genoa, Italy.</location>
<contexts>
<context position="24637" citStr="Taboada et al., 2006" startWordPosition="3967" endWordPosition="3970">er reviews both suggest there is more potential for improvement, it will probably require more sophisticated and precise models. 6 Related work The bulk of the work in sentiment analysis has focused on classification at either the sentence level, e.g., the subjectivity/polarity detection of Wiebe and Riloff (2005), or alternatively at the level of the entire text. With regards to the latter, two major approaches have emerged: the use of machine learning classifiers trained on n-grams 67 or similar features (Pang et al., 2002), and the use of sentiment dictionaries (Esuli and Sebastiani, 2006; Taboada et al., 2006). Support Vector Machine (SVM) classifiers have been shown to out-perform lexicon-based models within a single domain (Kennedy and Inkpen, 2006); however they have trouble with cross-domain tasks (Aue and Gamon, 2005), and some researchers have argued for hybrid classifiers (Andreevskaia and Bergler, 2008). Pang and Lee (2004) attempted to improve the performance of an SVM classifier by identifying and removing objective sentences from the texts. Results were mixed: The improvement was minimal for the SVM classifier (though the performance of a naïve Bayes classifier was significantly boosted)</context>
</contexts>
<marker>Taboada, Anthony, Voll, 2006</marker>
<rawString>Taboada, Maite, Caroline Anthony &amp; Kimberly Voll. 2006. Creating semantic orientation dictionaries. Proceedings of 5th International Conference on Language Resources and Evaluation (LREC) (pp. 427-432). Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maite Taboada</author>
<author>Jack Grieve</author>
</authors>
<title>Analyzing appraisal automatically.</title>
<date>2004</date>
<booktitle>Proceedings of AAAI Spring Symposium on Exploring Attitude and Affect in Text (AAAI</booktitle>
<tech>Technical Report SS-04-07</tech>
<pages>158--161</pages>
<institution>Stanford University, CA.</institution>
<contexts>
<context position="15379" citStr="Taboada and Grieve (2004)" startWordPosition="2490" endWordPosition="2493">text, and aggregates their semantic orientation value, which is in turn extracted from a set of dictionaries. SO-CAL uses five dictionaries: four lexical dictionaries with 2,257 adjectives, 1,142 nouns, 903 verbs, and 745 adverbs, and a fifth dictionary containing 177 intensifying expressions. Although the majority of the entries are single words, the calculator also allows for multiword entries written in regular expressionlike language. The SO-carrying words in these dictionaries were taken from a variety of sources, the three largest a corpus of 400 reviews from Epinions.com, first used by Taboada and Grieve (2004), a 100 text subset of the 2,000 movie reviews in the Polarity Dataset (Pang and Lee, 2004), and words from the General Inquirer dictionary (Stone, 1997). Each of the open-class words were given a hand-ranked SO value between 5 and -5 (neutral or zero-value words are not included in the dictionary) by a native English speaker. The numerical values were chosen to reflect both the prior polarity and strength of the word, averaged across likely interpretations. For example, the word phenomenal is a 5, nicely a 2, disgust a -3, and monstrosity a -5. The dictionary was later reviewed by a committee</context>
<context position="25686" citStr="Taboada and Grieve (2004)" startWordPosition="4132" endWordPosition="4135"> sentences from the texts. Results were mixed: The improvement was minimal for the SVM classifier (though the performance of a naïve Bayes classifier was significantly boosted), however testing with parts of the text classified as subjective showed that the eliminated parts were indeed irrelevant. In contrast to our findings, they reported a drop in performance when paragraphs were taken as the only possible boundary between subjective and objective text spans. Other research that has dealt with identifying more or less relevant parts of the text for the purposes of sentiment analysis include Taboada and Grieve (2004), who improved the performance of a lexicon-based model by weighing words towards the end of the text; Nigam and Hurst (2006), who detect polar expressions in topic sentences; and Voll and Taboada (2007), who used a topic classifier and discourse parser to eliminate potentially off-topic or less important sentences. 7 Conclusions We have described a genre-based taxonomy for classifying paragraphs in movie reviews, with the main classification being a distinction between formal and functional stages, and, within those, between mainly descriptive vs. comment stages. The taxonomy was used to anno</context>
</contexts>
<marker>Taboada, Grieve, 2004</marker>
<rawString>Taboada, Maite &amp; Jack Grieve. 2004. Analyzing appraisal automatically. Proceedings of AAAI Spring Symposium on Exploring Attitude and Affect in Text (AAAI Technical Report SS-04-07) (pp. 158-161). Stanford University, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews.</title>
<date>2002</date>
<booktitle>Proceedings of 40th Meeting of the Association for Computational Linguistics</booktitle>
<pages>417--424</pages>
<contexts>
<context position="4079" citStr="Turney, 2002" startWordPosition="625" endWordPosition="626">ribing our approach to sentiment classification of texts in Section 4, we describe experiments to improve its performance with the information on stages in Section 5. Section 6 disProceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 62–70, Queen Mary University of London, September 2009. c�2009 Association for Computational Linguistics 62 cusses related work, and Section 7 provides conclusions. 2 Stages in movie reviews Within the larger review genre, we focus on movie reviews. Movie reviews are particularly difficult to classify (Turney, 2002), because large portions of the review contain description of the plot, the characters, actors, director, etc., or background information about the film. Our approach is based on the work of Bieler et al. (2007), who identify formal and functional zones (stages) within German movie reviews. Formal zones are parts of the text that contribute factual information about the cast and the credits, and also about the review itself (author, date of publication and the reviewer’s rating of the movie). Functional zones contain the main gist of the review, and can be divided roughly into description and </context>
</contexts>
<marker>Turney, 2002</marker>
<rawString>Turney, Peter. 2002. Thumbs up or thumbs down? Semantic orientation applied to unsupervised classification of reviews. Proceedings of 40th Meeting of the Association for Computational Linguistics (pp. 417-424).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kimberly Voll</author>
<author>Maite Taboada</author>
</authors>
<title>Not all words are created equal: Extracting semantic orientation as a function of adjective relevance.</title>
<date>2007</date>
<booktitle>Proceedings of the 20th Australian Joint Conference on Artificial Intelligence</booktitle>
<pages>337--346</pages>
<location>Gold Coast, Australia.</location>
<contexts>
<context position="25889" citStr="Voll and Taboada (2007)" startWordPosition="4166" endWordPosition="4169">of the text classified as subjective showed that the eliminated parts were indeed irrelevant. In contrast to our findings, they reported a drop in performance when paragraphs were taken as the only possible boundary between subjective and objective text spans. Other research that has dealt with identifying more or less relevant parts of the text for the purposes of sentiment analysis include Taboada and Grieve (2004), who improved the performance of a lexicon-based model by weighing words towards the end of the text; Nigam and Hurst (2006), who detect polar expressions in topic sentences; and Voll and Taboada (2007), who used a topic classifier and discourse parser to eliminate potentially off-topic or less important sentences. 7 Conclusions We have described a genre-based taxonomy for classifying paragraphs in movie reviews, with the main classification being a distinction between formal and functional stages, and, within those, between mainly descriptive vs. comment stages. The taxonomy was used to annotate 100 movie reviews, as the basis for building classifiers. We tested a number of different classifiers. Our results suggest that a simple, two-way or continuous classification using a small set of li</context>
</contexts>
<marker>Voll, Taboada, 2007</marker>
<rawString>Voll, Kimberly &amp; Maite Taboada. 2007. Not all words are created equal: Extracting semantic orientation as a function of adjective relevance. Proceedings of the 20th Australian Joint Conference on Artificial Intelligence (pp. 337-346). Gold Coast, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Whitelaw</author>
</authors>
<title>Navendu Garg &amp; Shlomo Argamon.</title>
<date>2005</date>
<booktitle>Proceedings of ACM SIGIR Conference on Information and Knowledge Management (CIKM</booktitle>
<pages>625--631</pages>
<location>Bremen, Germany.</location>
<marker>Whitelaw, 2005</marker>
<rawString>Whitelaw, Casey, Navendu Garg &amp; Shlomo Argamon. 2005. Using Appraisal groups for sentiment analysis. Proceedings of ACM SIGIR Conference on Information and Knowledge Management (CIKM 2005) (pp. 625-631). Bremen, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Janyce Wiebe</author>
<author>Ellen Riloff</author>
</authors>
<title>Creating subjective and objective sentence classifiers from unannotated texts.</title>
<date>2005</date>
<booktitle>Proceedings of Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2005). Mexico City,</booktitle>
<location>Mexico.</location>
<contexts>
<context position="24331" citStr="Wiebe and Riloff (2005)" startWordPosition="3916" endWordPosition="3919">or worse performance. We have not tested all possible options (there are simply too many), but we think it unlikely that additional gains will be made with these simple, surface feature sets. Although our testing with human annotated texts and the large performance gap between movie reviews and other consumer reviews both suggest there is more potential for improvement, it will probably require more sophisticated and precise models. 6 Related work The bulk of the work in sentiment analysis has focused on classification at either the sentence level, e.g., the subjectivity/polarity detection of Wiebe and Riloff (2005), or alternatively at the level of the entire text. With regards to the latter, two major approaches have emerged: the use of machine learning classifiers trained on n-grams 67 or similar features (Pang et al., 2002), and the use of sentiment dictionaries (Esuli and Sebastiani, 2006; Taboada et al., 2006). Support Vector Machine (SVM) classifiers have been shown to out-perform lexicon-based models within a single domain (Kennedy and Inkpen, 2006); however they have trouble with cross-domain tasks (Aue and Gamon, 2005), and some researchers have argued for hybrid classifiers (Andreevskaia and B</context>
</contexts>
<marker>Wiebe, Riloff, 2005</marker>
<rawString>Wiebe, Janyce &amp; Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. Proceedings of Sixth International Conference on Intelligent Text Processing and Computational Linguistics (CICLing-2005). Mexico City, Mexico.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ian H Witten</author>
<author>Eibe Frank</author>
</authors>
<date>2005</date>
<booktitle>Data Mining: Practical Machine Learning Tools and Techniques (2nd edn.).</booktitle>
<publisher>Morgan Kaufmann.</publisher>
<location>San Francisco:</location>
<contexts>
<context position="10128" citStr="Witten and Frank, 2005" startWordPosition="1607" endWordPosition="1610">at indicate contrast, comparison, causation, evidence, condition, and similar relations. The third type of genre features was a list of 500 adjectives classified in terms of Appraisal (Martin and White, 2005) as indicating Appreciation, Judgment or Affect. Appraisal categories have been shown to be useful in improving the performance of polarity classifiers (Whitelaw et al., 2005). Finally, we also include text statistics as features, such as average length of words and sentences and position of paragraphs in the text. 3.2 Classifiers To classify paragraphs in the text, we use the WEKA suite (Witten and Frank, 2005), testing three popular machine learning algorithms: Naïve Bayes, Support Vector Machine, and Linear Regression (preliminary testing with Decision Trees suggests that it is not appropriate for this task). Training parameters were set to default values. In order to use Linear Regression, which provides a numerical output based on feature values and derived feature weights, we have to conceive of Comment/Describe/Describe+Comment not as nominal (or ordinal) classes, but rather as corresponding to a Comment/Describe ratio, with “pure” Describe at one end and “pure” Comment at the other. For train</context>
</contexts>
<marker>Witten, Frank, 2005</marker>
<rawString>Witten, Ian H. &amp; Eibe Frank. 2005. Data Mining: Practical Machine Learning Tools and Techniques (2nd edn.). San Francisco: Morgan Kaufmann.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>