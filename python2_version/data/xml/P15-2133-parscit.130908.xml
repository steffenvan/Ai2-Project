<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.023255">
<title confidence="0.972867">
The Impact of Listener Gaze on Predicting Reference Resolution
</title>
<author confidence="0.993712">
Nikolina Koleva&apos; Martin Villalba2 Maria Staudte&apos; Alexander Koller2
</author>
<affiliation confidence="0.934686">
&apos;Embodied Spoken Interaction Group, Saarland University, Saarbr¨ucken, Germany
2 Department of Linguistics, University of Potsdam, Potsdam, Germany
</affiliation>
<email confidence="0.779371">
{nikkol  |masta}@coli.uni-saarland.de
{martin.villalba  |alexander.koller}@uni-potsdam.de
</email>
<sectionHeader confidence="0.984817" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999609">
We investigate the impact of listener’s
gaze on predicting reference resolution in
situated interactions. We extend an ex-
isting model that predicts to which entity
in the environment listeners will resolve
a referring expression (RE). Our model
makes use of features that capture which
objects were looked at and for how long,
reflecting listeners’ visual behavior. We
improve a probabilistic model that consid-
ers a basic set of features for monitoring
listeners’ movements in a virtual environ-
ment. Particularly, in complex referential
scenes, where more objects next to the tar-
get are possible referents, gaze turns out to
be beneficial and helps deciphering listen-
ers’ intention. We evaluate performance at
several prediction times before the listener
performs an action, obtaining a highly sig-
nificant accuracy gain.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99996562264151">
Speakers tend to follow the listener’s behavior in
order to determine whether their communicated
message was received and understood. This phe-
nomenon is known as grounding, it is well estab-
lished in the dialogue literature (Clark, 1996), and
it plays an important role in collaborative tasks
and goal–oriented conversations. Solving a col-
laborative task in a shared environment is an ef-
fective way of studying the alignment of commu-
nication channels (Clark and Krych, 2004; Hanna
and Brennan, 2007).
In situated spoken conversations ambiguous lin-
guistic expressions are common, where additional
modalities are available. While Gargett et al.
(2010) studied instruction giving and following in
virtual environments, Brennan et al. (2013) ex-
amined pedestrian guidance in outdoor real envi-
ronments. Both studies investigate the interaction
of human interlocutors but neither study exploits
listeners’ eye movements. In contrast, Koller et
al. (2012) designed a task in which a natural lan-
guage generation (NLG) system gives instructions
to a human player in virtual environment whose
eye movements were tracked. They outperformed
similar systems in both successful reference res-
olution and listener confusion. Engonopoulos et
al. (2013) attempted to predict the resolution of
an RE, achieving good performance by combining
two probabilistic log–linear models: a semantic
model Psem that analyzes the semantics of a given
instruction, and an observational model Pobs that
inspects the player’s behavior. However, they did
not include listener’s gaze. They observed that the
accuracy for Pobs reaches its highest point at a rel-
atively late stage in an interaction. Similar obser-
vations are reported by Kennington and Schlangen
(2014): they compare listener gaze and an incre-
mental update model (IUM) as predictors for the
resolution of an RE, noting that gaze is more ac-
curate before the onset of an utterance, whereas
the model itself is more accurate afterwards.
In this paper we report on the extension of the
Pobs model to also consider listener’s visual be-
haviour. More precisely we implement features
that encode listener’s eye movement patterns and
evaluate their performance on a multi–modal data
collection. We show that such a model as it
takes an additional communication channel pro-
vides more accurate predictions especially when
dealing with complex scenes. We also expand on
concepts from the IUM, by applying the conclu-
sions drawn from its behaviour to a dynamic task
with a naturalistic interactive scenario.
</bodyText>
<sectionHeader confidence="0.931224" genericHeader="introduction">
2 Problem definition
</sectionHeader>
<bodyText confidence="0.999856">
We address the research question of how to auto-
matically predict an RE resolution, i.e., answer-
ing the question of which entity in a virtual en-
vironment has been understood by the listener af-
</bodyText>
<page confidence="0.905333">
812
</page>
<bodyText confidence="0.958487375">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 812–817,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
ter receiving an instruction. While the linguistic
material in instructions carries a lot of informa-
tion, even completely unambiguous descriptions
may be misunderstood. A robust NLG system
should be capable of detecting misunderstandings
and preventing its users from making mistakes.
Language comprehension is mirrored by inter-
locutors’ non verbal behavior, and this can help
when decoding the listener’s interpretation. Pre-
cise automatic estimates may be crucial when de-
veloping a real–time NLG system, as such a mech-
anism would be more robust and capable at avoid-
ing misunderstandings. As mentioned in section 1,
Engonopoulos et al. (2013) propose two statistical
models to solve that problem: a semantic model
Psem based on the linguistic content, and an ob-
servation model Pobs based on listener behavior
features.
More formally, let’s assume a system generates
an expression r that aims to identify a target ob-
ject ot among a set O of possible objects, i.e. those
available in the scene view. Given the state of the
world s at time point t, and the observed listener’s
behavior Q(t) of the user at time t &gt; tb (where
tb denotes the end of an interaction), we estimated
the conditional probability p(op|r, s, Q(t)) that in-
dicates how probable it is that the listener resolved
r to op. This probability can be also expressed as
follows:
</bodyText>
<equation confidence="0.997665666666667">
Psem(op|r, s)Pobs(op|Q(t))
P(op|r, s, Q(t)) OC
P(op|r, s, Q(t)) OC Psem(op|r, s)Pobs(op|Q(t))
</equation>
<bodyText confidence="0.999901428571429">
We expect an NLG system to compute and out-
put an expression that maximizes the probability
of op. Due to the dynamic nature of our scenar-
ios, we also require the probability value to be up-
dated at certain time intervals throughout an in-
teraction. Tracking the probability changes over
time, an NLG system could proactively react to
changes in its environment. Henderson and Smith
(2007) show that accounting for both fixation lo-
cation and duration are key to identify a player’s
focus of attention.
The technical contribution of this paper is to ex-
tend the Pobs model of Engonopoulos et al. (2013)
with gaze features to account for these variables.
</bodyText>
<sectionHeader confidence="0.933054" genericHeader="method">
3 Episodes and feature functions
</sectionHeader>
<bodyText confidence="0.999984757575758">
The data for our experiment was obtained from
the GIVE Challenge (Koller et al., 2010), an inter-
active task in a 3D virtual environment in which
a human player (instruction follower, IF) is navi-
gated through a maze, locating and pressing but-
tons in a predefined order aiming to unlock a safe.
While pressing the wrong button in the sequences
doesn’t always have negative effects, it can also
lead to restarting or losing the game. The IF re-
ceives instructions from either another player or
an automated system (instruction giver, IG). The
IF’s behavior was recorded every 200ms, along
with the IG’s instructions and the state of the
virtual world. The result is an interaction cor-
pus comprising over 2500 games and spanning
over 340 hours of interactions. These interactions
were mainly collected during the GIVE-2 and the
GIVE-2.5 challenges. A laboratory study con-
ducted by Staudte et al. (2012) comprises a data
collection that contains eye-tracking records for
the IF. Although the corpus contains both success-
ful and unsuccessful games, we have decided to
consider only the successful ones.
We define an episode over this corpus as a typ-
ically short sequence of recorded behavior states,
beginning with a manipulation instruction gener-
ated by the IG and ending with a button press by
the IF (at time point tb). In order to make sure
that the recorded button press is a direct response
to the IG’s instruction, an episode is defined such
that it doesn’t contain further utterances after the
first one. Both the target intended by the IG (ot)
and the one selected by the IF (op) were recorded.
</bodyText>
<figureCaption confidence="0.999815">
Figure 1: The structure of the interactions.
</figureCaption>
<bodyText confidence="0.4710145">
Figure 1 depicts the structure of an episode
when eye-tracking data is available. Each episode
</bodyText>
<equation confidence="0.696789">
P(op)
</equation>
<bodyText confidence="0.99787475">
Following Engonopoulos et al. (2013) we make
the simplifying assumption that the distribution of
the probability among the possible targets is uni-
form and obtain:
</bodyText>
<page confidence="0.992787">
813
</page>
<bodyText confidence="0.9999068">
can be seen as a sequence of interaction states
(s1, ... , sn), and each state has a set of visible
objects ({o1, o2, o3, o10, o12}). We then compute
the subset of fixated objects ({o2, o3, o12}). We
update both sets of visible and fixated objects dy-
namically in each interaction state with respect to
the change in visual scene and the corresponding
record of the listener’s eye movements.
We developed feature functions over these
episodes. Along with the episode’s data, each
function takes two parameters: an object op for
which the function is evaluated, and a parameter
d seconds that defines how much of the episode’s
data is the feature allowed to analyze. Each feature
looks only at the behavior that happens in the time
interval −d to 0. Henceforth we refer to the value
of a feature function over this interval as its value
at time −d. The value of a feature function evalu-
ated on episodes with length less than d seconds is
undefined.
</bodyText>
<sectionHeader confidence="0.991552" genericHeader="method">
4 Prediction models
</sectionHeader>
<bodyText confidence="0.999987810810811">
Given an RE uttered by an IG, the semantic model
Psem estimates the probability for each possible
object in the environment to have been understood
as the referent, ranks all candidates and selects the
most probable one in a current scene. This prob-
ability represents the semantics of the utterance,
and is evaluated at a single time point immediately
after the instruction (e.g. “press the blue button”)
has been uttered. The model takes into account
features that encode the presence or absence of ad-
jectives carrying information about the spatial or
color properties (like the adjective “blue”), along
with landmarks appearing as post modifiers of the
target noun.
In contrast to the semantic model, the observa-
tional model Pobs evaluates the changes in the vi-
sual context and player’s behavior after an instruc-
tion has been received. The estimated probabil-
ity is updated constantly before an action, as the
listener in our task–oriented interactions is con-
stantly in motion, altering the visual context. The
model evaluates the distance of the listener posi-
tion to a potential target, whether it is visible or
not, and also how salient an object is in that par-
ticular time window.
As we have seen above, eye movements pro-
vide useful information indicating language com-
prehension, and also how to map a semantic repre-
sentation to an entity in a shared environment. In-
terlocutors constantly interact with their surround-
ing and point to specific entities with gestures.
Gaze behaviour is also driven by the current state
of an interaction. Thus, we extend the basic set
of Pobs features and implement eye–tracking fea-
tures that capture gaze information. We call this
the extended observational model PEobs and con-
sider the following additional features:
</bodyText>
<listItem confidence="0.983913588235294">
1. Looked at: feature counts the number of
interaction states in which an object has
been fixated at least once during the current
episode.
2. Longest Sequence: detects the longest con-
tinuous sequence of interaction states in
which a particular object has been fixated.
3. Linear Distance: returns the euclidean dis-
tance dist on screen between the gaze cursor
and the center of an object.
4. Inv-Squared Distance: returns 1
1+dist2 .
5. Update Fixated Objects: expands the list of
fixated objects in order to consider the IF’s
focus of attention. It successively searches in
10 pixel steps and stops as soon as an object
is found (the threshold is 100 pixels). This
</listItem>
<bodyText confidence="0.999167590909091">
feature evaluates to 1 if the list of fixated ob-
jects is been expanded and 0 otherwise.
When training our model at time −dtrain, we
generate a feature matrix. Given a training
episode, each possible (located in the same room)
object op is added as a new row, where each col-
umn contains the value of a different feature func-
tion for op over this episode at time −dtrain. Fi-
nally, the row based on the target selected by the
IF is marked as a positive example. We then train
a log-linear model, where the weights assigned
to each feature function are learned via optimiza-
tion with the L-BFGS algorithm. By training our
model to correctly predict a target button based
only on data observed up until −dtrain seconds
before the actual action tb, we expect our model to
reliably predict which button the user will select.
Analogously, we define accuracy at testing time
−dtest as the percentage of correctly predicted tar-
get objects when predicting over episodes at time
−dtest. This pair of training and test parameters is
denoted as the tuple (dtrain, dtest).
</bodyText>
<page confidence="0.998431">
814
</page>
<sectionHeader confidence="0.99785" genericHeader="method">
5 Dataset
</sectionHeader>
<bodyText confidence="0.999996733333333">
We evaluated the performance of our improved
model over data collected by Staudte et al. (2012)
using the GIVE Challenge platform. Both training
and testing were performed over a subset of the
data obtained during a collection task involving
worlds created by Gargett et al. (2010), designed
to provide the task with varying levels of diffi-
culty. This corpus provides recorded eye-tracking
data, collected with a remote faceLAB system. In
contrast, the evaluation presented by Engonopou-
los et al. (2013) uses only games collected for the
GIVE 2 and GIVE 2.5 challenges, for which no
eye-tracking data is available. Here, we do not in-
vestigate the performance of Psem and concentrate
on the direct comparison between Pobs and PEobs
in order to find out if and when eye–tracking can
improve the prediction of an RE resolution.
We further filtered our corpus in order to re-
move noisy games following Koller et al. (2012),
considering only interactions for which the eye-
tracker calibration detected inspection of either the
target or another button object in at least 75% of all
referential scenes in an interaction. The resulting
corpus comprises 75 games, for a combined length
of 8 hours. We extracted 761 episodes from this
corpus, amounting to 47m 58s of recorded interac-
tions, with an average length per episode of 3.78
seconds (σ = 3.03sec.). There are 261 episodes
shorter than 2 sec., 207 in the 2-4 sec. range, 139
in the 4-6 sec. range, and 154 episodes longer than
</bodyText>
<sectionHeader confidence="0.861384" genericHeader="method">
6 sec.
6 Evaluation and results
</sectionHeader>
<bodyText confidence="0.999943676470589">
The accuracy of our probabilistic models depends
on the parameters (dtrain, dtest). At different
stages of an interaction the difficulty to predict an
intended target varies as the visual context changes
and in particular the number of visible objects. As
the weights of the features are optimized at time
−dtrain, it would be expected that testing also at
time −dtest = −dtrain yields the highest accu-
racy. However, the difficulty to make a predic-
tion decreases as tb − dtest approaches tb, i.e. as
the player moves towards the intended target. We
expect that testing at −dtrain works best, but we
need to be able to update continuously. Thus we
also evaluate at other timepoints and test several
combinations of the (dtrain, dtest) parameters.
Given the limited amount of eye-tracking data
available in our corpus, we replaced the cross-
corpora-challenge test setting from the original
Pobs study with a ten fold cross validation setup.
As training and testing were performed over in-
stances of a certain minimum length according to
(dtrain, dtest), we first removed all instances with
length less than max(dtrain, dtest), and then per-
form the cross validation split. In this way we
ensure that the number of instances in the folds
are not unbalanced. Moreover, each instance was
classified as easy or hard depending on the num-
ber of visible objects at time tb. An instance
was considered easy if no more than three objects
were visible at that point, or hard otherwise. For
−dtest = 0, 59.5% of all instances are considered
hard, but this proportion decreases as −dtest in-
creases. At −dtest = −6, the number of hard in-
stances amounts to 72.7%.
We evaluated both the original Pobs model and
the PEobs model on the same data set. We also cal-
culated accuracy values for each feature function,
in order to test whether a single function could out-
perform Pobs. We included as baselines two ver-
sions of Pobs using only the features InRoom and
Visual Salience proposed by Engonopoulos et al.
(2013).
The accuracy results on Figure 2 show our ob-
servations for −6 ≤ −dtrain ≤ −2 and −dtrain ≤
−dtest ≤ 0. The graph shows that PEobs performs
similarly as Pobs on the easy instances, i.e. the
eye-tracking features are not contributing in those
scenarios. However, PEobs shows a consistent im-
provement on the hard instances over Pobs.
For each permutation of the training and test-
ing parameters (dtrain, dtest), we obtain a set of
episodes that fulfil the length criteria for the given
parameters. We apply Pobs and PEobs on the ob-
tained set of instances and measure two corre-
sponding accuracy values. We compared the ac-
curacy values of Pobs and PEobs over all 25 differ-
ent (dtrain, dtest) pairs, using a paired samples t-
test. The test indicated that the PEobs performance
(M = 83.72, SD = 3.56) is significantly better
than the Pobs performance (M = 79.33, SD =
3.89), (t(24) = 9.51, p &lt; .001, Cohen&apos;s d =
1.17). Thus eye-tracking features seem to be par-
ticularly helpful for predicting to which entity an
RE is resolved in hard scenes.
The results also show a peak in accuracy near
the -3 seconds mark. We computed a 2x2 con-
tingency table that contrasts correct and incorrect
predictions for Pobs and PEobs, i.e. whether oi was
</bodyText>
<page confidence="0.99772">
815
</page>
<figureCaption confidence="0.999903">
Figure 2: Accuracy as a function of training and testing time.
</figureCaption>
<bodyText confidence="0.999961272727273">
classified as target object or not. Data for this ta-
ble was collected from all episode judgements for
models trained at times in the [−6 sec., −3 sec.]
range and tested at -3 seconds. McNemar’s test
showed that the marginal row and column frequen-
cies are significantly different (p &lt; 0.05). This
peak is related to the average required time be-
tween an utterance and the resulting target manip-
ulation. This result shows that our model is more
accurate precisely at points in time when we ex-
pect fixations to a target object.
</bodyText>
<sectionHeader confidence="0.998515" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999881270833333">
In this paper we have shown that listener’s gaze
is useful by showing that accuracy improves over
Pobs in the context of predicting the resolution of
an RE. In addition, we observed that our model
PEobs proves to be more robust than Pobs when the
time interval between the prediction (tb − dtest)
and the button press (tb) increases, i.e. gaze is
especially beneficial in an early stage of an in-
teraction. This approach shows significant ac-
curacy improvement on hard referential scenes
where more objects are visible.
We have also established that gaze is particu-
larly useful when combined with some other sim-
ple features, as the features that capture listeners
visual behaviour are not powerful enough to out-
perform even the simplest baseline. Gaze only
benefits the model when it is added on top of fea-
tures that capture the visual context, i.e. the current
scene.
The most immediate future line of research is
the combination of our PEobs model with the se-
mantic model Psem, in order to test the impact of
the extended features in a combined model. If suc-
cessful, such a model could provide reliable pre-
dictions for a significant amount of time before an
action takes place. This is of particular importance
when it comes to designing a system that auto-
matically generates and online outputs feedback to
confirm correct and reject incorrect intentions.
Testing with users in real time is also an area
for future research. An implementation of the Pobs
model is currently in the test phase, and an exten-
sion for the PEobs model would be the immediate
next step. The model could be embedded in an
NLG system to improve the automatic language
generation in such scenarios.
Given that our work refers only to NLG sys-
tems, there’s no possible analysis of speaker’s
gaze. However, it may be interesting to ask
whether a human IG could benefit from the pre-
dictions of PEobs. We could study whether pre-
dictions based on the gaze (mis-)match between
both interlocutors are more effective than simply
presenting the IF’s gaze to the IG and trusting the
IG to correctly interpret this data. If such a sys-
tem proved to be effective, it could point misun-
derstandings to the IG before either of the partici-
pants becomes aware of them.
</bodyText>
<sectionHeader confidence="0.997117" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.7686265">
This work was funded by the Cluster of Excel-
lence on “Multimodal Computing and Interaction”
of the German Excellence Initiative and the SFB
632 “Information Structure”.
</bodyText>
<page confidence="0.998211">
816
</page>
<sectionHeader confidence="0.995868" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998771072727273">
Susan E. Brennan, Katharina S. Schuhmann, and
Karla M. Batres. 2013. Entrainment on the move
and in the lab: The walking around corpus. In Pro-
ceedings of the 35th Annual Conference of the Cog-
nitive Science Society, Berlin, Germany.
Herbert H. Clark and Meredyth A. Krych. 2004.
Speaking while monitoring addressees for under-
standing. Journal of Memory and Language,
50(1):62–81, January.
Herbert H. Clark. 1996. Using Language. Cambridge
University Press, May.
Nikos Engonopoulos, Martin Villalba, Ivan Titov, and
Alexander Koller. 2013. Predicting the resolution
of referring expressions from user behavior. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing (EMNLP), Seattle.
Andrew Gargett, Konstantina Garoufi, Alexander
Koller, and Kristina Striegnitz. 2010. The give-
2 corpus of giving instructions in virtual environ-
ments. In Proceedings of the Seventh International
Conference on Language Resources and Evaluation
(LREC’10), Valletta, Malta, May. European Lan-
guage Resources Association (ELRA).
Joy E. Hanna and Susan E. Brennan. 2007. Speakers’
eye gaze disambiguates referring expressions early
during face-to-face conversation. Journal of Mem-
ory and Language, 57(4):596–615, November.
John M. Henderson and Tim J. Smith. 2007. How
are eye fixation durations controlled during scene
viewing? further evidence from a scene onset delay
paradigm. Visual Cognition, 17(6-7):1055–1082.
Casey Kennington and David Schlangen. 2014. Com-
paring listener gaze with predictions of an incremen-
tal reference resolution model. RefNet workshop on
Psychological and Computational Models of Refer-
ence Comprehension and Production.
Alexander Koller, Kristina Striegnitz, Andrew Gargett,
Donna Byron, Justine Cassell, Robert Dale, Johanna
Moore, and Jon Oberlander. 2010. Report on the
Second NLG Challenge on Generating Instructions
in Virtual Environments (GIVE-2). In Proceedings
of the 6th International Natural Language Genera-
tion Conference (INLG).
Alexander Koller, Maria Staudte, Konstantina Garoufi,
and Matthew Crocker. 2012. Enhancing referen-
tial success by tracking hearer gaze. In Proceed-
ings of the 13th Annual Meeting of the Special In-
terest Group on Discourse and Dialogue, SIGDIAL
’12, pages 30–39, Stroudsburg, PA, USA. Associa-
tion for Computational Linguistics.
Maria Staudte, Alexander Koller, Konstantina Garoufi,
and Matthew Crocker. 2012. Using listener gaze to
augment speech generation in a virtual 3D environ-
ment. In Proceedings of the 34th Annual Meeting of
the Cognitive Science Society (CogSci), Sapporo.
</reference>
<page confidence="0.997654">
817
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.423911">
<title confidence="0.999453">The Impact of Listener Gaze on Predicting Reference Resolution</title>
<author confidence="0.982985">Maria</author>
<affiliation confidence="0.947078">Spoken Interaction Group, Saarland University, Saarbr¨ucken,</affiliation>
<address confidence="0.974489">of Linguistics, University of Potsdam, Potsdam,</address>
<email confidence="0.7048585">||</email>
<abstract confidence="0.997693523809524">We investigate the impact of listener’s gaze on predicting reference resolution in situated interactions. We extend an existing model that predicts to which entity in the environment listeners will resolve a referring expression (RE). Our model makes use of features that capture which objects were looked at and for how long, reflecting listeners’ visual behavior. We improve a probabilistic model that considers a basic set of features for monitoring listeners’ movements in a virtual environment. Particularly, in complex referential scenes, where more objects next to the target are possible referents, gaze turns out to be beneficial and helps deciphering listeners’ intention. We evaluate performance at several prediction times before the listener performs an action, obtaining a highly significant accuracy gain.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Susan E Brennan</author>
<author>Katharina S Schuhmann</author>
<author>Karla M Batres</author>
</authors>
<title>Entrainment on the move and in the lab: The walking around corpus.</title>
<date>2013</date>
<booktitle>In Proceedings of the 35th Annual Conference of the Cognitive Science Society,</booktitle>
<location>Berlin, Germany.</location>
<contexts>
<context position="1947" citStr="Brennan et al. (2013)" startWordPosition="274" endWordPosition="277">ed and understood. This phenomenon is known as grounding, it is well established in the dialogue literature (Clark, 1996), and it plays an important role in collaborative tasks and goal–oriented conversations. Solving a collaborative task in a shared environment is an effective way of studying the alignment of communication channels (Clark and Krych, 2004; Hanna and Brennan, 2007). In situated spoken conversations ambiguous linguistic expressions are common, where additional modalities are available. While Gargett et al. (2010) studied instruction giving and following in virtual environments, Brennan et al. (2013) examined pedestrian guidance in outdoor real environments. Both studies investigate the interaction of human interlocutors but neither study exploits listeners’ eye movements. In contrast, Koller et al. (2012) designed a task in which a natural language generation (NLG) system gives instructions to a human player in virtual environment whose eye movements were tracked. They outperformed similar systems in both successful reference resolution and listener confusion. Engonopoulos et al. (2013) attempted to predict the resolution of an RE, achieving good performance by combining two probabilisti</context>
</contexts>
<marker>Brennan, Schuhmann, Batres, 2013</marker>
<rawString>Susan E. Brennan, Katharina S. Schuhmann, and Karla M. Batres. 2013. Entrainment on the move and in the lab: The walking around corpus. In Proceedings of the 35th Annual Conference of the Cognitive Science Society, Berlin, Germany.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
<author>Meredyth A Krych</author>
</authors>
<title>Speaking while monitoring addressees for understanding.</title>
<date>2004</date>
<journal>Journal of Memory and Language,</journal>
<volume>50</volume>
<issue>1</issue>
<contexts>
<context position="1683" citStr="Clark and Krych, 2004" startWordPosition="238" endWordPosition="241">valuate performance at several prediction times before the listener performs an action, obtaining a highly significant accuracy gain. 1 Introduction Speakers tend to follow the listener’s behavior in order to determine whether their communicated message was received and understood. This phenomenon is known as grounding, it is well established in the dialogue literature (Clark, 1996), and it plays an important role in collaborative tasks and goal–oriented conversations. Solving a collaborative task in a shared environment is an effective way of studying the alignment of communication channels (Clark and Krych, 2004; Hanna and Brennan, 2007). In situated spoken conversations ambiguous linguistic expressions are common, where additional modalities are available. While Gargett et al. (2010) studied instruction giving and following in virtual environments, Brennan et al. (2013) examined pedestrian guidance in outdoor real environments. Both studies investigate the interaction of human interlocutors but neither study exploits listeners’ eye movements. In contrast, Koller et al. (2012) designed a task in which a natural language generation (NLG) system gives instructions to a human player in virtual environme</context>
</contexts>
<marker>Clark, Krych, 2004</marker>
<rawString>Herbert H. Clark and Meredyth A. Krych. 2004. Speaking while monitoring addressees for understanding. Journal of Memory and Language, 50(1):62–81, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Herbert H Clark</author>
</authors>
<title>Using Language.</title>
<date>1996</date>
<publisher>Cambridge University Press,</publisher>
<contexts>
<context position="1447" citStr="Clark, 1996" startWordPosition="202" endWordPosition="203">rs’ movements in a virtual environment. Particularly, in complex referential scenes, where more objects next to the target are possible referents, gaze turns out to be beneficial and helps deciphering listeners’ intention. We evaluate performance at several prediction times before the listener performs an action, obtaining a highly significant accuracy gain. 1 Introduction Speakers tend to follow the listener’s behavior in order to determine whether their communicated message was received and understood. This phenomenon is known as grounding, it is well established in the dialogue literature (Clark, 1996), and it plays an important role in collaborative tasks and goal–oriented conversations. Solving a collaborative task in a shared environment is an effective way of studying the alignment of communication channels (Clark and Krych, 2004; Hanna and Brennan, 2007). In situated spoken conversations ambiguous linguistic expressions are common, where additional modalities are available. While Gargett et al. (2010) studied instruction giving and following in virtual environments, Brennan et al. (2013) examined pedestrian guidance in outdoor real environments. Both studies investigate the interaction</context>
</contexts>
<marker>Clark, 1996</marker>
<rawString>Herbert H. Clark. 1996. Using Language. Cambridge University Press, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nikos Engonopoulos</author>
<author>Martin Villalba</author>
<author>Ivan Titov</author>
<author>Alexander Koller</author>
</authors>
<title>Predicting the resolution of referring expressions from user behavior.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<location>Seattle.</location>
<contexts>
<context position="2444" citStr="Engonopoulos et al. (2013)" startWordPosition="347" endWordPosition="350">re available. While Gargett et al. (2010) studied instruction giving and following in virtual environments, Brennan et al. (2013) examined pedestrian guidance in outdoor real environments. Both studies investigate the interaction of human interlocutors but neither study exploits listeners’ eye movements. In contrast, Koller et al. (2012) designed a task in which a natural language generation (NLG) system gives instructions to a human player in virtual environment whose eye movements were tracked. They outperformed similar systems in both successful reference resolution and listener confusion. Engonopoulos et al. (2013) attempted to predict the resolution of an RE, achieving good performance by combining two probabilistic log–linear models: a semantic model Psem that analyzes the semantics of a given instruction, and an observational model Pobs that inspects the player’s behavior. However, they did not include listener’s gaze. They observed that the accuracy for Pobs reaches its highest point at a relatively late stage in an interaction. Similar observations are reported by Kennington and Schlangen (2014): they compare listener gaze and an incremental update model (IUM) as predictors for the resolution of an</context>
<context position="4865" citStr="Engonopoulos et al. (2013)" startWordPosition="723" endWordPosition="726"> instruction. While the linguistic material in instructions carries a lot of information, even completely unambiguous descriptions may be misunderstood. A robust NLG system should be capable of detecting misunderstandings and preventing its users from making mistakes. Language comprehension is mirrored by interlocutors’ non verbal behavior, and this can help when decoding the listener’s interpretation. Precise automatic estimates may be crucial when developing a real–time NLG system, as such a mechanism would be more robust and capable at avoiding misunderstandings. As mentioned in section 1, Engonopoulos et al. (2013) propose two statistical models to solve that problem: a semantic model Psem based on the linguistic content, and an observation model Pobs based on listener behavior features. More formally, let’s assume a system generates an expression r that aims to identify a target object ot among a set O of possible objects, i.e. those available in the scene view. Given the state of the world s at time point t, and the observed listener’s behavior Q(t) of the user at time t &gt; tb (where tb denotes the end of an interaction), we estimated the conditional probability p(op|r, s, Q(t)) that indicates how prob</context>
<context position="6254" citStr="Engonopoulos et al. (2013)" startWordPosition="962" endWordPosition="965">C Psem(op|r, s)Pobs(op|Q(t)) We expect an NLG system to compute and output an expression that maximizes the probability of op. Due to the dynamic nature of our scenarios, we also require the probability value to be updated at certain time intervals throughout an interaction. Tracking the probability changes over time, an NLG system could proactively react to changes in its environment. Henderson and Smith (2007) show that accounting for both fixation location and duration are key to identify a player’s focus of attention. The technical contribution of this paper is to extend the Pobs model of Engonopoulos et al. (2013) with gaze features to account for these variables. 3 Episodes and feature functions The data for our experiment was obtained from the GIVE Challenge (Koller et al., 2010), an interactive task in a 3D virtual environment in which a human player (instruction follower, IF) is navigated through a maze, locating and pressing buttons in a predefined order aiming to unlock a safe. While pressing the wrong button in the sequences doesn’t always have negative effects, it can also lead to restarting or losing the game. The IF receives instructions from either another player or an automated system (inst</context>
<context position="8118" citStr="Engonopoulos et al. (2013)" startWordPosition="1274" endWordPosition="1277">t sequence of recorded behavior states, beginning with a manipulation instruction generated by the IG and ending with a button press by the IF (at time point tb). In order to make sure that the recorded button press is a direct response to the IG’s instruction, an episode is defined such that it doesn’t contain further utterances after the first one. Both the target intended by the IG (ot) and the one selected by the IF (op) were recorded. Figure 1: The structure of the interactions. Figure 1 depicts the structure of an episode when eye-tracking data is available. Each episode P(op) Following Engonopoulos et al. (2013) we make the simplifying assumption that the distribution of the probability among the possible targets is uniform and obtain: 813 can be seen as a sequence of interaction states (s1, ... , sn), and each state has a set of visible objects ({o1, o2, o3, o10, o12}). We then compute the subset of fixated objects ({o2, o3, o12}). We update both sets of visible and fixated objects dynamically in each interaction state with respect to the change in visual scene and the corresponding record of the listener’s eye movements. We developed feature functions over these episodes. Along with the episode’s d</context>
<context position="13211" citStr="Engonopoulos et al. (2013)" startWordPosition="2133" endWordPosition="2137"> episodes at time −dtest. This pair of training and test parameters is denoted as the tuple (dtrain, dtest). 814 5 Dataset We evaluated the performance of our improved model over data collected by Staudte et al. (2012) using the GIVE Challenge platform. Both training and testing were performed over a subset of the data obtained during a collection task involving worlds created by Gargett et al. (2010), designed to provide the task with varying levels of difficulty. This corpus provides recorded eye-tracking data, collected with a remote faceLAB system. In contrast, the evaluation presented by Engonopoulos et al. (2013) uses only games collected for the GIVE 2 and GIVE 2.5 challenges, for which no eye-tracking data is available. Here, we do not investigate the performance of Psem and concentrate on the direct comparison between Pobs and PEobs in order to find out if and when eye–tracking can improve the prediction of an RE resolution. We further filtered our corpus in order to remove noisy games following Koller et al. (2012), considering only interactions for which the eyetracker calibration detected inspection of either the target or another button object in at least 75% of all referential scenes in an int</context>
<context position="16205" citStr="Engonopoulos et al. (2013)" startWordPosition="2647" endWordPosition="2650">nstance was considered easy if no more than three objects were visible at that point, or hard otherwise. For −dtest = 0, 59.5% of all instances are considered hard, but this proportion decreases as −dtest increases. At −dtest = −6, the number of hard instances amounts to 72.7%. We evaluated both the original Pobs model and the PEobs model on the same data set. We also calculated accuracy values for each feature function, in order to test whether a single function could outperform Pobs. We included as baselines two versions of Pobs using only the features InRoom and Visual Salience proposed by Engonopoulos et al. (2013). The accuracy results on Figure 2 show our observations for −6 ≤ −dtrain ≤ −2 and −dtrain ≤ −dtest ≤ 0. The graph shows that PEobs performs similarly as Pobs on the easy instances, i.e. the eye-tracking features are not contributing in those scenarios. However, PEobs shows a consistent improvement on the hard instances over Pobs. For each permutation of the training and testing parameters (dtrain, dtest), we obtain a set of episodes that fulfil the length criteria for the given parameters. We apply Pobs and PEobs on the obtained set of instances and measure two corresponding accuracy values. </context>
</contexts>
<marker>Engonopoulos, Villalba, Titov, Koller, 2013</marker>
<rawString>Nikos Engonopoulos, Martin Villalba, Ivan Titov, and Alexander Koller. 2013. Predicting the resolution of referring expressions from user behavior. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), Seattle.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew Gargett</author>
<author>Konstantina Garoufi</author>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
</authors>
<title>The give2 corpus of giving instructions in virtual environments.</title>
<date>2010</date>
<journal>European Language Resources Association (ELRA).</journal>
<booktitle>In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10),</booktitle>
<location>Valletta, Malta,</location>
<contexts>
<context position="1859" citStr="Gargett et al. (2010)" startWordPosition="262" endWordPosition="265"> listener’s behavior in order to determine whether their communicated message was received and understood. This phenomenon is known as grounding, it is well established in the dialogue literature (Clark, 1996), and it plays an important role in collaborative tasks and goal–oriented conversations. Solving a collaborative task in a shared environment is an effective way of studying the alignment of communication channels (Clark and Krych, 2004; Hanna and Brennan, 2007). In situated spoken conversations ambiguous linguistic expressions are common, where additional modalities are available. While Gargett et al. (2010) studied instruction giving and following in virtual environments, Brennan et al. (2013) examined pedestrian guidance in outdoor real environments. Both studies investigate the interaction of human interlocutors but neither study exploits listeners’ eye movements. In contrast, Koller et al. (2012) designed a task in which a natural language generation (NLG) system gives instructions to a human player in virtual environment whose eye movements were tracked. They outperformed similar systems in both successful reference resolution and listener confusion. Engonopoulos et al. (2013) attempted to p</context>
<context position="12989" citStr="Gargett et al. (2010)" startWordPosition="2100" endWordPosition="2103">action tb, we expect our model to reliably predict which button the user will select. Analogously, we define accuracy at testing time −dtest as the percentage of correctly predicted target objects when predicting over episodes at time −dtest. This pair of training and test parameters is denoted as the tuple (dtrain, dtest). 814 5 Dataset We evaluated the performance of our improved model over data collected by Staudte et al. (2012) using the GIVE Challenge platform. Both training and testing were performed over a subset of the data obtained during a collection task involving worlds created by Gargett et al. (2010), designed to provide the task with varying levels of difficulty. This corpus provides recorded eye-tracking data, collected with a remote faceLAB system. In contrast, the evaluation presented by Engonopoulos et al. (2013) uses only games collected for the GIVE 2 and GIVE 2.5 challenges, for which no eye-tracking data is available. Here, we do not investigate the performance of Psem and concentrate on the direct comparison between Pobs and PEobs in order to find out if and when eye–tracking can improve the prediction of an RE resolution. We further filtered our corpus in order to remove noisy </context>
</contexts>
<marker>Gargett, Garoufi, Koller, Striegnitz, 2010</marker>
<rawString>Andrew Gargett, Konstantina Garoufi, Alexander Koller, and Kristina Striegnitz. 2010. The give2 corpus of giving instructions in virtual environments. In Proceedings of the Seventh International Conference on Language Resources and Evaluation (LREC’10), Valletta, Malta, May. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joy E Hanna</author>
<author>Susan E Brennan</author>
</authors>
<title>Speakers’ eye gaze disambiguates referring expressions early during face-to-face conversation.</title>
<date>2007</date>
<journal>Journal of Memory and Language,</journal>
<volume>57</volume>
<issue>4</issue>
<contexts>
<context position="1709" citStr="Hanna and Brennan, 2007" startWordPosition="242" endWordPosition="245">several prediction times before the listener performs an action, obtaining a highly significant accuracy gain. 1 Introduction Speakers tend to follow the listener’s behavior in order to determine whether their communicated message was received and understood. This phenomenon is known as grounding, it is well established in the dialogue literature (Clark, 1996), and it plays an important role in collaborative tasks and goal–oriented conversations. Solving a collaborative task in a shared environment is an effective way of studying the alignment of communication channels (Clark and Krych, 2004; Hanna and Brennan, 2007). In situated spoken conversations ambiguous linguistic expressions are common, where additional modalities are available. While Gargett et al. (2010) studied instruction giving and following in virtual environments, Brennan et al. (2013) examined pedestrian guidance in outdoor real environments. Both studies investigate the interaction of human interlocutors but neither study exploits listeners’ eye movements. In contrast, Koller et al. (2012) designed a task in which a natural language generation (NLG) system gives instructions to a human player in virtual environment whose eye movements wer</context>
</contexts>
<marker>Hanna, Brennan, 2007</marker>
<rawString>Joy E. Hanna and Susan E. Brennan. 2007. Speakers’ eye gaze disambiguates referring expressions early during face-to-face conversation. Journal of Memory and Language, 57(4):596–615, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John M Henderson</author>
<author>Tim J Smith</author>
</authors>
<title>How are eye fixation durations controlled during scene viewing? further evidence from a scene onset delay paradigm.</title>
<date>2007</date>
<journal>Visual Cognition,</journal>
<pages>17--6</pages>
<contexts>
<context position="6043" citStr="Henderson and Smith (2007)" startWordPosition="925" endWordPosition="928">ability p(op|r, s, Q(t)) that indicates how probable it is that the listener resolved r to op. This probability can be also expressed as follows: Psem(op|r, s)Pobs(op|Q(t)) P(op|r, s, Q(t)) OC P(op|r, s, Q(t)) OC Psem(op|r, s)Pobs(op|Q(t)) We expect an NLG system to compute and output an expression that maximizes the probability of op. Due to the dynamic nature of our scenarios, we also require the probability value to be updated at certain time intervals throughout an interaction. Tracking the probability changes over time, an NLG system could proactively react to changes in its environment. Henderson and Smith (2007) show that accounting for both fixation location and duration are key to identify a player’s focus of attention. The technical contribution of this paper is to extend the Pobs model of Engonopoulos et al. (2013) with gaze features to account for these variables. 3 Episodes and feature functions The data for our experiment was obtained from the GIVE Challenge (Koller et al., 2010), an interactive task in a 3D virtual environment in which a human player (instruction follower, IF) is navigated through a maze, locating and pressing buttons in a predefined order aiming to unlock a safe. While press</context>
</contexts>
<marker>Henderson, Smith, 2007</marker>
<rawString>John M. Henderson and Tim J. Smith. 2007. How are eye fixation durations controlled during scene viewing? further evidence from a scene onset delay paradigm. Visual Cognition, 17(6-7):1055–1082.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Casey Kennington</author>
<author>David Schlangen</author>
</authors>
<title>Comparing listener gaze with predictions of an incremental reference resolution model.</title>
<date>2014</date>
<booktitle>RefNet workshop on Psychological and Computational Models of Reference Comprehension and Production.</booktitle>
<contexts>
<context position="2939" citStr="Kennington and Schlangen (2014)" startWordPosition="423" endWordPosition="426">re tracked. They outperformed similar systems in both successful reference resolution and listener confusion. Engonopoulos et al. (2013) attempted to predict the resolution of an RE, achieving good performance by combining two probabilistic log–linear models: a semantic model Psem that analyzes the semantics of a given instruction, and an observational model Pobs that inspects the player’s behavior. However, they did not include listener’s gaze. They observed that the accuracy for Pobs reaches its highest point at a relatively late stage in an interaction. Similar observations are reported by Kennington and Schlangen (2014): they compare listener gaze and an incremental update model (IUM) as predictors for the resolution of an RE, noting that gaze is more accurate before the onset of an utterance, whereas the model itself is more accurate afterwards. In this paper we report on the extension of the Pobs model to also consider listener’s visual behaviour. More precisely we implement features that encode listener’s eye movement patterns and evaluate their performance on a multi–modal data collection. We show that such a model as it takes an additional communication channel provides more accurate predictions especia</context>
</contexts>
<marker>Kennington, Schlangen, 2014</marker>
<rawString>Casey Kennington and David Schlangen. 2014. Comparing listener gaze with predictions of an incremental reference resolution model. RefNet workshop on Psychological and Computational Models of Reference Comprehension and Production.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
<author>Andrew Gargett</author>
<author>Donna Byron</author>
<author>Justine Cassell</author>
<author>Robert Dale</author>
<author>Johanna Moore</author>
<author>Jon Oberlander</author>
</authors>
<date>2010</date>
<booktitle>Report on the Second NLG Challenge on Generating Instructions in Virtual Environments (GIVE-2). In Proceedings of the 6th International Natural Language Generation Conference (INLG).</booktitle>
<contexts>
<context position="6425" citStr="Koller et al., 2010" startWordPosition="990" endWordPosition="993">o require the probability value to be updated at certain time intervals throughout an interaction. Tracking the probability changes over time, an NLG system could proactively react to changes in its environment. Henderson and Smith (2007) show that accounting for both fixation location and duration are key to identify a player’s focus of attention. The technical contribution of this paper is to extend the Pobs model of Engonopoulos et al. (2013) with gaze features to account for these variables. 3 Episodes and feature functions The data for our experiment was obtained from the GIVE Challenge (Koller et al., 2010), an interactive task in a 3D virtual environment in which a human player (instruction follower, IF) is navigated through a maze, locating and pressing buttons in a predefined order aiming to unlock a safe. While pressing the wrong button in the sequences doesn’t always have negative effects, it can also lead to restarting or losing the game. The IF receives instructions from either another player or an automated system (instruction giver, IG). The IF’s behavior was recorded every 200ms, along with the IG’s instructions and the state of the virtual world. The result is an interaction corpus co</context>
</contexts>
<marker>Koller, Striegnitz, Gargett, Byron, Cassell, Dale, Moore, Oberlander, 2010</marker>
<rawString>Alexander Koller, Kristina Striegnitz, Andrew Gargett, Donna Byron, Justine Cassell, Robert Dale, Johanna Moore, and Jon Oberlander. 2010. Report on the Second NLG Challenge on Generating Instructions in Virtual Environments (GIVE-2). In Proceedings of the 6th International Natural Language Generation Conference (INLG).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Maria Staudte</author>
<author>Konstantina Garoufi</author>
<author>Matthew Crocker</author>
</authors>
<title>Enhancing referential success by tracking hearer gaze.</title>
<date>2012</date>
<booktitle>In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12,</booktitle>
<pages>30--39</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2157" citStr="Koller et al. (2012)" startWordPosition="304" endWordPosition="307">olving a collaborative task in a shared environment is an effective way of studying the alignment of communication channels (Clark and Krych, 2004; Hanna and Brennan, 2007). In situated spoken conversations ambiguous linguistic expressions are common, where additional modalities are available. While Gargett et al. (2010) studied instruction giving and following in virtual environments, Brennan et al. (2013) examined pedestrian guidance in outdoor real environments. Both studies investigate the interaction of human interlocutors but neither study exploits listeners’ eye movements. In contrast, Koller et al. (2012) designed a task in which a natural language generation (NLG) system gives instructions to a human player in virtual environment whose eye movements were tracked. They outperformed similar systems in both successful reference resolution and listener confusion. Engonopoulos et al. (2013) attempted to predict the resolution of an RE, achieving good performance by combining two probabilistic log–linear models: a semantic model Psem that analyzes the semantics of a given instruction, and an observational model Pobs that inspects the player’s behavior. However, they did not include listener’s gaze.</context>
<context position="13625" citStr="Koller et al. (2012)" startWordPosition="2207" endWordPosition="2210">ovide the task with varying levels of difficulty. This corpus provides recorded eye-tracking data, collected with a remote faceLAB system. In contrast, the evaluation presented by Engonopoulos et al. (2013) uses only games collected for the GIVE 2 and GIVE 2.5 challenges, for which no eye-tracking data is available. Here, we do not investigate the performance of Psem and concentrate on the direct comparison between Pobs and PEobs in order to find out if and when eye–tracking can improve the prediction of an RE resolution. We further filtered our corpus in order to remove noisy games following Koller et al. (2012), considering only interactions for which the eyetracker calibration detected inspection of either the target or another button object in at least 75% of all referential scenes in an interaction. The resulting corpus comprises 75 games, for a combined length of 8 hours. We extracted 761 episodes from this corpus, amounting to 47m 58s of recorded interactions, with an average length per episode of 3.78 seconds (σ = 3.03sec.). There are 261 episodes shorter than 2 sec., 207 in the 2-4 sec. range, 139 in the 4-6 sec. range, and 154 episodes longer than 6 sec. 6 Evaluation and results The accuracy</context>
</contexts>
<marker>Koller, Staudte, Garoufi, Crocker, 2012</marker>
<rawString>Alexander Koller, Maria Staudte, Konstantina Garoufi, and Matthew Crocker. 2012. Enhancing referential success by tracking hearer gaze. In Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue, SIGDIAL ’12, pages 30–39, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Staudte</author>
<author>Alexander Koller</author>
<author>Konstantina Garoufi</author>
<author>Matthew Crocker</author>
</authors>
<title>Using listener gaze to augment speech generation in a virtual 3D environment.</title>
<date>2012</date>
<booktitle>In Proceedings of the 34th Annual Meeting of the Cognitive Science Society (CogSci),</booktitle>
<location>Sapporo.</location>
<contexts>
<context position="7236" citStr="Staudte et al. (2012)" startWordPosition="1125" endWordPosition="1128">o unlock a safe. While pressing the wrong button in the sequences doesn’t always have negative effects, it can also lead to restarting or losing the game. The IF receives instructions from either another player or an automated system (instruction giver, IG). The IF’s behavior was recorded every 200ms, along with the IG’s instructions and the state of the virtual world. The result is an interaction corpus comprising over 2500 games and spanning over 340 hours of interactions. These interactions were mainly collected during the GIVE-2 and the GIVE-2.5 challenges. A laboratory study conducted by Staudte et al. (2012) comprises a data collection that contains eye-tracking records for the IF. Although the corpus contains both successful and unsuccessful games, we have decided to consider only the successful ones. We define an episode over this corpus as a typically short sequence of recorded behavior states, beginning with a manipulation instruction generated by the IG and ending with a button press by the IF (at time point tb). In order to make sure that the recorded button press is a direct response to the IG’s instruction, an episode is defined such that it doesn’t contain further utterances after the fi</context>
<context position="12803" citStr="Staudte et al. (2012)" startWordPosition="2070" endWordPosition="2073">are learned via optimization with the L-BFGS algorithm. By training our model to correctly predict a target button based only on data observed up until −dtrain seconds before the actual action tb, we expect our model to reliably predict which button the user will select. Analogously, we define accuracy at testing time −dtest as the percentage of correctly predicted target objects when predicting over episodes at time −dtest. This pair of training and test parameters is denoted as the tuple (dtrain, dtest). 814 5 Dataset We evaluated the performance of our improved model over data collected by Staudte et al. (2012) using the GIVE Challenge platform. Both training and testing were performed over a subset of the data obtained during a collection task involving worlds created by Gargett et al. (2010), designed to provide the task with varying levels of difficulty. This corpus provides recorded eye-tracking data, collected with a remote faceLAB system. In contrast, the evaluation presented by Engonopoulos et al. (2013) uses only games collected for the GIVE 2 and GIVE 2.5 challenges, for which no eye-tracking data is available. Here, we do not investigate the performance of Psem and concentrate on the direc</context>
</contexts>
<marker>Staudte, Koller, Garoufi, Crocker, 2012</marker>
<rawString>Maria Staudte, Alexander Koller, Konstantina Garoufi, and Matthew Crocker. 2012. Using listener gaze to augment speech generation in a virtual 3D environment. In Proceedings of the 34th Annual Meeting of the Cognitive Science Society (CogSci), Sapporo.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>