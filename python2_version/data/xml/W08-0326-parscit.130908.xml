<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000593">
<title confidence="0.920385">
MATREX: the DCU MT System for WMT 2008
</title>
<author confidence="0.999098">
John Tinsley, Yanjun Ma, Sylwia Ozdowska, Andy Way
</author>
<affiliation confidence="0.984929">
National Centre for Language Technology
Dublin City University
</affiliation>
<address confidence="0.85543">
Dublin 9, Ireland
</address>
<email confidence="0.980938">
jjtinsley, yma, sozdowska, away}@computing.dcu.ie
</email>
<sectionHeader confidence="0.99553" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999801894736842">
In this paper, we give a description of the ma-
chine translation system developed at DCU
that was used for our participation in the eval-
uation campaign of the Third Workshop on
Statistical Machine Translation at ACL 2008.
We describe the modular design of our data-
driven MT system with particular focus on
the components used in this participation. We
also describe some of the significant modules
which were unused in this task.
We participated in the EuroParl task for the
following translation directions: Spanish–
English and French–English, in which we em-
ployed our hybrid EBMT-SMT architecture to
translate. We also participated in the Czech–
English News and News Commentary tasks
which represented a previously untested lan-
guage pair for our system. We report results
on the provided development and test sets.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999940166666667">
In this paper, we present the Data-Driven MT sys-
tems developed at DCU, MATREX (Machine Trans-
lation using Examples). This system is a hybrid sys-
tem which exploits EBMT and SMT techniques to
build a combined translation model.
We participated in both the French–English and
Spanish–English EuroParl tasks. In these two tasks,
we monolingually chunk both source and target
sides of the dataset using a marker-based chunker
(Gough and Way, 2004). We then align these chunks
using a dynamic programming, edit-distance-style
algorithm and combine them with phrase-based
SMT-style chunks into a single translation model.
We also participated in the Czech–English News
Commentary and News tasks. This language pair
represents a new challenge for our system and pro-
vides a good test of its flexibility.
The remainder of this paper is organised as fol-
lows: Section 2 details the various components of
our system, in particular the chunking and chunk
alignment strategies used for the shared task. In Sec-
tion 3, we outline the complete system setup for the
shared task, and in Section 4 we give some results
and discussion thereof.
</bodyText>
<sectionHeader confidence="0.969022" genericHeader="method">
2 The MATREX System
</sectionHeader>
<bodyText confidence="0.996355166666667">
The MATREX system is a modular hybrid data-
driven MT system, built following established De-
sign Patterns, which exploits aspects of both the
EBMT and SMT paradigms. It consists of a num-
ber of extendible and re-implementable modules, the
most significant of which are:
</bodyText>
<listItem confidence="0.9974216">
• Word Alignment Module: outputs a set of word
alignments given a parallel corpus,
• Chunking Module: outputs a set of chunks
given an input corpus,
• Chunk Alignment Module: outputs aligned
chunk pairs given source and target chunks ex-
tracted from comparable corpora,
• Decoder: returns optimal translation given a
set of aligned sentence, chunk/phrase and word
pairs.
</listItem>
<bodyText confidence="0.9988785">
In some cases, these modules may comprise
wrappers around pre-existing software. For exam-
ple, our system configuration for the shared task
incorporates a wrapper around GIZA++ (Och and
Ney, 2003) for word alignment and a wrapper
around Moses (Koehn et al., 2007) for decoding. It
</bodyText>
<page confidence="0.939894">
171
</page>
<bodyText confidence="0.9537643">
Proceedings of the Third Workshop on Statistical Machine Translation, pages 171–174,
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
should be noted, however, that the complete system Assuming that the parameters P(etk|f1k) are
is not limited to using only these specific module known, the most likely alignment is computed by
choices. The following subsections describe those a simple dynamic-programming algorithm.1
modules unique to our system. Instead of using an Expectation-Maximization al-
2.1 Marker-Based Chunking gorithm to estimate these parameters, as commonly
The chunking module used for the shared task is done when performing word alignment (Brown
based on the Marker Hypothesis, a psycholinguistic et al., 1993; Och and Ney, 2003), we directly com-
constraint which posits that all languages are marked pute these parameters by relying on the information
for surface syntax by a specific closed set of lex- contained within the chunks. The conditional prob-
emes or morphemes which signify context. Using a ability P(etk|f1k) can be computed in several ways.
set of closed-class (or “marker”) words for a particu- In our experiments, we have considered three main
lar language, such as determiners, prepositions, con- sources of knowledge: (i) word-to-word translation
junctions and pronouns, sentences are segmented probabilities, (ii) word-to-word cognates, and (iii)
into chunks. A chunk is created at each new occur- chunk labels. These sources of knowledge are com-
rence of a marker word with the restriction that each bined in a log-linear framework. The weights of
chunk must contain at least one content (or non- the log-linear model are not optimised; we experi-
marker) word. An example of this chunking strategy mented with different sets of parameters and did not
for English and Spanish is given in Figure 1. find any significant difference as long as the weights
2.2 Chunk Alignment stay in the interval [0.5 − 1.5]. Outside this inter-
In order to align the chunks obtained by the chunk- val, the quality of the model decreases. More details
ing procedures described in Section 2.1, we make about the combination of knowledge sources can be
use of an “edit-distance-style” dynamic program- found in (Stroppa and Way, 2006).
ming alignment algorithm.
In the following, a denotes an alignment between
a target sequence e consisting of I chunks and a
source sequence f consisting of J chunks. Given
these sequences of chunks, we are looking for the
most likely alignment a:
a� = argmax P(a|e, f) = argmax P(a, e|f).
a a
We first consider alignments such as those ob-
tained by an edit-distance algorithm, i.e.
a = (t1, s1)(t2, s2) ... (tn, sn),
with bk E Q1, n�, tk E Q0, I� and sk E Q0, J�, and
bk &lt; k′:
tk &lt; tk′ or tk′ = 0,
sk &lt; sk′ or sk′ = 0,
where tk = 0 (resp. sk = 0) denotes a non-aligned
target (resp. source) chunk.
We then assume the following model:
P(a, e|f) = HkP(tk, sk, e|f) = HkP(etk|f1k),
where P(e0|fj) (resp. P(ei|f0)) denotes an “inser-
tion” (resp. “deletion”) probability.
172
2.3 Unused Modules
There are numerous other features available in our
system which, due to time constraints, were not ex-
ploited for the purposes of the shared task. They
</bodyText>
<listItem confidence="0.917174909090909">
include:
• Word packing (Ma et al., 2007): a bilingually
motivated packing of words that changes the
basic unit of the alignment process in order to
simplify word alignment.
• Supertagging (Hassan et al., 2007b): incorpo-
rating lexical syntactic descriptions, in the form
of supertags, to the language model and target
side of the translation model in order to better
inform decoding.
• Source-context features (Stroppa et al., 2007):
</listItem>
<figureCaption confidence="0.584117">
use memory-based classification to incorporate
context-informed features on the source side of
the translation model.
• Treebank-based phrase extraction (Tinsley
et al., 2007): extract word and phrase align-
ments based on linguistically informed sub-
sentential alignment of the parallel data.
1This algorithm is actually a classical edit-distance al-
gorithm in which distances are replaced by opposite-log-
conditional probabilities.
</figureCaption>
<figure confidence="0.920246">
English: [I voted] [in favour] [of the strategy presented] [by the council] [concerning relations] [with
Mediterranean countries]
Spanish: [He votado] [a favor] [de la estrategia presentada] [por el consejo] [relativa las relaciones]
[con los paises mediterran´eos]
</figure>
<figureCaption confidence="0.99569">
Figure 1: English and Spanish Marker-Based chunking
</figureCaption>
<table confidence="0.9995385">
Filter criteria es–en fr–en cz–en
Initial Total 1258778 1288074 1096941
Blank Lines 5632 4200 2
Length 6794 8361 2922
Fertility 120 82 1672
Final Total 1246234 1275432 1092345
</table>
<tableCaption confidence="0.999883">
Table 1: Summary of pre-processing on training data.
</tableCaption>
<sectionHeader confidence="0.98001" genericHeader="method">
3 Shared Task Setup
</sectionHeader>
<bodyText confidence="0.999473333333333">
The following section describes the system setup
using the Spanish–English and French–English Eu-
roParl, and Czech–English CzEng training data.
</bodyText>
<subsectionHeader confidence="0.999737">
3.1 Pre-processing
</subsectionHeader>
<bodyText confidence="0.9999835">
For all tasks we initially tokenised the data (Czech
data was already tokenised) and removed blank
lines. We then filtered out sentence pairs based on
length (&gt;100 words) and fertility (9:1 word ratio).
Finally we lowercased the data. Details of this pre-
processing are given in Table 1.
</bodyText>
<subsectionHeader confidence="0.997567">
3.2 System Configuration
</subsectionHeader>
<bodyText confidence="0.9999734">
As mentioned in Section 2, our word alignment
module employs a wrapper around GIZA++.
We built a 5-gram language model based the tar-
get side of the training data. This was done using
the SRI Language Modelling toolkit (Stolcke, 2002)
employing linear interpolation and modified Kneser-
Ney discounting (Chen and Goodman, 1996).
Our phrase-table comprised a combination of
marker-based chunk pairs2, extracted as described
in Sections 2.1 and 2.2, and word-alignment-based
phrase pairs extracted using the “grow-diag-final”
method of Koehn et al. (2003), with a maximum
phrase length of 7 words. Phrase translation proba-
bilities were estimated by relative frequency over all
phrase pairs and were combined with other features,
</bodyText>
<footnote confidence="0.962742666666667">
2This module was omitted from the Czech–English system
as we have yet to verify whether marker-based chunking is ap-
propriate for Czech.
</footnote>
<table confidence="0.99953775">
System BLEU (-EBMT) BLEU (+EBMT)
es–en 0.3283 0.3287
fr–en 0.2768 0.2770
cz–en 0.2235 -
</table>
<tableCaption confidence="0.987851">
Table 2: Summary of results on developments sets de-
vtest2006 for EuroParl tasks and nc-test2007 for cz–en
tasks.
</tableCaption>
<table confidence="0.9997024">
System BLEU (-EBMT) BLEU (+EBMT)
es–en 0.3274 0.3285
fr–en 0.3163 0.3174
cz–en (news) 0.1458 -
cz–en (nc) 0.2217 -
</table>
<tableCaption confidence="0.999869">
Table 3: Summary of results on 2008 test data.
</tableCaption>
<bodyText confidence="0.999721625">
such as a reordering model, in a log-linear combina-
tion of functions.
We tuned our system on the development set de-
vtest2006 for the EuroParl tasks and on nc-test2007
for Czech–English, using minimum error-rate train-
ing (Och, 2003) to optimise BLEU score.
Finally, we carried out decoding using a wrapper
around the Moses decoder.
</bodyText>
<subsectionHeader confidence="0.999719">
3.3 Post-processing
</subsectionHeader>
<bodyText confidence="0.999959875">
Case restoration was carried out by training the sys-
tem outlined above - without the EBMT chunk ex-
traction - to translate from the lowercased version
of the applicable target language training data to the
truecased version. We have previously shown this
approach to be very effective for both case and punc-
tuation restoration (Hassan et al., 2007a). The trans-
lations were then detokenised.
</bodyText>
<sectionHeader confidence="0.999983" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999950166666667">
The system output is evaluated with respect to
BLEU score. Results on the development sets and
test sets for each task are given in Tables 2 and 3
respectively, where “-EBMT” indicates that EBMT
chunk modules were not used, and “+EBMT” indi-
cates that they were used.
</bodyText>
<page confidence="0.7284825">
173
4.1 Discussion Hassan, H., Sima’an, K., and Way, A. (2007b). Su-
</page>
<bodyText confidence="0.93594596">
Those configurations which incorporated the EBMT pertagged Phrase-based Statistical Machine Transla-
chunks improved slightly over those which did not. tion. In Proceedings of the 45th Annual Meeting of the
Groves (2007) has shown previously that combin- Association for Computational Linguistics (ACL’07),
ing EBMT and SMT translation models can lead to pages 288–295, Prague, Czech Republic.
considerable improvement over the baseline systems Koehn, P., Hoang, H., Birch, A., Callison-Burch, C., Fed-
from which they are derived. The results achieved erico, M., Bertoldi, N., Cowan, B., Shen, W., Moran,
here lead us to believe that on such a large scale C., Zens, R., Dyer, C., Bojar, O., Constantin, A., and
there may be a more effective way to incorporate the Herbst, E. (2007). Moses: Open Source Toolkit for
EBMT chunks. Statistical Machine Translation. In Annual Meeting of
Previous work has shown the EBMT chunks to the Association for Computational Linguistics (ACL),
have higher precision than their SMT counterparts, demonstration session, pages 177–180, Prague, Czech
but they lack sufficient recall when used in isola- Republic.
tion (Groves, 2007). We believe that increasing their Koehn, P., Och, F. J., and Marcu, D. (2003). Statisti-
influence in the translation model may lead to im- cal Phrase-Based Translation. In Proceedings of the
proved translation accuracy. One experiment to this 2003 Conference ofthe North American Chapter of the
effect would be to add the EBMT chunks as a sep- Association for Computational Linguistics on Human
arate phrase table in the log-linear model and allow Language Technology (NAACL ’03), pages 48–54, Ed-
the decoder to chose when to use them. monton, Canada.
Finally, we intend to exploit the unused modules Ma, Y., Stroppa, N., and Way, A. (2007). Boostrap-
of the system in future experiments to investigate ping Word Alignment via Word Packing. In Proceed-
their effects on the tasks presented here. ings of the 45th Annual Meeting of the Association for
Acknowledgments Computational Linguistics (ACL’07), pages 304–311,
This work is supported by Science Foundation Ireland Prague, Czech Republic.
(grant nos. 05/RF/CMS064 and OS/IN/1732). Thanks Och, F. (2003). Minimum error rate training in statistical
also to the reviewers for their insightful comments and machine translation. In Proceedings of the 41st Annual
</bodyText>
<figureCaption confidence="0.88047572">
suggestions. Meeting ofthe Association for Computational Linguis-
References tics (ACL), pages 160–167, Sapporo, Japan., Sapporo,
Brown, P. F., Pietra, S. A. D., Pietra, V. J. D., and Mercer, Japan.
R. L. (1993). The mathematics of statistical machine Och, F. J. and Ney, H. (2003). A Systematic Comparison
translation: Parameter estimation. Computational Lin- of Various Statistical Alignment Models. Computa-
guistics, 19(2):263–311. tional Linguistics, 29(1):19–51.
Chen, S. F. and Goodman, J. (1996). An Empirical Study Stolcke, A. (2002). SRILM - An Extensible Language
of Smoothing Techniques for Language Modeling. In Modeling Toolkit. In Proceedings of the Interna-
Proceedings of the Thirty-Fourth Annual Meeting of tional Conference Spoken Language Processing, Den-
the Association for Computational Linguistics, pages ver, CO.
310–318, San Francisco, CA. Stroppa, N., van den Bosch, A., and Way, A. (2007).
Gough, N. and Way, A. (2004). Robust Large-Scale Exploiting Source Similarity for SMT using Context-
EBMT with Marker-Based Segmentation. In Proceed- Informed Features. In Proceedings ofthe 11th Interna-
ings of the 10th International Conference on Theoreti- tional Conference on Theoretical and Methodological
cal and Methodological Issues in Machine Translation Issues in Machine Translation (TMI-07), pages 231–
(TMI-04), pages 95–104, Baltimore, MD. 240, Sk¨ovde, Sweden.
Groves, D. (2007). Hybrid Data-Driven Models of Ma- Stroppa, N. and Way, A. (2006). MaTrEx: the DCU ma-
chine Translation. PhD thesis, Dublin City University, chine translation system for IWSLT 2006. In Proceed-
Dublin, Ireland. ings of the International Workshop on Spoken Lan-
Hassan, H., Ma, Y., and Way, A. (2007a). MATREX: the guage Translation, pages 31–36, Kyoto, Japan.
DCU Machine Translation System for IWSLT 2007. In Tinsley, J., Hearne, M., and Way, A. (2007). Exploiting
Proceedings of the International Workshop on Spoken Parallel Treebanks to Improve Phrase-Based Statisti-
Language Translation, pages 69–75, Trento, Italy. cal Machine Translation. In Proceedings of the Sixth
174 International Workshop on Treebanks and Linguistic
Theories (TLT-07), pages 175–187, Bergen, Norway.
</figureCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.543699">
<title confidence="0.565661">the DCU MT System for WMT 2008</title>
<author confidence="0.992001">John Tinsley</author>
<author confidence="0.992001">Yanjun Ma</author>
<author confidence="0.992001">Sylwia Ozdowska</author>
<author confidence="0.992001">Andy Way</author>
<affiliation confidence="0.9930705">National Centre for Language Dublin City</affiliation>
<address confidence="0.994139">Dublin 9, Ireland</address>
<email confidence="0.983768">yma,sozdowska,</email>
<abstract confidence="0.99970025">In this paper, we give a description of the machine translation system developed at DCU that was used for our participation in the evaluation campaign of the Third Workshop on Statistical Machine Translation at ACL 2008. We describe the modular design of our datadriven MT system with particular focus on the components used in this participation. We also describe some of the significant modules which were unused in this task. participated in the for the following translation directions: Spanish– English and French–English, in which we employed our hybrid EBMT-SMT architecture to translate. We also participated in the Czech– Commentary which represented a previously untested language pair for our system. We report results on the provided development and test sets.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>