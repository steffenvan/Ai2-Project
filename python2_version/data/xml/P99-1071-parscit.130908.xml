<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003866">
<title confidence="0.9981245">
Information Fusion in the Context of Multi-Document
Summarization
</title>
<author confidence="0.995779">
Regina Barzilay and Kathleen R. McKeown Michael Elhadad
</author>
<affiliation confidence="0.9997005">
Dept. of Computer Science Dept. of Computer Science
Columbia University Ben-Gurion University
</affiliation>
<address confidence="0.910121">
New York, NY 10027, USA Beer-Sheva, Israel
</address>
<sectionHeader confidence="0.945517" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999980833333333">
We present a method to automatically generate
a concise summary by identifying and synthe-
sizing similar elements across related text from
a set of multiple documents. Our approach is
unique in its usage of language generation to
reformulate the wording of the summary.
</bodyText>
<sectionHeader confidence="0.99906" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999862607142858">
Information overload has created an acute need
for summarization. Typically, the same infor-
mation is described by many different online
documents. Hence, summaries that synthesize
common information across documents and em-
phasize the differences would significantly help
readers. Such a summary would be beneficial,
for example, to a user who follows a single event
through several newswires. In this paper, we
present research on the automatic fusion of simi-
lar information across multiple documents using
language generation to produce a concise sum-
mary.
We propose a method for summarizing a spe-
cific type of input: news articles presenting dif-
ferent descriptions of the same event. Hundreds
of news stories on the same event are produced
daily by news agencies. Repeated information
about the event is a good indicator of its impor-
tancy to the event, and can be used for summary
generation.
Most research on single document summa-
rization, particularly for domain independent
tasks, uses sentence extraction to produce a
summary (Lin and Hovy, 1997; Marcu, 1997;
Salton et al., 1991). In the case of multi-
document summarization of articles about the
same event, the original articles can include
both similar and contradictory information.
Extracting all similar sentences would produce
a verbose and repetitive summary, while ex-
tracting some similar sentences could produce
a summary biased towards some sources.
Instead, we move beyond sentence extraction,
using a comparison of extracted similar sen-
tences to select the phrases that should be in-
cluded in the summary and sentence generation
to reformulate them as new text. Our work
is part of a full summarization system (McK-
eown et al., 1999), which extracts sets of simi-
lar sentences, themes (Eskin et al., 1999), in the
first stage for input to the components described
here.
Our model for multi-document summariza-
tion represents a number of departures from
traditional language generation. Typically, lan-
guage generation systems have access to a full
semantic representation of the domain. A con-
tent planner selects and orders propositions
from an underlying knowledge base to form text
content. A sentence planner determines how to
combine propositions into a single sentence, and
a sentence generator realizes each set of com-
bined propositions as a sentence, mapping from
concepts to words and building syntactic struc-
ture. Our approach differs in the following ways:
</bodyText>
<listItem confidence="0.832118142857143">
• Content planning operates over full
sentences, producing sentence frag-
ments. Thus, content planning straddles
the border between interpretation and gen-
eration. We preprocess the similar sen-
tences using an existing shallow parser
(Collins, 1996) and a mapping to predicate-
argument structure. The content planner
finds an intersection of phrases by com-
paring the predicate-argument structures;
through this process it selects the phrases
that can adequately convey the common
information of the theme. It also orders
selected phrases and augments them with
</listItem>
<page confidence="0.991931">
550
</page>
<bodyText confidence="0.406795545454545">
On 3th of September 1995, 120 hostages were released
by Bosnian Serbs. Serbs were holding over 250 U.N. per-
sonnel. Bosnian serb leader Radovan Karadjic said he ex-
pected &amp;quot;a sign of goodwill&amp;quot; from the international com-
munity. U.S. F-16 fighter jet was shot down by Bosnian
Serbs. Electronic beacon signals, which might have been
transmitted by a downed U.S. fighter pilot in Bosnia,
were no longer being received. After six days, O&apos;Grady,
downed pilot, was rescued by Marine force. The mission
was carried out by CH-53 helicopters with an escort of
missile- and rocket-armed Cobra helicopters.
</bodyText>
<figureCaption confidence="0.97167">
Figure 1: Summary produced by our system us-
ing 12 news articles as input.
</figureCaption>
<bodyText confidence="0.995919333333333">
information needed for clarification (en-
tity descriptions, temporal references, and
newswire source references).
</bodyText>
<listItem confidence="0.916729">
• Sentence generation begins with
phrases. Our task is to produce fluent sen-
tences that combine these phrases, arrang-
ing them in novel contexts. In this process,
new grammatical constraints may be im-
posed and paraphrasing may be required.
</listItem>
<bodyText confidence="0.994124407407407">
We developed techniques to map predicate-
argument structure produced by the
content-planner to the functional represen-
tation expected by FUF/SURGE(Elhaklad,
1993; Robin, 1994) and to integrate new
constraints on realization choice, using sur-
face features in place of semantic or prag-
matic ones typically used in sentence gen-
eration.
An example summary automatically gener-
ated by the system from our corpus of themes
is shown in Figure 1. We collected a corpus
of themes, that was divided into a training por-
tion and a testing portion. We used the training
data for identification of paraphrasing rules on
which our comparison algorithm is built. The
system we describe has been fully implemented
and tested on a variety of input articles; there
are, of course, many open research issues that
we are continuing to explore.
In the following sections, we provide an
overview of existing multi-document summa-
rization systems, then we will detail our sen-
tence comparison technique, and describe the
sentence generation component. We provide ex-
amples of generated summaries and conclude
with a discussion of evaluation.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999923204081633">
Automatic summarizers typically identify and
extract the most important sentences from an
input article. A variety of approaches exist for
determining the salient sentences in the text:
statistical techniques based on word distribu-
tion (Salton et a., 1991), symbolic techniques
based on discourse structure (Marcu, 1997),
and semantic relations between words (Barzi-
lay and Elhadakl, 1997). Extraction techniques
can work only if summary sentences already ap-
pear in the article. Extraction cannot handle
the task we address, because summarization of
multiple documents requires information about
similarities and differences across articles.
While most of the summarization work has
focused on single articles, a few initial projects
have started to study multi-document summa-
rization documents. In constrained domains,
e.g., terrorism, a coherent summary of sev-
eral articles can be generated, when a detailed
semantic representation of the source text is
available. For example, information extraction
systems can be used to interpret the source
text. In this framework, (Radev and McKe-
own, 1998) use generation techniques to high-
light changes over time across input articles
about the same event. In an arbitrary domain,
statistical techniques are used to identify simi-
larities and differences across documents. Some
approaches directly exploit word distribution in
the text (Salton et al., 1991; Carbonell and
Goldstein, 1998). Recent work (Mani and Bloe-
dorn, 1997) exploits semantic relations between
text units for content representation, such as
synonymy and co-reference. A spreading acti-
vation algorithm and graph matching is used to
identify similarities and differences across doc-
uments. The output is presented as a set of
paragraphs with similar and unique words high-
lighted. However, if the same information is
mentioned several times in different documents,
much of the summary will be redundant. While
some researchers address this problem by select-
ing a subset of the repetitions (Carbonell and
Goldstein, 1998), this approach is not always
satisfactory. As we will see in the next section,
we can both eliminate redundancy from the out-
put and retain balance through the selection of
common information.
</bodyText>
<page confidence="0.972093">
551
</page>
<bodyText confidence="0.96238625">
On Friday, a U.S. F-16 fighter jet was shot down by
Bosnian Serb missile while policing the no-fly zone over
the region.
A Bosnian Serb missile shot down a U.S. F-16 over
northern Bosnia on Friday.
On the eve of the meeting, a U.S. F-16 fighter was shot
down while on a routine patrol over northern Bosnia.
O&apos;Grady&apos;s F-16 fighter jet, based in Aviano, Italy, was
shot down by a Bosnian Serb SA-6 anti-aircraft missile
last Friday and hopes had diminished for finding him
alive despite intermittent electronic signals from the area
which later turned out to be a navigational beacon.
</bodyText>
<figureCaption confidence="0.9139705">
Figure 2: A collection of similar sentences —
theme.
</figureCaption>
<sectionHeader confidence="0.970811" genericHeader="method">
3 Content Selection: Theme
Intersection
</sectionHeader>
<bodyText confidence="0.999953121212121">
To avoid redundant statements in a summary,
we could select one sentence from the set of sim-
ilar sentences that meets some criteria (e.g., a
threshold number of common content words).
Unfortunately, any representative sentence usu-
ally includes embedded phrases containing in-
formation that is not common to other similar
sentences. Therefore, we need to intersect the
theme sentences to identify the common phrases
and then generate a new sentence. Phrases pro-
duced by theme intersection will form the con-
tent of the generated summary.
Given the theme shown in Figure 2, how can
we determine which phrases should be selected
to form the summary content? For our example
theme, the problem is to determine that only
the phrase &amp;quot;On Friday, U.S. F-16 fighter jet
was shot down by a Bosnian Serb missile&amp;quot; is
common across all sentences.
The first sentence includes the clause; how-
ever, in other sentences, it appears in differ-
ent paraphrased forms, such as &amp;quot;A Bosnian
Serb missile shot down a U.S. F-16 on Fri-
day.&amp;quot;. Hence, we need to identify similari-
ties between phrases that are not identical in
wording, but do report the same fact. If para-
phrasing rules are known, we can compare the
predicate-argument structure of the sentences
and find common parts. Finally, having selected
the common parts, we must decide how to com-
bine phrases, whether additional information is
needed for clarification, and how to order the
resulting sentences to form the summary.
</bodyText>
<figureCaption confidence="0.792772">
Figure 3: DSYNT of the sentence &amp;quot;U.S. fighter
</figureCaption>
<bodyText confidence="0.691984">
was shot by missile.&amp;quot;
</bodyText>
<subsectionHeader confidence="0.9871">
3.1 An Algorithm for Theme
Intersection
</subsectionHeader>
<bodyText confidence="0.999982783783784">
In order to identify theme intersections, sen-
tences must be compared. To do this, we
need a sentence representation that emphasizes
sentence features that are relevant for com-
parison such as dependencies between sentence
constituents, while ignoring irrelevant features
such as constituent ordering. Since predicate-
argument structure is a natural way to repre-
sent constituent dependencies, we chose a de-
pendency based representation called DSYNT
(Kittredge and Mel&apos;aik, 1983). An example of
a sentence and its DSYNT tree is shown in Fig-
ure 3. Each non-auxiliary word in the sentence
has a node in the DSYNT tree, and this node is
connected to its direct dependents. Grammat-
ical features of each word are also kept in the
node. In order to facilitate comparison, words
are kept in canonical form.
In order to construct a DSYNT we first run
our sentences through Collin&apos;s robust, statisti-
cal parser (Collins, 1996). We developed a rule-
based component that transforms the phrase-
structure output of the parser to a DSYNT rep-
resentation. Functional words (determiners and
auxiliaries) are eliminated from the tree and the
corresponding syntactic features are updated.
The comparison algorithm starts with all sen-
tence trees rooted at verbs from the input
DSYNT, and traverses them recursively: if two
nodes are identical, they are added to the out-
put tree, and their children are compared. Once
a full phrase (a verb with at least two con-
stituents) has been found, it is added to the
intersection. If nodes are not identical, the
algorithm tries to apply an appropriate para-
phrasing rule from a set of rules described in
the next section. For example, if the phrases
</bodyText>
<figure confidence="0.83206775">
shoot
class: verb voice :passive
tense: past polarity: +
fighter c\kr.
class: noun 7-ek•
U.S. missile
class: noun class: noun
definite: yes
</figure>
<page confidence="0.988582">
552
</page>
<bodyText confidence="0.999930533333334">
&amp;quot;group of students&amp;quot; and &amp;quot;students&amp;quot; are com-
pared, then the omit empty head rule is appli-
cable, since &amp;quot;group&amp;quot; is an empty noun and can
be dropped from the comparison, leaving two
identical words, &amp;quot;students&amp;quot;. If there is no ap-
plicable paraphrasing rule, then the comparison
is finished and the intersection result is empty.
All the sentences in the theme are compared
in pairs. Then, these intersections are sorted
according to their frequencies and all intersec-
tions above a given threshold result in theme
intersection.
For the theme in Figure 2, the intersection
result is &amp;quot;On Friday, a U.S. F-16 fighter jet was
shot down by Bosnian Serb missile.&amp;quot;1
</bodyText>
<subsectionHeader confidence="0.9996095">
3.2 Paraphrasing Rules Derived from
Corpus Analysis
</subsectionHeader>
<bodyText confidence="0.999983766666667">
Identification of theme intersection requires col-
lecting paraphrasing patterns which occur in
our corpus. Paraphrasing is defined as alter-
native ways a human speaker can choose to
&amp;quot;say the same thing&amp;quot; by using linguistic knowl-
edge (as opposed to world knowledge) (Iordan-
slcaja et al., 1991). Paraphrasing has been
widely investigated in the generation commu-
nity (Iordanskaja et al., 1991; Robin, 1994).
(Dras, 1997) considered sets of paraphrases re-
quired for text transformation in order to meet
external constraints such as length or read-
ability. (Jacquemin et al., 1997) investigated
morphology-based paraphrasing in the context
of a term recognition task. However, there is no
general algorithm capable of identifying a sen-
tence as a paraphrase of another.
In our case, such a comparison is less difficult
since theme sentences are a priori close semanti-
cally, which significantly constrains the kinds of
paraphrasing we need to check. In order to ver-
ify this assumption, we analyzed paraphrasing
patterns through themes of our training corpus
derived from the Topic Detection and Tracking
corpus (Allan et al., 1998). Overall, 200 pairs of
sentences conveying the same information were
analyzed. We found that 85% of the paraphras-
ing is achieved by syntactic and lexical transfor-
mations. Examples of paraphrasing that require
world knowledge are presented below:
</bodyText>
<listItem confidence="0.569926">
1. &amp;quot;The Bosnian Serbs freed 121 U.N. soldiers
</listItem>
<bodyText confidence="0.952131066666667">
&apos;To be exact, the result of the algorithm is a DSYNT
that linearizes as this sentence.
last week at Zvornik&amp;quot; and &amp;quot;Bosnian Serb
leaders freed about one-third of the U.N.
personnel&amp;quot;
2. &amp;quot;Sheinbein showed no visible reaction to the
ruling.&amp;quot; and &amp;quot;Samuel Sheinbein showed no
reaction when Chief Justice Aharon Barak
read the 3-2 decision&amp;quot;
Since &amp;quot;surface&amp;quot; level paraphrasing comprises
the vast majority of paraphrases in our corpus
and is easier to identify than those requiring
world-knowledge, we studied paraphrasing pat-
terns in the corpus. We found the following
most frequent paraphrasing categories:
</bodyText>
<listItem confidence="0.989974545454546">
1. ordering of sentence components: &amp;quot;Tuesday
they met...&amp;quot; and &amp;quot;They met ... tuesday&amp;quot;;
2. main clause vs. a relative clause: &amp;quot;...a
building was devastated by the bomb&amp;quot; and
&amp;quot;...a building, devastated by the bomb&amp;quot;;
3. realization in different syntactic categories,
e.g., classifier vs. apposition: &amp;quot;Palestinian
leader Arafat&amp;quot; and &amp;quot;Arafat, palestinian
leader&amp;quot;, &amp;quot;Pentagon speaker&amp;quot; and &amp;quot;speaker
from the Pentagon&amp;quot;;
4. change in grammatical features: ac-
tive/passive, time, number. &amp;quot;...a building
was devastated by the bomb&amp;quot; and &amp;quot;...the
bomb devastated a building&amp;quot;;
5. head omission: &amp;quot;group of students&amp;quot; and
&amp;quot;students&amp;quot;;
6. transformation from one part of speech
to another: &amp;quot;building devastation&amp;quot; and
&amp;quot;...building was devastated&amp;quot;;
7. using semantically related words such
as synonyms: &amp;quot;return&amp;quot; and &amp;quot;alight&amp;quot;,
&amp;quot;regime&amp;quot; and &amp;quot;government&amp;quot;.
</listItem>
<bodyText confidence="0.999944090909091">
The patterns presented above cover 82% of
the syntactic and lexical paraphrases (which is,
in turn, 70% of all variants). These categories
form the basis for paraphrasing rules used by
our intersection algorithm.
The majority of these categories can be iden-
tified in an automatic way. However, some of
the rules can only be approximated to a certain
degree. For example, identification of similar-
ity based on semantic relations between words
depends on the coverage of the thesaurus. We
</bodyText>
<page confidence="0.994122">
553
</page>
<bodyText confidence="0.999844666666667">
identify word similarity using synonym relations
from WordNet. Currently, paraphrasing using
part of speech transformations is not supported
by the system. All other paraphrase classes we
identified are implemented in our algorithm for
theme intersection.
</bodyText>
<subsectionHeader confidence="0.998882">
3.3 Temporal Ordering
</subsectionHeader>
<bodyText confidence="0.9990275">
A property that is unique to multi-document
summarization is the effect of time perspective
(Radev and McKeown, 1998). When reading an
original text, it is possible to retrieve the cor-
rect temporal sequence of events which is usu-
ally available explicitly. However, when we put
pieces of text from different sources together,
we must provide the correct time perspective to
the reader, including the order of events, the
temporal distance between events and correct
temporal references.
In single-document summarization, one of the
possible orderings of the extracted information
is provided by the input document itself. How-
ever, in the case of multiple-document summa-
rization, some events may not be described in
the same article. Furthermore, the order be-
tween phrases can change significantly from one
article to another. For example, in a set of ar-
ticles about the Oklahoma bombing from our
training set, information about the &amp;quot;bombing&amp;quot;
itself, &amp;quot;the death toll&amp;quot; and &amp;quot;the suspects&amp;quot; appear
in three different orders in the articles. This
phenomenon can be explained by the fact that
the order of the sentences is highly influenced
by the focus of the article.
One possible discourse strategy for sum-
maries is to base ordering of sentences on
chronological order of events. To find the time
an event occurred, we use the publication date
of the phrase referring to the event. This gives
us the best approximation to the order of events
without carrying out a detailed interpretation
of temporal references to events in the article,
which are not always present. Typically, an
event is first referred to on the day it occurred.
Thus, for each phrase, we must find the earliest
publication date in the theme, create a &amp;quot;time
stamp&amp;quot;, and order phrases in the summary ac-
cording to this time stamp.
Temporal distance between events is an essen-
tial part of the summary. For example, in the
summary in Figure 1 about a &amp;quot;U.S. pilot downed
in Bosnia&amp;quot;, the lengthy duration between &amp;quot;the
helicopter was shot down&amp;quot; and &amp;quot;the pilot was
rescued&amp;quot; is the main point of the story. We
want to identify significant time gaps between
events, and include them in the summary. To do
so, we compare the time stamps of the themes,
and when the difference between two subse-
quent time stamps exceeds a certain threshold
(currently two days), the gap is recorded. A
time marker will be added to the output sum-
mary for each gap, for example &amp;quot;According to a
Reuters report on the 10/21&amp;quot;.
Another time-related issue that we address
is normalization of temporal references in the
summary. If the word &amp;quot;today&amp;quot; is used twice
in the summary, and each time it refers to a
different date, then the resulting summary can
be misleading. Time references such as &amp;quot;to-
day&amp;quot; and &amp;quot;Monday&amp;quot; are clear in the context of
a source article, but can be ambiguous when ex-
tracted from the article. This ambiguity can be
corrected by substitution of this temporal ref-
erence with the full time/date reference, such
as &amp;quot;10/21&amp;quot;. By corpus analysis, we collected
a set of patterns for identification of ambigu-
ous dates. However, we currently don&apos;t handle
temporal references requiring inference to re-
solve (e.g., &amp;quot;the day before the plane crashed,&amp;quot;
&amp;quot;around Christmas&amp;quot;).
</bodyText>
<sectionHeader confidence="0.977135" genericHeader="method">
4 Sentence Generation
</sectionHeader>
<bodyText confidence="0.999970095238095">
The input to the sentence generator is a set of
phrases that are to be combined and realized
as a sentence. Input features for each phrase
are determined by the information recovered by
shallow analysis during content planning. Be-
-cause this input structure and the requirements
on the generator are quite different from typical
language generators, we had to address the de-
sign of the input language specification and its
interaction with existing features in a new way,
instead of using the existing SURGE syntactic
realization in a &amp;quot;black box&amp;quot; manner.
As an example, consider the case of tempo-
ral modifiers. The DSYNT for an input phrase
will simply note that it contains a prepositional
phrase. FUF/SURGE, our language generator,
requires that the input contain a semantic role,
circumstantial which in turn contains a tempo-
ral feature.
The labelling of the circumstantial as time
allows SURGE to make the following decisions
</bodyText>
<page confidence="0.996808">
554
</page>
<bodyText confidence="0.991324">
given a sentence such as: &amp;quot;After they made
an emergency landing, the pilots were reported
missing.&amp;quot;
</bodyText>
<listItem confidence="0.91590575">
• The selection of the position of the time
circumstantial in front of the clause
• The selection of the mood of the embedded
clause as &amp;quot;finite&amp;quot;.
</listItem>
<bodyText confidence="0.9932306875">
The semantic input also provides a solid ba-
sis to authorize sophisticated revisions to a base
input. If the sentence planner decides to ad-
join a source to the clause, SURGE can decide
to move the time circumstantial to the end of
the clause, leading to: &amp;quot;According to Reuters on
Thursday night, the pilots were reported miss-
ing after making an emergency landing.&amp;quot; With-
out such paraphrasing ability, which might be
decided based on the semantic roles, time and
sources, the system would have to generate an
awkward sentence with both circumstantials ap-
pearing one after another at the front of the
sentence.
While in the typical generation scenario
above, the generator can make choices based on
semantic information, in our situation, the gen-
erator has only a low-level syntactic structure,
represented as a DSYNT. It would seem at first
glance that realizing such an input should be
easier for the syntactic realization component.
The generator in that case is left with little less
to do than just linearizing the input specifica-
tion. The task we had to solve, however, is more
difficult for two reasons:
1. The input specification we define must al-
low the sentence planner to perform revi-
sions; that is, to attach new constituents
(such as source) to a base input specifica-
tion without taking into account all possi-
ble syntactic interactions between the new
constituent and existing ones;
2. SURGE relies on semantic information to
make decisions and verify that these deci-
sions are compatible with the rest of the
sentence structure. When the semantic in-
formation is not available, it is more diffi-
cult to predict that the decisions are com-
patible with the input provided in syntactic
form.
We modified the input specification language
for FUF/SURGE to account for these problems.
We added features that indicate the ordering of
circumstantials in the output. Ordering of cir-
cumstantials can easily be derived from their
ordering in the input. Thus, we label circum-
stantials with the features front-i (i-th circum-
stantial at the front of the sentence) and end-i
(i-th circumstantial at the end), where i indi-
cates the relative ordering of the circumstantial
within the clause.
In addition, if possible, when mapping input
phrases to a SURGE syntactic input, the sen-
tence planner tries to determine the semantic
type of circumstantial by looking up the prepo-
sition (for example: &amp;quot;after&amp;quot; indicates a &amp;quot;time&amp;quot;
circumstantial). This allows FUF/SURGE to
map the syntactic category of the circumstan-
tial to the semantic and syntactic features ex-
pected by SURGE. However, in cases where the
preposition is ambiguous (e.g., &amp;quot;in&amp;quot; can indi-
cate &amp;quot;time&amp;quot; or &amp;quot;location&amp;quot;) the generator must
rely solely on ordering circumstantials based on
ordering found in the input.
We have modified SURGE to accept this type
of input: in all places SURGE checks the se-
mantic type of the circumstantial before making
choices, we verified that the absence of the cor-
responding input feature would not lead to an
inappropriate default being selected. In sum-
mary, this new application for syntactic realiza-
tion highlights the need for supporting hybrid
inputs of variable abstraction levels. The imple-
mentation benefited from the bidirectional na-
ture of FUF unification in the handling of hy-
brid constraints and required little change to
the existing SURGE grammar. While we used
circumstantials to illustrate the issues, we also
handled revision for a variety of other categories
in the same manner.
</bodyText>
<sectionHeader confidence="0.996358" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999654545454546">
Evaluation of multi-document summarization is
difficult. First, we have not yet found an exist-
ing collection of human written summaries of
multiple documents which could serve as a gold
standard. We have begun a joint project with
the Columbia Journalism School which will pro-
vide such data in the future. Second, methods
used for evaluation of extraction-based systems
are not applicable for a system which involves
text regeneration. Finally, the manual effort
needed to develop test beds and to judge sys-
</bodyText>
<page confidence="0.996111">
555
</page>
<bodyText confidence="0.99972048">
tern output is far more extensive than for single
document summarization; consider that a hu-
man judge would have to read many input ar-
ticles (our largest test set contained 27 input
articles) to rate the validity of a summary.
Consequently, the evaluation that we per-
formed to date is limited. We performed a quan-
titative evaluation of our content-selection com-
ponent. In order to prevent noisy input from
the theme construction component from skew-
ing the evaluation, we manually constructed
26 themes, each containing 4 sentences on aver-
age. Far more training data is needed to tune
the generation portion. While we have tuned
the system to perform with minor errors on the
manual set of themes we have created (the miss-
ing article in the fourth sentence of the sum-
mary in Figure 1 is an example), we need more
robust input data from the theme construction
component, which is still under development, to
train the generator before beginning large scale
testing. One problem in improving output is
determining how to recover from errors in tools
used in early stages of the process, such as the
tagger and the parser.
</bodyText>
<subsectionHeader confidence="0.971311">
5.1 Intersection Component
</subsectionHeader>
<bodyText confidence="0.9999508125">
The evaluation task for the content selection
stage is to measure how well we identify com-
mon phrases throughout multiple sentences.
Our algorithm was compared against intersec-
tions extracted by human judges from each
theme, producing 39 sentence-level predicate-
argument structures. Our intersection algo-
rithm identified 29 (74%) predicate-argument
structures and was able to identify correctly
69% of the subjects, 74% of the main verbs,
and 65% of the other constituents in our list
of model predicate-argument structures. We
present system accuracy separately for each cat-
egory, since identifying a verb or a subject is,
in most cases, more important than identifying
other sentence constituents.
</bodyText>
<sectionHeader confidence="0.998597" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999986933333334">
In this paper, we presented an implemented
algorithm for multi-document summarization
which moves beyond the sentence extraction
paradigm. Assuming a set of similar sentences
as input extracted from multiple documents on
the same event (McKeown et al., 1999; Eskin et
al., 1999), our system identifies common phrases
across sentences and uses language generation
to reformulate them as a coherent summary.
The use of generation to merge similar infor-
mation is a new approach that significantly im-
proves the quality of the resulting summaries,
reducing repetition and increasing fluency.
The system we have developed serves as a
point of departure for research in a variety of
directions. First is the need to use learning tech-
niques to identify paraphrasing patterns in cor-
pus data. As a first pass, we found paraphrasing
rules manually. This initial set might allow us to
automatically identify more rules and increase
the performance of our comparison algorithm.
From the generation side, our main goal is to
make the generated summary more concise, pri-
marily by combining clauses together. We will
be investigating what factors influence the com-
bination process and how they can be computed
from input articles. Part of combination will in-
volve increasing coherence of the generated text
through the use of connectives, anaphora or lex-
ical relations (Jing, 1999).
One interesting problem for future work is the
question of how much context to include from
a sentence from which an intersected phrase is
drawn. Currently, we include no context, but
in some cases context is crucial even though it
is not a part of the intersection. This is the
case, for example, when the context negates, or
denies, the embedded sub-clause which matches
a sub-clause in another negating context. In
such cases, the resulting summary is actually
false. This occurs just once in our test cases, but
it is a serious error. Our work will characterize
the types of contextual information that should
be retained and will develop algorithms for the
case of negation, among others.
</bodyText>
<sectionHeader confidence="0.998027" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999927125">
We would like to thank Ya.el Dahan-Netzer for
her help with SURGE. This material is based
upon work supported by the National Science
Foundation under grant No. IRI-96-1879. Any
opinions, findings, and conclusions or recom-
mendations expressed in this material are those
of the authors and do not necessarily reflect the
views of the National Science Foundation.
</bodyText>
<page confidence="0.997536">
556
</page>
<sectionHeader confidence="0.995341" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998371138297872">
James Allan, Jaime Carbonell, George Dod-
dington, Jon Yamron, and Y. Yang. 1998.
Topic detection and tracking pilot study:
Final report. In Proceedings of the Broad-
cast News Understanding and Transcription
Workshop, pages 194-218.
Regina Barzilay and Michael Elhadad. 1997.
Using lexical chains for text summarization.
In Proceedings of the ACL Workshop on In-
telligent Scalable Text Summarization, pages
10-17, Madrid, Spain, August. ACL.
Jaime Carbonell and Jade Goldstein. 1998.
The use of mmr, diversity-based reranking
for reordering documents and producing sum-
maries. In Proceedings of the 21st Annual In-
ternational ACM SIGIR Conference on Re-
search and Development in Information Re-
trieval, Melbourne, Australia, August.
Michael Collins. 1996. A new statistical parser
based on bigram lexical dependencies. In
Proceedings of the 35th Annual Meeting of
the Association for Computational Linguis-
tics, Santa Cruz, California.
Mark Dras. 1997. Reluctant paraphrase: Tex-
tual restructuring under an optimisation
model. In Proceedings of PACLING97, pages
98-104, Ohme, Japan.
Michael Elhadad. 1993. Using Argumentation
to Control Lexical Choice: A Functional Uni-
fication Implementation. Ph.D. thesis, De-
partment of Computer Science, Columbia
University, New York.
Eleazar Eskin, Judith Klavans, and Vasileios
Hatzivassiloglou. 1999. Detecting similarity
by apllying learning over indicators. submit-
ted.
Lidija Iordanskaja, Richard Kittredge, and
Alain Polguere, 1991. Natural language Gen-
eration in Artificial Intelligence and Compu-
tational Linguistics, chapter 11. Kluwer Aca-
demic Publishers.
Cristian Jacquemin, Judith L. Klavans, and
Evelyne Tzoukermann. 1997. Expansion of
multi-word terms for indexing and retrieval
using morphology and syntax. In proceedings
of the 35th Annual Meeting of the ACL, pages
24-31, Madrid, Spain, July. ACL.
Hongyan Jing. 1999. Summary generation
through intelligent cutting and pasting of the
input document. PhD thesis proposal.
Richard Kittredge and Igor A. Mereuk. 1983.
Towards a computable model of meaning-text
relations within a natural sublanguage. In
Proceedings of the Eighth International Joint
Conference on Artificial Intelligence (IJCAI-
83), pages 657-659, Karlsruhe, West Ger-
many, August.
Chin-Yew Lin and Eduard Hovy. 1997. Iden-
tifying topics by position. In Proceedings of
the 5th ACL Conference on Applied Natural
Language Processing, pages 283-290, Wash-
ington, D.C., April.
Inderjeet Mani and Eric Bloedorn. 1997. Multi-
document summarization by graph search
and matching. In Proceedings of the Fif-
teenth National Conference on Artificial In-
telligence (AAAI-97), pages 622-628, Provi-
dence, Rhode Island. AAAI.
Daniel Marcu. 1997. From discourse structures
to text summaries. In Proceedings of the ACL
Workshop on Intelligent Scalable Text Sum-
marization, pages 82-88, Madrid, Spain, Au-
gust. ACL.
Kathleen R McKeown, Judith Klavans,
Vasileios Hatzivassiloglou, Regina Barzilay,
and Eleazar Eskin. 1999. Towards multi-
document summarization by reformulation:
Progress and prospects. submitted.
Dragomir R. Radev and Kathleen R. McKeown.
1998. Generating natural language sum-
maries from multiple on-line sources. Compu-
tational Linguistics, 24(3):469-500, Septem-
ber.
Jacques Robin. 1994. Revision-Based Gener-
ation of Natural Language Summaries Pro-
viding Historical Background: Corpus-Based
Analysis, Design, Implementation, and Eval-
uation. Ph.D. thesis, Department of Com-
puter Science, Columbia University, NY.
Gerald Salton, James Allan, Chris Buckley,
and Amit Singhal. 1991. Automatic analy-
sis, theme generation, and summarization of
machine-readable texts. Science, 264:1421-
1426, June.
</reference>
<page confidence="0.997364">
557
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.986273">
<title confidence="0.9991275">Information Fusion in the Context of Multi-Document Summarization</title>
<author confidence="0.99166">Barzilay</author>
<author confidence="0.99166">Kathleen Michael Elhadad</author>
<affiliation confidence="0.9998365">Dept. of Computer Science Dept. of Computer Science Columbia University Ben-Gurion University</affiliation>
<address confidence="0.999668">New York, NY 10027, USA Beer-Sheva, Israel</address>
<abstract confidence="0.999543714285714">We present a method to automatically generate a concise summary by identifying and synthesizing similar elements across related text from a set of multiple documents. Our approach is unique in its usage of language generation to reformulate the wording of the summary.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime Carbonell</author>
<author>George Doddington</author>
<author>Jon Yamron</author>
<author>Y Yang</author>
</authors>
<title>Topic detection and tracking pilot study: Final report.</title>
<date>1998</date>
<booktitle>In Proceedings of the Broadcast News Understanding and Transcription Workshop,</booktitle>
<pages>194--218</pages>
<contexts>
<context position="13782" citStr="Allan et al., 1998" startWordPosition="2180" endWordPosition="2183">nal constraints such as length or readability. (Jacquemin et al., 1997) investigated morphology-based paraphrasing in the context of a term recognition task. However, there is no general algorithm capable of identifying a sentence as a paraphrase of another. In our case, such a comparison is less difficult since theme sentences are a priori close semantically, which significantly constrains the kinds of paraphrasing we need to check. In order to verify this assumption, we analyzed paraphrasing patterns through themes of our training corpus derived from the Topic Detection and Tracking corpus (Allan et al., 1998). Overall, 200 pairs of sentences conveying the same information were analyzed. We found that 85% of the paraphrasing is achieved by syntactic and lexical transformations. Examples of paraphrasing that require world knowledge are presented below: 1. &amp;quot;The Bosnian Serbs freed 121 U.N. soldiers &apos;To be exact, the result of the algorithm is a DSYNT that linearizes as this sentence. last week at Zvornik&amp;quot; and &amp;quot;Bosnian Serb leaders freed about one-third of the U.N. personnel&amp;quot; 2. &amp;quot;Sheinbein showed no visible reaction to the ruling.&amp;quot; and &amp;quot;Samuel Sheinbein showed no reaction when Chief Justice Aharon Bar</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>James Allan, Jaime Carbonell, George Doddington, Jon Yamron, and Y. Yang. 1998. Topic detection and tracking pilot study: Final report. In Proceedings of the Broadcast News Understanding and Transcription Workshop, pages 194-218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Michael Elhadad</author>
</authors>
<title>Using lexical chains for text summarization.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>10--17</pages>
<publisher>ACL.</publisher>
<location>Madrid, Spain,</location>
<marker>Barzilay, Elhadad, 1997</marker>
<rawString>Regina Barzilay and Michael Elhadad. 1997. Using lexical chains for text summarization. In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pages 10-17, Madrid, Spain, August. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jaime Carbonell</author>
<author>Jade Goldstein</author>
</authors>
<title>The use of mmr, diversity-based reranking for reordering documents and producing summaries.</title>
<date>1998</date>
<booktitle>In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,</booktitle>
<location>Melbourne, Australia,</location>
<contexts>
<context position="7118" citStr="Carbonell and Goldstein, 1998" startWordPosition="1092" endWordPosition="1095">constrained domains, e.g., terrorism, a coherent summary of several articles can be generated, when a detailed semantic representation of the source text is available. For example, information extraction systems can be used to interpret the source text. In this framework, (Radev and McKeown, 1998) use generation techniques to highlight changes over time across input articles about the same event. In an arbitrary domain, statistical techniques are used to identify similarities and differences across documents. Some approaches directly exploit word distribution in the text (Salton et al., 1991; Carbonell and Goldstein, 1998). Recent work (Mani and Bloedorn, 1997) exploits semantic relations between text units for content representation, such as synonymy and co-reference. A spreading activation algorithm and graph matching is used to identify similarities and differences across documents. The output is presented as a set of paragraphs with similar and unique words highlighted. However, if the same information is mentioned several times in different documents, much of the summary will be redundant. While some researchers address this problem by selecting a subset of the repetitions (Carbonell and Goldstein, 1998), </context>
</contexts>
<marker>Carbonell, Goldstein, 1998</marker>
<rawString>Jaime Carbonell and Jade Goldstein. 1998. The use of mmr, diversity-based reranking for reordering documents and producing summaries. In Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Melbourne, Australia, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>A new statistical parser based on bigram lexical dependencies.</title>
<date>1996</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Santa Cruz, California.</location>
<contexts>
<context position="3229" citStr="Collins, 1996" startWordPosition="495" endWordPosition="496">anner selects and orders propositions from an underlying knowledge base to form text content. A sentence planner determines how to combine propositions into a single sentence, and a sentence generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure. Our approach differs in the following ways: • Content planning operates over full sentences, producing sentence fragments. Thus, content planning straddles the border between interpretation and generation. We preprocess the similar sentences using an existing shallow parser (Collins, 1996) and a mapping to predicateargument structure. The content planner finds an intersection of phrases by comparing the predicate-argument structures; through this process it selects the phrases that can adequately convey the common information of the theme. It also orders selected phrases and augments them with 550 On 3th of September 1995, 120 hostages were released by Bosnian Serbs. Serbs were holding over 250 U.N. personnel. Bosnian serb leader Radovan Karadjic said he expected &amp;quot;a sign of goodwill&amp;quot; from the international community. U.S. F-16 fighter jet was shot down by Bosnian Serbs. Electro</context>
<context position="11065" citStr="Collins, 1996" startWordPosition="1742" endWordPosition="1743">. Since predicateargument structure is a natural way to represent constituent dependencies, we chose a dependency based representation called DSYNT (Kittredge and Mel&apos;aik, 1983). An example of a sentence and its DSYNT tree is shown in Figure 3. Each non-auxiliary word in the sentence has a node in the DSYNT tree, and this node is connected to its direct dependents. Grammatical features of each word are also kept in the node. In order to facilitate comparison, words are kept in canonical form. In order to construct a DSYNT we first run our sentences through Collin&apos;s robust, statistical parser (Collins, 1996). We developed a rulebased component that transforms the phrasestructure output of the parser to a DSYNT representation. Functional words (determiners and auxiliaries) are eliminated from the tree and the corresponding syntactic features are updated. The comparison algorithm starts with all sentence trees rooted at verbs from the input DSYNT, and traverses them recursively: if two nodes are identical, they are added to the output tree, and their children are compared. Once a full phrase (a verb with at least two constituents) has been found, it is added to the intersection. If nodes are not id</context>
</contexts>
<marker>Collins, 1996</marker>
<rawString>Michael Collins. 1996. A new statistical parser based on bigram lexical dependencies. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, Santa Cruz, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Dras</author>
</authors>
<title>Reluctant paraphrase: Textual restructuring under an optimisation model.</title>
<date>1997</date>
<booktitle>In Proceedings of PACLING97,</booktitle>
<pages>98--104</pages>
<location>Ohme, Japan.</location>
<contexts>
<context position="13076" citStr="Dras, 1997" startWordPosition="2071" endWordPosition="2072">ntersection. For the theme in Figure 2, the intersection result is &amp;quot;On Friday, a U.S. F-16 fighter jet was shot down by Bosnian Serb missile.&amp;quot;1 3.2 Paraphrasing Rules Derived from Corpus Analysis Identification of theme intersection requires collecting paraphrasing patterns which occur in our corpus. Paraphrasing is defined as alternative ways a human speaker can choose to &amp;quot;say the same thing&amp;quot; by using linguistic knowledge (as opposed to world knowledge) (Iordanslcaja et al., 1991). Paraphrasing has been widely investigated in the generation community (Iordanskaja et al., 1991; Robin, 1994). (Dras, 1997) considered sets of paraphrases required for text transformation in order to meet external constraints such as length or readability. (Jacquemin et al., 1997) investigated morphology-based paraphrasing in the context of a term recognition task. However, there is no general algorithm capable of identifying a sentence as a paraphrase of another. In our case, such a comparison is less difficult since theme sentences are a priori close semantically, which significantly constrains the kinds of paraphrasing we need to check. In order to verify this assumption, we analyzed paraphrasing patterns throu</context>
</contexts>
<marker>Dras, 1997</marker>
<rawString>Mark Dras. 1997. Reluctant paraphrase: Textual restructuring under an optimisation model. In Proceedings of PACLING97, pages 98-104, Ohme, Japan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Elhadad</author>
</authors>
<title>Using Argumentation to Control Lexical Choice: A Functional Unification Implementation.</title>
<date>1993</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Columbia University,</institution>
<location>New York.</location>
<marker>Elhadad, 1993</marker>
<rawString>Michael Elhadad. 1993. Using Argumentation to Control Lexical Choice: A Functional Unification Implementation. Ph.D. thesis, Department of Computer Science, Columbia University, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eleazar Eskin</author>
<author>Judith Klavans</author>
<author>Vasileios Hatzivassiloglou</author>
</authors>
<title>Detecting similarity by apllying learning over indicators.</title>
<date>1999</date>
<note>submitted.</note>
<contexts>
<context position="2323" citStr="Eskin et al., 1999" startWordPosition="357" endWordPosition="360"> same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while extracting some similar sentences could produce a summary biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be included in the summary and sentence generation to reformulate them as new text. Our work is part of a full summarization system (McKeown et al., 1999), which extracts sets of similar sentences, themes (Eskin et al., 1999), in the first stage for input to the components described here. Our model for multi-document summarization represents a number of departures from traditional language generation. Typically, language generation systems have access to a full semantic representation of the domain. A content planner selects and orders propositions from an underlying knowledge base to form text content. A sentence planner determines how to combine propositions into a single sentence, and a sentence generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building synta</context>
<context position="26830" citStr="Eskin et al., 1999" startWordPosition="4288" endWordPosition="4291">ectly 69% of the subjects, 74% of the main verbs, and 65% of the other constituents in our list of model predicate-argument structures. We present system accuracy separately for each category, since identifying a verb or a subject is, in most cases, more important than identifying other sentence constituents. 6 Conclusions and Future Work In this paper, we presented an implemented algorithm for multi-document summarization which moves beyond the sentence extraction paradigm. Assuming a set of similar sentences as input extracted from multiple documents on the same event (McKeown et al., 1999; Eskin et al., 1999), our system identifies common phrases across sentences and uses language generation to reformulate them as a coherent summary. The use of generation to merge similar information is a new approach that significantly improves the quality of the resulting summaries, reducing repetition and increasing fluency. The system we have developed serves as a point of departure for research in a variety of directions. First is the need to use learning techniques to identify paraphrasing patterns in corpus data. As a first pass, we found paraphrasing rules manually. This initial set might allow us to autom</context>
</contexts>
<marker>Eskin, Klavans, Hatzivassiloglou, 1999</marker>
<rawString>Eleazar Eskin, Judith Klavans, and Vasileios Hatzivassiloglou. 1999. Detecting similarity by apllying learning over indicators. submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lidija Iordanskaja</author>
<author>Richard Kittredge</author>
<author>Alain Polguere</author>
</authors>
<date>1991</date>
<booktitle>Natural language Generation in Artificial Intelligence and Computational Linguistics, chapter 11.</booktitle>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="13048" citStr="Iordanskaja et al., 1991" startWordPosition="2065" endWordPosition="2068">above a given threshold result in theme intersection. For the theme in Figure 2, the intersection result is &amp;quot;On Friday, a U.S. F-16 fighter jet was shot down by Bosnian Serb missile.&amp;quot;1 3.2 Paraphrasing Rules Derived from Corpus Analysis Identification of theme intersection requires collecting paraphrasing patterns which occur in our corpus. Paraphrasing is defined as alternative ways a human speaker can choose to &amp;quot;say the same thing&amp;quot; by using linguistic knowledge (as opposed to world knowledge) (Iordanslcaja et al., 1991). Paraphrasing has been widely investigated in the generation community (Iordanskaja et al., 1991; Robin, 1994). (Dras, 1997) considered sets of paraphrases required for text transformation in order to meet external constraints such as length or readability. (Jacquemin et al., 1997) investigated morphology-based paraphrasing in the context of a term recognition task. However, there is no general algorithm capable of identifying a sentence as a paraphrase of another. In our case, such a comparison is less difficult since theme sentences are a priori close semantically, which significantly constrains the kinds of paraphrasing we need to check. In order to verify this assumption, we analyzed</context>
</contexts>
<marker>Iordanskaja, Kittredge, Polguere, 1991</marker>
<rawString>Lidija Iordanskaja, Richard Kittredge, and Alain Polguere, 1991. Natural language Generation in Artificial Intelligence and Computational Linguistics, chapter 11. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cristian Jacquemin</author>
<author>Judith L Klavans</author>
<author>Evelyne Tzoukermann</author>
</authors>
<title>Expansion of multi-word terms for indexing and retrieval using morphology and syntax.</title>
<date>1997</date>
<booktitle>In proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>24--31</pages>
<publisher>ACL.</publisher>
<location>Madrid, Spain,</location>
<contexts>
<context position="13234" citStr="Jacquemin et al., 1997" startWordPosition="2094" endWordPosition="2097">2 Paraphrasing Rules Derived from Corpus Analysis Identification of theme intersection requires collecting paraphrasing patterns which occur in our corpus. Paraphrasing is defined as alternative ways a human speaker can choose to &amp;quot;say the same thing&amp;quot; by using linguistic knowledge (as opposed to world knowledge) (Iordanslcaja et al., 1991). Paraphrasing has been widely investigated in the generation community (Iordanskaja et al., 1991; Robin, 1994). (Dras, 1997) considered sets of paraphrases required for text transformation in order to meet external constraints such as length or readability. (Jacquemin et al., 1997) investigated morphology-based paraphrasing in the context of a term recognition task. However, there is no general algorithm capable of identifying a sentence as a paraphrase of another. In our case, such a comparison is less difficult since theme sentences are a priori close semantically, which significantly constrains the kinds of paraphrasing we need to check. In order to verify this assumption, we analyzed paraphrasing patterns through themes of our training corpus derived from the Topic Detection and Tracking corpus (Allan et al., 1998). Overall, 200 pairs of sentences conveying the same</context>
</contexts>
<marker>Jacquemin, Klavans, Tzoukermann, 1997</marker>
<rawString>Cristian Jacquemin, Judith L. Klavans, and Evelyne Tzoukermann. 1997. Expansion of multi-word terms for indexing and retrieval using morphology and syntax. In proceedings of the 35th Annual Meeting of the ACL, pages 24-31, Madrid, Spain, July. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hongyan Jing</author>
</authors>
<title>Summary generation through intelligent cutting and pasting of the input document.</title>
<date>1999</date>
<note>PhD thesis proposal.</note>
<contexts>
<context position="27917" citStr="Jing, 1999" startWordPosition="4464" endWordPosition="4465">ing patterns in corpus data. As a first pass, we found paraphrasing rules manually. This initial set might allow us to automatically identify more rules and increase the performance of our comparison algorithm. From the generation side, our main goal is to make the generated summary more concise, primarily by combining clauses together. We will be investigating what factors influence the combination process and how they can be computed from input articles. Part of combination will involve increasing coherence of the generated text through the use of connectives, anaphora or lexical relations (Jing, 1999). One interesting problem for future work is the question of how much context to include from a sentence from which an intersected phrase is drawn. Currently, we include no context, but in some cases context is crucial even though it is not a part of the intersection. This is the case, for example, when the context negates, or denies, the embedded sub-clause which matches a sub-clause in another negating context. In such cases, the resulting summary is actually false. This occurs just once in our test cases, but it is a serious error. Our work will characterize the types of contextual informat</context>
</contexts>
<marker>Jing, 1999</marker>
<rawString>Hongyan Jing. 1999. Summary generation through intelligent cutting and pasting of the input document. PhD thesis proposal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Kittredge</author>
<author>Igor A Mereuk</author>
</authors>
<title>Towards a computable model of meaning-text relations within a natural sublanguage.</title>
<date>1983</date>
<booktitle>In Proceedings of the Eighth International Joint Conference on Artificial Intelligence (IJCAI83),</booktitle>
<pages>657--659</pages>
<location>Karlsruhe, West Germany,</location>
<marker>Kittredge, Mereuk, 1983</marker>
<rawString>Richard Kittredge and Igor A. Mereuk. 1983. Towards a computable model of meaning-text relations within a natural sublanguage. In Proceedings of the Eighth International Joint Conference on Artificial Intelligence (IJCAI83), pages 657-659, Karlsruhe, West Germany, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
<author>Eduard Hovy</author>
</authors>
<title>Identifying topics by position.</title>
<date>1997</date>
<booktitle>In Proceedings of the 5th ACL Conference on Applied Natural Language Processing,</booktitle>
<pages>283--290</pages>
<location>Washington, D.C.,</location>
<contexts>
<context position="1603" citStr="Lin and Hovy, 1997" startWordPosition="242" endWordPosition="245">e automatic fusion of similar information across multiple documents using language generation to produce a concise summary. We propose a method for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for summary generation. Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while extracting some similar sentences could produce a summary biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be included in the summary and sentence generation to reformulate them as new text. Our work is part of a</context>
</contexts>
<marker>Lin, Hovy, 1997</marker>
<rawString>Chin-Yew Lin and Eduard Hovy. 1997. Identifying topics by position. In Proceedings of the 5th ACL Conference on Applied Natural Language Processing, pages 283-290, Washington, D.C., April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Inderjeet Mani</author>
<author>Eric Bloedorn</author>
</authors>
<title>Multidocument summarization by graph search and matching.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-97),</booktitle>
<pages>622--628</pages>
<publisher>AAAI.</publisher>
<location>Providence, Rhode Island.</location>
<contexts>
<context position="7157" citStr="Mani and Bloedorn, 1997" startWordPosition="1098" endWordPosition="1102">ent summary of several articles can be generated, when a detailed semantic representation of the source text is available. For example, information extraction systems can be used to interpret the source text. In this framework, (Radev and McKeown, 1998) use generation techniques to highlight changes over time across input articles about the same event. In an arbitrary domain, statistical techniques are used to identify similarities and differences across documents. Some approaches directly exploit word distribution in the text (Salton et al., 1991; Carbonell and Goldstein, 1998). Recent work (Mani and Bloedorn, 1997) exploits semantic relations between text units for content representation, such as synonymy and co-reference. A spreading activation algorithm and graph matching is used to identify similarities and differences across documents. The output is presented as a set of paragraphs with similar and unique words highlighted. However, if the same information is mentioned several times in different documents, much of the summary will be redundant. While some researchers address this problem by selecting a subset of the repetitions (Carbonell and Goldstein, 1998), this approach is not always satisfactor</context>
</contexts>
<marker>Mani, Bloedorn, 1997</marker>
<rawString>Inderjeet Mani and Eric Bloedorn. 1997. Multidocument summarization by graph search and matching. In Proceedings of the Fifteenth National Conference on Artificial Intelligence (AAAI-97), pages 622-628, Providence, Rhode Island. AAAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>From discourse structures to text summaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization,</booktitle>
<pages>82--88</pages>
<publisher>ACL.</publisher>
<location>Madrid, Spain,</location>
<contexts>
<context position="1616" citStr="Marcu, 1997" startWordPosition="246" endWordPosition="247">f similar information across multiple documents using language generation to produce a concise summary. We propose a method for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for summary generation. Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while extracting some similar sentences could produce a summary biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be included in the summary and sentence generation to reformulate them as new text. Our work is part of a full summari</context>
<context position="6010" citStr="Marcu, 1997" startWordPosition="931" endWordPosition="932">g sections, we provide an overview of existing multi-document summarization systems, then we will detail our sentence comparison technique, and describe the sentence generation component. We provide examples of generated summaries and conclude with a discussion of evaluation. 2 Related Work Automatic summarizers typically identify and extract the most important sentences from an input article. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Salton et a., 1991), symbolic techniques based on discourse structure (Marcu, 1997), and semantic relations between words (Barzilay and Elhadakl, 1997). Extraction techniques can work only if summary sentences already appear in the article. Extraction cannot handle the task we address, because summarization of multiple documents requires information about similarities and differences across articles. While most of the summarization work has focused on single articles, a few initial projects have started to study multi-document summarization documents. In constrained domains, e.g., terrorism, a coherent summary of several articles can be generated, when a detailed semantic re</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. From discourse structures to text summaries. In Proceedings of the ACL Workshop on Intelligent Scalable Text Summarization, pages 82-88, Madrid, Spain, August. ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown</author>
</authors>
<title>Judith Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin.</title>
<date>1999</date>
<note>submitted.</note>
<marker>McKeown, 1999</marker>
<rawString>Kathleen R McKeown, Judith Klavans, Vasileios Hatzivassiloglou, Regina Barzilay, and Eleazar Eskin. 1999. Towards multidocument summarization by reformulation: Progress and prospects. submitted.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Kathleen R McKeown</author>
</authors>
<title>Generating natural language summaries from multiple on-line sources.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<pages>24--3</pages>
<contexts>
<context position="6786" citStr="Radev and McKeown, 1998" startWordPosition="1042" endWordPosition="1046">icle. Extraction cannot handle the task we address, because summarization of multiple documents requires information about similarities and differences across articles. While most of the summarization work has focused on single articles, a few initial projects have started to study multi-document summarization documents. In constrained domains, e.g., terrorism, a coherent summary of several articles can be generated, when a detailed semantic representation of the source text is available. For example, information extraction systems can be used to interpret the source text. In this framework, (Radev and McKeown, 1998) use generation techniques to highlight changes over time across input articles about the same event. In an arbitrary domain, statistical techniques are used to identify similarities and differences across documents. Some approaches directly exploit word distribution in the text (Salton et al., 1991; Carbonell and Goldstein, 1998). Recent work (Mani and Bloedorn, 1997) exploits semantic relations between text units for content representation, such as synonymy and co-reference. A spreading activation algorithm and graph matching is used to identify similarities and differences across documents.</context>
<context position="16387" citStr="Radev and McKeown, 1998" startWordPosition="2571" endWordPosition="2574">d in an automatic way. However, some of the rules can only be approximated to a certain degree. For example, identification of similarity based on semantic relations between words depends on the coverage of the thesaurus. We 553 identify word similarity using synonym relations from WordNet. Currently, paraphrasing using part of speech transformations is not supported by the system. All other paraphrase classes we identified are implemented in our algorithm for theme intersection. 3.3 Temporal Ordering A property that is unique to multi-document summarization is the effect of time perspective (Radev and McKeown, 1998). When reading an original text, it is possible to retrieve the correct temporal sequence of events which is usually available explicitly. However, when we put pieces of text from different sources together, we must provide the correct time perspective to the reader, including the order of events, the temporal distance between events and correct temporal references. In single-document summarization, one of the possible orderings of the extracted information is provided by the input document itself. However, in the case of multiple-document summarization, some events may not be described in the</context>
</contexts>
<marker>Radev, McKeown, 1998</marker>
<rawString>Dragomir R. Radev and Kathleen R. McKeown. 1998. Generating natural language summaries from multiple on-line sources. Computational Linguistics, 24(3):469-500, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacques Robin</author>
</authors>
<title>Revision-Based Generation of Natural Language Summaries Providing Historical Background: Corpus-Based Analysis, Design, Implementation, and Evaluation.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, Columbia University, NY.</institution>
<contexts>
<context position="4736" citStr="Robin, 1994" startWordPosition="729" endWordPosition="730">elicopters. Figure 1: Summary produced by our system using 12 news articles as input. information needed for clarification (entity descriptions, temporal references, and newswire source references). • Sentence generation begins with phrases. Our task is to produce fluent sentences that combine these phrases, arranging them in novel contexts. In this process, new grammatical constraints may be imposed and paraphrasing may be required. We developed techniques to map predicateargument structure produced by the content-planner to the functional representation expected by FUF/SURGE(Elhaklad, 1993; Robin, 1994) and to integrate new constraints on realization choice, using surface features in place of semantic or pragmatic ones typically used in sentence generation. An example summary automatically generated by the system from our corpus of themes is shown in Figure 1. We collected a corpus of themes, that was divided into a training portion and a testing portion. We used the training data for identification of paraphrasing rules on which our comparison algorithm is built. The system we describe has been fully implemented and tested on a variety of input articles; there are, of course, many open rese</context>
<context position="13062" citStr="Robin, 1994" startWordPosition="2069" endWordPosition="2070">sult in theme intersection. For the theme in Figure 2, the intersection result is &amp;quot;On Friday, a U.S. F-16 fighter jet was shot down by Bosnian Serb missile.&amp;quot;1 3.2 Paraphrasing Rules Derived from Corpus Analysis Identification of theme intersection requires collecting paraphrasing patterns which occur in our corpus. Paraphrasing is defined as alternative ways a human speaker can choose to &amp;quot;say the same thing&amp;quot; by using linguistic knowledge (as opposed to world knowledge) (Iordanslcaja et al., 1991). Paraphrasing has been widely investigated in the generation community (Iordanskaja et al., 1991; Robin, 1994). (Dras, 1997) considered sets of paraphrases required for text transformation in order to meet external constraints such as length or readability. (Jacquemin et al., 1997) investigated morphology-based paraphrasing in the context of a term recognition task. However, there is no general algorithm capable of identifying a sentence as a paraphrase of another. In our case, such a comparison is less difficult since theme sentences are a priori close semantically, which significantly constrains the kinds of paraphrasing we need to check. In order to verify this assumption, we analyzed paraphrasing </context>
</contexts>
<marker>Robin, 1994</marker>
<rawString>Jacques Robin. 1994. Revision-Based Generation of Natural Language Summaries Providing Historical Background: Corpus-Based Analysis, Design, Implementation, and Evaluation. Ph.D. thesis, Department of Computer Science, Columbia University, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gerald Salton</author>
<author>James Allan</author>
<author>Chris Buckley</author>
<author>Amit Singhal</author>
</authors>
<title>Automatic analysis, theme generation, and summarization of machine-readable texts.</title>
<date>1991</date>
<journal>Science,</journal>
<pages>264--1421</pages>
<contexts>
<context position="1638" citStr="Salton et al., 1991" startWordPosition="248" endWordPosition="251">ormation across multiple documents using language generation to produce a concise summary. We propose a method for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for summary generation. Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while extracting some similar sentences could produce a summary biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be included in the summary and sentence generation to reformulate them as new text. Our work is part of a full summarization system (McKeown</context>
<context position="7086" citStr="Salton et al., 1991" startWordPosition="1088" endWordPosition="1091">zation documents. In constrained domains, e.g., terrorism, a coherent summary of several articles can be generated, when a detailed semantic representation of the source text is available. For example, information extraction systems can be used to interpret the source text. In this framework, (Radev and McKeown, 1998) use generation techniques to highlight changes over time across input articles about the same event. In an arbitrary domain, statistical techniques are used to identify similarities and differences across documents. Some approaches directly exploit word distribution in the text (Salton et al., 1991; Carbonell and Goldstein, 1998). Recent work (Mani and Bloedorn, 1997) exploits semantic relations between text units for content representation, such as synonymy and co-reference. A spreading activation algorithm and graph matching is used to identify similarities and differences across documents. The output is presented as a set of paragraphs with similar and unique words highlighted. However, if the same information is mentioned several times in different documents, much of the summary will be redundant. While some researchers address this problem by selecting a subset of the repetitions (</context>
</contexts>
<marker>Salton, Allan, Buckley, Singhal, 1991</marker>
<rawString>Gerald Salton, James Allan, Chris Buckley, and Amit Singhal. 1991. Automatic analysis, theme generation, and summarization of machine-readable texts. Science, 264:1421-1426, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>