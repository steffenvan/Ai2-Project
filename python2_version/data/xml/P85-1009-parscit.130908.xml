<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.012508">
<note confidence="0.791924333333333">
REVERSIBLE AUTOMATA AND INDUCTION OF THE ENGLISH AUXILIARY SYSTEM
Samuel F. Pilato
Robert C. Berwick
</note>
<sectionHeader confidence="0.9253505" genericHeader="abstract">
MIT Artificial Intelligence Laboratory
545 Technology Square
Cambridge, MA 02139, USA
ABSTRACT
</sectionHeader>
<bodyText confidence="0.9967475">
In this paper we apply some recent work of Angluin
(1982) to the induction of the English auxiliary verb system.
In general, the induction of finite automata is computation-
ally intractable. However, Angluin shows that restricted
finite automata, the k-reversible automata, can be learned
by efficient (polynomial time) algorithms. We present an ex-
plicit computer model demonstrating that the English aux-
iliary verb system can in fact be learned as a 1-reversible
automaton, and hence in a computationally feasible amount
of time. The entire system can be acquired by looking at
only half the possible auxiliary verb sequences, and the pat-
tern of generalization seems compatible with what is known
about human acquisition of auxiliaries. We conclude that
certain linguistic subsystems may well be learnable by in-
ductive inference methods of this kind, and suggest an ex-
tension to context-free languages.
</bodyText>
<sectionHeader confidence="0.997756" genericHeader="introduction">
INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999909185185185">
Formal inductive inference methods have rarely been ap-
plied to actual natural language systems. Linguists gener-
ally suppose that languages are easy to learn because gram-
mars are highly constrained; no &amp;quot;general purpose&amp;quot; inductive
inference methods are required. This assumption has gener-
ally led to fruitful insights on the nature of grammars. Yet
it remains to determine whether all of a language is learned
in a grammar-specific manner. In this paper we show how
to successfully apply one computationally efficient inductive
inference algorithm to the acquisition of a domain of English
syntax. Our results suggest that particular language subsys-
tems can be learned by general induction procedures, given
certain general constraints.
The problem is that these methods are in general com-
putationally intractable. Even for regular languages induc-
tion can be exponentially difficult (Gold, 1978). This sug-
gests that there may be general constraints on the design
of certain linguistic subsystems to make them easy to learn
by general inductive inference methods. We propose the
constraint of k-reversibility as one such restriction. This
constraint guarantees polynomial time inference (Angluin,
1982). In the remainder of this paper, we also show, by
an explicit computer model, that the English auxiliary verb
system meets this constraint, and so is easily inferred from a
corpus. The theory gives one precise characterization of just
where we may expect general inductive inference methods
to be of value in language acquisition.
</bodyText>
<sectionHeader confidence="0.999262" genericHeader="method">
LEARNING K-REVERSIBLE LANGUAGES
FROM EXAMPLES
</sectionHeader>
<bodyText confidence="0.991104964285714">
The question we address is, If a learner presumes that
a natural language domain is systematic in some way, can
the learner intelligently infer the complete system from only
a subset of sample sentences? Let is develop an example
to formally describe what we mean by &amp;quot;systematic in some
way,&amp;quot; and how such a systematic domain allows the infer-
ence of a complete system from examples. If you were told
that Mary bakes cakes, John bakes cakes, and Mary eats
pies are legal strings in sonic language, you might guess
that John eats pies is also in that language. Strings in the
language seem to follow a recognizable pattern, so you ex-
pect other strings that follow the same pattern to be in the
language also.
In this particular case, you are presuming that the to-
be-learned language is a zero-reversible regular language.
Angluin (1982) has defined and explored the formal proper-
ties of reversible regular languages. We here translate SOME
of her formal definitions into less technical terms.
A regular language is any language that can be generated
from a furmula called a regular expression. For example the
strings mentioned above might have come from the language
that the following regular expression generates:
(MaryiJoirin) (bakesleats) ([very&apos; delicious) (cakeslpies)]
A complete natural language is too complex to be gen-
erated by some concise regular expression, but some simple
subsets of a natural language can fit this kind of pattern.
To formally define when a regular language is reversible,
let us first define a prefix as any substring (possibly zero-
</bodyText>
<page confidence="0.999502">
70
</page>
<tableCaption confidence="0.999908">
Table 1: Example of incremental k-reversible inference for several values of k.
</tableCaption>
<table confidence="0.998250066666667">
SEQUENCE OF NEW NEW STRINGS INFERRED: k = 1 k = 2
STRINGS PRESENTED k =0
Mary bakes cakes NONE NONE NONE
John bakes cakes NONE NONE NONE
Mary eats pies John eats pies NONE NONE
Mary bakes pies John bakes pies John bakes pies NONE
Mary eats cakes
John eats cakes
Mary bakes John bakes John bakes NONE
Mary eats
John eats
Mary bakes cakes cakes
John bakes cakes cakes
Mary bakes pies cakes
(MaryiJohn)(bakesleats)(cakesipies)*
</table>
<bodyText confidence="0.999166326530612">
length) that can be found at the very beginning of some legal
string in a language, and a suffix as any substring (again,
possibly zero-length) that can be found at the very end of
some legal string in a language. In our case the strings are
sequences of words, and the language is the set of all legal
sentences in our simplified subset of English. Also, in any
legal string say that the suffix that immediately follows a
prefix is a tail for that prefix. Then a regular language
is zero-reversible if whenever two prefixes in the language
have a tail in common, then the two prefixes have all tails
in common.
In the above example, prefixes Mary and John have the
tail bakes cakes in common. If we presume that the language
these two strings come from is zero-reversible, then Mary
and John must have all tails in common. In particular, the
third string shows that Mary has eats pies as a tail, so John
must also have eats pies as a tail. Our current hypothesis
after having seen these three strings is that they come not
frnm the three-string language expressed by (Maryj John)
bakes cakes 1 Mary eats pies, which is not zero-reversible,
but rather from the four-string language (MaryiJohn) (bakes
cakes I eats pies), which is zero-reversible. Notice that we
have enlarged the corpus just enough to make the language
zero-reversible.
A regular language is k-reversible, where k is a non-
negative integer, if whenever two prefixes whose last lc words
match have a tail in common, then the two prefixes have all
tails in common. A higher value of k gives a more conser-
vative condition for inference. For example, if we presume
that the aforementioned strings come from a 1-reversible
language, then instead of presuming that whatever Mary
does John does, we would presume only that whatever Mary
bakes, John bakes. In this case the third string fails to yield
any inference, but if we were later told that Mary bakes
pies is in the language, we could infer that John bakes pies
is also in the language. Further adding the sentence Mary
bakes would allow 1-reversible inference to also induce John
bakes, resulting in the seven-string 1-reversible language ex-
pressed by (Maryi,/ohn) bakes icakesipiesi i Mary eats pies.
With these examples zero-reversible inference would
have generated (Mary, John) (bake sj eats) (cake si pies)* by
now, which overgeneralizes an optional direct object into
zero or more direct objects. On the other hand, two-
reversible inference would have inferred no additional strings
yet. For a particular language we hope to find a k that is
small enough to yield some inference but not so small that
we overgeneralize and start inferring strings that are in fact
not in the true language we are trying to learn. Table 1
summarizes our examples of k-reversible inference.
</bodyText>
<sectionHeader confidence="0.995554" genericHeader="method">
AN INFERENCE ALGORITHM
</sectionHeader>
<bodyText confidence="0.999876375">
In addition to formally characterizing k-reversible lan-
guages, Angluin also developed an algorithm for inferring
a k-reversible language from a finite set of positive exam-
ples, as well as a method for discovering an appropriate k
when negative examples (strings known not to be in the lan-
guage) are also presented. She also presented an algorithm
for determining, given some k-reversible regular language,
a minimal set of examples from which the entire language
</bodyText>
<page confidence="0.992231">
71
</page>
<bodyText confidence="0.999860659574468">
can be induced. We have implemented these procedures on
a computer in MACLISP and have applied them to all of
the artificial languages in Angluin&apos;s paper as well as to all
of the natural language examples in this paper.
To describe the inference algorithm, we make use of the
fact that every regular language can be associated with a
corresponding deterministic finite-state automaton (DFA)
which accepts or generates exactly that language.
Given a sample of strings taken from the full corpus, we
first generate a prefix-tree automaton which accepts or gen-
erates exactly those strings and no others. We now want
to infer additional strings so as to induce a k-reversible lan-
guage, for some chosen k. Let us say that when accepting
a string, the last k symbols encountered before arriving at
a state is a k-leader of that state. Then to generalize the
language, we recursively merge any two states where any of
the following is true:
eAnother state arcs to both states on the same word.
(This enforces determinism.)
.Both states have a common k-leader and either
—both states are accepting states or
—both states arc to a common state on the same
word.
When none of these conditions obtains any longer, the re-
sulting DFA accepts or generates the smallest k-reversible
language that includes the original sample of strings. (The
term &amp;quot;reversible&amp;quot; is used because a k-reversible DFA is still
deterministic with lookahead k when its sets of initial and
final states are swapped and all of its arcs are reversed.)
This procedure works incrementally. Each new string
may be added to the DFA in prefix-tree fashion and the
state-merging algorithm repeated. The resulting language
induced is independent of the order of presentation of sam-
ple strings.
If an appropriate k is not known a priori, but some
negative as well as positive examples are presented, then
one can try increasing values of k until the induced language
contains none of the negative examples.
Though the inference algorithm takes a sample and in-
duces a k-reversible language, it is quite helpful to use An-
gluin&apos;s algorithm for going in the reverse direction: given a
k-reversible language we can determine what minimal set of
shortest possible examples (a &amp;quot;characteristic&amp;quot; or &amp;quot;covering&amp;quot;
sample) will be sufficient for inducing the language. Though
the minimal number of examples is of course unique, the set
of particular strings in the covering sample is not necessarily
unique.
</bodyText>
<sectionHeader confidence="0.667137" genericHeader="method">
INFERENCE OF THE ENGLISH AUXILIARY
SYSTEM
</sectionHeader>
<bodyText confidence="0.999817530612245">
We have chosen to test the English auxiliary system un-
der k-reversible inference because English verb sequences
are highly regular, yet they have some degree of complexity
and admit to some exceptions. We represent the English
auxiliary system as a corpus of 92 variants of a declarative
statement in third person singular. The variants cover all
standard legal permutations of tense, aspect, and voice, in-
cluding do support and nine modals. We simply use the
surface forms, which are strings of words with no additional
information such as syntactic category or root-by-inflection
breakdown. For instance, the present, simple, active ex-
ample is Judy gives bread. One modal, perfective, passive
variant is Judy would have been given bread.
We have explored the k-reversible properties of this nat-
ural language subsystem in two main steps. First we deter-
mined for what values of k the corpus is in fact k-reversible.
(Given a finite corpus, we could be sure the language is
k-reversible for all k at or above some value.) To do this
we treated the full corpus as a set of sample strings and
tried successively larger values of k until finding one where
k-reversible inference applied to the corpus generates no ad-
ditional strings. We could then be sure that any k of that
value or greater could be used to infer an accurate model of
the English auxiliary system without overgeneralizing.
After finding the range of values of k to work with, we
were interested in determining which, if any, of those values
of k would yield some power to infer the full corpus from
a proper subset of examples. To do this we took the DFA
which represents the full corpus and computed, for a trial
k, a set of sample strings that would be minimally sufficient
to induce the full corpus. If any such values of k exist, then
we can say that, in a nontrivial way, the English auxiliary
system is learnable as a k-reversible language from exam-
ples.
We found that the English auxiliary system can be faith-
fully modeled as a k-reversible regular language for k &gt; 1.
Only zero-reversible inference overgeneralizes the full corpus
as well as the active and passive corpora treated as separate
languages. For the active corpus, zero-reversible inference
groups the forms of do with the other modals. The DFAs for
the passive and full corpora also contain loops and thereby
generate infinite numbers of illegal variants.
Figure 1 compares a correct DFA for the English auxil-
iary system with an overgeneralized DFA. Both are shown in
a minimized, canonical form. The top, correct, automaton
can be generated by either minimizing the prefix tree for the
full corpus or by minimizing the result of k-reversible infer-
ence applied to any sufficiently characteristic set of sample
sentences, for any k &gt; 1. One can read off all 92 variants
</bodyText>
<page confidence="0.986983">
72
</page>
<figure confidence="0.98891975">
(givesignve)
give
THE ENGLISH AUXILIARY SYSTEM
(givesigave)
</figure>
<sectionHeader confidence="0.907863" genericHeader="method">
ZERO-REVERSIBLE OVERGENERALIZATION
OF THE ENGLISH AUXILIARY SYSTEM
</sectionHeader>
<figureCaption confidence="0.999036333333333">
Figure 1: The top automaton generates the English auxiliary system. Zero-reversible inference
merges state 3 with state 2 and merges states 7 and 6 with state 5, resulting in the bottom
avergeneralized version.
</figureCaption>
<figure confidence="0.999660666666666">
(islwas)
Judy
(haelhad)
(givingigiven1.1
given ....,
(rnsylmighcimust
Icon could
Ishaillshou
&apos;wink/auk&apos;)
have
been
get
being
a 0 given 0 bread
Judy
(dotsidid
laisymightinaust
Icanlenuld
shallishou
iwillIwould)
bread
</figure>
<page confidence="0.996509">
73
</page>
<bodyText confidence="0.999410375">
in the language by taking different paths from initial state
to final state. The bottom. overgeneralized, automaton is
generated by subjecting the top one to zero-reversible infer-
ence.
Does treating the English auxiliary system as a 1-or-
more-reversible language yield any inferential power? The
English auxiliary system as a 1-reversible language can in
fact be inferred from a cover of only 48 examples out of
the 92 variants in the corpus. The active corpus treated
separately requires 38 examples out of 46 and the passive
corpus requires 28 out of 46. Treating the full corpus as
a 2-reversible language requires 76 examples, and a 3&amp;quot;-
reversible model cannot infer the corpus from any proper
subset whatsoever.
For 1-reversible inference, 45 of the verb sequences of
length three or shorter will yield the remaining nine such
strings and none longer. Verb sequences of length four
or five can be divided into two patterns, &lt;modal&gt; have
been giv(ing; en) and ... beeni being given. Adding any one
(length-four) string from the first pattern will yield the re-
maining 17 strings of that pattern. Further adding two
length-four strings from the awkward second pattern will
yield the remaining 18 strings of that pattern, nine of which
are of length five. This completes the corpus.
</bodyText>
<sectionHeader confidence="0.974354" genericHeader="discussions">
DISCUSSION
</sectionHeader>
<bodyText confidence="0.981241684210526">
The auxiliary system has often been regarded as an acid
test for a theory of language acquisition. Given this, we are
encouraged that it is in fact learnable via a computationally
efficient general method. It is significant that at least in
this domain we have found a k (of 1) that is low enough to
generate a good amount of inference from examples yet high
enough to avoid overgeneralization. Even more conservative
2-reversibility generates a little inference.
This inductive power derives from the systematic se-
quential structure of the English auxiliary system. In an
idealized form (ignoring tense and inflections) the regular
expression
(DO kmodal&gt;1 [HAVE; [BEI] [13Epassive] GIVE
generates all English verb sequence patterns in our corpus.
Zero-reversible inference basically attempts to simplify
any partial, disjunctive permutation like (a;b)i ay into an
exhaustive, combinatorial permutation like (a.b)(x;y). Since
the active corpus (excluding BE-passive from the idealized
regular expression) in fact has such a simple form except for
the DO disjunction, zero-reversible inference productively
completes the three-place permutation but also destroys the
disjunction, by overgeneralizing what patterns can follow
both DO and &lt;modal&gt;. One-reversible inference requires
that disjuncts share some final word to be mergeable, so
that DO cannot merge with any auxiliary triplet, yet the
permutation of &lt;modal&gt; IIA VE by BE is still productive.
Similar considerations obtain in the passive case, as well as
for the joint corpus. Table 2 illustrates the trade-off in this
case between inferential power and the proper handling of
exceptions.
In complex environments, rather than reduce the infer-
ential power by raising k one could instead embed this al-
gorithm within a larger system. For example, a more re-
alistic model of processing English verb sequences would
have an external, more linguistically motivated mechanism
force the separate treatment of active versus passive forms.
Then if, say on considerations of frequency of occurrence,
do exceptions were externally handled and the infrequent
</bodyText>
<tableCaption confidence="0.98945">
Table 2: Incremental k-reversible inference of some English auxiliary verb sequences.
</tableCaption>
<table confidence="0.287888592592593">
SEQUENCE OF NEW NEW STRINGS INFERRED:
STRINGS PRESENTED k = 0 k = I
• could give NONE NONE
NONE
NONE
NONE
NONE
(ALREADY INFERRED)
may have given
k=2
NONE
NONE
NONE
NONE
NONE
NONE
may give
does give
could have given
NONE
NONE
may have given
does have given
could have been giving
may have been giving
does have been giving
may have been giving
</table>
<page confidence="0.978367">
74
</page>
<bodyText confidence="0.999952361111112">
... BE being ... cases were similarly excluded from the im-
mature learner, then one could apply the more powerful
zero-reversible inference to the remaining active and passive
forms without overgeneralizing. In such a case the active
system can be induced from 18 examples out of 44 variants
and the passive system from 14 out of 22. The entire active
system is learnable once examples of each form of each verb
and each modal have been seen, plus one example to fix the
relative order of have vs. be, and one example each to fix
the order of modal vs. have or be.
Though a more complex model must ultimately repre-
sent a domain like the English auxiliary system, the way
k-reversible inference in itself handles a complex territory
satisfies some conditions of psychological fidelity. Especially
zcro-reversibility is a rather simple form of generalization
of sequential patterns with which we believe humans read-
ily identify. In general the longer, more complex cases can
be inferred from simpler cases. Also, there is a reasonable
degree of play in the composition of the covering sample,
and the order of presentation does not affect the language
learned.
Children evidently never make mistakes on the relative
order of auxiliaries, which is consistent with the reversibility
model, but they do mistakenly combine do with tensed verb
forms (Pinker, 1984). Given that the appearance of do in
declarative sentences is also fairly rare, one might prefer
the aforementioned zero-reversible system that handles do
support as an exception, rather than opt for a 1-reversible
inference which is flawless but a slower learner.
The ... BE being ... cases are systematically related
to the rest, but also have a natural boundary: 1-reversible
inference from simpler cases doesn&apos;t intrude into that ter-
ritory. yet only a few such examples allow one to infer the
remainder. Very rare sequences like could have been be-
ing given will be successfully acquired even if they are not
seen. This seems consistent with human judgments that
such phrasing is awkward but apparently legal.
k-Reversibility is essentially a model of simplicity, not of
complexity. As such, it induces not linguistic structure but
the substitution classes that linguistic structures typically
work with, building these by analogy from examples. In the
linguistic structure for which k-reversibility is defined —
regular grammars — it functions to induce the classes that
fill &amp;quot;slots&amp;quot; in a regular expression, based on the similarity
of tail sets. Increasing the value of k is a way of requiring
a higher degree of similarity before calling a match. (See
Gonzalez and Thomason, 1978, for other approaches to k-
tail inference that are not so efficient.)
The same principle can apply to the induction of substi-
tution classes in other linguistic domains including morpho-
logical, syntactic, and semantic systems. For a particularly
direct example, consider the right-hand sides of context-free
rewrite rules. Any subset of such rules having the same left-
hand side constitutes a regular language over the set of ter-
minal and nonterminal symbols, and is therefore a candidate
for induction. One might thus infer new rewrite rules from
the pattern of existing ones, thereby not only concluding
that words are members of certain simple syntactic classes,
but also simplifying a disjunctive set of rules into a more
concise set that exhibits systematic properties. Berwick&apos;s
Lparsifal system (1982) is an example of this kind of exten-
sion.
We believe that k-reversibility illustrates a psycholog-
ically plausible pattern induction process for natural lan-
guage learning that in its simplest form has an efficient
computational algorithm associated with it. The basic prin-
ciple behind k-reversible inference shows some promise as a
flexible tool within more complex models of language ac-
quisition. It is encouraging that, at least in a simple case,
computational linguistic models can suggest formal learn-
ability constraints that are natural enough to be useful in
the learning of human languages.
</bodyText>
<sectionHeader confidence="0.998566" genericHeader="acknowledgments">
ACKNOWLEDGMENTS
</sectionHeader>
<bodyText confidence="0.999651166666667">
This paper describes research done at the Artificial Intel-
ligence Laboratory of the Massachusetts Institute of Tech-
nology. Support for the laboratory&apos;s artificial intelligence
research is provided in part by the Advanced Research
Projects Agency of the Department of Defense under the
Office of Naval Research Contract N00014-80-C-0505.
</bodyText>
<sectionHeader confidence="0.999431" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.99425225">
Angluin, D., &amp;quot;Inference of reversible languages,&amp;quot; Journal
of the Association for Computing Machinery, 29(3), 741-
765, 1982.
Berwick, R., Locality Principles and the Acquisition of
Syntactic Knowledge, PhD, MIT Department of Electrical
Engineering and Computer Science, 1982.
Gold, E., &amp;quot;Complexity of Automaton Identification from
Given Data,&amp;quot; Information and Control, 37, 1978.
Gonzalez, R., and Thomason, M., Syntactic Pattern
Recognition, Reading, MA: Addison-Wesley, 1978.
Pinker, S., Language Learnability and Language Devel-
opment, Cambridge, MA: Harvard University Press, 1984.
</reference>
<page confidence="0.999134">
75
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.923853">
<title confidence="0.998859">REVERSIBLE AUTOMATA AND INDUCTION OF THE ENGLISH AUXILIARY SYSTEM</title>
<author confidence="0.9986795">Samuel F Pilato Robert C Berwick</author>
<affiliation confidence="0.999979">MIT Artificial Intelligence Laboratory</affiliation>
<address confidence="0.9978045">545 Technology Square Cambridge, MA 02139, USA</address>
<abstract confidence="0.995923588235294">In this paper we apply some recent work of Angluin (1982) to the induction of the English auxiliary verb system. In general, the induction of finite automata is computationally intractable. However, Angluin shows that restricted automata, the can be learned by efficient (polynomial time) algorithms. We present an explicit computer model demonstrating that the English auxiliary verb system can in fact be learned as a 1-reversible automaton, and hence in a computationally feasible amount of time. The entire system can be acquired by looking at half the possible auxiliary verb sequences, and the patgeneralization seems compatible with what is known about human acquisition of auxiliaries. We conclude that certain linguistic subsystems may well be learnable by inductive inference methods of this kind, and suggest an extension to context-free languages.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>Inference of reversible languages,&amp;quot;</title>
<date>1982</date>
<journal>Journal of the Association for Computing Machinery,</journal>
<volume>29</volume>
<issue>3</issue>
<pages>741--765</pages>
<contexts>
<context position="2311" citStr="Angluin, 1982" startWordPosition="347" endWordPosition="348">sh syntax. Our results suggest that particular language subsystems can be learned by general induction procedures, given certain general constraints. The problem is that these methods are in general computationally intractable. Even for regular languages induction can be exponentially difficult (Gold, 1978). This suggests that there may be general constraints on the design of certain linguistic subsystems to make them easy to learn by general inductive inference methods. We propose the constraint of k-reversibility as one such restriction. This constraint guarantees polynomial time inference (Angluin, 1982). In the remainder of this paper, we also show, by an explicit computer model, that the English auxiliary verb system meets this constraint, and so is easily inferred from a corpus. The theory gives one precise characterization of just where we may expect general inductive inference methods to be of value in language acquisition. LEARNING K-REVERSIBLE LANGUAGES FROM EXAMPLES The question we address is, If a learner presumes that a natural language domain is systematic in some way, can the learner intelligently infer the complete system from only a subset of sample sentences? Let is develop an </context>
</contexts>
<marker>Angluin, 1982</marker>
<rawString>Angluin, D., &amp;quot;Inference of reversible languages,&amp;quot; Journal of the Association for Computing Machinery, 29(3), 741-765, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Berwick</author>
</authors>
<title>Locality Principles and the Acquisition of Syntactic Knowledge,</title>
<date>1982</date>
<tech>PhD,</tech>
<institution>MIT Department of Electrical Engineering and Computer Science,</institution>
<marker>Berwick, 1982</marker>
<rawString>Berwick, R., Locality Principles and the Acquisition of Syntactic Knowledge, PhD, MIT Department of Electrical Engineering and Computer Science, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Gold</author>
</authors>
<title>Complexity of Automaton Identification from Given Data,&amp;quot;</title>
<date>1978</date>
<journal>Information and Control,</journal>
<volume>37</volume>
<contexts>
<context position="2005" citStr="Gold, 1978" startWordPosition="302" endWordPosition="303">enerally led to fruitful insights on the nature of grammars. Yet it remains to determine whether all of a language is learned in a grammar-specific manner. In this paper we show how to successfully apply one computationally efficient inductive inference algorithm to the acquisition of a domain of English syntax. Our results suggest that particular language subsystems can be learned by general induction procedures, given certain general constraints. The problem is that these methods are in general computationally intractable. Even for regular languages induction can be exponentially difficult (Gold, 1978). This suggests that there may be general constraints on the design of certain linguistic subsystems to make them easy to learn by general inductive inference methods. We propose the constraint of k-reversibility as one such restriction. This constraint guarantees polynomial time inference (Angluin, 1982). In the remainder of this paper, we also show, by an explicit computer model, that the English auxiliary verb system meets this constraint, and so is easily inferred from a corpus. The theory gives one precise characterization of just where we may expect general inductive inference methods to</context>
</contexts>
<marker>Gold, 1978</marker>
<rawString>Gold, E., &amp;quot;Complexity of Automaton Identification from Given Data,&amp;quot; Information and Control, 37, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Gonzalez</author>
<author>M Thomason</author>
</authors>
<title>Syntactic Pattern Recognition,</title>
<date>1978</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, MA:</location>
<contexts>
<context position="20363" citStr="Gonzalez and Thomason, 1978" startWordPosition="3311" endWordPosition="3314">ts that such phrasing is awkward but apparently legal. k-Reversibility is essentially a model of simplicity, not of complexity. As such, it induces not linguistic structure but the substitution classes that linguistic structures typically work with, building these by analogy from examples. In the linguistic structure for which k-reversibility is defined — regular grammars — it functions to induce the classes that fill &amp;quot;slots&amp;quot; in a regular expression, based on the similarity of tail sets. Increasing the value of k is a way of requiring a higher degree of similarity before calling a match. (See Gonzalez and Thomason, 1978, for other approaches to ktail inference that are not so efficient.) The same principle can apply to the induction of substitution classes in other linguistic domains including morphological, syntactic, and semantic systems. For a particularly direct example, consider the right-hand sides of context-free rewrite rules. Any subset of such rules having the same lefthand side constitutes a regular language over the set of terminal and nonterminal symbols, and is therefore a candidate for induction. One might thus infer new rewrite rules from the pattern of existing ones, thereby not only conclud</context>
</contexts>
<marker>Gonzalez, Thomason, 1978</marker>
<rawString>Gonzalez, R., and Thomason, M., Syntactic Pattern Recognition, Reading, MA: Addison-Wesley, 1978.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Pinker</author>
</authors>
<title>Language Learnability and Language Development,</title>
<date>1984</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="19078" citStr="Pinker, 1984" startWordPosition="3107" endWordPosition="3108">me conditions of psychological fidelity. Especially zcro-reversibility is a rather simple form of generalization of sequential patterns with which we believe humans readily identify. In general the longer, more complex cases can be inferred from simpler cases. Also, there is a reasonable degree of play in the composition of the covering sample, and the order of presentation does not affect the language learned. Children evidently never make mistakes on the relative order of auxiliaries, which is consistent with the reversibility model, but they do mistakenly combine do with tensed verb forms (Pinker, 1984). Given that the appearance of do in declarative sentences is also fairly rare, one might prefer the aforementioned zero-reversible system that handles do support as an exception, rather than opt for a 1-reversible inference which is flawless but a slower learner. The ... BE being ... cases are systematically related to the rest, but also have a natural boundary: 1-reversible inference from simpler cases doesn&apos;t intrude into that territory. yet only a few such examples allow one to infer the remainder. Very rare sequences like could have been being given will be successfully acquired even if t</context>
</contexts>
<marker>Pinker, 1984</marker>
<rawString>Pinker, S., Language Learnability and Language Development, Cambridge, MA: Harvard University Press, 1984.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>