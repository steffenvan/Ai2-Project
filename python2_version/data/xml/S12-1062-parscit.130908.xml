<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007127">
<title confidence="0.586498">
UNED: Evaluating Text Similarity Measures without Human Assessments*
</title>
<author confidence="0.7602">
Enrique Amig´o t Julio Gonzalo t Jes´us Gim´enez t Felisa Verdejot
</author>
<affiliation confidence="0.567949">
t UNED, Madrid
</affiliation>
<email confidence="0.853685">
{enrique,julio,felisa}@lsi.uned.es
</email>
<address confidence="0.552079">
t Google, Dublin
</address>
<email confidence="0.992772">
jesgim@gmail.com
</email>
<sectionHeader confidence="0.998567" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998164">
This paper describes the participation of
UNED NLP group in the SEMEVAL 2012 Se-
mantic Textual Similarity task. Our contribu-
tion consists of an unsupervised method, Het-
erogeneity Based Ranking (HBR), to combine
similarity measures. Our runs focus on com-
bining standard similarity measures for Ma-
chine Translation. The Pearson correlation
achieved is outperformed by other systems,
due to the limitation of MT evaluation mea-
sures in the context of this task. However,
the combination of system outputs that partici-
pated in the campaign produces three interest-
ing results: (i) Combining all systems without
considering any kind of human assessments
achieve a similar performance than the best
peers in all test corpora, (ii) combining the 40
less reliable peers in the evaluation campaign
achieves similar results; and (iii) the correla-
tion between peers and HBR predicts, with a
0.94 correlation, the performance of measures
according to human assessments.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999923">
Imagine that we are interested in developing com-
putable measures that estimate the semantic simi-
larity between two sentences. This is the focus of
the STS workshop in which this paper is presented.
In order to optimize the approaches, the organizers
</bodyText>
<footnote confidence="0.790773166666667">
*This work has been partially funded by the Madrid gov-
ernment, grant MA2VICMR (S-2009/TIC- 1542), the Span-
ish government, grant Holopedia (TIN2010-21128-C02-01) and
the European Community’s Seventh Framework Programme
(FP7/ 2007-2013) under grant agreement nr. 288024 (LiMo-
SINe project).
</footnote>
<bodyText confidence="0.999782794117647">
provide a training corpus with human assessments.
The participants must improve their approaches and
select three runs to participate. Unfortunately, we
can not ensure that systems will behave similarly
in both the training and test corpora. For instance,
some Pearson correlations between system achieve-
ments across test corpora in this competition are:
0.61 (MSRpar-MSRvid), 0.34 (MSRvid-SMTeur),
or 0.49 (MSRpar-SMTeur). Therefore, we cannot
expect a high correlation between the system per-
formance in a specific corpus and the test corpora
employed in the competition.
Now, imagine that we have a magic box that,
given a set of similarity measures, is able to pre-
dict which measures will obtain the highest corre-
lation with human assessments without actually re-
quiring those assessments. For instance, suppose
that putting all system outputs in the magic box, we
obtain a 0.94 Pearson correlation between the pre-
diction and the system achievements according to
human assessments, as in Figure 1. The horizontal
axis represents the magic box ouput, and the vertical
axis represents the achievement in the competition.
Each dot represents one system. In this case, we
could decide which system or system combination
to employ for a certain test set.
Is there something like this magic box? The
answer is yes. Indeed, what Figure 1 shows is
the results of an unsupervised method to combine
measures, the Heterogeneity Based Ranking (HBR).
This method is grounded on a generalization of the
heterogeneity property of text evaluation measures
proposed in (Amig´o et al., 2011), which states that
the more a set of measures is heterogeneous, the
</bodyText>
<page confidence="0.987709">
454
</page>
<note confidence="0.8248345">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 454–460,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999169">
Figure 1: Correspondence between the magic box infor-
mation and the (unknown) correlation with human assess-
ments, considering all runs in the evaluation campaign.
</figureCaption>
<bodyText confidence="0.999986380952381">
more a score increase according to all the mea-
sures is reliable. In brief, the HBR method consists
of computing the heterogeneity of the set of mea-
sures (systems) for which a similarity instance (pair
of texts) improves each of the rest of similarity in-
stances in comparison. The result is that HBR tends
to achieve a similar or higher correlation with human
assessments than the single measures. In order to
select the most appropriate single measure, we can
meta-evaluate measures in terms of correlation with
HBR, which is what the previous figure showed.
We participated in the STS evaluation campaign
employing HBR over automatic text evaluation mea-
sures (e.g. ROUGE (Lin, 2004)), which are not actu-
ally designed for this specific problem. For this rea-
son our results were suboptimal. However, accord-
ing to our experiments this method seem highly use-
ful for combining and evaluating current systems.
In this paper, we describe the HBR method and we
present experiments employing the rest of partici-
pant methods as similarity measures.
</bodyText>
<sectionHeader confidence="0.999122" genericHeader="introduction">
2 Definitions
</sectionHeader>
<subsectionHeader confidence="0.99924">
2.1 Similarity measures
</subsectionHeader>
<bodyText confidence="0.7904268">
In (Amig´o et al., 2011) a novel definition of sim-
ilarity is proposed in the context of automatic text
evaluation measures. Here we extend the definition
for text similarity problems in general.
Being Q the universe of texts d, we assume that
a similarity measure, is a function x : Q2 −�
R such that there exists a decomposition function
f : Q −� {e1..e�1 (e.g., words or other linguis-
tic units or relationships) satisfying the following
constraints; (i) maximum similarity is achieved only
when the text decomposition resembles exactly the
other text; (ii) adding one element from the second
text increases the similarity; and (iii) removing one
element that does not appear in the second text also
increases the similarity.
</bodyText>
<equation confidence="0.999926">
f(d1) = f(d2) H x(d1, d2) = 1
(f(d1) = f(di) U {e E f(d2) \ f(d1)1)
� x(di, d2) &gt; x(d1, d2)
(f(d1) = f(di) − {e E f(d1) \ f(d2)1)
� x(di, d2) &gt; x(d1, d2)
</equation>
<bodyText confidence="0.97436">
According to this definition, a random function,
or the inverse of a similarity function (e.g. 1 ),
</bodyText>
<equation confidence="0.773755">
x(d1d2)
</equation>
<bodyText confidence="0.999884545454546">
do not satisfy the similarity constraints, and there-
fore cannot be considered as similarity measures.
However, this definition covers any kind of overlap-
ping or precision/recall measure over words, syntac-
tic structures or semantic units, which is the case of
most systems here.
Our definition assumes that measures are granu-
lated: they decompose text in a certain amount of
elements (e.g. words, grammatical tags, etc.) which
are the basic representation and comparison units to
estimate textual similarity.
</bodyText>
<subsectionHeader confidence="0.995836">
2.2 Heterogeneity
</subsectionHeader>
<bodyText confidence="0.9997687">
Heterogeneity (Amig´o et al., 2011) represents to
what extent a set of measures differ from each other.
Let us refer to a pair of texts i = (i1, i2) with a
certain degree of similarity to be computed as a sim-
ilarity instance. Then we estimate the Heterogene-
ity H(X) of a set of similarity measures X as the
probability over similarity instances i = (i1, i2) and
j = (j1, j2) between distinct texts, that there exist
two measures in X that contradict each other. For-
mally:
</bodyText>
<equation confidence="0.925458">
H(X) = P%14%2 (�]x, x&apos; E X|x(i) &gt; x(j) n x&apos;(j) &lt; x&apos;(i))
715472
</equation>
<bodyText confidence="0.999735">
where x(i) stands for the similarity, according to
measure x, between the texts i1, i2.
</bodyText>
<page confidence="0.7862845">
455
3 Proposal: Heterogeneity-Based Applying the heterogeneity principle we can esti-
</page>
<bodyText confidence="0.971226166666667">
Similarity Ranking mate this as:
The heterogeneity property of text evaluation mea-
sures (in fact, text similarity measures to human ref-
erences) introduced in (Amig´o et al., 2011) states
that the quality difference between two texts is lower
bounded by the heterogeneity of the set of evalua-
tion measures that corroborate the quality increase.
Based on this, we define the Heterogeneity Principle
which is applied to text similarity in general as: the
probability of a real similarity increase between ran-
dom text pairs is correlated with the Heterogeneity
of the set of measures that corroborate this increase:
</bodyText>
<equation confidence="0.948711">
P(h(i) &gt; h(j)) — H({x|x(i) &gt; x(j)})
</equation>
<construct confidence="0.62342925">
where h(i) is the similarity between i1, i2 accord-
ing to human assessments (gold standard). In addi-
tion, the probability is maximal if the heterogeneity
is maximal:
</construct>
<equation confidence="0.978902">
H({x|x(i) &gt; x(j)}) = 1 =� P(h(i) &gt; h(j)) = 1
</equation>
<bodyText confidence="0.9990425">
The first part is derived from the fact that increas-
ing Heterogeneity requires additional diverse mea-
sures corroborating the similarity increase. The di-
rect relationship is the result of assuming that a sim-
ilarity increase according to any aspect is always a
positive evidence of true similarity. In other words,
a positive match between two texts according to any
feature can never be a negative evidence of similar-
ity.
As for the second part, if the heterogeneity of a
measure set is maximal, then the condition of the
heterogeneity definition holds for any pair of dis-
tinct documents (i1 =� i2 and j1 =� j2). Given that
all measures corroborate the similarity increase, the
heterogeneity condition does not hold. Then, the
compared texts in (i1, i2) are not different. There-
fore, we can ensure that P(h(i) &gt; h(j)) = 1.
The proposal in this paper consists of rank-
ing similarity instances by estimating, for each in-
stance i, the average probability of its texts (i1, i2)
being closer to each other than texts in a different
instance j:
</bodyText>
<equation confidence="0.9999255">
R(i) = Avgj(P(h(i) &gt; h(j)))
HBRX(i) = Avgj(H({x|x(i) &gt; x(j)}))
</equation>
<bodyText confidence="0.999538333333333">
We refer to this ranking function as the Heterogene-
ity Based Ranking (HBR). It satisfies three crucial
properties for a measure combining function:
</bodyText>
<listItem confidence="0.99823725">
1. HBR is independent from measure scales and
it does not require relative weighting schemes
between measures. Formally, being f any strict
growing function:
</listItem>
<equation confidence="0.929952333333333">
HBRx,..xn(i) = HBRx,..f(xn)(i)
2. HBR is not sensitive to redundant measures:
HBRx,..xn(i) = HBRx,..xn,xn(i)
</equation>
<listItem confidence="0.9968086">
3. Given a large enough set of similarity
instances, HBR is not sensitive to non-
informative measures. Being xr a random
function such that P(xr(i) &gt; xr(j)) = 12,
then:
</listItem>
<equation confidence="0.981937">
HBRx,..xn(i) — HBRx,..xn,xr(i)
</equation>
<bodyText confidence="0.999492166666667">
The first two properties are trivially satisfied: the
I operator in H and the score comparisons are not af-
fected by redundant measures nor their scales prop-
erties. Regarding the third property, the heterogene-
ity of a set of measures plus a random function xr
is:
</bodyText>
<equation confidence="0.9961952">
H(X U {xr}) �
Pi1Oi2 (fix, x� E X U {xr}1x(z) &gt; x(j) ∧ x�(j) &lt; x�(z)) =
j1Xj2
H(X) + (1 − H(X)) * 2 = H(X)
+ 1 2
</equation>
<bodyText confidence="0.999871285714286">
That is, the heterogeneity grows proportionally
when including a random function. Assuming that
the random function corroborates the similarity in-
crease in a half of cases, the result is a proportional
relationship between HBR and HBR with the addi-
tional measure. Note that we need to assume a large
enough amount of data to avoid random effects.
</bodyText>
<page confidence="0.999265">
456
</page>
<sectionHeader confidence="0.999078" genericHeader="method">
4 Official Runs
</sectionHeader>
<bodyText confidence="0.999965088888889">
We have applied the HBR method with excellent
results in different tasks such as Machine Transla-
tion and Summarization evaluation measures, Infor-
mation Retrieval and Document Clustering. How-
ever, we had not previously applied our method to
semantic similarity. Therefore, we decided to ap-
ply directly automatic evaluation measures for Ma-
chine Translation as single similarity measures to be
combined by means of HBR. We have used 64 auto-
matic evaluation measures provided by the ASIYA
Toolkit (Gim´enez and M`arquez, 2010)1. This set in-
cludes measures operating at different linguistic lev-
els (lexical, syntactic, and semantic) and includes all
popular measures (BLEU, NIST, GTM, METEOR,
ROUGE, etc.) The similarity formal constraints in
this set of measures is preserved by considering lex-
ical overlap when the target linguistic elements (i.e.
named entities) do not appear in the texts.
We participated with three runs. The first one con-
sisted of selecting the best measure according to hu-
man assessments in the training corpus. It was the
INIST measure (Doddington, 2002). The second run
consisted of selecting the best 34 measures in the
training corpus and combining them with HBR, and
the last run consisted of combining all evaluation
measures with HBR. The heterogeneity of measures
was computed over 1000 samples of similarity in-
stance pairs (pairs of sentences pairs) extracted from
the five test sets. Similarity instances were ranked
over each test set independently.
In essence, the main contribution of these runs is
to corroborate that Machine Translation evaluation
measures are not enough to solve this task. Our runs
appear at the Mean Rank positions 42, 28 and 77.
Apart of this, our results corroborate our main hy-
pothesis: without considering human assessment or
any kind of supervised tunning, combining the mea-
sures with HBR resembles the best measure (INIST)
in the combined measure set. However, when in-
cluding all measures the evaluation result decreases
(rank 77). The reason is that some Machine Trans-
lation evaluation measures do not represent a posi-
tive evidence of semantic similarity in this corpus.
Therefore, the HBR assumptions are not satisfied
and the final correlation achieved is lower. In sum-
</bodyText>
<footnote confidence="0.989089">
1http://www.lsi.upc.edu/ nlp/Asiya
</footnote>
<bodyText confidence="0.9997715">
mary, our approach is suitable if we can ensure that
all measures (systems) combined are at least a posi-
tive (high or low) evidence of semantic similarity.
But let us focus on the HBR behavior when com-
bining participant measures, which are specifically
designed to address this problem.
</bodyText>
<sectionHeader confidence="0.854938" genericHeader="method">
5 Experiment with Participant Systems
</sectionHeader>
<subsectionHeader confidence="0.999397">
5.1 Combining System Outputs
</subsectionHeader>
<bodyText confidence="0.999983394736842">
We can confirm empirically in the official results
that all participants runs are positive evidence of se-
mantic similarity. That is, they achieve a correlation
with human assessments higher than 0. Therefore,
the conditions to apply HBR are satisfied. Our goal
now is to resemble the best performance without ac-
cessing human assessments neither from the training
nor the test corpora. Figure 2 illustrates the Pear-
son correlation (averaged across test sets) achieved
by single measures (participants) and all peers com-
bined in an unsupervised manner by HBR (black
column). As the figure shows, HBR results are com-
parable with the best systems appearing in the ninth
position. In addition, Figure 4 shows the differences
over particular test sets between HBR and the best
system. The figure shows that there are not con-
sistent differences between these approaches across
test beds.
The next question is why HBR is not able to im-
prove the best system. Our intuition is that, in this
test set, average quality systems do not contribute
with additional information. That is, the similarity
aspects that the average quality systems are able to
capture are also captured by the best system.
However, the best system within the combined set
is not a theoretical upper bound for HBR. We can
prove it with the following experiment. We apply
HBR considering only the 40 less predictive systems
in the set (the rest of measures are not considered
when computing HBR). Then we compare the re-
sults of HBR regarding the considered single sys-
tems. As Figure 3 shows, HBR improves substan-
tially all single systems achieving the same result
than when combining all systems (0.61). The rea-
son is that all these systems are positive evidences
but they consider partial similarity aspects. But the
most important issue here is that combining the 40
less predictive systems in the evaluation campaign
</bodyText>
<page confidence="0.997657">
457
</page>
<figureCaption confidence="0.9953836">
Figure 2: Measures (runs) and HBR sorted by average correlation with human assessments.
Figure 3: 40 less predictive measures (runs) and HBR
sorted by average correlation with human assessments.
Figure 4: Average correlation with human assessments
for the best runs and HBR.
</figureCaption>
<bodyText confidence="0.999928181818182">
is enough to achieve high final scores. This means
that the drawback of these measures as a whole is
not what information is employed but how this in-
formation is scaled and combined. This drawback is
solved by the HBR approach.
In summary, the main conclusion that we can ex-
tract from these results is that, in the absence of hu-
man assessments, HBR ensures a high performance
without the risk derived from employing potentially
biased training corpora or measures based on partial
similarity aspects.
</bodyText>
<sectionHeader confidence="0.9826745" genericHeader="method">
6 An Unsupervised Meta-evaluation
Method
</sectionHeader>
<bodyText confidence="0.999968388888889">
But HBR has an important drawback: its computa-
tional cost, which is O(n4 * m), being n the number
of texts involved in the computation and m the num-
ber of measures. The reason is that computing H is
quadratic with the number of texts, and the method
requires to compute H for every pair of texts. In
addition, HBR does not improve the best systems.
However, HBR can be employed as an unsuper-
vised evaluation method. For this, it is enough to
compute the Pearson correlation between runs and
HBR. This is what Figure 1 showed at the beginning
of this article. For each dot (participant run), the
horizontal axis represent the correlation with HBR
(magic box) and the vertical axis represent the cor-
relation with human assessments. This graph has a
Pearson correlation of 0.94 between both variables.
In other words, without accessing human assess-
ments, this method is able to predict the quality of
</bodyText>
<page confidence="0.997506">
458
</page>
<figureCaption confidence="0.999662">
Figure 5: Predicting the quality of measures over a single test set.
</figureCaption>
<bodyText confidence="0.999951625">
textual similarity system with a 0.94 of accuracy in
this test bed.
In this point, we have two options for optimiz-
ing systems. First, we can optimize measures ac-
cording to the results achieved in an annotated train-
ing corpus. The other option consists of considering
the correlation with HBR in the test corpus. In or-
der to compare both approaches we have developed
the following experiment. Given a test corpus t, we
compute the correlation between system scores in t
versus a training corpus t&apos;. This approach emulates
the scenario of training systems over a (training) set
and evaluating over a different (test) set. We also
compute the correlation between system scores in
all corpora vs. the scores in t. Finally, we compute
the correlation between system scores in t and our
predictor in t (which is the correlation system/HBR
across similarity instances in t). This approach em-
ulates the use of HBR as unsupervised optimization
method.
Figure 5 shows the results. The horizontal axis
represents the test set t. The black columns rep-
resent the prediction over HBR in the correspond-
ing test set. The grey columns represent the predic-
tion by using the average correlation across test sets.
The light grey columns represents the prediction us-
ing the correlation with humans in other single test
set. Given that there are five test sets, the figure in-
cludes four grey columns for each test set. The fig-
ure clearly shows the superiority of HBR as measure
quality predictor, even when it does not employ hu-
man assessments.
</bodyText>
<sectionHeader confidence="0.999807" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.999864666666667">
The Heterogeneity Based Ranking provides a mech-
anism to combine similarity measures (systems)
without considering human assessments. Interest-
ingly, the combined measure always improves or
achieves similar results than the best single measure
in the set. The main drawback is its computational
cost. However, the correlation between single mea-
sures and HBR predicts with a high confidence the
accuracy of measures regarding human assessments.
Therefore, HBR is a very useful tool when optimiz-
ing systems, specially when a representative training
corpus is not available. In addition, our results shed
some light on the contribution of measures to the
task. According to our experiments, the less reliable
measures as a whole can produce reliable results if
they are combined according to HBR.
The HBR software is available at
http://nlp.uned.es/∼enrique/
</bodyText>
<sectionHeader confidence="0.999475" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997922">
Enrique Amig´o, Julio Gonzalo, Jesus Gimenez, and Fe-
lisa Verdejo. 2011. Corroborating text evaluation re-
sults with heterogeneous measures. In Proceedings of
the 2011 Conference on Empirical Methods in Natu-
ral Language Processing, pages 455–466, Edinburgh,
Scotland, UK., July. Association for Computational
Linguistics.
George Doddington. 2002. Automatic Evaluation
of Machine Translation Quality Using N-gram Co-
Occurrence Statistics. In Proceedings of the 2nd Inter-
</reference>
<page confidence="0.988784">
459
</page>
<reference confidence="0.999280583333333">
national Conference on Human Language Technology,
pages 138–145.
Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya:
An Open Toolkit for Automatic Machine Translation
(Meta-)Evaluation. The Prague Bulletin of Mathemat-
ical Linguistics, (94):77–86.
Chin-Yew Lin. 2004. Rouge: A Package for Auto-
matic Evaluation of Summaries. In Marie-Francine
Moens and Stan Szpakowicz, editors, Text Summariza-
tion Branches Out: Proceedings of the ACL-04 Work-
shop, pages 74–81, Barcelona, Spain, July. Associa-
tion for Computational Linguistics.
</reference>
<page confidence="0.999213">
460
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.884724">
<title confidence="0.998669">Evaluating Text Similarity Measures without Human</title>
<author confidence="0.997207">Amig´o Gonzalo Gim´enez</author>
<affiliation confidence="0.946154">Madrid Dublin</affiliation>
<email confidence="0.999573">jesgim@gmail.com</email>
<abstract confidence="0.999396608695652">This paper describes the participation of UNED NLP group in the SEMEVAL 2012 Semantic Textual Similarity task. Our contribution consists of an unsupervised method, Heterogeneity Based Ranking (HBR), to combine similarity measures. Our runs focus on combining standard similarity measures for Machine Translation. The Pearson correlation achieved is outperformed by other systems, due to the limitation of MT evaluation measures in the context of this task. However, the combination of system outputs that participated in the campaign produces three interesting results: (i) Combining all systems without considering any kind of human assessments achieve a similar performance than the best peers in all test corpora, (ii) combining the 40 less reliable peers in the evaluation campaign achieves similar results; and (iii) the correlation between peers and HBR predicts, with a 0.94 correlation, the performance of measures according to human assessments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Enrique Amig´o</author>
<author>Julio Gonzalo</author>
<author>Jesus Gimenez</author>
<author>Felisa Verdejo</author>
</authors>
<title>Corroborating text evaluation results with heterogeneous measures.</title>
<date>2011</date>
<booktitle>In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>455--466</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Edinburgh, Scotland, UK.,</location>
<marker>Amig´o, Gonzalo, Gimenez, Verdejo, 2011</marker>
<rawString>Enrique Amig´o, Julio Gonzalo, Jesus Gimenez, and Felisa Verdejo. 2011. Corroborating text evaluation results with heterogeneous measures. In Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 455–466, Edinburgh, Scotland, UK., July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Doddington</author>
</authors>
<title>Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2nd International Conference on Human Language Technology,</booktitle>
<pages>138--145</pages>
<contexts>
<context position="11405" citStr="Doddington, 2002" startWordPosition="1851" endWordPosition="1852">sures provided by the ASIYA Toolkit (Gim´enez and M`arquez, 2010)1. This set includes measures operating at different linguistic levels (lexical, syntactic, and semantic) and includes all popular measures (BLEU, NIST, GTM, METEOR, ROUGE, etc.) The similarity formal constraints in this set of measures is preserved by considering lexical overlap when the target linguistic elements (i.e. named entities) do not appear in the texts. We participated with three runs. The first one consisted of selecting the best measure according to human assessments in the training corpus. It was the INIST measure (Doddington, 2002). The second run consisted of selecting the best 34 measures in the training corpus and combining them with HBR, and the last run consisted of combining all evaluation measures with HBR. The heterogeneity of measures was computed over 1000 samples of similarity instance pairs (pairs of sentences pairs) extracted from the five test sets. Similarity instances were ranked over each test set independently. In essence, the main contribution of these runs is to corroborate that Machine Translation evaluation measures are not enough to solve this task. Our runs appear at the Mean Rank positions 42, 2</context>
</contexts>
<marker>Doddington, 2002</marker>
<rawString>George Doddington. 2002. Automatic Evaluation of Machine Translation Quality Using N-gram CoOccurrence Statistics. In Proceedings of the 2nd International Conference on Human Language Technology, pages 138–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jes´us Gim´enez</author>
<author>Llu´ıs M`arquez</author>
</authors>
<title>Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics,</title>
<date>2010</date>
<marker>Gim´enez, M`arquez, 2010</marker>
<rawString>Jes´us Gim´enez and Llu´ıs M`arquez. 2010. Asiya: An Open Toolkit for Automatic Machine Translation (Meta-)Evaluation. The Prague Bulletin of Mathematical Linguistics, (94):77–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chin-Yew Lin</author>
</authors>
<title>Rouge: A Package for Automatic Evaluation of Summaries.</title>
<date>2004</date>
<booktitle>In Marie-Francine Moens and Stan Szpakowicz, editors, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop,</booktitle>
<pages>74--81</pages>
<publisher>Association for Computational Linguistics.</publisher>
<location>Barcelona, Spain,</location>
<contexts>
<context position="4399" citStr="Lin, 2004" startWordPosition="674" endWordPosition="675">, the HBR method consists of computing the heterogeneity of the set of measures (systems) for which a similarity instance (pair of texts) improves each of the rest of similarity instances in comparison. The result is that HBR tends to achieve a similar or higher correlation with human assessments than the single measures. In order to select the most appropriate single measure, we can meta-evaluate measures in terms of correlation with HBR, which is what the previous figure showed. We participated in the STS evaluation campaign employing HBR over automatic text evaluation measures (e.g. ROUGE (Lin, 2004)), which are not actually designed for this specific problem. For this reason our results were suboptimal. However, according to our experiments this method seem highly useful for combining and evaluating current systems. In this paper, we describe the HBR method and we present experiments employing the rest of participant methods as similarity measures. 2 Definitions 2.1 Similarity measures In (Amig´o et al., 2011) a novel definition of similarity is proposed in the context of automatic text evaluation measures. Here we extend the definition for text similarity problems in general. Being Q th</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Chin-Yew Lin. 2004. Rouge: A Package for Automatic Evaluation of Summaries. In Marie-Francine Moens and Stan Szpakowicz, editors, Text Summarization Branches Out: Proceedings of the ACL-04 Workshop, pages 74–81, Barcelona, Spain, July. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>