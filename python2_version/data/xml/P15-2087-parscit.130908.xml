<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.027757">
<title confidence="0.996453">
MT Quality Estimation for Computer-assisted Translation:
Does it Really Help?
</title>
<author confidence="0.789485">
Marco Turchi, Matteo Negri, Marcello Federico
</author>
<address confidence="0.4604045">
FBK - Fondazione Bruno Kessler,
Via Sommarive 18, 38123 Trento, Italy
</address>
<email confidence="0.997899">
{turchi,negri,federico}@fbk.eu
</email>
<sectionHeader confidence="0.99477" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953148148148">
The usefulness of translation quality es-
timation (QE) to increase productivity
in a computer-assisted translation (CAT)
framework is a widely held assumption
(Specia, 2011; Huang et al., 2014). So far,
however, the validity of this assumption
has not been yet demonstrated through
sound evaluations in realistic settings. To
this aim, we report on an evaluation in-
volving professional translators operating
with a CAT tool in controlled but natural
conditions. Contrastive experiments are
carried out by measuring post-editing time
differences when: i) translation sugges-
tions are presented together with binary
quality estimates, and ii) the same sug-
gestions are presented without quality in-
dicators. Translators’ productivity in the
two conditions is analysed in a principled
way, accounting for the main factors (e.g.
differences in translators’ behaviour, qual-
ity of the suggestions) that directly impact
on time measurements. While the gen-
eral assumption about the usefulness of
QE is verified, significance testing results
reveal that real productivity gains can be
observed only under specific conditions.
</bodyText>
<sectionHeader confidence="0.998882" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999950270833334">
Machine translation (MT) quality estimation aims
to automatically predict the expected time (e.g. in
seconds) or effort (e.g. number of editing opera-
tions) required to correct machine-translated sen-
tences into publishable translations (Specia et al.,
2009; Mehdad et al., 2012; Turchi et al., 2014a;
C. de Souza et al., 2015). In principle, the task
has a number of practical applications. An intu-
itive one is speeding-up the work of human trans-
lators operating with a CAT tool, a software de-
signed to support and facilitate the translation pro-
cess by proposing suggestions that can be edited
by the user. The idea is that, since the suggestions
can be useful (good, hence post-editable) or use-
less (poor, hence requiring complete re-writing),
reliable quality indicators could help to reduce the
time spent by the user to decide which action to
take (to correct or re-translate).
So far, despite the potential practical benefits,
the progress in QE research has not been followed
by conclusive results that demonstrate whether the
use of quality labels can actually lead to noticeable
productivity gains in the CAT framework. To the
best of our knowledge, most prior works limit the
analysis to the intrinsic evaluation of QE perfor-
mance on gold-standard data (Callison-Burch et
al., 2012; Bojar et al., 2013; Bojar et al., 2014).
On-field evaluation is indeed a complex task, as
it requires: i) the availability of a CAT tool ca-
pable to integrate MT QE functionalities, ii) pro-
fessional translators used to MT post-editing, iii)
a sound evaluation protocol to perform between-
subject comparisons,1 and iv) robust analysis tech-
niques to measure statistical significance under
variable conditions (e.g. differences in users’ post-
editing behavior).
To bypass these issues, the works more closely
related to our investigation resort to controlled and
simplified evaluation protocols. For instance, in
(Specia, 2011) the impact of QE predictions on
translators’ productivity is analysed by measuring
the number of words that can be post-edited in a
fixed amount of time. The evaluation, however,
only concentrates on the use of QE to rank MT
outputs, and the gains in translation speed are mea-
sured against the contrastive condition in which no
QE-based ranking mechanism is used. In this arti-
ficial scenario, the analysis disregards the relation
</bodyText>
<footnote confidence="0.989995666666667">
1Notice that the same sentence cannot be post-edited
twice (e.g. with/without quality labels) by the same translator
without introducing a bias in the time measurements.
</footnote>
<page confidence="0.571455">
530
</page>
<bodyText confidence="0.890480944444444">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 530–535,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
between the usefulness of QE and the intrinsic fea-
tures of the top-ranked translations (e.g. sentence
length, quality of the MT). More recently, Huang
et al. (2014) claimed a 10% productivity increase
when translation is supported by the estimates of
an adaptive QE model. Their analysis, however,
compares a condition in which MT suggestions are
presented with confidence labels (the two factors
are not decoupled) against the contrastive condi-
tion in which no MT suggestion is presented at all.
Significance testing, moreover, is not performed.
The remainder of this work describes our
on-field evaluation addressing (through objective
measurements and robust significance tests) the
two key questions:
</bodyText>
<listItem confidence="0.9999185">
• Does QE really help in the CAT scenario?
• If yes, under what conditions?
</listItem>
<sectionHeader confidence="0.875902" genericHeader="method">
2 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999802555555556">
One of the key questions in utilising QE in the
CAT scenario is how to relay QE information to
the user. In our experiments, we evaluate a way of
visualising MT quality estimates that is based on a
color-coded binary classification (green vs. red) as
an alternative to real-valued quality labels. In our
context, ‘green’ means that post-editing the trans-
lation is expected to be faster than translation from
scratch, while ‘red’ means that post-editing the
translation is expected to take longer than trans-
lating from scratch.
This decision rests on the assumption that the
two-color scheme is more immediate than real-
valued scores, which require some interpretation
by the user. Analysing the difference between al-
ternative visualisation schemes, however, is cer-
tainly an aspect that we want to explore in the fu-
ture.
</bodyText>
<subsectionHeader confidence="0.976521">
2.1 The CAT Framework
</subsectionHeader>
<bodyText confidence="0.999984133333333">
To keep the experimental conditions as natural as
possible, we analyse the impact of QE labels on
translators’ productivity in a real CAT environ-
ment. To this aim, we use the open-source Mate-
Cat tool (Federico et al., 2014), which has been
slightly changed in two ways. First, the tool has
been adapted to provide only one single transla-
tion suggestion (MT output) per segment, instead
of the usual three (one MT suggestion plus two
Translation Memory matches). Second, each sug-
gestion is presented with a colored flag (green for
good, red for bad), which indicates its expected
quality and usefulness to the post-editor. In the
contrastive condition (no binary QE visualization),
grey is used as the neutral and uniform flag color.
</bodyText>
<subsectionHeader confidence="0.99851">
2.2 Getting binary quality labels.
</subsectionHeader>
<bodyText confidence="0.999880575">
The experiment is set up for a between-subject
comparison on a single long document as follows.
First, the document is split in two parts. The
first part serves as the training portion for a bi-
nary quality estimator; the second part is re-
served for evaluation. The training portion is
machine-translated with a state-of-the-art, phrase-
based Moses system (Koehn et al., 2007)2 and
post-edited under standard conditions (i.e. with-
out visualising QE information) by the same users
involved in the testing phase. Based on their post-
edits, the raw MT output samples are then la-
beled as ‘good’ or ‘bad’ by considering the HTER
(Snover et al., 2006) calculated between raw MT
output and its post-edited version.3 Our labeling
criterion follows the empirical findings of (Turchi
et al., 2013; Turchi et al., 2014b), which indicate
an HTER value of 0.4 as boundary between post-
editable (HTER ≤ 0.4) and useless suggestions
(HTER&gt; 0.4).
Then, to model the subjective concept of qual-
ity of different subjects, for of each translator
we train a separate binary QE classifier on the
labeled samples. For this purpose we use the
Scikit-learn implementation of support vector ma-
chines (Pedregosa et al., 2011), training our mod-
els with the 17 baseline features proposed by Spe-
cia et al. (2009). This feature set mainly takes
into account the complexity of the source sentence
(e.g. number of tokens, number of translations per
source word) and the fluency of the target trans-
lation (e.g. language model probabilities). The
features are extracted from the data available at
prediction time (source text and raw MT output)
by using an adapted version (Shah et al., 2014)
of the open-source QuEst software (Specia et al.,
2013). The SVM parameters are optimized by
cross-validation on the training set.
With these classifiers, we finally assign quality
flags to the raw segment translations in the test
</bodyText>
<footnote confidence="0.992475666666667">
2The system was trained with 60M running words from
the same domain (Information Technology) of the input doc-
ument.
3HTER measures the minimum edit distance (# word In-
sertions + Deletions + Substitutions + Shifts / # Reference
Words) between the MT output and its manual post-edition.
</footnote>
<page confidence="0.985874">
531
</page>
<table confidence="0.990983">
Average PET colored 8.086 p = 0.33
(sec/word) grey 9.592
% Wins 51.7 p = 0.039
of colored
</table>
<tableCaption confidence="0.99914">
Table 1: Comparison (Avg. PET and ranking) be-
</tableCaption>
<bodyText confidence="0.9577492">
tween the two testing conditions (with and without
QE labels).
portion of the respective document, which is even-
tually sent to each post-editor to collect time and
productivity measurements.
</bodyText>
<subsectionHeader confidence="0.999433">
2.3 Getting post-editing time measurements.
</subsectionHeader>
<bodyText confidence="0.99980275">
While translating the test portion of the docu-
ment, each translator is given an even and ran-
dom distribution of segments labeled according to
the test condition (colored flags) and segments la-
beled according to the baseline, contrastive condi-
tion (uniform grey flags). In the distribution of the
data, some constraints were identified to ensure
the soundness of the evaluation in the two condi-
tions: i) each translator must post-edit all the seg-
ments of the test portion of the document, ii) each
translator must post-edit the segments of the test
set only once, iii) all translators must post-edit the
same amount of segments with colored and grey
labels. After post-editing, the post-editing times
are analysed to assess the impact of the binary col-
oring scheme on translators’ productivity.
</bodyText>
<sectionHeader confidence="0.999956" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999794">
We applied our procedure on an English user man-
ual (Information Technology domain) to be trans-
lated into Italian. Post-editing was performed in-
dependently by four professional translators, so
that two measurements (post-editing time) for
each segment and condition could be collected.
Training and and test respectively contained 542
and 847 segments. Half of the 847 test segments
were presented with colored QE flags, with a ra-
tio of green to red labels of about 75% ‘good’ and
25% ’bad’.
</bodyText>
<subsectionHeader confidence="0.999781">
3.1 Preliminary analysis
</subsectionHeader>
<bodyText confidence="0.999995538461539">
Before addressing our research questions, we per-
formed a preliminary analysis aimed to verify the
reliability of our experimental protocol and the
consequent findings. Indeed, an inherent risk of
presenting post-editors with an unbalanced distri-
bution of colored flags is to incur in unexpected
subconscious effects. For instance, green flags
could be misinterpreted as a sort of pre-validation,
and induce post-editors to spend less time on
the corresponding segments (by producing fewer
changes). To check this hypothesis we compared
the HTER scores obtained in the two conditions
(colored vs. grey flags), assuming that noticeable
differences would be evidence of unwanted psy-
chological effects. The very close values mea-
sured in the two conditions (the average HTER is
respectively 23.9 and 24.1) indicate that the pro-
fessional post-editors involved in the experiment
did what they were asked for, by always changing
what had to be corrected in the proposed sugges-
tions, independently from the color of the associ-
ated flags. In light of this, post-editing time varia-
tions in different conditions can be reasonably as-
cribed to the effect of QE labels on the time spent
by the translators to decide whether correcting or
re-translating a given suggestion.
</bodyText>
<subsectionHeader confidence="0.993558">
3.2 Does QE Really Help?
</subsectionHeader>
<bodyText confidence="0.999920681818182">
To analyse the impact of our quality estimates on
translators’ productivity, we first compared the av-
erage post-editing time (PET – seconds per word)
under the two conditions (colored vs. grey flags).
The results of this rough, global analysis are re-
ported in Table 1, first row. As can be seen, the av-
erage PET values indicate a productivity increase
of about 1.5 seconds per word when colored flags
are provided. Significance tests, however, indicate
that such increase is not significant (p &gt; 0.05,
measured by approximate randomization (Noreen,
1989; Riezler and Maxwell, 2005)).
An analysis of the collected data to better un-
derstand these results and the rather high average
PET values observed (8 to 9.5 secs. per word) evi-
denced both a large number of outliers, and a high
PET variability across post-editors.4 To check
whether these factors make existing PET differ-
ences opaque to our study, we performed further
analysis by normalizing the PET of each transla-
tor with the robust z-score technique (Rousseeuw
and Leroy, 1987).5 The twofold advantage of
</bodyText>
<footnote confidence="0.986607888888889">
4We consider as outliers the segments with a PET lower
than 0.5 or higher than 30. Segments with unrealistically
short post-editing times may not even have been read com-
pletely, while very long post-editing times suggest that the
post-editor interrupted his/her work or got distracted. The
average PET for the four post-editors ranges from 2.266 to
13.783. In total, 48 segments have a PET higher than 30, and
6 segments were post-edited in more than 360 seconds.
5For each post-editor, it is computed by removing from
</footnote>
<page confidence="0.990947">
532
</page>
<figure confidence="0.965603133333333">
% Wins of Colored
49
48
47
46
56
55
54
53
52
51
50
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
LONG MEDIUM SHORT
HTER
% Wins of Colored
49
48
47
46
56
55
54
53
52
51
50
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
LONG MEDIUM SHORT
HTER
</figure>
<figureCaption confidence="0.999238">
Figure 1: % wins of colored with respect to length and quality of MT output. Left: all pairs. Right: only
pairs with correct color predictions.
</figureCaption>
<bodyText confidence="0.99993">
this method is to mitigate idiosyncratic differences
in translators’ behavior, and reduce the influence
of outliers. To further limit the impact of out-
liers, we also moved from a comparison based
on average PET measurements to a ranking-based
method in which we count the number of times
the segments presented with colored flags were
post-edited faster than those presented with grey
flags. For each of the (PET colored, PET grey)
pairs measured for the test segments, the percent-
age of wins (i.e. lower time) of PET colored is
calculated. As shown in the second row of Ta-
ble 1, a small but statistically significant difference
between the two conditions indeed exists.
Although the usefulness of QE in the CAT
framework seems hence to be verified, the extent
of its contribution is rather small (51.7% of wins).
This motivates an additional analysis, aimed to
verify if such marginal global gains hide larger lo-
cal productivity improvements under specific con-
ditions.
</bodyText>
<subsectionHeader confidence="0.995786">
3.3 Under what Conditions does QE Help?
</subsectionHeader>
<bodyText confidence="0.999849212765958">
To address this question, we analysed two im-
portant factors that can influence translators’ pro-
ductivity measurements: the length (number of
tokens) of the source sentences and the quality
(HTER) of the proposed MT suggestions. To
this aim, all the (PET colored, PET grey) pairs
were assigned to three bins based on the length of
the source sentences: short (length&lt;5), medium
(5&lt;length&lt;20), and long (length&gt;20). Then, in
each bin, ten levels of MT quality were identi-
fied (HTER &lt; 0.1, 0.2, ..., 1). Finally, for each
bin and HTER threshold, we applied the ranking-
the PET of each segment the post-editor median and dividing
by the post-editor median absolute deviation (MAD).
based method described in the previous section.
The left plot of Figure 1 shows how the “% wins
of colored” varies depending on the two factors on
all the collected pairs. As can be seen, for MT sug-
gestions of short and medium length the percent-
age of wins is always above 50%, while its value is
systematically lower for the long sentences when
HTER&gt;0.1. However, the differences are statis-
tically significant only for medium-length sugges-
tions, and when HTER&gt;0.1. Such condition, in
particular when 0.2&lt;HTER&lt;0.5, seems to rep-
resent the ideal situation in which QE labels can
actually contribute to speed-up translators’ work.
Indeed, in terms of PET, the average productiv-
ity gain of 0.663 secs. per word measured in the
[0.2 − 0.5] HTER interval is statistically signifi-
cant.
Although our translator-specific binary QE clas-
sifiers (see Section 2) have acceptable perfor-
mance (on average 80% accuracy on the test data
for all post-editors),6 to check the validity of our
conclusions we also investigated if, and to what
extent, our results are influenced by classification
errors. To this aim, we removed from the three
bins those pairs that contain a misclassified in-
stance (i.e. the pairs in which there is a mismatch
between the predicted label and the true HTER
measured after post-editing).7
The results obtained by applying our ranking-
based method to the remaining pairs are shown in
the right plot of Figure 1. In this “ideal”, error-free
scenario the situation slightly changes (unsurpris-
ingly, the “% wins of colored” slightly increases,
</bodyText>
<footnote confidence="0.9978038">
6Measured by comparing each predicted binary label with
the ‘true’ label obtained applying the 0.4 HTER threshold as
a separator between good and bad MT suggestions.
7The three bins contained 502, 792, 214 pairs before mis-
classification removal and 339, 604, 160 pairs after cleaning.
</footnote>
<page confidence="0.998158">
533
</page>
<bodyText confidence="0.999815714285714">
especially for long suggestions for which we have
the highest number of misclassifications), but the
overall conclusions remain the same. In particular,
the higher percentage of wins is statistically sig-
nificant only for medium-length suggestions with
HTER&gt;0.1 and, in the best case (HTER&lt;0.2) it is
about 56.0%.
</bodyText>
<sectionHeader confidence="0.994103" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.99995345">
We presented the results of an on-field evalua-
tion aimed to verify the widely held assumption
that QE information can be useful to speed-up
MT post-editing in the CAT scenario. Our results
suggest that this assumption should be put into
perspective. On one side, global PET measure-
ments do not necessarily show statistically signif-
icant productivity gains,8 indicating that the con-
tribution of QE falls below expectations (our first
contribution). On the other side, an in-depth anal-
ysis abstracting from the presence of outliers and
the high variability across post-editors, indicates
that the usefulness of QE is verified, at least to
some extent (our second contribution). Indeed,
the marginal productivity gains observed with QE
at a global level become statistically significant in
specific conditions, depending on the length (be-
tween 5 and 20 words) of the source sentences and
the quality (0.2&lt;HTER&lt;0.5) of the proposed MT
suggestions (our third contribution).
</bodyText>
<sectionHeader confidence="0.995136" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99986625">
This work has been partially supported by the EC-
funded projects MateCat (FP7 grant agreement
no. 287688) and QT21 (H2020 innovation pro-
gramme, grant agreement no. 645452).
</bodyText>
<sectionHeader confidence="0.997921" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999186811594203">
Ondrej Bojar, Christian Buck, Chris Callison-Burch,
Christian Federmann, Barry Haddow, Philipp
Koehn, Christof Monz, Matt Post, Radu Soricut, and
Lucia Specia. 2013. Findings of the 2013 Workshop
on Statistical Machine Translation. In Proceedings
of the 8th Workshop on Statistical Machine Transla-
tion, WMT-2013, pages 1–44, Sofia, Bulgaria.
Ondrej Bojar, Christian Buck, Christian Federmann,
Barry Haddow, Philipp Koehn, Johannes Leveling,
Christof Monz, Pavel Pecina, Matt Post, Herve
Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs
Tamchyna. 2014. Findings of the 2014 workshop
8Unless, for instance, robust and non-arbitrary methods to
identify and remove outliers are applied.
on statistical machine translation. In Proceedings of
the Ninth Workshop on Statistical Machine Transla-
tion, pages 12–58, Baltimore, Maryland, USA.
Jos´e G. C. de Souza, Matteo Negri, Marco Turchi, and
Elisa Ricci. 2015. Online Multitask Learning For
Machine Translation Quality Estimation. In Pro-
ceedings of the 53rd Annual Meeting of the Associa-
tion for Computational Linguistics), Beijing, China.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
Matt Post, Radu Soricut, and Lucia Specia. 2012.
Findings of the 2012 Workshop on Statistical Ma-
chine Translation. In Proceedings of the 7th Work-
shop on Statistical Machine Translation (WMT’12),
pages 10–51, Montr´eal, Canada.
Marcello Federico, Nicola Bertoldi, Mauro Cettolo,
Matteo Negri, Marco Turchi, Marco Trombetti,
Alessandro Cattelan, Antonio Farina, Domenico
Lupinetti, Andrea Martines, Alberto Massidda, Hol-
ger Schwenk, Loic Barrault, Frederic Blain, Philipp
Koehn, Christian Buck, and Ulrich Germann. 2014.
The MateCat tool. In Proceedings of COLING 2014,
the 25th International Conference on Computational
Linguistics: System Demonstrations, pages 129–
132, Dublin, Ireland.
Fei Huang, Jian-Ming Xu, Abraham Ittycheriah, and
Salim Roukos. 2014. Adaptive HTER Estimation
for Document-Specific MT Post-Editing. In Pro-
ceedings of the 52nd Annual Meeting of the Associa-
tion for Computational Linguistics (Volume 1: Long
Papers), pages 861–870, Baltimore, Maryland.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45th Annual Meeting of the
ACL on Interactive Poster and Demonstration Ses-
sions, pages 177–180, Stroudsburg, PA, USA.
Yashar Mehdad, Matteo Negri, and Marcello Fed-
erico. 2012. Match without a Referee: Evaluating
MT Adequacy without Reference Translations. In
Proceedings of the Machine Translation Workshop
(WMT2012), pages 171–180, Montr´eal, Canada.
Eric W. Noreen. 1989. Computer-intensive methods
for testing hypotheses: an introduction. Wiley Inter-
science.
Fabian Pedregosa, Gal Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and douard Duchesnay. 2011.
Scikit-learn: Machine Learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
</reference>
<page confidence="0.983745">
534
</page>
<reference confidence="0.999902055555556">
Stefan Riezler and John T Maxwell. 2005. On
some Pitfalls in Automatic Evaluation and Signifi-
cance Testing for MT. In Proceedings of the ACL
workshop on intrinsic and extrinsic evaluation mea-
sures for machine translation and/or summarization,
pages 57–64.
Peter J Rousseeuw and Annick M Leroy. 1987. Robust
regression and outlier detection, volume 589. John
Wiley &amp; Sons.
Kashif Shah, Marco Turchi, and Lucia Specia. 2014.
An efficient and user-friendly tool for machine trans-
lation quality estimation. In Proceedings of the
Ninth International Conference on Language Re-
sources and Evaluation (LREC’14), Reykjavik, Ice-
land.
Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-
nea Micciulla, and John Makhoul. 2006. A Study
of Translation Edit Rate with Targeted Human An-
notation. In Proceedings ofAssociation for Machine
Translation in the Americas, pages 223–231, Cam-
bridge, Massachusetts, USA.
Lucia Specia, Nicola Cancedda, Marc Dymetman,
Marco Turchi, and Nello Cristianini. 2009. Es-
timating the Sentence-Level Quality of Machine
Translation Systems. In Proceedings of the 13th
Annual Conference of the European Association
for Machine Translation (EAMT’09), pages 28–35,
Barcelona, Spain.
Lucia Specia, Kashif Shah, Jos´e G.C. de Souza, and
Trevor Cohn. 2013. QuEst - A Translation Qual-
ity Estimation Framework. In Proceedings of the
51st Annual Meeting of the Association for Compu-
tational Linguistics: System Demonstrations, ACL-
2013, pages 79–84, Sofia, Bulgaria.
Lucia Specia. 2011. Exploiting Objective Annotations
for Minimising Translation Post-editing Effort. In
Proceedings of the 15th Conference of the European
Association for Machine Translation (EAMT 2011),
pages 73–80, Leuven, Belgium.
Marco Turchi, Matteo Negri, and Marcello Federico.
2013. Coping with the Subjectivity of Human
Judgements in MT Quality Estimation. In Proceed-
ings of the 8th Workshop on Statistical Machine
Translation, pages 240–251, Sofia, Bulgaria.
Marco Turchi, Antonios Anastasopoulos, Jos´e G. C. de
Souza, and Matteo Negri. 2014a. Adaptive Qual-
ity Estimation for Machine Translation. In Proceed-
ings of the 52nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Pa-
pers), pages 710–720, Baltimore, Maryland, USA.
Marco Turchi, Matteo Negri, and Marcello Federico.
2014b. Data-driven Annotation of Binary MT
Quality Estimation Corpora Based on Human Post-
editions. Machine translation, 28(3-4):281–308.
</reference>
<page confidence="0.998536">
535
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.425829">
<title confidence="0.998799">MT Quality Estimation for Computer-assisted Does it Really Help?</title>
<author confidence="0.997268">Marco Turchi</author>
<author confidence="0.997268">Matteo Negri</author>
<author confidence="0.997268">Marcello</author>
<affiliation confidence="0.593205">FBK - Fondazione Bruno Via Sommarive 18, 38123 Trento,</affiliation>
<abstract confidence="0.999607785714286">The usefulness of translation quality estimation (QE) to increase productivity in a computer-assisted translation (CAT) framework is a widely held assumption (Specia, 2011; Huang et al., 2014). So far, however, the validity of this assumption has not been yet demonstrated through sound evaluations in realistic settings. To this aim, we report on an evaluation involving professional translators operating with a CAT tool in controlled but natural conditions. Contrastive experiments are carried out by measuring post-editing time when: suggestions are presented together with binary estimates, and same suggestions are presented without quality indicators. Translators’ productivity in the two conditions is analysed in a principled accounting for the main factors differences in translators’ behaviour, quality of the suggestions) that directly impact on time measurements. While the general assumption about the usefulness of QE is verified, significance testing results reveal that real productivity gains can be observed only under specific conditions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Chris Callison-Burch</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2013</date>
<booktitle>Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the 8th Workshop on Statistical Machine Translation, WMT-2013,</booktitle>
<pages>1--44</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="2671" citStr="Bojar et al., 2013" startWordPosition="402" endWordPosition="405">useless (poor, hence requiring complete re-writing), reliable quality indicators could help to reduce the time spent by the user to decide which action to take (to correct or re-translate). So far, despite the potential practical benefits, the progress in QE research has not been followed by conclusive results that demonstrate whether the use of quality labels can actually lead to noticeable productivity gains in the CAT framework. To the best of our knowledge, most prior works limit the analysis to the intrinsic evaluation of QE performance on gold-standard data (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). On-field evaluation is indeed a complex task, as it requires: i) the availability of a CAT tool capable to integrate MT QE functionalities, ii) professional translators used to MT post-editing, iii) a sound evaluation protocol to perform betweensubject comparisons,1 and iv) robust analysis techniques to measure statistical significance under variable conditions (e.g. differences in users’ postediting behavior). To bypass these issues, the works more closely related to our investigation resort to controlled and simplified evaluation protocols. For instance, in (Specia, 20</context>
</contexts>
<marker>Bojar, Buck, Callison-Burch, Federmann, Haddow, Koehn, Monz, Post, Soricut, Specia, 2013</marker>
<rawString>Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2013. Findings of the 2013 Workshop on Statistical Machine Translation. In Proceedings of the 8th Workshop on Statistical Machine Translation, WMT-2013, pages 1–44, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Ondrej Bojar</author>
<author>Christian Buck</author>
<author>Christian Federmann</author>
<author>Barry Haddow</author>
<author>Philipp Koehn</author>
<author>Johannes Leveling</author>
<author>Christof Monz</author>
<author>Pavel Pecina</author>
<author>Matt Post</author>
<author>Herve Saint-Amand</author>
</authors>
<title>Radu Soricut, Lucia Specia, and Aleˇs Tamchyna.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth Workshop on Statistical Machine Translation,</booktitle>
<pages>12--58</pages>
<location>Baltimore, Maryland, USA.</location>
<contexts>
<context position="2692" citStr="Bojar et al., 2014" startWordPosition="406" endWordPosition="409"> requiring complete re-writing), reliable quality indicators could help to reduce the time spent by the user to decide which action to take (to correct or re-translate). So far, despite the potential practical benefits, the progress in QE research has not been followed by conclusive results that demonstrate whether the use of quality labels can actually lead to noticeable productivity gains in the CAT framework. To the best of our knowledge, most prior works limit the analysis to the intrinsic evaluation of QE performance on gold-standard data (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). On-field evaluation is indeed a complex task, as it requires: i) the availability of a CAT tool capable to integrate MT QE functionalities, ii) professional translators used to MT post-editing, iii) a sound evaluation protocol to perform betweensubject comparisons,1 and iv) robust analysis techniques to measure statistical significance under variable conditions (e.g. differences in users’ postediting behavior). To bypass these issues, the works more closely related to our investigation resort to controlled and simplified evaluation protocols. For instance, in (Specia, 2011) the impact of QE </context>
</contexts>
<marker>Bojar, Buck, Federmann, Haddow, Koehn, Leveling, Monz, Pecina, Post, Saint-Amand, 2014</marker>
<rawString>Ondrej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, Radu Soricut, Lucia Specia, and Aleˇs Tamchyna. 2014. Findings of the 2014 workshop 8Unless, for instance, robust and non-arbitrary methods to identify and remove outliers are applied. on statistical machine translation. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 12–58, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e G C de Souza</author>
<author>Matteo Negri</author>
<author>Marco Turchi</author>
<author>Elisa Ricci</author>
</authors>
<title>Online Multitask Learning For Machine Translation Quality Estimation.</title>
<date>2015</date>
<booktitle>In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics),</booktitle>
<location>Beijing, China.</location>
<marker>de Souza, Negri, Turchi, Ricci, 2015</marker>
<rawString>Jos´e G. C. de Souza, Matteo Negri, Marco Turchi, and Elisa Ricci. 2015. Online Multitask Learning For Machine Translation Quality Estimation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics), Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Matt Post</author>
<author>Radu Soricut</author>
<author>Lucia Specia</author>
</authors>
<date>2012</date>
<booktitle>Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical Machine Translation (WMT’12),</booktitle>
<pages>10--51</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="2651" citStr="Callison-Burch et al., 2012" startWordPosition="398" endWordPosition="401">ood, hence post-editable) or useless (poor, hence requiring complete re-writing), reliable quality indicators could help to reduce the time spent by the user to decide which action to take (to correct or re-translate). So far, despite the potential practical benefits, the progress in QE research has not been followed by conclusive results that demonstrate whether the use of quality labels can actually lead to noticeable productivity gains in the CAT framework. To the best of our knowledge, most prior works limit the analysis to the intrinsic evaluation of QE performance on gold-standard data (Callison-Burch et al., 2012; Bojar et al., 2013; Bojar et al., 2014). On-field evaluation is indeed a complex task, as it requires: i) the availability of a CAT tool capable to integrate MT QE functionalities, ii) professional translators used to MT post-editing, iii) a sound evaluation protocol to perform betweensubject comparisons,1 and iv) robust analysis techniques to measure statistical significance under variable conditions (e.g. differences in users’ postediting behavior). To bypass these issues, the works more closely related to our investigation resort to controlled and simplified evaluation protocols. For inst</context>
</contexts>
<marker>Callison-Burch, Koehn, Monz, Post, Soricut, Specia, 2012</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia. 2012. Findings of the 2012 Workshop on Statistical Machine Translation. In Proceedings of the 7th Workshop on Statistical Machine Translation (WMT’12), pages 10–51, Montr´eal, Canada.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Mauro Cettolo</author>
<author>Matteo Negri</author>
<author>Marco Turchi</author>
<author>Marco Trombetti</author>
<author>Alessandro Cattelan</author>
<author>Antonio Farina</author>
<author>Domenico Lupinetti</author>
<author>Andrea Martines</author>
</authors>
<title>Alberto Massidda, Holger Schwenk, Loic Barrault,</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations,</booktitle>
<pages>129--132</pages>
<location>Frederic Blain, Philipp</location>
<contexts>
<context position="6021" citStr="Federico et al., 2014" startWordPosition="926" endWordPosition="929">st-editing the translation is expected to take longer than translating from scratch. This decision rests on the assumption that the two-color scheme is more immediate than realvalued scores, which require some interpretation by the user. Analysing the difference between alternative visualisation schemes, however, is certainly an aspect that we want to explore in the future. 2.1 The CAT Framework To keep the experimental conditions as natural as possible, we analyse the impact of QE labels on translators’ productivity in a real CAT environment. To this aim, we use the open-source MateCat tool (Federico et al., 2014), which has been slightly changed in two ways. First, the tool has been adapted to provide only one single translation suggestion (MT output) per segment, instead of the usual three (one MT suggestion plus two Translation Memory matches). Second, each suggestion is presented with a colored flag (green for good, red for bad), which indicates its expected quality and usefulness to the post-editor. In the contrastive condition (no binary QE visualization), grey is used as the neutral and uniform flag color. 2.2 Getting binary quality labels. The experiment is set up for a between-subject comparis</context>
</contexts>
<marker>Federico, Bertoldi, Cettolo, Negri, Turchi, Trombetti, Cattelan, Farina, Lupinetti, Martines, 2014</marker>
<rawString>Marcello Federico, Nicola Bertoldi, Mauro Cettolo, Matteo Negri, Marco Turchi, Marco Trombetti, Alessandro Cattelan, Antonio Farina, Domenico Lupinetti, Andrea Martines, Alberto Massidda, Holger Schwenk, Loic Barrault, Frederic Blain, Philipp Koehn, Christian Buck, and Ulrich Germann. 2014. The MateCat tool. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations, pages 129– 132, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Huang</author>
<author>Jian-Ming Xu</author>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>Adaptive HTER Estimation for Document-Specific MT Post-Editing.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>861--870</pages>
<location>Baltimore, Maryland.</location>
<contexts>
<context position="4319" citStr="Huang et al. (2014)" startWordPosition="652" endWordPosition="655">tion 1Notice that the same sentence cannot be post-edited twice (e.g. with/without quality labels) by the same translator without introducing a bias in the time measurements. 530 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 530–535, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics between the usefulness of QE and the intrinsic features of the top-ranked translations (e.g. sentence length, quality of the MT). More recently, Huang et al. (2014) claimed a 10% productivity increase when translation is supported by the estimates of an adaptive QE model. Their analysis, however, compares a condition in which MT suggestions are presented with confidence labels (the two factors are not decoupled) against the contrastive condition in which no MT suggestion is presented at all. Significance testing, moreover, is not performed. The remainder of this work describes our on-field evaluation addressing (through objective measurements and robust significance tests) the two key questions: • Does QE really help in the CAT scenario? • If yes, under </context>
</contexts>
<marker>Huang, Xu, Ittycheriah, Roukos, 2014</marker>
<rawString>Fei Huang, Jian-Ming Xu, Abraham Ittycheriah, and Salim Roukos. 2014. Adaptive HTER Estimation for Document-Specific MT Post-Editing. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 861–870, Baltimore, Maryland.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondˇrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions,</booktitle>
<pages>177--180</pages>
<location>Alexandra</location>
<contexts>
<context position="6940" citStr="Koehn et al., 2007" startWordPosition="1076" endWordPosition="1079">en for good, red for bad), which indicates its expected quality and usefulness to the post-editor. In the contrastive condition (no binary QE visualization), grey is used as the neutral and uniform flag color. 2.2 Getting binary quality labels. The experiment is set up for a between-subject comparison on a single long document as follows. First, the document is split in two parts. The first part serves as the training portion for a binary quality estimator; the second part is reserved for evaluation. The training portion is machine-translated with a state-of-the-art, phrasebased Moses system (Koehn et al., 2007)2 and post-edited under standard conditions (i.e. without visualising QE information) by the same users involved in the testing phase. Based on their postedits, the raw MT output samples are then labeled as ‘good’ or ‘bad’ by considering the HTER (Snover et al., 2006) calculated between raw MT output and its post-edited version.3 Our labeling criterion follows the empirical findings of (Turchi et al., 2013; Turchi et al., 2014b), which indicate an HTER value of 0.4 as boundary between posteditable (HTER ≤ 0.4) and useless suggestions (HTER&gt; 0.4). Then, to model the subjective concept of qualit</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the 45th Annual Meeting of the ACL on Interactive Poster and Demonstration Sessions, pages 177–180, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yashar Mehdad</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Match without a Referee: Evaluating MT Adequacy without Reference Translations.</title>
<date>2012</date>
<booktitle>In Proceedings of the Machine Translation Workshop (WMT2012),</booktitle>
<pages>171--180</pages>
<location>Montr´eal, Canada.</location>
<contexts>
<context position="1638" citStr="Mehdad et al., 2012" startWordPosition="230" endWordPosition="233">counting for the main factors (e.g. differences in translators’ behaviour, quality of the suggestions) that directly impact on time measurements. While the general assumption about the usefulness of QE is verified, significance testing results reveal that real productivity gains can be observed only under specific conditions. 1 Introduction Machine translation (MT) quality estimation aims to automatically predict the expected time (e.g. in seconds) or effort (e.g. number of editing operations) required to correct machine-translated sentences into publishable translations (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014a; C. de Souza et al., 2015). In principle, the task has a number of practical applications. An intuitive one is speeding-up the work of human translators operating with a CAT tool, a software designed to support and facilitate the translation process by proposing suggestions that can be edited by the user. The idea is that, since the suggestions can be useful (good, hence post-editable) or useless (poor, hence requiring complete re-writing), reliable quality indicators could help to reduce the time spent by the user to decide which action to take (to correct or re-transla</context>
</contexts>
<marker>Mehdad, Negri, Federico, 2012</marker>
<rawString>Yashar Mehdad, Matteo Negri, and Marcello Federico. 2012. Match without a Referee: Evaluating MT Adequacy without Reference Translations. In Proceedings of the Machine Translation Workshop (WMT2012), pages 171–180, Montr´eal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric W Noreen</author>
</authors>
<title>Computer-intensive methods for testing hypotheses: an introduction. Wiley Interscience.</title>
<date>1989</date>
<contexts>
<context position="12266" citStr="Noreen, 1989" startWordPosition="1940" endWordPosition="1941">e-translating a given suggestion. 3.2 Does QE Really Help? To analyse the impact of our quality estimates on translators’ productivity, we first compared the average post-editing time (PET – seconds per word) under the two conditions (colored vs. grey flags). The results of this rough, global analysis are reported in Table 1, first row. As can be seen, the average PET values indicate a productivity increase of about 1.5 seconds per word when colored flags are provided. Significance tests, however, indicate that such increase is not significant (p &gt; 0.05, measured by approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005)). An analysis of the collected data to better understand these results and the rather high average PET values observed (8 to 9.5 secs. per word) evidenced both a large number of outliers, and a high PET variability across post-editors.4 To check whether these factors make existing PET differences opaque to our study, we performed further analysis by normalizing the PET of each translator with the robust z-score technique (Rousseeuw and Leroy, 1987).5 The twofold advantage of 4We consider as outliers the segments with a PET lower than 0.5 or higher than 30. Segments</context>
</contexts>
<marker>Noreen, 1989</marker>
<rawString>Eric W. Noreen. 1989. Computer-intensive methods for testing hypotheses: an introduction. Wiley Interscience.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Gal Varoquaux</author>
<author>Alexandre Gramfort</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine Learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<contexts>
<context position="7760" citStr="Pedregosa et al., 2011" startWordPosition="1211" endWordPosition="1214">abeled as ‘good’ or ‘bad’ by considering the HTER (Snover et al., 2006) calculated between raw MT output and its post-edited version.3 Our labeling criterion follows the empirical findings of (Turchi et al., 2013; Turchi et al., 2014b), which indicate an HTER value of 0.4 as boundary between posteditable (HTER ≤ 0.4) and useless suggestions (HTER&gt; 0.4). Then, to model the subjective concept of quality of different subjects, for of each translator we train a separate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline features proposed by Specia et al. (2009). This feature set mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities). The features are extracted from the data available at prediction time (source text and raw MT output) by using an adapted version (Shah et al., 2014) of the open-source QuEst software (Specia et al., 2013). The SVM parameters are optimized by cross-validation on the training set. With thes</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Gal Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and douard Duchesnay. 2011. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Riezler</author>
<author>John T Maxwell</author>
</authors>
<title>On some Pitfalls in Automatic Evaluation and Significance Testing for MT.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL workshop</booktitle>
<pages>57--64</pages>
<contexts>
<context position="12294" citStr="Riezler and Maxwell, 2005" startWordPosition="1942" endWordPosition="1945">a given suggestion. 3.2 Does QE Really Help? To analyse the impact of our quality estimates on translators’ productivity, we first compared the average post-editing time (PET – seconds per word) under the two conditions (colored vs. grey flags). The results of this rough, global analysis are reported in Table 1, first row. As can be seen, the average PET values indicate a productivity increase of about 1.5 seconds per word when colored flags are provided. Significance tests, however, indicate that such increase is not significant (p &gt; 0.05, measured by approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005)). An analysis of the collected data to better understand these results and the rather high average PET values observed (8 to 9.5 secs. per word) evidenced both a large number of outliers, and a high PET variability across post-editors.4 To check whether these factors make existing PET differences opaque to our study, we performed further analysis by normalizing the PET of each translator with the robust z-score technique (Rousseeuw and Leroy, 1987).5 The twofold advantage of 4We consider as outliers the segments with a PET lower than 0.5 or higher than 30. Segments with unrealistically short </context>
</contexts>
<marker>Riezler, Maxwell, 2005</marker>
<rawString>Stefan Riezler and John T Maxwell. 2005. On some Pitfalls in Automatic Evaluation and Significance Testing for MT. In Proceedings of the ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 57–64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter J Rousseeuw</author>
<author>Annick M Leroy</author>
</authors>
<title>Robust regression and outlier detection, volume 589.</title>
<date>1987</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="12747" citStr="Rousseeuw and Leroy, 1987" startWordPosition="2018" endWordPosition="2021">vided. Significance tests, however, indicate that such increase is not significant (p &gt; 0.05, measured by approximate randomization (Noreen, 1989; Riezler and Maxwell, 2005)). An analysis of the collected data to better understand these results and the rather high average PET values observed (8 to 9.5 secs. per word) evidenced both a large number of outliers, and a high PET variability across post-editors.4 To check whether these factors make existing PET differences opaque to our study, we performed further analysis by normalizing the PET of each translator with the robust z-score technique (Rousseeuw and Leroy, 1987).5 The twofold advantage of 4We consider as outliers the segments with a PET lower than 0.5 or higher than 30. Segments with unrealistically short post-editing times may not even have been read completely, while very long post-editing times suggest that the post-editor interrupted his/her work or got distracted. The average PET for the four post-editors ranges from 2.266 to 13.783. In total, 48 segments have a PET higher than 30, and 6 segments were post-edited in more than 360 seconds. 5For each post-editor, it is computed by removing from 532 % Wins of Colored 49 48 47 46 56 55 54 53 52 51 5</context>
</contexts>
<marker>Rousseeuw, Leroy, 1987</marker>
<rawString>Peter J Rousseeuw and Annick M Leroy. 1987. Robust regression and outlier detection, volume 589. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kashif Shah</author>
<author>Marco Turchi</author>
<author>Lucia Specia</author>
</authors>
<title>An efficient and user-friendly tool for machine translation quality estimation.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<location>Reykjavik, Iceland.</location>
<contexts>
<context position="8219" citStr="Shah et al., 2014" startWordPosition="1288" endWordPosition="1291">parate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline features proposed by Specia et al. (2009). This feature set mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities). The features are extracted from the data available at prediction time (source text and raw MT output) by using an adapted version (Shah et al., 2014) of the open-source QuEst software (Specia et al., 2013). The SVM parameters are optimized by cross-validation on the training set. With these classifiers, we finally assign quality flags to the raw segment translations in the test 2The system was trained with 60M running words from the same domain (Information Technology) of the input document. 3HTER measures the minimum edit distance (# word Insertions + Deletions + Substitutions + Shifts / # Reference Words) between the MT output and its manual post-edition. 531 Average PET colored 8.086 p = 0.33 (sec/word) grey 9.592 % Wins 51.7 p = 0.039 </context>
</contexts>
<marker>Shah, Turchi, Specia, 2014</marker>
<rawString>Kashif Shah, Marco Turchi, and Lucia Specia. 2014. An efficient and user-friendly tool for machine translation quality estimation. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew Snover</author>
<author>Bonnie Dorr</author>
<author>Richard Schwartz</author>
<author>Linnea Micciulla</author>
<author>John Makhoul</author>
</authors>
<title>A Study of Translation Edit Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proceedings ofAssociation for Machine Translation in the Americas,</booktitle>
<pages>223--231</pages>
<location>Cambridge, Massachusetts, USA.</location>
<contexts>
<context position="7208" citStr="Snover et al., 2006" startWordPosition="1122" endWordPosition="1125">up for a between-subject comparison on a single long document as follows. First, the document is split in two parts. The first part serves as the training portion for a binary quality estimator; the second part is reserved for evaluation. The training portion is machine-translated with a state-of-the-art, phrasebased Moses system (Koehn et al., 2007)2 and post-edited under standard conditions (i.e. without visualising QE information) by the same users involved in the testing phase. Based on their postedits, the raw MT output samples are then labeled as ‘good’ or ‘bad’ by considering the HTER (Snover et al., 2006) calculated between raw MT output and its post-edited version.3 Our labeling criterion follows the empirical findings of (Turchi et al., 2013; Turchi et al., 2014b), which indicate an HTER value of 0.4 as boundary between posteditable (HTER ≤ 0.4) and useless suggestions (HTER&gt; 0.4). Then, to model the subjective concept of quality of different subjects, for of each translator we train a separate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline featu</context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>Matthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A Study of Translation Edit Rate with Targeted Human Annotation. In Proceedings ofAssociation for Machine Translation in the Americas, pages 223–231, Cambridge, Massachusetts, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
<author>Marco Turchi</author>
<author>Nello Cristianini</author>
</authors>
<title>Estimating the Sentence-Level Quality of Machine Translation Systems.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT’09),</booktitle>
<pages>28--35</pages>
<location>Barcelona,</location>
<contexts>
<context position="1617" citStr="Specia et al., 2009" startWordPosition="226" endWordPosition="229"> a principled way, accounting for the main factors (e.g. differences in translators’ behaviour, quality of the suggestions) that directly impact on time measurements. While the general assumption about the usefulness of QE is verified, significance testing results reveal that real productivity gains can be observed only under specific conditions. 1 Introduction Machine translation (MT) quality estimation aims to automatically predict the expected time (e.g. in seconds) or effort (e.g. number of editing operations) required to correct machine-translated sentences into publishable translations (Specia et al., 2009; Mehdad et al., 2012; Turchi et al., 2014a; C. de Souza et al., 2015). In principle, the task has a number of practical applications. An intuitive one is speeding-up the work of human translators operating with a CAT tool, a software designed to support and facilitate the translation process by proposing suggestions that can be edited by the user. The idea is that, since the suggestions can be useful (good, hence post-editable) or useless (poor, hence requiring complete re-writing), reliable quality indicators could help to reduce the time spent by the user to decide which action to take (to </context>
<context position="7844" citStr="Specia et al. (2009)" startWordPosition="1226" endWordPosition="1230">een raw MT output and its post-edited version.3 Our labeling criterion follows the empirical findings of (Turchi et al., 2013; Turchi et al., 2014b), which indicate an HTER value of 0.4 as boundary between posteditable (HTER ≤ 0.4) and useless suggestions (HTER&gt; 0.4). Then, to model the subjective concept of quality of different subjects, for of each translator we train a separate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline features proposed by Specia et al. (2009). This feature set mainly takes into account the complexity of the source sentence (e.g. number of tokens, number of translations per source word) and the fluency of the target translation (e.g. language model probabilities). The features are extracted from the data available at prediction time (source text and raw MT output) by using an adapted version (Shah et al., 2014) of the open-source QuEst software (Specia et al., 2013). The SVM parameters are optimized by cross-validation on the training set. With these classifiers, we finally assign quality flags to the raw segment translations in th</context>
</contexts>
<marker>Specia, Cancedda, Dymetman, Turchi, Cristianini, 2009</marker>
<rawString>Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini. 2009. Estimating the Sentence-Level Quality of Machine Translation Systems. In Proceedings of the 13th Annual Conference of the European Association for Machine Translation (EAMT’09), pages 28–35, Barcelona, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Kashif Shah</author>
<author>Jos´e G C de Souza</author>
<author>Trevor Cohn</author>
</authors>
<title>QuEst - A Translation Quality Estimation Framework.</title>
<date>2013</date>
<booktitle>In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL2013,</booktitle>
<pages>79--84</pages>
<location>Sofia, Bulgaria.</location>
<marker>Specia, Shah, de Souza, Cohn, 2013</marker>
<rawString>Lucia Specia, Kashif Shah, Jos´e G.C. de Souza, and Trevor Cohn. 2013. QuEst - A Translation Quality Estimation Framework. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL2013, pages 79–84, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Exploiting Objective Annotations for Minimising Translation Post-editing Effort.</title>
<date>2011</date>
<booktitle>In Proceedings of the 15th Conference of the European Association for Machine Translation (EAMT 2011),</booktitle>
<pages>73--80</pages>
<location>Leuven, Belgium.</location>
<contexts>
<context position="3274" citStr="Specia, 2011" startWordPosition="494" endWordPosition="495"> al., 2013; Bojar et al., 2014). On-field evaluation is indeed a complex task, as it requires: i) the availability of a CAT tool capable to integrate MT QE functionalities, ii) professional translators used to MT post-editing, iii) a sound evaluation protocol to perform betweensubject comparisons,1 and iv) robust analysis techniques to measure statistical significance under variable conditions (e.g. differences in users’ postediting behavior). To bypass these issues, the works more closely related to our investigation resort to controlled and simplified evaluation protocols. For instance, in (Specia, 2011) the impact of QE predictions on translators’ productivity is analysed by measuring the number of words that can be post-edited in a fixed amount of time. The evaluation, however, only concentrates on the use of QE to rank MT outputs, and the gains in translation speed are measured against the contrastive condition in which no QE-based ranking mechanism is used. In this artificial scenario, the analysis disregards the relation 1Notice that the same sentence cannot be post-edited twice (e.g. with/without quality labels) by the same translator without introducing a bias in the time measurements.</context>
</contexts>
<marker>Specia, 2011</marker>
<rawString>Lucia Specia. 2011. Exploiting Objective Annotations for Minimising Translation Post-editing Effort. In Proceedings of the 15th Conference of the European Association for Machine Translation (EAMT 2011), pages 73–80, Leuven, Belgium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<title>Coping with the Subjectivity of Human Judgements in MT Quality Estimation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 8th Workshop on Statistical Machine Translation,</booktitle>
<pages>240--251</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="7349" citStr="Turchi et al., 2013" startWordPosition="1143" endWordPosition="1146">the training portion for a binary quality estimator; the second part is reserved for evaluation. The training portion is machine-translated with a state-of-the-art, phrasebased Moses system (Koehn et al., 2007)2 and post-edited under standard conditions (i.e. without visualising QE information) by the same users involved in the testing phase. Based on their postedits, the raw MT output samples are then labeled as ‘good’ or ‘bad’ by considering the HTER (Snover et al., 2006) calculated between raw MT output and its post-edited version.3 Our labeling criterion follows the empirical findings of (Turchi et al., 2013; Turchi et al., 2014b), which indicate an HTER value of 0.4 as boundary between posteditable (HTER ≤ 0.4) and useless suggestions (HTER&gt; 0.4). Then, to model the subjective concept of quality of different subjects, for of each translator we train a separate binary QE classifier on the labeled samples. For this purpose we use the Scikit-learn implementation of support vector machines (Pedregosa et al., 2011), training our models with the 17 baseline features proposed by Specia et al. (2009). This feature set mainly takes into account the complexity of the source sentence (e.g. number of tokens</context>
</contexts>
<marker>Turchi, Negri, Federico, 2013</marker>
<rawString>Marco Turchi, Matteo Negri, and Marcello Federico. 2013. Coping with the Subjectivity of Human Judgements in MT Quality Estimation. In Proceedings of the 8th Workshop on Statistical Machine Translation, pages 240–251, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Turchi</author>
<author>Antonios Anastasopoulos</author>
<author>Jos´e G C de Souza</author>
<author>Matteo Negri</author>
</authors>
<title>Adaptive Quality Estimation for Machine Translation.</title>
<date>2014</date>
<booktitle>In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),</booktitle>
<pages>710--720</pages>
<location>Baltimore, Maryland, USA.</location>
<marker>Turchi, Anastasopoulos, de Souza, Negri, 2014</marker>
<rawString>Marco Turchi, Antonios Anastasopoulos, Jos´e G. C. de Souza, and Matteo Negri. 2014a. Adaptive Quality Estimation for Machine Translation. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 710–720, Baltimore, Maryland, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Marco Turchi</author>
<author>Matteo Negri</author>
<author>Marcello Federico</author>
</authors>
<booktitle>2014b. Data-driven Annotation of Binary MT Quality Estimation Corpora Based on Human Posteditions. Machine translation,</booktitle>
<pages>28--3</pages>
<marker>Turchi, Negri, Federico, </marker>
<rawString>Marco Turchi, Matteo Negri, and Marcello Federico. 2014b. Data-driven Annotation of Binary MT Quality Estimation Corpora Based on Human Posteditions. Machine translation, 28(3-4):281–308.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>