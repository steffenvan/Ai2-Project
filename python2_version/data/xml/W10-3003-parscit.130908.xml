<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002240">
<title confidence="0.9986815">
Detecting Speculative Language Using Syntactic Dependencies
and Logistic Regression
</title>
<author confidence="0.994388">
Andreas Vlachos and Mark Craven
</author>
<affiliation confidence="0.997603">
Department of Biostatistics and Medical Informatics
University of Wisconsin-Madison
</affiliation>
<email confidence="0.999377">
{vlachos,craven}@biostat.wisc.edu
</email>
<sectionHeader confidence="0.994857" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999246173913044">
In this paper we describe our approach
to the CoNLL-2010 shared task on de-
tecting speculative language in biomedical
text. We treat the detection of sentences
containing uncertain information (Task1)
as a token classification task since the
existence or absence of cues determines
the sentence label. We distinguish words
that have speculative and non-speculative
meaning by employing syntactic features
as a proxy for their semantic content. In
order to identify the scope of each cue
(Task2), we learn a classifier that predicts
whether each token of a sentence belongs
to the scope of a given cue. The features
in the classifier are based on the syntactic
dependency path between the cue and the
token. In both tasks, we use a Bayesian
logistic regression classifier incorporating
a sparsity-enforcing Laplace prior. Over-
all, the performance achieved is 85.21%
F-score and 44.11% F-score in Task1 and
Task2, respectively.
</bodyText>
<sectionHeader confidence="0.998886" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999960822222223">
The term speculative language, also known as
hedging, refers to expressions of uncertainty over
statements. Recognition of such statements is im-
portant for higher-level applications. For exam-
ple, a multi-document summarization system can
assign different weights to speculative and non-
speculative statements when aggregating informa-
tion on a particular issue.
The CoNLL-2010 shared task (Farkas et al.,
2010) formulates speculative language detection
as two subtasks. In the first subtask (Task1), sys-
tems need to determine whether a sentence con-
tains uncertain information or not. In the sec-
ond subtask (Task2), systems need to identify the
hedge cues and their scope in the sentence. Table 1
provides an example from the training data.
The participants are provided with data from
two domains: biomedical scientific literature (both
abstracts and full articles) and Wikipedia. We
choose to focus on the former. The training data
for this domain are nine full articles and 1,273 ab-
stracts from the BioScope corpus (Szarvas et al.,
2008) and the test data are 15 full articles.
Our approach to speculative language detection
relies on syntactic parsing and machine learning.
We give a description of the techniques used in
Sections 2 and 3. We treat the detection of sen-
tences containing uncertain information (Task1) as
a token classification task in which we learn a clas-
sifier to predict whether a token is a cue or not. In
order to handle words that have speculative and
non-speculative meaning (e.g. “indicating” in the
example of Table 1), we employ syntactic features
as a proxy for their semantic content (Section 4).
For scope identification (Task2), we learn a clas-
sifier that predicts whether each token of the sen-
tence belongs to the scope of a particular cue (Sec-
tion 6). The features used are based on the syntac-
tic dependency path between the cue and the to-
ken. We report results and perform error analysis
for both tasks, pointing out annotation issues that
could be ameliorated (Sections 5 and 7). Based on
our experience we suggest improvements on the
task definition taking into account work from the
broader field (Section 8).
</bodyText>
<sectionHeader confidence="0.927157" genericHeader="introduction">
2 Syntactic parsing for the biomedical
domain
</sectionHeader>
<bodyText confidence="0.999814857142857">
The syntactic parser we chose for our experi-
ments is the C&amp;C Combinatory Categorial Gram-
mar (CCG) parser adapted to the biomedical do-
main (Rimell and Clark, 2009). In this frame-
work, parsing is performed in three stages: part-
of-speech (PoS) tagging, CCG supertagging and
parse selection. The parse selection module de-
</bodyText>
<page confidence="0.989081">
18
</page>
<note confidence="0.9628615">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 18–25,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.787496333333333">
The Orthology and Combined modules both have states that achieve likelihood ratios above 400 (as
high as 1207 for the Orthology module and 613 for the Combined module), {indicating that both these
modules {can, on their own, predict some interacting protein pairs with a posterior odds ratio above 1}}.
</bodyText>
<tableCaption confidence="0.995737">
Table 1: Sentence annotated as speculative with two cues (in boldface) and their scopes (in brackets).
</tableCaption>
<bodyText confidence="0.999744790697675">
rives the actual parse tree using the information
from the other two components. The intermediate
CCG supertagging stage assigns each token to a
lexical category which attempts to capture its syn-
tactic role in the sentence. Lexical categories con-
tain more information than PoS tags (mainly on
subcategorization) and they are more numerous,
thereby making their assignment a relatively dif-
ficult task. Therefore, the parse selection module
takes into account multiple predictions per token
which allows recovery from supertagging errors
while still reducing the ambiguity propagated. An
interesting aspect of this three-stage parsing ap-
proach is that, if the parse selection module fails to
construct a parse tree for the sentence (a common
issue when syntactic parsers are ported to new do-
mains), the lexical categories obtained by the su-
pertagger preserve some of the syntactic informa-
tion that would not be found in PoS tags.
The adaptation to the biomedical domain by
Rimell and Clark (2009) involved re-training the
PoS tagger and the CCG supertagger using in-
domain resources, while the parse selection com-
ponent was left intact. As recent work in the
BioNLP 2009 shared task has shown (Kim et al.,
2009), domain-adapted parsing benefits informa-
tion extraction systems.
The native output of the C&amp;C parser is con-
verted into the Stanford Dependency (SD) col-
lapsed dependency format (de Marneffe and Man-
ning, 2008). These dependencies define binary re-
lations between tokens and the labels of these re-
lations are obtained from a hierarchy. While the
conversion is unlikely to be perfect given that the
native C&amp;C output follows a different formalism,
we made this choice because it allows for the use
of different parsers with minimal adaptation.
Finally, an important pre-processing step we
take is tokenization of the original text. Since the
PoS tagger is trained on the GENIA corpus which
follows the Penn TreeBank tokenization scheme,
we use the tokenization script provided by the tree-
bank.1
</bodyText>
<footnote confidence="0.945085">
1http://www.cis.upenn.edu/˜treebank/tokenization.html
</footnote>
<sectionHeader confidence="0.842296" genericHeader="method">
3 Bayesian logistic regression
</sectionHeader>
<bodyText confidence="0.99945875">
In both tasks, we use a Bayesian logistic regres-
sion classifier incorporating a sparsity-enforcing
Laplace prior (Genkin et al., 2006). Logistic re-
gression models are of the form:
</bodyText>
<equation confidence="0.995229">
exp(xβT)
p(y = +1|β, x) =
1 + exp(xβT) (1)
</equation>
<bodyText confidence="0.999093692307692">
where y ∈ {+1, −1} is a binary class label, x
is the feature vector representation of the instance
to be classified and β is the feature weight vec-
tor which is learnt from the training data. Since
feature interactions are not directly represented,
the interactions that are expected to matter for the
task considered must be specified as additional
features. In Bayesian logistic regression, a prior
distribution on β is used which encodes our prior
beliefs on the feature weights. In this work, we
use the Laplace prior which encourages the fea-
ture weight vector to be sparse, reflecting our be-
lief that most features will be irrelevant to the task.
</bodyText>
<sectionHeader confidence="0.976466" genericHeader="method">
4 Detecting sentences containing
speculation
</sectionHeader>
<bodyText confidence="0.999796904761905">
In Task1, systems need to determine whether a
sentence contains uncertain information (labeled
uncertain) or not (labeled certain). A sentence is
uncertain if one or more of its tokens denote un-
certainty. Such tokens are labeled as cues and they
are provided by the organizers for training. If a
cue is a present, any other (potentially “unhedg-
ing”) token becomes irrelevant to the task. There-
fore, we cast the task as a binary token classifi-
cation problem and determine the sentence label
from the token-level decisions.
Words used as speculative cues do not always
denote speculation. For example, in BioScope “if”
and “appear” are annotated as cues 4% and 83%
of the times they are encountered. In order to
gain better understanding of the task, we build a
dictionary-based cue extractor. First we extract all
the cues from the training data and use their lem-
mas, obtained using morpha (Minnen et al., 2001),
to tag tokens in the test data. We keep only single-
token cues in order to avoid non-indicative lem-
</bodyText>
<page confidence="0.997944">
19
</page>
<table confidence="0.7800545">
token=indicating lemma=indicate
PoS=VBG lemma+PoS=indicate+VBG
CCG=(S[ng]\NP)/S[em]
lemma+CCG=indicate+(S[ng]\NP)/S[em]
</table>
<tableCaption confidence="0.950529">
Table 2: Features extracted for the token “indicat-
</tableCaption>
<bodyText confidence="0.998269045454546">
ing” from the Example in Table 1. CCG supertag
(S[ng]\NP)/S[em] denotes that “indicating” ex-
pects an embedded clause (S[em]) to its right (in-
dicated by the forward slash /) and a noun phrase
(NP) to its left (indicated by the backward slash \)
to form a present participle (S[ng]).
mas entering the dictionary (e.g. “that” in “in-
dicate that”). Since the test data consist of full
articles only, we evaluate the performance of the
dictionary-based approach using four-fold cross-
validation on the nine full articles of the training
data with the abstracts added as training data in
every fold, but not used as test data. The recall
achieved is 98.07%, but F-score is lower (59.53%)
demonstrating that the single-token cues in the
training data provide adequate coverage, but low
precision. The restricted domain helps precision
as it precludes some word meanings from appear-
ing. For example “might” is unlikely to be encoun-
tered as a noun in the biomedical domain. Never-
theless, in order to achieve better performance it
is important to further refine the cue identification
procedure.
Determining whether a token is used as a specu-
lative cue or not resembles supervised word sense
disambiguation. The main difference is that in-
stead of having an inventory of senses for each
word, we have two senses applicable to all words.
As in most word sense disambiguation tasks, the
classification of a word as cue or not is dependent
on the other words in the sentence, which we take
into account using syntax. The syntactic context
of words is a useful proxy to their semantics, as
shown in recent work on verb sense disambigua-
tion (Chen and Palmer, 2009). Furthermore, it is
easy to obtain syntactic information automatically
using a parser, even though there will be some
noise due to parsing errors. Similar intuitions were
exploited by Kilicoglu and Bergler (2008) in refin-
ing a dictionary of cues with syntactic rules.
In what follows, we present the features ex-
tracted for each token for our final system, along
with an example of their application in Table 2.
Where appropriate we give the relevant labels in
</bodyText>
<listItem confidence="0.9563213">
the Stanford Dependency (SD) scheme in paren-
theses for reproducibility:
• We extract the token itself and its lemma as
features.
• To handle cases where word senses are identi-
fiable by the PoS of a token (“might result” vs
“the might”), we combine the latter with the
lemma and add it as a feature.
• We combine the lemma with the CCG supertag
and add it as a feature in order to capture cases
</listItem>
<bodyText confidence="0.986876888888889">
where the hedging function of a word is de-
termined by its syntactic role in the sentence.
For example, “indicating” in the example of
Table 1 is followed by a clausal complement (a
very reliable predictor of hedging function for
epistemic verbs), which is captured by its CCG
supertag. As explained in Section 2, this in-
formation can be recovered even in sentences
where the parser fails to produce a parse.
</bodyText>
<listItem confidence="0.974439419354838">
• Passive voice is commonly employed to limit
commitment to the statement made, therefore
we add it as a feature combined with the
lemma to verbs in that voice (nsubjpass).
• Modal auxiliaries are prime hedging devices
but they are also among the most ambiguous.
For example, “can” is annotated as a cue in
16% of its occurrences and it is the fifth most
frequent cue in the full articles. To resolve
this ambiguity, we add as features the lemma
of the main verb the auxiliary is dependent on
(aux) as well as the lemmas of any dependents
of the main verb. Thus we can capture some
stereotypical speculative expressions in scien-
tific articles (e.g “could be due”), while avoid-
ing false positives that are distinguished by the
use of first person plural pronoun and/or ref-
erence to objective enabling conditions (Kil-
icoglu and Bergler, 2008).
• Speculation can be expressed via negation of
a word expressing certainty (e.g. “we do not
know”), therefore we add the lemma of the to-
ken prefixed with “not” (neg).
• In order to capture stereotypical hedging ex-
pressions such as “raises the question” and
“on the assumption” we add as features the di-
rect object of verbs combined with the lemma
of their object (dobj) and the preposition for
nouns in a prepositional relation (prep *).
• In order to capture the effect of adverbs on the
hedging function of verbs (e.g. “theoretically
</listItem>
<page confidence="0.977515">
20
</page>
<table confidence="0.99731">
features Recall Precision F-score
tokens, lemmas 75.92 81.07 78.41
+PoS, CCG 78.23 83.71 80.88
+syntax 81.00 81.31 81.15
+combs 79.58 84.98 82.19
</table>
<tableCaption confidence="0.9935445">
Table 3: Performance of various feature sets on
Task1 using cross-validation on full articles.
</tableCaption>
<bodyText confidence="0.939594615384615">
considered”) we add the lemma of the adverb
as a feature to the verb (advmod).
• To distinguish the probabilistic/numerical
sense from the hedging sense of adjectives
such as “possible”, we add the lemma and the
number of the noun they modify as features
(amod), since plural number tends to be as-
sociated with the probabilistic/numerical sense
(e.g. “all possible combinations”).
Finally, given that this stage is meant to identify
cues in order to recover their scopes in Task2, we
attempt to resolve multi-token cues in the train-
ing data into single-token ones. This agrees with
the minimal strategy for marking cues stated in the
corpus guidelines (Szarvas et al., 2008) and it sim-
plifies scope detection. Therefore, during train-
ing multi-token cues are resolved to their syntactic
head according to the dependency output, e.g. in
Table 1 “indicate that” is restricted to “indicate”
only. There were two cases in which this process
failed; the cues being “cannot” (S3.167) and “not
clear” (S3.269). We argue that the former is in-
consistently annotated (the sentence reads “cannot
be defined... ” and it would have been resolved to
“defined”), while the latter is headed syntactically
by the verb “be” which is preceding it.
</bodyText>
<sectionHeader confidence="0.843308" genericHeader="method">
5 Task1 results and error analysis
</sectionHeader>
<bodyText confidence="0.999985461538461">
Initially we experiment using the full-articles part
of the training data only divided in four folds. The
reason for this choice is that the language of the
abstracts is relatively restricted and phenomena
that appear only in full papers could be obscured
by the abstracts, especially since the latter con-
sist of more sentences in total (11,871 vs. 2,670).
Such phenomena include language related to fig-
ures and descriptions of probabilistic models.
Each row in Table 3 is produced by adding
extra features to the feature set represented on
the row directly above it. First we consider us-
ing only the tokens and their lemmas as features
</bodyText>
<table confidence="0.9993534">
features Recall Precision F-score
tokens, lemmas 79.19 80.43 79.81
+PoS, CCG 81.12 85.22 83.12
+syntax 83.43 84.57 84.00
+combs 85.16 85.99 85.58
</table>
<tableCaption confidence="0.877610333333333">
Table 4: Performance of various feature sets on
Task1 using cross-validation on full articles incor-
porating the abstracts as training data.
</tableCaption>
<bodyText confidence="0.999883707317073">
which amounts to a weighted dictionary but which
achieves reasonable performance. The inclusion
of PoS tags and CCG supertags improves perfor-
mance, whereas syntactic context increases recall
while decreasing precision slightly. This is due
to the fact that logistic regression does not rep-
resent feature interactions and the effect of these
features varies across words. For example, clausal
complements affect epistemic verbs but not other
words (“indicate” vs. “state” in the example of
Table 1) and negation affects only words express-
ing certainty. In order to ameliorate this limitation
we add the lexicalized features described in Sec-
tion 4, for example the combination of the lemma
with the negation syntactic dependency. These ad-
ditional features improved precision from 81.31%
to 84.98%.
Finally, we add the abstracts to the training data
which improves recall but harms precision slightly
(Table 4) when only tokens and lemmas are used
as features. Nevertheless, we decided to keep them
as they have a positive effect for all other feature
representations.
A misinterpretation of the BioScope paper
(Szarvas et al., 2008) led us to believe that five of
the nine full articles in the training data were anno-
tated using the guidelines of Medlock and Briscoe
(2007). After the shared task, the organizers clar-
ified to us that all the full articles were annotated
using the BioScope guidelines. Due to our misin-
terpretation, we change our experimental setup to
cross-validate on the four full articles annotated in
BioScope only, considering the other five full ar-
ticles and the abstracts only as training data. We
keep this setup for the remainder of the paper.
We repeat the cross-validation experiments with
the full feature set and this new experimental setup
and report the results in Table 5. Using the same
feature set, we experiment with the Gaussian prior
instead of the sparsity-enforcing Laplace prior
which results in decreased precision and F-score,
</bodyText>
<page confidence="0.995736">
21
</page>
<table confidence="0.9998525">
Recall Precision F-score
cross-Laplace 80.33 84.21 82.23
cross-Gaussian 81.59 80.58 81.08
test 84.94 85.48 85.21
</table>
<tableCaption confidence="0.999205">
Table 5: Performance of the final system in Task1.
</tableCaption>
<bodyText confidence="0.99102">
therefore confirming our intuition that most fea-
tures extracted are irrelevant to the task and should
have zero weight. Finally, we report our perfor-
mance on the test data using the Laplace prior.
</bodyText>
<sectionHeader confidence="0.976105" genericHeader="method">
6 Detecting the scope of the hedges
</sectionHeader>
<bodyText confidence="0.998746239130435">
In Task2, the systems need to identify speculative
cues and their respective scopes. Since our system
for Task1 identifies cues, our discussion of Task2
focuses on identifying the scope of a given cue.
It is a non-trivial task, since scopes can be nested
and can span over a large number of tokens of the
sentence.
An initial approach explored was to associate
each cue with the token representing the syntactic
head of its scope and then to infer the scope us-
ing syntactic parsing. In order to achieve this, we
resolved the (almost always multi-token) scopes
to their syntactic heads and then built a classi-
fier whose features are based on syntactic depen-
dency paths. Multi-token scopes which were not
headed syntactically by a single token (according
to the parser) were discarded in order to obtain a
cleaner dataset for training. This phenomenon oc-
curs rather frequently, therefore reducing the train-
ing instances. At testing, the classifier identifies
the syntactic head of the scope for each cue and
we infer the scope from the syntactic parser’s out-
put. If more than one scope head is identified for
a particular cue, then the scopes are concatenated.
The performance of this approach turned out to
be very low, 10.34% in F-score. We identified two
principal reasons for this. First, relying on the syn-
tactic parser’s output to infer the scope is unavoid-
ably affected by parsing errors. Second, the scope
annotation takes into account semantics instead of
syntax. For example bibliographic references are
excluded based on their semantics.
In order to handle these issues, we developed an
approach that predicts whether each token of the
sentence belongs to the scope of a given cue. The
overall scope for that cue becomes the string en-
closed by the left- and right-most tokens predicted
to belong to the scope. The features used by the
classifier to predict whether a token belongs to the
scope of a particular cue are based on the short-
est syntactic dependency path connecting them,
which is found using Dijkstra’s algorithm. If no
such path is found (commonly due to parsing fail-
ure), then the token is classified as not belonging
to the scope of that cue. The features we use are
the following:
</bodyText>
<listItem confidence="0.837576333333333">
• The dependency path between the cue and the
token, combined with both their lemmas.
• According to the guidelines, different cues
</listItem>
<bodyText confidence="0.952062956521739">
have different preferences in having their
scopes extended to their left or to their right.
For example modal auxiliaries like “can” in
Table 1 extend their scope to their right. There-
fore we add the dependency path feature de-
fined above refined by whether the token is on
the left or the right of the cue in question.
• We combine the dependency path and the lem-
mas of the cue and the token with their PoS
tags and CCG supertags, since these tags re-
fine the syntactic function of the tokens.
The features defined above are very sparse, espe-
cially when longer dependency paths are involved.
This can affect performance substantially, as the
scopes can be rather long, in many cases spanning
over the whole sentence. An unseen dependency
path between a cue and a token during testing re-
sults in the token being excluded from the scope
of that cue. In turn, this causes predicted scopes to
be shorter than they should be. We attempt to al-
leviate this sparsity in two stages. First, we make
the following simplifications to the labels of the
dependencies:
</bodyText>
<listItem confidence="0.9986982">
• Adjectival, noun compound, relative clause
and participial modifiers (amod, nn, rcmod,
partmod) are converted to generic modifiers
(mod).
• Passive auxiliary (auxpass) and copula (cop)
relations are converted to auxiliary relations
(aux).
• Clausal complement relations with inter-
nal/external subject (ccomp/xcomp) are con-
verted to complement relations (comp).
• All subject relations in passive or active voice
(nsubj, nsubjpass, csubj, csubjpass) are con-
verted to subjects (subj).
• Direct and indirect object relations (iobj, dobj)
are converted to objects (obj).
</listItem>
<page confidence="0.967187">
22
</page>
<bodyText confidence="0.672325">
• We de-lexicalize conjunct (conj *) and prepo-
sitional modifier relations (prep *).
Second, we shorten the dependency paths:
</bodyText>
<listItem confidence="0.974626888888889">
• Since the SD collapsed dependencies format
treats conjunctions asymmetrically (conj), we
propagate the subject and object dependencies
of the head of the conjunction to the depen-
dent. We process appositional and abbrevi-
ation modifiers (appos, abbrev) in the same
way.
• Determiner and predeterminer relations (det,
predet) in the end of the dependency path are
removed, since the determiners (e.g. “the”)
and predeterminers (e.g. “both”) are included
in/excluded from the scope following their
syntactic heads.
• Consecutive modifier and dependent relations
(mod, dep) are replaced by a single relation of
the same type.
• Auxiliary relations (aux) that are not in the be-
ginning or the end of the path are removed.
</listItem>
<bodyText confidence="0.999885705882353">
Despite these simplifications, it is still possible
during testing to encounter dependency paths un-
seen in the training data. In order to ameliorate
this issue, we implement a backoff strategy that
progressively shortens the dependency path until
it matches a dependency path seen in the training
data. For example, if the path from a cue to a token
is subj-mod-mod and it has not been seen in the
training data, we test if subj-mod has been seen.
If it has, we consider it as the dependency path to
define the features described earlier. If not, we test
for subj in the same way. This strategy relies on
the assumption that tokens that are likely to be in-
cluded in/excluded from the scope following the
tokens they are syntactically dependent on. For
example, modifiers are likely to follow the token
being modified.
</bodyText>
<subsectionHeader confidence="0.496136">
7 Task2 results and error analysis
</subsectionHeader>
<bodyText confidence="0.9994415">
In order to evaluate the performance of our ap-
proach, we performed four-fold cross-validation
on the four BioScope full articles, using the re-
maining full articles and the abstracts as training
data only. The performance achieved using the
features mentioned in Section 6 is 28.62% F-score,
while using the simplified dependency paths in-
stead of the path extracted from the parser’s out-
put improves it to 34.35% F-score. Applying the
back-off strategy for unseen dependency paths to
</bodyText>
<table confidence="0.9954768">
features Recall Precision F-score
standard 27.54 29.79 28.62
simplified 33.11 35.69 34.35
+backoff 34.10 36.75 35.37
+post 40.98 44.17 42.52
</table>
<tableCaption confidence="0.946516">
Table 6: Performance on Task2 using cross-
validation on BioScope full articles.
</tableCaption>
<bodyText confidence="0.999816142857143">
the simplified paths results in 35.37% F-score (Ta-
ble 6).
Our system predicts only single token cues.
This agrees in spirit with the minimal cue an-
notation strategy stated in the BioScope guide-
lines. The guidelines allow for multi-token cues,
referred to as complex keywords, which are de-
fined as cases where the tokens of a phrase cannot
express uncertainty independently. We argue that
this definition is rather vague, and combined with
the requirement for contiguity, results in cue in-
stances such as “indicating that” (multiple occur-
rences), “looks as” (S4.232) and “address a num-
ber of questions” (S4.36) annotated as cues. It is
unclear why “suggesting that” or “appears that”
are not annotated as cues as well, or why “that”
contributes to the semantic content of “indicate”.
“that” does help determine the sense of “indicate”,
but we argue that it should not be part of the cue as
it does not contribute to its semantic content. “in-
dicate that” is the only consistent multi-token cue
pattern in the training data. Therefore, when our
system identifies as a cue a token with the lemma
“indicate”, if this token is followed by “that”,
“that” is added to the cue. Given the annotation
difficulties multi-token cues present, it would be
useful during evaluation to relax cue matching in
the same way as in the BioNLP 2009 shared task,
i.e. considering as correct those cues predicted
within one token of the gold standard annotation.
As explained in Section 6, bibliographic ref-
erences are excluded from scopes and cannot be
recognized by means of syntactic parsing only.
Additionally, in some cases the XML formatting
does not preserve the parentheses and/or brack-
ets around numerical references. We employ two
post-processing steps to deal with these issues.
First, if the ultimate token of a scope happens to
be the penultimate token of the sentence and a
number, then it is removed from the scope. This
step can have a negative effect when the last to-
ken of the scope and penultimate token of the sen-
</bodyText>
<page confidence="0.996844">
23
</page>
<table confidence="0.999589">
Recall Precision F-score
Cues cross 74.52 81.63 77.91
test 74.50 81.85 78.00
Task2 cross 40.98 44.17 42.52
test 42.40 45.96 44.11
</table>
<tableCaption confidence="0.964462">
Table 7: Performance on cue identification and
cue/scope identification in Task2.
</tableCaption>
<bodyText confidence="0.998207666666667">
tence happens to be a genuine number, as in Fig-
ure 1. In our experiments however, this heuristic
always increased performance. Second, if a scope
contains an opening parenthesis but not its clos-
ing one, then the scope is limited to the token im-
mediately before the opening one. Note that the
training data annotation allows for partial paren-
thetical statements to be included in scopes, as a
result of terminating scopes at bibliographic ref-
erences which are not the only tokens in a paren-
theses. For example, in S7.259: “expressed (ED,
unpublished)” the scope is terminated after “ED”.
These post-processing steps improved the perfor-
mance substantially to 42.52% F-score (Table 6).
The requirement for contiguous scope spans
which include their cue(s) is not treated appropri-
ately by our system, since we predict each token of
the scope independently. Combined with the fact
that the guidelines frequently define scopes to ex-
tend either to the left or to the right of the cue, an
approach based on sequential tagging and/or pre-
dicting boundaries could perform better. However,
as mentioned in the guidelines, the contiguity re-
quirement sometimes forced the inclusion of to-
kens that should have been excluded given the pur-
pose of the task.
Our final performance on the test data is 44.11%
in F-score (Table 7). This is higher than the one re-
ported in the official results (38.37%) because we
subsequently increased the coverage of the C&amp;C
parser (parse failures resulted in 63 cues not re-
ceiving a scope), the addition of the back-off strat-
egy for unseen dependency paths and the clarifica-
tion on the inclusion of bibliographic references in
the scopes which resulted in improving the paren-
theses post-processing steps.
</bodyText>
<sectionHeader confidence="0.999927" genericHeader="method">
8 Related work
</sectionHeader>
<bodyText confidence="0.999960647058823">
The shared task uses only full articles for testing
while both abstracts and full articles are used for
training. We argue that this represents a realistic
scenario for system developers since annotated re-
sources consist mainly of abstracts, while most in-
formation extraction systems are applied to full ar-
ticles. Also, the shared task aimed at detecting the
scope of speculation, while most previous work
(Light et al., 2004; Medlock and Briscoe, 2007;
Kilicoglu and Bergler, 2008) considered only clas-
sification of sentences, possibly due to the lack of
appropriately annotated resources.
The increasing interest in detecting speculative
language in scientific text resulted in a number of
guidelines. Compared to the most recent previous
definition by Medlock and Briscoe (2007), Bio-
Scope differs in the following ways:
</bodyText>
<listItem confidence="0.989161333333333">
• BioScope does not annotate anaphoric hedge
references.
• BioScope annotates indications of experimen-
tally observed non-universal behaviour.
• BioScope annotates statements of explicitly
proposed alternatives.
</listItem>
<bodyText confidence="0.999130903225807">
The first difference is due to the requirement that
the scope of the speculation be annotated, which
is not possible when it is present in a different sen-
tence. The other two differences follow from the
stated purpose which is the detection of sentences
containing uncertain information.
In related work, Hyland (1996) associates the
use of speculative language in scholarly publica-
tions with the purpose for which they are em-
ployed by the authors. In particular, he dis-
tinguishes content-oriented hedges from reader-
oriented ones. The former are used to calibrate
the strength of the claims made, while the latter
are employed in order to soften anticipated crit-
icism on behalf of the reader. Content-oriented
hedges are further distinguished as accuracy-
oriented ones, used to express uncertain claims
more accurately, and writer-oriented ones, used
to limit the commitment of the author(s) to the
claims. While the boundaries between these dis-
tinctions are not clear-cut and instances of hedging
can serve more than one of these purposes simulta-
neously, it is worth bearing them in mind while ap-
proaching the task. With respect to the shared task,
taking into account that hedging is used to ex-
press statements more accurately can help resolve
the ambiguity when annotating certain statements
about uncertainty. Such statements, which involve
words such as “estimate”, “possible”, “predict”,
occur frequently in full articles.
Wilson (2008) analyzes speculation detection
</bodyText>
<page confidence="0.995482">
24
</page>
<bodyText confidence="0.999890416666667">
inside a general framework for sentiment analysis
centered around the notion of private states (emo-
tions, thoughts, intentions, etc.) that are not open
to objective observation or verification. Specu-
lation is annotated with a spec-span/spec-target
scheme by answering the questions what the spec-
ulation is and what the speculation is about. With
respect to the BioScope guidelines, spec-span is
similar to what scope attempts to capture. spec-
span and spec-target do not need to be present
at the same time, which could help annotating
anaphoric cues.
</bodyText>
<sectionHeader confidence="0.997863" genericHeader="conclusions">
9 Conclusions
</sectionHeader>
<bodyText confidence="0.999957090909091">
This paper describes our approach to the CoNLL-
2010 shared task on speculative language detec-
tion using logistic regression and syntactic depen-
dencies. We achieved competitive performance on
sentence level uncertainty classification (Task1),
but not on scope identification (Task2). Motivated
by our error analysis we suggest refinements to the
task definition that could improve annotation.
Our approach to detecting speculation cues suc-
cessfully employed syntax as a proxy for the se-
mantic content of words. In addition, we demon-
strated that performance gains can be obtained by
choosing an appropriate prior for feature weights
in logistic regression. Finally, our performance in
scope detection was improved substantially by the
simplification scheme used to reduce the sparsity
of the dependency paths. It was devised using hu-
man judgment, but as information extraction sys-
tems become increasingly reliant on syntax and
each task is likely to need a different scheme, fu-
ture work should investigate how this could be
achieved using machine learning.
</bodyText>
<sectionHeader confidence="0.996523" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999944833333333">
We would like to thank the organizers for provid-
ing the infrastructure and the data for the shared
task, Laura Rimell for her help with the C&amp;C
parser and Marina Terkourafi for useful discus-
sions. The authors were funded by NIH/NLM
grant R01/LM07050.
</bodyText>
<sectionHeader confidence="0.999268" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999082783333333">
Jinying Chen and Martha Palmer. 2009. Improving
English Verb Sense Disambiguation Performance
with Linguistically Motivated Features and Clear
Sense Distinction Boundaries. Language Resources
and Evaluation, 43(2):143–172.
Marie-Catherine de Marneffe and Christopher D. Man-
ning. 2008. The Stanford typed dependencies rep-
resentation. In CrossParser ’08: Coling 2008: Pro-
ceedings of the Workshop on Cross-Framework and
Cross-Domain Parser Evaluation, pages 1–8.
Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos
Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL-
2010 Shared Task: Learning to Detect Hedges and
their Scope in Natural Language Text. In Proceed-
ings of CoNLL-2010 Shared Task, pages 1–12.
Alexander Genkin, David D. Lewis, and David Madi-
gan. 2006. Large-scale Bayesian Logistic Re-
gression for Text Classification. Technometrics,
49(3):291–304.
Ken Hyland. 1996. Writing Without Conviction?
Hedging in Science Research Articles. Applied Lin-
guistics, 17(4):433–454.
Halil Kilicoglu and Sabine Bergler. 2008. Recog-
nizing speculative language in biomedical research
articles: a linguistically motivated perspective. In
BioNLP ’08: Proceedings of the Workshop on Cur-
rent Trends in Biomedical Natural Language Pro-
cessing, pages 46–53.
Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-
nobu Kano, and Jun’ichi Tsujii. 2009. Overview
of BioNLP’09 shared task on event extraction. In
BioNLP ’09: Proceedings of the Workshop on
BioNLP, pages 1–9.
Marc Light, Xin Ying Qiu, and Padmini Srinivasan.
2004. The Language of Bioscience: Facts, Specu-
lations, and Statements In Between. In HLT-NAACL
2004 Workshop: BioLINK 2004, Linking Biological
Literature, Ontologies and Databases, pages 17–24.
Ben Medlock and Ted Briscoe. 2007. Weakly Super-
vised Learning for Hedge Classification in Scientific
Literature. In Proceedings of the 45th Annual Meet-
ing of the Association of Computational Linguistics,
pages 992–999.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of English. Nat-
ural Language Engineering, 7(3):207–223.
Laura Rimell and Stephen Clark. 2009. Port-
ing a lexicalized-grammar parser to the biomedi-
cal domain. Journal of Biomedical Informatics,
42(5):852–865.
Gy¨orgy Szarvas, Veronika Vincze, Rich´ard Farkas, and
J´anos Csirik. 2008. The BioScope corpus: anno-
tation for negation, uncertainty and their scope in
biomedical texts. In BioNLP ’08: Proceedings of
the Workshop on Current Trends in Biomedical Nat-
ural Language Processing, pages 38–45.
Theresa Ann Wilson. 2008. Fine-grained Subjectivity
and Sentiment Analysis: Recognizing the Intensity,
Polarity, and Attitudes of Private States. Ph.D. the-
sis, University of Pittsburgh.
</reference>
<page confidence="0.998756">
25
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.279068">
<title confidence="0.9976115">Detecting Speculative Language Using Syntactic and Logistic Regression</title>
<author confidence="0.805234">Vlachos</author>
<affiliation confidence="0.99826">Department of Biostatistics and Medical University of</affiliation>
<abstract confidence="0.996340181818182">In this paper we describe our approach to the CoNLL-2010 shared task on detecting speculative language in biomedical text. We treat the detection of sentences containing uncertain information (Task1) as a token classification task since the or absence of the sentence label. We distinguish words that have speculative and non-speculative meaning by employing syntactic features as a proxy for their semantic content. In to identify the each cue (Task2), we learn a classifier that predicts whether each token of a sentence belongs to the scope of a given cue. The features in the classifier are based on the syntactic dependency path between the cue and the token. In both tasks, we use a Bayesian logistic regression classifier incorporating a sparsity-enforcing Laplace prior. Overall, the performance achieved is 85.21%</abstract>
<note confidence="0.5192835">F-score and 44.11% F-score in Task1 and Task2, respectively.</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jinying Chen</author>
<author>Martha Palmer</author>
</authors>
<title>Improving English Verb Sense Disambiguation Performance with Linguistically Motivated Features and Clear Sense Distinction Boundaries. Language Resources and Evaluation,</title>
<date>2009</date>
<pages>43--2</pages>
<contexts>
<context position="10128" citStr="Chen and Palmer, 2009" startWordPosition="1611" endWordPosition="1614">portant to further refine the cue identification procedure. Determining whether a token is used as a speculative cue or not resembles supervised word sense disambiguation. The main difference is that instead of having an inventory of senses for each word, we have two senses applicable to all words. As in most word sense disambiguation tasks, the classification of a word as cue or not is dependent on the other words in the sentence, which we take into account using syntax. The syntactic context of words is a useful proxy to their semantics, as shown in recent work on verb sense disambiguation (Chen and Palmer, 2009). Furthermore, it is easy to obtain syntactic information automatically using a parser, even though there will be some noise due to parsing errors. Similar intuitions were exploited by Kilicoglu and Bergler (2008) in refining a dictionary of cues with syntactic rules. In what follows, we present the features extracted for each token for our final system, along with an example of their application in Table 2. Where appropriate we give the relevant labels in the Stanford Dependency (SD) scheme in parentheses for reproducibility: • We extract the token itself and its lemma as features. • To handl</context>
</contexts>
<marker>Chen, Palmer, 2009</marker>
<rawString>Jinying Chen and Martha Palmer. 2009. Improving English Verb Sense Disambiguation Performance with Linguistically Motivated Features and Clear Sense Distinction Boundaries. Language Resources and Evaluation, 43(2):143–172.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>The Stanford typed dependencies representation.</title>
<date>2008</date>
<booktitle>In CrossParser ’08: Coling 2008: Proceedings of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation,</booktitle>
<pages>1--8</pages>
<marker>de Marneffe, Manning, 2008</marker>
<rawString>Marie-Catherine de Marneffe and Christopher D. Manning. 2008. The Stanford typed dependencies representation. In CrossParser ’08: Coling 2008: Proceedings of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rich´ard Farkas</author>
<author>Veronika Vincze</author>
<author>Gy¨orgy M´ora</author>
<author>J´anos Csirik</author>
<author>Gy¨orgy Szarvas</author>
</authors>
<title>The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text.</title>
<date>2010</date>
<booktitle>In Proceedings of CoNLL-2010 Shared Task,</booktitle>
<pages>1--12</pages>
<marker>Farkas, Vincze, M´ora, Csirik, Szarvas, 2010</marker>
<rawString>Rich´ard Farkas, Veronika Vincze, Gy¨orgy M´ora, J´anos Csirik, and Gy¨orgy Szarvas. 2010. The CoNLL2010 Shared Task: Learning to Detect Hedges and their Scope in Natural Language Text. In Proceedings of CoNLL-2010 Shared Task, pages 1–12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Genkin</author>
<author>David D Lewis</author>
<author>David Madigan</author>
</authors>
<title>Large-scale Bayesian Logistic Regression for Text Classification.</title>
<date>2006</date>
<journal>Technometrics,</journal>
<volume>49</volume>
<issue>3</issue>
<contexts>
<context position="6514" citStr="Genkin et al., 2006" startWordPosition="1011" endWordPosition="1014"> the native C&amp;C output follows a different formalism, we made this choice because it allows for the use of different parsers with minimal adaptation. Finally, an important pre-processing step we take is tokenization of the original text. Since the PoS tagger is trained on the GENIA corpus which follows the Penn TreeBank tokenization scheme, we use the tokenization script provided by the treebank.1 1http://www.cis.upenn.edu/˜treebank/tokenization.html 3 Bayesian logistic regression In both tasks, we use a Bayesian logistic regression classifier incorporating a sparsity-enforcing Laplace prior (Genkin et al., 2006). Logistic regression models are of the form: exp(xβT) p(y = +1|β, x) = 1 + exp(xβT) (1) where y ∈ {+1, −1} is a binary class label, x is the feature vector representation of the instance to be classified and β is the feature weight vector which is learnt from the training data. Since feature interactions are not directly represented, the interactions that are expected to matter for the task considered must be specified as additional features. In Bayesian logistic regression, a prior distribution on β is used which encodes our prior beliefs on the feature weights. In this work, we use the Lapl</context>
</contexts>
<marker>Genkin, Lewis, Madigan, 2006</marker>
<rawString>Alexander Genkin, David D. Lewis, and David Madigan. 2006. Large-scale Bayesian Logistic Regression for Text Classification. Technometrics, 49(3):291–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ken Hyland</author>
</authors>
<title>Writing Without Conviction? Hedging in Science Research Articles.</title>
<date>1996</date>
<journal>Applied Linguistics,</journal>
<volume>17</volume>
<issue>4</issue>
<contexts>
<context position="29238" citStr="Hyland (1996)" startWordPosition="4765" endWordPosition="4766">s definition by Medlock and Briscoe (2007), BioScope differs in the following ways: • BioScope does not annotate anaphoric hedge references. • BioScope annotates indications of experimentally observed non-universal behaviour. • BioScope annotates statements of explicitly proposed alternatives. The first difference is due to the requirement that the scope of the speculation be annotated, which is not possible when it is present in a different sentence. The other two differences follow from the stated purpose which is the detection of sentences containing uncertain information. In related work, Hyland (1996) associates the use of speculative language in scholarly publications with the purpose for which they are employed by the authors. In particular, he distinguishes content-oriented hedges from readeroriented ones. The former are used to calibrate the strength of the claims made, while the latter are employed in order to soften anticipated criticism on behalf of the reader. Content-oriented hedges are further distinguished as accuracyoriented ones, used to express uncertain claims more accurately, and writer-oriented ones, used to limit the commitment of the author(s) to the claims. While the bo</context>
</contexts>
<marker>Hyland, 1996</marker>
<rawString>Ken Hyland. 1996. Writing Without Conviction? Hedging in Science Research Articles. Applied Linguistics, 17(4):433–454.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Halil Kilicoglu</author>
<author>Sabine Bergler</author>
</authors>
<title>Recognizing speculative language in biomedical research articles: a linguistically motivated perspective.</title>
<date>2008</date>
<booktitle>In BioNLP ’08: Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,</booktitle>
<pages>46--53</pages>
<contexts>
<context position="10341" citStr="Kilicoglu and Bergler (2008)" startWordPosition="1643" endWordPosition="1646"> of having an inventory of senses for each word, we have two senses applicable to all words. As in most word sense disambiguation tasks, the classification of a word as cue or not is dependent on the other words in the sentence, which we take into account using syntax. The syntactic context of words is a useful proxy to their semantics, as shown in recent work on verb sense disambiguation (Chen and Palmer, 2009). Furthermore, it is easy to obtain syntactic information automatically using a parser, even though there will be some noise due to parsing errors. Similar intuitions were exploited by Kilicoglu and Bergler (2008) in refining a dictionary of cues with syntactic rules. In what follows, we present the features extracted for each token for our final system, along with an example of their application in Table 2. Where appropriate we give the relevant labels in the Stanford Dependency (SD) scheme in parentheses for reproducibility: • We extract the token itself and its lemma as features. • To handle cases where word senses are identifiable by the PoS of a token (“might result” vs “the might”), we combine the latter with the lemma and add it as a feature. • We combine the lemma with the CCG supertag and add </context>
<context position="12231" citStr="Kilicoglu and Bergler, 2008" startWordPosition="1978" endWordPosition="1982">evices but they are also among the most ambiguous. For example, “can” is annotated as a cue in 16% of its occurrences and it is the fifth most frequent cue in the full articles. To resolve this ambiguity, we add as features the lemma of the main verb the auxiliary is dependent on (aux) as well as the lemmas of any dependents of the main verb. Thus we can capture some stereotypical speculative expressions in scientific articles (e.g “could be due”), while avoiding false positives that are distinguished by the use of first person plural pronoun and/or reference to objective enabling conditions (Kilicoglu and Bergler, 2008). • Speculation can be expressed via negation of a word expressing certainty (e.g. “we do not know”), therefore we add the lemma of the token prefixed with “not” (neg). • In order to capture stereotypical hedging expressions such as “raises the question” and “on the assumption” we add as features the direct object of verbs combined with the lemma of their object (dobj) and the preposition for nouns in a prepositional relation (prep *). • In order to capture the effect of adverbs on the hedging function of verbs (e.g. “theoretically 20 features Recall Precision F-score tokens, lemmas 75.92 81.0</context>
<context position="28368" citStr="Kilicoglu and Bergler, 2008" startWordPosition="4635" endWordPosition="4638">tion on the inclusion of bibliographic references in the scopes which resulted in improving the parentheses post-processing steps. 8 Related work The shared task uses only full articles for testing while both abstracts and full articles are used for training. We argue that this represents a realistic scenario for system developers since annotated resources consist mainly of abstracts, while most information extraction systems are applied to full articles. Also, the shared task aimed at detecting the scope of speculation, while most previous work (Light et al., 2004; Medlock and Briscoe, 2007; Kilicoglu and Bergler, 2008) considered only classification of sentences, possibly due to the lack of appropriately annotated resources. The increasing interest in detecting speculative language in scientific text resulted in a number of guidelines. Compared to the most recent previous definition by Medlock and Briscoe (2007), BioScope differs in the following ways: • BioScope does not annotate anaphoric hedge references. • BioScope annotates indications of experimentally observed non-universal behaviour. • BioScope annotates statements of explicitly proposed alternatives. The first difference is due to the requirement t</context>
</contexts>
<marker>Kilicoglu, Bergler, 2008</marker>
<rawString>Halil Kilicoglu and Sabine Bergler. 2008. Recognizing speculative language in biomedical research articles: a linguistically motivated perspective. In BioNLP ’08: Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 46–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jin-Dong Kim</author>
<author>Tomoko Ohta</author>
<author>Sampo Pyysalo</author>
<author>Yoshinobu Kano</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Overview of BioNLP’09 shared task on event extraction.</title>
<date>2009</date>
<booktitle>In BioNLP ’09: Proceedings of the Workshop on BioNLP,</booktitle>
<pages>1--9</pages>
<contexts>
<context position="5504" citStr="Kim et al., 2009" startWordPosition="860" endWordPosition="863">esting aspect of this three-stage parsing approach is that, if the parse selection module fails to construct a parse tree for the sentence (a common issue when syntactic parsers are ported to new domains), the lexical categories obtained by the supertagger preserve some of the syntactic information that would not be found in PoS tags. The adaptation to the biomedical domain by Rimell and Clark (2009) involved re-training the PoS tagger and the CCG supertagger using indomain resources, while the parse selection component was left intact. As recent work in the BioNLP 2009 shared task has shown (Kim et al., 2009), domain-adapted parsing benefits information extraction systems. The native output of the C&amp;C parser is converted into the Stanford Dependency (SD) collapsed dependency format (de Marneffe and Manning, 2008). These dependencies define binary relations between tokens and the labels of these relations are obtained from a hierarchy. While the conversion is unlikely to be perfect given that the native C&amp;C output follows a different formalism, we made this choice because it allows for the use of different parsers with minimal adaptation. Finally, an important pre-processing step we take is tokeniz</context>
</contexts>
<marker>Kim, Ohta, Pyysalo, Kano, Tsujii, 2009</marker>
<rawString>Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshinobu Kano, and Jun’ichi Tsujii. 2009. Overview of BioNLP’09 shared task on event extraction. In BioNLP ’09: Proceedings of the Workshop on BioNLP, pages 1–9.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Light</author>
<author>Xin Ying Qiu</author>
<author>Padmini Srinivasan</author>
</authors>
<title>The Language of Bioscience: Facts, Speculations, and Statements In Between.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: BioLINK</booktitle>
<pages>17--24</pages>
<contexts>
<context position="28311" citStr="Light et al., 2004" startWordPosition="4627" endWordPosition="4630">y for unseen dependency paths and the clarification on the inclusion of bibliographic references in the scopes which resulted in improving the parentheses post-processing steps. 8 Related work The shared task uses only full articles for testing while both abstracts and full articles are used for training. We argue that this represents a realistic scenario for system developers since annotated resources consist mainly of abstracts, while most information extraction systems are applied to full articles. Also, the shared task aimed at detecting the scope of speculation, while most previous work (Light et al., 2004; Medlock and Briscoe, 2007; Kilicoglu and Bergler, 2008) considered only classification of sentences, possibly due to the lack of appropriately annotated resources. The increasing interest in detecting speculative language in scientific text resulted in a number of guidelines. Compared to the most recent previous definition by Medlock and Briscoe (2007), BioScope differs in the following ways: • BioScope does not annotate anaphoric hedge references. • BioScope annotates indications of experimentally observed non-universal behaviour. • BioScope annotates statements of explicitly proposed alter</context>
</contexts>
<marker>Light, Qiu, Srinivasan, 2004</marker>
<rawString>Marc Light, Xin Ying Qiu, and Padmini Srinivasan. 2004. The Language of Bioscience: Facts, Speculations, and Statements In Between. In HLT-NAACL 2004 Workshop: BioLINK 2004, Linking Biological Literature, Ontologies and Databases, pages 17–24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ben Medlock</author>
<author>Ted Briscoe</author>
</authors>
<title>Weakly Supervised Learning for Hedge Classification in Scientific Literature.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>992--999</pages>
<contexts>
<context position="16470" citStr="Medlock and Briscoe (2007)" startWordPosition="2668" endWordPosition="2671">, for example the combination of the lemma with the negation syntactic dependency. These additional features improved precision from 81.31% to 84.98%. Finally, we add the abstracts to the training data which improves recall but harms precision slightly (Table 4) when only tokens and lemmas are used as features. Nevertheless, we decided to keep them as they have a positive effect for all other feature representations. A misinterpretation of the BioScope paper (Szarvas et al., 2008) led us to believe that five of the nine full articles in the training data were annotated using the guidelines of Medlock and Briscoe (2007). After the shared task, the organizers clarified to us that all the full articles were annotated using the BioScope guidelines. Due to our misinterpretation, we change our experimental setup to cross-validate on the four full articles annotated in BioScope only, considering the other five full articles and the abstracts only as training data. We keep this setup for the remainder of the paper. We repeat the cross-validation experiments with the full feature set and this new experimental setup and report the results in Table 5. Using the same feature set, we experiment with the Gaussian prior i</context>
<context position="28338" citStr="Medlock and Briscoe, 2007" startWordPosition="4631" endWordPosition="4634">ncy paths and the clarification on the inclusion of bibliographic references in the scopes which resulted in improving the parentheses post-processing steps. 8 Related work The shared task uses only full articles for testing while both abstracts and full articles are used for training. We argue that this represents a realistic scenario for system developers since annotated resources consist mainly of abstracts, while most information extraction systems are applied to full articles. Also, the shared task aimed at detecting the scope of speculation, while most previous work (Light et al., 2004; Medlock and Briscoe, 2007; Kilicoglu and Bergler, 2008) considered only classification of sentences, possibly due to the lack of appropriately annotated resources. The increasing interest in detecting speculative language in scientific text resulted in a number of guidelines. Compared to the most recent previous definition by Medlock and Briscoe (2007), BioScope differs in the following ways: • BioScope does not annotate anaphoric hedge references. • BioScope annotates indications of experimentally observed non-universal behaviour. • BioScope annotates statements of explicitly proposed alternatives. The first differen</context>
</contexts>
<marker>Medlock, Briscoe, 2007</marker>
<rawString>Ben Medlock and Ted Briscoe. 2007. Weakly Supervised Learning for Hedge Classification in Scientific Literature. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 992–999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of English.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<contexts>
<context position="8209" citStr="Minnen et al., 2001" startWordPosition="1299" endWordPosition="1302"> If a cue is a present, any other (potentially “unhedging”) token becomes irrelevant to the task. Therefore, we cast the task as a binary token classification problem and determine the sentence label from the token-level decisions. Words used as speculative cues do not always denote speculation. For example, in BioScope “if” and “appear” are annotated as cues 4% and 83% of the times they are encountered. In order to gain better understanding of the task, we build a dictionary-based cue extractor. First we extract all the cues from the training data and use their lemmas, obtained using morpha (Minnen et al., 2001), to tag tokens in the test data. We keep only singletoken cues in order to avoid non-indicative lem19 token=indicating lemma=indicate PoS=VBG lemma+PoS=indicate+VBG CCG=(S[ng]\NP)/S[em] lemma+CCG=indicate+(S[ng]\NP)/S[em] Table 2: Features extracted for the token “indicating” from the Example in Table 1. CCG supertag (S[ng]\NP)/S[em] denotes that “indicating” expects an embedded clause (S[em]) to its right (indicated by the forward slash /) and a noun phrase (NP) to its left (indicated by the backward slash \) to form a present participle (S[ng]). mas entering the dictionary (e.g. “that” in “</context>
</contexts>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of English. Natural Language Engineering, 7(3):207–223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Rimell</author>
<author>Stephen Clark</author>
</authors>
<title>Porting a lexicalized-grammar parser to the biomedical domain.</title>
<date>2009</date>
<journal>Journal of Biomedical Informatics,</journal>
<volume>42</volume>
<issue>5</issue>
<contexts>
<context position="3538" citStr="Rimell and Clark, 2009" startWordPosition="552" endWordPosition="555">elongs to the scope of a particular cue (Section 6). The features used are based on the syntactic dependency path between the cue and the token. We report results and perform error analysis for both tasks, pointing out annotation issues that could be ameliorated (Sections 5 and 7). Based on our experience we suggest improvements on the task definition taking into account work from the broader field (Section 8). 2 Syntactic parsing for the biomedical domain The syntactic parser we chose for our experiments is the C&amp;C Combinatory Categorial Grammar (CCG) parser adapted to the biomedical domain (Rimell and Clark, 2009). In this framework, parsing is performed in three stages: partof-speech (PoS) tagging, CCG supertagging and parse selection. The parse selection module de18 Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 18–25, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics The Orthology and Combined modules both have states that achieve likelihood ratios above 400 (as high as 1207 for the Orthology module and 613 for the Combined module), {indicating that both these modules {can, on their own, predict some interactin</context>
<context position="5290" citStr="Rimell and Clark (2009)" startWordPosition="823" endWordPosition="826"> a relatively difficult task. Therefore, the parse selection module takes into account multiple predictions per token which allows recovery from supertagging errors while still reducing the ambiguity propagated. An interesting aspect of this three-stage parsing approach is that, if the parse selection module fails to construct a parse tree for the sentence (a common issue when syntactic parsers are ported to new domains), the lexical categories obtained by the supertagger preserve some of the syntactic information that would not be found in PoS tags. The adaptation to the biomedical domain by Rimell and Clark (2009) involved re-training the PoS tagger and the CCG supertagger using indomain resources, while the parse selection component was left intact. As recent work in the BioNLP 2009 shared task has shown (Kim et al., 2009), domain-adapted parsing benefits information extraction systems. The native output of the C&amp;C parser is converted into the Stanford Dependency (SD) collapsed dependency format (de Marneffe and Manning, 2008). These dependencies define binary relations between tokens and the labels of these relations are obtained from a hierarchy. While the conversion is unlikely to be perfect given </context>
</contexts>
<marker>Rimell, Clark, 2009</marker>
<rawString>Laura Rimell and Stephen Clark. 2009. Porting a lexicalized-grammar parser to the biomedical domain. Journal of Biomedical Informatics, 42(5):852–865.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gy¨orgy Szarvas</author>
<author>Veronika Vincze</author>
<author>Rich´ard Farkas</author>
<author>J´anos Csirik</author>
</authors>
<title>The BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts.</title>
<date>2008</date>
<booktitle>In BioNLP ’08: Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing,</booktitle>
<pages>38--45</pages>
<contexts>
<context position="2220" citStr="Szarvas et al., 2008" startWordPosition="328" endWordPosition="331">tes speculative language detection as two subtasks. In the first subtask (Task1), systems need to determine whether a sentence contains uncertain information or not. In the second subtask (Task2), systems need to identify the hedge cues and their scope in the sentence. Table 1 provides an example from the training data. The participants are provided with data from two domains: biomedical scientific literature (both abstracts and full articles) and Wikipedia. We choose to focus on the former. The training data for this domain are nine full articles and 1,273 abstracts from the BioScope corpus (Szarvas et al., 2008) and the test data are 15 full articles. Our approach to speculative language detection relies on syntactic parsing and machine learning. We give a description of the techniques used in Sections 2 and 3. We treat the detection of sentences containing uncertain information (Task1) as a token classification task in which we learn a classifier to predict whether a token is a cue or not. In order to handle words that have speculative and non-speculative meaning (e.g. “indicating” in the example of Table 1), we employ syntactic features as a proxy for their semantic content (Section 4). For scope i</context>
<context position="13687" citStr="Szarvas et al., 2008" startWordPosition="2221" endWordPosition="2224">ure to the verb (advmod). • To distinguish the probabilistic/numerical sense from the hedging sense of adjectives such as “possible”, we add the lemma and the number of the noun they modify as features (amod), since plural number tends to be associated with the probabilistic/numerical sense (e.g. “all possible combinations”). Finally, given that this stage is meant to identify cues in order to recover their scopes in Task2, we attempt to resolve multi-token cues in the training data into single-token ones. This agrees with the minimal strategy for marking cues stated in the corpus guidelines (Szarvas et al., 2008) and it simplifies scope detection. Therefore, during training multi-token cues are resolved to their syntactic head according to the dependency output, e.g. in Table 1 “indicate that” is restricted to “indicate” only. There were two cases in which this process failed; the cues being “cannot” (S3.167) and “not clear” (S3.269). We argue that the former is inconsistently annotated (the sentence reads “cannot be defined... ” and it would have been resolved to “defined”), while the latter is headed syntactically by the verb “be” which is preceding it. 5 Task1 results and error analysis Initially w</context>
<context position="16329" citStr="Szarvas et al., 2008" startWordPosition="2642" endWordPosition="2645">n affects only words expressing certainty. In order to ameliorate this limitation we add the lexicalized features described in Section 4, for example the combination of the lemma with the negation syntactic dependency. These additional features improved precision from 81.31% to 84.98%. Finally, we add the abstracts to the training data which improves recall but harms precision slightly (Table 4) when only tokens and lemmas are used as features. Nevertheless, we decided to keep them as they have a positive effect for all other feature representations. A misinterpretation of the BioScope paper (Szarvas et al., 2008) led us to believe that five of the nine full articles in the training data were annotated using the guidelines of Medlock and Briscoe (2007). After the shared task, the organizers clarified to us that all the full articles were annotated using the BioScope guidelines. Due to our misinterpretation, we change our experimental setup to cross-validate on the four full articles annotated in BioScope only, considering the other five full articles and the abstracts only as training data. We keep this setup for the remainder of the paper. We repeat the cross-validation experiments with the full featu</context>
</contexts>
<marker>Szarvas, Vincze, Farkas, Csirik, 2008</marker>
<rawString>Gy¨orgy Szarvas, Veronika Vincze, Rich´ard Farkas, and J´anos Csirik. 2008. The BioScope corpus: annotation for negation, uncertainty and their scope in biomedical texts. In BioNLP ’08: Proceedings of the Workshop on Current Trends in Biomedical Natural Language Processing, pages 38–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Ann Wilson</author>
</authors>
<title>Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of Private States.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pittsburgh.</institution>
<contexts>
<context position="30361" citStr="Wilson (2008)" startWordPosition="4940" endWordPosition="4941">r-oriented ones, used to limit the commitment of the author(s) to the claims. While the boundaries between these distinctions are not clear-cut and instances of hedging can serve more than one of these purposes simultaneously, it is worth bearing them in mind while approaching the task. With respect to the shared task, taking into account that hedging is used to express statements more accurately can help resolve the ambiguity when annotating certain statements about uncertainty. Such statements, which involve words such as “estimate”, “possible”, “predict”, occur frequently in full articles. Wilson (2008) analyzes speculation detection 24 inside a general framework for sentiment analysis centered around the notion of private states (emotions, thoughts, intentions, etc.) that are not open to objective observation or verification. Speculation is annotated with a spec-span/spec-target scheme by answering the questions what the speculation is and what the speculation is about. With respect to the BioScope guidelines, spec-span is similar to what scope attempts to capture. specspan and spec-target do not need to be present at the same time, which could help annotating anaphoric cues. 9 Conclusions </context>
</contexts>
<marker>Wilson, 2008</marker>
<rawString>Theresa Ann Wilson. 2008. Fine-grained Subjectivity and Sentiment Analysis: Recognizing the Intensity, Polarity, and Attitudes of Private States. Ph.D. thesis, University of Pittsburgh.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>