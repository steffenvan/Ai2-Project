<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000916">
<title confidence="0.999072">
A Distributional Analysis of a Lexicalized Statistical Parsing Model
</title>
<author confidence="0.999009">
Daniel M. Bikel
</author>
<affiliation confidence="0.998719">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.831834">
3330 Walnut Street
Philadelphia, PA 19104
</address>
<email confidence="0.999704">
dbikel@cis.upenn.edu
</email>
<sectionHeader confidence="0.993955" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999741368421053">
This paper presents some of the first data visualiza-
tions and analysis of distributions for a lexicalized
statistical parsing model, in order to better under-
stand their nature. In the course of this analysis,
we have paid particular attention to parameters that
include bilexical dependencies. The prevailing view
has been that such statistics are very informative but
suffer greatly from sparse data problems. By using a
parser to constrain-parse its own output, and by hy-
pothesizing and testing for distributional similarity
with back-off distributions, we have evidence that
finally explains that (a) bilexical statistics are actu-
ally getting used quite often but that (b) the distri-
butions are so similar to those that do not include
head words as to be nearly indistinguishable inso-
far as making parse decisions. Finally, our analysis
has provided for the first time an effective way to
do parameter selection for a generative lexicalized
statistical parsing model.
</bodyText>
<sectionHeader confidence="0.998995" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997540984375">
Lexicalized statistical parsing models, such as those
built by Black et al. (1992a), Magerman (1994),
Collins (1999) and Charniak (2000), have been
enormously successful, but they also have an enor-
mous complexity. Their success has often been
attributed to their sensitivity to individual lexical
items, and it is precisely this incorporation of lexical
items into features or parameter schemata that gives
rise to their complexity. In order to help determine
which features are helpful, the somewhat crude-but-
effective method has been to compare a model’s
overall parsing performance with and without a fea-
ture. Often, it has seemed that features that are
derived from linguistic principles result in higher-
performing models (cf. (Collins, 1999)). While
this may be true, it is clearly inappropriate to high-
light ex post facto the linguistically-motivated fea-
tures and rationalize their inclusion and state how
effective they are. A rigorous analysis of features or
parameters in relation to the entire model is called
for. Accordingly, this work aims to provide a thor-
ough analysis of the nature of the parameters in a
Collins-style parsing model, with particular focus
on the two parameter classes that generate lexical-
ized modifying nonterminals, for these are where
all a sentence’s words are generated except for the
head word of the entire sentence; also, these two
parameter classes have by far the most parameters
and suffer the most from sparse data problems. In
spite of using a Collins-style model as the basis for
analysis, throughout this paper, we will attempt to
present information that is widely applicable be-
cause it pertains to properties of the widely-used
Treebank (Marcus et al., 1993) and lexicalized pars-
ing models in general.
This work also sheds light on the much-discussed
“bilexical dependencies” of statistical parsing mod-
els. Beginning with the seminal work at IBM (Black
et al., 1991; Black et al., 1992b; Black et al., 1992a),
and continuing with such lexicalist approaches as
(Eisner, 1996), these features have been lauded for
their ability to approximate a word’s semantics as
a means to override syntactic preferences with se-
mantic ones (Collins, 1999; Eisner, 2000). How-
ever, the work of Gildea (2001) showed that, with
an approximate reimplementation of Collins’ Model
1, removing all parameters that involved dependen-
cies between a modifier word and its head resulted
in a surprisingly small decrease in overall parse ac-
curacy. The prevailing assumption was then that
such bilexical statistics were not useful for mak-
ing syntactic decisions, although it was not entirely
clear why. Subsequently, we replicated Gildea’s
experiment with a complete emulation of Model
2 and presented additional evidence that bilexical
statistics were barely getting used during decod-
ing (Bikel, 2004), appearing to confirm the origi-
nal result. However, the present work will show
that such statistics do get frequently used for the
highest-probability parses, but that when a Collins-
style model generates modifier words, the bilexical
parameters are so similar to their back-off distribu-
tions as to provide almost no extra predictive infor-
mation.
</bodyText>
<sectionHeader confidence="0.977912" genericHeader="introduction">
2 Motivation
</sectionHeader>
<bodyText confidence="0.999896609756098">
A parsing model coupled with a decoder (an al-
gorithm to search the space of possible trees for a
given terminal sequence) is largely an engineering
effort. In the end, the performance of the parser
with respect to its evaluation criteria—typically ac-
curacy, and perhaps also speed—are all that matter.
Consequently, the engineer must understand what
the model is doing only to the point that it helps
make the model perform better. Given the some-
what crude method of determining a feature’s ben-
efit by testing a model with and without the fea-
ture, a researcher can argue for the efficacy of that
feature without truly understanding its effect on the
model. For example, while adding a particular fea-
ture may improve parse accuracy, the reason may
have little to do with the nature of the feature and
everything to do with its canceling other features
that were theretofore hurting performance. In any
case, since this is engineering, the rationalization
for a feature is far less important than the model’s
overall performance increase.
On the other hand, science would demand that,
at some point, we analyze the multitude of features
in a state-of-the-art lexicalized statistical parsing
model. Such analysis is warranted for two reasons:
replicability and progress. The first is a basic tenet
of most sciences: without proper understanding of
what has been done, the relevant experiment(s) can-
not be replicated and therefore verified. The sec-
ond has to do with the idea that, when a discipline
matures, it can be difficult to determine what new
features can provide the most gain (or any gain, for
that matter). A thorough analysis of the various dis-
tributions being estimated in a parsing model allows
researchers to discover what is being learned most
and least well. Understanding what is learned most
well can shed light on the types of features or depen-
dencies that are most efficacious, pointing the way
to new features of that type. Understanding what is
learned least well defines the space in which to look
for those new features.
</bodyText>
<sectionHeader confidence="0.999769" genericHeader="method">
3 Frequencies
</sectionHeader>
<subsectionHeader confidence="0.999575">
3.1 Definitions and notation
</subsectionHeader>
<bodyText confidence="0.99984952631579">
In this paper we will refer to any estimated dis-
tribution as a parameter that has been instantiated
from a parameter class. For example, in an n-
gram language model, p(wi  |wi−1) is a parameter
class, whereas the estimated distribution ˆp(·  |the)
is a particular parameter from this class, consisting
of estimates of every word that can follow the word
“the”.
For this work, we used the model described in
(Bikel, 2002; Bikel, 2004). Our emulation of
Collins’ Model 2 (hereafter referred to simply as
“the model”) has eleven parameter classes, each of
which employs up to three back-off levels, where
back-off level 0 is just the “un-backed-off” maximal
context history.1 In other words, a smoothed prob-
ability estimate is the interpolation of up to three
different unsmoothed estimates. The notation and
description for each of these parameter classes is
shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.999529">
3.2 Basic frequencies
</subsectionHeader>
<bodyText confidence="0.999993125">
Before looking at the number of parameters in the
model, it is important to bear in mind the amount
of data on which the model is trained and on which
actual parameters will be induced from parameter
classes. The standard training set for English con-
sists of Sections 02–21 of the Penn Treebank, which
in turn consist of 39,832 sentences with a total of
950,028 word tokens (not including null elements).
There are 44,113 unique words (again, not includ-
ing null elements), 10,437 of which occur 6 times
or more.2 The trees consist of 904,748 brackets
with 28 basic nonterminal labels, to which func-
tion tags such as -TMP and indices are added in
the data to form 1184 observed nonterminals, not
including preterminals. After tree transformations,
the model maps these 1184 nonterminals down to
just 43. There are 42 unique part of speech tags that
serve as preterminals in the trees; the model prunes
away three of these (”, “ and .).
Induced from these training data, the model con-
tains 727,930 parameters; thus, there are nearly as
many parameters as there are brackets or word to-
kens. From a history-based grammar perspective,
there are 727,930 types of history contexts from
which futures are generated. However, 401,447 of
these are singletons. The average count for a history
context is approximately 35.56, while the average
diversity is approximately 1.72. The model contains
1,252,280 unsmoothed maximum-likelihood proba-
bility estimates (727, 930·1.72 7z� 1, 252, 280). Even
when a given future was not seen with a particu-
lar history, it is possible that one of its associated
</bodyText>
<footnote confidence="0.958207375">
1Collins’ model splits out the PM and PMw classes into left-
and right-specific versions, and has two additional classes for
dealing with coordinating conjunctions and inter-phrasal punc-
tuation. Our emulation of Collins’ model incorporates the in-
formation of these specialized parameter classes into the exist-
ing PM and PMw parameters.
2We mention this statistic because Collins’ thesis experi-
ments were performed with an unknown word threshold of 6.
</footnote>
<table confidence="0.9961318">
Notation Description No. of back-off levels
PH Generates unlexicalized head child given lexicalized parent 3
PsubcatL Generates subcat bag on left side of head child 3
PsubcatR Generates subcat bag on right side of head child 3
PM (PM,NPB) Generates partially-lexicalized modifying nonterminal (with NPB parent) 3
PMw (PMw,NPB) Generates head word of modifying nonterminal (with NPB parent) 3
PpriorNT Priors for nonterminal conditioning on its head word and part of speech 2
Ppriorlex Priors for head word/part of speech pairs (unconditional probabilities) 0
PTOPNT Generates partially-lexicalized child of +TOP+† 1
PTOPw Generates the head word for children of +TOP+† 2
</table>
<tableCaption confidence="0.870448333333333">
Table 1: All eleven parameter classes in our emulation of Collins’ Model 2. A partially-lexicalized nonter-
minal is a nonterminal label and its head word’s part of speech (such as NP(NN)). †The hidden nonterminal
+TOP+ is added during training to be the parent of every observed tree.
</tableCaption>
<figure confidence="0.974464923076923">
PP(IN/with) 1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
cummulative density
IN(IN/with) {NP–A} NP–A(NN/ ... )
</figure>
<figureCaption confidence="0.999727">
Figure 1: A frequent PMw history context, illustrated
</figureCaption>
<bodyText confidence="0.948777">
as a tree fragment. The ... represents the future that
is to be generated given this history.
0 500 1000 1500 2000 2500 3000 3500
rank
back-off contexts was seen with that future, leading
to a non-zero smoothed estimate. The total num-
ber of possible non-zero smoothed estimates in the
model is 562,596,053. Table 2 contains count and
diversity statistics for the two parameter classes on
which we will focus much of our attention, PM and
PMw. Note how the maximal-context back-off lev-
els (level 0) for both parameter classes have rela-
tively little training: on average, raw estimates are
obtained with history counts of only 10.3 and 4.4 in
the PM and PMw classes, respectively. Conversely,
observe how drastically the average number of tran-
sitions n increases as we remove dependence on the
head word going from back-off level 0 to 1.
</bodyText>
<subsectionHeader confidence="0.520401">
3.3 Exploratory data analysis: a common
distribution
</subsectionHeader>
<bodyText confidence="0.999962285714286">
To begin to get a handle on these distributions, par-
ticularly the relatively poorly-trained and/or high-
entropy distributions of the PMw class, it is useful to
perform some exploratory data analysis. Figure 1
illustrates the 25th-most-frequent PMw history con-
text as a tree fragment. In the top-down model, the
following elements have been generated:
</bodyText>
<listItem confidence="0.844841636363636">
• a parent nonterminal PP(IN/with) (a PP
headed by the word with with the part-of-
speech tag IN)
• the parent’s head child IN
• a right subcat bag containing NP-A (a single NP
argument must be generated somewhere on the
Figure 2: Cumulative density function for the PMw
history context illustrated in Figure 1.
right side of the head child)
• a partially-lexicalized right-modifying nonter-
minal
</listItem>
<bodyText confidence="0.999952913043478">
At this point in the process, a PMw parameter condi-
tioning on all of this context will be used to estimate
the probability of the head word of the NP-A(NN),
completing the lexicalization of that nonterminal. If
a candidate head word was seen in training in this
configuration, then it will be generated conditioning
on the full context that crucially includes the head
word with; otherwise, the model will back off to a
history context that does not include the head word.
In Figure 2, we plot the cumulative density func-
tion of this history context. We note that of the
3258 words with non-zero probability in this con-
text, 95% of the probability mass is covered by the
1596 most likely words.
In order to get a better visualization of the proba-
bility distribution, we plotted smoothed probability
estimates versus the training-data frequencies of the
words being generated. Figure 3(a) shows smoothed
estimates that make use of the full context (i.e., in-
clude the head word with) wherever possible, and
Figure 3(b) shows smoothed estimates that do not
use the head word. Note how the plot in Figure 3(b)
appears remarkably similar to the “true” distribu-
</bodyText>
<table confidence="0.9800098">
Back-off PM PMI
level c¯ d n c¯ d n
0 10.268 1.437 7.145 4.413 1.949 2.264
1 558.047 3.643 153.2 60.19 8.454 7.120
2 1169.6 5.067 230.8 21132.1 370.6 57.02
</table>
<tableCaption confidence="0.904420333333333">
Table 2: Average counts and diversities of histories of the PM and PMw parameter classes. c and d are
average history count and diversity, respectively. n = cd is the average number of transitions from a history
context to some future.
</tableCaption>
<figure confidence="0.99097435">
smoothed probability estimate
smoothed probability estimate
0.1
0.01
0.001
0.0001
1e-05
1e-06
1 10 100 1000 10000
word frequency
(a) prob. vs. word freq., back-off level 1
0.1
0.01
0.001
0.0001
1e-05
1e-06
1 10 100 1000 10000 100000
word frequency
(b) prob. vs. word freq., back-off level 2
</figure>
<figureCaption confidence="0.999919">
Figure 3: Probability versus word frequency for head words of NP-A(NN) in the PP construction.
</figureCaption>
<bodyText confidence="0.9999672">
tion of 3(a). 3(b) looks like a slightly “compressed”
version of 3(b) (in the vertical dimension), but the
shape of the two distributions appears to be roughly
the same. This observation will be confirmed and
quantified by the experiments of §5.3
</bodyText>
<sectionHeader confidence="0.998874" genericHeader="method">
4 Entropies
</sectionHeader>
<bodyText confidence="0.999927307692308">
A good measure of the discriminative efficacy of a
parameter is its entropy. Table 3 shows the aver-
age entropy of all distributions for each parameter
class.4 By far the highest average entropy is for the
PMw parameter class.
Having computed the entropy for every distri-
bution in every parameter class, we can actually
plot a “meta-distribution” of entropies for a pa-
rameter class, as shown in Figure 4. As an ex-
ample of one of the data points of Figure 4, con-
sider the history context explored in the previous
section. While it may be one of the most fre-
quent, it also has the highest entropy at 9.141
</bodyText>
<footnote confidence="0.981901">
3The astute reader will further note that the plots in Figure
3 both look bizarrely truncated with respect to low-frequency
words. This is simply due to the fact that all words below a
fixed frequency are generated as the +UNKNOWN+ word.
4The decoder makes use of two additional parameter classes
that jointly estimate the prior probability of a lexicalized non-
terminal; however, these two parameter classes are not part of
the generative model.
</footnote>
<table confidence="0.9997822">
PH 0.2516 PTOPNT 2.517
PsubcatL 0.02342 PTOPw 2.853
PsubcatR 0.2147
PM 1.121
PMw 3.923
</table>
<tableCaption confidence="0.999324">
Table 3: Average entropies for each parameter class.
</tableCaption>
<figure confidence="0.982523461538461">
entropy 10
9
8
7
6
5
4
3
2
1
0
0 50000 100000 150000 200000 250000
rank
</figure>
<figureCaption confidence="0.960227">
Figure 4: Entropy distribution for the PMw parame-
ters.
</figureCaption>
<bodyText confidence="0.99870525">
bits, as shown by Table 4. This value not only
confirms but quantifies the long-held intuition that
PP-attachment requires more than just the local
phrasal context; it is, e.g., precisely why the PP-
specific features of (Collins, 2000) were likely to
be very helpful, as cases such as these are among
the most difficult that the model must discrimi-
nate. In fact, of the top 50 of the highest-entropy
</bodyText>
<table confidence="0.987279833333333">
Back-off min PM avg median min PMw avg median
level max max
0 3.080E-10 4.351 1.128 0.931 4.655E-8 9.141 3.904 3.806
1 4.905E-7 4.254 0.910 0.667 2.531E-6 9.120 4.179 4.224
2 8.410E-4 3.501 0.754 0.520 0.002 8.517 3.182 2.451
Overall 3.080E-10 4.351 1.121 0.917 4.655E-8 9.141 3.922 3.849
</table>
<tableCaption confidence="0.999165">
Table 4: Entropy distribution statistics for PM and PMw.
</tableCaption>
<figureCaption confidence="0.863231">
Figure 5: Total modifier word–generation entropy
broken down by parent-head-modifier triple.
</figureCaption>
<bodyText confidence="0.996885923076923">
distributions from PMw, 25 involve the config-
uration PP --&gt; IN(IN/&lt;prep&gt;) NP-A(NN/...),
where &lt;prep&gt; is some preposition whose tag is IN.
Somewhat disturbingly, these are also some of the
most frequent constructions.
To gauge roughly the importance of these
high-frequency, high-entropy distributions, we per-
formed the following analysis. Assume for the mo-
ment that every word-generation decision is roughly
independent from all others (this is clearly not true,
given head-propagation). We can then compute the
total entropy of word-generation decisions for the
entire training corpus via
</bodyText>
<equation confidence="0.9770125">
�HPMw = f(c) · H(c) (1)
cEPMw
</equation>
<bodyText confidence="0.999992366666666">
where f(c) is the frequency of some history con-
text c and H(c) is that context’s entropy. The to-
tal modifier word-generation entropy for the cor-
pus with the independence assumption is 3,903,224
bits. Of these, the total entropy for contexts of the
form PP —� IN NP-A is 618,640 bits, representing
a sizable 15.9% of the total entropy, and the sin-
gle largest percentage of total entropy of any parent-
head-modifier triple (see Figure 5).
On the opposite end of the entropy spectrum,
there are tens of thousands of PMw parameters
with extremely low entropies, mostly having to do
with extremely low-diversity, low-entropy part-of-
speech tags, such as DT, CC, IN or WRB. Perhaps even
more interesting is the number of distributions with
identical entropies: of the 206,234 distributions,
there are only 92,065 unique entropy values. Dis-
tributions with the same entropy are all candidates
for removal from the model, because most of their
probability mass resides in the back-off distribution.
Many of these distributions are low- or one-count
history contexts, justifying the common practice of
removing transitions whose history count is below a
certain threshold. This practice could be made more
rigorous by relying on distributional similarity. Fi-
nally, we note that the most numerous low-entropy
distributions (that are not trivial) involve generating
right-modifier words of the head child of an SBAR
parent. The model is able to learn these construc-
tions extremely well, as one might expect.
</bodyText>
<sectionHeader confidence="0.9040995" genericHeader="method">
5 Distributional similarity and bilexical
statistics
</sectionHeader>
<bodyText confidence="0.999992568965517">
We now return to the issue of bilexical statis-
tics. As alluded to earlier, Gildea (2001) per-
formed an experiment with his partial reimplemen-
tation of Collins’ Model 1 in which he removed the
maximal-context back-off level from PMw, which
effectively removed all bilexical statistics from his
model. Gildea observed that this change resulted
in only a 0.5% drop in parsing performance. There
were two logical possibilities for this behavior: ei-
ther such statistics were not getting used due to
sparse data problems, or they were not informa-
tive for some reason. The prevailing view of the
NLP community had been that bilexical statistics
were sparse, and Gildea (2001) adopted this view
to explain his results. Subsequently, we duplicated
Gildea’s experiment with a complete emulation of
Collins’ Model 2, and found that when the decoder
requested a smoothed estimate involving a bigram
when testing on held-out data, it only received an
estimate that made use of bilexical statistics a mere
1.49% of the time (Bikel, 2004). The conclusion
was that the minuscule drop in performance from re-
moving bigrams must have been due to the fact that
they were barely able to be used. In other words, it
appeared that bigram coverage was not nearly good
enough for bigrams to have an impact on parsing
performance, seemingly confirming the prevailing
view.
But the 1.49% figure does not tell the whole story.
The parser pursues many incorrect and ultimately
low-scoring theories in its search (in this case, us-
ing probabilistic CKY). So rather than asking how
many times the decoder makes use of bigram statis-
tics on average, a better question is to ask how
many times the decoder can use bigram statistics
while pursuing the top-ranked theory. To answer
this question, we used our parser to constrain-parse
its own output. That is, having trained it on Sec-
tions 02–21, we used it to parse Section 00 of the
Penn Treebank (the canonical development test set)
and then re-parse that section using its own highest-
scoring trees (without lexicalization) as constraints,
so that it only pursued theories consistent with those
trees. As it happens, the number of times the de-
coder was able to use bigram statistics shot up to
28.8% overall, with a rate of 22.4% for NPB con-
stituents.
So, bigram statistics are getting used; in fact, they
are getting used more than 19 times as often when
pursuing the highest-scoring theory as when pursu-
ing any theory on average. And yet there is no dis-
puting the fact that their use has a surprisingly small
effect on parsing performance. The exploratory data
analysis of §3.3 suggests an explanation for this per-
plexing behavior: the distributions that include the
head word versus those that do not are so similar
as to make almost no difference in terms of parse
accuracy.
</bodyText>
<subsectionHeader confidence="0.996818">
5.1 Distributional similarity
</subsectionHeader>
<bodyText confidence="0.951761428571429">
A useful metric for measuring distributional simi-
larity, as explored by (Lee, 1999), is the Jensen-
Shannon divergence (Lin, 1991):
JS (p 11 q) = 1ID lp I I avgp,q ) + D lq I I avgp,q )�
2
(2)
where D is the Kullback-Leibler divergence
(Cover and Thomas, 1991) and where avgp,q =
2 (p(A) + q(A)) for an event A in the event space
1
of at least one of the two distributions. One inter-
pretation for the Jensen-Shannon divergence due to
Slonim et al. (2002) is that it is related to the log-
likelihood that “the two sample distributions orig-
inate by the most likely common source,” relating
the quantity to the “two-sample problem”.
In our case, we have p = p(y  |x1, x2) and q =
p(y  |x1), where y is a possible future and x1, x2 are
elements of a history context, with q representing
a back-off distribution using less context. There-
fore, whereas the standard JS formulation is agnos-
</bodyText>
<table confidence="0.98940025">
min max avg. median
JS0,1 2.729E-7 2.168 0.1148 0.09672
JS1,2 0.001318 1.962 0.6929 0.6986
JS0,2 0.001182 1.180 0.3774 0.3863
</table>
<tableCaption confidence="0.9778835">
Table 5: Jensen-Shannon statistics for back-off pa-
rameters in PMw.
</tableCaption>
<bodyText confidence="0.996096333333333">
tic with respect to its two distributions, and averages
them in part to ensure that the quantity is defined
over the entire space, we have the prior knowledge
that one history context is a superset of the other,
that (x1) is defined wherever (x1, x2) is. In this case,
then, we have a simpler, “one-sided” definition for
the Jensen-Shannon divergence, but generalized to
the multiple distributions that include an extra his-
tory component:
</bodyText>
<equation confidence="0.999699">
JS(p 11 q) =
Z p(x2) - D (p(y  |x1, x2) 11 p(y  |x1))
x2
= Ex2D(p(y |x1, x2) 11 p(y  |x1)) (3)
</equation>
<bodyText confidence="0.999822285714286">
An interpretation in our case is that this is the ex-
pected number of bits x2 gives you when trying to
predict y.5 If we allow x2 to represent an arbitrary
amount of context, then the Jensen-Shannon diver-
gence JS b,a = JS(pb  ||pa) can be computed for
any two back-off levels, where a, b are back-off lev-
els s.t. b &lt; a (meaning pb is a distribution using
more context than pa). The actual value in bits of
the Jensen-Shannon divergence between two distri-
butions should be considered in relation to the num-
ber of bits of entropy of the more detailed distribu-
tion; that is, JSb,a should be considered relative to
H(pb). Having explored entropy in §4, we will now
look at some summary statistics for JS divergence.
</bodyText>
<subsectionHeader confidence="0.925407">
5.2 Results
</subsectionHeader>
<bodyText confidence="0.999893272727273">
We computed the quantity in Equation 3 for every
parameter in PMw that used maximal context (con-
tained a head word) and its associated parameter
that did not contain the head word. The results are
listed in Table 5. Note that, for this parameter class
with a median entropy of 3.8 bits, we have a median
JS divergence of only 0.097 bits. The distributions
are so similar that the 28.8% of the time that the de-
coder uses an estimate based on a bigram, it might
as well be using one that does not include the head
word.
</bodyText>
<footnote confidence="0.74011575">
5Or, following from Slonim et al.’s interpretation, this quan-
tity is the (negative of the) log-likelihood that all distributions
that include an x2 component come from a “common source”
that does not include this component.
</footnote>
<table confidence="0.999328454545455">
&lt; 40 words
Model §00 §23
LR LP LR LP
m3 n/a n/a 88.6 88.7
m2-emu 89.9 90.0 88.8 88.9
reduced 90.0 90.2 88.7 88.9
Model all sentences
§00 §23
m3 n/a n/a 88.0 88.3
m2-emu 88.8 89.0 88.2 88.3
reduced 89.0 89.0 88.0 88.2
</table>
<tableCaption confidence="0.9671">
Table 6: Parsing results on Sections 00 and 23 with
</tableCaption>
<bodyText confidence="0.906876333333333">
Collins’ Model 3, our emulation of Collins’ Model
2 and the reduced version at a threshold of 0.06. LR
= labeled recall, LP = labeled precision.6
</bodyText>
<sectionHeader confidence="0.911031" genericHeader="method">
6 Distributional Similarity and Parameter
Selection
</sectionHeader>
<bodyText confidence="0.999536411764706">
The analysis of the previous two sections provides
a window onto what types of parameters the pars-
ing model is learning most and least well, and onto
what parameters carry more and less useful infor-
mation. Having such a window holds the promise
of discovering new parameter types or features that
would lead to greater parsing accuracy; such is the
scientific, or at least, the forward-minded research
perspective.
From a much more purely engineering perspec-
tive, one can also use the analysis of the previous
two sections to identify individual parameters that
carry little to no useful information and simply re-
move them from the model. Specifically, if pb is
a particular distribution and pb+1 is its correspond-
ing back-off distribution, then one can remove all
parameters pb such that
</bodyText>
<equation confidence="0.95168">
JS (pb||pb+1)
H(pb)
</equation>
<bodyText confidence="0.999792416666666">
where 0 &lt; t &lt; 1 is some threshold. Table 6 shows
the results of this experiment using a threshold of
0.06. To our knowledge, this is the first example
of detailed parameter selection in the context of a
generative lexicalized statistical parsing model. The
consequence is a significantly smaller model that
performs with no loss of accuracy compared to the
full model.6
Further insight is gained by looking at the per-
centage of parameters removed from each parame-
ter class. The results of (Bikel, 2004) suggested that
the power of Collins-style parsing models did not
</bodyText>
<footnote confidence="0.8854935">
6None of the differences between the Model 2–emulation
results and the reduced model results is statistically significant.
</footnote>
<table confidence="0.999102666666667">
PH 13.5% PTOPw 0.023%
PsubcatL 0.67% PM 10.1%
PsubcatR 1.8% PMw 29.4%
</table>
<tableCaption confidence="0.7590345">
Table 7: Percentage of parameters removed from
each parameter class for the 0.06-reduced model.
</tableCaption>
<bodyText confidence="0.9999553">
lie primarily with the use of bilexical dependencies
as was once thought, but in lexico-structural depen-
dencies, that is, predicting syntactic structures con-
ditioning on head words. The percentages of Table
7 provide even more concrete evidence of this as-
sertion, for whereas nearly a third of the PMw pa-
rameters were removed, a much smaller fraction of
parameters were removed from the PsubcatL, PsubcatR
and PM classes that generate structure conditioning
on head words.
</bodyText>
<sectionHeader confidence="0.999687" genericHeader="method">
7 Discussion
</sectionHeader>
<bodyText confidence="0.999933115384615">
Examining the lower-entropy PMw distributions re-
vealed that, in many cases, the model was not so
much learning how to disambiguate a given syn-
tactic/lexical choice, but simply not having much
to learn. For example, once a partially-lexicalized
nonterminal has been generated whose tag is fairly
specialized, such as IN, then the model has “painted
itself into a lexical corner”, as it were (the extreme
example is TO, a tag that can only be assigned to the
word to). This is an example of the “label bias”
problem, which has been the subject of recent dis-
cussion (Lafferty et al., 2001; Klein and Manning,
2002). Of course, just because there is “label bias”
does not necessarily mean there is a problem. If
the decoder pursues a theory to a nonterminal/part-
of-speech tag preterminal that has an extremely low
entropy distribution for possible head words, then
there is certainly a chance that it will get “stuck” in a
potentially bad theory. This is of particular concern
when a head word—which the top-down model gen-
erates at its highest point in the tree—influences an
attachment decision. However, inspecting the low-
entropy word-generation histories of PMw revealed
that almost all such cases are when the model is
generating a preterminal, and are thus of little to no
consequence vis-a-vis syntactic disambiguation.
</bodyText>
<sectionHeader confidence="0.974551" genericHeader="conclusions">
8 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.993486428571429">
With so many parameters, a lexicalized statistical
parsing model seems like an intractable behemoth.
However, as statisticians have long known, an ex-
cellent angle of attack for a mass of unruly data
is exploratory data analysis. This paper presents
some of the first data visualizations of parameters
&lt; t,
in a parsing model, and follows up with a numerical
analysis of properties of those distributions. In the
course of this analysis, we have focused in on the
question of bilexical dependencies. By constrain-
parsing the parser’s own output, and by hypothe-
sizing and testing for distributional similarity, we
have presented evidence that finally explains that
(a) bilexical statistics are actually getting used with
great frequency in the parse theories that will ulti-
mately have the highest score, but (b) the distribu-
tions involving bilexical statistics are so similar to
their back-off counterparts as to make them nearly
indistinguishable insofar as making different parse
decisions. Finally, our analysis has provided for the
first time an effective way to do parameter selec-
tion with a generative lexicalized statistical parsing
model.
Of course, there is still much more analysis, hy-
pothesizing, testing and extrapolation to be done. A
thorough study of the highest-entropy distributions
should reveal new ways in which to use grammar
transforms or develop features to reduce the entropy
and increase parse accuracy. A closer look at the
low-entropy distributions may reveal additional re-
ductions in the size of the model, and, perhaps, a
way to incorporate hard constraints without disturb-
ing the more ambiguous parts of the model more
suited to machine learning than human engineering.
</bodyText>
<sectionHeader confidence="0.99778" genericHeader="acknowledgments">
9 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999917125">
Thanks to Mitch Marcus, David Chiang and Ju-
lia Hockenmaier for their helpful comments on this
work. I would also like to thank Bob Moore for
asking some insightful questions that helped prompt
this line of research. Thanks also to Fernando
Pereira, with whom I had invaluable discussions
about distributional similarity. This work was sup-
ported in part by DARPA grant N66001-00-1-9815.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999923381578947">
Daniel M. Bikel. 2002. Design of a multi-lingual,
parallel-processing statistical parsing engine. In Pro-
ceedings ofHLT2002, San Diego, CA.
Daniel M. Bikel. 2004. Intricacies of Collins’ parsing
model. Computational Linguistics. To appear.
E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-
ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,
J. Klavens, M. Liberman, M. Marcus, S. Roukos,
B. Santorini, and T. Strzalkowski. 1991. A procedure
for quantitatively comparing the syntactic coverage of
English grammars. In Speech and Natural Language
Workshop, pages 306–311, Pacific Grove, California.
Morgan Kaufmann Publishers.
Ezra Black, Frederick Jelinek, John Lafferty, David
Magerman, Robert Mercer, and Salim Roukos.
1992a. Towards history-based grammars: Using
richer models for probabilistic parsing. In Proceed-
ings of the 5th DARPA Speech and Natural Language
Workshop, Harriman, New York.
Ezra Black, John Lafferty, and Salim Roukos. 1992b.
Development and evaluation of a broad-coverage
probabilistic grammar of english-language computer
manuals. In Proceedings of the 30th ACL, pages 185–
192.
Eugene Charniak. 2000. A maximum entropy–inspired
parser. In Proceedings of the 1st NAACL, pages 132–
139, Seattle, Washington, April 29 to May 4.
Michael John Collins. 1999. Head-Driven Statistical
Models for Natural Language Parsing. Ph.D. thesis,
University of Pennsylvania.
Michael Collins. 2000. Discriminative reranking for
natural language parsing. In International Conference
on Machine Learning.
Thomas Cover and Joy A. Thomas. 1991. Elements of
Information Theory. John Wiley &amp; Sons, Inc., New
York.
Jason Eisner. 1996. Three new probabilistic models for
dependency parsing: An exploration. In Proceed-
ings of the 16th International Conference on Com-
putational Linguistics (COLING-96), pages 340–345,
Copenhagen, August.
Jason Eisner. 2000. Bilexical grammars and their cubic-
time parsing algorithms. In Harry Bunt and An-
ton Nijholt, editors, Advances in Probabilistic and
Other Parsing Technologies, pages 29–62. Kluwer
Academic Publishers, October.
Daniel Gildea. 2001. Corpus variation and parser per-
formance. In Proceedings of the 2001 Conference on
Empirical Methods in Natural Language Processing,
Pittsburgh, Pennsylvania.
Dan Klein and Christopher D. Manning. 2002. Condi-
tional structure versus conditional estimation in NLP
models. In Proceedings of the 2002 Conference on
Empirical Methods for Natural Language Processing.
John Lafferty, Fernando Pereira, and Andrew McCal-
lum. 2001. Conditional random fields: Probabilistic
models for segmenting and labeling sequence data. In
ICML.
Lillian Lee. 1999. Measures of distributional similarity.
In Proceedings of the 37th ACL, pages 25–32.
Jianhua Lin. 1991. Divergence measures based on the
Shannon entropy. IEEE Transactions on Information
Theory, 37(1):145–151.
David Magerman. 1994. Natural Language Parsing as
Statistical Pattern Recognition. Ph.D. thesis, Univer-
sity of Pennsylvania, Philadelphia, Pennsylvania.
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1993. Building a large annotated cor-
pus of English: The Penn Treebank. Computational
Linguistics, 19:313–330.
Noam Slonim, Nir Friedman, and Naftali Tishby.
2002. Unsupervised document classification using
sequential information maximization. Technical Re-
port 2002–19, Leibniz Center, The School of Com-
puter Science and Engineering, Hebrew University,
Jerusalem, Israel.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951139">
<title confidence="0.999853">A Distributional Analysis of a Lexicalized Statistical Parsing Model</title>
<author confidence="0.999884">M Daniel</author>
<affiliation confidence="0.999508">Department of Computer and Information University of</affiliation>
<address confidence="0.9793795">3330 Walnut Philadelphia, PA</address>
<email confidence="0.999631">dbikel@cis.upenn.edu</email>
<abstract confidence="0.99963365">This paper presents some of the first data visualizations and analysis of distributions for a lexicalized statistical parsing model, in order to better understand their nature. In the course of this analysis, we have paid particular attention to parameters that include bilexical dependencies. The prevailing view has been that such statistics are very informative but greatly from sparse data problems. By using a parser to constrain-parse its own output, and by hypothesizing and testing for distributional similarity we have evidence that finally explains that (a) bilexical statistics are actually getting used quite often but that (b) the distributions are so similar to those that do not include head words as to be nearly indistinguishable insofar as making parse decisions. Finally, our analysis provided for the first time an way to do parameter selection for a generative lexicalized statistical parsing model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Design of a multi-lingual, parallel-processing statistical parsing engine.</title>
<date>2002</date>
<booktitle>In Proceedings ofHLT2002,</booktitle>
<location>San Diego, CA.</location>
<contexts>
<context position="6899" citStr="Bikel, 2002" startWordPosition="1099" endWordPosition="1100">ous, pointing the way to new features of that type. Understanding what is learned least well defines the space in which to look for those new features. 3 Frequencies 3.1 Definitions and notation In this paper we will refer to any estimated distribution as a parameter that has been instantiated from a parameter class. For example, in an ngram language model, p(wi |wi−1) is a parameter class, whereas the estimated distribution ˆp(· |the) is a particular parameter from this class, consisting of estimates of every word that can follow the word “the”. For this work, we used the model described in (Bikel, 2002; Bikel, 2004). Our emulation of Collins’ Model 2 (hereafter referred to simply as “the model”) has eleven parameter classes, each of which employs up to three back-off levels, where back-off level 0 is just the “un-backed-off” maximal context history.1 In other words, a smoothed probability estimate is the interpolation of up to three different unsmoothed estimates. The notation and description for each of these parameter classes is shown in Table 1. 3.2 Basic frequencies Before looking at the number of parameters in the model, it is important to bear in mind the amount of data on which the m</context>
</contexts>
<marker>Bikel, 2002</marker>
<rawString>Daniel M. Bikel. 2002. Design of a multi-lingual, parallel-processing statistical parsing engine. In Proceedings ofHLT2002, San Diego, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel M Bikel</author>
</authors>
<title>Intricacies of Collins’ parsing model. Computational Linguistics.</title>
<date>2004</date>
<note>To appear.</note>
<contexts>
<context position="4038" citStr="Bikel, 2004" startWordPosition="624" endWordPosition="625">ver, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins’ Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy. The prevailing assumption was then that such bilexical statistics were not useful for making syntactic decisions, although it was not entirely clear why. Subsequently, we replicated Gildea’s experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to confirm the original result. However, the present work will show that such statistics do get frequently used for the highest-probability parses, but that when a Collinsstyle model generates modifier words, the bilexical parameters are so similar to their back-off distributions as to provide almost no extra predictive information. 2 Motivation A parsing model coupled with a decoder (an algorithm to search the space of possible trees for a given terminal sequence) is largely an engineering effort. In the end, the performance of the parser with respect to its evaluation criteria—ty</context>
<context position="6913" citStr="Bikel, 2004" startWordPosition="1101" endWordPosition="1102"> the way to new features of that type. Understanding what is learned least well defines the space in which to look for those new features. 3 Frequencies 3.1 Definitions and notation In this paper we will refer to any estimated distribution as a parameter that has been instantiated from a parameter class. For example, in an ngram language model, p(wi |wi−1) is a parameter class, whereas the estimated distribution ˆp(· |the) is a particular parameter from this class, consisting of estimates of every word that can follow the word “the”. For this work, we used the model described in (Bikel, 2002; Bikel, 2004). Our emulation of Collins’ Model 2 (hereafter referred to simply as “the model”) has eleven parameter classes, each of which employs up to three back-off levels, where back-off level 0 is just the “un-backed-off” maximal context history.1 In other words, a smoothed probability estimate is the interpolation of up to three different unsmoothed estimates. The notation and description for each of these parameter classes is shown in Table 1. 3.2 Basic frequencies Before looking at the number of parameters in the model, it is important to bear in mind the amount of data on which the model is traine</context>
<context position="19696" citStr="Bikel, 2004" startWordPosition="3204" endWordPosition="3205">ssibilities for this behavior: either such statistics were not getting used due to sparse data problems, or they were not informative for some reason. The prevailing view of the NLP community had been that bilexical statistics were sparse, and Gildea (2001) adopted this view to explain his results. Subsequently, we duplicated Gildea’s experiment with a complete emulation of Collins’ Model 2, and found that when the decoder requested a smoothed estimate involving a bigram when testing on held-out data, it only received an estimate that made use of bilexical statistics a mere 1.49% of the time (Bikel, 2004). The conclusion was that the minuscule drop in performance from removing bigrams must have been due to the fact that they were barely able to be used. In other words, it appeared that bigram coverage was not nearly good enough for bigrams to have an impact on parsing performance, seemingly confirming the prevailing view. But the 1.49% figure does not tell the whole story. The parser pursues many incorrect and ultimately low-scoring theories in its search (in this case, using probabilistic CKY). So rather than asking how many times the decoder makes use of bigram statistics on average, a bette</context>
<context position="26341" citStr="Bikel, 2004" startWordPosition="4373" endWordPosition="4374"> its corresponding back-off distribution, then one can remove all parameters pb such that JS (pb||pb+1) H(pb) where 0 &lt; t &lt; 1 is some threshold. Table 6 shows the results of this experiment using a threshold of 0.06. To our knowledge, this is the first example of detailed parameter selection in the context of a generative lexicalized statistical parsing model. The consequence is a significantly smaller model that performs with no loss of accuracy compared to the full model.6 Further insight is gained by looking at the percentage of parameters removed from each parameter class. The results of (Bikel, 2004) suggested that the power of Collins-style parsing models did not 6None of the differences between the Model 2–emulation results and the reduced model results is statistically significant. PH 13.5% PTOPw 0.023% PsubcatL 0.67% PM 10.1% PsubcatR 1.8% PMw 29.4% Table 7: Percentage of parameters removed from each parameter class for the 0.06-reduced model. lie primarily with the use of bilexical dependencies as was once thought, but in lexico-structural dependencies, that is, predicting syntactic structures conditioning on head words. The percentages of Table 7 provide even more concrete evidence </context>
</contexts>
<marker>Bikel, 2004</marker>
<rawString>Daniel M. Bikel. 2004. Intricacies of Collins’ parsing model. Computational Linguistics. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Black</author>
<author>S Abney</author>
<author>D Flickenger</author>
<author>C Gdaniec</author>
<author>R Grishman</author>
<author>P Harrison</author>
<author>D Hindle</author>
<author>R Ingria</author>
<author>F Jelinek</author>
<author>J Klavens</author>
<author>M Liberman</author>
<author>M Marcus</author>
<author>S Roukos</author>
<author>B Santorini</author>
<author>T Strzalkowski</author>
</authors>
<title>A procedure for quantitatively comparing the syntactic coverage of English grammars.</title>
<date>1991</date>
<booktitle>In Speech and Natural Language Workshop,</booktitle>
<pages>306--311</pages>
<publisher>Morgan Kaufmann Publishers.</publisher>
<location>Pacific Grove, California.</location>
<contexts>
<context position="3132" citStr="Black et al., 1991" startWordPosition="483" endWordPosition="486"> generated except for the head word of the entire sentence; also, these two parameter classes have by far the most parameters and suffer the most from sparse data problems. In spite of using a Collins-style model as the basis for analysis, throughout this paper, we will attempt to present information that is widely applicable because it pertains to properties of the widely-used Treebank (Marcus et al., 1993) and lexicalized parsing models in general. This work also sheds light on the much-discussed “bilexical dependencies” of statistical parsing models. Beginning with the seminal work at IBM (Black et al., 1991; Black et al., 1992b; Black et al., 1992a), and continuing with such lexicalist approaches as (Eisner, 1996), these features have been lauded for their ability to approximate a word’s semantics as a means to override syntactic preferences with semantic ones (Collins, 1999; Eisner, 2000). However, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins’ Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy. The prevailing assumption was then that such bi</context>
</contexts>
<marker>Black, Abney, Flickenger, Gdaniec, Grishman, Harrison, Hindle, Ingria, Jelinek, Klavens, Liberman, Marcus, Roukos, Santorini, Strzalkowski, 1991</marker>
<rawString>E. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Klavens, M. Liberman, M. Marcus, S. Roukos, B. Santorini, and T. Strzalkowski. 1991. A procedure for quantitatively comparing the syntactic coverage of English grammars. In Speech and Natural Language Workshop, pages 306–311, Pacific Grove, California. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>Frederick Jelinek</author>
<author>John Lafferty</author>
<author>David Magerman</author>
<author>Robert Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Towards history-based grammars: Using richer models for probabilistic parsing.</title>
<date>1992</date>
<booktitle>In Proceedings of the 5th DARPA Speech and Natural Language Workshop,</booktitle>
<location>Harriman, New York.</location>
<contexts>
<context position="1298" citStr="Black et al. (1992" startWordPosition="192" endWordPosition="195">ts own output, and by hypothesizing and testing for distributional similarity with back-off distributions, we have evidence that finally explains that (a) bilexical statistics are actually getting used quite often but that (b) the distributions are so similar to those that do not include head words as to be nearly indistinguishable insofar as making parse decisions. Finally, our analysis has provided for the first time an effective way to do parameter selection for a generative lexicalized statistical parsing model. 1 Introduction Lexicalized statistical parsing models, such as those built by Black et al. (1992a), Magerman (1994), Collins (1999) and Charniak (2000), have been enormously successful, but they also have an enormous complexity. Their success has often been attributed to their sensitivity to individual lexical items, and it is precisely this incorporation of lexical items into features or parameter schemata that gives rise to their complexity. In order to help determine which features are helpful, the somewhat crude-buteffective method has been to compare a model’s overall parsing performance with and without a feature. Often, it has seemed that features that are derived from linguistic </context>
<context position="3152" citStr="Black et al., 1992" startWordPosition="487" endWordPosition="490">r the head word of the entire sentence; also, these two parameter classes have by far the most parameters and suffer the most from sparse data problems. In spite of using a Collins-style model as the basis for analysis, throughout this paper, we will attempt to present information that is widely applicable because it pertains to properties of the widely-used Treebank (Marcus et al., 1993) and lexicalized parsing models in general. This work also sheds light on the much-discussed “bilexical dependencies” of statistical parsing models. Beginning with the seminal work at IBM (Black et al., 1991; Black et al., 1992b; Black et al., 1992a), and continuing with such lexicalist approaches as (Eisner, 1996), these features have been lauded for their ability to approximate a word’s semantics as a means to override syntactic preferences with semantic ones (Collins, 1999; Eisner, 2000). However, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins’ Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy. The prevailing assumption was then that such bilexical statistics w</context>
</contexts>
<marker>Black, Jelinek, Lafferty, Magerman, Mercer, Roukos, 1992</marker>
<rawString>Ezra Black, Frederick Jelinek, John Lafferty, David Magerman, Robert Mercer, and Salim Roukos. 1992a. Towards history-based grammars: Using richer models for probabilistic parsing. In Proceedings of the 5th DARPA Speech and Natural Language Workshop, Harriman, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
<author>John Lafferty</author>
<author>Salim Roukos</author>
</authors>
<title>Development and evaluation of a broad-coverage probabilistic grammar of english-language computer manuals.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th ACL,</booktitle>
<pages>185--192</pages>
<contexts>
<context position="1298" citStr="Black et al. (1992" startWordPosition="192" endWordPosition="195">ts own output, and by hypothesizing and testing for distributional similarity with back-off distributions, we have evidence that finally explains that (a) bilexical statistics are actually getting used quite often but that (b) the distributions are so similar to those that do not include head words as to be nearly indistinguishable insofar as making parse decisions. Finally, our analysis has provided for the first time an effective way to do parameter selection for a generative lexicalized statistical parsing model. 1 Introduction Lexicalized statistical parsing models, such as those built by Black et al. (1992a), Magerman (1994), Collins (1999) and Charniak (2000), have been enormously successful, but they also have an enormous complexity. Their success has often been attributed to their sensitivity to individual lexical items, and it is precisely this incorporation of lexical items into features or parameter schemata that gives rise to their complexity. In order to help determine which features are helpful, the somewhat crude-buteffective method has been to compare a model’s overall parsing performance with and without a feature. Often, it has seemed that features that are derived from linguistic </context>
<context position="3152" citStr="Black et al., 1992" startWordPosition="487" endWordPosition="490">r the head word of the entire sentence; also, these two parameter classes have by far the most parameters and suffer the most from sparse data problems. In spite of using a Collins-style model as the basis for analysis, throughout this paper, we will attempt to present information that is widely applicable because it pertains to properties of the widely-used Treebank (Marcus et al., 1993) and lexicalized parsing models in general. This work also sheds light on the much-discussed “bilexical dependencies” of statistical parsing models. Beginning with the seminal work at IBM (Black et al., 1991; Black et al., 1992b; Black et al., 1992a), and continuing with such lexicalist approaches as (Eisner, 1996), these features have been lauded for their ability to approximate a word’s semantics as a means to override syntactic preferences with semantic ones (Collins, 1999; Eisner, 2000). However, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins’ Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy. The prevailing assumption was then that such bilexical statistics w</context>
</contexts>
<marker>Black, Lafferty, Roukos, 1992</marker>
<rawString>Ezra Black, John Lafferty, and Salim Roukos. 1992b. Development and evaluation of a broad-coverage probabilistic grammar of english-language computer manuals. In Proceedings of the 30th ACL, pages 185– 192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum entropy–inspired parser.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st NAACL,</booktitle>
<volume>29</volume>
<pages>132--139</pages>
<location>Seattle, Washington,</location>
<contexts>
<context position="1353" citStr="Charniak (2000)" startWordPosition="201" endWordPosition="202">butional similarity with back-off distributions, we have evidence that finally explains that (a) bilexical statistics are actually getting used quite often but that (b) the distributions are so similar to those that do not include head words as to be nearly indistinguishable insofar as making parse decisions. Finally, our analysis has provided for the first time an effective way to do parameter selection for a generative lexicalized statistical parsing model. 1 Introduction Lexicalized statistical parsing models, such as those built by Black et al. (1992a), Magerman (1994), Collins (1999) and Charniak (2000), have been enormously successful, but they also have an enormous complexity. Their success has often been attributed to their sensitivity to individual lexical items, and it is precisely this incorporation of lexical items into features or parameter schemata that gives rise to their complexity. In order to help determine which features are helpful, the somewhat crude-buteffective method has been to compare a model’s overall parsing performance with and without a feature. Often, it has seemed that features that are derived from linguistic principles result in higherperforming models (cf. (Coll</context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum entropy–inspired parser. In Proceedings of the 1st NAACL, pages 132– 139, Seattle, Washington, April 29 to May 4.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael John Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="1333" citStr="Collins (1999)" startWordPosition="198" endWordPosition="199"> testing for distributional similarity with back-off distributions, we have evidence that finally explains that (a) bilexical statistics are actually getting used quite often but that (b) the distributions are so similar to those that do not include head words as to be nearly indistinguishable insofar as making parse decisions. Finally, our analysis has provided for the first time an effective way to do parameter selection for a generative lexicalized statistical parsing model. 1 Introduction Lexicalized statistical parsing models, such as those built by Black et al. (1992a), Magerman (1994), Collins (1999) and Charniak (2000), have been enormously successful, but they also have an enormous complexity. Their success has often been attributed to their sensitivity to individual lexical items, and it is precisely this incorporation of lexical items into features or parameter schemata that gives rise to their complexity. In order to help determine which features are helpful, the somewhat crude-buteffective method has been to compare a model’s overall parsing performance with and without a feature. Often, it has seemed that features that are derived from linguistic principles result in higherperformi</context>
<context position="3405" citStr="Collins, 1999" startWordPosition="528" endWordPosition="529"> to present information that is widely applicable because it pertains to properties of the widely-used Treebank (Marcus et al., 1993) and lexicalized parsing models in general. This work also sheds light on the much-discussed “bilexical dependencies” of statistical parsing models. Beginning with the seminal work at IBM (Black et al., 1991; Black et al., 1992b; Black et al., 1992a), and continuing with such lexicalist approaches as (Eisner, 1996), these features have been lauded for their ability to approximate a word’s semantics as a means to override syntactic preferences with semantic ones (Collins, 1999; Eisner, 2000). However, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins’ Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy. The prevailing assumption was then that such bilexical statistics were not useful for making syntactic decisions, although it was not entirely clear why. Subsequently, we replicated Gildea’s experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting u</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael John Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Discriminative reranking for natural language parsing.</title>
<date>2000</date>
<booktitle>In International Conference on Machine Learning.</booktitle>
<contexts>
<context position="15903" citStr="Collins, 2000" startWordPosition="2600" endWordPosition="2601">ability of a lexicalized nonterminal; however, these two parameter classes are not part of the generative model. PH 0.2516 PTOPNT 2.517 PsubcatL 0.02342 PTOPw 2.853 PsubcatR 0.2147 PM 1.121 PMw 3.923 Table 3: Average entropies for each parameter class. entropy 10 9 8 7 6 5 4 3 2 1 0 0 50000 100000 150000 200000 250000 rank Figure 4: Entropy distribution for the PMw parameters. bits, as shown by Table 4. This value not only confirms but quantifies the long-held intuition that PP-attachment requires more than just the local phrasal context; it is, e.g., precisely why the PPspecific features of (Collins, 2000) were likely to be very helpful, as cases such as these are among the most difficult that the model must discriminate. In fact, of the top 50 of the highest-entropy Back-off min PM avg median min PMw avg median level max max 0 3.080E-10 4.351 1.128 0.931 4.655E-8 9.141 3.904 3.806 1 4.905E-7 4.254 0.910 0.667 2.531E-6 9.120 4.179 4.224 2 8.410E-4 3.501 0.754 0.520 0.002 8.517 3.182 2.451 Overall 3.080E-10 4.351 1.121 0.917 4.655E-8 9.141 3.922 3.849 Table 4: Entropy distribution statistics for PM and PMw. Figure 5: Total modifier word–generation entropy broken down by parent-head-modifier trip</context>
</contexts>
<marker>Collins, 2000</marker>
<rawString>Michael Collins. 2000. Discriminative reranking for natural language parsing. In International Conference on Machine Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas Cover</author>
<author>Joy A Thomas</author>
</authors>
<title>Elements of Information Theory.</title>
<date>1991</date>
<publisher>John Wiley &amp; Sons, Inc.,</publisher>
<location>New York.</location>
<contexts>
<context position="21756" citStr="Cover and Thomas, 1991" startWordPosition="3560" endWordPosition="3563">e is no disputing the fact that their use has a surprisingly small effect on parsing performance. The exploratory data analysis of §3.3 suggests an explanation for this perplexing behavior: the distributions that include the head word versus those that do not are so similar as to make almost no difference in terms of parse accuracy. 5.1 Distributional similarity A useful metric for measuring distributional similarity, as explored by (Lee, 1999), is the JensenShannon divergence (Lin, 1991): JS (p 11 q) = 1ID lp I I avgp,q ) + D lq I I avgp,q )� 2 (2) where D is the Kullback-Leibler divergence (Cover and Thomas, 1991) and where avgp,q = 2 (p(A) + q(A)) for an event A in the event space 1 of at least one of the two distributions. One interpretation for the Jensen-Shannon divergence due to Slonim et al. (2002) is that it is related to the loglikelihood that “the two sample distributions originate by the most likely common source,” relating the quantity to the “two-sample problem”. In our case, we have p = p(y |x1, x2) and q = p(y |x1), where y is a possible future and x1, x2 are elements of a history context, with q representing a back-off distribution using less context. Therefore, whereas the standard JS f</context>
</contexts>
<marker>Cover, Thomas, 1991</marker>
<rawString>Thomas Cover and Joy A. Thomas. 1991. Elements of Information Theory. John Wiley &amp; Sons, Inc., New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96),</booktitle>
<pages>340--345</pages>
<location>Copenhagen,</location>
<contexts>
<context position="3241" citStr="Eisner, 1996" startWordPosition="502" endWordPosition="503"> parameters and suffer the most from sparse data problems. In spite of using a Collins-style model as the basis for analysis, throughout this paper, we will attempt to present information that is widely applicable because it pertains to properties of the widely-used Treebank (Marcus et al., 1993) and lexicalized parsing models in general. This work also sheds light on the much-discussed “bilexical dependencies” of statistical parsing models. Beginning with the seminal work at IBM (Black et al., 1991; Black et al., 1992b; Black et al., 1992a), and continuing with such lexicalist approaches as (Eisner, 1996), these features have been lauded for their ability to approximate a word’s semantics as a means to override syntactic preferences with semantic ones (Collins, 1999; Eisner, 2000). However, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins’ Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy. The prevailing assumption was then that such bilexical statistics were not useful for making syntactic decisions, although it was not entirely clear why. Su</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>Jason Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proceedings of the 16th International Conference on Computational Linguistics (COLING-96), pages 340–345, Copenhagen, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Bilexical grammars and their cubictime parsing algorithms.</title>
<date>2000</date>
<booktitle>Advances in Probabilistic and Other Parsing Technologies,</booktitle>
<pages>29--62</pages>
<editor>In Harry Bunt and Anton Nijholt, editors,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="3420" citStr="Eisner, 2000" startWordPosition="530" endWordPosition="531">ormation that is widely applicable because it pertains to properties of the widely-used Treebank (Marcus et al., 1993) and lexicalized parsing models in general. This work also sheds light on the much-discussed “bilexical dependencies” of statistical parsing models. Beginning with the seminal work at IBM (Black et al., 1991; Black et al., 1992b; Black et al., 1992a), and continuing with such lexicalist approaches as (Eisner, 1996), these features have been lauded for their ability to approximate a word’s semantics as a means to override syntactic preferences with semantic ones (Collins, 1999; Eisner, 2000). However, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins’ Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy. The prevailing assumption was then that such bilexical statistics were not useful for making syntactic decisions, although it was not entirely clear why. Subsequently, we replicated Gildea’s experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during deco</context>
</contexts>
<marker>Eisner, 2000</marker>
<rawString>Jason Eisner. 2000. Bilexical grammars and their cubictime parsing algorithms. In Harry Bunt and Anton Nijholt, editors, Advances in Probabilistic and Other Parsing Technologies, pages 29–62. Kluwer Academic Publishers, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Corpus variation and parser performance.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<location>Pittsburgh, Pennsylvania.</location>
<contexts>
<context position="3456" citStr="Gildea (2001)" startWordPosition="537" endWordPosition="538">ecause it pertains to properties of the widely-used Treebank (Marcus et al., 1993) and lexicalized parsing models in general. This work also sheds light on the much-discussed “bilexical dependencies” of statistical parsing models. Beginning with the seminal work at IBM (Black et al., 1991; Black et al., 1992b; Black et al., 1992a), and continuing with such lexicalist approaches as (Eisner, 1996), these features have been lauded for their ability to approximate a word’s semantics as a means to override syntactic preferences with semantic ones (Collins, 1999; Eisner, 2000). However, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins’ Model 1, removing all parameters that involved dependencies between a modifier word and its head resulted in a surprisingly small decrease in overall parse accuracy. The prevailing assumption was then that such bilexical statistics were not useful for making syntactic decisions, although it was not entirely clear why. Subsequently, we replicated Gildea’s experiment with a complete emulation of Model 2 and presented additional evidence that bilexical statistics were barely getting used during decoding (Bikel, 2004), appearing to con</context>
<context position="18762" citStr="Gildea (2001)" startWordPosition="3054" endWordPosition="3055">w- or one-count history contexts, justifying the common practice of removing transitions whose history count is below a certain threshold. This practice could be made more rigorous by relying on distributional similarity. Finally, we note that the most numerous low-entropy distributions (that are not trivial) involve generating right-modifier words of the head child of an SBAR parent. The model is able to learn these constructions extremely well, as one might expect. 5 Distributional similarity and bilexical statistics We now return to the issue of bilexical statistics. As alluded to earlier, Gildea (2001) performed an experiment with his partial reimplementation of Collins’ Model 1 in which he removed the maximal-context back-off level from PMw, which effectively removed all bilexical statistics from his model. Gildea observed that this change resulted in only a 0.5% drop in parsing performance. There were two logical possibilities for this behavior: either such statistics were not getting used due to sparse data problems, or they were not informative for some reason. The prevailing view of the NLP community had been that bilexical statistics were sparse, and Gildea (2001) adopted this view to</context>
</contexts>
<marker>Gildea, 2001</marker>
<rawString>Daniel Gildea. 2001. Corpus variation and parser performance. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, Pittsburgh, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Conditional structure versus conditional estimation in NLP models.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods for Natural Language Processing.</booktitle>
<contexts>
<context position="27793" citStr="Klein and Manning, 2002" startWordPosition="4606" endWordPosition="4609">7 Discussion Examining the lower-entropy PMw distributions revealed that, in many cases, the model was not so much learning how to disambiguate a given syntactic/lexical choice, but simply not having much to learn. For example, once a partially-lexicalized nonterminal has been generated whose tag is fairly specialized, such as IN, then the model has “painted itself into a lexical corner”, as it were (the extreme example is TO, a tag that can only be assigned to the word to). This is an example of the “label bias” problem, which has been the subject of recent discussion (Lafferty et al., 2001; Klein and Manning, 2002). Of course, just because there is “label bias” does not necessarily mean there is a problem. If the decoder pursues a theory to a nonterminal/partof-speech tag preterminal that has an extremely low entropy distribution for possible head words, then there is certainly a chance that it will get “stuck” in a potentially bad theory. This is of particular concern when a head word—which the top-down model generates at its highest point in the tree—influences an attachment decision. However, inspecting the lowentropy word-generation histories of PMw revealed that almost all such cases are when the m</context>
</contexts>
<marker>Klein, Manning, 2002</marker>
<rawString>Dan Klein and Christopher D. Manning. 2002. Conditional structure versus conditional estimation in NLP models. In Proceedings of the 2002 Conference on Empirical Methods for Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Fernando Pereira</author>
<author>Andrew McCallum</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="27767" citStr="Lafferty et al., 2001" startWordPosition="4602" endWordPosition="4605">tioning on head words. 7 Discussion Examining the lower-entropy PMw distributions revealed that, in many cases, the model was not so much learning how to disambiguate a given syntactic/lexical choice, but simply not having much to learn. For example, once a partially-lexicalized nonterminal has been generated whose tag is fairly specialized, such as IN, then the model has “painted itself into a lexical corner”, as it were (the extreme example is TO, a tag that can only be assigned to the word to). This is an example of the “label bias” problem, which has been the subject of recent discussion (Lafferty et al., 2001; Klein and Manning, 2002). Of course, just because there is “label bias” does not necessarily mean there is a problem. If the decoder pursues a theory to a nonterminal/partof-speech tag preterminal that has an extremely low entropy distribution for possible head words, then there is certainly a chance that it will get “stuck” in a potentially bad theory. This is of particular concern when a head word—which the top-down model generates at its highest point in the tree—influences an attachment decision. However, inspecting the lowentropy word-generation histories of PMw revealed that almost all</context>
</contexts>
<marker>Lafferty, Pereira, McCallum, 2001</marker>
<rawString>John Lafferty, Fernando Pereira, and Andrew McCallum. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lillian Lee</author>
</authors>
<title>Measures of distributional similarity.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th ACL,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="21581" citStr="Lee, 1999" startWordPosition="3525" endWordPosition="3526">ing used; in fact, they are getting used more than 19 times as often when pursuing the highest-scoring theory as when pursuing any theory on average. And yet there is no disputing the fact that their use has a surprisingly small effect on parsing performance. The exploratory data analysis of §3.3 suggests an explanation for this perplexing behavior: the distributions that include the head word versus those that do not are so similar as to make almost no difference in terms of parse accuracy. 5.1 Distributional similarity A useful metric for measuring distributional similarity, as explored by (Lee, 1999), is the JensenShannon divergence (Lin, 1991): JS (p 11 q) = 1ID lp I I avgp,q ) + D lq I I avgp,q )� 2 (2) where D is the Kullback-Leibler divergence (Cover and Thomas, 1991) and where avgp,q = 2 (p(A) + q(A)) for an event A in the event space 1 of at least one of the two distributions. One interpretation for the Jensen-Shannon divergence due to Slonim et al. (2002) is that it is related to the loglikelihood that “the two sample distributions originate by the most likely common source,” relating the quantity to the “two-sample problem”. In our case, we have p = p(y |x1, x2) and q = p(y |x1), </context>
</contexts>
<marker>Lee, 1999</marker>
<rawString>Lillian Lee. 1999. Measures of distributional similarity. In Proceedings of the 37th ACL, pages 25–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianhua Lin</author>
</authors>
<title>Divergence measures based on the Shannon entropy.</title>
<date>1991</date>
<journal>IEEE Transactions on Information Theory,</journal>
<volume>37</volume>
<issue>1</issue>
<contexts>
<context position="21626" citStr="Lin, 1991" startWordPosition="3532" endWordPosition="3533"> than 19 times as often when pursuing the highest-scoring theory as when pursuing any theory on average. And yet there is no disputing the fact that their use has a surprisingly small effect on parsing performance. The exploratory data analysis of §3.3 suggests an explanation for this perplexing behavior: the distributions that include the head word versus those that do not are so similar as to make almost no difference in terms of parse accuracy. 5.1 Distributional similarity A useful metric for measuring distributional similarity, as explored by (Lee, 1999), is the JensenShannon divergence (Lin, 1991): JS (p 11 q) = 1ID lp I I avgp,q ) + D lq I I avgp,q )� 2 (2) where D is the Kullback-Leibler divergence (Cover and Thomas, 1991) and where avgp,q = 2 (p(A) + q(A)) for an event A in the event space 1 of at least one of the two distributions. One interpretation for the Jensen-Shannon divergence due to Slonim et al. (2002) is that it is related to the loglikelihood that “the two sample distributions originate by the most likely common source,” relating the quantity to the “two-sample problem”. In our case, we have p = p(y |x1, x2) and q = p(y |x1), where y is a possible future and x1, x2 are e</context>
</contexts>
<marker>Lin, 1991</marker>
<rawString>Jianhua Lin. 1991. Divergence measures based on the Shannon entropy. IEEE Transactions on Information Theory, 37(1):145–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Magerman</author>
</authors>
<title>Natural Language Parsing as Statistical Pattern Recognition.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania,</institution>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="1317" citStr="Magerman (1994)" startWordPosition="196" endWordPosition="197">hypothesizing and testing for distributional similarity with back-off distributions, we have evidence that finally explains that (a) bilexical statistics are actually getting used quite often but that (b) the distributions are so similar to those that do not include head words as to be nearly indistinguishable insofar as making parse decisions. Finally, our analysis has provided for the first time an effective way to do parameter selection for a generative lexicalized statistical parsing model. 1 Introduction Lexicalized statistical parsing models, such as those built by Black et al. (1992a), Magerman (1994), Collins (1999) and Charniak (2000), have been enormously successful, but they also have an enormous complexity. Their success has often been attributed to their sensitivity to individual lexical items, and it is precisely this incorporation of lexical items into features or parameter schemata that gives rise to their complexity. In order to help determine which features are helpful, the somewhat crude-buteffective method has been to compare a model’s overall parsing performance with and without a feature. Often, it has seemed that features that are derived from linguistic principles result i</context>
</contexts>
<marker>Magerman, 1994</marker>
<rawString>David Magerman. 1994. Natural Language Parsing as Statistical Pattern Recognition. Ph.D. thesis, University of Pennsylvania, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Beatrice Santorini</author>
<author>Mary Ann Marcinkiewicz</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2925" citStr="Marcus et al., 1993" startWordPosition="450" endWordPosition="453">he nature of the parameters in a Collins-style parsing model, with particular focus on the two parameter classes that generate lexicalized modifying nonterminals, for these are where all a sentence’s words are generated except for the head word of the entire sentence; also, these two parameter classes have by far the most parameters and suffer the most from sparse data problems. In spite of using a Collins-style model as the basis for analysis, throughout this paper, we will attempt to present information that is widely applicable because it pertains to properties of the widely-used Treebank (Marcus et al., 1993) and lexicalized parsing models in general. This work also sheds light on the much-discussed “bilexical dependencies” of statistical parsing models. Beginning with the seminal work at IBM (Black et al., 1991; Black et al., 1992b; Black et al., 1992a), and continuing with such lexicalist approaches as (Eisner, 1996), these features have been lauded for their ability to approximate a word’s semantics as a means to override syntactic preferences with semantic ones (Collins, 1999; Eisner, 2000). However, the work of Gildea (2001) showed that, with an approximate reimplementation of Collins’ Model </context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19:313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Slonim</author>
<author>Nir Friedman</author>
<author>Naftali Tishby</author>
</authors>
<title>Unsupervised document classification using sequential information maximization.</title>
<date>2002</date>
<tech>Technical Report 2002–19,</tech>
<institution>Leibniz Center, The School of Computer Science and Engineering, Hebrew University,</institution>
<location>Jerusalem,</location>
<contexts>
<context position="21950" citStr="Slonim et al. (2002)" startWordPosition="3598" endWordPosition="3601">tributions that include the head word versus those that do not are so similar as to make almost no difference in terms of parse accuracy. 5.1 Distributional similarity A useful metric for measuring distributional similarity, as explored by (Lee, 1999), is the JensenShannon divergence (Lin, 1991): JS (p 11 q) = 1ID lp I I avgp,q ) + D lq I I avgp,q )� 2 (2) where D is the Kullback-Leibler divergence (Cover and Thomas, 1991) and where avgp,q = 2 (p(A) + q(A)) for an event A in the event space 1 of at least one of the two distributions. One interpretation for the Jensen-Shannon divergence due to Slonim et al. (2002) is that it is related to the loglikelihood that “the two sample distributions originate by the most likely common source,” relating the quantity to the “two-sample problem”. In our case, we have p = p(y |x1, x2) and q = p(y |x1), where y is a possible future and x1, x2 are elements of a history context, with q representing a back-off distribution using less context. Therefore, whereas the standard JS formulation is agnosmin max avg. median JS0,1 2.729E-7 2.168 0.1148 0.09672 JS1,2 0.001318 1.962 0.6929 0.6986 JS0,2 0.001182 1.180 0.3774 0.3863 Table 5: Jensen-Shannon statistics for back-off p</context>
</contexts>
<marker>Slonim, Friedman, Tishby, 2002</marker>
<rawString>Noam Slonim, Nir Friedman, and Naftali Tishby. 2002. Unsupervised document classification using sequential information maximization. Technical Report 2002–19, Leibniz Center, The School of Computer Science and Engineering, Hebrew University, Jerusalem, Israel.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>