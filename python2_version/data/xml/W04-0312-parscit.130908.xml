<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000011">
<title confidence="0.953085">
Incremental Parsing, or Incremental Grammar?*
</title>
<author confidence="0.89534">
Matthew Purver† and Ruth Kempson$
</author>
<affiliation confidence="0.786685">
Departments of †Computer Science and $Philosophy,
</affiliation>
<note confidence="0.63255">
King’s College London, Strand,
London WC2R 2LS,
UK
</note>
<email confidence="0.991208">
{matthew.purver, ruth.kempson}@kcl.ac.uk
</email>
<sectionHeader confidence="0.993761" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999466357142857">
Standard grammar formalisms are defined with-
out reflection of the incremental and serial na-
ture of language processing, and incremental-
ity must therefore be reflected by independently
defined parsing and/or generation techniques.
We argue that this leads to a poor setup for
modelling dialogue, with its rich speaker-hearer
interaction, and instead propose context-based
parsing and generation models defined in terms
of an inherently incremental grammar formal-
ism (Dynamic Syntax), which allow a straight-
forward model of otherwise problematic dia-
logue phenomena such as shared utterances, el-
lipsis and alignment.
</bodyText>
<sectionHeader confidence="0.998986" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998663578947369">
Despite increasing psycholinguistic evidence of
incrementality in language processing, both in
parsing (see e.g. (Crocker et al., 2000)) and in
production (Ferreira, 1996), there is almost uni-
versal agreement that this should not be re-
flected in grammar formalisms which constitute
the underlying model of language (for rare ex-
ceptions, see (Hausser, 1989; Kempson et al.,
2001)). Constraint-based grammar formalisms
are accordingly defined neutrally between either
of these applications, with parsing/generation
systems (whether incremental or not) defined
as independent architectures manipulating the
same underlying system.1 Such assumptions
however lead to formal architectures that are
relatively poorly set up for modelling dialogue,
for they provide no basis for the very rich de-
gree of interaction between participants in dia-
logue. A common phenomenon in dialogue is
</bodyText>
<tableCaption confidence="0.530846428571429">
* Related papers from the point of view of generation
rather than parsing, and from the point of view of align-
ment rather than incrementality, are to be presented at
INLG’04 and Catalog’04 respectively.
1Authors vary as to the extent to which these archi-
tectures might be defined to be reversible. See (Neu-
mann, 1994).
</tableCaption>
<bodyText confidence="0.902328">
that of shared utterances (Clark and Wilkes-
Gibbs, 1986), with exchange of roles of parser
and producer midway through an utterance:2
Daniel: Why don’t you stop mumbling
and
</bodyText>
<listItem confidence="0.81183775">
Marc: Speak proper like?
Daniel: speak proper?
(2) Ruth: What did Alex ...
Hugh: Design? A kaleidoscope.
</listItem>
<bodyText confidence="0.999800724137931">
Such utterances clearly show the need for a
strictly incremental model: however, they are
particularly problematic for any overall archi-
tecture in which parsing and generation are in-
dependently defined as applications of a use-
neutral grammar formalism which yields as out-
put the set of well-formed strings, for in these
types of exchange, the string uttered by one and
parsed by the other need not be a wellformed
string in its own right, so will not fall within
the set of data which the underlying formalism
is set up to capture. Yet, with the transition be-
tween interlocutors seen as a shift from one sys-
tem to another, each such substring will have to
be characterised independently. Many other di-
alogue phenomena also show the need for inter-
action between the parsing and generation pro-
cesses, among them cross-speaker ellipsis (e.g.
simple bare fragment answers to wh-questions),
and alignment (Pickering and Garrod, 2004), in
which conversational participants mirror each
other’s patterns at many levels (including lexi-
cal choice and syntactic structure).
The challenge of being able to model these
phenomena, problematic for theorists but ex-
tremely easy and natural for dialogue partici-
pants themselves, has recently been put out by
Pickering and Garrod (2004) as a means of eval-
uating both putative grammar formalisms and
</bodyText>
<footnote confidence="0.9356095">
2Example (1) from the BNC, file KNY (sentences
315–317).
</footnote>
<equation confidence="0.820365">
(1)
</equation>
<bodyText confidence="0.999970357142857">
models of language use. In response to this chal-
lenge, we suggest that an alternative means of
evaluating parsing implementations is by eval-
uation of paired parsing-generation models and
the dialogue model that results. As an illustra-
tion of this, we show how if we drop the assump-
tion that grammar formalisms are defined neu-
trally between parsing and production in favour
of frameworks in which the serial nature of lan-
guage processing is a central design feature (as
in Dynamic Syntax: (Kempson et al., 2001)),
then we can define a model in which incremental
sub-systems of parsing and generation are nec-
essarily tightly coordinated, and can thus pro-
vide a computational model of dialogue which
directly corresponds with currently emerging
psycholinguistic results (Branigan et al., 2000).
In particular, by adding a shared model of con-
text to previously defined word-by-word incre-
mental parsing and generation models, we show
how the switch in speaker/hearer roles during
a shared utterance can be seen as a switch be-
tween processes which are directed by different
goals, but which share the same incrementally
built data structures. We then show how this
inherent incrementality and structure/context
sharing also allows a straightforward model of
cross-speaker ellipsis and alignment.
</bodyText>
<sectionHeader confidence="0.972026" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999323643835617">
Dynamic Syntax (DS) (Kempson et al., 2001) is
a parsing-directed grammar formalism in which
a decorated tree structure representing a se-
mantic interpretation for a string is incremen-
tally projected following the left-right sequence
of the words. Importantly, this tree is not a
model of syntactic structure, but is strictly se-
mantic, being a representation of the predicate-
argument structure of the sentence. In DS,
grammaticality is defined as parsability (the
successful incremental construction of a tree-
structure logical form, using all the information
given by the words in sequence): there is no cen-
tral use-neutral grammar of the kind assumed
by most approaches to parsing and/or gener-
ation. The logical forms are lambda terms of
the epsilon calculus (see (Meyer-Viol, 1995) for
a recent development), so quantification is ex-
pressed through terms of type e whose complex-
ity is reflected in evaluation procedures that ap-
ply to propositional formulae once constructed,
and not in the tree itself. With all quantification
expressed as type e terms, the standard grounds
for mismatch between syntactic and semantic
analysis for all NPs is removed; and, indeed, all
syntactic distributions are explained in terms of
this incremental and monotonic growth of par-
tial representations of content. Hence the claim
that the model itself constitutes a NL grammar
formalism.
Parsing (Kempson et al., 2001) defines pars-
ing as a process of building labelled semantic
trees in a strictly left-to-right, word-by-word in-
cremental fashion by using computational ac-
tions and lexical actions defined (for some natu-
ral language) using the modal tree logic LOFT
(Blackburn and Meyer-Viol, 1994). These ac-
tions are defined as transition functions be-
tween intermediate states, which monotonically
extend tree structures and node decorations.
Words are specified in the lexicon to have as-
sociated lexical actions: the (possibly partial)
semantic trees are monotonically extended by
applying these actions as each word is consumed
from the input string. Partial trees may be un-
derspecified: tree node relations may be only
partially specified; node decorations may be de-
fined in terms of unfulfilled requirements and
metavariables; and trees may lack a full set of
scope constraints. Anaphora resolution is a fa-
miliar case of update: pronouns are defined to
project metavariables that are substituted from
context as part of the construction process. Rel-
ative to the same tree-growth dynamics, long-
distance dependency effects are characterised
through restricted licensing of partial trees with
relation between nodes introduced with merely
a constraint on some fixed extension (following
D-Tree grammar formalisms (Marcus, 1987)),
an underspecification that gets resolved within
the left-to-right construction process.3 Quanti-
fying terms are also built up using determiner
and noun to yield a partially specified term e.g.
(E, y, Man&apos;(y)) with a requirement for a scope
statement. These scope statements, of the form
x &lt; y (‘the term binding x is to be evaluated
as taking scope over the term binding y’), are
added to a locally dominating type-t-requiring
node. Generally, they are added to an accu-
mulating set following the serial order of pro-
cessing in determining the scope dependency,
but indefinites (freer in scope potential) are as-
signed a metavariable as first argument, allow-
</bodyText>
<footnote confidence="0.988618">
3In this, the system is also like LFG, modelling long-
distance dependency in the same terms as functional un-
certainty (Kaplan and Zaenen, 1989), differing from that
concept in the dynamics of update internal to the con-
struction of a single tree.
</footnote>
<figureCaption confidence="0.999445">
Figure 1: Parsing “john likes mary” . . . . . . and generating “john likes mary”
</figureCaption>
<figure confidence="0.9152676">
{john&apos;} {}
likes
mary
{like&apos;(mary&apos;)(john&apos;), }
{john&apos;} {like&apos;(mary&apos;)}
{like&apos;} {mary&apos;}
{}
FAIL FAIL
john likes mary
mary
{}
{john&apos;} {}
{like&apos;} {}
FAIL
john
{}
{john&apos;} {}
{like&apos;(mary&apos;)(john&apos;), }
{john&apos;} {like&apos;(mary&apos;)}
{like&apos;} {mary&apos;}
likes
mary
{}
{john&apos;} {}
{like&apos;} {}
</figure>
<bodyText confidence="0.94746578">
ing selection from any term already added, in-
cluding temporally-sorted variables associated
with tense/modality specifications. The gen-
eral mechanism is the incremental analogue of
quantifier storage; and once a propositional for-
mula of type t has been derived at a node with
some collection of scope statements, these are
jointly evaluated to yield fully expanded terms
that reflect all relative dependencies within the
restrictor of the terms themselves. For example,
parsing A man coughed yields the pair Si &lt; x,
Cough&apos;(E, x, Man&apos;(x)) (Si the index of evalua-
tion), then evaluated as Man&apos;(a) n Cough&apos;(a)
where a = (E, x, Man&apos;(x) n Cough&apos;(x)).4
Once all requirements are satisfied and all
partiality and underspecification is resolved,
trees are complete, parsing is successful and the
input string is said to be grammatical. Central
to the formalism is the incremental and mono-
tonic growth of labelled partial trees: the parser
state at any point contains all the partial trees
which have been produced by the portion of the
string so far consumed and which remain can-
didates for completion.5
4For formal details of this approach to quantification
see (Kempson et al., 2001) chapter 7; for an early imple-
mentation see (Kibble et al., 2001).
5Figure 1 assumes, simplistically, that linguistic
names correspond directly to scopeless names in the log-
Generation (Otsuka and Purver, 2003;
Purver and Otsuka, 2003) (hereafter O&amp;P)
give an initial method of context-independent
tactical generation based on the same incre-
mental parsing process, in which an output
string is produced according to an input
semantic tree, the goal tree. The generator
incrementally produces a set of corresponding
output strings and their associated partial trees
(again, on a left-to-right, word-by-word basis)
by following standard parsing routines and
using the goal tree as a subsumption check.
At each stage, partial strings and trees are
tentatively extended using some word/action
pair from the lexicon; only those candidates
which produce trees which subsume the goal
tree are kept, and the process succeeds when
a complete tree identical to the goal tree is
produced. Generation and parsing thus use
the same tree representations and tree-building
actions throughout.
</bodyText>
<sectionHeader confidence="0.999026" genericHeader="method">
3 Contextual Model
</sectionHeader>
<bodyText confidence="0.981954752380953">
The current proposed model (and its imple-
mentation) is based on these earlier definitions
but modifies them in several ways, most signif-
icantly by the addition of a model of context:
ical language that decorate the tree.
while they assume some notion of context they
give no formal model or implementation.6 The
contextual model we now assume is made up not
only of the semantic trees built by the DS pars-
ing process, but also the sequences of words and
associated lexical actions that have been used
to build them. It is the presence of (and as-
sociations between) all three, together with the
fact that this context is equally available to both
parsing and generation processes, that allow our
straightforward model of dialogue phenomena.?
For the purposes of the current implementa-
tion, we make a simplifying assumption that
the length of context is finite and limited to the
result of some immediately previous parse (al-
though information that is independently avail-
able can be represented in the DS tree format,
so that, in reality, larger and only partially or-
dered contexts are no doubt possible): context
at any point is therefore made up of the trees
and word/action sequences obtained in parsing
the previous sentence and the current (incom-
plete) sentence.
Parsing in Context A parser state is there-
fore defined to be a set of triples (T, W, A),
where T is a (possibly partial) semantic tree,$
W the sequence of words and A the sequence
of lexical and computational actions that have
been used in building it. This set will initially
contain only a single triple (Ta, 0, 0) (where Ta
is the basic axiom taken as the starting point of
the parser, and the word and action sequences
are empty), but will expand as words are con-
sumed from the input string and the corre-
sponding actions produce multiple possible par-
tial trees. At any point in the parsing process,
the context for a particular partial tree T in
6There are other departures in the treatment of linked
structures (for relatives and other modifiers) and quan-
tification, and more relevantly to improve the incremen-
tality of the generation process: we do not adopt the
proposal of O&amp;P to speed up generation by use of a re-
stricted multiset of lexical entries selected on the basis
of goal tree features, which prevents strictly incremental
generation and excludes modification of the goal tree.
7In building n-tuples of trees corresponding to
predicate-argument structures, the system is similar to
LTAG formalisms (Joshi and Kulick, 1997). However,
unlike LTAG systems (see e.g. (Stone and Doran, 1997)),
both parsing and generation are not head-driven, but
fully (word-by-word) incremental. This has the ad-
vantage of allowing fully incremental models for all
languages, matching psycholinguistic observations (Fer-
reira, 1996).
8Strictly speaking, scope statements should be in-
cluded in these n-tuples – for now we consider them as
part of the tree.
this set can then be taken to consist of: (a) a
similar triple (T0, W0, A0) given by the previous
sentence, where T0 is its semantic tree repre-
sentation, W0 and A0 the sequences of words
and actions that were used in building it; and
(b) the triple (T, W, A) itself. Once parsing is
complete, the final parser state, a set of triples,
will form the new starting context for the next
sentence. In the simple case where the sentence
is unambiguous (or all ambiguity has been re-
moved) this set will again have been reduced
to a single triple (T1, W1, A1), corresponding to
the final interpretation of the string T1 with its
sequence of words W1 and actions A1, and this
replaces (T0, W0, A0) as the new context; in the
presence of persistent ambiguity there will sim-
ply be more than one triple in the new context.9
Generation in Context A generator state
is now defined as a pair (Tg, X) of a goal tree
Tg and a set X of pairs (5, P), where 5 is a
candidate partial string and P is the associated
parser state (a set of (T, W, A) triples). Ini-
tially, the set X will usually contain only one
pair, of an empty candidate string and the stan-
dard initial parser state, (0, {(Ta, 0, 0)1). How-
ever, as both parsing and generation processes
are strictly incremental, they can in theory start
from any state. The context for any partial tree
T is defined exactly as for parsing: the previ-
ous sentence triple (T0, W0, A0); and the cur-
rent triple (T, W, A). Generation and parsing
are thus very closely coupled, with the central
part of both processes being a parser state: a set
of tree/word-sequence/action-sequence triples.
Essential to this correspondence is the lack of
construction of higher-level hypotheses about
the state of the interlocutor. All transitions
are defined over the context for the individ-
ual (parser or generator). In principle, con-
texts could be extended to include high-level
hypotheses, but these are not essential and are
not implemented in our model (see (Millikan,
2004) for justification of this stance).
</bodyText>
<sectionHeader confidence="0.997457" genericHeader="method">
4 Shared Utterances
</sectionHeader>
<bodyText confidence="0.99613925">
One primary evidence for this close coupling
and sharing of structures and context is the ease
with which shared utterances can be expressed.
O&amp;P suggest an analysis of shared utterances,
</bodyText>
<tableCaption confidence="0.7428864">
9The current implementation of the formalism does
not include any disambiguation mechanism. We simply
assume that selection of some (minimal) context and at-
tendant removal of any remaining ambiguity is possible
by inference.
</tableCaption>
<figureCaption confidence="0.846413">
Figure 2: Transition from hearer to speaker: “What did Alex ... / ... design?”
</figureCaption>
<figure confidence="0.914680181818182">
{+Q}
Pt =
, {what, did, alex}, {a1, a2, a3}
{WH} {alex}{?Ty(e  t), }
{+Q, design(WH)(alex)}
{alex} {design(WH)}
{WH}{design}
Gt =
 
, 0,
{+Q}
</figure>
<equation confidence="0.733865909090909">
, {what, did, alex}, {a1, a2, a3}
{alex}{?Ty(e t),}
{+Q, design(WH)(alex)}
G1 =
{alex} {design(WH)}
{WH}{design}
{+Q}
 
, {design}, {WH}{alex} {?Ty(e  t)}
, {..., design}, {..., a4}
{}{design}
</equation>
<bodyText confidence="0.952456721518988">
and this can now be formalised given the cur-
rent model. As the parsing and generation pro-
cesses are both fully incremental, they can start
from any state (not just the basic axiom state
(Ta, 0, 0)). As they share the same lexical en-
tries, the same context and the same semantic
tree representations, a model of the switch of
roles now becomes relatively straightforward.
Transition from Hearer to Speaker Nor-
mally, the generation process begins with
the initial generator state as defined above:
(Tg, {(0, P0)}), where P0 is the standard initial
“empty” parser state {(Ta, 0, 0)}. As long as a
suitable goal tree Tg is available to guide gen-
eration, the only change required to generate a
continuation from a heard partial string is to
replace P0 with the parser state (a set of triples
(T, W, A)) as produced from that partial string:
we call this the transition state Pt. The initial
hearer A therefore parses as usual until transi-
tion,10 then given a suitable goal tree Tg, forms
a transition generator state Gt = (Tg, {(0, Pt)}),
from which generation can begin directly – see
figure 2.11 Note that the context does not
change between processes.
For generation to begin from this transition
state, the new goal tree Tg must be subsumed
by at least one of the partial trees in Pt (i.e.
the proposition to be expressed must be sub-
sumed by the incomplete proposition that has
been built so far by the parser). Constructing
10We have little to say about exactly when transitions
occur. Presumably speaker pauses and the availability
to the hearer of a possible goal tree both play a part.
11Figure 2 contains several simplifications to aid read-
ability, both to tree structure details and by show-
ing parser/generator states as single triples/pairs rather
than sets thereof.
Tg prior to the generation task will often be a
complex process involving inference and/or ab-
duction over context and world/domain knowl-
edge – Poesio and Rieser (2003) give some idea
as to how this inference might be possible – for
now, we make the simplifying assumption that
a suitable propositional structure is available.
Transition from Speaker to Hearer At
transition, the initial speaker B’s generator
state G&apos;t contains the pair (5t, Pt), where 5t is
the partial string output so far, and Pt� is the
corresponding parser state, the transition state
as far as B is concerned.12 In order for B to
interpret A’s continuation, B need only use Pt�
as the initial parser state which is extended as
the string produced by A is consumed.
As there will usually be multiple possible par-
tial trees at the transition point, A may con-
tinue in a way that does not correspond to B’s
initial intentions – i.e. in a way that does not
match B’s initial goal tree. For B to be able
to understand such continuations, the genera-
tion process must preserve all possible partial
parse trees (just as the parsing process does),
whether they subsume the goal tree or not, as
long as at least one tree in the current state does
subsume the goal tree. A generator state must
therefore rule out only pairs (5, P) for which P
contains no trees which subsume the goal tree,
rather than thinning the set P directly via the
subsumption check as proposed by O&amp;P.
It is the incrementality of the underlying
grammar formalism that allows this simple
switch: the parsing process can begin directly
{WH}
12Of course, if both A and B share the same lexical
entries and communication is perfect, Pt = Pt, but we
do not have to assume that this is the case.
from a state produced by an incomplete gener-
ation process, and vice versa, as their interme-
diate representations are necessarily the same.
</bodyText>
<sectionHeader confidence="0.996678" genericHeader="method">
5 Cross-Speaker Ellipsis
</sectionHeader>
<bodyText confidence="0.999413076923077">
This inherent close coupling of the two incre-
mental processes, together with the inclusion
of tree-building actions in the model of con-
text, also allows a simple analysis of many cross-
speaker elliptical phenomena.
Fragments Bare fragments (3) may be anal-
ysed as taking a previous structure from con-
text as a starting point for parsing (or genera-
tion). WH-expressions are analysed as partic-
ular forms of metavariables, so parsing a wh-
question yields a type-complete but open for-
mula, which the term presented by a subsequent
fragment can update:
</bodyText>
<listItem confidence="0.997683">
(3) A: What did you eat for breakfast?
B: Porridge.
</listItem>
<bodyText confidence="0.999492777777778">
Parsing the fragment involves constructing an
unfixed node, and merging it with the contex-
tually available structure, so characterising the
wellformedness/interpretation of fragment an-
swers to questions without any additional mech-
anisms: the term (E, x, porridge&apos;(x)) stands in a
licensed growth relation from the metavariable
WH provided by the lexical actions of what.
Functional questions (Ginzburg and Sag,
2000) with their fragment answers (4) pose
no problem. As the wh-question contains a
metavariable, the scope evaluation cannot be
completed; completion of structure and evalu-
ation of scope can then be effected by merg-
ing in the term the answer provides, identifying
any introduced metavariable in this context (the
genitive imposes narrow scope of the introduced
epsilon term):
</bodyText>
<listItem confidence="0.995888">
(4) A: Who did every student ignore?
B: Their supervisor.
</listItem>
<construct confidence="0.509315">
_* {Si &lt; x, x &lt; y}
{(c, x, stud&apos;(x))} {}
{(e, y, sup&apos;(x)(y)}{ignr&apos;}
</construct>
<bodyText confidence="0.978708303571429">
VP Ellipsis Anaphoric devices such as pro-
nouns and VP ellipsis are analysed as decorating
tree nodes with metavariables licensing update
from context using either established terms, or,
for ellipsis, (lexical) tree-update actions. Strict
readings of VP ellipsis result from taking a suit-
able semantic formula directly from a tree node
in context: any node n E (T0 U T) of suitable
type (e _* t) with no outstanding requirements.
Sloppy readings involve re-use of actions: any
sequence of actions (a1; a2; ... ; an) E (A0 U A)
can be used (given the appropriate elliptical
trigger) to extend the current tree T if this pro-
vides a formula of type e _* t.13 This latter
approach, combined with the representation of
quantified elements as terms, allows a range of
phenomena, including those which are problem-
atic for other (abstraction-based) approaches
(for discussion see (Dalrymple et al., 1991)):
A: A policeman who arrested Bill read
him his rights.
B: The policeman who arrested Tom
did too.
The actions associated with A’s use of read
him his rights in (5) include the projection of a
metavariable associated with him, and its res-
olution to the term in context associated with
Bill. B’s ellipsis allows this action sequence to
be re-used, again projecting a metavariable and
resolving it, this time (given the new context) to
the term provided by parsing Tom. This leads
to a copy of Tom within the constructed predi-
cate, and a sloppy reading.
This analysis also applies to yield parallellism
effects in scoping (Hirschb¨uhler, 1982; Shieber
et al., 1996), allowing narrow scope construal
for indefinites in subject position:
A: A nurse interviewed every patient.
B: An orderly did too.
Resolution of the underspecification in the
scope statement associated with an indefinite
can be performed at two points: either at the
immediate point of processing the lexical ac-
tions, or at the later point of compiling the re-
sulting node’s interpretation within the emer-
gent tree.14 In (6), narrow scope can be as-
signed to the subject in A’s utterance via this
late assignment of scope; at this late point in the
13In its re-use of actions provided by context, this ap-
proach to ellipsis is essentially similar to the glue lan-
guage approach (see (Asudeh and Crouch, 2002) and
papers in (Dalrymple, 1999) but, given the lack of in-
dependent syntax /semantics vocabularies, the need for
an intermediate mapping language is removed.
14This pattern parallels expletive pronouns which
equally allow a delayed update (Cann, 2003).
</bodyText>
<equation confidence="0.95484">
{Si &lt; x}
{(c, x, stud&apos;(x))} {}
{WH, Q} {ignr&apos;}
(5)
(6)
</equation>
<bodyText confidence="0.999979785714286">
parse process, the term constructed from the ob-
ject node will have been entered into the set of
scope statements, allowing the subject node to
be dependent on the following quantified expres-
sion. The elliptical word did in B’s utterance
will then license re-use of these late actions, re-
peating the procedures used in interpreting A’s
antecedent and so determining scope of the new
subject relative to the object.
Again, these analyses are possible because
parsing and generation processes share incre-
mentally built structures and contextual pars-
ing actions, with this being ensured by the in-
crementality of the grammar formalism itself.
</bodyText>
<sectionHeader confidence="0.992788" genericHeader="method">
6 Alignment &amp; Routinization
</sectionHeader>
<bodyText confidence="0.999925885714286">
The parsing and generation processes must both
search the lexicon for suitable entries at ev-
ery step (i.e. when parsing or generating each
word). For generation in particular, this is a
computationally expensive process in principle:
every possible word/action pair must be tested –
the current partial tree extended and the result
checked for goal tree subsumption. As proposed
by O&amp;P (though without formal definitions or
implementation) our model of context now al-
lows a strategy for minimising this effort: as
it includes previously used words and actions,
a subset of such actions can be re-used in ex-
tending the current tree, avoiding full lexical
search altogether. High frequency of elliptical
constructions is therefore expected, as ellipsis
licenses such re-use; the same can be said for
pronouns, as long as they (and their correspond-
ing actions) are assumed to be pre-activated or
otherwise readily available from the lexicon.
As suggested by O&amp;P, this can now lead di-
rectly to a model of alignment phenomena, char-
acterisable as follows. For the generator, if there
is some action a E (A0 U A) suitable for extend-
ing the current tree, a can be re-used, generat-
ing the word w which occupies the correspond-
ing position in the sequence W0 or W. This re-
sults in lexical alignment – repeating w rather
than choosing an alternative word from the lex-
icon. Alignment of syntactic structure (e.g. pre-
serving double-object or full PP forms in the use
of a verb such as give rather than shifting to
the semantically equivalent form (Branigan et
al., 2000)) also follows in virtue of the procedu-
ral action-based specification of lexical content.
A word such as give has two possible lexical
actions a&apos; and a&apos;&apos; despite semantic equivalence
of output, corresponding to the two alternative
forms. A previous use will cause either a&apos; or a&apos;&apos;
to be present in (A0 U A); re-use of this action
will cause the same form to be repeated.15
A similar definition holds for the parser: for a
word w presented as input, if w E (W0UW) then
the corresponding action a in the sequence A0 or
A can be used without consulting the lexicon.
Words will therefore be interpreted as having
the same sense or reference as before, modelling
the semantic alignment described by (Garrod
and Anderson, 1987). These characterisations
can also be extended to sequences of words –
a sub-sequence (a1; a2; ... ; an) E (A0 U A) can
be re-used by a generator, producing the cor-
responding word sequence (w1; w2; ... ; wn) E
(W0 U W); and similarly the sub-sequence of
words (w1; w2; ... ; wn) E (W0 U W) will cause
the parser to use the corresponding action se-
quence (a1; a2; ... ; an) E (A0 U A). This will
result in sequences or phrases being repeatedly
associated by both parser and generator with
the same sense or reference, leading to what
Pickering and Garrod (2004) call routinization
(construction and re-use of word sequences with
consistent meanings).
It is notable that these various patterns of
alignment, said by Pickering and Garrod (2004)
to be alignment across different levels, are ex-
pressible without invoking distinct levels of syn-
tactic or lexical structure, since context, content
and lexical actions are all defined in terms of the
same tree configurations.
</bodyText>
<sectionHeader confidence="0.998151" genericHeader="conclusions">
7 Summary
</sectionHeader>
<bodyText confidence="0.932211476190476">
The inherent left-to-right incrementality and
monotonicity of DS as a grammar formalism al-
lows both parsing and generation processes to
be not only incremental but closely coupled,
sharing structures and context. This enables
shared utterances, cross-speaker elliptical phe-
nomena and alignment to be modelled straight-
forwardly. A prototype system has been im-
plemented in Prolog which reflects the model
given here, demonstrating shared utterances
and alignment phenomena in simple dialogue
sequences. The significance of this direct re-
flection of psycholinguistic data is to buttress
the DS claim that the strictly serial incremen-
tality of processing is not merely essential to
the modelling of natural-language parsing, but
15Most frameworks would have to reflect this via pref-
erences defined over syntactic rules or parallelisms with
syntactic trees in context, both problematic.
to the design of the underlying grammar formal-
ism itself.
</bodyText>
<sectionHeader confidence="0.99028" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999962272727273">
This paper is an extension of joint work on the
DS framework with Wilfried Meyer-Viol, on ex-
pletives and on defining a context-dependent
formalism with Ronnie Cann, and on DS gen-
eration with Masayuki Otsuka. Each has pro-
vided ideas and input without which the cur-
rent results would have differed, although any
mistakes here are ours. Thanks are also due to
the ACL reviewers. This work was supported
by the ESRC (RES-000-22-0355) and (for the
second author) by the Leverhulme Trust.
</bodyText>
<sectionHeader confidence="0.999417" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999264622222222">
A. Asudeh and R. Crouch. 2002. Derivational
parallelism and ellipsis parallelism. In Pro-
ceedings of WCCFL 21.
P. Blackburn and W. Meyer-Viol. 1994. Lin-
guistics, logic and finite trees. Bulletin of the
IGPL, 2:3–31.
H. Branigan, M. Pickering, and A. Cleland.
2000. Syntactic co-ordination in dialogue.
Cognition, 75:13–25.
R. Cann. 2003. Semantic underspecification
and the interpretation of copular clauses in
English. In Where Semantics Meets Pragmat-
ics. University of Michigan.
H. H. Clark and D. Wilkes-Gibbs. 1986. Re-
ferring as a collaborative process. Cognition,
22:1–39.
M. Crocker, M. Pickering, and C. Clifton, ed-
itors. 2000. Architectures and Mechanisms
in Sentence Comprehension. Cambridge Uni-
versity Press.
M. Dalrymple, S. Shieber, and F. Pereira. 1991.
Ellipsis and higher-order unification. Linguis-
tics and Philosophy, 14(4):399–452.
M. Dalrymple, editor. 1999. Syntax and Se-
mantics in Lexical Functional Grammar: The
Resource-Logic Approach. MIT Press.
V. Ferreira. 1996. Is it better to give than to
donate? Syntactic flexibility in language pro-
duction. Journal of Memory and Language,
35:724–755.
S. Garrod and A. Anderson. 1987. Saying what
you mean in dialogue. Cognition, 27:181–218.
J. Ginzburg and I. A. Sag. 2000. Interrogative
Investigations. CSLI Publications.
R. Hausser. 1989. Computation of Language.
Springer-Verlag.
P. Hirschb¨uhler. 1982. VP deletion and across-
the-board quantifier scope. In Proceedings of
NELS 12.
A. Joshi and S. Kulick. 1997. Partial proof trees
as building blocks for a categorial grammar.
Linguistics and Philosophy, 20:637–667.
R. Kaplan and A. Zaenen. 1989. Long-
distance dependencies, constituent structure,
and functional uncertainty. In M. Baltin and
A. Kroch, editors, Alternative Conceptions of
Phrase Structure, pages 17–42. University of
Chicago Press.
R. Kempson, W. Meyer-Viol, and D. Gabbay.
2001. Dynamic Syntax: The Flow of Lan-
guage Understanding. Blackwell.
R. Kibble, W. Meyer-Viol, D. Gabbay, and
R. Kempson. 2001. Epsilon terms: a la-
belled deduction account. In H. Bunt and
R. Muskens, editors, Computing Meaning.
Kluwer Academic Publishers.
M. Marcus. 1987. Deterministic parsing and
description theory. In P. Whitelock et al., ed-
itor, Linguistic Theory and Computer Appli-
cations, pages 69–112. Academic Press.
W. Meyer-Viol. 1995. Instantial Logic. Ph.D.
thesis, University of Utrecht.
R. Millikan. 2004. The Varieties of Meaning.
MIT Press.
G. Neumann. 1994. A Uniform Computa-
tional Model for Natural Language Parsing
and Generation. Ph.D. thesis, Universit¨at des
Saarlandes.
M. Otsuka and M. Purver. 2003. Incremental
generation by incremental parsing. In Pro-
ceedings of the 6th CLUK Colloquium.
M. Pickering and S. Garrod. 2004. Toward a
mechanistic psychology of dialogue. Behav-
ioral and Brain Sciences, forthcoming.
M. Poesio and H. Rieser. 2003. Coordination in
a PTT approach to dialogue. In Proceedings
of the 7th Workshop on the Semantics and
Pragmatics of Dialogue (DiaBruck).
M. Purver and M. Otsuka. 2003. Incremental
generation by incremental parsing: Tactical
generation in Dynamic Syntax. In Proceed-
ings of the 9th European Workshop in Natural
Language Generation (ENLG-2003).
S. Shieber, F. Pereira, and M. Dalrymple. 1996.
Interactions of scope and ellipsis. Linguistics
and Philosophy, 19:527–552.
M. Stone and C. Doran. 1997. Sentence plan-
ning as description using tree-adjoining gram-
mar. In Proceedings of the 35th Annual Meet-
ing of the ACL, pages 198–205.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493370">
<title confidence="0.637315">Parsing, or Incremental</title>
<affiliation confidence="0.949191">of Science and King’s College London,</affiliation>
<address confidence="0.910321">London WC2R</address>
<abstract confidence="0.9891638">Standard grammar formalisms are defined without reflection of the incremental and serial nature of language processing, and incrementality must therefore be reflected by independently defined parsing and/or generation techniques. We argue that this leads to a poor setup for modelling dialogue, with its rich speaker-hearer interaction, and instead propose context-based parsing and generation models defined in terms of an inherently incremental grammar formalism (Dynamic Syntax), which allow a straightforward model of otherwise problematic dialogue phenomena such as shared utterances, ellipsis and alignment.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Asudeh</author>
<author>R Crouch</author>
</authors>
<title>Derivational parallelism and ellipsis parallelism.</title>
<date>2002</date>
<booktitle>In Proceedings of WCCFL</booktitle>
<pages>21</pages>
<contexts>
<context position="24524" citStr="Asudeh and Crouch, 2002" startWordPosition="3994" endWordPosition="3997"> interviewed every patient. B: An orderly did too. Resolution of the underspecification in the scope statement associated with an indefinite can be performed at two points: either at the immediate point of processing the lexical actions, or at the later point of compiling the resulting node’s interpretation within the emergent tree.14 In (6), narrow scope can be assigned to the subject in A’s utterance via this late assignment of scope; at this late point in the 13In its re-use of actions provided by context, this approach to ellipsis is essentially similar to the glue language approach (see (Asudeh and Crouch, 2002) and papers in (Dalrymple, 1999) but, given the lack of independent syntax /semantics vocabularies, the need for an intermediate mapping language is removed. 14This pattern parallels expletive pronouns which equally allow a delayed update (Cann, 2003). {Si &lt; x} {(c, x, stud&apos;(x))} {} {WH, Q} {ignr&apos;} (5) (6) parse process, the term constructed from the object node will have been entered into the set of scope statements, allowing the subject node to be dependent on the following quantified expression. The elliptical word did in B’s utterance will then license re-use of these late actions, repeati</context>
</contexts>
<marker>Asudeh, Crouch, 2002</marker>
<rawString>A. Asudeh and R. Crouch. 2002. Derivational parallelism and ellipsis parallelism. In Proceedings of WCCFL 21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Blackburn</author>
<author>W Meyer-Viol</author>
</authors>
<title>Linguistics, logic and finite trees.</title>
<date>1994</date>
<journal>Bulletin of the IGPL,</journal>
<pages>2--3</pages>
<contexts>
<context position="6706" citStr="Blackburn and Meyer-Viol, 1994" startWordPosition="1032" endWordPosition="1035">erms, the standard grounds for mismatch between syntactic and semantic analysis for all NPs is removed; and, indeed, all syntactic distributions are explained in terms of this incremental and monotonic growth of partial representations of content. Hence the claim that the model itself constitutes a NL grammar formalism. Parsing (Kempson et al., 2001) defines parsing as a process of building labelled semantic trees in a strictly left-to-right, word-by-word incremental fashion by using computational actions and lexical actions defined (for some natural language) using the modal tree logic LOFT (Blackburn and Meyer-Viol, 1994). These actions are defined as transition functions between intermediate states, which monotonically extend tree structures and node decorations. Words are specified in the lexicon to have associated lexical actions: the (possibly partial) semantic trees are monotonically extended by applying these actions as each word is consumed from the input string. Partial trees may be underspecified: tree node relations may be only partially specified; node decorations may be defined in terms of unfulfilled requirements and metavariables; and trees may lack a full set of scope constraints. Anaphora resol</context>
</contexts>
<marker>Blackburn, Meyer-Viol, 1994</marker>
<rawString>P. Blackburn and W. Meyer-Viol. 1994. Linguistics, logic and finite trees. Bulletin of the IGPL, 2:3–31.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Branigan</author>
<author>M Pickering</author>
<author>A Cleland</author>
</authors>
<title>Syntactic co-ordination in dialogue.</title>
<date>2000</date>
<journal>Cognition,</journal>
<pages>75--13</pages>
<contexts>
<context position="4515" citStr="Branigan et al., 2000" startWordPosition="692" endWordPosition="695">eration models and the dialogue model that results. As an illustration of this, we show how if we drop the assumption that grammar formalisms are defined neutrally between parsing and production in favour of frameworks in which the serial nature of language processing is a central design feature (as in Dynamic Syntax: (Kempson et al., 2001)), then we can define a model in which incremental sub-systems of parsing and generation are necessarily tightly coordinated, and can thus provide a computational model of dialogue which directly corresponds with currently emerging psycholinguistic results (Branigan et al., 2000). In particular, by adding a shared model of context to previously defined word-by-word incremental parsing and generation models, we show how the switch in speaker/hearer roles during a shared utterance can be seen as a switch between processes which are directed by different goals, but which share the same incrementally built data structures. We then show how this inherent incrementality and structure/context sharing also allows a straightforward model of cross-speaker ellipsis and alignment. 2 Background Dynamic Syntax (DS) (Kempson et al., 2001) is a parsing-directed grammar formalism in w</context>
<context position="27054" citStr="Branigan et al., 2000" startWordPosition="4410" endWordPosition="4413">con. As suggested by O&amp;P, this can now lead directly to a model of alignment phenomena, characterisable as follows. For the generator, if there is some action a E (A0 U A) suitable for extending the current tree, a can be re-used, generating the word w which occupies the corresponding position in the sequence W0 or W. This results in lexical alignment – repeating w rather than choosing an alternative word from the lexicon. Alignment of syntactic structure (e.g. preserving double-object or full PP forms in the use of a verb such as give rather than shifting to the semantically equivalent form (Branigan et al., 2000)) also follows in virtue of the procedural action-based specification of lexical content. A word such as give has two possible lexical actions a&apos; and a&apos;&apos; despite semantic equivalence of output, corresponding to the two alternative forms. A previous use will cause either a&apos; or a&apos;&apos; to be present in (A0 U A); re-use of this action will cause the same form to be repeated.15 A similar definition holds for the parser: for a word w presented as input, if w E (W0UW) then the corresponding action a in the sequence A0 or A can be used without consulting the lexicon. Words will therefore be interpreted a</context>
</contexts>
<marker>Branigan, Pickering, Cleland, 2000</marker>
<rawString>H. Branigan, M. Pickering, and A. Cleland. 2000. Syntactic co-ordination in dialogue. Cognition, 75:13–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Cann</author>
</authors>
<title>Semantic underspecification and the interpretation of copular clauses in English. In Where Semantics Meets Pragmatics.</title>
<date>2003</date>
<institution>University of Michigan.</institution>
<contexts>
<context position="24775" citStr="Cann, 2003" startWordPosition="4033" endWordPosition="4034">ompiling the resulting node’s interpretation within the emergent tree.14 In (6), narrow scope can be assigned to the subject in A’s utterance via this late assignment of scope; at this late point in the 13In its re-use of actions provided by context, this approach to ellipsis is essentially similar to the glue language approach (see (Asudeh and Crouch, 2002) and papers in (Dalrymple, 1999) but, given the lack of independent syntax /semantics vocabularies, the need for an intermediate mapping language is removed. 14This pattern parallels expletive pronouns which equally allow a delayed update (Cann, 2003). {Si &lt; x} {(c, x, stud&apos;(x))} {} {WH, Q} {ignr&apos;} (5) (6) parse process, the term constructed from the object node will have been entered into the set of scope statements, allowing the subject node to be dependent on the following quantified expression. The elliptical word did in B’s utterance will then license re-use of these late actions, repeating the procedures used in interpreting A’s antecedent and so determining scope of the new subject relative to the object. Again, these analyses are possible because parsing and generation processes share incrementally built structures and contextual p</context>
</contexts>
<marker>Cann, 2003</marker>
<rawString>R. Cann. 2003. Semantic underspecification and the interpretation of copular clauses in English. In Where Semantics Meets Pragmatics. University of Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H H Clark</author>
<author>D Wilkes-Gibbs</author>
</authors>
<title>Referring as a collaborative process.</title>
<date>1986</date>
<journal>Cognition,</journal>
<pages>22--1</pages>
<marker>Clark, Wilkes-Gibbs, 1986</marker>
<rawString>H. H. Clark and D. Wilkes-Gibbs. 1986. Referring as a collaborative process. Cognition, 22:1–39.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Crocker</author>
<author>M Pickering</author>
<author>C Clifton</author>
<author>editors</author>
</authors>
<date>2000</date>
<booktitle>Architectures and Mechanisms in Sentence Comprehension.</booktitle>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="995" citStr="Crocker et al., 2000" startWordPosition="132" endWordPosition="135"> therefore be reflected by independently defined parsing and/or generation techniques. We argue that this leads to a poor setup for modelling dialogue, with its rich speaker-hearer interaction, and instead propose context-based parsing and generation models defined in terms of an inherently incremental grammar formalism (Dynamic Syntax), which allow a straightforward model of otherwise problematic dialogue phenomena such as shared utterances, ellipsis and alignment. 1 Introduction Despite increasing psycholinguistic evidence of incrementality in language processing, both in parsing (see e.g. (Crocker et al., 2000)) and in production (Ferreira, 1996), there is almost universal agreement that this should not be reflected in grammar formalisms which constitute the underlying model of language (for rare exceptions, see (Hausser, 1989; Kempson et al., 2001)). Constraint-based grammar formalisms are accordingly defined neutrally between either of these applications, with parsing/generation systems (whether incremental or not) defined as independent architectures manipulating the same underlying system.1 Such assumptions however lead to formal architectures that are relatively poorly set up for modelling dial</context>
</contexts>
<marker>Crocker, Pickering, Clifton, editors, 2000</marker>
<rawString>M. Crocker, M. Pickering, and C. Clifton, editors. 2000. Architectures and Mechanisms in Sentence Comprehension. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Dalrymple</author>
<author>S Shieber</author>
<author>F Pereira</author>
</authors>
<title>Ellipsis and higher-order unification.</title>
<date>1991</date>
<journal>Linguistics and Philosophy,</journal>
<volume>14</volume>
<issue>4</issue>
<contexts>
<context position="23155" citStr="Dalrymple et al., 1991" startWordPosition="3762" endWordPosition="3765">m taking a suitable semantic formula directly from a tree node in context: any node n E (T0 U T) of suitable type (e _* t) with no outstanding requirements. Sloppy readings involve re-use of actions: any sequence of actions (a1; a2; ... ; an) E (A0 U A) can be used (given the appropriate elliptical trigger) to extend the current tree T if this provides a formula of type e _* t.13 This latter approach, combined with the representation of quantified elements as terms, allows a range of phenomena, including those which are problematic for other (abstraction-based) approaches (for discussion see (Dalrymple et al., 1991)): A: A policeman who arrested Bill read him his rights. B: The policeman who arrested Tom did too. The actions associated with A’s use of read him his rights in (5) include the projection of a metavariable associated with him, and its resolution to the term in context associated with Bill. B’s ellipsis allows this action sequence to be re-used, again projecting a metavariable and resolving it, this time (given the new context) to the term provided by parsing Tom. This leads to a copy of Tom within the constructed predicate, and a sloppy reading. This analysis also applies to yield parallellis</context>
</contexts>
<marker>Dalrymple, Shieber, Pereira, 1991</marker>
<rawString>M. Dalrymple, S. Shieber, and F. Pereira. 1991. Ellipsis and higher-order unification. Linguistics and Philosophy, 14(4):399–452.</rawString>
</citation>
<citation valid="true">
<title>Syntax and Semantics in Lexical Functional Grammar: The Resource-Logic Approach.</title>
<date>1999</date>
<editor>M. Dalrymple, editor.</editor>
<publisher>MIT Press.</publisher>
<marker>1999</marker>
<rawString>M. Dalrymple, editor. 1999. Syntax and Semantics in Lexical Functional Grammar: The Resource-Logic Approach. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ferreira</author>
</authors>
<title>Is it better to give than to donate? Syntactic flexibility in language production.</title>
<date>1996</date>
<journal>Journal of Memory and Language,</journal>
<pages>35--724</pages>
<contexts>
<context position="1031" citStr="Ferreira, 1996" startWordPosition="139" endWordPosition="140">efined parsing and/or generation techniques. We argue that this leads to a poor setup for modelling dialogue, with its rich speaker-hearer interaction, and instead propose context-based parsing and generation models defined in terms of an inherently incremental grammar formalism (Dynamic Syntax), which allow a straightforward model of otherwise problematic dialogue phenomena such as shared utterances, ellipsis and alignment. 1 Introduction Despite increasing psycholinguistic evidence of incrementality in language processing, both in parsing (see e.g. (Crocker et al., 2000)) and in production (Ferreira, 1996), there is almost universal agreement that this should not be reflected in grammar formalisms which constitute the underlying model of language (for rare exceptions, see (Hausser, 1989; Kempson et al., 2001)). Constraint-based grammar formalisms are accordingly defined neutrally between either of these applications, with parsing/generation systems (whether incremental or not) defined as independent architectures manipulating the same underlying system.1 Such assumptions however lead to formal architectures that are relatively poorly set up for modelling dialogue, for they provide no basis for </context>
<context position="14058" citStr="Ferreira, 1996" startWordPosition="2213" endWordPosition="2215">se of a restricted multiset of lexical entries selected on the basis of goal tree features, which prevents strictly incremental generation and excludes modification of the goal tree. 7In building n-tuples of trees corresponding to predicate-argument structures, the system is similar to LTAG formalisms (Joshi and Kulick, 1997). However, unlike LTAG systems (see e.g. (Stone and Doran, 1997)), both parsing and generation are not head-driven, but fully (word-by-word) incremental. This has the advantage of allowing fully incremental models for all languages, matching psycholinguistic observations (Ferreira, 1996). 8Strictly speaking, scope statements should be included in these n-tuples – for now we consider them as part of the tree. this set can then be taken to consist of: (a) a similar triple (T0, W0, A0) given by the previous sentence, where T0 is its semantic tree representation, W0 and A0 the sequences of words and actions that were used in building it; and (b) the triple (T, W, A) itself. Once parsing is complete, the final parser state, a set of triples, will form the new starting context for the next sentence. In the simple case where the sentence is unambiguous (or all ambiguity has been rem</context>
</contexts>
<marker>Ferreira, 1996</marker>
<rawString>V. Ferreira. 1996. Is it better to give than to donate? Syntactic flexibility in language production. Journal of Memory and Language, 35:724–755.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Garrod</author>
<author>A Anderson</author>
</authors>
<title>Saying what you mean in dialogue.</title>
<date>1987</date>
<journal>Cognition,</journal>
<pages>27--181</pages>
<contexts>
<context position="27775" citStr="Garrod and Anderson, 1987" startWordPosition="4535" endWordPosition="4538">d such as give has two possible lexical actions a&apos; and a&apos;&apos; despite semantic equivalence of output, corresponding to the two alternative forms. A previous use will cause either a&apos; or a&apos;&apos; to be present in (A0 U A); re-use of this action will cause the same form to be repeated.15 A similar definition holds for the parser: for a word w presented as input, if w E (W0UW) then the corresponding action a in the sequence A0 or A can be used without consulting the lexicon. Words will therefore be interpreted as having the same sense or reference as before, modelling the semantic alignment described by (Garrod and Anderson, 1987). These characterisations can also be extended to sequences of words – a sub-sequence (a1; a2; ... ; an) E (A0 U A) can be re-used by a generator, producing the corresponding word sequence (w1; w2; ... ; wn) E (W0 U W); and similarly the sub-sequence of words (w1; w2; ... ; wn) E (W0 U W) will cause the parser to use the corresponding action sequence (a1; a2; ... ; an) E (A0 U A). This will result in sequences or phrases being repeatedly associated by both parser and generator with the same sense or reference, leading to what Pickering and Garrod (2004) call routinization (construction and re-</context>
</contexts>
<marker>Garrod, Anderson, 1987</marker>
<rawString>S. Garrod and A. Anderson. 1987. Saying what you mean in dialogue. Cognition, 27:181–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ginzburg</author>
<author>I A Sag</author>
</authors>
<title>Interrogative Investigations.</title>
<date>2000</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="21761" citStr="Ginzburg and Sag, 2000" startWordPosition="3532" endWordPosition="3535">lar forms of metavariables, so parsing a whquestion yields a type-complete but open formula, which the term presented by a subsequent fragment can update: (3) A: What did you eat for breakfast? B: Porridge. Parsing the fragment involves constructing an unfixed node, and merging it with the contextually available structure, so characterising the wellformedness/interpretation of fragment answers to questions without any additional mechanisms: the term (E, x, porridge&apos;(x)) stands in a licensed growth relation from the metavariable WH provided by the lexical actions of what. Functional questions (Ginzburg and Sag, 2000) with their fragment answers (4) pose no problem. As the wh-question contains a metavariable, the scope evaluation cannot be completed; completion of structure and evaluation of scope can then be effected by merging in the term the answer provides, identifying any introduced metavariable in this context (the genitive imposes narrow scope of the introduced epsilon term): (4) A: Who did every student ignore? B: Their supervisor. _* {Si &lt; x, x &lt; y} {(c, x, stud&apos;(x))} {} {(e, y, sup&apos;(x)(y)}{ignr&apos;} VP Ellipsis Anaphoric devices such as pronouns and VP ellipsis are analysed as decorating tree nodes </context>
</contexts>
<marker>Ginzburg, Sag, 2000</marker>
<rawString>J. Ginzburg and I. A. Sag. 2000. Interrogative Investigations. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hausser</author>
</authors>
<title>Computation of Language.</title>
<date>1989</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="1215" citStr="Hausser, 1989" startWordPosition="169" endWordPosition="170">parsing and generation models defined in terms of an inherently incremental grammar formalism (Dynamic Syntax), which allow a straightforward model of otherwise problematic dialogue phenomena such as shared utterances, ellipsis and alignment. 1 Introduction Despite increasing psycholinguistic evidence of incrementality in language processing, both in parsing (see e.g. (Crocker et al., 2000)) and in production (Ferreira, 1996), there is almost universal agreement that this should not be reflected in grammar formalisms which constitute the underlying model of language (for rare exceptions, see (Hausser, 1989; Kempson et al., 2001)). Constraint-based grammar formalisms are accordingly defined neutrally between either of these applications, with parsing/generation systems (whether incremental or not) defined as independent architectures manipulating the same underlying system.1 Such assumptions however lead to formal architectures that are relatively poorly set up for modelling dialogue, for they provide no basis for the very rich degree of interaction between participants in dialogue. A common phenomenon in dialogue is * Related papers from the point of view of generation rather than parsing, and </context>
</contexts>
<marker>Hausser, 1989</marker>
<rawString>R. Hausser. 1989. Computation of Language. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Hirschb¨uhler</author>
</authors>
<title>VP deletion and acrossthe-board quantifier scope.</title>
<date>1982</date>
<booktitle>In Proceedings of NELS 12.</booktitle>
<marker>Hirschb¨uhler, 1982</marker>
<rawString>P. Hirschb¨uhler. 1982. VP deletion and acrossthe-board quantifier scope. In Proceedings of NELS 12.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Joshi</author>
<author>S Kulick</author>
</authors>
<title>Partial proof trees as building blocks for a categorial grammar. Linguistics and Philosophy,</title>
<date>1997</date>
<pages>20--637</pages>
<contexts>
<context position="13770" citStr="Joshi and Kulick, 1997" startWordPosition="2172" endWordPosition="2175">or a particular partial tree T in 6There are other departures in the treatment of linked structures (for relatives and other modifiers) and quantification, and more relevantly to improve the incrementality of the generation process: we do not adopt the proposal of O&amp;P to speed up generation by use of a restricted multiset of lexical entries selected on the basis of goal tree features, which prevents strictly incremental generation and excludes modification of the goal tree. 7In building n-tuples of trees corresponding to predicate-argument structures, the system is similar to LTAG formalisms (Joshi and Kulick, 1997). However, unlike LTAG systems (see e.g. (Stone and Doran, 1997)), both parsing and generation are not head-driven, but fully (word-by-word) incremental. This has the advantage of allowing fully incremental models for all languages, matching psycholinguistic observations (Ferreira, 1996). 8Strictly speaking, scope statements should be included in these n-tuples – for now we consider them as part of the tree. this set can then be taken to consist of: (a) a similar triple (T0, W0, A0) given by the previous sentence, where T0 is its semantic tree representation, W0 and A0 the sequences of words a</context>
</contexts>
<marker>Joshi, Kulick, 1997</marker>
<rawString>A. Joshi and S. Kulick. 1997. Partial proof trees as building blocks for a categorial grammar. Linguistics and Philosophy, 20:637–667.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kaplan</author>
<author>A Zaenen</author>
</authors>
<title>Longdistance dependencies, constituent structure, and functional uncertainty.</title>
<date>1989</date>
<booktitle>Alternative Conceptions of Phrase Structure,</booktitle>
<pages>17--42</pages>
<editor>In M. Baltin and A. Kroch, editors,</editor>
<publisher>University of Chicago Press.</publisher>
<contexts>
<context position="8536" citStr="Kaplan and Zaenen, 1989" startWordPosition="1317" endWordPosition="1320">specified term e.g. (E, y, Man&apos;(y)) with a requirement for a scope statement. These scope statements, of the form x &lt; y (‘the term binding x is to be evaluated as taking scope over the term binding y’), are added to a locally dominating type-t-requiring node. Generally, they are added to an accumulating set following the serial order of processing in determining the scope dependency, but indefinites (freer in scope potential) are assigned a metavariable as first argument, allow3In this, the system is also like LFG, modelling longdistance dependency in the same terms as functional uncertainty (Kaplan and Zaenen, 1989), differing from that concept in the dynamics of update internal to the construction of a single tree. Figure 1: Parsing “john likes mary” . . . . . . and generating “john likes mary” {john&apos;} {} likes mary {like&apos;(mary&apos;)(john&apos;), } {john&apos;} {like&apos;(mary&apos;)} {like&apos;} {mary&apos;} {} FAIL FAIL john likes mary mary {} {john&apos;} {} {like&apos;} {} FAIL john {} {john&apos;} {} {like&apos;(mary&apos;)(john&apos;), } {john&apos;} {like&apos;(mary&apos;)} {like&apos;} {mary&apos;} likes mary {} {john&apos;} {} {like&apos;} {} ing selection from any term already added, including temporally-sorted variables associated with tense/modality specifications. The general mec</context>
</contexts>
<marker>Kaplan, Zaenen, 1989</marker>
<rawString>R. Kaplan and A. Zaenen. 1989. Longdistance dependencies, constituent structure, and functional uncertainty. In M. Baltin and A. Kroch, editors, Alternative Conceptions of Phrase Structure, pages 17–42. University of Chicago Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kempson</author>
<author>W Meyer-Viol</author>
<author>D Gabbay</author>
</authors>
<title>Dynamic Syntax: The Flow of Language Understanding.</title>
<date>2001</date>
<publisher>Blackwell.</publisher>
<contexts>
<context position="1238" citStr="Kempson et al., 2001" startWordPosition="171" endWordPosition="174">eration models defined in terms of an inherently incremental grammar formalism (Dynamic Syntax), which allow a straightforward model of otherwise problematic dialogue phenomena such as shared utterances, ellipsis and alignment. 1 Introduction Despite increasing psycholinguistic evidence of incrementality in language processing, both in parsing (see e.g. (Crocker et al., 2000)) and in production (Ferreira, 1996), there is almost universal agreement that this should not be reflected in grammar formalisms which constitute the underlying model of language (for rare exceptions, see (Hausser, 1989; Kempson et al., 2001)). Constraint-based grammar formalisms are accordingly defined neutrally between either of these applications, with parsing/generation systems (whether incremental or not) defined as independent architectures manipulating the same underlying system.1 Such assumptions however lead to formal architectures that are relatively poorly set up for modelling dialogue, for they provide no basis for the very rich degree of interaction between participants in dialogue. A common phenomenon in dialogue is * Related papers from the point of view of generation rather than parsing, and from the point of view </context>
<context position="4235" citStr="Kempson et al., 2001" startWordPosition="651" endWordPosition="654">f evaluating both putative grammar formalisms and 2Example (1) from the BNC, file KNY (sentences 315–317). (1) models of language use. In response to this challenge, we suggest that an alternative means of evaluating parsing implementations is by evaluation of paired parsing-generation models and the dialogue model that results. As an illustration of this, we show how if we drop the assumption that grammar formalisms are defined neutrally between parsing and production in favour of frameworks in which the serial nature of language processing is a central design feature (as in Dynamic Syntax: (Kempson et al., 2001)), then we can define a model in which incremental sub-systems of parsing and generation are necessarily tightly coordinated, and can thus provide a computational model of dialogue which directly corresponds with currently emerging psycholinguistic results (Branigan et al., 2000). In particular, by adding a shared model of context to previously defined word-by-word incremental parsing and generation models, we show how the switch in speaker/hearer roles during a shared utterance can be seen as a switch between processes which are directed by different goals, but which share the same incrementa</context>
<context position="6427" citStr="Kempson et al., 2001" startWordPosition="989" endWordPosition="992">95) for a recent development), so quantification is expressed through terms of type e whose complexity is reflected in evaluation procedures that apply to propositional formulae once constructed, and not in the tree itself. With all quantification expressed as type e terms, the standard grounds for mismatch between syntactic and semantic analysis for all NPs is removed; and, indeed, all syntactic distributions are explained in terms of this incremental and monotonic growth of partial representations of content. Hence the claim that the model itself constitutes a NL grammar formalism. Parsing (Kempson et al., 2001) defines parsing as a process of building labelled semantic trees in a strictly left-to-right, word-by-word incremental fashion by using computational actions and lexical actions defined (for some natural language) using the modal tree logic LOFT (Blackburn and Meyer-Viol, 1994). These actions are defined as transition functions between intermediate states, which monotonically extend tree structures and node decorations. Words are specified in the lexicon to have associated lexical actions: the (possibly partial) semantic trees are monotonically extended by applying these actions as each word </context>
<context position="10169" citStr="Kempson et al., 2001" startWordPosition="1581" endWordPosition="1584">evaluation), then evaluated as Man&apos;(a) n Cough&apos;(a) where a = (E, x, Man&apos;(x) n Cough&apos;(x)).4 Once all requirements are satisfied and all partiality and underspecification is resolved, trees are complete, parsing is successful and the input string is said to be grammatical. Central to the formalism is the incremental and monotonic growth of labelled partial trees: the parser state at any point contains all the partial trees which have been produced by the portion of the string so far consumed and which remain candidates for completion.5 4For formal details of this approach to quantification see (Kempson et al., 2001) chapter 7; for an early implementation see (Kibble et al., 2001). 5Figure 1 assumes, simplistically, that linguistic names correspond directly to scopeless names in the logGeneration (Otsuka and Purver, 2003; Purver and Otsuka, 2003) (hereafter O&amp;P) give an initial method of context-independent tactical generation based on the same incremental parsing process, in which an output string is produced according to an input semantic tree, the goal tree. The generator incrementally produces a set of corresponding output strings and their associated partial trees (again, on a left-to-right, word-by-</context>
</contexts>
<marker>Kempson, Meyer-Viol, Gabbay, 2001</marker>
<rawString>R. Kempson, W. Meyer-Viol, and D. Gabbay. 2001. Dynamic Syntax: The Flow of Language Understanding. Blackwell.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kibble</author>
<author>W Meyer-Viol</author>
<author>D Gabbay</author>
<author>R Kempson</author>
</authors>
<title>Epsilon terms: a labelled deduction account.</title>
<date>2001</date>
<editor>In H. Bunt and R. Muskens, editors, Computing Meaning.</editor>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="10234" citStr="Kibble et al., 2001" startWordPosition="1593" endWordPosition="1596">, Man&apos;(x) n Cough&apos;(x)).4 Once all requirements are satisfied and all partiality and underspecification is resolved, trees are complete, parsing is successful and the input string is said to be grammatical. Central to the formalism is the incremental and monotonic growth of labelled partial trees: the parser state at any point contains all the partial trees which have been produced by the portion of the string so far consumed and which remain candidates for completion.5 4For formal details of this approach to quantification see (Kempson et al., 2001) chapter 7; for an early implementation see (Kibble et al., 2001). 5Figure 1 assumes, simplistically, that linguistic names correspond directly to scopeless names in the logGeneration (Otsuka and Purver, 2003; Purver and Otsuka, 2003) (hereafter O&amp;P) give an initial method of context-independent tactical generation based on the same incremental parsing process, in which an output string is produced according to an input semantic tree, the goal tree. The generator incrementally produces a set of corresponding output strings and their associated partial trees (again, on a left-to-right, word-by-word basis) by following standard parsing routines and using the </context>
</contexts>
<marker>Kibble, Meyer-Viol, Gabbay, Kempson, 2001</marker>
<rawString>R. Kibble, W. Meyer-Viol, D. Gabbay, and R. Kempson. 2001. Epsilon terms: a labelled deduction account. In H. Bunt and R. Muskens, editors, Computing Meaning. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
</authors>
<title>Deterministic parsing and description theory.</title>
<date>1987</date>
<booktitle>Linguistic Theory and Computer Applications,</booktitle>
<pages>69--112</pages>
<editor>In P. Whitelock et al., editor,</editor>
<publisher>Academic Press.</publisher>
<contexts>
<context position="7737" citStr="Marcus, 1987" startWordPosition="1188" endWordPosition="1189">e only partially specified; node decorations may be defined in terms of unfulfilled requirements and metavariables; and trees may lack a full set of scope constraints. Anaphora resolution is a familiar case of update: pronouns are defined to project metavariables that are substituted from context as part of the construction process. Relative to the same tree-growth dynamics, longdistance dependency effects are characterised through restricted licensing of partial trees with relation between nodes introduced with merely a constraint on some fixed extension (following D-Tree grammar formalisms (Marcus, 1987)), an underspecification that gets resolved within the left-to-right construction process.3 Quantifying terms are also built up using determiner and noun to yield a partially specified term e.g. (E, y, Man&apos;(y)) with a requirement for a scope statement. These scope statements, of the form x &lt; y (‘the term binding x is to be evaluated as taking scope over the term binding y’), are added to a locally dominating type-t-requiring node. Generally, they are added to an accumulating set following the serial order of processing in determining the scope dependency, but indefinites (freer in scope potent</context>
</contexts>
<marker>Marcus, 1987</marker>
<rawString>M. Marcus. 1987. Deterministic parsing and description theory. In P. Whitelock et al., editor, Linguistic Theory and Computer Applications, pages 69–112. Academic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Meyer-Viol</author>
</authors>
<title>Instantial Logic.</title>
<date>1995</date>
<tech>Ph.D. thesis,</tech>
<publisher>MIT Press.</publisher>
<institution>University of</institution>
<contexts>
<context position="5809" citStr="Meyer-Viol, 1995" startWordPosition="894" endWordPosition="895">string is incrementally projected following the left-right sequence of the words. Importantly, this tree is not a model of syntactic structure, but is strictly semantic, being a representation of the predicateargument structure of the sentence. In DS, grammaticality is defined as parsability (the successful incremental construction of a treestructure logical form, using all the information given by the words in sequence): there is no central use-neutral grammar of the kind assumed by most approaches to parsing and/or generation. The logical forms are lambda terms of the epsilon calculus (see (Meyer-Viol, 1995) for a recent development), so quantification is expressed through terms of type e whose complexity is reflected in evaluation procedures that apply to propositional formulae once constructed, and not in the tree itself. With all quantification expressed as type e terms, the standard grounds for mismatch between syntactic and semantic analysis for all NPs is removed; and, indeed, all syntactic distributions are explained in terms of this incremental and monotonic growth of partial representations of content. Hence the claim that the model itself constitutes a NL grammar formalism. Parsing (Kem</context>
</contexts>
<marker>Meyer-Viol, 1995</marker>
<rawString>W. Meyer-Viol. 1995. Instantial Logic. Ph.D. thesis, University of Utrecht. R. Millikan. 2004. The Varieties of Meaning. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Neumann</author>
</authors>
<title>A Uniform Computational Model for Natural Language Parsing and Generation.</title>
<date>1994</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit¨at des Saarlandes.</institution>
<contexts>
<context position="2053" citStr="Neumann, 1994" startWordPosition="295" endWordPosition="297">ectures manipulating the same underlying system.1 Such assumptions however lead to formal architectures that are relatively poorly set up for modelling dialogue, for they provide no basis for the very rich degree of interaction between participants in dialogue. A common phenomenon in dialogue is * Related papers from the point of view of generation rather than parsing, and from the point of view of alignment rather than incrementality, are to be presented at INLG’04 and Catalog’04 respectively. 1Authors vary as to the extent to which these architectures might be defined to be reversible. See (Neumann, 1994). that of shared utterances (Clark and WilkesGibbs, 1986), with exchange of roles of parser and producer midway through an utterance:2 Daniel: Why don’t you stop mumbling and Marc: Speak proper like? Daniel: speak proper? (2) Ruth: What did Alex ... Hugh: Design? A kaleidoscope. Such utterances clearly show the need for a strictly incremental model: however, they are particularly problematic for any overall architecture in which parsing and generation are independently defined as applications of a useneutral grammar formalism which yields as output the set of well-formed strings, for in these </context>
</contexts>
<marker>Neumann, 1994</marker>
<rawString>G. Neumann. 1994. A Uniform Computational Model for Natural Language Parsing and Generation. Ph.D. thesis, Universit¨at des Saarlandes.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Otsuka</author>
<author>M Purver</author>
</authors>
<title>Incremental generation by incremental parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 6th CLUK Colloquium.</booktitle>
<contexts>
<context position="10377" citStr="Otsuka and Purver, 2003" startWordPosition="1613" endWordPosition="1616">ing is successful and the input string is said to be grammatical. Central to the formalism is the incremental and monotonic growth of labelled partial trees: the parser state at any point contains all the partial trees which have been produced by the portion of the string so far consumed and which remain candidates for completion.5 4For formal details of this approach to quantification see (Kempson et al., 2001) chapter 7; for an early implementation see (Kibble et al., 2001). 5Figure 1 assumes, simplistically, that linguistic names correspond directly to scopeless names in the logGeneration (Otsuka and Purver, 2003; Purver and Otsuka, 2003) (hereafter O&amp;P) give an initial method of context-independent tactical generation based on the same incremental parsing process, in which an output string is produced according to an input semantic tree, the goal tree. The generator incrementally produces a set of corresponding output strings and their associated partial trees (again, on a left-to-right, word-by-word basis) by following standard parsing routines and using the goal tree as a subsumption check. At each stage, partial strings and trees are tentatively extended using some word/action pair from the lexico</context>
</contexts>
<marker>Otsuka, Purver, 2003</marker>
<rawString>M. Otsuka and M. Purver. 2003. Incremental generation by incremental parsing. In Proceedings of the 6th CLUK Colloquium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Pickering</author>
<author>S Garrod</author>
</authors>
<title>Toward a mechanistic psychology of dialogue.</title>
<date>2004</date>
<journal>Behavioral and Brain Sciences, forthcoming.</journal>
<contexts>
<context position="3261" citStr="Pickering and Garrod, 2004" startWordPosition="493" endWordPosition="496">ings, for in these types of exchange, the string uttered by one and parsed by the other need not be a wellformed string in its own right, so will not fall within the set of data which the underlying formalism is set up to capture. Yet, with the transition between interlocutors seen as a shift from one system to another, each such substring will have to be characterised independently. Many other dialogue phenomena also show the need for interaction between the parsing and generation processes, among them cross-speaker ellipsis (e.g. simple bare fragment answers to wh-questions), and alignment (Pickering and Garrod, 2004), in which conversational participants mirror each other’s patterns at many levels (including lexical choice and syntactic structure). The challenge of being able to model these phenomena, problematic for theorists but extremely easy and natural for dialogue participants themselves, has recently been put out by Pickering and Garrod (2004) as a means of evaluating both putative grammar formalisms and 2Example (1) from the BNC, file KNY (sentences 315–317). (1) models of language use. In response to this challenge, we suggest that an alternative means of evaluating parsing implementations is by </context>
<context position="28334" citStr="Pickering and Garrod (2004)" startWordPosition="4641" endWordPosition="4644">ling the semantic alignment described by (Garrod and Anderson, 1987). These characterisations can also be extended to sequences of words – a sub-sequence (a1; a2; ... ; an) E (A0 U A) can be re-used by a generator, producing the corresponding word sequence (w1; w2; ... ; wn) E (W0 U W); and similarly the sub-sequence of words (w1; w2; ... ; wn) E (W0 U W) will cause the parser to use the corresponding action sequence (a1; a2; ... ; an) E (A0 U A). This will result in sequences or phrases being repeatedly associated by both parser and generator with the same sense or reference, leading to what Pickering and Garrod (2004) call routinization (construction and re-use of word sequences with consistent meanings). It is notable that these various patterns of alignment, said by Pickering and Garrod (2004) to be alignment across different levels, are expressible without invoking distinct levels of syntactic or lexical structure, since context, content and lexical actions are all defined in terms of the same tree configurations. 7 Summary The inherent left-to-right incrementality and monotonicity of DS as a grammar formalism allows both parsing and generation processes to be not only incremental but closely coupled, s</context>
</contexts>
<marker>Pickering, Garrod, 2004</marker>
<rawString>M. Pickering and S. Garrod. 2004. Toward a mechanistic psychology of dialogue. Behavioral and Brain Sciences, forthcoming.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Poesio</author>
<author>H Rieser</author>
</authors>
<title>Coordination in a PTT approach to dialogue.</title>
<date>2003</date>
<booktitle>In Proceedings of the 7th Workshop on the Semantics and Pragmatics of Dialogue (DiaBruck).</booktitle>
<contexts>
<context position="19025" citStr="Poesio and Rieser (2003)" startWordPosition="3067" endWordPosition="3070">ssed must be subsumed by the incomplete proposition that has been built so far by the parser). Constructing 10We have little to say about exactly when transitions occur. Presumably speaker pauses and the availability to the hearer of a possible goal tree both play a part. 11Figure 2 contains several simplifications to aid readability, both to tree structure details and by showing parser/generator states as single triples/pairs rather than sets thereof. Tg prior to the generation task will often be a complex process involving inference and/or abduction over context and world/domain knowledge – Poesio and Rieser (2003) give some idea as to how this inference might be possible – for now, we make the simplifying assumption that a suitable propositional structure is available. Transition from Speaker to Hearer At transition, the initial speaker B’s generator state G&apos;t contains the pair (5t, Pt), where 5t is the partial string output so far, and Pt� is the corresponding parser state, the transition state as far as B is concerned.12 In order for B to interpret A’s continuation, B need only use Pt� as the initial parser state which is extended as the string produced by A is consumed. As there will usually be mult</context>
</contexts>
<marker>Poesio, Rieser, 2003</marker>
<rawString>M. Poesio and H. Rieser. 2003. Coordination in a PTT approach to dialogue. In Proceedings of the 7th Workshop on the Semantics and Pragmatics of Dialogue (DiaBruck).</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Purver</author>
<author>M Otsuka</author>
</authors>
<title>Incremental generation by incremental parsing: Tactical generation in Dynamic Syntax.</title>
<date>2003</date>
<booktitle>In Proceedings of the 9th European Workshop in Natural Language Generation (ENLG-2003).</booktitle>
<contexts>
<context position="10403" citStr="Purver and Otsuka, 2003" startWordPosition="1617" endWordPosition="1620"> input string is said to be grammatical. Central to the formalism is the incremental and monotonic growth of labelled partial trees: the parser state at any point contains all the partial trees which have been produced by the portion of the string so far consumed and which remain candidates for completion.5 4For formal details of this approach to quantification see (Kempson et al., 2001) chapter 7; for an early implementation see (Kibble et al., 2001). 5Figure 1 assumes, simplistically, that linguistic names correspond directly to scopeless names in the logGeneration (Otsuka and Purver, 2003; Purver and Otsuka, 2003) (hereafter O&amp;P) give an initial method of context-independent tactical generation based on the same incremental parsing process, in which an output string is produced according to an input semantic tree, the goal tree. The generator incrementally produces a set of corresponding output strings and their associated partial trees (again, on a left-to-right, word-by-word basis) by following standard parsing routines and using the goal tree as a subsumption check. At each stage, partial strings and trees are tentatively extended using some word/action pair from the lexicon; only those candidates w</context>
</contexts>
<marker>Purver, Otsuka, 2003</marker>
<rawString>M. Purver and M. Otsuka. 2003. Incremental generation by incremental parsing: Tactical generation in Dynamic Syntax. In Proceedings of the 9th European Workshop in Natural Language Generation (ENLG-2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>F Pereira</author>
<author>M Dalrymple</author>
</authors>
<title>Interactions of scope and ellipsis. Linguistics and Philosophy,</title>
<date>1996</date>
<contexts>
<context position="23819" citStr="Shieber et al., 1996" startWordPosition="3875" endWordPosition="3878">m his rights. B: The policeman who arrested Tom did too. The actions associated with A’s use of read him his rights in (5) include the projection of a metavariable associated with him, and its resolution to the term in context associated with Bill. B’s ellipsis allows this action sequence to be re-used, again projecting a metavariable and resolving it, this time (given the new context) to the term provided by parsing Tom. This leads to a copy of Tom within the constructed predicate, and a sloppy reading. This analysis also applies to yield parallellism effects in scoping (Hirschb¨uhler, 1982; Shieber et al., 1996), allowing narrow scope construal for indefinites in subject position: A: A nurse interviewed every patient. B: An orderly did too. Resolution of the underspecification in the scope statement associated with an indefinite can be performed at two points: either at the immediate point of processing the lexical actions, or at the later point of compiling the resulting node’s interpretation within the emergent tree.14 In (6), narrow scope can be assigned to the subject in A’s utterance via this late assignment of scope; at this late point in the 13In its re-use of actions provided by context, this</context>
</contexts>
<marker>Shieber, Pereira, Dalrymple, 1996</marker>
<rawString>S. Shieber, F. Pereira, and M. Dalrymple. 1996. Interactions of scope and ellipsis. Linguistics and Philosophy, 19:527–552.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Stone</author>
<author>C Doran</author>
</authors>
<title>Sentence planning as description using tree-adjoining grammar.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the ACL,</booktitle>
<pages>198--205</pages>
<contexts>
<context position="13834" citStr="Stone and Doran, 1997" startWordPosition="2182" endWordPosition="2185">the treatment of linked structures (for relatives and other modifiers) and quantification, and more relevantly to improve the incrementality of the generation process: we do not adopt the proposal of O&amp;P to speed up generation by use of a restricted multiset of lexical entries selected on the basis of goal tree features, which prevents strictly incremental generation and excludes modification of the goal tree. 7In building n-tuples of trees corresponding to predicate-argument structures, the system is similar to LTAG formalisms (Joshi and Kulick, 1997). However, unlike LTAG systems (see e.g. (Stone and Doran, 1997)), both parsing and generation are not head-driven, but fully (word-by-word) incremental. This has the advantage of allowing fully incremental models for all languages, matching psycholinguistic observations (Ferreira, 1996). 8Strictly speaking, scope statements should be included in these n-tuples – for now we consider them as part of the tree. this set can then be taken to consist of: (a) a similar triple (T0, W0, A0) given by the previous sentence, where T0 is its semantic tree representation, W0 and A0 the sequences of words and actions that were used in building it; and (b) the triple (T,</context>
</contexts>
<marker>Stone, Doran, 1997</marker>
<rawString>M. Stone and C. Doran. 1997. Sentence planning as description using tree-adjoining grammar. In Proceedings of the 35th Annual Meeting of the ACL, pages 198–205.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>