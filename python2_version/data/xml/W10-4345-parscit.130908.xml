<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.136058">
<title confidence="0.993168">
I’ve said it before, and I’ll say it again: An empirical investigation of the
upper bound of the selection approach to dialogue
</title>
<author confidence="0.944738">
Sudeep Gandhe and David Traum
</author>
<affiliation confidence="0.778454">
Institute for Creative Technologies
13274 Fiji way, Marina del Rey, CA 90292
</affiliation>
<email confidence="0.9994">
{gandhe,traum}@ict.usc.edu
</email>
<sectionHeader confidence="0.997397" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999909666666667">
We perform a study of existing dialogue
corpora to establish the theoretical max-
imum performance of the selection ap-
proach to simulating human dialogue be-
havior in unseen dialogues. This maxi-
mum is the proportion of test utterances
for which an exact or approximate match
exists in the corresponding training cor-
pus. The results indicate that some do-
mains seem quite suitable for a corpus-
based selection approach, with over half of
the test utterances having been seen before
in the corpus, while other domains show
much more novelty compared to previous
dialogues.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.993561587301588">
There are two main approaches toward automat-
ically producing dialogue utterances. One is the
selection approach, in which the task is to pick
the appropriate output from a corpus of possible
outputs. The other is the generation approach, in
which the output is dynamically assembled using
some composition procedure, e.g. grammar rules
used to convert information from semantic repre-
sentations and/or context to text.
The generation approach has the advantage of
a more compact representation for a given gener-
ative capacity. But for any finite set of sentences
produced, the selection approach could perfectly
simulate the generation approach. The generation
approach generally requires more analytical effort
to devise a good set of grammar rules that cover
the range of desired sentences but do not admit un-
desirable or unnatural sentences. Whereas, in the
selection approach, outputs can be limited to those
that have been observed in human speech. This
affords complex and human-like sentences with-
out much detailed analysis. Moreover, when the
output is not just text but presented as speech, the
system may easily use recorded audio clips rather
than speech synthesis. This argument also extends
to multi-modal performances, e.g. using artist an-
imation motion capture or recorded video for an-
imating virtual human dialogue characters. Often
one is willing to sacrifice some generality in or-
der to achieve more human-like behavior than is
currently possible from generation approaches.
The selection approach has been used for a
number of dialogue agents, including question-
answering characters at ICT (Leuski et al., 2006;
Artstein et al., 2009; Kenny et al., 2007), FAQ
bots (Zukerman and Marom, 2006; Sellberg and
J¨onsson, 2008) and web-site information charac-
ters. It is also possible to use the selection ap-
proach as a part of the process, e.g. from words to
a semantic representation or from a semantic rep-
resentation to words, while using other approaches
for other parts of dialogue processing.
The selection approach presents two challenges
for finding an appropriate utterance:
o Is there a good enough utterance to select?
o How good is the selection algorithm at find-
ing this utterance?
We have previously attempted to address the sec-
ond question, by proposing the information or-
dering task for evaluating dialogue coherence
(Gandhe and Traum, 2008). Here we try to ad-
dress the first question, which would provide a
theoretical upper bound in quality for any selec-
tion approach. We examine a number of different
dialogue corpora as to their suitability for the se-
lection approach.
We make the following assumptions to allow
automatic evaluation across a range of corpora.
Actual human dialogues represent a gold-standard
for computer systems to emulate; i.e. choosing an
actual utterance in the correct place is the best pos-
sible result. Other utterances can be evaluated as
to how close they come to the original utterance,
</bodyText>
<subsubsectionHeader confidence="0.677666">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 245–248,
</subsubsectionHeader>
<affiliation confidence="0.891017">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999589">
245
</page>
<bodyText confidence="0.99856075">
using a similarity metric.
Our methodology is to examine a test corpus of
human dialogue utterances to see how well a se-
lection approach could approximate these, given a
training corpus of utterances in that domain. We
look at exact matches as well as utterances having
their similarity score above a threshold. We in-
vestigate the effect of the size of training corpora,
which lets us know how much data we might need
to achieve a certain level of performance. We also
investigate the effect of domain of training cor-
pora.
</bodyText>
<sectionHeader confidence="0.981783" genericHeader="method">
2 Dialogue Corpora
</sectionHeader>
<bodyText confidence="0.999985411764706">
We examine human dialogue utterances from a va-
riety of domains. Our initial set contains six dia-
logue corpora from ICT as well as three other pub-
licly available corpora.
SGT Blackwell is a question-answering char-
acter who answers questions about the U.S. Army,
himself, and his technology. The corpus con-
sists of visitors interacting with SGT Blackwell at
an exhibition booth at a museum. SGT Star is
a question-answering character, like SGT Black-
well, who talks about careers in the U.S. Army.
The corpus consists of trained handlers present-
ing the system. Amani is a bargaining character
used as a prototype for training soldiers to perform
tactical questioning. The SASO system is a ne-
gotiation training prototype in which two virtual
characters negotiate with a human “trainee” about
moving a medical clinic. The Radiobots system is
a training prototype that responds to military calls
for artillery fire. IOTA is an extension of the Ra-
diobots system. The corpus consists of training
sessions between a human trainee and a human in-
structor on a variety of missions. Yao et al. (2010)
provides details about the ICT corpora.
Other corpora involved dialogues between
two people playing specific roles in planning,
scheduling problem for railroad transportation,
the Trains-93 corpus (Heeman and Allen, 1994)
and for emergency services, the Monroe corpus
(Stent, 2000). The Switchboard corpus (Godfrey
et al., 1992) consists of telephone conversations
between two people, based on provided topics.
We divided the data from each corpus into a
training set and a test set, as shown in Table 1. The
data consists of utterances from one or more hu-
man speakers who engage in dialogue with either
virtual characters (Radiobots, Blackwell, Amani,
Star, SASO) or other humans (Switchboard, Mon-
roe, IOTA, Trains-93). These corpora differ along
a number of dimensions such as the size of the
corpus, dialogue genre (question-answering, task-
oriented or conversational), types of tasks (ar-
tillery calls, moving and scheduling resources, in-
formation seeking) and motivation of the partici-
pants (exploring a new technology – SGT Black-
well , presenting a demo – SGT Star, undergo-
ing training – Amani, IOTA or simply for collect-
ing the corpus – Switchboard, Trains-93, Monroe).
While the set of corpora we include does not cover
all points in these dimensions, it does present an
interesting range.
</bodyText>
<sectionHeader confidence="0.998242" genericHeader="method">
3 Dialogue Utterance Similarity Metrics
</sectionHeader>
<bodyText confidence="0.999871257142857">
To answer the question of whether an adequate
utterance exists in our training corpus that could
be selected and used, we need an appropriate-
ness measure. We assume that an utterance pro-
duced by a human in a dialogue is appropriate,
and thus the problem becomes one of construct-
ing an appropriate similarity function to compare
the human-produced utterance with the utterances
available from the training corpus. Given a train-
ing corpus Utrain and a similarity function f ,
we calculate the score for a test utterance ut as,
maxsimf(ut) = maxi f(ut, ui); ui E Utrain
There are several choices for the utterance simi-
larity function f . Ideally such a function would
take meaning and context into account rather than
just surface similarity, but these aspects are harder
to automate, so for our initial experiments we look
at several surface metrics, as described below.
Exact measure returns 1 if the utterances are ex-
actly same and 0 otherwise. 1-WER, a similar-
ity measure related to word error rate, is defined
as min (0,1 − levenshtein(ut, ui)/length(ut)).
METEOR (Lavie and Denkowski, 2009), one of
the automatic evaluation metrics used in machine
translation is a good candidate for f. METEOR
finds optimal word-to-word alignment between
test and reference strings based on several modules
that match exact words, stemmed words and syn-
onyms. METEOR is a tunable metric and for our
analysis we used the default parameters tuned for
the Adequacy &amp; Fluency task. All previous mea-
sures take into account the word ordering of test
and reference strings. In contrast, document simi-
larity measures used in information retrieval gen-
erally follow the bag of words assumption, where a
</bodyText>
<page confidence="0.997044">
246
</page>
<table confidence="0.9999335">
Domain Train Test mean(maxsim f) % of utterances
# utt words # utt words MET - 1-WER Dice Cosine Exact METEOR &gt; 0.8
EOR &gt; 0.9
Blackwell 17755 84.7k 2500 12.0k 0.913 0.878 0.917 0.921 69.6 75.8 82.1
Radiobots 995 6.8k 155 1.2k 0.905 0.864 0.920 0.924 53.6 67.7 83.2
SGT Star 2974 16.6k 400 2.2k 0.897 0.860 0.906 0.911 65.0 70.5 78.0
SASO 3602 23.3k 510 3.6k 0.821 0.742 0.830 0.837 38.4 48.6 62.6
IOTA 4935 50.4k 650 5.6k 0.768 0.697 0.800 0.808 36.2 42.8 51.4
Trains 93 5554 47.2k 745 6.0k 0.729 0.633 0.758 0.769 34.5 36.9 42.8
SWBD1 19741 138.2k 3173 21.5k 0.716 0.628 0.736 0.753 35.8 37.9 44.2
Amani 1455 15.8k 182 1.9k 0.675 0.562 0.694 0.706 18.7 25.8 30.8
Monroe 5765 43.0k 917 8.8k 0.594 0.491 0.639 0.658 22.3 23.6 26.1
</table>
<tableCaption confidence="0.999955">
Table 1: Corpus details and within domain results
</tableCaption>
<bodyText confidence="0.998425">
string is converted to a set of tokens. Here we also
considered Cosine and Dice coefficients using the
standard boolean model. In our experiments, the
surface text was normalized and all punctuation
was removed.
</bodyText>
<sectionHeader confidence="0.999902" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.947839">
Results Within a Domain
</subsectionHeader>
<bodyText confidence="0.999560692307692">
In our first experiment, we computed maxsimf
scores for all test corpus utterances in a given
domain using the training utterances from the
same domain. For the domains Blackwell, SGT
Star, SASO, Amani &amp; Radiobots which are imple-
mented dialogue systems our corpus consists of
user utterances only. For Trains 93 and Monroe
corpora, we make sure to match the speaker roles
for ut and ui. For Switchboard, where speakers
do not have any special roles and for IOTA, where
the speaker information was not readily accessi-
ble, we ignore the speaker information and select
utterances from either speaker.
Table 1 reports the mean of maxsimf scores.
These can be interpreted as the expectation of
maxsimf score for a new test utterance. The
higher this expectation, the more likely it is that
an utterance similar to the new one has been
seen before and thus the domain will be more
amenable to selection approaches. This table
also shows the percentage of utterances that had
a maxsimMeteor score above a certain thresh-
old. The correlation between maxsimf for dif-
ferent choices of f (except Exact match) is very
high (Pearson’s r &gt; 0.94). The histogram anal-
ysis shows that SGT Star, Blackwell, Radiobots
</bodyText>
<footnote confidence="0.994833666666667">
1Switchboard (SWBD) is a very large corpus and for run-
ning our experiments in a reasonable computing time we only
selected a small portion of it.
</footnote>
<figureCaption confidence="0.921999">
Figure 1: maxsimMeteor vs # utterances in train-
ing data for different domains
</figureCaption>
<bodyText confidence="0.952592428571429">
and SASO domains are better suited for selec-
tion approaches. Domains like Trains-93, Monroe,
Switchboard and Amani have a more diffuse dis-
tribution and are not best suited for selection ap-
proaches, at least with the amount of data we have
available. The IOTA domain falls somewhere in
between these two domain classes.
</bodyText>
<subsectionHeader confidence="0.99939">
Effect of Training Data Size
</subsectionHeader>
<bodyText confidence="0.9998764">
Figure 1 shows the effect of training data size
on the maxsimMeteor score. Radiobots shows
very high scores even for small amounts of train-
ing data. SGT Star and SGT Blackwell also con-
verge fairly early. Switchboard, on the other hand,
does not achieve very high scores even with a
large number of utterances. For all domains, with
around 2500 training utterances maxsimMeteor
reaches 90% of its maximum possible value for
the training set.
</bodyText>
<subsectionHeader confidence="0.885596">
Comparing Different Domains
</subsectionHeader>
<bodyText confidence="0.999569333333333">
In order to understand the similarities be-
tween different dialogue domains, we computed
maxsimMeteor for a test domain using training
</bodyText>
<page confidence="0.993397">
247
</page>
<table confidence="0.999878416666667">
Training Domains
IOTA Radio- SGT Black- Amani SASO Trains- Monroe SWBD
bots Star well 93
Testing Domains IOTA 0.768 0.440 0.247 0.334 0.196 0.242 0.255 0.297 0.334
Radiobots 0.842 0.905 0.216 0.259 0.161 0.183 0.222 0.270 0.284
SGT Star 0.324 0.136 0.897 0.622 0.372 0.438 0.339 0.417 0.527
Blackwell 0.443 0.124 0.671 0.913 0.507 0.614 0.424 0.534 0.696
Amani 0.393 0.134 0.390 0.561 0.675 0.478 0.389 0.420 0.509
SASO 0.390 0.125 0.341 0.516 0.459 0.821 0.443 0.454 0.541
Trains 93 0.434 0.112 0.214 0.468 0.272 0.429 0.753 0.627 0.557
Monroe 0.409 0.119 0.217 0.428 0.276 0.404 0.534 0.630 0.557
SWBD 0.368 0.110 0.280 0.490 0.362 0.383 0.562 0.599 0.716
</table>
<tableCaption confidence="0.992787">
Table 2: Mean of maxsimMeteor for comparing different dialogue domains. The bold-faced values are
</tableCaption>
<bodyText confidence="0.983474">
the highest in the corresponding row.
sets from other domains. In this exercise, we ig-
nored the speaker information. Table 2 reports
the mean values of maxsimMeteor for different
training domains. For all the testing domains,
using the training corpus from the same domain
produces the best results. Notice that Radiobots
also has good performance with the IOTA train-
ing data. This is as expected since IOTA is an
extension of Radiobots and should cover a lot of
utterances from the Radiobots domain. Switch-
board and Blackwell training corpora have a over-
all higher score for all testing domains. This may
be due to the breadth and size of these corpora. On
the other extreme, the Radiobots training domain
performs very poorly on all testing domains other
than itself.
</bodyText>
<sectionHeader confidence="0.999831" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.999995764705882">
We have examined how well suited a corpus-
based selection approach to dialogue can succeed
at mimicking human dialogue performance across
a range of domains. The results show that such an
approach has the potential of doing quite well for
some domains, but much less well for others. Re-
sults also show that for some domains, quite mod-
est amounts of training data are needed for this
operation. Applying this method across corpora
from different domains can also give us a simi-
larity metric for dialogue domains. Our hope is
that this kind of analysis can help inform the de-
cision of what kind of language processing meth-
ods and dialogue architectures are most appropri-
ate for building a dialogue system for a new do-
main, particularly one in which the system is to
act like a human.
</bodyText>
<sectionHeader confidence="0.999519" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.974525">
This work has been sponsored by the U.S. Army Re-
search, Development, and Engineering Command (RDE-
COM). Statements and opinions expressed do not necessarily
reflect the position or the policy of the United States Gov-
ernment, and no official endorsement should be inferred. We
would like to thank Ron Artstein and others at ICT for com-
piling the ICT Corpora used in this study.
</bodyText>
<sectionHeader confidence="0.998636" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9997633125">
R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and D. Traum.
2009. Semi-formal evaluation of conversational charac-
ters. In Languages: From Formal to Natural. Essays Ded-
icated to Nissim Francez on the Occasion of His 65th
Birthday, volume 5533 of LNCS. Springer.
S. Gandhe and D. Traum. 2008. Evaluation understudy for
dialogue coherence models. In Proc. of SIGdial 08.
J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992.
Switchboard: Telephone speech corpus for research and
development. In Proc. of ICASSP-92, pages 517–520.
P. A. Heeman and J. Allen. 1994. The TRAINS 93 dialogues.
TRAINS Technical Note 94-2, Department of Computer
Science, University of Rochester.
P. Kenny, T. Parsons, J. Gratch, A. Leuski, and A. Rizzo.
2007. Virtual patients for clinical therapist skills training.
In Proc. of IVA 07, Paris, France. Springer.
A. Lavie and M. J. Denkowski. 2009. The meteor metric
for automatic evaluation of machine translation. Machine
Translation, 23:105–115.
A. Leuski, R. Patel, D. Traum, and B. Kennedy. 2006. Build-
ing effective question answering characters. In Proc. of
SIGdial 06, pages 18–27, Sydney, Australia.
L. Sellberg and A. J¨onsson. 2008. Using random indexing to
improve singular value decomposition for latent semantic
analysis. In Proc. of LREC’08, Morocco.
A. J. Stent. 2000. The monroe corpus. Technical Report
728, Computer Science Dept. University of Rochester.
X. Yao, P. Bhutada, K. Georgila, K. Sagae, R. Artstein, and
D. Traum. 2010. Practical evaluation of speech recogniz-
ers for virtual human dialogue systems. In LREC 2010.
I. Zukerman and Y. Marom. 2006. A corpus-based approach
to help-desk response generation. In CIMCA/IAWTIC ’06.
</reference>
<page confidence="0.996995">
248
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.461555">
<title confidence="0.678615">I’ve said it before, and I’ll say it again: An empirical investigation of upper bound of the selection approach to dialogue</title>
<author confidence="0.596531">Gandhe</author>
<affiliation confidence="0.912049">Institute for Creative</affiliation>
<address confidence="0.957885">13274 Fiji way, Marina del Rey, CA</address>
<abstract confidence="0.9960086875">We perform a study of existing dialogue corpora to establish the theoretical maximum performance of the selection approach to simulating human dialogue behavior in unseen dialogues. This maximum is the proportion of test utterances for which an exact or approximate match exists in the corresponding training corpus. The results indicate that some domains seem quite suitable for a corpusbased selection approach, with over half of the test utterances having been seen before in the corpus, while other domains show much more novelty compared to previous dialogues.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R Artstein</author>
<author>S Gandhe</author>
<author>J Gerten</author>
<author>A Leuski</author>
<author>D Traum</author>
</authors>
<title>Semi-formal evaluation of conversational characters.</title>
<date>2009</date>
<booktitle>In Languages: From Formal to Natural. Essays Dedicated to Nissim Francez on the Occasion of His 65th Birthday,</booktitle>
<volume>5533</volume>
<publisher>Springer.</publisher>
<contexts>
<context position="2503" citStr="Artstein et al., 2009" startWordPosition="388" endWordPosition="391"> Moreover, when the output is not just text but presented as speech, the system may easily use recorded audio clips rather than speech synthesis. This argument also extends to multi-modal performances, e.g. using artist animation motion capture or recorded video for animating virtual human dialogue characters. Often one is willing to sacrifice some generality in order to achieve more human-like behavior than is currently possible from generation approaches. The selection approach has been used for a number of dialogue agents, including questionanswering characters at ICT (Leuski et al., 2006; Artstein et al., 2009; Kenny et al., 2007), FAQ bots (Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) and web-site information characters. It is also possible to use the selection approach as a part of the process, e.g. from words to a semantic representation or from a semantic representation to words, while using other approaches for other parts of dialogue processing. The selection approach presents two challenges for finding an appropriate utterance: o Is there a good enough utterance to select? o How good is the selection algorithm at finding this utterance? We have previously attempted to address the s</context>
</contexts>
<marker>Artstein, Gandhe, Gerten, Leuski, Traum, 2009</marker>
<rawString>R. Artstein, S. Gandhe, J. Gerten, A. Leuski, and D. Traum. 2009. Semi-formal evaluation of conversational characters. In Languages: From Formal to Natural. Essays Dedicated to Nissim Francez on the Occasion of His 65th Birthday, volume 5533 of LNCS. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Gandhe</author>
<author>D Traum</author>
</authors>
<title>Evaluation understudy for dialogue coherence models.</title>
<date>2008</date>
<booktitle>In Proc. of SIGdial 08.</booktitle>
<contexts>
<context position="3220" citStr="Gandhe and Traum, 2008" startWordPosition="505" endWordPosition="508">b-site information characters. It is also possible to use the selection approach as a part of the process, e.g. from words to a semantic representation or from a semantic representation to words, while using other approaches for other parts of dialogue processing. The selection approach presents two challenges for finding an appropriate utterance: o Is there a good enough utterance to select? o How good is the selection algorithm at finding this utterance? We have previously attempted to address the second question, by proposing the information ordering task for evaluating dialogue coherence (Gandhe and Traum, 2008). Here we try to address the first question, which would provide a theoretical upper bound in quality for any selection approach. We examine a number of different dialogue corpora as to their suitability for the selection approach. We make the following assumptions to allow automatic evaluation across a range of corpora. Actual human dialogues represent a gold-standard for computer systems to emulate; i.e. choosing an actual utterance in the correct place is the best possible result. Other utterances can be evaluated as to how close they come to the original utterance, Proceedings of SIGDIAL 2</context>
</contexts>
<marker>Gandhe, Traum, 2008</marker>
<rawString>S. Gandhe and D. Traum. 2008. Evaluation understudy for dialogue coherence models. In Proc. of SIGdial 08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Godfrey</author>
<author>E C Holliman</author>
<author>J McDaniel</author>
</authors>
<title>Switchboard: Telephone speech corpus for research and development.</title>
<date>1992</date>
<booktitle>In Proc. of ICASSP-92,</booktitle>
<pages>517--520</pages>
<contexts>
<context position="5980" citStr="Godfrey et al., 1992" startWordPosition="953" endWordPosition="956">a medical clinic. The Radiobots system is a training prototype that responds to military calls for artillery fire. IOTA is an extension of the Radiobots system. The corpus consists of training sessions between a human trainee and a human instructor on a variety of missions. Yao et al. (2010) provides details about the ICT corpora. Other corpora involved dialogues between two people playing specific roles in planning, scheduling problem for railroad transportation, the Trains-93 corpus (Heeman and Allen, 1994) and for emergency services, the Monroe corpus (Stent, 2000). The Switchboard corpus (Godfrey et al., 1992) consists of telephone conversations between two people, based on provided topics. We divided the data from each corpus into a training set and a test set, as shown in Table 1. The data consists of utterances from one or more human speakers who engage in dialogue with either virtual characters (Radiobots, Blackwell, Amani, Star, SASO) or other humans (Switchboard, Monroe, IOTA, Trains-93). These corpora differ along a number of dimensions such as the size of the corpus, dialogue genre (question-answering, taskoriented or conversational), types of tasks (artillery calls, moving and scheduling r</context>
</contexts>
<marker>Godfrey, Holliman, McDaniel, 1992</marker>
<rawString>J. J. Godfrey, E. C. Holliman, and J. McDaniel. 1992. Switchboard: Telephone speech corpus for research and development. In Proc. of ICASSP-92, pages 517–520.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P A Heeman</author>
<author>J Allen</author>
</authors>
<title>The TRAINS 93 dialogues.</title>
<date>1994</date>
<tech>TRAINS Technical Note 94-2,</tech>
<institution>Department of Computer Science, University of Rochester.</institution>
<contexts>
<context position="5873" citStr="Heeman and Allen, 1994" startWordPosition="937" endWordPosition="940">negotiation training prototype in which two virtual characters negotiate with a human “trainee” about moving a medical clinic. The Radiobots system is a training prototype that responds to military calls for artillery fire. IOTA is an extension of the Radiobots system. The corpus consists of training sessions between a human trainee and a human instructor on a variety of missions. Yao et al. (2010) provides details about the ICT corpora. Other corpora involved dialogues between two people playing specific roles in planning, scheduling problem for railroad transportation, the Trains-93 corpus (Heeman and Allen, 1994) and for emergency services, the Monroe corpus (Stent, 2000). The Switchboard corpus (Godfrey et al., 1992) consists of telephone conversations between two people, based on provided topics. We divided the data from each corpus into a training set and a test set, as shown in Table 1. The data consists of utterances from one or more human speakers who engage in dialogue with either virtual characters (Radiobots, Blackwell, Amani, Star, SASO) or other humans (Switchboard, Monroe, IOTA, Trains-93). These corpora differ along a number of dimensions such as the size of the corpus, dialogue genre (qu</context>
</contexts>
<marker>Heeman, Allen, 1994</marker>
<rawString>P. A. Heeman and J. Allen. 1994. The TRAINS 93 dialogues. TRAINS Technical Note 94-2, Department of Computer Science, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Kenny</author>
<author>T Parsons</author>
<author>J Gratch</author>
<author>A Leuski</author>
<author>A Rizzo</author>
</authors>
<title>Virtual patients for clinical therapist skills training.</title>
<date>2007</date>
<booktitle>In Proc. of IVA 07,</booktitle>
<publisher>Springer.</publisher>
<location>Paris, France.</location>
<contexts>
<context position="2524" citStr="Kenny et al., 2007" startWordPosition="392" endWordPosition="395">put is not just text but presented as speech, the system may easily use recorded audio clips rather than speech synthesis. This argument also extends to multi-modal performances, e.g. using artist animation motion capture or recorded video for animating virtual human dialogue characters. Often one is willing to sacrifice some generality in order to achieve more human-like behavior than is currently possible from generation approaches. The selection approach has been used for a number of dialogue agents, including questionanswering characters at ICT (Leuski et al., 2006; Artstein et al., 2009; Kenny et al., 2007), FAQ bots (Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) and web-site information characters. It is also possible to use the selection approach as a part of the process, e.g. from words to a semantic representation or from a semantic representation to words, while using other approaches for other parts of dialogue processing. The selection approach presents two challenges for finding an appropriate utterance: o Is there a good enough utterance to select? o How good is the selection algorithm at finding this utterance? We have previously attempted to address the second question, by pr</context>
</contexts>
<marker>Kenny, Parsons, Gratch, Leuski, Rizzo, 2007</marker>
<rawString>P. Kenny, T. Parsons, J. Gratch, A. Leuski, and A. Rizzo. 2007. Virtual patients for clinical therapist skills training. In Proc. of IVA 07, Paris, France. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Lavie</author>
<author>M J Denkowski</author>
</authors>
<title>The meteor metric for automatic evaluation of machine translation.</title>
<date>2009</date>
<journal>Machine Translation,</journal>
<pages>23--105</pages>
<contexts>
<context position="8081" citStr="Lavie and Denkowski, 2009" startWordPosition="1299" endWordPosition="1302">on f , we calculate the score for a test utterance ut as, maxsimf(ut) = maxi f(ut, ui); ui E Utrain There are several choices for the utterance similarity function f . Ideally such a function would take meaning and context into account rather than just surface similarity, but these aspects are harder to automate, so for our initial experiments we look at several surface metrics, as described below. Exact measure returns 1 if the utterances are exactly same and 0 otherwise. 1-WER, a similarity measure related to word error rate, is defined as min (0,1 − levenshtein(ut, ui)/length(ut)). METEOR (Lavie and Denkowski, 2009), one of the automatic evaluation metrics used in machine translation is a good candidate for f. METEOR finds optimal word-to-word alignment between test and reference strings based on several modules that match exact words, stemmed words and synonyms. METEOR is a tunable metric and for our analysis we used the default parameters tuned for the Adequacy &amp; Fluency task. All previous measures take into account the word ordering of test and reference strings. In contrast, document similarity measures used in information retrieval generally follow the bag of words assumption, where a 246 Domain Tra</context>
</contexts>
<marker>Lavie, Denkowski, 2009</marker>
<rawString>A. Lavie and M. J. Denkowski. 2009. The meteor metric for automatic evaluation of machine translation. Machine Translation, 23:105–115.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Leuski</author>
<author>R Patel</author>
<author>D Traum</author>
<author>B Kennedy</author>
</authors>
<title>Building effective question answering characters.</title>
<date>2006</date>
<booktitle>In Proc. of SIGdial 06,</booktitle>
<pages>18--27</pages>
<location>Sydney, Australia.</location>
<contexts>
<context position="2480" citStr="Leuski et al., 2006" startWordPosition="384" endWordPosition="387">ch detailed analysis. Moreover, when the output is not just text but presented as speech, the system may easily use recorded audio clips rather than speech synthesis. This argument also extends to multi-modal performances, e.g. using artist animation motion capture or recorded video for animating virtual human dialogue characters. Often one is willing to sacrifice some generality in order to achieve more human-like behavior than is currently possible from generation approaches. The selection approach has been used for a number of dialogue agents, including questionanswering characters at ICT (Leuski et al., 2006; Artstein et al., 2009; Kenny et al., 2007), FAQ bots (Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) and web-site information characters. It is also possible to use the selection approach as a part of the process, e.g. from words to a semantic representation or from a semantic representation to words, while using other approaches for other parts of dialogue processing. The selection approach presents two challenges for finding an appropriate utterance: o Is there a good enough utterance to select? o How good is the selection algorithm at finding this utterance? We have previously att</context>
</contexts>
<marker>Leuski, Patel, Traum, Kennedy, 2006</marker>
<rawString>A. Leuski, R. Patel, D. Traum, and B. Kennedy. 2006. Building effective question answering characters. In Proc. of SIGdial 06, pages 18–27, Sydney, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Sellberg</author>
<author>A J¨onsson</author>
</authors>
<title>Using random indexing to improve singular value decomposition for latent semantic analysis.</title>
<date>2008</date>
<booktitle>In Proc. of LREC’08,</booktitle>
<marker>Sellberg, J¨onsson, 2008</marker>
<rawString>L. Sellberg and A. J¨onsson. 2008. Using random indexing to improve singular value decomposition for latent semantic analysis. In Proc. of LREC’08, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A J Stent</author>
</authors>
<title>The monroe corpus.</title>
<date>2000</date>
<tech>Technical Report 728,</tech>
<institution>Computer Science Dept. University of Rochester.</institution>
<contexts>
<context position="5933" citStr="Stent, 2000" startWordPosition="948" endWordPosition="949">e with a human “trainee” about moving a medical clinic. The Radiobots system is a training prototype that responds to military calls for artillery fire. IOTA is an extension of the Radiobots system. The corpus consists of training sessions between a human trainee and a human instructor on a variety of missions. Yao et al. (2010) provides details about the ICT corpora. Other corpora involved dialogues between two people playing specific roles in planning, scheduling problem for railroad transportation, the Trains-93 corpus (Heeman and Allen, 1994) and for emergency services, the Monroe corpus (Stent, 2000). The Switchboard corpus (Godfrey et al., 1992) consists of telephone conversations between two people, based on provided topics. We divided the data from each corpus into a training set and a test set, as shown in Table 1. The data consists of utterances from one or more human speakers who engage in dialogue with either virtual characters (Radiobots, Blackwell, Amani, Star, SASO) or other humans (Switchboard, Monroe, IOTA, Trains-93). These corpora differ along a number of dimensions such as the size of the corpus, dialogue genre (question-answering, taskoriented or conversational), types of </context>
</contexts>
<marker>Stent, 2000</marker>
<rawString>A. J. Stent. 2000. The monroe corpus. Technical Report 728, Computer Science Dept. University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Yao</author>
<author>P Bhutada</author>
<author>K Georgila</author>
<author>K Sagae</author>
<author>R Artstein</author>
<author>D Traum</author>
</authors>
<title>Practical evaluation of speech recognizers for virtual human dialogue systems.</title>
<date>2010</date>
<booktitle>In LREC</booktitle>
<contexts>
<context position="5651" citStr="Yao et al. (2010)" startWordPosition="907" endWordPosition="910">areers in the U.S. Army. The corpus consists of trained handlers presenting the system. Amani is a bargaining character used as a prototype for training soldiers to perform tactical questioning. The SASO system is a negotiation training prototype in which two virtual characters negotiate with a human “trainee” about moving a medical clinic. The Radiobots system is a training prototype that responds to military calls for artillery fire. IOTA is an extension of the Radiobots system. The corpus consists of training sessions between a human trainee and a human instructor on a variety of missions. Yao et al. (2010) provides details about the ICT corpora. Other corpora involved dialogues between two people playing specific roles in planning, scheduling problem for railroad transportation, the Trains-93 corpus (Heeman and Allen, 1994) and for emergency services, the Monroe corpus (Stent, 2000). The Switchboard corpus (Godfrey et al., 1992) consists of telephone conversations between two people, based on provided topics. We divided the data from each corpus into a training set and a test set, as shown in Table 1. The data consists of utterances from one or more human speakers who engage in dialogue with ei</context>
</contexts>
<marker>Yao, Bhutada, Georgila, Sagae, Artstein, Traum, 2010</marker>
<rawString>X. Yao, P. Bhutada, K. Georgila, K. Sagae, R. Artstein, and D. Traum. 2010. Practical evaluation of speech recognizers for virtual human dialogue systems. In LREC 2010.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Zukerman</author>
<author>Y Marom</author>
</authors>
<title>A corpus-based approach to help-desk response generation.</title>
<date>2006</date>
<booktitle>In CIMCA/IAWTIC ’06.</booktitle>
<contexts>
<context position="2560" citStr="Zukerman and Marom, 2006" startWordPosition="398" endWordPosition="401">ted as speech, the system may easily use recorded audio clips rather than speech synthesis. This argument also extends to multi-modal performances, e.g. using artist animation motion capture or recorded video for animating virtual human dialogue characters. Often one is willing to sacrifice some generality in order to achieve more human-like behavior than is currently possible from generation approaches. The selection approach has been used for a number of dialogue agents, including questionanswering characters at ICT (Leuski et al., 2006; Artstein et al., 2009; Kenny et al., 2007), FAQ bots (Zukerman and Marom, 2006; Sellberg and J¨onsson, 2008) and web-site information characters. It is also possible to use the selection approach as a part of the process, e.g. from words to a semantic representation or from a semantic representation to words, while using other approaches for other parts of dialogue processing. The selection approach presents two challenges for finding an appropriate utterance: o Is there a good enough utterance to select? o How good is the selection algorithm at finding this utterance? We have previously attempted to address the second question, by proposing the information ordering tas</context>
</contexts>
<marker>Zukerman, Marom, 2006</marker>
<rawString>I. Zukerman and Y. Marom. 2006. A corpus-based approach to help-desk response generation. In CIMCA/IAWTIC ’06.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>