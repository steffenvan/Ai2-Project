<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.036884">
<title confidence="0.998261">
Gaussian Processes for Fast Policy Optimisation of POMDP-based
Dialogue Managers
</title>
<author confidence="0.980203">
M. Gaˇsi´c, F. Jurˇc´ıˇcek, S. Keizer, F. Mairesse, B. Thomson, K. Yu and S. Young
</author>
<affiliation confidence="0.982395">
Cambridge University Engineering Department
</affiliation>
<address confidence="0.669053">
Trumpington Street, Cambridge CB2 1PZ, UK
</address>
<email confidence="0.946899">
{mg436, fj228, sk561, farm2, brmt2, ky219, sjy}@eng.cam.ac.uk
</email>
<sectionHeader confidence="0.991523" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99994475">
Modelling dialogue as a Partially Observ-
able Markov Decision Process (POMDP)
enables a dialogue policy robust to speech
understanding errors to be learnt. How-
ever, a major challenge in POMDP pol-
icy learning is to maintain tractability, so
the use of approximation is inevitable.
We propose applying Gaussian Processes
in Reinforcement learning of optimal
POMDP dialogue policies, in order (1) to
make the learning process faster and (2) to
obtain an estimate of the uncertainty of the
approximation. We first demonstrate the
idea on a simple voice mail dialogue task
and then apply this method to a real-world
tourist information dialogue task.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999979098360656">
One of the main challenges in dialogue manage-
ment is effective handling of speech understand-
ing errors. Instead of hand-crafting the error han-
dler for each dialogue step, statistical approaches
allow the optimal dialogue manager behaviour
to be learnt automatically. Reinforcement learn-
ing (RL), in particular, enables the notion of plan-
ning to be embedded in the dialogue management
criteria. The objective of the dialogue manager is
for each dialogue state to choose such an action
that leads to the highest expected long-term re-
ward, which is defined in this framework by the Q-
function. This is in contrast to Supervised learn-
ing, which estimates a dialogue strategy in such a
way as to make it resemble the behaviour from a
given corpus, but without directly optimising over-
all dialogue success.
Modelling dialogue as a Partially Observable
Markov Decision Process (POMDP) allows action
selection to be based on the differing levels of un-
certainty in each dialogue state as well as the over-
all reward. This approach requires that a distribu-
tion of states (belief state) is maintained at each
turn. This explicit representation of uncertainty in
the POMDP gives it the potential to produce more
robust dialogue policies (Young et al., 2010).
The main challenge in the POMDP approach is
the tractability of the learning process. A dis-
crete state space POMDP can be perceived as a
continuous space MDP where the state space con-
sists of the belief states of the original POMDP.
A grid-based approach to policy optimisation as-
sumes discretisation of this space, allowing for
discrete space MDP algorithms to be used for
learning (Brafman, 1997) and thus approximating
the optimal Q-function. Such an approach takes
the order of 100, 000 dialogues to train a real-
world dialogue manager. Therefore, the training
normally takes place in interaction with a simu-
lated user, rather than real users. This raises ques-
tions regarding the quality of the approximation
as well as the potential discrepancy between sim-
ulated and real user behaviour.
Gaussian Processes have been successfully used
in Reinforcement learning for continuous space
MDPs, for both model-free approaches (Engel et
al., 2005) and model-based approaches (Deisen-
roth et al., 2009). We propose using GP Rein-
forcement learning in a POMDP dialogue man-
ager to, firstly, speed up the learning process and,
secondly, obtain the uncertainty of the approxima-
tion. We opt for the model-free approach since it
has the potential to allow the policy obtained in
interaction with the simulated user to be further
refined in interaction with real users.
In the next section, the core idea of the method is
explained on a toy dialogue problem where differ-
ent aspects of GP learning are examined. Follow-
ing that, in Section 3, it is demonstrated how this
methodology can be effectively applied to a real
world dialogue. We conclude with Section 4.
</bodyText>
<sectionHeader confidence="0.994827" genericHeader="method">
2 Gaussian Process RL on a Toy Problem
</sectionHeader>
<subsectionHeader confidence="0.610448">
2.1 Gaussian Process RL
</subsectionHeader>
<bodyText confidence="0.9996765">
A Gaussian Process is a generative model of
Bayesian inference that can be used for function
regression (Rasmussen and Williams, 2005). A
Gaussian Process is fully defined by a mean and a
kernel function. The kernel function defines prior
function correlations, which is crucial for obtain-
ing good posterior estimates with just a few ob-
servations. GP-Sarsa is an on-line reinforcement
learning algorithm for both continuous and dis-
crete MDPs that incorporates GP regression (En-
</bodyText>
<subsubsectionHeader confidence="0.677056">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 201–204,
</subsubsectionHeader>
<affiliation confidence="0.890803">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999509">
201
</page>
<bodyText confidence="0.999467166666667">
gel et al., 2005). Given the observation of rewards,
it estimates the Q-function utilising its correlations
in different parts of the state and the action space
defined by the kernel function. It also gives a vari-
ance of the estimate, thus modelling the uncer-
tainty of the approximation.
</bodyText>
<subsectionHeader confidence="0.997714">
2.2 Voice Mail Dialogue Task
</subsectionHeader>
<bodyText confidence="0.99998892">
In order to demonstrate how this methodology
can be applied to a dialogue system, we first ex-
plain the idea on the voice mail dialogue prob-
lem (Williams, 2006).
The state space of this task consists of three states:
the user asked for the message either to be saved
or deleted, or the dialogue ended. The system
can take three actions: ask the user what to do,
save or delete the message. The observation of
what the user wants is corrupted with noise, there-
fore we model this as a three-state POMDP. This
POMDP can be viewed as a continuous MDP,
where the MDP state is the POMDP belief state,
a 3-dimensional vector of probabilities. For both
learning and evaluation, a simulated user is used
which makes an error with probability 0.3 and ter-
minates the dialogue after at most 10 turns. In the
final state, it gives a positive reward of 10 or a
penalty of −100 depending on whether the system
performed a correct action or not. Each interme-
diate state receives the penalty of −1. In order to
keep the problem simple, a model defining tran-
sition and observation probabilities is assumed so
that the belief can be easily updated, but the policy
optimisation is performed in an on-line fashion.
</bodyText>
<subsectionHeader confidence="0.997201">
2.3 Kernel Choice for GP-Sarsa
</subsectionHeader>
<bodyText confidence="0.9999195">
The choice of kernel function is very important
since it defines the prior knowledge about the Q-
function correlations. They have to be defined on
both states and actions. In the voice mail dialogue
problem the action space is discrete, so we opt for
a simple S kernel over actions:
</bodyText>
<equation confidence="0.948057">
k(a, a′) = 1 − Sa(a′), (1)
</equation>
<bodyText confidence="0.99944925">
where Sa is the Kronecker delta function. The
state space is a 3-dimensional continuous space
and the kernel functions over the state space that
we explore are given in Table 1. Each kernel func-
</bodyText>
<tableCaption confidence="0.981039">
Table 1: Kernel functions
</tableCaption>
<bodyText confidence="0.9998935">
tion defines a different correlation. The polyno-
mial kernel views elements of the state vector as
features, the dot-product of which defines the cor-
relation. They can be given different relevance ri
in the parametrised version. The Gaussian ker-
nel accounts for smoothness, i.e., if two states are
close to each other the Q-function in these states
is correlated. The scaled norm kernel defines posi-
tive correlations in the points that are close to each
other and a negative correlation otherwise. This
is particularly useful for the voice mail problem,
where, if two belief states are very different, tak-
ing the same action in these states generates a neg-
atively correlated reward.
</bodyText>
<subsectionHeader confidence="0.999699">
2.4 Optimisation of Kernel Parameters
</subsectionHeader>
<bodyText confidence="0.9999803">
Some kernel functions are in a parametrised
form, such as Gaussian or parametrised polyno-
mial kernel. These parameters, also called the
hyper-parameters, are estimated by maximising
the marginal likelihood1 on a given corpus (Ras-
mussen and Williams, 2005). We adapted the
available code (Rasmussen and Williams, 2005)
for the Reinforcement learning framework to ob-
tain the optimal hyper-parameters using a dialogue
corpus labelled with states, actions and rewards.
</bodyText>
<subsectionHeader confidence="0.978058">
2.5 Grid-based RL Algorithms
</subsectionHeader>
<bodyText confidence="0.999868875">
To assess the performance of GP-Sarsa, it was
compared with a standard grid-based algorithm
used in (Young et al., 2010). The grid-based ap-
proach discretises the continuous space into re-
gions with their representative points. This then
allows discrete MDP algorithms to be used for pol-
icy optimisation, in this case the Monte Carlo Con-
trol (MCC) algorithm (Sutton and Barto, 1998).
</bodyText>
<subsectionHeader confidence="0.911332">
2.6 Optimal POMDP Policy
</subsectionHeader>
<bodyText confidence="0.999986333333333">
The optimal POMDP policy was obtained us-
ing the POMDP solver toolkit (Cassandra, 2005),
which implements the Point Based Value Itera-
tion algorithm to solve the POMDP off-line using
the underlying transition and observation proba-
bilities. We used 300 sample dialogues between
the dialogue manager governed by this policy and
the simulated user as data for optimisation of the
kernel hyper-parameters (see Section 2.4).
</bodyText>
<subsectionHeader confidence="0.999036">
2.7 Training set-up and Evaluation
</subsectionHeader>
<bodyText confidence="0.999661636363636">
The dialogue manager was trained in interaction
with the simulated user and the performance was
compared between the grid-based MCC algorithm
and GP-Sarsa across different kernel functions
from Table 1.
The intention was, not only to test which algo-
rithm yields the best policy performance, but also
to examine the speed of convergence to the opti-
mal policy. All the algorithms use an c-greedy
approach where the exploration rate c was fixed
at 0.1. The learning process greatly depends on
</bodyText>
<footnote confidence="0.907801">
1Also called evidence maximisation in the literature.
</footnote>
<equation confidence="0.8672019">
expression
− IIx − x′II2
2σ_k
k(x x′) = 1 − IIx − x′II
IIxII2IIx′II2
k(x, x′) = (x, x′�
a�a′ �
k(x, x′) = �D �=1 ��
�
k(x, x′) = p2 exp
</equation>
<figure confidence="0.9423666">
kernel function
polynomial
parametrised poly.
Gaussian
scaled norm
</figure>
<page confidence="0.987792">
202
</page>
<bodyText confidence="0.996227666666667">
the actions that are taken during exploration. If
early on during the training, the systems discovers
a path that generates high rewards due to a lucky
choice of actions, then the convergence is faster.
To alleviate this, we adopted the following proce-
dure. For every training set-up, exactly the same
training iterations were performed using 1000 dif-
ferent random generator seedings. After every 20
dialogues the resulting 1000 partially optimised
policies were evaluated. Each of them was tested
on 1000 dialogues. The average reward of these
1000 dialogues provides just one point in Fig. 1.
</bodyText>
<figure confidence="0.958709">
20 60 100 140 180 220 260 300 340 380 420 460 500 540 580 620
Training dialogues
</figure>
<figureCaption confidence="0.999951">
Figure 1: Evaluation results on Voice Mail task
</figureCaption>
<bodyText confidence="0.999983333333333">
The grid-based MCC algorithm used a Euclid-
ian distance to generate the grid by adding every
point that was further than 0.01 from other points
as a representative of a new region. As can be
seen from Fig 1, the grid-Based MCC algorithm
has a relatively slow convergence rate. GP-Sarsa
with the polynomial kernel exhibited a learning
rate similar to MCC in the first 300 training di-
alogues, continuing with a more upward learning
trend. The parametrised polynomial kernel per-
forms slightly better. The Gaussian kernel, how-
ever, achieves a much faster learning rate. The
scaled norm kernel achieved close to optimal per-
formance in 400 dialogues, with a much higher
convergence rate then the other methods.
</bodyText>
<sectionHeader confidence="0.9977605" genericHeader="method">
3 Gaussian Process RL on a Real-world
Task
</sectionHeader>
<subsectionHeader confidence="0.96123">
3.1 HIS Dialogue Manager on CamInfo
Domain
</subsectionHeader>
<bodyText confidence="0.999994148148148">
We investigate the use of GP-Sarsa in a real-
world task by extending the Hidden Information
State (HIS) dialogue manager (Young et al., 2010).
The application domain is tourist information for
Cambridge, whereby the user can ask for informa-
tion about a restaurant, hotel, museum or another
tourist attraction in the local area. The database
consists of more than 400 entities each of which
has up to 10 attributes that the user can query.
The HIS dialogue manager is a POMDP-based di-
alogue manager that can tractably maintain belief
states for large domains. The key feature of this
approach is the grouping of possible user goals
into partitions, using relationships between differ-
ent attributes from possible user goals. Partitions
are combined with possible user dialogue actions
from the N-best user input as well as with the di-
alogue history. This combination forms the state
space – the set of hypotheses, the probability dis-
tribution over which is maintained during the di-
alogue. Since the number of states for any real-
world problem is too large, for tractable policy
learning, both the state and the action space are
mapped into smaller scale summary spaces. Once
an adequate summary action is found in the sum-
mary space, it is mapped back to form an action in
the original master space.
</bodyText>
<subsectionHeader confidence="0.999092">
3.2 Kernel Choice for GP-Sarsa
</subsectionHeader>
<bodyText confidence="0.9999920625">
The summary state in the HIS system is a four-
dimensional space consisting of two elements that
are continuous (the probability of the top two hy-
potheses) and two discrete elements (one relating
the portion of the database entries that matches the
top partition and the other relating to the last user
action type). The summary action space is discrete
and consists of eleven elements.
In order to apply the GP-Sarsa algorithm, a kernel
function needs to be specified for both the sum-
mary state space and the summary action space.
The nature of this space is quite different from the
one described in the toy problem. Therefore, ap-
plying a kernel that has negative correlations, such
as the scaled norm kernel (Table 1) might give un-
expected results. More specifically, for a given
summary action, the mapping procedure finds the
most appropriate action to perform if such an ac-
tion exists. This can lead to a lower reward if
the summary action is not adequate but would
rarely lead to negatively correlated rewards. Also,
parametrised kernels could not be used for this
task, since there was no corpus available for hyper-
parameter optimisation. The polynomial kernel
(Table 1) assumes that the elements of the space
are features. Due to the way the probability is
maintained over this very large state space, the
continuous variables potentially encode more in-
formation than in the simple toy problem. There-
fore, we used the polynomial kernel for the con-
tinuous elements. For discrete elements, we utilise
the δ-kernel (Eq. 2.3).
</bodyText>
<subsectionHeader confidence="0.999781">
3.3 Active Learning GP-Sarsa
</subsectionHeader>
<bodyText confidence="0.9999125">
The GP RL framework enables modelling the un-
certainty of the approximation. The uncertainty
estimate can be used to decide which actions
to take during the exploration (Deisenroth et al.,
</bodyText>
<figure confidence="0.9972543">
0
−5
← scaled norm kernel
← Gaussian kernel with learned hyper−parameters
↓
polynomial kernel with learned hyper−parameters
polynomial kernel →
−35
−40
Optimal POMDP Policy
GP−Sarsa
Grid−based Monte Carlo Control
Average reward
−20
−25
−30
−10
−15
−45
−50
</figure>
<page confidence="0.997655">
203
</page>
<bodyText confidence="0.99921">
2009). In detail, instead of a random action, the
action in which the Q-function for the current state
has the highest variance is taken.
</bodyText>
<subsectionHeader confidence="0.998008">
3.4 Training Set-up and Evaluation
</subsectionHeader>
<bodyText confidence="0.9777234">
Policy optimisation is performed by interacting
with a simulated user on the dialogue act level.
The simulated user gives a reward at the final state
of the dialogue, and that is 20 if the dialogue was
successful, 0 otherwise, less the number of turns
taken to fulfil the user goal. The simulated user
takes a maximum of 100 turns in each dialogue,
terminating it when all the necessary information
has been obtained or if it looses patience.
A grid-based MCC algorithm provides the base-
line method. The distance metric used ensures
that the number of regions in the grid is small
enough for the learning to be tractable (Young et
al., 2010).
In order to measure how fast each algorithm
learns, a similar training set-up to the one pre-
sented in Section 2.7 was adopted and the aver-
aged results are plotted on the graph, Fig. 2.
200 400 600 800 1000 1200 1400 1600 1800 2000 2200 2400 2600 2800 3000
Training dialogues
</bodyText>
<figureCaption confidence="0.995841">
Figure 2: Evaluation results on CamInfo task
</figureCaption>
<bodyText confidence="0.9999951">
The results show that in the very early stage of
learning, i.e., during the first 400 dialogues, the
GP-based method learns faster. Also, the learning
process can be accelerated by adopting the active
learning framework where the actions are selected
based on the estimated uncertainty.
After performing many iterations in an incremen-
tal noise learning set-up (Young et al., 2010) both
the GP-Sarsa and the grid-based MCC algorithms
converge to the same performance.
</bodyText>
<sectionHeader confidence="0.999726" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999975">
This paper has described how Gaussian Processes
in Reinforcement learning can be successfully ap-
plied to dialogue management. We implemented
a GP-Sarsa algorithm on a toy dialogue prob-
lem, showing that with an appropriate kernel func-
tion faster convergence can be achieved. We also
demonstrated how kernel parameters can be learnt
from a dialogue corpus, thus creating a bridge
between Supervised and Reinforcement learning
methods in dialogue management. We applied
GP-Sarsa to a real-world dialogue task showing
that, on average, this method can learn faster than
a grid-based algorithm. We also showed that the
variance that GP is estimating can be used in an
Active learning setting to further accelerate policy
optimisation.
Further research is needed in the area of kernel
function selection. The results here suggest that
the GP framework can facilitate faster learning,
which potentially allows the use of larger sum-
mary spaces. In addition, being able to learn ef-
ficiently from a small number of dialogues offers
the potential for learning from direct interaction
with real users.
</bodyText>
<sectionHeader confidence="0.9976" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9999842">
The authors would like to thank Carl Rasmussen
for valuable discussions. This research was partly
funded by the UK EPSRC under grant agreement
EP/F013930/1 and by the EU FP7 Programme un-
der grant agreement 216594 (CLASSiC project).
</bodyText>
<sectionHeader confidence="0.999113" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998155333333333">
RI Brafman. 1997. A Heuristic Variable Grid Solution
Method for POMDPs. In AAAI, Cambridge, MA.
AR Cassandra. 2005. POMDP solver.
http://www.cassandra.org/pomdp/
code/index.shtml.
MP Deisenroth, CE Rasmussen, and J Peters. 2009.
Gaussian Process Dynamic Programming. Neuro-
comput., 72(7-9):1508–1524.
Y Engel, S Mannor, and R Meir. 2005. Reinforcement
learning with Gaussian processes. In ICML ’05:
Proceedings of the 22nd international conference on
Machine learning, pages 201–208, New York, NY.
CE Rasmussen and CKI Williams. 2005. Gaussian
Processes for Machine Learning. MIT Press, Cam-
bridge, MA.
RS Sutton and AG Barto. 1998. Reinforcement Learn-
ing: An Introduction. Adaptive Computation and
Machine Learning. MIT Press, Cambridge, MA.
JD Williams. 2006. Partially Observable Markov De-
cision Processes for Spoken Dialogue Management.
Ph.D. thesis, University of Cambridge.
SJ Young, M Gaˇsi´c, S Keizer, F Mairesse, J Schatz-
mann, B Thomson, and K Yu. 2010. The Hid-
den Information State Model: a practical frame-
work for POMDP-based spoken dialogue manage-
ment. Computer Speech and Language, 24(2):150–
174.
</reference>
<figure confidence="0.997686583333333">
Average reward
9
8
7
6
5
4
3
2
← Active learning GP−Sarsa with polynomial kernel
← GP−Sarsa with polynomial kernel
← Grid−based Monte Carlo Control
</figure>
<page confidence="0.978967">
204
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.378163">
<title confidence="0.997244">Gaussian Processes for Fast Policy Optimisation of Dialogue Managers</title>
<author confidence="0.885907">M Gaˇsi´c</author>
<author confidence="0.885907">F Jurˇc´ıˇcek</author>
<author confidence="0.885907">S Keizer</author>
<author confidence="0.885907">F Mairesse</author>
<author confidence="0.885907">B Thomson</author>
<author confidence="0.885907">K Yu</author>
<author confidence="0.885907">S</author>
<affiliation confidence="0.996608">Cambridge University Engineering</affiliation>
<address confidence="0.535861">Trumpington Street, Cambridge CB2 1PZ,</address>
<email confidence="0.66735">fj228,sk561,farm2,brmt2,ky219,</email>
<abstract confidence="0.999213117647059">Modelling dialogue as a Partially Observable Markov Decision Process (POMDP) enables a dialogue policy robust to speech understanding errors to be learnt. However, a major challenge in POMDP policy learning is to maintain tractability, so the use of approximation is inevitable. We propose applying Gaussian Processes in Reinforcement learning of optimal POMDP dialogue policies, in order (1) to make the learning process faster and (2) to obtain an estimate of the uncertainty of the approximation. We first demonstrate the idea on a simple voice mail dialogue task and then apply this method to a real-world tourist information dialogue task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>RI Brafman</author>
</authors>
<title>A Heuristic Variable Grid Solution Method for POMDPs. In AAAI,</title>
<date>1997</date>
<location>Cambridge, MA.</location>
<contexts>
<context position="2624" citStr="Brafman, 1997" startWordPosition="415" endWordPosition="416">that a distribution of states (belief state) is maintained at each turn. This explicit representation of uncertainty in the POMDP gives it the potential to produce more robust dialogue policies (Young et al., 2010). The main challenge in the POMDP approach is the tractability of the learning process. A discrete state space POMDP can be perceived as a continuous space MDP where the state space consists of the belief states of the original POMDP. A grid-based approach to policy optimisation assumes discretisation of this space, allowing for discrete space MDP algorithms to be used for learning (Brafman, 1997) and thus approximating the optimal Q-function. Such an approach takes the order of 100, 000 dialogues to train a realworld dialogue manager. Therefore, the training normally takes place in interaction with a simulated user, rather than real users. This raises questions regarding the quality of the approximation as well as the potential discrepancy between simulated and real user behaviour. Gaussian Processes have been successfully used in Reinforcement learning for continuous space MDPs, for both model-free approaches (Engel et al., 2005) and model-based approaches (Deisenroth et al., 2009). </context>
</contexts>
<marker>Brafman, 1997</marker>
<rawString>RI Brafman. 1997. A Heuristic Variable Grid Solution Method for POMDPs. In AAAI, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>AR Cassandra</author>
</authors>
<date>2005</date>
<note>POMDP solver. http://www.cassandra.org/pomdp/ code/index.shtml.</note>
<contexts>
<context position="8417" citStr="Cassandra, 2005" startWordPosition="1373" endWordPosition="1374">e optimal hyper-parameters using a dialogue corpus labelled with states, actions and rewards. 2.5 Grid-based RL Algorithms To assess the performance of GP-Sarsa, it was compared with a standard grid-based algorithm used in (Young et al., 2010). The grid-based approach discretises the continuous space into regions with their representative points. This then allows discrete MDP algorithms to be used for policy optimisation, in this case the Monte Carlo Control (MCC) algorithm (Sutton and Barto, 1998). 2.6 Optimal POMDP Policy The optimal POMDP policy was obtained using the POMDP solver toolkit (Cassandra, 2005), which implements the Point Based Value Iteration algorithm to solve the POMDP off-line using the underlying transition and observation probabilities. We used 300 sample dialogues between the dialogue manager governed by this policy and the simulated user as data for optimisation of the kernel hyper-parameters (see Section 2.4). 2.7 Training set-up and Evaluation The dialogue manager was trained in interaction with the simulated user and the performance was compared between the grid-based MCC algorithm and GP-Sarsa across different kernel functions from Table 1. The intention was, not only to</context>
</contexts>
<marker>Cassandra, 2005</marker>
<rawString>AR Cassandra. 2005. POMDP solver. http://www.cassandra.org/pomdp/ code/index.shtml.</rawString>
</citation>
<citation valid="true">
<authors>
<author>MP Deisenroth</author>
<author>CE Rasmussen</author>
<author>J Peters</author>
</authors>
<date>2009</date>
<booktitle>Gaussian Process Dynamic Programming. Neurocomput.,</booktitle>
<pages>72--7</pages>
<contexts>
<context position="3222" citStr="Deisenroth et al., 2009" startWordPosition="504" endWordPosition="508">r learning (Brafman, 1997) and thus approximating the optimal Q-function. Such an approach takes the order of 100, 000 dialogues to train a realworld dialogue manager. Therefore, the training normally takes place in interaction with a simulated user, rather than real users. This raises questions regarding the quality of the approximation as well as the potential discrepancy between simulated and real user behaviour. Gaussian Processes have been successfully used in Reinforcement learning for continuous space MDPs, for both model-free approaches (Engel et al., 2005) and model-based approaches (Deisenroth et al., 2009). We propose using GP Reinforcement learning in a POMDP dialogue manager to, firstly, speed up the learning process and, secondly, obtain the uncertainty of the approximation. We opt for the model-free approach since it has the potential to allow the policy obtained in interaction with the simulated user to be further refined in interaction with real users. In the next section, the core idea of the method is explained on a toy dialogue problem where different aspects of GP learning are examined. Following that, in Section 3, it is demonstrated how this methodology can be effectively applied to</context>
</contexts>
<marker>Deisenroth, Rasmussen, Peters, 2009</marker>
<rawString>MP Deisenroth, CE Rasmussen, and J Peters. 2009. Gaussian Process Dynamic Programming. Neurocomput., 72(7-9):1508–1524.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Engel</author>
<author>S Mannor</author>
<author>R Meir</author>
</authors>
<title>Reinforcement learning with Gaussian processes.</title>
<date>2005</date>
<booktitle>In ICML ’05: Proceedings of the 22nd international conference on Machine learning,</booktitle>
<pages>201--208</pages>
<location>New York, NY.</location>
<contexts>
<context position="3169" citStr="Engel et al., 2005" startWordPosition="497" endWordPosition="500"> for discrete space MDP algorithms to be used for learning (Brafman, 1997) and thus approximating the optimal Q-function. Such an approach takes the order of 100, 000 dialogues to train a realworld dialogue manager. Therefore, the training normally takes place in interaction with a simulated user, rather than real users. This raises questions regarding the quality of the approximation as well as the potential discrepancy between simulated and real user behaviour. Gaussian Processes have been successfully used in Reinforcement learning for continuous space MDPs, for both model-free approaches (Engel et al., 2005) and model-based approaches (Deisenroth et al., 2009). We propose using GP Reinforcement learning in a POMDP dialogue manager to, firstly, speed up the learning process and, secondly, obtain the uncertainty of the approximation. We opt for the model-free approach since it has the potential to allow the policy obtained in interaction with the simulated user to be further refined in interaction with real users. In the next section, the core idea of the method is explained on a toy dialogue problem where different aspects of GP learning are examined. Following that, in Section 3, it is demonstrat</context>
</contexts>
<marker>Engel, Mannor, Meir, 2005</marker>
<rawString>Y Engel, S Mannor, and R Meir. 2005. Reinforcement learning with Gaussian processes. In ICML ’05: Proceedings of the 22nd international conference on Machine learning, pages 201–208, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>CE Rasmussen</author>
<author>CKI Williams</author>
</authors>
<date>2005</date>
<booktitle>Gaussian Processes for Machine Learning.</booktitle>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="4071" citStr="Rasmussen and Williams, 2005" startWordPosition="650" endWordPosition="653">has the potential to allow the policy obtained in interaction with the simulated user to be further refined in interaction with real users. In the next section, the core idea of the method is explained on a toy dialogue problem where different aspects of GP learning are examined. Following that, in Section 3, it is demonstrated how this methodology can be effectively applied to a real world dialogue. We conclude with Section 4. 2 Gaussian Process RL on a Toy Problem 2.1 Gaussian Process RL A Gaussian Process is a generative model of Bayesian inference that can be used for function regression (Rasmussen and Williams, 2005). A Gaussian Process is fully defined by a mean and a kernel function. The kernel function defines prior function correlations, which is crucial for obtaining good posterior estimates with just a few observations. GP-Sarsa is an on-line reinforcement learning algorithm for both continuous and discrete MDPs that incorporates GP regression (EnProceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 201–204, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 201 gel et al., 2005). Given the ob</context>
<context position="7685" citStr="Rasmussen and Williams, 2005" startWordPosition="1256" endWordPosition="1260">s is correlated. The scaled norm kernel defines positive correlations in the points that are close to each other and a negative correlation otherwise. This is particularly useful for the voice mail problem, where, if two belief states are very different, taking the same action in these states generates a negatively correlated reward. 2.4 Optimisation of Kernel Parameters Some kernel functions are in a parametrised form, such as Gaussian or parametrised polynomial kernel. These parameters, also called the hyper-parameters, are estimated by maximising the marginal likelihood1 on a given corpus (Rasmussen and Williams, 2005). We adapted the available code (Rasmussen and Williams, 2005) for the Reinforcement learning framework to obtain the optimal hyper-parameters using a dialogue corpus labelled with states, actions and rewards. 2.5 Grid-based RL Algorithms To assess the performance of GP-Sarsa, it was compared with a standard grid-based algorithm used in (Young et al., 2010). The grid-based approach discretises the continuous space into regions with their representative points. This then allows discrete MDP algorithms to be used for policy optimisation, in this case the Monte Carlo Control (MCC) algorithm (Sutt</context>
</contexts>
<marker>Rasmussen, Williams, 2005</marker>
<rawString>CE Rasmussen and CKI Williams. 2005. Gaussian Processes for Machine Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>RS Sutton</author>
<author>AG Barto</author>
</authors>
<title>Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning.</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA.</location>
<contexts>
<context position="8304" citStr="Sutton and Barto, 1998" startWordPosition="1353" endWordPosition="1356">005). We adapted the available code (Rasmussen and Williams, 2005) for the Reinforcement learning framework to obtain the optimal hyper-parameters using a dialogue corpus labelled with states, actions and rewards. 2.5 Grid-based RL Algorithms To assess the performance of GP-Sarsa, it was compared with a standard grid-based algorithm used in (Young et al., 2010). The grid-based approach discretises the continuous space into regions with their representative points. This then allows discrete MDP algorithms to be used for policy optimisation, in this case the Monte Carlo Control (MCC) algorithm (Sutton and Barto, 1998). 2.6 Optimal POMDP Policy The optimal POMDP policy was obtained using the POMDP solver toolkit (Cassandra, 2005), which implements the Point Based Value Iteration algorithm to solve the POMDP off-line using the underlying transition and observation probabilities. We used 300 sample dialogues between the dialogue manager governed by this policy and the simulated user as data for optimisation of the kernel hyper-parameters (see Section 2.4). 2.7 Training set-up and Evaluation The dialogue manager was trained in interaction with the simulated user and the performance was compared between the gri</context>
</contexts>
<marker>Sutton, Barto, 1998</marker>
<rawString>RS Sutton and AG Barto. 1998. Reinforcement Learning: An Introduction. Adaptive Computation and Machine Learning. MIT Press, Cambridge, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>JD Williams</author>
</authors>
<title>Partially Observable Markov Decision Processes for Spoken Dialogue Management.</title>
<date>2006</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Cambridge.</institution>
<contexts>
<context position="5116" citStr="Williams, 2006" startWordPosition="820" endWordPosition="821">iscourse and Dialogue, pages 201–204, The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics 201 gel et al., 2005). Given the observation of rewards, it estimates the Q-function utilising its correlations in different parts of the state and the action space defined by the kernel function. It also gives a variance of the estimate, thus modelling the uncertainty of the approximation. 2.2 Voice Mail Dialogue Task In order to demonstrate how this methodology can be applied to a dialogue system, we first explain the idea on the voice mail dialogue problem (Williams, 2006). The state space of this task consists of three states: the user asked for the message either to be saved or deleted, or the dialogue ended. The system can take three actions: ask the user what to do, save or delete the message. The observation of what the user wants is corrupted with noise, therefore we model this as a three-state POMDP. This POMDP can be viewed as a continuous MDP, where the MDP state is the POMDP belief state, a 3-dimensional vector of probabilities. For both learning and evaluation, a simulated user is used which makes an error with probability 0.3 and terminates the dial</context>
</contexts>
<marker>Williams, 2006</marker>
<rawString>JD Williams. 2006. Partially Observable Markov Decision Processes for Spoken Dialogue Management. Ph.D. thesis, University of Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>SJ Young</author>
<author>M Gaˇsi´c</author>
<author>S Keizer</author>
<author>F Mairesse</author>
<author>J Schatzmann</author>
<author>B Thomson</author>
<author>K Yu</author>
</authors>
<title>The Hidden Information State Model: a practical framework for POMDP-based spoken dialogue management.</title>
<date>2010</date>
<journal>Computer Speech and Language,</journal>
<volume>24</volume>
<issue>2</issue>
<pages>174</pages>
<marker>Young, Gaˇsi´c, Keizer, Mairesse, Schatzmann, Thomson, Yu, 2010</marker>
<rawString>SJ Young, M Gaˇsi´c, S Keizer, F Mairesse, J Schatzmann, B Thomson, and K Yu. 2010. The Hidden Information State Model: a practical framework for POMDP-based spoken dialogue management. Computer Speech and Language, 24(2):150– 174.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>