<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.025736">
<note confidence="0.8374834">
Zoom: a corpus of natural language descriptions of map locations
Romina Altamirano Thiago C. Ferreira Ivandr´e Paraboni Luciana Benotti
FaMAF EACH-USP EACH-USP FaMAF
Nat.Univ. of C´ordoba Univ. of S˜ao Paulo Univ. of S˜ao Paulo Nat.Univ. of C´ordoba
C´ordoba, Argentina S˜ao Paulo, Brazil S˜ao Paulo, Brazil C´ordoba, Argentina
</note>
<email confidence="0.72447">
ialtamir@famaf.unc.edu.ar thiago.castro.ferreira@usp.br ivandre@usp.br benotti@famaf.unc.edu.ar
</email>
<sectionHeader confidence="0.988867" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999913235294118">
This paper describes an experiment to
elicit referring expressions from human
subjects for research in natural language
generation and related fields, and prelim-
inary results of a computational model for
the generation of these expressions. Un-
like existing resources of this kind, the re-
sulting data set - the Zoom corpus of natu-
ral language descriptions of map locations
- takes into account a domain that is sig-
nificantly closer to real-world applications
than what has been considered in previous
work, and addresses more complex situa-
tions of reference, including contexts with
different levels of detail, and instances of
singular and plural reference produced by
speakers of Spanish and Portuguese.
</bodyText>
<sectionHeader confidence="0.998958" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999722709677419">
Referring Expression Generation (REG) is the
computational task of producing adequate natural
language descriptions (e.g., pronouns, definite de-
scriptions, proper names, etc.) of domain entities.
In particular, the issue of how to determine the se-
mantic contents of definite descriptions (e.g., ‘the
Indian restaurant on 5th street’, ‘the restaurant we
went to last night’, etc.) has received significant
attention in the field, and it is also the focus of the
present work.
The input to a REG algorithm is a context set
C containing an intended referent r and a number
of distractor objects. All objects are represented
as attribute-value pairs representing either atomic
(type-restaurant) or relational (on-5thstreet) prop-
erties (Krahmer and Theune, 2002; Krahmer et al.,
2003; Kelleher and Kruijff, 2006; Viethen et al.,
2013). The expected output is a uniquely identi-
fying list L of properties known to be true of r
so that L distinguishes r from all distractors in C
(Dale and Reiter, 1995).
Properties are selected for inclusion in L ac-
cording to multiple - and often conflicting - cri-
teria, including discriminatory power (i.e., the
ability to rule out distractors) as in (Dale, 2002;
Gardent, 2002), domain preferences (Pechmann,
1989; Gatt et al., 2013) and many others. A de-
scription that conveys more information than what
is strictly required for disambiguation is said to
be overspecified (Arts et al., 2011; Koolen et al.,
2011; van Gompel et al., 2012; Engelhardt and
Ferreira, 2006; Engelhardt et al., 2011). For a
review of the research challenges in REG, see
(Krahmer and van Deemter, 2012).
Existing approaches to REG largely consist of
algorithmic solutions, many of which have been
influenced by, or adapted from, the Dale &amp; Reiter
Incremental algorithm in (Dale and Reiter, 1995).
The use of machine learning (ML) techniques, by
contrast, seems to be less frequent than in other
NLG tasks, although a number of exceptions do
exist (e.g., (Jordan and Walker, 2005; Viethen and
Dale, 2010; Viethen, 2011; Garoufi and Koller,
2013; Ferreira and Paraboni, 2014)).
A possible explanation for the small interest in
ML for REG may be the relatively low availabil-
ity of data. While research in many fields may
benefit from the wide availability of text corpora
(e.g., obtainable from the web), research in REG
usually requires highly specialised data - hereby
called REG corpora - conveying not only refer-
ring expressions produced by human speakers, but
also a fully-annotated representation of the con-
text (i.e., all objects and their semantic properties)
within which the expressions have been produced.
REG corpora such as TUNA (Gatt et al., 2007)
and GRE3D3 (Dale and Viethen, 2009) are useful
both to gain general insights on human language
production, and to benefit from data-intensive
computational techniques such as ML. However,
being usually the final product of controlled ex-
periments involving human subjects, REG cor-
</bodyText>
<page confidence="0.977067">
69
</page>
<bodyText confidence="0.95397337037037">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 69–75,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
pora tend to address highly specific research ques-
tions. For instance, GRE3D3 is largely devoted
to the investigation of relational referring expres-
sions (Kelleher and Kruijff, 2006) in simple visual
scenes involving geometric shapes, as in ‘the large
ball next to the red cube’. As a result, and despite
the usefulness of these resources to a large body of
work in REG, further research questions will usu-
ally require the collection of new data.
In this paper we introduce the Zoom corpus of
referring expressions. Zoom addresses a domain
that is considerably closer to real-world applica-
tions (namely, city maps in different degrees of
detail represented by zoom levels) than what has
been considered in previous work, involving both
singular and plural reference, and making exten-
sive use of relational properties. Moreover, Zoom
descriptions were produced by both Spanish and
Portuguese speakers, which will allow (to the best
of our knowledge, for the first time) a comprehen-
sive study of the REG surface realisation subtask
in these languages, and enable research on the is-
sues of human variation in REG (Fabbrizio et al.,
2008; Altamirano et al., 2012; Gatt et al., 2011).
</bodyText>
<sectionHeader confidence="0.999423" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999926161290322">
TUNA (Gatt et al., 2007) was the first promi-
nent REG corpus to be made publicly available
for research purposes. The corpus was developed
in a series of controlled experiments, containing
2280 atomic descriptions produced by 60 speakers
of English in two domains (1200 descriptions of
furniture items and 1080 descriptions of people’s
photographs). TUNA has been used in a series of
REG shared tasks (Gatt et al., 2009).
GRE3D3 and its extension GRE3D7 (Dale and
Viethen, 2009; Viethen and Dale, 2011) were de-
veloped in a series of web-based experiments pri-
marily focussed on the study of relational descrip-
tions. GRE3D3 contains 630 descriptions pro-
duced by 63 speakers, and GRE3D7 contains 4480
descriptions produced by 287 speakers. In both
cases, the language of the experiment was English.
The domain consists of simple visual scenes con-
veying boxes and spheres.
Stars (Teixeira et al., 2014) and its extension
Stars2 were collected for the study of referential
overspecification. Stars contains 704 descriptions
produced by 64 speakers in a web-based exper-
iment. Stars2 was produced in dialogue situa-
tions involving subject pairs, and it contains 884
descriptions produced by 56 speakers. Both do-
mains make use of simple visual scenes containing
up to four object types (e.g., stars, boxes, cones
and spheres) and include atomic and relational de-
scriptions alike. The language of both experiments
was Brazilian Portuguese.
</bodyText>
<sectionHeader confidence="0.997412" genericHeader="method">
3 Experiment
</sectionHeader>
<bodyText confidence="0.9999631">
We designed a web-based experiment to collect
natural language descriptions of map locations in
both Spanish and Portuguese. The collected data
set comprises a corpus of referring expressions for
research in REG and related fields. The situations
of reference under consideration make use of map
scenes in two degrees of detail (represented by low
and high zoom levels), and address instances of
singular and plural reference. A fragment of the
experiment interface is shown in Fig. 1.
</bodyText>
<figureCaption confidence="0.999198">
Figure 1: Experiment interface
</figureCaption>
<subsectionHeader confidence="0.995433">
3.1 Subjects
</subsectionHeader>
<bodyText confidence="0.9999754">
Volunteers were recruited upon invitation sent by
email. The Portuguese data had 93 participants,
being 66 (71.0%) male and 27 (29.0%) female.
The Spanish data had 80 participants, being 59
male (69.4%) and 26 female (30.6%).
</bodyText>
<subsectionHeader confidence="0.999345">
3.2 Procedure
</subsectionHeader>
<bodyText confidence="0.9999659">
Subjects received a web link to the on-line experi-
ment interface (cf. Fig. 1) with self-contained in-
structions. Age and gender details were collected
for statistical purposes. The experiment consisted
of a series of map images presented in random or-
der, one by one. Each map scene showed a partic-
ular location (e.g., a restaurant, pub, theatre etc.)
pointed by an arrow. For each scene, subjects were
required to imagine that they were giving travel
advice to a friend, and to complete the sentence ‘It
</bodyText>
<page confidence="0.987971">
70
</page>
<bodyText confidence="0.999929333333333">
would be interesting to visit...’ with a description
of the location pointed by the arrow. After press-
ing a ‘Next’ button, another stimulus was selected,
until the end of the experiment. The first two im-
ages were fillers solely intended to make subjects
familiar with the experiment setting, and the cor-
responding responses were not recorded. Incom-
plete trials, and ill-formed descriptions, were also
discarded.
</bodyText>
<subsectionHeader confidence="0.995958">
3.3 Materials
</subsectionHeader>
<bodyText confidence="0.9999968">
The experiment made use of the purpose-built in-
terface illustrated in Fig. 1, and a set of map im-
ages obtained from OpenStreetMap1, which con-
sisted of selected portions of maps of Madrid and
Lisbon to be presented to Spanish and Portuguese
speakers, respectively. For each city, 10 map lo-
cations were used. Each location was shown in
low and high zoom levels, making 20 images in
total. In both cases, the intended target was kept
the same, but the more detailed version would dis-
play a larger number of distractors and additional
details in general. In addition to that, certain street
and landmark names might not be depicted at dif-
ferent zoom levels. Half images showed a single
arrow pointing to one map location (i.e., requir-
ing a single description as ‘the restaurant on Baker
street’), whereas the other half showed two arrows
pointed to two different locations (and hence re-
quiring a reference to a set, as in ‘the two restau-
rants near the museum’).
</bodyText>
<subsectionHeader confidence="0.993434">
3.4 Data collection
</subsectionHeader>
<bodyText confidence="0.971977611111111">
Upon manual verification, 602 ill-formed Por-
tuguese descriptions and 366 Spanish descriptions
were discarded. Thus, the Portuguese subcor-
pus consists of 1358 descriptions, and the Span-
ish subcorpus consists of 1234 descriptions. In
the Portuguese subcorpus, 78.6% of the descrip-
tions include relational properties. In addition to
that, 36.4% were minimally distinguishing, 44.3%
were overspecified, and 19.3% were underspeci-
fied. In the Spanish subcorpus, 70% of the de-
scriptions include relational properties, 35% were
minimally distinguishing, 40% were overspeci-
fied, and 25% were underspecified. Underspeci-
fied descriptions are not common in existing REG
corpora (i.e., certainly not in this proportion),
which may reflect the complexity of the domain
and/or limitations of the web-based setting.
1openstreetmap.org
</bodyText>
<subsectionHeader confidence="0.969609">
3.5 Annotation
</subsectionHeader>
<bodyText confidence="0.999984722222222">
Each referring expression was modelled as con-
veying a description of the main target object and,
optionally, up to four descriptions of related land-
marks. The annotation scheme consisted of three
target attributes, four landmark attributes for each
of the four possible landmark objects, and seven
relational properties. This makes 26 possible at-
tributes for each referring expression. In the case
of plural descriptions (i.e., those involving two tar-
get objects), this attribute set is doubled.
Every object was annotated with the atomic at-
tributes type, name and others and, in the case of
landmark objects, also with their id. In addition
to that, seven relational properties were consid-
ered: in/on/at2, next-to, right-of, left-of, in-front-
of, behind-of, and the multivalue relation between
intended to represent ‘corner’ relations.
Possible values for the type and name attributes
are predefined by each referential context. The
others attribute may be assigned any string value,
and it is intended to represent any non-standard
piece of information conveyed by the expression.
For the spatial relations, possible values are the
object identifiers available from each scene.
The collected descriptions were fully annotated
by two independent annotators. After completion,
a third annotator assumed the role of judge and
provided the final annotation. Since the annotation
scheme was fairly straightforward (i.e., largely be-
cause all non-standard responses were simply as-
signed to the others attribute), agreement between
judges as measured by Kappa (Cohen, 1960) was
84% at the attribute level. Both referential con-
texts and referring expressions were represented
in XML format using a relational version of the
format adopted in TUNA (Gatt et al., 2007).
</bodyText>
<subsectionHeader confidence="0.999373">
3.6 Comparison with previous work
</subsectionHeader>
<bodyText confidence="0.999973125">
Table 1 presents a comparison between the col-
lected data and existing REG corpora3: the num-
ber of referring expressions (REs), the number of
subjects in each experiment, the number of possi-
ble atomic attributes (Attrib.) and possible land-
marks (LMs) in a description, the average descrip-
tion size (in number of annotated properties), and
the proportion of property usage, which is taken to
</bodyText>
<footnote confidence="0.9989226">
2The three prepositions were aggregated as a single at-
tribute because they have approximately the same meaning
in the languages under consideration
3The information on TUNA and Zoom descriptions is
based on the singular portion of each corpus only
</footnote>
<page confidence="0.998175">
71
</page>
<bodyText confidence="0.9995408">
be the proportion of properties that appear in the
description over the total number of possible at-
tributes and landmarks. From a REG perspective,
larger description sizes and lower usage rates may
suggest more complex situations of reference.
</bodyText>
<tableCaption confidence="0.999943">
Table 1: Comparison with existing REG corpora
</tableCaption>
<table confidence="0.999759888888889">
Corpus REs Subj. Attrib. LMs Avg.size Usage
TUNA-F 1200 60 4 0 3.1 0.8
TUNA-P 1080 60 10 0 3.1 0.3
GRE3D3 630 63 9 1 3.4 0.3
GRE3D7 4480 287 6 1 3.0 0.4
Stars 704 64 8 2 4.4 0.4
Stars2 884 56 9 2 3.3 0.3
Zoom-Pt 1358 93 19 4 6.7 0.3
Zoom-Sp 1234 80 19 4 7.2 0.3
</table>
<sectionHeader confidence="0.97903" genericHeader="method">
4 REG evaluation
</sectionHeader>
<bodyText confidence="0.999975853333333">
In what follows we illustrate the use of the Zoom
corpus as training and test data for a simple ma-
chine learning approach to REG adapted from
(Ferreira and Paraboni, 2014). The goal of this
evaluation is to provide reference results for future
comparison with purpose-built REG algorithms,
and not to present a complete REG solution for
the Zoom domain or others.
The present model consists of 12 binary clas-
sifiers representing whether individual referential
attributes should be selected for inclusion in an
output description. The classifiers correspond to
atomic attributes of the target and first landmark
object (type, name and others), and relations. Ref-
erential attributes of other landmark objects were
not modelled due to data sparsity and also to re-
duce computational costs. For similar reasons, the
multivalue between relation is also presently disre-
garded, and ‘corner’ relations involving two land-
marks (e.g., two streets) will be modelled as two
independent classification tasks.
Only two learning features are considered by
each classifier: landmarWount, which represents
the number of landmark objects near the main
target, and distractorCount, which represents the
number of objects of the same type as the target
within the relevant context in the map. For other
possible features applicable to this task, see, for
instance, (dos Santos Silva and Paraboni, 2015).
From the outcome of the 12 binary classifiers,
a description is built by considering atomic target
attributes in the first place. All attributes that cor-
respond to a positive prediction are selected for in-
clusion in the output description. Next, relations
are considered. If no relation is predicted, the
algorithm terminates by returning an atomic de-
scription of the main target object. If the descrip-
tion includes a relation, the corresponding land-
mark object is selected, and the algorithm is called
recursively to describe it as well. Since every at-
tribute that corresponds to a positive prediction
is always selected, the algorithm does not regard
uniqueness as a stop condition. As a result, the
output description may convey a certain amount
of overspecification.
For evaluation purposes, we used the subset of
singular descriptions from the Portuguese portion
of the corpus, comprising 821 descriptions. Evalu-
ation was carried out by comparing the corpus de-
scription with the system output to measure over-
all accuracy (i.e., the number of exact matches be-
tween the two descriptions), Dice (Dice, 1945) and
MASI (Passonneau, 2006) coefficients.
Following (Ferreira and Paraboni, 2014), we
built a REG model using support vector machines
with radial basis function kernel. The classifiers
were trained and tested using 6-fold cross valida-
tion. Optimal parameters were selected using grid
search as follows: for each step in the main k-fold
validation, one fold was reserved for testing, and
the remaining k − 1 folds were subject to a sec-
ondary cross-validation procedure in which differ-
ent parameter combinations were attempted. The
C parameter was assigned the values 1, 10, 100
and 1000, and γ was assigned 1, 0.1, 0.001 and
0.0001. The best-performing parameter set was
selected to build a classifier trained from the k −1
fold, and tested on the test data. This was repeated
for every iteration of the main cross-validation
procedure.
Table 2 summarises the results obtained by the
REG algorithm built from SVM classifiers, those
obtained by a baseline system representing a rela-
tional extension of the Dale &amp; Reiter Incremental
Algorithm, and by a Random selection strategy.
</bodyText>
<tableCaption confidence="0.982728">
Table 2: REG results
</tableCaption>
<table confidence="0.997011">
Algorithm Acc. Dice MASI
SVM 0.15 0.51 0.28
Incremental 0.04 0.53 0.21
Random selection 0.03 0.45 0.15
</table>
<bodyText confidence="0.83392325">
We compare accuracy scores obtained by ev-
ery algorithm pair using the chi-square test, and
we compare Dice scores using Wilcoxon’s signed-
rank test. In terms of overall accuracy, the SVM
</bodyText>
<page confidence="0.996425">
72
</page>
<bodyText confidence="0.9962972">
approach outperforms both alternatives. The dif-
ference from the second best-performing algo-
rithm (i.e., the Incremental approach) is significant
(χ2 = 79.87, df=1, p&lt;0.0001). Only in terms of
Dice scores a small effect in the opposite direction
is observed (T=137570.5, p= 0.01413).
We also assessed the performance of the indi-
vidual classifiers. Table 3 shows these results as
measured by precision (P), recall (R), F1-measure
(F1) and area under the ROC curve (AUC).
</bodyText>
<tableCaption confidence="0.992526">
Table 3: Classifier results
</tableCaption>
<table confidence="0.9760439">
Classifier P R Fl AUC
tg type 0.95 1.00 0.98 0.25
tg name 0.09 0.05 0.07 0.41
tg other 0.00 0.00 0.00 0.05
lm type 0.93 1.00 0.96 0.44
lm name 0.97 1.00 0.98 0.35
lm other 0.00 0.00 0.00 0.43
next-to 0.50 0.24 0.32 0.63
right-of 0.00 0.00 0.00 0.28
left-of 0.00 0.00 0.00 0.27
</table>
<bodyText confidence="0.879277625">
in-front-of 0.00 0.00 0.00 0.42
behind-of 0.00 0.00 0.00 0.17
in/on/at 0.60 0.60 0.60 0.61
From these results we notice that highly fre-
quent attributes (e.g., target type and landmark
name) were classified with high accuracy, whereas
others (e.g., multivalue attributes and relations)
were not.
</bodyText>
<sectionHeader confidence="0.998268" genericHeader="conclusions">
5 Discussion
</sectionHeader>
<bodyText confidence="0.9999301875">
This paper has introduced the Zoom corpus of nat-
ural language descriptions of map locations, a re-
source intended to support future research in REG
and related fields. Preliminary results of a SVM-
based approach to REG - which were solely pre-
sented for the future assessment of REG algo-
rithms based on Zoom data - hint at the actual
complexity of the REG task in this domain in a
number of ways. First, we notice that a simi-
lar approach in (Ferreira and Paraboni, 2014) on
GRE3D3 and GRE3D7 data has obtained consid-
erably higher mean accuracy. This is partially ex-
plained by the increased complexity of the Zoom
domain, but also by the currently simple annota-
tion scheme.
Second, we notice that Zoom descriptions are
prone to convey relations between a single target
and multiple landmark objects, as in ‘the restau-
rant between the 5th and 6th streets’. Although
common in language use, the use of multiple rela-
tional properties in this way has been little investi-
gated in the REG field.
Finally, we notice that the Zoom domain con-
tains two descriptions for every target object,
which are based on different - but related - mod-
els corresponding to the same map location seen
at different zoom levels. Interestingly, the refer-
ring expression in a 1X situation may or may not
be the same as in a 2X situation. Consider a map
with higher zoom level (2X) as illustrated in the
previous Fig. 2, and the same map location as seen
with lower zoom level in the previous Fig. 1.
</bodyText>
<figureCaption confidence="0.980993">
Figure 2: Map with a more detailed zoom level
</figureCaption>
<bodyText confidence="0.999949842105263">
The underlying models for these two maps are
certainly different, but not unrelated. The map
with 2X zoom contains fewer objects but may in-
clude more properties due to the added level of de-
tail. The referring expression for the target in the
1X map may or may not be the same as in the 2X
map. For instance, the referring expression “the
pub at Cowgate” is underspecified on the 1X map,
but it is minimally distinguishing on the 2X map.
Differences of this kind are common in inter-
active applications (e.g., in which the context of
reference may change in structure or in the num-
ber of objects and referable properties), and the
challenge for REG algorithms would be to pro-
duced an appropriate description for the modified
context without starting from scratch. REG algo-
rithms based on local context partitioning (Areces
et al., 2008) may have an advantage in this respect,
but further investigation is still required.
</bodyText>
<sectionHeader confidence="0.996535" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99092025">
This work has been supported by FAPESP and
CAPES. The authors are also grateful to the an-
notators Adriano S. R. da Silva, Jefferson S. F. da
Silva and Alan K. Yamasaki.
</bodyText>
<page confidence="0.998451">
73
</page>
<sectionHeader confidence="0.990037" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999681730769231">
Romina Altamirano, Carlos Areces, and Luciana
Benotti. 2012. Probabilistic refinement algorithms
for the generation of referring expressions. In COL-
ING (Posters), pages 53–62.
Carlos Areces, Alexander Koller, and Kristina Strieg-
nitz. 2008. Referring expressions as formulas of
description logic. In Proceedings of the Fifth Inter-
national Natural Language Generation Conference,
INLG ’08, pages 42–49, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
A. Arts, A. Maes, L. G. M. Noordman, and C. Jansen.
2011. Overspecification facilitates object identifica-
tion. Journal of Pragmatics, 43(1):361–374.
J. Cohen. 1960. A coeficient of agreement for nom-
inal scales. Educational and Psychological Mea-
surement, 20(1):37–46.
Robert Dale and Ehud Reiter. 1995. Computational
interpretations of the Gricean maxims in the gener-
ation of referring expressions. Cognitive Science,
19(2):233–263.
Robert Dale and Jette Viethen. 2009. Referring ex-
pression generation through attribute-based heuris-
tics. In Proceedings of ENLG-2009, pages 58–65.
Robert Dale. 2002. Cooking up referring expressions.
In Proceedings of the 27th Annual Meeting of the As-
sociation for Computational Linguistics, pages 68–
75.
L. R. Dice. 1945. Measures of the amount of ecologic
association between species. Ecology, 26(3):297–
302.
Diego dos Santos Silva and Ivandr´e Paraboni. 2015.
Generating spatial referring expressions in interac-
tive 3D worlds. Spatial Cognition and Computation.
P. E. Engelhardt and K. Baileyand F. Ferreira. 2006.
Do speakers and listeners observe the Gricean
maxim of quantity? Journal of Memory and Lan-
guage, 54(4):554–573.
P. E. Engelhardt, S. B. Demiral, and Fernanda Ferreira.
2011. Over-specified referring expressions impair
comprehension: An ERP study. Brain and Cogni-
tion, 77(2):304–314.
Giuseppe Di Fabbrizio, Amanda J. Stent, and Srini-
vas Bangalore. 2008. Trainable speaker-based re-
ferring expression generation. In Proceedings of
the Twelfth Conference on Computational Natural
Language Learning, CoNLL ’08, pages 151–158,
Stroudsburg, PA, USA.
Thiago Castro Ferreira and Ivandr´e Paraboni. 2014.
Classification-based referring expression genera-
tion. Lecture Notes in Computer Science, 8403:481–
491.
C. Gardent. 2002. Generating minimal definite de-
scriptions. In Proceedings of the 40th Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 96–103.
Konstantina Garoufi and Alexander Koller. 2013.
Generation of effective referring expressions in sit-
uated context. Language and Cognitive Processes,
29(8):986–1001.
Albert Gatt, Ilka van der Sluis, and Kees van Deemter.
2007. Evaluating algorithms for the generation of
referring expressions using a balanced corpus. In
Proceedings of ENLG-07.
Albert Gatt, Anja Belz, and Eric Kow. 2009. The
TUNA challenge 2009: Overview and evaluation re-
sults. In Proceedings of the 12nd European Work-
shop on Natural Language Generation, pages 174–
182.
Albert Gatt, R. van Gompel, E. Krahmer, and K. van
Deemter. 2011. Non-deterministic attribute selec-
tion in reference production. In Workshop on the
Production of Referring Expressions (PRE-CogSci
2011), pages 1–7.
Albert Gatt, E. Krahmer, R. van Gompel, and K. van
Deemter. 2013. Production of referring expres-
sions: Preference trumps discrimination. In 35th
Meeting of the Cognitive Science Society, pages
483–488.
Pamela W. Jordan and Marilyn A. Walker. 2005.
Learning content selection rules for generating ob-
ject descriptions in dialogue. J. Artif. Int. Res.,
24(1):157–194.
J. D. Kelleher and G. Kruijff. 2006. Incremental gen-
eration of spatial referring expressions in situated di-
alog. In Proceedings of the 21st International Con-
ference on Computational Linguistics and 44th An-
nual Meeting of the ACL, pages 1041–1048.
Ruud Koolen, Albert Gatt, Martijn Goudbeek, and
Emiel Krahmer. 2011. Factors causing overspec-
ification in definite descriptions. Journal of Prag-
matics, 43(13):3231–3250.
Emiel Krahmer and Mariet Theune. 2002. Effi-
cient context-sensitive generation of referring ex-
pressions. In Kees van Deemter and Rodger Kibble,
editors, Information Sharing: Reference and Pre-
supposition in Language Generation and Interpre-
tation, pages 223–264. CSLI Publications, Stanford,
CA.
Emiel Krahmer and Kees van Deemter. 2012. Compu-
tational generation of referring expressions: A sur-
vey. Computational Linguistics, 38(1):173–218.
Emiel Krahmer, Sebastiaan van Erk, and Andre Verleg.
2003. Graph-based generation of referring expres-
sions. Computational Linguistics, 29(1):53–72.
</reference>
<page confidence="0.979947">
74
</page>
<reference confidence="0.999600861111111">
Rebecca Passonneau. 2006. Measuring agreement
on set-valued items (MASI) for semantic and prag-
matic annotation. In Proceedings of the Interna-
tional Conference on Language Resources and Eval-
uation (LREC).
T. Pechmann. 1989. Incremental speech produc-
tion and referential overspecification. Linguistics,
27(1):98–110.
Caio V. M. Teixeira, Ivandr´e Paraboni, Adriano S. R.
da Silva, and Alan K. Yamasaki. 2014. Gener-
ating relational descriptions involving mutual dis-
ambiguation. Lecture Notes in Computer Science,
8403:492–502.
R. van Gompel, Albert Gatt, E. Krahmer, and K. van
Deemter. 2012. PRO: A computational model
of referential overspecification. In Proceedings of
AMLAP-2012.
Jette Viethen and Robert Dale. 2010. Speaker-
dependent variation in content selection for referring
expression generation. In Proceedings of the Aus-
tralasian Language Technology Association Work-
shop 2010, pages 81–89, Melbourne, Australia.
Jette Viethen and Robert Dale. 2011. GRE3D7: A cor-
pus of distinguishing descriptions for objects in vi-
sual scenes. In Proceedings of UCNLG+Eval-2011,
pages 12–22.
Jette Viethen, Margaret Mitchell, and Emiel Krahmer.
2013. Graphs and spatial relations in the generation
of referring expressions. In Proceedings of the 14th
European Workshop on Natural Language Genera-
tion, pages 72–81, Sofia, Bulgaria, August. Associ-
ation for Computational Linguistics.
Jette Viethen. 2011. The Generation of Natural De-
scriptions: Corpus-Based Investigations of Refer-
ring Expressions in Visual Domains. Ph.D. thesis,
Macquarie University, Sydney, Australia.
</reference>
<page confidence="0.999138">
75
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.697189">
<title confidence="0.999431">Zoom: a corpus of natural language descriptions of map locations</title>
<author confidence="0.984079">Romina Altamirano Thiago C Ferreira Ivandr´e Paraboni Luciana Benotti</author>
<affiliation confidence="0.9273825">FaMAF EACH-USP EACH-USP FaMAF Nat.Univ. of C´ordoba of Paulo of Paulo Nat.Univ. of C´ordoba</affiliation>
<address confidence="0.992018">C´ordoba, Argentina Paulo, Brazil Paulo, Brazil C´ordoba, Argentina</address>
<email confidence="0.887863">ialtamir@famaf.unc.edu.arthiago.castro.ferreira@usp.brivandre@usp.brbenotti@famaf.unc.edu.ar</email>
<abstract confidence="0.995489944444444">This paper describes an experiment to elicit referring expressions from human subjects for research in natural language generation and related fields, and preliminary results of a computational model for the generation of these expressions. Unlike existing resources of this kind, the resulting data set the Zoom corpus of natural language descriptions of map locations takes into account a domain that is significantly closer to real-world applications than what has been considered in previous work, and addresses more complex situations of reference, including contexts with different levels of detail, and instances of singular and plural reference produced by speakers of Spanish and Portuguese.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Romina Altamirano</author>
<author>Carlos Areces</author>
<author>Luciana Benotti</author>
</authors>
<title>Probabilistic refinement algorithms for the generation of referring expressions.</title>
<date>2012</date>
<booktitle>In COLING (Posters),</booktitle>
<pages>53--62</pages>
<contexts>
<context position="5532" citStr="Altamirano et al., 2012" startWordPosition="855" endWordPosition="858">main that is considerably closer to real-world applications (namely, city maps in different degrees of detail represented by zoom levels) than what has been considered in previous work, involving both singular and plural reference, and making extensive use of relational properties. Moreover, Zoom descriptions were produced by both Spanish and Portuguese speakers, which will allow (to the best of our knowledge, for the first time) a comprehensive study of the REG surface realisation subtask in these languages, and enable research on the issues of human variation in REG (Fabbrizio et al., 2008; Altamirano et al., 2012; Gatt et al., 2011). 2 Related work TUNA (Gatt et al., 2007) was the first prominent REG corpus to be made publicly available for research purposes. The corpus was developed in a series of controlled experiments, containing 2280 atomic descriptions produced by 60 speakers of English in two domains (1200 descriptions of furniture items and 1080 descriptions of people’s photographs). TUNA has been used in a series of REG shared tasks (Gatt et al., 2009). GRE3D3 and its extension GRE3D7 (Dale and Viethen, 2009; Viethen and Dale, 2011) were developed in a series of web-based experiments primarily</context>
</contexts>
<marker>Altamirano, Areces, Benotti, 2012</marker>
<rawString>Romina Altamirano, Carlos Areces, and Luciana Benotti. 2012. Probabilistic refinement algorithms for the generation of referring expressions. In COLING (Posters), pages 53–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carlos Areces</author>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
</authors>
<title>Referring expressions as formulas of description logic.</title>
<date>2008</date>
<booktitle>In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08,</booktitle>
<pages>42--49</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Areces, Koller, Striegnitz, 2008</marker>
<rawString>Carlos Areces, Alexander Koller, and Kristina Striegnitz. 2008. Referring expressions as formulas of description logic. In Proceedings of the Fifth International Natural Language Generation Conference, INLG ’08, pages 42–49, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Arts</author>
<author>A Maes</author>
<author>L G M Noordman</author>
<author>C Jansen</author>
</authors>
<title>Overspecification facilitates object identification.</title>
<date>2011</date>
<journal>Journal of Pragmatics,</journal>
<volume>43</volume>
<issue>1</issue>
<contexts>
<context position="2573" citStr="Arts et al., 2011" startWordPosition="386" endWordPosition="389">006; Viethen et al., 2013). The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g., (Jordan and Walker, 2005; Viethen and Dale, 2010; Viethen, 2</context>
</contexts>
<marker>Arts, Maes, Noordman, Jansen, 2011</marker>
<rawString>A. Arts, A. Maes, L. G. M. Noordman, and C. Jansen. 2011. Overspecification facilitates object identification. Journal of Pragmatics, 43(1):361–374.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Cohen</author>
</authors>
<title>A coeficient of agreement for nominal scales.</title>
<date>1960</date>
<booktitle>Educational and Psychological Measurement,</booktitle>
<pages>20--1</pages>
<contexts>
<context position="12091" citStr="Cohen, 1960" startWordPosition="1889" endWordPosition="1890">be assigned any string value, and it is intended to represent any non-standard piece of information conveyed by the expression. For the spatial relations, possible values are the object identifiers available from each scene. The collected descriptions were fully annotated by two independent annotators. After completion, a third annotator assumed the role of judge and provided the final annotation. Since the annotation scheme was fairly straightforward (i.e., largely because all non-standard responses were simply assigned to the others attribute), agreement between judges as measured by Kappa (Cohen, 1960) was 84% at the attribute level. Both referential contexts and referring expressions were represented in XML format using a relational version of the format adopted in TUNA (Gatt et al., 2007). 3.6 Comparison with previous work Table 1 presents a comparison between the collected data and existing REG corpora3: the number of referring expressions (REs), the number of subjects in each experiment, the number of possible atomic attributes (Attrib.) and possible landmarks (LMs) in a description, the average description size (in number of annotated properties), and the proportion of property usage, </context>
</contexts>
<marker>Cohen, 1960</marker>
<rawString>J. Cohen. 1960. A coeficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Ehud Reiter</author>
</authors>
<title>Computational interpretations of the Gricean maxims in the generation of referring expressions.</title>
<date>1995</date>
<journal>Cognitive Science,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2148" citStr="Dale and Reiter, 1995" startWordPosition="318" endWordPosition="321"> etc.) has received significant attention in the field, and it is also the focus of the present work. The input to a REG algorithm is a context set C containing an intended referent r and a number of distractor objects. All objects are represented as attribute-value pairs representing either atomic (type-restaurant) or relational (on-5thstreet) properties (Krahmer and Theune, 2002; Krahmer et al., 2003; Kelleher and Kruijff, 2006; Viethen et al., 2013). The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van D</context>
</contexts>
<marker>Dale, Reiter, 1995</marker>
<rawString>Robert Dale and Ehud Reiter. 1995. Computational interpretations of the Gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
<author>Jette Viethen</author>
</authors>
<title>Referring expression generation through attribute-based heuristics.</title>
<date>2009</date>
<booktitle>In Proceedings of ENLG-2009,</booktitle>
<pages>58--65</pages>
<contexts>
<context position="3853" citStr="Dale and Viethen, 2009" startWordPosition="594" endWordPosition="597">4)). A possible explanation for the small interest in ML for REG may be the relatively low availability of data. While research in many fields may benefit from the wide availability of text corpora (e.g., obtainable from the web), research in REG usually requires highly specialised data - hereby called REG corpora - conveying not only referring expressions produced by human speakers, but also a fully-annotated representation of the context (i.e., all objects and their semantic properties) within which the expressions have been produced. REG corpora such as TUNA (Gatt et al., 2007) and GRE3D3 (Dale and Viethen, 2009) are useful both to gain general insights on human language production, and to benefit from data-intensive computational techniques such as ML. However, being usually the final product of controlled experiments involving human subjects, REG cor69 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 69–75, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics pora tend to address highly specific research questions. For instance, GRE3D3 i</context>
<context position="6045" citStr="Dale and Viethen, 2009" startWordPosition="940" endWordPosition="943"> and enable research on the issues of human variation in REG (Fabbrizio et al., 2008; Altamirano et al., 2012; Gatt et al., 2011). 2 Related work TUNA (Gatt et al., 2007) was the first prominent REG corpus to be made publicly available for research purposes. The corpus was developed in a series of controlled experiments, containing 2280 atomic descriptions produced by 60 speakers of English in two domains (1200 descriptions of furniture items and 1080 descriptions of people’s photographs). TUNA has been used in a series of REG shared tasks (Gatt et al., 2009). GRE3D3 and its extension GRE3D7 (Dale and Viethen, 2009; Viethen and Dale, 2011) were developed in a series of web-based experiments primarily focussed on the study of relational descriptions. GRE3D3 contains 630 descriptions produced by 63 speakers, and GRE3D7 contains 4480 descriptions produced by 287 speakers. In both cases, the language of the experiment was English. The domain consists of simple visual scenes conveying boxes and spheres. Stars (Teixeira et al., 2014) and its extension Stars2 were collected for the study of referential overspecification. Stars contains 704 descriptions produced by 64 speakers in a web-based experiment. Stars2 </context>
</contexts>
<marker>Dale, Viethen, 2009</marker>
<rawString>Robert Dale and Jette Viethen. 2009. Referring expression generation through attribute-based heuristics. In Proceedings of ENLG-2009, pages 58–65.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Dale</author>
</authors>
<title>Cooking up referring expressions.</title>
<date>2002</date>
<booktitle>In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>68--75</pages>
<contexts>
<context position="2343" citStr="Dale, 2002" startWordPosition="352" endWordPosition="353">tor objects. All objects are represented as attribute-value pairs representing either atomic (type-restaurant) or relational (on-5thstreet) properties (Krahmer and Theune, 2002; Krahmer et al., 2003; Kelleher and Kruijff, 2006; Viethen et al., 2013). The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Re</context>
</contexts>
<marker>Dale, 2002</marker>
<rawString>Robert Dale. 2002. Cooking up referring expressions. In Proceedings of the 27th Annual Meeting of the Association for Computational Linguistics, pages 68– 75.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L R Dice</author>
</authors>
<title>Measures of the amount of ecologic association between species.</title>
<date>1945</date>
<journal>Ecology,</journal>
<volume>26</volume>
<issue>3</issue>
<pages>302</pages>
<contexts>
<context position="16021" citStr="Dice, 1945" startWordPosition="2533" endWordPosition="2534">lled recursively to describe it as well. Since every attribute that corresponds to a positive prediction is always selected, the algorithm does not regard uniqueness as a stop condition. As a result, the output description may convey a certain amount of overspecification. For evaluation purposes, we used the subset of singular descriptions from the Portuguese portion of the corpus, comprising 821 descriptions. Evaluation was carried out by comparing the corpus description with the system output to measure overall accuracy (i.e., the number of exact matches between the two descriptions), Dice (Dice, 1945) and MASI (Passonneau, 2006) coefficients. Following (Ferreira and Paraboni, 2014), we built a REG model using support vector machines with radial basis function kernel. The classifiers were trained and tested using 6-fold cross validation. Optimal parameters were selected using grid search as follows: for each step in the main k-fold validation, one fold was reserved for testing, and the remaining k − 1 folds were subject to a secondary cross-validation procedure in which different parameter combinations were attempted. The C parameter was assigned the values 1, 10, 100 and 1000, and γ was as</context>
</contexts>
<marker>Dice, 1945</marker>
<rawString>L. R. Dice. 1945. Measures of the amount of ecologic association between species. Ecology, 26(3):297– 302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diego dos Santos Silva</author>
<author>Ivandr´e Paraboni</author>
</authors>
<title>Generating spatial referring expressions in interactive 3D worlds. Spatial Cognition and Computation.</title>
<date>2015</date>
<contexts>
<context position="14910" citStr="Silva and Paraboni, 2015" startWordPosition="2354" endWordPosition="2357">o reduce computational costs. For similar reasons, the multivalue between relation is also presently disregarded, and ‘corner’ relations involving two landmarks (e.g., two streets) will be modelled as two independent classification tasks. Only two learning features are considered by each classifier: landmarWount, which represents the number of landmark objects near the main target, and distractorCount, which represents the number of objects of the same type as the target within the relevant context in the map. For other possible features applicable to this task, see, for instance, (dos Santos Silva and Paraboni, 2015). From the outcome of the 12 binary classifiers, a description is built by considering atomic target attributes in the first place. All attributes that correspond to a positive prediction are selected for inclusion in the output description. Next, relations are considered. If no relation is predicted, the algorithm terminates by returning an atomic description of the main target object. If the description includes a relation, the corresponding landmark object is selected, and the algorithm is called recursively to describe it as well. Since every attribute that corresponds to a positive predic</context>
</contexts>
<marker>Silva, Paraboni, 2015</marker>
<rawString>Diego dos Santos Silva and Ivandr´e Paraboni. 2015. Generating spatial referring expressions in interactive 3D worlds. Spatial Cognition and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P E Engelhardt</author>
<author>K Baileyand F Ferreira</author>
</authors>
<title>Do speakers and listeners observe the Gricean maxim of quantity?</title>
<date>2006</date>
<journal>Journal of Memory and Language,</journal>
<volume>54</volume>
<issue>4</issue>
<contexts>
<context position="2650" citStr="Engelhardt and Ferreira, 2006" startWordPosition="399" endWordPosition="402">ntifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g., (Jordan and Walker, 2005; Viethen and Dale, 2010; Viethen, 2011; Garoufi and Koller, 2013; Ferreira and Paraboni, 2014)). A possible expl</context>
</contexts>
<marker>Engelhardt, Ferreira, 2006</marker>
<rawString>P. E. Engelhardt and K. Baileyand F. Ferreira. 2006. Do speakers and listeners observe the Gricean maxim of quantity? Journal of Memory and Language, 54(4):554–573.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P E Engelhardt</author>
<author>S B Demiral</author>
<author>Fernanda Ferreira</author>
</authors>
<title>Over-specified referring expressions impair comprehension: An ERP study.</title>
<date>2011</date>
<journal>Brain and Cognition,</journal>
<volume>77</volume>
<issue>2</issue>
<contexts>
<context position="2676" citStr="Engelhardt et al., 2011" startWordPosition="403" endWordPosition="406">nown to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g., (Jordan and Walker, 2005; Viethen and Dale, 2010; Viethen, 2011; Garoufi and Koller, 2013; Ferreira and Paraboni, 2014)). A possible explanation for the small inte</context>
</contexts>
<marker>Engelhardt, Demiral, Ferreira, 2011</marker>
<rawString>P. E. Engelhardt, S. B. Demiral, and Fernanda Ferreira. 2011. Over-specified referring expressions impair comprehension: An ERP study. Brain and Cognition, 77(2):304–314.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Giuseppe Di Fabbrizio</author>
<author>Amanda J Stent</author>
<author>Srinivas Bangalore</author>
</authors>
<title>Trainable speaker-based referring expression generation.</title>
<date>2008</date>
<booktitle>In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08,</booktitle>
<pages>151--158</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>Di Fabbrizio, Stent, Bangalore, 2008</marker>
<rawString>Giuseppe Di Fabbrizio, Amanda J. Stent, and Srinivas Bangalore. 2008. Trainable speaker-based referring expression generation. In Proceedings of the Twelfth Conference on Computational Natural Language Learning, CoNLL ’08, pages 151–158, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thiago Castro Ferreira</author>
<author>Ivandr´e Paraboni</author>
</authors>
<title>Classification-based referring expression generation.</title>
<date>2014</date>
<journal>Lecture Notes in Computer Science,</journal>
<volume>8403</volume>
<pages>491</pages>
<contexts>
<context position="3232" citStr="Ferreira and Paraboni, 2014" startWordPosition="493" endWordPosition="496">el et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g., (Jordan and Walker, 2005; Viethen and Dale, 2010; Viethen, 2011; Garoufi and Koller, 2013; Ferreira and Paraboni, 2014)). A possible explanation for the small interest in ML for REG may be the relatively low availability of data. While research in many fields may benefit from the wide availability of text corpora (e.g., obtainable from the web), research in REG usually requires highly specialised data - hereby called REG corpora - conveying not only referring expressions produced by human speakers, but also a fully-annotated representation of the context (i.e., all objects and their semantic properties) within which the expressions have been produced. REG corpora such as TUNA (Gatt et al., 2007) and GRE3D3 (Da</context>
<context position="13700" citStr="Ferreira and Paraboni, 2014" startWordPosition="2170" endWordPosition="2173">landmarks. From a REG perspective, larger description sizes and lower usage rates may suggest more complex situations of reference. Table 1: Comparison with existing REG corpora Corpus REs Subj. Attrib. LMs Avg.size Usage TUNA-F 1200 60 4 0 3.1 0.8 TUNA-P 1080 60 10 0 3.1 0.3 GRE3D3 630 63 9 1 3.4 0.3 GRE3D7 4480 287 6 1 3.0 0.4 Stars 704 64 8 2 4.4 0.4 Stars2 884 56 9 2 3.3 0.3 Zoom-Pt 1358 93 19 4 6.7 0.3 Zoom-Sp 1234 80 19 4 7.2 0.3 4 REG evaluation In what follows we illustrate the use of the Zoom corpus as training and test data for a simple machine learning approach to REG adapted from (Ferreira and Paraboni, 2014). The goal of this evaluation is to provide reference results for future comparison with purpose-built REG algorithms, and not to present a complete REG solution for the Zoom domain or others. The present model consists of 12 binary classifiers representing whether individual referential attributes should be selected for inclusion in an output description. The classifiers correspond to atomic attributes of the target and first landmark object (type, name and others), and relations. Referential attributes of other landmark objects were not modelled due to data sparsity and also to reduce comput</context>
<context position="16103" citStr="Ferreira and Paraboni, 2014" startWordPosition="2541" endWordPosition="2544">at corresponds to a positive prediction is always selected, the algorithm does not regard uniqueness as a stop condition. As a result, the output description may convey a certain amount of overspecification. For evaluation purposes, we used the subset of singular descriptions from the Portuguese portion of the corpus, comprising 821 descriptions. Evaluation was carried out by comparing the corpus description with the system output to measure overall accuracy (i.e., the number of exact matches between the two descriptions), Dice (Dice, 1945) and MASI (Passonneau, 2006) coefficients. Following (Ferreira and Paraboni, 2014), we built a REG model using support vector machines with radial basis function kernel. The classifiers were trained and tested using 6-fold cross validation. Optimal parameters were selected using grid search as follows: for each step in the main k-fold validation, one fold was reserved for testing, and the remaining k − 1 folds were subject to a secondary cross-validation procedure in which different parameter combinations were attempted. The C parameter was assigned the values 1, 10, 100 and 1000, and γ was assigned 1, 0.1, 0.001 and 0.0001. The best-performing parameter set was selected to</context>
<context position="18961" citStr="Ferreira and Paraboni, 2014" startWordPosition="3023" endWordPosition="3026">utes (e.g., target type and landmark name) were classified with high accuracy, whereas others (e.g., multivalue attributes and relations) were not. 5 Discussion This paper has introduced the Zoom corpus of natural language descriptions of map locations, a resource intended to support future research in REG and related fields. Preliminary results of a SVMbased approach to REG - which were solely presented for the future assessment of REG algorithms based on Zoom data - hint at the actual complexity of the REG task in this domain in a number of ways. First, we notice that a similar approach in (Ferreira and Paraboni, 2014) on GRE3D3 and GRE3D7 data has obtained considerably higher mean accuracy. This is partially explained by the increased complexity of the Zoom domain, but also by the currently simple annotation scheme. Second, we notice that Zoom descriptions are prone to convey relations between a single target and multiple landmark objects, as in ‘the restaurant between the 5th and 6th streets’. Although common in language use, the use of multiple relational properties in this way has been little investigated in the REG field. Finally, we notice that the Zoom domain contains two descriptions for every targe</context>
</contexts>
<marker>Ferreira, Paraboni, 2014</marker>
<rawString>Thiago Castro Ferreira and Ivandr´e Paraboni. 2014. Classification-based referring expression generation. Lecture Notes in Computer Science, 8403:481– 491.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Gardent</author>
</authors>
<title>Generating minimal definite descriptions.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="2359" citStr="Gardent, 2002" startWordPosition="354" endWordPosition="355"> All objects are represented as attribute-value pairs representing either atomic (type-restaurant) or relational (on-5thstreet) properties (Krahmer and Theune, 2002; Krahmer et al., 2003; Kelleher and Kruijff, 2006; Viethen et al., 2013). The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The</context>
</contexts>
<marker>Gardent, 2002</marker>
<rawString>C. Gardent. 2002. Generating minimal definite descriptions. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Konstantina Garoufi</author>
<author>Alexander Koller</author>
</authors>
<title>Generation of effective referring expressions in situated context. Language and Cognitive Processes,</title>
<date>2013</date>
<contexts>
<context position="3202" citStr="Garoufi and Koller, 2013" startWordPosition="489" endWordPosition="492">len et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g., (Jordan and Walker, 2005; Viethen and Dale, 2010; Viethen, 2011; Garoufi and Koller, 2013; Ferreira and Paraboni, 2014)). A possible explanation for the small interest in ML for REG may be the relatively low availability of data. While research in many fields may benefit from the wide availability of text corpora (e.g., obtainable from the web), research in REG usually requires highly specialised data - hereby called REG corpora - conveying not only referring expressions produced by human speakers, but also a fully-annotated representation of the context (i.e., all objects and their semantic properties) within which the expressions have been produced. REG corpora such as TUNA (Gat</context>
</contexts>
<marker>Garoufi, Koller, 2013</marker>
<rawString>Konstantina Garoufi and Alexander Koller. 2013. Generation of effective referring expressions in situated context. Language and Cognitive Processes, 29(8):986–1001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Ilka van der Sluis</author>
<author>Kees van Deemter</author>
</authors>
<title>Evaluating algorithms for the generation of referring expressions using a balanced corpus.</title>
<date>2007</date>
<booktitle>In Proceedings of ENLG-07.</booktitle>
<marker>Gatt, van der Sluis, van Deemter, 2007</marker>
<rawString>Albert Gatt, Ilka van der Sluis, and Kees van Deemter. 2007. Evaluating algorithms for the generation of referring expressions using a balanced corpus. In Proceedings of ENLG-07.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>Anja Belz</author>
<author>Eric Kow</author>
</authors>
<title>The TUNA challenge 2009: Overview and evaluation results.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12nd European Workshop on Natural Language Generation,</booktitle>
<pages>174--182</pages>
<contexts>
<context position="5988" citStr="Gatt et al., 2009" startWordPosition="931" endWordPosition="934">e REG surface realisation subtask in these languages, and enable research on the issues of human variation in REG (Fabbrizio et al., 2008; Altamirano et al., 2012; Gatt et al., 2011). 2 Related work TUNA (Gatt et al., 2007) was the first prominent REG corpus to be made publicly available for research purposes. The corpus was developed in a series of controlled experiments, containing 2280 atomic descriptions produced by 60 speakers of English in two domains (1200 descriptions of furniture items and 1080 descriptions of people’s photographs). TUNA has been used in a series of REG shared tasks (Gatt et al., 2009). GRE3D3 and its extension GRE3D7 (Dale and Viethen, 2009; Viethen and Dale, 2011) were developed in a series of web-based experiments primarily focussed on the study of relational descriptions. GRE3D3 contains 630 descriptions produced by 63 speakers, and GRE3D7 contains 4480 descriptions produced by 287 speakers. In both cases, the language of the experiment was English. The domain consists of simple visual scenes conveying boxes and spheres. Stars (Teixeira et al., 2014) and its extension Stars2 were collected for the study of referential overspecification. Stars contains 704 descriptions p</context>
</contexts>
<marker>Gatt, Belz, Kow, 2009</marker>
<rawString>Albert Gatt, Anja Belz, and Eric Kow. 2009. The TUNA challenge 2009: Overview and evaluation results. In Proceedings of the 12nd European Workshop on Natural Language Generation, pages 174– 182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>R van Gompel</author>
<author>E Krahmer</author>
<author>K van Deemter</author>
</authors>
<title>Non-deterministic attribute selection in reference production.</title>
<date>2011</date>
<booktitle>In Workshop on the Production of Referring Expressions</booktitle>
<pages>1--7</pages>
<marker>Gatt, van Gompel, Krahmer, van Deemter, 2011</marker>
<rawString>Albert Gatt, R. van Gompel, E. Krahmer, and K. van Deemter. 2011. Non-deterministic attribute selection in reference production. In Workshop on the Production of Referring Expressions (PRE-CogSci 2011), pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Albert Gatt</author>
<author>E Krahmer</author>
<author>R van Gompel</author>
<author>K van Deemter</author>
</authors>
<title>Production of referring expressions: Preference trumps discrimination.</title>
<date>2013</date>
<booktitle>In 35th Meeting of the Cognitive Science Society,</booktitle>
<pages>483--488</pages>
<marker>Gatt, Krahmer, van Gompel, van Deemter, 2013</marker>
<rawString>Albert Gatt, E. Krahmer, R. van Gompel, and K. van Deemter. 2013. Production of referring expressions: Preference trumps discrimination. In 35th Meeting of the Cognitive Science Society, pages 483–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pamela W Jordan</author>
<author>Marilyn A Walker</author>
</authors>
<title>Learning content selection rules for generating object descriptions in dialogue.</title>
<date>2005</date>
<journal>J. Artif. Int. Res.,</journal>
<volume>24</volume>
<issue>1</issue>
<contexts>
<context position="3137" citStr="Jordan and Walker, 2005" startWordPosition="479" endWordPosition="482">sambiguation is said to be overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g., (Jordan and Walker, 2005; Viethen and Dale, 2010; Viethen, 2011; Garoufi and Koller, 2013; Ferreira and Paraboni, 2014)). A possible explanation for the small interest in ML for REG may be the relatively low availability of data. While research in many fields may benefit from the wide availability of text corpora (e.g., obtainable from the web), research in REG usually requires highly specialised data - hereby called REG corpora - conveying not only referring expressions produced by human speakers, but also a fully-annotated representation of the context (i.e., all objects and their semantic properties) within which </context>
</contexts>
<marker>Jordan, Walker, 2005</marker>
<rawString>Pamela W. Jordan and Marilyn A. Walker. 2005. Learning content selection rules for generating object descriptions in dialogue. J. Artif. Int. Res., 24(1):157–194.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J D Kelleher</author>
<author>G Kruijff</author>
</authors>
<title>Incremental generation of spatial referring expressions in situated dialog.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,</booktitle>
<pages>1041--1048</pages>
<contexts>
<context position="1959" citStr="Kelleher and Kruijff, 2006" startWordPosition="282" endWordPosition="285">omain entities. In particular, the issue of how to determine the semantic contents of definite descriptions (e.g., ‘the Indian restaurant on 5th street’, ‘the restaurant we went to last night’, etc.) has received significant attention in the field, and it is also the focus of the present work. The input to a REG algorithm is a context set C containing an intended referent r and a number of distractor objects. All objects are represented as attribute-value pairs representing either atomic (type-restaurant) or relational (on-5thstreet) properties (Krahmer and Theune, 2002; Krahmer et al., 2003; Kelleher and Kruijff, 2006; Viethen et al., 2013). The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Art</context>
<context position="4556" citStr="Kelleher and Kruijff, 2006" startWordPosition="694" endWordPosition="697">to benefit from data-intensive computational techniques such as ML. However, being usually the final product of controlled experiments involving human subjects, REG cor69 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 69–75, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics pora tend to address highly specific research questions. For instance, GRE3D3 is largely devoted to the investigation of relational referring expressions (Kelleher and Kruijff, 2006) in simple visual scenes involving geometric shapes, as in ‘the large ball next to the red cube’. As a result, and despite the usefulness of these resources to a large body of work in REG, further research questions will usually require the collection of new data. In this paper we introduce the Zoom corpus of referring expressions. Zoom addresses a domain that is considerably closer to real-world applications (namely, city maps in different degrees of detail represented by zoom levels) than what has been considered in previous work, involving both singular and plural reference, and making exte</context>
</contexts>
<marker>Kelleher, Kruijff, 2006</marker>
<rawString>J. D. Kelleher and G. Kruijff. 2006. Incremental generation of spatial referring expressions in situated dialog. In Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 1041–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ruud Koolen</author>
<author>Albert Gatt</author>
<author>Martijn Goudbeek</author>
<author>Emiel Krahmer</author>
</authors>
<title>Factors causing overspecification in definite descriptions.</title>
<date>2011</date>
<journal>Journal of Pragmatics,</journal>
<volume>43</volume>
<issue>13</issue>
<contexts>
<context position="2594" citStr="Koolen et al., 2011" startWordPosition="390" endWordPosition="393">, 2013). The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g., (Jordan and Walker, 2005; Viethen and Dale, 2010; Viethen, 2011; Garoufi and Koll</context>
</contexts>
<marker>Koolen, Gatt, Goudbeek, Krahmer, 2011</marker>
<rawString>Ruud Koolen, Albert Gatt, Martijn Goudbeek, and Emiel Krahmer. 2011. Factors causing overspecification in definite descriptions. Journal of Pragmatics, 43(13):3231–3250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Mariet Theune</author>
</authors>
<title>Efficient context-sensitive generation of referring expressions.</title>
<date>2002</date>
<booktitle>Information Sharing: Reference and Presupposition in Language Generation and Interpretation,</booktitle>
<pages>223--264</pages>
<editor>In Kees van Deemter and Rodger Kibble, editors,</editor>
<publisher>CSLI Publications,</publisher>
<location>Stanford, CA.</location>
<contexts>
<context position="1909" citStr="Krahmer and Theune, 2002" startWordPosition="274" endWordPosition="277"> definite descriptions, proper names, etc.) of domain entities. In particular, the issue of how to determine the semantic contents of definite descriptions (e.g., ‘the Indian restaurant on 5th street’, ‘the restaurant we went to last night’, etc.) has received significant attention in the field, and it is also the focus of the present work. The input to a REG algorithm is a context set C containing an intended referent r and a number of distractor objects. All objects are represented as attribute-value pairs representing either atomic (type-restaurant) or relational (on-5thstreet) properties (Krahmer and Theune, 2002; Krahmer et al., 2003; Kelleher and Kruijff, 2006; Viethen et al., 2013). The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required f</context>
</contexts>
<marker>Krahmer, Theune, 2002</marker>
<rawString>Emiel Krahmer and Mariet Theune. 2002. Efficient context-sensitive generation of referring expressions. In Kees van Deemter and Rodger Kibble, editors, Information Sharing: Reference and Presupposition in Language Generation and Interpretation, pages 223–264. CSLI Publications, Stanford, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Kees van Deemter</author>
</authors>
<title>Computational generation of referring expressions: A survey.</title>
<date>2012</date>
<journal>Computational Linguistics,</journal>
<volume>38</volume>
<issue>1</issue>
<marker>Krahmer, van Deemter, 2012</marker>
<rawString>Emiel Krahmer and Kees van Deemter. 2012. Computational generation of referring expressions: A survey. Computational Linguistics, 38(1):173–218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiel Krahmer</author>
<author>Sebastiaan van Erk</author>
<author>Andre Verleg</author>
</authors>
<title>Graph-based generation of referring expressions.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<marker>Krahmer, van Erk, Verleg, 2003</marker>
<rawString>Emiel Krahmer, Sebastiaan van Erk, and Andre Verleg. 2003. Graph-based generation of referring expressions. Computational Linguistics, 29(1):53–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Passonneau</author>
</authors>
<title>Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation.</title>
<date>2006</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</booktitle>
<contexts>
<context position="16049" citStr="Passonneau, 2006" startWordPosition="2537" endWordPosition="2538">scribe it as well. Since every attribute that corresponds to a positive prediction is always selected, the algorithm does not regard uniqueness as a stop condition. As a result, the output description may convey a certain amount of overspecification. For evaluation purposes, we used the subset of singular descriptions from the Portuguese portion of the corpus, comprising 821 descriptions. Evaluation was carried out by comparing the corpus description with the system output to measure overall accuracy (i.e., the number of exact matches between the two descriptions), Dice (Dice, 1945) and MASI (Passonneau, 2006) coefficients. Following (Ferreira and Paraboni, 2014), we built a REG model using support vector machines with radial basis function kernel. The classifiers were trained and tested using 6-fold cross validation. Optimal parameters were selected using grid search as follows: for each step in the main k-fold validation, one fold was reserved for testing, and the remaining k − 1 folds were subject to a secondary cross-validation procedure in which different parameter combinations were attempted. The C parameter was assigned the values 1, 10, 100 and 1000, and γ was assigned 1, 0.1, 0.001 and 0.0</context>
</contexts>
<marker>Passonneau, 2006</marker>
<rawString>Rebecca Passonneau. 2006. Measuring agreement on set-valued items (MASI) for semantic and pragmatic annotation. In Proceedings of the International Conference on Language Resources and Evaluation (LREC).</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pechmann</author>
</authors>
<title>Incremental speech production and referential overspecification.</title>
<date>1989</date>
<journal>Linguistics,</journal>
<volume>27</volume>
<issue>1</issue>
<contexts>
<context position="2395" citStr="Pechmann, 1989" startWordPosition="358" endWordPosition="359">ibute-value pairs representing either atomic (type-restaurant) or relational (on-5thstreet) properties (Krahmer and Theune, 2002; Krahmer et al., 2003; Kelleher and Kruijff, 2006; Viethen et al., 2013). The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techni</context>
</contexts>
<marker>Pechmann, 1989</marker>
<rawString>T. Pechmann. 1989. Incremental speech production and referential overspecification. Linguistics, 27(1):98–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caio V M Teixeira</author>
<author>Ivandr´e Paraboni</author>
<author>Adriano S R da Silva</author>
<author>Alan K Yamasaki</author>
</authors>
<title>Generating relational descriptions involving mutual disambiguation.</title>
<date>2014</date>
<journal>Lecture Notes in Computer Science,</journal>
<pages>8403--492</pages>
<contexts>
<context position="6466" citStr="Teixeira et al., 2014" startWordPosition="1008" endWordPosition="1011">ptions of furniture items and 1080 descriptions of people’s photographs). TUNA has been used in a series of REG shared tasks (Gatt et al., 2009). GRE3D3 and its extension GRE3D7 (Dale and Viethen, 2009; Viethen and Dale, 2011) were developed in a series of web-based experiments primarily focussed on the study of relational descriptions. GRE3D3 contains 630 descriptions produced by 63 speakers, and GRE3D7 contains 4480 descriptions produced by 287 speakers. In both cases, the language of the experiment was English. The domain consists of simple visual scenes conveying boxes and spheres. Stars (Teixeira et al., 2014) and its extension Stars2 were collected for the study of referential overspecification. Stars contains 704 descriptions produced by 64 speakers in a web-based experiment. Stars2 was produced in dialogue situations involving subject pairs, and it contains 884 descriptions produced by 56 speakers. Both domains make use of simple visual scenes containing up to four object types (e.g., stars, boxes, cones and spheres) and include atomic and relational descriptions alike. The language of both experiments was Brazilian Portuguese. 3 Experiment We designed a web-based experiment to collect natural l</context>
</contexts>
<marker>Teixeira, Paraboni, Silva, Yamasaki, 2014</marker>
<rawString>Caio V. M. Teixeira, Ivandr´e Paraboni, Adriano S. R. da Silva, and Alan K. Yamasaki. 2014. Generating relational descriptions involving mutual disambiguation. Lecture Notes in Computer Science, 8403:492–502.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R van Gompel</author>
<author>Albert Gatt</author>
<author>E Krahmer</author>
<author>K van Deemter</author>
</authors>
<title>PRO: A computational model of referential overspecification.</title>
<date>2012</date>
<booktitle>In Proceedings of AMLAP-2012.</booktitle>
<marker>van Gompel, Gatt, Krahmer, van Deemter, 2012</marker>
<rawString>R. van Gompel, Albert Gatt, E. Krahmer, and K. van Deemter. 2012. PRO: A computational model of referential overspecification. In Proceedings of AMLAP-2012.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>Speakerdependent variation in content selection for referring expression generation.</title>
<date>2010</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop</booktitle>
<pages>81--89</pages>
<location>Melbourne, Australia.</location>
<contexts>
<context position="3161" citStr="Viethen and Dale, 2010" startWordPosition="483" endWordPosition="486">e overspecified (Arts et al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g., (Jordan and Walker, 2005; Viethen and Dale, 2010; Viethen, 2011; Garoufi and Koller, 2013; Ferreira and Paraboni, 2014)). A possible explanation for the small interest in ML for REG may be the relatively low availability of data. While research in many fields may benefit from the wide availability of text corpora (e.g., obtainable from the web), research in REG usually requires highly specialised data - hereby called REG corpora - conveying not only referring expressions produced by human speakers, but also a fully-annotated representation of the context (i.e., all objects and their semantic properties) within which the expressions have bee</context>
</contexts>
<marker>Viethen, Dale, 2010</marker>
<rawString>Jette Viethen and Robert Dale. 2010. Speakerdependent variation in content selection for referring expression generation. In Proceedings of the Australasian Language Technology Association Workshop 2010, pages 81–89, Melbourne, Australia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Robert Dale</author>
</authors>
<title>GRE3D7: A corpus of distinguishing descriptions for objects in visual scenes.</title>
<date>2011</date>
<booktitle>In Proceedings of UCNLG+Eval-2011,</booktitle>
<pages>12--22</pages>
<contexts>
<context position="6070" citStr="Viethen and Dale, 2011" startWordPosition="944" endWordPosition="947">the issues of human variation in REG (Fabbrizio et al., 2008; Altamirano et al., 2012; Gatt et al., 2011). 2 Related work TUNA (Gatt et al., 2007) was the first prominent REG corpus to be made publicly available for research purposes. The corpus was developed in a series of controlled experiments, containing 2280 atomic descriptions produced by 60 speakers of English in two domains (1200 descriptions of furniture items and 1080 descriptions of people’s photographs). TUNA has been used in a series of REG shared tasks (Gatt et al., 2009). GRE3D3 and its extension GRE3D7 (Dale and Viethen, 2009; Viethen and Dale, 2011) were developed in a series of web-based experiments primarily focussed on the study of relational descriptions. GRE3D3 contains 630 descriptions produced by 63 speakers, and GRE3D7 contains 4480 descriptions produced by 287 speakers. In both cases, the language of the experiment was English. The domain consists of simple visual scenes conveying boxes and spheres. Stars (Teixeira et al., 2014) and its extension Stars2 were collected for the study of referential overspecification. Stars contains 704 descriptions produced by 64 speakers in a web-based experiment. Stars2 was produced in dialogue </context>
</contexts>
<marker>Viethen, Dale, 2011</marker>
<rawString>Jette Viethen and Robert Dale. 2011. GRE3D7: A corpus of distinguishing descriptions for objects in visual scenes. In Proceedings of UCNLG+Eval-2011, pages 12–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
<author>Margaret Mitchell</author>
<author>Emiel Krahmer</author>
</authors>
<title>Graphs and spatial relations in the generation of referring expressions.</title>
<date>2013</date>
<booktitle>In Proceedings of the 14th European Workshop on Natural Language Generation,</booktitle>
<pages>72--81</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Sofia, Bulgaria,</location>
<contexts>
<context position="1982" citStr="Viethen et al., 2013" startWordPosition="286" endWordPosition="289">r, the issue of how to determine the semantic contents of definite descriptions (e.g., ‘the Indian restaurant on 5th street’, ‘the restaurant we went to last night’, etc.) has received significant attention in the field, and it is also the focus of the present work. The input to a REG algorithm is a context set C containing an intended referent r and a number of distractor objects. All objects are represented as attribute-value pairs representing either atomic (type-restaurant) or relational (on-5thstreet) properties (Krahmer and Theune, 2002; Krahmer et al., 2003; Kelleher and Kruijff, 2006; Viethen et al., 2013). The expected output is a uniquely identifying list L of properties known to be true of r so that L distinguishes r from all distractors in C (Dale and Reiter, 1995). Properties are selected for inclusion in L according to multiple - and often conflicting - criteria, including discriminatory power (i.e., the ability to rule out distractors) as in (Dale, 2002; Gardent, 2002), domain preferences (Pechmann, 1989; Gatt et al., 2013) and many others. A description that conveys more information than what is strictly required for disambiguation is said to be overspecified (Arts et al., 2011; Koolen </context>
</contexts>
<marker>Viethen, Mitchell, Krahmer, 2013</marker>
<rawString>Jette Viethen, Margaret Mitchell, and Emiel Krahmer. 2013. Graphs and spatial relations in the generation of referring expressions. In Proceedings of the 14th European Workshop on Natural Language Generation, pages 72–81, Sofia, Bulgaria, August. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jette Viethen</author>
</authors>
<title>The Generation of Natural Descriptions: Corpus-Based Investigations of Referring Expressions in Visual Domains.</title>
<date>2011</date>
<tech>Ph.D. thesis,</tech>
<institution>Macquarie University,</institution>
<location>Sydney, Australia.</location>
<contexts>
<context position="3176" citStr="Viethen, 2011" startWordPosition="487" endWordPosition="488"> al., 2011; Koolen et al., 2011; van Gompel et al., 2012; Engelhardt and Ferreira, 2006; Engelhardt et al., 2011). For a review of the research challenges in REG, see (Krahmer and van Deemter, 2012). Existing approaches to REG largely consist of algorithmic solutions, many of which have been influenced by, or adapted from, the Dale &amp; Reiter Incremental algorithm in (Dale and Reiter, 1995). The use of machine learning (ML) techniques, by contrast, seems to be less frequent than in other NLG tasks, although a number of exceptions do exist (e.g., (Jordan and Walker, 2005; Viethen and Dale, 2010; Viethen, 2011; Garoufi and Koller, 2013; Ferreira and Paraboni, 2014)). A possible explanation for the small interest in ML for REG may be the relatively low availability of data. While research in many fields may benefit from the wide availability of text corpora (e.g., obtainable from the web), research in REG usually requires highly specialised data - hereby called REG corpora - conveying not only referring expressions produced by human speakers, but also a fully-annotated representation of the context (i.e., all objects and their semantic properties) within which the expressions have been produced. REG</context>
</contexts>
<marker>Viethen, 2011</marker>
<rawString>Jette Viethen. 2011. The Generation of Natural Descriptions: Corpus-Based Investigations of Referring Expressions in Visual Domains. Ph.D. thesis, Macquarie University, Sydney, Australia.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>