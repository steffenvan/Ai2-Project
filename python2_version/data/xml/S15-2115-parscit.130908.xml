<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007717">
<title confidence="0.974758">
LT3: Sentiment Analysis of Figurative Tweets: piece of cake #NotReally
</title>
<author confidence="0.983546">
Cynthia Van Hee, Els Lefever and Véronique hoste
</author>
<affiliation confidence="0.973954">
LT3, Language and Translation Technology Team
Department of Translation, Interpreting and Communication – Ghent University
</affiliation>
<address confidence="0.935342">
Groot-Brittanniëlaan 45, 9000 Ghent, Belgium
</address>
<email confidence="0.99577">
Firstname.Lastname@UGent.be
</email>
<sectionHeader confidence="0.995574" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999625692307692">
This paper describes our contribution to the
SemEval-2015 Task 11 on sentiment analysis
of figurative language in Twitter. We consid-
ered two approaches, classification and regres-
sion, to provide fine-grained sentiment scores
for a set of tweets that are rich in sarcasm,
irony and metaphor. To this end, we combined
a variety of standard lexical and syntactic fea-
tures with specific features for capturing fig-
urative content. All experiments were done
using supervised learning with LIBSVM. For
both runs, our system ranked fourth among fif-
teen submissions.
</bodyText>
<sectionHeader confidence="0.998993" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998104076923077">
Handling figurative language is currently one of
the most challenging tasks in NLP. Figurative lan-
guage is often characterized by linguistic devices
such as sarcasm, irony, metaphors, and humour.
Their meaning goes beyond the literal meaning and
is therefore often hard to capture, even for humans.
However, as an increasing part of our daily commu-
nication takes place on social media (e.g. Twitter,
Facebook), which are prone to figurative language
use, there is an urgent need for automatic systems
that recognize and understand figurative online con-
tent. This is especially the case in the field of senti-
ment analysis where the presence of figurative lan-
guage in subjective text can significantly undermine
the classification accuracy.
Understanding figurative language often requires
world knowledge, which cannot easily be accessed
by machines. Moreover, figurative language rapidly
evolves due to changes in vocabulary and language,
which makes it difficult to train machine learning
algorithms. Nevertheless, the identification of non-
literal uses of language has attracted a fair amount
of research interest recently. Veale (2012) investi-
gated the relation between irony and our stereotyp-
ical knowledge of a domain and showed how the
insight in stereotypical norms helps to recognize
and understand ironic utterances. Reyes et al. (2013)
built an irony model for Twitter for which they re-
lied on a set of textual features for capturing ironic
tweets. Their model obtained promising results con-
cerning recall (84%). In what relates to the detec-
tion of metaphors, Turney et al. (2011) introduced
an algorithm for distinguishing between metaphor-
ical and literal word usages based on the degree of
abstractness of a word’s context. More recent work
by Tsvetkov et al. (2014) presents a cross-lingual
model based on lexical semantic word features for
metaphor detection in English, Spanish, Farsi and
Russian.
To date, most studies on figurative language use
have focussed on the detection of linguistic devices
such as sarcasm, irony and metaphor. By contrast,
only a few studies have investigated how these de-
vices affect sentiment analysis. Indeed, as stated by
Maynard (2014), it is not sufficient to determine
whether a text contains sarcasm or not. Instead, we
need to measure its impact on sentiment analysis if
we want to improve the state-of-the-art in sentiment
analysis systems.
In this paper we describe our contribution to the
SemEval-2015 shared task: Sentiment Analysis of
Figurative Language in Twitter (Ghosh et al., 2015).
</bodyText>
<page confidence="0.98265">
684
</page>
<bodyText confidence="0.964220575757576">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 684–688,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
Our objective is to provide fine-grained sentiment
scores for a set of tweets that are rich in sarcasm,
irony and metaphor. The datasets for training, de-
velopment and testing were provided by the task or-
ganizers. The training dataset contains 8,000 tweets
(5,000 sarcastic, 1,000 ironic and 2,000 metaphori-
cal) labeled with a sentiment score between -5 and 5.
This training set was provided with both integer and
real-valued sentiment scores. The trial and test sets
were comparable to the training corpus and contain
1,0001 and 4,000 labeled instances, respectively. All
experiments were done using LIBSVM (Chang and
Lin, 2011).
We submitted two runs for the competition. To
this end, we built two models based on supervised
learning: 1) a classification-based (C-SVC) and 2) a
regression-based approach (epsilon-SVR). For both
models, we implemented a number of word-based,
lexical, sentiment and syntactic features in combi-
nation with specific features for capturing figurative
content such as sarcasm. Evaluation was done by
calculating the cosine similarity distance between
the predicted and the gold-standard sentiment labels.
The remainder of this paper is structured as fol-
lows: Section 2 presents our system description
whereas Section 2.2 gives an overview of the fea-
tures we implemented. The experimental setup is de-
scribed in Section 3, followed by our results in Sec-
tion 4. Finally, we draw conclusions in Section 5
where we also suggest some directions for future re-
search.
</bodyText>
<sectionHeader confidence="0.967114" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.999967">
The main purpose of this paper was to develop a
system for the fine-grained sentiment classification
of figurative tweets. We tackled this problem by us-
ing classification and regression approaches and pro-
vided each instance with a sentiment score between
-5 and 5. In addition to more standard NLP features
(bags-of-words, PoS-tags, etc.), we implemented a
number of features for capturing the figurative char-
acter of the tweets. In this section, we outline our
sentiment analysis pipeline and describe the linguis-
tic preprocessing and feature extraction.
</bodyText>
<footnote confidence="0.9320035">
1As some tweets were made inaccessible by their creators,
we were able to download only 914 of them
</footnote>
<subsectionHeader confidence="0.995199">
2.1 Linguistic Preprocessing
</subsectionHeader>
<bodyText confidence="0.9997428">
All tweets were tokenized and PoS-tagged using the
Carnegie Mellon University Twitter Part-of-Speech-
Tagger (Gimpel et al., 2011). Lemmatization was
done using the LT3 LeTs Preprocess Toolkit (Van de
Kauter et al., 2013). We used a caseless parsing
model of the Stanford parser (de Marneffe et al.,
2006) for a dependency representation of the mes-
sages. As a final step, we tagged all named enti-
ties using the Twitter NLP tools for Named Entity
Recognition (Ritter et al., 2011).
</bodyText>
<subsectionHeader confidence="0.982555">
2.2 Features
</subsectionHeader>
<bodyText confidence="0.975052413793103">
As a first step, we implemented a set of features
that have shown to perform well for sentiment
classification in previous research (Van Hee et al.,
2014). These include word-based features (e.g. bag-
of-words), lexical features (e.g. character flooding),
sentiment features (e.g. an overall sentiment score
per tweet, based on existing sentiment lexicons),
and syntactic features (e.g. dependency relation fea-
tures)2. To provide some abstraction, we also added
PoS n-gram features to the set of bag-of-words fea-
tures.
Nevertheless, as a substantial part of the data we
are confronted with is of a figurative nature, we im-
plemented a series of additional features for captur-
ing potential clues, for example of sarcasm, in the
tweets3.
Contrast – Binary feature indicating whether a
contrastive sentiment (i.e. at least one positive and
one negative sentiment word) is contained by the in-
stance.
Interjection Count – Numeric feature indicating
how many interjections are contained by an instance.
This value is normalized by dividing it by the num-
ber of tokens in the instance. As stated by (Carvalho
et al., 2009), interjections may be potential clues for
irony detection.
Sarcasm Hashtag – Binary feature indicating
whether an instance contains a hashtag that may in-
dicate the presence of sarcasm. To this end, a list of
</bodyText>
<footnote confidence="0.9201324">
2For a detailed description of these features we refer to Van
Hee et al. (2014).
3A number of these features (i.e. contradiction, sudden
change, and temporal imbalance) are inspired by Reyes et
al. (2013).
</footnote>
<page confidence="0.996014">
685
</page>
<bodyText confidence="0.981221423076923">
≈100 sarcasm-related hashtags was extracted from
the training data.
Punctuation Mark Count – Normalized numeric
feature indicating the number of punctuation marks
that are contained by an instance.
Emoticon count – Normalized numeric feature
indicating the number of emoticons that are con-
tained by an instance.
Contradiction – Binary feature that indicates
whether an instance contains a linguistic contradic-
tion marker (i.e. words like nonetheless, yet, how-
ever).
Sudden Change – Binary feature that indicates
whether an instance contains a linguistic marker
of a sudden change in the narrative of the tweet
(i.e. words like suddenly, out of the blue).
Temporal Imbalance – Binary feature indicat-
ing the presence of a temporal imbalance (i.e. both
present and past tenses are used) in the narrative of
a message.
Polysemy – Normalized numeric feature indicat-
ing how many polyseme words are contained by an
instance. As polyseme are considered those words
that have more than seven different meanings ac-
cording to WordNet4, which may be an indication
of metaphorical language.
</bodyText>
<sectionHeader confidence="0.999233" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999957294117647">
As the training instances were provided with both
integer and real-valued sentiment scores, we used
two different approaches to the fine-grained senti-
ment labeling. Firstly, we implemented a classifi-
cation approach where each tweet had to be given
a sentiment label on an eleven-point scale ranging
from -5 to 5. Secondly, we used regression to predict
a real-valued sentiment score for each tweet, which
could be any numeric value between -5 and 5.
Two feature sets were used throughout the experi-
ments: firstly, we included a number of word-based,
lexical, sentiment and syntactic features (we refer to
these as the sentiment feature set). Secondly, we
implemented an additional set of features for cap-
turing possibly figurative content such as irony and
metaphors. These features are referred to as the fig-
urative feature set.
</bodyText>
<equation confidence="0.314274">
4Fellbaum, C. (1998)
</equation>
<bodyText confidence="0.999562857142857">
Using 5-fold cross-validation on the training data,
we performed a grid search to find the optimal cost
and gamma parameters for both classification (c =
0.03, g = 0.008) and regression (c = 8, g = 0.063).
For regression, an optimal epsilon value of p = 0.5
was determined.
As a first approach to evaluating our features, we
used a subset of the trial data5. Secondly, we (ran-
domly) split the data into 90% for training and 10%
for testing. We calculated a baseline using the ma-
jority class label -3 (see Table 1). Tables 2 and 3
present the results on the training and trial data that
were obtained throughout the experiments both for
classification and for regression.
</bodyText>
<tableCaption confidence="0.998632">
Table 1: Majority class baseline.
</tableCaption>
<table confidence="0.9304906">
Evaluation Set feature set Cosine Similarity
Trial data sentiment 0.72
figurative 0.74
10% training set sentiment 0.82
figurative 0.83
</table>
<tableCaption confidence="0.968888">
Table 2: Experimental results for classification
(after a parameter grid search).
</tableCaption>
<table confidence="0.9159428">
Evaluation Set feature set Cosine Similarity
Trial data sentiment 0.75
figurative 0.74
10% training set sentiment 0.85
figurative 0.84
</table>
<tableCaption confidence="0.920896">
Table 3: Experimental results for regression
(after a parameter grid search).
</tableCaption>
<bodyText confidence="0.999438333333333">
As the table shows, adding figurative language
specific features proves to be beneficial for classi-
fication. For regression, by contrast, adding more
features does not improve the results on the train-
ing and trial data. However, both approaches clearly
outperform the baseline.
</bodyText>
<footnote confidence="0.8526905">
5We only considered the tweets that were not included by
the training data.
</footnote>
<figure confidence="0.99598225">
Evaluation Set
Cosine Similarity
0.59
0.80
Trial data
10% training set
Averaged baseline
0.70
</figure>
<page confidence="0.994134">
686
</page>
<sectionHeader confidence="0.988948" genericHeader="method">
4 Competition Results
</sectionHeader>
<bodyText confidence="0.999310375">
We submitted two runs for this task. For our first run,
we implemented a classification approach whereas
we used regression for the second run. As the offi-
cial test data also contains a substantial part of regu-
lar Twitter data, we included both the standard sen-
timent feature set and the figurative feature set.
Our competition results can be found in Tables 4
and 5.
</bodyText>
<table confidence="0.99910675">
Overall Sarcasm Irony Metaphor Other
Cosine 0.66 (4/15) 0.89 0.90 0.44 0.35
Similarity
MSE 3.40 (4/15) 1.29 1.22 5.67 5.44
</table>
<tableCaption confidence="0.993512">
Table 4: Competition results for classification.
</tableCaption>
<table confidence="0.9997205">
Overall Sarcasm Irony Metaphor Other
Cosine 0.65 (4/15) 0.87 0.86 0.36 0.36
Similarity
MSE 2.91 (4/15) 1.29 1.08 4.79 4.50
</table>
<tableCaption confidence="0.999708">
Table 5: Competition results for regression.
</tableCaption>
<bodyText confidence="0.9999543">
As shown in tables 4 and 5, our system achieved
an overall cosine similarity score of 0.66 and 0.65
for the classification-based and regression-based ap-
proaches respectively and ranked fourth among fif-
teen submissions for both runs. When considering
the competition results per category, we see that our
system performs particularly well on the sarcasm
and irony classes. For the latter, our classification
performance (cosine similarity = 0.90) corresponds
with that of the best reported system.
</bodyText>
<sectionHeader confidence="0.989735" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.989202">
We experimented with two experimental setups to
compare the performance of a sentiment classifier
using 1) more standard sentiment features and 2)
features that may capture sarcastic content. The re-
sults of our experiments show that adding features
that are specific to figurative language improves the
performance of our classification approach. How-
ever, it does not improve the performance for regres-
sion.
An error analysis revealed that our system’s per-
formance benefits from the information provided by
sentiment lexicon features. Given the high distribu-
tion of the negative class labels in this corpus, some
positive instances are incorrectly assigned a negative
class label:
</bodyText>
<listItem confidence="0.976455">
• Im not about that life though lol, Im literally a
natural woman and I am proud of it :) (-3)
</listItem>
<bodyText confidence="0.994244833333333">
Another remark that should be made is that some of
our irony-specific features are possibly too coarse-
grained. The contrast feature for instance, was
sometimes activated even though the tweet under in-
vestigation was meant rather literally than sarcasti-
cally:
</bodyText>
<listItem confidence="0.998142">
• RT @laurenwalter: underwater walking
was pretty bloody amazing! literally wanted
to stay under there! was such an experience!!
loved it!!
</listItem>
<bodyText confidence="0.99998675862069">
The contrast feature was activated for this tweet
since bloody was identified as a negative sentiment
word whereas pretty and amazing are positive sen-
timent words. This problem may be solved by only
considering the head of the adjectival phrase (amaz-
ing) as a sentiment word.
In this paper, we developed a sentiment analysis
pipeline that takes irony and sarcasm clues into ac-
count to provide a fine-grained sentiment score for
tweets. In future research, it would be interesting to
implement a cascaded approach where 1) the output
of a sarcasm detection system is used as a feature
for a sentiment classifier or 2) a sarcasm detection
system is used as a post-processing step where the
sentiment label given by a regular sentiment classi-
fier is flipped if the utterance is meant sarcastically.
Moreover, we will search for better features for
modeling sarcasm in tweets and we aim to rebal-
ance the data to approximate a realistic distribution
of sarcastic messages in a random stream of Twitter
messages.
To improve sentiment classification of metaphor-
ical tweets, a classifier might benefit from word
sense disambiguation and knowledge about stereo-
types and commonly used similes.
Finally, we aim to perform feature selection since
abounding bag-of-words features often suffer from
overfitting. This way, they may introduce noise and
hence decrease the classification accuracy.
</bodyText>
<page confidence="0.997241">
687
</page>
<sectionHeader confidence="0.984655" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999055232876712">
Paula Carvalho, Luís Sarmento, Mário J. Silva, and Eu-
génio de Oliveira. 2009. Clues for detecting irony in
user-generated contents: Oh...!! it’s &amp;quot;so easy&amp;quot; ;-). In
Proceedings of the 1st International CIKM Workshop
on Topic-sentiment Analysis for Mass Opinion, TSA
’09, pages 53–56, New York, NY, USA.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM:
A library for support vector machines. ACM Transac-
tions on Intelligent Systems and Technology, 2:27:1–
27:27.
Marie-Catherine de Marneffe, Bill MacCartney, and
Christopher D. Manning. 2006. Generating Typed
Dependency Parses from Phrase Structure Parses. In
Proc. of LREC’06, pages 449–454, Genoa, Italy.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database.
A. Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina
Shutova, Antonio Reyes, and John Barnden. 2015.
Semeval-2015 task 11: Sentiment analysis of figura-
tive language in Twitter. In Proceedings of the Inter-
national Workshop on Semantic Evaluation (SemEval-
2015), Denver, Colorado, USA.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A. Smith. 2011. Part-of-speech tagging for
Twitter: Annotation, features, and experiments. In
Proceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics: Human Lan-
guage Technologies: Short Papers - Volume 2, HLT
’11, pages 42–47, Stroudsburg, PA, USA.
Diana Maynard and Mark Greenwood. 2014. Who
cares about sarcastic tweets? investigating the impact
of sarcasm on sentiment analysis. In Proceedings of
the Ninth International Conference on Language Re-
sources and Evaluation (LREC’14), Reykjavik, Ice-
land.
Antonio Reyes, Paolo Rosso, and Tony Veale. 2013. A
multidimensional approach for detecting irony in Twit-
ter. Language Resources and Evaluation, pages 238–
269.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An ex-
perimental study. In Proceedings of the Conference
on Empirical Methods in Natural Language Process-
ing, EMNLP ’11, pages 1524–1534, Stroudsburg, PA,
USA.
Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric
Nyberg, and Chris Dyer. 2014. Metaphor detection
with cross-lingual model transfer. ACL 2014, pages
248–258.
Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai
Cohen. 2011. Literal and metaphorical sense iden-
tification through concrete and abstract context. In
Proceedings of the Conference on Empirical Methods
in Natural Language Processing, EMNLP ’11, pages
680–690, Stroudsburg, PA, USA.
Marjan Van de Kauter, Geert Coorman, Els Lefever, Bart
Desmet, Lieve Macken, and Véronique Hoste. 2013.
LeTs Preprocess: the multilingual LT3 linguistic pre-
processing toolkit. Computational Linguistics in the
Netherlands Journal, 3:103–120.
Cynthia Van Hee, Marjan Van de Kauter, Orphée
De Clercq, Els Lefever, and Véronique Hoste. 2014.
LT3: Sentiment classification in user-generated con-
tent using a rich feature set. In Proceedings of the 8th
International Workshop on Semantic Evaluation (Se-
mEval 2014), pages 406–410, Dublin, Ireland.
Tony Veale. 2012. Detecting and generating ironic com-
parisons: An application of creative information re-
trieval. In AAAI Fall Symposium: Artificial Intelli-
gence of Humor, volume FS-12-02 of AAAI Technical
Report.
</reference>
<page confidence="0.997315">
688
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.303441">
<title confidence="0.983366">LT3: Sentiment Analysis of Figurative Tweets: piece of cake #NotReally</title>
<author confidence="0.8021525">Cynthia Van_Hee</author>
<author confidence="0.8021525">Els Lefever</author>
<author confidence="0.8021525">Véronique Language</author>
<author confidence="0.8021525">Translation Technology</author>
<affiliation confidence="0.99701">Department of Translation, Interpreting and Communication – Ghent</affiliation>
<address confidence="0.590401">Groot-Brittanniëlaan 45, 9000 Ghent,</address>
<email confidence="0.752342">Firstname.Lastname@UGent.be</email>
<abstract confidence="0.997780714285714">This paper describes our contribution to the SemEval-2015 Task 11 on sentiment analysis of figurative language in Twitter. We considered two approaches, classification and regression, to provide fine-grained sentiment scores for a set of tweets that are rich in sarcasm, irony and metaphor. To this end, we combined a variety of standard lexical and syntactic features with specific features for capturing figurative content. All experiments were done using supervised learning with LIBSVM. For both runs, our system ranked fourth among fifteen submissions.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Paula Carvalho</author>
<author>Luís Sarmento</author>
<author>Mário J Silva</author>
<author>Eugénio de Oliveira</author>
</authors>
<title>Clues for detecting irony in user-generated contents: Oh...!! it’s &amp;quot;so easy&amp;quot; ;-).</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st International CIKM Workshop on Topic-sentiment Analysis for Mass Opinion, TSA ’09,</booktitle>
<pages>53--56</pages>
<location>New York, NY, USA.</location>
<marker>Carvalho, Sarmento, Silva, de Oliveira, 2009</marker>
<rawString>Paula Carvalho, Luís Sarmento, Mário J. Silva, and Eugénio de Oliveira. 2009. Clues for detecting irony in user-generated contents: Oh...!! it’s &amp;quot;so easy&amp;quot; ;-). In Proceedings of the 1st International CIKM Workshop on Topic-sentiment Analysis for Mass Opinion, TSA ’09, pages 53–56, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<volume>2</volume>
<pages>27--27</pages>
<contexts>
<context position="4256" citStr="Chang and Lin, 2011" startWordPosition="644" endWordPosition="647">ctive is to provide fine-grained sentiment scores for a set of tweets that are rich in sarcasm, irony and metaphor. The datasets for training, development and testing were provided by the task organizers. The training dataset contains 8,000 tweets (5,000 sarcastic, 1,000 ironic and 2,000 metaphorical) labeled with a sentiment score between -5 and 5. This training set was provided with both integer and real-valued sentiment scores. The trial and test sets were comparable to the training corpus and contain 1,0001 and 4,000 labeled instances, respectively. All experiments were done using LIBSVM (Chang and Lin, 2011). We submitted two runs for the competition. To this end, we built two models based on supervised learning: 1) a classification-based (C-SVC) and 2) a regression-based approach (epsilon-SVR). For both models, we implemented a number of word-based, lexical, sentiment and syntactic features in combination with specific features for capturing figurative content such as sarcasm. Evaluation was done by calculating the cosine similarity distance between the predicted and the gold-standard sentiment labels. The remainder of this paper is structured as follows: Section 2 presents our system descriptio</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1– 27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-Catherine de Marneffe</author>
<author>Bill MacCartney</author>
<author>Christopher D Manning</author>
</authors>
<title>Generating Typed Dependency Parses from Phrase Structure Parses.</title>
<date>2006</date>
<booktitle>In Proc. of LREC’06,</booktitle>
<pages>449--454</pages>
<location>Genoa, Italy.</location>
<marker>de Marneffe, MacCartney, Manning, 2006</marker>
<rawString>Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning. 2006. Generating Typed Dependency Parses from Phrase Structure Parses. In Proc. of LREC’06, pages 449–454, Genoa, Italy.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ghosh</author>
<author>Guofu Li</author>
<author>Tony Veale</author>
<author>Paolo Rosso</author>
<author>Ekaterina Shutova</author>
<author>Antonio Reyes</author>
<author>John Barnden</author>
</authors>
<title>Semeval-2015 task 11: Sentiment analysis of figurative language in Twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation (SemEval2015),</booktitle>
<location>Denver, Colorado, USA.</location>
<contexts>
<context position="3439" citStr="Ghosh et al., 2015" startWordPosition="520" endWordPosition="523">ost studies on figurative language use have focussed on the detection of linguistic devices such as sarcasm, irony and metaphor. By contrast, only a few studies have investigated how these devices affect sentiment analysis. Indeed, as stated by Maynard (2014), it is not sufficient to determine whether a text contains sarcasm or not. Instead, we need to measure its impact on sentiment analysis if we want to improve the state-of-the-art in sentiment analysis systems. In this paper we describe our contribution to the SemEval-2015 shared task: Sentiment Analysis of Figurative Language in Twitter (Ghosh et al., 2015). 684 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 684–688, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Our objective is to provide fine-grained sentiment scores for a set of tweets that are rich in sarcasm, irony and metaphor. The datasets for training, development and testing were provided by the task organizers. The training dataset contains 8,000 tweets (5,000 sarcastic, 1,000 ironic and 2,000 metaphorical) labeled with a sentiment score between -5 and 5. This training set was provided with both integer an</context>
</contexts>
<marker>Ghosh, Li, Veale, Rosso, Shutova, Reyes, Barnden, 2015</marker>
<rawString>A. Ghosh, Guofu Li, Tony Veale, Paolo Rosso, Ekaterina Shutova, Antonio Reyes, and John Barnden. 2015. Semeval-2015 task 11: Sentiment analysis of figurative language in Twitter. In Proceedings of the International Workshop on Semantic Evaluation (SemEval2015), Denver, Colorado, USA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for Twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11,</booktitle>
<pages>42--47</pages>
<location>Stroudsburg, PA, USA.</location>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. Smith. 2011. Part-of-speech tagging for Twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: Short Papers - Volume 2, HLT ’11, pages 42–47, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Diana Maynard</author>
<author>Mark Greenwood</author>
</authors>
<title>Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14),</booktitle>
<location>Reykjavik, Iceland.</location>
<marker>Maynard, Greenwood, 2014</marker>
<rawString>Diana Maynard and Mark Greenwood. 2014. Who cares about sarcastic tweets? investigating the impact of sarcasm on sentiment analysis. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC’14), Reykjavik, Iceland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Antonio Reyes</author>
<author>Paolo Rosso</author>
<author>Tony Veale</author>
</authors>
<title>A multidimensional approach for detecting irony in Twitter. Language Resources and Evaluation,</title>
<date>2013</date>
<pages>238--269</pages>
<contexts>
<context position="2242" citStr="Reyes et al. (2013)" startWordPosition="328" endWordPosition="331">ion accuracy. Understanding figurative language often requires world knowledge, which cannot easily be accessed by machines. Moreover, figurative language rapidly evolves due to changes in vocabulary and language, which makes it difficult to train machine learning algorithms. Nevertheless, the identification of nonliteral uses of language has attracted a fair amount of research interest recently. Veale (2012) investigated the relation between irony and our stereotypical knowledge of a domain and showed how the insight in stereotypical norms helps to recognize and understand ironic utterances. Reyes et al. (2013) built an irony model for Twitter for which they relied on a set of textual features for capturing ironic tweets. Their model obtained promising results concerning recall (84%). In what relates to the detection of metaphors, Turney et al. (2011) introduced an algorithm for distinguishing between metaphorical and literal word usages based on the degree of abstractness of a word’s context. More recent work by Tsvetkov et al. (2014) presents a cross-lingual model based on lexical semantic word features for metaphor detection in English, Spanish, Farsi and Russian. To date, most studies on figurat</context>
<context position="7839" citStr="Reyes et al. (2013)" startWordPosition="1214" endWordPosition="1217"> Count – Numeric feature indicating how many interjections are contained by an instance. This value is normalized by dividing it by the number of tokens in the instance. As stated by (Carvalho et al., 2009), interjections may be potential clues for irony detection. Sarcasm Hashtag – Binary feature indicating whether an instance contains a hashtag that may indicate the presence of sarcasm. To this end, a list of 2For a detailed description of these features we refer to Van Hee et al. (2014). 3A number of these features (i.e. contradiction, sudden change, and temporal imbalance) are inspired by Reyes et al. (2013). 685 ≈100 sarcasm-related hashtags was extracted from the training data. Punctuation Mark Count – Normalized numeric feature indicating the number of punctuation marks that are contained by an instance. Emoticon count – Normalized numeric feature indicating the number of emoticons that are contained by an instance. Contradiction – Binary feature that indicates whether an instance contains a linguistic contradiction marker (i.e. words like nonetheless, yet, however). Sudden Change – Binary feature that indicates whether an instance contains a linguistic marker of a sudden change in the narrati</context>
</contexts>
<marker>Reyes, Rosso, Veale, 2013</marker>
<rawString>Antonio Reyes, Paolo Rosso, and Tony Veale. 2013. A multidimensional approach for detecting irony in Twitter. Language Resources and Evaluation, pages 238– 269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>1524--1534</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="6300" citStr="Ritter et al., 2011" startWordPosition="967" endWordPosition="970">e extraction. 1As some tweets were made inaccessible by their creators, we were able to download only 914 of them 2.1 Linguistic Preprocessing All tweets were tokenized and PoS-tagged using the Carnegie Mellon University Twitter Part-of-SpeechTagger (Gimpel et al., 2011). Lemmatization was done using the LT3 LeTs Preprocess Toolkit (Van de Kauter et al., 2013). We used a caseless parsing model of the Stanford parser (de Marneffe et al., 2006) for a dependency representation of the messages. As a final step, we tagged all named entities using the Twitter NLP tools for Named Entity Recognition (Ritter et al., 2011). 2.2 Features As a first step, we implemented a set of features that have shown to perform well for sentiment classification in previous research (Van Hee et al., 2014). These include word-based features (e.g. bagof-words), lexical features (e.g. character flooding), sentiment features (e.g. an overall sentiment score per tweet, based on existing sentiment lexicons), and syntactic features (e.g. dependency relation features)2. To provide some abstraction, we also added PoS n-gram features to the set of bag-of-words features. Nevertheless, as a substantial part of the data we are confronted wi</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 1524–1534, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yulia Tsvetkov</author>
<author>Leonid Boytsov</author>
<author>Anatole Gershman</author>
<author>Eric Nyberg</author>
<author>Chris Dyer</author>
</authors>
<title>Metaphor detection with cross-lingual model transfer. ACL</title>
<date>2014</date>
<pages>248--258</pages>
<contexts>
<context position="2675" citStr="Tsvetkov et al. (2014)" startWordPosition="401" endWordPosition="404">relation between irony and our stereotypical knowledge of a domain and showed how the insight in stereotypical norms helps to recognize and understand ironic utterances. Reyes et al. (2013) built an irony model for Twitter for which they relied on a set of textual features for capturing ironic tweets. Their model obtained promising results concerning recall (84%). In what relates to the detection of metaphors, Turney et al. (2011) introduced an algorithm for distinguishing between metaphorical and literal word usages based on the degree of abstractness of a word’s context. More recent work by Tsvetkov et al. (2014) presents a cross-lingual model based on lexical semantic word features for metaphor detection in English, Spanish, Farsi and Russian. To date, most studies on figurative language use have focussed on the detection of linguistic devices such as sarcasm, irony and metaphor. By contrast, only a few studies have investigated how these devices affect sentiment analysis. Indeed, as stated by Maynard (2014), it is not sufficient to determine whether a text contains sarcasm or not. Instead, we need to measure its impact on sentiment analysis if we want to improve the state-of-the-art in sentiment ana</context>
</contexts>
<marker>Tsvetkov, Boytsov, Gershman, Nyberg, Dyer, 2014</marker>
<rawString>Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman, Eric Nyberg, and Chris Dyer. 2014. Metaphor detection with cross-lingual model transfer. ACL 2014, pages 248–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter D Turney</author>
<author>Yair Neuman</author>
<author>Dan Assaf</author>
<author>Yohai Cohen</author>
</authors>
<title>Literal and metaphorical sense identification through concrete and abstract context.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>680--690</pages>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2487" citStr="Turney et al. (2011)" startWordPosition="371" endWordPosition="374">ain machine learning algorithms. Nevertheless, the identification of nonliteral uses of language has attracted a fair amount of research interest recently. Veale (2012) investigated the relation between irony and our stereotypical knowledge of a domain and showed how the insight in stereotypical norms helps to recognize and understand ironic utterances. Reyes et al. (2013) built an irony model for Twitter for which they relied on a set of textual features for capturing ironic tweets. Their model obtained promising results concerning recall (84%). In what relates to the detection of metaphors, Turney et al. (2011) introduced an algorithm for distinguishing between metaphorical and literal word usages based on the degree of abstractness of a word’s context. More recent work by Tsvetkov et al. (2014) presents a cross-lingual model based on lexical semantic word features for metaphor detection in English, Spanish, Farsi and Russian. To date, most studies on figurative language use have focussed on the detection of linguistic devices such as sarcasm, irony and metaphor. By contrast, only a few studies have investigated how these devices affect sentiment analysis. Indeed, as stated by Maynard (2014), it is </context>
</contexts>
<marker>Turney, Neuman, Assaf, Cohen, 2011</marker>
<rawString>Peter D. Turney, Yair Neuman, Dan Assaf, and Yohai Cohen. 2011. Literal and metaphorical sense identification through concrete and abstract context. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 680–690, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marjan Van de Kauter</author>
<author>Geert Coorman</author>
<author>Els Lefever</author>
<author>Bart Desmet</author>
<author>Lieve Macken</author>
<author>Véronique Hoste</author>
</authors>
<date>2013</date>
<booktitle>LeTs Preprocess: the multilingual LT3 linguistic preprocessing toolkit. Computational Linguistics in the Netherlands Journal,</booktitle>
<pages>3--103</pages>
<marker>Van de Kauter, Coorman, Lefever, Desmet, Macken, Hoste, 2013</marker>
<rawString>Marjan Van de Kauter, Geert Coorman, Els Lefever, Bart Desmet, Lieve Macken, and Véronique Hoste. 2013. LeTs Preprocess: the multilingual LT3 linguistic preprocessing toolkit. Computational Linguistics in the Netherlands Journal, 3:103–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cynthia Van Hee</author>
<author>Marjan Van de Kauter</author>
<author>Orphée De Clercq</author>
<author>Els Lefever</author>
<author>Véronique Hoste</author>
</authors>
<title>LT3: Sentiment classification in user-generated content using a rich feature set.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>406--410</pages>
<location>Dublin, Ireland.</location>
<marker>Van Hee, Van de Kauter, De Clercq, Lefever, Hoste, 2014</marker>
<rawString>Cynthia Van Hee, Marjan Van de Kauter, Orphée De Clercq, Els Lefever, and Véronique Hoste. 2014. LT3: Sentiment classification in user-generated content using a rich feature set. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 406–410, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony Veale</author>
</authors>
<title>Detecting and generating ironic comparisons: An application of creative information retrieval.</title>
<date>2012</date>
<booktitle>In AAAI Fall Symposium: Artificial Intelligence of Humor,</booktitle>
<tech>Technical Report.</tech>
<volume>volume</volume>
<pages>12--02</pages>
<contexts>
<context position="2035" citStr="Veale (2012)" startWordPosition="297" endWordPosition="298">stand figurative online content. This is especially the case in the field of sentiment analysis where the presence of figurative language in subjective text can significantly undermine the classification accuracy. Understanding figurative language often requires world knowledge, which cannot easily be accessed by machines. Moreover, figurative language rapidly evolves due to changes in vocabulary and language, which makes it difficult to train machine learning algorithms. Nevertheless, the identification of nonliteral uses of language has attracted a fair amount of research interest recently. Veale (2012) investigated the relation between irony and our stereotypical knowledge of a domain and showed how the insight in stereotypical norms helps to recognize and understand ironic utterances. Reyes et al. (2013) built an irony model for Twitter for which they relied on a set of textual features for capturing ironic tweets. Their model obtained promising results concerning recall (84%). In what relates to the detection of metaphors, Turney et al. (2011) introduced an algorithm for distinguishing between metaphorical and literal word usages based on the degree of abstractness of a word’s context. Mo</context>
</contexts>
<marker>Veale, 2012</marker>
<rawString>Tony Veale. 2012. Detecting and generating ironic comparisons: An application of creative information retrieval. In AAAI Fall Symposium: Artificial Intelligence of Humor, volume FS-12-02 of AAAI Technical Report.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>