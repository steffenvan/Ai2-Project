<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005907">
<title confidence="0.98328">
The RWTH System Combination System for WMT 2010
</title>
<author confidence="0.994993">
Gregor Leusch and Hermann Ney
</author>
<affiliation confidence="0.7757465">
RWTH Aachen University
Aachen, Germany
</affiliation>
<email confidence="0.993537">
{leusch,ney}@cs.rwth-aachen.de
</email>
<sectionHeader confidence="0.9943" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.996339210526316">
RWTH participated in the System Combi-
nation task of the Fifth Workshop on Sta-
tistical Machine Translation (WMT 2010).
For 7 of the 8 language pairs, we com-
bine 5 to 13 systems into a single con-
sensus translation, using additional n-best
reranking techniques in two of these lan-
guage pairs. Depending on the language
pair, improvements versus the best sin-
gle system are in the range of +0.5 and
+1.7 on BLEU, and between −0.4 and
−2.3 on TER. Novel techniques compared
with RWTH’s submission to WMT 2009
include the utilization of n-best reranking
techniques, a consensus true casing ap-
proach, a different tuning algorithm, and
the separate selection of input systems
for CN construction, primary/skeleton hy-
potheses, HypLM, and true casing.
</bodyText>
<sectionHeader confidence="0.998786" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999814407407407">
The RWTH approach to MT system combination
is a refined version of the ROVER approach in
ASR (Fiscus, 1997), with additional steps to cope
with reordering between different hypotheses, and
to use true casing information from the input hy-
potheses. The basic concept of the approach has
been described by Matusov et al. (2006). Several
improvements have been added later (Matusov et
al., 2008). This approach includes an enhanced
alignment and reordering framework. In con-
trast to existing approaches (Jayaraman and Lavie,
2005; Rosti et al., 2007), the context of the whole
corpus rather than a single sentence is considered
in this iterative, unsupervised procedure, yielding
a more reliable alignment. Majority voting on the
generated lattice is performed using prior weights
for each system as well as other statistical mod-
els such as a special n-gram language model. In
addition to lattice rescoring, n-best list reranking
techniques can be applied to n best paths of this
lattice. True casing is considered a separate step
in RWTH’s approach, which also takes the input
hypotheses into account.
The pipeline, and consequently the description
of the pipeline given in this paper, is based on our
pipeline for WMT 2009 (Leusch et al., 2009), with
several extensions as described.
</bodyText>
<sectionHeader confidence="0.980418" genericHeader="method">
2 System Combination Algorithm
</sectionHeader>
<bodyText confidence="0.999390266666667">
In this section we present the details of our system
combination method. Figure 1 gives an overview
of the system combination architecture described
in this section. After preprocessing the MT hy-
potheses, pairwise alignments between the hy-
potheses are calculated. The hypotheses are then
reordered to match the word order of a selected
primary or skeleton hypothesis. From this, we
create a lattice which we then rescore using sys-
tem prior weights and a language model (LM).
The single best path in this CN then constitutes
the consensus translation; alternatively the n best
paths are generated and reranked using additional
statistical models. The consensus translation is
then true cased and postprocessed.
</bodyText>
<subsectionHeader confidence="0.958902">
2.1 Word Alignment
</subsectionHeader>
<bodyText confidence="0.999658130434783">
The proposed alignment approach is a statistical
one. It takes advantage of multiple translations for
a whole corpus to compute a consensus translation
for each sentence in this corpus. It also takes ad-
vantage of the fact that the sentences to be aligned
are in the same language.
For each of the K source sentences in the
test corpus, we select one of its translations
En, n = 1, ... , M, as the primary hypothesis.
Then we align the secondary hypotheses Em(m=
1, ... , M; n =� m) with En to match the word or-
der in En. Since it is not clear which hypothesis
should be primary, i. e. has the “best” word order,
we let several or all hypothesis play the role of the
primary translation, and align all pairs of hypothe-
ses (En, Em); n =� m. In this paper, we denote
the number of possible primary hypotheses by N.
The word alignment is trained in analogy to
the alignment training procedure in statistical MT.
The difference is that the two sentences that have
to be aligned are in the same language. We use the
IBM Model 1 (Brown et al., 1993) and the Hid-
den Markov Model (HMM, (Vogel et al., 1996))
</bodyText>
<page confidence="0.991568">
315
</page>
<note confidence="0.797907">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 315–320,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<figureCaption confidence="0.999906">
Figure 1: The system combination architecture.
</figureCaption>
<figure confidence="0.996709105263158">
Hyp 1
...
Hyp k
GIZA++-
alignment
Reordering
Network
generation
Weighting
&amp;
Rescoring
200-best
list
nbest
rescoring
(Triplets,
LM, ...)
Consensus
Translation
</figure>
<bodyText confidence="0.9737883125">
to estimate the alignment model.
The alignment training corpus is created from a
test corpus of effectively N ·(M −1)·K sentences
translated by the involved MT engines. Model pa-
rameters are trained iteratively using the GIZA++
toolkit (Och and Ney, 2003). The training is per-
formed in the directions Em —* En and En —*
Em. The final alignments are determined using
a cost matrix C for each sentence pair (Em, En).
Elements of this matrix are the local costs C(j, i)
of aligning a word em,j from Em to a word en,i
from En. Following Matusov et al. (2004), we
compute these local costs by interpolating the
negated logarithms of the state occupation proba-
bilities from the “source-to-target” and “target-to-
source” training of the HMM model.
</bodyText>
<subsectionHeader confidence="0.9934965">
2.2 Word Reordering and Confusion
Network Generation
</subsectionHeader>
<bodyText confidence="0.999810357142857">
After reordering each secondary hypothesis Em
and the rows of the corresponding alignment cost
matrix, we determine M −1 monotone one-to-one
alignments between En as the primary translation
and Em, m = 1, ... , M; m =� n. We then con-
struct the confusion network.
We consider words without a correspondence to
the primary translation (and vice versa) to have a
null alignment with the empty word e, which will
be transformed to an e-arc in the corresponding
confusion network.
The M−1 monotone one-to-one alignments can
then be transformed into a confusion network, as
described by Matusov et al. (2008).
</bodyText>
<subsectionHeader confidence="0.997619">
2.3 Voting in the Confusion Network
</subsectionHeader>
<bodyText confidence="0.995461142857143">
Instead of choosing a fixed sentence to define the
word order for the consensus translation, we gen-
erate confusion networks for N possible hypothe-
ses as primary, and unite them into a single lattice.
In our experience, this approach is advantageous
in terms of translation quality compared to a min-
imum Bayes risk primary (Rosti et al., 2007).
Weighted majority voting on a single confu-
sion network is straightforward and analogous to
ROVER (Fiscus, 1997). We sum up the probabil-
ities of the arcs which are labeled with the same
word and have the same start state and the same
end state. This can also be regarded as having a
binary system feature in a log-linear model.
</bodyText>
<subsectionHeader confidence="0.994696">
2.4 Language Models
</subsectionHeader>
<bodyText confidence="0.999991">
The lattice representing a union of several confu-
sion networks can then be directly rescored with
an n-gram language model (LM). A transforma-
tion of the lattice is required, since LM history has
to be memorized.
We train a trigram LM on the outputs of the sys-
tems involved in system combination. For LM
training, we take the system hypotheses for the
same test corpus for which the consensus transla-
tions are to be produced. Using this “adapted” LM
for lattice rescoring thus gives bonus to n-grams
from the original system hypotheses, in most cases
from the original phrases. Presumably, many of
these phrases have a correct word order. Previous
experimental results show that using this LM in
rescoring together with a word penalty notably im-
proves translation quality. This even results in bet-
ter translations than using a “classical” LM trained
on a monolingual training corpus. We attribute
this to the fact that most of the systems we com-
bine already include such general LMs.
</bodyText>
<subsectionHeader confidence="0.99162">
2.5 Extracting Consensus Translations
</subsectionHeader>
<bodyText confidence="0.999935">
To generate our consensus translation, we extract
the single-best path from the rescored lattice, us-
ing “classical” decoding as in MT. Alternatively,
we can extract the n best paths for n-best list
rescoring.
</bodyText>
<subsectionHeader confidence="0.988204">
2.6 n-best-List Reranking
</subsectionHeader>
<bodyText confidence="0.999516125">
If n-best lists were generated in the previous steps,
additional sentence-based features can be calcu-
lated on these sentences, and combined in a log-
linear way. These scores can then be used to re-
rank the sentences.
For the WMT 2010 FR–EN and the DE–EN
task, we generated 200-best lists, and calculated
the following features:
</bodyText>
<listItem confidence="0.9980825">
1. Total score from the lattice rescoring
2. NGram posterior weights on those (Zens and
Ney, 2006)
3. Word Penalty
4. HypLM trained on a different set of hypothe-
ses (FR–EN only)
5. Large fourgram model trained on Gigaword
(DE–EN) or Europarl (FR–EN)
6. IBM1 scores and deletion counts based on a
word lexicon trained on WMT training data
</listItem>
<page confidence="0.885411">
316
</page>
<listItem confidence="0.976567666666667">
7. Discriminative word lexicon score (Mauser et
al., 2009)
8. Triplet lexicon score (Hasan et al., 2008)
</listItem>
<bodyText confidence="0.999766">
Other features were also calculated, but did not
seem to give an improvement on the DEV set.
</bodyText>
<subsectionHeader confidence="0.997605">
2.7 Consensus True Casing
</subsectionHeader>
<bodyText confidence="0.999982954545455">
Previous approaches to achieve true cased output
in system combination operated on true-cased lat-
tices, used a separate input-independent true caser,
or used a general true-cased LM to differenti-
ate between alternative arcs in the lattice, as in
(Leusch et al., 2009). For WMT 2010, we use
per-sentence information from the input systems
to determine the consensus case of each output
word. Lattice generation, rescoring, and rerank-
ing are performed on lower-cased input, with a
lower-cased consensus hypothesis as their result.
For each word in this hypothesis, we count how
often each casing variant occurs in the input hy-
potheses for this sentence. We then use the vari-
ant with the highest support for the final consen-
sus output. One advantage is that the set of sys-
tems used to determine the consensus case does
not have to be identical to those used for building
the lattice: Assuming that each word from the con-
sensus hypothesis also occurs in one or several of
the true casing input hypotheses, we can focus on
systems that show a good true casing performance.
</bodyText>
<sectionHeader confidence="0.998813" genericHeader="method">
3 Tuning
</sectionHeader>
<subsectionHeader confidence="0.991713">
3.1 Tuning Weights for Lattice and n-best
Rescoring
</subsectionHeader>
<bodyText confidence="0.999977392857143">
For lattice rescoring, we need to tune system
weights, LM factor, and word penalty to produce
good consensus translations. The same holds for
the log-linear weights in n-best reranking.
For the WMT 2010 Workshop, we selected
a linear combination of BLEU (Papineni et al.,
2002) and TER (Snover et al., 2006) as optimiza-
tion criterion, O := argmaxp {BLEU − TER},
based on previous experience (Mauser et al.,
2008). For more stable results, we use the case-
insensitive variants for both measures, despite the
explicit use of case information in the pipeline.
System weights were tuned to this criterion us-
ing the Downhill Simplex method. Because we
considered the number of segments in the tuning
set to be too small to allow for a further split into
an actual tuning and a control (dev) part, we went
for a method closely related to 5-fold cross valida-
tion: We randomly split the tuning set into 5 equal-
sized parts, and tune parameters on four fifth of
the set, measuring progress on the remaining fifth.
This was repeated for the other four choices for the
“dev” part. Only settings which reliably showed
progress on these five different versions were used
later on the test set. For the actual weights and
numerical parameters to be used on the test set,
we calculate the median of the five variants, which
lowered the risk of outliers and overfitting.
</bodyText>
<subsectionHeader confidence="0.998759">
3.2 System Selection
</subsectionHeader>
<bodyText confidence="0.99999347368421">
With the large numbers of input systems – e.g., 17
for DE–EN – and their large spread in translation
quality – e.g. 10% abs. in BLEU – not all sys-
tems should participate in the system combination
process. For the generation of lattices, we con-
sidered several variants of systems, often starting
from the top, and either replacing some of the sys-
tems very similar to others with systems further
down the list, or not considering those as primary,
adding further systems as additional secondaries.
For true casing, and the additional HypLM for
FR–EN, we selected a set of 8 to 12 promising
systems, and ran an exhaustive search on all com-
binations of those to optimize the LM perplexity
on the dev set (LM) or the true case BLEU/TER
score on a consensus translation (TC). Further re-
search may include a weighted combination here,
followed by an optimization of the weights as de-
scribed in the previous paragraph.
</bodyText>
<sectionHeader confidence="0.997746" genericHeader="method">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.9998625">
Each language pair and each direction in
WMT 2010 had its own set of systems, so we se-
lected and tuned for each direction separately. Af-
ter submission of our system combination output
to WMT 2010, we also calculated scores on the
test set (TEST), to validate our results, and as a
preparation for this report. Note that the scores re-
ported for DEV are calculated on the full DEV set,
but not on any combination of the one-fifth “cross
validation” subcorpora.
</bodyText>
<subsectionHeader confidence="0.908209">
4.1 FR–EN and EN–FR
</subsectionHeader>
<bodyText confidence="0.9999324">
For French–English, we selected a set of eight
systems for the primary submission, and eleven
systems for the contrastive system, of which six
served as skeleton. Six different systems were
used for an additional HypLM, five for consen-
sus true casing. Table 1 shows the distribution of
these systems. We see the results of system com-
bination on DEV and TEST (the latter calculated
after submission) in Table 2. System combination
itself turns out to have the largest improvement,
+0.5 in BLEU and -0.7 in TER on TEST over the
best single system. n-best reranking improves this
result even more, by +0.3/-0.3. The influence of
tuning and of TC selection is measurable on DEV,
but rather small on TEST.
For English–French, 13 systems were used to
construct the lattice, 5 serving as skeleton. Five
different systems were used for true casing. No
n-best list reranking was performed here, as pre-
liminary experiments did not show any significant
</bodyText>
<page confidence="0.999036">
317
</page>
<tableCaption confidence="0.999993">
Table 4: Overview of systems used for DE/EN.
Table 1: Overview of systems used for FR/EN.
</tableCaption>
<table confidence="0.978003944444444">
System FR–EN B EN–FR B
A A
cambridge PLC p P p
cu-zeman S
cmu-statxfer L s S
dfki s S
eu S
geneva
huicong
jhu P L p S p
koc s S
lig
limsi P C p S C p
lium PLC s P C p
nrc P C s S p
rali P L p P C p
rwth P p P C p
uedin PLC p P C p
</table>
<figure confidence="0.992059333333333">
“A” is the primary, “B” the contrastive submission.
“P” denotes a system that served as skeleton.
“S” a system that was only aligned to others.
“L” denotes a system used for a larger HypLM-n-best-
rescoring.
”C” is a system used for consensus true casing.
</figure>
<tableCaption confidence="0.913212">
Table 2: Results for FR–EN.
</tableCaption>
<table confidence="0.999797222222222">
TUNE TER TEST TER
BLEU BLEU
Best single 27.9 55.4 28.5 54.0
Lattice SC 28.4 55.0 29.0 53.3
+ tuning 28.8 54.5 29.1 53.3
+ CV tuning 28.6 54.7 29.1 53.3
+ nbest rerank. 29.0 54.4 29.4 53.0
+ sel. for TC 29.1 54.3 29.3 53.0
Contrast. SC 28.9 54.3 28.8 53.4
</table>
<tableCaption confidence="0.960189333333333">
“SC” stands for System Combination output.
“CV” denotes the split into five different tuning and valida-
tion parts.
“sel. TC” is the separate selection for consensus true casing.
Systems in bold were submitted for WMT 2010.
Table 3: Results for EN–FR.
</tableCaption>
<table confidence="0.996648954545455">
TUNE TER TEST TER
BLEU BLEU
Best single 27.1 55.7 26.5 56.1
Primary SC 28.3 55.2 28.2 54.7
Contrast. SC 28.5 54.7 28.1 54.6
System DE–EN B EN–DE B
A A
cu-zeman S
cmu C P
dfki S p
fbk P C p P p
jhu
kit P C p P C p
koc S C p
limsi P p P C p
liu C S C p
rwth P p P C p
sfu S
uedin P C p P C p
umd P p
uppsala p S
For abbreviations see Table 1.
</table>
<tableCaption confidence="0.791533">
Table 5: Results for DE–EN.
</tableCaption>
<table confidence="0.999772">
TUNE TER TEST TER
BLEU BLEU
Best single 23.8 59.7 23.5 59.7
Lattice SC 24.7 58.5 25.0 57.9
+ tuning 25.1 57.6 25.0 57.6
+ CV tuning 24.8 58.0 24.9 57.8
+ nbest rerank. 25.3 57.6 24.9 57.6
+ sel. for TC 25.5 57.5 24.9 57.6
Contrast. SC 25.2 57.7 24.8 57.7
</table>
<subsubsectionHeader confidence="0.303804">
For abbreviations see Table 2.
</subsubsectionHeader>
<bodyText confidence="0.9997425">
gain in this direction. As a contrastive submission,
we submitted the consensus of 8 systems. These
are also listed in Table 1. The results can be found
in Table 3. Note that the contrastive system was
not tuned using the “cross validation” approach;
as a result, we expected it to be sensitive to over-
fitting. We see improvements around +1.7/-1.4 on
TEST.
</bodyText>
<subsectionHeader confidence="0.981222">
4.2 DE–EN and EN–DE
</subsectionHeader>
<bodyText confidence="0.9999344">
In the German–English language pair, 17 systems
were available, but incorporating only six of them
turned out to deliver optimal results on DEV. As
shown in Table 4, we used a combination of seven
systems in the contrastive submission. While a
</bodyText>
<tableCaption confidence="0.911481">
Table 6: Results for EN–DE.
</tableCaption>
<table confidence="0.9995126">
TUNE TER TEST TER
BLEU BLEU
Best single 16.1 66.3 16.4 65.7
Primary SC 16.4 64.9 17.0 63.7
Contrast. SC 16.4 64.9 17.3 63.4
</table>
<page confidence="0.986539">
318
</page>
<tableCaption confidence="0.998782">
Table 7: Overview of systems used for CZ/EN.
</tableCaption>
<table confidence="0.9953645">
System CZ–EN EN–CZ
aalto P
cmu P C
cu-bojar P P
cu-tecto S
cu-zeman P S C
dcu P
eurotrans S
google P C P C
koc P C
pc-trans S
potsdam P C
sfu S
uedin P C P C
For abbreviations see Table 1.
No contrastive systems were built for this language pair.
</table>
<tableCaption confidence="0.732602">
Table 8: Results for CZ–EN and EN–CZ.
</tableCaption>
<table confidence="0.999869125">
TUNE TER TEST TER
BLEU BLEU
CZ–EN
Best single 21.8 58.4 22.9 57.5
Primary SC 22.4 59.1 23.4 57.9
EN–CZ
Best single 17.0 67.1 16.6 66.4
Primary SC 16.7 65.4 17.4 63.6
</table>
<bodyText confidence="0.999847739130435">
different set of five systems was used for consen-
sus true casing, it turned out that using the same
six systems for the “additional” HypLM as for
the lattice seemed to be optimal in our approach.
Table 5 shows the outcome of our experiments:
Again, we see that the largest effect on TEST re-
sults from system combination as such (+1.5/-1.8).
The other steps, in particular tuning and selection
for TC, seem to help on DEV, but make hardly
a difference on TEST. n-best reranking brings an
improvement of -0.2 in TER, but at a minor dete-
rioration (-0.1) in BLEU.
In the opposite direction, English–German, we
combined all twelve systems, five of them serv-
ing as skeleton. The contrastive submission con-
sists of a combination of eight systems. Six sys-
tems were used for true casing. Again, n-best
list rescoring did not result in any improvement
in preliminary experiments, and was skipped. Re-
sults are shown in Table 6: We see that even
though both versions perform equally well on
DEV (+0.4/-1.4), the contrastive system performs
better by +0.3/-0.3 on TEST (+0.9/-2.3).
</bodyText>
<subsectionHeader confidence="0.98227">
4.3 CZ–EN and EN–CZ
</subsectionHeader>
<bodyText confidence="0.9887985">
In both directions involving Czech, the number of
systems was rather limited, so no additional se-
</bodyText>
<tableCaption confidence="0.995425">
Table 9: Overview of systems used for ES/EN.
</tableCaption>
<table confidence="0.9998201">
System EN–ES B
A
cambridge P C p
dcu P p
dfki P C p
jhu P C p
sfu P C p
uedin P C p
upv p
upv-nnlm P p
</table>
<tableCaption confidence="0.728136">
Table 10: Results for EN–ES.
</tableCaption>
<table confidence="0.999724333333333">
TUNE TER TEST TER
BLEU BLEU
ES–EN
Best single 28.7 53.6 – –
SC 29.0 53.3 – –
EN–ES
Best single 27.8 55.2 28.7 54.0
Primary SC 29.5 52.9 30.0 51.4
Contrast. SC 29.6 52.8 30.1 51.7
</table>
<bodyText confidence="0.998953230769231">
lection turned out to be necessary, and we did not
build a contrastive system. For Czech–English, all
six systems were used; three of them for true cas-
ing. For English–Czech, all eleven systems were
used in building the lattice, six of them also as
skeleton. Five systems were used in the true cas-
ing step. Table 7 lists these systems. From the
results in Table 8, we see that for CZ–EN, system
combination gains around +0.5 in BLEU, but at
costs of +0.4 to +0.7 in TER. For EN–CZ, the re-
sults look more positive: While we see only -0.3/-
1.7 on DEV, there is a significant improvement of
+1.2/-2.8 on TEST.
</bodyText>
<subsectionHeader confidence="0.903786">
4.4 ES–EN and EN–ES
</subsectionHeader>
<bodyText confidence="0.9999926875">
In the Spanish–English language pair, we did not
see any improvement at all on the direction with
English as target in preliminary experiments. Con-
sequently, and given the time constraints, we did
not further investigate on this language pair. Post-
eval experiments revealed that improvements of
+0.3/-0.3 are possible, with far off-center weights
favoring the top three systems.
On English–Spanish, where these preliminary
experiments showed a gain, we used seven out of
the available ten systems in building the lattice
for the primary system, eight for the contrastive.
Five of those were uses for consensus true cas-
ing. Table 9 lists these systems. Table 10 shows
the results on this language pair: For both the pri-
mary and the contrastive systems we see improve-
</bodyText>
<page confidence="0.997677">
319
</page>
<bodyText confidence="0.996085666666667">
ments of around +1.7/-2.3 on DEV, and +1.3/-2.6
on TEST. Except for the TER on TEST, these two
submissions differ only by f0.1 from each other.
</bodyText>
<sectionHeader confidence="0.9742" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.975720529411765">
E. Matusov, N. Ueffing, and H. Ney. 2006. Computing
consensus translation from multiple machine trans-
lation systems using enhanced hypotheses align-
ment. In Conference of the European Chapter of the
Association for Computational Linguistics (EACL),
pages 33–40, Trento, Italy, April.
We have shown that our system combination sys-
tem can lead to significant improvements over sin-
gle best MT output where a significant number of
comparably good translations is available on a sin-
gle language pair. n-best reranking can further
improve the quality of the consensus translation;
results vary though. While consensus true casing
turned out to be very useful despite of its simplic-
ity, we were unable to find significant improve-
ments on TEST from the selection of a separate
set of true casing input systems.
</bodyText>
<sectionHeader confidence="0.998464" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999676666666667">
This work was partly realized as part of the
Quaero Programme, funded by OSEO, French
State agency for innovation. This work was
partly supported by the Defense Advanced Re-
search Projects Agency (DARPA) under Contract
No. HR0011-06-C-0023.
</bodyText>
<sectionHeader confidence="0.997174" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999879602564102">
P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and
R. L. Mercer. 1993. The mathematics of statistical
machine translation: parameter estimation. Compu-
tational Linguistics, 19(2):263–311, June.
J. Fiscus. 1997. A post-processing system to yield re-
duced word error rates: Recognizer output voting er-
ror reduction (ROVER). In IEEE Workshop on Au-
tomatic Speech Recognition and Understanding.
S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr´es-Ferrer.
2008. Triplet lexicon models for statistical machine
translation. In Conference on Empirical Methods in
Natural Language Processing, pages 372–381, Hon-
olulu, Hawaii, October. Association for Computa-
tional Linguistics.
S. Jayaraman and A. Lavie. 2005. Multi-engine ma-
chine translation guided by explicit word matching.
In Proc. of the 10th Annual Conf. of the European
Association for Machine Translation (EAMT), pages
143–152, Budapest, Hungary, May.
G. Leusch, E. Matusov, and H. Ney. 2009. The
RWTH system combination system for WMT 2009.
In Fourth Workshop on Statistical Machine Transla-
tion, pages 56–60, Athens, Greece, March. Associa-
tion for Computational Linguistics.
E. Matusov, R. Zens, and H. Ney. 2004. Symmetric
word alignments for statistical machine translation.
In COLING ’04: The 20th Int. Conf. on Computa-
tional Linguistics, pages 219–225, Geneva, Switzer-
land, August.
E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi,
D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee,
J. B. Marino, M. Paulik, S. Roukos, H. Schwenk,
and H. Ney. 2008. System combination for machine
translation of spoken and written language. IEEE
Transactions on Audio, Speech and Language Pro-
cessing, 16(7):1222–1237, September.
A. Mauser, S. Hasan, and H. Ney. 2008. Automatic
evaluation measures for statistical machine transla-
tion system optimization. In International Confer-
ence on Language Resources and Evaluation, Mar-
rakech, Morocco, May.
A. Mauser, S. Hasan, and H. Ney. 2009. Extending sta-
tistical machine translation with discriminative and
trigger-based lexicon models. In Conference on Em-
pirical Methods in Natural Language Processing,
pages 210–217, Singapore, August.
F. J. Och and H. Ney. 2003. A systematic comparison
of various statistical alignment models. Computa-
tional Linguistics, 29(1):19–51, March.
K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002.
BLEU: a Method for Automatic Evaluation of Ma-
chine Translation. In Proc. of the 40th Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 311–318, Philadelphia, PA, July.
A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007.
Improved word-level system combination for ma-
chine translation. In Proceedings of the 45th Annual
Meeting of the Association of Computational Lin-
guistics (ACL), pages 312–319, Prague, Czech Re-
public, June.
M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and
J. Makhoul. 2006. A Study of Translation Error
Rate with Targeted Human Annotation. In Proc. of
the 7th Conf. of the Association for Machine Trans-
lation in the Americas (AMTA), pages 223–231,
Boston, MA, August.
S. Vogel, H. Ney, and C. Tillmann. 1996. HMM-
based word alignment in statistical translation. In
COLING ’96: The 16th Int. Conf. on Computational
Linguistics, pages 836–841, Copenhagen, Denmark,
August.
R. Zens and H. Ney. 2006. N-gram posterior prob-
abilities for statistical machine translation. In Hu-
man Language Technology Conf. /North American
Chapter of the Assoc. for Computational Linguistics
Annual Meeting (HLT-NAACL), Workshop on Statis-
tical Machine Translation, pages 72–77, New York
City, June.
</reference>
<page confidence="0.998256">
320
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.523856">
<title confidence="0.924266">The RWTH System Combination System for WMT 2010</title>
<author confidence="0.991411">Gregor Leusch</author>
<author confidence="0.991411">Hermann</author>
<affiliation confidence="0.976863">RWTH Aachen</affiliation>
<address confidence="0.902521">Aachen,</address>
<abstract confidence="0.9815286">RWTH participated in the System Combination task of the Fifth Workshop on Statistical Machine Translation (WMT 2010). For 7 of the 8 language pairs, we combine 5 to 13 systems into a single contranslation, using additional reranking techniques in two of these language pairs. Depending on the language pair, improvements versus the best sinsystem are in the range of and on BLEU, and between and on TER. Novel techniques compared with RWTH’s submission to WMT 2009 the utilization of reranking techniques, a consensus true casing approach, a different tuning algorithm, and the separate selection of input systems for CN construction, primary/skeleton hypotheses, HypLM, and true casing.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
<author>S A Della Pietra</author>
<author>V J Della Pietra</author>
<author>R L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3997" citStr="Brown et al., 1993" startWordPosition="661" endWordPosition="664">e align the secondary hypotheses Em(m= 1, ... , M; n =� m) with En to match the word order in En. Since it is not clear which hypothesis should be primary, i. e. has the “best” word order, we let several or all hypothesis play the role of the primary translation, and align all pairs of hypotheses (En, Em); n =� m. In this paper, we denote the number of possible primary hypotheses by N. The word alignment is trained in analogy to the alignment training procedure in statistical MT. The difference is that the two sentences that have to be aligned are in the same language. We use the IBM Model 1 (Brown et al., 1993) and the Hidden Markov Model (HMM, (Vogel et al., 1996)) 315 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 315–320, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Figure 1: The system combination architecture. Hyp 1 ... Hyp k GIZA++- alignment Reordering Network generation Weighting &amp; Rescoring 200-best list nbest rescoring (Triplets, LM, ...) Consensus Translation to estimate the alignment model. The alignment training corpus is created from a test corpus of effectively N ·(M −1)·K sentences translated by th</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. F. Brown, S. A. Della Pietra, V. J. Della Pietra, and R. L. Mercer. 1993. The mathematics of statistical machine translation: parameter estimation. Computational Linguistics, 19(2):263–311, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Fiscus</author>
</authors>
<title>A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER).</title>
<date>1997</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition and Understanding.</booktitle>
<contexts>
<context position="1020" citStr="Fiscus, 1997" startWordPosition="162" endWordPosition="163">hniques in two of these language pairs. Depending on the language pair, improvements versus the best single system are in the range of +0.5 and +1.7 on BLEU, and between −0.4 and −2.3 on TER. Novel techniques compared with RWTH’s submission to WMT 2009 include the utilization of n-best reranking techniques, a consensus true casing approach, a different tuning algorithm, and the separate selection of input systems for CN construction, primary/skeleton hypotheses, HypLM, and true casing. 1 Introduction The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR (Fiscus, 1997), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. In contrast to existing approaches (Jayaraman and Lavie, 2005; Rosti et al., 2007), the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment. Majo</context>
<context position="6338" citStr="Fiscus, 1997" startWordPosition="1043" endWordPosition="1044">1 monotone one-to-one alignments can then be transformed into a confusion network, as described by Matusov et al. (2008). 2.3 Voting in the Confusion Network Instead of choosing a fixed sentence to define the word order for the consensus translation, we generate confusion networks for N possible hypotheses as primary, and unite them into a single lattice. In our experience, this approach is advantageous in terms of translation quality compared to a minimum Bayes risk primary (Rosti et al., 2007). Weighted majority voting on a single confusion network is straightforward and analogous to ROVER (Fiscus, 1997). We sum up the probabilities of the arcs which are labeled with the same word and have the same start state and the same end state. This can also be regarded as having a binary system feature in a log-linear model. 2.4 Language Models The lattice representing a union of several confusion networks can then be directly rescored with an n-gram language model (LM). A transformation of the lattice is required, since LM history has to be memorized. We train a trigram LM on the outputs of the systems involved in system combination. For LM training, we take the system hypotheses for the same test cor</context>
</contexts>
<marker>Fiscus, 1997</marker>
<rawString>J. Fiscus. 1997. A post-processing system to yield reduced word error rates: Recognizer output voting error reduction (ROVER). In IEEE Workshop on Automatic Speech Recognition and Understanding.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Hasan</author>
<author>J Ganitkevitch</author>
<author>H Ney</author>
<author>J Andr´es-Ferrer</author>
</authors>
<title>Triplet lexicon models for statistical machine translation.</title>
<date>2008</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>372--381</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Honolulu, Hawaii,</location>
<marker>Hasan, Ganitkevitch, Ney, Andr´es-Ferrer, 2008</marker>
<rawString>S. Hasan, J. Ganitkevitch, H. Ney, and J. Andr´es-Ferrer. 2008. Triplet lexicon models for statistical machine translation. In Conference on Empirical Methods in Natural Language Processing, pages 372–381, Honolulu, Hawaii, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Jayaraman</author>
<author>A Lavie</author>
</authors>
<title>Multi-engine machine translation guided by explicit word matching.</title>
<date>2005</date>
<booktitle>In Proc. of the 10th Annual Conf. of the European Association for Machine Translation (EAMT),</booktitle>
<pages>143--152</pages>
<location>Budapest, Hungary,</location>
<contexts>
<context position="1438" citStr="Jayaraman and Lavie, 2005" startWordPosition="224" endWordPosition="227">ut systems for CN construction, primary/skeleton hypotheses, HypLM, and true casing. 1 Introduction The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR (Fiscus, 1997), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. In contrast to existing approaches (Jayaraman and Lavie, 2005; Rosti et al., 2007), the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment. Majority voting on the generated lattice is performed using prior weights for each system as well as other statistical models such as a special n-gram language model. In addition to lattice rescoring, n-best list reranking techniques can be applied to n best paths of this lattice. True casing is considered a separate step in RWTH’s approach, which also takes the input hypotheses into account. The pipeline, and conseque</context>
</contexts>
<marker>Jayaraman, Lavie, 2005</marker>
<rawString>S. Jayaraman and A. Lavie. 2005. Multi-engine machine translation guided by explicit word matching. In Proc. of the 10th Annual Conf. of the European Association for Machine Translation (EAMT), pages 143–152, Budapest, Hungary, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Leusch</author>
<author>E Matusov</author>
<author>H Ney</author>
</authors>
<title>The RWTH system combination system for WMT</title>
<date>2009</date>
<booktitle>In Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>56--60</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece,</location>
<contexts>
<context position="2155" citStr="Leusch et al., 2009" startWordPosition="341" endWordPosition="344">d in this iterative, unsupervised procedure, yielding a more reliable alignment. Majority voting on the generated lattice is performed using prior weights for each system as well as other statistical models such as a special n-gram language model. In addition to lattice rescoring, n-best list reranking techniques can be applied to n best paths of this lattice. True casing is considered a separate step in RWTH’s approach, which also takes the input hypotheses into account. The pipeline, and consequently the description of the pipeline given in this paper, is based on our pipeline for WMT 2009 (Leusch et al., 2009), with several extensions as described. 2 System Combination Algorithm In this section we present the details of our system combination method. Figure 1 gives an overview of the system combination architecture described in this section. After preprocessing the MT hypotheses, pairwise alignments between the hypotheses are calculated. The hypotheses are then reordered to match the word order of a selected primary or skeleton hypothesis. From this, we create a lattice which we then rescore using system prior weights and a language model (LM). The single best path in this CN then constitutes the c</context>
<context position="8989" citStr="Leusch et al., 2009" startWordPosition="1487" endWordPosition="1490">Gigaword (DE–EN) or Europarl (FR–EN) 6. IBM1 scores and deletion counts based on a word lexicon trained on WMT training data 316 7. Discriminative word lexicon score (Mauser et al., 2009) 8. Triplet lexicon score (Hasan et al., 2008) Other features were also calculated, but did not seem to give an improvement on the DEV set. 2.7 Consensus True Casing Previous approaches to achieve true cased output in system combination operated on true-cased lattices, used a separate input-independent true caser, or used a general true-cased LM to differentiate between alternative arcs in the lattice, as in (Leusch et al., 2009). For WMT 2010, we use per-sentence information from the input systems to determine the consensus case of each output word. Lattice generation, rescoring, and reranking are performed on lower-cased input, with a lower-cased consensus hypothesis as their result. For each word in this hypothesis, we count how often each casing variant occurs in the input hypotheses for this sentence. We then use the variant with the highest support for the final consensus output. One advantage is that the set of systems used to determine the consensus case does not have to be identical to those used for building</context>
</contexts>
<marker>Leusch, Matusov, Ney, 2009</marker>
<rawString>G. Leusch, E. Matusov, and H. Ney. 2009. The RWTH system combination system for WMT 2009. In Fourth Workshop on Statistical Machine Translation, pages 56–60, Athens, Greece, March. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Symmetric word alignments for statistical machine translation.</title>
<date>2004</date>
<booktitle>In COLING ’04: The 20th Int. Conf. on Computational Linguistics,</booktitle>
<pages>219--225</pages>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="5004" citStr="Matusov et al. (2004)" startWordPosition="825" endWordPosition="828">best list nbest rescoring (Triplets, LM, ...) Consensus Translation to estimate the alignment model. The alignment training corpus is created from a test corpus of effectively N ·(M −1)·K sentences translated by the involved MT engines. Model parameters are trained iteratively using the GIZA++ toolkit (Och and Ney, 2003). The training is performed in the directions Em —* En and En —* Em. The final alignments are determined using a cost matrix C for each sentence pair (Em, En). Elements of this matrix are the local costs C(j, i) of aligning a word em,j from Em to a word en,i from En. Following Matusov et al. (2004), we compute these local costs by interpolating the negated logarithms of the state occupation probabilities from the “source-to-target” and “target-tosource” training of the HMM model. 2.2 Word Reordering and Confusion Network Generation After reordering each secondary hypothesis Em and the rows of the corresponding alignment cost matrix, we determine M −1 monotone one-to-one alignments between En as the primary translation and Em, m = 1, ... , M; m =� n. We then construct the confusion network. We consider words without a correspondence to the primary translation (and vice versa) to have a n</context>
</contexts>
<marker>Matusov, Zens, Ney, 2004</marker>
<rawString>E. Matusov, R. Zens, and H. Ney. 2004. Symmetric word alignments for statistical machine translation. In COLING ’04: The 20th Int. Conf. on Computational Linguistics, pages 219–225, Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Matusov</author>
<author>G Leusch</author>
<author>R E Banchs</author>
<author>N Bertoldi</author>
<author>D Dechelotte</author>
<author>M Federico</author>
<author>M Kolss</author>
<author>Y S Lee</author>
<author>J B Marino</author>
<author>M Paulik</author>
<author>S Roukos</author>
<author>H Schwenk</author>
<author>H Ney</author>
</authors>
<title>System combination for machine translation of spoken and written language.</title>
<date>2008</date>
<journal>IEEE Transactions on Audio, Speech and Language Processing,</journal>
<volume>16</volume>
<issue>7</issue>
<contexts>
<context position="1304" citStr="Matusov et al., 2008" startWordPosition="205" endWordPosition="208"> of n-best reranking techniques, a consensus true casing approach, a different tuning algorithm, and the separate selection of input systems for CN construction, primary/skeleton hypotheses, HypLM, and true casing. 1 Introduction The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR (Fiscus, 1997), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. In contrast to existing approaches (Jayaraman and Lavie, 2005; Rosti et al., 2007), the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment. Majority voting on the generated lattice is performed using prior weights for each system as well as other statistical models such as a special n-gram language model. In addition to lattice rescoring, n-best list reranking techniques can be applied to n best paths of this lattice. True c</context>
<context position="5845" citStr="Matusov et al. (2008)" startWordPosition="960" endWordPosition="963">Network Generation After reordering each secondary hypothesis Em and the rows of the corresponding alignment cost matrix, we determine M −1 monotone one-to-one alignments between En as the primary translation and Em, m = 1, ... , M; m =� n. We then construct the confusion network. We consider words without a correspondence to the primary translation (and vice versa) to have a null alignment with the empty word e, which will be transformed to an e-arc in the corresponding confusion network. The M−1 monotone one-to-one alignments can then be transformed into a confusion network, as described by Matusov et al. (2008). 2.3 Voting in the Confusion Network Instead of choosing a fixed sentence to define the word order for the consensus translation, we generate confusion networks for N possible hypotheses as primary, and unite them into a single lattice. In our experience, this approach is advantageous in terms of translation quality compared to a minimum Bayes risk primary (Rosti et al., 2007). Weighted majority voting on a single confusion network is straightforward and analogous to ROVER (Fiscus, 1997). We sum up the probabilities of the arcs which are labeled with the same word and have the same start stat</context>
</contexts>
<marker>Matusov, Leusch, Banchs, Bertoldi, Dechelotte, Federico, Kolss, Lee, Marino, Paulik, Roukos, Schwenk, Ney, 2008</marker>
<rawString>E. Matusov, G. Leusch, R. E. Banchs, N. Bertoldi, D. Dechelotte, M. Federico, M. Kolss, Y. S. Lee, J. B. Marino, M. Paulik, S. Roukos, H. Schwenk, and H. Ney. 2008. System combination for machine translation of spoken and written language. IEEE Transactions on Audio, Speech and Language Processing, 16(7):1222–1237, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mauser</author>
<author>S Hasan</author>
<author>H Ney</author>
</authors>
<title>Automatic evaluation measures for statistical machine translation system optimization.</title>
<date>2008</date>
<booktitle>In International Conference on Language Resources and Evaluation,</booktitle>
<location>Marrakech, Morocco,</location>
<contexts>
<context position="10263" citStr="Mauser et al., 2008" startWordPosition="1704" endWordPosition="1707">us hypothesis also occurs in one or several of the true casing input hypotheses, we can focus on systems that show a good true casing performance. 3 Tuning 3.1 Tuning Weights for Lattice and n-best Rescoring For lattice rescoring, we need to tune system weights, LM factor, and word penalty to produce good consensus translations. The same holds for the log-linear weights in n-best reranking. For the WMT 2010 Workshop, we selected a linear combination of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as optimization criterion, O := argmaxp {BLEU − TER}, based on previous experience (Mauser et al., 2008). For more stable results, we use the caseinsensitive variants for both measures, despite the explicit use of case information in the pipeline. System weights were tuned to this criterion using the Downhill Simplex method. Because we considered the number of segments in the tuning set to be too small to allow for a further split into an actual tuning and a control (dev) part, we went for a method closely related to 5-fold cross validation: We randomly split the tuning set into 5 equalsized parts, and tune parameters on four fifth of the set, measuring progress on the remaining fifth. This was </context>
</contexts>
<marker>Mauser, Hasan, Ney, 2008</marker>
<rawString>A. Mauser, S. Hasan, and H. Ney. 2008. Automatic evaluation measures for statistical machine translation system optimization. In International Conference on Language Resources and Evaluation, Marrakech, Morocco, May.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Mauser</author>
<author>S Hasan</author>
<author>H Ney</author>
</authors>
<title>Extending statistical machine translation with discriminative and trigger-based lexicon models.</title>
<date>2009</date>
<booktitle>In Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>210--217</pages>
<location>Singapore,</location>
<contexts>
<context position="8556" citStr="Mauser et al., 2009" startWordPosition="1416" endWordPosition="1419"> on these sentences, and combined in a loglinear way. These scores can then be used to rerank the sentences. For the WMT 2010 FR–EN and the DE–EN task, we generated 200-best lists, and calculated the following features: 1. Total score from the lattice rescoring 2. NGram posterior weights on those (Zens and Ney, 2006) 3. Word Penalty 4. HypLM trained on a different set of hypotheses (FR–EN only) 5. Large fourgram model trained on Gigaword (DE–EN) or Europarl (FR–EN) 6. IBM1 scores and deletion counts based on a word lexicon trained on WMT training data 316 7. Discriminative word lexicon score (Mauser et al., 2009) 8. Triplet lexicon score (Hasan et al., 2008) Other features were also calculated, but did not seem to give an improvement on the DEV set. 2.7 Consensus True Casing Previous approaches to achieve true cased output in system combination operated on true-cased lattices, used a separate input-independent true caser, or used a general true-cased LM to differentiate between alternative arcs in the lattice, as in (Leusch et al., 2009). For WMT 2010, we use per-sentence information from the input systems to determine the consensus case of each output word. Lattice generation, rescoring, and rerankin</context>
</contexts>
<marker>Mauser, Hasan, Ney, 2009</marker>
<rawString>A. Mauser, S. Hasan, and H. Ney. 2009. Extending statistical machine translation with discriminative and trigger-based lexicon models. In Conference on Empirical Methods in Natural Language Processing, pages 210–217, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="4705" citStr="Och and Ney, 2003" startWordPosition="766" endWordPosition="769">Workshop on Statistical Machine Translation and MetricsMATR, pages 315–320, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Figure 1: The system combination architecture. Hyp 1 ... Hyp k GIZA++- alignment Reordering Network generation Weighting &amp; Rescoring 200-best list nbest rescoring (Triplets, LM, ...) Consensus Translation to estimate the alignment model. The alignment training corpus is created from a test corpus of effectively N ·(M −1)·K sentences translated by the involved MT engines. Model parameters are trained iteratively using the GIZA++ toolkit (Och and Ney, 2003). The training is performed in the directions Em —* En and En —* Em. The final alignments are determined using a cost matrix C for each sentence pair (Em, En). Elements of this matrix are the local costs C(j, i) of aligning a word em,j from Em to a word en,i from En. Following Matusov et al. (2004), we compute these local costs by interpolating the negated logarithms of the state occupation probabilities from the “source-to-target” and “target-tosource” training of the HMM model. 2.2 Word Reordering and Confusion Network Generation After reordering each secondary hypothesis Em and the rows of </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F. J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Papineni</author>
<author>S Roukos</author>
<author>T Ward</author>
<author>W J Zhu</author>
</authors>
<title>BLEU: a Method for Automatic Evaluation of Machine Translation.</title>
<date>2002</date>
<booktitle>In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>311--318</pages>
<location>Philadelphia, PA,</location>
<contexts>
<context position="10128" citStr="Papineni et al., 2002" startWordPosition="1680" endWordPosition="1683">termine the consensus case does not have to be identical to those used for building the lattice: Assuming that each word from the consensus hypothesis also occurs in one or several of the true casing input hypotheses, we can focus on systems that show a good true casing performance. 3 Tuning 3.1 Tuning Weights for Lattice and n-best Rescoring For lattice rescoring, we need to tune system weights, LM factor, and word penalty to produce good consensus translations. The same holds for the log-linear weights in n-best reranking. For the WMT 2010 Workshop, we selected a linear combination of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as optimization criterion, O := argmaxp {BLEU − TER}, based on previous experience (Mauser et al., 2008). For more stable results, we use the caseinsensitive variants for both measures, despite the explicit use of case information in the pipeline. System weights were tuned to this criterion using the Downhill Simplex method. Because we considered the number of segments in the tuning set to be too small to allow for a further split into an actual tuning and a control (dev) part, we went for a method closely related to 5-fold cross validation: We randomly split the</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. 2002. BLEU: a Method for Automatic Evaluation of Machine Translation. In Proc. of the 40th Annual Meeting of the Association for Computational Linguistics (ACL), pages 311–318, Philadelphia, PA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Rosti</author>
<author>S Matsoukas</author>
<author>R Schwartz</author>
</authors>
<title>Improved word-level system combination for machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL),</booktitle>
<pages>312--319</pages>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="1459" citStr="Rosti et al., 2007" startWordPosition="228" endWordPosition="231">ion, primary/skeleton hypotheses, HypLM, and true casing. 1 Introduction The RWTH approach to MT system combination is a refined version of the ROVER approach in ASR (Fiscus, 1997), with additional steps to cope with reordering between different hypotheses, and to use true casing information from the input hypotheses. The basic concept of the approach has been described by Matusov et al. (2006). Several improvements have been added later (Matusov et al., 2008). This approach includes an enhanced alignment and reordering framework. In contrast to existing approaches (Jayaraman and Lavie, 2005; Rosti et al., 2007), the context of the whole corpus rather than a single sentence is considered in this iterative, unsupervised procedure, yielding a more reliable alignment. Majority voting on the generated lattice is performed using prior weights for each system as well as other statistical models such as a special n-gram language model. In addition to lattice rescoring, n-best list reranking techniques can be applied to n best paths of this lattice. True casing is considered a separate step in RWTH’s approach, which also takes the input hypotheses into account. The pipeline, and consequently the description </context>
<context position="6225" citStr="Rosti et al., 2007" startWordPosition="1024" endWordPosition="1027">l alignment with the empty word e, which will be transformed to an e-arc in the corresponding confusion network. The M−1 monotone one-to-one alignments can then be transformed into a confusion network, as described by Matusov et al. (2008). 2.3 Voting in the Confusion Network Instead of choosing a fixed sentence to define the word order for the consensus translation, we generate confusion networks for N possible hypotheses as primary, and unite them into a single lattice. In our experience, this approach is advantageous in terms of translation quality compared to a minimum Bayes risk primary (Rosti et al., 2007). Weighted majority voting on a single confusion network is straightforward and analogous to ROVER (Fiscus, 1997). We sum up the probabilities of the arcs which are labeled with the same word and have the same start state and the same end state. This can also be regarded as having a binary system feature in a log-linear model. 2.4 Language Models The lattice representing a union of several confusion networks can then be directly rescored with an n-gram language model (LM). A transformation of the lattice is required, since LM history has to be memorized. We train a trigram LM on the outputs of</context>
</contexts>
<marker>Rosti, Matsoukas, Schwartz, 2007</marker>
<rawString>A. V. Rosti, S. Matsoukas, and R. Schwartz. 2007. Improved word-level system combination for machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics (ACL), pages 312–319, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Snover</author>
<author>B Dorr</author>
<author>R Schwartz</author>
<author>L Micciulla</author>
<author>J Makhoul</author>
</authors>
<title>A Study of Translation Error Rate with Targeted Human Annotation.</title>
<date>2006</date>
<booktitle>In Proc. of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA),</booktitle>
<pages>223--231</pages>
<location>Boston, MA,</location>
<contexts>
<context position="10158" citStr="Snover et al., 2006" startWordPosition="1686" endWordPosition="1689">not have to be identical to those used for building the lattice: Assuming that each word from the consensus hypothesis also occurs in one or several of the true casing input hypotheses, we can focus on systems that show a good true casing performance. 3 Tuning 3.1 Tuning Weights for Lattice and n-best Rescoring For lattice rescoring, we need to tune system weights, LM factor, and word penalty to produce good consensus translations. The same holds for the log-linear weights in n-best reranking. For the WMT 2010 Workshop, we selected a linear combination of BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) as optimization criterion, O := argmaxp {BLEU − TER}, based on previous experience (Mauser et al., 2008). For more stable results, we use the caseinsensitive variants for both measures, despite the explicit use of case information in the pipeline. System weights were tuned to this criterion using the Downhill Simplex method. Because we considered the number of segments in the tuning set to be too small to allow for a further split into an actual tuning and a control (dev) part, we went for a method closely related to 5-fold cross validation: We randomly split the tuning set into 5 equalsized </context>
</contexts>
<marker>Snover, Dorr, Schwartz, Micciulla, Makhoul, 2006</marker>
<rawString>M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul. 2006. A Study of Translation Error Rate with Targeted Human Annotation. In Proc. of the 7th Conf. of the Association for Machine Translation in the Americas (AMTA), pages 223–231, Boston, MA, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Vogel</author>
<author>H Ney</author>
<author>C Tillmann</author>
</authors>
<title>HMMbased word alignment in statistical translation.</title>
<date>1996</date>
<booktitle>In COLING ’96: The 16th Int. Conf. on Computational Linguistics,</booktitle>
<pages>836--841</pages>
<location>Copenhagen, Denmark,</location>
<contexts>
<context position="4052" citStr="Vogel et al., 1996" startWordPosition="672" endWordPosition="675"> m) with En to match the word order in En. Since it is not clear which hypothesis should be primary, i. e. has the “best” word order, we let several or all hypothesis play the role of the primary translation, and align all pairs of hypotheses (En, Em); n =� m. In this paper, we denote the number of possible primary hypotheses by N. The word alignment is trained in analogy to the alignment training procedure in statistical MT. The difference is that the two sentences that have to be aligned are in the same language. We use the IBM Model 1 (Brown et al., 1993) and the Hidden Markov Model (HMM, (Vogel et al., 1996)) 315 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 315–320, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics Figure 1: The system combination architecture. Hyp 1 ... Hyp k GIZA++- alignment Reordering Network generation Weighting &amp; Rescoring 200-best list nbest rescoring (Triplets, LM, ...) Consensus Translation to estimate the alignment model. The alignment training corpus is created from a test corpus of effectively N ·(M −1)·K sentences translated by the involved MT engines. Model parameters are trained ite</context>
</contexts>
<marker>Vogel, Ney, Tillmann, 1996</marker>
<rawString>S. Vogel, H. Ney, and C. Tillmann. 1996. HMMbased word alignment in statistical translation. In COLING ’96: The 16th Int. Conf. on Computational Linguistics, pages 836–841, Copenhagen, Denmark, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>N-gram posterior probabilities for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL), Workshop on Statistical Machine Translation,</booktitle>
<pages>72--77</pages>
<location>New York City,</location>
<contexts>
<context position="8254" citStr="Zens and Ney, 2006" startWordPosition="1364" endWordPosition="1367">e extract the single-best path from the rescored lattice, using “classical” decoding as in MT. Alternatively, we can extract the n best paths for n-best list rescoring. 2.6 n-best-List Reranking If n-best lists were generated in the previous steps, additional sentence-based features can be calculated on these sentences, and combined in a loglinear way. These scores can then be used to rerank the sentences. For the WMT 2010 FR–EN and the DE–EN task, we generated 200-best lists, and calculated the following features: 1. Total score from the lattice rescoring 2. NGram posterior weights on those (Zens and Ney, 2006) 3. Word Penalty 4. HypLM trained on a different set of hypotheses (FR–EN only) 5. Large fourgram model trained on Gigaword (DE–EN) or Europarl (FR–EN) 6. IBM1 scores and deletion counts based on a word lexicon trained on WMT training data 316 7. Discriminative word lexicon score (Mauser et al., 2009) 8. Triplet lexicon score (Hasan et al., 2008) Other features were also calculated, but did not seem to give an improvement on the DEV set. 2.7 Consensus True Casing Previous approaches to achieve true cased output in system combination operated on true-cased lattices, used a separate input-indepe</context>
</contexts>
<marker>Zens, Ney, 2006</marker>
<rawString>R. Zens and H. Ney. 2006. N-gram posterior probabilities for statistical machine translation. In Human Language Technology Conf. /North American Chapter of the Assoc. for Computational Linguistics Annual Meeting (HLT-NAACL), Workshop on Statistical Machine Translation, pages 72–77, New York City, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>