<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000022">
<title confidence="0.987473">
How to make words with vectors:
Phrase generation in distributional semantics
</title>
<author confidence="0.990925">
Georgiana Dinu and Marco Baroni
</author>
<affiliation confidence="0.9960725">
Center for Mind/Brain Sciences
University of Trento, Italy
</affiliation>
<email confidence="0.995849">
(georgiana.dinu|marco.baroni)@unitn.it
</email>
<sectionHeader confidence="0.993839" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999989714285714">
We introduce the problem of generation
in distributional semantics: Given a distri-
butional vector representing some mean-
ing, how can we generate the phrase that
best expresses that meaning? We mo-
tivate this novel challenge on theoretical
and practical grounds and propose a sim-
ple data-driven approach to the estimation
of generation functions. We test this in
a monolingual scenario (paraphrase gen-
eration) as well as in a cross-lingual set-
ting (translation by synthesizing adjective-
noun phrase vectors in English and gener-
ating the equivalent expressions in Italian).
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999234">
Distributional methods for semantics approximate
the meaning of linguistic expressions with vectors
that summarize the contexts in which they occur
in large samples of text. This has been a very suc-
cessful approach to lexical semantics (Erk, 2012),
where semantic relatedness is assessed by compar-
ing vectors. Recently these methods have been
extended to phrases and sentences by means of
composition operations (see Baroni (2013) for an
overview). For example, given the vectors repre-
senting red and car, composition derives a vector
that approximates the meaning of red car.
However, the link between language and mean-
ing is, obviously, bidirectional: As message recip-
ients we are exposed to a linguistic expression and
we must compute its meaning (the synthesis prob-
lem). As message producers we start from the
meaning we want to communicate (a “thought”)
and we must encode it into a word sequence (the
generation problem). If distributional semantics
is to be considered a proper semantic theory, then
it must deal not only with synthesis (going from
words to vectors), but also with generation (from
vectors to words).
Besides these theoretical considerations, phrase
generation from vectors has many useful applica-
tions. We can, for example, synthesize the vector
representing the meaning of a phrase or sentence,
and then generate alternative phrases or sentences
from this vector to accomplish true paraphrase
generation (as opposed to paraphrase detection or
ranking of candidate paraphrases).
Generation can be even more useful when the
source vector comes from another modality or lan-
guage. Recent work on grounding language in vi-
sion shows that it is possible to represent images
and linguistic expressions in a common vector-
based semantic space (Frome et al., 2013; Socher
et al., 2013). Given a vector representing an im-
age, generation can be used to productively con-
struct phrases or sentences that describe the im-
age (as opposed to simply retrieving an existing
description from a set of candidates). Translation
is another potential application of the generation
framework: Given a semantic space shared be-
tween two or more languages, one can compose a
word sequence in one language and generate trans-
lations in another, with the shared semantic vector
space functioning as interlingua.
Distributional semantics assumes a lexicon of
atomic expressions (that, for simplicity, we take
to be words), each associated to a vector. Thus,
at the single-word level, the problem of genera-
tion is solved by a trivial generation-by-synthesis
approach: Given an arbitrary target vector, “gener-
ate” the corresponding word by searching through
the lexicon for the word with the closest vector to
the target. This is however unfeasible for larger
expressions: Given n vocabulary elements, this
approach requires checking nk phrases of length
k. This becomes prohibitive already for relatively
short phrases, as reasonably-sized vocabularies do
not go below tens of thousands of words. The
search space for 3-word phrases in a 10K-word
vocabulary is already in the order of trillions. In
</bodyText>
<page confidence="0.980663">
624
</page>
<note confidence="0.831078">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 624–633,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9998819">
this paper, we introduce a more direct approach to
phrase generation, inspired by the work in com-
positional distributional semantics. In short, we
revert the composition process and we propose
a framework of data-induced, syntax-dependent
functions that decompose a single vector into a
vector sequence. The generated vectors can then
be efficiently matched against those in the lexicon
or fed to the decomposition system again to pro-
duce longer phrases recursively.
</bodyText>
<sectionHeader confidence="0.999622" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999990151515151">
To the best of our knowledge, we are the first to
explicitly and systematically pursue the generation
problem in distributional semantics. Kalchbrenner
and Blunsom (2013) use top-level, composed dis-
tributed representations of sentences to guide gen-
eration in a machine translation setting. More pre-
cisely, they condition the target language model
on the composed representation (addition of word
vectors) of the source language sentence.
Andreas and Ghahramani (2013) discuss the
the issue of generating language from vectors and
present a probabilistic generative model for distri-
butional vectors. However, their emphasis is on
reversing the generative story in order to derive
composed meaning representations from word se-
quences. The theoretical generating capabilities of
the methods they propose are briefly exemplified,
but not fully explored or tested.
Socher et al. (2011) come closest to our target
problem. They introduce a bidirectional language-
to-meaning model for compositional distributional
semantics that is similar in spirit to ours. How-
ever, we present a clearer decoupling of synthesis
and generation and we use different (and simpler)
training methods and objective functions. More-
over, Socher and colleagues do not train separate
decomposition rules for different syntactic config-
urations, so it is not clear how they would be able
to control the generation of different output struc-
tures. Finally, the potential for generation is only
addressed in passing, by presenting a few cases
where the generated sequence has the same syn-
tactic structure of the input sequence.
</bodyText>
<sectionHeader confidence="0.998948" genericHeader="method">
3 General framework
</sectionHeader>
<bodyText confidence="0.999949833333333">
We start by presenting the familiar synthesis set-
ting, focusing on two-word phrases. We then in-
troduce generation for the same structures. Fi-
nally, we show how synthesis and generation of
longer phrases is handled by recursive extension
of the two-word case. We assume a lexicon L,
that is, a bi-directional look-up table containing a
list of words Lw linked to a matrix Lv of vectors.
Both synthesis and generation involve a trivial lex-
icon look-up step to retrieve vectors associated to
words and vice versa: We ignore it in the exposi-
tion below.
</bodyText>
<subsectionHeader confidence="0.998939">
3.1 Synthesis
</subsectionHeader>
<bodyText confidence="0.980959">
To construct the vector representing a two-word
phrase, we must compose the vectors associated
to the input words. More formally, similarly to
Mitchell and Lapata (2008), we define a syntax-
dependent composition function yielding a phrase
vector P.
</bodyText>
<equation confidence="0.912066">
p� = fcompR(u,v)
</equation>
<bodyText confidence="0.999529692307693">
where i and v� are the vector representations asso-
ciated to words u and v. fcompR : Rd x Rd , Rd
(for d the dimensionality of vectors) is a compo-
sition function specific to the syntactic relation R
holding between the two words.1
Although we are not bound to a specific com-
position model, throughout this paper we use the
method proposed by Guevara (2010) and Zanzotto
et al. (2010) which defines composition as appli-
cation of linear transformations to the two con-
stituents followed by summing the resulting vec-
tors: fcompR(u,v) = W1ii + W2v. We will further
use the following equivalent formulation:
</bodyText>
<equation confidence="0.935838">
fcompR(�u,�v) = WR[�u;v1
</equation>
<bodyText confidence="0.999570933333333">
where WR E Rd×2d and [u;I is the vertical con-
catenation of the two vectors (using Matlab no-
tation). Following Guevara, we learn WR using
examples of word and phrase vectors directly ex-
tracted from the corpus (for the rest of the pa-
per, we refer to these phrase vectors extracted
non-compositionally from the corpus as observed
vectors). To estimate, for example, the weights
in the WAN (adjective-noun) matrix, we use the
corpus-extracted vectors of the words in tuples
such as (red, car, red.car), (evil, cat, evil.cat),
etc. Given a set of training examples stacked into
matrices U, V (the constituent vectors) and P (the
corresponding observed vectors), we estimate WR
by solving the least-squares regression problem:
</bodyText>
<footnote confidence="0.996058">
1Here we make the simplifying assumption that all vec-
tors have the same dimensionality, however this need not nec-
essarily be the case.
</footnote>
<page confidence="0.981414">
625
</page>
<equation confidence="0.9025625">
min kP − WR[U; V ]k (1)
WR∈Rdx2d
</equation>
<bodyText confidence="0.999968916666667">
We use the approximation of observed phrase
vectors as objective because these vectors can pro-
vide direct evidence of the polysemous behaviour
of words: For example, the corpus-observed vec-
tors of green jacket and green politician reflect
how the meaning of green is affected by its occur-
rence with different nouns. Moreover, it has been
shown that for two-word phrases, despite their
relatively low frequency, such corpus-observed
representations are still difficult to outperform in
phrase similarity tasks (Dinu et al., 2013; Turney,
2012).
</bodyText>
<subsectionHeader confidence="0.993624">
3.2 Generation
</subsectionHeader>
<bodyText confidence="0.9864865">
Generation of a two-word sequence from a vec-
tor proceeds in two steps: decomposition of the
phrase vectors into two constituent vectors, and
search for the nearest neighbours of each con-
stituent vector in Lv (the lexical matrix) in order
to retrieve the corresponding words from Lw.
Decomposition We define a syntax-dependent
decomposition function:
</bodyText>
<equation confidence="0.943758">
[ii;V1 = fdecompR(�p)
</equation>
<bodyText confidence="0.999879">
where p� is a phrase vector, u and v� are vectors as-
sociated to words standing in the syntactic relation
R and fdecompR : Rd → Rd × Rd.
We assume that decomposition is also a linear
transformation, W0R ∈ R2d×d, which, given an in-
put phrase vector, returns two constituent vectors:
</bodyText>
<equation confidence="0.86167">
fdecompR(PI = W0R�p
</equation>
<bodyText confidence="0.999597">
Again, we can learn from corpus-observed vectors
associated to tuples of word pairs and the corre-
sponding phrases by solving:
</bodyText>
<equation confidence="0.985419333333333">
0mi dxd k [U; V] − W0RPk (2)
W
R ∈R
</equation>
<bodyText confidence="0.993477">
If a composition function fcompR is available, an
alternative is to learn a function that can best revert
this composition. The decomposition function is
then trained as follows:
</bodyText>
<equation confidence="0.824955333333333">
0mi dxd [U; V] − W0RWR[U; V] k (3)
W
min
</equation>
<bodyText confidence="0.989404769230769">
where the matrix WR is a given composition
function for the same relation R. Training with
observed phrases, as in eq. (2), should be better
at capturing the idiosyncrasies of the actual dis-
tribution of phrases in the corpus and it is more
robust by being independent from the availability
and quality of composition functions. On the other
hand, if the goal is to revert as faithfully as possi-
ble the composition process and retrieve the orig-
inal constituents (e.g., in a different modality or a
different language), then the objective in eq. (3) is
more motivated.
Nearest neighbour search We retrieve the near-
est neighbours of each constituent vector u ob-
tained by decomposition by applying a search
function s:
NNil = s(u, Lv, t)
where NNil is a list containing the t nearest
neighours of u from Lv, the lexical vectors. De-
pending on the task, t might be set to 1 to retrieve
just one word sequence, or to larger values to re-
trieve t alternatives. The similarity measure used
to determine the nearest neighbours is another pa-
rameter of the search function; we omit it here as
we only experiment with the standard cosine mea-
sure (Turney and Pantel, 2010).2
</bodyText>
<subsectionHeader confidence="0.991569">
3.3 Recursive (de)composition
</subsectionHeader>
<bodyText confidence="0.997442954545455">
Extension to longer sequences is straightforward
if we assume binary tree representations as syn-
tactic structures. In synthesis, the top-level
vector can be obtained by applying composi-
tion functions recursively. For example, the
vector of big red car would be obtained as:
fcompAN( big, fcompAN( red, cdr)), where fcompAN
is the composition function for adjective-noun
phrase combinations. Conversely, for generation,
we decompose the phrase vector with fdecompAN.
The first vector is used for retrieving the nearest
adjective from the lexicon, while the second vec-
tor is further decomposed.
In the experiments in this paper we assume that
the syntactic structure is given. In Section 7, we
discuss ways to eliminate this assumption.
2Note that in terms of computational efficiency, cosine-
based nearest neighbour searches reduce to vector-matrix
multiplications, for which many efficient implementations
exist. Methods such as locality sensitive hashing can be used
for further speedups when working with particularly large vo-
cabularies (Andoni and Indyk, 2008).
</bodyText>
<page confidence="0.998274">
626
</page>
<sectionHeader confidence="0.98395" genericHeader="method">
4 Evaluation setting
</sectionHeader>
<bodyText confidence="0.999990697674419">
In our empirical part, we focus on noun phrase
generation. A noun phrase can be a single noun or
a noun with one or more modifiers, where a mod-
ifier can be an adjective or a prepositional phrase.
A prepositional phrase is in turn composed of a
preposition and a noun phrase. We learn two com-
position (and corresponding decomposition) func-
tions: one for modifier-noun phrases, trained on
adjective-noun (AN) pairs, and a second one for
prepositional phrases, trained on preposition-noun
(PN) combinations. For the rest of this section we
describe the construction of the vector spaces and
the (de)composition function learning procedure.
Construction of vector spaces We test two
types of vector representations. The cbow model
introduced in Mikolov et al. (2013a) learns vec-
tor representations using a neural network archi-
tecture by trying to predict a target word given the
words surrounding it. We use the word2vec soft-
ware3 to build vectors of size 300 and using a con-
text window of 5 words to either side of the target.
We set the sub-sampling option to 1e-05 and esti-
mate the probability of a target word with the neg-
ative sampling method, drawing 10 samples from
the noise distribution (see Mikolov et al. (2013a)
for details). We also implement a standard count-
based bag-of-words distributional space (Turney
and Pantel, 2010) which counts occurrences of a
target word with other words within a symmetric
window of size 5. We build a 300Kx300K sym-
metric co-occurrence matrix using the top most
frequent words in our source corpus, apply posi-
tive PMI weighting and Singular Value Decompo-
sition to reduce the space to 300 dimensions. For
both spaces, the vectors are finally normalized to
unit length.4
For both types of vectors we use 2.8 billion to-
kens as input (ukWaC + Wikipedia + BNC). The
Italian language vectors for the cross-lingual ex-
periments of Section 6 were trained on 1.6 bil-
lion tokens from itWaC.5 A word token is a word-
form + POS-tag string. We extract both word vec-
tors and the observed phrase vectors which are
</bodyText>
<footnote confidence="0.998292428571429">
3Available at https://code.google.com/p/
word2vec/
4The parameters of both models have been chosen without
specific tuning, based on their observed stable performance in
previous independent experiments.
5Corpus sources: http://wacky.sslmit.unibo.
it,http://www.natcorp.ox.ac.uk
</footnote>
<bodyText confidence="0.999959638297872">
required for the training procedures. We sanity-
check the two spaces on MEN (Bruni et al., 2012),
a 3,000 items word similarity data set. cbow sig-
nificantly outperforms count (0.80 vs. 0.72 Spear-
man correlations with human judgments). count
performance is consistent with previously reported
results.6
(De)composition function training The train-
ing data sets consist of the 50K most frequent
(u, v, p) tuples for each phrase type, for example,
(red, car, red.car) or (in, car, in.car).7 We con-
catenate i and v� vectors to obtain the [U; V ] ma-
trix and we use the observed p� vectors (e.g., the
corpus vector of the red.car bigram) to obtain the
phrase matrix P. We use these data sets to solve
the least squares regression problems in eqs. (1)
and (2), obtaining estimates of the composition
and decomposition matrices, respectively. For the
decomposition function in eq. (3), we replace the
observed phrase vectors with those composed with
fcompR(u,v), where fcompR is the previously esti-
mated composition function for relation R.
Composition function performance Since the
experiments below also use composed vectors as
input to the generation process, it is important to
provide independent evidence that the composi-
tion model is of high quality. This is indeed the
case: We tested our composition approach on the
task of retrieving observed AN and PN vectors,
based on their composed vectors (similarly to Ba-
roni and Zamparelli (2010), we want to retrieve the
observed red.car vector using fcompAN(red, car)).
We obtain excellent results, with minimum accu-
racy of 0.23 (chance level &lt;0.0001). We also test
on the AN-N paraphrasing test set used in Dinu
et al. (2013) (in turn adapting Turney (2012)).
The dataset contains 620 ANs, each paired with
a single-noun paraphrase (e.g., false belief/fallacy,
personal appeal/charisma). The task is to rank
all nouns in the lexicon by their similarity to the
phrase, and return the rank of the correct para-
phrase. Results are reported in the first row of Ta-
ble 1. To facilitate comparison, we search, like
Dinu et al., through a vocabulary containing the
20K most frequent nouns. The count vectors re-
sults are similar to those reported by Dinu and col-
leagues for the same model, and with cbow vec-
</bodyText>
<footnote confidence="0.9976655">
6See Baroni et al. (2014) for an extensive comparison of
the two types of vector representations.
7For PNs, we ignore determiners and we collapse, for ex-
ample, in.the.car and in.car occurrences.
</footnote>
<page confidence="0.971752">
627
</page>
<table confidence="0.999781">
Input Output cbow count
AoN N 11 171
N A, N 67,29 204,168
</table>
<tableCaption confidence="0.997401">
Table 1: Median rank on the AN-N set of Dinu et
</tableCaption>
<bodyText confidence="0.838444375">
al. (2013) (e.g., personal appeal/charisma). First
row: the A and N are composed and the closest
N is returned as a paraphrase. Second row: the
N vector is decomposed into A and N vectors and
their nearest (POS-tag consistent) neighbours are
returned.
tors we obtain a median rank that is considerably
higher than that of the methods they test.
</bodyText>
<sectionHeader confidence="0.976605" genericHeader="method">
5 Noun phrase generation
</sectionHeader>
<subsectionHeader confidence="0.670904">
5.1 One-step decomposition
</subsectionHeader>
<bodyText confidence="0.999584029411765">
We start with testing one-step decomposition by
generating two-word phrases. A first straightfor-
ward evaluation consists in decomposing a phrase
vector into the correct constituent words. For this
purpose, we randomly select (and consequently re-
move) from the training sets 200 phrases of each
type (AN and PN) and apply decomposition op-
erations to 1) their corpus-observed vectors and
2) their composed representations. We generate
two words by returning the nearest neighbours
(with appropriate POS tags) of the two vectors
produced by the decomposition functions. Ta-
ble 2 reports generation accuracy, i.e., the pro-
portion of times in which we retrieved the cor-
rect constituents. The search space consists of
the top most frequent 20K nouns, 20K adjec-
tives and 25 prepositions respectively, leading to
chance accuracy &lt;0.0001 for nouns and adjectives
and &lt;0.05 for prepositions. We obtain relatively
high accuracy, with cbow vectors consistently out-
performing count ones. Decomposing composed
rather than observed phrase representations is eas-
ier, which is to be expected given that composed
representations are obtained with a simpler, lin-
ear model. Most of the errors consist in generat-
ing synonyms (hard case—*difficult case, true cost
—* actual cost) or related phrases (stereo speak-
ers—*omni-directional sound).
Next, we use the AN-N dataset of Dinu and
colleagues for a more interesting evaluation of
one-step decomposition. In particular, we reverse
the original paraphrasing direction by attempting
to generate, for example, personal charm from
charisma. It is worth stressing the nature of the
</bodyText>
<table confidence="0.999779">
Input Output cbow count
A.N A, N 0.36,0.61 0.20,0.41
P.N P, N 0.93,0.79 0.60,0.57
AoN A, N 1.00,1.00 0.86,0.99
PoN P, N 1.00,1.00 1.00,1.00
</table>
<tableCaption confidence="0.994283">
Table 2: Accuracy of generation models at re-
</tableCaption>
<bodyText confidence="0.979184906976744">
trieving (at rank 1) the constituent words of
adjective-noun (AN) and preposition-noun (PN)
phrases. Observed (A.N) and composed repre-
sentations (AoN) are decomposed with observed-
(eq. 2) and composed-trained (eq. 3) functions re-
spectively.
paraphrase-by-generation task we tackle here and
in the next experiments. Compositional distri-
butional semantic systems are often evaluated on
phrase and sentence paraphrasing data sets (Bla-
coe and Lapata, 2012; Mitchell and Lapata, 2010;
Socher et al., 2011; Turney, 2012). However,
these experiments assume a pre-compiled list of
candidate paraphrases, and the task is to rank
correct paraphrases above foils (paraphrase rank-
ing) or to decide, for a given pair, if the two
phrases/sentences are mutual paraphrases (para-
phrase detection). Here, instead, we do not as-
sume a given set of candidates: For example, in
N—*AN paraphrasing, any of 20K2 possible com-
binations of adjectives and nouns from the lexicon
could be generated. This is a much more challeng-
ing task and it paves the way to more realistic ap-
plications of distributional semantics in generation
scenarios.
The median ranks of the gold A and N of the
Dinu set are shown in the second row of Table
1. As the top-generated noun is almost always,
uninterestingly, the input one, we return the next
noun. Here we report results for the more moti-
vated corpus-observed training of eq. (2) (unsur-
prisingly, using composed-phrase training for the
task of decomposing single nouns leads to lower
performance).
Although considerably more difficult than the
previous task, the results are still very good, with
median ranks under 100 for the cbow vectors (ran-
dom median rank at 10K). Also, the dataset pro-
vides only one AN paraphrase for each noun, out
of many acceptable ones. Examples of generated
phrases are given in Table 3. In addition to gen-
erating topically related ANs, we also see nouns
disambiguated in different ways than intended in
</bodyText>
<page confidence="0.997034">
628
</page>
<table confidence="0.993233222222222">
Input Output Gold
reasoning deductive thinking abstract thought
jurisdiction legal authority legal power
thunderstorm thundery storm electrical storm
folk local music common people
superstition old-fashioned religion superstitious notion
vitriol political bitterness sulfuric acid
zoom fantastic camera rapid growth
religion religious religion religious belief
</table>
<tableCaption confidence="0.7613895">
Table 3: Examples of generating ANs from Ns us-
ing the data set of Dinu et al. (2013).
the gold standard (for example vitriol and folk in
Table 3). Other interesting errors consist of de-
</tableCaption>
<bodyText confidence="0.994438272727273">
composing a noun into two words which both have
the same meaning as the noun, generating for ex-
ample religion —* religious religions. We observe
moreover that sometimes the decomposition re-
flects selectional preference effects, by generat-
ing adjectives that denote typical properties of the
noun to be paraphrased (e.g., animosity is a (po-
litical, personal,...) hostility or a fridge is a (big,
large, small,...) refrigerator). This effect could be
exploited for tasks such as property-based concept
description (Kelly et al., 2012).
</bodyText>
<subsectionHeader confidence="0.996494">
5.2 Recursive decomposition
</subsectionHeader>
<bodyText confidence="0.99869747826087">
We continue by testing generation through recur-
sive decomposition on the task of generating noun-
preposition-noun (NPN) paraphrases of adjective-
nouns (AN) phrases. We introduce a dataset con-
taining 192 AN-NPN pairs (such as pre-election
promises—* promises before election), which was
created by the second author and additionally cor-
rected by an English native speaker. The data set
was created by analyzing a list of randomly se-
lected frequent ANs. 49 further ANs (with adjec-
tives such as amazing and great) were judged not
NPN-paraphrasable and were used for the experi-
ment reported in Section 7. The paraphrased sub-
set focuses on preposition diversity and on includ-
ing prepositions which are rich in semantic content
and relevant to paraphrasing the AN. This has led
to excluding of, which in most cases has the purely
syntactic function of connecting the two nouns.
The data set contains the following 14 preposi-
tions: after, against, at, before, between, by, for,
from, in, on, per, under, with, without.8
NPN phrase generation involves the applica-
tion of two decomposition functions. In the first
</bodyText>
<footnote confidence="0.913547">
8This dataset is available at http://clic.cimec.
unitn.it/composes
</footnote>
<bodyText confidence="0.999964125">
/step we decompose using the modifier-noun rule
(fdecomPAN). We generate a noun from the head
slot vector and the “adjective” vector is further de-
composed using fdecomPPN (returning the top noun
which is not identical to the previously generated
one). The results, in terms of top 1 accuracy and
median rank, are shown in Table 4. Examples are
given in Table 5.
For observed phrase vector training, accuracy
and rank are well above chance for all constituents
(random accuracy 0.00005 for nouns and 0.04 for
prepositions, corresponding median ranks: 10K,
12). Preposition generation is clearly a more diffi-
cult task. This is due at least in part to their highly
ambiguous and broad semantics, and the way in
which they interact with the nouns. For exam-
ple, cable through ocean in Table 5 is a reason-
able paraphrase of undersea cable despite the gold
preposition being under. Other than several cases
which are acceptable paraphrases but not in the
gold standard, phrases related in meaning but not
synonymous are the most common error (overcast
skies —* skies in sunshine). We also observe that
often the A and N meanings are not fully separated
when decomposing and “traces” of the adjective
or of the original noun meaning can be found in
both generated nouns (for example nearby school
—* schools after school). To a lesser degree, this
might be desirable as a disambiguation-in-context
effect as, for example, in underground cavern, in
secret would not be a context-appropriate para-
phrase of underground.
</bodyText>
<sectionHeader confidence="0.953035" genericHeader="method">
6 Noun phrase translation
</sectionHeader>
<bodyText confidence="0.999074882352941">
This section describes preliminary experiments
performed in a cross-lingual setting on the task
of composing English AN phrases and generating
Italian translations.
Creation of cross-lingual vector spaces A
common semantic space is required in order to
map words and phrases across languages. This
problem has been extensively addressed in the
bilingual lexicon acquisition literature (Haghighi
et al., 2008; Koehn and Knight, 2002). We opt for
a very simple yet accurate method (Klementiev et
al., 2012; Rapp, 1999) in which a bilingual dictio-
nary is used to identify a set of shared dimensions
across spaces and the vectors of both languages are
projected into the subspace defined by these (Sub-
space Projection - SP). This method is applicable
to count-type vector spaces, for which the dimen-
</bodyText>
<page confidence="0.998063">
629
</page>
<table confidence="0.986654">
Input Output Training cbow count
AoN N, P, N observed 0.98(1),0.08(5.5),0.13(20.5) 0.82(1),0.17(4.5),0.05(71.5)
AoN N, P, N composed 0.99(1),0.02(12), 0.12(24) 0.99(1),0.06(10), 0.05(150.5)
</table>
<tableCaption confidence="0.8776325">
Table 4: Top 1 accuracy (median rank) on the AN—*NPN paraphrasing data set. AN phrases are com-
posed and then recursively decomposed into N, (P, N). Comma-delimited scores reported for first noun,
preposition, second noun in this order. Training is performed on observed (eq. 2) and composed (eq. 3)
phrase representations.
Input Output Gold
Table 5: Examples of generating NPN phrases from composed ANs.
</tableCaption>
<figure confidence="0.97140152">
mountainous region
undersea cable
underground cavern
interdisciplinary field
inter-war years
post-operative pain
pre-war days
intergroup differences
superficial level
region in highlands
cable through ocean
cavern through rock
field into research
years during 1930s
pain through patient
days after wartime
differences between intergroup
level between levels
region with mountains
cable under sea
cavern under ground
field between disciplines
years between wars
pain after operation
days before war
</figure>
<figureCaption confidence="0.623244">
differences between minorities
</figureCaption>
<bodyText confidence="0.994746322580645">
level on surface
sions correspond to actual words. As the cbow di-
mensions do not correspond to words, we align the
cbow spaces by using a small dictionary to learn
a linear map which transforms the English vectors
into Italian ones as done in Mikolov et al. (2013b).
This method (Translation Matrix - TM) is applica-
ble to both cbow and count spaces. We tune the pa-
rameters (TM or SP for count and dictionary size
5K or 25K for both spaces) on a standard task of
translating English words into Italian. We obtain
TM-5K for cbow and SP-25K for count as opti-
mal settings. The two methods perform similarly
for low frequency words while cbow-TM-5K sig-
nificantly outperforms count-SP-25K for high fre-
quency words. Our results for the cbow-TM-5K
setting are similar to those reported by Mikolov et
al. (2013b).
Cross-lingual decomposition training Train-
ing proceeds as in the monolingual case, this time
concatenating the training data sets and estimating
a single (de)-composition function for the two lan-
guages in the shared semantic space. We train both
on observed phrase representations (eq. 2) and on
composed phrase representations (eq. 3).
Adjective-noun translation dataset We ran-
domly extract 1,000 AN-AN En-It phrase pairs
from a phrase table built from parallel movie sub-
titles, available at http://opus.lingfil.
uu.se/ (OpenSubtitles2012, en-it) (Tiedemann,
2012).
</bodyText>
<table confidence="0.998187666666667">
Input Output cbow count
AoN(En) A,N (It) 0.31,0.59 0.24,0.54
AoN (It) A,N(En) 0.50,0.62 0.28,0.48
</table>
<tableCaption confidence="0.827711">
Table 6: Accuracy of En—*It and It—*En phrase
translation: phrases are composed in source lan-
</tableCaption>
<bodyText confidence="0.965787086956522">
guage and decomposed in target language. Train-
ing on composed phrase representations (eq. (3))
(with observed phrase training (eq. 2) results are
Pz50% lower).
Results are presented in Table 6. While in
these preliminary experiments we lack a proper
term of comparison, the performance is very good
both quantitatively (random &lt; 0.0001) and quali-
tatively. The En—*It examples in Table 7 are repre-
sentative. In many cases (e.g., vicious killer, rough
neighborhood) we generate translations that are
arguably more natural than those in the gold stan-
dard. Again, some differences can be explained
by different disambiguations (chest as breast, as
in the generated translation, or box, as in the gold).
Translation into related but not equivalent phrases
and generating the same meaning in both con-
stituents (stellar star) are again the most signifi-
cant errors. We also see cases in which this has the
desired effect of disambiguating the constituents,
such as in the examples in Table 8, showing the
nearest neighbours when translating black tie and
indissoluble tie.
</bodyText>
<page confidence="0.995768">
630
</page>
<tableCaption confidence="0.667448">
Input Output Gold
Table 7: En→It translation examples (back-translations of generated phrases in parenthesis).
</tableCaption>
<figure confidence="0.803430384615385">
vicious killer
spectacular woman
huge chest
rough neighborhood
mortal sin
canine star
assassino feroce (ferocious killer)
donna affascinante (fascinating woman)
petto grande (big chest)
zona malfamata (ill-repute zone)
peccato eterno (eternal sin)
stella stellare (stellar star)
killer pericoloso
donna eccezionale
scrigno immenso
quartiere difficile
pecato mortale
star canina
black tie
cravatta (tie) nero (black)
velluto (velvet) bianco (white)
giacca (jacket) giallo (yellow)
indissoluble tie
alleanza (alliance) indissolubile (indissoluble)
legame (bond) sacramentale (sacramental)
amicizia (friendship) inscindibile (inseparable)
</figure>
<tableCaption confidence="0.99502">
Table 8: Top 3 translations of black tie and indis-
soluble tie, showing correct disambiguation of tie.
</tableCaption>
<sectionHeader confidence="0.8454165" genericHeader="method">
7 Generation confidence and generation
quality
</sectionHeader>
<bodyText confidence="0.999788766666667">
In Section 3.2 we have defined a search function
s returning a list of lexical nearest neighbours for
a constituent vector produced by decomposition.
Together with the neighbours, this function can
naturally return their similarity score (in our case,
the cosine). We call the score associated to the
top neighbour the generation confidence: if this
score is low, the vector has no good match in the
lexicon. We observe significant Spearman cor-
relations between the generation confidence of a
constituent and its quality (e.g., accuracy, inverse
rank) in all the experiments. For example, for the
AN(En)→AN(It) experiment, the correlations be-
tween the confidence scores and the inverse ranks
for As and Ns, for both cbow and count vectors,
range between 0.34 (p &lt; 1e−28) and 0.42. In
the translation experiments, we can use this to au-
tomatically determine a subset on which we can
translate with very high accuracy. Table 9 shows
AN-AN accuracies and coverage when translating
only if confidence is above a certain threshold.
Throughout this paper we have assumed that the
syntactic structure of the phrase to be generated is
given. In future work we will exploit the corre-
lation between confidence and quality for the pur-
pose of eliminating this assumption. As a concrete
example, we can use confidence scores to distin-
guish the two subsets of the AN-NPN dataset in-
troduced in Section 5: the ANs which are para-
phrasable with an NPN from those that do not
</bodyText>
<table confidence="0.9987095">
En→It It→En
Thr. Accuracy Cov. Accuracy Cov.
0.00 0.21 100% 0.32 100%
0.55 0.25 70% 0.40 63%
0.60 0.31 32% 0.45 37%
0.65 0.45 9% 0.52 16%
</table>
<tableCaption confidence="0.826548">
Table 9: AN-AN translation accuracy (both A and
N correct) when imposing a confidence threshold
(random: 1/20K2).
</tableCaption>
<figureCaption confidence="0.991908">
Figure 1: ROC of distinguishing ANs para-
</figureCaption>
<bodyText confidence="0.950782">
phrasable as NPNs from non-paraphrasable ones.
have this property. We assign an AN to the NPN-
paraphrasable class if the mean confidence of the
PN expansion in its attempted N(PN) decomposi-
tion is above a certain threshold. We plot the ROC
curve in Figure 1. We obtain a significant AUC of
0.71.
</bodyText>
<sectionHeader confidence="0.997682" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9986403">
In this paper we have outlined a framework for
the task of generation with distributional semantic
models. We proposed a simple but effective ap-
proach to reverting the composition process to ob-
tain meaningful reformulations of phrases through
a synthesis-generation process.
For future work we would like to experiment
with more complex models for (de-)composition
in order to improve the performance on the tasks
we used in this paper. Following this, we
</bodyText>
<page confidence="0.996134">
631
</page>
<bodyText confidence="0.999992526315789">
would like to extend the framework to handle
arbitrary phrases, including making (confidence-
based) choices on the syntactic structure of the
phrase to be generated, which we have assumed
to be given throughout this paper.
In terms of applications, we believe that the line
of research in machine translation that is currently
focusing on replacing parallel resources with large
amounts of monolingual text provides an inter-
esting setup to test our methods. For example,
Klementiev et al. (2012) reconstruct phrase ta-
bles based on phrase similarity scores in seman-
tic space. However, they resort to scoring phrase
pairs extracted from an aligned parallel corpus, as
they do not have a method to freely generate these.
Similarly, in the recent work on common vector
spaces for the representation of images and text,
the current emphasis is on retrieving existing cap-
tions (Socher et al., 2014) and not actual genera-
tion of image descriptions.
From a more theoretical point of view, our work
fills an important gap in distributional semantics,
making it a bidirectional theory of the connec-
tion between language and meaning. We can now
translate linguistic strings into vector “thoughts”,
and the latter into their most appropriate linguis-
tic expression. Several neuroscientific studies sug-
gest that thoughts are represented in the brain by
patterns of activation over broad neural areas, and
vectors are a natural way to encode such patterns
(Haxby et al., 2001; Huth et al., 2012). Some
research has already established a connection be-
tween neural and distributional semantic vector
spaces (Mitchell et al., 2008; Murphy et al., 2012).
Generation might be the missing link to power-
ful computational models that take the neural foot-
print of a thought as input and produce its linguis-
tic expression.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999870714285714">
We thank Kevin Knight, Andrew Anderson,
Roberto Zamparelli, Angeliki Lazaridou, Nghia
The Pham, Germ´an Kruszewski and Peter Tur-
ney for helpful discussions and the anonymous re-
viewers for their useful comments. We acknowl-
edge the ERC 2011 Starting Independent Research
Grant n. 283554 (COMPOSES).
</bodyText>
<sectionHeader confidence="0.997082" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997863578947369">
Alexandr Andoni and Piotr Indyk. 2008. Near-optimal
hashing algorithms for approximate nearest neigh-
bor in high dimensions. Commun. ACM, 51(1):117–
122, January.
Jacob Andreas and Zoubin Ghahramani. 2013. A
generative model of vector space semantics. In
Proceedings of the Workshop on Continuous Vector
Space Models and their Compositionality, pages 91–
99, Sofia, Bulgaria.
Marco Baroni and Roberto Zamparelli. 2010. Nouns
are vectors, adjectives are matrices: Representing
adjective-noun constructions in semantic space. In
Proceedings of EMNLP, pages 1183–1193, Boston,
MA.
Marco Baroni, Georgiana Dinu, and Germ´an
Kruszewski. 2014. Don’t count, predict! A
systematic comparison of context-counting vs.
context-predicting semantic vectors. In Proceedings
ofACL, To appear, Baltimore, MD.
Marco Baroni. 2013. Composition in distributional
semantics. Language and Linguistics Compass,
7(10):511–522.
William Blacoe and Mirella Lapata. 2012. A com-
parison of vector-based representations for seman-
tic composition. In Proceedings of EMNLP, pages
546–556, Jeju Island, Korea.
Elia Bruni, Gemma Boleda, Marco Baroni, and
Nam Khanh Tran. 2012. Distributional semantics
in Technicolor. In Proceedings of ACL, pages 136–
145, Jeju Island, Korea.
Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013. General estimation and evaluation of com-
positional distributional semantic models. In Pro-
ceedings of ACL Workshop on Continuous Vector
Space Models and their Compositionality, pages 50–
58, Sofia, Bulgaria.
Katrin Erk. 2012. Vector space models of word mean-
ing and phrase meaning: A survey. Language and
Linguistics Compass, 6(10):635–653.
Andrea Frome, Greg Corrado, Jon Shlens, Samy Ben-
gio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas
Mikolov. 2013. DeViSE: A deep visual-semantic
embedding model. In Proceedings of NIPS, pages
2121–2129, Lake Tahoe, Nevada.
Emiliano Guevara. 2010. A regression model of
adjective-noun compositionality in distributional se-
mantics. In Proceedings of GEMS, pages 33–37,
Uppsala, Sweden.
Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,
and Dan Klein. 2008. Learning bilingual lexicons
from monolingual corpora. In Proceedings of ACL,
pages 771–779, Columbus, OH, USA, June.
James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai,
Jennifer Schouten, and Pietro Pietrini. 2001. Dis-
tributed and overlapping representations of faces
and objects in ventral temporal cortex. Science,
293:2425–2430.
</reference>
<page confidence="0.974167">
632
</page>
<reference confidence="0.999916482352941">
Alexander Huth, Shinji Nishimoto, An Vu, and Jack
Gallant. 2012. A continuous semantic space de-
scribes the representation of thousands of object and
action categories across the human brain. Neuron,
76(6):1210–1224.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Natu-
ral Language Processing, Seattle, October. Associa-
tion for Computational Linguistics.
Colin Kelly, Barry Devereux, and Anna Korhonen.
2012. Semi-supervised learning for automatic con-
ceptual property extraction. In Proceedings of the
3rd Workshop on Cognitive Modeling and Computa-
tional Linguistics, pages 11–20, Montreal, Canada.
Alexandre Klementiev, Ann Irvine, Chris Callison-
Burch, and David Yarowsky. 2012. Toward sta-
tistical machine translation without parallel corpora.
In Proceedings of EACL, pages 130–140, Avignon,
France.
Philipp Koehn and Kevin Knight. 2002. Learning a
translation lexicon from monolingual corpora. In
In Proceedings of ACL Workshop on Unsupervised
Lexical Acquisition, pages 9–16, Philadelphia, PA,
USA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013a. Efficient estimation of word represen-
tations in vector space. http://arxiv.org/
abs/1301.3781/.
Tomas Mikolov, Quoc Le, and Ilya Sutskever. 2013b.
Exploiting similarities among languages for Ma-
chine Translation. http://arxiv.org/abs/
1309.4168.
Jeff Mitchell and Mirella Lapata. 2008. Vector-based
models of semantic composition. In Proceedings of
ACL, pages 236–244, Columbus, OH.
Jeff Mitchell and Mirella Lapata. 2010. Composition
in distributional models of semantics. Cognitive Sci-
ence, 34(8):1388–1429.
Tom Mitchell, Svetlana Shinkareva, Andrew Carlson,
Kai-Min Chang, Vincente Malave, Robert Mason,
and Marcel Just. 2008. Predicting human brain ac-
tivity associated with the meanings of nouns. Sci-
ence, 320:1191–1195.
Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012. Selecting corpus-semantic models for neu-
rolinguistic decoding. In Proceedings of *SEM,
pages 114–123, Montreal, Canada.
Reinhard Rapp. 1999. Automatic identification of
word translations from unrelated english and german
corpora. In Proceedings of the 37th annual meet-
ing of the Association for Computational Linguistics
on Computational Linguistics, ACL ’99, pages 519–
526. Association for Computational Linguistics.
Richard Socher, Eric Huang, Jeffrey Pennin, Andrew
Ng, and Christopher Manning. 2011. Dynamic
pooling and unfolding recursive autoencoders for
paraphrase detection. In Proceedings of NIPS, pages
801–809, Granada, Spain.
Richard Socher, Milind Ganjoo, Christopher Manning,
and Andrew Ng. 2013. Zero-shot learning through
cross-modal transfer. In Proceedings of NIPS, pages
935–943, Lake Tahoe, Nevada.
Richard Socher, Quoc Le, Christopher Manning, and
Andrew Ng. 2014. Grounded compositional se-
mantics for finding and describing images with sen-
tences. Transactions of the Association for Compu-
tational Linguistics. In press.
J¨org Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In Proceedings of the Eight Interna-
tional Conference on Language Resources and Eval-
uation (LREC’12), Istanbul, Turkey.
Peter Turney and Patrick Pantel. 2010. From fre-
quency to meaning: Vector space models of se-
mantics. Journal of Artificial Intelligence Research,
37:141–188.
Peter Turney. 2012. Domain and function: A dual-
space model of semantic relations and compositions.
Journal of Artificial Intelligence Research, 44:533–
585.
Fabio Zanzotto, Ioannis Korkontzelos, Francesca
Falucchi, and Suresh Manandhar. 2010. Estimat-
ing linear models for compositional distributional
semantics. In Proceedings of COLING, pages 1263–
1271, Beijing, China.
</reference>
<page confidence="0.999146">
633
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.756134">
<title confidence="0.9839605">How to make words with Phrase generation in distributional semantics</title>
<author confidence="0.79237">Dinu</author>
<affiliation confidence="0.9971325">Center for Mind/Brain University of Trento,</affiliation>
<email confidence="0.998414">(georgiana.dinu|marco.baroni)@unitn.it</email>
<abstract confidence="0.997507">introduce the problem of in distributional semantics: Given a distributional vector representing some meaning, how can we generate the phrase that best expresses that meaning? We motivate this novel challenge on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions. We test this in a monolingual scenario (paraphrase generation) as well as in a cross-lingual setting (translation by synthesizing adjectivenoun phrase vectors in English and generating the equivalent expressions in Italian).</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Alexandr Andoni</author>
<author>Piotr Indyk</author>
</authors>
<title>Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions.</title>
<date>2008</date>
<journal>Commun. ACM,</journal>
<volume>51</volume>
<issue>1</issue>
<pages>122</pages>
<contexts>
<context position="12397" citStr="Andoni and Indyk, 2008" startWordPosition="1963" endWordPosition="1966">ase vector with fdecompAN. The first vector is used for retrieving the nearest adjective from the lexicon, while the second vector is further decomposed. In the experiments in this paper we assume that the syntactic structure is given. In Section 7, we discuss ways to eliminate this assumption. 2Note that in terms of computational efficiency, cosinebased nearest neighbour searches reduce to vector-matrix multiplications, for which many efficient implementations exist. Methods such as locality sensitive hashing can be used for further speedups when working with particularly large vocabularies (Andoni and Indyk, 2008). 626 4 Evaluation setting In our empirical part, we focus on noun phrase generation. A noun phrase can be a single noun or a noun with one or more modifiers, where a modifier can be an adjective or a prepositional phrase. A prepositional phrase is in turn composed of a preposition and a noun phrase. We learn two composition (and corresponding decomposition) functions: one for modifier-noun phrases, trained on adjective-noun (AN) pairs, and a second one for prepositional phrases, trained on preposition-noun (PN) combinations. For the rest of this section we describe the construction of the vec</context>
</contexts>
<marker>Andoni, Indyk, 2008</marker>
<rawString>Alexandr Andoni and Piotr Indyk. 2008. Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions. Commun. ACM, 51(1):117– 122, January.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Andreas</author>
<author>Zoubin Ghahramani</author>
</authors>
<title>A generative model of vector space semantics.</title>
<date>2013</date>
<booktitle>In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>91--99</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="5041" citStr="Andreas and Ghahramani (2013)" startWordPosition="764" endWordPosition="767">ctors can then be efficiently matched against those in the lexicon or fed to the decomposition system again to produce longer phrases recursively. 2 Related work To the best of our knowledge, we are the first to explicitly and systematically pursue the generation problem in distributional semantics. Kalchbrenner and Blunsom (2013) use top-level, composed distributed representations of sentences to guide generation in a machine translation setting. More precisely, they condition the target language model on the composed representation (addition of word vectors) of the source language sentence. Andreas and Ghahramani (2013) discuss the the issue of generating language from vectors and present a probabilistic generative model for distributional vectors. However, their emphasis is on reversing the generative story in order to derive composed meaning representations from word sequences. The theoretical generating capabilities of the methods they propose are briefly exemplified, but not fully explored or tested. Socher et al. (2011) come closest to our target problem. They introduce a bidirectional languageto-meaning model for compositional distributional semantics that is similar in spirit to ours. However, we pres</context>
</contexts>
<marker>Andreas, Ghahramani, 2013</marker>
<rawString>Jacob Andreas and Zoubin Ghahramani. 2013. A generative model of vector space semantics. In Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 91– 99, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Roberto Zamparelli</author>
</authors>
<title>Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.</title>
<date>2010</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>1183--1193</pages>
<location>Boston, MA.</location>
<contexts>
<context position="16176" citStr="Baroni and Zamparelli (2010)" startWordPosition="2576" endWordPosition="2580">matrices, respectively. For the decomposition function in eq. (3), we replace the observed phrase vectors with those composed with fcompR(u,v), where fcompR is the previously estimated composition function for relation R. Composition function performance Since the experiments below also use composed vectors as input to the generation process, it is important to provide independent evidence that the composition model is of high quality. This is indeed the case: We tested our composition approach on the task of retrieving observed AN and PN vectors, based on their composed vectors (similarly to Baroni and Zamparelli (2010), we want to retrieve the observed red.car vector using fcompAN(red, car)). We obtain excellent results, with minimum accuracy of 0.23 (chance level &lt;0.0001). We also test on the AN-N paraphrasing test set used in Dinu et al. (2013) (in turn adapting Turney (2012)). The dataset contains 620 ANs, each paired with a single-noun paraphrase (e.g., false belief/fallacy, personal appeal/charisma). The task is to rank all nouns in the lexicon by their similarity to the phrase, and return the rank of the correct paraphrase. Results are reported in the first row of Table 1. To facilitate comparison, we</context>
</contexts>
<marker>Baroni, Zamparelli, 2010</marker>
<rawString>Marco Baroni and Roberto Zamparelli. 2010. Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space. In Proceedings of EMNLP, pages 1183–1193, Boston, MA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
<author>Georgiana Dinu</author>
<author>Germ´an Kruszewski</author>
</authors>
<title>Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors.</title>
<date>2014</date>
<booktitle>In Proceedings ofACL, To appear,</booktitle>
<location>Baltimore, MD.</location>
<contexts>
<context position="17005" citStr="Baroni et al. (2014)" startWordPosition="2719" endWordPosition="2722">d in Dinu et al. (2013) (in turn adapting Turney (2012)). The dataset contains 620 ANs, each paired with a single-noun paraphrase (e.g., false belief/fallacy, personal appeal/charisma). The task is to rank all nouns in the lexicon by their similarity to the phrase, and return the rank of the correct paraphrase. Results are reported in the first row of Table 1. To facilitate comparison, we search, like Dinu et al., through a vocabulary containing the 20K most frequent nouns. The count vectors results are similar to those reported by Dinu and colleagues for the same model, and with cbow vec6See Baroni et al. (2014) for an extensive comparison of the two types of vector representations. 7For PNs, we ignore determiners and we collapse, for example, in.the.car and in.car occurrences. 627 Input Output cbow count AoN N 11 171 N A, N 67,29 204,168 Table 1: Median rank on the AN-N set of Dinu et al. (2013) (e.g., personal appeal/charisma). First row: the A and N are composed and the closest N is returned as a paraphrase. Second row: the N vector is decomposed into A and N vectors and their nearest (POS-tag consistent) neighbours are returned. tors we obtain a median rank that is considerably higher than that o</context>
</contexts>
<marker>Baroni, Dinu, Kruszewski, 2014</marker>
<rawString>Marco Baroni, Georgiana Dinu, and Germ´an Kruszewski. 2014. Don’t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors. In Proceedings ofACL, To appear, Baltimore, MD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marco Baroni</author>
</authors>
<title>Composition in distributional semantics.</title>
<date>2013</date>
<journal>Language and Linguistics Compass,</journal>
<volume>7</volume>
<issue>10</issue>
<contexts>
<context position="1232" citStr="Baroni (2013)" startWordPosition="179" endWordPosition="180">tion) as well as in a cross-lingual setting (translation by synthesizing adjectivenoun phrase vectors in English and generating the equivalent expressions in Italian). 1 Introduction Distributional methods for semantics approximate the meaning of linguistic expressions with vectors that summarize the contexts in which they occur in large samples of text. This has been a very successful approach to lexical semantics (Erk, 2012), where semantic relatedness is assessed by comparing vectors. Recently these methods have been extended to phrases and sentences by means of composition operations (see Baroni (2013) for an overview). For example, given the vectors representing red and car, composition derives a vector that approximates the meaning of red car. However, the link between language and meaning is, obviously, bidirectional: As message recipients we are exposed to a linguistic expression and we must compute its meaning (the synthesis problem). As message producers we start from the meaning we want to communicate (a “thought”) and we must encode it into a word sequence (the generation problem). If distributional semantics is to be considered a proper semantic theory, then it must deal not only w</context>
</contexts>
<marker>Baroni, 2013</marker>
<rawString>Marco Baroni. 2013. Composition in distributional semantics. Language and Linguistics Compass, 7(10):511–522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Blacoe</author>
<author>Mirella Lapata</author>
</authors>
<title>A comparison of vector-based representations for semantic composition.</title>
<date>2012</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>546--556</pages>
<location>Jeju Island,</location>
<contexts>
<context position="19921" citStr="Blacoe and Lapata, 2012" startWordPosition="3175" endWordPosition="3179">t cbow count A.N A, N 0.36,0.61 0.20,0.41 P.N P, N 0.93,0.79 0.60,0.57 AoN A, N 1.00,1.00 0.86,0.99 PoN P, N 1.00,1.00 1.00,1.00 Table 2: Accuracy of generation models at retrieving (at rank 1) the constituent words of adjective-noun (AN) and preposition-noun (PN) phrases. Observed (A.N) and composed representations (AoN) are decomposed with observed(eq. 2) and composed-trained (eq. 3) functions respectively. paraphrase-by-generation task we tackle here and in the next experiments. Compositional distributional semantic systems are often evaluated on phrase and sentence paraphrasing data sets (Blacoe and Lapata, 2012; Mitchell and Lapata, 2010; Socher et al., 2011; Turney, 2012). However, these experiments assume a pre-compiled list of candidate paraphrases, and the task is to rank correct paraphrases above foils (paraphrase ranking) or to decide, for a given pair, if the two phrases/sentences are mutual paraphrases (paraphrase detection). Here, instead, we do not assume a given set of candidates: For example, in N—*AN paraphrasing, any of 20K2 possible combinations of adjectives and nouns from the lexicon could be generated. This is a much more challenging task and it paves the way to more realistic appl</context>
</contexts>
<marker>Blacoe, Lapata, 2012</marker>
<rawString>William Blacoe and Mirella Lapata. 2012. A comparison of vector-based representations for semantic composition. In Proceedings of EMNLP, pages 546–556, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Elia Bruni</author>
<author>Gemma Boleda</author>
<author>Marco Baroni</author>
<author>Nam Khanh Tran</author>
</authors>
<title>Distributional semantics in Technicolor.</title>
<date>2012</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>136--145</pages>
<location>Jeju Island,</location>
<contexts>
<context position="14834" citStr="Bruni et al., 2012" startWordPosition="2360" endWordPosition="2363">NC). The Italian language vectors for the cross-lingual experiments of Section 6 were trained on 1.6 billion tokens from itWaC.5 A word token is a wordform + POS-tag string. We extract both word vectors and the observed phrase vectors which are 3Available at https://code.google.com/p/ word2vec/ 4The parameters of both models have been chosen without specific tuning, based on their observed stable performance in previous independent experiments. 5Corpus sources: http://wacky.sslmit.unibo. it,http://www.natcorp.ox.ac.uk required for the training procedures. We sanitycheck the two spaces on MEN (Bruni et al., 2012), a 3,000 items word similarity data set. cbow significantly outperforms count (0.80 vs. 0.72 Spearman correlations with human judgments). count performance is consistent with previously reported results.6 (De)composition function training The training data sets consist of the 50K most frequent (u, v, p) tuples for each phrase type, for example, (red, car, red.car) or (in, car, in.car).7 We concatenate i and v� vectors to obtain the [U; V ] matrix and we use the observed p� vectors (e.g., the corpus vector of the red.car bigram) to obtain the phrase matrix P. We use these data sets to solve th</context>
</contexts>
<marker>Bruni, Boleda, Baroni, Tran, 2012</marker>
<rawString>Elia Bruni, Gemma Boleda, Marco Baroni, and Nam Khanh Tran. 2012. Distributional semantics in Technicolor. In Proceedings of ACL, pages 136– 145, Jeju Island, Korea.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Georgiana Dinu</author>
<author>Nghia The Pham</author>
<author>Marco Baroni</author>
</authors>
<title>General estimation and evaluation of compositional distributional semantic models.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality,</booktitle>
<pages>50--58</pages>
<location>Sofia, Bulgaria.</location>
<contexts>
<context position="9058" citStr="Dinu et al., 2013" startWordPosition="1413" endWordPosition="1416">lity, however this need not necessarily be the case. 625 min kP − WR[U; V ]k (1) WR∈Rdx2d We use the approximation of observed phrase vectors as objective because these vectors can provide direct evidence of the polysemous behaviour of words: For example, the corpus-observed vectors of green jacket and green politician reflect how the meaning of green is affected by its occurrence with different nouns. Moreover, it has been shown that for two-word phrases, despite their relatively low frequency, such corpus-observed representations are still difficult to outperform in phrase similarity tasks (Dinu et al., 2013; Turney, 2012). 3.2 Generation Generation of a two-word sequence from a vector proceeds in two steps: decomposition of the phrase vectors into two constituent vectors, and search for the nearest neighbours of each constituent vector in Lv (the lexical matrix) in order to retrieve the corresponding words from Lw. Decomposition We define a syntax-dependent decomposition function: [ii;V1 = fdecompR(�p) where p� is a phrase vector, u and v� are vectors associated to words standing in the syntactic relation R and fdecompR : Rd → Rd × Rd. We assume that decomposition is also a linear transformation</context>
<context position="16408" citStr="Dinu et al. (2013)" startWordPosition="2616" endWordPosition="2619"> performance Since the experiments below also use composed vectors as input to the generation process, it is important to provide independent evidence that the composition model is of high quality. This is indeed the case: We tested our composition approach on the task of retrieving observed AN and PN vectors, based on their composed vectors (similarly to Baroni and Zamparelli (2010), we want to retrieve the observed red.car vector using fcompAN(red, car)). We obtain excellent results, with minimum accuracy of 0.23 (chance level &lt;0.0001). We also test on the AN-N paraphrasing test set used in Dinu et al. (2013) (in turn adapting Turney (2012)). The dataset contains 620 ANs, each paired with a single-noun paraphrase (e.g., false belief/fallacy, personal appeal/charisma). The task is to rank all nouns in the lexicon by their similarity to the phrase, and return the rank of the correct paraphrase. Results are reported in the first row of Table 1. To facilitate comparison, we search, like Dinu et al., through a vocabulary containing the 20K most frequent nouns. The count vectors results are similar to those reported by Dinu and colleagues for the same model, and with cbow vec6See Baroni et al. (2014) fo</context>
<context position="21860" citStr="Dinu et al. (2013)" startWordPosition="3487" endWordPosition="3490"> Examples of generated phrases are given in Table 3. In addition to generating topically related ANs, we also see nouns disambiguated in different ways than intended in 628 Input Output Gold reasoning deductive thinking abstract thought jurisdiction legal authority legal power thunderstorm thundery storm electrical storm folk local music common people superstition old-fashioned religion superstitious notion vitriol political bitterness sulfuric acid zoom fantastic camera rapid growth religion religious religion religious belief Table 3: Examples of generating ANs from Ns using the data set of Dinu et al. (2013). the gold standard (for example vitriol and folk in Table 3). Other interesting errors consist of decomposing a noun into two words which both have the same meaning as the noun, generating for example religion —* religious religions. We observe moreover that sometimes the decomposition reflects selectional preference effects, by generating adjectives that denote typical properties of the noun to be paraphrased (e.g., animosity is a (political, personal,...) hostility or a fridge is a (big, large, small,...) refrigerator). This effect could be exploited for tasks such as property-based concept</context>
</contexts>
<marker>Dinu, Pham, Baroni, 2013</marker>
<rawString>Georgiana Dinu, Nghia The Pham, and Marco Baroni. 2013. General estimation and evaluation of compositional distributional semantic models. In Proceedings of ACL Workshop on Continuous Vector Space Models and their Compositionality, pages 50– 58, Sofia, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Katrin Erk</author>
</authors>
<title>Vector space models of word meaning and phrase meaning: A survey.</title>
<date>2012</date>
<journal>Language and Linguistics Compass,</journal>
<volume>6</volume>
<issue>10</issue>
<contexts>
<context position="1049" citStr="Erk, 2012" startWordPosition="152" endWordPosition="153"> on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions. We test this in a monolingual scenario (paraphrase generation) as well as in a cross-lingual setting (translation by synthesizing adjectivenoun phrase vectors in English and generating the equivalent expressions in Italian). 1 Introduction Distributional methods for semantics approximate the meaning of linguistic expressions with vectors that summarize the contexts in which they occur in large samples of text. This has been a very successful approach to lexical semantics (Erk, 2012), where semantic relatedness is assessed by comparing vectors. Recently these methods have been extended to phrases and sentences by means of composition operations (see Baroni (2013) for an overview). For example, given the vectors representing red and car, composition derives a vector that approximates the meaning of red car. However, the link between language and meaning is, obviously, bidirectional: As message recipients we are exposed to a linguistic expression and we must compute its meaning (the synthesis problem). As message producers we start from the meaning we want to communicate (a</context>
</contexts>
<marker>Erk, 2012</marker>
<rawString>Katrin Erk. 2012. Vector space models of word meaning and phrase meaning: A survey. Language and Linguistics Compass, 6(10):635–653.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Frome</author>
<author>Greg Corrado</author>
<author>Jon Shlens</author>
<author>Samy Bengio</author>
<author>Jeff Dean</author>
<author>Marc’Aurelio Ranzato</author>
<author>Tomas Mikolov</author>
</authors>
<title>DeViSE: A deep visual-semantic embedding model.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>2121--2129</pages>
<location>Lake Tahoe, Nevada.</location>
<contexts>
<context position="2580" citStr="Frome et al., 2013" startWordPosition="391" endWordPosition="394">tions, phrase generation from vectors has many useful applications. We can, for example, synthesize the vector representing the meaning of a phrase or sentence, and then generate alternative phrases or sentences from this vector to accomplish true paraphrase generation (as opposed to paraphrase detection or ranking of candidate paraphrases). Generation can be even more useful when the source vector comes from another modality or language. Recent work on grounding language in vision shows that it is possible to represent images and linguistic expressions in a common vectorbased semantic space (Frome et al., 2013; Socher et al., 2013). Given a vector representing an image, generation can be used to productively construct phrases or sentences that describe the image (as opposed to simply retrieving an existing description from a set of candidates). Translation is another potential application of the generation framework: Given a semantic space shared between two or more languages, one can compose a word sequence in one language and generate translations in another, with the shared semantic vector space functioning as interlingua. Distributional semantics assumes a lexicon of atomic expressions (that, f</context>
</contexts>
<marker>Frome, Corrado, Shlens, Bengio, Dean, Ranzato, Mikolov, 2013</marker>
<rawString>Andrea Frome, Greg Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. 2013. DeViSE: A deep visual-semantic embedding model. In Proceedings of NIPS, pages 2121–2129, Lake Tahoe, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emiliano Guevara</author>
</authors>
<title>A regression model of adjective-noun compositionality in distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of GEMS,</booktitle>
<pages>33--37</pages>
<location>Uppsala,</location>
<contexts>
<context position="7367" citStr="Guevara (2010)" startWordPosition="1143" endWordPosition="1144">struct the vector representing a two-word phrase, we must compose the vectors associated to the input words. More formally, similarly to Mitchell and Lapata (2008), we define a syntaxdependent composition function yielding a phrase vector P. p� = fcompR(u,v) where i and v� are the vector representations associated to words u and v. fcompR : Rd x Rd , Rd (for d the dimensionality of vectors) is a composition function specific to the syntactic relation R holding between the two words.1 Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzotto et al. (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors: fcompR(u,v) = W1ii + W2v. We will further use the following equivalent formulation: fcompR(�u,�v) = WR[�u;v1 where WR E Rd×2d and [u;I is the vertical concatenation of the two vectors (using Matlab notation). Following Guevara, we learn WR using examples of word and phrase vectors directly extracted from the corpus (for the rest of the paper, we refer to these phrase vectors extracted non-compositionally from the corpus as observed ve</context>
</contexts>
<marker>Guevara, 2010</marker>
<rawString>Emiliano Guevara. 2010. A regression model of adjective-noun compositionality in distributional semantics. In Proceedings of GEMS, pages 33–37, Uppsala, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aria Haghighi</author>
<author>Percy Liang</author>
<author>Taylor Berg-Kirkpatrick</author>
<author>Dan Klein</author>
</authors>
<title>Learning bilingual lexicons from monolingual corpora.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>771--779</pages>
<location>Columbus, OH, USA,</location>
<contexts>
<context position="25635" citStr="Haghighi et al., 2008" startWordPosition="4086" endWordPosition="4089">esser degree, this might be desirable as a disambiguation-in-context effect as, for example, in underground cavern, in secret would not be a context-appropriate paraphrase of underground. 6 Noun phrase translation This section describes preliminary experiments performed in a cross-lingual setting on the task of composing English AN phrases and generating Italian translations. Creation of cross-lingual vector spaces A common semantic space is required in order to map words and phrases across languages. This problem has been extensively addressed in the bilingual lexicon acquisition literature (Haghighi et al., 2008; Koehn and Knight, 2002). We opt for a very simple yet accurate method (Klementiev et al., 2012; Rapp, 1999) in which a bilingual dictionary is used to identify a set of shared dimensions across spaces and the vectors of both languages are projected into the subspace defined by these (Subspace Projection - SP). This method is applicable to count-type vector spaces, for which the dimen629 Input Output Training cbow count AoN N, P, N observed 0.98(1),0.08(5.5),0.13(20.5) 0.82(1),0.17(4.5),0.05(71.5) AoN N, P, N composed 0.99(1),0.02(12), 0.12(24) 0.99(1),0.06(10), 0.05(150.5) Table 4: Top 1 acc</context>
</contexts>
<marker>Haghighi, Liang, Berg-Kirkpatrick, Klein, 2008</marker>
<rawString>Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and Dan Klein. 2008. Learning bilingual lexicons from monolingual corpora. In Proceedings of ACL, pages 771–779, Columbus, OH, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Haxby</author>
<author>Ida Gobbini</author>
<author>Maura Furey</author>
<author>Alumit Ishai</author>
<author>Jennifer Schouten</author>
<author>Pietro Pietrini</author>
</authors>
<title>Distributed and overlapping representations of faces and objects in ventral temporal cortex.</title>
<date>2001</date>
<journal>Science,</journal>
<pages>293--2425</pages>
<contexts>
<context position="34653" citStr="Haxby et al., 2001" startWordPosition="5496" endWordPosition="5499">retrieving existing captions (Socher et al., 2014) and not actual generation of image descriptions. From a more theoretical point of view, our work fills an important gap in distributional semantics, making it a bidirectional theory of the connection between language and meaning. We can now translate linguistic strings into vector “thoughts”, and the latter into their most appropriate linguistic expression. Several neuroscientific studies suggest that thoughts are represented in the brain by patterns of activation over broad neural areas, and vectors are a natural way to encode such patterns (Haxby et al., 2001; Huth et al., 2012). Some research has already established a connection between neural and distributional semantic vector spaces (Mitchell et al., 2008; Murphy et al., 2012). Generation might be the missing link to powerful computational models that take the neural footprint of a thought as input and produce its linguistic expression. Acknowledgments We thank Kevin Knight, Andrew Anderson, Roberto Zamparelli, Angeliki Lazaridou, Nghia The Pham, Germ´an Kruszewski and Peter Turney for helpful discussions and the anonymous reviewers for their useful comments. We acknowledge the ERC 2011 Startin</context>
</contexts>
<marker>Haxby, Gobbini, Furey, Ishai, Schouten, Pietrini, 2001</marker>
<rawString>James Haxby, Ida Gobbini, Maura Furey, Alumit Ishai, Jennifer Schouten, and Pietro Pietrini. 2001. Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science, 293:2425–2430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Huth</author>
<author>Shinji Nishimoto</author>
<author>An Vu</author>
<author>Jack Gallant</author>
</authors>
<title>A continuous semantic space describes the representation of thousands of object and action categories across the human brain.</title>
<date>2012</date>
<journal>Neuron,</journal>
<volume>76</volume>
<issue>6</issue>
<contexts>
<context position="34673" citStr="Huth et al., 2012" startWordPosition="5500" endWordPosition="5503">captions (Socher et al., 2014) and not actual generation of image descriptions. From a more theoretical point of view, our work fills an important gap in distributional semantics, making it a bidirectional theory of the connection between language and meaning. We can now translate linguistic strings into vector “thoughts”, and the latter into their most appropriate linguistic expression. Several neuroscientific studies suggest that thoughts are represented in the brain by patterns of activation over broad neural areas, and vectors are a natural way to encode such patterns (Haxby et al., 2001; Huth et al., 2012). Some research has already established a connection between neural and distributional semantic vector spaces (Mitchell et al., 2008; Murphy et al., 2012). Generation might be the missing link to powerful computational models that take the neural footprint of a thought as input and produce its linguistic expression. Acknowledgments We thank Kevin Knight, Andrew Anderson, Roberto Zamparelli, Angeliki Lazaridou, Nghia The Pham, Germ´an Kruszewski and Peter Turney for helpful discussions and the anonymous reviewers for their useful comments. We acknowledge the ERC 2011 Starting Independent Resear</context>
</contexts>
<marker>Huth, Nishimoto, Vu, Gallant, 2012</marker>
<rawString>Alexander Huth, Shinji Nishimoto, An Vu, and Jack Gallant. 2012. A continuous semantic space describes the representation of thousands of object and action categories across the human brain. Neuron, 76(6):1210–1224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Seattle,</location>
<contexts>
<context position="4744" citStr="Kalchbrenner and Blunsom (2013)" startWordPosition="721" endWordPosition="724">uce a more direct approach to phrase generation, inspired by the work in compositional distributional semantics. In short, we revert the composition process and we propose a framework of data-induced, syntax-dependent functions that decompose a single vector into a vector sequence. The generated vectors can then be efficiently matched against those in the lexicon or fed to the decomposition system again to produce longer phrases recursively. 2 Related work To the best of our knowledge, we are the first to explicitly and systematically pursue the generation problem in distributional semantics. Kalchbrenner and Blunsom (2013) use top-level, composed distributed representations of sentences to guide generation in a machine translation setting. More precisely, they condition the target language model on the composed representation (addition of word vectors) of the source language sentence. Andreas and Ghahramani (2013) discuss the the issue of generating language from vectors and present a probabilistic generative model for distributional vectors. However, their emphasis is on reversing the generative story in order to derive composed meaning representations from word sequences. The theoretical generating capabiliti</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, Seattle, October. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Kelly</author>
<author>Barry Devereux</author>
<author>Anna Korhonen</author>
</authors>
<title>Semi-supervised learning for automatic conceptual property extraction.</title>
<date>2012</date>
<booktitle>In Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics,</booktitle>
<pages>11--20</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="22493" citStr="Kelly et al., 2012" startWordPosition="3586" endWordPosition="3589">ndard (for example vitriol and folk in Table 3). Other interesting errors consist of decomposing a noun into two words which both have the same meaning as the noun, generating for example religion —* religious religions. We observe moreover that sometimes the decomposition reflects selectional preference effects, by generating adjectives that denote typical properties of the noun to be paraphrased (e.g., animosity is a (political, personal,...) hostility or a fridge is a (big, large, small,...) refrigerator). This effect could be exploited for tasks such as property-based concept description (Kelly et al., 2012). 5.2 Recursive decomposition We continue by testing generation through recursive decomposition on the task of generating nounpreposition-noun (NPN) paraphrases of adjectivenouns (AN) phrases. We introduce a dataset containing 192 AN-NPN pairs (such as pre-election promises—* promises before election), which was created by the second author and additionally corrected by an English native speaker. The data set was created by analyzing a list of randomly selected frequent ANs. 49 further ANs (with adjectives such as amazing and great) were judged not NPN-paraphrasable and were used for the exper</context>
</contexts>
<marker>Kelly, Devereux, Korhonen, 2012</marker>
<rawString>Colin Kelly, Barry Devereux, and Anna Korhonen. 2012. Semi-supervised learning for automatic conceptual property extraction. In Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics, pages 11–20, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexandre Klementiev</author>
<author>Ann Irvine</author>
<author>Chris CallisonBurch</author>
<author>David Yarowsky</author>
</authors>
<title>Toward statistical machine translation without parallel corpora.</title>
<date>2012</date>
<booktitle>In Proceedings of EACL,</booktitle>
<pages>130--140</pages>
<location>Avignon, France.</location>
<contexts>
<context position="25731" citStr="Klementiev et al., 2012" startWordPosition="4103" endWordPosition="4106">n underground cavern, in secret would not be a context-appropriate paraphrase of underground. 6 Noun phrase translation This section describes preliminary experiments performed in a cross-lingual setting on the task of composing English AN phrases and generating Italian translations. Creation of cross-lingual vector spaces A common semantic space is required in order to map words and phrases across languages. This problem has been extensively addressed in the bilingual lexicon acquisition literature (Haghighi et al., 2008; Koehn and Knight, 2002). We opt for a very simple yet accurate method (Klementiev et al., 2012; Rapp, 1999) in which a bilingual dictionary is used to identify a set of shared dimensions across spaces and the vectors of both languages are projected into the subspace defined by these (Subspace Projection - SP). This method is applicable to count-type vector spaces, for which the dimen629 Input Output Training cbow count AoN N, P, N observed 0.98(1),0.08(5.5),0.13(20.5) 0.82(1),0.17(4.5),0.05(71.5) AoN N, P, N composed 0.99(1),0.02(12), 0.12(24) 0.99(1),0.06(10), 0.05(150.5) Table 4: Top 1 accuracy (median rank) on the AN—*NPN paraphrasing data set. AN phrases are composed and then recur</context>
<context position="33688" citStr="Klementiev et al. (2012)" startWordPosition="5340" endWordPosition="5343">omplex models for (de-)composition in order to improve the performance on the tasks we used in this paper. Following this, we 631 would like to extend the framework to handle arbitrary phrases, including making (confidencebased) choices on the syntactic structure of the phrase to be generated, which we have assumed to be given throughout this paper. In terms of applications, we believe that the line of research in machine translation that is currently focusing on replacing parallel resources with large amounts of monolingual text provides an interesting setup to test our methods. For example, Klementiev et al. (2012) reconstruct phrase tables based on phrase similarity scores in semantic space. However, they resort to scoring phrase pairs extracted from an aligned parallel corpus, as they do not have a method to freely generate these. Similarly, in the recent work on common vector spaces for the representation of images and text, the current emphasis is on retrieving existing captions (Socher et al., 2014) and not actual generation of image descriptions. From a more theoretical point of view, our work fills an important gap in distributional semantics, making it a bidirectional theory of the connection be</context>
</contexts>
<marker>Klementiev, Irvine, CallisonBurch, Yarowsky, 2012</marker>
<rawString>Alexandre Klementiev, Ann Irvine, Chris CallisonBurch, and David Yarowsky. 2012. Toward statistical machine translation without parallel corpora. In Proceedings of EACL, pages 130–140, Avignon, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Kevin Knight</author>
</authors>
<title>Learning a translation lexicon from monolingual corpora. In</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Workshop on Unsupervised Lexical Acquisition,</booktitle>
<pages>9--16</pages>
<location>Philadelphia, PA, USA.</location>
<contexts>
<context position="25660" citStr="Koehn and Knight, 2002" startWordPosition="4090" endWordPosition="4093">t be desirable as a disambiguation-in-context effect as, for example, in underground cavern, in secret would not be a context-appropriate paraphrase of underground. 6 Noun phrase translation This section describes preliminary experiments performed in a cross-lingual setting on the task of composing English AN phrases and generating Italian translations. Creation of cross-lingual vector spaces A common semantic space is required in order to map words and phrases across languages. This problem has been extensively addressed in the bilingual lexicon acquisition literature (Haghighi et al., 2008; Koehn and Knight, 2002). We opt for a very simple yet accurate method (Klementiev et al., 2012; Rapp, 1999) in which a bilingual dictionary is used to identify a set of shared dimensions across spaces and the vectors of both languages are projected into the subspace defined by these (Subspace Projection - SP). This method is applicable to count-type vector spaces, for which the dimen629 Input Output Training cbow count AoN N, P, N observed 0.98(1),0.08(5.5),0.13(20.5) 0.82(1),0.17(4.5),0.05(71.5) AoN N, P, N composed 0.99(1),0.02(12), 0.12(24) 0.99(1),0.06(10), 0.05(150.5) Table 4: Top 1 accuracy (median rank) on th</context>
</contexts>
<marker>Koehn, Knight, 2002</marker>
<rawString>Philipp Koehn and Kevin Knight. 2002. Learning a translation lexicon from monolingual corpora. In In Proceedings of ACL Workshop on Unsupervised Lexical Acquisition, pages 9–16, Philadelphia, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<note>http://arxiv.org/ abs/1301.3781/.</note>
<contexts>
<context position="13185" citStr="Mikolov et al. (2013" startWordPosition="2090" endWordPosition="2093">odifier can be an adjective or a prepositional phrase. A prepositional phrase is in turn composed of a preposition and a noun phrase. We learn two composition (and corresponding decomposition) functions: one for modifier-noun phrases, trained on adjective-noun (AN) pairs, and a second one for prepositional phrases, trained on preposition-noun (PN) combinations. For the rest of this section we describe the construction of the vector spaces and the (de)composition function learning procedure. Construction of vector spaces We test two types of vector representations. The cbow model introduced in Mikolov et al. (2013a) learns vector representations using a neural network architecture by trying to predict a target word given the words surrounding it. We use the word2vec software3 to build vectors of size 300 and using a context window of 5 words to either side of the target. We set the sub-sampling option to 1e-05 and estimate the probability of a target word with the negative sampling method, drawing 10 samples from the noise distribution (see Mikolov et al. (2013a) for details). We also implement a standard countbased bag-of-words distributional space (Turney and Pantel, 2010) which counts occurrences of</context>
<context position="27413" citStr="Mikolov et al. (2013" startWordPosition="4356" endWordPosition="4359">n in highlands cable through ocean cavern through rock field into research years during 1930s pain through patient days after wartime differences between intergroup level between levels region with mountains cable under sea cavern under ground field between disciplines years between wars pain after operation days before war differences between minorities level on surface sions correspond to actual words. As the cbow dimensions do not correspond to words, we align the cbow spaces by using a small dictionary to learn a linear map which transforms the English vectors into Italian ones as done in Mikolov et al. (2013b). This method (Translation Matrix - TM) is applicable to both cbow and count spaces. We tune the parameters (TM or SP for count and dictionary size 5K or 25K for both spaces) on a standard task of translating English words into Italian. We obtain TM-5K for cbow and SP-25K for count as optimal settings. The two methods perform similarly for low frequency words while cbow-TM-5K significantly outperforms count-SP-25K for high frequency words. Our results for the cbow-TM-5K setting are similar to those reported by Mikolov et al. (2013b). Cross-lingual decomposition training Training proceeds as </context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. http://arxiv.org/ abs/1301.3781/.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Tomas Mikolov</author>
</authors>
<title>Quoc Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for Machine Translation. http://arxiv.org/abs/</title>
<pages>1309--4168</pages>
<marker>Mikolov, </marker>
<rawString>Tomas Mikolov, Quoc Le, and Ilya Sutskever. 2013b. Exploiting similarities among languages for Machine Translation. http://arxiv.org/abs/ 1309.4168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Vector-based models of semantic composition.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>236--244</pages>
<location>Columbus, OH.</location>
<contexts>
<context position="6916" citStr="Mitchell and Lapata (2008)" startWordPosition="1060" endWordPosition="1063">eration for the same structures. Finally, we show how synthesis and generation of longer phrases is handled by recursive extension of the two-word case. We assume a lexicon L, that is, a bi-directional look-up table containing a list of words Lw linked to a matrix Lv of vectors. Both synthesis and generation involve a trivial lexicon look-up step to retrieve vectors associated to words and vice versa: We ignore it in the exposition below. 3.1 Synthesis To construct the vector representing a two-word phrase, we must compose the vectors associated to the input words. More formally, similarly to Mitchell and Lapata (2008), we define a syntaxdependent composition function yielding a phrase vector P. p� = fcompR(u,v) where i and v� are the vector representations associated to words u and v. fcompR : Rd x Rd , Rd (for d the dimensionality of vectors) is a composition function specific to the syntactic relation R holding between the two words.1 Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzotto et al. (2010) which defines composition as application of linear transformations to the two constituents followed by summing the result</context>
</contexts>
<marker>Mitchell, Lapata, 2008</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2008. Vector-based models of semantic composition. In Proceedings of ACL, pages 236–244, Columbus, OH.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeff Mitchell</author>
<author>Mirella Lapata</author>
</authors>
<title>Composition in distributional models of semantics.</title>
<date>2010</date>
<journal>Cognitive Science,</journal>
<volume>34</volume>
<issue>8</issue>
<contexts>
<context position="19948" citStr="Mitchell and Lapata, 2010" startWordPosition="3180" endWordPosition="3183">6,0.61 0.20,0.41 P.N P, N 0.93,0.79 0.60,0.57 AoN A, N 1.00,1.00 0.86,0.99 PoN P, N 1.00,1.00 1.00,1.00 Table 2: Accuracy of generation models at retrieving (at rank 1) the constituent words of adjective-noun (AN) and preposition-noun (PN) phrases. Observed (A.N) and composed representations (AoN) are decomposed with observed(eq. 2) and composed-trained (eq. 3) functions respectively. paraphrase-by-generation task we tackle here and in the next experiments. Compositional distributional semantic systems are often evaluated on phrase and sentence paraphrasing data sets (Blacoe and Lapata, 2012; Mitchell and Lapata, 2010; Socher et al., 2011; Turney, 2012). However, these experiments assume a pre-compiled list of candidate paraphrases, and the task is to rank correct paraphrases above foils (paraphrase ranking) or to decide, for a given pair, if the two phrases/sentences are mutual paraphrases (paraphrase detection). Here, instead, we do not assume a given set of candidates: For example, in N—*AN paraphrasing, any of 20K2 possible combinations of adjectives and nouns from the lexicon could be generated. This is a much more challenging task and it paves the way to more realistic applications of distributional </context>
</contexts>
<marker>Mitchell, Lapata, 2010</marker>
<rawString>Jeff Mitchell and Mirella Lapata. 2010. Composition in distributional models of semantics. Cognitive Science, 34(8):1388–1429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tom Mitchell</author>
<author>Svetlana Shinkareva</author>
<author>Andrew Carlson</author>
<author>Kai-Min Chang</author>
<author>Vincente Malave</author>
<author>Robert Mason</author>
<author>Marcel Just</author>
</authors>
<title>Predicting human brain activity associated with the meanings of nouns.</title>
<date>2008</date>
<journal>Science,</journal>
<pages>320--1191</pages>
<marker>Mitchell, Shinkareva, Carlson, Chang, Malave, Mason, Just, 2008</marker>
<rawString>Tom Mitchell, Svetlana Shinkareva, Andrew Carlson, Kai-Min Chang, Vincente Malave, Robert Mason, and Marcel Just. 2008. Predicting human brain activity associated with the meanings of nouns. Science, 320:1191–1195.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Murphy</author>
<author>Partha Talukdar</author>
<author>Tom Mitchell</author>
</authors>
<title>Selecting corpus-semantic models for neurolinguistic decoding.</title>
<date>2012</date>
<booktitle>In Proceedings of *SEM,</booktitle>
<pages>114--123</pages>
<location>Montreal, Canada.</location>
<marker>Murphy, Talukdar, Mitchell, 2012</marker>
<rawString>Brian Murphy, Partha Talukdar, and Tom Mitchell. 2012. Selecting corpus-semantic models for neurolinguistic decoding. In Proceedings of *SEM, pages 114–123, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Reinhard Rapp</author>
</authors>
<title>Automatic identification of word translations from unrelated english and german corpora.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99,</booktitle>
<pages>519--526</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="25744" citStr="Rapp, 1999" startWordPosition="4107" endWordPosition="4108">secret would not be a context-appropriate paraphrase of underground. 6 Noun phrase translation This section describes preliminary experiments performed in a cross-lingual setting on the task of composing English AN phrases and generating Italian translations. Creation of cross-lingual vector spaces A common semantic space is required in order to map words and phrases across languages. This problem has been extensively addressed in the bilingual lexicon acquisition literature (Haghighi et al., 2008; Koehn and Knight, 2002). We opt for a very simple yet accurate method (Klementiev et al., 2012; Rapp, 1999) in which a bilingual dictionary is used to identify a set of shared dimensions across spaces and the vectors of both languages are projected into the subspace defined by these (Subspace Projection - SP). This method is applicable to count-type vector spaces, for which the dimen629 Input Output Training cbow count AoN N, P, N observed 0.98(1),0.08(5.5),0.13(20.5) 0.82(1),0.17(4.5),0.05(71.5) AoN N, P, N composed 0.99(1),0.02(12), 0.12(24) 0.99(1),0.06(10), 0.05(150.5) Table 4: Top 1 accuracy (median rank) on the AN—*NPN paraphrasing data set. AN phrases are composed and then recursively decomp</context>
</contexts>
<marker>Rapp, 1999</marker>
<rawString>Reinhard Rapp. 1999. Automatic identification of word translations from unrelated english and german corpora. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, ACL ’99, pages 519– 526. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Eric Huang</author>
<author>Jeffrey Pennin</author>
<author>Andrew Ng</author>
<author>Christopher Manning</author>
</authors>
<title>Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.</title>
<date>2011</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>801--809</pages>
<location>Granada,</location>
<contexts>
<context position="5454" citStr="Socher et al. (2011)" startWordPosition="825" endWordPosition="828"> machine translation setting. More precisely, they condition the target language model on the composed representation (addition of word vectors) of the source language sentence. Andreas and Ghahramani (2013) discuss the the issue of generating language from vectors and present a probabilistic generative model for distributional vectors. However, their emphasis is on reversing the generative story in order to derive composed meaning representations from word sequences. The theoretical generating capabilities of the methods they propose are briefly exemplified, but not fully explored or tested. Socher et al. (2011) come closest to our target problem. They introduce a bidirectional languageto-meaning model for compositional distributional semantics that is similar in spirit to ours. However, we present a clearer decoupling of synthesis and generation and we use different (and simpler) training methods and objective functions. Moreover, Socher and colleagues do not train separate decomposition rules for different syntactic configurations, so it is not clear how they would be able to control the generation of different output structures. Finally, the potential for generation is only addressed in passing, b</context>
<context position="19969" citStr="Socher et al., 2011" startWordPosition="3184" endWordPosition="3187">.93,0.79 0.60,0.57 AoN A, N 1.00,1.00 0.86,0.99 PoN P, N 1.00,1.00 1.00,1.00 Table 2: Accuracy of generation models at retrieving (at rank 1) the constituent words of adjective-noun (AN) and preposition-noun (PN) phrases. Observed (A.N) and composed representations (AoN) are decomposed with observed(eq. 2) and composed-trained (eq. 3) functions respectively. paraphrase-by-generation task we tackle here and in the next experiments. Compositional distributional semantic systems are often evaluated on phrase and sentence paraphrasing data sets (Blacoe and Lapata, 2012; Mitchell and Lapata, 2010; Socher et al., 2011; Turney, 2012). However, these experiments assume a pre-compiled list of candidate paraphrases, and the task is to rank correct paraphrases above foils (paraphrase ranking) or to decide, for a given pair, if the two phrases/sentences are mutual paraphrases (paraphrase detection). Here, instead, we do not assume a given set of candidates: For example, in N—*AN paraphrasing, any of 20K2 possible combinations of adjectives and nouns from the lexicon could be generated. This is a much more challenging task and it paves the way to more realistic applications of distributional semantics in generati</context>
</contexts>
<marker>Socher, Huang, Pennin, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Eric Huang, Jeffrey Pennin, Andrew Ng, and Christopher Manning. 2011. Dynamic pooling and unfolding recursive autoencoders for paraphrase detection. In Proceedings of NIPS, pages 801–809, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Milind Ganjoo</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Zero-shot learning through cross-modal transfer.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS,</booktitle>
<pages>935--943</pages>
<location>Lake Tahoe, Nevada.</location>
<contexts>
<context position="2602" citStr="Socher et al., 2013" startWordPosition="395" endWordPosition="398">tion from vectors has many useful applications. We can, for example, synthesize the vector representing the meaning of a phrase or sentence, and then generate alternative phrases or sentences from this vector to accomplish true paraphrase generation (as opposed to paraphrase detection or ranking of candidate paraphrases). Generation can be even more useful when the source vector comes from another modality or language. Recent work on grounding language in vision shows that it is possible to represent images and linguistic expressions in a common vectorbased semantic space (Frome et al., 2013; Socher et al., 2013). Given a vector representing an image, generation can be used to productively construct phrases or sentences that describe the image (as opposed to simply retrieving an existing description from a set of candidates). Translation is another potential application of the generation framework: Given a semantic space shared between two or more languages, one can compose a word sequence in one language and generate translations in another, with the shared semantic vector space functioning as interlingua. Distributional semantics assumes a lexicon of atomic expressions (that, for simplicity, we take</context>
</contexts>
<marker>Socher, Ganjoo, Manning, Ng, 2013</marker>
<rawString>Richard Socher, Milind Ganjoo, Christopher Manning, and Andrew Ng. 2013. Zero-shot learning through cross-modal transfer. In Proceedings of NIPS, pages 935–943, Lake Tahoe, Nevada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Quoc Le</author>
<author>Christopher Manning</author>
<author>Andrew Ng</author>
</authors>
<title>Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics.</title>
<date>2014</date>
<note>In press.</note>
<contexts>
<context position="34085" citStr="Socher et al., 2014" startWordPosition="5406" endWordPosition="5409">e of research in machine translation that is currently focusing on replacing parallel resources with large amounts of monolingual text provides an interesting setup to test our methods. For example, Klementiev et al. (2012) reconstruct phrase tables based on phrase similarity scores in semantic space. However, they resort to scoring phrase pairs extracted from an aligned parallel corpus, as they do not have a method to freely generate these. Similarly, in the recent work on common vector spaces for the representation of images and text, the current emphasis is on retrieving existing captions (Socher et al., 2014) and not actual generation of image descriptions. From a more theoretical point of view, our work fills an important gap in distributional semantics, making it a bidirectional theory of the connection between language and meaning. We can now translate linguistic strings into vector “thoughts”, and the latter into their most appropriate linguistic expression. Several neuroscientific studies suggest that thoughts are represented in the brain by patterns of activation over broad neural areas, and vectors are a natural way to encode such patterns (Haxby et al., 2001; Huth et al., 2012). Some resea</context>
</contexts>
<marker>Socher, Le, Manning, Ng, 2014</marker>
<rawString>Richard Socher, Quoc Le, Christopher Manning, and Andrew Ng. 2014. Grounded compositional semantics for finding and describing images with sentences. Transactions of the Association for Computational Linguistics. In press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Parallel data, tools and interfaces in opus.</title>
<date>2012</date>
<booktitle>In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="28520" citStr="Tiedemann, 2012" startWordPosition="4531" endWordPosition="4532">milar to those reported by Mikolov et al. (2013b). Cross-lingual decomposition training Training proceeds as in the monolingual case, this time concatenating the training data sets and estimating a single (de)-composition function for the two languages in the shared semantic space. We train both on observed phrase representations (eq. 2) and on composed phrase representations (eq. 3). Adjective-noun translation dataset We randomly extract 1,000 AN-AN En-It phrase pairs from a phrase table built from parallel movie subtitles, available at http://opus.lingfil. uu.se/ (OpenSubtitles2012, en-it) (Tiedemann, 2012). Input Output cbow count AoN(En) A,N (It) 0.31,0.59 0.24,0.54 AoN (It) A,N(En) 0.50,0.62 0.28,0.48 Table 6: Accuracy of En—*It and It—*En phrase translation: phrases are composed in source language and decomposed in target language. Training on composed phrase representations (eq. (3)) (with observed phrase training (eq. 2) results are Pz50% lower). Results are presented in Table 6. While in these preliminary experiments we lack a proper term of comparison, the performance is very good both quantitatively (random &lt; 0.0001) and qualitatively. The En—*It examples in Table 7 are representative. </context>
</contexts>
<marker>Tiedemann, 2012</marker>
<rawString>J¨org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
<author>Patrick Pantel</author>
</authors>
<title>From frequency to meaning: Vector space models of semantics.</title>
<date>2010</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<pages>37--141</pages>
<contexts>
<context position="11302" citStr="Turney and Pantel, 2010" startWordPosition="1803" endWordPosition="1806">ive in eq. (3) is more motivated. Nearest neighbour search We retrieve the nearest neighbours of each constituent vector u obtained by decomposition by applying a search function s: NNil = s(u, Lv, t) where NNil is a list containing the t nearest neighours of u from Lv, the lexical vectors. Depending on the task, t might be set to 1 to retrieve just one word sequence, or to larger values to retrieve t alternatives. The similarity measure used to determine the nearest neighbours is another parameter of the search function; we omit it here as we only experiment with the standard cosine measure (Turney and Pantel, 2010).2 3.3 Recursive (de)composition Extension to longer sequences is straightforward if we assume binary tree representations as syntactic structures. In synthesis, the top-level vector can be obtained by applying composition functions recursively. For example, the vector of big red car would be obtained as: fcompAN( big, fcompAN( red, cdr)), where fcompAN is the composition function for adjective-noun phrase combinations. Conversely, for generation, we decompose the phrase vector with fdecompAN. The first vector is used for retrieving the nearest adjective from the lexicon, while the second vect</context>
<context position="13757" citStr="Turney and Pantel, 2010" startWordPosition="2189" endWordPosition="2192">s. The cbow model introduced in Mikolov et al. (2013a) learns vector representations using a neural network architecture by trying to predict a target word given the words surrounding it. We use the word2vec software3 to build vectors of size 300 and using a context window of 5 words to either side of the target. We set the sub-sampling option to 1e-05 and estimate the probability of a target word with the negative sampling method, drawing 10 samples from the noise distribution (see Mikolov et al. (2013a) for details). We also implement a standard countbased bag-of-words distributional space (Turney and Pantel, 2010) which counts occurrences of a target word with other words within a symmetric window of size 5. We build a 300Kx300K symmetric co-occurrence matrix using the top most frequent words in our source corpus, apply positive PMI weighting and Singular Value Decomposition to reduce the space to 300 dimensions. For both spaces, the vectors are finally normalized to unit length.4 For both types of vectors we use 2.8 billion tokens as input (ukWaC + Wikipedia + BNC). The Italian language vectors for the cross-lingual experiments of Section 6 were trained on 1.6 billion tokens from itWaC.5 A word token </context>
</contexts>
<marker>Turney, Pantel, 2010</marker>
<rawString>Peter Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of Artificial Intelligence Research, 37:141–188.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Turney</author>
</authors>
<title>Domain and function: A dualspace model of semantic relations and compositions.</title>
<date>2012</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>44</volume>
<pages>585</pages>
<contexts>
<context position="9073" citStr="Turney, 2012" startWordPosition="1417" endWordPosition="1418">need not necessarily be the case. 625 min kP − WR[U; V ]k (1) WR∈Rdx2d We use the approximation of observed phrase vectors as objective because these vectors can provide direct evidence of the polysemous behaviour of words: For example, the corpus-observed vectors of green jacket and green politician reflect how the meaning of green is affected by its occurrence with different nouns. Moreover, it has been shown that for two-word phrases, despite their relatively low frequency, such corpus-observed representations are still difficult to outperform in phrase similarity tasks (Dinu et al., 2013; Turney, 2012). 3.2 Generation Generation of a two-word sequence from a vector proceeds in two steps: decomposition of the phrase vectors into two constituent vectors, and search for the nearest neighbours of each constituent vector in Lv (the lexical matrix) in order to retrieve the corresponding words from Lw. Decomposition We define a syntax-dependent decomposition function: [ii;V1 = fdecompR(�p) where p� is a phrase vector, u and v� are vectors associated to words standing in the syntactic relation R and fdecompR : Rd → Rd × Rd. We assume that decomposition is also a linear transformation, W0R ∈ R2d×d, </context>
<context position="16440" citStr="Turney (2012)" startWordPosition="2623" endWordPosition="2624">low also use composed vectors as input to the generation process, it is important to provide independent evidence that the composition model is of high quality. This is indeed the case: We tested our composition approach on the task of retrieving observed AN and PN vectors, based on their composed vectors (similarly to Baroni and Zamparelli (2010), we want to retrieve the observed red.car vector using fcompAN(red, car)). We obtain excellent results, with minimum accuracy of 0.23 (chance level &lt;0.0001). We also test on the AN-N paraphrasing test set used in Dinu et al. (2013) (in turn adapting Turney (2012)). The dataset contains 620 ANs, each paired with a single-noun paraphrase (e.g., false belief/fallacy, personal appeal/charisma). The task is to rank all nouns in the lexicon by their similarity to the phrase, and return the rank of the correct paraphrase. Results are reported in the first row of Table 1. To facilitate comparison, we search, like Dinu et al., through a vocabulary containing the 20K most frequent nouns. The count vectors results are similar to those reported by Dinu and colleagues for the same model, and with cbow vec6See Baroni et al. (2014) for an extensive comparison of the</context>
<context position="19984" citStr="Turney, 2012" startWordPosition="3188" endWordPosition="3189">N A, N 1.00,1.00 0.86,0.99 PoN P, N 1.00,1.00 1.00,1.00 Table 2: Accuracy of generation models at retrieving (at rank 1) the constituent words of adjective-noun (AN) and preposition-noun (PN) phrases. Observed (A.N) and composed representations (AoN) are decomposed with observed(eq. 2) and composed-trained (eq. 3) functions respectively. paraphrase-by-generation task we tackle here and in the next experiments. Compositional distributional semantic systems are often evaluated on phrase and sentence paraphrasing data sets (Blacoe and Lapata, 2012; Mitchell and Lapata, 2010; Socher et al., 2011; Turney, 2012). However, these experiments assume a pre-compiled list of candidate paraphrases, and the task is to rank correct paraphrases above foils (paraphrase ranking) or to decide, for a given pair, if the two phrases/sentences are mutual paraphrases (paraphrase detection). Here, instead, we do not assume a given set of candidates: For example, in N—*AN paraphrasing, any of 20K2 possible combinations of adjectives and nouns from the lexicon could be generated. This is a much more challenging task and it paves the way to more realistic applications of distributional semantics in generation scenarios. T</context>
</contexts>
<marker>Turney, 2012</marker>
<rawString>Peter Turney. 2012. Domain and function: A dualspace model of semantic relations and compositions. Journal of Artificial Intelligence Research, 44:533– 585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabio Zanzotto</author>
<author>Ioannis Korkontzelos</author>
<author>Francesca Falucchi</author>
<author>Suresh Manandhar</author>
</authors>
<title>Estimating linear models for compositional distributional semantics.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING,</booktitle>
<pages>1263--1271</pages>
<location>Beijing, China.</location>
<contexts>
<context position="7394" citStr="Zanzotto et al. (2010)" startWordPosition="1146" endWordPosition="1149">epresenting a two-word phrase, we must compose the vectors associated to the input words. More formally, similarly to Mitchell and Lapata (2008), we define a syntaxdependent composition function yielding a phrase vector P. p� = fcompR(u,v) where i and v� are the vector representations associated to words u and v. fcompR : Rd x Rd , Rd (for d the dimensionality of vectors) is a composition function specific to the syntactic relation R holding between the two words.1 Although we are not bound to a specific composition model, throughout this paper we use the method proposed by Guevara (2010) and Zanzotto et al. (2010) which defines composition as application of linear transformations to the two constituents followed by summing the resulting vectors: fcompR(u,v) = W1ii + W2v. We will further use the following equivalent formulation: fcompR(�u,�v) = WR[�u;v1 where WR E Rd×2d and [u;I is the vertical concatenation of the two vectors (using Matlab notation). Following Guevara, we learn WR using examples of word and phrase vectors directly extracted from the corpus (for the rest of the paper, we refer to these phrase vectors extracted non-compositionally from the corpus as observed vectors). To estimate, for ex</context>
</contexts>
<marker>Zanzotto, Korkontzelos, Falucchi, Manandhar, 2010</marker>
<rawString>Fabio Zanzotto, Ioannis Korkontzelos, Francesca Falucchi, and Suresh Manandhar. 2010. Estimating linear models for compositional distributional semantics. In Proceedings of COLING, pages 1263– 1271, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>