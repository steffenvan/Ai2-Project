<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005268">
<title confidence="0.989046">
On the Role of Morphosyntactic Features in Hindi Dependency Parsing
</title>
<author confidence="0.984349">
Bharat Ram Ambati*, Samar Husain*, Joakim Nivre† and Rajeev Sangal*
</author>
<affiliation confidence="0.988514">
*Language Technologies Research Centre, IIIT-Hyderabad, India.
†Department of Linguistics and Philology, Uppsala University, Sweden.
</affiliation>
<email confidence="0.836893">
{bharat,samar}@research.iiit.ac.in, joakim.nivre@lingfil.uu.se, san-
gal@mail.iiit.ac.in
</email>
<sectionHeader confidence="0.992972" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999360928571429">
This paper analyzes the relative importance of
different linguistic features for data-driven
dependency parsing of Hindi, using a feature
pool derived from two state-of-the-art parsers.
The analysis shows that the greatest gain in
accuracy comes from the addition of morpho-
syntactic features related to case, tense, aspect
and modality. Combining features from the
two parsers, we achieve a labeled attachment
score of 76.5%, which is 2 percentage points
better than the previous state of the art. We fi-
nally provide a detailed error analysis and
suggest possible improvements to the parsing
scheme.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99997125">
The dependency parsing community has since a
few years shown considerable interest in parsing
morphologically rich languages with flexible word
order. This is partly due to the increasing availabil-
ity of dependency treebanks for such languages,
but it is also motivated by the observation that the
performance obtained for these languages has not
been very high (Nivre et al., 2007a). Attempts at
handling various non-configurational aspects in
these languages have pointed towards shortcom-
ings in traditional parsing methodologies (Tsarfaty
and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et
al., 2009; Husain et al., 2009; Gadde et al., 2010).
Among other things, it has been pointed out that
the use of language specific features may play a
crucial role in improving the overall parsing per-
formance. Different languages tend to encode syn-
tactically relevant information in different ways,
and it has been hypothesized that the integration of
morphological and syntactic information could be
</bodyText>
<page confidence="0.990673">
94
</page>
<bodyText confidence="0.999918538461539">
a key to better accuracy. However, it has also been
noted that incorporating these language specific
features in parsing is not always straightforward
and many intuitive features do not always work in
expected ways.
In this paper, we are concerned with Hindi, an
Indian language with moderately rich morphology
and relatively free word order. There have been
several previous attempts at parsing Hindi as well
as other Indian languages (Bharati et al., 1995,
Bharati et al., 2009b). Many techniques were tried
out recently at the ICON09 dependency parsing
tools contest (Husain, 2009). Both the best per-
forming system (Ambati et al., 2009a) and the sys-
tem in second place (Nivre, 2009b) used a
transition-based approach to dependency parsing,
as implemented in MaltParser (Nivre et al., 2007b).
Other data driven parsing efforts for Indian lan-
guages in the past have been Bharati et al. (2008),
Husain et al. (2009), Mannem et al. (2009b) and
Gadde et al. (2010).
In this paper, we continue to explore the transi-
tion-based approach to Hindi dependency parsing,
building on the state-of-the-art results of Ambati et
al. (2009a) and Nivre (2009b) and exploring the
common pool of features used by those systems.
Through a series of experiments we select features
incrementally to arrive at the best parser features.
The primary purpose of this investigation is to
study the role of different morphosyntactic features
in Hindi dependency parsing, but we also want to
improve the overall parsing accuracy. Our final
results are 76.5% labeled and 91.1% unlabeled at-
tachment score, improving previous results by 2
and 1 percent absolute, respectively. In addition to
this, we also provide an error analysis, isolating
specific linguistic phenomena and/or other factors
that impede the overall parsing performance, and
suggest possible remedies for these problems.
</bodyText>
<note confidence="0.9930065">
Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 94–102,
Los Angeles, California, June 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.906746" genericHeader="method">
2 The Hindi Dependency Treebank
</sectionHeader>
<bodyText confidence="0.9981982">
Hindi is a free word order language with SOV as
the default order. This can be seen in (1), where
(1a) shows the constituents in the default order,
and the remaining examples show some of the
word order variants of (1a).
</bodyText>
<listItem confidence="0.722558875">
(1) a. malaya ne sameer ko kitaba dii.
Malay ERG Sameer DAT book gave
“Malay gave the book to Sameer” (S-IO-DO-V)1
b. malaya ne kitaba sameer ko dii. (S-DO-IO-V)
c. sameer ko malaya ne kitaba dii. (IO-S-DO-V)
d. sameer ko kitaba malaya ne dii. (IO-DO-S-V)
e. kitaba malaya ne sameer ko dii. (DO-S-IO-V)
f. kitaba sameer ko malaya ne dii. (DO-IO-S-V)
</listItem>
<bodyText confidence="0.999458392857143">
Hindi also has a rich case marking system, al-
though case marking is not obligatory. For exam-
ple, in (1), while the subject and indirect object are
explicitly marked for the ergative (ERG) and da-
tive (DAT) cases, the direct object is unmarked for
the accusative.
The Hindi dependency treebank (Begum et al.,
2008) used for the experiment was released as part
of the ICON09 dependency parsing tools contest
(Husain, 2009). The dependency framework (Bha-
rati et al., 1995) used in the treebank is inspired by
Panini’s grammar of Sanskrit. The core labels,
called karakas, are syntactico-semantic relations
that identify the participant in the action denoted
by the verb. For example, in (1), ‘Malay’ is the
agent, ‘book’ is the theme, and ‘Sameer’ is the be-
neficiary in the activity of ‘give’. In the treebank,
these three labels are marked as k1, k2, and k4 re-
spectively. Note, however, that the notion of kara-
ka does not capture the ‘global’ semantics of
thematic roles; rather it captures the elements of
the ‘local semantics’ of a verb, while also taking
cues from the surface level morpho-syntactic in-
formation (Vaidya et al., 2009). The syntactic re-
lational cues (such as case markers) help identify
many of the karakas. In general, the highest availa-
ble karaka,2 if not case-marked, agrees with the
verb in an active sentence. In addition, the tense,
</bodyText>
<footnote confidence="0.810084428571429">
1 S=Subject; IO=Indirect Object; DO=Direct Object;
V=Verb; ERG=Ergative; DAT=Dative
2 These are the karta karaka (k1) and karma karaka (k2). k1
and k2 can be roughly translated as ‘agent’ and ‘theme’ re-
spectively. For a complete description of the tagset and the
dependency scheme, see Begum et al. (2008) and Bharati et al.
(2009a).
</footnote>
<bodyText confidence="0.99959925">
aspect and modality (TAM) marker can many a
times control the case markers that appear on k1.
For example, in (1) ‘Malay’ takes an ergative case
because of the past perfective TAM marker (that
appears as a suffix in this case) of the main verb
‘gave’. Many dependency relations other than ka-
rakas are purely syntactic. These include relations
such as noun modifier (nmod), verb modifier
(vmod), conjunct relation (ccof), etc.
Each sentence is manually chunked and then an-
notated for dependency relations. A chunk is a mi-
nimal, non-recursive structure consisting of
correlated groups of words (Bharati et al., 2006). A
node in a dependency tree represents a chunk head.
Each lexical item in a sentence is also annotated
with its part-of-speech (POS). For all the experi-
ments described in this paper we use gold POS and
chunk tags. Together, a group of lexical items with
some POS tags within a chunk can be utilized to
automatically compute coarse grained morphosyn-
tactic information. For example, such information
can represent the postposition/case-marking in the
case of noun chunks, or it may represent the TAM
information in the case of verb chunks. In the ex-
periments conducted for this paper this local in-
formation is automatically computed and
incorporated as a feature of the head of a chunk.
As we will see later, such information proves to be
extremely crucial during dependency parsing.
For all the experiments discussed in section 4,
the training and development data size was 1500
and 150 sentences respectively. The training and
development data consisted of —22k and —1.7k
words respectively. The test data consisted of 150
sentences (—1.6k words). The average sentence
length is 19.85.
</bodyText>
<sectionHeader confidence="0.969258" genericHeader="method">
3 Transition-Based Dependency Parsing
</sectionHeader>
<bodyText confidence="0.994797">
A transition-based dependency parser is built of
two essential components (Nivre, 2008):
</bodyText>
<listItem confidence="0.99791775">
· A transition system for mapping sentences to
dependency trees
· A classifier for predicting the next transition for
every possible system configuration
</listItem>
<page confidence="0.995511">
95
</page>
<table confidence="0.986703285714286">
PTAG CTAG FORM LEMMA DEPREL CTAM OTHERS
Stack: top 1 5 1 7 9
Input: next 1 5 1 7 9
Input: next+1 2 5 6 7
Input: next+2 2
Input: next+3 2
Stack: top-1 3
String: predecessor of top 3
Tree: head of top 4
Tree: leftmost dep of next 4 5 6
Tree: rightmost dep of top 8
Tree: left sibling of rightmost dep of top 8
Merge: PTAG of top and next 10
Merge: CTAM and DEPREL of top 10
</table>
<tableCaption confidence="0.999865">
Table 1. Feature pool based on selection from Ambati et al. (2009a) and Nivre (2009b).
</tableCaption>
<bodyText confidence="0.999974444444444">
Given these two components, dependency parsing
can be realized as deterministic search through the
transition system, guided by the classifier. With
this technique, parsing can be performed in linear
time for projective dependency trees. Like Ambati
et al. (2009a) and Nivre (2009b), we use MaltPars-
er, an open-source implementation of transition-
based dependency parsing with a variety of transi-
tion systems and customizable classifiers.3
</bodyText>
<subsectionHeader confidence="0.999597">
3.1 Transition System
</subsectionHeader>
<bodyText confidence="0.9999144">
Previous work has shown that the arc-eager projec-
tive transition system first described in Nivre
(2003) works well for Hindi (Ambati et al., 2009a;
Nivre, 2009b). A parser configuration in this sys-
tem contains a stack holding partially processed
tokens, an input buffer containing the remaining
tokens, and a set of arcs representing the partially
built dependency tree. There are four possible tran-
sitions (where top is the token on top of the stack
and next is the next token in the input buffer):
</bodyText>
<listItem confidence="0.9986935">
· Left-Arc(r): Add an arc labeled r from next to
top; pop the stack.
· Right-Arc(r): Add an arc labeled r from top to
next; push next onto the stack.
· Reduce: Pop the stack.
· Shift: Push next onto the stack.
</listItem>
<bodyText confidence="0.9433635">
Although this system can only derive projective
dependency trees, the fact that the trees are labeled
</bodyText>
<footnote confidence="0.485921">
3 MaltParser is available at http://maltparser.org.
</footnote>
<bodyText confidence="0.999676333333333">
allows non-projective dependencies to be captured
using the pseudo-projective parsing technique pro-
posed in Nivre and Nilsson (2005).
</bodyText>
<subsectionHeader confidence="0.998413">
3.2 Classifiers
</subsectionHeader>
<bodyText confidence="0.999781818181818">
Classifiers can be induced from treebank data us-
ing a wide variety of different machine learning
methods, but all experiments reported below use
support vector machines with a polynomial kernel,
as implemented in the LIBSVM package (Chang
and Lin, 2001) included in MaltParser. The task of
the classifier is to map a high-dimensional feature
vector representation of a parser configuration to
the optimal transition out of that configuration. The
features used in our experiments represent the fol-
lowing attributes of input tokens:
</bodyText>
<listItem confidence="0.998834666666667">
· PTAG: POS tag of chunk head.
· CTAG: Chunk tag.
· FORM: Word form of chunk head.
· LEMMA: Lemma of chunk head.
· DEPREL: Dependency relation of chunk.
· CTAM: Case and TAM markers of chunk.
</listItem>
<bodyText confidence="0.999774">
The PTAG corresponds to the POS tag associated
with the head of the chunk, whereas the CTAG
represent the chunk tag. The FORM is the word
form of the chunk head, and the LEMMA is auto-
matically computed with the help of a morphologi-
cal analyzer. CTAM gives the local
morphosyntactic features such as case markers
(postpositions/suffixes) for nominals and TAM
markers for verbs (cf. Section 2).
</bodyText>
<page confidence="0.946569">
96
</page>
<bodyText confidence="0.999969142857143">
The pool of features used in the experiments are
shown in Table 1, where rows denote tokens in a
parser configuration – defined relative to the stack,
the input buffer, the partially built dependency tree
and the input string – and columns correspond to
attributes. Each non-empty cell represents a fea-
ture, and features are numbered for easy reference.
</bodyText>
<sectionHeader confidence="0.991617" genericHeader="method">
4 Feature Selection Experiments
</sectionHeader>
<bodyText confidence="0.999988333333333">
Starting from the union of the feature sets used by
Ambati et al. (2009a and by Nivre (2009b), we
first used 5-fold cross-validation on the combined
training and development sets from the ICON09
tools contest to select the pool of features depicted
in Table 1, keeping all features that had a positive
effect on both labeled and unlabeled accuracy. We
then grouped the features into 10 groups (indicated
by numbers 1–10 in Table 1) and reran the cross-
validation, incrementally adding different feature
groups in order to analyze their impact on parsing
accuracy. The result is shown in Figure 1.
</bodyText>
<figureCaption confidence="0.925131625">
Figure 1. UAS and LAS of experiments 1-10; 5-fold
cross-validation on training and development data of the
ICON09 tools contest.
Experiment 1: Experiment 1 uses a baseline
model with only four basic features: PTAG and
FORM of top and next. This results in a labeled
attachment score (LAS) of 41.7% and an unlabeled
attachment score (UAS) of 68.2%.
</figureCaption>
<bodyText confidence="0.978204843137255">
Experiments 2–3: In experiments 2 and 3, the
PTAG of contextual words of next and top are
added. Of all the contextual words, next+1,
next+2, next+3, top-1 and predecessor of top were
found to be useful.4 Adding these contextual fea-
tures gave a modest improvement to 45.7% LAS
and 72.7% UAS.
Experiment 4: In experiment 4, we used the
PTAG information of nodes in the partially built
tree, more specifically the syntactic head of top
and the leftmost dependent of next. Using these
features gave a large jump in accuracy to 52%
LAS and 76.8% UAS. This is because partial in-
formation is helpful in making future decisions.
For example, a coordinating conjunction can have
a node of any PTAG category as its child. But all
the children should be of same category. Knowing
the PTAG of one child therefore helps in identify-
ing other children as well.
Experiments 5–7: In experiments 5, 6 and 7,
we explored the usefulness of CTAG, FORM, and
LEMMA attributes. These features gave small in-
cremental improvements in accuracy; increasing
LAS to 56.4% and UAS to 78.5%. It is worth not-
ing in particular that the addition of LEMMA
attributes only had a marginal effect on accuracy,
given that it is generally believed that this type of
information should be beneficial for richly in-
flected languages.
Experiment 8: In experiment 8, the DEPREL of
nodes in the partially formed tree is used. The
rightmost child and the left sibling of the rightmost
child of top were found to be useful. This is be-
cause, if we know the dependency label of one of
the children, then the search space for other child-
ren gets reduced. For example, a verb cannot have
more than one k1 or k2. If we know that the parser
has assigned k1 to one of its children, then it
should use different labels for the other children.
The overall effect on parsing accuracy is neverthe-
less very marginal, bringing LAS to 56.5% and
UAS to 78.6%.
Experiment 9: In experiment 9, the CTAM
attribute of top and next is used. This gave by far
the greatest improvement in accuracy with a huge
jump of around 10% in LAS (to 66.3%) and
slightly less in UAS (to 84.7%). Recall that CTAM
consists of two important morphosyntactic fea-
tures, namely, case markers (as suffixes or postpo-
sitions) and TAM markers. These feature help
because (a) case markers are important surface
</bodyText>
<footnote confidence="0.928531666666667">
4 The predecessor of top is the word occurring immediately
before top in the input string, as opposed to top-1, which is the
word immediately below top in the current stack.
</footnote>
<figure confidence="0.981822923076923">
90
84
78
72
66
60
54
48
42
36
30
UAS
LAS
</figure>
<page confidence="0.840339">
97
</page>
<figureCaption confidence="0.999938">
Figure 2. Precision and Recall of some important dependency labels.
</figureCaption>
<bodyText confidence="0.998142243902439">
cues that help identify various dependency rela-
tions, and (b) there exists a direct mapping be-
tween many TAM labels and the nominal case
markers because TAMs control the case markers of
some nominals. As expected, our experiments
show that the parsing decisions are certainly more
accurate after using these features. In particular, (a)
and (b) are incorporated easily in the parsing
process.
In a separate experiment we also added some
other morphological features such as gender, num-
ber and person for each node. Through these fea-
tures we expected to capture the agreement in
Hindi. The verb agrees in gender, number and per-
son with the highest available karaka. However,
incorporating these features did not improve pars-
ing accuracy and hence these features were not
used in the final setting. We will have more to say
about agreement in section 5.
Experiment 10: In experiment 10, finally, we
added conjoined features, where the conjunction of
POS of next and top and of CTAM and DEPREL
of top gave slight improvements. This is because a
child-parent pair type can only take certain labels.
For example, if the child is a noun and the parent is
a verb, then all the dependency labels reflecting
noun, adverb and adjective modifications are not
relevant. Similarly, as noted earlier, certain case-
TAM combinations demand a particular set of la-
bels only. This can be captured by the combination
tried in this experiment.
Experiment 10 gave the best results in the cross-
validation experiments. The settings from this ex-
periment were used to get the final performance on
the test data. Table 2 shows the final results along
with the results of the first and second best per-
forming systems in the ICON09 tools contest. We
see that our system achieved an improvement of 2
percentage points in LAS and 1 percentage point in
UAS over the previous state of the art reported in
Ambati et al. (2009a).
</bodyText>
<table confidence="0.998463">
System LAS UAS
Ambati et al. (2009a) 74.5 90.1
Nivre (2009b) 73.4 89.8
Our system 76.5 91.1
</table>
<tableCaption confidence="0.995463">
Table 2. Final results on the test data from the ICON09
tools contest.
</tableCaption>
<sectionHeader confidence="0.998356" genericHeader="method">
5 Error Analysis
</sectionHeader>
<bodyText confidence="0.99916296">
In this section we provide a detailed error analysis
on the test data and suggest possible remedies for
problems noted. We note here that other than the
reasons mentioned in this section, small treebank
size could be another reason for low accuracy of
the parser. The training data used for the experi-
ments only had ~28.5k words. With recent work on
Hindi Treebanking (Bhatt et al., 2009) we expect
to get more annotated data in the near future.
Figure 2 shows the precision and recall of some
important dependency labels in the test data. The
labels in the treebank are syntacto-semantic in na-
ture. Morph-syntactic features such as case mark-
ers and/or TAM labels help in identifying these
labels correctly. But lack of nominal postpositions
can pose problems. Recall that many case mark-
ings in Hindi are optional. Also recall that the verb
agrees with the highest available karaka. Since
agreement features do not seem to help, if both k1
and k2 lack case markers, k1-k2 disambiguation
becomes difficult (considering that word order in-
formation cannot help in this disambiguation). In
the case of k1 and k2, error rates for instances that
lack post-position markers are 60.9% (14/23) and
65.8% (25/38), respectively.
</bodyText>
<page confidence="0.994244">
98
</page>
<table confidence="0.999682">
Correct Incorrect
k1 k1s k2 pof k7p k7t k7 others
k1 184 5 3 8 3 1 3
k1s 12 6 1 6 1
k2 126 14 1 7 5 11
pof 54 1 8 4
k7p 54 3 7 1 2 3
k7t 27 3 3 3 1 10
k7 3 2 2 4
</table>
<tableCaption confidence="0.988768">
Table 3. Confusion matrix for important labels. The
diagonal under ‘Incorrect’ represents attachment errors.
</tableCaption>
<bodyText confidence="0.999483194444444">
Table 3 shows the confusion matrix for some
important labels in the test data. As the present
information available for disambiguation is not
sufficient, we can make use of some semantics to
resolve these ambiguities. Bharati et al. (2008) and
Ambati et al. (2009b) have shown that this ambi-
guity can be reduced using minimal semantics.
They used six semantic features: human, non-
human, in-animate, time, place and abstract. Using
these features they showed that k1-k2 and k7p-k7t
ambiguities can be resolved to a great extent. Of
course, automatically extracting these semantic
features is in itself a challenging task, although
Øvrelid (2008) has shown that animacy features
can be induced automatically from data.
In section 4 we mentioned that a separate expe-
riment explored the effectiveness of morphological
features like gender, number and person. Counter
to our intuitions, these features did not improve the
overall accuracy. Accuracies on cross-validated
data while using these features were less than the
best results with 66.2% LAS and 84.6% UAS.
Agreement patterns in Hindi are not straightfor-
ward. For example, the verb agrees with k2 if the
k1 has a post-position; it may also sometimes take
the default features. In a passive sentence, the verb
agrees only with k2. The agreement problem wor-
sens when there is coordination or when there is a
complex verb. It is understandable then that the
parser is unable to learn the selective agreement
pattern which needs to be followed. Similar prob-
lems with agreement features have also been noted
by Goldberg and Elhadad (2009).
In the following sections, we analyze the errors
due to different constructions and suggest possible
remedies.
</bodyText>
<subsectionHeader confidence="0.999299">
5.1 Simple Sentences
</subsectionHeader>
<bodyText confidence="0.999646043478261">
A simple sentence is one that has only one main
verb. In these sentences, the root of the dependen-
cy tree is the main verb, which is easily identified
by the parser. The main problem is the correct
identification of the argument structure. Although
the attachments are mostly correct, the dependency
labels are error prone. Unlike in English and other
more configurational languages, one of the main
cues that help us identify the arguments is to be
found in the nominal postpositions. Also, as noted
earlier these postpositions are many times con-
trolled by the TAM labels that appear on the verb.
There are four major reasons for label errors in
simple sentences: (a) absence of postpositions, (b)
ambiguous postpositions, (c) ambiguous TAMs,
and (d) inability of the parser to exploit agreement
features. For example in (2), raama and phala are
arguments of the verb khaata. Neither of them has
any explicit case marker. This makes it difficult for
the parser to identify the correct label for these
nodes. In (3a) and (3b) the case marker se is ambi-
guous. It signifies ‘instrument’ in (3b) and ‘agent’
in (3a).
</bodyText>
<listItem confidence="0.9677865">
(2) raama phala khaata hai
‘Ram’ ‘fruit’ ‘eat’ ‘is’
‘Ram eats a fruit’
(3) a. raama se phala khaayaa nahi gaya
</listItem>
<bodyText confidence="0.9889042">
‘Ram’ INST ‘fruit’ ‘eat’ ’not’ ‘PAST’
‘Ram could not eat the fruit’
b. raama chamach se phala khaata hai
‘Ram’ ‘spoon’ INST ‘fruit’ ‘eat’ ‘is’
‘Ram eats fruit with spoon’
</bodyText>
<subsectionHeader confidence="0.999365">
5.2 Embedded Clauses
</subsectionHeader>
<bodyText confidence="0.99998">
Two major types of embedded constructions in-
volve participles and relative clause constructions.
Participles in Hindi are identified through a set of
TAM markers. In the case of participle embed-
dings, a sentence will have more than one verb,
i.e., at least one participle and the matrix verb.
Both the matrix (finite) verb and the participle can
take their own arguments that can be identified via
the case-TAM mapping discussed earlier. Howev-
er, there are certain syntactic constraints that limit
the type of arguments a participle can take. There
</bodyText>
<page confidence="0.996914">
99
</page>
<bodyText confidence="0.999775">
are two sources of errors here: (a) argument shar-
ing, and (b) ambiguous attachment sites.
Some arguments such as place/time nominals
can be shared. Shared arguments are assigned to
only one verb in the dependency tree. So the task
of identifying the shared arguments, if any, and
attaching them to the correct parent is a complex
task. Note that the dependency labels can be identi-
fied based on the morphosyntactic features. The
task becomes more complex if there is more than
one participle in a sentence. 12 out of 130 in-
stances (9.23%) of shared arguments has an incor-
rect attachment.
Many participles are ambiguous and making the
correct attachment choice is difficult. Similar par-
ticiples, depending on the context, can behave as
adverbials and attach to a verb, or can behave as
adjectives and attach to a noun. Take (4) as a case
in point.
</bodyText>
<listItem confidence="0.971013">
(4) maine daurte hue kutte ko dekhaa
‘I’-ERG (while) running dog ACC ‘saw’
</listItem>
<bodyText confidence="0.9971782">
In (4) based on how one interprets ‘daurte hue’,
one gets either the reading that ‘I saw a running
dog’ or that ‘I saw a dog while running’. In case of
the adjectival participle construction (VJJ), 2 out of
3 errors are due to wrong attachment.
</bodyText>
<subsectionHeader confidence="0.997139">
5.3 Coordination
</subsectionHeader>
<bodyText confidence="0.999974384615385">
Coordination poses problems as it often gives rise
to long-distance dependencies. Moreover, the tree-
bank annotation treats the coordinating conjunction
as the head of the coordinated structure. Therefore,
a coordinating conjunction can potentially become
the root of the entire dependency tree. This is simi-
lar to Prague style dependency annotation (Hajico-
va, 1998). Coordinating conjunctions pose
additional problems in such a scenario as they can
appear as the child of different heads. A coordinat-
ing conjunction takes children of similar POS cat-
egory, but the parent of the conjunction depends on
the type of the children.
</bodyText>
<listItem confidence="0.873783333333333">
(5) a. raama aur shyaama ne khaana khaayaa
‘Ram’ ‘and’ ‘Shyam’ ‘ERG’ ‘food’ ‘ate’
‘Ram and Shyam ate the food.’
</listItem>
<bodyText confidence="0.987601714285714">
b. raama ne khaanaa khaayaa aur paanii
‘Ram’ ‘ERG’ ‘food’ ‘ate’ ‘and’ ‘water’
piyaa
‘drank’
‘Ram ate food and drank water.’
In (5a), raama and shyaama are children of the
coordinating conjunction aur, which gets attached
to the main verb khaayaa with the label k1. In ef-
fect, syntactically aur becomes the argument of the
main verb. In (5b), however, the verbs khaayaa
and piyaa are the children of aur. In this case, aur
becomes the root of the sentence. Identifying the
nature of the conjunction and its children becomes
a challenging task for the parser. Note that the
number of children that a coordinating conjunction
can take is not fixed either. The parser could iden-
tify the correct head of the conjunctions with an
accuracy of 75.7% and the correct children with an
accuracy of 85.7%.
The nature of the conjunction will also affect the
dependency relation it has with its head. For ex-
ample, if the children are nouns, then the conjunc-
tion behaves as a noun and can potentially be an
argument of a verb. By contrast, if the children are
finite verbs, then it behaves as a finite verb and can
become the root of the dependency tree. Unlike
nouns and verbs, however, conjunctions do not
have morphological features. So a child-to-head
feature percolation should help make a coordinat-
ing node more transparent. For example, in (5a) the
Ergative case ne is a strong cue for the dependency
label k1. If we copy this information from one of
its children (here shyaama) to the conjunct, then
the parser can possibly make use of this informa-
tion.
</bodyText>
<subsectionHeader confidence="0.993909">
5.4 Complex Predicates
</subsectionHeader>
<bodyText confidence="0.999943166666667">
Complex predicates are formed by combining a
noun or an adjective with a verbalizer kar or ho.
For instance, in taariif karanaa ‘to praise’, taariif
‘praise’ is a noun and karanaa ‘to do’ is a verb.
Together they form the main verb. Complex predi-
cates are highly productive in Hindi. Combination
of the light verb and the noun/adjective is depen-
dent on not only syntax but also semantics and
therefore its automatic identification is not always
straightforward (Butt, 1995). A noun-verb com-
plex predicate in the treebank is linked via the de-
pendency label pof. The parser makes mistakes in
</bodyText>
<page confidence="0.982099">
100
</page>
<bodyText confidence="0.999972428571429">
identifying pof or misclassifies other labels as pof.
In particular, the confusion is with k2 and k1s
which are object/theme and noun complements of
k1, respectively. These labels share similar contex-
tual features like the nominal element in the verb
complex. Table 3 includes the confusion matrix for
pof errors.
</bodyText>
<subsectionHeader confidence="0.972892">
5.5 Non-Projectivity
</subsectionHeader>
<bodyText confidence="0.999824434782609">
As noted earlier, MaltParser’s arc-eager parsing
algorithm can be combined with the pseudo-
projective parsing techniques proposed in Nivre
and Nilsson (2005), which potentially helps in
identifying non-projective arcs. The Hindi treebank
has ~14% non-projective arcs (Mannem et al.,
2009a). In the test set, there were a total of 11 non-
projective arcs, but the parser did not find any of
them. This is consistent with earlier results show-
ing that pseudo-projective parsing has high preci-
sion but low recall, especially when the percentage
of non-projective relations is small (Nilsson et al,
2007).
Non-projectivity has proven to be one of the ma-
jor problems in dependency parsing, especially for
free word-order languages. In Hindi, the majority
of non-projective arcs are inter-clausal (Mannem et
al., 2009a), involving conjunctions and relative
clauses. There have been some attempts at han-
dling inter-clausal non-projectivity in Hindi. Hu-
sain et al. (2009) proposed a two-stage approach
that can handle some of the inter-clausal non-
projective structures.
</bodyText>
<subsectionHeader confidence="0.91327">
5.6 Long-Distance Dependencies
</subsectionHeader>
<bodyText confidence="0.999879666666667">
Previous results on parsing other languages have
shown that MaltParser has lower accuracy on long-
distance dependencies. Our results confirm this.
Errors in the case of relative clauses and coordina-
tion can mainly be explained in this way. For ex-
ample, there are 8 instances of relative clauses in
the test data. The system could identify only 2 of
them correctly. These two are at a distance of 1
from its parent. For the remaining 6 instances the
distance to the parent of the relative clause ranges
from 4 to 12.
Figure 3 shows how parser performance de-
creases with increasing distance between the head
and the dependent. Recently, Husain et al. (2009)
have proposed a two-stage setup to parse inter-
clausal and intra-clausal dependencies separately.
They have shown that most long distance relations
are inter-clausal, and therefore, using such a clause
motivated parsing setup helps in maximizing both
short distance and long distance dependency accu-
racy. In a similar spirit, Gadde et al. (2010) showed
that using clausal features helps in identifying long
distance dependencies. They have shown that pro-
viding clause information in the form of clause
boundaries and clausal heads can help a parser
make better predictions about long distance depen-
dencies.
</bodyText>
<subsectionHeader confidence="0.6443475">
Dependency Length
Dependency Length
</subsectionHeader>
<figureCaption confidence="0.964105">
Figure 3. Dependency arc precision/recall relative to
dependency length, where the length of a dependency
from wi to wj is |i-j |and roots are assumed to have dis-
tance 0 to their head.
</figureCaption>
<sectionHeader confidence="0.998493" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998940384615385">
In this paper we have analyzed the importance of
different linguistic features in data-driven parsing
of Hindi and at the same time improved the state of
the art. Our main finding is that the combination of
case markers on nominals with TAM markers on
verbs is crucially important for syntactic disambig-
uation, while the inclusion of features such as per-
son, number gender that help in agreement has not
yet resulted in any improvement. We have also
presented a detailed error analysis and discussed
possible techniques targeting different error
classes. We plan to use these techniques to im-
prove our results in the near future.
</bodyText>
<figure confidence="0.999450266666667">
Dependency Precision
100
40
80
60
20
0
0 1 2 3 4 5 6 7 8 9 10 11 12
Dependency Recall
100
40
80
60
20
0
</figure>
<page confidence="0.989673">
101
</page>
<sectionHeader confidence="0.989282" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999908205607477">
B. R. Ambati, P. Gadde, and K. Jindal. 2009a. Experi-
ments in Indian Language Dependency Parsing.
Proc. of ICON09 NLP Tools Contest: Indian Lan-
guage Dependency Parsing, 32-37.
B. R. Ambati, P. Gade, C. GSK and S. Husain. 2009b.
Effect of Minimal Semantics on Dependency Pars-
ing. Proc. of RANLP Student Research Workshop.
R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and
R. Sangal. 2008. Dependency annotation scheme for
Indian languages. Proc. of IJCNLP.
A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural
Language Processing: A Paninian Perspective, Pren-
tice-Hall of India, New Delhi.
A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma,
and R. Sangal. 2008. Two semantic features make all
the difference in parsing accuracy. Proc. of ICON.
A. Bharati, R. Sangal, D. M. Sharma and L. Bai. 2006.
AnnCorra: Annotating Corpora Guidelines for POS
and Chunk Annotation for Indian Languages. Tech-
nical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad.
A. Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begam
and R. Sangal. 2009a. AnnCorra: TreeBanks for In-
dian Languages, Guidelines for Annotating Hindi
TreeBank.
http://ltrc.iiit.ac.in/MachineTrans/research/tb/DS-
guidelines/DS-guidelines-ver2-28-05-09.pdf
A. Bharati, S. Husain, D. M. Sharma and R. Sangal.
2009b. Two stage constraint based hybrid approach
to free word order language dependency parsing. In
Proc. of IWPT.
R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D.
M. Sharma and F. Xia. 2009. Multi-Representational
and Multi-Layered Treebank for Hindi/Urdu. Proc.
of the Third LAW at ACL-IJCNLP, 186-189.
M. Butt. 1995. The Structure of Complex Predicates in
Urdu. CSLI Publications.
G. Eryigit, J. Nivre, and K. Oflazer. 2008. Dependency
Parsing of Turkish. Computational Linguistics 34(3),
357-389.
P. Gadde, K. Jindal, S. Husain, D. M. Sharma, and R.
Sangal. 2010. Improving Data Driven Dependency
Parsing using Clausal Information. Proc. of NAACL-
HLT.
Y. Goldberg and M. Elhadad. 2009. Hebrew Dependen-
cy Parsing: Initial Results. Proc. of IWPT, 129-133.
E. Hajicova. 1998. Prague Dependency Treebank: From
Analytic to Tectogrammatical Annotation. Proc. of
TSD.
J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M.
Nilsson and M. Saers. 2007. Single Malt or Blended?
A Study in Multilingual Parser Optimization. Proc.
of EMNLP-CoNLL, 933-939.
S. Husain. 2009. Dependency Parsers for Indian Lan-
guages. Proc. of ICON09 NLP Tools Contest: Indian
Language Dependency Parsing.
S. Husain, P. Gadde, B. Ambati, D. M. Sharma and R.
Sangal. 2009. A modular cascaded approach to com-
plete parsing. Proc. of the COLIPS International
Conference on Asian Language Processing.
P. Mannem, H. Chaudhry, and A. Bharati. 2009a. In-
sights into non-projectivity in Hindi. Proc. of ACL-
IJCNLP Student Research Workshop.
P. Mannem, A. Abhilash and A. Bharati. 2009b. LTAG-
spinal Treebank and Parser for Hindi. Proceedings of
International Conference on NLP, Hyderabad. 2009.
R. McDonald, K. Lerman, and F. Pereira. 2006. Multi-
lingual dependency analysis with a two-stage discri-
minative parser. Proc. of CoNLL, 216-220.
R. McDonald and J. Nivre. 2007. Characterizing the
errors of data-driven dependency parsing models.
Proc. of EMNLP-CoNLL, 122-131.
I. A. Mel&apos;Cuk. 1988. Dependency Syntax: Theory and
Practice, State University Press of New York.
J. Nilsson, J. Nivre and J. Hall. 2007. Generalizing Tree
Transformations for Inductive Dependency Parsing.
Proc. of ACL, 968-975.
J. Nivre. 2008. Algorithms for Deterministic Incremen-
tal Dependency Parsing. Computational Linguistics
34(4), 513-553.
J. Nivre and R. McDonald. 2008. Integrating graph-
based and transition-based dependency parsers. In
Proceedings of ACL-HLT, pp. 950-958.
J. Nivre. 2009a. Non-Projective Dependency Parsing in
Expected Linear Time. Proc. of ACL-IJCNLP, 351-
359.
J. Nivre. 2009b. Parsing Indian Languages with Malt-
Parser. Proc. of ICON09 NLP Tools Contest: Indian
Language Dependency Parsing, 12-18.
J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S.
Riedel and D. Yuret. 2007a. The CoNLL 2007
Shared Task on Dependency Parsing. Proc. of
EMNLP/CoNLL, 915-932.
J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S.
Kübler, S. Marinov and E Marsi. 2007b. MaltParser:
A language-independent system for data-driven de-
pendency parsing. NLE, 13(2), 95-135.
L. Øvrelid. 2008. Argument Differentiation. Soft con-
straints and data-driven models. PhD Thesis, Uni-
versity of Gothenburg.
D. Seddah, M. Candito and B. Crabbé. 2009. Cross
parser evaluation: a French Treebanks study. Proc. of
IWPT, 150-161.
R. Tsarfaty and K. Sima&apos;an. 2008. Relational-
Realizational Parsing. Proc. of CoLing, 889-896.
A. Vaidya, S. Husain, P. Mannem, and D. M. Sharma.
2009. A karaka-based dependency annotation scheme
for English. Proc. of CICLing, 41-52.
</reference>
<page confidence="0.998625">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.708138">
<title confidence="0.999985">On the Role of Morphosyntactic Features in Hindi Dependency Parsing</title>
<author confidence="0.989345">Ram Ambati</author>
<author confidence="0.989345">Samar Husain</author>
<author confidence="0.989345">Nivre† Rajeev</author>
<affiliation confidence="0.9365525">Language Technologies Research Centre, IIIT-Hyderabad, †Department of Linguistics and Philology, Uppsala University,</affiliation>
<email confidence="0.912889">joakim.nivre@lingfil.uu.se,gal@mail.iiit.ac.in</email>
<abstract confidence="0.998408">This paper analyzes the relative importance of different linguistic features for data-driven dependency parsing of Hindi, using a feature pool derived from two state-of-the-art parsers. The analysis shows that the greatest gain in accuracy comes from the addition of morphosyntactic features related to case, tense, aspect and modality. Combining features from the two parsers, we achieve a labeled attachment score of 76.5%, which is 2 percentage points better than the previous state of the art. We finally provide a detailed error analysis and suggest possible improvements to the parsing scheme.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>B R Ambati</author>
<author>P Gadde</author>
<author>K Jindal</author>
</authors>
<title>Experiments in Indian Language Dependency Parsing.</title>
<date>2009</date>
<booktitle>Proc. of ICON09 NLP Tools Contest: Indian Language Dependency Parsing,</booktitle>
<pages>32--37</pages>
<contexts>
<context position="2615" citStr="Ambati et al., 2009" startWordPosition="384" endWordPosition="387">uracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experime</context>
<context position="8755" citStr="Ambati et al. (2009" startWordPosition="1404" endWordPosition="1407">omponents (Nivre, 2008): · A transition system for mapping sentences to dependency trees · A classifier for predicting the next transition for every possible system configuration 95 PTAG CTAG FORM LEMMA DEPREL CTAM OTHERS Stack: top 1 5 1 7 9 Input: next 1 5 1 7 9 Input: next+1 2 5 6 7 Input: next+2 2 Input: next+3 2 Stack: top-1 3 String: predecessor of top 3 Tree: head of top 4 Tree: leftmost dep of next 4 5 6 Tree: rightmost dep of top 8 Tree: left sibling of rightmost dep of top 8 Merge: PTAG of top and next 10 Merge: CTAM and DEPREL of top 10 Table 1. Feature pool based on selection from Ambati et al. (2009a) and Nivre (2009b). Given these two components, dependency parsing can be realized as deterministic search through the transition system, guided by the classifier. With this technique, parsing can be performed in linear time for projective dependency trees. Like Ambati et al. (2009a) and Nivre (2009b), we use MaltParser, an open-source implementation of transitionbased dependency parsing with a variety of transition systems and customizable classifiers.3 3.1 Transition System Previous work has shown that the arc-eager projective transition system first described in Nivre (2003) works well fo</context>
<context position="11829" citStr="Ambati et al. (2009" startWordPosition="1907" endWordPosition="1910"> analyzer. CTAM gives the local morphosyntactic features such as case markers (postpositions/suffixes) for nominals and TAM markers for verbs (cf. Section 2). 96 The pool of features used in the experiments are shown in Table 1, where rows denote tokens in a parser configuration – defined relative to the stack, the input buffer, the partially built dependency tree and the input string – and columns correspond to attributes. Each non-empty cell represents a feature, and features are numbered for easy reference. 4 Feature Selection Experiments Starting from the union of the feature sets used by Ambati et al. (2009a and by Nivre (2009b), we first used 5-fold cross-validation on the combined training and development sets from the ICON09 tools contest to select the pool of features depicted in Table 1, keeping all features that had a positive effect on both labeled and unlabeled accuracy. We then grouped the features into 10 groups (indicated by numbers 1–10 in Table 1) and reran the crossvalidation, incrementally adding different feature groups in order to analyze their impact on parsing accuracy. The result is shown in Figure 1. Figure 1. UAS and LAS of experiments 1-10; 5-fold cross-validation on train</context>
<context position="17194" citStr="Ambati et al. (2009" startWordPosition="2837" endWordPosition="2840"> noted earlier, certain caseTAM combinations demand a particular set of labels only. This can be captured by the combination tried in this experiment. Experiment 10 gave the best results in the crossvalidation experiments. The settings from this experiment were used to get the final performance on the test data. Table 2 shows the final results along with the results of the first and second best performing systems in the ICON09 tools contest. We see that our system achieved an improvement of 2 percentage points in LAS and 1 percentage point in UAS over the previous state of the art reported in Ambati et al. (2009a). System LAS UAS Ambati et al. (2009a) 74.5 90.1 Nivre (2009b) 73.4 89.8 Our system 76.5 91.1 Table 2. Final results on the test data from the ICON09 tools contest. 5 Error Analysis In this section we provide a detailed error analysis on the test data and suggest possible remedies for problems noted. We note here that other than the reasons mentioned in this section, small treebank size could be another reason for low accuracy of the parser. The training data used for the experiments only had ~28.5k words. With recent work on Hindi Treebanking (Bhatt et al., 2009) we expect to get more annot</context>
<context position="19133" citStr="Ambati et al. (2009" startWordPosition="3186" endWordPosition="3189">hat lack post-position markers are 60.9% (14/23) and 65.8% (25/38), respectively. 98 Correct Incorrect k1 k1s k2 pof k7p k7t k7 others k1 184 5 3 8 3 1 3 k1s 12 6 1 6 1 k2 126 14 1 7 5 11 pof 54 1 8 4 k7p 54 3 7 1 2 3 k7t 27 3 3 3 1 10 k7 3 2 2 4 Table 3. Confusion matrix for important labels. The diagonal under ‘Incorrect’ represents attachment errors. Table 3 shows the confusion matrix for some important labels in the test data. As the present information available for disambiguation is not sufficient, we can make use of some semantics to resolve these ambiguities. Bharati et al. (2008) and Ambati et al. (2009b) have shown that this ambiguity can be reduced using minimal semantics. They used six semantic features: human, nonhuman, in-animate, time, place and abstract. Using these features they showed that k1-k2 and k7p-k7t ambiguities can be resolved to a great extent. Of course, automatically extracting these semantic features is in itself a challenging task, although Øvrelid (2008) has shown that animacy features can be induced automatically from data. In section 4 we mentioned that a separate experiment explored the effectiveness of morphological features like gender, number and person. Counter </context>
</contexts>
<marker>Ambati, Gadde, Jindal, 2009</marker>
<rawString>B. R. Ambati, P. Gadde, and K. Jindal. 2009a. Experiments in Indian Language Dependency Parsing. Proc. of ICON09 NLP Tools Contest: Indian Language Dependency Parsing, 32-37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B R Ambati</author>
<author>P Gade</author>
<author>C GSK</author>
<author>S Husain</author>
</authors>
<date>2009</date>
<booktitle>Effect of Minimal Semantics on Dependency Parsing. Proc. of RANLP Student Research Workshop.</booktitle>
<contexts>
<context position="2615" citStr="Ambati et al., 2009" startWordPosition="384" endWordPosition="387">uracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experime</context>
<context position="8755" citStr="Ambati et al. (2009" startWordPosition="1404" endWordPosition="1407">omponents (Nivre, 2008): · A transition system for mapping sentences to dependency trees · A classifier for predicting the next transition for every possible system configuration 95 PTAG CTAG FORM LEMMA DEPREL CTAM OTHERS Stack: top 1 5 1 7 9 Input: next 1 5 1 7 9 Input: next+1 2 5 6 7 Input: next+2 2 Input: next+3 2 Stack: top-1 3 String: predecessor of top 3 Tree: head of top 4 Tree: leftmost dep of next 4 5 6 Tree: rightmost dep of top 8 Tree: left sibling of rightmost dep of top 8 Merge: PTAG of top and next 10 Merge: CTAM and DEPREL of top 10 Table 1. Feature pool based on selection from Ambati et al. (2009a) and Nivre (2009b). Given these two components, dependency parsing can be realized as deterministic search through the transition system, guided by the classifier. With this technique, parsing can be performed in linear time for projective dependency trees. Like Ambati et al. (2009a) and Nivre (2009b), we use MaltParser, an open-source implementation of transitionbased dependency parsing with a variety of transition systems and customizable classifiers.3 3.1 Transition System Previous work has shown that the arc-eager projective transition system first described in Nivre (2003) works well fo</context>
<context position="11829" citStr="Ambati et al. (2009" startWordPosition="1907" endWordPosition="1910"> analyzer. CTAM gives the local morphosyntactic features such as case markers (postpositions/suffixes) for nominals and TAM markers for verbs (cf. Section 2). 96 The pool of features used in the experiments are shown in Table 1, where rows denote tokens in a parser configuration – defined relative to the stack, the input buffer, the partially built dependency tree and the input string – and columns correspond to attributes. Each non-empty cell represents a feature, and features are numbered for easy reference. 4 Feature Selection Experiments Starting from the union of the feature sets used by Ambati et al. (2009a and by Nivre (2009b), we first used 5-fold cross-validation on the combined training and development sets from the ICON09 tools contest to select the pool of features depicted in Table 1, keeping all features that had a positive effect on both labeled and unlabeled accuracy. We then grouped the features into 10 groups (indicated by numbers 1–10 in Table 1) and reran the crossvalidation, incrementally adding different feature groups in order to analyze their impact on parsing accuracy. The result is shown in Figure 1. Figure 1. UAS and LAS of experiments 1-10; 5-fold cross-validation on train</context>
<context position="17194" citStr="Ambati et al. (2009" startWordPosition="2837" endWordPosition="2840"> noted earlier, certain caseTAM combinations demand a particular set of labels only. This can be captured by the combination tried in this experiment. Experiment 10 gave the best results in the crossvalidation experiments. The settings from this experiment were used to get the final performance on the test data. Table 2 shows the final results along with the results of the first and second best performing systems in the ICON09 tools contest. We see that our system achieved an improvement of 2 percentage points in LAS and 1 percentage point in UAS over the previous state of the art reported in Ambati et al. (2009a). System LAS UAS Ambati et al. (2009a) 74.5 90.1 Nivre (2009b) 73.4 89.8 Our system 76.5 91.1 Table 2. Final results on the test data from the ICON09 tools contest. 5 Error Analysis In this section we provide a detailed error analysis on the test data and suggest possible remedies for problems noted. We note here that other than the reasons mentioned in this section, small treebank size could be another reason for low accuracy of the parser. The training data used for the experiments only had ~28.5k words. With recent work on Hindi Treebanking (Bhatt et al., 2009) we expect to get more annot</context>
<context position="19133" citStr="Ambati et al. (2009" startWordPosition="3186" endWordPosition="3189">hat lack post-position markers are 60.9% (14/23) and 65.8% (25/38), respectively. 98 Correct Incorrect k1 k1s k2 pof k7p k7t k7 others k1 184 5 3 8 3 1 3 k1s 12 6 1 6 1 k2 126 14 1 7 5 11 pof 54 1 8 4 k7p 54 3 7 1 2 3 k7t 27 3 3 3 1 10 k7 3 2 2 4 Table 3. Confusion matrix for important labels. The diagonal under ‘Incorrect’ represents attachment errors. Table 3 shows the confusion matrix for some important labels in the test data. As the present information available for disambiguation is not sufficient, we can make use of some semantics to resolve these ambiguities. Bharati et al. (2008) and Ambati et al. (2009b) have shown that this ambiguity can be reduced using minimal semantics. They used six semantic features: human, nonhuman, in-animate, time, place and abstract. Using these features they showed that k1-k2 and k7p-k7t ambiguities can be resolved to a great extent. Of course, automatically extracting these semantic features is in itself a challenging task, although Øvrelid (2008) has shown that animacy features can be induced automatically from data. In section 4 we mentioned that a separate experiment explored the effectiveness of morphological features like gender, number and person. Counter </context>
</contexts>
<marker>Ambati, Gade, GSK, Husain, 2009</marker>
<rawString>B. R. Ambati, P. Gade, C. GSK and S. Husain. 2009b. Effect of Minimal Semantics on Dependency Parsing. Proc. of RANLP Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Begum</author>
<author>S Husain</author>
<author>A Dhwaj</author>
<author>D Sharma</author>
<author>L Bai</author>
<author>R Sangal</author>
</authors>
<title>Dependency annotation scheme for Indian languages.</title>
<date>2008</date>
<booktitle>Proc. of IJCNLP.</booktitle>
<contexts>
<context position="4956" citStr="Begum et al., 2008" startWordPosition="762" endWordPosition="765">y ERG Sameer DAT book gave “Malay gave the book to Sameer” (S-IO-DO-V)1 b. malaya ne kitaba sameer ko dii. (S-DO-IO-V) c. sameer ko malaya ne kitaba dii. (IO-S-DO-V) d. sameer ko kitaba malaya ne dii. (IO-DO-S-V) e. kitaba malaya ne sameer ko dii. (DO-S-IO-V) f. kitaba sameer ko malaya ne dii. (DO-IO-S-V) Hindi also has a rich case marking system, although case marking is not obligatory. For example, in (1), while the subject and indirect object are explicitly marked for the ergative (ERG) and dative (DAT) cases, the direct object is unmarked for the accusative. The Hindi dependency treebank (Begum et al., 2008) used for the experiment was released as part of the ICON09 dependency parsing tools contest (Husain, 2009). The dependency framework (Bharati et al., 1995) used in the treebank is inspired by Panini’s grammar of Sanskrit. The core labels, called karakas, are syntactico-semantic relations that identify the participant in the action denoted by the verb. For example, in (1), ‘Malay’ is the agent, ‘book’ is the theme, and ‘Sameer’ is the beneficiary in the activity of ‘give’. In the treebank, these three labels are marked as k1, k2, and k4 respectively. Note, however, that the notion of karaka do</context>
<context position="6303" citStr="Begum et al. (2008)" startWordPosition="985" endWordPosition="988">le also taking cues from the surface level morpho-syntactic information (Vaidya et al., 2009). The syntactic relational cues (such as case markers) help identify many of the karakas. In general, the highest available karaka,2 if not case-marked, agrees with the verb in an active sentence. In addition, the tense, 1 S=Subject; IO=Indirect Object; DO=Direct Object; V=Verb; ERG=Ergative; DAT=Dative 2 These are the karta karaka (k1) and karma karaka (k2). k1 and k2 can be roughly translated as ‘agent’ and ‘theme’ respectively. For a complete description of the tagset and the dependency scheme, see Begum et al. (2008) and Bharati et al. (2009a). aspect and modality (TAM) marker can many a times control the case markers that appear on k1. For example, in (1) ‘Malay’ takes an ergative case because of the past perfective TAM marker (that appears as a suffix in this case) of the main verb ‘gave’. Many dependency relations other than karakas are purely syntactic. These include relations such as noun modifier (nmod), verb modifier (vmod), conjunct relation (ccof), etc. Each sentence is manually chunked and then annotated for dependency relations. A chunk is a minimal, non-recursive structure consisting of correl</context>
</contexts>
<marker>Begum, Husain, Dhwaj, Sharma, Bai, Sangal, 2008</marker>
<rawString>R. Begum, S. Husain, A. Dhwaj, D. Sharma, L. Bai, and R. Sangal. 2008. Dependency annotation scheme for Indian languages. Proc. of IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bharati</author>
<author>V Chaitanya</author>
<author>R Sangal</author>
</authors>
<title>Natural Language Processing: A Paninian Perspective, Prentice-Hall of India,</title>
<date>1995</date>
<location>New Delhi.</location>
<contexts>
<context position="2434" citStr="Bharati et al., 1995" startWordPosition="355" endWordPosition="358">ode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be 94 a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsi</context>
<context position="5112" citStr="Bharati et al., 1995" startWordPosition="786" endWordPosition="790">(IO-S-DO-V) d. sameer ko kitaba malaya ne dii. (IO-DO-S-V) e. kitaba malaya ne sameer ko dii. (DO-S-IO-V) f. kitaba sameer ko malaya ne dii. (DO-IO-S-V) Hindi also has a rich case marking system, although case marking is not obligatory. For example, in (1), while the subject and indirect object are explicitly marked for the ergative (ERG) and dative (DAT) cases, the direct object is unmarked for the accusative. The Hindi dependency treebank (Begum et al., 2008) used for the experiment was released as part of the ICON09 dependency parsing tools contest (Husain, 2009). The dependency framework (Bharati et al., 1995) used in the treebank is inspired by Panini’s grammar of Sanskrit. The core labels, called karakas, are syntactico-semantic relations that identify the participant in the action denoted by the verb. For example, in (1), ‘Malay’ is the agent, ‘book’ is the theme, and ‘Sameer’ is the beneficiary in the activity of ‘give’. In the treebank, these three labels are marked as k1, k2, and k4 respectively. Note, however, that the notion of karaka does not capture the ‘global’ semantics of thematic roles; rather it captures the elements of the ‘local semantics’ of a verb, while also taking cues from the</context>
</contexts>
<marker>Bharati, Chaitanya, Sangal, 1995</marker>
<rawString>A. Bharati, V. Chaitanya and R. Sangal. 1995. Natural Language Processing: A Paninian Perspective, Prentice-Hall of India, New Delhi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bharati</author>
<author>S Husain</author>
<author>B Ambati</author>
<author>S Jain</author>
<author>D Sharma</author>
<author>R Sangal</author>
</authors>
<title>Two semantic features make all the difference in parsing accuracy.</title>
<date>2008</date>
<booktitle>Proc. of ICON.</booktitle>
<contexts>
<context position="2870" citStr="Bharati et al. (2008)" startWordPosition="426" endWordPosition="429">nguage with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experiments we select features incrementally to arrive at the best parser features. The primary purpose of this investigation is to study the role of different morphosyntactic features in Hindi dependency parsing, but we also want to improve the overall parsing a</context>
<context position="19109" citStr="Bharati et al. (2008)" startWordPosition="3181" endWordPosition="3184">rror rates for instances that lack post-position markers are 60.9% (14/23) and 65.8% (25/38), respectively. 98 Correct Incorrect k1 k1s k2 pof k7p k7t k7 others k1 184 5 3 8 3 1 3 k1s 12 6 1 6 1 k2 126 14 1 7 5 11 pof 54 1 8 4 k7p 54 3 7 1 2 3 k7t 27 3 3 3 1 10 k7 3 2 2 4 Table 3. Confusion matrix for important labels. The diagonal under ‘Incorrect’ represents attachment errors. Table 3 shows the confusion matrix for some important labels in the test data. As the present information available for disambiguation is not sufficient, we can make use of some semantics to resolve these ambiguities. Bharati et al. (2008) and Ambati et al. (2009b) have shown that this ambiguity can be reduced using minimal semantics. They used six semantic features: human, nonhuman, in-animate, time, place and abstract. Using these features they showed that k1-k2 and k7p-k7t ambiguities can be resolved to a great extent. Of course, automatically extracting these semantic features is in itself a challenging task, although Øvrelid (2008) has shown that animacy features can be induced automatically from data. In section 4 we mentioned that a separate experiment explored the effectiveness of morphological features like gender, num</context>
</contexts>
<marker>Bharati, Husain, Ambati, Jain, Sharma, Sangal, 2008</marker>
<rawString>A. Bharati, S. Husain, B. Ambati, S. Jain, D. Sharma, and R. Sangal. 2008. Two semantic features make all the difference in parsing accuracy. Proc. of ICON.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bharati</author>
<author>R Sangal</author>
<author>D M Sharma</author>
<author>L Bai</author>
</authors>
<title>AnnCorra: Annotating Corpora Guidelines for POS and Chunk Annotation for Indian Languages.</title>
<date>2006</date>
<tech>Technical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad.</tech>
<contexts>
<context position="6946" citStr="Bharati et al., 2006" startWordPosition="1091" endWordPosition="1094">009a). aspect and modality (TAM) marker can many a times control the case markers that appear on k1. For example, in (1) ‘Malay’ takes an ergative case because of the past perfective TAM marker (that appears as a suffix in this case) of the main verb ‘gave’. Many dependency relations other than karakas are purely syntactic. These include relations such as noun modifier (nmod), verb modifier (vmod), conjunct relation (ccof), etc. Each sentence is manually chunked and then annotated for dependency relations. A chunk is a minimal, non-recursive structure consisting of correlated groups of words (Bharati et al., 2006). A node in a dependency tree represents a chunk head. Each lexical item in a sentence is also annotated with its part-of-speech (POS). For all the experiments described in this paper we use gold POS and chunk tags. Together, a group of lexical items with some POS tags within a chunk can be utilized to automatically compute coarse grained morphosyntactic information. For example, such information can represent the postposition/case-marking in the case of noun chunks, or it may represent the TAM information in the case of verb chunks. In the experiments conducted for this paper this local infor</context>
</contexts>
<marker>Bharati, Sangal, Sharma, Bai, 2006</marker>
<rawString>A. Bharati, R. Sangal, D. M. Sharma and L. Bai. 2006. AnnCorra: Annotating Corpora Guidelines for POS and Chunk Annotation for Indian Languages. Technical Report (TR-LTRC-31), LTRC, IIIT-Hyderabad.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bharati</author>
<author>D M Sharma</author>
<author>S Husain</author>
<author>L Bai</author>
<author>R Begam</author>
<author>R Sangal</author>
</authors>
<title>AnnCorra: TreeBanks for Indian Languages, Guidelines for Annotating Hindi TreeBank.</title>
<date>2009</date>
<pages>2--28</pages>
<contexts>
<context position="2456" citStr="Bharati et al., 2009" startWordPosition="359" endWordPosition="362">vant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be 94 a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the st</context>
<context position="6328" citStr="Bharati et al. (2009" startWordPosition="990" endWordPosition="993"> the surface level morpho-syntactic information (Vaidya et al., 2009). The syntactic relational cues (such as case markers) help identify many of the karakas. In general, the highest available karaka,2 if not case-marked, agrees with the verb in an active sentence. In addition, the tense, 1 S=Subject; IO=Indirect Object; DO=Direct Object; V=Verb; ERG=Ergative; DAT=Dative 2 These are the karta karaka (k1) and karma karaka (k2). k1 and k2 can be roughly translated as ‘agent’ and ‘theme’ respectively. For a complete description of the tagset and the dependency scheme, see Begum et al. (2008) and Bharati et al. (2009a). aspect and modality (TAM) marker can many a times control the case markers that appear on k1. For example, in (1) ‘Malay’ takes an ergative case because of the past perfective TAM marker (that appears as a suffix in this case) of the main verb ‘gave’. Many dependency relations other than karakas are purely syntactic. These include relations such as noun modifier (nmod), verb modifier (vmod), conjunct relation (ccof), etc. Each sentence is manually chunked and then annotated for dependency relations. A chunk is a minimal, non-recursive structure consisting of correlated groups of words (Bha</context>
</contexts>
<marker>Bharati, Sharma, Husain, Bai, Begam, Sangal, 2009</marker>
<rawString>A. Bharati, D. M. Sharma, S. Husain, L. Bai, R. Begam and R. Sangal. 2009a. AnnCorra: TreeBanks for Indian Languages, Guidelines for Annotating Hindi TreeBank. http://ltrc.iiit.ac.in/MachineTrans/research/tb/DSguidelines/DS-guidelines-ver2-28-05-09.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Bharati</author>
<author>S Husain</author>
<author>D M Sharma</author>
<author>R Sangal</author>
</authors>
<title>Two stage constraint based hybrid approach to free word order language dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of</booktitle>
<pages>186--189</pages>
<contexts>
<context position="2456" citStr="Bharati et al., 2009" startWordPosition="359" endWordPosition="362">vant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be 94 a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the st</context>
<context position="6328" citStr="Bharati et al. (2009" startWordPosition="990" endWordPosition="993"> the surface level morpho-syntactic information (Vaidya et al., 2009). The syntactic relational cues (such as case markers) help identify many of the karakas. In general, the highest available karaka,2 if not case-marked, agrees with the verb in an active sentence. In addition, the tense, 1 S=Subject; IO=Indirect Object; DO=Direct Object; V=Verb; ERG=Ergative; DAT=Dative 2 These are the karta karaka (k1) and karma karaka (k2). k1 and k2 can be roughly translated as ‘agent’ and ‘theme’ respectively. For a complete description of the tagset and the dependency scheme, see Begum et al. (2008) and Bharati et al. (2009a). aspect and modality (TAM) marker can many a times control the case markers that appear on k1. For example, in (1) ‘Malay’ takes an ergative case because of the past perfective TAM marker (that appears as a suffix in this case) of the main verb ‘gave’. Many dependency relations other than karakas are purely syntactic. These include relations such as noun modifier (nmod), verb modifier (vmod), conjunct relation (ccof), etc. Each sentence is manually chunked and then annotated for dependency relations. A chunk is a minimal, non-recursive structure consisting of correlated groups of words (Bha</context>
</contexts>
<marker>Bharati, Husain, Sharma, Sangal, 2009</marker>
<rawString>A. Bharati, S. Husain, D. M. Sharma and R. Sangal. 2009b. Two stage constraint based hybrid approach to free word order language dependency parsing. In Proc. of IWPT. R. Bhatt, B. Narasimhan, M. Palmer, O. Rambow, D. M. Sharma and F. Xia. 2009. Multi-Representational and Multi-Layered Treebank for Hindi/Urdu. Proc. of the Third LAW at ACL-IJCNLP, 186-189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Butt</author>
</authors>
<title>The Structure of Complex Predicates in Urdu.</title>
<date>1995</date>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="26524" citStr="Butt, 1995" startWordPosition="4427" endWordPosition="4428">rom one of its children (here shyaama) to the conjunct, then the parser can possibly make use of this information. 5.4 Complex Predicates Complex predicates are formed by combining a noun or an adjective with a verbalizer kar or ho. For instance, in taariif karanaa ‘to praise’, taariif ‘praise’ is a noun and karanaa ‘to do’ is a verb. Together they form the main verb. Complex predicates are highly productive in Hindi. Combination of the light verb and the noun/adjective is dependent on not only syntax but also semantics and therefore its automatic identification is not always straightforward (Butt, 1995). A noun-verb complex predicate in the treebank is linked via the dependency label pof. The parser makes mistakes in 100 identifying pof or misclassifies other labels as pof. In particular, the confusion is with k2 and k1s which are object/theme and noun complements of k1, respectively. These labels share similar contextual features like the nominal element in the verb complex. Table 3 includes the confusion matrix for pof errors. 5.5 Non-Projectivity As noted earlier, MaltParser’s arc-eager parsing algorithm can be combined with the pseudoprojective parsing techniques proposed in Nivre and Ni</context>
</contexts>
<marker>Butt, 1995</marker>
<rawString>M. Butt. 1995. The Structure of Complex Predicates in Urdu. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Eryigit</author>
<author>J Nivre</author>
<author>K Oflazer</author>
</authors>
<title>Dependency Parsing of Turkish.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<volume>34</volume>
<issue>3</issue>
<pages>357--389</pages>
<contexts>
<context position="1561" citStr="Eryigit et al., 2008" startWordPosition="215" endWordPosition="218"> to the parsing scheme. 1 Introduction The dependency parsing community has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages has not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be 94 a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do no</context>
</contexts>
<marker>Eryigit, Nivre, Oflazer, 2008</marker>
<rawString>G. Eryigit, J. Nivre, and K. Oflazer. 2008. Dependency Parsing of Turkish. Computational Linguistics 34(3), 357-389.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Gadde</author>
<author>K Jindal</author>
<author>S Husain</author>
<author>D M Sharma</author>
<author>R Sangal</author>
</authors>
<title>Improving Data Driven Dependency Parsing using Clausal Information.</title>
<date>2010</date>
<booktitle>Proc. of NAACLHLT.</booktitle>
<contexts>
<context position="1624" citStr="Gadde et al., 2010" startWordPosition="227" endWordPosition="230">mmunity has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages has not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be 94 a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper, we are concerned</context>
<context position="2939" citStr="Gadde et al. (2010)" startWordPosition="439" endWordPosition="442">There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experiments we select features incrementally to arrive at the best parser features. The primary purpose of this investigation is to study the role of different morphosyntactic features in Hindi dependency parsing, but we also want to improve the overall parsing accuracy. Our final results are 76.5% labeled and 91.1% unlabeled atta</context>
<context position="29068" citStr="Gadde et al. (2010)" startWordPosition="4828" endWordPosition="4831"> a distance of 1 from its parent. For the remaining 6 instances the distance to the parent of the relative clause ranges from 4 to 12. Figure 3 shows how parser performance decreases with increasing distance between the head and the dependent. Recently, Husain et al. (2009) have proposed a two-stage setup to parse interclausal and intra-clausal dependencies separately. They have shown that most long distance relations are inter-clausal, and therefore, using such a clause motivated parsing setup helps in maximizing both short distance and long distance dependency accuracy. In a similar spirit, Gadde et al. (2010) showed that using clausal features helps in identifying long distance dependencies. They have shown that providing clause information in the form of clause boundaries and clausal heads can help a parser make better predictions about long distance dependencies. Dependency Length Dependency Length Figure 3. Dependency arc precision/recall relative to dependency length, where the length of a dependency from wi to wj is |i-j |and roots are assumed to have distance 0 to their head. 6 Conclusion In this paper we have analyzed the importance of different linguistic features in data-driven parsing of</context>
</contexts>
<marker>Gadde, Jindal, Husain, Sharma, Sangal, 2010</marker>
<rawString>P. Gadde, K. Jindal, S. Husain, D. M. Sharma, and R. Sangal. 2010. Improving Data Driven Dependency Parsing using Clausal Information. Proc. of NAACLHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>M Elhadad</author>
</authors>
<title>Hebrew Dependency Parsing: Initial Results.</title>
<date>2009</date>
<booktitle>Proc. of IWPT,</booktitle>
<pages>129--133</pages>
<contexts>
<context position="20455" citStr="Goldberg and Elhadad (2009)" startWordPosition="3398" endWordPosition="3401">idated data while using these features were less than the best results with 66.2% LAS and 84.6% UAS. Agreement patterns in Hindi are not straightforward. For example, the verb agrees with k2 if the k1 has a post-position; it may also sometimes take the default features. In a passive sentence, the verb agrees only with k2. The agreement problem worsens when there is coordination or when there is a complex verb. It is understandable then that the parser is unable to learn the selective agreement pattern which needs to be followed. Similar problems with agreement features have also been noted by Goldberg and Elhadad (2009). In the following sections, we analyze the errors due to different constructions and suggest possible remedies. 5.1 Simple Sentences A simple sentence is one that has only one main verb. In these sentences, the root of the dependency tree is the main verb, which is easily identified by the parser. The main problem is the correct identification of the argument structure. Although the attachments are mostly correct, the dependency labels are error prone. Unlike in English and other more configurational languages, one of the main cues that help us identify the arguments is to be found in the nom</context>
</contexts>
<marker>Goldberg, Elhadad, 2009</marker>
<rawString>Y. Goldberg and M. Elhadad. 2009. Hebrew Dependency Parsing: Initial Results. Proc. of IWPT, 129-133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Hajicova</author>
</authors>
<title>Prague Dependency Treebank: From Analytic to Tectogrammatical Annotation.</title>
<date>1998</date>
<booktitle>Proc. of TSD.</booktitle>
<contexts>
<context position="24107" citStr="Hajicova, 1998" startWordPosition="4011" endWordPosition="4013"> on how one interprets ‘daurte hue’, one gets either the reading that ‘I saw a running dog’ or that ‘I saw a dog while running’. In case of the adjectival participle construction (VJJ), 2 out of 3 errors are due to wrong attachment. 5.3 Coordination Coordination poses problems as it often gives rise to long-distance dependencies. Moreover, the treebank annotation treats the coordinating conjunction as the head of the coordinated structure. Therefore, a coordinating conjunction can potentially become the root of the entire dependency tree. This is similar to Prague style dependency annotation (Hajicova, 1998). Coordinating conjunctions pose additional problems in such a scenario as they can appear as the child of different heads. A coordinating conjunction takes children of similar POS category, but the parent of the conjunction depends on the type of the children. (5) a. raama aur shyaama ne khaana khaayaa ‘Ram’ ‘and’ ‘Shyam’ ‘ERG’ ‘food’ ‘ate’ ‘Ram and Shyam ate the food.’ b. raama ne khaanaa khaayaa aur paanii ‘Ram’ ‘ERG’ ‘food’ ‘ate’ ‘and’ ‘water’ piyaa ‘drank’ ‘Ram ate food and drank water.’ In (5a), raama and shyaama are children of the coordinating conjunction aur, which gets attached to th</context>
</contexts>
<marker>Hajicova, 1998</marker>
<rawString>E. Hajicova. 1998. Prague Dependency Treebank: From Analytic to Tectogrammatical Annotation. Proc. of TSD.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Hall</author>
<author>J Nilsson</author>
<author>J Nivre</author>
<author>G Eryigit</author>
<author>B Megyesi</author>
<author>M Nilsson</author>
<author>M Saers</author>
</authors>
<title>Single Malt or Blended? A Study in Multilingual Parser Optimization.</title>
<date>2007</date>
<booktitle>Proc. of EMNLP-CoNLL,</booktitle>
<pages>933--939</pages>
<marker>Hall, Nilsson, Nivre, Eryigit, Megyesi, Nilsson, Saers, 2007</marker>
<rawString>J. Hall, J. Nilsson, J. Nivre, G. Eryigit, B. Megyesi, M. Nilsson and M. Saers. 2007. Single Malt or Blended? A Study in Multilingual Parser Optimization. Proc. of EMNLP-CoNLL, 933-939.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Husain</author>
</authors>
<title>Dependency Parsers for Indian Languages.</title>
<date>2009</date>
<booktitle>Proc. of ICON09 NLP Tools Contest: Indian Language Dependency Parsing.</booktitle>
<contexts>
<context position="2561" citStr="Husain, 2009" startWordPosition="376" endWordPosition="377">ctic information could be 94 a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of featur</context>
<context position="5063" citStr="Husain, 2009" startWordPosition="781" endWordPosition="782">IO-V) c. sameer ko malaya ne kitaba dii. (IO-S-DO-V) d. sameer ko kitaba malaya ne dii. (IO-DO-S-V) e. kitaba malaya ne sameer ko dii. (DO-S-IO-V) f. kitaba sameer ko malaya ne dii. (DO-IO-S-V) Hindi also has a rich case marking system, although case marking is not obligatory. For example, in (1), while the subject and indirect object are explicitly marked for the ergative (ERG) and dative (DAT) cases, the direct object is unmarked for the accusative. The Hindi dependency treebank (Begum et al., 2008) used for the experiment was released as part of the ICON09 dependency parsing tools contest (Husain, 2009). The dependency framework (Bharati et al., 1995) used in the treebank is inspired by Panini’s grammar of Sanskrit. The core labels, called karakas, are syntactico-semantic relations that identify the participant in the action denoted by the verb. For example, in (1), ‘Malay’ is the agent, ‘book’ is the theme, and ‘Sameer’ is the beneficiary in the activity of ‘give’. In the treebank, these three labels are marked as k1, k2, and k4 respectively. Note, however, that the notion of karaka does not capture the ‘global’ semantics of thematic roles; rather it captures the elements of the ‘local sema</context>
</contexts>
<marker>Husain, 2009</marker>
<rawString>S. Husain. 2009. Dependency Parsers for Indian Languages. Proc. of ICON09 NLP Tools Contest: Indian Language Dependency Parsing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Husain</author>
<author>P Gadde</author>
<author>B Ambati</author>
<author>D M Sharma</author>
<author>R Sangal</author>
</authors>
<title>A modular cascaded approach to complete parsing.</title>
<date>2009</date>
<booktitle>Proc. of the COLIPS International Conference on Asian Language Processing.</booktitle>
<contexts>
<context position="1603" citStr="Husain et al., 2009" startWordPosition="223" endWordPosition="226">dependency parsing community has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages has not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be 94 a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this pa</context>
<context position="2892" citStr="Husain et al. (2009)" startWordPosition="430" endWordPosition="433">rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experiments we select features incrementally to arrive at the best parser features. The primary purpose of this investigation is to study the role of different morphosyntactic features in Hindi dependency parsing, but we also want to improve the overall parsing accuracy. Our final res</context>
<context position="27939" citStr="Husain et al. (2009)" startWordPosition="4645" endWordPosition="4649">ective arcs, but the parser did not find any of them. This is consistent with earlier results showing that pseudo-projective parsing has high precision but low recall, especially when the percentage of non-projective relations is small (Nilsson et al, 2007). Non-projectivity has proven to be one of the major problems in dependency parsing, especially for free word-order languages. In Hindi, the majority of non-projective arcs are inter-clausal (Mannem et al., 2009a), involving conjunctions and relative clauses. There have been some attempts at handling inter-clausal non-projectivity in Hindi. Husain et al. (2009) proposed a two-stage approach that can handle some of the inter-clausal nonprojective structures. 5.6 Long-Distance Dependencies Previous results on parsing other languages have shown that MaltParser has lower accuracy on longdistance dependencies. Our results confirm this. Errors in the case of relative clauses and coordination can mainly be explained in this way. For example, there are 8 instances of relative clauses in the test data. The system could identify only 2 of them correctly. These two are at a distance of 1 from its parent. For the remaining 6 instances the distance to the parent</context>
</contexts>
<marker>Husain, Gadde, Ambati, Sharma, Sangal, 2009</marker>
<rawString>S. Husain, P. Gadde, B. Ambati, D. M. Sharma and R. Sangal. 2009. A modular cascaded approach to complete parsing. Proc. of the COLIPS International Conference on Asian Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mannem</author>
<author>H Chaudhry</author>
<author>A Bharati</author>
</authors>
<title>Insights into non-projectivity in Hindi.</title>
<date>2009</date>
<booktitle>Proc. of ACLIJCNLP Student Research Workshop.</booktitle>
<contexts>
<context position="2913" citStr="Mannem et al. (2009" startWordPosition="434" endWordPosition="437">latively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experiments we select features incrementally to arrive at the best parser features. The primary purpose of this investigation is to study the role of different morphosyntactic features in Hindi dependency parsing, but we also want to improve the overall parsing accuracy. Our final results are 76.5% labele</context>
<context position="27266" citStr="Mannem et al., 2009" startWordPosition="4539" endWordPosition="4542">entifying pof or misclassifies other labels as pof. In particular, the confusion is with k2 and k1s which are object/theme and noun complements of k1, respectively. These labels share similar contextual features like the nominal element in the verb complex. Table 3 includes the confusion matrix for pof errors. 5.5 Non-Projectivity As noted earlier, MaltParser’s arc-eager parsing algorithm can be combined with the pseudoprojective parsing techniques proposed in Nivre and Nilsson (2005), which potentially helps in identifying non-projective arcs. The Hindi treebank has ~14% non-projective arcs (Mannem et al., 2009a). In the test set, there were a total of 11 nonprojective arcs, but the parser did not find any of them. This is consistent with earlier results showing that pseudo-projective parsing has high precision but low recall, especially when the percentage of non-projective relations is small (Nilsson et al, 2007). Non-projectivity has proven to be one of the major problems in dependency parsing, especially for free word-order languages. In Hindi, the majority of non-projective arcs are inter-clausal (Mannem et al., 2009a), involving conjunctions and relative clauses. There have been some attempts </context>
</contexts>
<marker>Mannem, Chaudhry, Bharati, 2009</marker>
<rawString>P. Mannem, H. Chaudhry, and A. Bharati. 2009a. Insights into non-projectivity in Hindi. Proc. of ACLIJCNLP Student Research Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mannem</author>
<author>A Abhilash</author>
<author>A Bharati</author>
</authors>
<title>LTAGspinal Treebank and Parser for Hindi.</title>
<date>2009</date>
<booktitle>Proceedings of International Conference on NLP,</booktitle>
<location>Hyderabad.</location>
<contexts>
<context position="2913" citStr="Mannem et al. (2009" startWordPosition="434" endWordPosition="437">latively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experiments we select features incrementally to arrive at the best parser features. The primary purpose of this investigation is to study the role of different morphosyntactic features in Hindi dependency parsing, but we also want to improve the overall parsing accuracy. Our final results are 76.5% labele</context>
<context position="27266" citStr="Mannem et al., 2009" startWordPosition="4539" endWordPosition="4542">entifying pof or misclassifies other labels as pof. In particular, the confusion is with k2 and k1s which are object/theme and noun complements of k1, respectively. These labels share similar contextual features like the nominal element in the verb complex. Table 3 includes the confusion matrix for pof errors. 5.5 Non-Projectivity As noted earlier, MaltParser’s arc-eager parsing algorithm can be combined with the pseudoprojective parsing techniques proposed in Nivre and Nilsson (2005), which potentially helps in identifying non-projective arcs. The Hindi treebank has ~14% non-projective arcs (Mannem et al., 2009a). In the test set, there were a total of 11 nonprojective arcs, but the parser did not find any of them. This is consistent with earlier results showing that pseudo-projective parsing has high precision but low recall, especially when the percentage of non-projective relations is small (Nilsson et al, 2007). Non-projectivity has proven to be one of the major problems in dependency parsing, especially for free word-order languages. In Hindi, the majority of non-projective arcs are inter-clausal (Mannem et al., 2009a), involving conjunctions and relative clauses. There have been some attempts </context>
</contexts>
<marker>Mannem, Abhilash, Bharati, 2009</marker>
<rawString>P. Mannem, A. Abhilash and A. Bharati. 2009b. LTAGspinal Treebank and Parser for Hindi. Proceedings of International Conference on NLP, Hyderabad. 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Lerman</author>
<author>F Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>Proc. of CoNLL,</booktitle>
<pages>216--220</pages>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>R. McDonald, K. Lerman, and F. Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. Proc. of CoNLL, 216-220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>J Nivre</author>
</authors>
<title>Characterizing the errors of data-driven dependency parsing models.</title>
<date>2007</date>
<booktitle>Proc. of EMNLP-CoNLL,</booktitle>
<pages>122--131</pages>
<marker>McDonald, Nivre, 2007</marker>
<rawString>R. McDonald and J. Nivre. 2007. Characterizing the errors of data-driven dependency parsing models. Proc. of EMNLP-CoNLL, 122-131.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I A Mel&apos;Cuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice,</title>
<date>1988</date>
<publisher>State University Press of</publisher>
<location>New York.</location>
<marker>Mel&apos;Cuk, 1988</marker>
<rawString>I. A. Mel&apos;Cuk. 1988. Dependency Syntax: Theory and Practice, State University Press of New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nilsson</author>
<author>J Nivre</author>
<author>J Hall</author>
</authors>
<title>Generalizing Tree Transformations for Inductive Dependency Parsing.</title>
<date>2007</date>
<booktitle>Proc. of ACL,</booktitle>
<pages>968--975</pages>
<contexts>
<context position="27576" citStr="Nilsson et al, 2007" startWordPosition="4592" endWordPosition="4595">s. 5.5 Non-Projectivity As noted earlier, MaltParser’s arc-eager parsing algorithm can be combined with the pseudoprojective parsing techniques proposed in Nivre and Nilsson (2005), which potentially helps in identifying non-projective arcs. The Hindi treebank has ~14% non-projective arcs (Mannem et al., 2009a). In the test set, there were a total of 11 nonprojective arcs, but the parser did not find any of them. This is consistent with earlier results showing that pseudo-projective parsing has high precision but low recall, especially when the percentage of non-projective relations is small (Nilsson et al, 2007). Non-projectivity has proven to be one of the major problems in dependency parsing, especially for free word-order languages. In Hindi, the majority of non-projective arcs are inter-clausal (Mannem et al., 2009a), involving conjunctions and relative clauses. There have been some attempts at handling inter-clausal non-projectivity in Hindi. Husain et al. (2009) proposed a two-stage approach that can handle some of the inter-clausal nonprojective structures. 5.6 Long-Distance Dependencies Previous results on parsing other languages have shown that MaltParser has lower accuracy on longdistance d</context>
</contexts>
<marker>Nilsson, Nivre, Hall, 2007</marker>
<rawString>J. Nilsson, J. Nivre and J. Hall. 2007. Generalizing Tree Transformations for Inductive Dependency Parsing. Proc. of ACL, 968-975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Algorithms for Deterministic Incremental Dependency Parsing.</title>
<date>2008</date>
<journal>Computational Linguistics</journal>
<volume>34</volume>
<issue>4</issue>
<pages>513--553</pages>
<contexts>
<context position="8159" citStr="Nivre, 2008" startWordPosition="1287" endWordPosition="1288">ation is automatically computed and incorporated as a feature of the head of a chunk. As we will see later, such information proves to be extremely crucial during dependency parsing. For all the experiments discussed in section 4, the training and development data size was 1500 and 150 sentences respectively. The training and development data consisted of —22k and —1.7k words respectively. The test data consisted of 150 sentences (—1.6k words). The average sentence length is 19.85. 3 Transition-Based Dependency Parsing A transition-based dependency parser is built of two essential components (Nivre, 2008): · A transition system for mapping sentences to dependency trees · A classifier for predicting the next transition for every possible system configuration 95 PTAG CTAG FORM LEMMA DEPREL CTAM OTHERS Stack: top 1 5 1 7 9 Input: next 1 5 1 7 9 Input: next+1 2 5 6 7 Input: next+2 2 Input: next+3 2 Stack: top-1 3 String: predecessor of top 3 Tree: head of top 4 Tree: leftmost dep of next 4 5 6 Tree: rightmost dep of top 8 Tree: left sibling of rightmost dep of top 8 Merge: PTAG of top and next 10 Merge: CTAM and DEPREL of top 10 Table 1. Feature pool based on selection from Ambati et al. (2009a) a</context>
</contexts>
<marker>Nivre, 2008</marker>
<rawString>J. Nivre. 2008. Algorithms for Deterministic Incremental Dependency Parsing. Computational Linguistics 34(4), 513-553.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>R McDonald</author>
</authors>
<title>Integrating graphbased and transition-based dependency parsers.</title>
<date>2008</date>
<booktitle>In Proceedings of ACL-HLT,</booktitle>
<pages>950--958</pages>
<marker>Nivre, McDonald, 2008</marker>
<rawString>J. Nivre and R. McDonald. 2008. Integrating graphbased and transition-based dependency parsers. In Proceedings of ACL-HLT, pp. 950-958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Non-Projective Dependency Parsing in Expected Linear Time.</title>
<date>2009</date>
<booktitle>Proc. of ACL-IJCNLP,</booktitle>
<pages>351--359</pages>
<contexts>
<context position="2661" citStr="Nivre, 2009" startWordPosition="395" endWordPosition="396">ing these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experiments we select features incrementally to arrive</context>
<context position="8773" citStr="Nivre (2009" startWordPosition="1409" endWordPosition="1410"> A transition system for mapping sentences to dependency trees · A classifier for predicting the next transition for every possible system configuration 95 PTAG CTAG FORM LEMMA DEPREL CTAM OTHERS Stack: top 1 5 1 7 9 Input: next 1 5 1 7 9 Input: next+1 2 5 6 7 Input: next+2 2 Input: next+3 2 Stack: top-1 3 String: predecessor of top 3 Tree: head of top 4 Tree: leftmost dep of next 4 5 6 Tree: rightmost dep of top 8 Tree: left sibling of rightmost dep of top 8 Merge: PTAG of top and next 10 Merge: CTAM and DEPREL of top 10 Table 1. Feature pool based on selection from Ambati et al. (2009a) and Nivre (2009b). Given these two components, dependency parsing can be realized as deterministic search through the transition system, guided by the classifier. With this technique, parsing can be performed in linear time for projective dependency trees. Like Ambati et al. (2009a) and Nivre (2009b), we use MaltParser, an open-source implementation of transitionbased dependency parsing with a variety of transition systems and customizable classifiers.3 3.1 Transition System Previous work has shown that the arc-eager projective transition system first described in Nivre (2003) works well for Hindi (Ambati et</context>
<context position="11849" citStr="Nivre (2009" startWordPosition="1913" endWordPosition="1914">cal morphosyntactic features such as case markers (postpositions/suffixes) for nominals and TAM markers for verbs (cf. Section 2). 96 The pool of features used in the experiments are shown in Table 1, where rows denote tokens in a parser configuration – defined relative to the stack, the input buffer, the partially built dependency tree and the input string – and columns correspond to attributes. Each non-empty cell represents a feature, and features are numbered for easy reference. 4 Feature Selection Experiments Starting from the union of the feature sets used by Ambati et al. (2009a and by Nivre (2009b), we first used 5-fold cross-validation on the combined training and development sets from the ICON09 tools contest to select the pool of features depicted in Table 1, keeping all features that had a positive effect on both labeled and unlabeled accuracy. We then grouped the features into 10 groups (indicated by numbers 1–10 in Table 1) and reran the crossvalidation, incrementally adding different feature groups in order to analyze their impact on parsing accuracy. The result is shown in Figure 1. Figure 1. UAS and LAS of experiments 1-10; 5-fold cross-validation on training and development </context>
<context position="17256" citStr="Nivre (2009" startWordPosition="2850" endWordPosition="2851">f labels only. This can be captured by the combination tried in this experiment. Experiment 10 gave the best results in the crossvalidation experiments. The settings from this experiment were used to get the final performance on the test data. Table 2 shows the final results along with the results of the first and second best performing systems in the ICON09 tools contest. We see that our system achieved an improvement of 2 percentage points in LAS and 1 percentage point in UAS over the previous state of the art reported in Ambati et al. (2009a). System LAS UAS Ambati et al. (2009a) 74.5 90.1 Nivre (2009b) 73.4 89.8 Our system 76.5 91.1 Table 2. Final results on the test data from the ICON09 tools contest. 5 Error Analysis In this section we provide a detailed error analysis on the test data and suggest possible remedies for problems noted. We note here that other than the reasons mentioned in this section, small treebank size could be another reason for low accuracy of the parser. The training data used for the experiments only had ~28.5k words. With recent work on Hindi Treebanking (Bhatt et al., 2009) we expect to get more annotated data in the near future. Figure 2 shows the precision and</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>J. Nivre. 2009a. Non-Projective Dependency Parsing in Expected Linear Time. Proc. of ACL-IJCNLP, 351-359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
</authors>
<title>Parsing Indian Languages with MaltParser.</title>
<date>2009</date>
<booktitle>Proc. of ICON09 NLP Tools Contest: Indian Language Dependency Parsing,</booktitle>
<pages>12--18</pages>
<contexts>
<context position="2661" citStr="Nivre, 2009" startWordPosition="395" endWordPosition="396">ing these language specific features in parsing is not always straightforward and many intuitive features do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experiments we select features incrementally to arrive</context>
<context position="8773" citStr="Nivre (2009" startWordPosition="1409" endWordPosition="1410"> A transition system for mapping sentences to dependency trees · A classifier for predicting the next transition for every possible system configuration 95 PTAG CTAG FORM LEMMA DEPREL CTAM OTHERS Stack: top 1 5 1 7 9 Input: next 1 5 1 7 9 Input: next+1 2 5 6 7 Input: next+2 2 Input: next+3 2 Stack: top-1 3 String: predecessor of top 3 Tree: head of top 4 Tree: leftmost dep of next 4 5 6 Tree: rightmost dep of top 8 Tree: left sibling of rightmost dep of top 8 Merge: PTAG of top and next 10 Merge: CTAM and DEPREL of top 10 Table 1. Feature pool based on selection from Ambati et al. (2009a) and Nivre (2009b). Given these two components, dependency parsing can be realized as deterministic search through the transition system, guided by the classifier. With this technique, parsing can be performed in linear time for projective dependency trees. Like Ambati et al. (2009a) and Nivre (2009b), we use MaltParser, an open-source implementation of transitionbased dependency parsing with a variety of transition systems and customizable classifiers.3 3.1 Transition System Previous work has shown that the arc-eager projective transition system first described in Nivre (2003) works well for Hindi (Ambati et</context>
<context position="11849" citStr="Nivre (2009" startWordPosition="1913" endWordPosition="1914">cal morphosyntactic features such as case markers (postpositions/suffixes) for nominals and TAM markers for verbs (cf. Section 2). 96 The pool of features used in the experiments are shown in Table 1, where rows denote tokens in a parser configuration – defined relative to the stack, the input buffer, the partially built dependency tree and the input string – and columns correspond to attributes. Each non-empty cell represents a feature, and features are numbered for easy reference. 4 Feature Selection Experiments Starting from the union of the feature sets used by Ambati et al. (2009a and by Nivre (2009b), we first used 5-fold cross-validation on the combined training and development sets from the ICON09 tools contest to select the pool of features depicted in Table 1, keeping all features that had a positive effect on both labeled and unlabeled accuracy. We then grouped the features into 10 groups (indicated by numbers 1–10 in Table 1) and reran the crossvalidation, incrementally adding different feature groups in order to analyze their impact on parsing accuracy. The result is shown in Figure 1. Figure 1. UAS and LAS of experiments 1-10; 5-fold cross-validation on training and development </context>
<context position="17256" citStr="Nivre (2009" startWordPosition="2850" endWordPosition="2851">f labels only. This can be captured by the combination tried in this experiment. Experiment 10 gave the best results in the crossvalidation experiments. The settings from this experiment were used to get the final performance on the test data. Table 2 shows the final results along with the results of the first and second best performing systems in the ICON09 tools contest. We see that our system achieved an improvement of 2 percentage points in LAS and 1 percentage point in UAS over the previous state of the art reported in Ambati et al. (2009a). System LAS UAS Ambati et al. (2009a) 74.5 90.1 Nivre (2009b) 73.4 89.8 Our system 76.5 91.1 Table 2. Final results on the test data from the ICON09 tools contest. 5 Error Analysis In this section we provide a detailed error analysis on the test data and suggest possible remedies for problems noted. We note here that other than the reasons mentioned in this section, small treebank size could be another reason for low accuracy of the parser. The training data used for the experiments only had ~28.5k words. With recent work on Hindi Treebanking (Bhatt et al., 2009) we expect to get more annotated data in the near future. Figure 2 shows the precision and</context>
</contexts>
<marker>Nivre, 2009</marker>
<rawString>J. Nivre. 2009b. Parsing Indian Languages with MaltParser. Proc. of ICON09 NLP Tools Contest: Indian Language Dependency Parsing, 12-18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>S Kubler</author>
<author>R McDonald</author>
<author>J Nilsson</author>
<author>S Riedel</author>
<author>D Yuret</author>
</authors>
<date>2007</date>
<journal>The CoNLL</journal>
<booktitle>Shared Task on Dependency Parsing. Proc. of EMNLP/CoNLL,</booktitle>
<pages>915--932</pages>
<contexts>
<context position="1361" citStr="Nivre et al., 2007" startWordPosition="189" endWordPosition="192">we achieve a labeled attachment score of 76.5%, which is 2 percentage points better than the previous state of the art. We finally provide a detailed error analysis and suggest possible improvements to the parsing scheme. 1 Introduction The dependency parsing community has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages has not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic informat</context>
<context position="2768" citStr="Nivre et al., 2007" startWordPosition="408" endWordPosition="411">tures do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experiments we select features incrementally to arrive at the best parser features. The primary purpose of this investigation is to study the role of different m</context>
</contexts>
<marker>Nivre, Hall, Kubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>J. Nivre, J. Hall, S. Kubler, R. McDonald, J. Nilsson, S. Riedel and D. Yuret. 2007a. The CoNLL 2007 Shared Task on Dependency Parsing. Proc. of EMNLP/CoNLL, 915-932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
<author>A Chanev</author>
<author>G Eryigit</author>
<author>S Kübler</author>
<author>S Marinov</author>
<author>E Marsi</author>
</authors>
<title>MaltParser: A language-independent system for data-driven dependency parsing.</title>
<date>2007</date>
<journal>NLE,</journal>
<volume>13</volume>
<issue>2</issue>
<pages>95--135</pages>
<contexts>
<context position="1361" citStr="Nivre et al., 2007" startWordPosition="189" endWordPosition="192">we achieve a labeled attachment score of 76.5%, which is 2 percentage points better than the previous state of the art. We finally provide a detailed error analysis and suggest possible improvements to the parsing scheme. 1 Introduction The dependency parsing community has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages has not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic informat</context>
<context position="2768" citStr="Nivre et al., 2007" startWordPosition="408" endWordPosition="411">tures do not always work in expected ways. In this paper, we are concerned with Hindi, an Indian language with moderately rich morphology and relatively free word order. There have been several previous attempts at parsing Hindi as well as other Indian languages (Bharati et al., 1995, Bharati et al., 2009b). Many techniques were tried out recently at the ICON09 dependency parsing tools contest (Husain, 2009). Both the best performing system (Ambati et al., 2009a) and the system in second place (Nivre, 2009b) used a transition-based approach to dependency parsing, as implemented in MaltParser (Nivre et al., 2007b). Other data driven parsing efforts for Indian languages in the past have been Bharati et al. (2008), Husain et al. (2009), Mannem et al. (2009b) and Gadde et al. (2010). In this paper, we continue to explore the transition-based approach to Hindi dependency parsing, building on the state-of-the-art results of Ambati et al. (2009a) and Nivre (2009b) and exploring the common pool of features used by those systems. Through a series of experiments we select features incrementally to arrive at the best parser features. The primary purpose of this investigation is to study the role of different m</context>
</contexts>
<marker>Nivre, Hall, Nilsson, Chanev, Eryigit, Kübler, Marinov, Marsi, 2007</marker>
<rawString>J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, S. Kübler, S. Marinov and E Marsi. 2007b. MaltParser: A language-independent system for data-driven dependency parsing. NLE, 13(2), 95-135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Øvrelid</author>
</authors>
<title>Argument Differentiation. Soft constraints and data-driven models.</title>
<date>2008</date>
<tech>PhD Thesis,</tech>
<institution>University of Gothenburg.</institution>
<contexts>
<context position="19514" citStr="Øvrelid (2008)" startWordPosition="3246" endWordPosition="3247">trix for some important labels in the test data. As the present information available for disambiguation is not sufficient, we can make use of some semantics to resolve these ambiguities. Bharati et al. (2008) and Ambati et al. (2009b) have shown that this ambiguity can be reduced using minimal semantics. They used six semantic features: human, nonhuman, in-animate, time, place and abstract. Using these features they showed that k1-k2 and k7p-k7t ambiguities can be resolved to a great extent. Of course, automatically extracting these semantic features is in itself a challenging task, although Øvrelid (2008) has shown that animacy features can be induced automatically from data. In section 4 we mentioned that a separate experiment explored the effectiveness of morphological features like gender, number and person. Counter to our intuitions, these features did not improve the overall accuracy. Accuracies on cross-validated data while using these features were less than the best results with 66.2% LAS and 84.6% UAS. Agreement patterns in Hindi are not straightforward. For example, the verb agrees with k2 if the k1 has a post-position; it may also sometimes take the default features. In a passive se</context>
</contexts>
<marker>Øvrelid, 2008</marker>
<rawString>L. Øvrelid. 2008. Argument Differentiation. Soft constraints and data-driven models. PhD Thesis, University of Gothenburg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Seddah</author>
<author>M Candito</author>
<author>B Crabbé</author>
</authors>
<title>Cross parser evaluation: a French Treebanks study.</title>
<date>2009</date>
<booktitle>Proc. of IWPT,</booktitle>
<pages>150--161</pages>
<contexts>
<context position="1582" citStr="Seddah et al., 2009" startWordPosition="219" endWordPosition="222">. 1 Introduction The dependency parsing community has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages has not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be 94 a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many intuitive features do not always work in expe</context>
</contexts>
<marker>Seddah, Candito, Crabbé, 2009</marker>
<rawString>D. Seddah, M. Candito and B. Crabbé. 2009. Cross parser evaluation: a French Treebanks study. Proc. of IWPT, 150-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Tsarfaty</author>
<author>K Sima&apos;an</author>
</authors>
<title>RelationalRealizational Parsing.</title>
<date>2008</date>
<booktitle>Proc. of CoLing,</booktitle>
<pages>889--896</pages>
<contexts>
<context position="1539" citStr="Tsarfaty and Sima&apos;an, 2008" startWordPosition="211" endWordPosition="214">uggest possible improvements to the parsing scheme. 1 Introduction The dependency parsing community has since a few years shown considerable interest in parsing morphologically rich languages with flexible word order. This is partly due to the increasing availability of dependency treebanks for such languages, but it is also motivated by the observation that the performance obtained for these languages has not been very high (Nivre et al., 2007a). Attempts at handling various non-configurational aspects in these languages have pointed towards shortcomings in traditional parsing methodologies (Tsarfaty and Sima&apos;an, 2008; Eryigit et al., 2008; Seddah et al., 2009; Husain et al., 2009; Gadde et al., 2010). Among other things, it has been pointed out that the use of language specific features may play a crucial role in improving the overall parsing performance. Different languages tend to encode syntactically relevant information in different ways, and it has been hypothesized that the integration of morphological and syntactic information could be 94 a key to better accuracy. However, it has also been noted that incorporating these language specific features in parsing is not always straightforward and many in</context>
</contexts>
<marker>Tsarfaty, Sima&apos;an, 2008</marker>
<rawString>R. Tsarfaty and K. Sima&apos;an. 2008. RelationalRealizational Parsing. Proc. of CoLing, 889-896.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Vaidya</author>
<author>S Husain</author>
<author>P Mannem</author>
<author>D M Sharma</author>
</authors>
<title>A karaka-based dependency annotation scheme for English.</title>
<date>2009</date>
<booktitle>Proc. of CICLing,</booktitle>
<pages>41--52</pages>
<contexts>
<context position="5777" citStr="Vaidya et al., 2009" startWordPosition="899" endWordPosition="902"> grammar of Sanskrit. The core labels, called karakas, are syntactico-semantic relations that identify the participant in the action denoted by the verb. For example, in (1), ‘Malay’ is the agent, ‘book’ is the theme, and ‘Sameer’ is the beneficiary in the activity of ‘give’. In the treebank, these three labels are marked as k1, k2, and k4 respectively. Note, however, that the notion of karaka does not capture the ‘global’ semantics of thematic roles; rather it captures the elements of the ‘local semantics’ of a verb, while also taking cues from the surface level morpho-syntactic information (Vaidya et al., 2009). The syntactic relational cues (such as case markers) help identify many of the karakas. In general, the highest available karaka,2 if not case-marked, agrees with the verb in an active sentence. In addition, the tense, 1 S=Subject; IO=Indirect Object; DO=Direct Object; V=Verb; ERG=Ergative; DAT=Dative 2 These are the karta karaka (k1) and karma karaka (k2). k1 and k2 can be roughly translated as ‘agent’ and ‘theme’ respectively. For a complete description of the tagset and the dependency scheme, see Begum et al. (2008) and Bharati et al. (2009a). aspect and modality (TAM) marker can many a t</context>
</contexts>
<marker>Vaidya, Husain, Mannem, Sharma, 2009</marker>
<rawString>A. Vaidya, S. Husain, P. Mannem, and D. M. Sharma. 2009. A karaka-based dependency annotation scheme for English. Proc. of CICLing, 41-52.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>