<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000043">
<note confidence="0.860856">
SENSEVAL-3: Third International Workshop on the Evaluation of Systems
for the Semantic Analysis of Text, Barcelona, Spain, July 2004
</note>
<title confidence="0.7766165">
Association for Computational Linguistics
UBBNBC WSD System Description
</title>
<author confidence="0.847396">
Abstract Csomai ANDRÁS
</author>
<affiliation confidence="0.821990333333333">
Department of Computer Science
Babes-Bolyai University
Cluj-Napoca, Romania
</affiliation>
<email confidence="0.924612">
csomaia@personal.ro
</email>
<sectionHeader confidence="0.85331" genericHeader="abstract">
2 Stemming
</sectionHeader>
<bodyText confidence="0.998917">
The Naïve Bayes classification proves to
be a good performing tool in word sense
disambiguation, although it has not yet
been applied to the Romanian language.
The aim of this paper is to present our
WSD system, based on the NBC algorithm,
that performed quite well in Senseval 3.
</bodyText>
<sectionHeader confidence="0.998691" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912433333333">
According to the literature, the NBC algorithm is
very efficient, in many cases it outperforms more
sophisticated methods (Pedersen 1998). Therefore,
this is the approach we used in our research. The
word sense disambiguating process has three major
steps, therefore, the application has three main
components as follows:
Stemming – removal of suffixes, and the filter-
ing out of the irrelevant information from
the corpora. A simple dictionary based ap-
proach.
Learning – the training of the classifier, based
on the sense tagged corpora. A database
containing the number of co-occurrences is
built.
Disambiguating –on the basis of the database,
the correct sense of a word in a given con-
text is estimated.
In the followings the previously mentioned three
steps are described in detail.
The preprocessing of the corpora is one of the most
result-influential steps. The preprocessing consists
of the removal of suffixes and the elimination of
the irrelevant data. The removal of suffixes is per-
formed trough a simple dictionary based method.
For every wi word the possible wj candidates are
selected from the dictionary containing the word
stems. Then a similarity score is calculated be-
tween the word to be stemmed and the candidates,
as follows:
</bodyText>
<equation confidence="0.97895625">
li, lj is the length of word i, respectively j.
scorei= 2li if li ≤ lj and
li + lj
scorej=0, otherwise.
</equation>
<bodyText confidence="0.990670166666667">
The result is the candidate with the highest score if
its score is above a certain threshold, otherwise the
word is leaved untouched.
In the preprocessing phase we also erase the pro-
nouns and prepositions from the examined context.
This exclusion was made upon a list of stop words.
</bodyText>
<sectionHeader confidence="0.992963" genericHeader="introduction">
3 Learning
</sectionHeader>
<bodyText confidence="0.99876">
The training is conducted according to the NBC
algorithm. First a database is built, with the follow-
ing tables:
words – contains all the words found in the cor-
pora. Its role is to assign a sense id to every
word.
wordsenses – contains all the tagged words in
the corpora linked with their possible senses.
One entry for a given sense and word.
nosenses - number of tagged contexts, with a
given sense
nocontexts - number of tagged contexts of a
given word
occurrences – number of co-occurrences of a
given word with a given sense
step to next word
endscan
step to next entry
endscan corpora
As it is obvious, the database is filled up (so the
system is trained) only upon the training corpus
provided for the Senseval3 Romanian Lexical
Sample task.
</bodyText>
<sectionHeader confidence="0.996625" genericHeader="method">
4 Disambiguation
</sectionHeader>
<bodyText confidence="0.999556444444444">
The basic assumption of the Naïve Bayes method
is that the contextual features are not dependent on
each other. In this particular case, we assume that
the probability of co-occurrence of a word vi with
the ambiguous word w of sense s is not dependent
on other co-occurrences.
The goal is to find the correct sense s′ , of the
word w, for a given context. This s′ sense maxi-
mizes the following equation.
</bodyText>
<equation confidence="0.8869125625">
s′= arg maxsk P(
= arg max
sk  |c)
sk
P(c |
)P
sk
( )
s k
P(c)
sk
P
(c  |) (sk )
k P
s
= arg max
</equation>
<bodyText confidence="0.77508337037037">
Figure1: The tables of the database
The training of the system is nothing but filling up
the tables of the database.
fill NoSenses
fill NoContexts
fill Wordsenses
scan corpora
cakt=actual entry in corpora (a context)
w=actual word in entry (the ambiguous word)
sk=actual sense of entry
scan cakt
vj=actual word in entry
if vj&lt;&gt;w then
if vj in words then
vi=wordid from words where w=vj
else
add words vj
endif
if (exists entry in occurrences where
wordid=vi and senseid=sk) then
increment C(wordid,senseid) in occurrences,
where wordid=vi and senseid=sk
else
add occurrences(wordid, senseid, 1)
endif
At this point we make the simplifying “naïve” as-
sumption:
</bodyText>
<equation confidence="0.948313">
P(c  |sO = ∏P(vj  |sk )
vj∈c
</equation>
<bodyText confidence="0.97038275">
The algorithm (Tătar, 2003) for estimating the cor-
rect sense of word w according to its c context is
the following:
for every sk sense of w do
</bodyText>
<equation confidence="0.9099712">
score(sk)=P(sk)
for every vj from context c do
score(sk)= score(sk)*P(vj  |sk)
s’= arg maxs (score(sk ))
k
</equation>
<bodyText confidence="0.771138666666667">
where s’ is the estimated sense, vj is the j-th word
of the context, sk is the k-th possible sense for word
w.
</bodyText>
<equation confidence="0.962915666666667">
P(sk) and P(vj  |sk) are calculated as follows:
P(s k ) = C(s k
C(w)
</equation>
<bodyText confidence="0.956496">
where C(w) is the number of contexts for word w,
C(vj , sk) is the number of occurrences of word vj in
</bodyText>
<equation confidence="0.9426285">
)
 |s )
k =
C(s k
P(vj
)
,
)
s k
C(vj
</equation>
<bodyText confidence="0.808963666666667">
contexts tagged with sense sk , and C(sk) is the
number of contexts tagged with sense sk
The values are obtained from the database, as fol-
lows:
C(w)- from nocontexts,
C(vj , sk)- from occurrences,
C(sk)- from nosenses.
wordsenses is being used to determine the possible
senses of a given word.
</bodyText>
<sectionHeader confidence="0.998319" genericHeader="method">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.997649">
The described system was evaluated at Senseval 3.
The output was not weighted, therefore for every
ambiguous word, at most 1 solution (estimated
sense) was provided. The results achieved, are the
followings:
</bodyText>
<table confidence="0.9960462">
score correct/attempted
precision 0.710 2415 correct of
3403 attempted
recall 0.682 2415 correct of
3541 in total
attempted 96.10% 3403 attempted
of 3541 in total
Figure2: Fine-grained score
score correct/attempted
precision 0.750 2551 correct of
3403 attempted
recall 0.720 2551 correct of
3541 in total
attempted 96.10% 3403 attempted
of 3541 in total
</table>
<sectionHeader confidence="0.821419" genericHeader="method">
Figure2: Coarse-grained score
</sectionHeader>
<bodyText confidence="0.865359230769231">
A simple test was made, before the Senseval 3
evaluation. The system was trained on 90% of the
Romanian Lexical Sample training corpus, and
tested on the remaining 10%. The selection was
random, with a uniform distribution. A coarse
grained score was computed and compared to the
baseline score. A baseline method consists of de-
termining the most frequent sense for every word
(based upon the training corpus) and in the evalua-
tion phase always this sense is assigned.
UBBNBC Baseline
recall 0.66 0.56
precision 0.69 0.56
</bodyText>
<sectionHeader confidence="0.8222515" genericHeader="conclusions">
Figure3: baseline UBBNBC comparison
References
</sectionHeader>
<reference confidence="0.999509666666667">
Ted Pedersen. 1998. Naïve Bayes as a Satisficing
Model. Working Notes of the AAAI Spring Sympo-
sium on Satisficing Models, Palo Alto, CA
Doina Tătar. 2003. Inteligenfă artificială - Aplicafii în
prelucrarea limbajului natural. Editura Albastra,
Cluj-Napoca, Romania.
Manning, C. D., Schütze, H. 1999. Foundations of sta-
tistical natural language processing. MIT Press,
Cambridge, Massachusetts.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.025329">
<note confidence="0.7878855">SENSEVAL-3: Third International Workshop on the Evaluation of Systems for the Semantic Analysis of Text, Barcelona, Spain, July 2004</note>
<title confidence="0.6319945">Association for Computational Linguistics UBBNBC WSD System Description</title>
<author confidence="0.89735">Abstract Csomai</author>
<affiliation confidence="0.639559333333333">Department of Computer Babes-Bolyai Cluj-Napoca,</affiliation>
<email confidence="0.485776">csomaia@personal.ro</email>
<abstract confidence="0.899422125">2 Stemming The Naïve Bayes classification proves to be a good performing tool in word sense disambiguation, although it has not yet been applied to the Romanian language. The aim of this paper is to present our WSD system, based on the NBC algorithm, that performed quite well in Senseval 3.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
</authors>
<title>Naïve Bayes as a Satisficing Model. Working Notes of the AAAI Spring Symposium on Satisficing Models,</title>
<date>1998</date>
<location>Palo Alto, CA</location>
<contexts>
<context position="768" citStr="Pedersen 1998" startWordPosition="110" endWordPosition="111">tational Linguistics UBBNBC WSD System Description Abstract Csomai ANDRÁS Department of Computer Science Babes-Bolyai University Cluj-Napoca, Romania csomaia@personal.ro 2 Stemming The Naïve Bayes classification proves to be a good performing tool in word sense disambiguation, although it has not yet been applied to the Romanian language. The aim of this paper is to present our WSD system, based on the NBC algorithm, that performed quite well in Senseval 3. 1 Introduction According to the literature, the NBC algorithm is very efficient, in many cases it outperforms more sophisticated methods (Pedersen 1998). Therefore, this is the approach we used in our research. The word sense disambiguating process has three major steps, therefore, the application has three main components as follows: Stemming – removal of suffixes, and the filtering out of the irrelevant information from the corpora. A simple dictionary based approach. Learning – the training of the classifier, based on the sense tagged corpora. A database containing the number of co-occurrences is built. Disambiguating –on the basis of the database, the correct sense of a word in a given context is estimated. In the followings the previousl</context>
</contexts>
<marker>Pedersen, 1998</marker>
<rawString>Ted Pedersen. 1998. Naïve Bayes as a Satisficing Model. Working Notes of the AAAI Spring Symposium on Satisficing Models, Palo Alto, CA</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doina Tătar</author>
</authors>
<title>Inteligenfă artificială - Aplicafii în prelucrarea limbajului natural. Editura Albastra,</title>
<date>2003</date>
<location>Cluj-Napoca, Romania.</location>
<contexts>
<context position="4253" citStr="Tătar, 2003" startWordPosition="720" endWordPosition="721">f the database. fill NoSenses fill NoContexts fill Wordsenses scan corpora cakt=actual entry in corpora (a context) w=actual word in entry (the ambiguous word) sk=actual sense of entry scan cakt vj=actual word in entry if vj&lt;&gt;w then if vj in words then vi=wordid from words where w=vj else add words vj endif if (exists entry in occurrences where wordid=vi and senseid=sk) then increment C(wordid,senseid) in occurrences, where wordid=vi and senseid=sk else add occurrences(wordid, senseid, 1) endif At this point we make the simplifying “naïve” assumption: P(c |sO = ∏P(vj |sk ) vj∈c The algorithm (Tătar, 2003) for estimating the correct sense of word w according to its c context is the following: for every sk sense of w do score(sk)=P(sk) for every vj from context c do score(sk)= score(sk)*P(vj |sk) s’= arg maxs (score(sk )) k where s’ is the estimated sense, vj is the j-th word of the context, sk is the k-th possible sense for word w. P(sk) and P(vj |sk) are calculated as follows: P(s k ) = C(s k C(w) where C(w) is the number of contexts for word w, C(vj , sk) is the number of occurrences of word vj in ) |s ) k = C(s k P(vj ) , ) s k C(vj contexts tagged with sense sk , and C(sk) is the number of </context>
</contexts>
<marker>Tătar, 2003</marker>
<rawString>Doina Tătar. 2003. Inteligenfă artificială - Aplicafii în prelucrarea limbajului natural. Editura Albastra, Cluj-Napoca, Romania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of statistical natural language processing.</title>
<date>1999</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts.</location>
<marker>Manning, Schütze, 1999</marker>
<rawString>Manning, C. D., Schütze, H. 1999. Foundations of statistical natural language processing. MIT Press, Cambridge, Massachusetts.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>