<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001376">
<title confidence="0.990148">
SB: mmSystem - Using Decompositional Semantics for Lexical
Simplification
</title>
<author confidence="0.997511">
Marilisa Amoia Massimo Romanelli
</author>
<affiliation confidence="0.9978245">
Department of Applied Linguistics DFKI GmBH
University of Saarland Saarbrcken, Germany
</affiliation>
<email confidence="0.997163">
m.amoia@mx.uni-saarland.de romanell@dfki.de
</email>
<sectionHeader confidence="0.995609" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998713333333333">
In this paper, we describe the system we sub-
mitted to the SemEval-2012 Lexical Simplifi-
cation Task. Our system (mmSystem) com-
bines word frequency with decompositional
semantics criteria based on syntactic structure
in order to rank candidate substitutes of lexical
forms of arbitrary syntactic complexity (one-
word, multi-word, etc.) in descending order
of (cognitive) simplicity. We believe that the
proposed approach might help to shed light on
the interplay between linguistic features and
lexical complexity in general.
</bodyText>
<sectionHeader confidence="0.998987" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999961764705882">
Lexical simplification is a subtask of the more gen-
eral text simplification task which attempts at re-
ducing the cognitive complexity of a text so that
it can be (better) understood by a larger audience.
Text simplification has a wide range of applications
which includes applications for the elderly, learners
of a second language, children or people with cog-
nitive deficiencies, etc.
Works on text simplification mostly focus on re-
ducing the syntactic complexity of the text (Sid-
dharthan, 2011; Siddharthan, 2006) and only little
work has addressed the issue of lexical simplifica-
tion (Devlin, 1999; Carroll et al., 1999).
The Lexical Simplification Task (Specia et al.,
2012) proposed within the SemEval-2012 is the first
attempt to explore the nature of the lexical simpli-
fication more systematically. This task requires par-
ticipating systems, given a context and a target word,
to automatically generate a ranking of substitutes,
i.e. lexical forms conveying similar meanings to
the target word, such that cognitively simpler lexi-
cal forms are ranked higher than more difficult ones.
In this paper, we describe the system we sub-
mitted to the SemEval-2012 Lexical Simplification
Task. In order to rank the candidate substitutes of a
lexical form in descending order of simplicity, our
system (mmSystem) combines word frequency with
decompositional semantics criteria based on syntac-
tic structure. The mmSystem achieved an average
ranking if compared with the other participating sys-
tems and the baselines. We believe that the approach
proposed in this paper might help to shed light on
the interplay between linguistic features and cogni-
tive complexity in general.
</bodyText>
<sectionHeader confidence="0.893925" genericHeader="method">
2 The Lexical Simplification Task
</sectionHeader>
<bodyText confidence="0.999341588235294">
The SemEval-2012 Lexical Simplification Task re-
quires participating systems to automatically gen-
erate a ranking of lexical forms conveying similar
meanings on cognitive simplicity criteria and can be
defined as follows. Given a short text C called the
context and which generally corresponds to a sen-
tence, a target word T and a list Ls of candidate
substitutes for T, i.e. a list of quasi-synonyms of the
target word, the task for a system consists in pro-
viding a ranking on Ls such that the original list of
substitutes is sorted over simplicity, from the cogni-
tively simplest to the cognitively most difficult lexi-
cal form.
As the examples from (1) to (3) show, the Lexical
Simplification Task includes substitutes of different
syntactic complexity which might vary from simple
one-word substitutes as in (1) (the lexical forms that
</bodyText>
<page confidence="0.98024">
482
</page>
<note confidence="0.527837">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 482–486,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.982362944444444">
can function as substitutes include content words,
i.e. nouns (n), verbs (v), adjectives (a) and adverbs
(r)) to collocations, negated forms as in (2) or even
definition-like paraphrases as for instance wind and
knock the breath out of in example (3).
C: He suggested building an experimental hy-
pertext ’web’ for the worldwide.a community
of physicists who used CERN and its publica-
tions.
T: worldwide.a
Ls: worldwide, global, international
C: Go to hell! she remembers Paul yelling at
her shortly.r after their wedding.
T: shortly.r
Ls: soon, a little, just, almost immediately,
shortly, not long
C: Now however she was falling through that
skylight, the strong dark figure that had ap-
peared out of nowhere falling through with her,
his arms tightly entwined about her, his shoul-
der having winded.v her.
T: winded.v
Ls: knock her breathless, knock the wind out
of, choke, wind, knock the breath out of, knock
the air out of
The organizers of the Lexical Simplification Task
provide a corpus of 300 trial and 1710 test sentences
defining the context of the target word and the as-
sociated list of candidate substitutes. To produce a
gold standard, 5 human annotators manually ranked
the list of substitutes associated to each context. Fi-
nally, a scoring algorithm is provided for comput-
ing agreement between the output of the system and
the manually ranked gold standard. The scoring al-
gorithm is based on the Kappa measure for inter-
annotator agreement.
</bodyText>
<sectionHeader confidence="0.987275" genericHeader="method">
3 The mmSystem
</sectionHeader>
<bodyText confidence="0.997669130434783">
Our aim by participating in the SemEval-2012 Lexi-
cal Simplification Task (Task 1) was to investigate
the nature of lexical simplicity/complexity and to
identify the linguistic features that are responsible
for it. The system we have developed is a first step
in this direction. The idea behind our framework
is the following. We build on previous work (De-
vlin, 1999; Carroll et al., 1999) that approximate
simplicity with word frequency, such that the cog-
nitively simpler lexical form is the one that is more
frequent in the language. While this definition might
easily apply to one-word substitutes or collocations,
it poses some problems in the case of multi-word-
expressions or of syntactically more complex lexi-
cal forms (e.g. definition like paraphrases) like those
proposed in the substitute lists in the SemEval-2012
Task 1.
Our approach builds on the baseline definition of
simplicity based on word frequency and integrates
it with (de)compositional semantics considerations.
Therefore, in order to operationalize the notion of
simplicity in our system we adopt different strategies
depending on the syntactic complexity of the lexical
form that forms the substitute.
• In the case of one-word substitutes or common
collocations we use the frequency associated by
WordNet (Fellbaum, 1998) to the lexical form
as a metric to rank the substitutes, i.e. the
substitute with the highest frequency is ranked
higher. For instance, the lexical item intelligent
is ranked lower than clever as it has a lower
frequency in the language (as defined in Word-
Net).
• In the case of multi-words or syntactic complex
substitutes, we apply so-called relevance rules.
Those are based on (de)compositional semantic
criteria and attempt to identify a unique content
word in the substitute that better approximates
the whole lexical form. Thus, we assign to the
whole lexical form the frequency associated to
this most relevant content word and use it for
ranking the whole substitute. For instance, rel-
evance rules assign to multi-word substitutes
such as most able or not able the same fre-
quency, and namely that associated with the
content word able.
</bodyText>
<page confidence="0.996618">
483
</page>
<subsectionHeader confidence="0.968859">
3.1 Implementation
</subsectionHeader>
<bodyText confidence="0.999540166666667">
In this section we describe in more details the im-
plementation of the mmSystem. The system design
can be summarized as follows.
Step 1: POS-Tagging In the first step, context and
the associated substitutes are parsed1 so to ob-
tain a flat representation of their syntax. Ba-
sically at this level, we collect Part-Of-Speech
information for all content words in the context
as well as in the substitute list.
Step 2: Relevance Rules In the second step, de-
pending on the syntactic representation of the
substitutes, the system selects a relevance rule
that identifies the one-word lexical form that
will be used for representing the meaning of the
whole substitute.
Step 3: Word Sense Tagging The system ap-
plies word sense tagging and assigns a Word-
Net sense to the target words and their can-
didate substitutes. In this step, we rely
on the SenseRelate::TargetWord package (Pat-
wardhan et al., 2005) and use the Lesk algo-
rithm (Lesk, 1986) for word sense disambigua-
tion.
Step 4: Substitute Ranking Following (Carroll et
al., 1999) that pointed out that rare words gen-
erally have only one sense, in order to associate
a frequency index to each candidate substitute
(wi), we use the number of senses associated
by WordNet to a lexical item of a given part
of speech, as an approximation of its frequency
(fi). Further, we extract from WordNet the fre-
quency of the word sense (fwnsi) associated to
the lexical item wi at step 3. Words not found in
WordNet it assigned a null frequency (fi = 0,
fwnsi = 0). Finally, we rank the substitute in
the following way:
</bodyText>
<listItem confidence="0.575691">
• if f1 =� f2
</listItem>
<figure confidence="0.75030675">
w1 &lt; w2i if f1 &gt; f2 and
w2 &lt; w1 otherwise,
• else if f1 = f2
w1 &lt; w27 if fwns1 &gt; fwns2 and
</figure>
<table confidence="0.9413331875">
w2 &lt; w1 otherwise.
Input:
Sentence 993: ”It is light.a and easy to use.”
Substitutes: portable;unheavy;not heavy;light
Step 1: POS-Tagging
portable#A; unheavy#A; not#Neg heavy#A; light#A
Step 2: Relevance Rules
portable#A; unheavy#A; heavy#A#; light#A
Step 3: WSD
portable#A#wns:2; unheavy#A#wns:?; heavy#A#wns:2;
light#A#wns:25
Step 4: Ranking
portable#f:2; unheavy#f:0; heavy#f:27; light#f:25
not heavy &lt; light &lt; portable &lt; unheavy
Gold Ranking:
light &lt; not heavy &lt; portable &lt; unheavy
</table>
<tableCaption confidence="0.994015">
Table 1: Example of mmSystem processing steps.
Table 1 shows an example of data processing.
</tableCaption>
<subsectionHeader confidence="0.99557">
3.2 Relevance Rules
</subsectionHeader>
<bodyText confidence="0.999737272727273">
Relying on previous work on compositional seman-
tics of multi-word-expression (Reddy et al., 2011;
Venkatapathy and Joshi, 2005; Baldwin et al., 2003)
we defined a set of hand-written rules to assign the
relevant meaning to a complex substitute. Relevance
rules are used to decompose the meaning of a com-
plex structure and identify the most relevant word
conveying the semantics of the whole, so that the
frequency associated to the whole lexical form is ap-
proximated by the frequency of this most relevant
form:
</bodyText>
<listItem confidence="0.766297642857143">
• a one-word lexical item is mapped to itself, e.g.
run.v —* run.v
• a multi-word lexical form including only one
content word is mapped to this content word,
e.g. not.Neg nice.a —* nice.a or
be.Cop able.a —* able.a
• in the case of a multi-word lexical item includ-
ing more than one content word, we take into
account the syntactic structure of the lexical
item and apply heuristics to decide which con-
tent word is more relevant for the meaning of
the whole. The heuristics we used are based
on the empirical analysis of the trial data set
provided by the Task 1 organizers that contains
</listItem>
<footnote confidence="0.977019">
1We used the Stanford Parser (Klein and Manning, 2003).
</footnote>
<page confidence="0.998701">
484
</page>
<bodyText confidence="0.96462275">
about 300 contexts. As an example consider a
lexical item including a verb construction with
structure V1 + to + V2 that is mapped by our
rules to the second verb form V2, e.g. try.V1 to
</bodyText>
<note confidence="0.412869">
escape.V2 —* escape.V2.
</note>
<tableCaption confidence="0.8632855">
Table 2 shows some examples of relevance rules de-
fined in the mmSystem.
</tableCaption>
<table confidence="0.9864663">
Syntax Example R Form
V + Prep engage for V
Cop + Adj be able Adj
Cop + V be worried V
Adv + V anxiously anticipate Adv
Adj+N adnormal growth Adj
N1 + N2 death penalty N1
N1 + PrepOf + N2 person of authority N2
V+N take notice N
V1+to+V2 try to escape V2
</table>
<tableCaption confidence="0.999632">
Table 2: Example of relevance rules.
</tableCaption>
<bodyText confidence="0.9995734">
These relevance rules allow for a preliminary in-
vestigation of the nature of lexical complexity. For
instance, we found that in many cases, it is the mod-
ifying element of a complex expression that is re-
sponsible for a shift in lexical complexity:
</bodyText>
<listItem confidence="0.658011">
(4) a. lie&lt;say falsely&lt;say untruthfully
b. sample&lt; typical sample &lt; representative
sample
</listItem>
<sectionHeader confidence="0.999603" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999870085714286">
The Task 1 overall result can be found in (Specia
et al., 2012). The mmSystem achieved an average
ranking (score=0.289) if compared with the other
participating systems and the baselines that corre-
sponds to an absolute inter-annotator agreement be-
tween system output and golden-standard around
66%. Interestingly none of the systems achieved
an absolute agreement higher than 75% in this task.
This confirms that lexical simplification still remains
a difficult task and that the nature of the phenomena
underlying it should be better explored.
Table 3 shows the performance of our system per
syntactic category. The values are a bit higher than
in the official results of Task 1 as the system version
used for submission was buggy, however the rank-
ing of our system with respect to the other partici-
pating systems remains the same. Interestingly, the
best score were achieved for adverbs (0.352) and ad-
jectives (0.342). This can be explained with the fact
that the decompositional semantics of these category
is better accounted for by our rules.
The relative low performance achieved by the
mmSystem can be explained by the fact that our
rules only select one content word and use its fre-
quency for ranking. This metric alone is clearly not
enough to explain all cases of lexical simplification.
As an example of the complexity of this issue, con-
sider the interplay of negation and compositional se-
mantics: The negation of a very frequent verb form
might not be so simple to understand as its antonym,
e.g. don’t, not remember/forget vs. omit to, fail to
remember/forget. We believe, that a more system-
atic analysis of the lexical semantics involved in lex-
ical simplicity might improve the performance of the
system.
</bodyText>
<table confidence="0.99561225">
Noun Verb Adj Adv TOT
cAgr: 0.5 0.5 0.5 0.5 0.5
aAgr: 0.658 0.658 0.671 0.676 0.665
Score: 0.316 0.315 0.342 0.352 0.329
</table>
<tableCaption confidence="0.8664745">
Table 3: mmSystem scores per syntactic category. In the
table cAgr represents the agreement by chance, aAgr is
the absolute inter-annotator agreement between system
output and gold ranking and score is the normalized sys-
tem score. These values corresponds to P(A) and P(E)
observed in the data.
</tableCaption>
<sectionHeader confidence="0.997378" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.998911357142857">
In this paper we presented the mmSystem for lexical
simplification we submitted to the SemEval-2012
Task 1. The system combines simplification strate-
gies based on word frequency with decompositional
semantic criteria. The mmSystem achieved an aver-
age performance. The aim of our work was in fact
a preliminary investigation of the interplay between
(de)compositional semantics and lexical or cognitive
simplicity in general. Doubtlessly much remain to
be done in order to provide a more efficient formal-
ization of such effects. In future work, we want to
perform a wider corpus analysis and study the im-
pact of other linguistic features such as lexical se-
mantics on lexical simplicity.
</bodyText>
<page confidence="0.998847">
485
</page>
<sectionHeader confidence="0.990186" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999592578947369">
Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and
Dominic Widdows. 2003. An empirical model of
multiword expression decomposability. In Proceed-
ings of the ACL 2003 workshop on Multiword expres-
sions: analysis, acquisition and treatment - Volume 18,
MWE ’03, pages 89–96, Stroudsburg, PA, USA. Asso-
ciation for Computational Linguistics.
John Carroll, Guido Minnen, Darren Pearce, Yvonne
Canning, Siobhan Devlin, and John Tait. 1999. Sim-
plifying text for language-impaired readers. In In Pro-
ceedings of the 9th Conference of the European Chap-
ter of the Association for Computational Linguistics
(EACL, pages 269–270.
S. Devlin. 1999. Simplifying natural language for apha-
sic readers. Ph.D. thesis, University of Sunderland,
UK.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. Cambridge, MA: MIT Press.
Dan Klein and Christopher D. Manning. 2003. Accu-
rate unlexicalized parsing. In Proceedings of the 41st
Meeting of the Association for Computational Linguis-
tics, pages 423–430.
M. Lesk. 1986. Automatic sense disambiguation using
machine readable dictionaries: How to tell a pine cone
from a ice cream cone. In Proceedings of SIGDOV
’86.
Siddharth Patwardhan, Satanjeev Banerjee, and Ted Ped-
ersen. 2005. Senserelate::targetword - a generalized
framework for word sense disambiguation. In Pro-
ceedings of the Demonstration and Interactive Poster
Session of the 43rd Annual Meeting of the Association
for Computational Linguistics, pages 73–76, Ann Ar-
bor, MI.
Siva Reddy, Diana McCarthy, and Suresh Manandhar.
2011. An empirical study on compositionality in com-
pound nouns. In Proceedings of the International
Joint Conference on Natural Language Processing
2011 (IJCNLP-2011), Thailand.
Advaith Siddharthan. 2006. Syntactic simplification ant
text cohesion. Research on Language and Computa-
tion, 4(1):77–109.
Advaith Siddharthan. 2011. Text simplification using
typed dependencies: A comparision of the robustness
of different generation strategies. In Proceedings of
the 13th European Workshop on NLG.
Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea.
2012. Semeval-2012 task 1: English lexical simplifi-
cation. In Proceedings of the 6th International Work-
shop on Semantic Evaluation (SemEval 2012), Mon-
treal, Canada.
Sriram Venkatapathy and Aravind K. Joshi. 2005. Mea-
suring the relative compositionality of verb-noun (v-n)
collocations by integrating features. In Proceedings of
the conference on Human Language Technology and
Empirical Methods in Natural Language Processing,
HLT ’05, pages 899–906, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
</reference>
<page confidence="0.999041">
486
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.893791">
<title confidence="0.999134">SB: mmSystem - Using Decompositional Semantics for Simplification</title>
<author confidence="0.999976">Marilisa Amoia Massimo Romanelli</author>
<affiliation confidence="0.981791">Department of Applied Linguistics DFKI GmBH University of Saarland Saarbrcken, Germany</affiliation>
<email confidence="0.943593">m.amoia@mx.uni-saarland.deromanell@dfki.de</email>
<abstract confidence="0.998811307692308">In this paper, we describe the system we submitted to the SemEval-2012 Lexical Simplifi- Task. Our system combines word frequency with decompositional semantics criteria based on syntactic structure in order to rank candidate substitutes of lexical forms of arbitrary syntactic complexity (oneword, multi-word, etc.) in descending order of (cognitive) simplicity. We believe that the proposed approach might help to shed light on the interplay between linguistic features and lexical complexity in general.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Colin Bannard</author>
<author>Takaaki Tanaka</author>
<author>Dominic Widdows</author>
</authors>
<title>An empirical model of multiword expression decomposability.</title>
<date>2003</date>
<booktitle>In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment - Volume 18, MWE ’03,</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9531" citStr="Baldwin et al., 2003" startWordPosition="1522" endWordPosition="1525">POS-Tagging portable#A; unheavy#A; not#Neg heavy#A; light#A Step 2: Relevance Rules portable#A; unheavy#A; heavy#A#; light#A Step 3: WSD portable#A#wns:2; unheavy#A#wns:?; heavy#A#wns:2; light#A#wns:25 Step 4: Ranking portable#f:2; unheavy#f:0; heavy#f:27; light#f:25 not heavy &lt; light &lt; portable &lt; unheavy Gold Ranking: light &lt; not heavy &lt; portable &lt; unheavy Table 1: Example of mmSystem processing steps. Table 1 shows an example of data processing. 3.2 Relevance Rules Relying on previous work on compositional semantics of multi-word-expression (Reddy et al., 2011; Venkatapathy and Joshi, 2005; Baldwin et al., 2003) we defined a set of hand-written rules to assign the relevant meaning to a complex substitute. Relevance rules are used to decompose the meaning of a complex structure and identify the most relevant word conveying the semantics of the whole, so that the frequency associated to the whole lexical form is approximated by the frequency of this most relevant form: • a one-word lexical item is mapped to itself, e.g. run.v —* run.v • a multi-word lexical form including only one content word is mapped to this content word, e.g. not.Neg nice.a —* nice.a or be.Cop able.a —* able.a • in the case of a mu</context>
</contexts>
<marker>Baldwin, Bannard, Tanaka, Widdows, 2003</marker>
<rawString>Timothy Baldwin, Colin Bannard, Takaaki Tanaka, and Dominic Widdows. 2003. An empirical model of multiword expression decomposability. In Proceedings of the ACL 2003 workshop on Multiword expressions: analysis, acquisition and treatment - Volume 18, MWE ’03, pages 89–96, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Carroll</author>
<author>Guido Minnen</author>
<author>Darren Pearce</author>
<author>Yvonne Canning</author>
<author>Siobhan Devlin</author>
<author>John Tait</author>
</authors>
<title>Simplifying text for language-impaired readers. In</title>
<date>1999</date>
<booktitle>In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics (EACL,</booktitle>
<pages>269--270</pages>
<contexts>
<context position="1407" citStr="Carroll et al., 1999" startWordPosition="202" endWordPosition="205">exical simplification is a subtask of the more general text simplification task which attempts at reducing the cognitive complexity of a text so that it can be (better) understood by a larger audience. Text simplification has a wide range of applications which includes applications for the elderly, learners of a second language, children or people with cognitive deficiencies, etc. Works on text simplification mostly focus on reducing the syntactic complexity of the text (Siddharthan, 2011; Siddharthan, 2006) and only little work has addressed the issue of lexical simplification (Devlin, 1999; Carroll et al., 1999). The Lexical Simplification Task (Specia et al., 2012) proposed within the SemEval-2012 is the first attempt to explore the nature of the lexical simplification more systematically. This task requires participating systems, given a context and a target word, to automatically generate a ranking of substitutes, i.e. lexical forms conveying similar meanings to the target word, such that cognitively simpler lexical forms are ranked higher than more difficult ones. In this paper, we describe the system we submitted to the SemEval-2012 Lexical Simplification Task. In order to rank the candidate sub</context>
<context position="5352" citStr="Carroll et al., 1999" startWordPosition="836" endWordPosition="839">inally, a scoring algorithm is provided for computing agreement between the output of the system and the manually ranked gold standard. The scoring algorithm is based on the Kappa measure for interannotator agreement. 3 The mmSystem Our aim by participating in the SemEval-2012 Lexical Simplification Task (Task 1) was to investigate the nature of lexical simplicity/complexity and to identify the linguistic features that are responsible for it. The system we have developed is a first step in this direction. The idea behind our framework is the following. We build on previous work (Devlin, 1999; Carroll et al., 1999) that approximate simplicity with word frequency, such that the cognitively simpler lexical form is the one that is more frequent in the language. While this definition might easily apply to one-word substitutes or collocations, it poses some problems in the case of multi-wordexpressions or of syntactically more complex lexical forms (e.g. definition like paraphrases) like those proposed in the substitute lists in the SemEval-2012 Task 1. Our approach builds on the baseline definition of simplicity based on word frequency and integrates it with (de)compositional semantics considerations. There</context>
<context position="8151" citStr="Carroll et al., 1999" startWordPosition="1289" endWordPosition="1292">. Step 2: Relevance Rules In the second step, depending on the syntactic representation of the substitutes, the system selects a relevance rule that identifies the one-word lexical form that will be used for representing the meaning of the whole substitute. Step 3: Word Sense Tagging The system applies word sense tagging and assigns a WordNet sense to the target words and their candidate substitutes. In this step, we rely on the SenseRelate::TargetWord package (Patwardhan et al., 2005) and use the Lesk algorithm (Lesk, 1986) for word sense disambiguation. Step 4: Substitute Ranking Following (Carroll et al., 1999) that pointed out that rare words generally have only one sense, in order to associate a frequency index to each candidate substitute (wi), we use the number of senses associated by WordNet to a lexical item of a given part of speech, as an approximation of its frequency (fi). Further, we extract from WordNet the frequency of the word sense (fwnsi) associated to the lexical item wi at step 3. Words not found in WordNet it assigned a null frequency (fi = 0, fwnsi = 0). Finally, we rank the substitute in the following way: • if f1 =� f2 w1 &lt; w2i if f1 &gt; f2 and w2 &lt; w1 otherwise, • else if f1 = f</context>
</contexts>
<marker>Carroll, Minnen, Pearce, Canning, Devlin, Tait, 1999</marker>
<rawString>John Carroll, Guido Minnen, Darren Pearce, Yvonne Canning, Siobhan Devlin, and John Tait. 1999. Simplifying text for language-impaired readers. In In Proceedings of the 9th Conference of the European Chapter of the Association for Computational Linguistics (EACL, pages 269–270.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Devlin</author>
</authors>
<title>Simplifying natural language for aphasic readers.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Sunderland, UK.</institution>
<contexts>
<context position="1384" citStr="Devlin, 1999" startWordPosition="200" endWordPosition="201">Introduction Lexical simplification is a subtask of the more general text simplification task which attempts at reducing the cognitive complexity of a text so that it can be (better) understood by a larger audience. Text simplification has a wide range of applications which includes applications for the elderly, learners of a second language, children or people with cognitive deficiencies, etc. Works on text simplification mostly focus on reducing the syntactic complexity of the text (Siddharthan, 2011; Siddharthan, 2006) and only little work has addressed the issue of lexical simplification (Devlin, 1999; Carroll et al., 1999). The Lexical Simplification Task (Specia et al., 2012) proposed within the SemEval-2012 is the first attempt to explore the nature of the lexical simplification more systematically. This task requires participating systems, given a context and a target word, to automatically generate a ranking of substitutes, i.e. lexical forms conveying similar meanings to the target word, such that cognitively simpler lexical forms are ranked higher than more difficult ones. In this paper, we describe the system we submitted to the SemEval-2012 Lexical Simplification Task. In order to</context>
<context position="5329" citStr="Devlin, 1999" startWordPosition="833" endWordPosition="835">ach context. Finally, a scoring algorithm is provided for computing agreement between the output of the system and the manually ranked gold standard. The scoring algorithm is based on the Kappa measure for interannotator agreement. 3 The mmSystem Our aim by participating in the SemEval-2012 Lexical Simplification Task (Task 1) was to investigate the nature of lexical simplicity/complexity and to identify the linguistic features that are responsible for it. The system we have developed is a first step in this direction. The idea behind our framework is the following. We build on previous work (Devlin, 1999; Carroll et al., 1999) that approximate simplicity with word frequency, such that the cognitively simpler lexical form is the one that is more frequent in the language. While this definition might easily apply to one-word substitutes or collocations, it poses some problems in the case of multi-wordexpressions or of syntactically more complex lexical forms (e.g. definition like paraphrases) like those proposed in the substitute lists in the SemEval-2012 Task 1. Our approach builds on the baseline definition of simplicity based on word frequency and integrates it with (de)compositional semantic</context>
</contexts>
<marker>Devlin, 1999</marker>
<rawString>S. Devlin. 1999. Simplifying natural language for aphasic readers. Ph.D. thesis, University of Sunderland, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA:</location>
<contexts>
<context position="6259" citStr="Fellbaum, 1998" startWordPosition="975" endWordPosition="976">of syntactically more complex lexical forms (e.g. definition like paraphrases) like those proposed in the substitute lists in the SemEval-2012 Task 1. Our approach builds on the baseline definition of simplicity based on word frequency and integrates it with (de)compositional semantics considerations. Therefore, in order to operationalize the notion of simplicity in our system we adopt different strategies depending on the syntactic complexity of the lexical form that forms the substitute. • In the case of one-word substitutes or common collocations we use the frequency associated by WordNet (Fellbaum, 1998) to the lexical form as a metric to rank the substitutes, i.e. the substitute with the highest frequency is ranked higher. For instance, the lexical item intelligent is ranked lower than clever as it has a lower frequency in the language (as defined in WordNet). • In the case of multi-words or syntactic complex substitutes, we apply so-called relevance rules. Those are based on (de)compositional semantic criteria and attempt to identify a unique content word in the substitute that better approximates the whole lexical form. Thus, we assign to the whole lexical form the frequency associated to </context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Cambridge, MA: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Christopher D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Meeting of the Association for Computational Linguistics,</booktitle>
<pages>423--430</pages>
<contexts>
<context position="10536" citStr="Klein and Manning, 2003" startWordPosition="1700" endWordPosition="1703">al item is mapped to itself, e.g. run.v —* run.v • a multi-word lexical form including only one content word is mapped to this content word, e.g. not.Neg nice.a —* nice.a or be.Cop able.a —* able.a • in the case of a multi-word lexical item including more than one content word, we take into account the syntactic structure of the lexical item and apply heuristics to decide which content word is more relevant for the meaning of the whole. The heuristics we used are based on the empirical analysis of the trial data set provided by the Task 1 organizers that contains 1We used the Stanford Parser (Klein and Manning, 2003). 484 about 300 contexts. As an example consider a lexical item including a verb construction with structure V1 + to + V2 that is mapped by our rules to the second verb form V2, e.g. try.V1 to escape.V2 —* escape.V2. Table 2 shows some examples of relevance rules defined in the mmSystem. Syntax Example R Form V + Prep engage for V Cop + Adj be able Adj Cop + V be worried V Adv + V anxiously anticipate Adv Adj+N adnormal growth Adj N1 + N2 death penalty N1 N1 + PrepOf + N2 person of authority N2 V+N take notice N V1+to+V2 try to escape V2 Table 2: Example of relevance rules. These relevance rul</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Dan Klein and Christopher D. Manning. 2003. Accurate unlexicalized parsing. In Proceedings of the 41st Meeting of the Association for Computational Linguistics, pages 423–430.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of SIGDOV ’86.</booktitle>
<contexts>
<context position="8060" citStr="Lesk, 1986" startWordPosition="1277" endWordPosition="1278">nformation for all content words in the context as well as in the substitute list. Step 2: Relevance Rules In the second step, depending on the syntactic representation of the substitutes, the system selects a relevance rule that identifies the one-word lexical form that will be used for representing the meaning of the whole substitute. Step 3: Word Sense Tagging The system applies word sense tagging and assigns a WordNet sense to the target words and their candidate substitutes. In this step, we rely on the SenseRelate::TargetWord package (Patwardhan et al., 2005) and use the Lesk algorithm (Lesk, 1986) for word sense disambiguation. Step 4: Substitute Ranking Following (Carroll et al., 1999) that pointed out that rare words generally have only one sense, in order to associate a frequency index to each candidate substitute (wi), we use the number of senses associated by WordNet to a lexical item of a given part of speech, as an approximation of its frequency (fi). Further, we extract from WordNet the frequency of the word sense (fwnsi) associated to the lexical item wi at step 3. Words not found in WordNet it assigned a null frequency (fi = 0, fwnsi = 0). Finally, we rank the substitute in t</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: How to tell a pine cone from a ice cream cone. In Proceedings of SIGDOV ’86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siddharth Patwardhan</author>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>Senserelate::targetword - a generalized framework for word sense disambiguation.</title>
<date>2005</date>
<booktitle>In Proceedings of the Demonstration and Interactive Poster Session of the 43rd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>73--76</pages>
<location>Ann Arbor, MI.</location>
<contexts>
<context position="8020" citStr="Patwardhan et al., 2005" startWordPosition="1266" endWordPosition="1270"> Basically at this level, we collect Part-Of-Speech information for all content words in the context as well as in the substitute list. Step 2: Relevance Rules In the second step, depending on the syntactic representation of the substitutes, the system selects a relevance rule that identifies the one-word lexical form that will be used for representing the meaning of the whole substitute. Step 3: Word Sense Tagging The system applies word sense tagging and assigns a WordNet sense to the target words and their candidate substitutes. In this step, we rely on the SenseRelate::TargetWord package (Patwardhan et al., 2005) and use the Lesk algorithm (Lesk, 1986) for word sense disambiguation. Step 4: Substitute Ranking Following (Carroll et al., 1999) that pointed out that rare words generally have only one sense, in order to associate a frequency index to each candidate substitute (wi), we use the number of senses associated by WordNet to a lexical item of a given part of speech, as an approximation of its frequency (fi). Further, we extract from WordNet the frequency of the word sense (fwnsi) associated to the lexical item wi at step 3. Words not found in WordNet it assigned a null frequency (fi = 0, fwnsi = </context>
</contexts>
<marker>Patwardhan, Banerjee, Pedersen, 2005</marker>
<rawString>Siddharth Patwardhan, Satanjeev Banerjee, and Ted Pedersen. 2005. Senserelate::targetword - a generalized framework for word sense disambiguation. In Proceedings of the Demonstration and Interactive Poster Session of the 43rd Annual Meeting of the Association for Computational Linguistics, pages 73–76, Ann Arbor, MI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Siva Reddy</author>
<author>Diana McCarthy</author>
<author>Suresh Manandhar</author>
</authors>
<title>An empirical study on compositionality in compound nouns.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Joint Conference on Natural Language Processing</booktitle>
<contexts>
<context position="9478" citStr="Reddy et al., 2011" startWordPosition="1514" endWordPosition="1517">titutes: portable;unheavy;not heavy;light Step 1: POS-Tagging portable#A; unheavy#A; not#Neg heavy#A; light#A Step 2: Relevance Rules portable#A; unheavy#A; heavy#A#; light#A Step 3: WSD portable#A#wns:2; unheavy#A#wns:?; heavy#A#wns:2; light#A#wns:25 Step 4: Ranking portable#f:2; unheavy#f:0; heavy#f:27; light#f:25 not heavy &lt; light &lt; portable &lt; unheavy Gold Ranking: light &lt; not heavy &lt; portable &lt; unheavy Table 1: Example of mmSystem processing steps. Table 1 shows an example of data processing. 3.2 Relevance Rules Relying on previous work on compositional semantics of multi-word-expression (Reddy et al., 2011; Venkatapathy and Joshi, 2005; Baldwin et al., 2003) we defined a set of hand-written rules to assign the relevant meaning to a complex substitute. Relevance rules are used to decompose the meaning of a complex structure and identify the most relevant word conveying the semantics of the whole, so that the frequency associated to the whole lexical form is approximated by the frequency of this most relevant form: • a one-word lexical item is mapped to itself, e.g. run.v —* run.v • a multi-word lexical form including only one content word is mapped to this content word, e.g. not.Neg nice.a —* ni</context>
</contexts>
<marker>Reddy, McCarthy, Manandhar, 2011</marker>
<rawString>Siva Reddy, Diana McCarthy, and Suresh Manandhar. 2011. An empirical study on compositionality in compound nouns. In Proceedings of the International Joint Conference on Natural Language Processing 2011 (IJCNLP-2011), Thailand.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Syntactic simplification ant text cohesion.</title>
<date>2006</date>
<journal>Research on Language and Computation,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1299" citStr="Siddharthan, 2006" startWordPosition="186" endWordPosition="187">ed light on the interplay between linguistic features and lexical complexity in general. 1 Introduction Lexical simplification is a subtask of the more general text simplification task which attempts at reducing the cognitive complexity of a text so that it can be (better) understood by a larger audience. Text simplification has a wide range of applications which includes applications for the elderly, learners of a second language, children or people with cognitive deficiencies, etc. Works on text simplification mostly focus on reducing the syntactic complexity of the text (Siddharthan, 2011; Siddharthan, 2006) and only little work has addressed the issue of lexical simplification (Devlin, 1999; Carroll et al., 1999). The Lexical Simplification Task (Specia et al., 2012) proposed within the SemEval-2012 is the first attempt to explore the nature of the lexical simplification more systematically. This task requires participating systems, given a context and a target word, to automatically generate a ranking of substitutes, i.e. lexical forms conveying similar meanings to the target word, such that cognitively simpler lexical forms are ranked higher than more difficult ones. In this paper, we describe</context>
</contexts>
<marker>Siddharthan, 2006</marker>
<rawString>Advaith Siddharthan. 2006. Syntactic simplification ant text cohesion. Research on Language and Computation, 4(1):77–109.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Advaith Siddharthan</author>
</authors>
<title>Text simplification using typed dependencies: A comparision of the robustness of different generation strategies.</title>
<date>2011</date>
<booktitle>In Proceedings of the 13th European Workshop on NLG.</booktitle>
<contexts>
<context position="1279" citStr="Siddharthan, 2011" startWordPosition="183" endWordPosition="185">ch might help to shed light on the interplay between linguistic features and lexical complexity in general. 1 Introduction Lexical simplification is a subtask of the more general text simplification task which attempts at reducing the cognitive complexity of a text so that it can be (better) understood by a larger audience. Text simplification has a wide range of applications which includes applications for the elderly, learners of a second language, children or people with cognitive deficiencies, etc. Works on text simplification mostly focus on reducing the syntactic complexity of the text (Siddharthan, 2011; Siddharthan, 2006) and only little work has addressed the issue of lexical simplification (Devlin, 1999; Carroll et al., 1999). The Lexical Simplification Task (Specia et al., 2012) proposed within the SemEval-2012 is the first attempt to explore the nature of the lexical simplification more systematically. This task requires participating systems, given a context and a target word, to automatically generate a ranking of substitutes, i.e. lexical forms conveying similar meanings to the target word, such that cognitively simpler lexical forms are ranked higher than more difficult ones. In thi</context>
</contexts>
<marker>Siddharthan, 2011</marker>
<rawString>Advaith Siddharthan. 2011. Text simplification using typed dependencies: A comparision of the robustness of different generation strategies. In Proceedings of the 13th European Workshop on NLG.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
<author>Sujay K Jauhar</author>
<author>Rada Mihalcea</author>
</authors>
<title>Semeval-2012 task 1: English lexical simplification.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="1462" citStr="Specia et al., 2012" startWordPosition="210" endWordPosition="213">ext simplification task which attempts at reducing the cognitive complexity of a text so that it can be (better) understood by a larger audience. Text simplification has a wide range of applications which includes applications for the elderly, learners of a second language, children or people with cognitive deficiencies, etc. Works on text simplification mostly focus on reducing the syntactic complexity of the text (Siddharthan, 2011; Siddharthan, 2006) and only little work has addressed the issue of lexical simplification (Devlin, 1999; Carroll et al., 1999). The Lexical Simplification Task (Specia et al., 2012) proposed within the SemEval-2012 is the first attempt to explore the nature of the lexical simplification more systematically. This task requires participating systems, given a context and a target word, to automatically generate a ranking of substitutes, i.e. lexical forms conveying similar meanings to the target word, such that cognitively simpler lexical forms are ranked higher than more difficult ones. In this paper, we describe the system we submitted to the SemEval-2012 Lexical Simplification Task. In order to rank the candidate substitutes of a lexical form in descending order of simpl</context>
<context position="11527" citStr="Specia et al., 2012" startWordPosition="1885" endWordPosition="1888">ed V Adv + V anxiously anticipate Adv Adj+N adnormal growth Adj N1 + N2 death penalty N1 N1 + PrepOf + N2 person of authority N2 V+N take notice N V1+to+V2 try to escape V2 Table 2: Example of relevance rules. These relevance rules allow for a preliminary investigation of the nature of lexical complexity. For instance, we found that in many cases, it is the modifying element of a complex expression that is responsible for a shift in lexical complexity: (4) a. lie&lt;say falsely&lt;say untruthfully b. sample&lt; typical sample &lt; representative sample 4 Results The Task 1 overall result can be found in (Specia et al., 2012). The mmSystem achieved an average ranking (score=0.289) if compared with the other participating systems and the baselines that corresponds to an absolute inter-annotator agreement between system output and golden-standard around 66%. Interestingly none of the systems achieved an absolute agreement higher than 75% in this task. This confirms that lexical simplification still remains a difficult task and that the nature of the phenomena underlying it should be better explored. Table 3 shows the performance of our system per syntactic category. The values are a bit higher than in the official r</context>
</contexts>
<marker>Specia, Jauhar, Mihalcea, 2012</marker>
<rawString>Lucia Specia, Sujay K. Jauhar, and Rada Mihalcea. 2012. Semeval-2012 task 1: English lexical simplification. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sriram Venkatapathy</author>
<author>Aravind K Joshi</author>
</authors>
<title>Measuring the relative compositionality of verb-noun (v-n) collocations by integrating features.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>899--906</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="9508" citStr="Venkatapathy and Joshi, 2005" startWordPosition="1518" endWordPosition="1521">heavy;not heavy;light Step 1: POS-Tagging portable#A; unheavy#A; not#Neg heavy#A; light#A Step 2: Relevance Rules portable#A; unheavy#A; heavy#A#; light#A Step 3: WSD portable#A#wns:2; unheavy#A#wns:?; heavy#A#wns:2; light#A#wns:25 Step 4: Ranking portable#f:2; unheavy#f:0; heavy#f:27; light#f:25 not heavy &lt; light &lt; portable &lt; unheavy Gold Ranking: light &lt; not heavy &lt; portable &lt; unheavy Table 1: Example of mmSystem processing steps. Table 1 shows an example of data processing. 3.2 Relevance Rules Relying on previous work on compositional semantics of multi-word-expression (Reddy et al., 2011; Venkatapathy and Joshi, 2005; Baldwin et al., 2003) we defined a set of hand-written rules to assign the relevant meaning to a complex substitute. Relevance rules are used to decompose the meaning of a complex structure and identify the most relevant word conveying the semantics of the whole, so that the frequency associated to the whole lexical form is approximated by the frequency of this most relevant form: • a one-word lexical item is mapped to itself, e.g. run.v —* run.v • a multi-word lexical form including only one content word is mapped to this content word, e.g. not.Neg nice.a —* nice.a or be.Cop able.a —* able.</context>
</contexts>
<marker>Venkatapathy, Joshi, 2005</marker>
<rawString>Sriram Venkatapathy and Aravind K. Joshi. 2005. Measuring the relative compositionality of verb-noun (v-n) collocations by integrating features. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 899–906, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>