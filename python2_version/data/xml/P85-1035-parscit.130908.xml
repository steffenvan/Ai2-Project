<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.978435">
Towards a Self-Extending Lexicon*
</title>
<author confidence="0.6627755">
Uri Zernik
Michael G. Dyer
</author>
<affiliation confidence="0.684439">
Artificial Intelligence Laboratory
Computer Science Department
3531 Barker Hall
University of t alifornia
</affiliation>
<address confidence="0.510916">
Los Angeles, California 90024
</address>
<email confidence="0.156433">
David vs. Goliath
</email>
<sectionHeader confidence="0.969387" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999963722222222">
The problem of manually modifying the lexicon
appears with any natural language processing program.
Ideally, a program should be able to acquire new lexical
entries from context, the way people learn. We address
the problem of acquiring entire phrases, specifically
figurative phrases, through augmenting a phrasal lexicon.
Facilitating such a self-extending lexicon involves (a)
disambiguation—selection of the intended phrase from a
set of matching phrases, (b) robust
parsing—comprehension of partially-matching phrases,
and (c) error analysis—use of errors in forming hy-
potheses about new phrases. We have designed and im-
plemented a program called RINA which uses demons to
implement functional-grammar principles. RINA receives
new figurative phrases in context and through the appli-
cation of a sequence of failure-driven rules, creates and
refines both the patterns and the concepts which hold
syntactic and semantic information about phrases.
</bodyText>
<sectionHeader confidence="0.992135" genericHeader="introduction">
1. Introduction
</sectionHeader>
<bodyText confidence="0.999978375">
A language understanding program should be able
to acquire new lexical items from context, forming for
novel phrases their linguistic patterns and figuring out
their conceptual meanings. The lexicon of a learning
program should satisfy three requirements: Each lexical
entry should (1) be learnable, (2) facilitate conceptual
analysis, and (3) facilitate generation. In this paper we
focus on the first two aspects.
</bodyText>
<subsectionHeader confidence="0.999467">
1.1 The Task Domain
</subsectionHeader>
<bodyText confidence="0.999844571428571">
Two examples, which will be used throughout this
paper, are given below. In the first dialogue the learner
is introduced to an unknown phrase: take on. The
words take and on are familiar to the learner, who also
remembers the biblical story of David and Goliath. The
program, modeling a language learner, interacts with a
native speaker, as follows:
</bodyText>
<note confidence="0.634995">
e This work was made possible in part by a grant from the Keck
Foundation.
Native: Remember the story of David and Goliath?
David took on Goliath.
Learner: David took Goliath somewhere?
Native: No. David took on Goliath.
Learner: He took on him. He won the fight?
Native: No. He took him on.
David attacked him.
Learner: He took him on.
He accepted the challenge?
Native: Right.
Native: Here is another story.
John took on the third exam question.
Learner: He took on a hard problem.
</note>
<bodyText confidence="0.9717995">
Another dialogue involves put ones foot down. Again,
the phrase is unknown while its constituents are known:
</bodyText>
<subsectionHeader confidence="0.685277">
Going Punk
</subsectionHeader>
<bodyText confidence="0.910839208333333">
Native: Jenny wanted to go punk,
but her father put his foot down.
Learner: He moved his foot down?
It does not make sense.
Native: No. He put his foot down.
Learner: He put his foot down.
He refused to let her go punk.
)1 figurative phrase such as put one&apos;s foot down is a
linguistic pattern whose associated meaning cannot be
produced from the composition of its constituents.
Indeed, an interpretation of the phrase based on the
meanings of its constituents often exists, but it carries a
different meaning. The fact that this literal interpreta-
tion of the figurative phrase exists is a misleading clue in
learning. Furthermore, the learner may not even notice
that a novel phrase has been introduced since she is fam-
iliar with down as well as with foot. Becker Mecker75)
has described a space of phrases ranging in generality
from fixed proverbs such as charity begins at home
through idioms such as lay down the law and phrasal
verbs such as put up with one&apos;s spouse and look up the
name, to literal verb phrases such as sit on the chair.
He suggested employing a phrasal lexicon to capture this
entire range of language structures.
</bodyText>
<page confidence="0.994661">
284
</page>
<bodyText confidence="0.889768">
(3) Pattern Constructor: Learning of phrase patterns
is accomplished by analyzing parsing failures. Each
failure situation is associated with a pattern-
modification action.
</bodyText>
<subsectionHeader confidence="0.28713">
1.2 Issues in Phrase Acquisition
</subsectionHeader>
<bodyText confidence="0.6516235">
Three issues must be addressed when learning
phrases in context.
</bodyText>
<listItem confidence="0.809555111111111">
(1) Detecting failures: What are the indications that
the initial interpretation of the phrase take him on
as to take a person to a location&amp;quot; is incorrect? Since
all the words in the sentence are known, the problem
is detected both as a conceptual discrepancy (why
would he take his enemy anywhere?) and as a syn-
tactic failure (the expected location of the assumed
physical transfer is missing).
(2) Determining scope and generality of patterns:
</listItem>
<bodyText confidence="0.912974772727273">
The linguistic pattern of a phrase may be perceived
by the learner at various levels of generality. For ex-
ample, in the second dialogue, incorrect generaliza-
tions could yield patterns accepting sentences such
as:
Her boss put his left foot down.
He moved his foot down.
He put down his foot.
He put down his leg.
A decision is also required about the scope of the
pattern (i.e., the tokens included in the pattern).
For instance, the scope of the pattern in John put up
with Mary could be (1) ?x:person put:verb up where
with is associated with Mary or (2) ?x:person
put:verb up with ?y:person,wtere with is associated
with put up.
(3) Finding appropriate meanings: The conceptual
meaning of the phrase must be extracted from the
context which contains many concepts, both ap-
propriate and inappropriate for hypothesis forma-
tion. Thus there must be strategies for focusing on
appropriate elements in the context.
</bodyText>
<subsectionHeader confidence="0.992633">
1.3 The Program
</subsectionHeader>
<bodyText confidence="0.8854788">
RINA (Dyer851 is a computer program designed to
learn English phrases. It takes as input English sentences
which may include unknown phrases and conveys as out-
put its hypotheses about novel phrases. The program
consists of four components:
</bodyText>
<listItem confidence="0.7526735">
(I) Phrasal lexicon: This is a list of phrases where
each phrase is a declarative pattern-concept pair
(Wilensky811.
(2) Case-frame parser: In the parsing process, case-
frame expectations are handled by spawning demons
Pyer83I. The parser detects comprehension failures
which are used in learning.
(4) Concept Constructor: Learning of phrase concepts
</listItem>
<bodyText confidence="0.9684296">
is accomplished by a set of strategies which are
selected according to the context.
Schematically, the program receives a sequence of
sentence/context pairs from which it refines its current
pattern/concept pair. The pattern is derived from the
sentence and the concept is derived from the context.
However, the two processes are not independent since
the context influences construction of patterns while
linguistic clues in the sentence influence formation of
concepts.
</bodyText>
<sectionHeader confidence="0.961468" genericHeader="method">
2. Phrasal Representation of the Lexicon
</sectionHeader>
<bodyText confidence="0.98659725">
Parsing in RINA is central since learning is
evaluated in terms of parsing ability before and after
phrases are acquired. Moreover, learning is accomplished
through parsing.
</bodyText>
<subsectionHeader confidence="0.997226">
2.1 The Background
</subsectionHeader>
<bodyText confidence="0.955263392857143">
RINA combines elements of the following two ap-
proaches to language processing:
Phrase-based pattern matching: In the imple-
mentation of UC [Wilensky84j, an intelligent help system
for UNIX users, both PHRAN fArens821, the conceptual
analyzer. and PHRED Pacobs851 the generator, share a
phrasal lexicon. As outlined by Wilensky (Wilensky811
this lexicon provides a declarative database, being modu-
larly separated from the control part of the system which
carries out parsing and generation. This development in
representation of linguistic knowledge is paralleled by
theories of functional grammars Nay791, and lexical-
functional grammars Presnan781.
Case-based demon parsing: Boris (Dyer831
modeled reading and understanding stories in depth. Its
conceptual analyzer employed demon-based templates
for parsing and for generation. Demons are used in pars-
ing for two purposes: (I) to implement syntactic and se-
mantic expectations Pliesbeck741 and (2) to implement
memory operations such as search, match and update.
This approach implements Schank&apos;s (Schank771 theory of
representation of concepts, and follows case-grammar
IFillmore681 principles.
RINA uses a declarative phrasal lexicon as sug-
gested by Wilensky [Wilensky821, where a lexical phrase
is a pattern-concept pair. The pattern notation is
described below and the concept notation is Dyer&apos;s
(Dyer83] i-link notation.
</bodyText>
<page confidence="0.99603">
285
</page>
<subsectionHeader confidence="0.997523">
2.2 The Pattern Notation
</subsectionHeader>
<bodyText confidence="0.999800333333333">
To span English sentences, RINA uses two kinds
of patterns: lexical patterns and ordering patterns
jArens821. In Figure 1 we show sample lexical patterns
(patterns of lexical phrases). Such patterns are viewed as
the generic linguistic forms of their corresponding
phrases.
</bodyText>
<listItem confidence="0.870634333333333">
1. ?x:(animate.agent) nibble:verb &lt;on 7y:food&gt;
2. ?x:(person.agent) take:verb on ?y:patient
3.7k:(person,agent) &lt;put:verb foot:body-part down&gt;
</listItem>
<figureCaption confidence="0.994331">
Figure 1: The Pattern Notation
</figureCaption>
<bodyText confidence="0.688965">
The notation is explained below:
</bodyText>
<listItem confidence="0.980193785714286">
(1) A token is a literal unless otherwise specified. For ex-
ample, on is a literal in the patterns above.
(2) Tx:sort denotes a variable called ?x of a semantic
type sort. Ty:food above is a variable which stands
for references to objects of the semantic class food.
(3) Act:verb denotes any form of the verb syntactic
class with the root act. nibble:ver6 above stands for
expressions such as: nibbled, has never nibbled,
etc.
(4) By default, a pattern sequence does not specify the
order of its tokens.
(5) Tokens delimited by &lt; and &gt; are restricted to
their specified order. In Pattern 1 above, on must
directly precede ?y:food.
</listItem>
<bodyText confidence="0.9564555">
Ordering patterns pertain to language word-order con-
ventions in general. Some sample ordering patterns are:
</bodyText>
<figure confidence="0.36064075">
active: chcagent ?y:(verb.active),
passive: &lt;7x:patient ?y:(verb.passive)&gt;
*&lt;by ?z:agent&gt;
itlfinitive:&lt;to ft:verb.active&gt; .?y:agent
</figure>
<figureCaption confidence="0.5836885">
Figure 2: Ordering Patterns
The additional notation introduced here is:
</figureCaption>
<listItem confidence="0.890947647058823">
(6) An • preceding a term, such as s&lt;by ?a:agent&gt; in
the first pattern above indicates that the term is op-
tional.
denotes an omitted term. The concept for ?y in the
(7) third example above is extracted from the agent of
the pattern including the current pattern.
(8) By convention, the agent is the case-frame which
precedes the verb in the lexical pattern. Notice that
the notion of agent is necessary since (a) the agent is
not necessarily the subject (i.e., she was taken) and
(b) the agent is not necessarily the actor (i.e., She
received the book, he took a blow), and (c) in the
infinitive form, the agent must be referred to since
the agent is omitted from the pattern in the lexicon.
(9) Unification Kay79I accounts for the interaction of
lexical patterns with ordering patterns in matching
input sentences.
</listItem>
<bodyText confidence="0.99621775">
So far, we have given a declarative definition of our
grammar, a definition which is neutral with respect to ei-
ther parsing or generation. The parsing procedure which
is derived from the definitions above still has to be given.
</bodyText>
<subsectionHeader confidence="0.79733">
2...3 Parsing Objectives
</subsectionHeader>
<bodyText confidence="0.999859">
Three main tasks in phrasal parsing may be
identified, ordered by degree of difficulty.
</bodyText>
<listItem confidence="0.824407">
(1) Phrase disambiguation: When more than one lexi-
</listItem>
<bodyText confidence="0.860831695652174">
cal phrase matches the input sentence, the parser
must select the phrase intended by the speaker. For
example, the input the workers took to the streets
could mean either &amp;quot;they demonstrated&amp;quot; or &amp;quot;they were
fond of the streets&amp;quot;. In this case, the first phrase is
selected according to the principle of pattern
specificity (Arens821. The pattern 7x:person
take:verb &lt;to the streets&gt; is more specific then
?x:person take:verb &lt;to 7y:thing&gt; However, in
terms of our pattern notation, how do we define pat-
tern specificity?
(2) Ill-formed input comprehension: Even when an
input sentence is not well phrased according to text-
book grammar, it may be comprehensible by people
and so must be comprehensible to the parser. For
example, John took Mary school is telegraphic, but
comprehensible, while John took Mary to conveys
only a partial concept. Partially matching sentences
(or &amp;quot;near misses&amp;quot;) are not handled well by syntax-
driven pattern matchers. A deviation in a function
word (such as the word to above) might inhibit the
detection of the phrase which could be detected by a
semantics-driven parser.
</bodyText>
<listItem confidence="0.901879">
(3) Error-detection: when the hypothesized phrase
</listItem>
<bodyText confidence="0.932606111111111">
does not match the input sentence/context pair, the
parser is required to detect the failure and return
with an indication of its nature. Error analysis re-
quires that pattern tokens be assigned a case-
significance, as shown in Section 4.
Compounding requirements—disambiguation plus
error-analysis capability— complicate the design of the
parser. On one hand, analysis of &amp;quot;near misses&amp;quot; (they
bury a hatchet instead of they buried the hatchet) can
</bodyText>
<page confidence="0.986752">
286
</page>
<bodyText confidence="0.998522086956522">
be performed through a rigorous analysis—assuming the
presence of a single phrase only. On the other hand, in
the presence of multiple candidate phrases, disambigua-
tion could be made efficient by organizing sequences of
pattern tokens into a discrimination net. However, at-
tempting to perform both disambiguation and &amp;quot;near
miss&amp;quot; recognition and analysis simultaneously presents a
difficult problem. The discrimination net organization
would not enable comparing the input sentence, the
&amp;quot;near miss&amp;quot;, with existing phrases.
The solution is to organize the discrimination se-
quence by order of generality from the general to the
specific. According to this principle, verb phrases are
matched by conceptual features first and by syntactic
features only later on. For example, consider three ini-
tial erroneous hypotheses: (a) bury a hatchet (b) bury
the gun, and (c) bury the hash. On hearing the words
&amp;quot;bury the hatchet&amp;quot;, the first hypothesis would be the
easiest to analyze (it differs only by a function word
while the second differs by a content-holding word) and
the third one would be the hardest (as opposed to the
second, hash does not have a common concept with
hatchet).
</bodyText>
<subsectionHeader confidence="0.99699">
2.4 Case-Frames
</subsectionHeader>
<bodyText confidence="0.990276222222222">
Since these requirements are not facilitated by the
representation of patterns as given above, we slightly
modify our view of patterns. An entire pattern is con-
structed from a set of ease-frames where each case-frame
is constructed of single tokens: words and concepts.
Each frame has several slots containing information
about the case and pertaining to: (a) its syntactic ap-
pearance (b) its semantic concept and (c) its phrase role:
agent, patient. Variable identifiers (e.g., ?x, ?y) are
used for unification of phrase patterns with their
corresponding phrase concepts. Two example patterns
are given below:
The first example pattern denotes a simple literal
verb phrase:
{id:?x class:person role:agent}
{take:verb}
{id:?y class:person role:patient}
{id:?z class:location marker:to}
</bodyText>
<figureCaption confidence="0.983144">
Figure 3: Case Frames for &amp;quot;He took her to school&amp;quot;
</figureCaption>
<bodyText confidence="0.855296166666667">
Both the agent and the patient are of the class person;
the indirect object is a location marked by the preposi-
tion to. The second phrase is figurative:
{id:?x class:person role:agent}
{take:verb}
(markerto determiner:the word:streets}
</bodyText>
<figureCaption confidence="0.973588">
Figure 4: Case Frames for He took to the streets&amp;quot;
</figureCaption>
<bodyText confidence="0.999927857142857">
The third case frame in Figure 4 above, the indirect ob-
ject, does not have any corresponding concept. Rather it
is represented as a sequence of words. However the
words in the sequence are designated as the marker, the
determiner and the word itself.
Using this view of patterns enables the recognition
of &amp;quot;near misses&amp;quot; and facilitate error-analysis in parsing.
</bodyText>
<sectionHeader confidence="0.983796" genericHeader="method">
3. Demons Make Patterns Operational
</sectionHeader>
<bodyText confidence="0.94504168">
So far, we have described only the linguistic nota-
tion and indicated that unification [Kay791 accounts for
production of sentences from patterns. However, it is not
obvious how to make pattern unification operational in
parsing. One approach [Arens821 is to generate word se-
quences and to compare generated sequences with the in-
put sentence. Another approach [Pereira801 is to imple-
ment unification using PROLOG. Since our task is to
provide lenient parsing, namely also ill-formed sentences
must be handled by the parser, these two approaches are
not suitable. In our approach, parsing is carried out by
converting patterns into demons.
Conceptual analysis is the process which involves
reading input words left to right, matching them with
existing linguistic patterns and instantiating or modify-
ing in memory the associated conceptual meanings. For
example, assume that these are the phrases for take: in
the lexicon:
?x:person take:verb ?y:person ?z:locale
John took her to Boston.
?x:person take:verb ?y:phys-obj
He took the book.
?x:person take:verb off ?y:attire
He took off his coat.
?x:person take:verb on ?y:person
</bodyText>
<subsectionHeader confidence="0.734368">
David took on Goliath.
</subsectionHeader>
<bodyText confidence="0.942973857142857">
?x:person take:verb a bow
The actor took a bow.
?x:thing take:verb a blow
The wall took a blow.
?x:person take:verb &lt;to the streets&gt;
The workers took to the streets.
The juvenile took to the streets.
</bodyText>
<figureCaption confidence="0.997219">
Figure 5: A Variety of Phrases for TAKE
</figureCaption>
<bodyText confidence="0.999247">
where variables ?x, ?y and ?z also appear in correspond-
ing concepts (not shown here). How are these patterns
</bodyText>
<page confidence="0.952175">
287
</page>
<figure confidence="0.832747428571429">
actually applied in conceptual analysis?
3.1 Interaction of Lexical and Ordering Patterns
Token order in the lexical patterns themselves
(Figure 5) supports the derivation of simple active-voice
sentences only. Sentences such as:
• take (active, infinitive)
Do not anticipate the agent. The actor of the &amp;quot;take
on concept which is the agent, is extracted from the
agent of &amp;quot;decide&amp;quot;.
4. Failure-Driven Pattern Construction
Mary was taken on by John.
A weak contender David might have left alone,
but Goliath he took on.
David decided to take on Goliath.
</figure>
<figureCaption confidence="0.99994">
Figure 6: A Variety of Word Orders
</figureCaption>
<bodyText confidence="0.999959090909091">
cannot be derived directly by the given lexical patterns.
These sentences deviate from the order given by the
corresponding lexical patterns and require interaction
with language conventions such as passive voice and
infinitive. Ordering patterns are used to span a wider
range of sentences in the language. Ordering patterns
such as the one&apos;s given in Figure 2 depict the word order
involving verb phrases. In each pattern the case-frame
preceding the verb is specified. (In active voice, the agent
appears imediately before the verb, while in the passive
it is the patient that precedes the verb.)
</bodyText>
<subsectionHeader confidence="0.998938">
3.2 How Does It All Work?
</subsectionHeader>
<bodyText confidence="0.999365363636364">
Ordering patterns are compiled into demons. For
example, D_AGENT, the demon anticipating the agent
of the phrase is generated by the patterns in Figure 2. ti
has three clauses:
If the verb is in active form
then the agent is immediately before the verb
If the verb is in passive form
then the agent may appear, preceded by by.
If the verb is in infinitive
then the agent is omitted.
Its concept is obtained from the function verb.
</bodyText>
<figureCaption confidence="0.999196">
Figure 7: The Construction of D_AGENT
</figureCaption>
<bodyText confidence="0.961089666666667">
In parsing, this demon is spawned when a verb is en-
countered. For example, consider the process in parsing
the sentence
David decided to take on Goliath.
Through identifying the verbs and their forms, the pro-
cess is:
</bodyText>
<listItem confidence="0.68808">
• decided (active, simple)
</listItem>
<bodyText confidence="0.998887857142857">
Search for the agent before the verb, anticipate an
infinitive form.
Learning of phrases in RINA is an iterative pro-
cess. The input is a sequence of sentence-context pairs,
through which the program refines its current hypothesis
about the new phrase. The hypothesis pertains to both
the pattern and the concept of the phrase.
</bodyText>
<subsectionHeader confidence="0.98769">
4.2 The Learning Cycle
</subsectionHeader>
<bodyText confidence="0.908424">
The basic cycle in the process is:
</bodyText>
<listItem confidence="0.906666">
(a) A sentence is parsed on the background of a concep-
tual context.
(b) Using the current hypothesis, either the sentence is
comprehended smoothly, or a failure is detected.
(c) If a failure is detected then the current hypothesis is
updated.
</listItem>
<bodyText confidence="0.999594666666667">
The crucial point in this scheme is to obtain from the
parser an intelligible analysis of failures. As an example,
consider this part of the first dialog:
</bodyText>
<listItem confidence="0.653452666666667">
1 Program: He took on his. He won the fight?
2 User No. He took him on. David attacked him.
3 Program: He took him on.
</listItem>
<bodyText confidence="0.908695">
He accepted the challenge?
The first hypothesis is shown in Figure 8.
pattern: ?x:person take:verb &lt;on ?y:person&gt;
concept: Tx win the conflict with ?y
</bodyText>
<figureCaption confidence="0.989849">
Figure 8: First Hypothesis
</figureCaption>
<bodyText confidence="0.999837714285714">
Notice that the preposition on is attached to the object
?y, thus assuming that the phrase is similar to He looked
at Mary which cannot produce the following sentence: He
looked her at. This hypothesis underlies Sentence 1
which is erroneous in both its form and its meaning.
Two observations should be made by comparing this pat-
tern to Sentence 2:
</bodyText>
<listItem confidence="0.983589">
• The object is not preceded by the preposition on.
• The preposition on does not precede any object.
</listItem>
<bodyText confidence="0.987212">
These comments direct the construction of the new hy-
pothesis:
</bodyText>
<page confidence="0.994262">
288
</page>
<note confidence="0.3611965">
pattern: ?x:person take:verb on ?y:person
concept: ?x win the conflict with ?y
</note>
<figureCaption confidence="0.998143">
Figure 9: Second Hypothesis
</figureCaption>
<bodyText confidence="0.99241225">
where the preposition on is taken as a modifier of the
verb itself, thus correctly generating Sentence 3. In Fig-
ure 9 the conceptual hypothesis is still incorrect and
must itself be modified.
</bodyText>
<subsectionHeader confidence="0.999979">
4.3 Learning Strategies
</subsectionHeader>
<bodyText confidence="0.998238666666666">
A subset of RINA&apos;s learning strategies, the ones
used for the David and Goliath Dialog (Section 1.1) are
described in this section. In our exposition of failures
and actions we will illustrate the situations involved in
the dialogues above, where each situation is specified by
the following five ingredients:
</bodyText>
<listItem confidence="0.917996">
(1) the input sentence (Sentence),
(2) the context (not shown explicitly here),
(3) the active pattern: either the pattern under con-
</listItem>
<bodyText confidence="0.768002428571428">
struction, or the best matching pattern if this is the
first sentence in the dialogue (Pattern!).
(4) the failures detected in the current situation
(Failures),
&lt;on ?x:platform&gt; (indicating a spatial relation)
&lt;on ?x:time-unit&gt; (indicating a time of action)
&lt;on ?x:location&gt; (indicating a place)
</bodyText>
<note confidence="0.8657155">
Sentence: David took on Goliath.
Pattern!: ?x:person take:verb
</note>
<subsectionHeader confidence="0.3966855">
Failures: Prepositional mismatch
Pattern2: ?x:person take:verb &lt;on ?y:person&gt;
</subsectionHeader>
<bodyText confidence="0.583821363636364">
The preposition on is not part of the active pat-
tern. Neither does it match any of the prepositional
phrases which currently exist for on. Therefore, since it
cannot be interpreted in any other way, the ordering of
the sub-expression &lt;on ?rperson&gt; is frozen in the larger
pattern, using &lt; and &gt;.
Two-word verbs present a difficulty to language
learners [111m75) who tend to ignore the separated verb-
particle form, generating: take on his instead of take
him on. In the situation above, the learner produced this
typical error.
</bodyText>
<subsectionHeader confidence="0.95754">
Relaxing an Undergeneralized Pattern
</subsectionHeader>
<bodyText confidence="0.980711">
Two failures involving on: (1) case-role mismatch (on
?y:person is not found) and (2) prepositional mismatch
(on appears unmatched at the end of the sentence) are
encountered in the situation below:
(5) the pattern resulting from the application of the ac- Sentence: David took him on.
tion to the current pattern (Pattern2). Pattern!: ?x:person take:verb &lt;on ?y:person&gt;
Creating a New Phrase Failures: Prepositional and case-role mismatch.
Pattern2: ?x:person take:verb on ?y:person
A case-role mismatch occurs when the input sen-
tence can only be partially matched by the active pat-
tern. A goal mismatch occurs when the concept instan-
tiated by the selected pattern does not match the goal si-
tuation in the context.
</bodyText>
<subsectionHeader confidence="0.79842225">
Sentence: David took on Goliath.
Pattern!: ?x:person take:verb ?y:person ?z:location
Failures: Pattern and goal mismatch
Pattern2: ?x:person take:verb
</subsectionHeader>
<bodyText confidence="0.9268212">
David&apos;s physically transferring Goliath to a loca-
tion fails since (1) a location is not found and (2) the ac-
tion does not match David&apos;s goals. If these two failures
are encountered, then a new phrase is created. In ab-
sence of a better alternative, RINA initially generates
</bodyText>
<subsectionHeader confidence="0.657361666666667">
David took him somewhere.
Discriminating a Pattern by Freezing a Preposi-
tional Phrase
</subsectionHeader>
<bodyText confidence="0.99922175">
A prepositional mismatch occurs when a preposi-
tion P matches in neither the active pattern nor in one
of the lexical prepositional phrases, such as:
The combination of these two failures indicate
that the pattern is too restrictive. Therefore, the &lt; and
&gt; freezing delimiters are removed, and the pattern may
now account for two-word verbs. In this case on can be
separated from take.
</bodyText>
<subsectionHeader confidence="0.999791">
Generalizing a Semantic Restriction
</subsectionHeader>
<bodyText confidence="0.774527">
A semantic mismatch is marked when the seman-
tic class of a variable in the pattern does not subsume
the class of the corresponding concept in the sentence,
Sentence: John took on the third question.
Pattern!: ?x:person take:verb on ?y:person
</bodyText>
<subsectionHeader confidence="0.8784035">
Failures: Semantic mismatch
Pattern2: ?x:person take:verb on ?y:task
</subsectionHeader>
<bodyText confidence="0.9486125">
As a result, the type of ?y in the pattern is generalized to
include both cases.
</bodyText>
<page confidence="0.994003">
289
</page>
<subsectionHeader confidence="0.947511">
Freezing a Reference Which Relates to a Metaphor
</subsectionHeader>
<bodyText confidence="0.988307277777778">
An unrelated reference is marked when a reference
in the sentence does not relate to the context, but rather
it relates to a metaphor (see elaboration in [Zernik851 ).
The reference his foot cannot be resolved in the con-
text, rather it is resolved by a metaphoric gesture.
Sentence: Her father put his foot down.
Patternl: ?x:person put:verb down ?y:phys-obj
Failures: Goal mismatch and unrelated reference
Pattern2: ?x:person put:verb down foot:body-part
Since, (1) putting his foot on the floor does not
match any of the goals of Jenny&apos;s father and (2) the
reference his foot is related to the domain of metaphor-
ic gestures rather than to the context. Therefore, foot.
becomes frozen in the pattern. This method is similar to
a method suggested by Fass and Wilks [Fass831. In their
method, a metaphor is analyzed when an apparently ill-
formed input is detected, e.g.: the car drank a lot of
gas.
</bodyText>
<subsectionHeader confidence="0.999272">
4.4 Concept Constructor
</subsectionHeader>
<bodyText confidence="0.999661857142857">
Each pattern has an associated concept which is
specified using Dyer&apos;s Pyer831 i-link notation. The con-
cept of a new phrase is extracted from the context,
which may contain more than one element. For example,
in the first dialogue above, the given context contains
some salient story points [Wilensky82J which are indexed
in episodic memory as two violated expectations:
</bodyText>
<listItem confidence="0.99262925">
• David won the fight in spite of Goliath&apos;s physical su-
periority.
• David accepted the challenge in spite of the risk in-
volved.
</listItem>
<bodyText confidence="0.997441666666667">
The program extracts meanings from the given set of
points. Concept hypothesis construction is further dis-
cussed in [Zernik851.
</bodyText>
<sectionHeader confidence="0.975537" genericHeader="method">
5. Previous Work in Language Learning
</sectionHeader>
<bodyText confidence="0.995235490909091">
In RINA, the stimulus for learning is comprehen-
sion failure. In previous models language learning was
also driven by detection of failures.
PST (Reekerill] learned grammar by acting upon
differences detected between the input sentence and
internally generated sentences. Six types of differences
were classified, and the detection of a difference which
belonged to a class caused the associated alteration of
the grammar.
FOUL-LT [Granger771 learned meanings of single
words when an unknown word was encountered. The
meaning was extracted from the script (Schank771 which
was given as the context. A typical learning situation
was The car was driving on Hwy 66. when it careened
off the road. The meaning of the unknown verb
careened was guessed from the SACCIDENT script.
POLITICS [Carbone11791, which modeled
comprehension of text involving political concepts, ini-
tiated learning when semantic constrains were violated.
Constraints were generalized by analyzing underlying
metaphors.
AMBER (Langley82) modeled learning of basic
sentence structure. The process of learning was directed
by mismatches between input sentences and sentences
generated by the program. Learning involved recovery
from both errors of omission (omitting a function word
such as the or is in daddy bouncing ball) and errors Of
commission (producing daddy is liking dinner).
Thus, some programs acquired linguistic patterns
and some programs acquired meanings from context, but
none of the above programs acquired new phrases. Ac-
quisition of phrases involves two parallel processes: the
formation of the pattern from the given set of example
sentences, and the construction of the meaning from the
context. These two processes are not independent since
the construction of the conceptual meaning utilizes
linguistic clues while the selection of pattern elements of
new figurative phrases bears on concepts in the context.
8. Current and Future Work
Currently, RINA can learn a variety of phrasal
verbs and idioms. For example, RINA implements the
behavior of the learner in David vs. Goliath and in Go-
ing Punk in Section I. Modifications of lexical entries are
driven by analysis of failures. This analysis is similar to
analysis of ill-formed input, however, detection of failures
may result in the augmentation of the lexicon. Failures
appear as semantic discrepancies (e.g., goal-plan
mismatch), or syntactic discrepancies (e.g., case-role
mismatch). Finally, references in figurative phrases are
resolved by metaphor mapping.
Currently our efforts are focussed on learning the
conceptual elements of phrases. We attempt to develop
strategies for generalizing and refining acquired concepts.
For example, it is desirable to refine the concept for
&amp;quot;take on by this sequence of examples:
</bodyText>
<subsectionHeader confidence="0.494731">
David took on Goliath.
</subsectionHeader>
<bodyText confidence="0.887102">
The lakers took on the Celtics.
I took on a hard task.
I took on a new job.
In selecting the name °Towards a Self-Extending
Lexicon°, we took on an old name.
</bodyText>
<equation confidence="0.556037666666667">
[Fass831
(Fillmore68I
[Granger771
</equation>
<bodyText confidence="0.986163307692308">
The first three examples &amp;quot;deciding to fight someone&amp;quot;,
&amp;quot;playing against someone&amp;quot; and &amp;quot;accepting a challenge&amp;quot;
could be generalized into the same concept, but the last
two examples deviate in their meanings from that
developed concept. The problem is to determine the
desired level of generality. Clearly, the phrases in the
following examples:
take on an enemy
take on an old name
take on the shape of a snake
deserve separate entries in the phrasal lexicon. The
question is, at what stage is the advantage of further
generalization diminished?
</bodyText>
<sectionHeader confidence="0.992173" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.992545">
We wish to thank Erik Mueller and Mike Gasser
for their incisive comments on drafts of this paper.
</bodyText>
<sectionHeader confidence="0.954606" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9785651875">
Fass, Dan and Yorick Wilks, &amp;quot;Prefer-
ence Semantics, Ill-Formedness and
Metaphor,&amp;quot; American Journal of Com-
putational Linguistics 9(3-4), pp.178-
187 (1983).
Fillmore, C., &amp;quot;The Case for Case,&amp;quot; pp.
1-90 in Universals in Linguistic Theory,
ed. E. Bach R. Harms, Holt, Reinhart
and Winston, Chicago (1968).
Granger, R. H., &amp;quot;FOUL-UP: A Pro-
gram That Figures Out Meanings of
Words from Context,&amp;quot; pp. 172-178 in
Proceedings Fifth IJCAI , Cambridge,
Massachusets (August 1977).
Jacobs, Paul S., &amp;quot;PHRED: A Generator
for Natural Language Interfaces,&amp;quot;
UCB/CSD 85/198,, Computer Science
Division, University of California
Berkeley, Berkeley, California (Janu-
ary 1985).
pacobs851
[Kay791
[Arens82I Arens, Y., &amp;quot;The Context Model:
Language Understanding in a Con-
text,&amp;quot; in Proceedings Fourth Annual
Conference of the Cognitive Science So-
ciety, Ann Arbor, Michigan (1982).
lBecker751 Becker, Joseph D., &amp;quot;The Phrasal Lexi-
con,&amp;quot; pp. 70-73 in Proceedings Interdis-
ciplinary Workshop on Theoretical Is-
sues in Natural Language Processing,
Cambridge, Massachusets (June 1975).
[Bresnan781 Bresnan, Joan, &amp;quot;A Realistic Transfor-
mational Grammar,&amp;quot; pp. 1-59 in
Linguistic Theory and Psychological
Reality, ed. M. Halle J. Bresnan G.
Miller, MIT Press, Harvard, Mas-
sachusets (1978).
[Carbone1179] Carbonell, J. G., &amp;quot;Towards a Self-
Extending Parser,&amp;quot; pp. 3-7 in Proceed-
ings 17th Annual Meeting of the Associ-
ation for Computational Linguistics, La
Jolla, California (1979).
[Dyer83) Dyer, Michael G., In-Depth Under-
standing: A Computer Model of In-
tegrated Processing for Narrative
Comprehension, MIT Press, Cam-
bridge, MA (1983).
[Dyer851 Dyer, Michael G. and Uri Zernik,
&amp;quot;Parsing Paradigms and Language
Learning,&amp;quot; in Proceedings AI-85, Long
Beach, California (May 1085).
Kay, Martin, &amp;quot;Functional Grammar,&amp;quot;
pp. 142-158 in Proceedings 5th Annual
Meeting of the Berkeley Linguistic So-
ciety, Berkeley, California (1979).
[Lan gley82I Langley, Pat, &amp;quot;Language Acquisition
Through Error Recovery,&amp;quot; Cognition
and Brain Theory 6(3), pp.211-255
(1982).
[Pereira801 Pereira, F. C. N. and David H. D. War-
ren, &amp;quot;Definite Clause Grammars for
Language Analysis- A Survey of the
Formalism and a Comparison with
Augmented Transition Networks.&amp;quot;
Artificial Intelligence 13, pp.231-278
(1980).
[Reek er761 Reeker, L. H., &amp;quot;The Computational
Study of Language Learning,&amp;quot; in Ad-
vances in Computers, ed. M. Yovits M.
Rubinoff, Academic Press, New York
(1976).
[Riesbeck74) Riesbeck, C. K., &amp;quot;Computational
Understanding: Analysis of Sentences
and Context,&amp;quot; Memo 238, Al Labora-
tory (1974).
ISchank77) Schank, Roger and Robert Abelson,
Scripts Plans Coals and Understanding,
Lawrence Erlbaum Associates, Hills-
dale, New Jersey (1977).
</reference>
<page confidence="0.993074">
291
</page>
<note confidence="0.734245">
[Ulm751 Ulm, Susan C., &amp;quot;The Separation
</note>
<bodyText confidence="0.316747">
Phenomenon in English Phrasal Verbs,
Double trouble,&amp;quot; 601, University of
California Los Angeles (1975). M.A.
Thesis.
</bodyText>
<reference confidence="0.9966929">
[Wilensky811 Wilensky, R., &amp;quot;A Knowledge-Based
Approach to Natural Language Pro-
cessing: A progress Report,&amp;quot; in
Proceedings Seventh International Joint
Conference on Artificial Intelligence,
Vancouver, Canada (1081).
[Wilensky821 Wilensky, R., &amp;quot;Points: A Theory of
Structure of Stories in Memory,&amp;quot; pp.
345-375 in Strategies for Natural
Language Processing, ed. W. G.
Lehnert M. H. Ringle, Laurence Erl-
baum Associates, New Jersey (1082).
Milensky841 Wilensky, R., Y. Arens, and D. Chin,
&amp;quot;Talking to UNIX in English: an Over-
view of UC,&amp;quot; Communications of the
ACM 27(6), pp.574-593 (June 1984).
[Zernik851 Zernik, Uri and Michael G. Dyer,
Failure-Driven Aquisition of Figurative
Phrases by Second Language Speakers,
1085. (submitted to publication).
</reference>
<page confidence="0.997479">
292
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.911121">
<title confidence="0.999572">Towards a Self-Extending Lexicon*</title>
<author confidence="0.9785625">Uri Zernik Michael G Dyer</author>
<affiliation confidence="0.999978">Artificial Intelligence Laboratory Computer Science Department</affiliation>
<address confidence="0.99695">3531 Barker Hall</address>
<affiliation confidence="0.998265">University of t alifornia</affiliation>
<address confidence="0.999382">Los Angeles, California 90024</address>
<author confidence="0.970723">Goliath</author>
<abstract confidence="0.999259947368421">The problem of manually modifying the lexicon appears with any natural language processing program. Ideally, a program should be able to acquire new lexical entries from context, the way people learn. We address the problem of acquiring entire phrases, specifically phrases, augmenting a phrasal Facilitating such a self-extending lexicon involves (a) disambiguation—selection of the intended phrase from a of matching phrases, (b) robust parsing—comprehension of partially-matching phrases, and (c) error analysis—use of errors in forming hypotheses about new phrases. We have designed and ima program called RINA which uses to principles. RINA receives new figurative phrases in context and through the application of a sequence of failure-driven rules, creates and the and the concepts which hold syntactic and semantic information about phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Dan Fass</author>
<author>Yorick Wilks</author>
</authors>
<title>Preference Semantics, Ill-Formedness and Metaphor,&amp;quot;</title>
<date>1983</date>
<journal>American Journal of Computational Linguistics</journal>
<volume>9</volume>
<issue>3</issue>
<pages>178--187</pages>
<marker>Fass, Wilks, 1983</marker>
<rawString>Fass, Dan and Yorick Wilks, &amp;quot;Preference Semantics, Ill-Formedness and Metaphor,&amp;quot; American Journal of Computational Linguistics 9(3-4), pp.178-187 (1983).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fillmore</author>
</authors>
<title>The Case for Case,&amp;quot;</title>
<date>1968</date>
<booktitle>in Universals in Linguistic Theory,</booktitle>
<pages>1--90</pages>
<editor>ed. E. Bach R. Harms,</editor>
<location>Holt, Reinhart and Winston, Chicago</location>
<marker>Fillmore, 1968</marker>
<rawString>Fillmore, C., &amp;quot;The Case for Case,&amp;quot; pp. 1-90 in Universals in Linguistic Theory, ed. E. Bach R. Harms, Holt, Reinhart and Winston, Chicago (1968).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R H Granger</author>
</authors>
<title>FOUL-UP: A Program That Figures Out Meanings of Words from Context,&amp;quot;</title>
<date>1977</date>
<booktitle>in Proceedings Fifth IJCAI ,</booktitle>
<pages>172--178</pages>
<location>Cambridge, Massachusets</location>
<marker>Granger, 1977</marker>
<rawString>Granger, R. H., &amp;quot;FOUL-UP: A Program That Figures Out Meanings of Words from Context,&amp;quot; pp. 172-178 in Proceedings Fifth IJCAI , Cambridge, Massachusets (August 1977).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul S Jacobs</author>
</authors>
<title>PHRED: A Generator for Natural Language Interfaces,&amp;quot;</title>
<date>1985</date>
<tech>UCB/CSD 85/198,,</tech>
<institution>Computer Science Division, University of California Berkeley,</institution>
<location>Berkeley, California</location>
<marker>Jacobs, 1985</marker>
<rawString>Jacobs, Paul S., &amp;quot;PHRED: A Generator for Natural Language Interfaces,&amp;quot; UCB/CSD 85/198,, Computer Science Division, University of California Berkeley, Berkeley, California (January 1985).</rawString>
</citation>
<citation valid="true">
<title>The Context Model: Language Understanding in a Context,&amp;quot; in</title>
<date>1982</date>
<booktitle>Proceedings Fourth Annual Conference of the Cognitive Science Society,</booktitle>
<location>Ann Arbor, Michigan</location>
<marker>1982</marker>
<rawString>[Arens82I Arens, Y., &amp;quot;The Context Model: Language Understanding in a Context,&amp;quot; in Proceedings Fourth Annual Conference of the Cognitive Science Society, Ann Arbor, Michigan (1982).</rawString>
</citation>
<citation valid="true">
<authors>
<author>lBecker751 Becker</author>
<author>D Joseph</author>
</authors>
<title>The Phrasal Lexicon,&amp;quot;</title>
<date>1975</date>
<booktitle>in Proceedings Interdisciplinary Workshop on Theoretical Issues in Natural Language Processing,</booktitle>
<pages>70--73</pages>
<location>Cambridge, Massachusets</location>
<marker>Becker, Joseph, 1975</marker>
<rawString>lBecker751 Becker, Joseph D., &amp;quot;The Phrasal Lexicon,&amp;quot; pp. 70-73 in Proceedings Interdisciplinary Workshop on Theoretical Issues in Natural Language Processing, Cambridge, Massachusets (June 1975).</rawString>
</citation>
<citation valid="true">
<title>A Realistic Transformational Grammar,&amp;quot;</title>
<date>1978</date>
<booktitle>in Linguistic Theory and Psychological Reality,</booktitle>
<pages>1--59</pages>
<editor>ed. M. Halle J. Bresnan G. Miller,</editor>
<publisher>MIT Press,</publisher>
<location>Harvard, Massachusets</location>
<marker>1978</marker>
<rawString>[Bresnan781 Bresnan, Joan, &amp;quot;A Realistic Transformational Grammar,&amp;quot; pp. 1-59 in Linguistic Theory and Psychological Reality, ed. M. Halle J. Bresnan G. Miller, MIT Press, Harvard, Massachusets (1978).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J G Carbonell</author>
</authors>
<title>Towards a SelfExtending Parser,&amp;quot;</title>
<date>1979</date>
<booktitle>in Proceedings 17th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>3--7</pages>
<location>La Jolla, California</location>
<marker>Carbonell, 1979</marker>
<rawString>[Carbone1179] Carbonell, J. G., &amp;quot;Towards a SelfExtending Parser,&amp;quot; pp. 3-7 in Proceedings 17th Annual Meeting of the Association for Computational Linguistics, La Jolla, California (1979).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael G Dyer</author>
</authors>
<title>In-Depth Understanding: A Computer Model of Integrated Processing for Narrative Comprehension,</title>
<date>1983</date>
<publisher>MIT Press,</publisher>
<location>Cambridge, MA</location>
<marker>Dyer, 1983</marker>
<rawString>[Dyer83) Dyer, Michael G., In-Depth Understanding: A Computer Model of Integrated Processing for Narrative Comprehension, MIT Press, Cambridge, MA (1983).</rawString>
</citation>
<citation valid="true">
<title>Parsing Paradigms and Language Learning,&amp;quot;</title>
<date>1085</date>
<booktitle>in Proceedings AI-85,</booktitle>
<location>Long Beach, California</location>
<marker>1085</marker>
<rawString>[Dyer851 Dyer, Michael G. and Uri Zernik, &amp;quot;Parsing Paradigms and Language Learning,&amp;quot; in Proceedings AI-85, Long Beach, California (May 1085).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Functional Grammar,&amp;quot;</title>
<date>1979</date>
<booktitle>in Proceedings 5th Annual Meeting of the Berkeley Linguistic Society,</booktitle>
<pages>142--158</pages>
<location>Berkeley, California</location>
<marker>Kay, 1979</marker>
<rawString>Kay, Martin, &amp;quot;Functional Grammar,&amp;quot; pp. 142-158 in Proceedings 5th Annual Meeting of the Berkeley Linguistic Society, Berkeley, California (1979).</rawString>
</citation>
<citation valid="true">
<title>Language Acquisition Through Error Recovery,&amp;quot;</title>
<date>1982</date>
<journal>Cognition and Brain Theory</journal>
<volume>6</volume>
<issue>3</issue>
<pages>211--255</pages>
<marker>1982</marker>
<rawString>[Lan gley82I Langley, Pat, &amp;quot;Language Acquisition Through Error Recovery,&amp;quot; Cognition and Brain Theory 6(3), pp.211-255 (1982).</rawString>
</citation>
<citation valid="true">
<title>Definite Clause Grammars for Language Analysis- A Survey of the Formalism and a Comparison with Augmented Transition Networks.&amp;quot;</title>
<date>1980</date>
<journal>Artificial Intelligence</journal>
<volume>13</volume>
<pages>231--278</pages>
<marker>1980</marker>
<rawString>[Pereira801 Pereira, F. C. N. and David H. D. Warren, &amp;quot;Definite Clause Grammars for Language Analysis- A Survey of the Formalism and a Comparison with Augmented Transition Networks.&amp;quot; Artificial Intelligence 13, pp.231-278 (1980).</rawString>
</citation>
<citation valid="true">
<title>The Computational Study of Language Learning,&amp;quot;</title>
<date>1976</date>
<booktitle>in Advances in Computers,</booktitle>
<editor>ed. M. Yovits M. Rubinoff,</editor>
<publisher>Academic Press,</publisher>
<location>New York</location>
<marker>1976</marker>
<rawString>[Reek er761 Reeker, L. H., &amp;quot;The Computational Study of Language Learning,&amp;quot; in Advances in Computers, ed. M. Yovits M. Rubinoff, Academic Press, New York (1976).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C K Riesbeck</author>
</authors>
<title>Computational Understanding: Analysis of Sentences and Context,&amp;quot; Memo 238, Al Laboratory</title>
<date>1974</date>
<marker>Riesbeck, 1974</marker>
<rawString>[Riesbeck74) Riesbeck, C. K., &amp;quot;Computational Understanding: Analysis of Sentences and Context,&amp;quot; Memo 238, Al Laboratory (1974).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roger Schank</author>
<author>Robert Abelson</author>
</authors>
<title>Scripts Plans Coals and Understanding, Lawrence Erlbaum Associates,</title>
<date>1977</date>
<location>Hillsdale, New Jersey</location>
<marker>Schank, Abelson, 1977</marker>
<rawString>ISchank77) Schank, Roger and Robert Abelson, Scripts Plans Coals and Understanding, Lawrence Erlbaum Associates, Hillsdale, New Jersey (1977).</rawString>
</citation>
<citation valid="true">
<title>A Knowledge-Based Approach to Natural Language Processing: A progress Report,&amp;quot;</title>
<date>1081</date>
<booktitle>in Proceedings Seventh International Joint Conference on Artificial Intelligence,</booktitle>
<location>Vancouver, Canada</location>
<marker>1081</marker>
<rawString>[Wilensky811 Wilensky, R., &amp;quot;A Knowledge-Based Approach to Natural Language Processing: A progress Report,&amp;quot; in Proceedings Seventh International Joint Conference on Artificial Intelligence, Vancouver, Canada (1081).</rawString>
</citation>
<citation valid="true">
<title>Points: A Theory of Structure of Stories in Memory,&amp;quot;</title>
<date>1082</date>
<booktitle>in Strategies for Natural Language Processing,</booktitle>
<pages>345--375</pages>
<editor>ed. W. G. Lehnert M. H. Ringle, Laurence</editor>
<location>New Jersey</location>
<marker>1082</marker>
<rawString>[Wilensky821 Wilensky, R., &amp;quot;Points: A Theory of Structure of Stories in Memory,&amp;quot; pp. 345-375 in Strategies for Natural Language Processing, ed. W. G. Lehnert M. H. Ringle, Laurence Erlbaum Associates, New Jersey (1082).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Milensky841 Wilensky</author>
<author>Y Arens R</author>
<author>D Chin</author>
</authors>
<title>Talking to UNIX in English: an Overview of UC,&amp;quot;</title>
<date>1984</date>
<journal>Communications of the ACM</journal>
<volume>27</volume>
<issue>6</issue>
<pages>574--593</pages>
<marker>Wilensky, R, Chin, 1984</marker>
<rawString>Milensky841 Wilensky, R., Y. Arens, and D. Chin, &amp;quot;Talking to UNIX in English: an Overview of UC,&amp;quot; Communications of the ACM 27(6), pp.574-593 (June 1984).</rawString>
</citation>
<citation valid="true">
<title>Failure-Driven Aquisition of Figurative Phrases by Second Language Speakers,</title>
<date>1085</date>
<note>(submitted to publication).</note>
<marker>1085</marker>
<rawString>[Zernik851 Zernik, Uri and Michael G. Dyer, Failure-Driven Aquisition of Figurative Phrases by Second Language Speakers, 1085. (submitted to publication).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>