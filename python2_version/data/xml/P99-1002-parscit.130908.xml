<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.5551295">
AUTOMATIC SPEECH RECOGNITION AND ITS
APPLICATION TO INFORMATION EXTRACTION
</title>
<author confidence="0.977945">
Sadaoki Furui
</author>
<affiliation confidence="0.9995985">
Department of Computer Science
Tokyo Institute of Technology
</affiliation>
<address confidence="0.695423">
2-12-1, Ookayama, Meguro-ku, Tokyo, 152-8552 Japan
</address>
<email confidence="0.371478">
furui@c s .titech. ac jp
</email>
<sectionHeader confidence="0.746817" genericHeader="abstract">
ABSTRACT
</sectionHeader>
<bodyText confidence="0.999899125">
This paper describes recent progress and the
author&apos;s perspectives of speech recognition
technology. Applications of speech recognition
technology can be classified into two main areas,
dictation and human-computer dialogue systems.
In the dictation domain, the automatic broadcast
news transcription is now actively investigated,
especially under the DARPA project. The
broadcast news dictation technology has recently
been integrated with information extraction and
retrieval technology and many application
systems, such as automatic voice document
indexing and retrieval systems, are under
development. In the human-computer interaction
domain, a variety of experimental systems for
information retrieval through spoken dialogue are
being investigated. In spite of the remarkable
recent progress, we are still behind our ultimate
goal of understanding free conversational speech
uttered by any speaker under any environment.
This paper also describes the most important
research issues that we should attack in order to
advance to our ultimate goal of fluent speech
recognition.
</bodyText>
<sectionHeader confidence="0.998512" genericHeader="introduction">
1. INTRODUCTION
</sectionHeader>
<bodyText confidence="0.999962948717949">
The field of automatic speech recognition has
witnessed a number of significant advances in
the past 5 - 10 years, spurred on by advances in
signal processing, algorithms, computational
architectures, and hardware. These advances
include the widespread adoption of a statistical
pattern recognition paradigm, a data-driven
approach which makes use of a rich set of speech
utterances from a large population of speakers,
the use of stochastic acoustic and language
modeling, and the use of dynamic programming-
based search methods.
A series of (D)ARPA projects have been a major
driving force of the recent progress in research
on large-vocabulary, continuous-speech
recognition. Specifically, dictation of speech
reading newspapers, such as north America
business newspapers including the Wall Street
Journal (WSJ), and conversational speech
recognition using an Air Travel Information
System (ATIS) task were actively investigated.
More recent DARPA programs are the broadcast
news dictation and natural conversational speech
recognition using Switchboard and Call Home
tasks. Research on human-computer dialogue
systems, the Communicator program, has also
started [1]. Various other systems have been
actively investigated in US, Europe and Japan
stimulated by DARPA projects. Most of them
can be classified into either dictation systems or
human-computer dialogue systems.
Figure 1 shows a mechanism of state-of-the-art
speech recognizers [2]. Common features of
these systems are the use of cepstral parameters
and their regression coefficients as speech
features, triphone HMMs as acoustic models,
vocabularies of several thousand or several ten
thousand entries, and stochastic language models
such as bigrams and trigrams. Such methods have
</bodyText>
<page confidence="0.99848">
11
</page>
<bodyText confidence="0.961538869565218">
been applied not only to English but also to
French, German, Italian, Spanish, Chinese and
Japanese. Although there are several language-
specific characteristics, similar recognition
results have been obtained.
Recognized
word sequence
world domain of obvious value has lead to rapid
technology transfer of speech recognition into
other research areas and applications. Since the
variations in speaking style and accent as well
as in channel and environment conditions are
totally unconstrained, broadcast news
is a superb stress test that requires new
algorithms to work across widely
varying conditions. Algorithms need
to solve a specific problem without
degrading any other condition.
Another advantage of this domain is
that news is easy to collect and the
supply of data is boundless. The data
is found speech; it is completely
uncontrived.
</bodyText>
<subsectionHeader confidence="0.455009">
2.2 Japanese Broadcast News
Dictation System
</subsectionHeader>
<figure confidence="0.999184769230769">
Speechi input
Acoustic
analysis
Xi• • XT
Phoneme inventory
P(xi.-..xT I wr • .wk
Global search:
maximize
P xT P(111.-wk)
over w1••• wk
-Pronunciation lexicon
P(—wk
Language model
</figure>
<figureCaption confidence="0.987462">
Fig. 1 - Mechanism of state-of-the-art speech recognizers.
</figureCaption>
<bodyText confidence="0.999854692307692">
The remainder of this paper is organized as
follows. Section 2 describes recent progress in
broadcast news dictation and its application to
information extraction, and Section 3 describes
human-computer dialogue systems. In spite of
the remarkable recent progress, we are still far
behind our ultimate goal of understanding free
conversational speech uttered by any speaker
under any environment. Section 4 describes how
to increase the robustness of speech recognition,
and Section 5 describes perspectives of linguistic
modeling for spontaneous speech recognition/
understanding. Section 6 concludes the paper.
</bodyText>
<sectionHeader confidence="0.9999795" genericHeader="method">
2. BROADCAST NEWS DICTATION AND
INFORMATION EXTRACTION
</sectionHeader>
<subsectionHeader confidence="0.841353">
2.1 DARPA Broadcast News Dictation Project
</subsectionHeader>
<bodyText confidence="0.999678870967742">
With the introduction of the broadcast news test
bed to the DARPA project in 1995, the research
effort took a profound step forward. Many of
the deficiencies of the WSJ domain were resolved
in the broadcast news domain [3]. Most
importantly, the fact that broadcast news is a real-
We have been developing a large-
vocabulary continuous-speech recognition
(LVCSR) system for Japanese broadcast-news
speech transcription [4][5]. This is a part of a
joint research with the NHK broadcast company
whose goal is the closed-captioning of TV
programs. The broadcast-news manuscripts that
were used for constructing the language models
were taken from the period between July 1992
and May 1996, and comprised roughly 500k
sentences and 22M words. To calculate word n-
gram language models, we segmented the
broadcast-news manuscripts into words by using
a morphological analyzer since Japanese
sentences are written without spaces between
words. A word-frequency list was derived for the
news manuscripts, and the 20k most frequently
used words were selected as vocabulary words.
This 20k vocabulary covers about 98% of the
words in the broadcast-news manuscripts. We
calculated bigrams and trigrams and estimated
unseen n-grams using Katz&apos;s back-off smoothing
method.
Japanese text is written by a mixture of three
kinds of characters: Chinese characters (Kanji)
</bodyText>
<page confidence="0.997556">
12
</page>
<bodyText confidence="0.999372707317074">
and two kinds of Japanese characters (Hira-gana
and Kata-kana). Most Kanji have multiple
readings, and correct readings can only be
decided according to context. Conventional
language models usually assign equal probability
to all possible readings of each word. This causes
recognition errors because the assigned
probability is sometimes very different from the
true probability. We therefore constructed a
language model that depends on the readings of
words in order to take into account the frequency
and context-dependency of the readings.
Broadcast news speech includes filled pauses at
the beginning and in the middle of sentences,
which cause recognition errors in our language
models that use news manuscripts written prior
to broadcasting. To cope with this problem, we
introduced filled-pause modeling into the
language model.
News speech data, from TV broadcasts in July
1996, were divided into two parts, a clean part
and a noisy part, and were separately evaluated.
The clean part consisted of utterances with no
background noise, and the noisy part consisted
of utterances with background noise. The noisy
part included spontaneous speech such as reports
by correspondents. We extracted 50 male
utterances and 50 female utterances for each part,
yielding four evaluation sets; male-clean (m/c),
male-noisy (mm), female-clean (f/c), female-
noisy (f/n). Each set included utterances by five
or six speakers. All utterances were manually
segmented into sentences. Table 1 shows the
experimental results for the baseline language
model (LM1) and the new language models. LM2
is the reading-dependent language model, and
LM3 is a modification of LM2 by filled-pause
modeling. For clean speech, LM2 reduced the
word error rate by 4.7 % relative to LM1, and
LM3 model reduced the word error rate by 10.9
% relative to LM2 on average.
</bodyText>
<subsectionHeader confidence="0.9989565">
2.3 Information Extraction in the DARPA
Project
</subsectionHeader>
<bodyText confidence="0.999983791666667">
News is filled with events, people, and
organizations and all manner of relations among
them. The great richness of material and the
naturally evolving content in broadcast news has
leveraged its value into areas of research well
beyond speech recognition. In the DARPA
project, the Spoken Document Retrieval (SDR)
of TREC and the Topic Detection and Tracking
(TDT) program are supported by the same
materials and systems that have been
developed in the broadcast news dictation
arena [3]. BBN&apos;sRough&apos;n&apos;Reddy system
extracts structural features of broadcast
news. CMU&apos;s Informedia [6], MITRE&apos;s
Broadcast Navigator, and SRI&apos;s Maestro
have all exploited the multi-media features
of news producing a wide range of
capabilities for browsing news archives
interactively. These systems integrate
various diverse speech and language
technologies including speech recognition,
speaker change detection, speaker identification,
name extaction, topic classification and
information retrieval.
</bodyText>
<subsectionHeader confidence="0.9951705">
2.4 Information Extraction from Japanese
Broadcast News
</subsectionHeader>
<bodyText confidence="0.971962">
Summarizing transcribed news speech is useful
for retrieving or indexing broadcast news. We
investigated a method for extracting topic words
from nouns in the speech recognition results on
the basis of a significance measure [4][5]. The
extracted topic words were compared with &amp;quot;true&amp;quot;
topic words, which were given by three human
subjects. The results are shown in Figure 2.
Table 1 - Experimental results of Japanese broadcast news
dictation with various language models (word error rate [%])
</bodyText>
<table confidence="0.635368833333333">
Language Evaluation sets
model
m/c m/n f/c f/n
LM1 17.6 37.2 14.3 41.2
LM2 16.8 35.9 13.6 39.3
LM3 14.2 33.1 12.9 38.1
</table>
<page confidence="0.992141">
13
</page>
<bodyText confidence="0.999537333333333">
When the top five topic words were chosen
(recall=13%), 87% of them were correct on
average.
</bodyText>
<sectionHeader confidence="0.999452" genericHeader="method">
3. HUMAN-COMPUTER DIALOGUE
SYSTEMS
</sectionHeader>
<subsectionHeader confidence="0.999978">
3.1 Typical Systems in US and Europe
</subsectionHeader>
<bodyText confidence="0.990903448275862">
Recently a number of sites have been working
on human-computer dialogue systems. The
followings are typical examples.
system, German and Servocroatian public
newscasts are recorded daily. The newscasts are
automatically segmented and an index is created
for each of the segments by means of automatic
speech recognition. The user can query the
system in natural language by keyboard or
through a speech utterance. The system returns
a list of segments which is sorted by relevance
with respect to the user query. By selecting a
segment, the user can watch the corresponding
part of the news show on his/her computer screen.
The system overview is shown in Fig. 3.
(b) The SCAN- speech content based audio
navigator at AT&amp;T Labs
SCAN (Speech Content based Audio Navigator)
is a spoken document retrieval system developed
at AT&amp;T Labs integrating speaker-independent,
large-vocabulary speech recognition with
information-retrieval to support query-based
retrieval of information from speech archives [8].
Initial development focused on the application
of SCAN to the broadcast news domain. An
overview of the system architecture is provided
in Fig. 4. The system consists of three
components: (1) a speaker-independent large-
vocabulary speech recognition engine which
</bodyText>
<figure confidence="0.892356">
0 25 50 75 100
Recall[fo]
</figure>
<figureCaption confidence="0.356557">
Fig. 2 - Topic word extraction results.
</figureCaption>
<bodyText confidence="0.915224555555556">
(a) The View4You system
at the University of
Karksruhe
The University of Karlsruhe
focuses its speech research
on a content-addressable
multimedia information
retrieval system, under a
multi-lingual environment,
where queries and
multimedia documents may
appear in multiple
languages [7]. The system is
called &amp;quot;View4You&amp;quot; and
their research is conducted
in cooperation with the
Informedia project at CMU
[6]. In the View4You
</bodyText>
<figure confidence="0.988992304347826">
Query
Result
L
gm( Nleo query servier
11
Segmenter
MPEG-audio
,y Segment boundaries
(Speech recognizer) MPEG-audio
WWW
Internet newspapers
Fig. 3 - System overview of the View4You system.
14
Multimedia
database
Text
Segment boundaries
( Thesaurus )
Text
Front-end
Query
(Input speech recognizer
Result output
</figure>
<bodyText confidence="0.998048976190476">
segments the speech archive and generates
transcripts, (2) an information-retrieval engine
which indexes the transcriptions and formulates
hypotheses regarding document relevance to
user-submitted queries and (3) a graphical-user-
interface which supports search and local
contextual navigation based on the machine-
generated transcripts and graphical
representations of query-keyword distribution in
the retrieved speech transcripts. The speech
recognition component of SCAN includes an
intonational phrase boundary detection module
and a classification module, These
subcomponents preprocess the speech data before
passing the speech to the recognizer itself.
technology at MIT for several
years. Recently, they have
initiated a significant redesign
of the GALAXY architecture
to make it easier for
researchers to develop their
own applications, using either
exclusively their own servers
or intermixing them with
servers developed by others.
This redesign was done in part
due to the fact that GALAXY
has been designed as the first
reference architecture for the
new DARPA Communicator program. The
resulting configuration of the GALAXY-II
architecture is shown in Fig. 5. The boxes in
this figure represent various human language
technology servers as well as information and
domain servers. The label in italics next to each
box identifies the corresponding MIT system
component. Interactions between servers are
mediated by the hub and managed in the hub
script. A particular dialogue session is initiated
by a user either through interaction with a
graphical interface at a Web site, through direct
telephone dialup, or through a desktop agent.
</bodyText>
<figureCaption confidence="0.526898">
Fig. 4 - Overview of the SCAN spoken document system architecture.
</figureCaption>
<figure confidence="0.9721222">
1
Information
retrieval
Opeecl&gt; Intonational
corpus phrase boundary
detection
Classification
Recognition
Transcripts
User interface
GENESIS
DEC TALK
&amp; EN VOICE
Text-to-speech
conversion
Dialogue
mana ement
Application
back-ends
Audio
server
SUMMIT
Frame
construction
TINA
Context
tracking
Discourse
D-Server
Phone
I-Server
Speech
recognition
Language
generation
</figure>
<figureCaption confidence="0.639446">
Fig. 5 - Architecture of GALAXY-II.
</figureCaption>
<bodyText confidence="0.872955866666667">
(c) The
GALAXY-II
conversational
system at MIT
Galaxy is a client-
server architecture
developed at MIT
for accessing on-
line information
using spoken
dialogue [9]. It has
served as the
testbed for
developing human
language
</bodyText>
<page confidence="0.982991">
15
</page>
<bodyText confidence="0.976534541666667">
(d) The ARISE train travel information
system at LIMSI
The ARISE (Automatic Railway Information
Systems for Europe) projects aims developing
prototype telephone information services for train
travel information in several European countries
[10]. In collaboration with the Vecsys company
and with the SNCF (the French Railways),
LIMSI has developed a prototype telephone
service providing timetables, simulated fares and
reservations, and information on reductions and
services for the main French intercity
connections. A prototype French/English service
for the high speed trains between Paris and
London is also under development. The system
is based on the spoken language systems
developed for the RailTel project [11] and the
ESPRIT Mask project [12]. Compared to the
RailTel system, the main advances in ARISE are
in dialogue management, confidence measures,
inclusion of optional spell mode for city/station
names, and barge-in capability to allow more
natural interaction between the user and the
machine.
</bodyText>
<subsectionHeader confidence="0.9844495">
3.2 Designing a Multimodal Dialogue System
for Information Retrieval
</subsectionHeader>
<bodyText confidence="0.996111857142857">
We have recently investigated a paradigm for
designing multimodal dialogue systems [13]. An
example task of the system was to retrieve
particular information about different shops in
the Tokyo Metropolitan area, such as their names,
addresses and phone numbers. The system
accepted speech and screen touching as input,
and presented retrieved information on a screen
display or by synthesized speech as shown in Fig.
6. The speech recognition part was modeled by
the FSN (finite state network) consisting of
keywords and fillers, both of which were
implemented by the DAWG (directed acyclic
word-graph) structure. The number of keywords
was 306, consisting of district names and
business names. The fillers accepted roughly
100,000 non-keywords/phrases occuring in
spontaneous speech. A variety of dialogue
strategies were designed and evaluated based on
an objective cost function having a set of actions
and states as parameters. Expected dialogue cost
</bodyText>
<figure confidence="0.996001071428571">
Input
Speech
Mouse or
touch screen
Output
Display
Speech
Speech
recognizer
Dialogue
manager
Database
Speech
synthesizer
</figure>
<figureCaption confidence="0.81811">
Fig. 6 - Multimodal dialogue system structure for information retrieval.
</figureCaption>
<bodyText confidence="0.981386608695652">
The speech recognizer uses
n-gram backoff language
models estimated on the
transcriptions of spoken
queries. Since the amount
of language model training
data is small, some
grammatical classes, such
as cities, days and months,
are used to provide more
robust estimates of the n-
gram probabilities. A
confidence score is
associated with each
hypothesized word, and if the score is below an
empirically determined threshold, the
hypothesized word is marked as uncertain. The
uncertain words are ignored by the understanding
component or used by the dialogue manager to
start clarification subdialogues.
was calculated for each strategy, and the best
strategy was selected according to the keyword
recognition accuracy.
</bodyText>
<page confidence="0.973201">
16
</page>
<figure confidence="0.972236677419355">
4. ROBUST SPEECH
RECOGNITION
4.1 Automatic
adaptation
Noise
• Other speakers
• Background noise
• Reverberations
Channel
f
Distortion
Noise
Echoes
Dropouts
Speech
recognition
system
Speaker Task/context
• Voice quality • Man-machine
• Pitch dialogue
• Gender • Dictation
• Dialect • Free conversation
Speaking style • Interview
• Stress/emotion Phonetic/prosodic
• Speaking rate context
• Lombard effect
Microphone
• Distortion
• Electrical noise
• Directional
characteristics
</figure>
<figureCaption confidence="0.978432">
Fig. 7 - Main causes of acoustic variation in speech.
</figureCaption>
<bodyText confidence="0.84458958974359">
Ultimately, speech
recognition systems
should be capable of
robust, speaker-
independent or speaker-
adaptive, continuous
speech recognition.
Figure 7 shows main
causes of acoustic
variation in speech [14].
It is crucial to establish
methods that are robust
against voice variation due to
individuality, the physical and
psychological condition of the speaker,
telephone sets, microphones, network
characteristics, additive background
noise, speaking styles, and so on.
Figure 8 shows main methods for
making speech recognition systems
robust against voice variation. It is also
important for the systems to impose
few restrictions on tasks and
vocabulary. To solve these problems,
it is essential to develop automatic
adaptation techniques.
Extraction and normalization of.
(adaptation to) voice individuality is
one of the most important issues [14].
A small percentage of people
occasionally cause systems to produce
exceptionally low recognition rates.
This is an example of the &amp;quot;sheep and
goats&amp;quot; phenomenon. Speaker
adaptation (normalization) methods
can usually be classified into
supervised (text-dependent) and
unsupervised (text-independent)
methods. Unsupervised, on-line,
</bodyText>
<figure confidence="0.992109625">
Microphone
Analysis and feature extraction
Feature-level normalization/
adaptation
f Close-talking microphone
t Microphone array
(Auditory models
VEIH, SMC, PLP)
Adaptive filtering
Noise subtraction
Comb filtering
Spectral mapping
Cepstral mean normalization
A cepstra
RASTA
(Noise addition
HMM (de) composition(PMC)
Model transformation(MLLR)
Model-level • Bayesian adaptive learning
normalization/ (Frequency weighting measure
adaptation Weighted cepstral distance
Cepstrum projection measure
Distance/
similarity
measures
Reference {
templates/ Word spotting
models Utterance venfication
■
Robust matching
Linguistic processing-- Language model adaptation
.tecognition result&apos;)
</figure>
<figureCaption confidence="0.9470945">
Fig. 8 - Main methods to cope with voice variation in
speech recognition.
</figureCaption>
<page confidence="0.997193">
17
</page>
<bodyText confidence="0.999979909090909">
instantaneous/incremental adaptation is ideal,
since the system works as if it were a speaker-
independent system, and it performs increasingly
better as it is used. However, since we have to
adapt many phonemes using a limited size of
utterances including only a limited number of
phonemes, it is crucial to use reasonable
modeling of speaker-to-speaker variablity or
constraints. Modeling of the mechanism of
speech production is expected to provide a useful
modeling of speaker-to-speaker variability.
</bodyText>
<subsectionHeader confidence="0.968997">
4.2 On-line speaker adaptation in broadcast
news dictation
</subsectionHeader>
<bodyText confidence="0.999987542857143">
Since, in broadcast news, each speaker utters
several sentences in succession, the recognition
error rate can be reduced by adapting acoustic
models incrementally within a segment that
contains only one speaker. We applied on-line,
unsupervised, instantaneous and incremental
speaker adaptation combined with automatic
detection of speaker changes [4]. The MLLR [15]
-MAP [16] and VFS (vector-field smoothing)
[17] methods were instantaneously and
incrementally carried out for each utterance. The
adaptation process is as follows. For the first
input utterance, the speaker-independent model
is used for both recognition and adaptation, and
the first speaker-adapted model is created. For
the second input utterance, the likelihood value
of the utterance given the speaker-independent
model and that given the speaker-adapted model
are calculated and compared. If the former value
is larger, the utterance is considered to be the
beginning of a new speaker, and another speaker-
adapted model is created. Otherwise, the existing
speaker-adapted model is incrementally adapted.
For the succeeding input utterances, speaker
changes are detected in the same way by
comparing the acoustic likelihood values of each
utterance obtained from the speaker-independent
model and some speaker-adapted models. If the
speaker-independent model yields a larger
likelihood than any of the speaker-adapted
models, a speaker change is detected and a new
speaker-adapted model is constructed.
Experimental results show that the adaptation
reduced the word error rate by 11.8 % relative to
the speaker-independent models.
</bodyText>
<sectionHeader confidence="0.9989135" genericHeader="method">
5. PRESPECTIVES OF LANGUAGE
MODELING
</sectionHeader>
<subsectionHeader confidence="0.992731">
5.1 Language modeling for spontaneous
speech recognition
</subsectionHeader>
<bodyText confidence="0.999953714285714">
One of the most important issues for speech
recognition is how to create language models
(rules) for spontaneous speech. When
recognizing spontaneous speech in dialogues, it
is necessary to deal with variations that are not
encountered when recognizing speech that is read
from texts. These variations include extraneous
words, out-of-vocabulary words, ungrammatical
sentences, disfluency, partial words, repairs,
hesitations, and repetitions. It is crucial to
develop robust and flexible parsing algorithms
that match the characteristics of spontaneous
speech. A paradigm shift from the present
transcription-based approach to a detection-based
approach will be important to solve such
problems [2]. How to extract contextual
information, predict users&apos; responses, and focus
on key words are very important issues.
Stochastic language modeling, such as bigrams
and trigrams, has been a very powerful tool, so
it would be very effective to extend its utility by
incorporating semantic knowledge. It would also
be useful to integrate unification grammars and
context-free grammars for efficient word
prediction. Style shifting is also an important
problem in spontaneous speech recognition. In
typical laboratory experiments, speakers are
reading lists of words rather than trying to
accomplish a real task. Users actually trying to
accomplish a task, however, use a different
linguistic style. Adaptation of linguistic models
according to tasks, topics and speaking styles is
a very important issue, since collecting a large
linguistic database for every new task is difficult
and costly.
</bodyText>
<page confidence="0.99581">
18
</page>
<subsectionHeader confidence="0.998452">
5.2 Message-Driven Speech Recognition
</subsectionHeader>
<bodyText confidence="0.961988772727273">
State-of-the-art automatic speech recognition
systems employ the criterion of maximizing
P(WIX), where W is a word sequence, and Xis
an acoustic observation sequence. This criterion
is reasonable for dictating read speech. However,
the ultimate goal of automatic speech recognition
is to extract the underlying messages of the
speaker from the speech signals. Hence we need
to model the process of speech generation and
recognition as shown in Fig. 9 [18], where M is
the message (content) that a speaker intended to
convey.
Fig. 9 - A communication - theoretic view of speech
recognition.
According to this model, the speech recognition
process is represented as the maximization of the
following a posteriori probability [4][5],
models in the same way as in usual recognition
processes. We assume that P(M) has a uniform
probability for all M. Therefore, we only need to
consider further the term P( WM). We assume
that P(W1M) can be expressed as follows.
</bodyText>
<equation confidence="0.972668">
P(141M) w) 1 -AP( wi ApA , (4)
</equation>
<bodyText confidence="0.987990375">
where A, 0 is a weighting factor. P(W),
the first term of the right hand side, represents a
part ofP(WIM) that is independent ofMand can
be given by a general statistical language model.
P&apos;(WIM), the second term of the right hand side,
represents the part of P(WIM) that depends on
M. We consider that M is
represented by a co-occurrence
recognizer
Speech of words based on the
distributional hypothesis by
Harris [19]. Since this approach
formulates PVTIM) without
explicitly representing M, it can
use information about the
speaker&apos;s message M without
being affected by the
quantization problem of topic
classes. This new formulation
of speech recognition was
applied to the Japanese
broadcast news dictation, and it was found that
word error rates for the clean set were slightly
reduced by this method.
</bodyText>
<figure confidence="0.997815833333333">
P(M)
Message
source
P(WIM)
Linguistic
channel
Language
Vocabulary
Grammar
Semantics
Context
Habits
P(XIW)
Acoustic
channel
• Speaker
Reverberation
Noise
Transmission-
characteristics
Microphone
generation and
maxP(MI X) = max P(MIW)P(WIX). (1) 6. CONCLUSIONS
M MW
</figure>
<bodyText confidence="0.769875">
Using Bayes&apos; rule, Eq. (1) can be expressed as
For simplicity, we can approximate the equation
as
</bodyText>
<footnote confidence="0.55501325">
max P(Moo= max P(XIW) P(VV1M) P(M)
• (3)
m, w P(X)
P(X1W) is calculated using hidden Markov
</footnote>
<bodyText confidence="0.99996475">
Speech recognition technology has made a
remarkable progress in the past 5 - 10 years.
Based on the progress, various application
systems have been developed using dictation and
spoken dialogue technology. One of the most
important applications is information extraction
and retrieval. Using the speech recognition
technology, broadcast news can be automatically
indexed, producing a wide range of capabilities
for browsing news archives interactively. Since
speech is the most natural and efficient
communication method between humans,
</bodyText>
<equation confidence="0.794556">
maxP (M I X) = max I, POCII40 P(W1M) P(M)
MW
POO . (2)
</equation>
<page confidence="0.993948">
19
</page>
<bodyText confidence="0.999953352941177">
automatic speech recognition will continue to
find applications, such as meeting/conference
summarization, automatic closed captioning, and
interpreting telephony. It is expected that speech
recognizer will become the main input device of
the &amp;quot;wearable&amp;quot; computers that are now actively
investigated. In order to materialize these
applications, we have to solve many problems.
The most important issue is how to make the
speech recognition systems robust against
acoustic and lingustic variation in speech. In this
context, a paradigm shift from speech recognition
to understanding where underlying messages of
the speaker, that is, meaning/context that the
speaker intended to convey are extracted, instead
of transcribing all the spoken words, will be
indispensable.
</bodyText>
<sectionHeader confidence="0.999558" genericHeader="references">
REFERENCES
</sectionHeader>
<reference confidence="0.997037414634147">
[1] http://fofoca.mitre.org
[2] S. Furui: &amp;quot;Future directions in speech information
processing&amp;quot;, Proc. 16th ICA and 135th Meeting
ASA, Seattle, pp. 1-4 (1998)
[3] F. Kubala: &amp;quot;Broadcast news is good news&amp;quot;,
DARPA Broadcast News Workshop, Virginia
(1999)
[4] K. Ohtsuki, S. Furui, N. Sakurai, A. Iwasaki and
Z.-P. Zhang: &amp;quot;Improvements in Japanese broadcast
news transcription&amp;quot;, DARPA Broadcast News
Workshop, Virginia (1999)
[5] K. Ohtsuld, S. Furui, A. Iwasaki and N. Sakurai:
&amp;quot;Message-driven speech recognition and topic-
word extraction&amp;quot;, Proc. IEEE Int. Conf. Acoust.,
Speech, Signal Process., Phoenix, pp. 625-628
(1999)
[6] M. Witbrock and A. G. Hauptmann: &amp;quot;Speech
recognition and information retrieval:
Experiments in retrieving spoken documents&amp;quot;,
Proc. DARPA Speech Recognition Workshop,
Virginia, pp. 160-164 (1997). See also http://
www.informedia.cs.cmu.edu/
[7] T. Kemp, P. Geutner, M. Schmidt, B. Tomaz, M.
Weber, M. Westphal and A. Waibel: &amp;quot;The
interactive systems labs View4You video indexing
system&amp;quot;, Proc. Int. Conf. Spoken Language
Processing, Sydney, pp. 1639-1642 (1998)
[8] J. Choi, D. Hindle, J. Hirschberg, I. Magrin-
Chagnolleau, C. Nakatani, F. Pereira, A. Singhal
and S. Whittaker: &amp;quot;SCAN - speech content based
audio navigator: a systems overview&amp;quot;, Proc. Int.
Conf. Spoken Language Processing, Sydney, pp.
2867-2870 (1998)
[9] S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid
and V. Zue: &amp;quot;GALAXY-II: a reference architecture
for conversational system development&amp;quot;, Proc. Int.
Conf. Spoken Language Processing, Sydney, pp.
931-934 (1998)
[10] L. Lamel, S. Rosset, J. L. Gauvain and S.
Bennacef: &amp;quot;The LIMSI ARISE system for train
travel information&amp;quot;, Proc. IEEE Int. Conf. Acoust.,
Speech, Signal Process., Phoenix, pp. 501-504
(1999)
[11] L. F. Lamel, S. K. Bennacef, S. Rosset, L.
Devillers, S. Foukia, J. J. Gangolf and J. L.
Gauvain: &amp;quot;The LIMSI RailTel system: Field trial
of a telephone service for rail travel information&amp;quot;,
Speech Communication, 23, pp. 67-82 (1997)
[12] J. L. Gauvain, J. J. Gangolf and L. Lamel:
&amp;quot;Speech recognition for an information Kiosk&amp;quot;,
Proc. Int. Conf. Spoken Language Processing,
Philadelphia, pp. 849-852 (1998)
[13] S. Furui and K. Yamaguchi: &amp;quot;Designing a
multimodal dialogue system for information
retrieval&amp;quot;, Proc. Int. Conf. Spoken Language
Processing, Sydney, pp. 1191-1194 (1998)
[14] S. Furui: &amp;quot;Recent advances in robust speech
recognition&amp;quot;, Proc. ESCA-NATO Workshop on
Robust Speech Recognition for Unknown
Communication Channels, Pont-a-Mousson,
France, pp. 11-20 (1997)
[15] C. J. Leggetter and P. C. Woodland: &amp;quot;Maximum
likelihood linear regression for speaker adaptation
of continuous density hidden Markov models&amp;quot;,
Computer Speech and Language, pp. 171-185
(1995).
[16] J. -L. Gauvain and C.-H. Lee: &amp;quot;Maximum a
posteriori estimation for multivariate Gaussian
mixture observations of Markov chains&amp;quot; IEEE
Trans. on Speech and Audio Processing, 2, 2, pp.
291-298 (1994).
[17] K. Ohkura, M. Sugiyama and S. Sagayama:
&amp;quot;Speaker adaptation based on transfer vector field
smoothing with continuous mixture density
HMMs&apos;&amp;quot;, Proc. Int. Conf. Spoken Language
Processing, Banff, pp. 369-372 (1992)
[18] B.-H. Juang: &amp;quot;Automatic speech recognition:
Problems, progress &amp; prospects&amp;quot;, IEEE Workshop
on Neural Networks for Signal Processing (1996)
[19] Z. S. Harris: &amp;quot;Co-occurrence and transformation
in linguistic structure&amp;quot;, Language, 33, pp. 283-
340 (1957)
</reference>
<page confidence="0.993684">
20
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493922">
<title confidence="0.99645">AUTOMATIC SPEECH RECOGNITION AND ITS APPLICATION TO INFORMATION EXTRACTION</title>
<author confidence="0.999324">Sadaoki Furui</author>
<affiliation confidence="0.999968">Department of Computer Science Tokyo Institute of Technology</affiliation>
<address confidence="0.943328">2-12-1, Ookayama, Meguro-ku, Tokyo, 152-8552 Japan</address>
<email confidence="0.54172">furui@cs.titech.acjp</email>
<abstract confidence="0.99720648">This paper describes recent progress and the author&apos;s perspectives of speech recognition technology. Applications of speech recognition technology can be classified into two main areas, dictation and human-computer dialogue systems. In the dictation domain, the automatic broadcast news transcription is now actively investigated, especially under the DARPA project. The broadcast news dictation technology has recently been integrated with information extraction and retrieval technology and many application systems, such as automatic voice document indexing and retrieval systems, are under development. In the human-computer interaction domain, a variety of experimental systems for information retrieval through spoken dialogue are being investigated. In spite of the remarkable recent progress, we are still behind our ultimate goal of understanding free conversational speech uttered by any speaker under any environment. This paper also describes the most important research issues that we should attack in order to advance to our ultimate goal of fluent speech recognition.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<note>http://fofoca.mitre.org</note>
<contexts>
<context position="2502" citStr="[1]" startWordPosition="347" endWordPosition="347">een a major driving force of the recent progress in research on large-vocabulary, continuous-speech recognition. Specifically, dictation of speech reading newspapers, such as north America business newspapers including the Wall Street Journal (WSJ), and conversational speech recognition using an Air Travel Information System (ATIS) task were actively investigated. More recent DARPA programs are the broadcast news dictation and natural conversational speech recognition using Switchboard and Call Home tasks. Research on human-computer dialogue systems, the Communicator program, has also started [1]. Various other systems have been actively investigated in US, Europe and Japan stimulated by DARPA projects. Most of them can be classified into either dictation systems or human-computer dialogue systems. Figure 1 shows a mechanism of state-of-the-art speech recognizers [2]. Common features of these systems are the use of cepstral parameters and their regression coefficients as speech features, triphone HMMs as acoustic models, vocabularies of several thousand or several ten thousand entries, and stochastic language models such as bigrams and trigrams. Such methods have 11 been applied not o</context>
</contexts>
<marker>[1]</marker>
<rawString>http://fofoca.mitre.org</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
</authors>
<title>Future directions in speech information processing&amp;quot;,</title>
<date>1998</date>
<booktitle>Proc. 16th ICA and 135th Meeting ASA, Seattle,</booktitle>
<pages>1--4</pages>
<contexts>
<context position="2778" citStr="[2]" startWordPosition="387" endWordPosition="387">recognition using an Air Travel Information System (ATIS) task were actively investigated. More recent DARPA programs are the broadcast news dictation and natural conversational speech recognition using Switchboard and Call Home tasks. Research on human-computer dialogue systems, the Communicator program, has also started [1]. Various other systems have been actively investigated in US, Europe and Japan stimulated by DARPA projects. Most of them can be classified into either dictation systems or human-computer dialogue systems. Figure 1 shows a mechanism of state-of-the-art speech recognizers [2]. Common features of these systems are the use of cepstral parameters and their regression coefficients as speech features, triphone HMMs as acoustic models, vocabularies of several thousand or several ten thousand entries, and stochastic language models such as bigrams and trigrams. Such methods have 11 been applied not only to English but also to French, German, Italian, Spanish, Chinese and Japanese. Although there are several languagespecific characteristics, similar recognition results have been obtained. Recognized word sequence world domain of obvious value has lead to rapid technology </context>
<context position="22766" citStr="[2]" startWordPosition="3308" endWordPosition="3308"> for spontaneous speech. When recognizing spontaneous speech in dialogues, it is necessary to deal with variations that are not encountered when recognizing speech that is read from texts. These variations include extraneous words, out-of-vocabulary words, ungrammatical sentences, disfluency, partial words, repairs, hesitations, and repetitions. It is crucial to develop robust and flexible parsing algorithms that match the characteristics of spontaneous speech. A paradigm shift from the present transcription-based approach to a detection-based approach will be important to solve such problems [2]. How to extract contextual information, predict users&apos; responses, and focus on key words are very important issues. Stochastic language modeling, such as bigrams and trigrams, has been a very powerful tool, so it would be very effective to extend its utility by incorporating semantic knowledge. It would also be useful to integrate unification grammars and context-free grammars for efficient word prediction. Style shifting is also an important problem in spontaneous speech recognition. In typical laboratory experiments, speakers are reading lists of words rather than trying to accomplish a rea</context>
</contexts>
<marker>[2]</marker>
<rawString>S. Furui: &amp;quot;Future directions in speech information processing&amp;quot;, Proc. 16th ICA and 135th Meeting ASA, Seattle, pp. 1-4 (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Kubala</author>
</authors>
<title>Broadcast news is good news&amp;quot;, DARPA Broadcast News Workshop,</title>
<date>1999</date>
<location>Virginia</location>
<contexts>
<context position="5146" citStr="[3]" startWordPosition="741" endWordPosition="741">onversational speech uttered by any speaker under any environment. Section 4 describes how to increase the robustness of speech recognition, and Section 5 describes perspectives of linguistic modeling for spontaneous speech recognition/ understanding. Section 6 concludes the paper. 2. BROADCAST NEWS DICTATION AND INFORMATION EXTRACTION 2.1 DARPA Broadcast News Dictation Project With the introduction of the broadcast news test bed to the DARPA project in 1995, the research effort took a profound step forward. Many of the deficiencies of the WSJ domain were resolved in the broadcast news domain [3]. Most importantly, the fact that broadcast news is a realWe have been developing a largevocabulary continuous-speech recognition (LVCSR) system for Japanese broadcast-news speech transcription [4][5]. This is a part of a joint research with the NHK broadcast company whose goal is the closed-captioning of TV programs. The broadcast-news manuscripts that were used for constructing the language models were taken from the period between July 1992 and May 1996, and comprised roughly 500k sentences and 22M words. To calculate word ngram language models, we segmented the broadcast-news manuscripts i</context>
<context position="8656" citStr="[3]" startWordPosition="1281" endWordPosition="1281"> reduced the word error rate by 10.9 % relative to LM2 on average. 2.3 Information Extraction in the DARPA Project News is filled with events, people, and organizations and all manner of relations among them. The great richness of material and the naturally evolving content in broadcast news has leveraged its value into areas of research well beyond speech recognition. In the DARPA project, the Spoken Document Retrieval (SDR) of TREC and the Topic Detection and Tracking (TDT) program are supported by the same materials and systems that have been developed in the broadcast news dictation arena [3]. BBN&apos;sRough&apos;n&apos;Reddy system extracts structural features of broadcast news. CMU&apos;s Informedia [6], MITRE&apos;s Broadcast Navigator, and SRI&apos;s Maestro have all exploited the multi-media features of news producing a wide range of capabilities for browsing news archives interactively. These systems integrate various diverse speech and language technologies including speech recognition, speaker change detection, speaker identification, name extaction, topic classification and information retrieval. 2.4 Information Extraction from Japanese Broadcast News Summarizing transcribed news speech is useful for</context>
</contexts>
<marker>[3]</marker>
<rawString>F. Kubala: &amp;quot;Broadcast news is good news&amp;quot;, DARPA Broadcast News Workshop, Virginia (1999)</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ohtsuki</author>
<author>S Furui</author>
<author>N Sakurai</author>
<author>A Iwasaki</author>
<author>Z-P Zhang</author>
</authors>
<title>Improvements in Japanese broadcast news transcription&amp;quot;, DARPA Broadcast News Workshop,</title>
<date>1999</date>
<location>Virginia</location>
<contexts>
<context position="5343" citStr="[4]" startWordPosition="768" endWordPosition="768">ing for spontaneous speech recognition/ understanding. Section 6 concludes the paper. 2. BROADCAST NEWS DICTATION AND INFORMATION EXTRACTION 2.1 DARPA Broadcast News Dictation Project With the introduction of the broadcast news test bed to the DARPA project in 1995, the research effort took a profound step forward. Many of the deficiencies of the WSJ domain were resolved in the broadcast news domain [3]. Most importantly, the fact that broadcast news is a realWe have been developing a largevocabulary continuous-speech recognition (LVCSR) system for Japanese broadcast-news speech transcription [4][5]. This is a part of a joint research with the NHK broadcast company whose goal is the closed-captioning of TV programs. The broadcast-news manuscripts that were used for constructing the language models were taken from the period between July 1992 and May 1996, and comprised roughly 500k sentences and 22M words. To calculate word ngram language models, we segmented the broadcast-news manuscripts into words by using a morphological analyzer since Japanese sentences are written without spaces between words. A word-frequency list was derived for the news manuscripts, and the 20k most frequentl</context>
<context position="9435" citStr="[4]" startWordPosition="1383" endWordPosition="1383">dia features of news producing a wide range of capabilities for browsing news archives interactively. These systems integrate various diverse speech and language technologies including speech recognition, speaker change detection, speaker identification, name extaction, topic classification and information retrieval. 2.4 Information Extraction from Japanese Broadcast News Summarizing transcribed news speech is useful for retrieving or indexing broadcast news. We investigated a method for extracting topic words from nouns in the speech recognition results on the basis of a significance measure [4][5]. The extracted topic words were compared with &amp;quot;true&amp;quot; topic words, which were given by three human subjects. The results are shown in Figure 2. Table 1 - Experimental results of Japanese broadcast news dictation with various language models (word error rate [%]) Language Evaluation sets model m/c m/n f/c f/n LM1 17.6 37.2 14.3 41.2 LM2 16.8 35.9 13.6 39.3 LM3 14.2 33.1 12.9 38.1 13 When the top five topic words were chosen (recall=13%), 87% of them were correct on average. 3. HUMAN-COMPUTER DIALOGUE SYSTEMS 3.1 Typical Systems in US and Europe Recently a number of sites have been working on</context>
<context position="20722" citStr="[4]" startWordPosition="3017" endWordPosition="3017"> reasonable modeling of speaker-to-speaker variablity or constraints. Modeling of the mechanism of speech production is expected to provide a useful modeling of speaker-to-speaker variability. 4.2 On-line speaker adaptation in broadcast news dictation Since, in broadcast news, each speaker utters several sentences in succession, the recognition error rate can be reduced by adapting acoustic models incrementally within a segment that contains only one speaker. We applied on-line, unsupervised, instantaneous and incremental speaker adaptation combined with automatic detection of speaker changes [4]. The MLLR [15] -MAP [16] and VFS (vector-field smoothing) [17] methods were instantaneously and incrementally carried out for each utterance. The adaptation process is as follows. For the first input utterance, the speaker-independent model is used for both recognition and adaptation, and the first speaker-adapted model is created. For the second input utterance, the likelihood value of the utterance given the speaker-independent model and that given the speaker-adapted model are calculated and compared. If the former value is larger, the utterance is considered to be the beginning of a new s</context>
<context position="24423" citStr="[4]" startWordPosition="3559" endWordPosition="3559">, and Xis an acoustic observation sequence. This criterion is reasonable for dictating read speech. However, the ultimate goal of automatic speech recognition is to extract the underlying messages of the speaker from the speech signals. Hence we need to model the process of speech generation and recognition as shown in Fig. 9 [18], where M is the message (content) that a speaker intended to convey. Fig. 9 - A communication - theoretic view of speech recognition. According to this model, the speech recognition process is represented as the maximization of the following a posteriori probability [4][5], models in the same way as in usual recognition processes. We assume that P(M) has a uniform probability for all M. Therefore, we only need to consider further the term P( WM). We assume that P(W1M) can be expressed as follows. P(141M) w) 1 -AP( wi ApA , (4) where A, 0 is a weighting factor. P(W), the first term of the right hand side, represents a part ofP(WIM) that is independent ofMand can be given by a general statistical language model. P&apos;(WIM), the second term of the right hand side, represents the part of P(WIM) that depends on M. We consider that M is represented by a co-occurrence</context>
</contexts>
<marker>[4]</marker>
<rawString>K. Ohtsuki, S. Furui, N. Sakurai, A. Iwasaki and Z.-P. Zhang: &amp;quot;Improvements in Japanese broadcast news transcription&amp;quot;, DARPA Broadcast News Workshop, Virginia (1999)</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ohtsuld</author>
<author>S Furui</author>
<author>A Iwasaki</author>
<author>N Sakurai</author>
</authors>
<title>Message-driven speech recognition and topicword extraction&amp;quot;,</title>
<date>1999</date>
<booktitle>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.,</booktitle>
<pages>625--628</pages>
<location>Phoenix,</location>
<contexts>
<context position="5346" citStr="[5]" startWordPosition="768" endWordPosition="768"> for spontaneous speech recognition/ understanding. Section 6 concludes the paper. 2. BROADCAST NEWS DICTATION AND INFORMATION EXTRACTION 2.1 DARPA Broadcast News Dictation Project With the introduction of the broadcast news test bed to the DARPA project in 1995, the research effort took a profound step forward. Many of the deficiencies of the WSJ domain were resolved in the broadcast news domain [3]. Most importantly, the fact that broadcast news is a realWe have been developing a largevocabulary continuous-speech recognition (LVCSR) system for Japanese broadcast-news speech transcription [4][5]. This is a part of a joint research with the NHK broadcast company whose goal is the closed-captioning of TV programs. The broadcast-news manuscripts that were used for constructing the language models were taken from the period between July 1992 and May 1996, and comprised roughly 500k sentences and 22M words. To calculate word ngram language models, we segmented the broadcast-news manuscripts into words by using a morphological analyzer since Japanese sentences are written without spaces between words. A word-frequency list was derived for the news manuscripts, and the 20k most frequently u</context>
<context position="9438" citStr="[5]" startWordPosition="1383" endWordPosition="1383"> features of news producing a wide range of capabilities for browsing news archives interactively. These systems integrate various diverse speech and language technologies including speech recognition, speaker change detection, speaker identification, name extaction, topic classification and information retrieval. 2.4 Information Extraction from Japanese Broadcast News Summarizing transcribed news speech is useful for retrieving or indexing broadcast news. We investigated a method for extracting topic words from nouns in the speech recognition results on the basis of a significance measure [4][5]. The extracted topic words were compared with &amp;quot;true&amp;quot; topic words, which were given by three human subjects. The results are shown in Figure 2. Table 1 - Experimental results of Japanese broadcast news dictation with various language models (word error rate [%]) Language Evaluation sets model m/c m/n f/c f/n LM1 17.6 37.2 14.3 41.2 LM2 16.8 35.9 13.6 39.3 LM3 14.2 33.1 12.9 38.1 13 When the top five topic words were chosen (recall=13%), 87% of them were correct on average. 3. HUMAN-COMPUTER DIALOGUE SYSTEMS 3.1 Typical Systems in US and Europe Recently a number of sites have been working on hu</context>
<context position="24426" citStr="[5]" startWordPosition="3559" endWordPosition="3559">nd Xis an acoustic observation sequence. This criterion is reasonable for dictating read speech. However, the ultimate goal of automatic speech recognition is to extract the underlying messages of the speaker from the speech signals. Hence we need to model the process of speech generation and recognition as shown in Fig. 9 [18], where M is the message (content) that a speaker intended to convey. Fig. 9 - A communication - theoretic view of speech recognition. According to this model, the speech recognition process is represented as the maximization of the following a posteriori probability [4][5], models in the same way as in usual recognition processes. We assume that P(M) has a uniform probability for all M. Therefore, we only need to consider further the term P( WM). We assume that P(W1M) can be expressed as follows. P(141M) w) 1 -AP( wi ApA , (4) where A, 0 is a weighting factor. P(W), the first term of the right hand side, represents a part ofP(WIM) that is independent ofMand can be given by a general statistical language model. P&apos;(WIM), the second term of the right hand side, represents the part of P(WIM) that depends on M. We consider that M is represented by a co-occurrence re</context>
</contexts>
<marker>[5]</marker>
<rawString>K. Ohtsuld, S. Furui, A. Iwasaki and N. Sakurai: &amp;quot;Message-driven speech recognition and topicword extraction&amp;quot;, Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., Phoenix, pp. 625-628 (1999)</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Witbrock</author>
<author>A G Hauptmann</author>
</authors>
<title>Speech recognition and information retrieval: Experiments in retrieving spoken documents&amp;quot;,</title>
<date>1997</date>
<booktitle>Proc. DARPA Speech Recognition Workshop,</booktitle>
<pages>160--164</pages>
<location>Virginia,</location>
<note>See also http:// www.informedia.cs.cmu.edu/</note>
<contexts>
<context position="8752" citStr="[6]" startWordPosition="1292" endWordPosition="1292"> the DARPA Project News is filled with events, people, and organizations and all manner of relations among them. The great richness of material and the naturally evolving content in broadcast news has leveraged its value into areas of research well beyond speech recognition. In the DARPA project, the Spoken Document Retrieval (SDR) of TREC and the Topic Detection and Tracking (TDT) program are supported by the same materials and systems that have been developed in the broadcast news dictation arena [3]. BBN&apos;sRough&apos;n&apos;Reddy system extracts structural features of broadcast news. CMU&apos;s Informedia [6], MITRE&apos;s Broadcast Navigator, and SRI&apos;s Maestro have all exploited the multi-media features of news producing a wide range of capabilities for browsing news archives interactively. These systems integrate various diverse speech and language technologies including speech recognition, speaker change detection, speaker identification, name extaction, topic classification and information retrieval. 2.4 Information Extraction from Japanese Broadcast News Summarizing transcribed news speech is useful for retrieving or indexing broadcast news. We investigated a method for extracting topic words from</context>
<context position="11725" citStr="[6]" startWordPosition="1738" endWordPosition="1738">ided in Fig. 4. The system consists of three components: (1) a speaker-independent largevocabulary speech recognition engine which 0 25 50 75 100 Recall[fo] Fig. 2 - Topic word extraction results. (a) The View4You system at the University of Karksruhe The University of Karlsruhe focuses its speech research on a content-addressable multimedia information retrieval system, under a multi-lingual environment, where queries and multimedia documents may appear in multiple languages [7]. The system is called &amp;quot;View4You&amp;quot; and their research is conducted in cooperation with the Informedia project at CMU [6]. In the View4You Query Result L gm( Nleo query servier 11 Segmenter MPEG-audio ,y Segment boundaries (Speech recognizer) MPEG-audio WWW Internet newspapers Fig. 3 - System overview of the View4You system. 14 Multimedia database Text Segment boundaries ( Thesaurus ) Text Front-end Query (Input speech recognizer Result output segments the speech archive and generates transcripts, (2) an information-retrieval engine which indexes the transcriptions and formulates hypotheses regarding document relevance to user-submitted queries and (3) a graphical-userinterface which supports search and local co</context>
</contexts>
<marker>[6]</marker>
<rawString>M. Witbrock and A. G. Hauptmann: &amp;quot;Speech recognition and information retrieval: Experiments in retrieving spoken documents&amp;quot;, Proc. DARPA Speech Recognition Workshop, Virginia, pp. 160-164 (1997). See also http:// www.informedia.cs.cmu.edu/</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Kemp</author>
<author>P Geutner</author>
<author>M Schmidt</author>
<author>B Tomaz</author>
<author>M Weber</author>
<author>M Westphal</author>
<author>A Waibel</author>
</authors>
<title>The interactive systems labs View4You video indexing system&amp;quot;,</title>
<date>1998</date>
<booktitle>Proc. Int. Conf. Spoken Language Processing,</booktitle>
<pages>1639--1642</pages>
<location>Sydney,</location>
<contexts>
<context position="11606" citStr="[7]" startWordPosition="1719" endWordPosition="1719">lopment focused on the application of SCAN to the broadcast news domain. An overview of the system architecture is provided in Fig. 4. The system consists of three components: (1) a speaker-independent largevocabulary speech recognition engine which 0 25 50 75 100 Recall[fo] Fig. 2 - Topic word extraction results. (a) The View4You system at the University of Karksruhe The University of Karlsruhe focuses its speech research on a content-addressable multimedia information retrieval system, under a multi-lingual environment, where queries and multimedia documents may appear in multiple languages [7]. The system is called &amp;quot;View4You&amp;quot; and their research is conducted in cooperation with the Informedia project at CMU [6]. In the View4You Query Result L gm( Nleo query servier 11 Segmenter MPEG-audio ,y Segment boundaries (Speech recognizer) MPEG-audio WWW Internet newspapers Fig. 3 - System overview of the View4You system. 14 Multimedia database Text Segment boundaries ( Thesaurus ) Text Front-end Query (Input speech recognizer Result output segments the speech archive and generates transcripts, (2) an information-retrieval engine which indexes the transcriptions and formulates hypotheses rega</context>
</contexts>
<marker>[7]</marker>
<rawString>T. Kemp, P. Geutner, M. Schmidt, B. Tomaz, M. Weber, M. Westphal and A. Waibel: &amp;quot;The interactive systems labs View4You video indexing system&amp;quot;, Proc. Int. Conf. Spoken Language Processing, Sydney, pp. 1639-1642 (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Choi</author>
<author>D Hindle</author>
<author>J Hirschberg</author>
<author>I MagrinChagnolleau</author>
<author>C Nakatani</author>
<author>F Pereira</author>
<author>A Singhal</author>
<author>S Whittaker</author>
</authors>
<title>SCAN - speech content based audio navigator: a systems overview&amp;quot;,</title>
<date>1998</date>
<booktitle>Proc. Int. Conf. Spoken Language Processing,</booktitle>
<pages>2867--2870</pages>
<location>Sydney,</location>
<contexts>
<context position="10989" citStr="[8]" startWordPosition="1628" endWordPosition="1628">eech utterance. The system returns a list of segments which is sorted by relevance with respect to the user query. By selecting a segment, the user can watch the corresponding part of the news show on his/her computer screen. The system overview is shown in Fig. 3. (b) The SCAN- speech content based audio navigator at AT&amp;T Labs SCAN (Speech Content based Audio Navigator) is a spoken document retrieval system developed at AT&amp;T Labs integrating speaker-independent, large-vocabulary speech recognition with information-retrieval to support query-based retrieval of information from speech archives [8]. Initial development focused on the application of SCAN to the broadcast news domain. An overview of the system architecture is provided in Fig. 4. The system consists of three components: (1) a speaker-independent largevocabulary speech recognition engine which 0 25 50 75 100 Recall[fo] Fig. 2 - Topic word extraction results. (a) The View4You system at the University of Karksruhe The University of Karlsruhe focuses its speech research on a content-addressable multimedia information retrieval system, under a multi-lingual environment, where queries and multimedia documents may appear in multi</context>
</contexts>
<marker>[8]</marker>
<rawString>J. Choi, D. Hindle, J. Hirschberg, I. MagrinChagnolleau, C. Nakatani, F. Pereira, A. Singhal and S. Whittaker: &amp;quot;SCAN - speech content based audio navigator: a systems overview&amp;quot;, Proc. Int. Conf. Spoken Language Processing, Sydney, pp. 2867-2870 (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Seneff</author>
<author>E Hurley</author>
<author>R Lau</author>
<author>C Pao</author>
<author>P Schmid</author>
<author>V Zue</author>
</authors>
<title>GALAXY-II: a reference architecture for conversational system development&amp;quot;,</title>
<date>1998</date>
<booktitle>Proc. Int. Conf. Spoken Language Processing,</booktitle>
<pages>931--934</pages>
<location>Sydney,</location>
<contexts>
<context position="14326" citStr="[9]" startWordPosition="2110" endWordPosition="2110">he SCAN spoken document system architecture. 1 Information retrieval Opeecl&gt; Intonational corpus phrase boundary detection Classification Recognition Transcripts User interface GENESIS DEC TALK &amp; EN VOICE Text-to-speech conversion Dialogue mana ement Application back-ends Audio server SUMMIT Frame construction TINA Context tracking Discourse D-Server Phone I-Server Speech recognition Language generation Fig. 5 - Architecture of GALAXY-II. (c) The GALAXY-II conversational system at MIT Galaxy is a clientserver architecture developed at MIT for accessing online information using spoken dialogue [9]. It has served as the testbed for developing human language 15 (d) The ARISE train travel information system at LIMSI The ARISE (Automatic Railway Information Systems for Europe) projects aims developing prototype telephone information services for train travel information in several European countries [10]. In collaboration with the Vecsys company and with the SNCF (the French Railways), LIMSI has developed a prototype telephone service providing timetables, simulated fares and reservations, and information on reductions and services for the main French intercity connections. A prototype Fre</context>
</contexts>
<marker>[9]</marker>
<rawString>S. Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid and V. Zue: &amp;quot;GALAXY-II: a reference architecture for conversational system development&amp;quot;, Proc. Int. Conf. Spoken Language Processing, Sydney, pp. 931-934 (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Lamel</author>
<author>S Rosset</author>
<author>J L Gauvain</author>
<author>S Bennacef</author>
</authors>
<title>The LIMSI ARISE system for train travel information&amp;quot;,</title>
<date>1999</date>
<booktitle>Proc. IEEE Int. Conf. Acoust., Speech, Signal Process.,</booktitle>
<pages>501--504</pages>
<location>Phoenix,</location>
<contexts>
<context position="14635" citStr="[10]" startWordPosition="2154" endWordPosition="2154">ion TINA Context tracking Discourse D-Server Phone I-Server Speech recognition Language generation Fig. 5 - Architecture of GALAXY-II. (c) The GALAXY-II conversational system at MIT Galaxy is a clientserver architecture developed at MIT for accessing online information using spoken dialogue [9]. It has served as the testbed for developing human language 15 (d) The ARISE train travel information system at LIMSI The ARISE (Automatic Railway Information Systems for Europe) projects aims developing prototype telephone information services for train travel information in several European countries [10]. In collaboration with the Vecsys company and with the SNCF (the French Railways), LIMSI has developed a prototype telephone service providing timetables, simulated fares and reservations, and information on reductions and services for the main French intercity connections. A prototype French/English service for the high speed trains between Paris and London is also under development. The system is based on the spoken language systems developed for the RailTel project [11] and the ESPRIT Mask project [12]. Compared to the RailTel system, the main advances in ARISE are in dialogue management, </context>
</contexts>
<marker>[10]</marker>
<rawString>L. Lamel, S. Rosset, J. L. Gauvain and S. Bennacef: &amp;quot;The LIMSI ARISE system for train travel information&amp;quot;, Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., Phoenix, pp. 501-504 (1999)</rawString>
</citation>
<citation valid="true">
<authors>
<author>L F Lamel</author>
<author>S K Bennacef</author>
<author>S Rosset</author>
<author>L Devillers</author>
<author>S Foukia</author>
<author>J J Gangolf</author>
<author>J L Gauvain</author>
</authors>
<title>The LIMSI RailTel system: Field trial of a telephone service for rail travel information&amp;quot;,</title>
<date>1997</date>
<journal>Speech Communication,</journal>
<volume>23</volume>
<pages>67--82</pages>
<contexts>
<context position="15113" citStr="[11]" startWordPosition="2224" endWordPosition="2224">jects aims developing prototype telephone information services for train travel information in several European countries [10]. In collaboration with the Vecsys company and with the SNCF (the French Railways), LIMSI has developed a prototype telephone service providing timetables, simulated fares and reservations, and information on reductions and services for the main French intercity connections. A prototype French/English service for the high speed trains between Paris and London is also under development. The system is based on the spoken language systems developed for the RailTel project [11] and the ESPRIT Mask project [12]. Compared to the RailTel system, the main advances in ARISE are in dialogue management, confidence measures, inclusion of optional spell mode for city/station names, and barge-in capability to allow more natural interaction between the user and the machine. 3.2 Designing a Multimodal Dialogue System for Information Retrieval We have recently investigated a paradigm for designing multimodal dialogue systems [13]. An example task of the system was to retrieve particular information about different shops in the Tokyo Metropolitan area, such as their names, addres</context>
</contexts>
<marker>[11]</marker>
<rawString>L. F. Lamel, S. K. Bennacef, S. Rosset, L. Devillers, S. Foukia, J. J. Gangolf and J. L. Gauvain: &amp;quot;The LIMSI RailTel system: Field trial of a telephone service for rail travel information&amp;quot;, Speech Communication, 23, pp. 67-82 (1997)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J L Gauvain</author>
<author>J J Gangolf</author>
<author>L Lamel</author>
</authors>
<title>Speech recognition for an information Kiosk&amp;quot;,</title>
<date>1998</date>
<booktitle>Proc. Int. Conf. Spoken Language Processing,</booktitle>
<pages>849--852</pages>
<location>Philadelphia,</location>
<contexts>
<context position="15146" citStr="[12]" startWordPosition="2230" endWordPosition="2230">elephone information services for train travel information in several European countries [10]. In collaboration with the Vecsys company and with the SNCF (the French Railways), LIMSI has developed a prototype telephone service providing timetables, simulated fares and reservations, and information on reductions and services for the main French intercity connections. A prototype French/English service for the high speed trains between Paris and London is also under development. The system is based on the spoken language systems developed for the RailTel project [11] and the ESPRIT Mask project [12]. Compared to the RailTel system, the main advances in ARISE are in dialogue management, confidence measures, inclusion of optional spell mode for city/station names, and barge-in capability to allow more natural interaction between the user and the machine. 3.2 Designing a Multimodal Dialogue System for Information Retrieval We have recently investigated a paradigm for designing multimodal dialogue systems [13]. An example task of the system was to retrieve particular information about different shops in the Tokyo Metropolitan area, such as their names, addresses and phone numbers. The system</context>
</contexts>
<marker>[12]</marker>
<rawString>J. L. Gauvain, J. J. Gangolf and L. Lamel: &amp;quot;Speech recognition for an information Kiosk&amp;quot;, Proc. Int. Conf. Spoken Language Processing, Philadelphia, pp. 849-852 (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
<author>K Yamaguchi</author>
</authors>
<title>Designing a multimodal dialogue system for information retrieval&amp;quot;,</title>
<date>1998</date>
<booktitle>Proc. Int. Conf. Spoken Language Processing,</booktitle>
<pages>1191--1194</pages>
<location>Sydney,</location>
<contexts>
<context position="15561" citStr="[13]" startWordPosition="2289" endWordPosition="2289">h speed trains between Paris and London is also under development. The system is based on the spoken language systems developed for the RailTel project [11] and the ESPRIT Mask project [12]. Compared to the RailTel system, the main advances in ARISE are in dialogue management, confidence measures, inclusion of optional spell mode for city/station names, and barge-in capability to allow more natural interaction between the user and the machine. 3.2 Designing a Multimodal Dialogue System for Information Retrieval We have recently investigated a paradigm for designing multimodal dialogue systems [13]. An example task of the system was to retrieve particular information about different shops in the Tokyo Metropolitan area, such as their names, addresses and phone numbers. The system accepted speech and screen touching as input, and presented retrieved information on a screen display or by synthesized speech as shown in Fig. 6. The speech recognition part was modeled by the FSN (finite state network) consisting of keywords and fillers, both of which were implemented by the DAWG (directed acyclic word-graph) structure. The number of keywords was 306, consisting of district names and business</context>
</contexts>
<marker>[13]</marker>
<rawString>S. Furui and K. Yamaguchi: &amp;quot;Designing a multimodal dialogue system for information retrieval&amp;quot;, Proc. Int. Conf. Spoken Language Processing, Sydney, pp. 1191-1194 (1998)</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Furui</author>
</authors>
<title>Recent advances in robust speech recognition&amp;quot;,</title>
<date>1997</date>
<booktitle>Proc. ESCA-NATO Workshop on Robust Speech Recognition for Unknown Communication Channels,</booktitle>
<pages>11--20</pages>
<location>Pont-a-Mousson, France,</location>
<contexts>
<context position="18073" citStr="[14]" startWordPosition="2661" endWordPosition="2661">rtion Noise Echoes Dropouts Speech recognition system Speaker Task/context • Voice quality • Man-machine • Pitch dialogue • Gender • Dictation • Dialect • Free conversation Speaking style • Interview • Stress/emotion Phonetic/prosodic • Speaking rate context • Lombard effect Microphone • Distortion • Electrical noise • Directional characteristics Fig. 7 - Main causes of acoustic variation in speech. Ultimately, speech recognition systems should be capable of robust, speakerindependent or speakeradaptive, continuous speech recognition. Figure 7 shows main causes of acoustic variation in speech [14]. It is crucial to establish methods that are robust against voice variation due to individuality, the physical and psychological condition of the speaker, telephone sets, microphones, network characteristics, additive background noise, speaking styles, and so on. Figure 8 shows main methods for making speech recognition systems robust against voice variation. It is also important for the systems to impose few restrictions on tasks and vocabulary. To solve these problems, it is essential to develop automatic adaptation techniques. Extraction and normalization of. (adaptation to) voice individu</context>
</contexts>
<marker>[14]</marker>
<rawString>S. Furui: &amp;quot;Recent advances in robust speech recognition&amp;quot;, Proc. ESCA-NATO Workshop on Robust Speech Recognition for Unknown Communication Channels, Pont-a-Mousson, France, pp. 11-20 (1997)</rawString>
</citation>
<citation valid="true">
<authors>
<author>C J Leggetter</author>
<author>P C Woodland</author>
</authors>
<title>Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models&amp;quot;,</title>
<date>1995</date>
<journal>Computer Speech and Language,</journal>
<pages>171--185</pages>
<contexts>
<context position="20737" citStr="[15]" startWordPosition="3020" endWordPosition="3020">deling of speaker-to-speaker variablity or constraints. Modeling of the mechanism of speech production is expected to provide a useful modeling of speaker-to-speaker variability. 4.2 On-line speaker adaptation in broadcast news dictation Since, in broadcast news, each speaker utters several sentences in succession, the recognition error rate can be reduced by adapting acoustic models incrementally within a segment that contains only one speaker. We applied on-line, unsupervised, instantaneous and incremental speaker adaptation combined with automatic detection of speaker changes [4]. The MLLR [15] -MAP [16] and VFS (vector-field smoothing) [17] methods were instantaneously and incrementally carried out for each utterance. The adaptation process is as follows. For the first input utterance, the speaker-independent model is used for both recognition and adaptation, and the first speaker-adapted model is created. For the second input utterance, the likelihood value of the utterance given the speaker-independent model and that given the speaker-adapted model are calculated and compared. If the former value is larger, the utterance is considered to be the beginning of a new speaker, and ano</context>
</contexts>
<marker>[15]</marker>
<rawString>C. J. Leggetter and P. C. Woodland: &amp;quot;Maximum likelihood linear regression for speaker adaptation of continuous density hidden Markov models&amp;quot;, Computer Speech and Language, pp. 171-185 (1995).</rawString>
</citation>
<citation valid="true">
<authors>
<author>J -L Gauvain</author>
<author>C-H Lee</author>
</authors>
<title>Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains&amp;quot;</title>
<date>1994</date>
<journal>IEEE Trans. on Speech and Audio Processing,</journal>
<volume>2</volume>
<pages>291--298</pages>
<contexts>
<context position="20747" citStr="[16]" startWordPosition="3022" endWordPosition="3022">speaker-to-speaker variablity or constraints. Modeling of the mechanism of speech production is expected to provide a useful modeling of speaker-to-speaker variability. 4.2 On-line speaker adaptation in broadcast news dictation Since, in broadcast news, each speaker utters several sentences in succession, the recognition error rate can be reduced by adapting acoustic models incrementally within a segment that contains only one speaker. We applied on-line, unsupervised, instantaneous and incremental speaker adaptation combined with automatic detection of speaker changes [4]. The MLLR [15] -MAP [16] and VFS (vector-field smoothing) [17] methods were instantaneously and incrementally carried out for each utterance. The adaptation process is as follows. For the first input utterance, the speaker-independent model is used for both recognition and adaptation, and the first speaker-adapted model is created. For the second input utterance, the likelihood value of the utterance given the speaker-independent model and that given the speaker-adapted model are calculated and compared. If the former value is larger, the utterance is considered to be the beginning of a new speaker, and another speak</context>
</contexts>
<marker>[16]</marker>
<rawString>J. -L. Gauvain and C.-H. Lee: &amp;quot;Maximum a posteriori estimation for multivariate Gaussian mixture observations of Markov chains&amp;quot; IEEE Trans. on Speech and Audio Processing, 2, 2, pp. 291-298 (1994).</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Ohkura</author>
<author>M Sugiyama</author>
<author>S Sagayama</author>
</authors>
<title>Speaker adaptation based on transfer vector field smoothing with continuous mixture density HMMs&apos;&amp;quot;,</title>
<date>1992</date>
<booktitle>Proc. Int. Conf. Spoken Language Processing,</booktitle>
<pages>369--372</pages>
<location>Banff,</location>
<contexts>
<context position="20785" citStr="[17]" startWordPosition="3027" endWordPosition="3027">raints. Modeling of the mechanism of speech production is expected to provide a useful modeling of speaker-to-speaker variability. 4.2 On-line speaker adaptation in broadcast news dictation Since, in broadcast news, each speaker utters several sentences in succession, the recognition error rate can be reduced by adapting acoustic models incrementally within a segment that contains only one speaker. We applied on-line, unsupervised, instantaneous and incremental speaker adaptation combined with automatic detection of speaker changes [4]. The MLLR [15] -MAP [16] and VFS (vector-field smoothing) [17] methods were instantaneously and incrementally carried out for each utterance. The adaptation process is as follows. For the first input utterance, the speaker-independent model is used for both recognition and adaptation, and the first speaker-adapted model is created. For the second input utterance, the likelihood value of the utterance given the speaker-independent model and that given the speaker-adapted model are calculated and compared. If the former value is larger, the utterance is considered to be the beginning of a new speaker, and another speakeradapted model is created. Otherwise,</context>
</contexts>
<marker>[17]</marker>
<rawString>K. Ohkura, M. Sugiyama and S. Sagayama: &amp;quot;Speaker adaptation based on transfer vector field smoothing with continuous mixture density HMMs&apos;&amp;quot;, Proc. Int. Conf. Spoken Language Processing, Banff, pp. 369-372 (1992)</rawString>
</citation>
<citation valid="true">
<authors>
<author>B-H Juang</author>
</authors>
<title>Automatic speech recognition: Problems, progress &amp; prospects&amp;quot;,</title>
<date>1996</date>
<booktitle>IEEE Workshop on Neural Networks for Signal Processing</booktitle>
<contexts>
<context position="24152" citStr="[18]" startWordPosition="3516" endWordPosition="3516">y important issue, since collecting a large linguistic database for every new task is difficult and costly. 18 5.2 Message-Driven Speech Recognition State-of-the-art automatic speech recognition systems employ the criterion of maximizing P(WIX), where W is a word sequence, and Xis an acoustic observation sequence. This criterion is reasonable for dictating read speech. However, the ultimate goal of automatic speech recognition is to extract the underlying messages of the speaker from the speech signals. Hence we need to model the process of speech generation and recognition as shown in Fig. 9 [18], where M is the message (content) that a speaker intended to convey. Fig. 9 - A communication - theoretic view of speech recognition. According to this model, the speech recognition process is represented as the maximization of the following a posteriori probability [4][5], models in the same way as in usual recognition processes. We assume that P(M) has a uniform probability for all M. Therefore, we only need to consider further the term P( WM). We assume that P(W1M) can be expressed as follows. P(141M) w) 1 -AP( wi ApA , (4) where A, 0 is a weighting factor. P(W), the first term of the righ</context>
</contexts>
<marker>[18]</marker>
<rawString>B.-H. Juang: &amp;quot;Automatic speech recognition: Problems, progress &amp; prospects&amp;quot;, IEEE Workshop on Neural Networks for Signal Processing (1996)</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z S Harris</author>
</authors>
<title>Co-occurrence and transformation in linguistic structure&amp;quot;,</title>
<date>1957</date>
<journal>Language,</journal>
<volume>33</volume>
<pages>283--340</pages>
<contexts>
<context position="25104" citStr="[19]" startWordPosition="3680" endWordPosition="3680"> P(M) has a uniform probability for all M. Therefore, we only need to consider further the term P( WM). We assume that P(W1M) can be expressed as follows. P(141M) w) 1 -AP( wi ApA , (4) where A, 0 is a weighting factor. P(W), the first term of the right hand side, represents a part ofP(WIM) that is independent ofMand can be given by a general statistical language model. P&apos;(WIM), the second term of the right hand side, represents the part of P(WIM) that depends on M. We consider that M is represented by a co-occurrence recognizer Speech of words based on the distributional hypothesis by Harris [19]. Since this approach formulates PVTIM) without explicitly representing M, it can use information about the speaker&apos;s message M without being affected by the quantization problem of topic classes. This new formulation of speech recognition was applied to the Japanese broadcast news dictation, and it was found that word error rates for the clean set were slightly reduced by this method. P(M) Message source P(WIM) Linguistic channel Language Vocabulary Grammar Semantics Context Habits P(XIW) Acoustic channel • Speaker Reverberation Noise Transmissioncharacteristics Microphone generation and maxP</context>
</contexts>
<marker>[19]</marker>
<rawString>Z. S. Harris: &amp;quot;Co-occurrence and transformation in linguistic structure&amp;quot;, Language, 33, pp. 283-340 (1957)</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>