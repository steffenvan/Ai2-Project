<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.114068">
<title confidence="0.808034">
Lisbon: Evaluating TurboSemanticParser on
Multiple Languages and Out-of-Domain Data
Mariana S. C. Almeida*† Andr´e F. T. Martins*†
</title>
<author confidence="0.756167">
*Priberam Labs, Alameda D. Afonso Henriques, 41, 21, 1000-123 Lisboa, Portugal
</author>
<affiliation confidence="0.838947">
†Instituto de Telecomunicac¸˜oes, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal
</affiliation>
<email confidence="0.997937">
{atm,mla}@priberam.pt
</email>
<sectionHeader confidence="0.997373" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999941833333333">
As part of the SemEval-2015 shared task on
Broad-Coverage Semantic Dependency Pars-
ing, we evaluate the performace of our last
year’s system (TurboSemanticParser) on mul-
tiple languages and out-of-domain data. Our
system is characterized by a feature-rich lin-
ear model, that includes scores for first and
second-order dependencies (arcs, siblings,
grandparents and co-parents). For decoding
this second-order model, we solve a linear re-
laxation of that problem using alternating di-
rections dual decomposition (AD3). The ex-
periments have shown that, even though the
parser’s performance in Chinese and Czech at-
tains around 80% (not too far from English
performance), domain shift is a serious issue,
suggesting domain adaptation as an interest-
ing avenue for future research.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999977222222222">
The last years have witnessed a continuous progress
in statistical multilingual models for syntax, thanks
to shared tasks such as CoNLL 2006-7 (Buchholz
and Marsi, 2006; Nivre et al., 2007) and, more re-
cently, SPMRL 2013-14 (Seddah et al., 2013; Sed-
dah et al., 2014). As a global trend, we observe
that models that incorporate rich global features are
typically more accurate, even if pruning is neces-
sary or decoding needs to be approximate (McDon-
ald et al., 2006; Koo and Collins, 2010; Bohnet and
Nivre, 2012; Martins et al., 2009, 2013). The same
rationale applies to semantic dependency parsing,
also a structured prediction problem, but where the
output variable is a semantic graph, rather than a
syntactic tree. Indeed, the best performing systems
in last year shared task on broad-coverage seman-
tic dependency parsing follow this principle (Oepen
et al., 2014). This year, a new challenge was put
forth: how to handle multiple languages and out-of-
domain data?
Our proposed parser (§2) is essentially the same
that we submitted in the previous year to the same
SemEval task (Martins and Almeida, 2014), where
we scored top in the open challenge and second in
the closed track. This year, we report results using
new out-of-domain and multilingual data (namely,
Czech and Chinese, in addition to English). For the
English language, we participated in the closed and
open tracks, using as additional resources the syn-
tactic dependency annotations provided by the orga-
nizers. For Czech and Chinese, we only addressed
the closed track, since no companion data were pro-
vided for these languages. We did not participate in
the gold track that uses gold-standard syntactic an-
notations; and we did not address the prediction of
predicate senses.
</bodyText>
<sectionHeader confidence="0.925275" genericHeader="method">
2 Semantic Parser
</sectionHeader>
<bodyText confidence="0.9998095">
For this year’s shared task, we re-run the semantic
parser that we developed last year, which is fully
desc1ribed in Martins and Almeida (2014), on the
new datasets. Since this parser was designed to
be multi-lingual, it was straightforward to apply it
to the languages introduced this year (Chinese and
Czech), as well as on the out-of-domain data.
We briefly describe our semantic parser (which
we dub TurboSemanticParser and release as open-
source software1), and refer the interested reader to
</bodyText>
<footnote confidence="0.997678">
1http://labs.priberam.com/Resources/
TurboSemanticParser
</footnote>
<page confidence="0.902371">
970
</page>
<note confidence="0.759305">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 970–973,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<figureCaption confidence="0.982723333333334">
Figure 1: Parts considered by our semantic parser. The
top row illustrate the basic parts, representing the event
that a word is a predicate, or the existence of an arc be-
tween a predicate and an argument, eventually labeled
with a semantic role. Our second-order model looks at
some pairs of arcs: arcs bearing a grandparent relation-
ship, arguments of the same predicate, predicates shar-
ing the same argument, and consecutive versions of these
two.
</figureCaption>
<bodyText confidence="0.99949645">
Martins and Almeida (2014) for further details.
The parser was built as an extension of a re-
cent dependency parser, TurboParser (Martins et al.,
2010, 2013), with the goal of performing semantic
parsing using any of the three formalisms consid-
ered in the shared task (DM, PAS, and PSD). We
have followed prior work in semantic role label-
ing (Toutanova et al., 2005; Johansson and Nugues,
2008; Das et al., 2012; Flanigan et al., 2014), by
adding constraints and modeling interactions among
arguments within the same frame; however, we went
beyond such sibling interactions to consider more
complex grandparent and co-parent structures, ef-
fectively correlating different predicates. The over-
all set of parts used by our parser is illustrated in Fig-
ure 1; note that by using only a subset of the parts
(predicate, arc, labeled arc, and sibling parts), the
semantic parser decodes each predicate frame inde-
pendently from other predicates; it is the co-parent
and grandparent parts that have the effect of creating
inter-dependence among predicates; we will analyze
the effect of these dependencies in the experimental
section (§3).
For each part in our model (shown in Figure 1),
we computed binary features based on various com-
bination of lexical forms, lemmas, POS tags and
syntactic dependency relations of words related to
the corresponding predicates and arguments. Most
of these features were taken from TurboParser (Mar-
tins et al., 2013), and others were inspired by the
semantic parser of Johansson and Nugues (2008).
To tackle all the parts, we formulate parsing as a
global optimization problem and solve a relaxation
through AD3 (Martins et al., 2011), a fast dual de-
composition algorithm in which several simple local
subproblems are solved iteratively. Through a rich
set of features, we arrive at top accuracies at parsing
speeds around 1,000 tokens per second. See Mar-
tins and Almeida (2014) for details on the model,
features and decoding process that were used.
</bodyText>
<sectionHeader confidence="0.993494" genericHeader="method">
3 Experimental Results
</sectionHeader>
<bodyText confidence="0.999884657142857">
All models were trained by running 10 epochs of
max-loss MIRA with C = 0.01 (Crammer et al.,
2006). The cost function takes into account mis-
matches between predicted and gold dependencies,
with a cost cP on labeled arcs incorrectly predicted
(false positives) and a cost cR on gold labeled arcs
that were missed (false negatives). These values
were set through cross-validation in the dev set,
yielding cP = 0.4 and cR = 0.6 in all runs, ex-
cept for the English PSD dataset in the closed track,
for which cP = 0.3 and cR = 0.7.
As in the previous work, we speed up decoding by
training a probabilistic unlabeled first-order pruner
and discarding the arcs whose posterior probability
is below 10−4. This allows a significant reduction of
the search space with a very small drop in recall.
Table 1 shows our final results in the test set, for
a model trained in the train and development par-
titions. Note that we do not report scores for com-
plete predications, since we did not predict predicate
sense. Our system achieved the best final score in 3
out of the 4 tracks for the English language, and for
the in-domain closed track in the Czech language.
For the remaining 3 tracks we scored relatively close
to the best system (Peking), which consists of an
ensemble of various methods. For all languages,
the runtimes are in par with last year’s submission
(around 1,000 tokens per second).
As expected, the scores obtained for out-of-
domain data are significantly below those obtained
with in-domain data. This degradation becomes par-
ticularly striking for Czech, with F1-scores dropping
more than 15%. This suggests that domain adap-
tation (Blitzer et al., 2006; Daum´e III, 2007) is an
interesting research avenue for future work. In ad-
</bodyText>
<page confidence="0.984161">
971
</page>
<table confidence="0.999960588235294">
UP UR UF Our System LF Avg. LF Peking
LP LR Avg. LF
Eng. DM, closed, id 91.13 87.88 89.48 89.84 86.64 88.21
Eng. PAS, closed, id 93.12 91.14 92.12 91.87 89.92 90.88 85.15 85.33
Eng. PSD, closed, id 89.83 84.81 87.25 78.62 74.23 76.36
Eng. DM, open, id 91.62 89.46 90.52 90.52 88.39 89.44
Eng. PAS, open, id 93.50 91.93 92.71 92.45 90.90 91.67 86.23 –
Eng. PSD, open, id 91.27 86.16 88.64 79.88 75.41 77.58
Eng. DM, closed, ood 86.78 80.74 83.65 84.81 78.90 81.75
Eng. PAS, closed, ood 90.17 86.89 88.50 88.52 85.30 86.88 81.15 80.78
Eng. PSD, closed, ood 88.32 80.05 83.98 78.68 71.31 74.82
Eng. DM, open, ood 87.56 83.52 85.49 85.79 81.84 83.77
Eng. PAS, open, ood 90.42 87.91 89.15 88.88 86.41 87.63 82.53 –
Eng. PSD, open, ood 89.91 81.47 85.48 80.12 72.61 76.18
Chi. PAS, closed, id 85.56 81.99 83.74 83.81 80.31 82.02 82.02 83.43
Cze. PSD, closed, id 90.15 81.55 85.63 83.52 75.54 79.33 79.33 78.45
Cze. PSD, closed, ood 86.58 75.97 80.93 67.93 59.61 63.50 63.50 64.37
</table>
<tableCaption confidence="0.999851">
Table 1: Final scores in the test data. For comparison, we show the scores of the Peking system – our best competitor.
</tableCaption>
<bodyText confidence="0.999960142857143">
dition, as found last year for English, the gap be-
tween labeled and unlabeled scores is much higher
in the PSD formalism (for English and Czech) then
it is for the DM and PAS formalism (for English and
Chinese).
Finally, to assess the importance of the second or-
der features, Table 2 reports experiments in the dev-
set that progressively add several groups of features.
We can see that second order features provide valu-
able information that improves the final scores. In
particular, the higher-order features are extremely
useful for Chinese and Czech, where we can observe
gains of 1.5–2.0% over a sibling model that factors
over predicates.
</bodyText>
<sectionHeader confidence="0.999227" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999883333333333">
Our system, which is inspired by prior work in
syntactic parsing, implements a linear model with
second-order features, being able to model interac-
tions between siblings, grandparents and co-parents.
We have shown empirically that, for all the three lan-
guages, second-order features that correlate multiple
predicates have a strong impact in the final scores.
However, there is a large drop in accuracy when
moving to out-of-domain data.
</bodyText>
<table confidence="0.998868904761905">
UF LF
Eng. DM, arc-factored 90.19 89.20
Eng. DM, arc-factored, pruned 90.13 89.16
+siblings 90.56 89.53
full system 91.21 90.12
Eng. PAS, arc-factored 92.42 91.52
Eng. PAS, arc-factored, pruned 92.44 91.54
+siblings 92.50 91.53
full system 92.98 91.98
Eng. PSD, arc-factored 87.54 79.69
Eng. PSD, arc-factored, pruned 87.47 79.73
+siblings 88.10 79.87
full system 89.82 80.08
Chi. PAS, arc-factored 81.10 79.49
Chi. PAS, arc-factored, pruned 81.06 79.43
+siblings 81.54 79.70
full system 83.48 81.62
Cze. PSD, arc-factored 84.27 79.77
Cze. PSD, arc-factored, pruned 83.96 79.39
+siblings 85.53 80.44
full system 87.90 81.82
</table>
<tableCaption confidence="0.997906">
Table 2: Unlabeled/labeled Fl scores in the dev-set, pro-
</tableCaption>
<bodyText confidence="0.584728">
gressively adding groups of features. English results are
for the open track, while Czech and Chinese results are
for the closed track.
</bodyText>
<sectionHeader confidence="0.984418" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9964675">
We would like to thank the reviewers for
their helpful comments. This work was par-
</bodyText>
<page confidence="0.994739">
972
</page>
<bodyText confidence="0.9998738">
tially supported by the EU/FEDER programme,
QREN/POR Lisboa (Portugal), under the Intel-
ligo project (contract 2012/24803), and by the
FCT grants UID/EEA/50008/2013 and PTDC/EEI-
SII/2312/2012.
</bodyText>
<sectionHeader confidence="0.978731" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998536020618557">
John Blitzer, Ryan McDonald, and Fernando Pereira.
2006. Domain adaptation with structural correspon-
dence learning. In Proc. of Empirical Methods in Nat-
ural Language Processing (EMNLP), pages 120–128.
Bernd Bohnet and Joakim Nivre. 2012. A transition-
based system for joint part-of-speech tagging and la-
beled non-projective dependency parsing. In Proc. of
the Empirical Methods in Natural Language Process-
ing (EMNLP), pages 1455–1465.
Sabine Buchholz and Erwin Marsi. 2006. CoNLL-
X shared task on multilingual dependency parsing.
In Proc. Int. Conf. on Natural Language Learning
(CoNLL), pages 149–164.
Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-
Shwartz, and Yoram Singer. 2006. Online passive-
aggressive algorithms. Journal of Machine Learning
Research, 7:551–585.
Dipanjan Das, Andr´e F. T. Martins, and Noah A. Smith.
2012. An exact dual decomposition algorithm for
shallow semantic parsing with constraints. In Proc. of
First Joint Conference on Lexical and Computational
Semantics (*SEM 2012), pages 209–217.
Hall Daum´e III. 2007. Frustratingly easy domain adap-
tation. In Proc. of Annual Meeting of the Association
for Computational Linguistics (ACL), pages 256–263.
Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris
Dyer, and Noah A. Smith. 2014. A discriminative
graph-based parser for the abstract meaning represen-
tation. In Proc. of the Annual Meeting of the Associa-
tion for Computational Linguistics, pages 1426–1436.
Richard Johansson and Pierre Nugues. 2008.
Dependency-based syntactic–semantic analysis
with PropBank and NomBank. Int. Conf. on Natural
Language Learning (CoNLL), pages 183–187.
Terry Koo and Michael Collins. 2010. Efficient third-
order dependency parsers. In Proc. of Annual Meet-
ing of the Association for Computational Linguistics
(ACL), pages 1–11.
Andr´e F. T. Martins and Mariana S. C. Almeida. 2014.
Priberam: A turbo semantic parser with second order
features. In Proc. of the 8th Int. Workshop on Semantic
Evaluation (SemEval 2014), pages 471–476.
Andr´e F. T. Martins, Noah A. Smith, and Eric P. Xing.
2009. Concise integer linear programming formu-
lations for dependency parsing. In Proc. of Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 342–350.
Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing, Pe-
dro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2010.
Turbo parsers: Dependency parsing by approximate
variational inference. In Proc. of Empirical Meth-
ods for Natural Language Processing (EMNLP), pages
34–44.
Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar,
and M´ario A. T. Figueiredo. 2011. Dual decomposi-
tion with many overlapping components. In Proc. of
Empirical Methods for Natural Language Processing
(EMNLP), pages 238–249.
Andr´e F. T. Martins, Miguel B. Almeida, and Noah A.
Smith. 2013. Turning on the turbo: Fast third-order
non-projective turbo parsers. In Proc. of the Annual
Meeting of the Association for Computational Linguis-
tics (ACL), pages 617–622.
Ryan McDonald, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-
stage discriminative parser. In Proc. of Int. Conf. on
Natural Language Learning (CoNLL), pages 216–220.
Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDon-
ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.
2007. The CoNLL 2007 shared task on dependency
parsing. In Proc. of the CoNLL Shared Task Session of
Empirical Methods for Natural Language Processing,
volume 7, pages 915–932.
Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,
Daniel Zeman, Dan Flickinger, Jan Hajiˇc, Angelina
Ivanova, and Yi Zhang. 2014. Semeval 2014 task
8: broad-coverage semantic dependency parsing. In
Proc. of the 8th Int. Workshop on Semantic Evaluation
(SemEval 2014), pages 63–72.
Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie Can-
dito, Jinho D. Choi, Rich´ard Farkas, Jennifer Fos-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,
et al. 2013. Overview of the SPMRL 2013 shared
task: cross-framework evaluation of parsing morpho-
logically rich languages. In Proc. of the 4th Workshop
on Statistical Parsing of Morphologically Rich Lan-
guages (SPMRL 2013), pages 146–182.
Djam´e Seddah, Sandra K¨ubler, and Reut Tsarfaty. 2014.
Introducing the SPMRL 2014 shared task on parsing
morphologically-rich languages. In Proc. of the 5th
Workshop on Statistical Parsing of Morphologically
Rich Languages (SPMRL 2014), pages 23–29.
Kristina Toutanova, Aria Haghighi, and Christopher D.
Manning. 2005. Joint learning improves semantic role
labeling. In Proc. of the Annual Meeting of the As-
sociation for Computational Linguistics (ACL), pages
589–596.
</reference>
<page confidence="0.999197">
973
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.238281">
<title confidence="0.9727755">Lisbon: Evaluating TurboSemanticParser Multiple Languages and Out-of-Domain Data</title>
<author confidence="0.811524">S C F T</author>
<note confidence="0.373264">Labs, Alameda D. Afonso Henriques, 41, 1000-123 Lisboa, Portugal de Instituto Superior T´ecnico, 1049-001 Lisboa,</note>
<abstract confidence="0.996372947368421">As part of the SemEval-2015 shared task on Broad-Coverage Semantic Dependency Parsing, we evaluate the performace of our last system on multiple languages and out-of-domain data. Our system is characterized by a feature-rich linear model, that includes scores for first and second-order dependencies (arcs, siblings, grandparents and co-parents). For decoding this second-order model, we solve a linear relaxation of that problem using alternating didual decomposition The experiments have shown that, even though the parser’s performance in Chinese and Czech attains around 80% (not too far from English performance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>John Blitzer</author>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
</authors>
<title>Domain adaptation with structural correspondence learning.</title>
<date>2006</date>
<booktitle>In Proc. of Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>120--128</pages>
<contexts>
<context position="7729" citStr="Blitzer et al., 2006" startWordPosition="1232" endWordPosition="1235">re in 3 out of the 4 tracks for the English language, and for the in-domain closed track in the Czech language. For the remaining 3 tracks we scored relatively close to the best system (Peking), which consists of an ensemble of various methods. For all languages, the runtimes are in par with last year’s submission (around 1,000 tokens per second). As expected, the scores obtained for out-ofdomain data are significantly below those obtained with in-domain data. This degradation becomes particularly striking for Czech, with F1-scores dropping more than 15%. This suggests that domain adaptation (Blitzer et al., 2006; Daum´e III, 2007) is an interesting research avenue for future work. In ad971 UP UR UF Our System LF Avg. LF Peking LP LR Avg. LF Eng. DM, closed, id 91.13 87.88 89.48 89.84 86.64 88.21 Eng. PAS, closed, id 93.12 91.14 92.12 91.87 89.92 90.88 85.15 85.33 Eng. PSD, closed, id 89.83 84.81 87.25 78.62 74.23 76.36 Eng. DM, open, id 91.62 89.46 90.52 90.52 88.39 89.44 Eng. PAS, open, id 93.50 91.93 92.71 92.45 90.90 91.67 86.23 – Eng. PSD, open, id 91.27 86.16 88.64 79.88 75.41 77.58 Eng. DM, closed, ood 86.78 80.74 83.65 84.81 78.90 81.75 Eng. PAS, closed, ood 90.17 86.89 88.50 88.52 85.30 86.88</context>
</contexts>
<marker>Blitzer, McDonald, Pereira, 2006</marker>
<rawString>John Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proc. of Empirical Methods in Natural Language Processing (EMNLP), pages 120–128.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
<author>Joakim Nivre</author>
</authors>
<title>A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing.</title>
<date>2012</date>
<booktitle>In Proc. of the Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>1455--1465</pages>
<contexts>
<context position="1628" citStr="Bohnet and Nivre, 2012" startWordPosition="240" endWordPosition="243"> is a serious issue, suggesting domain adaptation as an interesting avenue for future research. 1 Introduction The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (Buchholz and Marsi, 2006; Nivre et al., 2007) and, more recently, SPMRL 2013-14 (Seddah et al., 2013; Seddah et al., 2014). As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014). This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data? Our proposed parser (§2) is essentially the same that we submitted in the previous year to the same SemEval task (Martins and Almeida, 2014), </context>
</contexts>
<marker>Bohnet, Nivre, 2012</marker>
<rawString>Bernd Bohnet and Joakim Nivre. 2012. A transitionbased system for joint part-of-speech tagging and labeled non-projective dependency parsing. In Proc. of the Empirical Methods in Natural Language Processing (EMNLP), pages 1455–1465.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sabine Buchholz</author>
<author>Erwin Marsi</author>
</authors>
<title>CoNLLX shared task on multilingual dependency parsing.</title>
<date>2006</date>
<booktitle>In Proc. Int. Conf. on Natural Language Learning (CoNLL),</booktitle>
<pages>149--164</pages>
<contexts>
<context position="1285" citStr="Buchholz and Marsi, 2006" startWordPosition="179" endWordPosition="182">rcs, siblings, grandparents and co-parents). For decoding this second-order model, we solve a linear relaxation of that problem using alternating directions dual decomposition (AD3). The experiments have shown that, even though the parser’s performance in Chinese and Czech attains around 80% (not too far from English performance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research. 1 Introduction The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (Buchholz and Marsi, 2006; Nivre et al., 2007) and, more recently, SPMRL 2013-14 (Seddah et al., 2013; Seddah et al., 2014). As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year</context>
</contexts>
<marker>Buchholz, Marsi, 2006</marker>
<rawString>Sabine Buchholz and Erwin Marsi. 2006. CoNLLX shared task on multilingual dependency parsing. In Proc. Int. Conf. on Natural Language Learning (CoNLL), pages 149–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Ofer Dekel</author>
<author>Joseph Keshet</author>
<author>Shai ShalevShwartz</author>
<author>Yoram Singer</author>
</authors>
<title>Online passiveaggressive algorithms.</title>
<date>2006</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>7--551</pages>
<contexts>
<context position="6169" citStr="Crammer et al., 2006" startWordPosition="965" endWordPosition="968">emantic parser of Johansson and Nugues (2008). To tackle all the parts, we formulate parsing as a global optimization problem and solve a relaxation through AD3 (Martins et al., 2011), a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively. Through a rich set of features, we arrive at top accuracies at parsing speeds around 1,000 tokens per second. See Martins and Almeida (2014) for details on the model, features and decoding process that were used. 3 Experimental Results All models were trained by running 10 epochs of max-loss MIRA with C = 0.01 (Crammer et al., 2006). The cost function takes into account mismatches between predicted and gold dependencies, with a cost cP on labeled arcs incorrectly predicted (false positives) and a cost cR on gold labeled arcs that were missed (false negatives). These values were set through cross-validation in the dev set, yielding cP = 0.4 and cR = 0.6 in all runs, except for the English PSD dataset in the closed track, for which cP = 0.3 and cR = 0.7. As in the previous work, we speed up decoding by training a probabilistic unlabeled first-order pruner and discarding the arcs whose posterior probability is below 10−4. T</context>
</contexts>
<marker>Crammer, Dekel, Keshet, ShalevShwartz, Singer, 2006</marker>
<rawString>Koby Crammer, Ofer Dekel, Joseph Keshet, Shai ShalevShwartz, and Yoram Singer. 2006. Online passiveaggressive algorithms. Journal of Machine Learning Research, 7:551–585.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dipanjan Das</author>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
</authors>
<title>An exact dual decomposition algorithm for shallow semantic parsing with constraints.</title>
<date>2012</date>
<booktitle>In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM</booktitle>
<pages>209--217</pages>
<contexts>
<context position="4482" citStr="Das et al., 2012" startWordPosition="694" endWordPosition="697">e. Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010, 2013), with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. The overall set of parts used by our parser is illustrated in Figure 1; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating</context>
</contexts>
<marker>Das, Martins, Smith, 2012</marker>
<rawString>Dipanjan Das, Andr´e F. T. Martins, and Noah A. Smith. 2012. An exact dual decomposition algorithm for shallow semantic parsing with constraints. In Proc. of First Joint Conference on Lexical and Computational Semantics (*SEM 2012), pages 209–217.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hall Daum´e</author>
</authors>
<title>Frustratingly easy domain adaptation.</title>
<date>2007</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>256--263</pages>
<marker>Daum´e, 2007</marker>
<rawString>Hall Daum´e III. 2007. Frustratingly easy domain adaptation. In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL), pages 256–263.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey Flanigan</author>
<author>Sam Thomson</author>
<author>Jaime Carbonell</author>
<author>Chris Dyer</author>
<author>Noah A Smith</author>
</authors>
<title>A discriminative graph-based parser for the abstract meaning representation.</title>
<date>2014</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>1426--1436</pages>
<contexts>
<context position="4506" citStr="Flanigan et al., 2014" startWordPosition="698" endWordPosition="701">r model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010, 2013), with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. The overall set of parts used by our parser is illustrated in Figure 1; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among </context>
</contexts>
<marker>Flanigan, Thomson, Carbonell, Dyer, Smith, 2014</marker>
<rawString>Jeffrey Flanigan, Sam Thomson, Jaime Carbonell, Chris Dyer, and Noah A. Smith. 2014. A discriminative graph-based parser for the abstract meaning representation. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 1426–1436.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<date>2008</date>
<booktitle>Dependency-based syntactic–semantic analysis with PropBank and NomBank. Int. Conf. on Natural Language Learning (CoNLL),</booktitle>
<pages>183--187</pages>
<contexts>
<context position="4464" citStr="Johansson and Nugues, 2008" startWordPosition="690" endWordPosition="693"> labeled with a semantic role. Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010, 2013), with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. The overall set of parts used by our parser is illustrated in Figure 1; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the </context>
</contexts>
<marker>Johansson, Nugues, 2008</marker>
<rawString>Richard Johansson and Pierre Nugues. 2008. Dependency-based syntactic–semantic analysis with PropBank and NomBank. Int. Conf. on Natural Language Learning (CoNLL), pages 183–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Michael Collins</author>
</authors>
<title>Efficient thirdorder dependency parsers.</title>
<date>2010</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1--11</pages>
<contexts>
<context position="1604" citStr="Koo and Collins, 2010" startWordPosition="236" endWordPosition="239">formance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research. 1 Introduction The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (Buchholz and Marsi, 2006; Nivre et al., 2007) and, more recently, SPMRL 2013-14 (Seddah et al., 2013; Seddah et al., 2014). As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014). This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data? Our proposed parser (§2) is essentially the same that we submitted in the previous year to the same SemEval task (Mart</context>
</contexts>
<marker>Koo, Collins, 2010</marker>
<rawString>Terry Koo and Michael Collins. 2010. Efficient thirdorder dependency parsers. In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL), pages 1–11.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Mariana S C Almeida</author>
</authors>
<title>Priberam: A turbo semantic parser with second order features.</title>
<date>2014</date>
<booktitle>In Proc. of the 8th Int. Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>471--476</pages>
<contexts>
<context position="2226" citStr="Martins and Almeida, 2014" startWordPosition="336" endWordPosition="339">2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014). This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data? Our proposed parser (§2) is essentially the same that we submitted in the previous year to the same SemEval task (Martins and Almeida, 2014), where we scored top in the open challenge and second in the closed track. This year, we report results using new out-of-domain and multilingual data (namely, Czech and Chinese, in addition to English). For the English language, we participated in the closed and open tracks, using as additional resources the syntactic dependency annotations provided by the organizers. For Czech and Chinese, we only addressed the closed track, since no companion data were provided for these languages. We did not participate in the gold track that uses gold-standard syntactic annotations; and we did not address</context>
<context position="4098" citStr="Martins and Almeida (2014)" startWordPosition="627" endWordPosition="630">nal Workshop on Semantic Evaluation (SemEval 2015), pages 970–973, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics Figure 1: Parts considered by our semantic parser. The top row illustrate the basic parts, representing the event that a word is a predicate, or the existence of an arc between a predicate and an argument, eventually labeled with a semantic role. Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010, 2013), with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent st</context>
<context position="5975" citStr="Martins and Almeida (2014)" startWordPosition="930" endWordPosition="934">actic dependency relations of words related to the corresponding predicates and arguments. Most of these features were taken from TurboParser (Martins et al., 2013), and others were inspired by the semantic parser of Johansson and Nugues (2008). To tackle all the parts, we formulate parsing as a global optimization problem and solve a relaxation through AD3 (Martins et al., 2011), a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively. Through a rich set of features, we arrive at top accuracies at parsing speeds around 1,000 tokens per second. See Martins and Almeida (2014) for details on the model, features and decoding process that were used. 3 Experimental Results All models were trained by running 10 epochs of max-loss MIRA with C = 0.01 (Crammer et al., 2006). The cost function takes into account mismatches between predicted and gold dependencies, with a cost cP on labeled arcs incorrectly predicted (false positives) and a cost cR on gold labeled arcs that were missed (false negatives). These values were set through cross-validation in the dev set, yielding cP = 0.4 and cR = 0.6 in all runs, except for the English PSD dataset in the closed track, for which </context>
</contexts>
<marker>Martins, Almeida, 2014</marker>
<rawString>Andr´e F. T. Martins and Mariana S. C. Almeida. 2014. Priberam: A turbo semantic parser with second order features. In Proc. of the 8th Int. Workshop on Semantic Evaluation (SemEval 2014), pages 471–476.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
</authors>
<title>Concise integer linear programming formulations for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>342--350</pages>
<contexts>
<context position="1650" citStr="Martins et al., 2009" startWordPosition="244" endWordPosition="247">gesting domain adaptation as an interesting avenue for future research. 1 Introduction The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (Buchholz and Marsi, 2006; Nivre et al., 2007) and, more recently, SPMRL 2013-14 (Seddah et al., 2013; Seddah et al., 2014). As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014). This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data? Our proposed parser (§2) is essentially the same that we submitted in the previous year to the same SemEval task (Martins and Almeida, 2014), where we scored top in</context>
</contexts>
<marker>Martins, Smith, Xing, 2009</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, and Eric P. Xing. 2009. Concise integer linear programming formulations for dependency parsing. In Proc. of Annual Meeting of the Association for Computational Linguistics (ACL), pages 342–350.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Eric P Xing</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Turbo parsers: Dependency parsing by approximate variational inference.</title>
<date>2010</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>34--44</pages>
<contexts>
<context position="4221" citStr="Martins et al., 2010" startWordPosition="648" endWordPosition="651">tational Linguistics Figure 1: Parts considered by our semantic parser. The top row illustrate the basic parts, representing the event that a word is a predicate, or the existence of an arc between a predicate and an argument, eventually labeled with a semantic role. Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010, 2013), with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. The overall set of parts used by our parser is illustrated in Figur</context>
</contexts>
<marker>Martins, Smith, Xing, Aguiar, Figueiredo, 2010</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Eric P. Xing, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2010. Turbo parsers: Dependency parsing by approximate variational inference. In Proc. of Empirical Methods for Natural Language Processing (EMNLP), pages 34–44.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Noah A Smith</author>
<author>Pedro M Q Aguiar</author>
<author>M´ario A T Figueiredo</author>
</authors>
<title>Dual decomposition with many overlapping components.</title>
<date>2011</date>
<booktitle>In Proc. of Empirical Methods for Natural Language Processing (EMNLP),</booktitle>
<pages>238--249</pages>
<contexts>
<context position="5731" citStr="Martins et al., 2011" startWordPosition="891" endWordPosition="894">icates; we will analyze the effect of these dependencies in the experimental section (§3). For each part in our model (shown in Figure 1), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments. Most of these features were taken from TurboParser (Martins et al., 2013), and others were inspired by the semantic parser of Johansson and Nugues (2008). To tackle all the parts, we formulate parsing as a global optimization problem and solve a relaxation through AD3 (Martins et al., 2011), a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively. Through a rich set of features, we arrive at top accuracies at parsing speeds around 1,000 tokens per second. See Martins and Almeida (2014) for details on the model, features and decoding process that were used. 3 Experimental Results All models were trained by running 10 epochs of max-loss MIRA with C = 0.01 (Crammer et al., 2006). The cost function takes into account mismatches between predicted and gold dependencies, with a cost cP on labeled arcs incorrectly predicted (false positives) </context>
</contexts>
<marker>Martins, Smith, Aguiar, Figueiredo, 2011</marker>
<rawString>Andr´e F. T. Martins, Noah A. Smith, Pedro M. Q. Aguiar, and M´ario A. T. Figueiredo. 2011. Dual decomposition with many overlapping components. In Proc. of Empirical Methods for Natural Language Processing (EMNLP), pages 238–249.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andr´e F T Martins</author>
<author>Miguel B Almeida</author>
<author>Noah A Smith</author>
</authors>
<title>Turning on the turbo: Fast third-order non-projective turbo parsers.</title>
<date>2013</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>617--622</pages>
<contexts>
<context position="5513" citStr="Martins et al., 2013" startWordPosition="854" endWordPosition="858">eled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and grandparent parts that have the effect of creating inter-dependence among predicates; we will analyze the effect of these dependencies in the experimental section (§3). For each part in our model (shown in Figure 1), we computed binary features based on various combination of lexical forms, lemmas, POS tags and syntactic dependency relations of words related to the corresponding predicates and arguments. Most of these features were taken from TurboParser (Martins et al., 2013), and others were inspired by the semantic parser of Johansson and Nugues (2008). To tackle all the parts, we formulate parsing as a global optimization problem and solve a relaxation through AD3 (Martins et al., 2011), a fast dual decomposition algorithm in which several simple local subproblems are solved iteratively. Through a rich set of features, we arrive at top accuracies at parsing speeds around 1,000 tokens per second. See Martins and Almeida (2014) for details on the model, features and decoding process that were used. 3 Experimental Results All models were trained by running 10 epoc</context>
</contexts>
<marker>Martins, Almeida, Smith, 2013</marker>
<rawString>Andr´e F. T. Martins, Miguel B. Almeida, and Noah A. Smith. 2013. Turning on the turbo: Fast third-order non-projective turbo parsers. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 617–622.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a twostage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proc. of Int. Conf. on Natural Language Learning (CoNLL),</booktitle>
<pages>216--220</pages>
<contexts>
<context position="1581" citStr="McDonald et al., 2006" startWordPosition="231" endWordPosition="235">oo far from English performance), domain shift is a serious issue, suggesting domain adaptation as an interesting avenue for future research. 1 Introduction The last years have witnessed a continuous progress in statistical multilingual models for syntax, thanks to shared tasks such as CoNLL 2006-7 (Buchholz and Marsi, 2006; Nivre et al., 2007) and, more recently, SPMRL 2013-14 (Seddah et al., 2013; Seddah et al., 2014). As a global trend, we observe that models that incorporate rich global features are typically more accurate, even if pruning is necessary or decoding needs to be approximate (McDonald et al., 2006; Koo and Collins, 2010; Bohnet and Nivre, 2012; Martins et al., 2009, 2013). The same rationale applies to semantic dependency parsing, also a structured prediction problem, but where the output variable is a semantic graph, rather than a syntactic tree. Indeed, the best performing systems in last year shared task on broad-coverage semantic dependency parsing follow this principle (Oepen et al., 2014). This year, a new challenge was put forth: how to handle multiple languages and out-ofdomain data? Our proposed parser (§2) is essentially the same that we submitted in the previous year to the </context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>Ryan McDonald, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a twostage discriminative parser. In Proc. of Int. Conf. on Natural Language Learning (CoNLL), pages 216–220.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Johan Hall</author>
<author>Sandra K¨ubler</author>
<author>Ryan McDonald</author>
<author>Jens Nilsson</author>
<author>Sebastian Riedel</author>
<author>Deniz Yuret</author>
</authors>
<title>The CoNLL</title>
<date>2007</date>
<booktitle>In Proc. of the CoNLL Shared Task Session of Empirical Methods for Natural Language Processing,</booktitle>
<volume>7</volume>
<pages>915--932</pages>
<marker>Nivre, Hall, K¨ubler, McDonald, Nilsson, Riedel, Yuret, 2007</marker>
<rawString>Joakim Nivre, Johan Hall, Sandra K¨ubler, Ryan McDonald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret. 2007. The CoNLL 2007 shared task on dependency parsing. In Proc. of the CoNLL Shared Task Session of Empirical Methods for Natural Language Processing, volume 7, pages 915–932.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Marco Kuhlmann</author>
<author>Yusuke Miyao</author>
<author>Daniel Zeman</author>
<author>Dan Flickinger</author>
<author>Jan Hajiˇc</author>
<author>Angelina Ivanova</author>
<author>Yi Zhang</author>
</authors>
<title>task 8: broad-coverage semantic dependency parsing.</title>
<date>2014</date>
<booktitle>In Proc. of the 8th Int. Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>63--72</pages>
<note>Semeval</note>
<marker>Oepen, Kuhlmann, Miyao, Zeman, Flickinger, Hajiˇc, Ivanova, Zhang, 2014</marker>
<rawString>Stephan Oepen, Marco Kuhlmann, Yusuke Miyao, Daniel Zeman, Dan Flickinger, Jan Hajiˇc, Angelina Ivanova, and Yi Zhang. 2014. Semeval 2014 task 8: broad-coverage semantic dependency parsing. In Proc. of the 8th Int. Workshop on Semantic Evaluation (SemEval 2014), pages 63–72.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
<author>Reut Tsarfaty</author>
<author>Sandra K¨ubler</author>
<author>Marie Candito</author>
<author>Jinho D Choi</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
<author>Iakes Goenaga</author>
<author>Koldo Gojenola</author>
<author>Yoav Goldberg</author>
</authors>
<title>Overview of the SPMRL</title>
<date>2013</date>
<booktitle>In Proc. of the 4th Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL</booktitle>
<pages>146--182</pages>
<marker>Seddah, Tsarfaty, K¨ubler, Candito, Choi, Farkas, Foster, Goenaga, Gojenola, Goldberg, 2013</marker>
<rawString>Djam´e Seddah, Reut Tsarfaty, Sandra K¨ubler, Marie Candito, Jinho D. Choi, Rich´ard Farkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, et al. 2013. Overview of the SPMRL 2013 shared task: cross-framework evaluation of parsing morphologically rich languages. In Proc. of the 4th Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2013), pages 146–182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
<author>Sandra K¨ubler</author>
<author>Reut Tsarfaty</author>
</authors>
<title>Introducing the SPMRL 2014 shared task on parsing morphologically-rich languages.</title>
<date>2014</date>
<booktitle>In Proc. of the 5th Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2014),</booktitle>
<pages>23--29</pages>
<marker>Seddah, K¨ubler, Tsarfaty, 2014</marker>
<rawString>Djam´e Seddah, Sandra K¨ubler, and Reut Tsarfaty. 2014. Introducing the SPMRL 2014 shared task on parsing morphologically-rich languages. In Proc. of the 5th Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2014), pages 23–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Aria Haghighi</author>
<author>Christopher D Manning</author>
</authors>
<title>Joint learning improves semantic role labeling.</title>
<date>2005</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>589--596</pages>
<contexts>
<context position="4436" citStr="Toutanova et al., 2005" startWordPosition="686" endWordPosition="689"> an argument, eventually labeled with a semantic role. Our second-order model looks at some pairs of arcs: arcs bearing a grandparent relationship, arguments of the same predicate, predicates sharing the same argument, and consecutive versions of these two. Martins and Almeida (2014) for further details. The parser was built as an extension of a recent dependency parser, TurboParser (Martins et al., 2010, 2013), with the goal of performing semantic parsing using any of the three formalisms considered in the shared task (DM, PAS, and PSD). We have followed prior work in semantic role labeling (Toutanova et al., 2005; Johansson and Nugues, 2008; Das et al., 2012; Flanigan et al., 2014), by adding constraints and modeling interactions among arguments within the same frame; however, we went beyond such sibling interactions to consider more complex grandparent and co-parent structures, effectively correlating different predicates. The overall set of parts used by our parser is illustrated in Figure 1; note that by using only a subset of the parts (predicate, arc, labeled arc, and sibling parts), the semantic parser decodes each predicate frame independently from other predicates; it is the co-parent and gran</context>
</contexts>
<marker>Toutanova, Haghighi, Manning, 2005</marker>
<rawString>Kristina Toutanova, Aria Haghighi, and Christopher D. Manning. 2005. Joint learning improves semantic role labeling. In Proc. of the Annual Meeting of the Association for Computational Linguistics (ACL), pages 589–596.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>