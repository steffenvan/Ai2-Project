<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006553">
<title confidence="0.9843235">
Corpus-based Adaptation Mechanisms
for C&apos;hinese Homophone Disambiguation
</title>
<author confidence="0.798362">
Chao-Huang Chang
</author>
<address confidence="0.8043015">
E000/CCL, Building 11, Industrial Technology Research Institute
Chutung, Hsinchu 31015, Taiwan, R.O.C.
</address>
<email confidence="0.969048">
E-mail: changch4le0sun3 .ccl . itrl.org .tw
</email>
<sectionHeader confidence="0.995756" genericHeader="abstract">
Abstract
</sectionHeader>
<construct confidence="0.989460466666667">
Based on the concepts of bidirectional conversion
and automatic evaluation, we propose two user-
adaptation mechanisms, character-preference learn-
ing and pseudo-word learning, for resolving Chinese
homophone ambiguities in syllable-to-character con-
version. The 1991 United Daily corpus of approxi-
mately 10 million Chinese characters is used for ex-
traction of 10 reporter-specific article databases and
for computation of word frequencies and character hi-
grams. Experiments show that 20.5 percent (testing
sets) to 71.8 percent (training sets) of conversion er-
rors can be eliminated through the proposed mecha-
nisms. These concepts are thus very useful in ap-
plications such as Chinese input methods and speech
recognition systems.
</construct>
<sectionHeader confidence="0.998732" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996975446428571">
Corpus-based Chinese NLP research has been very
active in the recent years as more and more computer
readable Chinese corpora are available. Reported
corpus-based NLP applications [10] include machine
translation, word segmentation, character recogni-
tion, text classification, lexicography, and spelling
checker. In this paper, we will describe our work on
adaptive Chinese homophone disambiguation (also
known as phonetic-input-to-character conversion or
phonetic decoding) using pan of the 1991 United
Daily (UD) corpus of approximately 10 million Chi-
nese characters (Hanzi).
It requires a coding method, structural or phonetic,
to input Chinese characters into a computer, since
there are more than 10,009 of them in common use
In the literature [3,7], there are several hundred dif-
ferent coding methods for this purpose. For most
users, phonetic coding (Pinyin or Bopomofo) is the
choice. To input a Chinese character, the user sim-
ply keys in its corresponding phonetic code. It is
easy to learn, but suffers from the homophone prob-
lem, i.e., a phonetic code corresponding to several
different characters. Therefore, the user needs to
choose the desired character from a (usually long)
list of candidate characters. It is inefficient and an-
noying. So, automatic homophone disambiguation is
highly desirable. Several disambiguation approaches
have been reported in the literature [3,7]. Some of
them have even been realized in commercial input
methods, e.g., Hanin, WangXing, Going. However,
the accuracies of these disambiguators are not sat-
isfactory. In this paper, we propose a corpus-based
adaptation method for improving the accuracy of ho-
mophone disambiguation.
For homophone disambiguation, what we need as
input is syllable (phonetic code) corpora instead of
text corpora. For adaptation, what we need is per-
sonal corpora instead of general corpora (such as
the UD carpus). Thus, we first design a selec-
tion procedure to extract articles by individual re-
porters. Ten personal corpora were set up in this
way. An additional domain-specific corpus, trans-
lated AP news, was built up similarly. Then, we
design a highly-reliable (99.7% correct) character-to-
syllable converter [1] to transfer the text corpora into
syllable corpora.
Our baseline disambiguator is rather conventional,
composed of a word-lattice searching module, a path
scorer, and a lexicon-driven word hypothesizer. Us-
ing the original text corpora and the correspond-
ing syllable corpora, we propose a user-adaptation
method, applying the concept of bidirectional con-
version [1] and automatic evaluation [2]. The adapta-
tion method includes two parts: character-preference
learning and pseudo word learning. Given a per-
sonal corpus (i.e., sample text), the adaptation pro-
</bodyText>
<page confidence="0.998913">
94
</page>
<bodyText confidence="0.999921105263158">
cedure is able to produce a user-specific character-
preference model and a pseudo word lexicon auto-
matically. Then the system can use the user-specific
parameters in the two models for improving the con-
version accuracy.
Extensive experiments have been conducted for (1)
ten sets of local-news articles (one set per reporter)
and (2) translated international news from AP News.
Each set is divided into two subsets: one for train-
ing, the other for testing. The character accuracy of
the baseline version is 93.46% on average. With the
proposed adaptation method, the augmented version
increases the accuracy to 98.16% for the training sets
and to 94.80% for the test sets. In other words, 71.8%
and 20.5% of the errors have been eliminated, respec-
tively. The results are encouraging for us to further
pursue corpus-based adaptive learning methods for
Chinese phonetic input and language modeling for
speech recognition.
</bodyText>
<sectionHeader confidence="0.984252" genericHeader="introduction">
2 Homophone Disambiguation
</sectionHeader>
<bodyText confidence="0.99979776923077">
Mandarin Chinese has approximately 1300 syllables,
13,051 commonly used characters, and more than
100,000 words. Each character is pronounced as a
syllable. Thus, it. is clear that there are many sylla-
bles are shared by numbers of characters. Actually,
some syllables correspond to more than 100 charac-
ters, e.g.. the syllable [y14] corresponds to 125 char-
acters, E, gE, ?);, etc. Thus, homophone
(character) disambiguation is difficult but important
in Chinese phonetic input methods and speech recog-
nition systems.
The problem of homophone disambiguation can be
defined as how to convert a sequence of syllables S =
si,s2, s,t (usually a sentence or a clause) into a cor-
responding sequence of characters C =
correctly. Here, each s, stands for one of the 1300
Chinese syllables and each c, one of the 13,051 char-
acters
Fortunately, when the characters are grouped into
words (the smallest meaningful unit), the homophone
problem is lessened. The number of homophone poly-
syllables is much less than that of homophone char-
acters. (A Chinese word is usually composed of 1 to
4 characters.) For the disambiguation, longer words
are usually correct and preferred. Thus, the ho-
mophone disambiguation problem is usually formu-
lated as a word-lattice optimal path finding prob-
lem. (Note that there is the problem of unknown
words, especially personal names, compound words,
and acronyms, which are not registered in the lexi-
con.)
For example, a sequence of three syllables sl, s2,
s3 involves six possible subsequences sl, s2, s3, sl-s2,
s2-s3, sl-s2-s3, which can correspond to some words.
Each subsequence could correspond to more than one
word, especially in the case of monosyllables. Accord-
ingly, a word lattice is formed by the words with one
of the six subsequences as pronunciation. See Figure
1 for a sample word lattice.
Note that syllables are chosen as input units
instead of word-sized units used in systems like
Tiankla. The major reason is Chinese is a mono-
syllabic language; characters/syllables are the most
natural units, while &amp;quot;words&amp;quot; are not well-defined in
Chinese. It is difficult for people to segment the words
correctly and consistently, especially according to the
dictionary provided by the system. This is also the
reason why newer intelligent Chinese input methods
in Taiwan like Hanin, WangXing, and Going, all use
syllables (for a sentence) as input units. In addi-
tion, our target system is an isolated-syllables speech
recognition system.
</bodyText>
<sectionHeader confidence="0.931402" genericHeader="method">
3 The Baseline System
</sectionHeader>
<bodyText confidence="0.999418">
The proposed system (Figure 2) is composed of
a baseline system plus two new features: character-
preference learning (CPL) and pseudo word learning
(PWL).
The baseline syllable-to-character converter con-
sists of three components: (1) a word hypothesizer,
(2) a word-lattice search algorithm, and (3) a score
function. The basic model used in our system is:
(-1) a Viterbi search algorithm, (2) a lexicon-based
word hypothesizer, and (3) a score function consider-
ing word length and word frequency.
The word hypothesizer matches the current input
syllable candidates with the lexical entries in the
lexicon (7,953 1-character words, 25,567 2-character,
12,216 3-character, 12,419 4-character, 55,155 words
totally). All matched words are proposed as word
hypotheses forming the word lattice. Currently, we
consider only those words with at most four syllables
(only less than 0.1% or words contain five or more
syllables). In addition, Determinative-Measure (DM)
</bodyText>
<page confidence="0.979613">
95
</page>
<figure confidence="0.832762">
[jing31 [fangl [huai2] [yi2) [quan2] [arvl] [you3] [yin3] [qing2]
</figure>
<figureCaption confidence="0.995843">
Figure 1. A Sample Word Lattice (*: correct words, • more monosyllabic words)
</figureCaption>
<figure confidence="0.992838375">
Chinese
Lexicon
Chinese
Syllables
Adaptive Word Lattice Search Algorithm
Pseudo Word
Lexicon
Word
Hypothesizer
Score
Function
Character
Preference
Table
Chinese
Characters
</figure>
<figureCaption confidence="0.999381">
Figure 2: The Overall Structure
</figureCaption>
<page confidence="0.932391">
96
</page>
<bodyText confidence="0.99875025">
compounds are proposed dynamically, i.e., not stored
in the lexicon.
Viterbi search is a well-known algorithm for op-
timal path-finding problems. The word lattice for a
whole clause (delimited by a punctuation) is searched
using the dynamic-programming-style Viterbi algo-
rithm.
The score function is defined as follows: If a path P
is composed of n words , w„ and two assumed
clause delimiters tve and the path score for P is
the sum of word scores for the n words and inter-word
link scores for the n+1 word links (n-1 between-word
links and 2 boundary links).
Homophone disambiguation can be considered as a
process of syllable-to-character (S2C) conversion. Its
reverse process, character-to-syllable (C2S) conver-
sion, is also nontrivial. There are more than 1000
characters, so-called Poyinzi (homographs), with
multiple pronunciations. However, a high-accuracy
C2S converter is achievable. Using an n-gram looka-
head scheme, we have designed such a converter with
99.71% accuracy. Because of the high accuracy, the
C2S converter can be used to convert a text corpus
to a syllable corpus automatically. The two processes
together form a bidirectional conversion model. The
point is: If we ignore the 0.29% error (could be re-
duced if a better C2S system is used), many applica-
tions of the model appear.
pa! hscore( P) 11 We have applied the bidirectional model to auto-
Ewortiscore(w,) matic evaluation of language models for speech recog-
:= nition. A more straightforward application is auto-
Eiinkscore(„„, matic evaluation of the S2C converter. A text is con-
rz.10 verted into a syllable sequence, which then is con-
verted back to an output text. Comparing the input
text with the output, we can compute the accuracy
of the S2C converter automatically.
The word score of a word is based on the word fre-
quency statistics computed by counting the number
of occurrences the word appears in the 10-million-
character UD corpus. The word frequency is mapped
into an integral score by taking its logarithm value
and truncating the value to an integer. Lee el a/. &apos;11]
recently presented a novel idea called word-lattice-
based Chinese character bigrani for Chinese language
modeling. Basically, they approximate the effect of
word bigrams by applying character bigranns to the
boundary characters of adjacent words. The ap-
proach is simple (easy to implement) and very ef-
fective. Following the idea, we built a Chinese char-
acter bigram based on the UD corpus and used it
to compute inter-word link scores. For two adjacent
words, the last character of the first word and the
first character of the second word are used to consult
the character bigram which recorded the number of
occurrences in the UD corpus. Inter-word link scores
are then computed similarly to word scores.
</bodyText>
<sectionHeader confidence="0.9383695" genericHeader="method">
4 Bidirectional Conversion
and Automatic Evaluation
</sectionHeader>
<bodyText confidence="0.999950666666667">
Here, we will only briefly review the concepts of bidi-
rectional conversion and automatic evaluation [1,2].
For more details, see the cited papers.
</bodyText>
<sectionHeader confidence="0.986806" genericHeader="method">
5 Corpus-based Adaptation
Mechanisms
</sectionHeader>
<bodyText confidence="0.998751">
In the following, we describe how to apply the model
to user-adaptation of homophone disambiguator.
</bodyText>
<subsectionHeader confidence="0.981327">
5.1 Character-Preference Learning
</subsectionHeader>
<bodyText confidence="0.999885">
Everyone has his own preference for characters and
words. A chemist might use the special characters for
chemical elements frequently. Different people uses
a different set of proper names that are usually not
stored in the lexicon. In this section, we propose an
adaptation method based on the bidirectional conver-
sion model.
From a sample text given by the user, the system
first converts it to a sequence of syllables. Then, the
baseline system is used to convert them back to Chi-
nese characters. After that, we can compare them
with the input to obtain the error records. From
the comparison report, we will derive three indices
for each character in the character set (say, 13,051
characters in the Big-5 coding used in Taiwan): A-
count, B-count, and C-count. A-count is cleaned as
</bodyText>
<page confidence="0.998049">
97
</page>
<bodyText confidence="0.979076733333333">
the number of times that the character is misrecog-
nized. B-count the number of times it is wrongly used.
while C-count the number of times it is correctly rec-
ognized. For example, if the user wants to input the
string AA and keys in the corresponding syllables
D13][zheril][zhen11 while the output is the in-
dices would be: A()=0, C(T)=1, A(A
)=2, B(X)=CI, C(A)=0, BM-Y=2, C(
)=O. From these indices, we propose a character-
preference learning procedure:
I. Convert the given sample text I, into a syllable
file I, using the character-to-syllable converter.
Let the baaeline version he Run If° with /, to
obtain an output 00. From 1 and 00, compute
the initial accuracy au.
</bodyText>
<listItem confidence="0.998824">
2. Initialize the 13051-entry character-preference
table CPT° to zeroes. Set n to 1.
3. From I, and On- I . compute the A. B. C indices
for each character.
4. For each character c, add to the corresponding
entry in CPT&amp;quot;&apos; a preference score (according
to a preference adjustment function pf of A(c),
B(c), (2(c)) to form CPT&apos;
5. Form a new version V&amp;quot; of the syllable-to-
character converter by considering CPT&amp;quot;. Run
V&amp;quot; with I, to obtain a new output On
6. From I, and 0&amp;quot;, compute the new accuracy rate
an.
7. If a&amp;quot; &gt; a&amp;quot;-1, set it to n + I and repeat steps 3-
6. Otherwise, stop and let CPT&apos; be the final
CPT for the user.
</listItem>
<subsectionHeader confidence="0.954762">
Adjustment Functions
</subsectionHeader>
<bodyText confidence="0.999131">
In step 4. the adjustment function pf is a function of
A(t), B(c), C(c). Several versions have been tried in
our experiments. Three of them are.
</bodyText>
<equation confidence="0.818472666666667">
{+1 if A(c)— B(c) + C(c)&gt; 0
0 otherwise (3)
Pf (1) is intuitive but easily suffers from over-
</equation>
<listItem confidence="0.662281857142857">
training since it only considers error cases. To avoid
the problem, we devise a new pf (2) taking the correct
cases into account. After trying several combinations
of A, B, C for pf, we observe that positive learning
(3) is most effective, i.e., achieving the highest accu-
racy. Therefore, in the current implementation, pf
(3) is used.
</listItem>
<subsectionHeader confidence="0.985025">
5.2 Pseudo Word Learning
</subsectionHeader>
<bodyText confidence="0.999765592592593">
The second adaptation mechanism is to treat error
N-grains as new words (called pseudo words). An er-
ror N-gram is defined as a sequence of N characters
in which at least (N - 1) characters are wrongly con-
verted (from syllables) by the system. (In practice,
2 &lt; N &lt; 4.) For example, if ifan4llzhen4][he21 (to
input maul) is converted to three pseudo
words are produced: Fala, lUll, and MERL There
are two modes for generating pseudo words: corpus
training and interactive correction. In the former,
the user-specific text corpus (or simply a sample text)
is used for generating the pseudo word lexicon (PW
lexicon), applying the concept of bidirectional con-
version. In the latter, pseudo words are produced
through user corrections in an interactive input pro-
cess. Both modes can be used at the same time.
In the following, we will describe how to build,
maintain, and use the user-specific PW lexicon. The
PW lexicon stores the 114 (lexicon size) pseudo words
that are produced or referenced in the most recent pe-
riod. It is structurally exactly the same as the general
lexicon, containing Hanzi, phonetic code, and word
frequency. The word frequency of a new PW is set to
Jo (3 in the implementation) and incremented by one
when referenced. Once the word frequency exceeds
an upper bound F, the PW would be considered as a
real word and no longer liable to replacement.
</bodyText>
<equation confidence="0.951447285714286">
/
+I if A(c) — B(c) &gt; 0
pf(c)= —1 if A(c)— B(c) &lt; 0
0 otherwise
{ +1 if A(c)— B(c)+ C(c) &gt;0
PRO= —1 if -.1(c) — 13(c)+C(c)&lt; o
0 otherwise
</equation>
<listItem confidence="0.744648166666667">
(1) The procedure is:
1. Segment the sample text into clauses (separated
by punctuations). For each clause I, do steps
2-4.
(2) 2. Convert the clause into syllable sequence I, us-
ing C2S, then convert I, back to a character se-
</listItem>
<page confidence="0.991961">
98
</page>
<bodyText confidence="0.72349">
quence 0, using baseline S2C. For each character
C. in Or. do steps 3-4.
</bodyText>
<listItem confidence="0.984490615384615">
3. Compare C. with the corresponding input char-
acter. Set the error flag if different.
4. If a pseudo word ending with C. is found (ac-
cording to error flags) then (1) increment the
word frequency if it is already in the PW lexi-
con, and check the upper bound F; (2) replace
the old entry and set frequency to Je if the lexi-
con has a homophone P1,1&apos;; (3) add a new entry
if the lexicon has vacancies; (4) otherwise, re-
move one of the entries that have the least word
frequency and add the new PW.
5. We have a new PW lexicon after the above steps
are done.
</listItem>
<bodyText confidence="0.999972157894737">
We observe that 3-character pseudo words are very
useful for dealing with the unregistered proper name
problem, which is a significant source of conversion
errors. The reasons are: (1) A large part of unknown
words in news articles are proper names, especially
three-character personal names; (2) It is not practi-
cal to store all the proper names beforehand; (3) The
proper names usually contain uncommon characters
which are difficult to convert from syllables. There-
fore, the user (or author) can have a personalized PW
lexicon which contains unregistered proper names he
will use, simply by providing a sample text.
The parameters for both CPL and PWL can be
trained by the bidirectional learning procedure. The
only input the user needs to provide is a sample text
sinilar to the texts lie wants to input by the phonetic-
input-to-character converter. The phonetic input file
will be automatically generated by the character-to-
syllable converter.
</bodyText>
<sectionHeader confidence="0.998649" genericHeader="evaluation">
6 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.999903">
6.1 The Corpora
</subsectionHeader>
<bodyText confidence="0.99913752173913">
Eleven sets of newspaper articles are extracted from
the 1991 United Daily News Corpus (kindly provided
by United Informatics. Inc.. Taiwan) Ten of them
are by specific reporters, i.e., one set per reporter.
The other is translated AP News. These corpora are
used to validate the proposed adaptation techniques.
We design an extraction procedure to select ar-
ticles written by a specific reporter from the cor-
pus with more than 10 million characters. First,
collect character-trigrams after the word gEg
(ji4zhe3,`reporter&apos;) and sort them according to the
number of occurrences. Most of these trigrams hap-
pen to be names of reporter. We use the top-10
names as the basis for selecting articles, Then, search
the names in the corpus in order to built the article
databases for the 10 reporters. The Al&apos; News corpus
is built in a similar way (searching for the word X
10?-± mei3lian2she4) Table 1 lists some statistics for
the article databases. The first column lists the set
names, the second column the numbers of articles in
the set, the third column the numbers of characters,
and the fourth column the numbers of pronounceable
characters.
</bodyText>
<table confidence="0.999650416666667">
Set #ArtIcles #Charl #Char2
Iwy 83 35,000 31,163
Ikq 52 22,424 20,214
hg 47 16,178 14,677
lxd 49 19,429 17,368
yft — 44 18,647 16,423
Yx1 45 14,791 13,132
fzh 45 17,154 15,299
tdc 44 15,323 13,638
Isq 44 . 18,804 17,150
tin 44 19,065 17,122
ap 408 122,554 109,218
</table>
<tableCaption confidence="0.999721">
Table 1: The Article Databases
</tableCaption>
<bodyText confidence="0.999743">
Each corpus is then divided into two parts accord-
ing to publication date: a training set and a test-
ing set. For example, the corpus lay is divided into
and lay-2.
In the following, we show the experimental results
for training sets and testing sets, respectively.
</bodyText>
<subsectionHeader confidence="0.999939">
6.2 Training Sets
</subsectionHeader>
<bodyText confidence="0.999932181818182">
Table 2 shows the adaptation results for the train-
ing sets. The RI column lists the accuracy rates for
the baseline system, while the R2 column lists those
for the adapted (or personalized) system. To avoid
the problem of over-training, we train the the system
only by two iterations in practice. More iterations
can improve the performance for training sets but
hurt the performance for testing sets. The average
character accuracy rate is improved by 4.68% (from
93.98% to 98.16%). That is, 71.8 percent of errors
are eliminated.
</bodyText>
<page confidence="0.994951">
99
</page>
<table confidence="0.999952">
Sel RI R2 R2-Rl Ratio
lwy-1 94.32 98.35 4.03 -70.9%
Ikq-1 93.07 97.85 4.76 68.9%
11g-1 94.19 98.42 4.23 72.8%
Ixd- 1 95.49 98.53 3.04 67.4%
yft-1 94.75 98.32 3.57 68.0%
yxj-1 91.83 98.23 6.40 78.3%
fzh-1 91.20 98.15 6.95 78.9%
tdc-1 92.13_ 97.71 5.58 70.9%
Isq- 1 93.63 98.23 4.60 72.2%
ljn- 1 93.64 98.09 4.45 69.9%
ap- 1 94.04 97.86 3.82 64.0%
Ave. 93.48 98.16 4.68 71.8%
</table>
<tableCaption confidence="0.997347">
Table 27 Accuracy Rates for Training Sets
</tableCaption>
<subsectionHeader confidence="0.999469">
6.3 Testing Sets
</subsectionHeader>
<bodyText confidence="0.99915875">
Table 3 shows the results for the testing sets. The
average accuracy is improved hy 1.34% (from 93.46%
to 94.80%) That is, 20.5 percent of errors are elimi-
nated.
</bodyText>
<table confidence="0.980315923076923">
Set RI Ri R2-RI Ratio
lwy-2 94.06 95.00 0.94 15.8%
Ikq-2 93.81 95.95 2.14 34.6%
11g-2 93.38 94.]8 0.80 12.1%
lxd-2 95,97 96.42 0.45 11.2%
yft-2 95.21 96.17 0.96 20.0%
yxj-2 91.67 93.93 2_26 27.1%
fzh-2 91.28 92.29 1.01 11.6%
tdc-2 92.25 93.58 1.33 17.2%
lsq-2 92.86 94.27 1.41 19.7%
ljn-2 93.66 95.52 1.86 29.2%
ap-2 93.93 95.54 1.61 26.5%
Ave. 93.46 94.80 1.34 20.5%
</table>
<tableCaption confidence="0.997525">
Table 3 Accuracy Rates for Testing Sets
</tableCaption>
<sectionHeader confidence="0.999933" genericHeader="related work">
7 Related Work
</sectionHeader>
<bodyText confidence="0.999972450980392">
The study of phonetic-input-to-character conversion
has been quite active in the recent years. There are
two different approaches for the problem: dictionary-
based and statistics-based.
Matsushita (Taipei) developed a Chinese word-
string input system. Hanin, as a new input method
(Chen 14]) in which phonetic symbols are continu-
ously converted to Chinese characters through dic-
tionary lookup. Commercial systems TianMa and
WangXing (ETen Corp.) also belong to this type. In
the mainland, there have been several groups involv-
ing in similar projects [14,15] although most of them
are pinyin-based and word-based.
In the statistics-based school are relaxation tech-
niques (Fan and Tsai [6] ), character bigrarns with
dynamic programming (Sproat [12]), constraint sat-
isfaction approaches (JS Chang [3]), and zero-order
or first-order Markov models (Gu et al. [7]).
Ni [91 mentioned a so-called self-learning capabil-
ity for his Chinese PC called LX-PC. However, the
method is (1) let the user define new words during
the input process (2) dynamically adjust the word
frequency of used words. Chen [4] also proposed a
learning function that uses a learning file to store
user-selected characters and words and the character
before them. The entries in the learning file are fa-
vored over those in the regular dictionary. Lua and
Can [8] describe a simple error-correcting mechanism:
increase the usage frequency of the desired word by
I unit when the user corrects the system&apos;s output.
These methods are either manual adaptation or sim-
ple word frequency counting.
Recently, Su et at. [5,13] proposed a discrimina-
tion oriented adaptive learning procedure for vari-
ous problems, e.g., speech recognition, part-of-speech
tagging, and word segmentation. The basic idea is:
When an error is made, i.e., the first candidate is not
correct, adjust the parameters in the score function
based on subspace projection. The parameters for
the correct candidate are increased, while those for
the first candidate are decreased, both in an amount
decided by the difference between the scores of the
two candidates. This process continues until the cor-
rect candidate becomes the new first candidate; that
is, the score of the correct candidate is greater than
that of the old first one. Our learning procedure is dif-
ferent from theirs because (1) ours is increment-based
while theirs is projection-based, (2) ours is not dis-
crimination oriented, (3) ours is coarse-grained learn-
ing while theirs is fine-grained, and (4) the applica-
tion domain is different.
</bodyText>
<sectionHeader confidence="0.982157" genericHeader="conclusions">
8 Concluding Remarks
</sectionHeader>
<bodyText confidence="0.999899666666667">
We have presented two corpus-based adaptation
mechanisms for Chinese homophone disambiguation:
character-preference learning and pseudo-word learn-
</bodyText>
<page confidence="0.974202">
100
</page>
<bodyText confidence="0.9997928">
ing. Experimental results show that the error rates
have been reduced significantly. This proves yet an-
other success of corpus-based NLP research
Future works include (1) more experiments using
various texts, (2) stud.), on more effective adjustment
functions for CPL, (3) study on weighting of differ-
ent lengths of pseudo words, (4) adaptation based on
other parameters, e.g., parts-of-speech, semantic cat-
egories, and (5) application to linguistic decoding for
speech recognition.
</bodyText>
<sectionHeader confidence="0.993865" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.994011">
The author is grateful to the Chinese Lexicon group
(CC1.11TRI) for the 90,000-word lexicon. This paper
is a partial result of the project no. 37112100 con-
duct ed by i he ITR I 11 nclor von,orslii p of lie Minister
of Economic Affairs, R.O.C.
</bodyText>
<sectionHeader confidence="0.998272" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999607934782609">
C.-H. Chang. Bidirectional conversion between
Mandarin syllables and Chinese characters. In
Proc. of 1997 international Conference on Com-
puter Processing of Chinese and Oriental Lan-
guages, Florida, USA, 1992
[6] C.-K. Fan and W.-H. Tsai. Relaxation-based
word identification for removing the ambiguity
in phonetic Chinese input. Int. J. of Pattern
Recognition and Artificial Intelligence, 4(4):651-
666,1990.
[7) • Cu, C.-Y. Tseng, and L.-S. Lee. Markov
modeling of Mandarin Chinese for decoding the
phonetic sequence into Chinese characters. Com-
puter Speech and Language, 5:363-377,1991.
[8] L.-S. Lee et al. Golden Mandarin (II) - an im-
proved single-chip real-time Mandarin dictation
machine for Chinese language with very large vo-
cabulary. In Proc. 1993 ICASSP, pages 11:503--
506, April 1993.
Pl K T. Lua and K.W. Can. A touch-typing Pinyin
input system. Computer Processing of Chinese
&amp; Oriental Languages, 6(1):85-94, June 1992.
[10] G. Ni. A Chinese PC features intelligent Hanzi
input. In Proc. 1986 Int. Conf. on Chinese Com-
puting, pages 155-159, August 1986.
El&apos;] ROC Computational Linguistics Society. Proc.
of Workshop on Corpus-based Researches and
Techniques for Natural Language Processing,
Taipei, Taiwan, September 1992.
[12) FL Sproat. An application of statistical optimiza-
tion with dynamic programming to phonetic-
input-to-character conversion for Chinese. In
Proc. ROCLING III, pages 379-390, September
1990.
[13] K.-*Y. Su and C.-H. Lee. Robustness and dis-
crimination oriented speech recognition using
weighted HMM and subspace projection ap-
proaches. In Proc. ICASSP91, pages 541-544,
Toronto, Ontario, Canada, May 1991.
[14] X. Wang, K. Wang, and X. Bai. Separating syl-
lables and characters into words in natural lan-
guage understanding. Journal of Chinese Infor-
mation Processing, 5(3).48-58, 1991.
[15] X. Zhong. A multiple phrase pinyin/Hanzi con-
version mechanism. Journal of Chinese Infor-
mation Processing, 4(2):55-64, 1990.
</reference>
<bodyText confidence="0.921254722222222">
[2) C.-H. Chang. Design and evaluation of language
models for Chinese speech recognition. in Proc.
of 199.2 CMEX Workshop on Chinese Speech
Recognition, Taipei, Taiwan, November 1992.
13) J.-S. Chang, S.-D. Chen, and C.-D. Chen.
Conversion of phonetic-input to Chinese text
through constriant satisfaction. In Proc. 1991
wrpro L , pages 30-36. Taipei. 1991.
[41 S.-1. Chen, C.-T. Chang, J.-J. Kuo, and M.-S.
Hsieh. The continuous conversion algorithm of
Chinese character&apos;s phonetic symbols to Chinese
character. In Proc. of National Computer Sym-
posium, pages 437-442, ]97.
[51 T.-11. Chiang, .1.-S. Chang, til.-Y. Lin, and K.-Y.
Su. Statistical niodels for word segmentation and
unknown word resolution In Pror, ROCL1NC
V. pages 121 146. Taipei. Taiwan. September
1991.
</bodyText>
<page confidence="0.998249">
101
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.477937">
<title confidence="0.9963685">Corpus-based Adaptation for C&apos;hinese Homophone Disambiguation</title>
<author confidence="0.557968">Chao-Huang</author>
<affiliation confidence="0.695092">E000/CCL, Building 11, Industrial Technology Research</affiliation>
<address confidence="0.97712">Chutung, Hsinchu 31015, Taiwan,</address>
<email confidence="0.956369">.ccl.itrl.org.tw</email>
<abstract confidence="0.999594125">Based on the concepts of bidirectional conversion and automatic evaluation, we propose two useradaptation mechanisms, character-preference learning and pseudo-word learning, for resolving Chinese homophone ambiguities in syllable-to-character conversion. The 1991 United Daily corpus of approximately 10 million Chinese characters is used for extraction of 10 reporter-specific article databases and for computation of word frequencies and character higrams. Experiments show that 20.5 percent (testing sets) to 71.8 percent (training sets) of conversion errors can be eliminated through the proposed mecha- These concepts are thus very useful applications such as Chinese input methods and speech recognition systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>C-H Chang</author>
</authors>
<title>Bidirectional conversion between Mandarin syllables and Chinese characters.</title>
<date>1992</date>
<booktitle>In Proc. of 1997 international Conference on Computer Processing of Chinese and Oriental Languages,</booktitle>
<location>Florida, USA,</location>
<marker>Chang, 1992</marker>
<rawString>C.-H. Chang. Bidirectional conversion between Mandarin syllables and Chinese characters. In Proc. of 1997 international Conference on Computer Processing of Chinese and Oriental Languages, Florida, USA, 1992</rawString>
</citation>
<citation valid="false">
<authors>
<author>C-K Fan</author>
<author>W-H Tsai</author>
</authors>
<title>Relaxation-based word identification for removing the ambiguity in phonetic Chinese input.</title>
<journal>Int. J. of Pattern Recognition and Artificial Intelligence,</journal>
<pages>4--4</pages>
<marker>Fan, Tsai, </marker>
<rawString>[6] C.-K. Fan and W.-H. Tsai. Relaxation-based word identification for removing the ambiguity in phonetic Chinese input. Int. J. of Pattern Recognition and Artificial Intelligence, 4(4):651-666,1990.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C-Y Tseng Cu</author>
<author>L-S Lee</author>
</authors>
<title>Markov modeling of Mandarin Chinese for decoding the phonetic sequence into Chinese characters. Computer Speech and Language,</title>
<pages>5--363</pages>
<marker>Cu, Lee, </marker>
<rawString>[7) • Cu, C.-Y. Tseng, and L.-S. Lee. Markov modeling of Mandarin Chinese for decoding the phonetic sequence into Chinese characters. Computer Speech and Language, 5:363-377,1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L-S Lee</author>
</authors>
<title>Golden Mandarin (II) - an improved single-chip real-time Mandarin dictation machine for Chinese language with very large vocabulary.</title>
<date>1993</date>
<booktitle>In Proc. 1993 ICASSP,</booktitle>
<pages>11--503</pages>
<marker>Lee, 1993</marker>
<rawString>[8] L.-S. Lee et al. Golden Mandarin (II) - an improved single-chip real-time Mandarin dictation machine for Chinese language with very large vocabulary. In Proc. 1993 ICASSP, pages 11:503--506, April 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pl K T Lua</author>
<author>K W Can</author>
</authors>
<title>A touch-typing Pinyin input system.</title>
<date>1992</date>
<booktitle>Computer Processing of Chinese &amp; Oriental Languages,</booktitle>
<pages>6--1</pages>
<marker>Lua, Can, 1992</marker>
<rawString>Pl K T. Lua and K.W. Can. A touch-typing Pinyin input system. Computer Processing of Chinese &amp; Oriental Languages, 6(1):85-94, June 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Ni</author>
</authors>
<title>A Chinese PC features intelligent Hanzi input.</title>
<date>1986</date>
<booktitle>In Proc. 1986 Int. Conf. on Chinese Computing,</booktitle>
<pages>155--159</pages>
<marker>Ni, 1986</marker>
<rawString>[10] G. Ni. A Chinese PC features intelligent Hanzi input. In Proc. 1986 Int. Conf. on Chinese Computing, pages 155-159, August 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>ROC</author>
</authors>
<title>Computational Linguistics Society.</title>
<date>1992</date>
<booktitle>Proc. of Workshop on Corpus-based Researches and Techniques for Natural Language Processing,</booktitle>
<location>Taipei, Taiwan,</location>
<marker>ROC, 1992</marker>
<rawString>El&apos;] ROC Computational Linguistics Society. Proc. of Workshop on Corpus-based Researches and Techniques for Natural Language Processing, Taipei, Taiwan, September 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>FL Sproat</author>
</authors>
<title>An application of statistical optimization with dynamic programming to phoneticinput-to-character conversion for Chinese.</title>
<date>1990</date>
<booktitle>In Proc. ROCLING III,</booktitle>
<pages>379--390</pages>
<marker>Sproat, 1990</marker>
<rawString>[12) FL Sproat. An application of statistical optimization with dynamic programming to phoneticinput-to-character conversion for Chinese. In Proc. ROCLING III, pages 379-390, September 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K-Y Su</author>
<author>C-H Lee</author>
</authors>
<title>Robustness and discrimination oriented speech recognition using weighted HMM and subspace projection approaches.</title>
<date>1991</date>
<booktitle>In Proc. ICASSP91,</booktitle>
<pages>541--544</pages>
<location>Toronto, Ontario, Canada,</location>
<marker>Su, Lee, 1991</marker>
<rawString>[13] K.-*Y. Su and C.-H. Lee. Robustness and discrimination oriented speech recognition using weighted HMM and subspace projection approaches. In Proc. ICASSP91, pages 541-544, Toronto, Ontario, Canada, May 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Wang</author>
<author>K Wang</author>
<author>X Bai</author>
</authors>
<title>Separating syllables and characters into words in natural language understanding.</title>
<date>1991</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>5--3</pages>
<marker>Wang, Wang, Bai, 1991</marker>
<rawString>[14] X. Wang, K. Wang, and X. Bai. Separating syllables and characters into words in natural language understanding. Journal of Chinese Information Processing, 5(3).48-58, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhong</author>
</authors>
<title>A multiple phrase pinyin/Hanzi conversion mechanism.</title>
<date>1990</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>4--2</pages>
<marker>Zhong, 1990</marker>
<rawString>[15] X. Zhong. A multiple phrase pinyin/Hanzi conversion mechanism. Journal of Chinese Information Processing, 4(2):55-64, 1990.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>