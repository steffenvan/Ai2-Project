<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000600">
<title confidence="0.503471">
DUAL-CODING THEORY AND CONNECTIONIST LEXICAL
SELECTION
</title>
<author confidence="0.977765">
Ye-Yi Wang*
</author>
<affiliation confidence="0.9649255">
Computational Linguistics Program
Carnegie Mellon University
</affiliation>
<address confidence="0.659144">
Pittsburgh, PA 15232
</address>
<email confidence="0.997635">
Internet: yyw@cs.cmu.edu
</email>
<sectionHeader confidence="0.99375" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999312">
We introduce the bilingual dual-coding theory as a
model for bilingual mental representation. Based on
this model, lexical selection neural networks are imple-
mented for a connectionist transfer project in machine
translation.
</bodyText>
<sectionHeader confidence="0.976361" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999939761904762">
Psycholinguistic knowledge would be greatly helpful,
as we believe, in constructing an artificial language
processing system. As for machine translation, we
should take advantage of our understandings of (1)
how the languages are represented in human mind; (2)
how the representation is mapped from one language
to another; (3) how the representation and mapping are
acquired by human.
The bilingual dual-coding theory (Paivio, 1986)
partially answers the above questions. It depicts the
verbal representations for two different languages as
two separate but connected logogen systems, charac-
terizes the translation process as the activation along
the connections between the logogen systems, and at-
tributes the acquisition of the representation to some
unspecified statistical processes.
We have explored an information theoretical neu-
ral network (Gorin and Levinson, 1989) that can ac-
quire the verbal associations in the dual-coding theory.
It provides a learnable lexical selection sub-system for
a connectionist transfer project in machine translation.
</bodyText>
<sectionHeader confidence="0.497123" genericHeader="method">
Dual-Coding Theory
</sectionHeader>
<bodyText confidence="0.999969375">
There is a well-known debate in psycholinguistics
concerning the bilingual mental representation: inde-
pendence position assumes that bilingual memory is
represented by two functionally independent storage
and retrieval systems, whereas interdependence po-
sition hypothesizes that all information of languages
exists in a common memory store. Studies on cross-
language transfer and cross-language priming have
</bodyText>
<footnote confidence="0.609586">
*This work was partly supported by ARPA and ATR In-
terpreting Telephony Research Laboratorie.
</footnote>
<bodyText confidence="0.998351">
provided evidence for both hypotheses (de Groot and
Nas, 1991; Lambert, 1958).
Dual-coding theory explains the coexistence of in-
dependent and interdependent phenomena with sepa-
rate but connected structures. The general dual-coding
theory hypothesizes that human represents language
with dual systems â€” the verbal system and the im-
agery system. The elements of the verbal system are
logogens for words in a language. The elements of
the imagery system, called &amp;quot;imagens&amp;quot;, are connected
to the logogens in the verbal systems via referential
connections. Logogens in a verbal system are also in-
terconnected with associative connections. The bilin-
gual dual-coding theory proposes an architecture in
which a common imagery system is connected to two
verbal systems, and the two verbal systems are inter-
connected to each other via associative connections
[Figure 1]. Unlike the within-language associations,
which are rich and diverse, these between-language
associations involve primarily translation equivalent
terms that are experienced together frequently. The
interconnections among the three systems explain the
interdependent functional behavior. On the other hand,
the different characteristics of within-language and
between-language associations account for the inde-
pendent functional behavior.
Based on the above structural assumption, dual-
coding theory proposes a parallel set of processing
assumptions. Activation of connections between ref-
erentially related imagens and logogens is called ref-
erential processing. Naming objects and imaging to
words are prototypical examples. Activation of asso-
ciative connections between logogens is called asso-
ciative processing. Lexical translation is an example
of associative processing between two languages.
</bodyText>
<subsectionHeader confidence="0.976414">
Connectionist Lexical Selection
Lexical Selection
</subsectionHeader>
<bodyText confidence="0.99978475">
Lexical selection is the task of choosing target lan-
guage words that accurately reflect the meaning of the
corresponding source language words. It plays an im-
portant role in machine translation (Pustejovsky and
</bodyText>
<page confidence="0.992992">
325
</page>
<figure confidence="0.728428">
Imagery System
</figure>
<figureCaption confidence="0.8244125">
Figure 1: Bilingual Dual-Coding Representation
Nirenburg, 1987).
</figureCaption>
<bodyText confidence="0.999093590909091">
A common lexical selection practice involves
an intermediate representation. It disambiguates the
source language words to entities in the intermediate
representation, then maps from the entities to the target
lexical entries. This intermediate representation may
be Lexical Concept Structure (Dorr, 1989) or inter-
lingua (Nirenberg, 1987). This engineering approach
requires great effort in designing the representation and
the mapping rules.
Currently, there are some efforts in statistical lex-
ical selection. A target language word Wt can be se-
lected with the posterior probability Pr(Wt I Ws) given
the source language word W. Several target language
lexical entries may be selected for a single source lan-
guage word. Then the correct selections can be iden-
tified by the language model of the target language
(Brown, 1990). This approach is learnable. However,
the accuracy is low. One reason is that it does not use
any structural information of a language.
In next subsections, we propose information-
theoretical networks based on the bilingual dual-coding
theory for lexical selection.
</bodyText>
<sectionHeader confidence="0.625533" genericHeader="method">
Information-Theoretical Networks
</sectionHeader>
<bodyText confidence="0.999940461538462">
Information-theoretical network is a neural network
formalism that is capable of doing associations be-
tween two layers of representations. The associations
can be obtained statistically according to the network&apos;s
experiences.
An information-theoretical network has two lay-
ers. Each unit of a layer represents an element in the
input or output of a training pattern, which might be a
logogen or a word. Units in different layers are con-
nected. The weight of the connection between unit i
in one layer and unit fin the other layer is assigned
with the mutual information between the elements rep-
resented by the two units
</bodyText>
<listItem confidence="0.4828526">
(1) wij = /(vi, vj) = log(Pr(vivi)1Pr(vi))1
Each layer also contains a bias unit, which is al-
ways activated. The weight of the connection between
the bias unit in one layer and unit j in the other layer is
(2) woj = log Pr(vi)
</listItem>
<bodyText confidence="0.999960357142857">
Both the information-theoretical network and the
back-propagation network compute the posterior prob-
abilities for an association task (Gorin and Levin-
son, 1989; Robinson, 1992). However, only the
information-theoretical network is isomorphic to the
directly interconnected verbal systems in the dual-
coding theory. Besides, an information-theoretical net-
work has the following advantages: (1) it learns fast.
The network can learn in a single pass without gra-
dient decent. (2) it is adaptive. It can incrementally
adapt to new experiences simply by adding new data
to the training samples and modifying the associations
according to the changed statistics. These make the
network more psychologically plausible.
</bodyText>
<subsectionHeader confidence="0.985729">
Lexical Selection as an Associative Process
</subsectionHeader>
<bodyText confidence="0.979244724137931">
We tried to map source language &amp;structures to target
language f-structure in a connectionist transfer project
(Wang, 1994). Functionally, there were two sub-tasks:
1. finding the target sub-structures, their phrasal cat-
egories and their corresponding source structures; 2.
finding the head of a target structure. The second sub-
task is a problem of lexical selection. It was first im-
plemented with a back-propagation network.
We replaced the back-propagation networks for
lexical selection with information-theoretical networks
simulating the associative process in the dual-coding
theory. The networks have two layers of units. Each
source (target) language lexical item is represented by
a unit in the input (output) layer. One network is con-
structed for each phrasal category (NP, VP, AP, etc.).
The networks works in the following way: for a
target-language f-structure to be generated, the transfer
system knows its phrasal category and its correspond-
ing source-language f-structure from the networks that
perform the sub-task 1. It then activates the lexical se-
lection network for that phrasal category with the input
units that correspond to the heads of the source lan-
guage f-structure and its sub-structures. Through the
connections between the two layers, the output units
are activated, and the lexical item that corresponds to
the most active output unit is selected as the head of
the target f-structure. The following example illus-
trates how the system selects the head anrrzelden for
&apos;Where vi means the event that unit i is activated.
</bodyText>
<figure confidence="0.632402857142857">
VI - I Connections
V2 -I Connections
Li Verbal System
V1 Association Network
L2 Verbal System
V2 Association Network
V1 - V2 Associations
</figure>
<page confidence="0.951581">
326
</page>
<bodyText confidence="0.918887133333333">
the German XCOMP sub-structure when it does the
transfer from
[sentence [subj n would [xcomp [subj 11 like [xcomp [subj
I] register [pp-adi for the conference]]]] to
[sentence [subj !ch] werde [xcomp [subj kid [adj gerne]
anmelden [pp-adj fuer der Konferenz]]]2.
Since the structure networks find that there is a
VP sub-structure of XCOMP in the target structure
whose corresponding input structure is [xcomp [subj
to register [pp-adj for the conference1]1, it activates the
VP lexical selection network&apos;s input units for 1 , register
and conference. By propagating the activation via the
associative connections, the unit for anmelden is the
most active output. Therefore, anmelden is chosen as
the head of the xcomp sub-structure.
</bodyText>
<subsectionHeader confidence="0.806571">
Preliminary Result
</subsectionHeader>
<bodyText confidence="0.999982133333333">
The domain of our work was the Conference Registra-
tion Telephony Conversations. The lexicon for the task
contained about 500 English and 500 German words.
There were 300 English/German f-structure pairs avail-
able from other research tasks (Osterholtz, 1992). A
separate set of 154 sentential f-structures was used to
test the generalization performance of the system. The
testing data was collected for an independent task (Jain,
1991).
From the 300 sentential f-structure pairs, every
German VP sub-structure is extracted and labeled with
its English counterpart. The English counterpart&apos;s head
and its immediate sub-structures&apos; heads serve as the
input in a sample of VP association, and the German
f-structure&apos;s head become the output of the association.
For the above example, the association ([input I, regis-
ter, conference] [output anmelden]) is a sample drawn
from the f-structures for the VP network. The training
samples for all the other networks are created in the
same way.
The accuracy of our system with information-
theoretical network lexical selection is lower than the
one with back-propagation networks (around 84% ver-
sus around 92%) for the training data. However, the
generalization performance on the unseen inputs is bet-
ter (around 70% versus around 62%). The information-
theoretical networks do not over-learn as the back-
propagation networks. This is partially due to the
reduced number of free parameters in the information-
theoretical networks.
</bodyText>
<sectionHeader confidence="0.943973" genericHeader="conclusions">
Summary
</sectionHeader>
<bodyText confidence="0.999728">
The lexical selection approach discussed here has two
advantages. First, it is learnable. Little human effort
on knowledge engineering is required. Secondly, it is
psycholinguistically well-founded in that the approach
</bodyText>
<footnote confidence="0.6375685">
2The f-structures are simplified here for the sake of
conciseness.
</footnote>
<bodyText confidence="0.993467">
adopts a local activation processing model instead of
relies upon symbol passing, as symbolic systems usu-
ally do.
</bodyText>
<sectionHeader confidence="0.989236" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999396152173913">
P. F. Brown and et al. A statistical approach to machine
translation. Computational Linguistics,16(2):73-
85, 1990.
A. M. de Groot and G. L. Nas. Lexical representation
of cognates and noncognates in compound bilin-
guals. Journal of Memory and Language, 30(1),
1991.
B. J. Don. Conceptual basis of the lexicon in ma-
chine translation. Technical Report A.I. Memo
No. 1166, Artificial Intelligence Laboratory, MIT,
August, 1989.
A. L. Gorin and S. E. Levinson. Adaptive acquisition of
language. Technical report, Speech Research De-
partment, AT&amp;T Bell Laboratories, Murray Hill,
1989.
A. N. Jain. Parsec: A connectionist learning archi-
tecture for parsing spoken language. Technical
Report CMU-CS-91-208, Carnegie Mellon Uni-
versity, 1991.
W. E. Lambert, J. Havelka and C. Crosby. The influ-
ence of language acquisition contexts on bilingual-
ism. Journal of Abnormal and Social Psychology,
56, 1958.
S. Nirenberg, V. Raskin and A. B. Tucker. The struc-
ture of interlingua in translator. In S. Niren-
burg, editor, Machine Translation: Theoretical
and Methodological Issues. Cambridge University
Press, Cambridge, England, 1987.
L. Osterholtz and et al. Janus: a multi-lingual speech
to speech translation system. In Proceedings of
the IEEE International Conference on Acoustics,
Speech and Signal Processing, volume 1, pages
209-212. IEEE, 1992.
A. Paivio. Mental Representationsâ€” A Dual Coding
Approach. Oxford University Press, New York,
1986.
J. Pustejovsky and S. Nirenburg. Lexical selection in
the process of language generation. In Proceed-
ings of the 25th Annual Conference of the Associ-
ation for Computational Linguistics, pages 201-
206, Standford University, Standford, CA, 1987.
A. Robinson. Practical network design and implemen-
tation. In Cambridge Neural Network Summer
School, 1992.
Y. Wang and A. Waibel. Connectionist transfer in ma-
chine translation. In prepare, 1994.
</reference>
<page confidence="0.998414">
327
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.906876">
<title confidence="0.9972065">DUAL-CODING THEORY AND CONNECTIONIST LEXICAL SELECTION</title>
<author confidence="0.998552">Ye-Yi Wang</author>
<affiliation confidence="0.992418">Computational Linguistics Program Carnegie Mellon University</affiliation>
<address confidence="0.99996">Pittsburgh, PA 15232</address>
<email confidence="0.989346">Internet:yyw@cs.cmu.edu</email>
<abstract confidence="0.988481666666667">We introduce the bilingual dual-coding theory as a model for bilingual mental representation. Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P F Brown</author>
</authors>
<title>A statistical approach to machine translation. Computational Linguistics,16(2):73-85,</title>
<date>1990</date>
<contexts>
<context position="4923" citStr="Brown, 1990" startWordPosition="692" endWordPosition="693">ical entries. This intermediate representation may be Lexical Concept Structure (Dorr, 1989) or interlingua (Nirenberg, 1987). This engineering approach requires great effort in designing the representation and the mapping rules. Currently, there are some efforts in statistical lexical selection. A target language word Wt can be selected with the posterior probability Pr(Wt I Ws) given the source language word W. Several target language lexical entries may be selected for a single source language word. Then the correct selections can be identified by the language model of the target language (Brown, 1990). This approach is learnable. However, the accuracy is low. One reason is that it does not use any structural information of a language. In next subsections, we propose informationtheoretical networks based on the bilingual dual-coding theory for lexical selection. Information-Theoretical Networks Information-theoretical network is a neural network formalism that is capable of doing associations between two layers of representations. The associations can be obtained statistically according to the network&apos;s experiences. An information-theoretical network has two layers. Each unit of a layer rep</context>
</contexts>
<marker>Brown, 1990</marker>
<rawString>P. F. Brown and et al. A statistical approach to machine translation. Computational Linguistics,16(2):73-85, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A M de Groot</author>
<author>G L Nas</author>
</authors>
<title>Lexical representation of cognates and noncognates in compound bilinguals.</title>
<date>1991</date>
<journal>Journal of Memory and Language,</journal>
<volume>30</volume>
<issue>1</issue>
<marker>de Groot, Nas, 1991</marker>
<rawString>A. M. de Groot and G. L. Nas. Lexical representation of cognates and noncognates in compound bilinguals. Journal of Memory and Language, 30(1), 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B J Don</author>
</authors>
<title>Conceptual basis of the lexicon in machine translation.</title>
<date>1989</date>
<booktitle>Artificial Intelligence Laboratory,</booktitle>
<tech>Technical Report A.I. Memo No. 1166,</tech>
<location>MIT,</location>
<marker>Don, 1989</marker>
<rawString>B. J. Don. Conceptual basis of the lexicon in machine translation. Technical Report A.I. Memo No. 1166, Artificial Intelligence Laboratory, MIT, August, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Gorin</author>
<author>S E Levinson</author>
</authors>
<title>Adaptive acquisition of language.</title>
<date>1989</date>
<tech>Technical report,</tech>
<institution>Speech Research Department, AT&amp;T Bell Laboratories,</institution>
<location>Murray Hill,</location>
<contexts>
<context position="1296" citStr="Gorin and Levinson, 1989" startWordPosition="175" endWordPosition="178">n mind; (2) how the representation is mapped from one language to another; (3) how the representation and mapping are acquired by human. The bilingual dual-coding theory (Paivio, 1986) partially answers the above questions. It depicts the verbal representations for two different languages as two separate but connected logogen systems, characterizes the translation process as the activation along the connections between the logogen systems, and attributes the acquisition of the representation to some unspecified statistical processes. We have explored an information theoretical neural network (Gorin and Levinson, 1989) that can acquire the verbal associations in the dual-coding theory. It provides a learnable lexical selection sub-system for a connectionist transfer project in machine translation. Dual-Coding Theory There is a well-known debate in psycholinguistics concerning the bilingual mental representation: independence position assumes that bilingual memory is represented by two functionally independent storage and retrieval systems, whereas interdependence position hypothesizes that all information of languages exists in a common memory store. Studies on crosslanguage transfer and cross-language prim</context>
<context position="6227" citStr="Gorin and Levinson, 1989" startWordPosition="898" endWordPosition="902">a logogen or a word. Units in different layers are connected. The weight of the connection between unit i in one layer and unit fin the other layer is assigned with the mutual information between the elements represented by the two units (1) wij = /(vi, vj) = log(Pr(vivi)1Pr(vi))1 Each layer also contains a bias unit, which is always activated. The weight of the connection between the bias unit in one layer and unit j in the other layer is (2) woj = log Pr(vi) Both the information-theoretical network and the back-propagation network compute the posterior probabilities for an association task (Gorin and Levinson, 1989; Robinson, 1992). However, only the information-theoretical network is isomorphic to the directly interconnected verbal systems in the dualcoding theory. Besides, an information-theoretical network has the following advantages: (1) it learns fast. The network can learn in a single pass without gradient decent. (2) it is adaptive. It can incrementally adapt to new experiences simply by adding new data to the training samples and modifying the associations according to the changed statistics. These make the network more psychologically plausible. Lexical Selection as an Associative Process We t</context>
</contexts>
<marker>Gorin, Levinson, 1989</marker>
<rawString>A. L. Gorin and S. E. Levinson. Adaptive acquisition of language. Technical report, Speech Research Department, AT&amp;T Bell Laboratories, Murray Hill, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A N Jain</author>
</authors>
<title>Parsec: A connectionist learning architecture for parsing spoken language.</title>
<date>1991</date>
<tech>Technical Report CMU-CS-91-208,</tech>
<institution>Carnegie Mellon University,</institution>
<contexts>
<context position="9703" citStr="Jain, 1991" startWordPosition="1435" endWordPosition="1436">tion via the associative connections, the unit for anmelden is the most active output. Therefore, anmelden is chosen as the head of the xcomp sub-structure. Preliminary Result The domain of our work was the Conference Registration Telephony Conversations. The lexicon for the task contained about 500 English and 500 German words. There were 300 English/German f-structure pairs available from other research tasks (Osterholtz, 1992). A separate set of 154 sentential f-structures was used to test the generalization performance of the system. The testing data was collected for an independent task (Jain, 1991). From the 300 sentential f-structure pairs, every German VP sub-structure is extracted and labeled with its English counterpart. The English counterpart&apos;s head and its immediate sub-structures&apos; heads serve as the input in a sample of VP association, and the German f-structure&apos;s head become the output of the association. For the above example, the association ([input I, register, conference] [output anmelden]) is a sample drawn from the f-structures for the VP network. The training samples for all the other networks are created in the same way. The accuracy of our system with informationtheore</context>
</contexts>
<marker>Jain, 1991</marker>
<rawString>A. N. Jain. Parsec: A connectionist learning architecture for parsing spoken language. Technical Report CMU-CS-91-208, Carnegie Mellon University, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W E Lambert</author>
<author>J Havelka</author>
<author>C Crosby</author>
</authors>
<title>The influence of language acquisition contexts on bilingualism.</title>
<date>1958</date>
<journal>Journal of Abnormal and Social Psychology,</journal>
<volume>56</volume>
<marker>Lambert, Havelka, Crosby, 1958</marker>
<rawString>W. E. Lambert, J. Havelka and C. Crosby. The influence of language acquisition contexts on bilingualism. Journal of Abnormal and Social Psychology, 56, 1958.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Nirenberg</author>
<author>V Raskin</author>
<author>A B Tucker</author>
</authors>
<title>The structure of interlingua in translator.</title>
<date>1987</date>
<booktitle>Machine Translation: Theoretical and Methodological Issues.</booktitle>
<editor>In S. Nirenburg, editor,</editor>
<publisher>Cambridge University Press,</publisher>
<location>Cambridge, England,</location>
<marker>Nirenberg, Raskin, Tucker, 1987</marker>
<rawString>S. Nirenberg, V. Raskin and A. B. Tucker. The structure of interlingua in translator. In S. Nirenburg, editor, Machine Translation: Theoretical and Methodological Issues. Cambridge University Press, Cambridge, England, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Osterholtz</author>
</authors>
<title>Janus: a multi-lingual speech to speech translation system.</title>
<date>1992</date>
<booktitle>In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<volume>1</volume>
<pages>209--212</pages>
<publisher>IEEE,</publisher>
<contexts>
<context position="9525" citStr="Osterholtz, 1992" startWordPosition="1407" endWordPosition="1408">ucture is [xcomp [subj to register [pp-adj for the conference1]1, it activates the VP lexical selection network&apos;s input units for 1 , register and conference. By propagating the activation via the associative connections, the unit for anmelden is the most active output. Therefore, anmelden is chosen as the head of the xcomp sub-structure. Preliminary Result The domain of our work was the Conference Registration Telephony Conversations. The lexicon for the task contained about 500 English and 500 German words. There were 300 English/German f-structure pairs available from other research tasks (Osterholtz, 1992). A separate set of 154 sentential f-structures was used to test the generalization performance of the system. The testing data was collected for an independent task (Jain, 1991). From the 300 sentential f-structure pairs, every German VP sub-structure is extracted and labeled with its English counterpart. The English counterpart&apos;s head and its immediate sub-structures&apos; heads serve as the input in a sample of VP association, and the German f-structure&apos;s head become the output of the association. For the above example, the association ([input I, register, conference] [output anmelden]) is a sam</context>
</contexts>
<marker>Osterholtz, 1992</marker>
<rawString>L. Osterholtz and et al. Janus: a multi-lingual speech to speech translation system. In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, volume 1, pages 209-212. IEEE, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Paivio</author>
</authors>
<title>Mental Representationsâ€” A Dual Coding Approach.</title>
<date>1986</date>
<publisher>Oxford University Press,</publisher>
<location>New York,</location>
<contexts>
<context position="855" citStr="Paivio, 1986" startWordPosition="115" endWordPosition="116">r bilingual mental representation. Based on this model, lexical selection neural networks are implemented for a connectionist transfer project in machine translation. Introduction Psycholinguistic knowledge would be greatly helpful, as we believe, in constructing an artificial language processing system. As for machine translation, we should take advantage of our understandings of (1) how the languages are represented in human mind; (2) how the representation is mapped from one language to another; (3) how the representation and mapping are acquired by human. The bilingual dual-coding theory (Paivio, 1986) partially answers the above questions. It depicts the verbal representations for two different languages as two separate but connected logogen systems, characterizes the translation process as the activation along the connections between the logogen systems, and attributes the acquisition of the representation to some unspecified statistical processes. We have explored an information theoretical neural network (Gorin and Levinson, 1989) that can acquire the verbal associations in the dual-coding theory. It provides a learnable lexical selection sub-system for a connectionist transfer project </context>
</contexts>
<marker>Paivio, 1986</marker>
<rawString>A. Paivio. Mental Representationsâ€” A Dual Coding Approach. Oxford University Press, New York, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>S Nirenburg</author>
</authors>
<title>Lexical selection in the process of language generation.</title>
<date>1987</date>
<booktitle>In Proceedings of the 25th Annual Conference of the Association for Computational Linguistics,</booktitle>
<pages>201--206</pages>
<institution>Standford University,</institution>
<location>Standford, CA,</location>
<marker>Pustejovsky, Nirenburg, 1987</marker>
<rawString>J. Pustejovsky and S. Nirenburg. Lexical selection in the process of language generation. In Proceedings of the 25th Annual Conference of the Association for Computational Linguistics, pages 201-206, Standford University, Standford, CA, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Robinson</author>
</authors>
<title>Practical network design and implementation.</title>
<date>1992</date>
<booktitle>In Cambridge Neural Network Summer School,</booktitle>
<contexts>
<context position="6244" citStr="Robinson, 1992" startWordPosition="903" endWordPosition="904"> in different layers are connected. The weight of the connection between unit i in one layer and unit fin the other layer is assigned with the mutual information between the elements represented by the two units (1) wij = /(vi, vj) = log(Pr(vivi)1Pr(vi))1 Each layer also contains a bias unit, which is always activated. The weight of the connection between the bias unit in one layer and unit j in the other layer is (2) woj = log Pr(vi) Both the information-theoretical network and the back-propagation network compute the posterior probabilities for an association task (Gorin and Levinson, 1989; Robinson, 1992). However, only the information-theoretical network is isomorphic to the directly interconnected verbal systems in the dualcoding theory. Besides, an information-theoretical network has the following advantages: (1) it learns fast. The network can learn in a single pass without gradient decent. (2) it is adaptive. It can incrementally adapt to new experiences simply by adding new data to the training samples and modifying the associations according to the changed statistics. These make the network more psychologically plausible. Lexical Selection as an Associative Process We tried to map sourc</context>
</contexts>
<marker>Robinson, 1992</marker>
<rawString>A. Robinson. Practical network design and implementation. In Cambridge Neural Network Summer School, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Wang</author>
<author>A Waibel</author>
</authors>
<title>Connectionist transfer in machine translation.</title>
<date>1994</date>
<booktitle>In prepare,</booktitle>
<marker>Wang, Waibel, 1994</marker>
<rawString>Y. Wang and A. Waibel. Connectionist transfer in machine translation. In prepare, 1994.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>