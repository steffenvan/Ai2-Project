<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.040313">
<title confidence="0.786398">
SemEval-2010 Task 3: Cross-lingual Word Sense Disambiguation
</title>
<author confidence="0.633241">
Els Lefever1 &apos;2 and Veronique Hoste1 &apos;2
</author>
<affiliation confidence="0.498573">
1LT3, Language and Translation Technology Team, University College Ghent
Groot-Brittanni¨elaan 45, 9000 Gent, Belgium
2Department of Applied Mathematics and Computer Science, Ghent University
</affiliation>
<address confidence="0.968718">
Krijgslaan 281 (S9), 9000 Gent, Belgium
</address>
<email confidence="0.998036">
{Els.Lefever, Veronique.Hoste}@hogent.be
</email>
<sectionHeader confidence="0.99562" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996556">
We propose a multilingual unsupervised Word
Sense Disambiguation (WSD) task for a sample of
English nouns. Instead of providing manually sense-
tagged examples for each sense of a polysemous
noun, our sense inventory is built up on the basis of
the Europarl parallel corpus. The multilingual setup
involves the translations of a given English polyse-
mous noun in five supported languages, viz. Dutch,
French, German, Spanish and Italian.
The task targets the following goals: (a) the man-
ual creation of a multilingual sense inventory for a
lexical sample of English nouns and (b) the eval-
uation of systems on their ability to disambiguate
new occurrences of the selected polysemous nouns.
For the creation of the hand-tagged gold standard,
all translations of a given polysemous English noun
are retrieved in the five languages and clustered by
meaning. Systems can participate in 5 bilingual
evaluation subtasks (English - Dutch, English - Ger-
man, etc.) and in a multilingual subtask covering all
language pairs.
As WSD from cross-lingual evidence is gaining
popularity, we believe it is important to create a mul-
tilingual gold standard and run cross-lingual WSD
benchmark tests.
</bodyText>
<sectionHeader confidence="0.999323" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999721228571429">
The Word Sense Disambiguation (WSD) task,
which consists in selecting the correct sense of a
given word in a given context, has been widely
studied in computational linguistics. For a recent
overview of WSD algorithms, resources and appli-
cations, we refer to Agirre and Edmonds (2006)
and Navigli (2009). Semantic evaluation competi-
tions such as Senseval1 and its successor Semeval
revealed that supervised approaches to WSD
usually achieve better results than unsupervised
methods (M`arquez et al., 2006). The former use
machine learning techniques to induce a classifier
from manually sense-tagged data, where each
occurrence of a polysemous word gets assigned a
sense label from a predefined sense inventory such
as WordNet (Fellbaum, 1998). These supervised
methods, however, heavily rely on large sense-
tagged corpora which are very time consuming and
expensive to build. This phenomenon, well known
as the knowledge acquisition bottleneck (Gale et
al., 1992), explains the modest use and success of
supervised WSD in real applications.
Although WSD has long time been studied as a
stand-alone NLP task, there is a growing feeling
in the WSD community that WSD should prefer-
ably be integrated in real applications such as
Machine Translation or multilingual information
retrieval (Agirre and Edmonds, 2006). Several
studies have demonstrated that for instance Sta-
tistical Machine Translation (SMT) benefits from
incorporating a dedicated WSD module (Chan et al.,
2007; Carpuat and Wu, 2007). Using translations
from a corpus instead of human-defined sense
labels is one way of facilitating the integration of
WSD in multilingual applications. It also implic-
</bodyText>
<footnote confidence="0.993087">
1http://www.senseval.org/
</footnote>
<page confidence="0.981878">
82
</page>
<note confidence="0.817353">
Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 82–87,
Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics
</note>
<bodyText confidence="0.994269905405405">
itly deals with the granularity problem as finer
sense distinctions are only relevant as far as they
are lexicalized in the translations. Furthermore,
this type of corpus-based approach is language-
independent, which makes it a valid alternative
for languages lacking sufficient sense inventories
and sense-tagged corpora, although one could
argue that the lack of parallel corpora for certain
language pairs might be problematic as well. The
methodology to deduce word senses from parallel
corpora starts from the hypothesis that the different
sense distinctions of a polysemous word are often
lexicalized cross-linguistically. For instance, if we
query the English noun “bill” in the English-Dutch
Europarl, the following top four translations are
retrieved: “rekening” (Eng.: “invoice”) (198 occur-
rences), “kosten” (Eng.: “costs”) (100 occ.), “Bill”
(96 occ.) and “wetsvoorstel” (Eng.: “piece of
legislation”) (77 occ.). If we make the simplifying
assumption for our example that (i) these are the
only Dutch translations of our focus word and that
(ii) all sense distinctions of “bill” are lexicalized
in Dutch, we can infer that the English noun “bill”
has at most four different senses. These different
senses in turn can be grouped in case of synonymy.
In the Dutch-French Europarl, for example, both
“rekening” and “kosten”, are translated by the
French “frais”, which might indicate that both
Dutch words are synonymous.
Several WSD studies are based on the idea of
cross-lingual evidence. Gale et al. (1993) use a
bilingual parallel corpus for the automatic creation
of a sense-tagged data set, where target words in the
source language are tagged with their translation
of the word in the target language. Diab and
Resnik (2002) present an unsupervised approach
to WSD that exploits translational correspondences
in parallel corpora that were artificially created by
applying commercial MT systems on a sense-tagged
English corpus. Ide et al. (2002) use a multilingual
parallel corpus (containing seven languages from
four language families) and show that sense dis-
tinctions derived from translation equivalents are at
least as reliable as those made by human annotators.
Moreover, some studies present multilingual WSD
systems that attain state-of-the-art performance in
all-words disambiguation (Ng et al., 2003). The
proposed Cross-lingual Word Sense Disambigua-
tion task differs from earlier work (e.g. Ide et al.
(2002)) through its independence from an externally
defined sense set.
The remainder of this paper is organized as follows.
In Section 2, we present a detailed description of
the cross-lingual WSD task. It introduces the par-
allel corpus we used, informs on the development
and test data and discusses the annotation procedure.
Section 3 gives an overview of the different scoring
strategies that will be applied. Section 4 concludes
this paper.
2 Task set up
The cross-lingual Word Sense Disambiguation task
involves a lexical sample of English nouns. We pro-
pose two subtasks, i.e. systems can either partici-
pate in the bilingual evaluation task (in which the
answer consists of translations in one language) or
in the multilingual evaluation task (in which the an-
swer consists of translations in all five supported lan-
guages). Table 1 shows an example of the bilingual
sense labels for two test occurrences of the English
noun bank in our parallel corpus which will be fur-
ther described in Section 2.1. Table 2 presents the
multilingual sense labels for the same sentences.
... giving fish to people living on the [bank] of the
river
</bodyText>
<table confidence="0.998173">
Language Sense label
Dutch (NL) oever/dijk
French (F) rives/rivage/bord/bords
German (D) Ufer
Italian (I) riva
Spanish (ES) orilla
The [bank] of Scotland ...
Language Sense label
Dutch (NL) bank/kredietinstelling
French (F) banque/´etablissement de cr´edit
German (D) Bank/Kreditinstitut
Italian (I) banca
Spanish (ES) banco
</table>
<tableCaption confidence="0.9921105">
Table 1: Example of bilingual sense labels for the English
noun bank
</tableCaption>
<page confidence="0.997479">
83
</page>
<bodyText confidence="0.960906">
... giving fish to people living on the [bank] of the
river
</bodyText>
<table confidence="0.939405555555556">
Language Sense label
NL,F,D,I,ES oever/dijk,
rives/rivage/bord/bords,
Ufer, riva, orilla
The [bank] of Scotland ...
Language Sense label
NL,F,D,I,ES bank/kredietinstelling, banque/
´etablissement de cr´edit, Bank/
Kreditinstitut, banca, banco
</table>
<tableCaption confidence="0.9897725">
Table 2: Example of multi-lingual sense labels for the
English noun bank
</tableCaption>
<subsectionHeader confidence="0.98274">
2.1 Corpus and word selection
</subsectionHeader>
<bodyText confidence="0.999881">
The document collection which serves as the basis
for the gold standard construction and system
evaluation is the Europarl parallel corpus2, which
is extracted from the proceedings of the European
Parliament (Koehn, 2005). We selected 6 languages
from the 11 European languages represented in
the corpus: English (our target language), Dutch,
French, German, Italian and Spanish. All sentences
are aligned using a tool based on the Gale and
Church (1991) algorithm. We only consider the 1-1
sentence alignments between English and the five
other languages (see also Tufis et al. (2004) for
a similar strategy). These 1-1 alignments will be
made available to all task participants. Participants
are free to use other training corpora, but additional
translations which are not present in Europarl will
not be included in the sense inventory that is used
for evaluation.
For the competition, two data sets will be developed.
The development and test sentences will be selected
from the JRC-ACQUIS Multilingual Parallel Cor-
pus3. The development data set contains 5 poly-
semous nouns, for which we provide the manually
built sense inventory based on Europarl and 50 ex-
ample instances, each annotated with one sense label
(cluster that contains all translations that have been
grouped together for that particular sense) per target
</bodyText>
<footnote confidence="0.999737">
2http://www.statmt.org/europarl/
3http://wt.jrc.it/lt/Acquis/
</footnote>
<bodyText confidence="0.999794307692308">
language. The manual construction of the sense in-
ventory will be discussed in Section 2.2. The test
data contains 50 instances for 20 nouns from the test
data as used in the Cross-Lingual Lexical Substitu-
tion Task4. In this task, annotators and systems are
asked to provide as many correct Spanish transla-
tions as possible for an English target word. They
are not bound to a predefined parallel corpus, but
can freely choose the translations from any available
resource. Selecting the target words from the set of
nouns thats will be used for the Lexical Substitution
Task should make it easier for systems to participate
in both tasks.
</bodyText>
<subsectionHeader confidence="0.996697">
2.2 Manual annotation
</subsectionHeader>
<bodyText confidence="0.99322175862069">
The sense inventory for the 5 target nouns in the de-
velopment data and the 20 nouns in the test data is
manually built up in three steps.
1. In the first annotation step, the 5 translations
of the English word are identified per sentence
ID. In order to speed up this identification,
GIZA++ (Och and Ney, 2003) is used to gen-
erate the initial word alignments for the 5 lan-
guages. All word alignments are manually ver-
ified.
In this step, we might come across multiword
translations, especially in Dutch and German
which tend to glue parts of compounds together
in one orthographic unit. We decided to keep
these translations as such, even if they do not
correspond exactly to the English target word.
In following sentence, the Dutch translation
witboek corresponds in fact to the English com-
pound white paper, and not to the English tar-
get word paper:
English: the European Commission
presented its white paper
Dutch: de presentatie van het
witboek door de Europese Com-
missie
Although we will not remove these compound
translations from our sense inventory, we will
make sure that the development and test sen-
tences do not contain target words that are part
</bodyText>
<footnote confidence="0.997337">
4http://lit.csci.unt.edu/index.php/Semeval 2010
</footnote>
<page confidence="0.999256">
84
</page>
<bodyText confidence="0.973571666666667">
of a larger multiword unit, in order not to dis-
advantage systems that do not deal with decom-
pounding.
</bodyText>
<listItem confidence="0.965417181818182">
2. In the second step, three annotators per lan-
guage will cluster the retrieved translations per
target language. On the basis of the sentence
IDs, the translations in all languages will be au-
tomatically coupled. Only translations above a
predefined frequency threshold are considered
for inclusion in a cluster. Clustering will hap-
pen in a trilingual setting, i.e. annotators al-
ways cluster two target languages simultane-
ously (with English being the constant source
language)5.
</listItem>
<bodyText confidence="0.999013125">
After the clustering of the translations, the an-
notators perform a joint evaluation per lan-
guage in order to reach a consensus clustering
for each target language. In case the annota-
tors do not reach a consensus, we apply soft-
clustering for that particular translation, i.e. we
assign the translation to two or more different
clusters.
</bodyText>
<listItem confidence="0.79352525">
3. In a last step, there will be a cross-lingual con-
flict resolution in which the resulting cluster-
ings are checked cross-lingually by the human
annotators.
</listItem>
<bodyText confidence="0.9998271875">
The resulting sense inventory is used to annotate the
sentences in the development set and the test set.
This implies that a given target word is annotated
with the appropriate sense cluster. This annotation
is done by the same native annotators as in steps
2 and 3. The goal is to reach a consensus cluster
per sentence. But again, if no consensus is reached,
soft-clustering is applied and as a consequence, the
correct answer for this particular test instance con-
sists of one of the clusters that were considered for
soft-clustering.
The resulting clusters are used by the three native
annotators to select their top 3 translations per
sentence. These potentially different translations
are kept to calculate frequency information for all
answer translations (discussed in section 3).
</bodyText>
<footnote confidence="0.985463666666667">
5The annotators will be selected from the master students
at the “University College Ghent – Faculty of Translation” that
trains certified translators in all six involved languages.
</footnote>
<bodyText confidence="0.989933666666667">
Table 3 shows an example of how the translation
clusters for the English noun “paper” could look
like in a trilingual setting.
</bodyText>
<sectionHeader confidence="0.964065" genericHeader="method">
3 System evaluation
</sectionHeader>
<bodyText confidence="0.999902875">
As stated before, systems can participate in two
tasks, i.e. systems can either participate in one or
more bilingual evaluation tasks or they can partici-
pate in the multilingual evaluation task incorporat-
ing the five supported languages. The evaluation of
the multilingual evaluation task is simply the aver-
age of the system scores on the five bilingual evalu-
ation tasks.
</bodyText>
<subsectionHeader confidence="0.999535">
3.1 Evaluation strategies
</subsectionHeader>
<bodyText confidence="0.999941612903226">
For the evaluation of the participating systems we
will use an evaluation scheme which is inspired
by the English lexical substitution task in SemEval
2007 (McCarthy and Navigli, 2007). The evaluation
will be performed using precision and recall (P and
R in the equations that follow). We perform both a
best result evaluation and a more relaxed evaluation
for the top five results.
Let H be the set of annotators, T be the set of test
items and hi be the set of responses for an item i E T
for annotator h E H. Let A be the set of items from
T where the system provides at least one answer and
ai : i E A be the set of guesses from the system for
item i. For each i, we calculate the multiset union
(Hi) for all hi for all h E H and for each unique
type (res) in Hi that has an associated frequency
(freqs.es). In the formula of (McCarthy and Navigli,
2007), the associated frequency (freqs.es) is equal
to the number of times an item appears in Hi. As
we define our answer clusters by consensus, this fre-
quency would always be “1”. In order to overcome
this, we ask our human annotators to indicate their
top 3 translations, which enables us to also obtain
meaningful associated frequencies (freqs.es) (“1” in
case the translation is not chosen by any annotator,
“2” in case a translation is picked by 1 annotator, “3”
if picked by two annotators and “4” if chosen by all
three annotators).
Best result evaluation For the best result evalu-
ation, systems can propose as many guesses as the
system believes are correct, but the resulting score is
</bodyText>
<page confidence="0.999427">
85
</page>
<bodyText confidence="0.9996095">
target languages for new instances of the selected
polysemous target nouns.
divided by the number of guesses. In this way, sys-
tems that output a lot of guesses are not favoured.
</bodyText>
<equation confidence="0.993229833333333">
Eres∈ai freqres
|ai|
|Hi |(1)
Eres∈ai freqres
|ai|
|Hi |(2)
</equation>
<bodyText confidence="0.99871475">
Relaxed evaluation For the more relaxed evalu-
ation, systems can propose up to five guesses. For
this evaluation, the resulting score is not divided by
the number of guesses.
</bodyText>
<subsectionHeader confidence="0.997515">
3.2 Baseline
</subsectionHeader>
<bodyText confidence="0.99997175">
We will produce two, both frequency-based, base-
lines. The first baseline, which will be used for the
best result evaluation, is based on the output of the
GIZA++ word alignments on the Europarl corpus
and just returns the most frequent translation of a
given word. The second baseline outputs the five
most frequent translations of a given word accord-
ing to the GIZA++ word alignments. This baseline
will be used for the relaxed evaluation. As a third
baseline, we will consider using a baseline based on
EuroWordNet6, which is available in the five target
languages.
</bodyText>
<sectionHeader confidence="0.999587" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.999661125">
We presented a multilingual unsupervised Word
Sense Disambiguation task for a sample of English
nouns. The lack of supervision refers to the con-
struction of the sense inventory, that is built up on
the basis of translations retrieved from the Europarl
corpus in five target languages. Systems can partici-
pate in a bilingual or multilingual evaluation and are
asked to provide correct translations in one or five
</bodyText>
<footnote confidence="0.882679">
6http://www.illc.uva.nl/EuroWordNet
</footnote>
<sectionHeader confidence="0.903997" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997918617021277">
E. Agirre and P. Edmonds, editors. 2006. Word Sense
Disambiguation. Text, Speech and Language Tech-
nology. Springer, Dordrecht.
M. Carpuat and D. Wu. 2007. Improving statistical
machine translation using word sense disambiguation.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 61–72, Prague, Czech Republic.
Y.S. Chan, H.T. Ng, and D. Chiang. 2007. Word sense
disambiguation improves statistical machine transla-
tion. In Proceedings of the 45th Annual Meeting of the
Association of Computational Linguistics, pages 33–
40, Prague, Czech Republic.
M. Diab and P. Resnik. 2002. An unsupervised method
for word sense tagging using parallel corpora. In Pro-
ceedings of ACL, pages 255–262.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
William A. Gale and Kenneth W. Church. 1991. A pro-
gram for aligning sentences in bilingual corpora. In
Computational Linguistics, pages 177–184.
W.A. Gale, K. Church, and D. Yarowsky. 1992. Esti-
mating upper and lower bounds on the performance
of word-sense disambiguation programs. In Proceed-
ings of the 30th Annual Meeting of the Association for
Computational Linguistics, pages 249–256.
W.A. Gale, K.W. Church, and D. Yarowsky. 1993. A
method for disambiguating word senses in a large cor-
pus. In Computers and the Humanities, volume 26,
pages 415–439.
N. Ide, T. Erjavec, and D. Tufis. 2002. Sense dis-
crimination with parallel corpora. In Proceedings of
ACL Workshop on Word Sense Disambiguation: Re-
cent Successes and Future Directions, pages 54–60.
P. Koehn. 2005. Europarl: A parallel corpus for statis-
tical machine translation. In Proceedings of the MT
Summit.
L. M`arquez, G. Escudero, D. Martinez, and G. Rigau.
2006. Supervised corpus-based methods for WSD. In
E. Agirre and P. Edmonds, editors, Word Sense Disam-
biguation: Algorithms and Applications, pages 167–
216. Eds Springer, New York, NY.
D. McCarthy and R. Navigli. 2007. Semeval-2007 task
10: English lexical substitution task. In Proceedings
of the 4th International Workshop on Semantic Evalu-
ations (SemEval-2007), pages 48–53.
</reference>
<equation confidence="0.9945049">
P = Eai:i∈A |A|
R Eai:i∈T
=
|T|
Eres∈ai freqres
P = Eai:i∈A |A ||Hi |(3)
Eres∈ai freqres
Eai:i∈T |Hi |(4)
R =
|T|
</equation>
<page confidence="0.995122">
86
</page>
<reference confidence="0.999602411764706">
R. Navigli. 2009. Word sense disambiguation: a survey.
In ACM Computing Surveys, volume 41, pages 1–69.
H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting
parallel texts for word sense disambiguation: An em-
pirical study. In Proceedings of the 41st Annual Meet-
ing of the Association for Computational Linguistics,
pages 455–462, Santa Cruz.
F.J. Och and H. Ney. 2003. A systematic comparison of
various statistical alignment models. Computational
Linguistics, 29(1):19–51.
Dan Tufis¸, Radu Ion, and Nancy Ide. 2004. Fine-Grained
Word Sense Disambiguation Based on Parallel Cor-
pora, Word Alignment, Word Clustering and Aligned
Wordnets. In Proceedings of the 20th International
Conference on Computational Linguistics (COLING
2004), pages 1312–1318, Geneva, Switzerland, Au-
gust. Association for Computational Linguistics.
</reference>
<bodyText confidence="0.999109607142857">
English ”paper” Dutch French Italian
Cluster 1 boek, verslag, wetsvoorstel livre, document, libro
green paper kaderbesluit paquet
Cluster 2 document, voorstel, paper document, rapport, travail documento, rapporto
present a paper nota, stuk, notitie publication, note testo, nota
proposition, avis
Cluster 3 krant, dagblad journal, quotidien giornale, quotidiano,
read a paper weekblad hebdomadaire settimanale, rivista
Cluster 4 papier papier carta, cartina
reams ofpaper
Cluster 5 papieren, papier papeterie, papeti`ere cartastraccia, cartaceo
ofpaper, paper prullenmand papier cartiera
industry, paper basket
Cluster 6 stembiljet, bulletin, vote scheda, scheda di voto
voting paper, stembriefje
ballot paper
Cluster 7 papiertje papier volant foglio, foglietto
piece ofpaper
Cluster 8 papier, administratie paperasse, paperasserie carta, amministrativo
excess ofpaper, administratief papier, administratif burocratico, cartaceo
generate paper bureaucratie
Cluster 9 in theorie, op papier, en th´eorie, in teoria,
on paper papieren, bij woorden conceptuellement di parole
Cluster 10 op papier ´ecrit, dans les textes, nero su bianco, (di natura)
on paper de nature typographique, par voie tipografica, per iscritto,
´epistolaire, sur (le) papier cartaceo, di parole
Cluster 11 agenda, zittingstuk, ordre du jour, ordine del giorno
order paper stuk ordre des votes
</bodyText>
<tableCaption confidence="0.998551">
Table 3: translation clusters for the English noun ”paper”
</tableCaption>
<page confidence="0.998966">
87
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235482">
<title confidence="0.824252333333333">SemEval-2010 Task 3: Cross-lingual Word Sense Disambiguation and Veronique Language and Translation Technology Team, University College</title>
<author confidence="0.394598">Groot-Brittanni¨elaan</author>
<affiliation confidence="0.305522">of Applied Mathematics and Computer Science, Ghent</affiliation>
<address confidence="0.651771">Krijgslaan 281 (S9), 9000 Gent, Belgium</address>
<abstract confidence="0.998860884615385">We propose a multilingual unsupervised Word Sense Disambiguation (WSD) task for a sample of English nouns. Instead of providing manually sensetagged examples for each sense of a polysemous noun, our sense inventory is built up on the basis of the Europarl parallel corpus. The multilingual setup involves the translations of a given English polysemous noun in five supported languages, viz. Dutch, French, German, Spanish and Italian. The task targets the following goals: (a) the manual creation of a multilingual sense inventory for a lexical sample of English nouns and (b) the evaluation of systems on their ability to disambiguate new occurrences of the selected polysemous nouns. For the creation of the hand-tagged gold standard, all translations of a given polysemous English noun are retrieved in the five languages and clustered by meaning. Systems can participate in 5 bilingual evaluation subtasks (English - Dutch, English - German, etc.) and in a multilingual subtask covering all language pairs. As WSD from cross-lingual evidence is gaining popularity, we believe it is important to create a multilingual gold standard and run cross-lingual WSD benchmark tests.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>P Edmonds</author>
<author>editors</author>
</authors>
<date>2006</date>
<booktitle>Word Sense Disambiguation. Text, Speech and Language Technology.</booktitle>
<publisher>Springer,</publisher>
<location>Dordrecht.</location>
<marker>Agirre, Edmonds, editors, 2006</marker>
<rawString>E. Agirre and P. Edmonds, editors. 2006. Word Sense Disambiguation. Text, Speech and Language Technology. Springer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Carpuat</author>
<author>D Wu</author>
</authors>
<title>Improving statistical machine translation using word sense disambiguation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>61--72</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3070" citStr="Carpuat and Wu, 2007" startWordPosition="459" endWordPosition="462">d. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrated that for instance Statistical Machine Translation (SMT) benefits from incorporating a dedicated WSD module (Chan et al., 2007; Carpuat and Wu, 2007). Using translations from a corpus instead of human-defined sense labels is one way of facilitating the integration of WSD in multilingual applications. It also implic1http://www.senseval.org/ 82 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 82–87, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics itly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the translations. Furthermore, this type of corpus-based approach is languageindepende</context>
</contexts>
<marker>Carpuat, Wu, 2007</marker>
<rawString>M. Carpuat and D. Wu. 2007. Improving statistical machine translation using word sense disambiguation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 61–72, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y S Chan</author>
<author>H T Ng</author>
<author>D Chiang</author>
</authors>
<title>Word sense disambiguation improves statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,</booktitle>
<pages>33--40</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3047" citStr="Chan et al., 2007" startWordPosition="455" endWordPosition="458">d expensive to build. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrated that for instance Statistical Machine Translation (SMT) benefits from incorporating a dedicated WSD module (Chan et al., 2007; Carpuat and Wu, 2007). Using translations from a corpus instead of human-defined sense labels is one way of facilitating the integration of WSD in multilingual applications. It also implic1http://www.senseval.org/ 82 Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 82–87, Boulder, Colorado, June 2009. c�2009 Association for Computational Linguistics itly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the translations. Furthermore, this type of corpus-based approa</context>
</contexts>
<marker>Chan, Ng, Chiang, 2007</marker>
<rawString>Y.S. Chan, H.T. Ng, and D. Chiang. 2007. Word sense disambiguation improves statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33– 40, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Diab</author>
<author>P Resnik</author>
</authors>
<title>An unsupervised method for word sense tagging using parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL,</booktitle>
<pages>255--262</pages>
<contexts>
<context position="5202" citStr="Diab and Resnik (2002)" startWordPosition="780" endWordPosition="783">e can infer that the English noun “bill” has at most four different senses. These different senses in turn can be grouped in case of synonymy. In the Dutch-French Europarl, for example, both “rekening” and “kosten”, are translated by the French “frais”, which might indicate that both Dutch words are synonymous. Several WSD studies are based on the idea of cross-lingual evidence. Gale et al. (1993) use a bilingual parallel corpus for the automatic creation of a sense-tagged data set, where target words in the source language are tagged with their translation of the word in the target language. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). The proposed</context>
</contexts>
<marker>Diab, Resnik, 2002</marker>
<rawString>M. Diab and P. Resnik. 2002. An unsupervised method for word sense tagging using parallel corpora. In Proceedings of ACL, pages 255–262.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="2319" citStr="Fellbaum, 1998" startWordPosition="346" endWordPosition="347">been widely studied in computational linguistics. For a recent overview of WSD algorithms, resources and applications, we refer to Agirre and Edmonds (2006) and Navigli (2009). Semantic evaluation competitions such as Senseval1 and its successor Semeval revealed that supervised approaches to WSD usually achieve better results than unsupervised methods (M`arquez et al., 2006). The former use machine learning techniques to induce a classifier from manually sense-tagged data, where each occurrence of a polysemous word gets assigned a sense label from a predefined sense inventory such as WordNet (Fellbaum, 1998). These supervised methods, however, heavily rely on large sensetagged corpora which are very time consuming and expensive to build. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrat</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William A Gale</author>
<author>Kenneth W Church</author>
</authors>
<title>A program for aligning sentences in bilingual corpora.</title>
<date>1991</date>
<booktitle>In Computational Linguistics,</booktitle>
<pages>177--184</pages>
<contexts>
<context position="8283" citStr="Gale and Church (1991)" startWordPosition="1249" endWordPosition="1252">e/ ´etablissement de cr´edit, Bank/ Kreditinstitut, banca, banco Table 2: Example of multi-lingual sense labels for the English noun bank 2.1 Corpus and word selection The document collection which serves as the basis for the gold standard construction and system evaluation is the Europarl parallel corpus2, which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus: English (our target language), Dutch, French, German, Italian and Spanish. All sentences are aligned using a tool based on the Gale and Church (1991) algorithm. We only consider the 1-1 sentence alignments between English and the five other languages (see also Tufis et al. (2004) for a similar strategy). These 1-1 alignments will be made available to all task participants. Participants are free to use other training corpora, but additional translations which are not present in Europarl will not be included in the sense inventory that is used for evaluation. For the competition, two data sets will be developed. The development and test sentences will be selected from the JRC-ACQUIS Multilingual Parallel Corpus3. The development data set con</context>
</contexts>
<marker>Gale, Church, 1991</marker>
<rawString>William A. Gale and Kenneth W. Church. 1991. A program for aligning sentences in bilingual corpora. In Computational Linguistics, pages 177–184.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>Estimating upper and lower bounds on the performance of word-sense disambiguation programs.</title>
<date>1992</date>
<booktitle>In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>249--256</pages>
<contexts>
<context position="2539" citStr="Gale et al., 1992" startWordPosition="377" endWordPosition="380"> Senseval1 and its successor Semeval revealed that supervised approaches to WSD usually achieve better results than unsupervised methods (M`arquez et al., 2006). The former use machine learning techniques to induce a classifier from manually sense-tagged data, where each occurrence of a polysemous word gets assigned a sense label from a predefined sense inventory such as WordNet (Fellbaum, 1998). These supervised methods, however, heavily rely on large sensetagged corpora which are very time consuming and expensive to build. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrated that for instance Statistical Machine Translation (SMT) benefits from incorporating a dedicated WSD module (Chan et al., 2007; Carpuat and Wu, 2007). Using translations from a corpus instead of human-defined sense lab</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>W.A. Gale, K. Church, and D. Yarowsky. 1992. Estimating upper and lower bounds on the performance of word-sense disambiguation programs. In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, pages 249–256.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W A Gale</author>
<author>K W Church</author>
<author>D Yarowsky</author>
</authors>
<title>A method for disambiguating word senses in a large corpus.</title>
<date>1993</date>
<booktitle>In Computers and the Humanities,</booktitle>
<volume>26</volume>
<pages>415--439</pages>
<contexts>
<context position="4980" citStr="Gale et al. (1993)" startWordPosition="743" endWordPosition="746">legislation”) (77 occ.). If we make the simplifying assumption for our example that (i) these are the only Dutch translations of our focus word and that (ii) all sense distinctions of “bill” are lexicalized in Dutch, we can infer that the English noun “bill” has at most four different senses. These different senses in turn can be grouped in case of synonymy. In the Dutch-French Europarl, for example, both “rekening” and “kosten”, are translated by the French “frais”, which might indicate that both Dutch words are synonymous. Several WSD studies are based on the idea of cross-lingual evidence. Gale et al. (1993) use a bilingual parallel corpus for the automatic creation of a sense-tagged data set, where target words in the source language are tagged with their translation of the word in the target language. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equiva</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1993</marker>
<rawString>W.A. Gale, K.W. Church, and D. Yarowsky. 1993. A method for disambiguating word senses in a large corpus. In Computers and the Humanities, volume 26, pages 415–439.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>T Erjavec</author>
<author>D Tufis</author>
</authors>
<title>Sense discrimination with parallel corpora.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,</booktitle>
<pages>54--60</pages>
<contexts>
<context position="5423" citStr="Ide et al. (2002)" startWordPosition="811" endWordPosition="814">ated by the French “frais”, which might indicate that both Dutch words are synonymous. Several WSD studies are based on the idea of cross-lingual evidence. Gale et al. (1993) use a bilingual parallel corpus for the automatic creation of a sense-tagged data set, where target words in the source language are tagged with their translation of the word in the target language. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). The proposed Cross-lingual Word Sense Disambiguation task differs from earlier work (e.g. Ide et al. (2002)) through its independence from an externally defined sense set. The remainder of this paper is organized as follows. In Secti</context>
</contexts>
<marker>Ide, Erjavec, Tufis, 2002</marker>
<rawString>N. Ide, T. Erjavec, and D. Tufis. 2002. Sense discrimination with parallel corpora. In Proceedings of ACL Workshop on Word Sense Disambiguation: Recent Successes and Future Directions, pages 54–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Koehn</author>
</authors>
<title>Europarl: A parallel corpus for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the MT Summit.</booktitle>
<contexts>
<context position="8050" citStr="Koehn, 2005" startWordPosition="1214" endWordPosition="1215"> to people living on the [bank] of the river Language Sense label NL,F,D,I,ES oever/dijk, rives/rivage/bord/bords, Ufer, riva, orilla The [bank] of Scotland ... Language Sense label NL,F,D,I,ES bank/kredietinstelling, banque/ ´etablissement de cr´edit, Bank/ Kreditinstitut, banca, banco Table 2: Example of multi-lingual sense labels for the English noun bank 2.1 Corpus and word selection The document collection which serves as the basis for the gold standard construction and system evaluation is the Europarl parallel corpus2, which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus: English (our target language), Dutch, French, German, Italian and Spanish. All sentences are aligned using a tool based on the Gale and Church (1991) algorithm. We only consider the 1-1 sentence alignments between English and the five other languages (see also Tufis et al. (2004) for a similar strategy). These 1-1 alignments will be made available to all task participants. Participants are free to use other training corpora, but additional translations which are not present in Europarl will not be included in t</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>P. Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of the MT Summit.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L M`arquez</author>
<author>G Escudero</author>
<author>D Martinez</author>
<author>G Rigau</author>
</authors>
<title>Supervised corpus-based methods for WSD.</title>
<date>2006</date>
<booktitle>Word Sense Disambiguation: Algorithms and Applications,</booktitle>
<pages>167--216</pages>
<editor>In E. Agirre and P. Edmonds, editors,</editor>
<publisher>Eds Springer,</publisher>
<location>New York, NY.</location>
<marker>M`arquez, Escudero, Martinez, Rigau, 2006</marker>
<rawString>L. M`arquez, G. Escudero, D. Martinez, and G. Rigau. 2006. Supervised corpus-based methods for WSD. In E. Agirre and P. Edmonds, editors, Word Sense Disambiguation: Algorithms and Applications, pages 167– 216. Eds Springer, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D McCarthy</author>
<author>R Navigli</author>
</authors>
<title>Semeval-2007 task 10: English lexical substitution task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007),</booktitle>
<pages>48--53</pages>
<contexts>
<context position="13859" citStr="McCarthy and Navigli, 2007" startWordPosition="2150" endWordPosition="2153">in a trilingual setting. 3 System evaluation As stated before, systems can participate in two tasks, i.e. systems can either participate in one or more bilingual evaluation tasks or they can participate in the multilingual evaluation task incorporating the five supported languages. The evaluation of the multilingual evaluation task is simply the average of the system scores on the five bilingual evaluation tasks. 3.1 Evaluation strategies For the evaluation of the participating systems we will use an evaluation scheme which is inspired by the English lexical substitution task in SemEval 2007 (McCarthy and Navigli, 2007). The evaluation will be performed using precision and recall (P and R in the equations that follow). We perform both a best result evaluation and a more relaxed evaluation for the top five results. Let H be the set of annotators, T be the set of test items and hi be the set of responses for an item i E T for annotator h E H. Let A be the set of items from T where the system provides at least one answer and ai : i E A be the set of guesses from the system for item i. For each i, we calculate the multiset union (Hi) for all hi for all h E H and for each unique type (res) in Hi that has an assoc</context>
</contexts>
<marker>McCarthy, Navigli, 2007</marker>
<rawString>D. McCarthy and R. Navigli. 2007. Semeval-2007 task 10: English lexical substitution task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 48–53.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
</authors>
<title>Word sense disambiguation: a survey. In</title>
<date>2009</date>
<journal>ACM Computing Surveys,</journal>
<volume>41</volume>
<pages>1--69</pages>
<contexts>
<context position="1879" citStr="Navigli (2009)" startWordPosition="282" endWordPosition="283">lingual evaluation subtasks (English - Dutch, English - German, etc.) and in a multilingual subtask covering all language pairs. As WSD from cross-lingual evidence is gaining popularity, we believe it is important to create a multilingual gold standard and run cross-lingual WSD benchmark tests. 1 Introduction The Word Sense Disambiguation (WSD) task, which consists in selecting the correct sense of a given word in a given context, has been widely studied in computational linguistics. For a recent overview of WSD algorithms, resources and applications, we refer to Agirre and Edmonds (2006) and Navigli (2009). Semantic evaluation competitions such as Senseval1 and its successor Semeval revealed that supervised approaches to WSD usually achieve better results than unsupervised methods (M`arquez et al., 2006). The former use machine learning techniques to induce a classifier from manually sense-tagged data, where each occurrence of a polysemous word gets assigned a sense label from a predefined sense inventory such as WordNet (Fellbaum, 1998). These supervised methods, however, heavily rely on large sensetagged corpora which are very time consuming and expensive to build. This phenomenon, well known</context>
</contexts>
<marker>Navigli, 2009</marker>
<rawString>R. Navigli. 2009. Word sense disambiguation: a survey. In ACM Computing Surveys, volume 41, pages 1–69.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>B Wang</author>
<author>Y S Chan</author>
</authors>
<title>Exploiting parallel texts for word sense disambiguation: An empirical study.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>455--462</pages>
<location>Santa Cruz.</location>
<contexts>
<context position="5788" citStr="Ng et al., 2003" startWordPosition="862" endWordPosition="865">nguage. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). The proposed Cross-lingual Word Sense Disambiguation task differs from earlier work (e.g. Ide et al. (2002)) through its independence from an externally defined sense set. The remainder of this paper is organized as follows. In Section 2, we present a detailed description of the cross-lingual WSD task. It introduces the parallel corpus we used, informs on the development and test data and discusses the annotation procedure. Section 3 gives an overview of the different scoring strategies that will be applied. Section 4 concludes this paper. 2 Task set up The cross-lingual Word Sense Disambigu</context>
</contexts>
<marker>Ng, Wang, Chan, 2003</marker>
<rawString>H.T. Ng, B. Wang, and Y.S. Chan. 2003. Exploiting parallel texts for word sense disambiguation: An empirical study. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages 455–462, Santa Cruz.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="10186" citStr="Och and Ney, 2003" startWordPosition="1558" endWordPosition="1561">They are not bound to a predefined parallel corpus, but can freely choose the translations from any available resource. Selecting the target words from the set of nouns thats will be used for the Lexical Substitution Task should make it easier for systems to participate in both tasks. 2.2 Manual annotation The sense inventory for the 5 target nouns in the development data and the 20 nouns in the test data is manually built up in three steps. 1. In the first annotation step, the 5 translations of the English word are identified per sentence ID. In order to speed up this identification, GIZA++ (Och and Ney, 2003) is used to generate the initial word alignments for the 5 languages. All word alignments are manually verified. In this step, we might come across multiword translations, especially in Dutch and German which tend to glue parts of compounds together in one orthographic unit. We decided to keep these translations as such, even if they do not correspond exactly to the English target word. In following sentence, the Dutch translation witboek corresponds in fact to the English compound white paper, and not to the English target word paper: English: the European Commission presented its white paper</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>F.J. Och and H. Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Tufis¸</author>
<author>Radu Ion</author>
<author>Nancy Ide</author>
</authors>
<title>Fine-Grained Word Sense Disambiguation Based on Parallel Corpora, Word Alignment, Word Clustering and Aligned Wordnets.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004),</booktitle>
<pages>1312--1318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Geneva, Switzerland,</location>
<marker>Tufis¸, Ion, Ide, 2004</marker>
<rawString>Dan Tufis¸, Radu Ion, and Nancy Ide. 2004. Fine-Grained Word Sense Disambiguation Based on Parallel Corpora, Word Alignment, Word Clustering and Aligned Wordnets. In Proceedings of the 20th International Conference on Computational Linguistics (COLING 2004), pages 1312–1318, Geneva, Switzerland, August. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>