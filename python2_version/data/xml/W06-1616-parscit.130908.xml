<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000025">
<title confidence="0.9834515">
Incremental Integer Linear Programming for Non-projective Dependency
Parsing
</title>
<author confidence="0.998814">
Sebastian Riedel and James Clarke
</author>
<affiliation confidence="0.812402">
School of Informatics, University of Edinburgh
2 Bucclecuch Place, Edinburgh EH8 9LW, UK
</affiliation>
<email confidence="0.99744">
s.r.riedel@sms.ed.ac.uk,jclarke@ed.ac.uk
</email>
<sectionHeader confidence="0.997371" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999681352941176">
Integer Linear Programming has recently
been used for decoding in a number of
probabilistic models in order to enforce
global constraints. However, in certain ap-
plications, such as non-projective depen-
dency parsing and machine translation,
the complete formulation of the decod-
ing problem as an integer linear program
renders solving intractable. We present an
approach which solves the problem in-
crementally, thus we avoid creating in-
tractable integer linear programs. This ap-
proach is applied to Dutch dependency
parsing and we show how the addition
of linguistically motivated constraints can
yield a significant improvement over state-
of-the-art.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999746254545455">
Many inference algorithms require models to
make strong assumptions of conditional indepen-
dence between variables. For example, the Viterbi
algorithm used for decoding in conditional ran-
dom fields requires the model to be Markovian.
Strong assumptions are also made in the case of
McDonald et al.’s (2005b) non-projective depen-
dency parsing model. Here attachment decisions
are made independently of one another&apos;. However,
often such assumptions can not be justified. For
example in dependency parsing, if a subject has
already been identified for a given verb, then the
probability of attaching a second subject to the
verb is zero. Similarly, if we find that one coor-
dination argument is a noun, then the other argu-
&apos;If we ignore the constraint that dependency trees must be
cycle-free (see sections 2 and 3 for details).
ment cannot be a verb. Thus decisions are often
co-dependent.
Integer Linear Programming (ILP) has recently
been applied to inference in sequential condi-
tional random fields (Roth and Yih, 2004), this
has allowed the use of truly global constraints
during inference. However, it is not possible to
use this approach directly for a complex task like
non-projective dependency parsing due to the ex-
ponential number of constraints required to pre-
vent cycles occurring in the dependency graph.
To model all these constraints explicitly would re-
sult in an ILP formulation too large to solve effi-
ciently (Williams, 2002). A similar problem also
occurs in an ILP formulation for machine transla-
tion which treats decoding as the Travelling Sales-
man Problem (Germann et al., 2001).
In this paper we present a method which extends
the applicability of ILP to a more complex set of
problems. Instead of adding all the constraints we
wish to capture to the formulation, we first solve
the program with a fraction of the constraints. The
solution is then examined and, if required, addi-
tional constraints are added. This procedure is re-
peated until all constraints are satisfied. We apply
this dependency parsing approach to Dutch due
to the language’s non-projective nature, and take
the parser of McDonald et al. (2005b) as a starting
point for our model.
In the following section we introduce depen-
dency parsing and review previous work. In Sec-
tion 3 we present our model and formulate it as
an ILP problem with a set of linguistically mo-
tivated constraints. We include details of an in-
cremental algorithm used to solve this formula-
tion. Our experimental set-up is provided in Sec-
tion 4 and is followed by results in Section 5 along
with runtime experiments. We finally discuss fu-
</bodyText>
<page confidence="0.974526">
129
</page>
<note confidence="0.991992">
Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 129–137,
Sydney, July 2006. c�2006 Association for Computational Linguistics
</note>
<figureCaption confidence="0.8513035">
Figure 1: A Dutch dependency tree for “I’ll come
at twelve and then you’ll get what you deserve”
</figureCaption>
<bodyText confidence="0.976497">
ture research and potential improvements to our
approach.
</bodyText>
<sectionHeader confidence="0.994161" genericHeader="introduction">
2 Dependency Parsing
</sectionHeader>
<bodyText confidence="0.999527853333334">
Dependency parsing is the task of attaching words
to their arguments. Figure 1 shows a dependency
graph for the Dutch sentence “I’ll come at twelve
and then you’ll get what you deserve” (taken from
the Alpino Corpus (van der Beek et al., 2002)). In
this dependency graph the verb “kom” is attached
to its subject “ik”. “kom” is referred to as the head
of the dependency and “ik” as its child. In labelled
dependency parsing edges between words are la-
belled with the relation captured. In the case of
the dependency between “kom” and “ik” the label
would be “subject”.
In a dependency tree every token must be the
child of exactly one other node, either another to-
ken or the dummy root node. By definition, a de-
pendency tree is free of cycles. For example, it
must not contain dependency chains such as “en”
__+ “kom” __+ “ik” —* “en”. For a more formal def-
inition see previous work (Nivre et al., 2004).
An important distinction between dependency
trees is whether they are projective or non-
projective. Figure 1 is an example of a projec-
tive dependency tree, in such trees dependencies
do not cross. In Dutch and other flexible word or-
der languages such as German and Czech we also
encounter non-projective trees, in these cases the
trees contain crossing dependencies.
Dependency parsing is useful for applications
such as relation extraction (Culotta and Sorensen,
2004) and machine translation (Ding and Palmer,
2005). Although less informative than lexicalised
phrase structures, dependency structures still cap-
ture most of the predicate-argument information
needed for applications. It has the advantage of be-
ing more efficient to learn and parse.
McDonald et al. (2005a) introduce a depen-
dency parsing framework which treats the task as
searching for the projective tree that maximises
the sum of local dependency scores. This frame-
Figure 2: An incorrect partial dependency tree.
The verb “krijg” is incorrectly coordinated with
the preposition “om”.
work is efficient and has also been extended to
non-projective trees (McDonald et al., 2005b). It
provides a discriminative online learning algo-
rithm which when combined with a rich feature set
reaches state-of-the-art performance across multi-
ple languages.
However, within this framework one can only
define features over single attachment decisions.
This leads to cases where basic linguistic con-
straints are not satisfied (e.g. verbs with two sub-
jects or incompatible coordination arguments). An
example of this for Dutch is illustrated in Figure 2
which was produced by the parser of McDonald
et al. (2005b). Here the parse contains a coordi-
nation of incompatible word classes (a preposition
and a verb).
Our approach is able to include additional con-
straints which forbid configurations such as those
in Figure 2. While McDonald and Pereira (2006)
address the issue of local attachment decisions by
defining scores over attachment pairs, our solution
is more general. Furthermore, it is complementary
in the sense that we could formulate their model
using ILP and then add constraints.
The method we present is not the only one that
can take global constraints into account. Deter-
ministic dependency parsing (Nivre et al., 2004;
Yamada and Matsumoto, 2003) can apply global
constraints by conditioning attachment decisions
on the intermediate parse built. However, for effi-
ciency a greedy search is used which may produce
sub-optimal solutions. This is not the case when
using ILP.
</bodyText>
<sectionHeader confidence="0.992691" genericHeader="method">
3 Model
</sectionHeader>
<bodyText confidence="0.9960775">
Our underlying model is a modified labelled ver-
sion2 of McDonald et al. (2005b):
</bodyText>
<equation confidence="0.929763">
s(x, y) _ � s(i, j, l)
(i,j,l)Ey
�� w · f(i, j, l)
(i,j,l)Ey
</equation>
<footnote confidence="0.820984">
2Note that this is not described in the McDonald papers
but implemented in his software.
</footnote>
<page confidence="0.99675">
130
</page>
<bodyText confidence="0.999048166666667">
where x is a sentence, y is a set of labelled de-
pendencies, f(i, j, l) is a multidimensional fea-
ture vector representation of the edge from token
i to token j with label l and w the correspond-
ing weight vector. For example, a feature f101 in f
could be:
</bodyText>
<equation confidence="0.986780333333333">
{ 1 if t(i) = “en” ∧ p(j) = V
∧l = “coordination”
0 otherwise
</equation>
<bodyText confidence="0.995853">
where t(i) is the word at token i and p(j) the part-
of-speech tag at token j.
Decoding in this model amounts to finding the
y for a given x that maximises s(x, y):
</bodyText>
<equation confidence="0.9974825">
y� = arg max s(x, y)
Y
</equation>
<bodyText confidence="0.990456651162791">
while fulfilling the following constraints:
T1 For every non-root token in x there exists ex-
actly one head; the root token has no head.
T2 There are no cycles.
Thus far, the formulation follows McDonald
et al. (2005b) and corresponds to the Maximum
Spanning Tree (MST) problem. In addition to T1
and T2, we include a set of linguistically moti-
vated constraints:
A1 Heads are not allowed to have more than one
outgoing edge labelled l for all l in a set of
labels U.
C1 In a symmetric coordination there is exactly
one argument to the right of the conjunction
and at least one argument to the left.
C2 In an asymmetric coordination there are no ar-
guments to the left of the conjunction and at
least two arguments to the right.
C3 There must be at least one comma between
subsequent arguments to the left of a sym-
metric coordination.
C4 Arguments of a coordination must have com-
patible word classes.
P1 Two dependencies must not cross if one of
their labels is in a set of labels P.
A1 covers constraints such as “there can only
be one subject” if U contains “subject” (see Sec-
tion 4.4 for more details of U). C1 applies to
configurations which contain conjunctions such as
“en”,“of” or “maar” (“and”, “or” and “but”). C2
will rule-out settings where a conjunction such as
“zowel” (translates as “both”) having arguments
to its left. C3 forces coordination arguments to
the left of a conjunction to have commas in be-
tween. C4 avoids parses in which incompatible
word classes are coordinated, such as nouns and
verbs. Finally, P1 allows selective projective pars-
ing: we can, for instance, forbid the crossing of
“Noun-Determiner” dependencies if we add the
corresponding label type to P(see Section 4.4 for
more details of P) . If we extend P to contain all
labels we forbid any type of crossing dependen-
cies. This corresponds to projective parsing.
</bodyText>
<subsectionHeader confidence="0.998493">
3.1 Decoding
</subsectionHeader>
<bodyText confidence="0.999982617647059">
McDonald et al. (2005b) use the Chu-Liu-
Edmonds (CLE) algorithm to solve the maxi-
mum spanning tree problem. However, global con-
straints cannot be incorporated into the CLE algo-
rithm (McDonald et al., 2005b). We alleviate this
problem by presenting an equivalent Integer Lin-
ear Programming formulation which allows us to
incorporate global constraints naturally.
Before giving full details of our formulation
we first introduce some of the concepts of lin-
ear and integer linear programming (for a more
thorough introduction see Winston and Venkatara-
manan (2003)).
Linear Programming (LP) is a tool for solving
optimisation problems in which the aim is to max-
imise (or minimise) a given linear function with
respect to a set of linear constraints. The func-
tion to be maximised (or minimised) is referred
to as the objective function. A number of decision
variables are under our control which exert influ-
ence on the objective function. Specifically, they
have to be optimised in order to maximise (or min-
imise) the objective function. Finally, a set of con-
straints restrict the values that the decision vari-
ables can take. Integer Linear Programming is an
extension of linear programming where all deci-
sion variables must take integer values.
There are several explicit formulations of the
MST problem as an integer linear program in the
literature (Williams, 2002). They are based on
the concept of eliminating subtours (cycles), cuts
(disconnections) or requiring intervertex flows
(paths). However, in practice these formulations
cause long solve times — as the first two meth-
</bodyText>
<equation confidence="0.874987">
f101(i, j, l) =
</equation>
<page confidence="0.953857">
131
</page>
<bodyText confidence="0.9547755">
Algorithm 1 Incremental Integer Linear Program-
ming
</bodyText>
<equation confidence="0.994164">
C +— Bx
repeat
y +— solve(C, Ox, Vx)
W +— violated(y, Ix)
C+—CUW
until V = 0
return y
</equation>
<bodyText confidence="0.999252191489362">
ods yield an exponential number of constraints.
Although the latter scales cubically, it produces
non-fractional solutions in its relaxed version; this
causes long runtimes for the branch and bound al-
gorithm (Williams, 2002) commonly used in inte-
ger linear programming. We found out experimen-
tally that dependency parsing models of this form
do not converge on a solution after multiple hours
of solving, even for small sentences.
As a workaround for this problem we follow an
incremental approach akin to the work of Warme
(1998). Instead of adding constraints which forbid
all possible cycles in advance (this would result
in an exponential number of constraints) we first
solve the problem without any cycle constraints.
The solution is then examined for cycles, and if
cycles are found we add constraints to forbid these
cycles; the solver is then run again. This process
is repeated until no more violated constraints are
found. The same procedure is used for other types
of constraints which are too expensive to add in
advance (e.g. the constraints of P1).
Algorithm 1 outlines our approach. For a sen-
tence x, Bx is the set of constraints that we add
in advance and Ix are the constraints we add in-
crementally. Ox is the objective function and Vx
is a set of variables including integer declarations.
solve(C, O, V ) maximises the objective function
O with respect to the set of constraints C and vari-
ables V . violated(y, I) inspects the proposed so-
lution (y) and returns all constraints in I which are
violated.
The number of iterations required in this ap-
proach is at most polynomial with respect to the
number of variables (Gr¨otschel et al., 1981). In
practice, this technique converges quickly (less
than 20 iterations in 99% of approximately 12,000
sentences), yielding average solve times of less
than 0.5 seconds.
Our approach converges quickly due to the
quality of the scoring function. Its weights have
been trained on cycle free data, thus it is more
likely to guide the search to a cycle free solution.
In the following section we present the objec-
tive function Ox, variables Vx and linear con-
straints Bx and Ix needed for parsing x using Al-
gorithm 1.
</bodyText>
<subsectionHeader confidence="0.611111">
3.1.1 Variables
</subsectionHeader>
<bodyText confidence="0.753083">
Vx contains a set of binary variables to represent
labelled edges:
</bodyText>
<equation confidence="0.56001">
ei,j,l Vi E 0..n, j E 1..n,
l E bestk(i, j)
</equation>
<bodyText confidence="0.996533692307692">
where n is the number of tokens and the index 0
represents the root token. bestk(i, j) is the set of k
labels with highest s(i, j, l). ei,j,l equals 1 if there
is a edge (i.e., a dependency) with the label l be-
tween token i (head) and j (child), 0 otherwise. k
depends on the type of constraints we want to add.
For the plain MST problem it is sufficient to set
k = 1 and only take the best scoring label for each
token pair. However, if we want a constraint which
forbids duplicate subjects we need to provide ad-
ditional labels to fall back on.
Vx also contains a set of binary auxiliary vari-
ables:
</bodyText>
<equation confidence="0.457363">
di,j Vi E 0..n, j E 1..n
</equation>
<bodyText confidence="0.978827">
which represent the existence of a dependency be-
tween tokens i and j. We connect these to the ei,j,l
variables by the constraint:
</bodyText>
<equation confidence="0.977944">
Edi,j = ei,j,l
lEbestk(i,j)
</equation>
<subsubsectionHeader confidence="0.482814">
3.1.2 Objective Function
</subsubsectionHeader>
<bodyText confidence="0.9947305">
Given the above variables our objective function
Ox can be represented as (using a suitable k):
</bodyText>
<equation confidence="0.957867">
E E s(i, j, l) · ei,j,l
i,j lEbestk(i,j)
</equation>
<subsectionHeader confidence="0.958312">
3.1.3 Base Constraints
</subsectionHeader>
<bodyText confidence="0.97490125">
We first introduce a set of base constraints Bx
which we add in advance.
Only One Head (T1) Every token has exactly
one head:
</bodyText>
<equation confidence="0.983747">
E di,j = 1
i
</equation>
<bodyText confidence="0.9994885">
for non-root tokens j &gt; 0 in x. An exception is
made for the artificial root node:
</bodyText>
<equation confidence="0.9863965">
E di,0 = 0
i
</equation>
<page confidence="0.964606">
132
</page>
<bodyText confidence="0.989970333333333">
Label Uniqueness (A1) To enforce uniqueness
of children with labels in U we augment our model
with the constraint:
</bodyText>
<equation confidence="0.984515">
X ei,j,l ≤ 1
j
</equation>
<bodyText confidence="0.94184075">
for every token i in x and label l in U.
Symmetric Coordination (C1) For each con-
junction token i which forms a symmetric coor-
dination we add:
</bodyText>
<equation confidence="0.951585">
X di,j ≥ 1
jGi
</equation>
<bodyText confidence="0.512828">
and
</bodyText>
<equation confidence="0.941432">
X di,j = 1
j&gt;i
</equation>
<bodyText confidence="0.960064333333333">
Asymmetric Coordination (C2) For each con-
junction token i which forms an asymmetric coor-
dination we add:
</bodyText>
<equation confidence="0.925674">
X di,j = 0
jGi
</equation>
<bodyText confidence="0.617569">
and
</bodyText>
<equation confidence="0.6030775">
X di,j ≥ 2
j&gt;i
</equation>
<sectionHeader confidence="0.439077" genericHeader="method">
3.1.4 Incremental Constraints
</sectionHeader>
<bodyText confidence="0.998883909090909">
Now we present the set of constraints IX we add
incrementally. The constraints are chosen based on
the two criteria: (1) adding them to the base con-
straints (those added in advance) would result in
an extremely large program, and (2) it must be ef-
ficient to detect whether the constraint is violated
in y.
No Cycles (T2) For every possible cycle c for
the sentence x we have a constraint which forbids
the case where all edges in c are active simultane-
ously:
</bodyText>
<equation confidence="0.9723075">
X di,j ≤ |c |− 1
(i,j)Ec
</equation>
<bodyText confidence="0.9865274">
Comma Coordination (C3) For each symmet-
ric conjunction token i which forms a symmetric
coordination and each set of tokens A in x to the
left of i with no comma between each pair of suc-
cessive tokens we add:
</bodyText>
<equation confidence="0.835864">
X di,a ≤ |A |− 1
aEA
</equation>
<bodyText confidence="0.904868857142857">
which forbids configurations where i has the argu-
ment tokens A.
Compatible Coordination Arguments (C4)
For each conjunction token i and each set of to-
kens A in x with incompatible POS tags, we add a
constraint to forbid configurations where i has the
argument tokens A.
</bodyText>
<equation confidence="0.9345445">
X di,a ≤ |A |− 1
aEA
</equation>
<bodyText confidence="0.963718333333333">
Selective Projective Parsing (P1) For each pair
of triplets (i, j, l1) and (m, n, l2) we add the con-
straint:
</bodyText>
<equation confidence="0.6769525">
ei,j,l1 + em,n,l2 ≤ 1
if l1 or l2 is in P.
</equation>
<subsectionHeader confidence="0.995918">
3.2 Training
</subsectionHeader>
<bodyText confidence="0.999971428571429">
For training we use single-best MIRA (McDon-
ald et al., 2005a). This is an online algorithm that
learns by parsing each sentence and comparing
the result with a gold standard. Training is com-
plete after multiple passes through the whole cor-
pus. Thus we decode using the Chu-Liu-Edmonds
(CLE) algorithm due to its speed advantage over
ILP (see Section 5.2 for a detailed comparison of
runtimes).
The fact that we decode differently during train-
ing (using CLE) and testing (using ILP) may de-
grade performance. In the presence of additional
constraints weights may be able to capture other
aspects of the data.
</bodyText>
<sectionHeader confidence="0.999708" genericHeader="method">
4 Experimental Set-up
</sectionHeader>
<bodyText confidence="0.9944685">
Our experiments were designed to answer the fol-
lowing questions:
</bodyText>
<listItem confidence="0.910684625">
1. How much do our additional constraints help
improve accuracy?
2. How fast is our generic inference method in
comparison with the Chu-Liu-Edmonds algo-
rithm?
3. Can approximations be used to increase the
speed of our method while remaining accu-
rate?
</listItem>
<bodyText confidence="0.99998025">
Before we try to answer these questions we briefly
describe our data, features used, settings for U and
P in our parametric constraints, our working envi-
ronment and our implementation.
</bodyText>
<page confidence="0.997923">
133
</page>
<subsectionHeader confidence="0.966333">
4.1 Data
</subsectionHeader>
<bodyText confidence="0.999332454545455">
We use the Alpino treebank (van der Beek et al.,
2002), taken from the CoNLL shared task of mul-
tilingual dependency parsing3. The CoNLL data
differs slightly from the original Alpino treebank
as the corpus has been part-of-speech tagged using
a Memory-Based-Tagger (Daelemans et al., 1996).
It consists of 13,300 sentences with an average
length of 14.6 tokens. The data is non-projective,
more specifically 5.4% of all dependencies are
crossed by at least one other dependency. It con-
tains approximately 6000 sentences more than the
Alpino corpus used by Malouf and van Noord
(2004).
The training set was divided into a 10% devel-
opment set (dev) while the remaining 90% is used
as a training and cross-validation set (cross). Fea-
ture sets, constraints and training parameters were
selected through training on cross and optimising
against dev.
Our final accuracy scores and runtime eval-
uations were acquired using a nine-fold cross-
validation on cross
</bodyText>
<subsectionHeader confidence="0.994656">
4.2 Environment and Implementation
</subsectionHeader>
<bodyText confidence="0.999957666666667">
All our experiments were conducted on a Intel
Xeon with 3.8 Ghz and 4Gb of RAM. We used
the open source Mixed Integer Programming li-
brary lp solve4 to solve the Integer Linear Pro-
grams. Our code ran in Java and called the JNI-
wrapper around the lp solve library.
</bodyText>
<subsectionHeader confidence="0.998966">
4.3 Feature Sets
</subsectionHeader>
<bodyText confidence="0.999838357142857">
Our feature set was determined through experi-
mentation with the development set. The features
are based upon the data provided within the Alpino
treebank. Along with POS tags the corpus contains
several additional attributes such as gender, num-
ber and case.
Our best results on the development set were
achieved using the feature set of McDonald et al.
(2005a) and a set of features based on the addi-
tional attributes. These features combine the at-
tributes of the head with those of the child. For
example, if token i has the attributes a1 and a2,
and token j has the attribute a3 then we created
the features (a1 ∧ a3) and (a2 ∧ a3).
</bodyText>
<footnote confidence="0.95042075">
3For details see http://nextens.uvt.nl/
˜conll.
4The software is available from http://www.
geocities.com/lpsolve.
</footnote>
<subsectionHeader confidence="0.986638">
4.4 Constraints
</subsectionHeader>
<bodyText confidence="0.9999596">
All the constraints presented in Section 3 were
used in our model. The set U of unique labels
constraints contained su, obj1, obj2, sup, ld, vc,
predc, predm, pc, pobj1, obcomp and body. Here
su stands for subject and obj1 for direct object (for
full details see Moortgat et al. (2000)).
The set of projective labels P contained cnj,
for coordination dependencies; and det, for de-
terminer dependencies. One exception was added
for the coordination constraint: dependencies can
cross when coordinated arguments are verbs.
One drawback of hard deterministic constraints
is the undesirable effect noisy data can cause. We
see this most prominently with coordination argu-
ment compatibility. Words ending in “en” are typ-
ically wrongly tagged and cause our coordination
argument constraint to discard correct coordina-
tions. As a workaround we assigned words ending
in “en” a wildcard POS tag which is compatible
with all POS tags.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.9996846">
In this section we report our results. We not only
present our accuracy but also provide an empiri-
cal evaluation of the runtime behaviour of this ap-
proach and show how parsing can be accelerated
using a simple approximation.
</bodyText>
<subsectionHeader confidence="0.9967">
5.1 Accuracy
</subsectionHeader>
<bodyText confidence="0.9999355">
An important question to answer when using
global constraints is: How much of a performance
boost is gained when using global constraints?
We ran the system without any linguistic con-
straints as a baseline (bl) and compared it to a
system with the additional constraints (cnstr). To
evaluate our systems we use the accuracy over la-
belled attachment decisions:
</bodyText>
<equation confidence="0.936426">
Nl
LAC = Nt
</equation>
<bodyText confidence="0.999207">
where Nl is the number of tokens with correct
head and label and Nt is the total number of to-
kens. For completeness we also report the unla-
belled accuracy:
</bodyText>
<equation confidence="0.8909505">
Nu
UAC = Nt
</equation>
<bodyText confidence="0.994418">
where Nu is the number of tokens with correct
head.
</bodyText>
<page confidence="0.995061">
134
</page>
<table confidence="0.998432666666667">
LAC UAC LC UC
bl 84.6% 88.9% 27.7% 42.2%
cnstr 85.1% 89.4% 29.7% 43.8%
</table>
<tableCaption confidence="0.998507">
Table 1: Labelled (LAC) and unlabelled (UAC) ac-
</tableCaption>
<bodyText confidence="0.995655245614035">
curacy using nine-fold cross-validation on cross
for baseline (bl) and constraint-based (constr) sys-
tem. LC and UC are the percentages of sentences
with 100% labelled and unlabelled accuracy, re-
spectively.
Table 1 shows our results using nine-fold cross-
validation on the cross set. The baseline system
(no additional constraints) gives an unlabelled ac-
curacy of 84.6% and labelled accuracy of 88.9%.
When we add our linguistic constraints the per-
formance increases by 0.5% for both labelled and
unlabelled accuracy. This increase is significant
(p &lt; 0.001) according to Dan Bikel’s parse com-
parison script and using the Sign test (p &lt; 0.001).
Now we give a little insight into how our results
compare with the rest of the community. The re-
ported state-of-the-art parser of Malouf and van
Noord (2004) achieves 84.4% labelled accuracy
which is very close numerically to our baseline.
However, they use a subset of the CoNLL Alpino
treebank with a higher average number of tokens
per sentences and also evaluate control relations,
thus results are not directly comparable. We have
also run our parser on the relatively small (approx-
imately 400 sentences) CoNNL test data. The best
performing system (McDonald et al. 2006; note:
this system is different to our baseline) achieves
79.2% labelled accuracy while our baseline sys-
tem achieves 78.6% and our constrained version
79.8%. However, a significant difference is only
observed between our baseline and our constraint-
based system.
Examining the errors produced using the dev
set highlight two key reasons why we do not see
a greater improvement using our constraint-based
system. Firstly, we cannot improve on coordina-
tions that include words ending with “en” based on
the workaround present in Section 4.4. This prob-
lem can only be solved by improving POS taggers
for Dutch or by performing POS tagging within
the dependency parsing framework.
Secondly, our system suffers from poor next
best solutions. That is, if the best solution violates
some constraints, then we find the next best solu-
tion is typically worse than the best solution with
violated constraints. This appears to be a conse-
quence of inaccurate local score distributions (as
opposed to inaccurate best local scores). For ex-
ample, suppose we attach two subjects, t1 and t2,
to a verb, where t1 is the actual subject while t2
is meant to be labelled as object. If we forbid this
configuration (two subjects) and if the score of la-
belling t1 object is higher than that for t2 being
labelled subject, then the next best solution will
label t1 incorrectly as object and t2 incorrectly as
subject. This is often the case, and thus results in a
drop of accuracy.
</bodyText>
<subsectionHeader confidence="0.999697">
5.2 Runtime Evaluation
</subsectionHeader>
<bodyText confidence="0.999955413793103">
We now concentrate on the runtime of our method.
While we expect a longer runtime than using the
Chu-Liu-Edmonds as in previous work (McDon-
ald et al., 2005b), we are interested in how large
the increase is.
Table 2 shows the average solve time (ST) for
sentences with respect to the number of tokens in
each sentence for our system with constraints (cn-
str) and the Chu-Liu-Edmonds (CLE) algorithm.
All solve times do not include feature extraction
as this is identical for all systems. For cnstr we
also show the number of sentences that could not
be parsed after two minutes, the average number
of iterations and the average duration of the first
iteration.
The results show that parsing using our generic
approach is still reasonably fast although signifi-
cantly slower than using the Chu-Liu-Edmonds al-
gorithm. Also, only a small number of sentences
take longer than two minutes to parse. Thus, in
practice we would not see a significant degrada-
tion in performance if we were to fall back on the
CLE algorithm after two minutes of solving.
When we examine the average duration of the
first iteration it appears that the majority of the
solve time is spent within this iteration. This could
be used to justify using the CLE algorithm to find
a initial solution as starting point for the ILP solver
(see Section 6).
</bodyText>
<subsectionHeader confidence="0.990996">
5.3 Approximation
</subsectionHeader>
<bodyText confidence="0.999784428571429">
Despite the fact that our parser can parse all sen-
tences in a reasonable amount of time, it is still sig-
nificantly slower than the CLE algorithm. While
this is not crucial during decoding, it does make
discriminative online training difficult as training
requires several iterations of parsing the whole
corpus.
</bodyText>
<page confidence="0.996592">
135
</page>
<table confidence="0.999701857142857">
Tokens 1-10 11-20 21-30 31-40 41-50 &gt;50
Count 5242 4037 1835 650 191 60
Avg. ST (CLE) 0.27ms 0.98ms 3.2ms 7.5ms 14ms 23ms
Avg. ST (cnstr) 5.6ms 52ms 460ms 1.5s 7.2s 33s
ST &gt; 120s (cnstr) 0 0 0 0 3 3
Avg. # iter. (cnstr) 2.08 2.87 4.48 5.82 8.40 15.17
Avg. ST 1st iter. (cnstr) 4.2ms 37ms 180ms 540ms 1.3s 2.6s
</table>
<tableCaption confidence="0.982846333333333">
Table 2: Runtime evaluation for different sentence lengths. Average solve time (ST) for our system
with constraints (constr), the Chu-Liu-Edmonds algorithm (CLE), number of sentences with solve times
greater than 120 seconds, average number of iterations and first iteration solve time.
</tableCaption>
<table confidence="0.997783666666667">
q=5 q=10 all CLE
LAC 84.90% 85.11% 85.14% 85.14%
ST 351s 760s 3640s 20s
</table>
<tableCaption confidence="0.998697">
Table 3: Labelled accuracy (LAC) and total solve
</tableCaption>
<bodyText confidence="0.99682004">
time (ST) for the cross dataset using varying q val-
ues and the Chu-Liu-Edmonds algorithm (CLE)
Thus we investigate if it is possible to speed up
our inference using a simple approximation. For
each token we now only consider the q variables
in VX with the highest scoring edges. For exam-
ple, if we set q = 2 the set of variables for a to-
ken j will contain two variables, either both for
the same head i but with different labels (variables
ei,j,l1 and ei,j,l2) or two distinct heads i1 and i2
(variables ei1,j,l1 and ei2,j,l2) where labels l1 and
l2 may be identical.
Table 3 shows the effect of different q values
on solve time for the full corpus cross (roughly
12,000 sentences) and overall accuracy. We see
that solve time can be reduced by 80% while only
losing a marginal amount of accuracy when we set
q to 10. However, we are unable to reach the 20
seconds solve time of the CLE algorithm. Despite
this, when we add the time for preprocessing and
feature extraction, the CLE system parses a cor-
pus in around 15 minutes whereas our system with
q = 10 takes approximately 25 minutes&apos;. When
we view the total runtime of each system we see
our system is more competitive.
</bodyText>
<sectionHeader confidence="0.999876" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.991816071428572">
While we have presented significant improve-
ments using additional constraints, one may won-
5Even when caching feature extraction during training
McDonald et al. (2005a) still takes approximately 10 minutes
to train.
der whether the improvements are large enough
to justify further research in this direction; espe-
cially since McDonald and Pereira (2006) present
an approximate algorithm which also makes more
global decisions. However, we believe that our ap-
proach is complementary to their model. We can
model higher order features by using an extended
set of variables and a modified objective function.
Although this is likely to increase runtime, it may
still be fast enough for real world applications. In
addition, it will allow exact inference, even in the
case of non-projective parsing. Also, we argue that
this approach has potential for interesting exten-
sions and applications.
For example, during our runtime evaluations we
find that a large fraction of solve time is spent in
the first iteration of our incremental algorithm. Af-
ter the first iteration the solver uses its last state to
efficiently search for solutions in the presence of
new constraints. Some solvers allow the specifica-
tion of an initial solution as a starting point, thus it
is expected that significant improvements in terms
of speed can be made by using the CLE algorithm
to provide an initial solution.
Our approach uses a generic algorithm to solve
a complex task. Thus other applications may ben-
efit from it. For instance, Germann et al. (2001)
present an ILP formulation of the Machine Trans-
lation (MT) decoding task in order to conduct ex-
act inference. However, their model suffers from
the same type of exponential blow-up we observe
when we add all our cycle constraints in advance.
In fact, the constraints which cause the exponential
explosion in their graphically formulation are of
the same nature as our cycle constraints. We hope
that the incremental approach will allow exact MT
decoding for longer sentences.
</bodyText>
<page confidence="0.998444">
136
</page>
<sectionHeader confidence="0.99946" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.9988661">
In this paper we have presented a novel ap-
proach for inference using ILP. While previous ap-
proaches which use ILP for decoding have solved
each integer linear program in one run, we incre-
mentally add constraints and solve the resulting
program until no more constraints are violated.
This allows us to efficiently use ILP for depen-
dency parsing and add constraints which provide
a significant improvement over the current state-
of-the-art parser (McDonald et al., 2005b) on the
Dutch Alpino corpus (see bl row in Table 1).
Although slower than the baseline approach,
our method can still parse large sentences (more
than 50 tokens) in a reasonable amount of time
(less than a minute). We have shown that pars-
ing time can be significantly reduced using a
simple approximation which only marginally de-
grades performance. Furthermore, we believe that
the method has potential for further extensions and
applications.
</bodyText>
<sectionHeader confidence="0.998195" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.997707">
Thanks to Ivan Meza-Ruiz, Ruken C¸ akıcı, Beata
Kouchnir and Abhishek Arun for their contribu-
tion during the CoNLL shared task and to Mirella
Lapata for helpful comments and suggestions.
</bodyText>
<sectionHeader confidence="0.999485" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999204901408451">
Culotta, Aron and Jeffery Sorensen. 2004. Dependency tree
kernels for relation extraction. In 42nd Annual Meeting of
the Association for Computational Linguistics. Barcelona,
Spain, pages 423–429.
Daelemans, W., J. Zavrel, and S. Berck. 1996. MBT: A
memory-based part of speech tagger-generator. In Pro-
ceedings of the Fourth Workshop on Very Large Corpora.
pages 14–27.
Ding, Yuan and Martha Palmer. 2005. Machine transla-
tion using probabilistic synchronous dependency insertion
grammars. In The 43rd Annual Meeting of the Association
of Computational Linguistics. Ann Arbor, MI, USA, pages
541–548.
Germann, Ulrich, Michael Jahr, Kevin Knight, Daniel Marcu,
and Kenji Yamada. 2001. Fast decoding and optimal de-
coding for machine translation. In Meeting of the Asso-
ciation for Computational Linguistics. Toulouse, France,
pages 228–235.
Gr¨otschel, M., L. Lov´asz, and A. Schrijver. 1981. The ellip-
soid method and its consequences in combina- torial opti-
mization. Combinatorica I:169– 197.
Malouf, Robert and Gertjan van Noord. 2004. Wide cover-
age parsing with stochastic attribute value grammars. In
Proc. of IJCNLP-04 Workshop ”Beyond Shallow Analy-
ses”. Sanya City, Hainan Island, China.
McDonald, R., K. Crammer, and F. Pereira. 2005a. Online
large-margin training of dependency parsers. In 43rd An-
nual Meeting of the Association for Computational Lin-
guistics. Ann Arbor, MI, USA, pages 91–98.
McDonald, R. and F. Pereira. 2006. Online learning of ap-
proximate dependency parsing algorithms. In 11th Con-
ference of the European Chapter of the Association for
Computational Linguistics. Trento, Italy, pages 81–88.
McDonald, R., F. Pereira, K. Ribarov, and J. Hajic. 2005b.
Non-projective dependency parsing using spanning tree
algorithms. In Proceedings of Human Language Technol-
ogy Conference and Conference on Empirical Methods in
Natural Language Processing. Association for Computa-
tional Linguistics, Vancouver, British Columbia, Canada,
pages 523–530.
McDonald, Ryan, Kevin Lerman, and Fernando Pereira.
2006. Multilingual dependency analysis with a two-stage
discriminative parser. In Proceedings of CoNLL-2006.
New York, USA.
Moortgat, M., I. Schuurman, and T. van der Wouden.
2000. Cgn syntactische annotatie. Internal report Corpus
Gesproken Nederlands.
Nivre, J., J. Hall, and J. Nilsson. 2004. Memory-based depen-
dency parsing. In Proceedings of CoNLL-2004. Boston,
MA, USA, pages 49–56.
Roth, D. and W. Yih. 2004. A linear programming formu-
lation for global inference in natural language tasks. In
Proceedings of CoNLL-2004,. Boston, MA, USA, pages
1–8.
van der Beek, L., G. Bouma, R. Malouf, G. van Noord,
Leonoor van der Beek, Gosse Bouma, Robert Malouf, and
Gertjan van Noord. 2002. The Alpino dependency tree-
bank. In Computational Linguistics in the Netherlands
(CLIN). Rodopi.
Warme, David Michael. 1998. Spanning Trees in Hyper-
graphs with Application to Steiner Trees. Ph.D. thesis,
University of Virginia.
Williams, Justin C. 2002. A linear-size zero - one program-
ming model for the minimum spanning tree problem in
planar graphs. Networks 39:53–60.
Winston, Wayne L. and Munirpallam Venkataramanan.
2003. Introduction to Mathematical Programming.
Brooks/Cole.
Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical de-
pendency analysis with support vector machines. In Pro-
ceedings ofIWPT. pages 195–206.
</reference>
<page confidence="0.997792">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.447428">
<title confidence="0.9962665">Incremental Integer Linear Programming for Non-projective Dependency Parsing</title>
<author confidence="0.977254">Riedel</author>
<affiliation confidence="0.8706415">School of Informatics, University of 2 Bucclecuch Place, Edinburgh EH8 9LW,</affiliation>
<abstract confidence="0.978597555555555">Integer Linear Programming has recently been used for decoding in a number of probabilistic models in order to enforce global constraints. However, in certain applications, such as non-projective dependency parsing and machine translation, the complete formulation of the decoding problem as an integer linear program renders solving intractable. We present an approach which solves the problem incrementally, thus we avoid creating intractable integer linear programs. This approach is applied to Dutch dependency parsing and we show how the addition of linguistically motivated constraints can yield a significant improvement over stateof-the-art.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffery Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In 42nd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>423--429</pages>
<location>Barcelona,</location>
<contexts>
<context position="5244" citStr="Culotta and Sorensen, 2004" startWordPosition="842" endWordPosition="845">r example, it must not contain dependency chains such as “en” __+ “kom” __+ “ik” —* “en”. For a more formal definition see previous work (Nivre et al., 2004). An important distinction between dependency trees is whether they are projective or nonprojective. Figure 1 is an example of a projective dependency tree, in such trees dependencies do not cross. In Dutch and other flexible word order languages such as German and Czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies. Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Although less informative than lexicalised phrase structures, dependency structures still capture most of the predicate-argument information needed for applications. It has the advantage of being more efficient to learn and parse. McDonald et al. (2005a) introduce a dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores. This frameFigure 2: An incorrect partial dependency tree. The verb “krijg” is incorrectly coordinated with the preposition “om”. work is efficie</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Culotta, Aron and Jeffery Sorensen. 2004. Dependency tree kernels for relation extraction. In 42nd Annual Meeting of the Association for Computational Linguistics. Barcelona, Spain, pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>S Berck</author>
</authors>
<title>MBT: A memory-based part of speech tagger-generator.</title>
<date>1996</date>
<booktitle>In Proceedings of the Fourth Workshop on Very Large Corpora.</booktitle>
<pages>14--27</pages>
<contexts>
<context position="18238" citStr="Daelemans et al., 1996" startWordPosition="3113" endWordPosition="3116">parison with the Chu-Liu-Edmonds algorithm? 3. Can approximations be used to increase the speed of our method while remaining accurate? Before we try to answer these questions we briefly describe our data, features used, settings for U and P in our parametric constraints, our working environment and our implementation. 133 4.1 Data We use the Alpino treebank (van der Beek et al., 2002), taken from the CoNLL shared task of multilingual dependency parsing3. The CoNLL data differs slightly from the original Alpino treebank as the corpus has been part-of-speech tagged using a Memory-Based-Tagger (Daelemans et al., 1996). It consists of 13,300 sentences with an average length of 14.6 tokens. The data is non-projective, more specifically 5.4% of all dependencies are crossed by at least one other dependency. It contains approximately 6000 sentences more than the Alpino corpus used by Malouf and van Noord (2004). The training set was divided into a 10% development set (dev) while the remaining 90% is used as a training and cross-validation set (cross). Feature sets, constraints and training parameters were selected through training on cross and optimising against dev. Our final accuracy scores and runtime evalua</context>
</contexts>
<marker>Daelemans, Zavrel, Berck, 1996</marker>
<rawString>Daelemans, W., J. Zavrel, and S. Berck. 1996. MBT: A memory-based part of speech tagger-generator. In Proceedings of the Fourth Workshop on Very Large Corpora. pages 14–27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probabilistic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In The 43rd Annual Meeting of the Association of Computational Linguistics.</booktitle>
<pages>541--548</pages>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="5292" citStr="Ding and Palmer, 2005" startWordPosition="849" endWordPosition="852"> as “en” __+ “kom” __+ “ik” —* “en”. For a more formal definition see previous work (Nivre et al., 2004). An important distinction between dependency trees is whether they are projective or nonprojective. Figure 1 is an example of a projective dependency tree, in such trees dependencies do not cross. In Dutch and other flexible word order languages such as German and Czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies. Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Although less informative than lexicalised phrase structures, dependency structures still capture most of the predicate-argument information needed for applications. It has the advantage of being more efficient to learn and parse. McDonald et al. (2005a) introduce a dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores. This frameFigure 2: An incorrect partial dependency tree. The verb “krijg” is incorrectly coordinated with the preposition “om”. work is efficient and has also been extended to non-projective </context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Ding, Yuan and Martha Palmer. 2005. Machine translation using probabilistic synchronous dependency insertion grammars. In The 43rd Annual Meeting of the Association of Computational Linguistics. Ann Arbor, MI, USA, pages 541–548.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulrich Germann</author>
<author>Michael Jahr</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Kenji Yamada</author>
</authors>
<title>Fast decoding and optimal decoding for machine translation.</title>
<date>2001</date>
<booktitle>In Meeting of the Association for Computational Linguistics.</booktitle>
<pages>228--235</pages>
<location>Toulouse, France,</location>
<contexts>
<context position="2509" citStr="Germann et al., 2001" startWordPosition="379" endWordPosition="382">nditional random fields (Roth and Yih, 2004), this has allowed the use of truly global constraints during inference. However, it is not possible to use this approach directly for a complex task like non-projective dependency parsing due to the exponential number of constraints required to prevent cycles occurring in the dependency graph. To model all these constraints explicitly would result in an ILP formulation too large to solve efficiently (Williams, 2002). A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al., 2001). In this paper we present a method which extends the applicability of ILP to a more complex set of problems. Instead of adding all the constraints we wish to capture to the formulation, we first solve the program with a fraction of the constraints. The solution is then examined and, if required, additional constraints are added. This procedure is repeated until all constraints are satisfied. We apply this dependency parsing approach to Dutch due to the language’s non-projective nature, and take the parser of McDonald et al. (2005b) as a starting point for our model. In the following section w</context>
<context position="29653" citStr="Germann et al. (2001)" startWordPosition="5045" endWordPosition="5048">ur runtime evaluations we find that a large fraction of solve time is spent in the first iteration of our incremental algorithm. After the first iteration the solver uses its last state to efficiently search for solutions in the presence of new constraints. Some solvers allow the specification of an initial solution as a starting point, thus it is expected that significant improvements in terms of speed can be made by using the CLE algorithm to provide an initial solution. Our approach uses a generic algorithm to solve a complex task. Thus other applications may benefit from it. For instance, Germann et al. (2001) present an ILP formulation of the Machine Translation (MT) decoding task in order to conduct exact inference. However, their model suffers from the same type of exponential blow-up we observe when we add all our cycle constraints in advance. In fact, the constraints which cause the exponential explosion in their graphically formulation are of the same nature as our cycle constraints. We hope that the incremental approach will allow exact MT decoding for longer sentences. 136 7 Conclusion In this paper we have presented a novel approach for inference using ILP. While previous approaches which </context>
</contexts>
<marker>Germann, Jahr, Knight, Marcu, Yamada, 2001</marker>
<rawString>Germann, Ulrich, Michael Jahr, Kevin Knight, Daniel Marcu, and Kenji Yamada. 2001. Fast decoding and optimal decoding for machine translation. In Meeting of the Association for Computational Linguistics. Toulouse, France, pages 228–235.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Gr¨otschel</author>
<author>L Lov´asz</author>
<author>A Schrijver</author>
</authors>
<title>The ellipsoid method and its consequences in combina- torial optimization.</title>
<date>1981</date>
<journal>Combinatorica</journal>
<volume>169</volume>
<pages>197</pages>
<marker>Gr¨otschel, Lov´asz, Schrijver, 1981</marker>
<rawString>Gr¨otschel, M., L. Lov´asz, and A. Schrijver. 1981. The ellipsoid method and its consequences in combina- torial optimization. Combinatorica I:169– 197.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>Wide coverage parsing with stochastic attribute value grammars.</title>
<date>2004</date>
<booktitle>In Proc. of IJCNLP-04 Workshop ”Beyond Shallow Analyses”.</booktitle>
<location>Sanya City, Hainan Island, China.</location>
<marker>Malouf, van Noord, 2004</marker>
<rawString>Malouf, Robert and Gertjan van Noord. 2004. Wide coverage parsing with stochastic attribute value grammars. In Proc. of IJCNLP-04 Workshop ”Beyond Shallow Analyses”. Sanya City, Hainan Island, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In 43rd Annual Meeting of the Association for Computational Linguistics.</booktitle>
<pages>91--98</pages>
<location>Ann Arbor, MI, USA,</location>
<contexts>
<context position="3045" citStr="McDonald et al. (2005" startWordPosition="469" endWordPosition="472">ation which treats decoding as the Travelling Salesman Problem (Germann et al., 2001). In this paper we present a method which extends the applicability of ILP to a more complex set of problems. Instead of adding all the constraints we wish to capture to the formulation, we first solve the program with a fraction of the constraints. The solution is then examined and, if required, additional constraints are added. This procedure is repeated until all constraints are satisfied. We apply this dependency parsing approach to Dutch due to the language’s non-projective nature, and take the parser of McDonald et al. (2005b) as a starting point for our model. In the following section we introduce dependency parsing and review previous work. In Section 3 we present our model and formulate it as an ILP problem with a set of linguistically motivated constraints. We include details of an incremental algorithm used to solve this formulation. Our experimental set-up is provided in Section 4 and is followed by results in Section 5 along with runtime experiments. We finally discuss fu129 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 129–137, Sydney, July 2006</context>
<context position="5546" citStr="McDonald et al. (2005" startWordPosition="886" endWordPosition="889">tree, in such trees dependencies do not cross. In Dutch and other flexible word order languages such as German and Czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies. Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Although less informative than lexicalised phrase structures, dependency structures still capture most of the predicate-argument information needed for applications. It has the advantage of being more efficient to learn and parse. McDonald et al. (2005a) introduce a dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores. This frameFigure 2: An incorrect partial dependency tree. The verb “krijg” is incorrectly coordinated with the preposition “om”. work is efficient and has also been extended to non-projective trees (McDonald et al., 2005b). It provides a discriminative online learning algorithm which when combined with a rich feature set reaches state-of-the-art performance across multiple languages. However, within this framework one can only define features</context>
<context position="7401" citStr="McDonald et al. (2005" startWordPosition="1179" endWordPosition="1182">tion is more general. Furthermore, it is complementary in the sense that we could formulate their model using ILP and then add constraints. The method we present is not the only one that can take global constraints into account. Deterministic dependency parsing (Nivre et al., 2004; Yamada and Matsumoto, 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built. However, for efficiency a greedy search is used which may produce sub-optimal solutions. This is not the case when using ILP. 3 Model Our underlying model is a modified labelled version2 of McDonald et al. (2005b): s(x, y) _ � s(i, j, l) (i,j,l)Ey �� w · f(i, j, l) (i,j,l)Ey 2Note that this is not described in the McDonald papers but implemented in his software. 130 where x is a sentence, y is a set of labelled dependencies, f(i, j, l) is a multidimensional feature vector representation of the edge from token i to token j with label l and w the corresponding weight vector. For example, a feature f101 in f could be: { 1 if t(i) = “en” ∧ p(j) = V ∧l = “coordination” 0 otherwise where t(i) is the word at token i and p(j) the partof-speech tag at token j. Decoding in this model amounts to finding the y f</context>
<context position="9942" citStr="McDonald et al. (2005" startWordPosition="1648" endWordPosition="1651">on such as “zowel” (translates as “both”) having arguments to its left. C3 forces coordination arguments to the left of a conjunction to have commas in between. C4 avoids parses in which incompatible word classes are coordinated, such as nouns and verbs. Finally, P1 allows selective projective parsing: we can, for instance, forbid the crossing of “Noun-Determiner” dependencies if we add the corresponding label type to P(see Section 4.4 for more details of P) . If we extend P to contain all labels we forbid any type of crossing dependencies. This corresponds to projective parsing. 3.1 Decoding McDonald et al. (2005b) use the Chu-LiuEdmonds (CLE) algorithm to solve the maximum spanning tree problem. However, global constraints cannot be incorporated into the CLE algorithm (McDonald et al., 2005b). We alleviate this problem by presenting an equivalent Integer Linear Programming formulation which allows us to incorporate global constraints naturally. Before giving full details of our formulation we first introduce some of the concepts of linear and integer linear programming (for a more thorough introduction see Winston and Venkataramanan (2003)). Linear Programming (LP) is a tool for solving optimisation </context>
<context position="16865" citStr="McDonald et al., 2005" startWordPosition="2888" endWordPosition="2892">A in x to the left of i with no comma between each pair of successive tokens we add: X di,a ≤ |A |− 1 aEA which forbids configurations where i has the argument tokens A. Compatible Coordination Arguments (C4) For each conjunction token i and each set of tokens A in x with incompatible POS tags, we add a constraint to forbid configurations where i has the argument tokens A. X di,a ≤ |A |− 1 aEA Selective Projective Parsing (P1) For each pair of triplets (i, j, l1) and (m, n, l2) we add the constraint: ei,j,l1 + em,n,l2 ≤ 1 if l1 or l2 is in P. 3.2 Training For training we use single-best MIRA (McDonald et al., 2005a). This is an online algorithm that learns by parsing each sentence and comparing the result with a gold standard. Training is complete after multiple passes through the whole corpus. Thus we decode using the Chu-Liu-Edmonds (CLE) algorithm due to its speed advantage over ILP (see Section 5.2 for a detailed comparison of runtimes). The fact that we decode differently during training (using CLE) and testing (using ILP) may degrade performance. In the presence of additional constraints weights may be able to capture other aspects of the data. 4 Experimental Set-up Our experiments were designed </context>
<context position="19573" citStr="McDonald et al. (2005" startWordPosition="3336" endWordPosition="3339">nts were conducted on a Intel Xeon with 3.8 Ghz and 4Gb of RAM. We used the open source Mixed Integer Programming library lp solve4 to solve the Integer Linear Programs. Our code ran in Java and called the JNIwrapper around the lp solve library. 4.3 Feature Sets Our feature set was determined through experimentation with the development set. The features are based upon the data provided within the Alpino treebank. Along with POS tags the corpus contains several additional attributes such as gender, number and case. Our best results on the development set were achieved using the feature set of McDonald et al. (2005a) and a set of features based on the additional attributes. These features combine the attributes of the head with those of the child. For example, if token i has the attributes a1 and a2, and token j has the attribute a3 then we created the features (a1 ∧ a3) and (a2 ∧ a3). 3For details see http://nextens.uvt.nl/ ˜conll. 4The software is available from http://www. geocities.com/lpsolve. 4.4 Constraints All the constraints presented in Section 3 were used in our model. The set U of unique labels constraints contained su, obj1, obj2, sup, ld, vc, predc, predm, pc, pobj1, obcomp and body. Here </context>
<context position="24721" citStr="McDonald et al., 2005" startWordPosition="4198" endWordPosition="4202">s). For example, suppose we attach two subjects, t1 and t2, to a verb, where t1 is the actual subject while t2 is meant to be labelled as object. If we forbid this configuration (two subjects) and if the score of labelling t1 object is higher than that for t2 being labelled subject, then the next best solution will label t1 incorrectly as object and t2 incorrectly as subject. This is often the case, and thus results in a drop of accuracy. 5.2 Runtime Evaluation We now concentrate on the runtime of our method. While we expect a longer runtime than using the Chu-Liu-Edmonds as in previous work (McDonald et al., 2005b), we are interested in how large the increase is. Table 2 shows the average solve time (ST) for sentences with respect to the number of tokens in each sentence for our system with constraints (cnstr) and the Chu-Liu-Edmonds (CLE) algorithm. All solve times do not include feature extraction as this is identical for all systems. For cnstr we also show the number of sentences that could not be parsed after two minutes, the average number of iterations and the average duration of the first iteration. The results show that parsing using our generic approach is still reasonably fast although signi</context>
<context position="28288" citStr="McDonald et al. (2005" startWordPosition="4822" endWordPosition="4825"> reduced by 80% while only losing a marginal amount of accuracy when we set q to 10. However, we are unable to reach the 20 seconds solve time of the CLE algorithm. Despite this, when we add the time for preprocessing and feature extraction, the CLE system parses a corpus in around 15 minutes whereas our system with q = 10 takes approximately 25 minutes&apos;. When we view the total runtime of each system we see our system is more competitive. 6 Discussion While we have presented significant improvements using additional constraints, one may won5Even when caching feature extraction during training McDonald et al. (2005a) still takes approximately 10 minutes to train. der whether the improvements are large enough to justify further research in this direction; especially since McDonald and Pereira (2006) present an approximate algorithm which also makes more global decisions. However, we believe that our approach is complementary to their model. We can model higher order features by using an extended set of variables and a modified objective function. Although this is likely to increase runtime, it may still be fast enough for real world applications. In addition, it will allow exact inference, even in the ca</context>
<context position="30614" citStr="McDonald et al., 2005" startWordPosition="5202" endWordPosition="5205">re of the same nature as our cycle constraints. We hope that the incremental approach will allow exact MT decoding for longer sentences. 136 7 Conclusion In this paper we have presented a novel approach for inference using ILP. While previous approaches which use ILP for decoding have solved each integer linear program in one run, we incrementally add constraints and solve the resulting program until no more constraints are violated. This allows us to efficiently use ILP for dependency parsing and add constraints which provide a significant improvement over the current stateof-the-art parser (McDonald et al., 2005b) on the Dutch Alpino corpus (see bl row in Table 1). Although slower than the baseline approach, our method can still parse large sentences (more than 50 tokens) in a reasonable amount of time (less than a minute). We have shown that parsing time can be significantly reduced using a simple approximation which only marginally degrades performance. Furthermore, we believe that the method has potential for further extensions and applications. Acknowledgements Thanks to Ivan Meza-Ruiz, Ruken C¸ akıcı, Beata Kouchnir and Abhishek Arun for their contribution during the CoNLL shared task and to Mir</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>McDonald, R., K. Crammer, and F. Pereira. 2005a. Online large-margin training of dependency parsers. In 43rd Annual Meeting of the Association for Computational Linguistics. Ann Arbor, MI, USA, pages 91–98.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
</authors>
<title>Online learning of approximate dependency parsing algorithms.</title>
<date>2006</date>
<booktitle>In 11th Conference of the European Chapter of the Association for Computational Linguistics.</booktitle>
<pages>81--88</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="6681" citStr="McDonald and Pereira (2006)" startWordPosition="1063" endWordPosition="1066">mance across multiple languages. However, within this framework one can only define features over single attachment decisions. This leads to cases where basic linguistic constraints are not satisfied (e.g. verbs with two subjects or incompatible coordination arguments). An example of this for Dutch is illustrated in Figure 2 which was produced by the parser of McDonald et al. (2005b). Here the parse contains a coordination of incompatible word classes (a preposition and a verb). Our approach is able to include additional constraints which forbid configurations such as those in Figure 2. While McDonald and Pereira (2006) address the issue of local attachment decisions by defining scores over attachment pairs, our solution is more general. Furthermore, it is complementary in the sense that we could formulate their model using ILP and then add constraints. The method we present is not the only one that can take global constraints into account. Deterministic dependency parsing (Nivre et al., 2004; Yamada and Matsumoto, 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built. However, for efficiency a greedy search is used which may produce sub-optimal solutions. Th</context>
<context position="28475" citStr="McDonald and Pereira (2006)" startWordPosition="4850" endWordPosition="4853">en we add the time for preprocessing and feature extraction, the CLE system parses a corpus in around 15 minutes whereas our system with q = 10 takes approximately 25 minutes&apos;. When we view the total runtime of each system we see our system is more competitive. 6 Discussion While we have presented significant improvements using additional constraints, one may won5Even when caching feature extraction during training McDonald et al. (2005a) still takes approximately 10 minutes to train. der whether the improvements are large enough to justify further research in this direction; especially since McDonald and Pereira (2006) present an approximate algorithm which also makes more global decisions. However, we believe that our approach is complementary to their model. We can model higher order features by using an extended set of variables and a modified objective function. Although this is likely to increase runtime, it may still be fast enough for real world applications. In addition, it will allow exact inference, even in the case of non-projective parsing. Also, we argue that this approach has potential for interesting extensions and applications. For example, during our runtime evaluations we find that a large</context>
</contexts>
<marker>McDonald, Pereira, 2006</marker>
<rawString>McDonald, R. and F. Pereira. 2006. Online learning of approximate dependency parsing algorithms. In 11th Conference of the European Chapter of the Association for Computational Linguistics. Trento, Italy, pages 81–88.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajic</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,</booktitle>
<pages>523--530</pages>
<location>Vancouver, British Columbia, Canada,</location>
<contexts>
<context position="3045" citStr="McDonald et al. (2005" startWordPosition="469" endWordPosition="472">ation which treats decoding as the Travelling Salesman Problem (Germann et al., 2001). In this paper we present a method which extends the applicability of ILP to a more complex set of problems. Instead of adding all the constraints we wish to capture to the formulation, we first solve the program with a fraction of the constraints. The solution is then examined and, if required, additional constraints are added. This procedure is repeated until all constraints are satisfied. We apply this dependency parsing approach to Dutch due to the language’s non-projective nature, and take the parser of McDonald et al. (2005b) as a starting point for our model. In the following section we introduce dependency parsing and review previous work. In Section 3 we present our model and formulate it as an ILP problem with a set of linguistically motivated constraints. We include details of an incremental algorithm used to solve this formulation. Our experimental set-up is provided in Section 4 and is followed by results in Section 5 along with runtime experiments. We finally discuss fu129 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 129–137, Sydney, July 2006</context>
<context position="5546" citStr="McDonald et al. (2005" startWordPosition="886" endWordPosition="889">tree, in such trees dependencies do not cross. In Dutch and other flexible word order languages such as German and Czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies. Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Although less informative than lexicalised phrase structures, dependency structures still capture most of the predicate-argument information needed for applications. It has the advantage of being more efficient to learn and parse. McDonald et al. (2005a) introduce a dependency parsing framework which treats the task as searching for the projective tree that maximises the sum of local dependency scores. This frameFigure 2: An incorrect partial dependency tree. The verb “krijg” is incorrectly coordinated with the preposition “om”. work is efficient and has also been extended to non-projective trees (McDonald et al., 2005b). It provides a discriminative online learning algorithm which when combined with a rich feature set reaches state-of-the-art performance across multiple languages. However, within this framework one can only define features</context>
<context position="7401" citStr="McDonald et al. (2005" startWordPosition="1179" endWordPosition="1182">tion is more general. Furthermore, it is complementary in the sense that we could formulate their model using ILP and then add constraints. The method we present is not the only one that can take global constraints into account. Deterministic dependency parsing (Nivre et al., 2004; Yamada and Matsumoto, 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built. However, for efficiency a greedy search is used which may produce sub-optimal solutions. This is not the case when using ILP. 3 Model Our underlying model is a modified labelled version2 of McDonald et al. (2005b): s(x, y) _ � s(i, j, l) (i,j,l)Ey �� w · f(i, j, l) (i,j,l)Ey 2Note that this is not described in the McDonald papers but implemented in his software. 130 where x is a sentence, y is a set of labelled dependencies, f(i, j, l) is a multidimensional feature vector representation of the edge from token i to token j with label l and w the corresponding weight vector. For example, a feature f101 in f could be: { 1 if t(i) = “en” ∧ p(j) = V ∧l = “coordination” 0 otherwise where t(i) is the word at token i and p(j) the partof-speech tag at token j. Decoding in this model amounts to finding the y f</context>
<context position="9942" citStr="McDonald et al. (2005" startWordPosition="1648" endWordPosition="1651">on such as “zowel” (translates as “both”) having arguments to its left. C3 forces coordination arguments to the left of a conjunction to have commas in between. C4 avoids parses in which incompatible word classes are coordinated, such as nouns and verbs. Finally, P1 allows selective projective parsing: we can, for instance, forbid the crossing of “Noun-Determiner” dependencies if we add the corresponding label type to P(see Section 4.4 for more details of P) . If we extend P to contain all labels we forbid any type of crossing dependencies. This corresponds to projective parsing. 3.1 Decoding McDonald et al. (2005b) use the Chu-LiuEdmonds (CLE) algorithm to solve the maximum spanning tree problem. However, global constraints cannot be incorporated into the CLE algorithm (McDonald et al., 2005b). We alleviate this problem by presenting an equivalent Integer Linear Programming formulation which allows us to incorporate global constraints naturally. Before giving full details of our formulation we first introduce some of the concepts of linear and integer linear programming (for a more thorough introduction see Winston and Venkataramanan (2003)). Linear Programming (LP) is a tool for solving optimisation </context>
<context position="16865" citStr="McDonald et al., 2005" startWordPosition="2888" endWordPosition="2892">A in x to the left of i with no comma between each pair of successive tokens we add: X di,a ≤ |A |− 1 aEA which forbids configurations where i has the argument tokens A. Compatible Coordination Arguments (C4) For each conjunction token i and each set of tokens A in x with incompatible POS tags, we add a constraint to forbid configurations where i has the argument tokens A. X di,a ≤ |A |− 1 aEA Selective Projective Parsing (P1) For each pair of triplets (i, j, l1) and (m, n, l2) we add the constraint: ei,j,l1 + em,n,l2 ≤ 1 if l1 or l2 is in P. 3.2 Training For training we use single-best MIRA (McDonald et al., 2005a). This is an online algorithm that learns by parsing each sentence and comparing the result with a gold standard. Training is complete after multiple passes through the whole corpus. Thus we decode using the Chu-Liu-Edmonds (CLE) algorithm due to its speed advantage over ILP (see Section 5.2 for a detailed comparison of runtimes). The fact that we decode differently during training (using CLE) and testing (using ILP) may degrade performance. In the presence of additional constraints weights may be able to capture other aspects of the data. 4 Experimental Set-up Our experiments were designed </context>
<context position="19573" citStr="McDonald et al. (2005" startWordPosition="3336" endWordPosition="3339">nts were conducted on a Intel Xeon with 3.8 Ghz and 4Gb of RAM. We used the open source Mixed Integer Programming library lp solve4 to solve the Integer Linear Programs. Our code ran in Java and called the JNIwrapper around the lp solve library. 4.3 Feature Sets Our feature set was determined through experimentation with the development set. The features are based upon the data provided within the Alpino treebank. Along with POS tags the corpus contains several additional attributes such as gender, number and case. Our best results on the development set were achieved using the feature set of McDonald et al. (2005a) and a set of features based on the additional attributes. These features combine the attributes of the head with those of the child. For example, if token i has the attributes a1 and a2, and token j has the attribute a3 then we created the features (a1 ∧ a3) and (a2 ∧ a3). 3For details see http://nextens.uvt.nl/ ˜conll. 4The software is available from http://www. geocities.com/lpsolve. 4.4 Constraints All the constraints presented in Section 3 were used in our model. The set U of unique labels constraints contained su, obj1, obj2, sup, ld, vc, predc, predm, pc, pobj1, obcomp and body. Here </context>
<context position="24721" citStr="McDonald et al., 2005" startWordPosition="4198" endWordPosition="4202">s). For example, suppose we attach two subjects, t1 and t2, to a verb, where t1 is the actual subject while t2 is meant to be labelled as object. If we forbid this configuration (two subjects) and if the score of labelling t1 object is higher than that for t2 being labelled subject, then the next best solution will label t1 incorrectly as object and t2 incorrectly as subject. This is often the case, and thus results in a drop of accuracy. 5.2 Runtime Evaluation We now concentrate on the runtime of our method. While we expect a longer runtime than using the Chu-Liu-Edmonds as in previous work (McDonald et al., 2005b), we are interested in how large the increase is. Table 2 shows the average solve time (ST) for sentences with respect to the number of tokens in each sentence for our system with constraints (cnstr) and the Chu-Liu-Edmonds (CLE) algorithm. All solve times do not include feature extraction as this is identical for all systems. For cnstr we also show the number of sentences that could not be parsed after two minutes, the average number of iterations and the average duration of the first iteration. The results show that parsing using our generic approach is still reasonably fast although signi</context>
<context position="28288" citStr="McDonald et al. (2005" startWordPosition="4822" endWordPosition="4825"> reduced by 80% while only losing a marginal amount of accuracy when we set q to 10. However, we are unable to reach the 20 seconds solve time of the CLE algorithm. Despite this, when we add the time for preprocessing and feature extraction, the CLE system parses a corpus in around 15 minutes whereas our system with q = 10 takes approximately 25 minutes&apos;. When we view the total runtime of each system we see our system is more competitive. 6 Discussion While we have presented significant improvements using additional constraints, one may won5Even when caching feature extraction during training McDonald et al. (2005a) still takes approximately 10 minutes to train. der whether the improvements are large enough to justify further research in this direction; especially since McDonald and Pereira (2006) present an approximate algorithm which also makes more global decisions. However, we believe that our approach is complementary to their model. We can model higher order features by using an extended set of variables and a modified objective function. Although this is likely to increase runtime, it may still be fast enough for real world applications. In addition, it will allow exact inference, even in the ca</context>
<context position="30614" citStr="McDonald et al., 2005" startWordPosition="5202" endWordPosition="5205">re of the same nature as our cycle constraints. We hope that the incremental approach will allow exact MT decoding for longer sentences. 136 7 Conclusion In this paper we have presented a novel approach for inference using ILP. While previous approaches which use ILP for decoding have solved each integer linear program in one run, we incrementally add constraints and solve the resulting program until no more constraints are violated. This allows us to efficiently use ILP for dependency parsing and add constraints which provide a significant improvement over the current stateof-the-art parser (McDonald et al., 2005b) on the Dutch Alpino corpus (see bl row in Table 1). Although slower than the baseline approach, our method can still parse large sentences (more than 50 tokens) in a reasonable amount of time (less than a minute). We have shown that parsing time can be significantly reduced using a simple approximation which only marginally degrades performance. Furthermore, we believe that the method has potential for further extensions and applications. Acknowledgements Thanks to Ivan Meza-Ruiz, Ruken C¸ akıcı, Beata Kouchnir and Abhishek Arun for their contribution during the CoNLL shared task and to Mir</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajic, 2005</marker>
<rawString>McDonald, R., F. Pereira, K. Ribarov, and J. Hajic. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Vancouver, British Columbia, Canada, pages 523–530.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Kevin Lerman</author>
<author>Fernando Pereira</author>
</authors>
<title>Multilingual dependency analysis with a two-stage discriminative parser.</title>
<date>2006</date>
<booktitle>In Proceedings of CoNLL-2006.</booktitle>
<location>New York, USA.</location>
<contexts>
<context position="23086" citStr="McDonald et al. 2006" startWordPosition="3923" endWordPosition="3926"> script and using the Sign test (p &lt; 0.001). Now we give a little insight into how our results compare with the rest of the community. The reported state-of-the-art parser of Malouf and van Noord (2004) achieves 84.4% labelled accuracy which is very close numerically to our baseline. However, they use a subset of the CoNLL Alpino treebank with a higher average number of tokens per sentences and also evaluate control relations, thus results are not directly comparable. We have also run our parser on the relatively small (approximately 400 sentences) CoNNL test data. The best performing system (McDonald et al. 2006; note: this system is different to our baseline) achieves 79.2% labelled accuracy while our baseline system achieves 78.6% and our constrained version 79.8%. However, a significant difference is only observed between our baseline and our constraintbased system. Examining the errors produced using the dev set highlight two key reasons why we do not see a greater improvement using our constraint-based system. Firstly, we cannot improve on coordinations that include words ending with “en” based on the workaround present in Section 4.4. This problem can only be solved by improving POS taggers for</context>
</contexts>
<marker>McDonald, Lerman, Pereira, 2006</marker>
<rawString>McDonald, Ryan, Kevin Lerman, and Fernando Pereira. 2006. Multilingual dependency analysis with a two-stage discriminative parser. In Proceedings of CoNLL-2006. New York, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Moortgat</author>
<author>I Schuurman</author>
<author>T van der Wouden</author>
</authors>
<title>Cgn syntactische annotatie. Internal report Corpus Gesproken Nederlands.</title>
<date>2000</date>
<marker>Moortgat, Schuurman, van der Wouden, 2000</marker>
<rawString>Moortgat, M., I. Schuurman, and T. van der Wouden. 2000. Cgn syntactische annotatie. Internal report Corpus Gesproken Nederlands.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Nivre</author>
<author>J Hall</author>
<author>J Nilsson</author>
</authors>
<title>Memory-based dependency parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004.</booktitle>
<pages>49--56</pages>
<location>Boston, MA, USA,</location>
<contexts>
<context position="4774" citStr="Nivre et al., 2004" startWordPosition="769" endWordPosition="772">m” is attached to its subject “ik”. “kom” is referred to as the head of the dependency and “ik” as its child. In labelled dependency parsing edges between words are labelled with the relation captured. In the case of the dependency between “kom” and “ik” the label would be “subject”. In a dependency tree every token must be the child of exactly one other node, either another token or the dummy root node. By definition, a dependency tree is free of cycles. For example, it must not contain dependency chains such as “en” __+ “kom” __+ “ik” —* “en”. For a more formal definition see previous work (Nivre et al., 2004). An important distinction between dependency trees is whether they are projective or nonprojective. Figure 1 is an example of a projective dependency tree, in such trees dependencies do not cross. In Dutch and other flexible word order languages such as German and Czech we also encounter non-projective trees, in these cases the trees contain crossing dependencies. Dependency parsing is useful for applications such as relation extraction (Culotta and Sorensen, 2004) and machine translation (Ding and Palmer, 2005). Although less informative than lexicalised phrase structures, dependency structu</context>
<context position="7061" citStr="Nivre et al., 2004" startWordPosition="1124" endWordPosition="1127">ere the parse contains a coordination of incompatible word classes (a preposition and a verb). Our approach is able to include additional constraints which forbid configurations such as those in Figure 2. While McDonald and Pereira (2006) address the issue of local attachment decisions by defining scores over attachment pairs, our solution is more general. Furthermore, it is complementary in the sense that we could formulate their model using ILP and then add constraints. The method we present is not the only one that can take global constraints into account. Deterministic dependency parsing (Nivre et al., 2004; Yamada and Matsumoto, 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built. However, for efficiency a greedy search is used which may produce sub-optimal solutions. This is not the case when using ILP. 3 Model Our underlying model is a modified labelled version2 of McDonald et al. (2005b): s(x, y) _ � s(i, j, l) (i,j,l)Ey �� w · f(i, j, l) (i,j,l)Ey 2Note that this is not described in the McDonald papers but implemented in his software. 130 where x is a sentence, y is a set of labelled dependencies, f(i, j, l) is a multidimensional feature v</context>
</contexts>
<marker>Nivre, Hall, Nilsson, 2004</marker>
<rawString>Nivre, J., J. Hall, and J. Nilsson. 2004. Memory-based dependency parsing. In Proceedings of CoNLL-2004. Boston, MA, USA, pages 49–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Roth</author>
<author>W Yih</author>
</authors>
<title>A linear programming formulation for global inference in natural language tasks.</title>
<date>2004</date>
<booktitle>In Proceedings of CoNLL-2004,.</booktitle>
<pages>1--8</pages>
<location>Boston, MA, USA,</location>
<contexts>
<context position="1932" citStr="Roth and Yih, 2004" startWordPosition="285" endWordPosition="288">f one another&apos;. However, often such assumptions can not be justified. For example in dependency parsing, if a subject has already been identified for a given verb, then the probability of attaching a second subject to the verb is zero. Similarly, if we find that one coordination argument is a noun, then the other argu&apos;If we ignore the constraint that dependency trees must be cycle-free (see sections 2 and 3 for details). ment cannot be a verb. Thus decisions are often co-dependent. Integer Linear Programming (ILP) has recently been applied to inference in sequential conditional random fields (Roth and Yih, 2004), this has allowed the use of truly global constraints during inference. However, it is not possible to use this approach directly for a complex task like non-projective dependency parsing due to the exponential number of constraints required to prevent cycles occurring in the dependency graph. To model all these constraints explicitly would result in an ILP formulation too large to solve efficiently (Williams, 2002). A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al., 2001). In this paper we pres</context>
</contexts>
<marker>Roth, Yih, 2004</marker>
<rawString>Roth, D. and W. Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of CoNLL-2004,. Boston, MA, USA, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L van der Beek</author>
<author>G Bouma</author>
<author>R Malouf</author>
<author>G van Noord</author>
<author>Leonoor van der Beek</author>
<author>Gosse Bouma</author>
<author>Robert Malouf</author>
<author>Gertjan van Noord</author>
</authors>
<title>The Alpino dependency treebank.</title>
<date>2002</date>
<booktitle>In Computational Linguistics in the Netherlands (CLIN). Rodopi.</booktitle>
<marker>van der Beek, Bouma, Malouf, van Noord, van der Beek, Bouma, Malouf, van Noord, 2002</marker>
<rawString>van der Beek, L., G. Bouma, R. Malouf, G. van Noord, Leonoor van der Beek, Gosse Bouma, Robert Malouf, and Gertjan van Noord. 2002. The Alpino dependency treebank. In Computational Linguistics in the Netherlands (CLIN). Rodopi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Michael Warme</author>
</authors>
<title>Spanning Trees in Hypergraphs with Application to Steiner Trees.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Virginia.</institution>
<contexts>
<context position="12184" citStr="Warme (1998)" startWordPosition="2015" endWordPosition="2016"> Bx repeat y +— solve(C, Ox, Vx) W +— violated(y, Ix) C+—CUW until V = 0 return y ods yield an exponential number of constraints. Although the latter scales cubically, it produces non-fractional solutions in its relaxed version; this causes long runtimes for the branch and bound algorithm (Williams, 2002) commonly used in integer linear programming. We found out experimentally that dependency parsing models of this form do not converge on a solution after multiple hours of solving, even for small sentences. As a workaround for this problem we follow an incremental approach akin to the work of Warme (1998). Instead of adding constraints which forbid all possible cycles in advance (this would result in an exponential number of constraints) we first solve the problem without any cycle constraints. The solution is then examined for cycles, and if cycles are found we add constraints to forbid these cycles; the solver is then run again. This process is repeated until no more violated constraints are found. The same procedure is used for other types of constraints which are too expensive to add in advance (e.g. the constraints of P1). Algorithm 1 outlines our approach. For a sentence x, Bx is the set</context>
</contexts>
<marker>Warme, 1998</marker>
<rawString>Warme, David Michael. 1998. Spanning Trees in Hypergraphs with Application to Steiner Trees. Ph.D. thesis, University of Virginia.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justin C Williams</author>
</authors>
<title>A linear-size zero - one programming model for the minimum spanning tree problem in planar graphs.</title>
<date>2002</date>
<journal>Networks</journal>
<pages>39--53</pages>
<contexts>
<context position="2352" citStr="Williams, 2002" startWordPosition="355" endWordPosition="356"> ment cannot be a verb. Thus decisions are often co-dependent. Integer Linear Programming (ILP) has recently been applied to inference in sequential conditional random fields (Roth and Yih, 2004), this has allowed the use of truly global constraints during inference. However, it is not possible to use this approach directly for a complex task like non-projective dependency parsing due to the exponential number of constraints required to prevent cycles occurring in the dependency graph. To model all these constraints explicitly would result in an ILP formulation too large to solve efficiently (Williams, 2002). A similar problem also occurs in an ILP formulation for machine translation which treats decoding as the Travelling Salesman Problem (Germann et al., 2001). In this paper we present a method which extends the applicability of ILP to a more complex set of problems. Instead of adding all the constraints we wish to capture to the formulation, we first solve the program with a fraction of the constraints. The solution is then examined and, if required, additional constraints are added. This procedure is repeated until all constraints are satisfied. We apply this dependency parsing approach to Du</context>
<context position="11283" citStr="Williams, 2002" startWordPosition="1866" endWordPosition="1867">e function to be maximised (or minimised) is referred to as the objective function. A number of decision variables are under our control which exert influence on the objective function. Specifically, they have to be optimised in order to maximise (or minimise) the objective function. Finally, a set of constraints restrict the values that the decision variables can take. Integer Linear Programming is an extension of linear programming where all decision variables must take integer values. There are several explicit formulations of the MST problem as an integer linear program in the literature (Williams, 2002). They are based on the concept of eliminating subtours (cycles), cuts (disconnections) or requiring intervertex flows (paths). However, in practice these formulations cause long solve times — as the first two methf101(i, j, l) = 131 Algorithm 1 Incremental Integer Linear Programming C +— Bx repeat y +— solve(C, Ox, Vx) W +— violated(y, Ix) C+—CUW until V = 0 return y ods yield an exponential number of constraints. Although the latter scales cubically, it produces non-fractional solutions in its relaxed version; this causes long runtimes for the branch and bound algorithm (Williams, 2002) comm</context>
</contexts>
<marker>Williams, 2002</marker>
<rawString>Williams, Justin C. 2002. A linear-size zero - one programming model for the minimum spanning tree problem in planar graphs. Networks 39:53–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wayne L Winston</author>
<author>Munirpallam Venkataramanan</author>
</authors>
<date>2003</date>
<booktitle>Introduction to Mathematical Programming. Brooks/Cole.</booktitle>
<contexts>
<context position="10480" citStr="Winston and Venkataramanan (2003)" startWordPosition="1730" endWordPosition="1734">ossing dependencies. This corresponds to projective parsing. 3.1 Decoding McDonald et al. (2005b) use the Chu-LiuEdmonds (CLE) algorithm to solve the maximum spanning tree problem. However, global constraints cannot be incorporated into the CLE algorithm (McDonald et al., 2005b). We alleviate this problem by presenting an equivalent Integer Linear Programming formulation which allows us to incorporate global constraints naturally. Before giving full details of our formulation we first introduce some of the concepts of linear and integer linear programming (for a more thorough introduction see Winston and Venkataramanan (2003)). Linear Programming (LP) is a tool for solving optimisation problems in which the aim is to maximise (or minimise) a given linear function with respect to a set of linear constraints. The function to be maximised (or minimised) is referred to as the objective function. A number of decision variables are under our control which exert influence on the objective function. Specifically, they have to be optimised in order to maximise (or minimise) the objective function. Finally, a set of constraints restrict the values that the decision variables can take. Integer Linear Programming is an extens</context>
</contexts>
<marker>Winston, Venkataramanan, 2003</marker>
<rawString>Winston, Wayne L. and Munirpallam Venkataramanan. 2003. Introduction to Mathematical Programming. Brooks/Cole.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroyasu Yamada</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Statistical dependency analysis with support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings ofIWPT.</booktitle>
<pages>195--206</pages>
<contexts>
<context position="7090" citStr="Yamada and Matsumoto, 2003" startWordPosition="1128" endWordPosition="1131">ns a coordination of incompatible word classes (a preposition and a verb). Our approach is able to include additional constraints which forbid configurations such as those in Figure 2. While McDonald and Pereira (2006) address the issue of local attachment decisions by defining scores over attachment pairs, our solution is more general. Furthermore, it is complementary in the sense that we could formulate their model using ILP and then add constraints. The method we present is not the only one that can take global constraints into account. Deterministic dependency parsing (Nivre et al., 2004; Yamada and Matsumoto, 2003) can apply global constraints by conditioning attachment decisions on the intermediate parse built. However, for efficiency a greedy search is used which may produce sub-optimal solutions. This is not the case when using ILP. 3 Model Our underlying model is a modified labelled version2 of McDonald et al. (2005b): s(x, y) _ � s(i, j, l) (i,j,l)Ey �� w · f(i, j, l) (i,j,l)Ey 2Note that this is not described in the McDonald papers but implemented in his software. 130 where x is a sentence, y is a set of labelled dependencies, f(i, j, l) is a multidimensional feature vector representation of the e</context>
</contexts>
<marker>Yamada, Matsumoto, 2003</marker>
<rawString>Yamada, Hiroyasu and Yuji Matsumoto. 2003. Statistical dependency analysis with support vector machines. In Proceedings ofIWPT. pages 195–206.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>