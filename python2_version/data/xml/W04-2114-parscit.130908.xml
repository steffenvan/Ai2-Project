<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004045">
<title confidence="0.95524">
Rebuilding the Oxford Dictionary of English as a semantic network
</title>
<author confidence="0.997186">
James McCRACKEN
</author>
<affiliation confidence="0.7851285">
Oxford University Press
Great Clarendon Street
</affiliation>
<address confidence="0.989405">
Oxford OX2 6DP, UK
</address>
<email confidence="0.997028">
james.mccracken@oup.com
</email>
<sectionHeader confidence="0.992444" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996486">
This paper describes a project to develop a lexicon
for use both as an electronic dictionary and as a
database for a range of NLP tasks. It proposes that
a lexicon for such open-ended application may be
derived from a human-user dictionary, retaining
and enhancing the richness of its editorial content
but abandoning its entry-list structure in favour of
networks of relationships between discrete lexical
objects, where each object represents a discrete
lexeme-meaning unit.
</bodyText>
<sectionHeader confidence="0.998481" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999920631578948">
Dictionaries intended for human users typically
sacrifice strict formalism, structure, and
consistency in the interests of space and
readability. Furthermore, a dictionary may assume
a certain level of knowledge on the part of the
reader, and so need not always be exhaustively
explicit about all aspects of a given lexeme. On the
other hand, traditional high-level lexicography is
valuable for the rich variety of detailed, complex
information integrated in each entry, usually with
very low error rates. Clearly, then, there are both
benefits and problems in attempting to mine or
query such a dictionary computationally.1 The
project described here attempts to retain the rich
editorial content of a dictionary but to reorganize it
in structured networks that are more readily
processed and traversed - in other words, to
preserve the benefits while smoothing out some of
the problems.
</bodyText>
<sectionHeader confidence="0.982973" genericHeader="introduction">
2 Method
</sectionHeader>
<bodyText confidence="0.998112">
The major steps in this process were as follows:
1. Decomposition of the dictionary as a flat
list of entries, to be replaced by a set of
lexical objects (each corresponding
roughly to a single lexeme-meaning pair in
the original dictionary, and minimally
retaining all the text of that meaning)
(section 4 below).
</bodyText>
<note confidence="0.5648055">
1 See Ide and Veronis 1993 for a good account of this
tension.
</note>
<listItem confidence="0.991147857142857">
2. Population of each lexical object with a
complete set of morphological and
syntactic data (section 5 below).
3. Classification connecting lexical objects to
each other in semantic hierarchies and
networks of domain relationships (section
6 below).
</listItem>
<sectionHeader confidence="0.520459" genericHeader="method">
3 Choice of dictionary
</sectionHeader>
<bodyText confidence="0.934111696969697">
The dictionary selected was the Oxford
Dictionary of English (2003) (ODE). This is a
relatively high-level dictionary, intended for fluent
English speakers rather than learners. Accordingly,
it assumes a body of general knowledge and a
degree of semantic and grammatical competence
on the part of the user: not everything is made
explicit in the text.2 Computational analysis of the
text must be designed to identify and aggregate
cues, and to employ rules for inheriting
information from entry or branch level down to
sense level, in order to generate the right data to
construct each lexical object (see section 4 below).
On the other hand, ODE has some key features
that make it particularly appropriate for
enhancement a as comprehensive electronic lexical
database, usable both as a dictionary and as a
resource for exploitation in NLP applications:
· even-handed treatment of both British and
American spellings, plus extensive
coverage of other World English;
· coverage of proper-name items (places,
people, events, etc.);
· relatively codified editorial style,
facilitating computational evaluation of
definitions;
· detailed and codified indication of
syntactic characteristics, sense by sense;
· example sentences linked to individual
senses, providing cues about grammatical
behaviour and collocation.
Furthermore, ODE is very much a corpus-based
dictionary: all key editorial decisions - including
</bodyText>
<footnote confidence="0.5994865">
2 An example of this in ODE is the high number of
run-on derivatives, which are undefined on the
assumption that the user will be able to infer the sense
morphologically.
</footnote>
<bodyText confidence="0.999941">
orthography, sense distinctions, choice of example
sentences, and syntactic information - are
motivated primarily by corpus evidence. This
means that a formalized encoding of the
information presented in the dictionary is likely to
generate robust and comprehensive resources for
NLP tasks dealing with arbitrary real-world text.
</bodyText>
<subsectionHeader confidence="0.55072">
4 Data structure and lexical objects
</subsectionHeader>
<bodyText confidence="0.999924122448979">
The ODE data structure dispenses with the idea
of the entry as the dictionary&apos;s basic unit. This
move reflects the proposition that the traditional
dictionary entry is an artefact of human-readable
dictionaries, and expresses a predominantly
etymological view of the language. The dictionary
entry brings together lexical elements (definitions,
parts of speech, phrases, etc.) which may have
little relationship to each other from the point of
view of language use (meaning, domain,
frequency, collocation, etc.). The entry is therefore
an inappropriate unit for computer processing.
Instead, the ODE data structure is built as a
series of discrete lexical objects which correspond
roughly to individual dictionary senses or to senses
of embedded lemmas (phrases, compounds,
derivatives, etc.). These lexical objects function as
packets of data which exist independently of each
other, each containing all relevant information
about meaning, morphology, lexical function,
semantic category, etc. Hence every sense may be
queried, extracted, or manipulated as an
independent object without any reference to the
entry in which it was originally situated.
Although search results, etc., may still
(typically) be represented to the user in the form of
traditional dictionary entries, from a functional
point of view the entry is rendered redundant as a
data object, and for almost all purposes is ignored
when handling search queries and other
processing.
The choice of lexeme-meaning pair rather than
entry as the basic lexical object allows a much
more detailed and exhaustive specification of the
way the language functions on a sense-by-sense
basis. Each object typically includes (minimally) a
definition, one or more example sentences, a full
specification of morphology and syntactic
behaviour (see section 5 below), and a range of
classifiers (see section 6 below). This creates some
redundancy: for example, an entry with numerous
senses may generate several lexical objects which
repeat the same morphology and syntactic
behaviour. However, this is more than offset by the
advantages: by integrating data into self-contained
lexical objects, the structure (a) supports robust
and positive Boolean operations and (b) opens up
the lexicon for reconfiguration according to any
new type of lexical or semantic relation.
</bodyText>
<sectionHeader confidence="0.877575" genericHeader="method">
5 Morphological and phrasal specification
</sectionHeader>
<bodyText confidence="0.992498666666667">
Every lexical object in ODE provides a complete
specification of all the lexical forms relevant to its
sense. This includes not only the morphology of
the lexemes themselves, but also structured data
about their syntactic roles, variant spellings, British
and US spellings, and alternative forms. This is
true not only for single-word lexemes but also for
abbreviations, initialisms, affixes, contractions,
compounds, phrases, and proper names. ODE thus
provides facility for look-up of real-world lexical
forms in context, including:
As mentioned above, such data was generated
for every lexical object (sense). For some
polysemous headwords this may mean multiple
repetitions of morphological listings. More
typically, however, listings vary from sense to
sense. One common variable, for example, is that
for a given noun headword, some lexical objects
may include a plural while others do not. Similarly,
variant spellings may often be associated with
some but not all lexical objects. More pertinently
perhaps, phrasal patterns associated with the
lexeme will vary significantly from sense to sense,
particularly for verbs.
It should be noted that this formal variability can
play an important role in the early stages of
disambiguation tasks; the particular form of a
lexeme or extended phrase may often allow groups
of senses to be reliably discounted or at least
downgraded as possible meanings.3 This simplifies
the set of candidate meanings subsequently
presented to semantic analysis algorithms.
Hence it was judged necessary to develop
distinctive morphological and phrasal
specifications for each lexical object. Principally
this was done algorithmically: the dictionary text
associated with each lexical object was examined
for clues not only in explicit syntax information
but also in definition patterns and especially in
example sentences. In default of any strong
pointers emerging from this process, forms were
produced by a simple morphological generator.
However, it became apparent that many formal
variations were not cued in any way by the
dictionary text. As a simple example, if an
adjective is gradable then this is always indicated
3 John Sinclair has proposed that &apos;Every distinction in
meaning is associated with a distinction in form&apos; (quoted
in Hanks 2003, p. 61). Although doubtful about this as a
universal rule, I agree that for a given lexeme one may
often link major categories of meaning to typical formal
characteristics.
at headword level in the dictionary, but there is
nothing to indicate which senses are gradable and
which are not. Because of limitations like this, a
significant stage of post-processing manual
correction was necessary.
</bodyText>
<sectionHeader confidence="0.995978" genericHeader="method">
6 Classification
</sectionHeader>
<bodyText confidence="0.998643071428572">
Each lexical object includes a set of classifiers
which are used to position the object&apos;s definition in
semantic taxonomies (structures of
superordination/subordination) and in subject areas
(domains).
This may be used in two principal ways. Firstly,
it allows the dictionary to be navigated along
associative or thesaurus-like routes: it structures
the lexicon’s set of objects into a network of
connections between meanings. Secondly, it allows
calculations of semantic distance between lexical
objects, supporting NLP tasks relating to sense
disambiguation, context-sensitive look-up, and
document classification.4
</bodyText>
<subsectionHeader confidence="0.998977">
6.1 Semantic taxonomy
</subsectionHeader>
<bodyText confidence="0.999425928571428">
All lexical objects representing noun meanings
(about 100,000) have been structured in a semantic
taxonomy. This represents a substantial majority of
all objects; the smaller sets of verbs and adjectives
will be similarly structured as the next stage of the
project.
The taxonomy&apos;s high-level nodes (major
classifications) were originally constructed to
follow the file structure of WordNet nouns.
Although the ODE taxonomy has subsequently
diverged in detail, mappings to WordNet have
been retained. At the early stages of the project, a
relatively small number of noun objects were
classified manually. The definitions contained in
these objects were then used as training data to
establish a definitional &apos;profile&apos; for each high-level
node. Definitional profiles were used to
automatically classify further objects. Applied
iteratively (with manual correction at each stage),
this process succeeded in classifying all noun
objects in a relatively coarse-grained way.
Beyond this stage, the statistical data which had
previously been integrated to build definition
profiles was instead decomposed to help identify
ways in which high-level nodes could be
subdivided to develop greater granularity.
Definitional profiling here involves two
elements:
</bodyText>
<footnote confidence="0.995509">
4 Metrics for using taxonomies to calculate semantic
similarity are discussed in Leacock and Chodorow
1998. An implementation of ODE as an electronic
dictionary uses such metrics in the automatic glossing of
its own text.
</footnote>
<listItem confidence="0.940961625">
1. Identification of the &apos;key term&apos; in the
definition. This is the most significant
noun in the definition. It is not always
coterminous with the genus term; for
example, in a definition beginning &apos;a
morsel of food which...&apos;, the &apos;key term&apos; is
calculated to be food rather than morsel.
2. Stemming and scoring of all other
</listItem>
<bodyText confidence="0.99239425">
meaningful lexemes in the definition
(ignoring articles, conjunctions, etc.). A
simple weighting scheme is used to give
more importance to lexemes at the
beginning of a definition (e.g. a modifier
of the key term) than to lexemes at the end.
These two elements are then assigned mutual
information scores in relation to each possible
classification, and the two MI scores are combined
in order to give an overall score. This overall score
is taken to be a measure of how &apos;typical&apos; a given
definition would be for a given classification. This
enables one very readily to rank all the lexical
objects attached to a given node, and to identify
other objects which are candidates for that node.
The semantic taxonomy currently has about
2200 nodes on up to 12 levels - on average, 46
objects per node. However, this average disguises
the fact that there are a small number of nodes
which classify significantly larger sets of objects.
Further subcategorization of large sets is desirable
in principle, but is not considered a priority in all
cases: subcategorization of a given set is
deprioritized if the set if relatively homogeneous,
i.e. if the distribution of &apos;typicality&apos; scores for each
object is relatively small.&apos;
Hence the goal is not achieve granularity on the
order of WordNet&apos;s &apos;synset&apos; (a set in which all
terms are synonymous, and hence are rarely more
than four or five in number). Instead, granularity is
based on a more thesaurus-like measure of parity
between objects in a set.
</bodyText>
<subsectionHeader confidence="0.998455">
6.2 Domains
</subsectionHeader>
<bodyText confidence="0.995426585365854">
As with semantic classification, a number of
domain indicators were assigned manually, and
these were then used iteratively to seed assignment
of further indicators to statistically similar
definitions. Automatic assignment is a little more
straightforward and robust here, since most of the
time the occurrence of strongly-typed vocabulary
will be a sufficient cue, and there is little necessity
&apos; For example, the tree set is several times larger than
average, but since tree definitions have similar profiles,
the set produces a high homogeneity score. This accords
loosely with the intuition that for most NLP purposes,
there is little value in making semantic distinctions
between different species of tree.
to identify a key term or otherwise to parse the
definition.
Not every lexical object is domain-specific:
currently 49% of all objects have been assigned at
least one domain marker. Each iteration of the
assignment process will continue to capture objects
which are less and less strongly related to the
domain. Beyond a certain point, the relationship
will become too tenuous to be of much use in most
contexts; but that point will differ for each subject
field (and for each context). Hence a further
objective is to implement a grading scheme which
not only classifies an object by domain but also
scores its relevance to that domain.
Currently, a set of about 220 domains are used,
both technical (Pharmaceutics, Oceanography ,)
and thematic (Crime, Sex). These were originally
planned to be organized in a Dewey-like
taxonomy. However, this was found to be
unworkable, since within the lexicon domains
emerged as a series of overlapping fields rather
than as a hierarchy. Hence the domains have now
been reorganized not taxonomically but rather as a
network of multiple connections. Within the
network, each edge connecting two domains
represents the strength of association between
those domains.&apos;
</bodyText>
<sectionHeader confidence="0.914035" genericHeader="method">
7 Work in progress
</sectionHeader>
<bodyText confidence="0.999969444444444">
Ongoing work focuses mainly on the
cumulative population of lexical objects with
additional data fields, in particular collocations and
frequency measures.
Additionally, further classifier types are being
trialled in order to define further relations between
lexical objects. These include linguistic relations
such as antonymy and root derivation, and real-
world relations such as geographical region.
</bodyText>
<sectionHeader confidence="0.997334" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.9999732">
The strategy used in constructing the ODE
electronic database was to preserve the full
editorial content of a human-user dictionary but to
rebuild its structure, replacing the entry-list
paradigm with a manifold network of relations
between meanings.
The key benefit of this approach is in the
versatility of the database. Lexical objects may be
reassembled into entries for display, so ODE can
still function as an electronic human-user
</bodyText>
<footnote confidence="0.457355166666667">
&apos; Strength of association from Domain_A to
Domain_B is determined internally to ODE, by
calculating the proportion of (a) lexemes and (b)
semantic sets in which both domains appear, as opposed
to those in which only Domain_A appears. Strength of
association is not mutual.
</footnote>
<bodyText confidence="0.99993496">
dictionary, albeit one that takes advantage of novel
search and navigation features. Additionally, ODE
is directly usable in a number of non-dictionary
applications. These include context-sensitive
spellchecking, tagging and parsing, document
categorization, and context-sensitive document
glossing.
Feedback from such applications is being
monitored not only to critically examine and
correct the source data, but also to examine the
source dictionary itself: because much of the
formal and classificatory data is generated
algorithmically from analysis of the source
editorial content of each lexical object (definition,
etc.), anomalies emerging in that data can often be
traced back to anomalies in the editorial content
(e.g. inconsistencies in defining style).
The ODE project is therefore in part an
attempt to bridge the distinction between human-
user dictionaries and WordNet-like associative
electronic lexicons. By deriving the one from the
other, it invites applications to navigate and mine
the rich lexicographic content of the original
dictionary by means of a new set of structured
relations and frameworks.
</bodyText>
<sectionHeader confidence="0.994752" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999976333333333">
I would like to thank Adam Kilgarriff of ITRI,
Brighton, who has played a key role in advising on
many aspects of the work described here.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.993156157894737">
Hanks, P. 2003. &apos;Lexicography&apos;. In R. Mitkov (ed.),
The Oxford Handbook of Computational
Linguistics. Oxford: Oxford University Press.
Ide, N. and J. Veronis, 1993. &apos;Extracting
knowledge bases from machine-readable
dictionaries: have we wasted our time?&apos;. in
KB&amp;KS Workshop . Tokyo.
Leacock, C. and M. Chodorow. 1998. &apos;Combining
local context and WordNet similarity for word
sense identification&apos;. In C. Fellbaum (ed.),
WordNet: An Electronic Lexical Database .
Cambridge, Mass.: MIT Press.
Rigau, G., 1994. &apos;An experiment on automatic
semantic tagging of dictionary senses&apos;. In
Proceedings of the workshop &apos;The Future of
Dictionary&apos;. Aix-les-Bains, France.
Soanes, C. and A. Stevenson. 2003. Oxford
Dictionary of English. Oxford: Oxford
University Press.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.521026">
<title confidence="0.999795">the Dictionary of English a semantic network</title>
<author confidence="0.999939">James McCRACKEN</author>
<affiliation confidence="0.777642">Oxford University Press Great Clarendon Street</affiliation>
<address confidence="0.970953">Oxford OX2 6DP, UK</address>
<email confidence="0.999734">james.mccracken@oup.com</email>
<abstract confidence="0.998175545454545">This paper describes a project to develop a lexicon for use both as an electronic dictionary and as a database for a range of NLP tasks. It proposes that a lexicon for such open-ended application may be derived from a human-user dictionary, retaining and enhancing the richness of its editorial content but abandoning its entry-list structure in favour of networks of relationships between discrete lexical objects, where each object represents a discrete lexeme-meaning unit.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>P Hanks</author>
</authors>
<title>Lexicography&apos;. In</title>
<date>2003</date>
<booktitle>The Oxford Handbook of Computational Linguistics. Oxford:</booktitle>
<editor>R. Mitkov (ed.),</editor>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="8822" citStr="Hanks 2003" startWordPosition="1336" endWordPosition="1337">ionary text associated with each lexical object was examined for clues not only in explicit syntax information but also in definition patterns and especially in example sentences. In default of any strong pointers emerging from this process, forms were produced by a simple morphological generator. However, it became apparent that many formal variations were not cued in any way by the dictionary text. As a simple example, if an adjective is gradable then this is always indicated 3 John Sinclair has proposed that &apos;Every distinction in meaning is associated with a distinction in form&apos; (quoted in Hanks 2003, p. 61). Although doubtful about this as a universal rule, I agree that for a given lexeme one may often link major categories of meaning to typical formal characteristics. at headword level in the dictionary, but there is nothing to indicate which senses are gradable and which are not. Because of limitations like this, a significant stage of post-processing manual correction was necessary. 6 Classification Each lexical object includes a set of classifiers which are used to position the object&apos;s definition in semantic taxonomies (structures of superordination/subordination) and in subject are</context>
</contexts>
<marker>Hanks, 2003</marker>
<rawString>Hanks, P. 2003. &apos;Lexicography&apos;. In R. Mitkov (ed.), The Oxford Handbook of Computational Linguistics. Oxford: Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>N Ide</author>
<author>J Veronis</author>
</authors>
<title>Extracting knowledge bases from machine-readable dictionaries: have we wasted our time?&apos;.</title>
<date>1993</date>
<booktitle>in KB&amp;KS Workshop .</booktitle>
<location>Tokyo.</location>
<contexts>
<context position="1912" citStr="Ide and Veronis 1993" startWordPosition="295" endWordPosition="298">lly.1 The project described here attempts to retain the rich editorial content of a dictionary but to reorganize it in structured networks that are more readily processed and traversed - in other words, to preserve the benefits while smoothing out some of the problems. 2 Method The major steps in this process were as follows: 1. Decomposition of the dictionary as a flat list of entries, to be replaced by a set of lexical objects (each corresponding roughly to a single lexeme-meaning pair in the original dictionary, and minimally retaining all the text of that meaning) (section 4 below). 1 See Ide and Veronis 1993 for a good account of this tension. 2. Population of each lexical object with a complete set of morphological and syntactic data (section 5 below). 3. Classification connecting lexical objects to each other in semantic hierarchies and networks of domain relationships (section 6 below). 3 Choice of dictionary The dictionary selected was the Oxford Dictionary of English (2003) (ODE). This is a relatively high-level dictionary, intended for fluent English speakers rather than learners. Accordingly, it assumes a body of general knowledge and a degree of semantic and grammatical competence on the </context>
</contexts>
<marker>Ide, Veronis, 1993</marker>
<rawString>Ide, N. and J. Veronis, 1993. &apos;Extracting knowledge bases from machine-readable dictionaries: have we wasted our time?&apos;. in KB&amp;KS Workshop . Tokyo.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Leacock</author>
<author>M Chodorow</author>
</authors>
<title>Combining local context and WordNet similarity for word sense identification&apos;.</title>
<date>1998</date>
<booktitle>WordNet: An Electronic Lexical Database .</booktitle>
<editor>In C. Fellbaum (ed.),</editor>
<publisher>MIT Press.</publisher>
<location>Cambridge, Mass.:</location>
<contexts>
<context position="11230" citStr="Leacock and Chodorow 1998" startWordPosition="1682" endWordPosition="1685">-level node. Definitional profiles were used to automatically classify further objects. Applied iteratively (with manual correction at each stage), this process succeeded in classifying all noun objects in a relatively coarse-grained way. Beyond this stage, the statistical data which had previously been integrated to build definition profiles was instead decomposed to help identify ways in which high-level nodes could be subdivided to develop greater granularity. Definitional profiling here involves two elements: 4 Metrics for using taxonomies to calculate semantic similarity are discussed in Leacock and Chodorow 1998. An implementation of ODE as an electronic dictionary uses such metrics in the automatic glossing of its own text. 1. Identification of the &apos;key term&apos; in the definition. This is the most significant noun in the definition. It is not always coterminous with the genus term; for example, in a definition beginning &apos;a morsel of food which...&apos;, the &apos;key term&apos; is calculated to be food rather than morsel. 2. Stemming and scoring of all other meaningful lexemes in the definition (ignoring articles, conjunctions, etc.). A simple weighting scheme is used to give more importance to lexemes at the beginni</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Leacock, C. and M. Chodorow. 1998. &apos;Combining local context and WordNet similarity for word sense identification&apos;. In C. Fellbaum (ed.), WordNet: An Electronic Lexical Database . Cambridge, Mass.: MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Rigau</author>
</authors>
<title>An experiment on automatic semantic tagging of dictionary senses&apos;.</title>
<date>1994</date>
<booktitle>In Proceedings of the workshop &apos;The Future of Dictionary&apos;. Aix-les-Bains,</booktitle>
<marker>Rigau, 1994</marker>
<rawString>Rigau, G., 1994. &apos;An experiment on automatic semantic tagging of dictionary senses&apos;. In Proceedings of the workshop &apos;The Future of Dictionary&apos;. Aix-les-Bains, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Soanes</author>
<author>A Stevenson</author>
</authors>
<date>2003</date>
<booktitle>Oxford Dictionary of English.</booktitle>
<publisher>University Press.</publisher>
<location>Oxford: Oxford</location>
<marker>Soanes, Stevenson, 2003</marker>
<rawString>Soanes, C. and A. Stevenson. 2003. Oxford Dictionary of English. Oxford: Oxford University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>