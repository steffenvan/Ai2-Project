<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000314">
<title confidence="0.999633333333333">
A Structure-Sharing Representation
for
Unification-Based Grammar Formalisms
</title>
<author confidence="0.88309">
Fernando C. N. Pereira
</author>
<affiliation confidence="0.5285195">
Artificial Intelligence Center, SRI International
and
Center for the Study of Language and Information
Stanford University
</affiliation>
<sectionHeader confidence="0.97204" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999168125">
This paper describes a structure-sharing method for the rep-
resentation of complex phrase types in a parser for PATR-II,
a unification-based grammar formalism.
In parsers for unification-based grammar formalisms,
complex phrase types are derived by incremental refinement
of the phrase types defined in grammar rules and lexical
entries. In a naïve implementation, a new phrase type is
built by copying older ones and then combining the copies
according to the constraints stated in a grammar rule. The
structure-sharing method was designed to eliminate most
such copying; indeed, practical tests suggest that the use of
this technique reduces parsing time by as much as 60%.
The present work is inspired by the structure-sharing
method for theorem proving introduced by Boyer and Moore
and on the variant of it that is used in some Prolog imple-
mentations.
</bodyText>
<sectionHeader confidence="0.997546" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.969048038461538">
In this paper I describe a method, structure sharing, for
the representation of complex phrase types in &apos;a parser for
PATR-II, a unification-based grammar formalism.
In parsers for unification-based grammar formalisms,
complex phrase types are derived by incremental refinement
of the phrase types defined in grammar rules and lexical
entries. In a naïve implementation, a new phrase type is
built by copying older ones and then combining the copies
according to the constraints stated in a grammar rule. The
structure-sharing method eliminates most such copying by
This research, made possible in part by a gift from the Systems De-
velopment Foundation, was also supported by the Defense Advanced
Research Projects Agency under Contracts N00039-80-C-0575 and
N00039-84-C-0524 with the Naval Electronic Systems Command. The
views and conclusions contained in this document are those of the au-
thor and should not be interpreted as representative of the official
policies, either expressed or implied, of the Defense Advanced Re-
search Projects Agency, or the United States government.
Thanks are due to Stuart Shieber, Lauri Karttunen. and Ray Per-
rault for their comments on earlier presentations of this material.
representing updates to objects (phrase types) separately
from the objects themselves.
The present work is inspired by the structure-sharing
method for theorem proving introduced by Boyer and Moore
[I] and on the variant of it that is used in some Prolog im-
plementations pl.
</bodyText>
<sectionHeader confidence="0.941574" genericHeader="introduction">
2 Grammars with Unification
</sectionHeader>
<bodyText confidence="0.945838111111111">
The data representation discussed in this paper is applicable,
with but minor changes, to a variety of grammar formalisms
based on unification, such as definite-clause grammars [61,
functional-unification grammar [11, lexical-functional gram-
mar [21 and PATR-II 181. For the sake of concreteness, how-
ever, our discussion will be in terms of the PATR-II formal-
ism.
The basic idea of unification-based grammar formalisms is
very simple. As with context-free g-ramtnars. grammar rules
state how phrase types combine to yield other phrase types.
But where:Ls a context-free grammar allows only a finite
number of predefined atomic phrase types or nonterntinabi,
a unification-based grammar will in general define implicitly
an infinity of phrase types.
A phrase type is defined by a set of constraints. A gram-
mar rule is a set of constraints bottreort the type V1 of a
phrase and the types V„ of its rt,iv-,titiierits. The
rule may be applied to the analysis of a string so As the
concao•nation of constituents st,.....q„ if and only if the
types the s, are compatible with the types and the
constraints in the rule.
Unification is the operation that determines whether two
types are compatible by building the most general type com-
patible with both.
If the constraints are equations between attribtites of
phrase types, as is the ease in PATR-11, two phrase R pes
can be unified %chem.\ er they do not assign instinct ■alues
</bodyText>
<footnote confidence="0.91996775">
to the same attribillv. The hum is is then j imsi i he cmi-
junction (set union) of the corresixinding sets of constraints
[51.
here is a sample rule, in a simplified version of the PATR-
</footnote>
<page confidence="0.985532">
137
</page>
<equation confidence="0.894876428571428">
II notation:
X0 Xi X2 : (X0 cat) = S
(X1 cat) = NP
(X2 cat) = VP
(X, agr) = (X2 agr)
(X0 trans) = (X2 trans)
(X0 trans arg,) = (X1 trans)
</equation>
<bodyText confidence="0.999978125">
This rule may be read as stating that a phrase of type X0
can be the concatenation of a phrase of type X, and a phrase
of type X2 provided that the attribute equations of the rule
are satisfied if the phrases are substituted for their types.
The equations state that phrases of types Xo, XI, and X2
have categories S, NP, and VP, respectively, that types X1
and X2 have the same agreement value, that types X0 and
X2 have the same translation, and that the first argument
of „Ws translation is the translation of X1.
Formally, the expressions of the form (l, 1,,,) used in
attribute equations are paths and each 1, is a label.
When all the phrase types in a rule are given constant
cat (category) values by the rule, we can use an abbreviated
notation in which the phrase type variables Xi are replaced
by their category values and the category-setting equations
are omitted. For example, rule (1) may be written as
</bodyText>
<equation confidence="0.988787">
S NP VP: (NP agr) = (VP agr)
(S trans)
= (VP trans) (2)
(S trans argi) = (NP trans)
</equation>
<bodyText confidence="0.999911086956522">
In existing PATR-II implementations, phrase types are
not actually represented by their sets of defining equations.
Instead, they are represented by symbolic solutions of the
equations in the form of directed acyclic graphs (dags) with
arcs labeled by the attributes used in the equations. Dag
nodes represent the values of attributes and an arc labeled
by I goes from node m to node n if and only if, according
to the equations, the value represented by m has n as the
value of its / attribute [51.
A dag node (and by extension a dag) is said to be atomic
if it represents a constant value; complex if it has some out-
going arcs; and a leaf if is is neither atomic or complex, that
is, if it represents an as yet completely undetermined value.
The domain dom(d) of a complex dag d is the set of labels
on arcs leaving the top node of d. Given a dag d and a label
I E dom(d) we denote by d/I the subdag of d at the end of
the arc labeled 1 from the top node of d. By extension, for
any path p whose labels are in the domains of the appropri-
ate subdags, d/p represents the subdag of d at the end of
path p from the root of d.
For uniformity, lexical entries and grammar rules are also
represented by appropriate dags. For example, the dag for
rule (1) is shown in Figure 1.
</bodyText>
<sectionHeader confidence="0.992941" genericHeader="method">
3 The Problem
</sectionHeader>
<bodyText confidence="0.972071">
In a chart parser 13) all the intermediate stages of deriva-
tions are encoded in edges, representing either incomplete
</bodyText>
<figure confidence="0.557069">
trans
</figure>
<figureCaption confidence="0.999448">
Figure 1: Dag Representation of a Rule
</figureCaption>
<bodyText confidence="0.999947227272727">
(active) or complete (passive) phrases. For PATR-II, each
edge contains a dag instance that represents the phrase type
of that edge. The problem we address here is how to encode
multiple dag instances efficiently.
In a chart parser for context-free grammars, the solution
is trivial: instances can be represented by the unique inter-
nal names (that is, addresses) of their objects because the
information contained in an instance is exactly the same as
that in the original object.
In a parser for PATR-II or any other unification-based for-
malism, however, distinct instances of an object will in gen-
eral specify different values for attributes left unspecified in
the original object. Clearly, the attribute values specified for
one instance are independent of those for another instance
of the same object.
One obvious solution is to build new instances by copy-
ing the original object and then updating the copy with the
new attribute values. This was the solution adopted in the
first PATR-II parser [SI. The high cost of this solution both
in time spent copying and in space required for the copies
themselves constitutes the principal justification for employ-
ing the method described here.
</bodyText>
<sectionHeader confidence="0.934563" genericHeader="method">
4 Structure Sharing
</sectionHeader>
<bodyText confidence="0.9999149">
Structure sharing is based on the observation that an ini-
tial object, together with a list of update records, contains
the same information as the object that results from apply-
ing the updates to the initial object. In this way, we can
trade the coat of actually applying the updates (with pos-
sible copying to avoid the destruction of the source object)
against the cost of having to compute the effects of updates
when examining the derived object. This reasoning applies
in particular to dag instances that are the result of adding
attribute values to other instances.
</bodyText>
<equation confidence="0.890356">
(1)
</equation>
<page confidence="0.966134">
138
</page>
<bodyText confidence="0.997564333333333">
As in the variant of Boyer and Moore&apos;s method (I] used
in Prolog [91, I shall represent a dag instance by a molecule
(see Figure 2) consisting of
</bodyText>
<listItem confidence="0.913909333333333">
1. [A pointer to( the initial dag, the instance&apos;s skeleton
2. [A pointer tol a table of updates of the skeleton, the
instance&apos;s environment.
</listItem>
<bodyText confidence="0.72974375">
Environments may contain two kinds of updates: reroutings
that replace a dag node with another dag; arc bindings that
add to a node a new outgoing arc pointing to a dag. Figure
3 shows the unification of the dags
</bodyText>
<equation confidence="0.9307455">
/I = [a:x,b:yj
/2 = (c : [el : el]
</equation>
<bodyText confidence="0.999806576923077">
After unification, the top node of h is rerouted to h and the
top node of /1 gets an arc binding with label c and a value
that is the subdag (d : el of /2. As we shall see later, any up-
date of a dag represented by a molecule is either an update
of the molecule&apos;s skeleton or an update of a dag (to which
the same reasoning applies) appearing in the molecule&apos;s en-
viroment. Therefore, the updates in a molecule&apos;s environ-
ment are always shown in figures tagged by a boxed number
identifying the affected node in the molecule&apos;s skeleton.
The choice of which dag is rerouted and which one gets
arc bindings is arbitrary.
For reasons discussed later, the cost of looking up instance
node updates in Boyer and Moore&apos;s environment represen-
tation is 0(Id1), where Idl is the length of the derivation (a
sequence of resolutions) of the instance. In the present rep-
resentation, however, this cost is only 0(log ldl). This better
performance is achieved by particularizing the environment
representation and by splitting the representational scheme
into two components: a memory organization and a dag rep-
resentation.
A dag representation is a way of mapping the mathemat-
ical entity dag onto a memory. A memory organization is a
way of putting together a memory that has certain proper-
ties with respect to lookup, updating and copying. One can
think of the memory organization as the hardware and the
dag representation as the data structure.
</bodyText>
<sectionHeader confidence="0.986003" genericHeader="method">
5 Memory organization
</sectionHeader>
<bodyText confidence="0.999884909090909">
In practice, random-access memory can be accessed and up-
dated in constant time. However, updates destroy old val-
ues, which is obviously unacceptable when dealing with al-
ternative updates of the same data structure. If we want to
keep the old version, we need to copy it first into a sepa-
rate part of memory and change the copy instead. For the
normal kind of memory, copying time is proportional to the
size of the object copied.
The present scheme uses another type of memory orga-
nization — virtual-copy arrays — which requires 0(logn)
time to access or update an array with highest used index
</bodyText>
<equation confidence="0.624149">
k =2 n = 30 = 132 (base 4)
a: a(n) = f
0(a) = 3
</equation>
<figureCaption confidence="0.998136">
Figure 4: Virtual-Copy Array
</figureCaption>
<bodyText confidence="0.999721250000001">
of n, but in which the old contents are not destroyed by up-
dating. Virtual-copy arrays were developed by David H. D.
Warren [101 as an implementation of extensible arrays for
Prolog.
Virtual-copy arrays provide a fully general memory struc-
ture: anything that can be stored in randorn-acress mem-
ory can be stored in virtual-copy arrays. although pointers
in machine memory correspond to indexes in a virtual-copy
array. An updating operation takes a virtual-copy array, an
index, and a new value and returns a new virtual-copy array
with the new value stored at the given index. An access op-
eration takes an array and an index, and returns the value
at that index.
Basically, virtual-copy arrays are 2h-ary trees for sonic
fixed k &gt; 0. Define the depth d(n) of a tree node re
to be 0 for the root and d(p) + 1 if p is the parent of
n. Each virtual-copy array a has also a positive depth
D(a) &gt; max{d(n) : n is a node of a}. A tree node at depth
D(a) (necessarily a leaf) can be either an array element
or the special marker for unassigned elements. All leaf
nodes at depths lower than D(a) are also 1, indicating that
no elements have yet been stored in the subarray below the
node. With this arrangement, the array can store at most
21.13(o) elements, numbered 0 through 2kD(a) — I, but unused
subarrays need not be allocated.
By numbering the 2h daughters of a nonleaf node from 0
to 2h —1, a path from a&apos;s root to an array element (a leaf at
depth D(a)) can be represented by a sequence n0.
in which nd is the number of the branch taken at depth d.
This sequence is just the base 21 representation of the index
n of the array element, with no the most significant digit
and n0(.1 the least significant (Figure 1).
When a virtual-copy array a is updated, one of two things
may happen. If the index for the updated element exceeds
the maximum for the current depth (as in the a[81 := g up-
date in Figure 5), a new root node is created for the updated
array and the old array becomes the leftmost daughter of the
new root. Other nodes are also created, as appropriate, to
reach the position of the new element. If, on the other hand,
the index for the update is within the range for the current
</bodyText>
<page confidence="0.982442">
139
</page>
<figure confidence="0.998880764705882">
molecule
skeleton
pred arg2
own
ref
Spot
initial
ref
Daniel
update
arg2
ref
Daniel Spot
own
MINN.
environment
EMMEN
</figure>
<figureCaption confidence="0.9998845">
Figure 2: Molecule
Figure 3: Unification of Two Molecules
</figureCaption>
<page confidence="0.54075">
140
</page>
<figure confidence="0.9519815">
unification
a: (0:e, 2:h, 8:91
</figure>
<figureCaption confidence="0.999891">
Figure 5: Updating Virtual-Copy Arrays
</figureCaption>
<bodyText confidence="0.99991425">
depth. the path from the root to the element being updated
is copied and the old element is replaced in the new tree by
the new element (as in the a[21 := h update in Figure 5).
This description assumes that the element being updated
has already been set. If not, the branch to the element may
terminate prematurely in a I leaf, in which case new nodes
are created to the required depth and attached to the ap-
propriate position at the end of the new path from the root.
</bodyText>
<sectionHeader confidence="0.987982" genericHeader="method">
6 Dag representation
</sectionHeader>
<bodyText confidence="0.991374702702702">
Any dag representation can be implemented with virtual-
copy memory instead of random-access memory. If that were
done for the original PATR-II copying implementation, a
certain measure of structure sharing would be achieved.
The present scheme, however, goes well beyond that by
using the method of structure sharing introduced in Section
4. As we saw there, an instance object is represented by a
molecule, a pair consisting of a skeleton dag (from a rule
or lexical entry) and an update environment. We shall now
examine the structure of environments.
In a chart parser for PATR-II, dag instances in the chart
fall into two classes.
Rase instances are those associated with edges that are
created directly from lexical entries or rules.
Derived instances occur in edges that result from the com-
bination of a left and a right parent edge containing the left
and right parent instances of the derived instance. The left
ancestors of an instance (edge) are its left parent and that
parent&apos;s ancestors, and similarly for right ancestors. I will
assume, for ease of exposition, that a derived instance is
always a subdag of the unification of its right parent with
a subdag of its left parent. This is the case for most com-
mon parsing algorithms, although more general schemes are
possible (71.
If the original Boyer-Moore scheme were used directly, the
environment for a derived instance would consist of point-
ers to left and right parent instances, as well as a list of
the updates needed to build the current instance from its
parents. As noted before, this method requires a worst-case
0(1d1) search to find the updates that result in the current
instance.
The present scheme relies on the fact that in the great
majority of cases no instance is both the left and the right
ancestor of another instance. I shall assume for the moment
that this is always the case. In Section 9 this restriction will
be removed.
It is a simple observation about unification that an update
of a node of an instance I is either an update of /&apos;s skeleton
or of the value (a subdag of another instance) of another
update of I. If we iterate this reasoning, it becomes clear
that every update is ultimately an update of the skeleton of
a base instance ancestor of I. Since we assumed above that
no instance could occur more than once in /&apos;s derivation, we
can therefore conclude that rs environment consists only of
updates of nodes in the skeletons of its base instance an-
cestors. By numbering the base instances of a derivation
consecutively, we can then represent an environment by an
array of frames, each containing all the updates of the skele-
ton of a given base instance.
Actually, the environment of an instance / will be a branch
environment containing not only those updates directly rele-
vant to I, but also all those that are relevant to the instances
of /&apos;s particular branch through the parsing search space.
In the context of a given branch environment, it is then
possible to represent a molecule by a pair consisting of a
skeleton and the index of a frame in the environment. In
particular. this representation can be used for all the values
(dags) in updates.
More specifically, the frame of a base instance is an array
of update records indexed by small integers representing the
nodes of the instance&apos;s skeleton. An update record is either
a list of arc bindings for distinct arc labels or a rerouting
update. An arc binding is a pair consisting of a label and
a molecule (the value of the arc binding). This represents
an addition of an arc with that label and that value at the
given node. A rerouting update is just a pointer to another
molecule; it says that the subdag at that node in the updated
dag is given by that molecule (rather than by whatever was
in the initial skeleton).
To see how skeletons and bindings work together to rep-
resent a dag, consider the operation of finding the subdag
d1(11-- •1„,) of dag d. For this purpose, we use a current
skeleton s and a current frame f, given initially by the skele-
ton and frame of the molecule representing d. Now assume
</bodyText>
<page confidence="0.996637">
141
</page>
<bodyText confidence="0.997568">
that the current skeleton s and current frame f correspond
to the subdag d&apos; = d/(11• • To find d/(11- • • =
we use the following method:
I. If the top node of s has been rerouted in f to a dag v,
dereferenee d&apos; by setting a and f from v and repeating
this step; otherwise
</bodyText>
<listItem confidence="0.8281022">
2. If the top node of a has an arc labeled by 1, with value
s&apos;, the subdag at 1, is given by the moledule (s&apos;, f);
otherwise
3. If f contains an arc binding labeled 1, for the top node
of .s, the subdag at l is the value of the binding
</listItem>
<bodyText confidence="0.997980333333333">
If none of these steps can be applied, (11. • • li) is not a path
from the root in d.
The details of the representation are illustrated by the
example in Figure 6, which shows the passive edges for the
chart analysis of the string ab according to the sample gram-
mar
</bodyText>
<figure confidence="0.995125833333333">
S A B : (Se) = (A)
(Sb) = (B)
(S a x) = (S b y)
(3)
A —• a: (A u v) = a
B b : (B u a) = b
</figure>
<bodyText confidence="0.998938692307692">
For the sake of simplicity, only the subdags corresponding
to the explicit equations in these rules are shown (ie., the
cat dag arcs and the rule arcs 0, I,... are omitted). In
the figure, the three nonterminal edges (for phrase types S,
.4 and B) are labeled by molecules representing the corre-
sponding dags. The skeleton of each of the three molecules
comes from the rule used to build the nonterminal. Each
molecule points (via a frame index not shown in the figure)
to a frame in the branch environment. The frames for the
A and B edges contain arc bindings for the top nodes of
the respective skeletons whereas the frame for the S edge
reroute nodes 1 and 2 of the S rule skeleton to the A and B
molecules respectively.
</bodyText>
<sectionHeader confidence="0.938341" genericHeader="method">
7 The Unification Algorithm
</sectionHeader>
<bodyText confidence="0.999916166666667">
I shall now give the unification algorithm for two molecules
(dags) in the same branch environment.
We can treat a complex dag d as a partial function from
labels to dags that maps the label on each arc leaving the top
node of the dag to the dag at the end of that arc. This allows
us to define the following two operations between dags:
</bodyText>
<equation confidence="0.755134">
d1 \d2 = {(l, d) E I 1 dom(d2)}
di 4 d2 = ((I, d) Edi I E dom(c12))
</equation>
<bodyText confidence="0.9258156">
It is clear that dom(di 4 d2) = dom(d2 4 di).
We also need the notion of dag dereferencing introduced
in the last section. As a side effect of successive unifications,
the top node of a dag may be rerouted to another dag whose
top node will also end up being rerouted. Dereferencing is
the process of following such chains of rerouting pointers to
reach a dag that has not been rerouted.
The unification of dags d, and d2 in environment e consists
of the following steps:
I. Dereference d, and d2
</bodyText>
<listItem confidence="0.9806268">
2. If d, and d2 are identical, the unification is immediately
successful
3. If di is a leaf, add to e a rerouting from the top node of
di to d2; otherwise
4. If d2 is a leaf, add to e a rerouting from the top node of
d2 to dt; otherwise
5. If ell and d2 are complex dags, for each arc (1, d) E dt 4
d2 unify the dag d with the dag d&apos; of the corresponding
arc (1,d&apos;) E (It 4 di. Each of those unifications may
add new bindings to e. If this unification of subdags
successful, all the arcs in di \ d2 are are entered in e
arc bindings for the top node of d2 and finally the top
node of di is rerouted to d2.
6. If none of the conditions above applies, the unification
fails.
</listItem>
<bodyText confidence="0.9998523">
To determine whether a dag node is a leaf or com-
plex, both the skeleton and the frame of the corresponding
molecule must be examined. For a dereferenced molecule.
the set of arcs leaving a node is just the union of the skele-
ton arcs and the arc bindings for the node. For this to make
sense, the skeleton arcs and arc bindings for any molecule
node must be disjoint. The interested reader will have no
difficulty in proving that this property is preserved by the
unification algorithm and therefore all molecules built from
skeletons and empty frames by unification will satisfy it.
</bodyText>
<sectionHeader confidence="0.9812385" genericHeader="method">
8 Mapping dags onto virtual-copy
memory
</sectionHeader>
<bodyText confidence="0.999922428571429">
As we saw above, any dag or set of dags constructed by
the parser is built from just two kinds of material: (1)
frames; (2) pieces of the initial skeletons from rules and
lexical entries. The initial skeletons can be represented triv-
ially by host language data structures, as they never change.
Frames, though, are always being updated. A new frame is
born with the creation of an instance of a rule or lexical
entry when the rule or entry is used in some parsing step
(uses of the same rule or entry in other steps beget their own
frames). A frame is updated when the instance it belongs
to participates in a unification.
During parsing, there are in general several possible ways
of continuing a derivation. These correspond to alternative
ways of updating a branch environment. In abstract terms,
</bodyText>
<page confidence="0.994697">
142
</page>
<figureCaption confidence="0.999589">
Figure 6: Structure-Sharing Chart
</figureCaption>
<bodyText confidence="0.999936409090909">
on coming to a choice point in the derivation with n possi-
ble continuations, n — 1 copies of the environment are made,
giving n environments — namely, one for each alternative.
In fact, the use of virtual-copy arrays for environments and
frames renders this copying unnecessary, so each continu-
ation path performs its own updating of its version of the
environment without interfering with the other paths. Thus,
all unchanged portions of the environment are shared.
In fact, derivations as such are not explicit in a chart
parser. Instead, the instance in each edge has its own branch
environment, as described previously. Therefore. when two
edges are combined, it is necessary to merge their environ-
ments. The cost of this merge operation is at most the same
as the worst case cost for unification proper (0(1d1 log kip)
However, in the very common cam in which the ranges of
frame indices of the two environments do not overlap, the
merge cost is only 0(log
To summarize, we have sharing at two levels: the Boyer-
Moore style dag representation allows derived &lt;lag in-
stances to share input data structures (skeletons), and the
virtual-copy array environment representation allows differ-
ent branches of the search space to share update records.
</bodyText>
<sectionHeader confidence="0.977" genericHeader="method">
9 The Renaming Problem
</sectionHeader>
<bodyText confidence="0.999811875">
In the foregoing discussion of the structure-sharing method,
I assumed that the left and right ancestors of a derived in-
stance were disjoint. In fact, it is easy to show that the con-
di tion holds whenever the grammar does mit allow empty
derived edges.
In contrast, it is possible to construct a grammar in which
an empty derived edge with dag D is both a left and a right
ancestor of another edge with dag E. Clearly, the two uses
of D as an ancestor of E are mutually independent and
the corresponding updates have to be segregated. In other
words, we need two copies of the instance D. ltv analogy
with theorem proving, I call this the renaminy prohletn.
The current solution is to use real copying to turn the
empty edge into a skeleton, which is then added to the chart.
The new skeleton is then used in the normal fashion to pro-
duce multiple instances that are free of mutual interference.
</bodyText>
<sectionHeader confidence="0.995818" genericHeader="conclusions">
10 Implementation
</sectionHeader>
<bodyText confidence="0.953304888888889">
The representation described here has been used in a PATR-
II parser implemented in Prolog. Two versions of the parser
exist -- one using an Earley-style algorithm related to Ear-
ley deduction (71, the other using a left-corner algorithm.
Preliminary tests of the left-corner algorithm with struc-
ture sharing on various grammars and input have shown
parsing times as much as 60% faster (never less, in fact,
than 40% faster) than those achieved by the same parsing
algorithm with structure copying.
</bodyText>
<page confidence="0.99834">
143
</page>
<sectionHeader confidence="0.998305" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999796073170732">
111 R. S. Boyer and J S. Moore. The sharing of structure in
theorem-proving programs. In Machine Intelligence 7,
pages 101-116, John Wiley and Sons, New York, New
York, 1972.
[21 J. Bresnan and R. Kaplan. Lexical-functional gram-
mar: a formal system for grammatical representation.
In J. Bresnan, editor, The Mental Representation of
Grammatical Relations, pages 173-281, MIT Press,
Cambridge, Massachusetts, 1982.
131 M. Kay. Algorithm Schemata and Data Structures in
Syntactic Processing. Technical Report, XEROX Palo
Alto Research Center, Palo Alto, California, 1980. A
version will appear in the proceedings of the Nobel
Symposium on Text Processing, Gothenburg, 1980.
kJ M. Kay. Functional grammar. In Proc. of the Fifth
Annual Meeting of the Berkeley Linguistic Society,
pages 142-158, Berkeley Linguistic Society, Berkeley,
California, February 17-19 1979.
151 Fernando C. N. Pereira and Stuart M. Shieber. The se-
mantics of grammar formalisms seen as computer lan-
guages. In Proc. of Coling84, pages 123-129, Associa-
tion for Computational Linguistics, 1984.
[61 Fernando C. N. Pereira and David H. D. Warren. Defi-
nite clause grammars for language analysis - a survey of
the formalism and a comparison with augmented transi-
tion networks. Artificial Intelligence, 13:231-278, 1080.
[71 Fernando C. N. Pereira and David H. D. Warren. Pars-
ing as deduction. In Proc. of the 21st Annual Meeting
of the Association for Computational Linguistics, NIIT,
Cambridge, Massachusetts, June 15-17 1983.
[81 Stuart M. Shieber. The design of a computer lan-
guage for linguistic information. In Proc. of Coling84,
pages 362-366, Association for Computational Linguis-
tics, 1984.
[91 David H. D. Warren. Applied Logic - its use and imple-
mentation as programming tool. PhD thesis, University
of Edinburgh, Scotland, 1977. Reprinted as Technical
Note 290, Artificial Intelligence Center, SRI, Interna-
tional, Menlo Park, California.
[101 David H. D. Warren. Logarithmic access arrays for
Prolog. Unpublished program, 1983.
</reference>
<page confidence="0.998598">
144
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.000734">
<title confidence="0.998855666666667">A Structure-Sharing Representation for Unification-Based Grammar Formalisms</title>
<author confidence="0.999958">Fernando C N Pereira</author>
<affiliation confidence="0.9985255">Artificial Intelligence Center, SRI International and Center for the Study of Language and Information Stanford University</affiliation>
<abstract confidence="0.993942302734376">This paper describes a structure-sharing method for the representation of complex phrase types in a parser for PATR-II, a unification-based grammar formalism. In parsers for unification-based grammar formalisms, complex phrase types are derived by incremental refinement of the phrase types defined in grammar rules and lexical entries. In a naïve implementation, a new phrase type is built by copying older ones and then combining the copies according to the constraints stated in a grammar rule. The structure-sharing method was designed to eliminate most such copying; indeed, practical tests suggest that the use of this technique reduces parsing time by as much as 60%. The present work is inspired by the structure-sharing method for theorem proving introduced by Boyer and Moore and on the variant of it that is used in some Prolog implementations. 1 Overview this paper I describe a method, sharing, of complex phrase types in &apos;a parser for PATR-II, a unification-based grammar formalism. parsers unification-based grammar formalisms, complex phrase types are derived by incremental refinement the phrase types defined in grammar rules and In a naïve implementation, a phrase type is built by copying older ones and then combining the copies according to the constraints stated in a grammar rule. The structure-sharing method eliminates most such copying by research, made possible in part by a gift from the Development Foundation, was also supported by the Defense Advanced Projects Agency under N00039-84-C-0524 with the Naval Electronic Systems Command. The conclusions contained in this document are those of the author and should not be interpreted as representative of the official policies, either expressed or implied, of the Defense Advanced Reor the United States government. to Stuart Shieber, Lauri Karttunen. and Ray Perfor their comments presentations of this material. representing updates to objects (phrase types) separately from the objects themselves. The present work is inspired by the structure-sharing method for theorem proving introduced by Boyer and Moore and on the variant of it that is used in im- 2 Grammars with Unification The data representation discussed in this paper is applicable, with but minor changes, to a variety of grammar formalisms on unification, such as definite-clause grammars grammar [11, gram- [21 PATR-II 181. For the sake of concreteness, however, our discussion will be in terms of the PATR-II formalism. basic idea of grammar formalisms is simple. As with context-free grammar rules state how phrase types combine to yield other phrase types. context-free grammar allows only a finite of predefined atomic phrase types or grammar will in general define an infinity of phrase types. A phrase type is defined by a set of constraints. A gramrule is a set of constraints type of a and the types V„ of rt,iv-,titiierits. The may be applied to the analysis of a string As of constituents if and only if the the s, are compatible with and the constraints in the rule. the operation that determines whether two types are compatible by building the most general type compatible with both. If the constraints are equations between attribtites of types, as is the phrase R pes can be unified %chem.\ er they do not assign instinct ■alues the same The hum is is jimsi he junction (set union) of the corresixinding sets of constraints is a sample rule, in a simplified version of the PATR- 137 II notation: X2 : cat) = S cat) NP cat) = VP agr) = agr) trans) = trans) (X0 trans arg,) = (X1 trans) rule may be read as stating that a phrase of type can be the concatenation of a phrase of type X, and a phrase type the attribute of the rule are satisfied if the phrases are substituted for their types. equations state that phrases of types Xo, and categories NP, that types the same agreement value, that types and the same translation, and that the first argument „Ws translation is the translation of the expressions of the form (l, in equations are paths and each 1, is a When all the phrase types in a rule are given constant values by the rule, we can use an abbreviated notation in which the phrase type variables Xi are replaced by their category values and the category-setting equations are omitted. For example, rule (1) may be written as NP VP: (NP agr) = (VP trans) (VP (2) = In existing PATR-II implementations, phrase types are not actually represented by their sets of defining equations. Instead, they are represented by symbolic solutions of the in the form of directed acyclic graphs arcs labeled by the attributes used in the equations. Dag nodes represent the values of attributes and an arc labeled from node m to node n if and only if, according to the equations, the value represented by m has n as the of / [51. dag node (and by extension a dag) is said to be it a constant value; it has some outarcs; and a is is neither atomic or complex, that is, if it represents an as yet completely undetermined value. domain dom(d) of a complex dag the set of labels arcs leaving the top node of a dag a label dom(d) we denote by subdag of d at the end of arc labeled the top node of extension, for any path p whose labels are in the domains of the approprisubdags, the subdag the end of p from the root For uniformity, lexical entries and grammar rules are also represented by appropriate dags. For example, the dag for rule (1) is shown in Figure 1. 3 The Problem a chart parser the intermediate stages of derivaare encoded in either incomplete trans Figure 1: Dag Representation of a Rule complete For PATR-II, each contains a dag instance that represents the phrase that edge. The problem we address here is how to multiple dag instances efficiently. In a chart parser for context-free grammars, the solution trivial: instances can be represented by the unique inter- (that is, addresses) of their objects because the contained in an instance is exactly the as that in the original object. In a parser for PATR-II or any other unification-based formalism, however, distinct instances of an object will in general specify different values for attributes left unspecified in original object. Clearly, the attribute specified for instance are independent of those for another of the same object. obvious solution is to build new instances by copyoriginal object and then updating the copy the new attribute values. This was the solution adopted in the PATR-II parser [SI. The high cost of this solution time spent copying and in space required for the constitutes the principal justification for employmethod described here. 4 Structure Sharing sharing is based the observation that an initial object, together with a list of update records, contains same information as the object that results from applythe to the initial object. In this way, can the coat of actually applying the updates posto avoid the destruction of the object) the cost of having to compute the effects of when examining the derived object. This reasoning applies in particular to dag instances that are the result of adding attribute values to other instances. 138 As in the variant of Boyer and Moore&apos;s method (I] used Prolog [91, I shall represent a dag instance by a (see Figure 2) consisting of [A pointer to( the initial dag, the instance&apos;s 2. [A pointer tol a table of updates of the skeleton, the instance&apos;s environment. Environments may contain two kinds of updates: reroutings replace a dag node with another dag; bindings add to a node a new outgoing arc pointing to a dag. Figure 3 shows the unification of the dags = = : unification, the top node of rerouted to the node of gets an arc binding with label a value is the subdag : el /2. As we shall see later, any update of a dag represented by a molecule is either an update the molecule&apos;s skeleton or an of a dag (to which the same reasoning applies) appearing in the molecule&apos;s enviroment. Therefore, the updates in a molecule&apos;s environshown in figures tagged by a boxed number the affected node the molecule&apos;s skeleton. The choice of which dag is rerouted and which one gets arc bindings is arbitrary. For reasons discussed later, the cost of looking up instance updates Boyer and Moore&apos;s environment represenis 0(Id1), where Idl is the of the derivation of resolutions) of the instance. In present representation, however, this cost is only 0(log ldl). This better performance is achieved by particularizing the environment and splitting the representational scheme two components: a organization and a representation. dag representation is a of mapping the mathematonto a memory. A memory organization is of putting together memory that has certain properwith respect to lookup, and copying. One can of the memory organization as hardware and the dag representation as the data structure. organization In practice, random-access memory can be accessed and upin constant However, updates destroy old valwhich is obviously when dealing with alupdates of the same data structure. we want to old version, we need to copy it first into sepaof memory change the copy instead. For the normal kind of memory, copying time is proportional to the the object copied. present scheme uses another type of memory organization — virtual-copy arrays — which requires 0(logn) time to access or update an array with highest used index =2 = = 132 (base 4) = f 0(a) = 3 Figure 4: Virtual-Copy Array of n, but in which the old contents are not destroyed by updating. Virtual-copy arrays were developed by David H. D. Warren [101 as an implementation of extensible arrays for Prolog. arrays provide a general memory structure: anything that can be stored in randorn-acress memcan stored in arrays. although pointers in machine memory correspond to indexes in a virtual-copy An takes a virtual-copy array, an index, and a new value and returns a new virtual-copy array the new value stored at the given index. access optakes an and an index, and the value at that index. virtual-copy arrays are trees for sonic &gt; the d(n) a tree node be the root and + if the parent of Each virtual-copy array a has also a &gt; : n is a node of a}. A node at depth D(a) (necessarily a leaf) can be either an array element or the special marker for unassigned elements. All leaf at depths lower D(a) also 1, indicating that no elements have yet been stored in the subarray below the With this arrangement, the array can store at numbered through — but unused subarrays need not be allocated. By numbering the 2h daughters of a nonleaf node from 0 2h —1, a from a&apos;s root to an element (a leaf D(a)) can be represented by a sequence which is the number of the branch taken at depth sequence is just the base representation the index of the array element, with the most significant digit the least significant (Figure 1). a virtual-copy array a is one of two things happen. If the index for the element maximum for the current (as in the a[81 := update in Figure 5), a new root node is created for the updated array and the old array becomes the leftmost daughter of the new root. Other nodes are also created, as appropriate, to reach the position of the new element. If, on the other hand, the index for the update is within the range for the current 139 molecule skeleton pred arg2 own ref Spot initial ref Daniel update arg2 ref Daniel Spot own MINN. environment EMMEN Figure 2: Molecule Figure 3: Unification of Two Molecules 140 unification a: (0:e, 2:h, 8:91 Figure 5: Updating Virtual-Copy Arrays depth. the path from the root to the element being updated is copied and the old element is replaced in the new tree by new element (as in the in Figure 5). This description assumes that the element being updated has already been set. If not, the branch to the element may terminate prematurely in a I leaf, in which case new nodes are created to the required depth and attached to the appropriate position at the end of the new path from the root. 6 Dag representation Any dag representation can be implemented with virtualcopy memory instead of random-access memory. If that were done for the original PATR-II copying implementation, a certain measure of structure sharing would be achieved. The present scheme, however, goes well beyond that by using the method of structure sharing introduced in Section 4. As we saw there, an instance object is represented by a molecule, a pair consisting of a skeleton dag (from a rule or lexical entry) and an update environment. We shall now examine the structure of environments. In a chart parser for PATR-II, dag instances in the chart fall into two classes. instances are associated with edges that are created directly from lexical entries or rules. instances in edges that result from the comof a a the the derived instance. an instance (edge) are its left parent and that ancestors, and similarly for ancestors. I assume, for ease of exposition, that a derived instance is always a subdag of the unification of its right parent with a subdag of its left parent. This is the case for most common parsing algorithms, although more general schemes are possible (71. If the original Boyer-Moore scheme were used directly, the environment for a derived instance would consist of pointto instances, as well as a list of updates needed to build the current instance from parents. As noted before, this method requires a worst-case 0(1d1) search to find the updates that result in the current instance. present scheme relies on the fact that in great majority of cases no instance is both the left and the right ancestor of another instance. I shall assume for the moment that this is always the case. In Section 9 this restriction will be removed. It is a simple observation about unification that an update a node of an instance either an update of /&apos;s skeleton or of the value (a subdag of another instance) of another of we iterate this reasoning, it becomes clear that every update is ultimately an update of the skeleton of base instance ancestor of we assumed above that no instance could occur more than once in /&apos;s derivation, we can therefore conclude that rs environment consists only of updates of nodes in the skeletons of its base instance an- By numbering the base instances of derivation consecutively, we can then represent an environment by an of containing all the updates of the skeleton of a given base instance. the environment of an instance be a not only those updates directly releto also all those that are relevant to the instances of /&apos;s particular branch through the parsing search space. In the context of a given branch environment, it is then possible to represent a molecule by a pair consisting of a skeleton and the index of a frame in the environment. In particular. this representation can be used for all the values (dags) in updates. More specifically, the frame of a base instance is an array records by small integers representing the nodes of the instance&apos;s skeleton. An update record is either a list of arc bindings for distinct arc labels or a rerouting update. An arc binding is a pair consisting of a label and a molecule (the value of the arc binding). This represents an addition of an arc with that label and that value at the given node. A rerouting update is just a pointer to another molecule; it says that the subdag at that node in the updated dag is given by that molecule (rather than by whatever was in the initial skeleton). To see how skeletons and bindings work together to represent a dag, consider the operation of finding the subdag •1„,) dag this purpose, we use a current s a current frame initially by the skeleand frame of the molecule representing assume 141 the current skeleton current frame the subdag = d/(11• • find • • = we use the following method: If the top node of been rerouted in a dag v, d&apos; setting v and repeating this step; otherwise 2. If the top node of a has an arc labeled by 1, with value the subdag at 1, is given by the moledule f); otherwise If an arc binding labeled 1, for the top node of .s, the subdag at l is the value of the binding none of steps can be applied, • • is not a path the root in The details of the representation are illustrated by the example in Figure 6, which shows the passive edges for the chart analysis of the string ab according to the sample grammar S A B : (Se) = (A) (Sb) = (B) (S a x) = (S b y) A —• a: (A u v) = a b : (B b For the sake of simplicity, only the subdags corresponding the explicit in these rules shown (ie., the dag and the rule arcs 0, I,... omitted). In figure, the three nonterminal edges (for phrase types and by molecules representing the corresponding dags. The skeleton of each of the three molecules comes from the rule used to build the nonterminal. Each molecule points (via a frame index not shown in the figure) a frame branch environment. The frames for the and contain arc bindings for the top of respective skeletons whereas the frame for the nodes and 2 of the skeleton to the A and molecules respectively. 7 The Unification Algorithm now give the unification algorithm for two molecules in same branch environment. can treat a complex dag as a function from labels to dags that maps the label on each arc leaving the top node of the dag to the dag at the end of that arc. This allows us to define the following two operations between dags: = {(l, E I 1 4 d2 = d) I dom(c12)) is clear 4 d2) = 4 also need the notion dag introduced in the last section. As a side effect of successive unifications, the top node of a dag may be rerouted to another dag whose top node will also end up being rerouted. Dereferencing is the process of following such chains of rerouting pointers to reach a dag that has not been rerouted. unification of dags d, and in e consists following steps: Dereference d, and If d, and are identical, the unification is successful If is a leaf, to e a rerouting from the top node of to otherwise If is a leaf, add to rerouting from the top node of to otherwise If and are complex dags, for each arc d) E 4 unify the dag d with the dag d&apos; of the corresponding 4 of those unifications may new bindings to this of subdags all the arcs in \ are are entered in bindings for the top node of and finally the top of is rerouted to 6. If none of the conditions above applies, the unification fails. To determine whether a dag node is a leaf or comboth the skeleton and the frame of the must be examined. For a molecule. set of arcs leaving a node is just the union of skeleton arcs and the arc bindings for the node. For this to make the skeleton arcs and arc bindings for any must be disjoint. The interested reader have no difficulty in proving that this property is preserved by the unification algorithm and therefore all molecules built from skeletons and empty frames by unification will satisfy it. 8 Mapping dags onto virtual-copy memory As we saw above, any dag or set of dags constructed by the parser is built from just two kinds of material: (1) frames; (2) pieces of the initial skeletons from rules and entries. The skeletons can be represented trivially by host language data structures, as they never change. Frames, though, are always being updated. A new frame is with the creation of instance of a rule or lexical entry when the rule or entry is used in some parsing step of the same rule or entry in other beget their A frame is updated when the instance it to participates in a unification. During parsing, there are in general several possible ways of continuing a derivation. These correspond to alternative ways of updating a branch environment. In abstract terms, 142 Figure 6: Structure-Sharing Chart on coming to a choice point in the derivation with n possible continuations, n — 1 copies of the environment are made, giving n environments — namely, one for each alternative. In fact, the use of virtual-copy arrays for environments and frames renders this copying unnecessary, so each continuation path performs its own updating of its version of the environment without interfering with the other paths. Thus, all unchanged portions of the environment are shared. In fact, derivations as such are not explicit in a chart parser. Instead, the instance in each edge has its own branch environment, as described previously. Therefore. when two edges are combined, it is necessary to merge their environments. The cost of this merge operation is at most the same as the worst case cost for unification proper (0(1d1 log kip) However, in the very common cam in which the ranges of frame indices of the two environments do not overlap, the merge cost is only 0(log To summarize, we have sharing at two levels: the Boyer- Moore style dag representation allows derived &lt;lag instances to share input data structures (skeletons), and the virtual-copy array environment representation allows different branches of the search space to share update records. 9 The Renaming Problem In the foregoing discussion of the structure-sharing method, I assumed that the left and right ancestors of a derived inwere disjoint. In fact, it is easy to show that the condi tion holds whenever the grammar does mit allow empty derived edges. In contrast, it is possible to construct a grammar in which empty derived edge with dag both a left and a right of another edge with dag the two uses an ancestor of mutually independent and the corresponding updates have to be segregated. In other we need copies of the instance D. ltv analogy theorem proving, I call this the current solution is to use to turn the empty edge into a skeleton, which is then added to the chart. The new skeleton is then used in the normal fashion to produce multiple instances that are free of mutual interference. 10 Implementation The representation described here has been used in a PATRimplemented in Prolog. Two versions of the parser exist -one using an Earley-style algorithm related to Ear- (71, the other using a left-corner algorithm. Preliminary tests of the left-corner algorithm with structure sharing on various grammars and input have shown parsing times as much as 60% faster (never less, in fact, than 40% faster) than those achieved by the same parsing algorithm with structure copying.</abstract>
<note confidence="0.6347291875">143 References S. Boyer and J S. Moore. The sharing of structure in programs. In Intelligence pages 101-116, John Wiley and Sons, New York, New York, 1972. [21 J. Bresnan and R. Kaplan. Lexical-functional grammar: a formal system for grammatical representation. J. Bresnan, editor, Mental Representation of Relations, 173-281, MIT Press, Cambridge, Massachusetts, 1982. M. Kay. Schemata Data in Processing. Report, XEROX Palo Alto Research Center, Palo Alto, California, 1980. A version will appear in the proceedings of the Nobel Symposium on Text Processing, Gothenburg, 1980.</note>
<author confidence="0.865188">Functional grammar In of the Fifth</author>
<affiliation confidence="0.854516">of the Berkeley Linguistic Society,</affiliation>
<address confidence="0.7664595">pages 142-158, Berkeley Linguistic Society, Berkeley, California, February 17-19 1979.</address>
<author confidence="0.263904">The se-</author>
<note confidence="0.885530043478261">of grammar formalisms seen as lan- In of Coling84, 123-129, Association for Computational Linguistics, 1984. [61 Fernando C. N. Pereira and David H. D. Warren. Definite clause grammars for language analysis a survey of the formalism and a comparison with augmented transinetworks. Artificial 1080. Fernando C. N. Pereira and David H. Warren. Parsas deduction. In of the 21st Annual Meeting the Association for Computational Linguistics, Cambridge, Massachusetts, June 15-17 1983. [81 Stuart M. Shieber. The design of a computer lanfor linguistic information. In of Coling84, pages 362-366, Association for Computational Linguistics, 1984. David H. D. Warren. Logic its use and impleprogramming thesis, University of Edinburgh, Scotland, 1977. Reprinted as Technical Note 290, Artificial Intelligence Center, SRI, International, Menlo Park, California. [101 David H. D. Warren. Logarithmic access arrays for Prolog. Unpublished program, 1983. 144</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>R S Boyer</author>
<author>J S Moore</author>
</authors>
<title>The sharing of structure in theorem-proving programs.</title>
<date>1972</date>
<journal>In Machine Intelligence</journal>
<volume>7</volume>
<pages>101--116</pages>
<publisher>John Wiley and Sons,</publisher>
<location>New York, New York,</location>
<marker>Boyer, Moore, 1972</marker>
<rawString>111 R. S. Boyer and J S. Moore. The sharing of structure in theorem-proving programs. In Machine Intelligence 7, pages 101-116, John Wiley and Sons, New York, New York, 1972.</rawString>
</citation>
<citation valid="true">
<title>Lexical-functional grammar: a formal system for grammatical representation.</title>
<date>1982</date>
<booktitle>The Mental Representation of Grammatical Relations,</booktitle>
<pages>173--281</pages>
<editor>In J. Bresnan, editor,</editor>
<publisher>MIT Press,</publisher>
<location>Cambridge, Massachusetts,</location>
<marker>1982</marker>
<rawString>[21 J. Bresnan and R. Kaplan. Lexical-functional grammar: a formal system for grammatical representation. In J. Bresnan, editor, The Mental Representation of Grammatical Relations, pages 173-281, MIT Press, Cambridge, Massachusetts, 1982.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Algorithm Schemata and Data Structures in Syntactic Processing.</title>
<date>1980</date>
<booktitle>in the proceedings of the Nobel Symposium on Text Processing,</booktitle>
<tech>Technical Report,</tech>
<institution>XEROX Palo Alto Research Center,</institution>
<location>Palo Alto, California,</location>
<marker>Kay, 1980</marker>
<rawString>131 M. Kay. Algorithm Schemata and Data Structures in Syntactic Processing. Technical Report, XEROX Palo Alto Research Center, Palo Alto, California, 1980. A version will appear in the proceedings of the Nobel Symposium on Text Processing, Gothenburg, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>kJ M Kay</author>
</authors>
<title>Functional grammar.</title>
<date>1979</date>
<booktitle>In Proc. of the Fifth Annual Meeting of the Berkeley Linguistic Society,</booktitle>
<pages>142--158</pages>
<institution>Berkeley Linguistic Society,</institution>
<location>Berkeley, California,</location>
<marker>Kay, 1979</marker>
<rawString>kJ M. Kay. Functional grammar. In Proc. of the Fifth Annual Meeting of the Berkeley Linguistic Society, pages 142-158, Berkeley Linguistic Society, Berkeley, California, February 17-19 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fernando C N Pereira</author>
<author>Stuart M Shieber</author>
</authors>
<title>The semantics of grammar formalisms seen as computer languages.</title>
<date>1984</date>
<booktitle>In Proc. of Coling84,</booktitle>
<pages>123--129</pages>
<institution>Association for Computational Linguistics,</institution>
<marker>Pereira, Shieber, 1984</marker>
<rawString>151 Fernando C. N. Pereira and Stuart M. Shieber. The semantics of grammar formalisms seen as computer languages. In Proc. of Coling84, pages 123-129, Association for Computational Linguistics, 1984.</rawString>
</citation>
<citation valid="false">
<title>Definite clause grammars for language analysis - a survey of the formalism and a comparison with augmented transition networks.</title>
<journal>Artificial Intelligence,</journal>
<volume>13</volume>
<pages>1080</pages>
<marker></marker>
<rawString>[61 Fernando C. N. Pereira and David H. D. Warren. Definite clause grammars for language analysis - a survey of the formalism and a comparison with augmented transition networks. Artificial Intelligence, 13:231-278, 1080.</rawString>
</citation>
<citation valid="true">
<title>Parsing as deduction.</title>
<date>1983</date>
<booktitle>In Proc. of the 21st Annual Meeting of the Association for Computational Linguistics, NIIT,</booktitle>
<location>Cambridge, Massachusetts,</location>
<marker>1983</marker>
<rawString>[71 Fernando C. N. Pereira and David H. D. Warren. Parsing as deduction. In Proc. of the 21st Annual Meeting of the Association for Computational Linguistics, NIIT, Cambridge, Massachusetts, June 15-17 1983.</rawString>
</citation>
<citation valid="true">
<title>The design of a computer language for linguistic information.</title>
<date>1984</date>
<booktitle>In Proc. of Coling84,</booktitle>
<pages>362--366</pages>
<institution>Association for Computational Linguistics,</institution>
<marker>1984</marker>
<rawString>[81 Stuart M. Shieber. The design of a computer language for linguistic information. In Proc. of Coling84, pages 362-366, Association for Computational Linguistics, 1984.</rawString>
</citation>
<citation valid="true">
<title>Applied Logic - its use and implementation as programming tool.</title>
<date>1977</date>
<booktitle>Reprinted as Technical Note 290, Artificial Intelligence Center, SRI, International,</booktitle>
<tech>PhD thesis,</tech>
<institution>University of Edinburgh,</institution>
<location>Scotland,</location>
<marker>1977</marker>
<rawString>[91 David H. D. Warren. Applied Logic - its use and implementation as programming tool. PhD thesis, University of Edinburgh, Scotland, 1977. Reprinted as Technical Note 290, Artificial Intelligence Center, SRI, International, Menlo Park, California.</rawString>
</citation>
<citation valid="true">
<title>Logarithmic access arrays for Prolog. Unpublished program,</title>
<date>1983</date>
<marker>1983</marker>
<rawString>[101 David H. D. Warren. Logarithmic access arrays for Prolog. Unpublished program, 1983.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>