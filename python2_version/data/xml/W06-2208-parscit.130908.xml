<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.9317">
Expanding the Recall of Relation Extraction by Bootstrapping
</title>
<author confidence="0.489496">
Junji Tomita
</author>
<note confidence="0.4998375">
NTT Cyber Solutions Laboratories,
NTT Corporation
</note>
<address confidence="0.428657">
1-1 Hikarinooka Yokosuka-Shi,
Kanagawa 239-0847, Japan
</address>
<email confidence="0.995923">
tomita.junji@lab.ntt.co.jp
</email>
<author confidence="0.974272">
Stephen Soderland Oren Etzioni
</author>
<affiliation confidence="0.869167">
Department of Computer Science
&amp; Engineering
University of Washington
</affiliation>
<address confidence="0.856535">
Seattle, WA 98195-2350
</address>
<email confidence="0.817663">
{soderlan,etzioni}
@cs.washington.edu
</email>
<sectionHeader confidence="0.998587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99981805">
Most works on relation extraction assume
considerable human effort for making an
annotated corpus or for knowledge engi-
neering. Generic patterns employed in
KnowItAll achieve unsupervised, high-
precision extraction, but often result in low
recall. This paper compares two boot-
strapping methods to expand recall that
start with automatically extracted seeds
by KnowItAll. The first method is string
pattern learning, which learns string con-
texts adjacent to a seed tuple. The second
method learns less restrictive patterns that
include bags of words and relation-specific
named entity tags. Both methods improve
the recall of the generic pattern method. In
particular, the less restrictive pattern learn-
ing method can achieve a 250% increase
in recall at 0.87 precision, compared to the
generic pattern method.
</bodyText>
<sectionHeader confidence="0.999467" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.98679902">
Relation extraction is a task to extract tu-
ples of entities that satisfy a given relation
from textual documents. Examples of rela-
tions include CeoOf(Company, Ceo) and Acquisi-
tion(Organization, Organization). There has been
much work on relation extraction; most of it em-
ploys knowledge engineering or supervised ma-
chine learning approaches (Feldman et al., 2002;
Zhao and Grishman, 2005). Both approaches are
labor intensive.
We begin with a baseline information extraction
system, KnowItAll (Etzioni et al., 2005), that does
unsupervised information extraction at Web scale.
KnowItAll uses a set of generic extraction pat-
terns, and automatically instantiates rules by com-
bining these patterns with user supplied relation
labels. For example, KnowItAll has patterns for a
generic “of” relation:
NP1 ’s &lt;relation&gt; , NP2
NP2 , &lt;relation&gt; of NP1
where NP1 and NP2 are simple noun phrases that
extract values of argument1 and argument2 of a
relation, and &lt;relation&gt; is a user-supplied string
associated with the relation. The rules may also
constrain NP1 and NP2 to be proper nouns.
If a user supplies the relation labels “ceo”
and “chief executive officer” for the relation
CeoOf(Company, Ceo), KnowItAll inserts these
labels into the generic patterns shown above, to
create 4 extraction rules:
NP1 ’s ceo , NP2
NP1 ’s chief executive officer, NP2
NP2 , ceo of NP1
NP2 , chief executive officer of NP1
The same generic patterns with different la-
bels can also produce extraction rules for a May-
orOf relation or an InventorOf relation. These
rules have alternating context strings (exact string
match) and extraction slots (typically an NP or
head of an NP). This can produce rules with high
precision, but low recall, due to the wide variety
of contexts describing a relation. This paper looks
at ways to enhance recall over this baseline system
while maintaining high precision.
To enhance recall, we employ bootstrapping
techniques which start with seed tuples, i.e. the
most frequently extracted tuples by the baseline
system. The first method represents rules with
three context strings of tokens immediately adja-
cent to the extracted arguments: a left context,
</bodyText>
<page confidence="0.99416">
56
</page>
<bodyText confidence="0.999564884615385">
middle context, and right context. These are in-
duced from context strings found adjacent to seed
tuples.
The second method uses a less restrictive pat-
tern representation such as bag of words, similar
to that of SnowBall(Agichtein, 2005). SnowBall is
a semi-supervised relation extraction system. The
input of Snowball is a few hand labeled correct
seed tuples for a relation (e.g. &lt;Microsoft, Steve
Ballmer&gt; for CeoOf relation). SnowBall clusters
the bag of words representations generated from
the context strings adjacent to each seed tuple, and
generates rules from them. It calculates the confi-
dence of candidate tuples and the rules iteratively
by using an EM-algorithm. Because it can extract
any tuple whose entities co-occur within a win-
dow, the recall can be higher than the string pat-
tern learning method. The main disadvantage of
SnowBall or a method which employs less restric-
tive patterns is that it requires Named Entity Rec-
ognizer (NER).
We introduce Relation-dependent NER (Rela-
tion NER), which trains an off-the-shelf super-
vised NER based on CRF(Lafferty et al., 2001)
with bootstrapping. This learns relation-specific
NE tags, and we present a method to use these tags
for relation extraction.
This paper compares the following two boot-
strapping strategies.
SPL: a simple string pattern learning method. It
learns string patterns adjacent to a seed tuple.
LRPL: a less restrictive pattern learning method.
It learns a variety of bag of words patterns,
after training a Relation NER.
Both methods are completely self-supervised ex-
tensions to the unsupervised KnowItAll. A user
supplies KnowItAll with one or more relation la-
bels to be applied to one or more generic extrac-
tion patterns. No further tagging or manual selec-
tion of seeds is required. Each of the bootstrapping
methods uses seeds that are automatically selected
from the output of the baseline KnowItAll system.
The results show that both bootstrapping meth-
ods improve the recall of the baseline system. The
two methods have comparable results, with LRPL
outperforms SPL for some relations and SPL out-
performs LRPL for other relations.
The rest of the paper is organized as follows.
Section 2 and 3 describe SPL and LRPL respec-
tively. Section 4 reports on our experiments, and
section 5 and 6 describe related works and conclu-
sions.
</bodyText>
<sectionHeader confidence="0.671602" genericHeader="method">
2 String Pattern Learning (SPL)
</sectionHeader>
<bodyText confidence="0.9536637">
Both SPL and LRPL start with seed tuples that
were extracted by the baseline KnowItAll system,
with extraction frequency at or above a threshold
(set to 2 in these experiments). In these experi-
ments, we downloaded a set of sentences from the
Web that contained an occurrence of at least one
relation label and used this as our reservoir of un-
labeled training and test sentences. We created a
set of positive training sentences from those sen-
tences that contained both argument values of a
seed tuple.
SPL employs a method similar to that of
(Downey et al., 2004). It generates candidate ex-
traction rules with a prefix context, a middle con-
text, and a right context. The prefix is zero to Lid,
tokens immediately to the left of extracted argu-
ment1, the middle context is all tokens between
argument1 and argument2, and the right context of
zero to Lside tokens immediately to the right of ar-
gument2. It discards patterns with more than Laid
intervening tokens or without a relation label.
SPL tabulates the occurrence of such patterns
in the set of positive training sentences (all sen-
tences from the reservoir that contain both argu-
ment values from a seed tuple in either order), and
also tabulates their occurrence in negative training
sentences. The negative training are sentences that
have one argument value from a seed tuple and a
nearest simple NP in place of the other argument
value. This idea is based on that of (Ravichan-
dran and Hovy, 2002) for a QA system. SPL
learns a possibly large set of strict extraction rules
that have alternating context strings and extraction
slots, with no gaps or wildcards in the rules.
SPL selects the best patterns as follows:
1. Groups the context strings that have the exact
same middle string.
2. Selects the best pattern having the largest pat-
tern score, PS, for each group of context
strings having the same middle string.
</bodyText>
<equation confidence="0.949885333333333">
�S���������p�� � �
PSp~ � �S���������p�� � �S���������p�� + a
(1)
</equation>
<listItem confidence="0.7591775">
3. Selects the patterns having PS greater than
Tpattern.
</listItem>
<page confidence="0.99684">
57
</page>
<figureCaption confidence="0.9841265">
Figure 1: The architecture of LRPL (Less Restric-
tive Pattern Learning).
</figureCaption>
<bodyText confidence="0.999833833333333">
where Spo,itive(p) is a set of sentences that match
pattern p and include both argument values of a
seed tuple. Snegative(p) is a set of sentences that
match p and include just one argument value of
a seed tuple (e.g. just a company or a person for
CeoOf). a is a constant for smoothing.
</bodyText>
<sectionHeader confidence="0.97255" genericHeader="method">
3 Less Restrictive Pattern Learning
(LRPL)
</sectionHeader>
<bodyText confidence="0.99773835">
LRPL uses a more flexible rule representation than
SPL. As before, the rules are based on a window of
tokens to the left of the first argument, a window
of middle tokens, and a window of tokens to the
right of the second argument. Rather than using
exact string match on a simple sequence of tokens,
LRPL uses a combination of bag of words and im-
mediately adjacent token. The left context is based
on a window of L,ide tokens immediately to the
left of argument1. It has two sets of tokens: the
token immediately to the left and a bag of words
for the remaining tokens. Each of these sets may
have zero or more tokens. The middle and right
contexts are similarly defined. We call this repre-
sentation extended bag of words.
Here is an example of how LRPL represents
the context of a training sentence with win-
dow size set to 4. “Yesterday , &lt;Arg2&gt;Steve
Ballmer&lt;/Arg2&gt;, the Chief Executive Officer of
&lt;Arg1&gt;Microsoft&lt;/Arg1&gt; said that he is ...”.
</bodyText>
<listItem confidence="0.3180814">
order: arg2_arg1
values: Steve Ballmer, Microsoft
L: {yesterday) {,)
M: {,) {chief executive officer the) {of)
R: {said) {he is that)
</listItem>
<bodyText confidence="0.998591433333333">
Some of the tokens in these bags of words may
be dropped in merging this with patterns from
other training sentences. Each rule also has a con-
fidence score, learned from EM-estimation.
We experimented with simply using three bags
of words as in SnowBall, but found that precision
was increased when we distinguished the tokens
immediately adjacent to argument values from the
other tokens in the left, middle, and right bag of
words.
Less restrictive patterns require a Named Entity
Recognizer (NER), because the patterns can not
extract candidate entities by themselves1. LRPL
trains a supervised NER in bootstrapping for ex-
tracting candidate entities.
Figure 1 overviews LRPL. It consists of two
bootstrapping modules: Relation NER and Rela-
tion Assessor. LRPL trains the Relational NER
from seed tuples provided by the baseline Know-
ItAll system and unlabeled sentences in the reser-
voir. Then it does NE tagging on the sentences to
learn the less restrictive rules and to extract can-
didate tuples. The learning and extraction steps at
Relation Assessor are similar to that of SnowBall;
it generates a set of rules and uses EM-estimation
to compute a confidence in each rule. When these
rules are applied, the system computes a probabil-
ity for each tuple based on the rule confidence, the
degree of match between a sentence and the rule,
and the extraction frequency.
</bodyText>
<subsectionHeader confidence="0.964522">
3.1 Relation dependent Named Entity
Recognizer
</subsectionHeader>
<bodyText confidence="0.999745235294118">
Relation NER leverages an off-the-shelf super-
vised NER, based on Conditional Random Fields
(CRF). In Figure 1, TrainSentenceGenerator auto-
matically generates training sentences from seeds
and unlabeled sentences in the reservoir. TrainEn-
tityRecognizer trains a CRF on the training sen-
tences and then EntityRecognizer applies the
trained CRF to all the unlabeled sentences, creat-
ing entity annotated sentences.
It can extract entities whose type matches an ar-
gument type of a particular relation. The type is
not explicitly specified by a user, but is automati-
cally determined according to the seed tuples. For
example, it can extract ‘City’ and ‘Mayor’ type en-
tities for MayorOf(City, Mayor) relation. We de-
scribe CRF in brief, and then how to train it in
bootstrapping.
</bodyText>
<footnote confidence="0.9568165">
1Although using all noun phrases in a sentence may be
possible, it apparently results in low precision.
</footnote>
<page confidence="0.995341">
58
</page>
<subsectionHeader confidence="0.332444">
3.1.1 Supervised Named Entity Recognizer
</subsectionHeader>
<bodyText confidence="0.9997655">
Several state-of-the-art supervised NERs are
based on a feature-rich probabilistic conditional
classifier such as Conditional Random Fields
(CRF) for sequential learning tasks(Lafferty et al.,
2001; Rosenfeld et al., 2005). The input of CRF is
a feature sequence X of features x,, and outputs a
tag sequence T of tags t,. In the training phrase, a
set of &lt; XZ, Ti &gt; is provided, and outputs a model
M,,f. In the applying phase, given X, it outputs a
tag sequence T by using M,,f. In the case of NE
tagging, given a sequence of tokens, it automat-
ically generates a sequence of feature sets; each
set is corresponding to a token. It can incorporate
any properties that can be represented as a binary
feature into the model, such as words, capitalized
patterns, part-of-speech tags and the existence of
the word in a dictionary. It works quite well on
NE tagging tasks (McCallum and Li, 2003).
</bodyText>
<subsectionHeader confidence="0.5944645">
3.1.2 How to Train Supervised NER in
Bootstrapping
</subsectionHeader>
<bodyText confidence="0.99988">
We use bootstrapping to train CRF for relation-
specific NE tagging as follows: 1) select the sen-
tences that include all the entity values of a seed
tuple, 2) automatically mark the argument values
in each sentence, and 3)train CRF on the seed
marked sentences. An example of a seed marked
sentence is the following:
</bodyText>
<table confidence="0.62579275">
seed tuple: &lt;Microsoft, Steve Ballmer&gt;
seed marked sentence:
&amp;quot;Yesterday, &lt;Arg2&gt;Steve Ballmer&lt;/Arg2&gt;,
CEO of &lt;Arg1&gt;Microsoft&lt;/Arg1&gt;
</table>
<tableCaption confidence="0.249987">
announced that ...&amp;quot;
</tableCaption>
<bodyText confidence="0.99653825">
Because of redundancy, we can expect to gen-
erate a fairly large number of seed marked sen-
tences by using a few highly frequent seed tuples.
To avoid overfitting on terms from these seed tu-
ples, we substitute the actual argument values with
random characters for each training sentence, pre-
serving capitalization patterns and number of char-
acters in each token.
</bodyText>
<subsectionHeader confidence="0.996942">
3.2 Relation Assessor
</subsectionHeader>
<bodyText confidence="0.997912173913043">
Relation Assessor employs several SnowBall-like
techniques including making rules by clustering
and EM-estimation for the confidence of the rules
and tuples.
In Figure 1, ContextRepresentationGenerator
generates extended bag of words contexts, from
entity annotated sentences, and classifies the con-
texts into two classes: training contexts C�,,i� (if
their entity values and their orders match a seed
tuple) and test contexts Ct,t (otherwise). Train-
ConfidenceEstimator clusters Ctra,i,,, based on the
match score between contexts, and generates a
rule from each cluster, that has average vectors
over contexts belonging to the cluster. Given a set
of generated rules R and test contexts C�,,t, Confi-
denceEstimator estimates each tuple confidence in
Ctest by using an EM algorithm. It also estimates
the confidence of the tuples extracted by the base-
line system, and outputs the merged result tuples
with confidence.
We describe the match score calculation
method, the EM-algorithm, and the merging
method in the following sub sections.
</bodyText>
<subsectionHeader confidence="0.67155">
3.2.1 Match Score Calculation
</subsectionHeader>
<bodyText confidence="0.999811">
The match score (or similarity) M of two ex-
tended bag of words contexts c~, cj is calculated
as the linear combination of the cosine values be-
tween the corresponding vectors.
</bodyText>
<equation confidence="0.9462385">
M(ci, cj) = wstcos(ci,t, cyst) (2)
s,t
</equation>
<bodyText confidence="0.957713173913043">
where, s is the index of left, middle, or right con-
texts. t is the index of left adjacent, right adjacent,
or other tokens. wst is the weight corresponding
to the context vector indexed by s and t.
To achieve high precision, Relation Assessor
uses only the entity annotated sentences that have
just one entity for each argument (two entities
in total) and where those entities co-occur within
L,,id tokens window, and it uses at most Lid, left
and right tokens. It discards patterns without a re-
lation label.
3.2.2 EM-estimation for tuple and rule
confidence
Several rules generated from only positive ev-
idence result in low precision (e.g. rule “of” for
MayorOf relation generated from “Rudolph Giu-
liani of New York”). This problem can be im-
proved by estimating the rule confidence by the
following EM-algorithm.
1. For each c2j in Ct,t, identifies the best match
rule r*(c2j), based on the match score be-
tween cii and each rule r. cid is the jth con-
text that includes tuple t~.
</bodyText>
<equation confidence="0.996231">
r&apos;(czj) = argmaxrM(r, czj) (3)
</equation>
<page confidence="0.986766">
59
</page>
<listItem confidence="0.9012258">
2. Initializes seed tuple confidence, TC(ts) = 1
for all ts, where ts is a seed tuple.
3. Calculates tuple confidence, TC, and rule
confidence, RC, by using EM-algorithm. E
and M stages are iterated several times.
</listItem>
<equation confidence="0.975497571428572">
E stage:
RC(rk) _ Eu TC(tu(rk)) + 1 (4)
numO f Match (rk) +
M stage:
TC(ti) _ (5)
1 —� (1 — RC(r«(czj))M(r«(czj), czj))(6)
�
</equation>
<bodyText confidence="0.891202">
where
</bodyText>
<equation confidence="0.9919405">
TC(tu(rk)) _1 TC(t�) if r~ _ r«(c��) 37
0 otherwise
</equation>
<bodyText confidence="0.9800426">
, is a constant for smoothing.
This algorithm assigns a high confidence to the
rules that frequently co-occur with only high con-
fident tuples. It also assigns a high confidence to
the tuples that frequently co-occur with the con-
texts that match high confidence rules.
When it merges the tuples extracted by the base-
line system, the algorithm uses the following con-
stant value for any context that matches a baseline
pattern.
</bodyText>
<equation confidence="0.903300333333333">
RC(rb)M(rb, cib) = max RC(r*(c))M(r*(c), c)
�
(7)
</equation>
<bodyText confidence="0.999861714285714">
where cib denotes the context of tuple ti that
matches a baseline pattern, and rb is any baseline
pattern. With this calculation, the confidence of
any tuple extracted by a baseline pattern is always
greater than or equal to that of any tuple that is
extracted by the learned rules and has the same
frequency.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="evaluation">
4 Evaluation
</sectionHeader>
<bodyText confidence="0.99591325">
The focus of this paper is the comparison be-
tween bootstrapping strategies for extraction, i.e.,
string pattern learning and less restrictive pattern
learning having Relation NER. Therefore, we first
compare these two bootstrapping methods with
the baseline system. Furthermore, we also com-
pare Relation NER with a generic NER, which is
trained on a pre-existing hand annotated corpus.
</bodyText>
<tableCaption confidence="0.840181">
Table 1: Weights corresponding to a context vector
</tableCaption>
<bodyText confidence="0.498266">
(wst).
</bodyText>
<subsectionHeader confidence="0.988909">
4.1 Relation Extraction Task
</subsectionHeader>
<bodyText confidence="0.990722717948718">
We compare SPL and LRPL with the baseline sys-
tem on 5 relations: Acquisition, Merger, CeoOf,
MayorOf, and InventorOf. We downloaded about
from 100,000 to 220,000 sentences for each of
these relations from the Web, which contained a
relation label (e.g. “acquisition”, “acquired”, “ac-
quiring” or “merger”, “merged”, “merging”). We
used all the tuples that co-occur with baseline pat-
terns at least twice as seeds. The numbers of seeds
are between 33 (Acquisition) and 289 (CeoOf).
For consistency, SPL employs the same assess-
ment methods with LRPL. It uses the EM algo-
rithm in Section 3.2.2 and merges the tuples ex-
tracted by the baseline system. In the EM algo-
rithm, the match score M(p, t) between a learned
pattern p and a tuple t is set to a constant T,,,,atch.
LRPL uses MinorThird (Cohen, 2004) imple-
mentation of CRF for Relation NER. The features
used in the experiments are the lower-case word,
capitalize pattern, part of speech tag of the cur-
rent and +-2 tokens, and the previous state (tag)
referring to (Minkov et al., 2005; Rosenfeld et al.,
2005). The parameters used for SPL and LRPL
are experimentally set as follows: Tpattern = 0.5,
Tmtcch = 0.6, Lmid = 5, Lside = 3, a = 3,
, = 3 and the context weights for LRPL shown in
Table 1.
Figure 2-6 show the recall-precision curves. We
use the number of correct extractions to serve as
a surrogate for recall, since computing actual re-
call would require extensive manual inspection of
the large data sets. Compared to the the baseline
system, both bootstrapping methods increases the
number of correct extractions for almost all the re-
lations at around 80% precision. For MayorOf re-
lation, LRPL achieves 250% increase in recall at
0.87 precision, while SPL’s precision is less than
the baseline system. This is because SPL can not
distinguish correct tuples from the error tuples that
</bodyText>
<figure confidence="0.995466611111111">
adjacency
left
other
right
total
left
0.067
0.133
0.2
context middle
right
0.24
0.12
0.24
0.133
0.067
0.6
0.2
</figure>
<page confidence="0.579782">
60
</page>
<figureCaption confidence="0.99570825">
Figure 2: The recall-precision curve of CeoOf re-
lation.
Figure 4: The recall-precision curve of Acquisi-
tion relation.
Figure 3: The recall-precision curve of MayorOf
relation.
Figure 5: The recall-precision curve of Merger re-
lation.
</figureCaption>
<bodyText confidence="0.968755333333333">
co-occur with a short strict pattern, and that have a
wrong entity type value. An example of the error
tuples extracted by SPL is the following:
</bodyText>
<table confidence="0.533584">
Learned Pattern: NP1 Mayor NP2
Sentence:
&amp;quot;When Lord Mayor Clover Moore spoke,...&amp;quot;
Tuple: &lt;Lord, Clover Moore&gt;
</table>
<bodyText confidence="0.997508857142857">
The improvement of Acquisition and Merger re-
lations is small for both methods; the rules learned
for Merger and Acquisition made erroneous ex-
tractions of mergers of geo-political entities, ac-
quisition of data, ball players, languages or dis-
eases. For InventorOf relation, LRPL does not
work well. This is because ‘Invention’ is not a
proper noun phrase, but a noun phrase. A noun
phrase includes not only nouns, but a particle,
a determiner, and adjectives in addition to non-
capitalized nouns. Our Relation NER was unable
to detect regularities in the capitalization pattern
and word length of invention phrases.
At around 60% precision, SPL achieves higher
recall for CeoOf and MayorOf relations, in con-
trast, LRPL achieves higher recall for Acquisition
and Merger. The reason can be that nominal style
relations (CeoOf and MayorOf) have a smaller
syntactic variety for describing them. Therefore,
learned string patterns are enough generic to ex-
tract many candidate tuples.
</bodyText>
<subsectionHeader confidence="0.990612">
4.2 Entity Recognition Task
</subsectionHeader>
<bodyText confidence="0.999830461538461">
Generic types such as person, organization, and
location cover many useful relations. One might
expect that NER trained for these generic types,
can be used for different relations without mod-
ifications, instead of creating a Relation NER.
To show the effectiveness of Relation NER, we
compare Relation NER with a generic NER
trained on a pre-existent hand annotated corpus
for generic types; we used MUC7 train, dry-run
test, and formal-test documents(Table 2) (Chin-
chor, 1997). We also incorporate the following
additional knowledge into the CRF’s features re-
ferring to (Minkov et al., 2005; Rosenfeld et al.,
</bodyText>
<page confidence="0.998354">
61
</page>
<figureCaption confidence="0.957103">
Figure 6: The recall-precision curve of InventorOf
relation.
</figureCaption>
<tableCaption confidence="0.818285666666667">
Table 2: The number of entities and unique entities
in MUC7 corpus. The number of documents is
225.
</tableCaption>
<table confidence="0.779452">
entity all uniq
Organization 3704 993
Person 2120 1088
Location 2912 692
</table>
<bodyText confidence="0.9993039375">
2005): first and last names, city names, corp des-
ignators, company words (such as “technology”),
and small size lists of person title (such as “mr.”)
and capitalized common words (such as “Mon-
day”). The base features for both methods are the
same as the ones described in Section 4.1.
The ideal entity recognizer for relation extrac-
tion is recognizing only entities that have an ar-
gument type for a particular relation. Therefore,
a generic test set such as MUC7 Named Entity
Recognition Task can not be used for our evalu-
ation. We randomly selected 200 test sentences
from our dataset that had a pair of correct enti-
ties for CeoOf or MayorOf relations, and were not
used as training for the Relation NER. We mea-
sured the accuracy as follows.
</bodyText>
<equation confidence="0.8917466">
�Eextracted � Etrue,arg�
Recallarg = (8)
lEtrue,arg l
Precisionarg = Eextracted n Etrue,arg (9)
Eextracted l
</equation>
<bodyText confidence="0.99995325">
where, Etrue,arg is a set of true entities that have
an argument type of a target relation. Eextracted is
a set of entities extracted as an argument.
Because Relation NER is trained for argument
types (such as ‘Mayor’), and the generic NER is
trained for generic types (such as person), this cal-
culation is in favor of Relation NER. For fair com-
parison, we also use the following measure.
</bodyText>
<equation confidence="0.896981666666667">
�Eextracted � Etrue,generic�
Precisiongeneric =
Eextractedl
</equation>
<bodyText confidence="0.9684189">
(10)
where, Etrue,generic is a set of true entities that
have a generic type 2.
Table 3 shows that the Relation NER consis-
tently works better than the generic NER, even
when additional knowledge much improved the
recall. This suggests that training a Relation NER
for each particular relation in bootstrapping is bet-
ter approach than using a NER trained for generic
types.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="related work">
5 Related Work
</sectionHeader>
<bodyText confidence="0.999942347826087">
SPL is a similar approach to DIPRE (Brin, 1998)
DIPRE uses a pre-defined simple regular expres-
sion to identify argument values. Therefore, it can
also suffer from the type error problem described
above. LRPL avoids this problem by using the Re-
lation NER.
LRPL is similar to SnowBall(Agichtein, 2005),
which employs a generic NER, and reported that
most errors come from NER errors. Because our
evaluation showed that Relation NER works better
than generic NER, a combination of Relation NER
and SnowBall can make a better result in other set-
tings. 3
(Collins and Singer, 1999) and (Jones, 2005)
describe self-training and co-training methods for
Named Entity Classification. However, the prob-
lem of NEC task, where the boundary of entities
are given by NP chunker or parser, is different
from NE tagging task. Because the boundary of an
entity is often different from a NP boundary, the
technique can not be used for our purpose; “Mi-
crosoft CEO Steve Ballmer” is tagged as a single
noun phrase.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.9992255">
This paper describes two bootstrapping strategies,
SPL, which learns simple string patterns, and
LRPL, which trains Relation NER and uses it with
less restrictive patterns. Evaluations showed both
</bodyText>
<footnote confidence="0.9319408">
2Although Recallyen,erz, can be defined in the same way,
we did not use it, because of our purpose and much effort
needed for complete annotation for generic types.
3Of course, further study needed for investigating whether
Relation NER works with a smaller number of seeds.
</footnote>
<page confidence="0.998762">
62
</page>
<tableCaption confidence="0.960469333333333">
Table 3: The argument precision and recall is the average over all arguments for CeoOf, and MayorOf
relations. The Location is for MayorOf, Organization is for CeoOf, and person is the average of both
relations.
</tableCaption>
<table confidence="0.9883714">
Argument Location Organization Person
Recall Precision F Precision Precision Precision
R-NER 0.650 0.912 0.758 0.922 0.906 0.955
G-NER 0.392 0.663 0.492 0.682 0.790 0.809
G-NER+dic 0.577 0.643 0.606 0.676 0.705 0.842
</table>
<bodyText confidence="0.999429642857143">
methods enhance the recall of the baseline sys-
tem for almost all the relations. For some rela-
tions, SPL and LRPL have comparable recall and
precision. For InventorOf, where the invention is
not a named entity, SPL performed better, because
its patterns are based on noun phrases rather than
named entities.
LRPL works better than SPL for MayorOf re-
lation by avoiding several errors caused by the tu-
ples that co-occur with a short strict context, but
have a wrong type entity value. Evaluations also
showed that Relation NER works better than the
generic NER trained on MUC7 corpus with addi-
tional dictionaries.
</bodyText>
<sectionHeader confidence="0.996041" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.99959725">
This work was done while the first author was a visit-
ing Scholar at the University of Washington. The work was
carried out at the University’s Turing Center and was sup-
ported in part by NSF grant IIS-0312988, DARPA contract
NBCHD030010, ONR grant N00014-02-1-0324, and a gift
from Google. We would like to thank Dr. Eugene Agichtein
for informing us the technical details of SnowBall, and Prof.
Ronen Feldman for a helpful discussion.
</bodyText>
<sectionHeader confidence="0.99927" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999885145454545">
Eugene Agichtein. 2005. Extracting Relations From
Large Text Collections. Ph.D. thesis, Columbia Uni-
versity.
Sergey Brin. 1998. Extracting Patterns and Relations
from the World Wide Web. In WebDB Workshop at
EDBT’98, pages 172–183, Valencia, Spain.
Nancy Chinchor. 1997. Muc-7 named entity task defi-
nition version 3.5.
William W. Cohen. 2004. Minorthird: Methods for
identifying names and ontological relations in text
using heuristics for inducing regularities from data.
Michael Collins and Yoram Singer. 1999. Unsuper-
vised models for named entity classification. In
EMNLP 99.
Doug Downey, Oren Etzioni, Stephen Soderland, and
Daniel S. Weld. 2004. Learning text patterns
for web information extraction and assessment. In
AAAI 2004 Workshop on ATEM.
Oren Etzioni, Michael Cafarella, Doug Downey, Ana-
Maria Popescu, Tal Shaked, Stephen Soderland,
Daniel S. Weld, and Alexander Yates. 2005. Unsu-
pervised named-entity extraction from the web: an
experimental study. Artif. Intell., 165(1):91–134.
Ronen Feldman, Yonatan Aumann, Michal Finkelstein-
Landau, Eyal Hurvitz, Yizhar Regev, and Ariel
Yaroshevich. 2002. A comparative study of in-
formation extraction strategies. In CICLing, pages
349–359.
Rosie Jones. 2005. Learning to Extract Entities from
Labeled and Unlabeled Texts. Ph.D. thesis, CMU-
LTI-05-191.
John D. Lafferty, Andrew McCallum, and Fernando
C. N. Pereira. 2001. Conditional random fields:
Probabilistic models for segmenting and labeling se-
quence data. In ICML ’01, pages 282–289.
Andrew McCallum and Wei Li. 2003. Early results for
named entity recognition with conditional random
fields, feature induction and web-enhanced lexicons.
In CoNLL-2003).
Einat Minkov, Richard C. Wang, and William W. Co-
hen. 2005. Extracting personal names from email:
Applying named entity recognition to informal text.
In EMNLP/HLT-2005.
D. Ravichandran and D. Hovy. 2002. Learning Sur-
face Text Patterns for a Question Answering Sys-
tem. In Procs. of the 40th Annual Meeting of the As-
sociation for Computational Linguistics, pages 41–
47, Philadelphia, Pennsylvania.
Binyamin Rosenfeld, Moshe Fresko, and Ronen Feld-
man. 2005. A systematic comparison of feature-
rich probabilistic classifiers for ner tasks. In PKDD,
pages 217–227.
Shubin Zhao and Ralph Grishman. 2005. Extracting
relations with integrated information using kernel
methods. In ACL’05, pages 419–426, June.
</reference>
<page confidence="0.999464">
63
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.505626">
<title confidence="0.998532">Expanding the Recall of Relation Extraction by Bootstrapping</title>
<author confidence="0.696651">Junji</author>
<affiliation confidence="0.7449155">NTT Cyber Solutions NTT</affiliation>
<address confidence="0.88595">1-1 Hikarinooka Kanagawa 239-0847,</address>
<email confidence="0.94649">tomita.junji@lab.ntt.co.jp</email>
<author confidence="0.998039">Stephen Soderland Oren</author>
<affiliation confidence="0.997370666666667">Department of Computer &amp; University of</affiliation>
<address confidence="0.983837">Seattle, WA</address>
<email confidence="0.999853">@cs.washington.edu</email>
<abstract confidence="0.999630047619047">Most works on relation extraction assume considerable human effort for making an annotated corpus or for knowledge engineering. Generic patterns employed in KnowItAll achieve unsupervised, highprecision extraction, but often result in low recall. This paper compares two bootstrapping methods to expand recall that start with automatically extracted seeds by KnowItAll. The first method is string pattern learning, which learns string contexts adjacent to a seed tuple. The second method learns less restrictive patterns that include bags of words and relation-specific named entity tags. Both methods improve the recall of the generic pattern method. In particular, the less restrictive pattern learning method can achieve a 250% increase in recall at 0.87 precision, compared to the generic pattern method.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eugene Agichtein</author>
</authors>
<title>Extracting Relations From Large Text Collections.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia University.</institution>
<contexts>
<context position="3603" citStr="Agichtein, 2005" startWordPosition="547" endWordPosition="548">ooks at ways to enhance recall over this baseline system while maintaining high precision. To enhance recall, we employ bootstrapping techniques which start with seed tuples, i.e. the most frequently extracted tuples by the baseline system. The first method represents rules with three context strings of tokens immediately adjacent to the extracted arguments: a left context, 56 middle context, and right context. These are induced from context strings found adjacent to seed tuples. The second method uses a less restrictive pattern representation such as bag of words, similar to that of SnowBall(Agichtein, 2005). SnowBall is a semi-supervised relation extraction system. The input of Snowball is a few hand labeled correct seed tuples for a relation (e.g. &lt;Microsoft, Steve Ballmer&gt; for CeoOf relation). SnowBall clusters the bag of words representations generated from the context strings adjacent to each seed tuple, and generates rules from them. It calculates the confidence of candidate tuples and the rules iteratively by using an EM-algorithm. Because it can extract any tuple whose entities co-occur within a window, the recall can be higher than the string pattern learning method. The main disadvantag</context>
<context position="23565" citStr="Agichtein, 2005" startWordPosition="3866" endWordPosition="3867"> type 2. Table 3 shows that the Relation NER consistently works better than the generic NER, even when additional knowledge much improved the recall. This suggests that training a Relation NER for each particular relation in bootstrapping is better approach than using a NER trained for generic types. 5 Related Work SPL is a similar approach to DIPRE (Brin, 1998) DIPRE uses a pre-defined simple regular expression to identify argument values. Therefore, it can also suffer from the type error problem described above. LRPL avoids this problem by using the Relation NER. LRPL is similar to SnowBall(Agichtein, 2005), which employs a generic NER, and reported that most errors come from NER errors. Because our evaluation showed that Relation NER works better than generic NER, a combination of Relation NER and SnowBall can make a better result in other settings. 3 (Collins and Singer, 1999) and (Jones, 2005) describe self-training and co-training methods for Named Entity Classification. However, the problem of NEC task, where the boundary of entities are given by NP chunker or parser, is different from NE tagging task. Because the boundary of an entity is often different from a NP boundary, the technique ca</context>
</contexts>
<marker>Agichtein, 2005</marker>
<rawString>Eugene Agichtein. 2005. Extracting Relations From Large Text Collections. Ph.D. thesis, Columbia University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sergey Brin</author>
</authors>
<title>Extracting Patterns and Relations from the World Wide Web.</title>
<date>1998</date>
<booktitle>In WebDB Workshop at EDBT’98,</booktitle>
<pages>172--183</pages>
<location>Valencia,</location>
<contexts>
<context position="23313" citStr="Brin, 1998" startWordPosition="3826" endWordPosition="3827">h as person), this calculation is in favor of Relation NER. For fair comparison, we also use the following measure. �Eextracted � Etrue,generic� Precisiongeneric = Eextractedl (10) where, Etrue,generic is a set of true entities that have a generic type 2. Table 3 shows that the Relation NER consistently works better than the generic NER, even when additional knowledge much improved the recall. This suggests that training a Relation NER for each particular relation in bootstrapping is better approach than using a NER trained for generic types. 5 Related Work SPL is a similar approach to DIPRE (Brin, 1998) DIPRE uses a pre-defined simple regular expression to identify argument values. Therefore, it can also suffer from the type error problem described above. LRPL avoids this problem by using the Relation NER. LRPL is similar to SnowBall(Agichtein, 2005), which employs a generic NER, and reported that most errors come from NER errors. Because our evaluation showed that Relation NER works better than generic NER, a combination of Relation NER and SnowBall can make a better result in other settings. 3 (Collins and Singer, 1999) and (Jones, 2005) describe self-training and co-training methods for N</context>
</contexts>
<marker>Brin, 1998</marker>
<rawString>Sergey Brin. 1998. Extracting Patterns and Relations from the World Wide Web. In WebDB Workshop at EDBT’98, pages 172–183, Valencia, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nancy Chinchor</author>
</authors>
<title>Muc-7 named entity task definition version 3.5.</title>
<date>1997</date>
<contexts>
<context position="21206" citStr="Chinchor, 1997" startWordPosition="3469" endWordPosition="3471">for describing them. Therefore, learned string patterns are enough generic to extract many candidate tuples. 4.2 Entity Recognition Task Generic types such as person, organization, and location cover many useful relations. One might expect that NER trained for these generic types, can be used for different relations without modifications, instead of creating a Relation NER. To show the effectiveness of Relation NER, we compare Relation NER with a generic NER trained on a pre-existent hand annotated corpus for generic types; we used MUC7 train, dry-run test, and formal-test documents(Table 2) (Chinchor, 1997). We also incorporate the following additional knowledge into the CRF’s features referring to (Minkov et al., 2005; Rosenfeld et al., 61 Figure 6: The recall-precision curve of InventorOf relation. Table 2: The number of entities and unique entities in MUC7 corpus. The number of documents is 225. entity all uniq Organization 3704 993 Person 2120 1088 Location 2912 692 2005): first and last names, city names, corp designators, company words (such as “technology”), and small size lists of person title (such as “mr.”) and capitalized common words (such as “Monday”). The base features for both met</context>
</contexts>
<marker>Chinchor, 1997</marker>
<rawString>Nancy Chinchor. 1997. Muc-7 named entity task definition version 3.5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William W Cohen</author>
</authors>
<title>Minorthird: Methods for identifying names and ontological relations in text using heuristics for inducing regularities from data.</title>
<date>2004</date>
<contexts>
<context position="18077" citStr="Cohen, 2004" startWordPosition="2958" endWordPosition="2959">ese relations from the Web, which contained a relation label (e.g. “acquisition”, “acquired”, “acquiring” or “merger”, “merged”, “merging”). We used all the tuples that co-occur with baseline patterns at least twice as seeds. The numbers of seeds are between 33 (Acquisition) and 289 (CeoOf). For consistency, SPL employs the same assessment methods with LRPL. It uses the EM algorithm in Section 3.2.2 and merges the tuples extracted by the baseline system. In the EM algorithm, the match score M(p, t) between a learned pattern p and a tuple t is set to a constant T,,,,atch. LRPL uses MinorThird (Cohen, 2004) implementation of CRF for Relation NER. The features used in the experiments are the lower-case word, capitalize pattern, part of speech tag of the current and +-2 tokens, and the previous state (tag) referring to (Minkov et al., 2005; Rosenfeld et al., 2005). The parameters used for SPL and LRPL are experimentally set as follows: Tpattern = 0.5, Tmtcch = 0.6, Lmid = 5, Lside = 3, a = 3, , = 3 and the context weights for LRPL shown in Table 1. Figure 2-6 show the recall-precision curves. We use the number of correct extractions to serve as a surrogate for recall, since computing actual recall</context>
</contexts>
<marker>Cohen, 2004</marker>
<rawString>William W. Cohen. 2004. Minorthird: Methods for identifying names and ontological relations in text using heuristics for inducing regularities from data.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Yoram Singer</author>
</authors>
<title>Unsupervised models for named entity classification.</title>
<date>1999</date>
<booktitle>In EMNLP 99.</booktitle>
<contexts>
<context position="23842" citStr="Collins and Singer, 1999" startWordPosition="3911" endWordPosition="3914"> a NER trained for generic types. 5 Related Work SPL is a similar approach to DIPRE (Brin, 1998) DIPRE uses a pre-defined simple regular expression to identify argument values. Therefore, it can also suffer from the type error problem described above. LRPL avoids this problem by using the Relation NER. LRPL is similar to SnowBall(Agichtein, 2005), which employs a generic NER, and reported that most errors come from NER errors. Because our evaluation showed that Relation NER works better than generic NER, a combination of Relation NER and SnowBall can make a better result in other settings. 3 (Collins and Singer, 1999) and (Jones, 2005) describe self-training and co-training methods for Named Entity Classification. However, the problem of NEC task, where the boundary of entities are given by NP chunker or parser, is different from NE tagging task. Because the boundary of an entity is often different from a NP boundary, the technique can not be used for our purpose; “Microsoft CEO Steve Ballmer” is tagged as a single noun phrase. 6 Conclusion This paper describes two bootstrapping strategies, SPL, which learns simple string patterns, and LRPL, which trains Relation NER and uses it with less restrictive patte</context>
</contexts>
<marker>Collins, Singer, 1999</marker>
<rawString>Michael Collins and Yoram Singer. 1999. Unsupervised models for named entity classification. In EMNLP 99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Downey</author>
<author>Oren Etzioni</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
</authors>
<title>Learning text patterns for web information extraction and assessment.</title>
<date>2004</date>
<booktitle>In AAAI 2004 Workshop on ATEM.</booktitle>
<contexts>
<context position="6274" citStr="Downey et al., 2004" startWordPosition="984" endWordPosition="987">ed works and conclusions. 2 String Pattern Learning (SPL) Both SPL and LRPL start with seed tuples that were extracted by the baseline KnowItAll system, with extraction frequency at or above a threshold (set to 2 in these experiments). In these experiments, we downloaded a set of sentences from the Web that contained an occurrence of at least one relation label and used this as our reservoir of unlabeled training and test sentences. We created a set of positive training sentences from those sentences that contained both argument values of a seed tuple. SPL employs a method similar to that of (Downey et al., 2004). It generates candidate extraction rules with a prefix context, a middle context, and a right context. The prefix is zero to Lid, tokens immediately to the left of extracted argument1, the middle context is all tokens between argument1 and argument2, and the right context of zero to Lside tokens immediately to the right of argument2. It discards patterns with more than Laid intervening tokens or without a relation label. SPL tabulates the occurrence of such patterns in the set of positive training sentences (all sentences from the reservoir that contain both argument values from a seed tuple </context>
</contexts>
<marker>Downey, Etzioni, Soderland, Weld, 2004</marker>
<rawString>Doug Downey, Oren Etzioni, Stephen Soderland, and Daniel S. Weld. 2004. Learning text patterns for web information extraction and assessment. In AAAI 2004 Workshop on ATEM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Oren Etzioni</author>
<author>Michael Cafarella</author>
<author>Doug Downey</author>
<author>AnaMaria Popescu</author>
<author>Tal Shaked</author>
<author>Stephen Soderland</author>
<author>Daniel S Weld</author>
<author>Alexander Yates</author>
</authors>
<title>Unsupervised named-entity extraction from the web: an experimental study.</title>
<date>2005</date>
<journal>Artif. Intell.,</journal>
<volume>165</volume>
<issue>1</issue>
<contexts>
<context position="1716" citStr="Etzioni et al., 2005" startWordPosition="239" endWordPosition="242">hieve a 250% increase in recall at 0.87 precision, compared to the generic pattern method. 1 Introduction Relation extraction is a task to extract tuples of entities that satisfy a given relation from textual documents. Examples of relations include CeoOf(Company, Ceo) and Acquisition(Organization, Organization). There has been much work on relation extraction; most of it employs knowledge engineering or supervised machine learning approaches (Feldman et al., 2002; Zhao and Grishman, 2005). Both approaches are labor intensive. We begin with a baseline information extraction system, KnowItAll (Etzioni et al., 2005), that does unsupervised information extraction at Web scale. KnowItAll uses a set of generic extraction patterns, and automatically instantiates rules by combining these patterns with user supplied relation labels. For example, KnowItAll has patterns for a generic “of” relation: NP1 ’s &lt;relation&gt; , NP2 NP2 , &lt;relation&gt; of NP1 where NP1 and NP2 are simple noun phrases that extract values of argument1 and argument2 of a relation, and &lt;relation&gt; is a user-supplied string associated with the relation. The rules may also constrain NP1 and NP2 to be proper nouns. If a user supplies the relation lab</context>
</contexts>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>Oren Etzioni, Michael Cafarella, Doug Downey, AnaMaria Popescu, Tal Shaked, Stephen Soderland, Daniel S. Weld, and Alexander Yates. 2005. Unsupervised named-entity extraction from the web: an experimental study. Artif. Intell., 165(1):91–134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronen Feldman</author>
</authors>
<title>Yonatan Aumann, Michal FinkelsteinLandau, Eyal Hurvitz, Yizhar Regev, and Ariel Yaroshevich.</title>
<date>2002</date>
<booktitle>In CICLing,</booktitle>
<pages>349--359</pages>
<marker>Feldman, 2002</marker>
<rawString>Ronen Feldman, Yonatan Aumann, Michal FinkelsteinLandau, Eyal Hurvitz, Yizhar Regev, and Ariel Yaroshevich. 2002. A comparative study of information extraction strategies. In CICLing, pages 349–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rosie Jones</author>
</authors>
<title>Learning to Extract Entities from Labeled and Unlabeled Texts.</title>
<date>2005</date>
<tech>Ph.D. thesis,</tech>
<pages>05--191</pages>
<contexts>
<context position="23860" citStr="Jones, 2005" startWordPosition="3916" endWordPosition="3917">s. 5 Related Work SPL is a similar approach to DIPRE (Brin, 1998) DIPRE uses a pre-defined simple regular expression to identify argument values. Therefore, it can also suffer from the type error problem described above. LRPL avoids this problem by using the Relation NER. LRPL is similar to SnowBall(Agichtein, 2005), which employs a generic NER, and reported that most errors come from NER errors. Because our evaluation showed that Relation NER works better than generic NER, a combination of Relation NER and SnowBall can make a better result in other settings. 3 (Collins and Singer, 1999) and (Jones, 2005) describe self-training and co-training methods for Named Entity Classification. However, the problem of NEC task, where the boundary of entities are given by NP chunker or parser, is different from NE tagging task. Because the boundary of an entity is often different from a NP boundary, the technique can not be used for our purpose; “Microsoft CEO Steve Ballmer” is tagged as a single noun phrase. 6 Conclusion This paper describes two bootstrapping strategies, SPL, which learns simple string patterns, and LRPL, which trains Relation NER and uses it with less restrictive patterns. Evaluations s</context>
</contexts>
<marker>Jones, 2005</marker>
<rawString>Rosie Jones. 2005. Learning to Extract Entities from Labeled and Unlabeled Texts. Ph.D. thesis, CMULTI-05-191.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>ICML ’01,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="4452" citStr="Lafferty et al., 2001" startWordPosition="680" endWordPosition="683"> representations generated from the context strings adjacent to each seed tuple, and generates rules from them. It calculates the confidence of candidate tuples and the rules iteratively by using an EM-algorithm. Because it can extract any tuple whose entities co-occur within a window, the recall can be higher than the string pattern learning method. The main disadvantage of SnowBall or a method which employs less restrictive patterns is that it requires Named Entity Recognizer (NER). We introduce Relation-dependent NER (Relation NER), which trains an off-the-shelf supervised NER based on CRF(Lafferty et al., 2001) with bootstrapping. This learns relation-specific NE tags, and we present a method to use these tags for relation extraction. This paper compares the following two bootstrapping strategies. SPL: a simple string pattern learning method. It learns string patterns adjacent to a seed tuple. LRPL: a less restrictive pattern learning method. It learns a variety of bag of words patterns, after training a Relation NER. Both methods are completely self-supervised extensions to the unsupervised KnowItAll. A user supplies KnowItAll with one or more relation labels to be applied to one or more generic ex</context>
<context position="11722" citStr="Lafferty et al., 2001" startWordPosition="1892" endWordPosition="1895"> The type is not explicitly specified by a user, but is automatically determined according to the seed tuples. For example, it can extract ‘City’ and ‘Mayor’ type entities for MayorOf(City, Mayor) relation. We describe CRF in brief, and then how to train it in bootstrapping. 1Although using all noun phrases in a sentence may be possible, it apparently results in low precision. 58 3.1.1 Supervised Named Entity Recognizer Several state-of-the-art supervised NERs are based on a feature-rich probabilistic conditional classifier such as Conditional Random Fields (CRF) for sequential learning tasks(Lafferty et al., 2001; Rosenfeld et al., 2005). The input of CRF is a feature sequence X of features x,, and outputs a tag sequence T of tags t,. In the training phrase, a set of &lt; XZ, Ti &gt; is provided, and outputs a model M,,f. In the applying phase, given X, it outputs a tag sequence T by using M,,f. In the case of NE tagging, given a sequence of tokens, it automatically generates a sequence of feature sets; each set is corresponding to a token. It can incorporate any properties that can be represented as a binary feature into the model, such as words, capitalized patterns, part-of-speech tags and the existence </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, and Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML ’01, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrew McCallum</author>
<author>Wei Li</author>
</authors>
<title>Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons.</title>
<date>2003</date>
<booktitle>In CoNLL-2003).</booktitle>
<contexts>
<context position="12414" citStr="McCallum and Li, 2003" startWordPosition="2021" endWordPosition="2024"> features x,, and outputs a tag sequence T of tags t,. In the training phrase, a set of &lt; XZ, Ti &gt; is provided, and outputs a model M,,f. In the applying phase, given X, it outputs a tag sequence T by using M,,f. In the case of NE tagging, given a sequence of tokens, it automatically generates a sequence of feature sets; each set is corresponding to a token. It can incorporate any properties that can be represented as a binary feature into the model, such as words, capitalized patterns, part-of-speech tags and the existence of the word in a dictionary. It works quite well on NE tagging tasks (McCallum and Li, 2003). 3.1.2 How to Train Supervised NER in Bootstrapping We use bootstrapping to train CRF for relationspecific NE tagging as follows: 1) select the sentences that include all the entity values of a seed tuple, 2) automatically mark the argument values in each sentence, and 3)train CRF on the seed marked sentences. An example of a seed marked sentence is the following: seed tuple: &lt;Microsoft, Steve Ballmer&gt; seed marked sentence: &amp;quot;Yesterday, &lt;Arg2&gt;Steve Ballmer&lt;/Arg2&gt;, CEO of &lt;Arg1&gt;Microsoft&lt;/Arg1&gt; announced that ...&amp;quot; Because of redundancy, we can expect to generate a fairly large number of seed ma</context>
</contexts>
<marker>McCallum, Li, 2003</marker>
<rawString>Andrew McCallum and Wei Li. 2003. Early results for named entity recognition with conditional random fields, feature induction and web-enhanced lexicons. In CoNLL-2003).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Einat Minkov</author>
<author>Richard C Wang</author>
<author>William W Cohen</author>
</authors>
<title>Extracting personal names from email: Applying named entity recognition to informal text.</title>
<date>2005</date>
<booktitle>In EMNLP/HLT-2005.</booktitle>
<contexts>
<context position="18312" citStr="Minkov et al., 2005" startWordPosition="2997" endWordPosition="3000">e numbers of seeds are between 33 (Acquisition) and 289 (CeoOf). For consistency, SPL employs the same assessment methods with LRPL. It uses the EM algorithm in Section 3.2.2 and merges the tuples extracted by the baseline system. In the EM algorithm, the match score M(p, t) between a learned pattern p and a tuple t is set to a constant T,,,,atch. LRPL uses MinorThird (Cohen, 2004) implementation of CRF for Relation NER. The features used in the experiments are the lower-case word, capitalize pattern, part of speech tag of the current and +-2 tokens, and the previous state (tag) referring to (Minkov et al., 2005; Rosenfeld et al., 2005). The parameters used for SPL and LRPL are experimentally set as follows: Tpattern = 0.5, Tmtcch = 0.6, Lmid = 5, Lside = 3, a = 3, , = 3 and the context weights for LRPL shown in Table 1. Figure 2-6 show the recall-precision curves. We use the number of correct extractions to serve as a surrogate for recall, since computing actual recall would require extensive manual inspection of the large data sets. Compared to the the baseline system, both bootstrapping methods increases the number of correct extractions for almost all the relations at around 80% precision. For Ma</context>
<context position="21320" citStr="Minkov et al., 2005" startWordPosition="3486" endWordPosition="3489">.2 Entity Recognition Task Generic types such as person, organization, and location cover many useful relations. One might expect that NER trained for these generic types, can be used for different relations without modifications, instead of creating a Relation NER. To show the effectiveness of Relation NER, we compare Relation NER with a generic NER trained on a pre-existent hand annotated corpus for generic types; we used MUC7 train, dry-run test, and formal-test documents(Table 2) (Chinchor, 1997). We also incorporate the following additional knowledge into the CRF’s features referring to (Minkov et al., 2005; Rosenfeld et al., 61 Figure 6: The recall-precision curve of InventorOf relation. Table 2: The number of entities and unique entities in MUC7 corpus. The number of documents is 225. entity all uniq Organization 3704 993 Person 2120 1088 Location 2912 692 2005): first and last names, city names, corp designators, company words (such as “technology”), and small size lists of person title (such as “mr.”) and capitalized common words (such as “Monday”). The base features for both methods are the same as the ones described in Section 4.1. The ideal entity recognizer for relation extraction is rec</context>
</contexts>
<marker>Minkov, Wang, Cohen, 2005</marker>
<rawString>Einat Minkov, Richard C. Wang, and William W. Cohen. 2005. Extracting personal names from email: Applying named entity recognition to informal text. In EMNLP/HLT-2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Ravichandran</author>
<author>D Hovy</author>
</authors>
<title>Learning Surface Text Patterns for a Question Answering System.</title>
<date>2002</date>
<booktitle>In Procs. of the 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>41--47</pages>
<location>Philadelphia, Pennsylvania.</location>
<contexts>
<context position="7164" citStr="Ravichandran and Hovy, 2002" startWordPosition="1136" endWordPosition="1140">ght context of zero to Lside tokens immediately to the right of argument2. It discards patterns with more than Laid intervening tokens or without a relation label. SPL tabulates the occurrence of such patterns in the set of positive training sentences (all sentences from the reservoir that contain both argument values from a seed tuple in either order), and also tabulates their occurrence in negative training sentences. The negative training are sentences that have one argument value from a seed tuple and a nearest simple NP in place of the other argument value. This idea is based on that of (Ravichandran and Hovy, 2002) for a QA system. SPL learns a possibly large set of strict extraction rules that have alternating context strings and extraction slots, with no gaps or wildcards in the rules. SPL selects the best patterns as follows: 1. Groups the context strings that have the exact same middle string. 2. Selects the best pattern having the largest pattern score, PS, for each group of context strings having the same middle string. �S���������p�� � � PSp~ � �S���������p�� � �S���������p�� + a (1) 3. Selects the patterns having PS greater than Tpattern. 57 Figure 1: The architecture of LRPL (Less Restrictive P</context>
</contexts>
<marker>Ravichandran, Hovy, 2002</marker>
<rawString>D. Ravichandran and D. Hovy. 2002. Learning Surface Text Patterns for a Question Answering System. In Procs. of the 40th Annual Meeting of the Association for Computational Linguistics, pages 41– 47, Philadelphia, Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Binyamin Rosenfeld</author>
<author>Moshe Fresko</author>
<author>Ronen Feldman</author>
</authors>
<title>A systematic comparison of featurerich probabilistic classifiers for ner tasks. In</title>
<date>2005</date>
<booktitle>PKDD,</booktitle>
<pages>217--227</pages>
<contexts>
<context position="11747" citStr="Rosenfeld et al., 2005" startWordPosition="1896" endWordPosition="1899">itly specified by a user, but is automatically determined according to the seed tuples. For example, it can extract ‘City’ and ‘Mayor’ type entities for MayorOf(City, Mayor) relation. We describe CRF in brief, and then how to train it in bootstrapping. 1Although using all noun phrases in a sentence may be possible, it apparently results in low precision. 58 3.1.1 Supervised Named Entity Recognizer Several state-of-the-art supervised NERs are based on a feature-rich probabilistic conditional classifier such as Conditional Random Fields (CRF) for sequential learning tasks(Lafferty et al., 2001; Rosenfeld et al., 2005). The input of CRF is a feature sequence X of features x,, and outputs a tag sequence T of tags t,. In the training phrase, a set of &lt; XZ, Ti &gt; is provided, and outputs a model M,,f. In the applying phase, given X, it outputs a tag sequence T by using M,,f. In the case of NE tagging, given a sequence of tokens, it automatically generates a sequence of feature sets; each set is corresponding to a token. It can incorporate any properties that can be represented as a binary feature into the model, such as words, capitalized patterns, part-of-speech tags and the existence of the word in a dictiona</context>
<context position="18337" citStr="Rosenfeld et al., 2005" startWordPosition="3001" endWordPosition="3004">e between 33 (Acquisition) and 289 (CeoOf). For consistency, SPL employs the same assessment methods with LRPL. It uses the EM algorithm in Section 3.2.2 and merges the tuples extracted by the baseline system. In the EM algorithm, the match score M(p, t) between a learned pattern p and a tuple t is set to a constant T,,,,atch. LRPL uses MinorThird (Cohen, 2004) implementation of CRF for Relation NER. The features used in the experiments are the lower-case word, capitalize pattern, part of speech tag of the current and +-2 tokens, and the previous state (tag) referring to (Minkov et al., 2005; Rosenfeld et al., 2005). The parameters used for SPL and LRPL are experimentally set as follows: Tpattern = 0.5, Tmtcch = 0.6, Lmid = 5, Lside = 3, a = 3, , = 3 and the context weights for LRPL shown in Table 1. Figure 2-6 show the recall-precision curves. We use the number of correct extractions to serve as a surrogate for recall, since computing actual recall would require extensive manual inspection of the large data sets. Compared to the the baseline system, both bootstrapping methods increases the number of correct extractions for almost all the relations at around 80% precision. For MayorOf relation, LRPL achi</context>
</contexts>
<marker>Rosenfeld, Fresko, Feldman, 2005</marker>
<rawString>Binyamin Rosenfeld, Moshe Fresko, and Ronen Feldman. 2005. A systematic comparison of featurerich probabilistic classifiers for ner tasks. In PKDD, pages 217–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shubin Zhao</author>
<author>Ralph Grishman</author>
</authors>
<title>Extracting relations with integrated information using kernel methods.</title>
<date>2005</date>
<booktitle>In ACL’05,</booktitle>
<pages>419--426</pages>
<contexts>
<context position="1589" citStr="Zhao and Grishman, 2005" startWordPosition="221" endWordPosition="224"> Both methods improve the recall of the generic pattern method. In particular, the less restrictive pattern learning method can achieve a 250% increase in recall at 0.87 precision, compared to the generic pattern method. 1 Introduction Relation extraction is a task to extract tuples of entities that satisfy a given relation from textual documents. Examples of relations include CeoOf(Company, Ceo) and Acquisition(Organization, Organization). There has been much work on relation extraction; most of it employs knowledge engineering or supervised machine learning approaches (Feldman et al., 2002; Zhao and Grishman, 2005). Both approaches are labor intensive. We begin with a baseline information extraction system, KnowItAll (Etzioni et al., 2005), that does unsupervised information extraction at Web scale. KnowItAll uses a set of generic extraction patterns, and automatically instantiates rules by combining these patterns with user supplied relation labels. For example, KnowItAll has patterns for a generic “of” relation: NP1 ’s &lt;relation&gt; , NP2 NP2 , &lt;relation&gt; of NP1 where NP1 and NP2 are simple noun phrases that extract values of argument1 and argument2 of a relation, and &lt;relation&gt; is a user-supplied string</context>
</contexts>
<marker>Zhao, Grishman, 2005</marker>
<rawString>Shubin Zhao and Ralph Grishman. 2005. Extracting relations with integrated information using kernel methods. In ACL’05, pages 419–426, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>