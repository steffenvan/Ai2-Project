<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001733">
<title confidence="0.957724">
LCC-SRN: LCC’s SRN System for SemEval 2007 Task 4
</title>
<author confidence="0.914794">
Adriana Badulescu
</author>
<affiliation confidence="0.6854585">
Language Computer Corporation
1701 N Collins Blvd #2000
</affiliation>
<address confidence="0.961061">
Richardson, TX, 75080
</address>
<email confidence="0.999013">
adriana@languagecomputer.com
</email>
<author confidence="0.774007">
Munirathnam Srikanth
</author>
<affiliation confidence="0.5872255">
Language Computer Corporation
1701 N Collins Blvd #2000
</affiliation>
<address confidence="0.942402">
Richardson, TX, 75080
</address>
<email confidence="0.997045">
srikanth@languagecomputer.com
</email>
<sectionHeader confidence="0.995613" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9930605">
This document provides a description of
the Language Computer Corporation (LCC)
SRN System that participated in the SemE-
val 2007 Semantic Relation between
Nominals task. The system combines the
outputs of different binary and multi-class
classifiers build using machine learning al-
gorithms like Decision Trees, Semantic
Scattering, Iterative Semantic Specializa-
tion, and Support Vector Machines.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999689366666667">
The Semantic Relations between Nominals task
from SemEval 2007 focuses on identifying the se-
mantic relations that hold between two arguments
manually annotated with word senses (Girju et al,
2007).
The previous work in identifying semantic rela-
tions between nominals focuses on finding one or
more relations in text for specific syntactic patterns
or constructions (like genitives and noun com-
pounds) using semi-automated and automated sys-
tems. An overview of some of these methods can
be found in (Badulescu, 2004).
The LCC SRN system, developed during the
SRN training period, was for us, the beginning of a
different approach to semantic relations detection:
detecting semantic relations in text without using a
syntactic pattern. Our existing work on semantic
relation detection was on detecting semantic rela-
tions in text (one or more at a time) at different
levels in the sentence using different syntactic pat-
terns like genitives, noun compounds, verb-
arguments, etc.
For SRN, we built a new system that combines
the output of the pattern dependent classifiers with
the new pattern-independent classifiers for better
results.
The remainder of this paper is organized as fol-
lows: Section 2 describes our system, Section 3
details the experimental results, and Section 4
summarizes the conclusions.
</bodyText>
<sectionHeader confidence="0.917166" genericHeader="method">
2 System description
</sectionHeader>
<bodyText confidence="0.99781215">
The system consists of two types of classifiers:
classifiers that do not use the syntactic parsed tree
and that were built specifically for the SemEval
2007 Task 4(SRN) and classifiers that use specific
syntactic pattern to determine the semantic rela-
tions and there were previously developed at LCC
and then adapted to the SRN task (SRNPAT).
The classifiers for each type were built from an-
notated examples using supervised machine learn-
ing algorithms like Decision Trees (DT)1, Support
Vector Machines (SVM) 2 , Semantic Scattering
(SS) (Moldovan and Badulescu, 2005) , Iterative
Semantic Specialization (ISS) (Girju, Badulescu,
and Moldovan, 2006), Naïve Bayes (NB) 3 and
Maximum Entropy (ME)4.
The outputs of different classifiers (built using
different types of machine learning algorithms
were combined and ranked using predefined rules.
Figure 1 shows the architecture of our SRN
system.
</bodyText>
<footnote confidence="0.9924168">
1 C5.0., http://www.rulequest.com/see5-info.html
2 LIBSVM, www.csie.ntu.edu.tw/~cjlin/libsvm/
3 jBNC, http://jbnc.sourceforge.net
4 http://homepages.inf.ed.ac.uk/s0450736/maxent_tool-
kit.html
</footnote>
<page confidence="0.988047">
215
</page>
<bodyText confidence="0.800747">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 215–218,
Prague, June 2007. c�2007 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.977344">
Sentences Annotations
</subsectionHeader>
<table confidence="0.999082">
Pre- Processing Tools
ocText
Annotated tree
Argument Detection Pattern Matching
[Arg1Arg2Pattern, Arg1, Arg2] [Pattern, NPArg1, NPArg2]
es Feature Extraction
Featur
Featu reSetsSRN FeatureSetsSRNPAT
[Arg1Arg2Pattern, Arg1, Arg2] [Pattern, NPArg1, NPArg2]
Classification SRN SRNPAT
RE All PATDT L=1, All
DT 3V.7, SS SV ISS
ME
REL1, ..., REL7, or NUL REL1, ..., REL7, or NUL
[Arg1, Arg2, Relation, [NPArg1, NPArg2, Relation,
Score,Classifier] Score, Classifier]
n Relation Selection
io
Siect
[Arg1, Arg2, Relation, Score]
</table>
<figureCaption confidence="0.997383">
Figure 1. The architecture of our SRN system.
</figureCaption>
<subsectionHeader confidence="0.970107">
2.1 Text Preprocessing
</subsectionHeader>
<bodyText confidence="0.999994214285714">
The sentences were processed using an in-house
text tokenizer, Brill’s part-of-speech tagger, an in-
house WordNet–based concept detector, an in-
house Named Entity Recognizer, and an in-house
syntactic parser.
Then, the syntactic and semantic information
obtained using these tools (concepts, part of
speech, named entities, etc) or obtained from the
sensekeys for the arguments as provided by the
Task 4 organizers (e.g. word senses, lemmas, etc)
were mapped into the syntactic trees. If an argu-
ment corresponds to more than one tree node, the
annotation was mapped to the phrase containing
the two nodes.
</bodyText>
<subsectionHeader confidence="0.999546">
2.2 Learning and Classification Methods
</subsectionHeader>
<bodyText confidence="0.999982875">
The core of our system is the learning and clas-
sification module.
We used two types of methods: pattern-
dependent that uses the syntactic parsed trees for
extracting and assigning a label to the arguments
and pattern-independent that creates classifiers
form all the examples disregarding the pattern in
the tree.
</bodyText>
<subsubsectionHeader confidence="0.585355">
2.2.1 Pattern-independent Methods (SRN)
</subsubsectionHeader>
<bodyText confidence="0.999087272727273">
Considering the limited number of examples for
each pattern, we developed pattern-independent
methods for classifying the semantic relations us-
ing the provided argument annotations and the
context from the sentence.
We built two types of classifiers: binary that
focuses on building a classifier for a specific rela-
tion (SRNREL) and multi-class methods that build
classifiers for all the SRN relations (SRN). Table 1
presents the accuracy of the classifiers built using
different machine learning algorithms.
</bodyText>
<figure confidence="0.714080111111111">
Relation DT SVM ME
1 52.10 46.15 46.67
2 41.40 30.76 60.00
3 61.70 51.61 63.33
4 59.30 52.17 53.33
5 58.60 39.99 50.00
6 71.70 24.99 73.33
7 50.00 57.13 43.33
Avg 56.40 43.26 55.71
</figure>
<tableCaption confidence="0.9797965">
Table 1. The accuracy of the SRNREL classifiers
built using different machine learning algorithms.
</tableCaption>
<bodyText confidence="0.999232222222222">
The classifiers were built using lexical, seman-
tic, and syntactic features of the arguments, their
phrases, their clauses, their common phrase/clause,
and their modifier or head phrase. The system uses
WordNet, an in-house Named Entity Recognizer,
and an in-house Syntactic Parser for determining
the values of some of these features. Table 2 pre-
sents the list of features used by the SRN classifi-
ers.
</bodyText>
<figureCaption confidence="0.911236421052632">
Argument’s lexical, semantic, and syntactic features: the
surface form, the label (POS tag or phrase label), the named
entity (human, group, location, etc), the WordNet hierarchy
(entity, group, abstraction, etc), the Semantic Scattering
class (e.g. object, substance, etc), the grammatical role (sub-
ject or object of the clause), the syntactic parser structure,
the POS Pattern (the sequence of POS of the words from the
argument), and the phrase pattern (the sequence of labels of
the phrases, words from the argument);
Argument&apos;s phrase features: surface form, label, gram-
matical role, named entity, POS pattern, Phrase patterns;
Argument&apos;s Modifier/Head features: the label, surface
forms, NE, and WN Hierarchy for the first modifier, post
modifier, pre-modifier, and head;
Arguments&apos; common tree node features: label, named
entity, grammatical role, POS pattern, and phrase pattern,
the tree path between arguments, and their order in tree;
Arguments&apos; clause: label, verb, voice, POS pattern, phrase
pattern.
</figureCaption>
<tableCaption confidence="0.986639">
Table 2. The list of features used for the SRN classi-
fiers.
</tableCaption>
<subsubsectionHeader confidence="0.632884">
2.2.2 Pattern-dependent Methods (SRNPAT)
</subsubsectionHeader>
<bodyText confidence="0.999774">
The second type of methods we used, were for
particular patterns frequent in the training corpus.
Table 3 shows the list of most frequent patterns in
the training corpus. For having general pattern and
covering the arguments that correspond to more
</bodyText>
<figure confidence="0.9849768">
Output
REL1: SENTID Value
Generate Output
...
REL7: SENTID Value
</figure>
<page confidence="0.99638">
216
</page>
<bodyText confidence="0.964503166666667">
40 LCC relations and respectively Part-Whole rela-
tion for the most frequent patterns from the SRN
corpus (Table 3).
than one node in a tree, we considered as argument
the noun phrase that contains the nominal instead
of the node for the nominal.
</bodyText>
<figureCaption confidence="0.903137741935484">
Pattern name Example
Noun compounds: If you are cleaning a &lt;e1&gt;coffee&lt;/e1&gt;
NN1 NN2 &lt;e2&gt;maker&lt;/e2&gt; that hasn&apos;t been
cleaned regularl271076ad
y, repeat this step again with a fresh
vinegar and water mixture.
Of-genitives: The incoming &lt;e1&gt;chairman&lt;/e1&gt; of
NP1 of NP2 the &lt;e2&gt;committee&lt;/e2&gt; is promising
an array of oversight investigations
that could provoke sharp disagreement
with Republicans and the White
House.
S-genitives: This is the &lt;e1&gt;government&lt;/e1&gt;&apos;s
NP1 ‘s NP2 &lt;e2&gt;effort&lt;/e2&gt; to encourage more
employers to open up childcare centres
at the respective ministries and gov-
ernment departments.
Prepositional I believe that unless we take this issue
constructions: seriously, the red squirrel is facing
NP1 IN NP2 eventual &lt;e1&gt;extinction&lt;/e1&gt; from
the &lt;e2&gt;woods&lt;/e2&gt; of Scotland.
Verbal construc- On both of my systems, the
tions: &lt;e1&gt;reboot&lt;/e1&gt; produced the omi-
NP1 VB NP2 nous &lt;e2&gt;message&lt;/e2&gt; &apos;Missing
operating system&apos;.
Verbal preposi- Manila radio station DZMM quoted
tional construc- survivors as saying that the
tions: &lt;e1&gt;fire&lt;/e1&gt; started with an
NP1 VB IN NP2 &lt;e2&gt;explosion&lt;/e2&gt; in the cargo hold
and spread across the ship within min-
utes.
</figureCaption>
<tableCaption confidence="0.7658145">
Table 3. The most frequent patterns found in the
training corpus.
</tableCaption>
<bodyText confidence="0.999365736842105">
For the pattern-dependent methods we adapted
some of our existing binary and multi-class classi-
fiers to work with the SRN relations.
For the SRN system we used only one binary
classifier built for the Part-Whole relation (relation
6) using the ISS learning algorithm and
trained/tested on the examples used in (Girju,
Badulescu, and Moldovan, 2006) and different
multi-class classifiers for the first 4 patterns from
Table 3 built using DT, SVM, SS, and NB learning
algorithms trained on a corpus annotated with 40
semantic relations (extracted from Wall Street
Journal articles from the TreeBank collection and
LATimes articles from TREC 9 collection) that
includes the 7 SRN relations (or equivalents).
(Badulescu, 2004) gives more details on this list of
relations (definitions, examples, distribution on
corpus, etc). Table 4 shows the accuracy of these
classifiers on other WSJ and LAT articles for the
</bodyText>
<table confidence="0.9985985">
Pattern cluster SS DT NB SVM ISS
Noun compounds 52.54 47.8 53.45 74.79 73.59
S-genitives 62.27 56.2 58.27 72.66 87.26
Of-genitives 67.55 53.1 54.63 72 87.26
Prepositional con- 43.48 43.3 41.92 64.52 75.97
structions
</table>
<tableCaption confidence="0.991158666666667">
Table 4. The accuracy of the SRNPAT classifiers for
the list of 40 LCC relations and the Part-Whole Rela-
tion.
</tableCaption>
<subsectionHeader confidence="0.998659">
2.3 Relation Selection
</subsectionHeader>
<bodyText confidence="0.999417333333333">
Any of the SRN or SRNPAT classifiers can return
a relation for a pair of arguments. The best relation
is selected by weighting them using the following
predefined rules:
■ The relations returned by the SRN classifiers
weight more than the ones returned by SRNPAT
classifiers because they were trained on the task
annotated examples
■ The relations returned by the binary classifiers
weight more than the ones returned by multi-class
classifiers because they focus on one relation and
therefore are more precise.
</bodyText>
<sectionHeader confidence="0.983779" genericHeader="method">
3 Experimental Results
</sectionHeader>
<subsectionHeader confidence="0.995095">
3.1 Experiments on Testing
</subsectionHeader>
<bodyText confidence="0.99781985">
During the competition we performed several
experiments to assess the correct combination of
classifiers that leads to the best results.
The organizer provided 140 examples for each
of the 7 relations. For testing the classifiers we
trained the system on the first 110 examples and
tested it on the last 30 of them.
We performed different sets of experiments.
■ Experiments with one type of classifiers.
These experiments showed that ME has a best per-
formance (55.1) 10.05 more than DT and 8.05
more than SV. ME also got the highest score for
Cause-Effect, while DT obtained the best score for
Product-Producer.
■ Experiments with multiple classifiers. These
experiments showed that DT+SV+SS+ISS has the
best score (66.72) followed by DT+SS+ISS with
55.66. Also by adding the SS and ISS classifiers
the DT score increased with 10.51, the SV score
with 5.81 and the DT+SV with 20.57.
</bodyText>
<page confidence="0.991336">
217
</page>
<bodyText confidence="0.9964151">
■ Experiments with types of methods. These
experiments showed that the SRN methods (with a
score 0.44) are better than the SRNPAT methods
(with a score of 0.41) with 0.03 which was ex-
pected since SRN were trained on provided exam-
ples.
Table 5 shows the results of our SRN system
when using specific classifiers or a combination of
classifiers. The time did not permit us to do any
experiments with the ME and NB classifiers.
</bodyText>
<table confidence="0.9976253">
Classifier Combination Average F-measure
DT 45.05
SV 47.05
ME 55.10
DT+SV 46.15
DT+SS+ISS 55.66
SV+SS+ISS 52.96
DT+SV+SS+ISS 66.72
SRN 44.31
SRNPAT 41.15
</table>
<tableCaption confidence="0.910662">
Table 5. The results of some of our experiments with
the different classifiers on the testing corpus.
</tableCaption>
<bodyText confidence="0.999935428571429">
We submitted the DT+SS+ISS version because
of its closeness to the normal distribution rather
than DT+SV+SS+ISS that had a better f-measure
but it was closer to All-True. The evaluation re-
sults showed that the testing examples we used
were representative and the DT+SV+SS+ISS pro-
duce better results.
</bodyText>
<subsectionHeader confidence="0.681383">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.99955825">
Table 6 shows the results obtained by our sys-
tem on the evaluation corpus for the B4 case (using
WordNet but not the query and all the training ex-
amples.
</bodyText>
<table confidence="0.999740222222222">
Relation Precision Recall F-measure Accuracy
1 50.8 73.2 60.0 50.0
2 54.5 31.6 40.0 53.8
3 66.7 100.0 80.0 66.7
4 80.0 22.2 34.8 63.0
5 42.2 65.5 51.4 42.3
6 39.6 80.8 53.2 48.6
7 57.1 31.6 40.7 51.4
Avg 55.9 57.8 51.4 53.7
</table>
<tableCaption confidence="0.9853105">
Table 6. The results of our system on the evaluation
corpus.
</tableCaption>
<bodyText confidence="0.923708777777778">
Table 7 shows a comparison of our results with
the following baseline systems: All-True, a system
that always returns true, Majority, a system that
always returns the majority value from the training,
and Prob-Match, a system that randomly generate
the value. We have obtained a larger precision and
accuracy than the All-True and the Prob-Match
systems. However, we obtained a lower recall and
therefore an F-measure.
</bodyText>
<table confidence="0.9960825">
System Preci- Recall F- Accuracy
sion measure
All-True 48.5 100.0 64.8 48.5
Majority 81.3 42.9 30.8 57.0
Prob-Match 48.5 48.5 48.5 51.7
LCC-SRN 55.9 57.8 51.4 53.7
</table>
<tableCaption confidence="0.999982">
Table 7. Comparison with the baselines.
</tableCaption>
<subsectionHeader confidence="0.96409">
3.3 Discussions
</subsectionHeader>
<bodyText confidence="0.99994975">
The results are promising. However, there is still
room for improvement. The system was developed
in a limited time, and therefore it could have been
benefited from more features, feature selection,
more experiments, a more complex relation selec-
tion scheme (using learning), more patterns, and
more types of machine learning algorithms (espe-
cially unsupervised ones).
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999987666666667">
We presented a system for classifying the semantic
relations between nominals that combines the re-
sults of different methods (pattern-dependent or
pattern-independent) and machine learning algo-
rithms (decision tree, support vector machines, se-
mantic scattering, maximum entropy, naïve bayes,
etc). The classifiers use lexical, semantic, and syn-
tactic features and external resources like WordNet
and an in-house Named Entity dictionary.
</bodyText>
<sectionHeader confidence="0.999276" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997608538461538">
Adriana Badulescu. 2004. Classification of Semantic
Relations between Nouns. PhD Dissertation. Univer-
sity of Texas at Dallas.
Dan Moldovan and Adriana Badulescu. 2005. A Seman-
tic Scattering Model for the Automatic Interpretation
of Genitives. In Proceedings of HLT/EMNLP 2005.
Roxana Girju, Adriana Badulescu, and Dan Moldovan.
2006. Automatic Discovery of Part-Whole Relations.
Computation Linguistics, 32:1.
Roxana Girju et al. 2007. Classification of Semantic
Relations between Nominals: Description of Task 4
in SemEval-1, In Proceedings of ACL-2007, SemE-
val-1 Workshop.
</reference>
<page confidence="0.99698">
218
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.459026">
<title confidence="0.635122">LCC-SRN: LCC’s SRN System for SemEval 2007 Task 4</title>
<author confidence="0.994645">Adriana Badulescu</author>
<affiliation confidence="0.999839">Language Computer Corporation</affiliation>
<address confidence="0.99559">1701 N Collins Blvd #2000 Richardson, TX, 75080</address>
<email confidence="0.99925">adriana@languagecomputer.com</email>
<author confidence="0.93893">Munirathnam Srikanth</author>
<affiliation confidence="0.999734">Language Computer Corporation</affiliation>
<address confidence="0.9953875">1701 N Collins Blvd #2000 Richardson, TX, 75080</address>
<email confidence="0.99964">srikanth@languagecomputer.com</email>
<abstract confidence="0.9990565">This document provides a description of the Language Computer Corporation (LCC) SRN System that participated in the SemEval 2007 Semantic Relation between Nominals task. The system combines the outputs of different binary and multi-class classifiers build using machine learning al-</abstract>
<keyword confidence="0.910622">gorithms like Decision Trees, Semantic Scattering, Iterative Semantic Specialization, and Support Vector Machines.</keyword>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Adriana Badulescu</author>
</authors>
<title>Classification of Semantic Relations between Nouns.</title>
<date>2004</date>
<tech>PhD</tech>
<institution>Dissertation. University of Texas at Dallas.</institution>
<contexts>
<context position="1239" citStr="Badulescu, 2004" startWordPosition="173" endWordPosition="174"> Semantic Scattering, Iterative Semantic Specialization, and Support Vector Machines. 1 Introduction The Semantic Relations between Nominals task from SemEval 2007 focuses on identifying the semantic relations that hold between two arguments manually annotated with word senses (Girju et al, 2007). The previous work in identifying semantic relations between nominals focuses on finding one or more relations in text for specific syntactic patterns or constructions (like genitives and noun compounds) using semi-automated and automated systems. An overview of some of these methods can be found in (Badulescu, 2004). The LCC SRN system, developed during the SRN training period, was for us, the beginning of a different approach to semantic relations detection: detecting semantic relations in text without using a syntactic pattern. Our existing work on semantic relation detection was on detecting semantic relations in text (one or more at a time) at different levels in the sentence using different syntactic patterns like genitives, noun compounds, verbarguments, etc. For SRN, we built a new system that combines the output of the pattern dependent classifiers with the new pattern-independent classifiers for</context>
<context position="9730" citStr="Badulescu, 2004" startWordPosition="1446" endWordPosition="1447">rk with the SRN relations. For the SRN system we used only one binary classifier built for the Part-Whole relation (relation 6) using the ISS learning algorithm and trained/tested on the examples used in (Girju, Badulescu, and Moldovan, 2006) and different multi-class classifiers for the first 4 patterns from Table 3 built using DT, SVM, SS, and NB learning algorithms trained on a corpus annotated with 40 semantic relations (extracted from Wall Street Journal articles from the TreeBank collection and LATimes articles from TREC 9 collection) that includes the 7 SRN relations (or equivalents). (Badulescu, 2004) gives more details on this list of relations (definitions, examples, distribution on corpus, etc). Table 4 shows the accuracy of these classifiers on other WSJ and LAT articles for the Pattern cluster SS DT NB SVM ISS Noun compounds 52.54 47.8 53.45 74.79 73.59 S-genitives 62.27 56.2 58.27 72.66 87.26 Of-genitives 67.55 53.1 54.63 72 87.26 Prepositional con- 43.48 43.3 41.92 64.52 75.97 structions Table 4. The accuracy of the SRNPAT classifiers for the list of 40 LCC relations and the Part-Whole Relation. 2.3 Relation Selection Any of the SRN or SRNPAT classifiers can return a relation for a </context>
</contexts>
<marker>Badulescu, 2004</marker>
<rawString>Adriana Badulescu. 2004. Classification of Semantic Relations between Nouns. PhD Dissertation. University of Texas at Dallas.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Moldovan</author>
<author>Adriana Badulescu</author>
</authors>
<title>A Semantic Scattering Model for the Automatic Interpretation of Genitives.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP</booktitle>
<contexts>
<context position="2617" citStr="Moldovan and Badulescu, 2005" startWordPosition="387" endWordPosition="390"> Section 4 summarizes the conclusions. 2 System description The system consists of two types of classifiers: classifiers that do not use the syntactic parsed tree and that were built specifically for the SemEval 2007 Task 4(SRN) and classifiers that use specific syntactic pattern to determine the semantic relations and there were previously developed at LCC and then adapted to the SRN task (SRNPAT). The classifiers for each type were built from annotated examples using supervised machine learning algorithms like Decision Trees (DT)1, Support Vector Machines (SVM) 2 , Semantic Scattering (SS) (Moldovan and Badulescu, 2005) , Iterative Semantic Specialization (ISS) (Girju, Badulescu, and Moldovan, 2006), Naïve Bayes (NB) 3 and Maximum Entropy (ME)4. The outputs of different classifiers (built using different types of machine learning algorithms were combined and ranked using predefined rules. Figure 1 shows the architecture of our SRN system. 1 C5.0., http://www.rulequest.com/see5-info.html 2 LIBSVM, www.csie.ntu.edu.tw/~cjlin/libsvm/ 3 jBNC, http://jbnc.sourceforge.net 4 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html 215 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-</context>
</contexts>
<marker>Moldovan, Badulescu, 2005</marker>
<rawString>Dan Moldovan and Adriana Badulescu. 2005. A Semantic Scattering Model for the Automatic Interpretation of Genitives. In Proceedings of HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
<author>Adriana Badulescu</author>
<author>Dan Moldovan</author>
</authors>
<title>Automatic Discovery of Part-Whole Relations.</title>
<date>2006</date>
<journal>Computation Linguistics,</journal>
<volume>32</volume>
<contexts>
<context position="2697" citStr="Girju, Badulescu, and Moldovan, 2006" startWordPosition="396" endWordPosition="400">nsists of two types of classifiers: classifiers that do not use the syntactic parsed tree and that were built specifically for the SemEval 2007 Task 4(SRN) and classifiers that use specific syntactic pattern to determine the semantic relations and there were previously developed at LCC and then adapted to the SRN task (SRNPAT). The classifiers for each type were built from annotated examples using supervised machine learning algorithms like Decision Trees (DT)1, Support Vector Machines (SVM) 2 , Semantic Scattering (SS) (Moldovan and Badulescu, 2005) , Iterative Semantic Specialization (ISS) (Girju, Badulescu, and Moldovan, 2006), Naïve Bayes (NB) 3 and Maximum Entropy (ME)4. The outputs of different classifiers (built using different types of machine learning algorithms were combined and ranked using predefined rules. Figure 1 shows the architecture of our SRN system. 1 C5.0., http://www.rulequest.com/see5-info.html 2 LIBSVM, www.csie.ntu.edu.tw/~cjlin/libsvm/ 3 jBNC, http://jbnc.sourceforge.net 4 http://homepages.inf.ed.ac.uk/s0450736/maxent_toolkit.html 215 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 215–218, Prague, June 2007. c�2007 Association for Computational Li</context>
<context position="9355" citStr="Girju, Badulescu, and Moldovan, 2006" startWordPosition="1386" endWordPosition="1390">bal preposi- Manila radio station DZMM quoted tional construc- survivors as saying that the tions: &lt;e1&gt;fire&lt;/e1&gt; started with an NP1 VB IN NP2 &lt;e2&gt;explosion&lt;/e2&gt; in the cargo hold and spread across the ship within minutes. Table 3. The most frequent patterns found in the training corpus. For the pattern-dependent methods we adapted some of our existing binary and multi-class classifiers to work with the SRN relations. For the SRN system we used only one binary classifier built for the Part-Whole relation (relation 6) using the ISS learning algorithm and trained/tested on the examples used in (Girju, Badulescu, and Moldovan, 2006) and different multi-class classifiers for the first 4 patterns from Table 3 built using DT, SVM, SS, and NB learning algorithms trained on a corpus annotated with 40 semantic relations (extracted from Wall Street Journal articles from the TreeBank collection and LATimes articles from TREC 9 collection) that includes the 7 SRN relations (or equivalents). (Badulescu, 2004) gives more details on this list of relations (definitions, examples, distribution on corpus, etc). Table 4 shows the accuracy of these classifiers on other WSJ and LAT articles for the Pattern cluster SS DT NB SVM ISS Noun c</context>
</contexts>
<marker>Girju, Badulescu, Moldovan, 2006</marker>
<rawString>Roxana Girju, Adriana Badulescu, and Dan Moldovan. 2006. Automatic Discovery of Part-Whole Relations. Computation Linguistics, 32:1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roxana Girju</author>
</authors>
<date>2007</date>
<booktitle>Classification of Semantic Relations between Nominals: Description of Task 4 in SemEval-1, In Proceedings of ACL-2007, SemEval-1 Workshop.</booktitle>
<marker>Girju, 2007</marker>
<rawString>Roxana Girju et al. 2007. Classification of Semantic Relations between Nominals: Description of Task 4 in SemEval-1, In Proceedings of ACL-2007, SemEval-1 Workshop.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>