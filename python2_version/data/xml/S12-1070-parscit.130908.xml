<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.021247">
<title confidence="0.9984945">
Duluth : Measuring Degrees of Relational Similarity
with the Gloss Vector Measure of Semantic Relatedness
</title>
<author confidence="0.999036">
Ted Pedersen
</author>
<affiliation confidence="0.878264333333333">
Department of Computer Science
University of Minnesota
Duluth, MN 55812 USA
</affiliation>
<email confidence="0.99833">
tpederse@d.umn.edu
</email>
<sectionHeader confidence="0.995632" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9998417">
This paper describes the Duluth systems that
participated in Task 2 of SemEval–2012.
These systems were unsupervised and relied
on variations of the Gloss Vector measure
found in the freely available software pack-
age WordNet::Similarity. This method was
moderately successful for the Class-Inclusion,
Similar, Contrast, and Non-Attribute cate-
gories of semantic relations, but mimicked a
random baseline for the other six categories.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999966962962964">
This paper describes the Duluth systems that par-
ticipated in Task 2 of SemEval–2012, Measuring
the Degree of Relational Similarity (Jurgens et al.,
2012). The goal of the task was to rank sets of
word pairs according to the degree to which they
represented an underlying category of semantic re-
lation. A highly ranked pair would be considered
a good or prototypical example of the relation. For
example, given the relation Y functions as an X the
pair weapon:knife (X:Y) would likely be considered
more representative of that relation than would be
tool:spoon.
The task included word pairs from 10 different
categories of relational similarity, each with a num-
ber of subcategories. In total the evaluation data
consisted of 69 files, each containing a set of ap-
proximately 40 word pairs. While training examples
were also provided, these were not used by the Du-
luth systems. The system–generated rankings were
compared with gold standard data created via Ama-
zon Mechanical Turk.
The Duluth systems relied on the Gloss Vec-
tor measure of semantic relatedness (Patwardhan
and Pedersen, 2006) as implemented in Word-
Net::Similarity (Pedersen et al., 2004)1. This quanti-
fies the degree of semantic relatedness between two
word senses. It does not, however, discover or in-
dicate the nature of the relation between the words.
When given two words as input (as was the case in
this task), it measures the relatedness of all possi-
ble combinations of word senses associated with this
pair and reports the highest resulting score. Note
that throughout this paper we use word and word
sense somewhat interchangeably. In general it may
be assumed that the term word or examples of words
refers to a word sense.
A key characteristic of this task was that the word
pairs in each of the 69 sets were scored assuming
a particular specified underlying semantic relation.
Given this, the limitation that the Gloss Vector mea-
sure does not discover the nature of relations was
less of a concern, and led to the hypothesis that a
word pair that was highly related would also be a
prototypical example of the underlying category of
semantic relation. Unfortunately the results from
this task do not generally support this hypothesis,
although for a few categories at least it appears to
have some validity.
This paper continues with a review of the Gloss
Vector measure, and explains its connections to the
Adapted Lesk measure. The paper then summarizes
the results of the three Duluth systems in this task,
and concludes with some discussion and analysis of
where this method had both successes and failures.
</bodyText>
<footnote confidence="0.426741">
1wn-similarity.sourceforge.net
</footnote>
<page confidence="0.954573">
497
</page>
<note confidence="0.7171055">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 497–501,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.96924" genericHeader="method">
2 Semantic Relatedness
</sectionHeader>
<bodyText confidence="0.9999735">
Semantic relatedness is a more general notion than
semantic similarity. We follow (Budanitsky and
Hirst, 2006) and limit semantic similarity to those
measures based on distances and perhaps depths in
a hierarchy made up of is–a relations. For exam-
ple, car and motorcycle are similar in that they are
connected via an is–a relation with vehicle. Seman-
tic similarity is most often applied to nouns, but can
also be used with verbs.
Two word senses can be related in many ways,
including similarity. car and furnace might be con-
sidered related because they are both made of steel,
and firefighter and hose might be considered related
because one uses the other, but neither pair is likely
to be considered similar. Measures of relatedness
generally do not specify the nature of the relation-
ship between two word senses, but rather indicate
that they are related to a certain degree in some un-
specified way. As a result, measures of relatedness
tend to be symmetric, so A is related to B to the same
degree that B is related to A. It should be noted that
some of the relations in Task 2 were not symmetric,
which was no doubt a complicating factor for the
Duluth systems.
</bodyText>
<sectionHeader confidence="0.991065" genericHeader="method">
3 Adapted Lesk Measure
</sectionHeader>
<bodyText confidence="0.993578885714286">
The Gloss Vector measure was originally devel-
oped in an effort to generalize and improve upon
the Adapted Lesk measure (Banerjee and Pedersen,
2003).2 Both the Gloss Vector measure and the
Adapted Lesk measure start with the idea of a su-
pergloss. A supergloss is the definition (or gloss) of
a word sense that is expanded by concatenating it
with the glosses of other surrounding senses that are
connected to it via some WordNet relation. For ex-
ample, a supergloss for car might consist of the def-
inition of car, the definition of car’s hypernym (e.g.,
vehicle), and the definitions of the meronyms (part-
of) of car (e.g., wheel, brake, bumper, etc.) Other
relations as detailed later in this paper may also be
used to expand a supergloss.
In the Adapted Lesk measure, the relatedness be-
tween two word senses is a function of the number
and length of their matching overlaps in their super-
glosses. Consecutive words that match are scored
2WordNet::Similarity::lesk
more highly than single words, and a higher score
for a pair of words indicates a stronger relation. The
Adapted Lesk measure was developed to overcome
the fact that most dictionary definitions are rela-
tively short, which was a concern noted by (Lesk,
1986) when he introduced the idea of using defini-
tion overlaps for word sense disambiguation. While
the Adapted Lesk measure expands the size of the
definitions, there are still difficulties. In particular,
the matches between words in superglosses must be
exact, so morphological variants (run versus ran),
synonyms (gas versus petrol), and closely related
words (tree versus shrub) won’t be considered over-
laps and will be treated the same as words with no
apparent connection (e.g., goat and vase).
</bodyText>
<sectionHeader confidence="0.982729" genericHeader="method">
4 Gloss Vector Measure
</sectionHeader>
<bodyText confidence="0.999922482758621">
The Gloss Vector measure3 is inspired by a 2nd or-
der word sense discrimination approach (Sch¨utze,
1998) which is in turn related to Latent Semantic
Indexing or Analysis (Deerwester et al., 1990). The
basic idea is to replace each word in a written con-
text with a vector of co-occurring words as observed
in some corpus. In this task, the contexts are def-
initions (and example text) from WordNet. A su-
pergloss is formed exactly as described for Adapted
Lesk, and then each word in the supergloss is re-
placed by a vector of co–occurring words. Then, all
the vectors in the supergloss are averaged together to
create a new high dimensional representation of that
word sense. The semantic relatedness between two
word senses is measured by taking the cosine be-
tween their two averaged vectors. The end result is
that rather than finding overlaps in definitions based
on exact matches, a word in a definition is matched
to whatever degree its co-occurrences match with
the co-occurrences of the words in the other super-
gloss. This results in a more subtle and fine grained
measure of relatedness than Adapted Lesk.
The three Duluth systems only differ in the re-
lations used to create the superglosses, otherwise
they are identical. The corpus used to collect co-
occurrence information was the complete collection
of glosses and examples from WordNet 3.0, which
consists of about 1.46 million word tokens and al-
most 118,000 glosses. Words that appeared in a
</bodyText>
<footnote confidence="0.549548">
3WordNet::Similarity::vector
</footnote>
<page confidence="0.994759">
498
</page>
<bodyText confidence="0.999876857142857">
stop list of about 200 common words were excluded
as co-occurrences, as were words that occurred less
than 5 times or more than 50 times in the WordNet
corpus. Two words are considered to co-occur if
they occur in the same definition (including the ex-
ample) and are adjacent to each other. These are the
default settings as used in WordNet::Similarity.
</bodyText>
<sectionHeader confidence="0.965616" genericHeader="method">
5 Creating the Duluth Systems
</sectionHeader>
<bodyText confidence="0.999989666666667">
There were three Duluth systems, V0, V1, and V2.
These all used the Gloss Vector measure, and differ
only in how their superglosses were created. The su-
pergloss is defined using a set of relations that indi-
cate which additional definitions should be included
in the definition for a sense. All systems start with
a gloss and example for each sense in a pair, which
is then augmented with definitions from additional
senses as defined for each system.
</bodyText>
<subsectionHeader confidence="0.971522">
5.1 Duluth-V0
</subsectionHeader>
<bodyText confidence="0.997968583333333">
V0 is identical to the default configuration of the
Gloss Vector measure in WordNet::Similarity. This
consists of the following relations:
hypernym (hype) : class that includes a member,
e.g., a car is a kind of vehicle (hypernym).
hyponym (hypo) : the member of a class, e.g., a
car (hyponym) is a kind of vehicle.
holonym (holo) : whole that includes the part,
e.g., a ship (holonym) includes a mast.
meronym (mero) : part included in a whole, e.g.,
a mast (meronym) is a part of a ship.
see also (also) : related adjectives, e.g., egocentric
see also selfish.
similar to (sim) : similar adjectives, satanic is
similar to evil.
is attribute of (attr) : adjective related to a noun,
e.g., measurable is an attribute of magnitude.
synset words (syns) : synonyms of a word, e.g.,
car and auto are synonyms.4
For V0 the definition and example of a noun
is augmented with its synonyms and the defini-
tions and examples of any hypernyms, hyponyms,
meronyms, and holonyms to which it is directly con-
nected. If the word is a verb it is augmented with
</bodyText>
<footnote confidence="0.5286675">
4Since synonyms have the same definition, this relation aug-
ments the supergloss with the synonyms themselves.
</footnote>
<bodyText confidence="0.999765">
its synonyms and any hypernyms/troponyms and hy-
ponyms to which it is directly connected. If the
word is an adjective then its definition and exam-
ple are augmented with those of adjectives directly
connected via see also, similar to, and is attribute of
relations.
</bodyText>
<subsectionHeader confidence="0.998243">
5.2 Duluth-V1
</subsectionHeader>
<bodyText confidence="0.999051444444444">
V1 uses the relations in V0, plus the holonyms, hy-
pernyms, hyponyms, and meronyms (X) of the see
also, holonym, hypernym, hyponym, and meronym
relations (Y). This leads to an additional 20 relations
that bring in definitions “2 steps” away from the
original word. These take the form of the holonym
of the hypernym of the word sense, or more gener-
ally the X of the Y of the word sense, where X and Y
are as noted above.
</bodyText>
<subsectionHeader confidence="0.987104">
5.3 Duluth-V2
</subsectionHeader>
<bodyText confidence="0.999788">
V2 uses the relations in V0 and V1, and then adds
the holonym, hypernyms, hyponyms, and meronyms
of the 20 relations added for V1. This leads to an
additional 80 relations of the form the hypernyms of
the meronym of the hyponym, or more generally the
X of the X of the Y of the word.
For example, if the word is weapon, then a hyper-
nym of the meronym of the hyponym (of weapon)
would add the definitions and example of bow (hy-
ponym), bowstring (meronym of the hyponym), and
cord (hypernym of the meronym of the hyponym) to
the gloss of weapon to create the supergloss.
</bodyText>
<sectionHeader confidence="0.999872" genericHeader="evaluation">
6 Results
</sectionHeader>
<bodyText confidence="0.999805444444444">
There were two evaluation scores reported for the
participating systems, Spearman’s Rank Correlation
Coefficient, and a score based on Maximum Differ-
ence Scaling. Since the Gloss Vector measure is
based on WordNet, there was a concern that a lack
of WordNet coverage might negatively impact the
results. However, of the 2,791 pairs used in the eval-
uation, there were only 3 that contained words un-
known to WordNet.
</bodyText>
<subsectionHeader confidence="0.999403">
6.1 Spearman’s Rank Correlation
</subsectionHeader>
<bodyText confidence="0.99883375">
The ranking of word pairs in each of the 69 files
were evaluated relative to the gold standard using
Spearman’s Rank Correlation Coefficient. The av-
erage of these results over all 10 categories of se-
</bodyText>
<page confidence="0.999577">
499
</page>
<tableCaption confidence="0.999848">
Table 1: Selected Spearman’s Values
Table 2: Selected MaxDiff Values
</tableCaption>
<table confidence="0.997944833333333">
Category rand v0 v1 v2
SIMILAR 31.5 37.1 39.2 37.4
CLASS-INCLUSION 31.0 29.2 35.6 33.1
CONTRAST 30.4 38.3 36.0 33.8
NON-ATTRIBUTE 28.9 36.0 33.0 33.5
average (of all 10) 31.2 32.4 31.5 31.1
</table>
<bodyText confidence="0.991332071428571">
mantic relations was quite low. Random guessing
achieved an averaged Spearman’s value 0.018, while
Duluth-V0 scored 0.050, Duluth-V1 scored 0.039,
and Duluth-V2 scored 0.038.
However, there were specific categories where the
Duluth systems fared somewhat better. In particular,
results for category 1 (CLASS-INCLUSION), cate-
gory 3 (SIMILAR) and category 4 (CONTRAST)
represent improvements on the random baseline
(shown in Table 1) and at least some modest agree-
ment with the gold standard.
The results from the other categories were gener-
ally equivalent to what would be obtained with ran-
dom selection.
</bodyText>
<subsectionHeader confidence="0.999584">
6.2 Maximum Difference Scaling
</subsectionHeader>
<bodyText confidence="0.999768461538462">
Maximum Difference Scaling is based on identify-
ing the least and most prototypical pair for a given
relation from among a set of four pairs. A ran-
dom baseline scores 31.2%, meaning that it got ap-
proximately 1 in 3 of the MaxDiff questions correct.
None of the Duluth systems improved upon random
to any significant degree : Duluth-V0 scored 32.4,
Duluth-V1 scored 31.5, and Duluth-V2 scored 31.1.
However, the same categories that did well with
Spearman’s also did well with MaxDiff (see Table
2). In addition, there is some improvement in cat-
egory 6 (NON-ATTRIBUTE) at least with MaxDiff
scoring.
</bodyText>
<sectionHeader confidence="0.981225" genericHeader="conclusions">
7 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999811069767442">
The Gloss Vector measure was able to perform rea-
sonably well in measuring the degree of relatedness
for the following four categories (where the defini-
tions come from (Bejar et al., 1991)):
CLASS-INCLUSION : one word names a class
that includes the entity named by the other word
SIMILAR : one word represents a different de-
gree or form of the ... other
CONTRAST : one word names an opposite or
incompatible of the other word
NON-ATTRIBUTE : one word names a quality,
property or action that is characteristically not an at-
tribute of the other word
Of these, CLASS-INCLUSION and SIMILAR
are well represented by the hypernym/hyponym re-
lations present in WordNet and used by the Gloss
Vector measure. WordNet’s greatest strength lies
in its hypernym tree for nouns, and that was most
likely the basis for the success of the CLASS-
INCLUSION and SIMILAR categories. While the
success with CONTRAST may seem unrelated, in
fact it may be that pairs of opposites are often quite
similar, for example happy and sad are both emo-
tions and are similar except for their polarity.
A number of the relations used in Task 2 are
not well represented in WordNet. For example,
there was a CASE RELATION which could ben-
efit from information about selectional restrictions
or case frames that just isn’t available in WordNet.
The same is true of the CAUSE-PURPOSE relation
as there is relatively little information about casual
relations in WordNet. While there are part-of rela-
tions in WordNet (meronyms/holonyms), these did
not prove to be common enough to be a significant
benefit for the PART-WHOLE relations in the task.
For many of the relations in the task the Gloss
Vector measure was most likely relying primarily on
hypernym and hyponym relations, which explains
the bias towards categories that featured similarity-
based relations. We are however optimistic that
a Gloss Vector approach could be more successful
given a richer set of relations from which to draw
information for superglosses.
</bodyText>
<table confidence="0.8583952">
Category rand v0 v1 v2
SIMILAR .026 .183 .206 .198
CLASS-INCLUSION .057 .045 .178 .168
CONTRAST -.049 .142 .120 .198
average (of all 10) .018 .050 .039 .038
</table>
<page confidence="0.986831">
500
</page>
<sectionHeader confidence="0.995349" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999657605263158">
S. Banerjee and T. Pedersen. 2003. Extended gloss over-
laps as a measure of semantic relatedness. In Proceed-
ings of the Eighteenth International Joint Conference
on Artificial Intelligence, pages 805–810, Acapulco,
August.
I. Bejar, R. Chaffin, and S. Embretson. 1991. Cogni-
tive and Psychometric Analysis ofAnalogical Problem
Solving. Springer–Verlag, New York, NY.
A. Budanitsky and G. Hirst. 2006. Evaluating WordNet-
based measures of semantic distance. Computational
Linguistics, 32(1):13–47.
S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer,
and R. Harshman. 1990. Indexing by Latent Semantic
Analysis. Journal of the American Society for Infor-
mation Science, 41:391–407.
D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak.
2012. Semeval-2012 task 2: Measuring degrees of
relational similarity. In Proceedings of the 6th Inter-
national Workshop on Semantic Evaluation (SemEval
2012), Montreal, June.
M.E. Lesk. 1986. Automatic sense disambiguation us-
ing machine readable dictionaries: how to tell a pine
cone from an ice cream cone. In Proceedings of the
5th annual international conference on Systems docu-
mentation, pages 24–26. ACM Press.
S. Patwardhan and T. Pedersen. 2006. Using WordNet-
based Context Vectors to Estimate the Semantic Relat-
edness of Concepts. In Proceedings of the EACL 2006
Workshop on Making Sense of Sense: Bringing Com-
putational Linguistics and Psycholinguistics Together,
pages 1–8, Trento, Italy, April.
T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004.
Wordnet::Similarity - Measuring the relatedness of
concepts. In Proceedings of the Nineteenth National
Conference on Artificial Intelligence, pages 1024–
1025, San Jose.
H. Sch¨utze. 1998. Automatic word sense discrimination.
Computational Linguistics, 24(1):97–123.
</reference>
<page confidence="0.997734">
501
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.902855">
<title confidence="0.9689705">Duluth : Measuring Degrees of Relational with the Gloss Vector Measure of Semantic Relatedness</title>
<author confidence="0.991931">Ted</author>
<affiliation confidence="0.99987">Department of Computer University of</affiliation>
<address confidence="0.976644">Duluth, MN 55812</address>
<email confidence="0.999762">tpederse@d.umn.edu</email>
<abstract confidence="0.999016">This paper describes the Duluth systems that participated in Task 2 of SemEval–2012. These systems were unsupervised and relied on variations of the Gloss Vector measure found in the freely available software package WordNet::Similarity. This method was moderately successful for the Class-Inclusion, Similar, Contrast, and Non-Attribute categories of semantic relations, but mimicked a random baseline for the other six categories.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Banerjee</author>
<author>T Pedersen</author>
</authors>
<title>Extended gloss overlaps as a measure of semantic relatedness.</title>
<date>2003</date>
<booktitle>In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>805--810</pages>
<location>Acapulco,</location>
<contexts>
<context position="4824" citStr="Banerjee and Pedersen, 2003" startWordPosition="780" endWordPosition="783"> Measures of relatedness generally do not specify the nature of the relationship between two word senses, but rather indicate that they are related to a certain degree in some unspecified way. As a result, measures of relatedness tend to be symmetric, so A is related to B to the same degree that B is related to A. It should be noted that some of the relations in Task 2 were not symmetric, which was no doubt a complicating factor for the Duluth systems. 3 Adapted Lesk Measure The Gloss Vector measure was originally developed in an effort to generalize and improve upon the Adapted Lesk measure (Banerjee and Pedersen, 2003).2 Both the Gloss Vector measure and the Adapted Lesk measure start with the idea of a supergloss. A supergloss is the definition (or gloss) of a word sense that is expanded by concatenating it with the glosses of other surrounding senses that are connected to it via some WordNet relation. For example, a supergloss for car might consist of the definition of car, the definition of car’s hypernym (e.g., vehicle), and the definitions of the meronyms (partof) of car (e.g., wheel, brake, bumper, etc.) Other relations as detailed later in this paper may also be used to expand a supergloss. In the Ad</context>
</contexts>
<marker>Banerjee, Pedersen, 2003</marker>
<rawString>S. Banerjee and T. Pedersen. 2003. Extended gloss overlaps as a measure of semantic relatedness. In Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence, pages 805–810, Acapulco, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>I Bejar</author>
<author>R Chaffin</author>
<author>S Embretson</author>
</authors>
<title>Cognitive and Psychometric Analysis ofAnalogical Problem Solving.</title>
<date>1991</date>
<publisher>Springer–Verlag,</publisher>
<location>New York, NY.</location>
<contexts>
<context position="13553" citStr="Bejar et al., 1991" startWordPosition="2259" endWordPosition="2262">proximately 1 in 3 of the MaxDiff questions correct. None of the Duluth systems improved upon random to any significant degree : Duluth-V0 scored 32.4, Duluth-V1 scored 31.5, and Duluth-V2 scored 31.1. However, the same categories that did well with Spearman’s also did well with MaxDiff (see Table 2). In addition, there is some improvement in category 6 (NON-ATTRIBUTE) at least with MaxDiff scoring. 7 Discussion and Conclusions The Gloss Vector measure was able to perform reasonably well in measuring the degree of relatedness for the following four categories (where the definitions come from (Bejar et al., 1991)): CLASS-INCLUSION : one word names a class that includes the entity named by the other word SIMILAR : one word represents a different degree or form of the ... other CONTRAST : one word names an opposite or incompatible of the other word NON-ATTRIBUTE : one word names a quality, property or action that is characteristically not an attribute of the other word Of these, CLASS-INCLUSION and SIMILAR are well represented by the hypernym/hyponym relations present in WordNet and used by the Gloss Vector measure. WordNet’s greatest strength lies in its hypernym tree for nouns, and that was most likel</context>
</contexts>
<marker>Bejar, Chaffin, Embretson, 1991</marker>
<rawString>I. Bejar, R. Chaffin, and S. Embretson. 1991. Cognitive and Psychometric Analysis ofAnalogical Problem Solving. Springer–Verlag, New York, NY.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Budanitsky</author>
<author>G Hirst</author>
</authors>
<title>Evaluating WordNetbased measures of semantic distance.</title>
<date>2006</date>
<journal>Computational Linguistics,</journal>
<volume>32</volume>
<issue>1</issue>
<contexts>
<context position="3595" citStr="Budanitsky and Hirst, 2006" startWordPosition="563" endWordPosition="566">tinues with a review of the Gloss Vector measure, and explains its connections to the Adapted Lesk measure. The paper then summarizes the results of the three Duluth systems in this task, and concludes with some discussion and analysis of where this method had both successes and failures. 1wn-similarity.sourceforge.net 497 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 497–501, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics 2 Semantic Relatedness Semantic relatedness is a more general notion than semantic similarity. We follow (Budanitsky and Hirst, 2006) and limit semantic similarity to those measures based on distances and perhaps depths in a hierarchy made up of is–a relations. For example, car and motorcycle are similar in that they are connected via an is–a relation with vehicle. Semantic similarity is most often applied to nouns, but can also be used with verbs. Two word senses can be related in many ways, including similarity. car and furnace might be considered related because they are both made of steel, and firefighter and hose might be considered related because one uses the other, but neither pair is likely to be considered similar</context>
</contexts>
<marker>Budanitsky, Hirst, 2006</marker>
<rawString>A. Budanitsky and G. Hirst. 2006. Evaluating WordNetbased measures of semantic distance. Computational Linguistics, 32(1):13–47.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Deerwester</author>
<author>S T Dumais</author>
<author>G W Furnas</author>
<author>T K Landauer</author>
<author>R Harshman</author>
</authors>
<title>Indexing by Latent Semantic Analysis.</title>
<date>1990</date>
<journal>Journal of the American Society for Information Science,</journal>
<pages>41--391</pages>
<contexts>
<context position="6612" citStr="Deerwester et al., 1990" startWordPosition="1077" endWordPosition="1080">ile the Adapted Lesk measure expands the size of the definitions, there are still difficulties. In particular, the matches between words in superglosses must be exact, so morphological variants (run versus ran), synonyms (gas versus petrol), and closely related words (tree versus shrub) won’t be considered overlaps and will be treated the same as words with no apparent connection (e.g., goat and vase). 4 Gloss Vector Measure The Gloss Vector measure3 is inspired by a 2nd order word sense discrimination approach (Sch¨utze, 1998) which is in turn related to Latent Semantic Indexing or Analysis (Deerwester et al., 1990). The basic idea is to replace each word in a written context with a vector of co-occurring words as observed in some corpus. In this task, the contexts are definitions (and example text) from WordNet. A supergloss is formed exactly as described for Adapted Lesk, and then each word in the supergloss is replaced by a vector of co–occurring words. Then, all the vectors in the supergloss are averaged together to create a new high dimensional representation of that word sense. The semantic relatedness between two word senses is measured by taking the cosine between their two averaged vectors. The </context>
</contexts>
<marker>Deerwester, Dumais, Furnas, Landauer, Harshman, 1990</marker>
<rawString>S. Deerwester, S.T. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. 1990. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41:391–407.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Jurgens</author>
<author>S Mohammad</author>
<author>P Turney</author>
<author>K Holyoak</author>
</authors>
<title>Semeval-2012 task 2: Measuring degrees of relational similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Montreal,</location>
<contexts>
<context position="824" citStr="Jurgens et al., 2012" startWordPosition="114" endWordPosition="117">@d.umn.edu Abstract This paper describes the Duluth systems that participated in Task 2 of SemEval–2012. These systems were unsupervised and relied on variations of the Gloss Vector measure found in the freely available software package WordNet::Similarity. This method was moderately successful for the Class-Inclusion, Similar, Contrast, and Non-Attribute categories of semantic relations, but mimicked a random baseline for the other six categories. 1 Introduction This paper describes the Duluth systems that participated in Task 2 of SemEval–2012, Measuring the Degree of Relational Similarity (Jurgens et al., 2012). The goal of the task was to rank sets of word pairs according to the degree to which they represented an underlying category of semantic relation. A highly ranked pair would be considered a good or prototypical example of the relation. For example, given the relation Y functions as an X the pair weapon:knife (X:Y) would likely be considered more representative of that relation than would be tool:spoon. The task included word pairs from 10 different categories of relational similarity, each with a number of subcategories. In total the evaluation data consisted of 69 files, each containing a s</context>
</contexts>
<marker>Jurgens, Mohammad, Turney, Holyoak, 2012</marker>
<rawString>D. Jurgens, S. Mohammad, P. Turney, and K. Holyoak. 2012. Semeval-2012 task 2: Measuring degrees of relational similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation (SemEval 2012), Montreal, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M E Lesk</author>
</authors>
<title>Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th annual international conference on Systems documentation,</booktitle>
<pages>24--26</pages>
<publisher>ACM Press.</publisher>
<contexts>
<context position="5897" citStr="Lesk, 1986" startWordPosition="965" endWordPosition="966"> car (e.g., wheel, brake, bumper, etc.) Other relations as detailed later in this paper may also be used to expand a supergloss. In the Adapted Lesk measure, the relatedness between two word senses is a function of the number and length of their matching overlaps in their superglosses. Consecutive words that match are scored 2WordNet::Similarity::lesk more highly than single words, and a higher score for a pair of words indicates a stronger relation. The Adapted Lesk measure was developed to overcome the fact that most dictionary definitions are relatively short, which was a concern noted by (Lesk, 1986) when he introduced the idea of using definition overlaps for word sense disambiguation. While the Adapted Lesk measure expands the size of the definitions, there are still difficulties. In particular, the matches between words in superglosses must be exact, so morphological variants (run versus ran), synonyms (gas versus petrol), and closely related words (tree versus shrub) won’t be considered overlaps and will be treated the same as words with no apparent connection (e.g., goat and vase). 4 Gloss Vector Measure The Gloss Vector measure3 is inspired by a 2nd order word sense discrimination a</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>M.E. Lesk. 1986. Automatic sense disambiguation using machine readable dictionaries: how to tell a pine cone from an ice cream cone. In Proceedings of the 5th annual international conference on Systems documentation, pages 24–26. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Patwardhan</author>
<author>T Pedersen</author>
</authors>
<title>Using WordNetbased Context Vectors to Estimate the Semantic Relatedness of Concepts.</title>
<date>2006</date>
<booktitle>In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together,</booktitle>
<pages>1--8</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="1759" citStr="Patwardhan and Pedersen, 2006" startWordPosition="268" endWordPosition="271">:knife (X:Y) would likely be considered more representative of that relation than would be tool:spoon. The task included word pairs from 10 different categories of relational similarity, each with a number of subcategories. In total the evaluation data consisted of 69 files, each containing a set of approximately 40 word pairs. While training examples were also provided, these were not used by the Duluth systems. The system–generated rankings were compared with gold standard data created via Amazon Mechanical Turk. The Duluth systems relied on the Gloss Vector measure of semantic relatedness (Patwardhan and Pedersen, 2006) as implemented in WordNet::Similarity (Pedersen et al., 2004)1. This quantifies the degree of semantic relatedness between two word senses. It does not, however, discover or indicate the nature of the relation between the words. When given two words as input (as was the case in this task), it measures the relatedness of all possible combinations of word senses associated with this pair and reports the highest resulting score. Note that throughout this paper we use word and word sense somewhat interchangeably. In general it may be assumed that the term word or examples of words refers to a wor</context>
</contexts>
<marker>Patwardhan, Pedersen, 2006</marker>
<rawString>S. Patwardhan and T. Pedersen. 2006. Using WordNetbased Context Vectors to Estimate the Semantic Relatedness of Concepts. In Proceedings of the EACL 2006 Workshop on Making Sense of Sense: Bringing Computational Linguistics and Psycholinguistics Together, pages 1–8, Trento, Italy, April.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>S Patwardhan</author>
<author>J Michelizzi</author>
</authors>
<title>Wordnet::Similarity - Measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Proceedings of the Nineteenth National Conference on Artificial Intelligence,</booktitle>
<pages>1024--1025</pages>
<location>San Jose.</location>
<contexts>
<context position="1821" citStr="Pedersen et al., 2004" startWordPosition="277" endWordPosition="280">lation than would be tool:spoon. The task included word pairs from 10 different categories of relational similarity, each with a number of subcategories. In total the evaluation data consisted of 69 files, each containing a set of approximately 40 word pairs. While training examples were also provided, these were not used by the Duluth systems. The system–generated rankings were compared with gold standard data created via Amazon Mechanical Turk. The Duluth systems relied on the Gloss Vector measure of semantic relatedness (Patwardhan and Pedersen, 2006) as implemented in WordNet::Similarity (Pedersen et al., 2004)1. This quantifies the degree of semantic relatedness between two word senses. It does not, however, discover or indicate the nature of the relation between the words. When given two words as input (as was the case in this task), it measures the relatedness of all possible combinations of word senses associated with this pair and reports the highest resulting score. Note that throughout this paper we use word and word sense somewhat interchangeably. In general it may be assumed that the term word or examples of words refers to a word sense. A key characteristic of this task was that the word p</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>T. Pedersen, S. Patwardhan, and J. Michelizzi. 2004. Wordnet::Similarity - Measuring the relatedness of concepts. In Proceedings of the Nineteenth National Conference on Artificial Intelligence, pages 1024– 1025, San Jose.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Sch¨utze</author>
</authors>
<title>Automatic word sense discrimination.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>1</issue>
<marker>Sch¨utze, 1998</marker>
<rawString>H. Sch¨utze. 1998. Automatic word sense discrimination. Computational Linguistics, 24(1):97–123.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>