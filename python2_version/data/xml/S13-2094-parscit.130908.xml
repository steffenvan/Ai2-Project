<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000023">
<title confidence="0.941876">
nlp.cs.aueb.gr: Two Stage Sentiment Analysis
</title>
<author confidence="0.95974">
Prodromos Malakasiotis, Rafael Michael Karampatsis
Konstantina Makrynioti and John Pavlopoulos
</author>
<affiliation confidence="0.9977575">
Department of Informatics
Athens University of Economics and Business
</affiliation>
<address confidence="0.616906">
Patission 76, GR-104 34 Athens, Greece
</address>
<sectionHeader confidence="0.973564" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9997488">
This paper describes the systems with which
we participated in the task Sentiment Analysis
in Twitter of SEMEVAL 2013 and specifically
the Message Polarity Classification. We used
a 2-stage pipeline approach employing a lin-
ear SVM classifier at each stage and several
features including BOW features, POS based
features and lexicon based features. We have
also experimented with Naive Bayes classi-
fiers trained with BOW features.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.938546190476191">
During the last years, Twitter has become a very
popular microblogging service. Millions of users
publish messages every day, often expressing their
feelings or opinion about a variety of events, top-
ics, products, etc. Analysing this kind of content
has drawn the attention of many companies and re-
searchers, as it can lead to useful information for
fields, such as personalized marketing or social pro-
filing. The informal language, the spelling mis-
takes, the slang and special abbreviations that are
frequently used in tweets differentiate them from
traditional texts, such as articles or reviews, and
present new challenges for the task of sentiment
analysis.
The Message Polarity Classification is defined as
the task of deciding whether a message M conveys a
positive, negative or neutral sentiment. For instance
M1 below expresses a positive sentiment, M2 a neg-
ative one, while M3 has no sentiment at all.
M1: GREAT GAME GIRLS!! On to districts Monday
at Fox!! Thanks to the fans for coming out :)
M2: Firework just came on my tv and I just broke down
and sat and cried, I need help okay
M3: Going to a bulls game with Aaliyah &amp; hope next
Thursday
As sentiment analysis in Twitter is a very recent
subject, it is certain that more research and improve-
ments are needed. This paper presents our approach
for the subtask of Message Polarity Classification
(Wilson et al., 2013) of SEMEVAL 2013. We used a
2-stage pipeline approach employing a linear SVM
classifier at each stage and several features includ-
ing bag of words (BOW) features, part-of-speech
(POS) based features and lexicon based features.
We have also experimented with Naive Bayes clas-
sifiers trained with BOW features.
The rest of the paper is organised as follows. Sec-
tion 2 provides a short analysis of the data used
while section 3 describes our approach. Section 4
describes the experiments we performed and the cor-
responding results and section 5 concludes and gives
hints for future work.
</bodyText>
<sectionHeader confidence="0.995631" genericHeader="introduction">
2 Data
</sectionHeader>
<bodyText confidence="0.9999365">
Before we proceed with our system description we
briefly describe the data released by the organisers.
The training set consists of a set of IDs correspond-
ing to tweet messages, along with their annotations.
A message can be annotated as positive, negative
or neutral. In order to address privacy concerns,
rather than releasing the original Tweets, the organ-
isers chose to provide a python script for download-
ing the data. This resulted to different training sets
for the participants since tweets may often become
</bodyText>
<page confidence="0.948014">
562
</page>
<bodyText confidence="0.507281">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic
Evaluation (SemEval 2013), pages 562–567, Atlanta, Georgia, June 14-15, 2013. c�2013 Association for Computational Linguistics
</bodyText>
<figure confidence="0.99583824137931">
Developent data clas distribution
Development data class distributio
4,68%
44,68%
20,56%
20,56
34,76%
34,76
Positve
Positiv
Negative
Negativ
Neutral
Neutra
rii ata clas distribution
Training data class distribution
47,6
47,66%
14,7%
4,77%
37,57%
7,57%
Positive
ositive
Negative
Negative
Neutral
Neutral
(a) (b)
</figure>
<figureCaption confidence="0.999851">
Figure 1: Train and Development data class distribution.
</figureCaption>
<figure confidence="0.729976833333333">
22,
unavailable due to a number of reasons. Concerning
Positive
Positive
the development and test sets the organisers down-
Negative
Negative
5769%
69
loaded and provided the tweets. 1 A first analysis
Neutral
Neutral
</figure>
<bodyText confidence="0.999818571428571">
of the data indicates that they suffer from a class im-
balance problem. Specifically the training data we
have downloaded contain 8730 tweets (3280 posi-
tive, 1289 negative, 4161 neutral), while the devel-
opment set contains 1654 tweets (575 positive, 340
negative, 739 neutral). Figure 1 illustrates the prob-
lem on train and development sets.
</bodyText>
<sectionHeader confidence="0.979853" genericHeader="method">
3 System Overview
</sectionHeader>
<bodyText confidence="0.863309333333333">
The system we propose is a 2–stage pipeline pro-
cedure employing SVM classifiers (Vapnik, 1998)
to detect whether each message M expresses pos-
itive, negative or no sentiment (figure 2). Specifi-
cally, during the first stage we attempt to detect if M
expresses a sentiment (positive or negative) or not.
If so, M is called “subjective”, otherwise it is called
“objective” or “neutral”.2 Each subjective message
is then classified in a second stage as “positive” or
“negative”. Such a 2–stage approach has also been
suggested in (Pang and Lee, 2004) to improve sen-
timent classification of reviews by discarding objec-
tive sentences, in (Wilson et al., 2005a) for phrase-
level sentiment analysis, and in (Barbosa and Feng,
2010) for sentiment analysis on Twitter messages.
1A separate test set with SMS messages was also provided
by the organisers to measure performance of systems over other
types of message data. No training and development data were
provided for this set.
2Hereafter we will use the terms “objective” and “neutral”
interchangeably.
</bodyText>
<subsectionHeader confidence="0.996389">
3.1 Data Preprocessing
</subsectionHeader>
<bodyText confidence="0.97751836">
ga g
Before we could proceed with feature engineering,
al
we performed several preprocessing steps. To be
more precise, a twitter specific tokeniser and part-
of-speech (POS) tagger (Ritter et al., 2011) were
used to obtain the tokens and the corresponding
POS tags which are necessary for a particular set
of features to be described later. In addition to these,
six lexicons, originating from Wilson’s (2005b) lexi-
con, were created. This lexicon contains expressions
that given a context (i.e., surrounding words) indi-
cate subjectivity. The expression that in most con-
text expresses sentiment is considered to be “strong”
subjective, otherwise it is considered weak subjec-
tive (i.e., it has specific subjective usages). So, we
first split the lexicon in two smaller, one contain-
ing strong and one containing weak subjective ex-
pressions. Moreover, Wilson also reports the polar-
ity of each expression out of context (prior polarity)
which can be positive, negative or neutral. As a con-
sequence, we further split each of the two lexicons
into three smaller according to the prior polarity of
the expression, resulting to the following six lexi-
cons:
</bodyText>
<listItem confidence="0.8895265">
S+ : Contains strong subjective expressions with
positive prior polarity.
S_ : Contains strong subjective expressions with
negative prior polarity.
S0 : Contains strong subjective expressions with
neutral prior polarity.
</listItem>
<page confidence="0.992187">
563
</page>
<figure confidence="0.996895545454545">
Messages
Subjective
messages
Subjectivity detection
SVM
Polarity detection
SVM
Objective
messages
Positive
messages
</figure>
<figureCaption confidence="0.987928">
Figure 2: Our 2–stage pipeline procedure.
</figureCaption>
<bodyText confidence="0.9103625">
Negative
messages
</bodyText>
<listItem confidence="0.796601833333333">
W+ : Contains weak subjective expressions with
positive prior polarity.
W_ : Contains weak subjective expressions with
negative prior polarity.
W0 : Contains weak subjective expressions with
neutral prior polarity.
</listItem>
<bodyText confidence="0.937591954545455">
Adding to these, three more lexicons were created,
one for each class (positive, negative, neutral). In
particular, we employed Chi Squared feature selec-
tion (Liu and Setiono, 1995) to obtain the 100 most
important tokens per class from the training set.
Very few tokens were manually erased to result to
the following three lexicons.
T+ : Contains the top-94 tokens appearing in posi-
tive tweets of the training set.
T_ : Contains the top-96 tokens appearing in nega-
tive tweets of the training set.
T0 : Contains the top-94 tokens appearing in neutral
tweets of the training set.
The nine lexicons described above are used to cal-
culate precision (P(t, c)), recall (R(t, c)) and F −
measure (F1(t, c)) of tokens appearing in a mes-
sage with respect to each class. Equations 1, 2 and 3
below provide the definitions of these metrics.
P(t, c) = #tweets that contain token t and belong to class c
#tweets that contain token t
R(t, c) = #tweets that contain token t and belong to class c
#tweets that belong to class c
</bodyText>
<equation confidence="0.9854285">
F1t, c) = 2 P(t, c) R(t, c) (3)
( P(t, c) + R(t, c)
</equation>
<subsectionHeader confidence="0.998301">
3.2 Feature engineering
</subsectionHeader>
<bodyText confidence="0.9999956">
We employed three types of features, namely
boolean features, POS based features and lexicon
based features. Our goal is to build a system that is
not explicitly based on the vocabulary of the training
set, having therefore better generalisation capability.
</bodyText>
<subsectionHeader confidence="0.581504">
3.2.1 Boolean features
</subsectionHeader>
<bodyText confidence="0.999971">
Bag of words (BOW): These features indicate the
existence of specific tokens in a message. We
used feature selection with Info Gain to obtain
the 600 most informative tokens of the training
set and we then manually removed 19 of them
</bodyText>
<page confidence="0.990873">
564
</page>
<bodyText confidence="0.99847009375">
to result in 581 tokens. As a consequence we
get 581 features that can take a value of 1 if a
message contains the corresponding token and
0 otherwise.
Time and date: We observed that time and date of-
ten indicated events in the train data and such
messages tend to be objective. Therefore, we
added two more features to indicate if a mes-
sage contains time and/or date expressions.
Character repetition: Repetitive characters are of-
ten added to words by users, in order to give
emphasis or to express themselves more in-
tensely. As a consequence they indicate sub-
jectivity. So we added one more feature having
a value of 1 if a message contains words with
repeating characters and 0 otherwise.
Negation: Negation not only is a good subjectivity
indicator but it also may change the polarity of
a message. We therefore add 5 more features,
one indicating the existence of negation, and
the remaining four indicating the existence of
negation that precedes (in a distance of at most
5 tokens) words from lexicons S+, S_, W+ and
W_.
Hash-tags with sentiment: These features are im-
plemented by getting all the possible sub-
strings of the string after the symbol # and
checking if any of them match with any word
from S+, S_, W+ and W_ (4 features). A
value of 1 means that a hash-tag containing a
word from the corresponding lexicon exists in
a message.
</bodyText>
<subsectionHeader confidence="0.748791">
3.2.2 POS based features
</subsectionHeader>
<bodyText confidence="0.999981">
Specific POS tags might be good indicators of
subjectivity or objectivity. For instance adjectives
often express sentiment (e.g., beautiful, frustrating)
while proper nouns are often reported in objective
messaged. We, therefore, added 10 more features
based on the following POS tags:
</bodyText>
<listItem confidence="0.9981898">
1. adjectives,
2. adverbs,
3. verbs,
4. nouns,
5. proper nouns,
6. urls,
7. interjections,
8. hash-tags,
9. happy emoticons, and
10. sad emoticons.
</listItem>
<bodyText confidence="0.952884052631579">
We then constructed our features as follows. For
each message we counted the occurrences of tokens
with these POS tags and we divided this number
with the number of tokens having any of these POS
tags. For instance if a message contains 2 adjectives,
1 adverb and 1 url then the features corresponding to
adjectives, adverbs and urls will have a value of 2�, 4 1
and 41 respectively while all the remaining features
will be 0. These features can be thought of as a way
to express how much specific POS tags affect the
sentiment of a message.
Going a step further we calculate precision
(P(b, c)), recall (R(b, c)) and F − measure
(F1(b, c)) of POS tags bigrams with respect to each
class (equations 4, 5 and 6 respectively).
P(b, c) = #tweets that contain bigram b and belong to class c
#tweets that contain bigram b
R(b, c) = #tweets that contain bigram b and belong to class c
#tweets that belong to class c
</bodyText>
<equation confidence="0.998504">
2 � P(b, c) � R(b, c)
F1(b, c) = (6)
P(b, c) + R(b, c)
</equation>
<bodyText confidence="0.986649285714286">
For each bigram (e.g., adjective-noun) in a mes-
sage we calculate F1(b, c) and then we use the aver-
age, the maximum and the minimum of these values
to create 9 additional features. We did not experi-
ment over measures that weight differently Precision
and Recall (e.g., Fb for b = 0.5) or with different
combinations (e.g., F1 and P).
</bodyText>
<subsectionHeader confidence="0.960811">
3.2.3 Lexicon based features
</subsectionHeader>
<bodyText confidence="0.999925">
This set of features associates the words of the
lexicons described earlier with the three classes.
Given a message M, similarly to the equations 4 and
</bodyText>
<page confidence="0.996011">
565
</page>
<bodyText confidence="0.9946791">
6 above, we calculate P(t, c) and F1(t, c) for every
token t E M with respect to a lexicon. We then ob-
tain the maximum, minimum and average values of
P(t, c) and F1(t, c) in M. We note that the combi-
nation of P and F1 appeared to be the best in our
experiments while R(t, c) was not helpful and thus
was not used. Also, similarly to section 3.2.2 we
did not experiment over measures that weight differ-
ently Precision and Recall (e.g., Fb for b = 0.5). The
former metrics are calculated with three variations:
(a) Using words: The values of the metrics con-
sider only the words of the message.
(b) Using words and priors: The same as (a) but
adding to the calculated metrics a prior value.
This value is calculated on the entire lexicon,
and roughly speaking it is an indicator of how
much we can trust L to predict class c. In cases
that a token t of a message M does not appear
in a lexicon L the corresponding scores of the
metrics will be 0.
</bodyText>
<listItem confidence="0.835502333333333">
(c) Using words and their POS tags: The values
of the metrics consider the words of the message
along with their POS tags.
</listItem>
<bodyText confidence="0.982709315789474">
(d) Using words, their POS tags and priors: The
same as (c) but adding to the calculated metrics
an apriori value. The apriori value is calculated
in a similar manner as in (b) with the difference
that we consider the POS tags of the words as
well.
For case (a) we calculated minimum, maximum
and average values of P(t, c) and F1(t, c) with re-
spect to S+, S_, S0, W+, W_ and W0 consider-
ing only the words of the message resulting to 108
features. Concerning case (b) we calculated average
P(t, c) and F1(t, c) with respect to S+, S_, S0, W+,
W_ and W0, and average P(t, c) with respect to T+,
T_ and T0 adding 45 more features. For case (c) we
calculated minimum, maximum and average P(t, c)
with respect to S+, S_, S0, W+, W_ and W0 (54
features), and, finally, for case (d) we calculated av-
erage P(t, c) and F1(t, c) with respect to S+, S_,
S0, W+, W_ and W0 to add 36 features.
</bodyText>
<table confidence="0.9967852">
Class F1
Positive 0.6496
Negative 0.4429
Neutral 0.7022
Average 0.5462
</table>
<tableCaption confidence="0.971048">
Table 1: F1 for development set.
</tableCaption>
<sectionHeader confidence="0.985952" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999978925925926">
As stated earlier we use a 2–stage pipeline approach
to identify the sentiment of a message. Preliminary
experiments on the development data showed that
this approach is better than attempting to address the
problem in one stage during which a classifier must
classify a message as positive, negative or neutral.
To be more precise we used a Naive Bayes classifier
and BOW features using both 1–stage and 2–stage
approaches. Although we considered the 2–stage
approach with a Naive Bayes classifier as a baseline
system we used it to submit results for both twitter
and sms test sets.
Having concluded to the 2–stage approach we
employed for each stage an SVM classifier, fed with
the 855 features described in section 3.2.3 Both
SVMs use linear kernel and are tuned in order to
find the optimum C parameter. Observe that we use
the same set of features in both stages and let the
classifier learn the appropriate weights for each fea-
ture. During the first stage, the classifier is trained
on the entire training set after merging positive and
negative classes to one superclass, namely subjec-
tive. In the second stage, the classifier is trained only
on positive and negative tweets of the training and
is asked to determine whether the messages classi-
fied as subjective during the first stage are positive
or negative.
</bodyText>
<sectionHeader confidence="0.66703" genericHeader="evaluation">
4.1 Results
</sectionHeader>
<bodyText confidence="0.999695714285714">
In order to obtain the best set of features we trained
our system on the downloaded training data and
measured its performance on the provided develop-
ment data. Table 1 illustrates the F1 results on the
development set. A first observation is that there
is a considerable difference between the F1 of the
negative class and the other two, with the former be-
</bodyText>
<footnote confidence="0.95627">
3We used the LIBLINEAR distribution (Fan et al., 2008)
</footnote>
<page confidence="0.987535">
566
</page>
<table confidence="0.964016636363636">
Class F1
Positive 0.6854
Negative 0.4929
Neutral 0.7117
Average 0.5891
Table 2: Fl for twitter test set.
Class F1
Positive 0.6349
Negative 0.5131
Neutral 0.7785
Average 0.5740
</table>
<tableCaption confidence="0.99777">
Table 3: Fl for sms test set.
</tableCaption>
<bodyText confidence="0.999967857142857">
ing significantly decreased. This might be due to
the quite low number of negative tweets of the ini-
tial training set in comparison with the rest of the
classes. Therefore, the addition of 340 negative ex-
amples from the development set emerged from this
imbalance and proved to be effective as shown in ta-
ble 2 illustrating our results on the test set regarding
tweets. Unfortunately we were not able to submit
results with this system for the sms test set. How-
ever, we performed post-experiments after the gold
sms test set was released. The results shown on table
3 are similar to the ones obtained for the twitter test
set which means that our model has a good general-
isation ability.
</bodyText>
<sectionHeader confidence="0.960777" genericHeader="conclusions">
5 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999289181818182">
In this paper we presented our approach for the
Message Polarity Classification task of SEMEVAL
2013. We proposed a pipeline approach to detect
sentiment in two stages; first we discard objective
messages and then we classify subjective (i.e., car-
rying sentiment) ones as positive or negative. We
used SVMs with various extracted features for both
stages and although the system performed reason-
ably well, there is still much room for improvement.
A first problem that should be addressed is the dif-
ficulty in identifying negative messages. This was
mainly due to small number of tweets in the train-
ing data. This was somewhat alleviated by adding
the negative instances of the development data but
still our system reports lower results for this class as
compared to positive and neutral classes. More data
or better features is a possible improvement. An-
other issue that has not an obvious answer is how to
proceed in order to improve the 2–stage pipeline ap-
proach. Should we try and optimise each stage sepa-
rately or should we optimise the second stage taking
into consideration the results of the first stage?
</bodyText>
<sectionHeader confidence="0.999145" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99958911627907">
Luciano Barbosa and Junlan Feng. 2010. Robust senti-
ment detection on twitter from biased and noisy data.
In Proceedings of the 23rd International Conference
on Computational Linguistics: Posters, COLING ’10,
pages 36–44, Beijing, China. Association for Compu-
tational Linguistics.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui
Wang, and Chih-Jen Lin. 2008. Liblinear: A library
for large linear classification. The Journal of Machine
Learning Research, 9:1871–1874.
Huan Liu and Rudy Setiono. 1995. Chi2: Feature se-
lection and discretization of numeric attributes. In
Tools with Artificial Intelligence, 1995. Proceedings.,
Seventh International Conference on, pages 388–391.
IEEE.
Bo Pang and Lillian Lee. 2004. A sentimental edu-
cation: sentiment analysis using subjectivity summa-
rization based on minimum cuts. In Proceedings of
the 42nd Annual Meeting on Association for Compu-
tational Linguistics, ACL ’04, Barcelona, Spain. As-
sociation for Computational Linguistics.
Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.
2011. Named entity recognition in tweets: An experi-
mental study. In EMNLP, pages 1524–1534.
V. Vapnik. 1998. Statistical learning theory. John Wiley.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005a. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the confer-
ence on Human Language Technology and Empirical
Methods in Natural Language Processing, HLT ’05,
pages 347–354, Vancouver, British Columbia, Canada.
Association for Computational Linguistics.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005b. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Empir-
ical Methods in Natural Language Processing, pages
347–354. Association for Computational Linguistics.
Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara
Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013.
SemEval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on Se-
mantic Evaluation, SemEval ’13, June.
</reference>
<page confidence="0.997312">
567
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.280879">
<title confidence="0.999098">nlp.cs.aueb.gr: Two Stage Sentiment Analysis</title>
<author confidence="0.7609185">Prodromos Malakasiotis</author>
<author confidence="0.7609185">Rafael Michael Makrynioti</author>
<affiliation confidence="0.9931935">Department of Athens University of Economics and</affiliation>
<note confidence="0.584094">Patission 76, GR-104 34 Athens, Greece</note>
<abstract confidence="0.992094727272727">This paper describes the systems with which we participated in the task Sentiment Analysis Twitter of and specifically the Message Polarity Classification. We used a 2-stage pipeline approach employing a linear SVM classifier at each stage and several features including BOW features, POS based features and lexicon based features. We have also experimented with Naive Bayes classifiers trained with BOW features.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Luciano Barbosa</author>
<author>Junlan Feng</author>
</authors>
<title>Robust sentiment detection on twitter from biased and noisy data.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10,</booktitle>
<pages>36--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Beijing, China.</location>
<contexts>
<context position="5108" citStr="Barbosa and Feng, 2010" startWordPosition="802" endWordPosition="805">detect whether each message M expresses positive, negative or no sentiment (figure 2). Specifically, during the first stage we attempt to detect if M expresses a sentiment (positive or negative) or not. If so, M is called “subjective”, otherwise it is called “objective” or “neutral”.2 Each subjective message is then classified in a second stage as “positive” or “negative”. Such a 2–stage approach has also been suggested in (Pang and Lee, 2004) to improve sentiment classification of reviews by discarding objective sentences, in (Wilson et al., 2005a) for phraselevel sentiment analysis, and in (Barbosa and Feng, 2010) for sentiment analysis on Twitter messages. 1A separate test set with SMS messages was also provided by the organisers to measure performance of systems over other types of message data. No training and development data were provided for this set. 2Hereafter we will use the terms “objective” and “neutral” interchangeably. 3.1 Data Preprocessing ga g Before we could proceed with feature engineering, al we performed several preprocessing steps. To be more precise, a twitter specific tokeniser and partof-speech (POS) tagger (Ritter et al., 2011) were used to obtain the tokens and the correspondi</context>
</contexts>
<marker>Barbosa, Feng, 2010</marker>
<rawString>Luciano Barbosa and Junlan Feng. 2010. Robust sentiment detection on twitter from biased and noisy data. In Proceedings of the 23rd International Conference on Computational Linguistics: Posters, COLING ’10, pages 36–44, Beijing, China. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>Liblinear: A library for large linear classification.</title>
<date>2008</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>9--1871</pages>
<contexts>
<context position="15924" citStr="Fan et al., 2008" startWordPosition="2685" endWordPosition="2688">assifier is trained only on positive and negative tweets of the training and is asked to determine whether the messages classified as subjective during the first stage are positive or negative. 4.1 Results In order to obtain the best set of features we trained our system on the downloaded training data and measured its performance on the provided development data. Table 1 illustrates the F1 results on the development set. A first observation is that there is a considerable difference between the F1 of the negative class and the other two, with the former be3We used the LIBLINEAR distribution (Fan et al., 2008) 566 Class F1 Positive 0.6854 Negative 0.4929 Neutral 0.7117 Average 0.5891 Table 2: Fl for twitter test set. Class F1 Positive 0.6349 Negative 0.5131 Neutral 0.7785 Average 0.5740 Table 3: Fl for sms test set. ing significantly decreased. This might be due to the quite low number of negative tweets of the initial training set in comparison with the rest of the classes. Therefore, the addition of 340 negative examples from the development set emerged from this imbalance and proved to be effective as shown in table 2 illustrating our results on the test set regarding tweets. Unfortunately we we</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. Liblinear: A library for large linear classification. The Journal of Machine Learning Research, 9:1871–1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Huan Liu</author>
<author>Rudy Setiono</author>
</authors>
<title>Chi2: Feature selection and discretization of numeric attributes.</title>
<date>1995</date>
<booktitle>In Tools with Artificial Intelligence,</booktitle>
<pages>388--391</pages>
<publisher>IEEE.</publisher>
<contexts>
<context position="7399" citStr="Liu and Setiono, 1995" startWordPosition="1149" endWordPosition="1152">tive expressions with neutral prior polarity. 563 Messages Subjective messages Subjectivity detection SVM Polarity detection SVM Objective messages Positive messages Figure 2: Our 2–stage pipeline procedure. Negative messages W+ : Contains weak subjective expressions with positive prior polarity. W_ : Contains weak subjective expressions with negative prior polarity. W0 : Contains weak subjective expressions with neutral prior polarity. Adding to these, three more lexicons were created, one for each class (positive, negative, neutral). In particular, we employed Chi Squared feature selection (Liu and Setiono, 1995) to obtain the 100 most important tokens per class from the training set. Very few tokens were manually erased to result to the following three lexicons. T+ : Contains the top-94 tokens appearing in positive tweets of the training set. T_ : Contains the top-96 tokens appearing in negative tweets of the training set. T0 : Contains the top-94 tokens appearing in neutral tweets of the training set. The nine lexicons described above are used to calculate precision (P(t, c)), recall (R(t, c)) and F − measure (F1(t, c)) of tokens appearing in a message with respect to each class. Equations 1, 2 and </context>
</contexts>
<marker>Liu, Setiono, 1995</marker>
<rawString>Huan Liu and Rudy Setiono. 1995. Chi2: Feature selection and discretization of numeric attributes. In Tools with Artificial Intelligence, 1995. Proceedings., Seventh International Conference on, pages 388–391. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Barcelona,</location>
<contexts>
<context position="4932" citStr="Pang and Lee, 2004" startWordPosition="774" endWordPosition="777"> illustrates the problem on train and development sets. 3 System Overview The system we propose is a 2–stage pipeline procedure employing SVM classifiers (Vapnik, 1998) to detect whether each message M expresses positive, negative or no sentiment (figure 2). Specifically, during the first stage we attempt to detect if M expresses a sentiment (positive or negative) or not. If so, M is called “subjective”, otherwise it is called “objective” or “neutral”.2 Each subjective message is then classified in a second stage as “positive” or “negative”. Such a 2–stage approach has also been suggested in (Pang and Lee, 2004) to improve sentiment classification of reviews by discarding objective sentences, in (Wilson et al., 2005a) for phraselevel sentiment analysis, and in (Barbosa and Feng, 2010) for sentiment analysis on Twitter messages. 1A separate test set with SMS messages was also provided by the organisers to measure performance of systems over other types of message data. No training and development data were provided for this set. 2Hereafter we will use the terms “objective” and “neutral” interchangeably. 3.1 Data Preprocessing ga g Before we could proceed with feature engineering, al we performed sever</context>
</contexts>
<marker>Pang, Lee, 2004</marker>
<rawString>Bo Pang and Lillian Lee. 2004. A sentimental education: sentiment analysis using subjectivity summarization based on minimum cuts. In Proceedings of the 42nd Annual Meeting on Association for Computational Linguistics, ACL ’04, Barcelona, Spain. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alan Ritter</author>
<author>Sam Clark</author>
<author>Mausam</author>
<author>Oren Etzioni</author>
</authors>
<title>Named entity recognition in tweets: An experimental study.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>1524--1534</pages>
<publisher>John Wiley.</publisher>
<contexts>
<context position="5657" citStr="Ritter et al., 2011" startWordPosition="887" endWordPosition="890">5a) for phraselevel sentiment analysis, and in (Barbosa and Feng, 2010) for sentiment analysis on Twitter messages. 1A separate test set with SMS messages was also provided by the organisers to measure performance of systems over other types of message data. No training and development data were provided for this set. 2Hereafter we will use the terms “objective” and “neutral” interchangeably. 3.1 Data Preprocessing ga g Before we could proceed with feature engineering, al we performed several preprocessing steps. To be more precise, a twitter specific tokeniser and partof-speech (POS) tagger (Ritter et al., 2011) were used to obtain the tokens and the corresponding POS tags which are necessary for a particular set of features to be described later. In addition to these, six lexicons, originating from Wilson’s (2005b) lexicon, were created. This lexicon contains expressions that given a context (i.e., surrounding words) indicate subjectivity. The expression that in most context expresses sentiment is considered to be “strong” subjective, otherwise it is considered weak subjective (i.e., it has specific subjective usages). So, we first split the lexicon in two smaller, one containing strong and one cont</context>
</contexts>
<marker>Ritter, Clark, Mausam, Etzioni, 2011</marker>
<rawString>Alan Ritter, Sam Clark, Mausam, and Oren Etzioni. 2011. Named entity recognition in tweets: An experimental study. In EMNLP, pages 1524–1534. V. Vapnik. 1998. Statistical learning theory. John Wiley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05,</booktitle>
<pages>347--354</pages>
<publisher>Association for</publisher>
<institution>Computational Linguistics.</institution>
<location>Vancouver, British Columbia, Canada.</location>
<contexts>
<context position="5038" citStr="Wilson et al., 2005" startWordPosition="791" endWordPosition="794">ge pipeline procedure employing SVM classifiers (Vapnik, 1998) to detect whether each message M expresses positive, negative or no sentiment (figure 2). Specifically, during the first stage we attempt to detect if M expresses a sentiment (positive or negative) or not. If so, M is called “subjective”, otherwise it is called “objective” or “neutral”.2 Each subjective message is then classified in a second stage as “positive” or “negative”. Such a 2–stage approach has also been suggested in (Pang and Lee, 2004) to improve sentiment classification of reviews by discarding objective sentences, in (Wilson et al., 2005a) for phraselevel sentiment analysis, and in (Barbosa and Feng, 2010) for sentiment analysis on Twitter messages. 1A separate test set with SMS messages was also provided by the organisers to measure performance of systems over other types of message data. No training and development data were provided for this set. 2Hereafter we will use the terms “objective” and “neutral” interchangeably. 3.1 Data Preprocessing ga g Before we could proceed with feature engineering, al we performed several preprocessing steps. To be more precise, a twitter specific tokeniser and partof-speech (POS) tagger (R</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005a. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, HLT ’05, pages 347–354, Vancouver, British Columbia, Canada. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="5038" citStr="Wilson et al., 2005" startWordPosition="791" endWordPosition="794">ge pipeline procedure employing SVM classifiers (Vapnik, 1998) to detect whether each message M expresses positive, negative or no sentiment (figure 2). Specifically, during the first stage we attempt to detect if M expresses a sentiment (positive or negative) or not. If so, M is called “subjective”, otherwise it is called “objective” or “neutral”.2 Each subjective message is then classified in a second stage as “positive” or “negative”. Such a 2–stage approach has also been suggested in (Pang and Lee, 2004) to improve sentiment classification of reviews by discarding objective sentences, in (Wilson et al., 2005a) for phraselevel sentiment analysis, and in (Barbosa and Feng, 2010) for sentiment analysis on Twitter messages. 1A separate test set with SMS messages was also provided by the organisers to measure performance of systems over other types of message data. No training and development data were provided for this set. 2Hereafter we will use the terms “objective” and “neutral” interchangeably. 3.1 Data Preprocessing ga g Before we could proceed with feature engineering, al we performed several preprocessing steps. To be more precise, a twitter specific tokeniser and partof-speech (POS) tagger (R</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005b. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Zornitsa Kozareva</author>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
</authors>
<title>SemEval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13,</booktitle>
<contexts>
<context position="2082" citStr="Wilson et al., 2013" startWordPosition="328" endWordPosition="331">s a positive, negative or neutral sentiment. For instance M1 below expresses a positive sentiment, M2 a negative one, while M3 has no sentiment at all. M1: GREAT GAME GIRLS!! On to districts Monday at Fox!! Thanks to the fans for coming out :) M2: Firework just came on my tv and I just broke down and sat and cried, I need help okay M3: Going to a bulls game with Aaliyah &amp; hope next Thursday As sentiment analysis in Twitter is a very recent subject, it is certain that more research and improvements are needed. This paper presents our approach for the subtask of Message Polarity Classification (Wilson et al., 2013) of SEMEVAL 2013. We used a 2-stage pipeline approach employing a linear SVM classifier at each stage and several features including bag of words (BOW) features, part-of-speech (POS) based features and lexicon based features. We have also experimented with Naive Bayes classifiers trained with BOW features. The rest of the paper is organised as follows. Section 2 provides a short analysis of the data used while section 3 describes our approach. Section 4 describes the experiments we performed and the corresponding results and section 5 concludes and gives hints for future work. 2 Data Before we</context>
</contexts>
<marker>Wilson, Kozareva, Nakov, Rosenthal, Stoyanov, Ritter, 2013</marker>
<rawString>Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, Sara Rosenthal, Veselin Stoyanov, and Alan Ritter. 2013. SemEval-2013 task 2: Sentiment analysis in twitter. In Proceedings of the International Workshop on Semantic Evaluation, SemEval ’13, June.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>