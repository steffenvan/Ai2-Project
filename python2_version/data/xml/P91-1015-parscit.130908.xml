<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<note confidence="0.564538">
Head Corner Parsing for Discontinuous Constituency
Gertjan van Noord
Lehrstuhl fiir Computerlinguistik
</note>
<address confidence="0.674988666666667">
Universitat des Saarlandes
Im Stadtwald 15
D-6600 Saarbrficken 11, FRG
</address>
<email confidence="0.802632">
vannoord©coli.uni-sb.de
</email>
<sectionHeader confidence="0.98634" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99920225">
I describe a head-driven parser for a class of gram-
mars that handle discontinuous constituency by a
richer notion of string combination than ordinary
concatenation. The parser is a generalization of
the left-corner parser (Matsumoto et al., 1983)
and can be used for grammars written in power-
ful formalisms such as non-concatenative versions
of HPSG (Pollard, 1984; Reape, 1989).
</bodyText>
<sectionHeader confidence="0.998121" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997643616666666">
Although most formalisms in computational lin-
guistics assume that phrases are built by string
concatenation (eg. as in PATR II, GPSG, LFG
and most versions of Categorial Grammar), this
assumption is challenged in non-concatenative
grammatical formalisms. In Pollard&apos;s dissertation
several versions of &amp;quot;head wrapping&amp;quot; are defined
(Pollard, 1984). In the analysis of the Australian
free word-order language Guugu Yimidhirr, Mark
Johnson uses a &apos;combine&apos; predicate in a DCG-like
grammar that corresponds to the union of words
(Johnson, 1985).
Mike Reape uses an operation called &apos;sequence
union&apos; to analyse Germanic semi-free word or-
der constructions (Reape, 1989; Reape, 1990a).
Other examples include Tree Adjoining Gram-
mars (Joshi et at., 1975; Vijay-Shankar and
Joshi, 1988), and versions of Categorial Gram-
mar (Dowty, 1990) and references cited there.
Motivation. There are several motivations for
non-concatenative grammars. First, specialized
string combination operations allow elegant lin-
guistic accounts of phenomena that are otherwise
notoriously hard. Examples are the analyses of
Dutch cross serial dependencies by head wrap-
ping or sequence union (Reape, 1990a).
Furthermore, in non-concatenative grammars
it is possible to relate (parts of) constituents that
belong together semantically, but which are not
adjacent. Hence such grammars facilitate a sim-
ple compositional semantics. In CF-based gram-
mars such phenomena usually are treated by com-
plex &apos;threading&apos; mechanisms.
Non-concatenative grammatical formalisms
may also be attractive from a computational
point of view. It is easier to define generation
algorithms if the semantics is built in a systemat-
ically constrained way (van Noord, 1990b). The
semantic-head-driven generation strategy (van
Noord, 1989; Calder et a/., 1989; Shieber et al.,
1989; van Noord, 1990a; Shieber et al., 1990)
faces problems in case semantic heads are &apos;dis-
placed&apos;, and this displacement is analyzed us-
ing threading. However, in this paper I sketch
a simple analysis of verb-second (an example of
a displacement of semantic heads) by an oper-
ation similar to head wrapping which a head-
driven generator processes without any problems
(or extensions) at all. Clearly, there are also some
computational problems, because most &apos;standard&apos;
parsing strategies assume context-free concatena-
tion of strings. These problems are the subject of
this paper.
The task. I will restrict the attention to a
class of constraint-based formalisms, in which
operations on strings are defined that are more
powerful than concatenation, but which opera-
tions are restricted to be nonerasing, and linear.
The resulting class of systems can be character-
ized as Linear Context-Free Rewriting Systems
</bodyText>
<page confidence="0.997652">
114
</page>
<bodyText confidence="0.9999452">
(LCFRS), augmented with feature-structures (F-
LCFRS). For a discussion of the properties of
LCFRS without feature-structures, see (Vijay-
Shanker et al., 1987). Note though that these
properties do not carry over to the current sys-
tem, because of the augmention with feature
structures.
As in LCFRS, the operations on strings in F-
LCFRS can be characterized as follows. First,
derived structures will be mapped onto a set of
occurances of words; i.e. each derived structure
&apos;knows&apos; which words it &apos;dominates&apos;. For example,
each derived feature structure may contain an at-
tribute `phon&apos; whose value is a list of atoms repre-
senting the string it dominates. I will write w(F)
for the set of occurances of words that the derived
structure F dominates. Rules combine structures
Dn into a new structure M. Nonerasure re-
quires that the union of w applied to each daugh-
ter is a subset of w(M):
</bodyText>
<equation confidence="0.949956">
U w(Di) C w(m)
</equation>
<bodyText confidence="0.994348012987013">
Linearity requires that the difference of the car-
dina1ities of these sets is a constant factor; i.e. a
rule may only introduce a fixed number of words
syncategorematically:
— U w(Di)i = c,c a constant
i=1
CF-based formalisms clearly fulfill this require-
ment, as do Head Grammars, grammars using
sequence union, and TAG&apos;s. I assume in the re-
mainder of this paper that t w(Di) =
for all rules other than lexical entries (i.e. all
words are introduced on a terminal). Note though
that a simple generalization of the algorithm pre-
sented below handles the general case (along the
lines of Shieber et al. (1989; 1990)by treating
rules that introduce extra lexical material as non-
chain-rules).
Furthermore, I will assume that each rule has a
designated daughter, called the head. Although
I will not impose any restrictions on the head, it
will turn out that the parsing strategy to be pro-
posed will be very sensitive to the choice of heads,
with the effect that F-LCFRS&apos;s in which the no-
tion &apos;head&apos; is defined in a systematic way (Pol-
lard&apos;s Head Grammars, Reape&apos;s version of HPSG,
Dowty&apos;s version of Categorial Grammar), may be
much more efficiently parsed than other gram-
mars. The notion seed of a parse tree is defined
recursively in terms of the head. The seed of a
tree will be the seed of its head. The seed of a
terminal will be that terminal itself.
Other approaches. In (Proudian and Pollard,
1985) a head-driven algorithm based on active
chart parsing is described. The details of the al-
gorithm are unclear from the paper which makes
a comparison with our approach hard; it is not
clear whether the parser indeed allows for ex-
ample the head-wrapping operations of Pollard
(1984). Reape presented two algorithms (Reape,
1990b) which are generalizations of a shift-reduce
parser, and the CKY algorithm, for the same class
of grammars. I present a head-driven bottom-up
algorithm for F-LCFR grammars. The algorithm
resembles the head-driven parser by Martin Kay
(Kay, 1989), but is generalized in order to be used
for this larger class of grammars. The disadvan-
tages Kay noted for his parser do not carry over
to this generalized version, as redundant search
paths for CF-based grammars turn out to be gen-
uine parts of the search space for F-LCFR gram-
mars.
The advantage of my algorithm is that it both
employs bottom-up and top-down filtering in a
straightforward way. The algorithm is closely re-
lated to head-driven generators (van Noord, 1989;
Calder et at., 1989; Shieber et at., 1989; van No-
ord, 1990a; Shieber et at., 1990). The algorithm
proceeds in a bottom-up, head-driven fashion. In
modern linguistic theories very much information
is defined in lexical entries, whereas rules are re-
duced to very general (and very uninformative)
schemata. More information usually implies less
search space, hence it is sensible to parse bottom-
up in order to obtain useful information as soon
as possible. Furthermore, in many linguistic the-
ories a special daughter called the head deter-
mines what kind of other daughters there may be.
Therefore, it is also sensible to start with the head
in order to know for what else you have to look
for. As the parser proceeds from head to head it
is furthermore possible to use powerful top-down
predictions based on the usual head feature per-
colations. Finally note that proceding bottom-up
solves some non-termination problems, because in
lexicalized theories it is often the case that infor-
mation in lexical entries limit the recursive appli-
cation of rules (eg. the size of the subcat list of
</bodyText>
<page confidence="0.994818">
115
</page>
<bodyText confidence="0.999976833333333">
an entry determines the depth of the derivation
tree of which this entry can be the seed).
Before I present the parser in section 3, I will
first present an example of a F-LCFR grammar,
to obtain a flavor of the type of problems the
parser handles reasonably well.
</bodyText>
<sectionHeader confidence="0.887999" genericHeader="method">
2 A sample grammar
</sectionHeader>
<bodyText confidence="0.999982904761905">
In this section I present a simple F-LCFR gram-
mar for a (tiny) fragment of Dutch. As a caveat
I want to stress that the purpose of the current
section is to provide an example of possible input
for the parser to be defined in the next section,
rather than to provide an account of phenomena
that is completely satisfactory from a linguistic
point of view.
Grammar rules are written as (pure) Prolog
clauses.&apos; Heads select arguments using a sub-
cat list. Argument structures are specified lexi-
cally and are percolated from head to head. Syn-
tactic features are shared between heads (hence
I make the simplifying assumption that head =
functor, which may have to be revised in order
to treat modification). In this grammar I use
revised versions of Pollard&apos;s head wrapping op-
erations to analyse cross serial dependency and
verb second constructions. For a linguistic back-
ground of these constructions and analyses, cf.
Evers (1975), Koster (1975) and many others.
</bodyText>
<subsectionHeader confidence="0.43645">
Rules are defined as
</subsectionHeader>
<bodyText confidence="0.953529">
rule(Read,Mother,Other)
or as
</bodyText>
<subsectionHeader confidence="0.767248">
rule (Mother)
</subsectionHeader>
<bodyText confidence="0.968105563636364">
(for lexical entries), where Head represents the
designated head daughter, Mother the mother
category and Other a list of the other daughters.
Each category is a term
x(Syn,Subcat,Phon,Sem,Rule)
where Syn describes the part of speech, Subcat
1 It should be stressed though that other unification
grammar formalisms can be extended quite easily to en-
code the same grammar. I implemented the algorithm for
several grammars written in a version of PATR II without
built-in string concatenation.
is a list of categories a category subcategorizes
for, Phon describes the string that is dominated
by this category, and Sem is the argument struc-
ture associated with this category. Rule indicates
which rule (i.e. version of the combine predicate
cb to be defined below) should be applied; it gen-
eralizes the &apos;Order&apos; feature of UCG. The value of
Phon is a term p(Left,Read,Right) where the
fields in this term are difference lists of words.
The first argument represents the string left of the
head, the second argument represents the head
and the third argument represents the string right
of the head. Hence, the string associated with
such a term is the concatenation of the three ar-
guments from left to right. There is only one pa-
rameterized, binary branching, rule in the gram-
mar:
rule(x(Syn,[x(C,L,P2,S,R)10,P1,Sem.-),
x(Syn,L,P,Sem,_),
[x(C,L,P2,S,R)]) :-
cb(R, Pi, P2, P).
In this rule the first element of the subcategoriza-
tion list of the head is selected as the (only) other
daughter of the mother of the rule. The syntac-
tic and semantic features of the mother and the
head are shared. Furthermore, the strings associ-
ated with the two daughters of the rule are to be
combined by the cb predicate. For simple (left or
right) concatenation this predicate is defined as
follows:
cb(left, p(L4-L,E,R),
p(L1-L2,L2-L3,L3-L4).
p(Li-L,R,R)).
cb(right, p(L,H,R1-R2),
p(R2-R3,R3-R4,R4-R),
p(L,H,R1-R)).
Although this looks horrible for people not famil-
iar with Prolog, the idea is really very simple.
In the first case the string associated with the
argument is appended to the left of the string
left of the head; in the second case this string is
appended to the right of the string right of the
head. In a friendlier notation the examples may
look like:
</bodyText>
<page confidence="0.964817">
116
</page>
<bodyText confidence="0.9041725">
p(A1.412•A31,H,R) Here the head and right string of the argument
/\ are appended to the right, whereas the left string
p(L,H,R) p(A1,A2,A3) of the argument is appended to the left. Again,
an illustration might help:
</bodyText>
<equation confidence="0.7824405">
p(L-A1,H,A2•A3•R)
p(L,H,11-411•A2•A3)
/\
p(L,H,R) p(A1,A2,A3)
</equation>
<bodyText confidence="0.949861626865672">
Lexical entries for the intransitive verb `slaapt&apos;
(sleeps) and the transitive verb lust&apos; (kisses) are
defined as follows:
rule( x(v, (n, ,_,A,left)] ,
p(P-P, Eslaapt -T,R-R) ,
sleep(A) ,_))
rule( x (v Cx(n, ,_,B,left),
x(n, , A ,left)3 ,
p(P-P, Ckust I -T,R-R) ,
kiss(A,B),_)).
Proper nouns are defined as:
rule( x(n, ,p(P-P, [piet I T] -T,H-R) ,
pete,_)) .
and a top category is defined as follows (comple-
mentizers that have selected all arguments, i.e.
sentences):
top(x (comp, 0 ,_,_,_)) .
Such a complementizer, eg. &apos;at&apos; (that) is defined
as:
rule( x(comp, Cx(v, ,_,A,right)] ,
P(P-P, Eclat 111-T,R-R),
that (A) _)) •
The choice of datastructure for the value of
Phon allows a simple definition of the verb raising
(vr) version of the combine predicate that may be
used for Dutch cross serial dependencies:
cb(vr, p(L1-L2,H,R3-11),
p(L2-L,R1-R2,R2-R3),
p(L1-L,H,R1-R)).
/\
p(L,H,R) p(A1,A2,A3)
A raising verb, eg. `ziet&apos; (sees) is defined as:
rule(x(v, [x(n, 0 ,_,InfSubj,left),
x (inf , Cx(_ , _ , _ , Inf Subj ,
,_,B,vr),
x(n, 0 ,_,A, left)] ,
p(P-P, Niet I ,
see(A,B) ,_)) •
In this entry `ziet&apos; selects — apart from its np-
subject — two objects, a np and a VP (with cat-
egory inf). The inf still has an element in its
sub cat list; this element is controlled by the np
(this is performed by the sharing of InfSubj). To
derive the subordinate phrase &apos;dal jan piet marie
ziet kussen&apos; (that john sees pete kiss mary), the
main verb `ziet&apos; first selects its np-object `piet&apos;
resulting in the string `piet ziet&apos;. Then it selects
the infinitival `marie kussen&apos;. These two strings
are combined into `piet marie ziet kussen&apos; (using
the vr version of the cb predicate). The subject
is selected resulting in the string jail piet marie
ziet kussen&apos;. This string is selected by the corn-
plementizer, resulting in `dat jan piet marie ziet
kussen&apos;. The argument structure will be instan-
tiated as that (sees (john, kiss (pete ,mary))).
In Dutch main clauses, there usually is no overt
complementizer; instead the finite verb occupies
the first position (in yes-no questions), or the
second position (right after the topic; ordinary
declarative sentences). In the following analysis
an empty complementizer selects an ordinary (fi-
nite) v; the resulting string is formed by the fol-
lowing definition of cb:
cb(v2, p(A-A,B-B,C-C),
p(111-R2,H,R2-11),
p(A-A,H,R1-R)) .
which may be illustrated with:
</bodyText>
<page confidence="0.953635">
117
</page>
<bodyText confidence="0.941274355555555">
p( 0 ,A2,A1-A3)
/\
p( , , p(A1,A2,A3)
The finite complementizer is defined as:
rule(x(comp, Ex(y, 0 ,_,A,v2)] ,
p(B-B,C-C,D-D),
that (A),_)).
Note that this analysis captures the special rela-
tionship between complementizers and (fronted)
finite verbs in Dutch. The sentence `ziet jan piet
marie kussen&apos; is derived as follows (where the
head of a string is represented in capitals):
inversion: ZIET jan piet marie kussen
left: jan piet marie ZIET kussen
z
raising: piet marie ZIET kussen JAN
most word of a phrase. The parser then proceeds
by proving that this word indeed can be the left-
corner of the phrase. It does so by selecting a rule
whose leftmost daughter unifies with the category
of the word. It then parses other daughters of the
rule recursively and then continues by connecting
the mother category of that rule upwards, recur-
sively. The left-corner algorithm can be general-
ized to the class of grammars under consideration
if we start with the seed of a phrase, instead of its
leftmost word. Furthermore the connect predi-
cate then connects smaller categories upwards by
unifying them with the head of a rule. The first
step of the algorithm consists of the prediction
step: which lexical entry is the seed of the phrase?
The first thing to note is that the words intro-
duced by this lexical entry should be part of the
input string, because of the nonerasure require-
ment (we use the string as a &apos;guide&apos; (Dymetman
et al., 1990) as in a left-corner parser, but we
change the way in which lexical entries &apos;consume
the guide&apos;). Furthermore in most linguistic theo-
ries it is assumed that certain features are shared
between the mother and the head. I assume that
the predicate head/2 defines these feature perco-
lations; for the grammar of the foregoing section
this predicate may be defined as:
head(x(Syn,_,_,Sem,_),
left: piet ZIET left: marie KUSSEN
</bodyText>
<sectionHeader confidence="0.7884155" genericHeader="method">
ZIET PIET KUSSEN MARIE
3 The head corner parser
</sectionHeader>
<bodyText confidence="0.999974733333333">
This section describes the head-driven parsing
algorithm for the type of grammars described
above. The parser is a generalization of a left-
corner parser. Such a parser, which may be called
a &apos;head-corner&apos; parser,2 proceeds in a bottom-up
way. Because the parser proceeds from head to
head it is easy to use powerful top-down pre-
dictions based on the usual head feature perco-
lations, and subcategorization requirements that
heads require from their arguments.
As we will proceed from head to head these fea-
tures will also be shared between the seed and
the top-goal; hence we can use this definition to
restrict lexical lookup by top-down prediction. 3
The first step in the algorithm is defined as:
</bodyText>
<equation confidence="0.93510525">
parse(Cat,PO,P) :-
predict_lex(Cat,SmallCat,PO,P1),
connect(SmallCat,Cat,P1,P).
predict_lex(Cat,SmallCat,PO,P) :-
</equation>
<bodyText confidence="0.970088375">
head(Cat,SmallCat),
rule(SmallCat),
string(SmallCat,Words),
subset(Words,PO,P).
Instead of taking the first word from the current
input string, the parser may select a lexical en-
In left-corner parsers (Matsumoto et aL, 1983)
the first step of the algorithm is to select the left-
</bodyText>
<footnote confidence="0.876936">
2This name is due to Pete Whitelock.
</footnote>
<bodyText confidence="0.91128">
31n the general case we need to compute the transitive
closure of (restrictions of) possible mother-head relation-
ships. The predicate &apos;head may also be used to compile
rules into the format adopted here (i.e. using the defini-
tion the compiler will identify the head of a rule).
</bodyText>
<page confidence="0.996989">
118
</page>
<bodyText confidence="0.976102857142857">
try dominating a subset of the words occuring in
the input string, provided this lexical entry can
be the seed of the current goal. The predicate
subset (L1,L2 ,L3) is true in case Ll is a subset
of L2 with complement L3.4
The second step of the algorithm, the connect
part, is identical to the connect part of the left-
corner parser, but instead of selecting the left-
most daughter of a rule the head-corner parser
selects the head of a rule:
connect(X,X,P,P).
connect(Small,Big,PO,P) :-
rule(Small, Mid, Others),
parse_rest(Others,PO,P1),
connect(Nid,Big,P1,P).
parse_rest( ,P,P) .
parse_rest( [HIT] ,PO,P) :-
parse(H,PO,P1) ,
parse_rest (T, P1 ,P) .
The predicate `start_parse&apos; starts the parse pro-
cess, and requires furthermore that the string as-
sociated with the category that has been found
spans the input string in the right order.
start_parse(String,Cat):-
top(Cat) ,
parse (Cat , String ,0),
string(Cat,String).
The definition of the predicate &apos;string&apos; depends
on the way strings are encoded in the grammar.
The predicate relates linguistic objects and the
string they dominate (as a list of words). I assume
that each grammar provides a definition of this
predicate. In the current grammar string/2 is
defined as follows:
&apos;In Prolog this predicate may be defined as follows:
</bodyText>
<equation confidence="0.66851025">
subset ([) ,P,P) .
subset ([H1 T] ,PO,P) :-
selectchk (H, PO, Pl) ,
subset (T,P1,P) .
selectchk (El, [El IP] ,P) :-
!
selectchk [HIP()) , [HIP] ) :-
selectchk (El, PO, P) .
</equation>
<bodyText confidence="0.990527416666667">
The cut in selectchk is necessary in case the same word
occurs twice in the input string; without it the parser
would not be &apos;minimal&apos;; this could be changed by index-
ing words w.r.t. their position, but I will not assume this
complication here.
copy_tez-m(Phon,Phon2) ,
str(Phon2,Str) .
str(p(P-P1,P1-P2,P2- 0),P) .
This predicate is complicated using the predi-
cate copy_term/2 to prevent any side-effects to
happen in the category. The parser thus needs
two grammar specific predicates: head/2 and
string/2.
Example. To parse the sentence &apos;dal jan
slaapt&apos;, the head corner parser will proceed as
follows. The first call to &apos;parse&apos; will look like:
parse(x(cumP.
[dat, jan,slaapt] . )
The prediction step selects the lexical entry &apos;dat&apos;.
The next goal is to show that this lexical entry is
the seed of the top goal; furthermore the string
that still has to be covered is now [j an, slaapt] .
Leaving details out the connect clause looks as :
connect(
x(comp, [x(v, , right)] • • )
x(comp, 0 , ), [j an, slaapt) . 0 )
The category of dat has to be matched with
the head of a rule. Notice that dat subcatego-
rizes for a v with rule feature right. Hence the
right version of the cb predicate applies, and the
next goal is to parse the v for which this comple-
mentizer subcategorizes, with input jan, slaapt&apos;.
Lexical lookup selects the word slaapt from this
string. The word slaapt has to be shown to be
the head of this v node, by the connect predi-
cate. This time the left combination rule applies
and the next goal consists in parsing a np (for
which slaapt subcategorizes) with input string
jan. This goal succeeds with an empty output
string. Hence the argument of the rule has been
found successfully and hence we need to connect
the mother of the rule up to the v node. This suc-
ceeds trivially, and therefore we now have found
the v for which dat subcategorizes. Hence the
next goal is to connect the complementizer with
an empty subcat list up to the topgoal; again this
succeeds trivially. Hence we obtain the instanti-
ated version of the parse call:
</bodyText>
<page confidence="0.996918">
119
</page>
<bodyText confidence="0.790232111111111">
parse(x(comp, 0 ,p(P-P, [dat
Cjan,slaaptIO-Q),
that (sleeps (j ohn) ) ,_) ,
[dat,jan,slaapt],0)
and the predicate start_parse will succeed,
yielding:
Cat = x(comp, ,p(P-P, [clat -T,
[j am, slaapt 10-0
that (sleeps (j ohn) ) ,_)
</bodyText>
<sectionHeader confidence="0.996276" genericHeader="conclusions">
4 Discussion and Extensions
</sectionHeader>
<bodyText confidence="0.999955137931035">
Sound and Complete. The algorithm as it is
defined is sound (assuming the Prolog interpreter
is sound), and complete in the usual Prolog sense.
Clearly the parser may enter an infinite loop (in
case non branching rules are defined that may
feed themselves or in case a grammar makes a
heavy use of empty categories). However, in case
the parser does terminate one can be sure that it
has found all solutions. Furthermore the parser is
minimal in the sense that it will return one solu-
tion for each possible derivation (of course if sev-
eral derivations yield identical results the parser
will return this result as often as there are deriva-
tions for it).
Efficiency. The parser turns out to be quite ef-
ficient in practice. There is one parameter that
influences efficiency quite dramatically. If the no-
tion &apos;syntactic head&apos; implies that much syntac-
tic information is shared between the head of a
phrase and its mother, then the prediction step
in the algorithm will be much better at &apos;predict-
ing&apos; the head of the phrase. If on the other hand
the notion &apos;head&apos; does not imply such feature per-
colations, then the parser must predict the head
randomly from the input string as no top-down
information is available.
Improvements. The efficiency of the parser
can be improved by common Prolog and parsing
techniques. Firstly, it is possible to compile the
grammar rules, lexical entries and parser a bit fur-
ther by (un)folding (eg. the string predicate can
be applied to each lexical entry in a compilation
stage). Secondly it is possible to integrate well-
formed and non-well-formed subgoal tables in the
parser, following the technique described by Mat-
sumoto et al. (1983). The usefulness of this tech-
nique strongly depends on the actual grammars
that are being used. Finally, the current indexing
of lexical entries is very bad indeed and can easily
be improved drastically.
In some grammars the string operations that
are defined are not only monotonic with respect
to the words they dominate, but also with respect
to the order constraints that are defined between
these words (&apos;order-monotonic&apos;). For example
in Reape&apos;s sequence union operation the linear
precedence constraints that are defined between
elements of a daughter are by definition part of
the linear precedence constraints of the mother.
Note though that the analysis of verb second in
the foregoing section uses a string operation that
does not satisfy this restriction. For grammars
that do satisfy this restriction it is possible to ex-
tend the top-down prediction possibilities by the
incorporation of an extra clause in the &apos;connect&apos;
predicate which will check that the phrase that
has been analysed up to that point can become a
substring of the top string.
</bodyText>
<sectionHeader confidence="0.994589" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999874333333333">
This research was partly supported by SFB 314,
Project N3 BiLD; and by the NBBI via the Eu-
rotra project.
I am grateful to Mike Reape for useful com-
ments, and an anonymous reviewer of ACL, for
pointing out the relevance of LCFRS.
</bodyText>
<sectionHeader confidence="0.912579" genericHeader="references">
Bibliography
</sectionHeader>
<reference confidence="0.9661845">
Jonathan Calder, Mike Reape, and Renk Zeevat.
An algorithm for generation in unification cat-
egorial grammar. In Fourth Conference of the
European Chapter of the Association for Com-
putational Linguistics, pages 233-240, Manch-
ester, 1989.
David Dowty. Towards a minimalist theory of
syntactic structure. In Proceedings of the Sym-
posium on Discontinuous Constituency, ITK
Tilburg, 1990.
Marc Dymetman, Pierre Isabelle, and Francois
Perrault. A symmetrical approach to parsing
</reference>
<page confidence="0.953978">
120
</page>
<reference confidence="0.999675065789474">
and generation. In Proceedings of the 13th In-
ternational Conference on Computational Lin-
guistics (COLING), Helsinki, 1990.
Arnold Evers. The Transformational Cycle in
Dutch and German. PhD thesis, Rijksuniver-
siteit Utrecht, 1975.
Mark Johnson. Parsing with discontinuous
constituents. In 28th Annual Meeting of
the Association for Computational Linguistics,
Chicago, 1985.
A.K. Joshi, L.S. Levy, and M. Takahashi. Tree
adjunct grammars. Journal Computer Systems
Science, 10(1), 1975.
Martin Kay. Head driven parsing. In Proceedings
of Workshop on Parsing Technologies, Pitts-
burgh, 1989.
Jan Koster. Dutch as an SOV language. Linguis-
tic Analysis, 1, 1975.
Y. Matsumoto, H. Tanaka, H. Hirakawa,
H. Miyoshi, and H. Yasukawa. BUP: a bottom
up parser embedded in Prolog. New Genera-
tion Computing, 1(2), 1983.
Carl Pollard. Generalized Context-Free Gram-
mars, Head Grammars, and Natural Language.
PhD thesis, Stanford, 1984.
C. Proudian and C. Pollard. Parsing head-driven
phrase structure grammar. In 28th Annual
Meeting of the Association for Computational
Linguistics, Chicago, 1985.
Mike Reape. A logical treatment ,of semi-free
word order and bounded discontinuous con-
stituency. In Fourth Conference of the Euro-
pean Chapter of the Association for Computa-
tional Linguistics, UMIST Manchester, 1989.
Mike Reape. Getting things in order. In Proceed-
ings of the Symposium on Discontinuous Con-
stituency, ITK Tilburg, 1990.
Mike Reape. Parsing bounded discontinous con-
stituents: Generalisations of the shift-reduce
and CKY algorithms, 1990. Paper presented
at the first CLIN meeting, October 26, OTS
Utrecht.
Stuart M. Shieber, Gertjan van Noord, Robert C.
Moore, and Fernando C.N. Pereira. A
semantic-head-driven generation algorithm for
unification based formalisms. In 27th Annual
Meeting of the Association for Computational
Linguistics, Vancouver, 1989.
Stuart M. Shieber, Gertjan van Noord, Robert C.
Moore, and Fernando C.N. Pereira. Semantic-
head-driven generation. Computational Lin-
guistics, 16(1), 1990.
Gertjan van Noord. BUG: A directed bottom-
up generator for unification based formalisms.
Working Papers in Natural Language Process-
ing, Katholieke Universiteit Leuven, Stichting
Taaltechnologie Utrecht, 4, 1989.
Gertjan van Noord. An overview of head-
driven bottom-up generation. In Robert Dale,
Chris Mellish, and Michael Zock, editors, Cur-
rent Research in Natural Language Generation.
Academic Press, 1990.
Gertjan van Noord. Reversible unification-based
machine translation. In Proceedings of the
13th International Conference on Computa-
tional Linguistics (COLING), Helsinki, 1990.
K. Vijay-Shankar and A. Joshi. Feature struc-
ture based tree adjoining grammar. In Pro-
ceedings of the 12th International Conference
on Computational Linguistics (COLING), Bu-
dapest, 1988.
K. Vijay-Shanker, David J. Weir, and Aravind K.
Joshi. Characterizing structural descriptions
produced by various grammatical formalisms.
In 25th Annual Meeting of the Association for
Computational Linguistics, Stanford, 1987.
</reference>
<page confidence="0.997893">
121
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.347136">
<title confidence="0.978031">Head Corner Parsing for Discontinuous Constituency</title>
<author confidence="0.973265">Gertjan van_Noord</author>
<affiliation confidence="0.829048">Lehrstuhl fiir Computerlinguistik Universitat des Saarlandes</affiliation>
<address confidence="0.8179495">Im Stadtwald 15 D-6600 Saarbrficken 11, FRG</address>
<email confidence="0.974602">vannoord©coli.uni-sb.de</email>
<abstract confidence="0.99237775">I describe a head-driven parser for a class of grammars that handle discontinuous constituency by a richer notion of string combination than ordinary concatenation. The parser is a generalization of the left-corner parser (Matsumoto et al., 1983) and can be used for grammars written in powerful formalisms such as non-concatenative versions</abstract>
<note confidence="0.662077">of HPSG (Pollard, 1984; Reape, 1989).</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jonathan Calder</author>
<author>Mike Reape</author>
<author>Renk Zeevat</author>
</authors>
<title>An algorithm for generation in unification categorial grammar.</title>
<date>1989</date>
<booktitle>In Fourth Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>233--240</pages>
<location>Manchester,</location>
<marker>Calder, Reape, Zeevat, 1989</marker>
<rawString>Jonathan Calder, Mike Reape, and Renk Zeevat. An algorithm for generation in unification categorial grammar. In Fourth Conference of the European Chapter of the Association for Computational Linguistics, pages 233-240, Manchester, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Dowty</author>
</authors>
<title>Towards a minimalist theory of syntactic structure.</title>
<date>1990</date>
<booktitle>In Proceedings of the Symposium on Discontinuous Constituency, ITK</booktitle>
<location>Tilburg,</location>
<contexts>
<context position="1427" citStr="Dowty, 1990" startWordPosition="204" endWordPosition="205">non-concatenative grammatical formalisms. In Pollard&apos;s dissertation several versions of &amp;quot;head wrapping&amp;quot; are defined (Pollard, 1984). In the analysis of the Australian free word-order language Guugu Yimidhirr, Mark Johnson uses a &apos;combine&apos; predicate in a DCG-like grammar that corresponds to the union of words (Johnson, 1985). Mike Reape uses an operation called &apos;sequence union&apos; to analyse Germanic semi-free word order constructions (Reape, 1989; Reape, 1990a). Other examples include Tree Adjoining Grammars (Joshi et at., 1975; Vijay-Shankar and Joshi, 1988), and versions of Categorial Grammar (Dowty, 1990) and references cited there. Motivation. There are several motivations for non-concatenative grammars. First, specialized string combination operations allow elegant linguistic accounts of phenomena that are otherwise notoriously hard. Examples are the analyses of Dutch cross serial dependencies by head wrapping or sequence union (Reape, 1990a). Furthermore, in non-concatenative grammars it is possible to relate (parts of) constituents that belong together semantically, but which are not adjacent. Hence such grammars facilitate a simple compositional semantics. In CF-based grammars such phenom</context>
</contexts>
<marker>Dowty, 1990</marker>
<rawString>David Dowty. Towards a minimalist theory of syntactic structure. In Proceedings of the Symposium on Discontinuous Constituency, ITK Tilburg, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marc Dymetman</author>
<author>Pierre Isabelle</author>
<author>Francois Perrault</author>
</authors>
<title>A symmetrical approach to parsing and generation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Helsinki,</location>
<contexts>
<context position="15424" citStr="Dymetman et al., 1990" startWordPosition="2513" endWordPosition="2516">at rule upwards, recursively. The left-corner algorithm can be generalized to the class of grammars under consideration if we start with the seed of a phrase, instead of its leftmost word. Furthermore the connect predicate then connects smaller categories upwards by unifying them with the head of a rule. The first step of the algorithm consists of the prediction step: which lexical entry is the seed of the phrase? The first thing to note is that the words introduced by this lexical entry should be part of the input string, because of the nonerasure requirement (we use the string as a &apos;guide&apos; (Dymetman et al., 1990) as in a left-corner parser, but we change the way in which lexical entries &apos;consume the guide&apos;). Furthermore in most linguistic theories it is assumed that certain features are shared between the mother and the head. I assume that the predicate head/2 defines these feature percolations; for the grammar of the foregoing section this predicate may be defined as: head(x(Syn,_,_,Sem,_), left: piet ZIET left: marie KUSSEN ZIET PIET KUSSEN MARIE 3 The head corner parser This section describes the head-driven parsing algorithm for the type of grammars described above. The parser is a generalization </context>
</contexts>
<marker>Dymetman, Isabelle, Perrault, 1990</marker>
<rawString>Marc Dymetman, Pierre Isabelle, and Francois Perrault. A symmetrical approach to parsing and generation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnold Evers</author>
</authors>
<title>The Transformational Cycle in Dutch and German.</title>
<date>1975</date>
<tech>PhD thesis,</tech>
<institution>Rijksuniversiteit Utrecht,</institution>
<contexts>
<context position="8967" citStr="Evers (1975)" startWordPosition="1449" endWordPosition="1450">factory from a linguistic point of view. Grammar rules are written as (pure) Prolog clauses.&apos; Heads select arguments using a subcat list. Argument structures are specified lexically and are percolated from head to head. Syntactic features are shared between heads (hence I make the simplifying assumption that head = functor, which may have to be revised in order to treat modification). In this grammar I use revised versions of Pollard&apos;s head wrapping operations to analyse cross serial dependency and verb second constructions. For a linguistic background of these constructions and analyses, cf. Evers (1975), Koster (1975) and many others. Rules are defined as rule(Read,Mother,Other) or as rule (Mother) (for lexical entries), where Head represents the designated head daughter, Mother the mother category and Other a list of the other daughters. Each category is a term x(Syn,Subcat,Phon,Sem,Rule) where Syn describes the part of speech, Subcat 1 It should be stressed though that other unification grammar formalisms can be extended quite easily to encode the same grammar. I implemented the algorithm for several grammars written in a version of PATR II without built-in string concatenation. is a list </context>
</contexts>
<marker>Evers, 1975</marker>
<rawString>Arnold Evers. The Transformational Cycle in Dutch and German. PhD thesis, Rijksuniversiteit Utrecht, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Johnson</author>
</authors>
<title>Parsing with discontinuous constituents.</title>
<date>1985</date>
<booktitle>In 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Chicago,</location>
<contexts>
<context position="1140" citStr="Johnson, 1985" startWordPosition="161" endWordPosition="162">concatenative versions of HPSG (Pollard, 1984; Reape, 1989). 1 Introduction Although most formalisms in computational linguistics assume that phrases are built by string concatenation (eg. as in PATR II, GPSG, LFG and most versions of Categorial Grammar), this assumption is challenged in non-concatenative grammatical formalisms. In Pollard&apos;s dissertation several versions of &amp;quot;head wrapping&amp;quot; are defined (Pollard, 1984). In the analysis of the Australian free word-order language Guugu Yimidhirr, Mark Johnson uses a &apos;combine&apos; predicate in a DCG-like grammar that corresponds to the union of words (Johnson, 1985). Mike Reape uses an operation called &apos;sequence union&apos; to analyse Germanic semi-free word order constructions (Reape, 1989; Reape, 1990a). Other examples include Tree Adjoining Grammars (Joshi et at., 1975; Vijay-Shankar and Joshi, 1988), and versions of Categorial Grammar (Dowty, 1990) and references cited there. Motivation. There are several motivations for non-concatenative grammars. First, specialized string combination operations allow elegant linguistic accounts of phenomena that are otherwise notoriously hard. Examples are the analyses of Dutch cross serial dependencies by head wrapping</context>
</contexts>
<marker>Johnson, 1985</marker>
<rawString>Mark Johnson. Parsing with discontinuous constituents. In 28th Annual Meeting of the Association for Computational Linguistics, Chicago, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A K Joshi</author>
<author>L S Levy</author>
<author>M Takahashi</author>
</authors>
<title>Tree adjunct grammars.</title>
<date>1975</date>
<journal>Journal Computer Systems Science,</journal>
<volume>10</volume>
<issue>1</issue>
<marker>Joshi, Levy, Takahashi, 1975</marker>
<rawString>A.K. Joshi, L.S. Levy, and M. Takahashi. Tree adjunct grammars. Journal Computer Systems Science, 10(1), 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Kay</author>
</authors>
<title>Head driven parsing.</title>
<date>1989</date>
<booktitle>In Proceedings of Workshop on Parsing Technologies,</booktitle>
<location>Pittsburgh,</location>
<contexts>
<context position="6194" citStr="Kay, 1989" startWordPosition="974" endWordPosition="975">ches. In (Proudian and Pollard, 1985) a head-driven algorithm based on active chart parsing is described. The details of the algorithm are unclear from the paper which makes a comparison with our approach hard; it is not clear whether the parser indeed allows for example the head-wrapping operations of Pollard (1984). Reape presented two algorithms (Reape, 1990b) which are generalizations of a shift-reduce parser, and the CKY algorithm, for the same class of grammars. I present a head-driven bottom-up algorithm for F-LCFR grammars. The algorithm resembles the head-driven parser by Martin Kay (Kay, 1989), but is generalized in order to be used for this larger class of grammars. The disadvantages Kay noted for his parser do not carry over to this generalized version, as redundant search paths for CF-based grammars turn out to be genuine parts of the search space for F-LCFR grammars. The advantage of my algorithm is that it both employs bottom-up and top-down filtering in a straightforward way. The algorithm is closely related to head-driven generators (van Noord, 1989; Calder et at., 1989; Shieber et at., 1989; van Noord, 1990a; Shieber et at., 1990). The algorithm proceeds in a bottom-up, hea</context>
</contexts>
<marker>Kay, 1989</marker>
<rawString>Martin Kay. Head driven parsing. In Proceedings of Workshop on Parsing Technologies, Pittsburgh, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jan Koster</author>
</authors>
<title>Dutch as an SOV language.</title>
<date>1975</date>
<journal>Linguistic Analysis,</journal>
<volume>1</volume>
<contexts>
<context position="8982" citStr="Koster (1975)" startWordPosition="1451" endWordPosition="1452"> linguistic point of view. Grammar rules are written as (pure) Prolog clauses.&apos; Heads select arguments using a subcat list. Argument structures are specified lexically and are percolated from head to head. Syntactic features are shared between heads (hence I make the simplifying assumption that head = functor, which may have to be revised in order to treat modification). In this grammar I use revised versions of Pollard&apos;s head wrapping operations to analyse cross serial dependency and verb second constructions. For a linguistic background of these constructions and analyses, cf. Evers (1975), Koster (1975) and many others. Rules are defined as rule(Read,Mother,Other) or as rule (Mother) (for lexical entries), where Head represents the designated head daughter, Mother the mother category and Other a list of the other daughters. Each category is a term x(Syn,Subcat,Phon,Sem,Rule) where Syn describes the part of speech, Subcat 1 It should be stressed though that other unification grammar formalisms can be extended quite easily to encode the same grammar. I implemented the algorithm for several grammars written in a version of PATR II without built-in string concatenation. is a list of categories a</context>
</contexts>
<marker>Koster, 1975</marker>
<rawString>Jan Koster. Dutch as an SOV language. Linguistic Analysis, 1, 1975.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Matsumoto</author>
<author>H Tanaka</author>
<author>H Hirakawa</author>
<author>H Miyoshi</author>
<author>H Yasukawa</author>
</authors>
<title>BUP: a bottom up parser embedded in Prolog.</title>
<date>1983</date>
<journal>New Generation Computing,</journal>
<volume>1</volume>
<issue>2</issue>
<contexts>
<context position="22740" citStr="Matsumoto et al. (1983)" startWordPosition="3738" endWordPosition="3742"> notion &apos;head&apos; does not imply such feature percolations, then the parser must predict the head randomly from the input string as no top-down information is available. Improvements. The efficiency of the parser can be improved by common Prolog and parsing techniques. Firstly, it is possible to compile the grammar rules, lexical entries and parser a bit further by (un)folding (eg. the string predicate can be applied to each lexical entry in a compilation stage). Secondly it is possible to integrate wellformed and non-well-formed subgoal tables in the parser, following the technique described by Matsumoto et al. (1983). The usefulness of this technique strongly depends on the actual grammars that are being used. Finally, the current indexing of lexical entries is very bad indeed and can easily be improved drastically. In some grammars the string operations that are defined are not only monotonic with respect to the words they dominate, but also with respect to the order constraints that are defined between these words (&apos;order-monotonic&apos;). For example in Reape&apos;s sequence union operation the linear precedence constraints that are defined between elements of a daughter are by definition part of the linear prec</context>
</contexts>
<marker>Matsumoto, Tanaka, Hirakawa, Miyoshi, Yasukawa, 1983</marker>
<rawString>Y. Matsumoto, H. Tanaka, H. Hirakawa, H. Miyoshi, and H. Yasukawa. BUP: a bottom up parser embedded in Prolog. New Generation Computing, 1(2), 1983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Pollard</author>
</authors>
<title>Generalized Context-Free Grammars, Head Grammars, and Natural Language. PhD thesis,</title>
<date>1984</date>
<location>Stanford,</location>
<contexts>
<context position="946" citStr="Pollard, 1984" startWordPosition="131" endWordPosition="132">ombination than ordinary concatenation. The parser is a generalization of the left-corner parser (Matsumoto et al., 1983) and can be used for grammars written in powerful formalisms such as non-concatenative versions of HPSG (Pollard, 1984; Reape, 1989). 1 Introduction Although most formalisms in computational linguistics assume that phrases are built by string concatenation (eg. as in PATR II, GPSG, LFG and most versions of Categorial Grammar), this assumption is challenged in non-concatenative grammatical formalisms. In Pollard&apos;s dissertation several versions of &amp;quot;head wrapping&amp;quot; are defined (Pollard, 1984). In the analysis of the Australian free word-order language Guugu Yimidhirr, Mark Johnson uses a &apos;combine&apos; predicate in a DCG-like grammar that corresponds to the union of words (Johnson, 1985). Mike Reape uses an operation called &apos;sequence union&apos; to analyse Germanic semi-free word order constructions (Reape, 1989; Reape, 1990a). Other examples include Tree Adjoining Grammars (Joshi et at., 1975; Vijay-Shankar and Joshi, 1988), and versions of Categorial Grammar (Dowty, 1990) and references cited there. Motivation. There are several motivations for non-concatenative grammars. First, specializ</context>
<context position="5902" citStr="Pollard (1984)" startWordPosition="931" endWordPosition="932">f HPSG, Dowty&apos;s version of Categorial Grammar), may be much more efficiently parsed than other grammars. The notion seed of a parse tree is defined recursively in terms of the head. The seed of a tree will be the seed of its head. The seed of a terminal will be that terminal itself. Other approaches. In (Proudian and Pollard, 1985) a head-driven algorithm based on active chart parsing is described. The details of the algorithm are unclear from the paper which makes a comparison with our approach hard; it is not clear whether the parser indeed allows for example the head-wrapping operations of Pollard (1984). Reape presented two algorithms (Reape, 1990b) which are generalizations of a shift-reduce parser, and the CKY algorithm, for the same class of grammars. I present a head-driven bottom-up algorithm for F-LCFR grammars. The algorithm resembles the head-driven parser by Martin Kay (Kay, 1989), but is generalized in order to be used for this larger class of grammars. The disadvantages Kay noted for his parser do not carry over to this generalized version, as redundant search paths for CF-based grammars turn out to be genuine parts of the search space for F-LCFR grammars. The advantage of my algo</context>
</contexts>
<marker>Pollard, 1984</marker>
<rawString>Carl Pollard. Generalized Context-Free Grammars, Head Grammars, and Natural Language. PhD thesis, Stanford, 1984.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Proudian</author>
<author>C Pollard</author>
</authors>
<title>Parsing head-driven phrase structure grammar.</title>
<date>1985</date>
<booktitle>In 28th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Chicago,</location>
<contexts>
<context position="5621" citStr="Proudian and Pollard, 1985" startWordPosition="882" endWordPosition="885">Although I will not impose any restrictions on the head, it will turn out that the parsing strategy to be proposed will be very sensitive to the choice of heads, with the effect that F-LCFRS&apos;s in which the notion &apos;head&apos; is defined in a systematic way (Pollard&apos;s Head Grammars, Reape&apos;s version of HPSG, Dowty&apos;s version of Categorial Grammar), may be much more efficiently parsed than other grammars. The notion seed of a parse tree is defined recursively in terms of the head. The seed of a tree will be the seed of its head. The seed of a terminal will be that terminal itself. Other approaches. In (Proudian and Pollard, 1985) a head-driven algorithm based on active chart parsing is described. The details of the algorithm are unclear from the paper which makes a comparison with our approach hard; it is not clear whether the parser indeed allows for example the head-wrapping operations of Pollard (1984). Reape presented two algorithms (Reape, 1990b) which are generalizations of a shift-reduce parser, and the CKY algorithm, for the same class of grammars. I present a head-driven bottom-up algorithm for F-LCFR grammars. The algorithm resembles the head-driven parser by Martin Kay (Kay, 1989), but is generalized in ord</context>
</contexts>
<marker>Proudian, Pollard, 1985</marker>
<rawString>C. Proudian and C. Pollard. Parsing head-driven phrase structure grammar. In 28th Annual Meeting of the Association for Computational Linguistics, Chicago, 1985.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
</authors>
<title>A logical treatment ,of semi-free word order and bounded discontinuous constituency.</title>
<date>1989</date>
<booktitle>In Fourth Conference of the European Chapter of the Association for Computational Linguistics, UMIST Manchester,</booktitle>
<contexts>
<context position="1262" citStr="Reape, 1989" startWordPosition="179" endWordPosition="180">tics assume that phrases are built by string concatenation (eg. as in PATR II, GPSG, LFG and most versions of Categorial Grammar), this assumption is challenged in non-concatenative grammatical formalisms. In Pollard&apos;s dissertation several versions of &amp;quot;head wrapping&amp;quot; are defined (Pollard, 1984). In the analysis of the Australian free word-order language Guugu Yimidhirr, Mark Johnson uses a &apos;combine&apos; predicate in a DCG-like grammar that corresponds to the union of words (Johnson, 1985). Mike Reape uses an operation called &apos;sequence union&apos; to analyse Germanic semi-free word order constructions (Reape, 1989; Reape, 1990a). Other examples include Tree Adjoining Grammars (Joshi et at., 1975; Vijay-Shankar and Joshi, 1988), and versions of Categorial Grammar (Dowty, 1990) and references cited there. Motivation. There are several motivations for non-concatenative grammars. First, specialized string combination operations allow elegant linguistic accounts of phenomena that are otherwise notoriously hard. Examples are the analyses of Dutch cross serial dependencies by head wrapping or sequence union (Reape, 1990a). Furthermore, in non-concatenative grammars it is possible to relate (parts of) constitu</context>
</contexts>
<marker>Reape, 1989</marker>
<rawString>Mike Reape. A logical treatment ,of semi-free word order and bounded discontinuous constituency. In Fourth Conference of the European Chapter of the Association for Computational Linguistics, UMIST Manchester, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
</authors>
<title>Getting things in order.</title>
<date>1990</date>
<booktitle>In Proceedings of the Symposium on Discontinuous Constituency, ITK</booktitle>
<location>Tilburg,</location>
<contexts>
<context position="1275" citStr="Reape, 1990" startWordPosition="181" endWordPosition="182">hat phrases are built by string concatenation (eg. as in PATR II, GPSG, LFG and most versions of Categorial Grammar), this assumption is challenged in non-concatenative grammatical formalisms. In Pollard&apos;s dissertation several versions of &amp;quot;head wrapping&amp;quot; are defined (Pollard, 1984). In the analysis of the Australian free word-order language Guugu Yimidhirr, Mark Johnson uses a &apos;combine&apos; predicate in a DCG-like grammar that corresponds to the union of words (Johnson, 1985). Mike Reape uses an operation called &apos;sequence union&apos; to analyse Germanic semi-free word order constructions (Reape, 1989; Reape, 1990a). Other examples include Tree Adjoining Grammars (Joshi et at., 1975; Vijay-Shankar and Joshi, 1988), and versions of Categorial Grammar (Dowty, 1990) and references cited there. Motivation. There are several motivations for non-concatenative grammars. First, specialized string combination operations allow elegant linguistic accounts of phenomena that are otherwise notoriously hard. Examples are the analyses of Dutch cross serial dependencies by head wrapping or sequence union (Reape, 1990a). Furthermore, in non-concatenative grammars it is possible to relate (parts of) constituents that bel</context>
<context position="5947" citStr="Reape, 1990" startWordPosition="937" endWordPosition="938">may be much more efficiently parsed than other grammars. The notion seed of a parse tree is defined recursively in terms of the head. The seed of a tree will be the seed of its head. The seed of a terminal will be that terminal itself. Other approaches. In (Proudian and Pollard, 1985) a head-driven algorithm based on active chart parsing is described. The details of the algorithm are unclear from the paper which makes a comparison with our approach hard; it is not clear whether the parser indeed allows for example the head-wrapping operations of Pollard (1984). Reape presented two algorithms (Reape, 1990b) which are generalizations of a shift-reduce parser, and the CKY algorithm, for the same class of grammars. I present a head-driven bottom-up algorithm for F-LCFR grammars. The algorithm resembles the head-driven parser by Martin Kay (Kay, 1989), but is generalized in order to be used for this larger class of grammars. The disadvantages Kay noted for his parser do not carry over to this generalized version, as redundant search paths for CF-based grammars turn out to be genuine parts of the search space for F-LCFR grammars. The advantage of my algorithm is that it both employs bottom-up and t</context>
</contexts>
<marker>Reape, 1990</marker>
<rawString>Mike Reape. Getting things in order. In Proceedings of the Symposium on Discontinuous Constituency, ITK Tilburg, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mike Reape</author>
</authors>
<title>Parsing bounded discontinous constituents: Generalisations of the shift-reduce and CKY algorithms,</title>
<date>1990</date>
<booktitle>Paper presented at the first CLIN meeting, October 26, OTS</booktitle>
<location>Utrecht.</location>
<contexts>
<context position="1275" citStr="Reape, 1990" startWordPosition="181" endWordPosition="182">hat phrases are built by string concatenation (eg. as in PATR II, GPSG, LFG and most versions of Categorial Grammar), this assumption is challenged in non-concatenative grammatical formalisms. In Pollard&apos;s dissertation several versions of &amp;quot;head wrapping&amp;quot; are defined (Pollard, 1984). In the analysis of the Australian free word-order language Guugu Yimidhirr, Mark Johnson uses a &apos;combine&apos; predicate in a DCG-like grammar that corresponds to the union of words (Johnson, 1985). Mike Reape uses an operation called &apos;sequence union&apos; to analyse Germanic semi-free word order constructions (Reape, 1989; Reape, 1990a). Other examples include Tree Adjoining Grammars (Joshi et at., 1975; Vijay-Shankar and Joshi, 1988), and versions of Categorial Grammar (Dowty, 1990) and references cited there. Motivation. There are several motivations for non-concatenative grammars. First, specialized string combination operations allow elegant linguistic accounts of phenomena that are otherwise notoriously hard. Examples are the analyses of Dutch cross serial dependencies by head wrapping or sequence union (Reape, 1990a). Furthermore, in non-concatenative grammars it is possible to relate (parts of) constituents that bel</context>
<context position="5947" citStr="Reape, 1990" startWordPosition="937" endWordPosition="938">may be much more efficiently parsed than other grammars. The notion seed of a parse tree is defined recursively in terms of the head. The seed of a tree will be the seed of its head. The seed of a terminal will be that terminal itself. Other approaches. In (Proudian and Pollard, 1985) a head-driven algorithm based on active chart parsing is described. The details of the algorithm are unclear from the paper which makes a comparison with our approach hard; it is not clear whether the parser indeed allows for example the head-wrapping operations of Pollard (1984). Reape presented two algorithms (Reape, 1990b) which are generalizations of a shift-reduce parser, and the CKY algorithm, for the same class of grammars. I present a head-driven bottom-up algorithm for F-LCFR grammars. The algorithm resembles the head-driven parser by Martin Kay (Kay, 1989), but is generalized in order to be used for this larger class of grammars. The disadvantages Kay noted for his parser do not carry over to this generalized version, as redundant search paths for CF-based grammars turn out to be genuine parts of the search space for F-LCFR grammars. The advantage of my algorithm is that it both employs bottom-up and t</context>
</contexts>
<marker>Reape, 1990</marker>
<rawString>Mike Reape. Parsing bounded discontinous constituents: Generalisations of the shift-reduce and CKY algorithms, 1990. Paper presented at the first CLIN meeting, October 26, OTS Utrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Gertjan van Noord</author>
<author>Robert C Moore</author>
<author>Fernando C N Pereira</author>
</authors>
<title>A semantic-head-driven generation algorithm for unification based formalisms.</title>
<date>1989</date>
<booktitle>In 27th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Vancouver,</location>
<marker>Shieber, van Noord, Moore, Pereira, 1989</marker>
<rawString>Stuart M. Shieber, Gertjan van Noord, Robert C. Moore, and Fernando C.N. Pereira. A semantic-head-driven generation algorithm for unification based formalisms. In 27th Annual Meeting of the Association for Computational Linguistics, Vancouver, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart M Shieber</author>
<author>Gertjan van Noord</author>
<author>Robert C Moore</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Semantichead-driven generation.</title>
<date>1990</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>1</issue>
<marker>Shieber, van Noord, Moore, Pereira, 1990</marker>
<rawString>Stuart M. Shieber, Gertjan van Noord, Robert C. Moore, and Fernando C.N. Pereira. Semantichead-driven generation. Computational Linguistics, 16(1), 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>BUG: A directed bottomup generator for unification based formalisms. Working Papers in Natural Language Processing, Katholieke Universiteit Leuven, Stichting Taaltechnologie Utrecht,</title>
<date>1989</date>
<volume>4</volume>
<marker>van Noord, 1989</marker>
<rawString>Gertjan van Noord. BUG: A directed bottomup generator for unification based formalisms. Working Papers in Natural Language Processing, Katholieke Universiteit Leuven, Stichting Taaltechnologie Utrecht, 4, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>An overview of headdriven bottom-up generation.</title>
<date>1990</date>
<booktitle>Current Research in Natural Language Generation.</booktitle>
<editor>In Robert Dale, Chris Mellish, and Michael Zock, editors,</editor>
<publisher>Academic Press,</publisher>
<marker>van Noord, 1990</marker>
<rawString>Gertjan van Noord. An overview of headdriven bottom-up generation. In Robert Dale, Chris Mellish, and Michael Zock, editors, Current Research in Natural Language Generation. Academic Press, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gertjan van Noord</author>
</authors>
<title>Reversible unification-based machine translation.</title>
<date>1990</date>
<booktitle>In Proceedings of the 13th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Helsinki,</location>
<marker>van Noord, 1990</marker>
<rawString>Gertjan van Noord. Reversible unification-based machine translation. In Proceedings of the 13th International Conference on Computational Linguistics (COLING), Helsinki, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shankar</author>
<author>A Joshi</author>
</authors>
<title>Feature structure based tree adjoining grammar.</title>
<date>1988</date>
<booktitle>In Proceedings of the 12th International Conference on Computational Linguistics (COLING),</booktitle>
<location>Budapest,</location>
<contexts>
<context position="1377" citStr="Vijay-Shankar and Joshi, 1988" startWordPosition="194" endWordPosition="197">t versions of Categorial Grammar), this assumption is challenged in non-concatenative grammatical formalisms. In Pollard&apos;s dissertation several versions of &amp;quot;head wrapping&amp;quot; are defined (Pollard, 1984). In the analysis of the Australian free word-order language Guugu Yimidhirr, Mark Johnson uses a &apos;combine&apos; predicate in a DCG-like grammar that corresponds to the union of words (Johnson, 1985). Mike Reape uses an operation called &apos;sequence union&apos; to analyse Germanic semi-free word order constructions (Reape, 1989; Reape, 1990a). Other examples include Tree Adjoining Grammars (Joshi et at., 1975; Vijay-Shankar and Joshi, 1988), and versions of Categorial Grammar (Dowty, 1990) and references cited there. Motivation. There are several motivations for non-concatenative grammars. First, specialized string combination operations allow elegant linguistic accounts of phenomena that are otherwise notoriously hard. Examples are the analyses of Dutch cross serial dependencies by head wrapping or sequence union (Reape, 1990a). Furthermore, in non-concatenative grammars it is possible to relate (parts of) constituents that belong together semantically, but which are not adjacent. Hence such grammars facilitate a simple composi</context>
</contexts>
<marker>Vijay-Shankar, Joshi, 1988</marker>
<rawString>K. Vijay-Shankar and A. Joshi. Feature structure based tree adjoining grammar. In Proceedings of the 12th International Conference on Computational Linguistics (COLING), Budapest, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Vijay-Shanker</author>
<author>David J Weir</author>
<author>Aravind K Joshi</author>
</authors>
<title>Characterizing structural descriptions produced by various grammatical formalisms.</title>
<date>1987</date>
<booktitle>In 25th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Stanford,</location>
<marker>Vijay-Shanker, Weir, Joshi, 1987</marker>
<rawString>K. Vijay-Shanker, David J. Weir, and Aravind K. Joshi. Characterizing structural descriptions produced by various grammatical formalisms. In 25th Annual Meeting of the Association for Computational Linguistics, Stanford, 1987.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>