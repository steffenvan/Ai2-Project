<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000293">
<title confidence="0.9984115">
The Discovery of Natural Typing Annotations:
User-produced Potential Chinese Word Delimiters
</title>
<author confidence="0.999522">
Dakui Zhang1, Yu Mao1, Yang Liu1, Hanshi Wang2, Chuyuan Wei1, Shiping Tang1
</author>
<affiliation confidence="0.9977">
1Beijing Institute of Technology, Beijing, China
2Capital Normal University, Beijing, China
</affiliation>
<email confidence="0.988963">
{sbirdge,maoyubit,yan9liu,necrostone,weichuyuan}@gmail.com, simontangbit@bit.edu.cn
</email>
<sectionHeader confidence="0.98339" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999762703703704">
Human labeled corpus is indispensable for
the training of supervised word segmenters.
However, it is time-consuming and labor-
intensive to label corpus manually. During
the process of typing Chinese text by Pingyin,
people usually need to type &amp;quot;space&amp;quot; or nu-
meric keys to choose the words due to homo-
phones, which can be viewed as a cue for
segmentation. We argue that such a process
can be used to build a labeled corpus in a
more natural way. Thus, in this paper, we in-
vestigate Natural Typing Annotations (NTAs)
that are potential word delimiters produced
by users while typing Chinese. A detailed
analysis on over three hundred user-produced
texts containing NTAs reveals that high-
quality NTAs mostly agree with gold seg-
mentation and, consequently, can be used for
improving the performance of supervised
word segmentation model in out-of-domain.
Experiments show that a classification model
combined with a voting mechanism can reli-
ably identify the high-quality NTAs texts that
are more readily available labeled corpus.
Furthermore, the NTAs might be particularly
useful to deal with out-of-vocabulary (OOV)
words such as proper names and neo-logisms.
</bodyText>
<sectionHeader confidence="0.992497" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999724595238096">
Unlike English text in which sentences are se-
quences of words delimited by white spaces, in
Chinese text, sentences are usually represented
and stored as strings of Chinese characters with-
out similar natural delimiters. To find the basic
language units, i.e. words, segmentation is a nec-
essary initial step for Chinese language pro-
cessing.
Currently most of state-of-the-art methods for
Chinese word segmentation (CWS) are based on
supervised learning, which depend on large scale
annotated corpus. These supervised methods ob-
tain high accuracies on newswire (Xue and Shen,
2003; Zhang and Clark, 2007; Jiang et al., 2009;
Zhao et al., 2010; Sun and Xu, 2011). However,
manually annotated training data mostly come
from the news domain, and the performance can
drop severely when the test data shift from
newswire to blogs, computer forums, and Inter-
net literature (Liu and Zhang, 2012;).Supervised
approaches often have a high requirement on the
quality and quantity of annotated corpus, which
is always not easy to build. As a result, many
previous methods utilize the information of free
data which contain limited but useful segmenta-
tion information over the Internet, including
large-scale unlabeled data, domain-specific lexi-
cons and semi-annotated web pages such as Wik-
ipedia. There has been work on making use of
both unlabeled data (Li and Sun, 2009; Sun and
Xu, 2011; Wang et al., 2011; Qiu et al., 2014)
and Wikipedia (Jiang et al., 2013; Liu et al.,
2014;) to improve segmentation. But none of
them notice the segmentation information pro-
duced by users while typing Chinese.
Chinese is unique due to its logographic writ-
ing system. Chinese users cannot directly type in
Chinese words using a QWERTY keyboard. In-
put methods have been proposed to assist users
to type in Chinese words (Chen, 1997). Substan-
tial information has been produced, but not rec-
orded and stored during text typing process.
</bodyText>
<figureCaption confidence="0.9823745">
Figure 1: Typical Chinese Pinyin input
method (Sogou-Pinyin).
</figureCaption>
<bodyText confidence="0.999699307692308">
The typical way to type in Chinese words is in
a sequential manner (Wang et al., 2001). iRearch
(2009) showed that Pinyin input methods have
the biggest share of Chinese speakers. We take
one of them for example. Suppose users want to
type in Chinese word “今 天 (today)”. Firstly,
they mentally generate and physically type in
corresponding Pinyin “jintian”. Then, a Chinese
Pinyin input method displays a list of Chinese
homophones, as shown in Figure 1. Finally, users
visually search the target word from candidates
and select numeric key, e.g. &apos;1&apos;-&apos;9&apos;(&lt;NUM#1&gt;-
&lt;NUM#9&gt;) or space key (&lt;SPACE&gt;, a shortcut
</bodyText>
<page confidence="0.461233">
662
</page>
<note confidence="0.605465333333333">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 662–667,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.993582985915494">
for numeric key &apos;1&apos;) to get the target word (Zheng
et al., 2011). Other Chinese input methods, like
Wubi, also take these three steps. Typing English
words does not involve the last two steps, which
indicates that it is on one side more complicated
for Chinese users to type in Chinese words than
English, but on the other side more convenient
for us to obtain additional information produced
by users in typing process. We define numeric
keys and the space key as selection keys for
choosing the target word. For sentence “今天天
气不错。(Nice weather today.)”,one general
sequence with selection keys is like “ 今 天
(today)&lt;SPACE&gt;天气(weather)&lt;NUM#2&gt;不错
(not bad)&lt;SPACE&gt;。” or “今天 (today) &lt;SPA-
CE&gt;天气不错 (weather is not bad) &lt;SPACE&gt;。”
In a certain sense, these user-produced selection
keys play a role of word delimiters in a very nat-
ural way.
In this paper, we propose the concept of Natu-
ral Typing Annotations (NTAs) that are potential
word delimiters produced by users while typing
Chinese words, and verify that it is plausible to
automatically generate labeled data for CWS by
exploiting NTAs. According to the principle of
statistical sampling, texts with NTAs are gath-
ered from 384 users. Specifically, since the ul-
timate goal is to exploit NTAs to automatically
generate labeled data for word segmentation, the
main task is to select high-quality NTAs, which
largely overlap with gold segmentation. We do
this by 1) training a classifier to distinguish ac-
ceptable-quality NTAs from low-quality ones,
and then 2) using a voting mechanism to further
locate the high-quality NTAs among those iden-
tified by the classifier in the first step. Experi-
ments show that Support Vector Machine (SVM)
and voting mechanism are effective for this work
and the high-quality NTAs texts can be used as
the training data for improving the performance
of supervised word segmentation model in out-
of-domain. In addition, some evidence is provid-
ed that user-produced NTAs might be particular-
ly useful to deal with out-of-vocabulary (OOV)
words.
In the rest of the paper, we briefly introduce
the gold standard and baseline segmenter of our
work in section 2, then describe the definition
and characteristic of natural typing annotations
(NTAs) in section 3, and finally elaborate on the
strategy of locating high-quality NTAs texts in
section 4.After giving the experimental results
and analysis in section 5, we come to the conclu-
sion and the implication of future work.
2 Gold Standard and Baseline segment-
er
There are many different standards for word
segmentation, and different tasks usually need
different standards. The Sighan Bakeoff uses
four well-known standards made by four differ-
ent organizations: Academia Sinica (AS), City
University of Hong Kong (CU), Peking Univer-
sity (PKU), and Microsoft Research (MSR). In
this study, we take MSR segmentation standard
as gold standard. Following the work of Zhao et
al. (2010) and Sun and Xu (2011), a Conditional
Random Fields (CRF) model (Lafferty et al.,
2001) is trained with the training corpus of MSR
from Sighan Bakeoff-2, to be a baseline seg-
menter. This general-purpose segmenter is called
as CRF+MSR in this paper.
</bodyText>
<sectionHeader confidence="0.978134" genericHeader="method">
3 Iatural Typing Annotations Texts
</sectionHeader>
<subsectionHeader confidence="0.988149">
3.1 Formulation
</subsectionHeader>
<bodyText confidence="0.99565">
A Chinese sentence is represented as
</bodyText>
<equation confidence="0.983197">
S = c1c2 ... c7 ( stands for a Chinese character,
</equation>
<bodyText confidence="0.885615">
I is the length of sentence S ). One of the possi-
ble sequences with selection keys is defined as
</bodyText>
<equation confidence="0.890974">
)r(S) _1 cl ... c,_1 I c, ...cj2_1 I ... I c„l...cl J .Here, we
</equation>
<bodyText confidence="0.9989974">
use the symbol “|” instead of each selection key.
“|” is the “Iatural Typing Annotation (ITA)”,
which is naturally annotated by users when typ-
ing Chinese words. Between the two neighboring
“|”s is a segment. Then the user-produced
</bodyText>
<equation confidence="0.992141666666667">
;r(S) _1 segment, I segment2 I ... I segments
( &lt;_ I , is the number of segments in sen-
tence S ) is called as ITAs text or ITAs corpus.
</equation>
<subsectionHeader confidence="0.999851">
3.2 Collection of ITAs Texts
</subsectionHeader>
<bodyText confidence="0.9999851875">
We need to collect user-produced NTAs texts
independently because there are no similar or
alternative open corpora. We posted a public no-
tice on the Internet to gather volunteer partici-
pants. For comparison, they were told to type in
the same assigned test text while our software
recorded the character sequence with NTAs.
Two explanations are given as followed. First, to
get more users’ feedback and keep the signifi-
cance level of the experiment, we only have 365
Chinese characters in the test text, which con-
tains words with ambiguous meaning, named
entities (NEs), neo-logisms and typo-prone
words. Even the state-of-the-art segmenters can-
not handle this test text very well. Second, ac-
cording to statistical sampling theory, if we want
</bodyText>
<page confidence="0.761738">
663
</page>
<bodyText confidence="0.9995874">
a 95% confidence interval to have a margin of
error less than 5%, the sample size should be no
less than 384. Therefore, we randomly accept
384 volunteers to join our typing experiment and
get user-produced NTAs texts from them.
</bodyText>
<subsectionHeader confidence="0.999633">
3.3 Analysis of Collected ITAs Texts
</subsectionHeader>
<bodyText confidence="0.999957357142857">
Users’ overall typing habit can be drawn through
the analysis of the collected NTAs texts. We
firstly focus on segment, because it is the basic
unit in our texts. A total of 66,232 segments are
obtained from all texts, but only 883 of them are
not repeated. Using Length(seg) to represent the
length of a segment is easy to get a frequency
distribution of different Length(seg) and find that
the length of frequent segments is largely con-
centrated during 1 to 4. The same statistics can
be conducted separately with the word segmenta-
tion results by gold standard and CRF+MSR. We
use relative frequencies to illustrate the overall
trend of three results, as shown in Figure 2.
</bodyText>
<figureCaption confidence="0.5890895">
Figure 2: Relative frequencies of segment length
from three segmentation.
</figureCaption>
<bodyText confidence="0.999922647058824">
The results suggest that most Chinese speakers
are reluctant to put a long text string into one
segment, which is roughly consistent with behav-
ioral economics and psycho-linguistic. Users
consciously avoid the mistakes that might be
brought by typing in long sequence at a time.
Besides, people seldom put illogical sequence of
characters into one segment. Taking “主人公严
守 一 把手 机给 扔了 。 (The leading character
Yan Shouyi has thrown his cellphone away.) ”
for example, when participants input “给扔了
(have thrown) ”, they choose to type in the mate-
rial as “|给|扔|了|”, “|给|扔了|” or “|给扔了|”. No
one types in the material as “|给扔|了|”, because
“ |给 扔 |” has no logical meaning in Chinese.
Consequently, the constitution of segment is a
reflection of natural language logic.
</bodyText>
<sectionHeader confidence="0.969265" genericHeader="method">
4 High-quality ITAs Texts
</sectionHeader>
<subsectionHeader confidence="0.998138">
4.1 User’s Typing Patterns
</subsectionHeader>
<bodyText confidence="0.999620272727273">
In this section, we investigate the collected
NTAs texts at the sentence level. Direct visual
impression is that different users use different
typing patterns to input Chinese. = “不过评价
在三星级以上的这几款电脑(However, these
several computers are assessed with more than 3
stars) ” is taken as an example to explain the dif-
ferent situations. Just as what is shown in the
following, is the gold segmentation of
S, , and others are representative sequences from
different users.
</bodyText>
<equation confidence="0.987277625">
lrgold (Sl)
=7r2 (Sl) “|不过|评价|在|三星级|以上|的|这|几|款|电脑|”
= “|不过|评价|在|三星级|以上|的|这几款|电脑|”
= “|不过|评价|在|三|星|级|以上|的|这几|款|电脑|”
7r (Si)
= “|不过评价|在|三星级以上|的|这几款电脑|”
= “|不过评价在三星级以上的|这几款电脑|”
= “|不|过|评价|在|三|星|级|以|上|的|这|几|款|电|脑|”
</equation>
<bodyText confidence="0.975319391304348">
We discover three typing patterns of users.
The first one is Discrete Pattern, where the
characters belonging to one segment in the light
of gold standard are separated into several seg-
ments, such as 7r5 (S,) . The second is Adhesive
Pattern, which suggests that two or more adja-
cent individual words by gold standard come
together to form one segment, like and
7r4 (S,) . The third is Acceptable Pattern, where
user-produced segmentation is largely or exactly
the same with the gold standard, such as )c, (S, )
and ire (S,) . We find that discrete pattern and
adhesive pattern are useless for word segmenta-
tion. So we call those NTAs texts that follow
acceptable pattern acceptable-quality ITAs
texts, and others low-quality ones. Furthermore,
among acceptable-quality NTAs texts, some of
them are more close to gold standard, which is
called as high-quality ITAs texts. Our strategy
is 1) to use a classifier to find all acceptable-
quality NTAs texts, and then 2) to further locate
the high-quality NTAs texts among those identi-
fied by the classifier in the previous step.
</bodyText>
<subsectionHeader confidence="0.973363">
4.2 The Classification Approach
</subsectionHeader>
<bodyText confidence="0.9998515">
Identification of acceptable-quality NTAs texts is
a typical binary classification problem. Effective
and logical features should be identified to model
a classifier. We select the following five features
because they are simple but outstanding against
other alternatives for this work.
</bodyText>
<equation confidence="0.6069815">



  SingleSegNum, 
 MaxConSingleSegNum, 
 
MaxSegLen  
Features
i
Len,
SegNum,
664
</equation>
<bodyText confidence="0.999942214285714">
Len is the abbreviation for length of a sen-
tence, and SegNum(SN) stands for the number
of the segments in a sentence. These two features
can be used to determine whether the percentage
of character number of a sentence and the seg-
ment number of a sentence is in a proper range.
SingleSegNum(SSN) stands for the number of
the segments whose length equals 1 in a sentence.
MaxConSingleSegNum(MCSSN) is the maxi-
mum number of continuous segments whose
length is 1. MaxSegLen(MSL) means the length
of segment with most characters. These three
features can be used to identify whether discrete
or adhesive phenomena prevail in a sentence.
</bodyText>
<subsectionHeader confidence="0.998568">
4.3 The Voting Mechanism
</subsectionHeader>
<bodyText confidence="0.916649833333333">
As the classification approach brings lots of ac-
ceptable-quality NTAs texts, voting mechanism
is introduced to further locate the high-quality
NTAs texts. For a sentence S, , there possibly
exist different user-produced segmentations
,r, (S,) , ;r2 (S,) , ... , (k is the total
number of these segmentations). If ap-
pears in different users’ texts, these texts practi-
cally vote for j (Si ) . Different users’ texts prac-
tically vote for Trj (S,) , which appears in these
texts. Thus every sentence S, in a text can get a
score:
</bodyText>
<equation confidence="0.983791636363636">
AyUUKE&amp;quot;,(S =1092 count(ir;(si)) (1)
count(;rj (Si)) calculates how many users input
S, with segmentation . A text (namely a
user) also has a score:
( Si
) text
( Si
) text
SCOREtext
num
j
</equation>
<bodyText confidence="0.97429875">
j(Si)text is the number of sentences in this
text.
This score helps us to identify high-quality
NTAs texts from all acceptable-quality ones.
</bodyText>
<sectionHeader confidence="0.99942" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.978534">
5.1 Identification of High-quality NTAs
Texts
</subsectionHeader>
<bodyText confidence="0.9999342">
In this experiment, we verify the effectiveness of
classifier and voting mechanism on locating
high-quality NTAs texts from 384 collected ones.
You can download part of our collected texts
from https://github.com/dakuiz/NTAs.
</bodyText>
<subsubsectionHeader confidence="0.895297">
5.1.1 The Classification Experiment
</subsubsectionHeader>
<bodyText confidence="0.999516857142857">
We randomly select 32 NTAs texts that contain
1,089 sentences, and then manually label them to
form training set. Taking S, mentioned in 4.1 as
an example, the manual-labeled training data are
shown in table 1. The label 1 and 0 represent ac-
ceptable-quality and low-quality NTAs sentence
separately.
</bodyText>
<table confidence="0.9963225">
Len SN SSN MCSSN MSL label
7rOO 16 8 2 1 3 1
;r2(s,) 16 11 6 3 2 1
;r3 (SO 16 5 2 1 5 0
,T,, (SO 16 2 0 0 11 0
;rs (SO 16 15 14 12 2 0
</table>
<tableCaption confidence="0.999651">
Table 1: examples of training data for classifier.
</tableCaption>
<bodyText confidence="0.999092363636364">
Package of libSVM (Chang and Lin, 2011) is
used here. Radial basis function is adopted as the
kernel function where gamma value is set to
1/num_features and cost value is 1.
10-fold cross validation is used to validate the
results. The 1,089 sentences are partitioned into
ten parts randomly. Ten runs are performed with
each run using a different part as the testing set.
It is conducted ten times and every part should
be testing set once. Classification accuracy of the
experiment is listed in the table 2.
</bodyText>
<figure confidence="0.969041333333333">
Num Accuracy(%)
1 96.33
2 97.22
3 97.25
4 97.25
5 89.91
6 98.17
7 94.50
8 94.59
9 94.55
10 98.11
Average 95.79
</figure>
<tableCaption confidence="0.978553">
Table 2: 10-fold cross validation results.
</tableCaption>
<bodyText confidence="0.999686857142857">
Since the results indicate the validity of our
classification approach, we use this classifier to
handle collected NTAs texts. If 85% of sentences
in a text are acceptable-quality, we select this
text as acceptable-quality NTAs text. Finally, we
obtain 211 acceptable-quality NTAs texts from
all 384 collected ones.
</bodyText>
<subsubsectionHeader confidence="0.877552">
5.1.2 The Voting Experiment
</subsubsectionHeader>
<bodyText confidence="0.999714333333333">
According to voting mechanism in section 4.3,
every acceptable-quality NTAs text can get a
score to rank itself. Table3 shows top three high-
quality NTAs texts with their user-produced
word segmentation results compared with that of
CRF+MSR. Because CRF+MSR is a general-
</bodyText>
<equation confidence="0.988525">
 log2 count(j (Si))
j (2)
</equation>
<page confidence="0.52541">
665
</page>
<bodyText confidence="0.999486">
purpose segmenter and test data does not come
from news wire, its performance drops signifi-
cantly in out-of-domain.
Table 3 suggests that high-quality NTAs texts
are very close to gold standard of word segmen-
tation. To discover the causes of errors, we man-
ually inspected these three texts and found the
major error is adhesive phenomenon between
simple words. For example, gold segmentation
“|这|几|款|” is formed as “|这几款|” by users.
This is an error in word segmentation competi-
tion, but in some application scenarios, like ma-
chine translation, “|这几款|”is better than “|这|几
|款|”. Similar phenomena shed light on under-
standing what a &amp;quot;word&amp;quot; really is.
</bodyText>
<table confidence="0.998613714285714">
Word seg- p r f rOOV
mentation
from
CRF+MSR 90.86 92.02 91.43 50.00
Text#top1 92.82 90.19 91.49 100.00
Text#top2 91.50 88.29 89.87 100.00
Text#top3 90.38 87.33 88.83 100.00
</table>
<tableCaption confidence="0.982063">
Table 3: Test text word segmentation results
from general-purpose segmenter and top 3 texts.
</tableCaption>
<note confidence="0.808199">
5.2 Effectiveness of High-quality NTAs
Corpus on Improving Word Segmenta-
</note>
<sectionHeader confidence="0.409832" genericHeader="evaluation">
tion
</sectionHeader>
<bodyText confidence="0.992075441176471">
It is generally agreed among researchers that us-
ers’ behavioral patterns maintain consistent over
a long period of time (Zhang et al., 2013;
Stephane, 2009). In table 3, we listed top 3 high-
quality NTAs texts. Users who generated these
three NTAs texts are stable sources to provide
more well-segmented texts.
To evaluate the effectiveness of high-quality
NTAs corpus on building training data for seg-
menter, we use a web crawler to get 40k Micro-
blog (weibo.com) corpus and randomly divided
it into 4 equal shares, i.e. A, B, C, T text. The
provider of top1 text is invited to retype A text to
produce A NTAs text. B and C NTAs texts are
separately obtained from other two providers.
We use A, B, C NTAs texts as training data to
get a CRF segmentation model, which is called
as CRF+NTAs. Then we train anther CRF seg-
menter with a combination of A, B, C NTAs
texts and the training corpus of MSR from
Bakeoff-2, called as CRF+MSR+NTAs. We
select 1,000 sentences from T text to manually
segment by gold standard, and use them to form
our test set that contains 6528 characters. The
results of the three segmenters on this Micro-
blog test set is shown in table 4.
The model directly trained by Micro-blog
high-quality NTAs corpus is better than general-
purpose segmenter but far from the model
trained by the combination of MSR and Micro-
blog high-quality NTAs corpus. This is the most
compelling evidence to show that high-quality
NTAs corpus can be used for improving word
segmentation model in out-of-domain.
</bodyText>
<table confidence="0.9984956">
Word segmenta- p r f
tion from
CRF+MSR 88.95 90.63 89.78
CRF+NTAs 92.38 89.76 91.05
CRF+MSR+NTAs 96.27 94.83 95.54
</table>
<tableCaption confidence="0.997586">
Table 4: Segmenters’ results on test data.
</tableCaption>
<bodyText confidence="0.999787333333333">
We also find out that the NTAs might be par-
ticularly useful to identify OOV words, such as
proper names and neo-logisms. If users frequent-
ly put some characters in one segment, this seg-
ment may be some new word or the new internet
slang, such as “白富美(white, rich and pretty) ”,
“萌萌哒(very cute)”, “十动然拒(someone is
moved but refuses to become girl/boyfriend)”,
etc.
</bodyText>
<sectionHeader confidence="0.985574" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.99997252631579">
In this paper, we investigate Natural Typing An-
notations (NTAs) that are potential word delimit-
ers generated by Chinese speakers while typing
Chinese words. The effectiveness of high-quality
NTAs corpus on improving word segmentation is
evaluated.
Though it is convenient for users to read, se-
quence of pure characters, namely without any
recorded delimiters produced by inputters, loses
lots of valuable information, e.g. NTAs. We
strongly recommend that NTAs can be recorded
in an invisible manner for normal users by domi-
nant text editors, such as MS Word, Notepad, vi,
emacs, etc.
In future, we will: 1) collect more NTAs texts
from various users; 2) do further work on how to
fully leverage NTAs to improve word segmenta-
tion; 3) call for dominant text editors to record
NTAs.
</bodyText>
<sectionHeader confidence="0.997139" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.989949125">
Sincere thanks to the three anonymous re-
viewers for their thorough reviewing and valua-
ble suggestions! The authors were supported by
the National Strategic Basic Research Program
(“973” Program) of the Ministry of Science and
Technology of China (No. 2012CB720702 and
2013CB329303) and National Science Founda-
tion of China (No. 61303105).
</bodyText>
<page confidence="0.88307">
666
</page>
<sectionHeader confidence="0.896786" genericHeader="references">
Reference
</sectionHeader>
<reference confidence="0.9998065">
Chih-Chung Chang, Chih-Jen Lin. 2011. LIBSVM: A
library for support vector machines. In TIST.
Yuan Chen. 1997.Chinese Language Processing.
Shanghai Education publishing company.
iRearch.2009. 2009 China Desktop Software Devel-
opment Research Report. http://report.iresearch.c-
n/1290.html.
Wenbin Jiang, Liang Huang, Qun Liu. 2009. Auto-
matic adaptation of annotation standards: Chinese
word segmentation and pos tagging – a case study.
In ACL-AFNLP.
John D. Lafferty, Andrew McCallum, Fernando C. N.
Pereira. 2001. Conditional random fields: Proba-
bilistic models for segmenting and labeling se-
quence data. In ICML.
Zhongguo Li, Maosong Sun. 2009. Punctuation as
Implicit Annotations for Chinese Word Segmenta-
tion. Computational Linguistics.
Wenbin Jiang, Meng Sun, Yajuan Lü, Yating Yang,
Qun Liu. 2013. Discriminative Learning with Nat-
ural Annotations: Word Segmentation as a Case
Study. In ACL.
Yang Liu, Yue Zhang. 2012. Unsupervised domain
adaptation for joint segmentation and POS-tagging.
In COLING.
Yijia Liu, Yue Zhang, Wanxiang Che, Ting Liu, Fan
Wu. 2014. Domain Adaptation for CRF-based
Chinese Word Segmentation using Free Annota-
tions. In EMNLP.
Xipeng Qiu, ChaoChao Huang, Xuanjing Huang.
2014. Automatic Corpus Expansion for Chinese
Word Segmentation by Exploiting the Redundancy
of Web Information. In COLING.
Weiwei Sun, Jia Xu. 2011. Enhancing chinese word
segmentation using unlabeled data. In EMNLP.
Lucas Stephane. 2009. User Behavior Patterns:
Gathering, Analysis, Simulation and Prediction. In
HCI.
Jingtao Wang, Shumin Zhai, Hui Su. 2001. Chinese
input with keyboard and eye-tracking: an anatomi-
cal study.In CHI.
Yiou Wang, Jun&apos;ichi Kazama, Yoshimasa Tsuru-
oka,Wenliang Chen, Yujie Zhang, Kentaro Torisa-
wa. 2011. Improving chinese word segmentation
and pos tagging with semi-supervised methods us-
ing large auto-analyzed data. In IJCNLP.
Nianwen Xue, Libin Shen. 2003. Chinese word seg-
mentation as lmr tagging. In SIGHAN.
Chunhong Zhang, Yaxi He, Yang Ji. 2013. Temporal
Pattern of User Behavior in Micro-blog. In JSW.
Yue Zhang, Stephen Clark. 2007. Chinese segmenta-
tion with a word-based perceptron algorithm. In
ACL.
Hai Zhao, Chang-Ning Huang, Mu Li, Bao-Liang Lu.
2010. A unified character-based tagging frame-
work for chinese word segmentation. In ACM.
Yabin Zheng, Lixing Xie, Zhiyuan Liu, Maosong Sun,
Yang zhang, Liyun Ru. 2011. Why Press Back-
space? Understanding User Input Behaviors in
Chinese Pinyin Input Method. In ACL.
</reference>
<page confidence="0.854898">
667
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.424711">
<title confidence="0.999253">The Discovery of Natural Typing User-produced Potential Chinese Word Delimiters</title>
<author confidence="0.998014">Yu Yang Hanshi Chuyuan Shiping</author>
<affiliation confidence="0.7978195">Institute of Technology, Beijing, Normal University, Beijing, China</affiliation>
<email confidence="0.839544">sbirdge@gmail.com,simontangbit@bit.edu.cn</email>
<email confidence="0.839544">maoyubit@gmail.com,simontangbit@bit.edu.cn</email>
<email confidence="0.839544">yan9liu@gmail.com,simontangbit@bit.edu.cn</email>
<email confidence="0.839544">necrostone@gmail.com,simontangbit@bit.edu.cn</email>
<email confidence="0.839544">weichuyuan@gmail.com,simontangbit@bit.edu.cn</email>
<abstract confidence="0.99641225">Human labeled corpus is indispensable for the training of supervised word segmenters. However, it is time-consuming and laborintensive to label corpus manually. During the process of typing Chinese text by Pingyin, people usually need to type &amp;quot;space&amp;quot; or numeric keys to choose the words due to homophones, which can be viewed as a cue for segmentation. We argue that such a process can be used to build a labeled corpus in a more natural way. Thus, in this paper, we investigate Natural Typing Annotations (NTAs) that are potential word delimiters produced by users while typing Chinese. A detailed analysis on over three hundred user-produced texts containing NTAs reveals that highquality NTAs mostly agree with gold segmentation and, consequently, can be used for improving the performance of supervised word segmentation model in out-of-domain. Experiments show that a classification model combined with a voting mechanism can reliably identify the high-quality NTAs texts that are more readily available labeled corpus. Furthermore, the NTAs might be particularly useful to deal with out-of-vocabulary (OOV) words such as proper names and neo-logisms.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<booktitle>In TIST.</booktitle>
<contexts>
<context position="15341" citStr="Chang and Lin, 2011" startWordPosition="2492" endWordPosition="2495">f our collected texts from https://github.com/dakuiz/NTAs. 5.1.1 The Classification Experiment We randomly select 32 NTAs texts that contain 1,089 sentences, and then manually label them to form training set. Taking S, mentioned in 4.1 as an example, the manual-labeled training data are shown in table 1. The label 1 and 0 represent acceptable-quality and low-quality NTAs sentence separately. Len SN SSN MCSSN MSL label 7rOO 16 8 2 1 3 1 ;r2(s,) 16 11 6 3 2 1 ;r3 (SO 16 5 2 1 5 0 ,T,, (SO 16 2 0 0 11 0 ;rs (SO 16 15 14 12 2 0 Table 1: examples of training data for classifier. Package of libSVM (Chang and Lin, 2011) is used here. Radial basis function is adopted as the kernel function where gamma value is set to 1/num_features and cost value is 1. 10-fold cross validation is used to validate the results. The 1,089 sentences are partitioned into ten parts randomly. Ten runs are performed with each run using a different part as the testing set. It is conducted ten times and every part should be testing set once. Classification accuracy of the experiment is listed in the table 2. Num Accuracy(%) 1 96.33 2 97.22 3 97.25 4 97.25 5 89.91 6 98.17 7 94.50 8 94.59 9 94.55 10 98.11 Average 95.79 Table 2: 10-fold c</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang, Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. In TIST.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Yuan Chen</author>
</authors>
<title>1997.Chinese Language Processing. Shanghai Education publishing company.</title>
<marker>Chen, </marker>
<rawString>Yuan Chen. 1997.Chinese Language Processing. Shanghai Education publishing company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>iRearch 2009</author>
</authors>
<date>2009</date>
<institution>China Desktop Software Development</institution>
<note>Research Report. http://report.iresearch.cn/1290.html.</note>
<contexts>
<context position="3598" citStr="(2009)" startWordPosition="557" endWordPosition="557">;) to improve segmentation. But none of them notice the segmentation information produced by users while typing Chinese. Chinese is unique due to its logographic writing system. Chinese users cannot directly type in Chinese words using a QWERTY keyboard. Input methods have been proposed to assist users to type in Chinese words (Chen, 1997). Substantial information has been produced, but not recorded and stored during text typing process. Figure 1: Typical Chinese Pinyin input method (Sogou-Pinyin). The typical way to type in Chinese words is in a sequential manner (Wang et al., 2001). iRearch (2009) showed that Pinyin input methods have the biggest share of Chinese speakers. We take one of them for example. Suppose users want to type in Chinese word “今 天 (today)”. Firstly, they mentally generate and physically type in corresponding Pinyin “jintian”. Then, a Chinese Pinyin input method displays a list of Chinese homophones, as shown in Figure 1. Finally, users visually search the target word from candidates and select numeric key, e.g. &apos;1&apos;-&apos;9&apos;(&lt;NUM#1&gt;- &lt;NUM#9&gt;) or space key (&lt;SPACE&gt;, a shortcut 662 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and</context>
</contexts>
<marker>2009, 2009</marker>
<rawString>iRearch.2009. 2009 China Desktop Software Development Research Report. http://report.iresearch.cn/1290.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
<author>Liang Huang</author>
<author>Qun Liu</author>
</authors>
<title>Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study.</title>
<date>2009</date>
<booktitle>In ACL-AFNLP.</booktitle>
<contexts>
<context position="2147" citStr="Jiang et al., 2009" startWordPosition="315" endWordPosition="318">ike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and stored as strings of Chinese characters without similar natural delimiters. To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-spe</context>
</contexts>
<marker>Jiang, Huang, Liu, 2009</marker>
<rawString>Wenbin Jiang, Liang Huang, Qun Liu. 2009. Automatic adaptation of annotation standards: Chinese word segmentation and pos tagging – a case study. In ACL-AFNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John D Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando C N Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In ICML.</booktitle>
<contexts>
<context position="7373" citStr="Lafferty et al., 2001" startWordPosition="1155" endWordPosition="1158"> in section 5, we come to the conclusion and the implication of future work. 2 Gold Standard and Baseline segmenter There are many different standards for word segmentation, and different tasks usually need different standards. The Sighan Bakeoff uses four well-known standards made by four different organizations: Academia Sinica (AS), City University of Hong Kong (CU), Peking University (PKU), and Microsoft Research (MSR). In this study, we take MSR segmentation standard as gold standard. Following the work of Zhao et al. (2010) and Sun and Xu (2011), a Conditional Random Fields (CRF) model (Lafferty et al., 2001) is trained with the training corpus of MSR from Sighan Bakeoff-2, to be a baseline segmenter. This general-purpose segmenter is called as CRF+MSR in this paper. 3 Iatural Typing Annotations Texts 3.1 Formulation A Chinese sentence is represented as S = c1c2 ... c7 ( stands for a Chinese character, I is the length of sentence S ). One of the possible sequences with selection keys is defined as )r(S) _1 cl ... c,_1 I c, ...cj2_1 I ... I c„l...cl J .Here, we use the symbol “|” instead of each selection key. “|” is the “Iatural Typing Annotation (ITA)”, which is naturally annotated by users when </context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John D. Lafferty, Andrew McCallum, Fernando C. N. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In ICML.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongguo Li</author>
<author>Maosong Sun</author>
</authors>
<title>Punctuation as Implicit Annotations for Chinese Word Segmentation. Computational Linguistics.</title>
<date>2009</date>
<contexts>
<context position="2884" citStr="Li and Sun, 2009" startWordPosition="433" endWordPosition="436">he performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia. There has been work on making use of both unlabeled data (Li and Sun, 2009; Sun and Xu, 2011; Wang et al., 2011; Qiu et al., 2014) and Wikipedia (Jiang et al., 2013; Liu et al., 2014;) to improve segmentation. But none of them notice the segmentation information produced by users while typing Chinese. Chinese is unique due to its logographic writing system. Chinese users cannot directly type in Chinese words using a QWERTY keyboard. Input methods have been proposed to assist users to type in Chinese words (Chen, 1997). Substantial information has been produced, but not recorded and stored during text typing process. Figure 1: Typical Chinese Pinyin input method (Sog</context>
</contexts>
<marker>Li, Sun, 2009</marker>
<rawString>Zhongguo Li, Maosong Sun. 2009. Punctuation as Implicit Annotations for Chinese Word Segmentation. Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wenbin Jiang</author>
</authors>
<title>Meng Sun, Yajuan Lü, Yating Yang, Qun Liu.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<marker>Jiang, 2013</marker>
<rawString>Wenbin Jiang, Meng Sun, Yajuan Lü, Yating Yang, Qun Liu. 2013. Discriminative Learning with Natural Annotations: Word Segmentation as a Case Study. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Yue Zhang</author>
</authors>
<title>Unsupervised domain adaptation for joint segmentation and POS-tagging.</title>
<date>2012</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2411" citStr="Liu and Zhang, 2012" startWordPosition="359" endWordPosition="362">segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia. There has been work on making use of both unlabeled data (Li and Sun, 2009; Sun and Xu, 2011; Wang et al., 2011; Qiu et al., 2014) and Wikipedia (Jiang et al., 2013; Liu et al., 2014;) to improve segme</context>
</contexts>
<marker>Liu, Zhang, 2012</marker>
<rawString>Yang Liu, Yue Zhang. 2012. Unsupervised domain adaptation for joint segmentation and POS-tagging. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yijia Liu</author>
<author>Yue Zhang</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
<author>Fan Wu</author>
</authors>
<title>Domain Adaptation for CRF-based Chinese Word Segmentation using Free Annotations.</title>
<date>2014</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2992" citStr="Liu et al., 2014" startWordPosition="455" endWordPosition="458">net literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web pages such as Wikipedia. There has been work on making use of both unlabeled data (Li and Sun, 2009; Sun and Xu, 2011; Wang et al., 2011; Qiu et al., 2014) and Wikipedia (Jiang et al., 2013; Liu et al., 2014;) to improve segmentation. But none of them notice the segmentation information produced by users while typing Chinese. Chinese is unique due to its logographic writing system. Chinese users cannot directly type in Chinese words using a QWERTY keyboard. Input methods have been proposed to assist users to type in Chinese words (Chen, 1997). Substantial information has been produced, but not recorded and stored during text typing process. Figure 1: Typical Chinese Pinyin input method (Sogou-Pinyin). The typical way to type in Chinese words is in a sequential manner (Wang et al., 2001). iRearch </context>
</contexts>
<marker>Liu, Zhang, Che, Liu, Wu, 2014</marker>
<rawString>Yijia Liu, Yue Zhang, Wanxiang Che, Ting Liu, Fan Wu. 2014. Domain Adaptation for CRF-based Chinese Word Segmentation using Free Annotations. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xipeng Qiu</author>
</authors>
<title>ChaoChao Huang, Xuanjing Huang.</title>
<date>2014</date>
<booktitle>In COLING.</booktitle>
<marker>Qiu, 2014</marker>
<rawString>Xipeng Qiu, ChaoChao Huang, Xuanjing Huang. 2014. Automatic Corpus Expansion for Chinese Word Segmentation by Exploiting the Redundancy of Web Information. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Weiwei Sun</author>
<author>Jia Xu</author>
</authors>
<title>Enhancing chinese word segmentation using unlabeled data.</title>
<date>2011</date>
<booktitle>In EMNLP.</booktitle>
<contexts>
<context position="2185" citStr="Sun and Xu, 2011" startWordPosition="323" endWordPosition="326"> sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and stored as strings of Chinese characters without similar natural delimiters. To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and semi-annotated web </context>
<context position="7308" citStr="Sun and Xu (2011)" startWordPosition="1145" endWordPosition="1148">section 4.After giving the experimental results and analysis in section 5, we come to the conclusion and the implication of future work. 2 Gold Standard and Baseline segmenter There are many different standards for word segmentation, and different tasks usually need different standards. The Sighan Bakeoff uses four well-known standards made by four different organizations: Academia Sinica (AS), City University of Hong Kong (CU), Peking University (PKU), and Microsoft Research (MSR). In this study, we take MSR segmentation standard as gold standard. Following the work of Zhao et al. (2010) and Sun and Xu (2011), a Conditional Random Fields (CRF) model (Lafferty et al., 2001) is trained with the training corpus of MSR from Sighan Bakeoff-2, to be a baseline segmenter. This general-purpose segmenter is called as CRF+MSR in this paper. 3 Iatural Typing Annotations Texts 3.1 Formulation A Chinese sentence is represented as S = c1c2 ... c7 ( stands for a Chinese character, I is the length of sentence S ). One of the possible sequences with selection keys is defined as )r(S) _1 cl ... c,_1 I c, ...cj2_1 I ... I c„l...cl J .Here, we use the symbol “|” instead of each selection key. “|” is the “Iatural Typi</context>
</contexts>
<marker>Sun, Xu, 2011</marker>
<rawString>Weiwei Sun, Jia Xu. 2011. Enhancing chinese word segmentation using unlabeled data. In EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucas Stephane</author>
</authors>
<title>User Behavior Patterns: Gathering, Analysis, Simulation and Prediction.</title>
<date>2009</date>
<booktitle>In HCI.</booktitle>
<contexts>
<context position="17761" citStr="Stephane, 2009" startWordPosition="2889" endWordPosition="2890">anslation, “|这几款|”is better than “|这|几 |款|”. Similar phenomena shed light on understanding what a &amp;quot;word&amp;quot; really is. Word seg- p r f rOOV mentation from CRF+MSR 90.86 92.02 91.43 50.00 Text#top1 92.82 90.19 91.49 100.00 Text#top2 91.50 88.29 89.87 100.00 Text#top3 90.38 87.33 88.83 100.00 Table 3: Test text word segmentation results from general-purpose segmenter and top 3 texts. 5.2 Effectiveness of High-quality NTAs Corpus on Improving Word Segmentation It is generally agreed among researchers that users’ behavioral patterns maintain consistent over a long period of time (Zhang et al., 2013; Stephane, 2009). In table 3, we listed top 3 highquality NTAs texts. Users who generated these three NTAs texts are stable sources to provide more well-segmented texts. To evaluate the effectiveness of high-quality NTAs corpus on building training data for segmenter, we use a web crawler to get 40k Microblog (weibo.com) corpus and randomly divided it into 4 equal shares, i.e. A, B, C, T text. The provider of top1 text is invited to retype A text to produce A NTAs text. B and C NTAs texts are separately obtained from other two providers. We use A, B, C NTAs texts as training data to get a CRF segmentation mod</context>
</contexts>
<marker>Stephane, 2009</marker>
<rawString>Lucas Stephane. 2009. User Behavior Patterns: Gathering, Analysis, Simulation and Prediction. In HCI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingtao Wang</author>
<author>Shumin Zhai</author>
<author>Hui Su</author>
</authors>
<title>Chinese input with keyboard and eye-tracking: an anatomical study.In CHI.</title>
<date>2001</date>
<contexts>
<context position="3582" citStr="Wang et al., 2001" startWordPosition="552" endWordPosition="555"> al., 2013; Liu et al., 2014;) to improve segmentation. But none of them notice the segmentation information produced by users while typing Chinese. Chinese is unique due to its logographic writing system. Chinese users cannot directly type in Chinese words using a QWERTY keyboard. Input methods have been proposed to assist users to type in Chinese words (Chen, 1997). Substantial information has been produced, but not recorded and stored during text typing process. Figure 1: Typical Chinese Pinyin input method (Sogou-Pinyin). The typical way to type in Chinese words is in a sequential manner (Wang et al., 2001). iRearch (2009) showed that Pinyin input methods have the biggest share of Chinese speakers. We take one of them for example. Suppose users want to type in Chinese word “今 天 (today)”. Firstly, they mentally generate and physically type in corresponding Pinyin “jintian”. Then, a Chinese Pinyin input method displays a list of Chinese homophones, as shown in Figure 1. Finally, users visually search the target word from candidates and select numeric key, e.g. &apos;1&apos;-&apos;9&apos;(&lt;NUM#1&gt;- &lt;NUM#9&gt;) or space key (&lt;SPACE&gt;, a shortcut 662 Proceedings of the 53rd Annual Meeting of the Association for Computational</context>
</contexts>
<marker>Wang, Zhai, Su, 2001</marker>
<rawString>Jingtao Wang, Shumin Zhai, Hui Su. 2001. Chinese input with keyboard and eye-tracking: an anatomical study.In CHI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yiou Wang</author>
<author>Jun&apos;ichi Kazama</author>
</authors>
<title>Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, Kentaro Torisawa.</title>
<date>2011</date>
<booktitle>In IJCNLP.</booktitle>
<marker>Wang, Kazama, 2011</marker>
<rawString>Yiou Wang, Jun&apos;ichi Kazama, Yoshimasa Tsuruoka,Wenliang Chen, Yujie Zhang, Kentaro Torisawa. 2011. Improving chinese word segmentation and pos tagging with semi-supervised methods using large auto-analyzed data. In IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nianwen Xue</author>
<author>Libin Shen</author>
</authors>
<title>Chinese word segmentation as lmr tagging.</title>
<date>2003</date>
<booktitle>In SIGHAN.</booktitle>
<contexts>
<context position="2104" citStr="Xue and Shen, 2003" startWordPosition="307" endWordPosition="310">r names and neo-logisms. 1 Introduction Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and stored as strings of Chinese characters without similar natural delimiters. To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, inclu</context>
</contexts>
<marker>Xue, Shen, 2003</marker>
<rawString>Nianwen Xue, Libin Shen. 2003. Chinese word segmentation as lmr tagging. In SIGHAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chunhong Zhang</author>
<author>Yaxi He</author>
<author>Yang Ji</author>
</authors>
<date>2013</date>
<booktitle>Temporal Pattern of User Behavior in Micro-blog. In JSW.</booktitle>
<contexts>
<context position="17744" citStr="Zhang et al., 2013" startWordPosition="2885" endWordPosition="2888">ios, like machine translation, “|这几款|”is better than “|这|几 |款|”. Similar phenomena shed light on understanding what a &amp;quot;word&amp;quot; really is. Word seg- p r f rOOV mentation from CRF+MSR 90.86 92.02 91.43 50.00 Text#top1 92.82 90.19 91.49 100.00 Text#top2 91.50 88.29 89.87 100.00 Text#top3 90.38 87.33 88.83 100.00 Table 3: Test text word segmentation results from general-purpose segmenter and top 3 texts. 5.2 Effectiveness of High-quality NTAs Corpus on Improving Word Segmentation It is generally agreed among researchers that users’ behavioral patterns maintain consistent over a long period of time (Zhang et al., 2013; Stephane, 2009). In table 3, we listed top 3 highquality NTAs texts. Users who generated these three NTAs texts are stable sources to provide more well-segmented texts. To evaluate the effectiveness of high-quality NTAs corpus on building training data for segmenter, we use a web crawler to get 40k Microblog (weibo.com) corpus and randomly divided it into 4 equal shares, i.e. A, B, C, T text. The provider of top1 text is invited to retype A text to produce A NTAs text. B and C NTAs texts are separately obtained from other two providers. We use A, B, C NTAs texts as training data to get a CRF</context>
</contexts>
<marker>Zhang, He, Ji, 2013</marker>
<rawString>Chunhong Zhang, Yaxi He, Yang Ji. 2013. Temporal Pattern of User Behavior in Micro-blog. In JSW.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yue Zhang</author>
<author>Stephen Clark</author>
</authors>
<title>Chinese segmentation with a word-based perceptron algorithm.</title>
<date>2007</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="2127" citStr="Zhang and Clark, 2007" startWordPosition="311" endWordPosition="314">sms. 1 Introduction Unlike English text in which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and stored as strings of Chinese characters without similar natural delimiters. To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabe</context>
</contexts>
<marker>Zhang, Clark, 2007</marker>
<rawString>Yue Zhang, Stephen Clark. 2007. Chinese segmentation with a word-based perceptron algorithm. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hai Zhao</author>
<author>Chang-Ning Huang</author>
<author>Mu Li</author>
<author>Bao-Liang Lu</author>
</authors>
<title>A unified character-based tagging framework for chinese word segmentation.</title>
<date>2010</date>
<booktitle>In ACM.</booktitle>
<contexts>
<context position="2166" citStr="Zhao et al., 2010" startWordPosition="319" endWordPosition="322">which sentences are sequences of words delimited by white spaces, in Chinese text, sentences are usually represented and stored as strings of Chinese characters without similar natural delimiters. To find the basic language units, i.e. words, segmentation is a necessary initial step for Chinese language processing. Currently most of state-of-the-art methods for Chinese word segmentation (CWS) are based on supervised learning, which depend on large scale annotated corpus. These supervised methods obtain high accuracies on newswire (Xue and Shen, 2003; Zhang and Clark, 2007; Jiang et al., 2009; Zhao et al., 2010; Sun and Xu, 2011). However, manually annotated training data mostly come from the news domain, and the performance can drop severely when the test data shift from newswire to blogs, computer forums, and Internet literature (Liu and Zhang, 2012;).Supervised approaches often have a high requirement on the quality and quantity of annotated corpus, which is always not easy to build. As a result, many previous methods utilize the information of free data which contain limited but useful segmentation information over the Internet, including large-scale unlabeled data, domain-specific lexicons and </context>
<context position="7286" citStr="Zhao et al. (2010)" startWordPosition="1140" endWordPosition="1143">-quality NTAs texts in section 4.After giving the experimental results and analysis in section 5, we come to the conclusion and the implication of future work. 2 Gold Standard and Baseline segmenter There are many different standards for word segmentation, and different tasks usually need different standards. The Sighan Bakeoff uses four well-known standards made by four different organizations: Academia Sinica (AS), City University of Hong Kong (CU), Peking University (PKU), and Microsoft Research (MSR). In this study, we take MSR segmentation standard as gold standard. Following the work of Zhao et al. (2010) and Sun and Xu (2011), a Conditional Random Fields (CRF) model (Lafferty et al., 2001) is trained with the training corpus of MSR from Sighan Bakeoff-2, to be a baseline segmenter. This general-purpose segmenter is called as CRF+MSR in this paper. 3 Iatural Typing Annotations Texts 3.1 Formulation A Chinese sentence is represented as S = c1c2 ... c7 ( stands for a Chinese character, I is the length of sentence S ). One of the possible sequences with selection keys is defined as )r(S) _1 cl ... c,_1 I c, ...cj2_1 I ... I c„l...cl J .Here, we use the symbol “|” instead of each selection key. “|</context>
</contexts>
<marker>Zhao, Huang, Li, Lu, 2010</marker>
<rawString>Hai Zhao, Chang-Ning Huang, Mu Li, Bao-Liang Lu. 2010. A unified character-based tagging framework for chinese word segmentation. In ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yabin Zheng</author>
</authors>
<title>Lixing Xie, Zhiyuan Liu, Maosong Sun, Yang zhang, Liyun Ru.</title>
<date>2011</date>
<booktitle>In ACL.</booktitle>
<marker>Zheng, 2011</marker>
<rawString>Yabin Zheng, Lixing Xie, Zhiyuan Liu, Maosong Sun, Yang zhang, Liyun Ru. 2011. Why Press Backspace? Understanding User Input Behaviors in Chinese Pinyin Input Method. In ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>