<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000289">
<title confidence="0.995549">
Bilingually-constrained Phrase Embeddings for Machine Translation
</title>
<author confidence="0.997783">
Jiajun Zhang1, Shujie Liu2, Mu Li2, Ming Zhou2 and Chengqing Zong1
</author>
<affiliation confidence="0.984485">
1National Laboratory of Pattern Recognition, CASIA, Beijing, P.R. China
</affiliation>
<email confidence="0.930834">
{jjzhang,cqzong}@nlpr.ia.ac.cn
</email>
<affiliation confidence="0.854565">
2Microsoft Research Asia, Beijing, P.R. China
</affiliation>
<email confidence="0.994206">
{shujliu,muli,mingzhou}@microsoft.com
</email>
<sectionHeader confidence="0.993796" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999979818181818">
We propose Bilingually-constrained Re-
cursive Auto-encoders (BRAE) to learn
semantic phrase embeddings (compact
vector representations for phrases), which
can distinguish the phrases with differ-
ent semantic meanings. The BRAE is
trained in a way that minimizes the seman-
tic distance of translation equivalents and
maximizes the semantic distance of non-
translation pairs simultaneously. After
training, the model learns how to embed
each phrase semantically in two languages
and also learns how to transform semantic
embedding space in one language to the
other. We evaluate our proposed method
on two end-to-end SMT tasks (phrase ta-
ble pruning and decoding with phrasal se-
mantic similarities) which need to mea-
sure semantic similarity between a source
phrase and its translation candidates. Ex-
tensive experiments show that the BRAE
is remarkably effective in these two tasks.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999985140350877">
Due to the powerful capacity of feature learn-
ing and representation, Deep (multi-layer) Neural
Networks (DNN) have achieved a great success in
speech and image processing (Kavukcuoglu et al.,
2010; Krizhevsky et al., 2012; Dahl et al., 2012).
Recently, statistical machine translation (SMT)
community has seen a strong interest in adapting
and applying DNN to many tasks, such as word
alignment (Yang et al., 2013), translation confi-
dence estimation (Mikolov et al., 2010; Liu et al.,
2013; Zou et al., 2013), phrase reordering predic-
tion (Li et al., 2013), translation modelling (Auli et
al., 2013; Kalchbrenner and Blunsom, 2013) and
language modelling (Duh et al., 2013; Vaswani et
al., 2013). Most of these works attempt to im-
prove some components in SMT based on word
embedding, which converts a word into a dense,
low dimensional, real-valued vector representation
(Bengio et al., 2003; Bengio et al., 2006; Collobert
and Weston, 2008; Mikolov et al., 2013).
However, in the conventional (phrase-based)
SMT, phrases are the basic translation units. The
models using word embeddings as the direct in-
puts to DNN cannot make full use of the whole
syntactic and semantic information of the phrasal
translation rules. Therefore, in order to success-
fully apply DNN to model the whole translation
process, such as modelling the decoding process,
learning compact vector representations for the ba-
sic phrasal translation units is the essential and
fundamental work.
In this paper, we explore the phrase embedding,
which represents a phrase (sequence of words)
with a real-valued vector. In some previous works,
phrase embedding has been discussed from differ-
ent views. Socher et al. (2011) make the phrase
embeddings capture the sentiment information.
Socher et al. (2013a) enable the phrase embed-
dings to mainly capture the syntactic knowledge.
Li et al. (2013) attempt to encode the reordering
pattern in the phrase embeddings. Kalchbrenner
and Blunsom (2013) utilize a simple convolution
model to generate phrase embeddings from word
embeddings. Mikolov et al. (2013) consider a
phrase as an indivisible n-gram. Obviously, these
methods of learning phrase embeddings either fo-
cus on some aspects of the phrase (e.g. reordering
pattern), or impose strong assumptions (e.g. bag-
of-words or indivisible n-gram). Therefore, these
phrase embeddings are not suitable to fully repre-
sent the phrasal translation units in SMT due to the
lack of semantic meanings of the phrase.
Instead, we focus on learning phrase embed-
dings from the view of semantic meaning, so
that our phrase embedding can fully represent the
phrase and best fit the phrase-based SMT. As-
suming the phrase is a meaningful composition
</bodyText>
<page confidence="0.983414">
111
</page>
<note confidence="0.830307">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 111–121,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.999453326530612">
of its internal words, we propose Bilingually-
constrained Recursive Auto-encoders (BRAE) to
learn semantic phrase embeddings. The core idea
behind is that a phrase and its correct translation
should share the same semantic meaning. Thus,
they can supervise each other to learn their seman-
tic phrase embeddings. Similarly, non-translation
pairs should have different semantic meanings,
and this information can also be used to guide
learning semantic phrase embeddings.
In our method, the standard recursive auto-
encoder (RAE) pre-trains the phrase embedding
with an unsupervised algorithm by minimizing the
reconstruction error (Socher et al., 2010), while
the bilingually-constrained model learns to fine-
tune the phrase embedding by minimizing the se-
mantic distance between translation equivalents
and maximizing the semantic distance between
non-translation pairs.
We use an example to explain our model. As
illustrated in Fig. 1, the Chinese phrase on the
left and the English phrase on the right are trans-
lations with each other. If we learn the embedding
of the Chinese phrase correctly, we can regard it
as the gold representation for the English phrase
and use it to guide the process of learning English
phrase embedding. In the other direction, the Chi-
nese phrase embedding can be learned in the same
way. This procedure can be performed with an
co-training style algorithm so as to minimize the
semantic distance between the translation equiva-
lents 1. In this way, the result Chinese and English
phrase embeddings will capture the semantics as
much as possible. Furthermore, a transformation
function between the Chinese and English seman-
tic spaces can be learned as well.
With the learned model, we can accurately mea-
sure the semantic similarity between a source
phrase and a translation candidate. Accordingly,
we evaluate the BRAE model on two end-to-
end SMT tasks (phrase table pruning and decod-
ing with phrasal semantic similarities) which need
to check whether a translation candidate and the
source phrase are in the same meaning. In phrase
table pruning, we discard the phrasal translation
rules with low semantic similarity. In decoding
with phrasal semantic similarities, we apply the
semantic similarities of the phrase pairs as new
features during decoding to guide translation can-
</bodyText>
<footnote confidence="0.995903">
1For simplicity, we do not show non-translation pairs
here.
</footnote>
<figureCaption confidence="0.9476405">
Figure 1: A motivation example for the BRAE
model.
</figureCaption>
<bodyText confidence="0.999501466666667">
didate selection. The experiments show that up to
72% of the phrase table can be discarded without
significant decrease on the translation quality, and
in decoding with phrasal semantic similarities up
to 1.7 BLEU score improvement over the state-of-
the-art baseline can be achieved.
In addition, our semantic phrase embeddings
have many other potential applications. For in-
stance, the semantic phrase embeddings can be
directly fed to DNN to model the decoding pro-
cess. Besides SMT, the semantic phrase embed-
dings can be used in other cross-lingual tasks (e.g.
cross-lingual question answering) and monolin-
gual applications such as textual entailment, ques-
tion answering and paraphrase detection.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999941375">
Recently, phrase embedding has drawn more and
more attention. There are three main perspectives
handling this task in monolingual languages.
One method considers the phrases as bag-of-
words and employs a convolution model to trans-
form the word embeddings to phrase embeddings
(Collobert et al., 2011; Kalchbrenner and Blun-
som, 2013). Gao et al. (2013) also use bag-of-
words but learn BLEU sensitive phrase embed-
dings. This kind of approaches does not take the
word order into account and loses much informa-
tion. Instead, our bilingually-constrained recur-
sive auto-encoders not only learn the composition
mechanism of generating phrases from words, but
also fine tune the word embeddings during the
model training stage, so that we can induce the full
information of the phrases and internal words.
Another method (Mikolov et al., 2013) deals
with the phrases having a meaning that is not a
simple composition of the meanings of its indi-
vidual words, such as New York Times. They first
find the phrases of this kind. Then, they regard
these phrases as indivisible units, and learn their
embeddings with the context information. How-
</bodyText>
<figure confidence="0.7406614">
MM &apos;fa 931VT France and Russia
source phrase
embedding ps
target phrase
embedding pt
</figure>
<page confidence="0.996248">
112
</page>
<bodyText confidence="0.999976914285714">
ever, this kind of phrase embedding is hard to cap-
ture full semantics since the context of a phrase
is limited. Furthermore, this method can only ac-
count for a very small part of phrases, since most
of the phrases are compositional. In contrast, our
method attempts to learn the semantic vector rep-
resentation for any phrase.
The third method views any phrase as the mean-
ingful composition of its internal words. The re-
cursive auto-encoder is typically adopted to learn
the way of composition (Socher et al., 2010;
Socher et al., 2011; Socher et al., 2013a; Socher
et al., 2013b; Li et al., 2013). They pre-train the
RAE with an unsupervised algorithm. And then,
they fine-tune the RAE according to the label of
the phrase, such as the syntactic category in pars-
ing (Socher et al., 2013a), the polarity in sentiment
analysis (Socher et al., 2011; Socher et al., 2013b),
and the reordering pattern in SMT (Li et al., 2013).
This kind of semi-supervised phrase embedding is
in fact performing phrase clustering with respect
to the phrase label. For example, in the RAE-
based phrase reordering model for SMT (Li et
al., 2013), the phrases with the similar reorder-
ing tendency (e.g. monotone or swap) are close
to each other in the embedding space, such as the
prepositional phrases. Obviously, this kind meth-
ods of semi-supervised phrase embedding do not
fully address the semantic meaning of the phrases.
Although we also follow the composition-based
phrase embedding, we are the first to focus on
the semantic meanings of the phrases and propose
a bilingually-constrained model to induce the se-
mantic information and learn transformation of the
semantic space in one language to the other.
</bodyText>
<sectionHeader confidence="0.9519095" genericHeader="method">
3 Bilingually-constrained Recursive
Auto-encoders
</sectionHeader>
<bodyText confidence="0.999337923076923">
This section introduces the Bilingually-
constrained Recursive Auto-encoders (BRAE),
that is inspired by two observations. First, the
recursive auto-encoder provides a reasonable
composition mechanism to embed each phrase.
And the semi-supervised phrase embedding
(Socher et al., 2011; Socher et al., 2013a; Li et
al., 2013) further indicates that phrase embedding
can be tuned with respect to the label. Second,
even though we have no correct semantic phrase
representation as the gold label, the phrases
sharing the same meaning provide an indirect but
feasible way.
</bodyText>
<figureCaption confidence="0.982448666666667">
Figure 2: A recursive auto-encoder for a four-
word phrase. The empty nodes are the reconstruc-
tions of the input.
</figureCaption>
<bodyText confidence="0.9999816">
We will first briefly present the unsupervised
phrase embedding, and then describe the semi-
supervised framework. After that, we introduce
the BRAE on the network structure, objective
function and parameter inference.
</bodyText>
<subsectionHeader confidence="0.999474">
3.1 Unsupervised Phrase Embedding
3.1.1 Word Vector Representations
</subsectionHeader>
<bodyText confidence="0.999386928571429">
In phrase embedding using composition, the word
vector representation is the basis and serves as the
input to the neural network. After learning word
embeddings with DNN (Bengio et al., 2003; Col-
lobert and Weston, 2008; Mikolov et al., 2013),
each word in the vocabulary V corresponds to a
vector x E Rn, and all the vectors are stacked into
an embedding matrix L E Rn×|V |.
Given a phrase which is an ordered list of m
words, each word has an index i into the columns
of the embedding matrix L. The index i is used to
retrieve the word’s vector representation using a
simple multiplication with a binary vector e which
is zero in all positions except for the ith index:
</bodyText>
<equation confidence="0.575063">
xi = Lei E Rn (1)
</equation>
<bodyText confidence="0.999556666666667">
Note that n is usually set empirically, such as n =
50,100, 200. Throughout this paper, n = 3 is used
for better illustration as shown in Fig. 1.
</bodyText>
<subsectionHeader confidence="0.783024">
3.1.2 RAE-based Phrase Embedding
</subsectionHeader>
<bodyText confidence="0.999805857142857">
Assuming we are given a phrase w1w2 · · · wm,
it is first projected into a list of vectors
(x1,x2, · · · , xm) using Eq. 1. The RAE learns
the vector representation of the phrase by recur-
sively combining two children vectors in a bottom-
up manner (Socher et al., 2011). Fig. 2 illustrates
an instance of a RAE applied to a binary tree, in
</bodyText>
<equation confidence="0.99917775">
y3=�(W1)[y2; x4]+b)
y2=�(W1)[y1; x3]+b)
y1=�(W1)[x1; x2]+b)
x1 x2 x3 x4
</equation>
<page confidence="0.982534">
113
</page>
<bodyText confidence="0.9999174">
which a standard auto-encoder (in box) is re-used
at each node. The standard auto-encoder aims at
learning an abstract representation of its input. For
two children c1 = x1 and c2 = x2, the auto-
encoder computes the parent vector y1 as follows:
</bodyText>
<equation confidence="0.998667">
p = f(W(1)[c1;c2] + b(1)) (2)
</equation>
<bodyText confidence="0.999980818181818">
Where we multiply the parameter matrix W (1) E
Rnx2n by the concatenation of two children
[c1; c2] E R2nx1. After adding a bias term b(1),
we apply an element-wise activation function such
as f = tanh(·), which is used in our experiments.
In order to apply this auto-encoder to each pair of
children, the representation of the parent p should
have the same dimensionality as the ci’s.
To assess how well the parent’s vector repre-
sents its children, the standard auto-encoder recon-
structs the children in a reconstruction layer:
</bodyText>
<equation confidence="0.930722">
[c&apos;1; c&apos;2] = f(2)(W (2)p + b(2)) (3)
</equation>
<bodyText confidence="0.996901285714286">
Where c&apos;1 and c&apos;2 are reconstructed children, W(2)
and b(2) are parameter matrix and bias term for re-
construction respectively, and f(2) = tanh(·).
To obtain the optimal abstract representation of
the inputs, the standard auto-encoder tries to min-
imize the reconstruction errors between the inputs
and the reconstructed ones during training:
</bodyText>
<equation confidence="0.997297">
1
Erec([c1; c2]) = 2||[c1; c2] − [c&apos;1; c&apos;2]||2 (4)
</equation>
<bodyText confidence="0.998395714285714">
Given y1 = p, we can use Eq. 2 again to com-
pute y2 by setting the children to be [c1; c2] =
[y1; x3]. The same auto-encoder is re-used until
the vector of the whole phrase is generated.
For unsupervised phrase embedding, the only
objective is to minimize the sum of reconstruction
errors at each node in the optimal binary tree:
</bodyText>
<equation confidence="0.979734">
1:
RAEθ(x) = argmin Erec([c1; c2]s) (5)
yEA(x) sEy
</equation>
<bodyText confidence="0.9978615">
Where x is the list of vectors of a phrase, and A(x)
denotes all the possible binary trees that can be
built from inputs x. A greedy algorithm (Socher
et al., 2011) is used to generate the optimal binary
tree y. The parameters 0 = (W, b) are optimized
over all the phrases in the training data.
</bodyText>
<subsectionHeader confidence="0.999171">
3.2 Semi-supervised Phrase Embedding
</subsectionHeader>
<bodyText confidence="0.999963">
The above RAE is completely unsupervised and
can only induce general representations of the
</bodyText>
<subsubsectionHeader confidence="0.895551">
Reconstruction Error Prediction Error
</subsubsectionHeader>
<figureCaption confidence="0.9971185">
Figure 3: An illustration of a semi-supervised
RAE unit. Red nodes show the label distribution.
</figureCaption>
<bodyText confidence="0.972699090909091">
multi-word phrases. Several researchers extend
the original RAEs to a semi-supervised setting so
that the induced phrase embedding can predict a
target label, such as polarity in sentiment analysis
(Socher et al., 2011), syntactic category in parsing
(Socher et al., 2013a) and phrase reordering pat-
tern in SMT (Li et al., 2013).
In the semi-supervised RAE for phrase embed-
ding, the objective function over a (phrase, label)
pair (x, t) includes the reconstruction error and the
prediction error, as illustrated in Fig. 3.
</bodyText>
<equation confidence="0.773337">
E(x, t; 0) = αErec(x, t; 0)+(1−α)Epred(x, t; 0)
(6)
</equation>
<bodyText confidence="0.999914166666667">
Where the hyper-parameter α is used to balance
the reconstruction and prediction error. For label
prediction, the cross-entropy error is usually used
to calculate Epred. By optimizing the above ob-
jective, the phrases in the vector embedding space
will be grouped according to the labels.
</bodyText>
<subsectionHeader confidence="0.989497">
3.3 The BRAE Model
</subsectionHeader>
<bodyText confidence="0.99955945">
We know from the semi-supervised phrase embed-
ding that the learned vector representation can be
well adapted to the given label. Therefore, we can
imagine that learning semantic phrase embedding
is reasonable if we are given gold vector represen-
tations of the phrases.
However, no gold semantic phrase embedding
exists. Fortunately, we know the fact that the
two phrases should share the same semantic rep-
resentation if they express the same meaning. We
can make inference from this fact that if a model
can learn the same embedding for any phrase pair
sharing the same meaning, the learned embedding
must encode the semantics of the phrases and the
corresponding model is our desire.
As translation equivalents share the same se-
mantic meaning, we employ high-quality phrase
translation pairs as training corpus in this
work. Accordingly, we propose the Bilingually-
constrained Recursive Auto-encoders (BRAE),
</bodyText>
<equation confidence="0.735588">
W(2) W(label)
W(1)
</equation>
<page confidence="0.771468">
114
</page>
<figure confidence="0.9882428">
Source Reconstruction Error Target Reconstruction Error
Ws(2)
Ws(1)
Ws(label)
Target Prediction Error
Source Prediction Error
Wt(label)
Wt(2)
Wt(1)
Source Language Phrase Target Language Phrase
</figure>
<bodyText confidence="0.821707">
way. For the phrase pair (s, t), the joint error is:
</bodyText>
<equation confidence="0.9271375">
E(s, t; θ) = αErec(s, t; θ) + (1 − α)Esem(s, t; θ)
(10)
</equation>
<bodyText confidence="0.991891333333333">
The hyper-parameter α weights the reconstruction
and semantic error. The final BRAE objective over
the phrase pairs training set (S, T) becomes:
</bodyText>
<figureCaption confidence="0.999557666666667">
Figure 4: An illustration of the bilingual- 1 λ
constrained recursive auto-encoders. The two JBRAE = N E(s, t; θ) + 2 ||θ||2 (11)
phrases are translations with each other. (s,t)E(S,T)
</figureCaption>
<bodyText confidence="0.994571">
whose basic goal is to minimize the semantic dis-
tance between the phrases and their translations.
</bodyText>
<subsectionHeader confidence="0.536006">
3.3.1 The Objective Function
</subsectionHeader>
<bodyText confidence="0.9998852">
Unlike previous methods, the BRAE model jointly
learns two RAEs (Fig. 4 shows the network struc-
ture): one for source language and the other for
target language. For a phrase pair (s, t), two kinds
of errors are involved:
</bodyText>
<listItem confidence="0.876939333333333">
1. reconstruction error Erec(s, t; θ): how well
the learned vector representations ps and pt repre-
sent the phrase s and t respectively?
</listItem>
<equation confidence="0.995266">
Erec(s, t; θ) = Erec(s; θ) + Erec(t; θ) (7)
</equation>
<bodyText confidence="0.951751846153846">
2. semantic error Esem(s, t; θ): what is the
semantic distance between the learned vector rep-
resentations ps and pt?
Since word embeddings for two languages are
learned separately and locate in different vector
space, we do not enforce the phrase embeddings
in two languages to be in the same semantic vector
space. We suppose there is a transformation be-
tween the two semantic embedding spaces. Thus,
the semantic distance is bidirectional: the distance
between pt and the transformation of ps, and that
between ps and the transformation of pt. As a re-
sult, the overall semantic error becomes:
</bodyText>
<equation confidence="0.999429">
Esem(s, t; θ) = Esem(s|t, θ) + Esem(t|s, θ) (8)
</equation>
<bodyText confidence="0.958951333333333">
Where Esem(s|t, θ) = Esem(pt, f(Wlsps + bls))
means the transformation of ps is performed as
follows: we first multiply a parameter matrix Wsl
by ps, and after adding a bias term bls we apply
an element-wise activation function f = tanh(·).
Finally, we calculate their Euclidean distance:
</bodyText>
<equation confidence="0.908390666666667">
Esem(s|t, θ) = 2||pt − f(W l
1 sps + bls)||2 (9)
Esem(t|s, θ) can be calculated in exactly the same
</equation>
<subsectionHeader confidence="0.801277">
3.3.2 Max-Semantic-Margin Error
</subsectionHeader>
<bodyText confidence="0.999993363636364">
Ideally, we want the learned BRAE model can
make sure that the semantic error for the positive
example (a source phrase s and its correct transla-
tion t) is much smaller than that for the negative
example (the source phrase s and a bad translation
t&apos;). However, the current model cannot guarantee
this since the above semantic error Esem(s|t, θ)
only accounts for positive ones.
We thus enhance the semantic error with both
positive and negative examples, and the corre-
sponding max-semantic-margin error becomes:
</bodyText>
<equation confidence="0.996266">
E∗sem(s|t, θ) = max10, Esem(s|t, θ)
− Esem(s|t&apos;, θ) + 11
(12)
</equation>
<bodyText confidence="0.9999954">
It tries to minimize the semantic distance between
translation equivalents and maximize the semantic
distance between non-translation pairs simultane-
ously. Using the above error function, we need
to construct a negative example for each positive
example. Suppose we are given a positive exam-
ple (s, t), the correct translation t can be converted
into a bad translation t&apos; by replacing the words
in t with randomly chosen target language words.
Then, a negative example (s, t&apos;) is available.
</bodyText>
<subsectionHeader confidence="0.613787">
3.3.3 Parameter Inference
</subsectionHeader>
<bodyText confidence="0.998594454545454">
Like semi-supervised RAE (Li et al., 2013), the
parameters θ in our BRAE model can also be di-
vided into three sets:
θL: word embedding matrix L for two lan-
guages (Section 3.1.1);
θrec: recursive auto-encoder parameter matrices
W (1), W(2), and bias terms b(1), b(2) for two lan-
guages (Section 3.1.2);
θsem: transformation matrix Wl and bias term
bl for two directions in semantic distance compu-
tation (Section 3.3.1).
</bodyText>
<page confidence="0.995772">
115
</page>
<bodyText confidence="0.9989015">
To have a deep understanding of the parameters,
we rewrite Eq. 10:
</bodyText>
<equation confidence="0.99993625">
E(s, t; θ) = α(Erec(s; θ) + Erec(t; θ))
+ (1 − α)(E∗ sem(s|t, θ) + E∗sem(t|s, θ))
= (αErec(s; θs) + (1 − α)E∗sem(s|t, θs))
+ (αErec(t; θt) + (1 − α)E∗sem(t|s, θt))
</equation>
<bodyText confidence="0.999757384615385">
We can see that the parameters θ can be divided
into two classes: θs for the source language and θt
for the target language. The above equation also
indicates that the source-side parameters θs can be
optimized independently as long as the semantic
representation pt of the target phrase t is given to
compute Esem(s|t, θ) with Eq. 9. It is similar for
the target-side parameters θt.
Assuming the target phrase representation pt
is available, the optimization of the source-side
parameters is similar to that of semi-supervised
RAE. We apply the Stochastic Gradient Descent
(SGD) algorithm to optimize each parameter:
</bodyText>
<equation confidence="0.99076">
θs = θs − η∂Js (14)
</equation>
<bodyText confidence="0.999921380952381">
In order to run SGD algorithm, we need to solve
two problems: one for parameter initialization and
the other for partial gradient calculation.
In parameter initialization, θrec and θsem for the
source language is randomly set according to a
normal distribution. For the word embedding Ls,
there are two choices. First, Ls is initialized ran-
domly like other parameters. Second, the word
embedding matrix Ls is pre-trained with DNN
(Bengio et al., 2003; Collobert and Weston, 2008;
Mikolov et al., 2013) using large-scale unlabeled
monolingual data. We prefer to the second one
since this kind of word embedding has already
encoded some semantics of the words. In this
work, we employ the toolkit Word2Vec (Mikolov
et al., 2013) to pre-train the word embedding for
the source and target languages. The word em-
beddings will be fine-tuned in our BRAE model to
capture much more semantics.
The partial gradient for one instance is com-
puted as follows:
</bodyText>
<equation confidence="0.9957825">
∂Js ∂E(s|t, θs) + λθs (15)
∂θs = ∂θs
</equation>
<bodyText confidence="0.999708333333333">
Where the source-side error given the target phrase
representation includes reconstruction error and
updated semantic error:
</bodyText>
<equation confidence="0.9841115">
E(s|t, θs) = αErec(s; θs) + (1 − α)E∗sem(s|t, θs)
(16)
</equation>
<bodyText confidence="0.998502666666667">
Given the current θs, we first construct the binary
tree (as illustrated in Fig. 2) for any source-side
phrase using the greedy algorithm (Socher et al.,
2011). Then, the derivatives for the parameters in
the fixed binary tree will be calculated via back-
propagation through structures (Goller and Kuch-
ler, 1996). Finally, the parameters will be updated
using Eq. 14 and a new θs is obtained.
The target-side parameters θt can be optimized
in the same way as long as the source-side phrase
representation ps is available. It seems a para-
dox that updating θs needs pt while updating θt
needs ps. To solve this problem, we propose an
co-training style algorithm which includes three
steps:
</bodyText>
<listItem confidence="0.993131846153846">
1. Pre-training: applying unsupervised phrase
embedding with standard RAE to pre-train the
source- and target-side phrase representations ps
and pt respectively (Section 2.1.2);
2. Fine-tuning: with the BRAE model, us-
ing target-side phrase representation pt to update
the source-side parameters θs and obtain the fine-
tuned source-side phrase representation p0s, and
meanwhile using ps to update θt and get the fine-
tuned p0t, and then calculate the joint error over the
training corpus;
3. Termination Check: if the joint error
reaches a local minima or the iterations reach
</listItem>
<bodyText confidence="0.90867075">
the pre-defined number (25 is used in our exper-
iments), we terminate the training procedure, oth-
erwise we set ps = p0s, pt = p0t, and go to step
2.
</bodyText>
<sectionHeader confidence="0.999546" genericHeader="method">
4 Experiments
</sectionHeader>
<bodyText confidence="0.999995">
With the semantic phrase embeddings and the vec-
tor space transformation function, we apply the
BRAE to measure the semantic similarity between
a source phrase and its translation candidates in
the phrase-based SMT. Two tasks are involved in
the experiments: phrase table pruning that dis-
cards entries whose semantic similarity is very low
and decoding with the phrasal semantic similari-
ties as additional new features.
</bodyText>
<subsectionHeader confidence="0.957606">
4.1 Hyper-Parameter Settings
</subsectionHeader>
<bodyText confidence="0.999932166666667">
The hyper-parameters in the BRAE model include
the dimensionality of the word embedding n in Eq.
1, the balance weight α in Eq. 10, λs in Eq. 11,
and the learning rate η in Eq. 14.
For the dimensionality n, we have tried three
settings n = 50,100, 200 in our experiments. We
</bodyText>
<equation confidence="0.851232">
(13)
</equation>
<page confidence="0.99074">
116
</page>
<bodyText confidence="0.9999475">
empirically set the learning rate q = 0.01. We
draw α from 0.05 to 0.5 with step 0.05, and As
from {10−6,10−5,10−4,10−3,10−2}. The over-
all error of the BRAE model is employed to guide
the search procedure. Finally, we choose α =
0.15, AL = 10−2, Arec = 10−3 and Asem = 10−3.
</bodyText>
<subsectionHeader confidence="0.980722">
4.2 SMT Setup
</subsectionHeader>
<bodyText confidence="0.999989333333333">
We have implemented a phrase-based translation
system with a maximum entropy based reordering
model using the bracketing transduction grammar
(Wu, 1997; Xiong et al., 2006).
The SMT evaluation is conducted on Chinese-
to-English translation. Accordingly, our BRAE
model is trained on Chinese and English. The
bilingual training data from LDC 2 contains 0.96M
sentence pairs and 1.1M entity pairs with 27.7M
Chinese words and 31.9M English words. A 5-
gram language model is trained on the Xinhua por-
tion of the English Gigaword corpus and the En-
glish part of bilingual training data. The NIST
MT03 is used as the development data. NIST
MT04-06 and MT08 (news data) are used as the
test data. Case-insensitive BLEU is employed
as the evaluation metric. The statistical signif-
icance test is performed by the re-sampling ap-
proach (Koehn, 2004).
In addition, we pre-train the word embedding
with toolkit Word2Vec on large-scale monolingual
data including the aforementioned data for SMT.
The monolingual data contains 1.06B words for
Chinese and 1.12B words for English. To ob-
tain high-quality bilingual phrase pairs to train
our BRAE model, we perform forced decoding
for the bilingual training sentences and collect the
phrase pairs used. After removing the duplicates,
the remaining 1.12M bilingual phrase pairs (length
ranging from 1 to 7) are obtained.
</bodyText>
<subsectionHeader confidence="0.999349">
4.3 Phrase Table Pruning
</subsectionHeader>
<bodyText confidence="0.999355625">
Pruning most of the phrase table without much
impact on translation quality is very important
for translation especially in environments where
memory and time constraints are imposed. Many
algorithms have been proposed to deal with this
problem, such as significance pruning (Johnson et
al., 2007; Tomeh et al., 2009), relevance prun-
ing (Eck et al., 2007) and entropy-based pruning
</bodyText>
<footnote confidence="0.904270666666667">
2LDC category numbers: LDC2000T50, LDC2002L27,
LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06,
LDC2005T10 and LDC2005T34.
</footnote>
<bodyText confidence="0.993205630434783">
(Ling et al., 2012; Zens et al., 2012). These algo-
rithms are based on corpus statistics including co-
occurrence statistics, phrase pair usage and com-
position information. For example, the signifi-
cance pruning, which is proven to be a very ef-
fective algorithm, computes the probability named
p-value, that tests whether a source phrase s and a
target phrase t co-occur more frequently in a bilin-
gual corpus than they happen just by chance. The
higher the p-value, the more likely of the phrase
pair to be spurious.
Our work has the same objective, but instead of
using corpus statistics, we attempt to measure the
quality of the phrase pair from the view of seman-
tic meaning. Given a phrase pair (s, t), the BRAE
model first obtains their semantic phrase represen-
tations (ps, pt), and then transforms ps into target
semantic space ps*, pt into source semantic space
pt*. We finally get two similarities 5im(ps*,pt)
and 5im(pt*, ps). Phrase pairs that have a low
similarity are more likely to be noise and more
prone to be pruned. In experiments, we discard
the phrase pair whose similarity in two directions
are smaller than a threshold 3.
Table 1 shows the comparison results between
our BRAE-based pruning method and the signif-
icance pruning algorithm. We can see a common
phenomenon in both of the algorithms: for the first
few thresholds, the phrase table becomes smaller
and smaller while the translation quality is not
much decreased, but the performance jumps a lot
at a certain threshold (16 for Significance pruning,
0.8 for BRAE-based one).
Specifically, the Significance algorithm can
safely discard 64% of the phrase table at its thresh-
old 12 with only 0.1 BLEU loss in the overall
test. In contrast, our BRAE-based algorithm can
remove 72% of the phrase table at its threshold
0.7 with only 0.06 BLEU loss in the overall eval-
uation. When the two algorithms using a similar
portion of the phrase table 4 (35% in BRAE and
36% in Significance), the BRAE-based algorithm
outperforms the Significance algorithm on all the
test sets except for MT04. It indicates that our
BRAE model is a good alternative for phrase table
pruning. Furthermore, our model is much more in-
</bodyText>
<footnote confidence="0.952517">
3To avoid the situation that all the translation candidates
for a source phrase are pruned, we always keep the first 10
best according to the semantic similarity.
4In the future, we will compare the performance by en-
forcing the two algorithms to use the same portion of phrase
table
</footnote>
<page confidence="0.987073">
117
</page>
<table confidence="0.999935">
Method Threshold PhraseTable MT03 MT04 MT05 MT06 MT08 ALL
Baseline 100% 35.81 36.91 34.69 33.83 27.17 34.82
0.4 52% 35.94 36.96 35.00 34.71 27.77 35.16
0.5 44% 35.67 36.59 34.86 33.91 27.25 34.89
BRAE 0.6 35% 35.86 36.71 34.93 34.63 27.34 35.05
0.7 28% 35.55 36.62 34.57 33.97 27.10 34.76
0.8 20% 35.06 36.01 34.13 33.04 26.66 34.04
8 48% 35.86 36.99 34.74 34.53 27.59 35.13
Significance 12 36% 35.59 36.73 34.65 34.17 27.16 34.72
16 25% 35.19 36.24 34.26 33.32 26.55 34.09
20 18% 35.05 36.09 34.02 32.98 26.37 33.97
</table>
<tableCaption confidence="0.79101575">
Table 1: Comparison between BRAE-based pruning and Significance pruning of phrase table. Threshold
means similarity in BRAE and negative-log-p-value in Significance. ”ALL” combines the development
and test sets. Bold numbers denote that the result is better than or comparable to that of baseline. n = 50
is used for embedding dimensionality.
</tableCaption>
<bodyText confidence="0.923599">
tuitive because it is directly based on the semantic
similarity.
</bodyText>
<subsectionHeader confidence="0.998404">
4.4 Decoding with Phrasal Semantic
Similarities
</subsectionHeader>
<bodyText confidence="0.999885366666666">
Besides using the semantic similarities to prune
the phrase table, we also employ them as two in-
formative features like the phrase translation prob-
ability to guide translation hypotheses selection
during decoding. Typically, four translation prob-
abilities are adopted in the phrase-based SMT, in-
cluding phrase translation probability and lexical
weights in both directions. The phrase transla-
tion probability is based on co-occurrence statis-
tics and the lexical weights consider the phrase as
bag-of-words. In contrast, our BRAE model fo-
cuses on compositional semantics from words to
phrases. Therefore, the semantic similarities com-
puted using our BRAE model are complementary
to the existing four translation probabilities.
The semantic similarities in two directions
Sim(ps*, pt) and Sim(pt*, ps) are integrated into
our baseline phrase-based model. In order to in-
vestigate the influence of the dimensionality of the
embedding space, we have tried three different set-
tings n = 50,100, 200.
As shown in Table 2, no matter what n is, the
BRAE model can significantly improve the trans-
lation quality in the overall test data. The largest
improvement can be up to 1.7 BLEU score (MT06
for n = 50). It is interesting that with dimen-
sionality growing, the translation performance is
not consistently improved. We speculate that us-
ing n = 50 or n = 100 can already distinguish
good translation candidates from bad ones.
</bodyText>
<subsectionHeader confidence="0.998901">
4.5 Analysis on Semantic Phrase Embedding
</subsectionHeader>
<bodyText confidence="0.999985631578948">
To have a better intuition about the power of the
BRAE model at learning semantic phrase embed-
dings, we show some examples in Table 3. Given
the BRAE model and the phrase training set, we
search from the set the most semantically similar
English phrases for any new input English phrase.
The input phrases contain different number of
words. The table shows that the unsupervised
RAE can at most capture the syntactic property
when the phrases are short. For example, the
unsupervised RAE finds do not want for the in-
put phrase do not agree. When the phrase be-
comes longer, the unsupervised RAE cannot even
capture the syntactic property. In contrast, our
BRAE model learns the semantic meaning for
each phrase no matter whether it is short or rel-
atively long. This indicates that the proposed
BRAE model is effective at learning semantic
phrase embeddings.
</bodyText>
<sectionHeader confidence="0.999847" genericHeader="method">
5 Discussions
</sectionHeader>
<subsectionHeader confidence="0.999818">
5.1 Applications of The BRAE model
</subsectionHeader>
<bodyText confidence="0.999968615384615">
As the semantic phrase embedding can fully rep-
resent the phrase, we can go a step further in the
phrase-based SMT and feed the semantic phrase
embeddings to DNN in order to model the whole
translation process (e.g. derivation structure pre-
diction). We will explore this direction in our fu-
ture work. Besides SMT, the semantic phrase em-
beddings can be used in other cross-lingual tasks,
such as cross-lingual question answering, since
the semantic similarity between phrases in differ-
ent languages can be calculated accurately.
In addition to the cross-lingual applications, we
believe the BRAE model can be applied in many
</bodyText>
<page confidence="0.994213">
118
</page>
<table confidence="0.9997136">
Method n MT03 MT04 MT05 MT06 MT08 ALL
Baseline 35.81 36.91 34.69 33.83 27.17 34.82
50 36.43 37.64 35.35 35.53 28.59 35.84+
BRAE 100 36.45 37.44 35.58 35.42 28.57 36.03+
200 36.34 37.35 35.78 34.87 27.84 35.62+
</table>
<tableCaption confidence="0.9660175">
Table 2: Experimental results of decoding with phrasal semantic similarities. n is the embedding dimen-
sionality. ”+” means that the model significantly outperforms the baseline with p &lt; 0.01.
</tableCaption>
<table confidence="0.998058461538462">
New Phrase Unsupervised RAE BRAE
military force core force military power
main force military strength
labor force armed forces
at a meeting to a meeting at the meeting
at a rate during the meeting
a meeting , at the conference
do not agree one can accept do not favor
i can understand will not compromise
do not want not to approve
each people in this nation each country regards every citizen in this country
each country has its all the people in the country
each other, and people all over the country
</table>
<tableCaption confidence="0.999855">
Table 3: Semantically similar phrases in the training set for the new phrases.
</tableCaption>
<bodyText confidence="0.9991988">
monolingual NLP tasks which depend on good
phrase representations or semantic similarity be-
tween phrases, such as named entity recognition,
parsing, textual entailment, question answering
and paraphrase detection.
</bodyText>
<subsectionHeader confidence="0.99924">
5.2 Model Extensions
</subsectionHeader>
<bodyText confidence="0.999981916666667">
In fact, the phrases having the same meaning are
translation equivalents in different languages, but
are paraphrases in one language. Therefore, our
model can be easily adapted to learn semantic
phrase embeddings using paraphrases.
Our BRAE model still has some limitations.
For example, as each node in the recursive auto-
encoder shares the same weight matrix, the BRAE
model would become weak at learning the seman-
tic representations for long sentences with tens of
words. Improving the model to semantically em-
bed sentences is left for our future work.
</bodyText>
<sectionHeader confidence="0.998619" genericHeader="conclusions">
6 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999993">
This paper has explored the bilingually-
constrained recursive auto-encoders in learning
phrase embeddings, which can distinguish phrases
with different semantic meanings. With the ob-
jective to minimize the semantic distance between
translation equivalents and maximize the semantic
distance between non-translation pairs simultane-
ously, the learned model can semantically embed
any phrase in two languages and can transform
the semantic space in one language to the other.
Two end-to-end SMT tasks are involved to test
the power of the proposed model at learning the
semantic phrase embeddings. The experimental
results show that the BRAE model is remarkably
effective in phrase table pruning and decoding
with phrasal semantic similarities.
We have also discussed many other potential ap-
plications and extensions of our BRAE model. In
the future work, we will explore four directions.
1) we will try to model the decoding process with
DNN based on our semantic embeddings of the
basic translation units. 2) we are going to learn
semantic phrase embeddings with the paraphrase
corpus. 3) we will apply the BRAE model in other
monolingual and cross-lingual tasks. 4) we plan to
learn semantic sentence embeddings by automati-
cally learning different weight matrices for differ-
ent nodes in the BRAE model.
</bodyText>
<sectionHeader confidence="0.997487" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.99885925">
We thank Nan Yang for sharing the baseline
code and anonymous reviewers for their valu-
able comments. The research work has been
partially funded by the Natural Science Founda-
tion of China under Grant No. 61333018 and
61303181, and Hi-Tech Research and Develop-
ment Program (863 Program) of China under
Grant No. 2012AA011102.
</bodyText>
<page confidence="0.998693">
119
</page>
<sectionHeader confidence="0.989743" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999666447619048">
Michael Auli, Michel Galley, Chris Quirk, and Geof-
frey Zweig. 2013. Joint language and translation
modeling with recurrent neural networks. In Pro-
ceedings of the 2013 Conference on Empirical Meth-
ods in Natural Language Processing, pages 1044–
1054.
Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and
Christian Jauvin. 2003. A neural probabilistic lan-
guage model. Journal of Machine Learning Re-
search, 3:1137–1155.
Yoshua Bengio, Holger Schwenk, Jean-S´ebastien
Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain.
2006. Neural probabilistic language models. In In-
novations in Machine Learning, pages 137–186.
Ronan Collobert and Jason Weston. 2008. A unified
architecture for natural language processing: Deep
neural networks with multitask learning. In Pro-
ceedings of the 25th international conference on
Machine learning, pages 160–167.
Ronan Collobert, Jason Weston, L´eon Bottou, Michael
Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011. Natural language processing (almost) from
scratch. The Journal of Machine Learning Re-
search, 12:2493–2537.
George E Dahl, Dong Yu, Li Deng, and Alex Acero.
2012. Context-dependent pre-trained deep neural
networks for large-vocabulary speech recognition.
IEEE Transactions on Audio, Speech, and Language
Processing, 20(1):30–42.
Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Ha-
jime Tsukada. 2013. Adaptation data selection us-
ing neural language models: Experiments in ma-
chine translation. In 51st Annual Meeting of the As-
sociation for Computational Linguistics, pages 678–
683.
Matthias Eck, Stephen Vogal, and Alex Waibel. 2007.
Estimating phrase pair relevance for translation
model pruning. In MTSummit XI.
Jianfeng Gao, Xiaodong He, Wen-tau Yih, and
Li Deng. 2013. Learning semantic representations
for the phrase translation model. arXiv preprint
arXiv:1312.0482.
Christoph Goller and Andreas Kuchler. 1996. Learn-
ing task-dependent distributed representations by
backpropagation through structure. In IEEE Inter-
national Conference on Neural Networks, volume 1,
pages 347–352.
John Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation quality
by discarding most of the phrasetable. In Proceed-
ings of EMNLP.
Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent
continuous translation models. In Proceedings of
the 2013 Conference on Empirical Methods in Nat-
ural Language Processing, pages 1700–1709.
Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,
Karol Gregor, Micha¨el Mathieu, and Yann L Cun.
2010. Learning convolutional feature hierarchies for
visual recognition. In Advances in neural informa-
tion processing systems, pages 1090–1098.
Philipp Koehn. 2004. Statistical significance tests for
machine translation evaluation. In Proceedings of
EMNLP, pages 388–395.
Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.
2012. Imagenet classification with deep convolu-
tional neural networks. In Advances in Neural Infor-
mation Processing Systems 25, pages 1106–1114.
Peng Li, Yang Liu, and Maosong Sun. 2013. Recur-
sive autoencoders for itg-based translation. In Pro-
ceedings of the Conference on Empirical Methods in
Natural Language Processing.
Wang Ling, Joao Grac¸a, Isabel Trancoso, and Alan
Black. 2012. Entropy-based pruning for phrase-
based machine translation. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Natural Language Processing and Computational
Natural Language Learning, pages 962–971.
Lemao Liu, Taro Watanabe, Eiichiro Sumita, and
Tiejun Zhao. 2013. Additive neural networks for
statistical machine translation. In 51st Annual Meet-
ing of the Association for Computational Linguis-
tics, pages 791–801.
Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan
Cernock`y, and Sanjeev Khudanpur. 2010. Recur-
rent neural network based language model. In IN-
TERSPEECH, pages 1045–1048.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Cor-
rado, and Jeffrey Dean. 2013. Distributed represen-
tations of words and phrases and their composition-
ality. In Proceedings of NIPS.
Richard Socher, Christopher D Manning, and An-
drew Y Ng. 2010. Learning continuous phrase
representations and syntactic parsing with recursive
neural networks. In Proceedings of the NIPS-2010
Deep Learning and Unsupervised Feature Learning
Workshop.
Richard Socher, Jeffrey Pennington, Eric H Huang,
Andrew Y Ng, and Christopher D Manning. 2011.
Semi-supervised recursive autoencoders for predict-
ing sentiment distributions. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 151–161.
Richard Socher, John Bauer, Christopher D Manning,
and Andrew Y Ng. 2013a. Parsing with composi-
tional vector grammars. In Proceedings of ACL.
</reference>
<page confidence="0.953755">
120
</page>
<reference confidence="0.996896846153846">
Richard Socher, Alex Perelygin, Jean Y Wu, Jason
Chuang, Christopher D Manning, Andrew Y Ng,
and Christopher Potts. 2013b. Recursive deep mod-
els for semantic compositionality over a sentiment
treebank. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing.
Nadi Tomeh, Nicola Cancedda, and Marc Dymetman.
2009. Complexity-based phrase-table filtering for
statistical machine translation. In Proceedings of
Summit XII, pages 144–151.
Ashish Vaswani, Yinggong Zhao, Victoria Fossum,
and David Chiang. 2013. Decoding with large-
scale neural language models improves translation.
In Proceedings of the 2013 Conference on Empiri-
cal Methods in Natural Language Processing, pages
1387–1392.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational linguistics, 23(3):377–403.
Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maxi-
mum entropy based phrase reordering model for sta-
tistical machine translation. In Proceedings of ACL-
COLING, pages 505–512.
Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Neng-
hai Yu. 2013. Word alignment modeling with con-
text dependent deep neural network. In 51st Annual
Meeting of the Association for Computational Lin-
guistics.
Richard Zens, Daisy Stanton, and Peng Xu. 2012. A
systematic comparison of phrase table pruning tech-
niques. In Proceedings of the 2012 Joint Confer-
ence on Empirical Methods in Natural Language
Processing and Computational Natural Language
Learning, pages 972–983.
Will Y Zou, Richard Socher, Daniel Cer, and Christo-
pher D Manning. 2013. Bilingual word embeddings
for phrase-based machine translation. In Proceed-
ings of the 2013 Conference on Empirical Methods
in Natural Language Processing, pages 1393–1398.
</reference>
<page confidence="0.99782">
121
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.491665">
<title confidence="0.999213">Bilingually-constrained Phrase Embeddings for Machine Translation</title>
<author confidence="0.635093">Shujie Mu Ming</author>
<author confidence="0.635093">Chengqing</author>
<affiliation confidence="0.960656">Laboratory of Pattern Recognition, CASIA, Beijing, P.R. China</affiliation>
<address confidence="0.763929">Research Asia, Beijing, P.R. China</address>
<abstract confidence="0.999647565217391">We propose Bilingually-constrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings (compact vector representations for phrases), which can distinguish the phrases with different semantic meanings. The BRAE is trained in a way that minimizes the semantic distance of translation equivalents and maximizes the semantic distance of nontranslation pairs simultaneously. After training, the model learns how to embed each phrase semantically in two languages and also learns how to transform semantic embedding space in one language to the other. We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Michael Auli</author>
<author>Michel Galley</author>
<author>Chris Quirk</author>
<author>Geoffrey Zweig</author>
</authors>
<title>Joint language and translation modeling with recurrent neural networks.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1044--1054</pages>
<contexts>
<context position="1817" citStr="Auli et al., 2013" startWordPosition="260" endWordPosition="263">ks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasa</context>
</contexts>
<marker>Auli, Galley, Quirk, Zweig, 2013</marker>
<rawString>Michael Auli, Michel Galley, Chris Quirk, and Geoffrey Zweig. 2013. Joint language and translation modeling with recurrent neural networks. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1044– 1054.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>R´ejean Ducharme</author>
<author>Pascal Vincent</author>
<author>Christian Jauvin</author>
</authors>
<title>A neural probabilistic language model.</title>
<date>2003</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>3--1137</pages>
<contexts>
<context position="2110" citStr="Bengio et al., 2003" startWordPosition="307" endWordPosition="310"> translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore t</context>
<context position="11346" citStr="Bengio et al., 2003" startWordPosition="1762" endWordPosition="1765">e an indirect but feasible way. Figure 2: A recursive auto-encoder for a fourword phrase. The empty nodes are the reconstructions of the input. We will first briefly present the unsupervised phrase embedding, and then describe the semisupervised framework. After that, we introduce the BRAE on the network structure, objective function and parameter inference. 3.1 Unsupervised Phrase Embedding 3.1.1 Word Vector Representations In phrase embedding using composition, the word vector representation is the basis and serves as the input to the neural network. After learning word embeddings with DNN (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013), each word in the vocabulary V corresponds to a vector x E Rn, and all the vectors are stacked into an embedding matrix L E Rn×|V |. Given a phrase which is an ordered list of m words, each word has an index i into the columns of the embedding matrix L. The index i is used to retrieve the word’s vector representation using a simple multiplication with a binary vector e which is zero in all positions except for the ith index: xi = Lei E Rn (1) Note that n is usually set empirically, such as n = 50,100, 200. Throughout this paper, n = 3 is used</context>
<context position="21478" citStr="Bengio et al., 2003" startWordPosition="3483" endWordPosition="3486">urce-side parameters is similar to that of semi-supervised RAE. We apply the Stochastic Gradient Descent (SGD) algorithm to optimize each parameter: θs = θs − η∂Js (14) In order to run SGD algorithm, we need to solve two problems: one for parameter initialization and the other for partial gradient calculation. In parameter initialization, θrec and θsem for the source language is randomly set according to a normal distribution. For the word embedding Ls, there are two choices. First, Ls is initialized randomly like other parameters. Second, the word embedding matrix Ls is pre-trained with DNN (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013) using large-scale unlabeled monolingual data. We prefer to the second one since this kind of word embedding has already encoded some semantics of the words. In this work, we employ the toolkit Word2Vec (Mikolov et al., 2013) to pre-train the word embedding for the source and target languages. The word embeddings will be fine-tuned in our BRAE model to capture much more semantics. The partial gradient for one instance is computed as follows: ∂Js ∂E(s|t, θs) + λθs (15) ∂θs = ∂θs Where the source-side error given the target phrase representation</context>
</contexts>
<marker>Bengio, Ducharme, Vincent, Jauvin, 2003</marker>
<rawString>Yoshua Bengio, R´ejean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A neural probabilistic language model. Journal of Machine Learning Research, 3:1137–1155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshua Bengio</author>
<author>Holger Schwenk</author>
<author>Jean-S´ebastien Sen´ecal</author>
<author>Fr´ederic Morin</author>
<author>Jean-Luc Gauvain</author>
</authors>
<title>Neural probabilistic language models.</title>
<date>2006</date>
<booktitle>In Innovations in Machine Learning,</booktitle>
<pages>137--186</pages>
<marker>Bengio, Schwenk, Sen´ecal, Morin, Gauvain, 2006</marker>
<rawString>Yoshua Bengio, Holger Schwenk, Jean-S´ebastien Sen´ecal, Fr´ederic Morin, and Jean-Luc Gauvain. 2006. Neural probabilistic language models. In Innovations in Machine Learning, pages 137–186.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
</authors>
<title>A unified architecture for natural language processing: Deep neural networks with multitask learning.</title>
<date>2008</date>
<booktitle>In Proceedings of the 25th international conference on Machine learning,</booktitle>
<pages>160--167</pages>
<contexts>
<context position="2159" citStr="Collobert and Weston, 2008" startWordPosition="315" endWordPosition="318">rong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (s</context>
<context position="11374" citStr="Collobert and Weston, 2008" startWordPosition="1766" endWordPosition="1770">sible way. Figure 2: A recursive auto-encoder for a fourword phrase. The empty nodes are the reconstructions of the input. We will first briefly present the unsupervised phrase embedding, and then describe the semisupervised framework. After that, we introduce the BRAE on the network structure, objective function and parameter inference. 3.1 Unsupervised Phrase Embedding 3.1.1 Word Vector Representations In phrase embedding using composition, the word vector representation is the basis and serves as the input to the neural network. After learning word embeddings with DNN (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013), each word in the vocabulary V corresponds to a vector x E Rn, and all the vectors are stacked into an embedding matrix L E Rn×|V |. Given a phrase which is an ordered list of m words, each word has an index i into the columns of the embedding matrix L. The index i is used to retrieve the word’s vector representation using a simple multiplication with a binary vector e which is zero in all positions except for the ith index: xi = Lei E Rn (1) Note that n is usually set empirically, such as n = 50,100, 200. Throughout this paper, n = 3 is used for better illustration as </context>
<context position="21506" citStr="Collobert and Weston, 2008" startWordPosition="3487" endWordPosition="3490">is similar to that of semi-supervised RAE. We apply the Stochastic Gradient Descent (SGD) algorithm to optimize each parameter: θs = θs − η∂Js (14) In order to run SGD algorithm, we need to solve two problems: one for parameter initialization and the other for partial gradient calculation. In parameter initialization, θrec and θsem for the source language is randomly set according to a normal distribution. For the word embedding Ls, there are two choices. First, Ls is initialized randomly like other parameters. Second, the word embedding matrix Ls is pre-trained with DNN (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013) using large-scale unlabeled monolingual data. We prefer to the second one since this kind of word embedding has already encoded some semantics of the words. In this work, we employ the toolkit Word2Vec (Mikolov et al., 2013) to pre-train the word embedding for the source and target languages. The word embeddings will be fine-tuned in our BRAE model to capture much more semantics. The partial gradient for one instance is computed as follows: ∂Js ∂E(s|t, θs) + λθs (15) ∂θs = ∂θs Where the source-side error given the target phrase representation includes reconstruction err</context>
</contexts>
<marker>Collobert, Weston, 2008</marker>
<rawString>Ronan Collobert and Jason Weston. 2008. A unified architecture for natural language processing: Deep neural networks with multitask learning. In Proceedings of the 25th international conference on Machine learning, pages 160–167.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ronan Collobert</author>
<author>Jason Weston</author>
<author>L´eon Bottou</author>
<author>Michael Karlen</author>
<author>Koray Kavukcuoglu</author>
<author>Pavel Kuksa</author>
</authors>
<title>Natural language processing (almost) from scratch.</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2493</pages>
<contexts>
<context position="7535" citStr="Collobert et al., 2011" startWordPosition="1148" endWordPosition="1151">antic phrase embeddings can be directly fed to DNN to model the decoding process. Besides SMT, the semantic phrase embeddings can be used in other cross-lingual tasks (e.g. cross-lingual question answering) and monolingual applications such as textual entailment, question answering and paraphrase detection. 2 Related Work Recently, phrase embedding has drawn more and more attention. There are three main perspectives handling this task in monolingual languages. One method considers the phrases as bag-ofwords and employs a convolution model to transform the word embeddings to phrase embeddings (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013). Gao et al. (2013) also use bag-ofwords but learn BLEU sensitive phrase embeddings. This kind of approaches does not take the word order into account and loses much information. Instead, our bilingually-constrained recursive auto-encoders not only learn the composition mechanism of generating phrases from words, but also fine tune the word embeddings during the model training stage, so that we can induce the full information of the phrases and internal words. Another method (Mikolov et al., 2013) deals with the phrases having a meaning that is not a simple com</context>
</contexts>
<marker>Collobert, Weston, Bottou, Karlen, Kavukcuoglu, Kuksa, 2011</marker>
<rawString>Ronan Collobert, Jason Weston, L´eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research, 12:2493–2537.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George E Dahl</author>
<author>Dong Yu</author>
<author>Li Deng</author>
<author>Alex Acero</author>
</authors>
<title>Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition.</title>
<date>2012</date>
<journal>IEEE Transactions on Audio, Speech, and Language Processing,</journal>
<volume>20</volume>
<issue>1</issue>
<contexts>
<context position="1460" citStr="Dahl et al., 2012" startWordPosition="203" endWordPosition="206">c embedding space in one language to the other. We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real</context>
</contexts>
<marker>Dahl, Yu, Deng, Acero, 2012</marker>
<rawString>George E Dahl, Dong Yu, Li Deng, and Alex Acero. 2012. Context-dependent pre-trained deep neural networks for large-vocabulary speech recognition. IEEE Transactions on Audio, Speech, and Language Processing, 20(1):30–42.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kevin Duh</author>
<author>Graham Neubig</author>
<author>Katsuhito Sudoh</author>
<author>Hajime Tsukada</author>
</authors>
<title>Adaptation data selection using neural language models: Experiments in machine translation.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>678--683</pages>
<contexts>
<context position="1891" citStr="Duh et al., 2013" startWordPosition="271" endWordPosition="274">resentation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to mode</context>
</contexts>
<marker>Duh, Neubig, Sudoh, Tsukada, 2013</marker>
<rawString>Kevin Duh, Graham Neubig, Katsuhito Sudoh, and Hajime Tsukada. 2013. Adaptation data selection using neural language models: Experiments in machine translation. In 51st Annual Meeting of the Association for Computational Linguistics, pages 678– 683.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Eck</author>
<author>Stephen Vogal</author>
<author>Alex Waibel</author>
</authors>
<title>Estimating phrase pair relevance for translation model pruning. In MTSummit XI.</title>
<date>2007</date>
<contexts>
<context position="26368" citStr="Eck et al., 2007" startWordPosition="4291" endWordPosition="4294">rain our BRAE model, we perform forced decoding for the bilingual training sentences and collect the phrase pairs used. After removing the duplicates, the remaining 1.12M bilingual phrase pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on translation quality is very important for translation especially in environments where memory and time constraints are imposed. Many algorithms have been proposed to deal with this problem, such as significance pruning (Johnson et al., 2007; Tomeh et al., 2009), relevance pruning (Eck et al., 2007) and entropy-based pruning 2LDC category numbers: LDC2000T50, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2005T34. (Ling et al., 2012; Zens et al., 2012). These algorithms are based on corpus statistics including cooccurrence statistics, phrase pair usage and composition information. For example, the significance pruning, which is proven to be a very effective algorithm, computes the probability named p-value, that tests whether a source phrase s and a target phrase t co-occur more frequently in a bilingual corpus than they happen just by chance. The higher th</context>
</contexts>
<marker>Eck, Vogal, Waibel, 2007</marker>
<rawString>Matthias Eck, Stephen Vogal, and Alex Waibel. 2007. Estimating phrase pair relevance for translation model pruning. In MTSummit XI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Xiaodong He</author>
<author>Wen-tau Yih</author>
<author>Li Deng</author>
</authors>
<title>Learning semantic representations for the phrase translation model. arXiv preprint arXiv:1312.0482.</title>
<date>2013</date>
<contexts>
<context position="7587" citStr="Gao et al. (2013)" startWordPosition="1157" endWordPosition="1160">el the decoding process. Besides SMT, the semantic phrase embeddings can be used in other cross-lingual tasks (e.g. cross-lingual question answering) and monolingual applications such as textual entailment, question answering and paraphrase detection. 2 Related Work Recently, phrase embedding has drawn more and more attention. There are three main perspectives handling this task in monolingual languages. One method considers the phrases as bag-ofwords and employs a convolution model to transform the word embeddings to phrase embeddings (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013). Gao et al. (2013) also use bag-ofwords but learn BLEU sensitive phrase embeddings. This kind of approaches does not take the word order into account and loses much information. Instead, our bilingually-constrained recursive auto-encoders not only learn the composition mechanism of generating phrases from words, but also fine tune the word embeddings during the model training stage, so that we can induce the full information of the phrases and internal words. Another method (Mikolov et al., 2013) deals with the phrases having a meaning that is not a simple composition of the meanings of its individual words, su</context>
</contexts>
<marker>Gao, He, Yih, Deng, 2013</marker>
<rawString>Jianfeng Gao, Xiaodong He, Wen-tau Yih, and Li Deng. 2013. Learning semantic representations for the phrase translation model. arXiv preprint arXiv:1312.0482.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Goller</author>
<author>Andreas Kuchler</author>
</authors>
<title>Learning task-dependent distributed representations by backpropagation through structure.</title>
<date>1996</date>
<booktitle>In IEEE International Conference on Neural Networks,</booktitle>
<volume>1</volume>
<pages>347--352</pages>
<contexts>
<context position="22503" citStr="Goller and Kuchler, 1996" startWordPosition="3652" endWordPosition="3656">el to capture much more semantics. The partial gradient for one instance is computed as follows: ∂Js ∂E(s|t, θs) + λθs (15) ∂θs = ∂θs Where the source-side error given the target phrase representation includes reconstruction error and updated semantic error: E(s|t, θs) = αErec(s; θs) + (1 − α)E∗sem(s|t, θs) (16) Given the current θs, we first construct the binary tree (as illustrated in Fig. 2) for any source-side phrase using the greedy algorithm (Socher et al., 2011). Then, the derivatives for the parameters in the fixed binary tree will be calculated via backpropagation through structures (Goller and Kuchler, 1996). Finally, the parameters will be updated using Eq. 14 and a new θs is obtained. The target-side parameters θt can be optimized in the same way as long as the source-side phrase representation ps is available. It seems a paradox that updating θs needs pt while updating θt needs ps. To solve this problem, we propose an co-training style algorithm which includes three steps: 1. Pre-training: applying unsupervised phrase embedding with standard RAE to pre-train the source- and target-side phrase representations ps and pt respectively (Section 2.1.2); 2. Fine-tuning: with the BRAE model, using tar</context>
</contexts>
<marker>Goller, Kuchler, 1996</marker>
<rawString>Christoph Goller and Andreas Kuchler. 1996. Learning task-dependent distributed representations by backpropagation through structure. In IEEE International Conference on Neural Networks, volume 1, pages 347–352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLP.</booktitle>
<contexts>
<context position="26309" citStr="Johnson et al., 2007" startWordPosition="4280" endWordPosition="4283">or English. To obtain high-quality bilingual phrase pairs to train our BRAE model, we perform forced decoding for the bilingual training sentences and collect the phrase pairs used. After removing the duplicates, the remaining 1.12M bilingual phrase pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on translation quality is very important for translation especially in environments where memory and time constraints are imposed. Many algorithms have been proposed to deal with this problem, such as significance pruning (Johnson et al., 2007; Tomeh et al., 2009), relevance pruning (Eck et al., 2007) and entropy-based pruning 2LDC category numbers: LDC2000T50, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2005T34. (Ling et al., 2012; Zens et al., 2012). These algorithms are based on corpus statistics including cooccurrence statistics, phrase pair usage and composition information. For example, the significance pruning, which is proven to be a very effective algorithm, computes the probability named p-value, that tests whether a source phrase s and a target phrase t co-occur more frequently in a bili</context>
</contexts>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>John Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of EMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nal Kalchbrenner</author>
<author>Phil Blunsom</author>
</authors>
<title>Recurrent continuous translation models.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1700--1709</pages>
<contexts>
<context position="1850" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="264" endWordPosition="267">Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, i</context>
<context position="3178" citStr="Kalchbrenner and Blunsom (2013)" startWordPosition="473" endWordPosition="476">ecoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the phrase. Instead, we focus on learning phrase embeddings from the view of semantic meaning, so that</context>
<context position="7568" citStr="Kalchbrenner and Blunsom, 2013" startWordPosition="1152" endWordPosition="1156">can be directly fed to DNN to model the decoding process. Besides SMT, the semantic phrase embeddings can be used in other cross-lingual tasks (e.g. cross-lingual question answering) and monolingual applications such as textual entailment, question answering and paraphrase detection. 2 Related Work Recently, phrase embedding has drawn more and more attention. There are three main perspectives handling this task in monolingual languages. One method considers the phrases as bag-ofwords and employs a convolution model to transform the word embeddings to phrase embeddings (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013). Gao et al. (2013) also use bag-ofwords but learn BLEU sensitive phrase embeddings. This kind of approaches does not take the word order into account and loses much information. Instead, our bilingually-constrained recursive auto-encoders not only learn the composition mechanism of generating phrases from words, but also fine tune the word embeddings during the model training stage, so that we can induce the full information of the phrases and internal words. Another method (Mikolov et al., 2013) deals with the phrases having a meaning that is not a simple composition of the meanings of its i</context>
</contexts>
<marker>Kalchbrenner, Blunsom, 2013</marker>
<rawString>Nal Kalchbrenner and Phil Blunsom. 2013. Recurrent continuous translation models. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1700–1709.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koray Kavukcuoglu</author>
<author>Pierre Sermanet</author>
<author>Y-Lan Boureau</author>
<author>Karol Gregor</author>
<author>Micha¨el Mathieu</author>
<author>Yann L Cun</author>
</authors>
<title>Learning convolutional feature hierarchies for visual recognition. In Advances in neural information processing systems,</title>
<date>2010</date>
<pages>1090--1098</pages>
<marker>Kavukcuoglu, Sermanet, Boureau, Gregor, Micha¨el Mathieu, Cun, 2010</marker>
<rawString>Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau, Karol Gregor, Micha¨el Mathieu, and Yann L Cun. 2010. Learning convolutional feature hierarchies for visual recognition. In Advances in neural information processing systems, pages 1090–1098.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Statistical significance tests for machine translation evaluation.</title>
<date>2004</date>
<booktitle>In Proceedings of EMNLP,</booktitle>
<pages>388--395</pages>
<contexts>
<context position="25474" citStr="Koehn, 2004" startWordPosition="4155" endWordPosition="4156"> translation. Accordingly, our BRAE model is trained on Chinese and English. The bilingual training data from LDC 2 contains 0.96M sentence pairs and 1.1M entity pairs with 27.7M Chinese words and 31.9M English words. A 5- gram language model is trained on the Xinhua portion of the English Gigaword corpus and the English part of bilingual training data. The NIST MT03 is used as the development data. NIST MT04-06 and MT08 (news data) are used as the test data. Case-insensitive BLEU is employed as the evaluation metric. The statistical significance test is performed by the re-sampling approach (Koehn, 2004). In addition, we pre-train the word embedding with toolkit Word2Vec on large-scale monolingual data including the aforementioned data for SMT. The monolingual data contains 1.06B words for Chinese and 1.12B words for English. To obtain high-quality bilingual phrase pairs to train our BRAE model, we perform forced decoding for the bilingual training sentences and collect the phrase pairs used. After removing the duplicates, the remaining 1.12M bilingual phrase pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on trans</context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Statistical significance tests for machine translation evaluation. In Proceedings of EMNLP, pages 388–395.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alex Krizhevsky</author>
<author>Ilya Sutskever</author>
<author>Geoff Hinton</author>
</authors>
<title>Imagenet classification with deep convolutional neural networks.</title>
<date>2012</date>
<booktitle>In Advances in Neural Information Processing Systems 25,</booktitle>
<pages>1106--1114</pages>
<contexts>
<context position="1440" citStr="Krizhevsky et al., 2012" startWordPosition="199" endWordPosition="202"> how to transform semantic embedding space in one language to the other. We evaluate our proposed method on two end-to-end SMT tasks (phrase table pruning and decoding with phrasal semantic similarities) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, l</context>
</contexts>
<marker>Krizhevsky, Sutskever, Hinton, 2012</marker>
<rawString>Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. 2012. Imagenet classification with deep convolutional neural networks. In Advances in Neural Information Processing Systems 25, pages 1106–1114.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peng Li</author>
<author>Yang Liu</author>
<author>Maosong Sun</author>
</authors>
<title>Recursive autoencoders for itg-based translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="1775" citStr="Li et al., 2013" startWordPosition="254" endWordPosition="257"> is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntac</context>
<context position="3079" citStr="Li et al. (2013)" startWordPosition="459" endWordPosition="462">ccessfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully represent the phrasal translation units in SMT due to the lack of semantic meanings of the</context>
<context position="9050" citStr="Li et al., 2013" startWordPosition="1405" endWordPosition="1408">se embedding pt 112 ever, this kind of phrase embedding is hard to capture full semantics since the context of a phrase is limited. Furthermore, this method can only account for a very small part of phrases, since most of the phrases are compositional. In contrast, our method attempts to learn the semantic vector representation for any phrase. The third method views any phrase as the meaningful composition of its internal words. The recursive auto-encoder is typically adopted to learn the way of composition (Socher et al., 2010; Socher et al., 2011; Socher et al., 2013a; Socher et al., 2013b; Li et al., 2013). They pre-train the RAE with an unsupervised algorithm. And then, they fine-tune the RAE according to the label of the phrase, such as the syntactic category in parsing (Socher et al., 2013a), the polarity in sentiment analysis (Socher et al., 2011; Socher et al., 2013b), and the reordering pattern in SMT (Li et al., 2013). This kind of semi-supervised phrase embedding is in fact performing phrase clustering with respect to the phrase label. For example, in the RAEbased phrase reordering model for SMT (Li et al., 2013), the phrases with the similar reordering tendency (e.g. monotone or swap) </context>
<context position="10514" citStr="Li et al., 2013" startWordPosition="1633" endWordPosition="1636">sed phrase embedding, we are the first to focus on the semantic meanings of the phrases and propose a bilingually-constrained model to induce the semantic information and learn transformation of the semantic space in one language to the other. 3 Bilingually-constrained Recursive Auto-encoders This section introduces the Bilinguallyconstrained Recursive Auto-encoders (BRAE), that is inspired by two observations. First, the recursive auto-encoder provides a reasonable composition mechanism to embed each phrase. And the semi-supervised phrase embedding (Socher et al., 2011; Socher et al., 2013a; Li et al., 2013) further indicates that phrase embedding can be tuned with respect to the label. Second, even though we have no correct semantic phrase representation as the gold label, the phrases sharing the same meaning provide an indirect but feasible way. Figure 2: A recursive auto-encoder for a fourword phrase. The empty nodes are the reconstructions of the input. We will first briefly present the unsupervised phrase embedding, and then describe the semisupervised framework. After that, we introduce the BRAE on the network structure, objective function and parameter inference. 3.1 Unsupervised Phrase Em</context>
<context position="14936" citStr="Li et al., 2013" startWordPosition="2398" endWordPosition="2401">es in the training data. 3.2 Semi-supervised Phrase Embedding The above RAE is completely unsupervised and can only induce general representations of the Reconstruction Error Prediction Error Figure 3: An illustration of a semi-supervised RAE unit. Red nodes show the label distribution. multi-word phrases. Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al., 2011), syntactic category in parsing (Socher et al., 2013a) and phrase reordering pattern in SMT (Li et al., 2013). In the semi-supervised RAE for phrase embedding, the objective function over a (phrase, label) pair (x, t) includes the reconstruction error and the prediction error, as illustrated in Fig. 3. E(x, t; 0) = αErec(x, t; 0)+(1−α)Epred(x, t; 0) (6) Where the hyper-parameter α is used to balance the reconstruction and prediction error. For label prediction, the cross-entropy error is usually used to calculate Epred. By optimizing the above objective, the phrases in the vector embedding space will be grouped according to the labels. 3.3 The BRAE Model We know from the semi-supervised phrase embedd</context>
<context position="19779" citStr="Li et al., 2013" startWordPosition="3194" endWordPosition="3197">(s|t, θ) = max10, Esem(s|t, θ) − Esem(s|t&apos;, θ) + 11 (12) It tries to minimize the semantic distance between translation equivalents and maximize the semantic distance between non-translation pairs simultaneously. Using the above error function, we need to construct a negative example for each positive example. Suppose we are given a positive example (s, t), the correct translation t can be converted into a bad translation t&apos; by replacing the words in t with randomly chosen target language words. Then, a negative example (s, t&apos;) is available. 3.3.3 Parameter Inference Like semi-supervised RAE (Li et al., 2013), the parameters θ in our BRAE model can also be divided into three sets: θL: word embedding matrix L for two languages (Section 3.1.1); θrec: recursive auto-encoder parameter matrices W (1), W(2), and bias terms b(1), b(2) for two languages (Section 3.1.2); θsem: transformation matrix Wl and bias term bl for two directions in semantic distance computation (Section 3.3.1). 115 To have a deep understanding of the parameters, we rewrite Eq. 10: E(s, t; θ) = α(Erec(s; θ) + Erec(t; θ)) + (1 − α)(E∗ sem(s|t, θ) + E∗sem(t|s, θ)) = (αErec(s; θs) + (1 − α)E∗sem(s|t, θs)) + (αErec(t; θt) + (1 − α)E∗sem</context>
</contexts>
<marker>Li, Liu, Sun, 2013</marker>
<rawString>Peng Li, Yang Liu, and Maosong Sun. 2013. Recursive autoencoders for itg-based translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wang Ling</author>
<author>Joao Grac¸a</author>
<author>Isabel Trancoso</author>
<author>Alan Black</author>
</authors>
<title>Entropy-based pruning for phrasebased machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>962--971</pages>
<marker>Ling, Grac¸a, Trancoso, Black, 2012</marker>
<rawString>Wang Ling, Joao Grac¸a, Isabel Trancoso, and Alan Black. 2012. Entropy-based pruning for phrasebased machine translation. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 962–971.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lemao Liu</author>
<author>Taro Watanabe</author>
<author>Eiichiro Sumita</author>
<author>Tiejun Zhao</author>
</authors>
<title>Additive neural networks for statistical machine translation.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>791--801</pages>
<contexts>
<context position="1708" citStr="Liu et al., 2013" startWordPosition="242" endWordPosition="245">ts translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings a</context>
</contexts>
<marker>Liu, Watanabe, Sumita, Zhao, 2013</marker>
<rawString>Lemao Liu, Taro Watanabe, Eiichiro Sumita, and Tiejun Zhao. 2013. Additive neural networks for statistical machine translation. In 51st Annual Meeting of the Association for Computational Linguistics, pages 791–801.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Martin Karafi´at</author>
<author>Lukas Burget</author>
<author>Jan Cernock`y</author>
<author>Sanjeev Khudanpur</author>
</authors>
<title>Recurrent neural network based language model.</title>
<date>2010</date>
<booktitle>In INTERSPEECH,</booktitle>
<pages>1045--1048</pages>
<marker>Mikolov, Karafi´at, Burget, Cernock`y, Khudanpur, 2010</marker>
<rawString>Tomas Mikolov, Martin Karafi´at, Lukas Burget, Jan Cernock`y, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In INTERSPEECH, pages 1045–1048.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Proceedings of NIPS.</booktitle>
<contexts>
<context position="2182" citStr="Mikolov et al., 2013" startWordPosition="319" endWordPosition="322">d applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with </context>
<context position="8070" citStr="Mikolov et al., 2013" startWordPosition="1234" endWordPosition="1237">on model to transform the word embeddings to phrase embeddings (Collobert et al., 2011; Kalchbrenner and Blunsom, 2013). Gao et al. (2013) also use bag-ofwords but learn BLEU sensitive phrase embeddings. This kind of approaches does not take the word order into account and loses much information. Instead, our bilingually-constrained recursive auto-encoders not only learn the composition mechanism of generating phrases from words, but also fine tune the word embeddings during the model training stage, so that we can induce the full information of the phrases and internal words. Another method (Mikolov et al., 2013) deals with the phrases having a meaning that is not a simple composition of the meanings of its individual words, such as New York Times. They first find the phrases of this kind. Then, they regard these phrases as indivisible units, and learn their embeddings with the context information. HowMM &apos;fa 931VT France and Russia source phrase embedding ps target phrase embedding pt 112 ever, this kind of phrase embedding is hard to capture full semantics since the context of a phrase is limited. Furthermore, this method can only account for a very small part of phrases, since most of the phrases ar</context>
<context position="11397" citStr="Mikolov et al., 2013" startWordPosition="1771" endWordPosition="1774">sive auto-encoder for a fourword phrase. The empty nodes are the reconstructions of the input. We will first briefly present the unsupervised phrase embedding, and then describe the semisupervised framework. After that, we introduce the BRAE on the network structure, objective function and parameter inference. 3.1 Unsupervised Phrase Embedding 3.1.1 Word Vector Representations In phrase embedding using composition, the word vector representation is the basis and serves as the input to the neural network. After learning word embeddings with DNN (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013), each word in the vocabulary V corresponds to a vector x E Rn, and all the vectors are stacked into an embedding matrix L E Rn×|V |. Given a phrase which is an ordered list of m words, each word has an index i into the columns of the embedding matrix L. The index i is used to retrieve the word’s vector representation using a simple multiplication with a binary vector e which is zero in all positions except for the ith index: xi = Lei E Rn (1) Note that n is usually set empirically, such as n = 50,100, 200. Throughout this paper, n = 3 is used for better illustration as shown in Fig. 1. 3.1.2 </context>
<context position="21529" citStr="Mikolov et al., 2013" startWordPosition="3491" endWordPosition="3494">upervised RAE. We apply the Stochastic Gradient Descent (SGD) algorithm to optimize each parameter: θs = θs − η∂Js (14) In order to run SGD algorithm, we need to solve two problems: one for parameter initialization and the other for partial gradient calculation. In parameter initialization, θrec and θsem for the source language is randomly set according to a normal distribution. For the word embedding Ls, there are two choices. First, Ls is initialized randomly like other parameters. Second, the word embedding matrix Ls is pre-trained with DNN (Bengio et al., 2003; Collobert and Weston, 2008; Mikolov et al., 2013) using large-scale unlabeled monolingual data. We prefer to the second one since this kind of word embedding has already encoded some semantics of the words. In this work, we employ the toolkit Word2Vec (Mikolov et al., 2013) to pre-train the word embedding for the source and target languages. The word embeddings will be fine-tuned in our BRAE model to capture much more semantics. The partial gradient for one instance is computed as follows: ∂Js ∂E(s|t, θs) + λθs (15) ∂θs = ∂θs Where the source-side error given the target phrase representation includes reconstruction error and updated semantic</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Proceedings of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning continuous phrase representations and syntactic parsing with recursive neural networks.</title>
<date>2010</date>
<booktitle>In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</booktitle>
<contexts>
<context position="4761" citStr="Socher et al., 2010" startWordPosition="710" endWordPosition="713">ropose Bilinguallyconstrained Recursive Auto-encoders (BRAE) to learn semantic phrase embeddings. The core idea behind is that a phrase and its correct translation should share the same semantic meaning. Thus, they can supervise each other to learn their semantic phrase embeddings. Similarly, non-translation pairs should have different semantic meanings, and this information can also be used to guide learning semantic phrase embeddings. In our method, the standard recursive autoencoder (RAE) pre-trains the phrase embedding with an unsupervised algorithm by minimizing the reconstruction error (Socher et al., 2010), while the bilingually-constrained model learns to finetune the phrase embedding by minimizing the semantic distance between translation equivalents and maximizing the semantic distance between non-translation pairs. We use an example to explain our model. As illustrated in Fig. 1, the Chinese phrase on the left and the English phrase on the right are translations with each other. If we learn the embedding of the Chinese phrase correctly, we can regard it as the gold representation for the English phrase and use it to guide the process of learning English phrase embedding. In the other direct</context>
<context position="8967" citStr="Socher et al., 2010" startWordPosition="1389" endWordPosition="1392"> information. HowMM &apos;fa 931VT France and Russia source phrase embedding ps target phrase embedding pt 112 ever, this kind of phrase embedding is hard to capture full semantics since the context of a phrase is limited. Furthermore, this method can only account for a very small part of phrases, since most of the phrases are compositional. In contrast, our method attempts to learn the semantic vector representation for any phrase. The third method views any phrase as the meaningful composition of its internal words. The recursive auto-encoder is typically adopted to learn the way of composition (Socher et al., 2010; Socher et al., 2011; Socher et al., 2013a; Socher et al., 2013b; Li et al., 2013). They pre-train the RAE with an unsupervised algorithm. And then, they fine-tune the RAE according to the label of the phrase, such as the syntactic category in parsing (Socher et al., 2013a), the polarity in sentiment analysis (Socher et al., 2011; Socher et al., 2013b), and the reordering pattern in SMT (Li et al., 2013). This kind of semi-supervised phrase embedding is in fact performing phrase clustering with respect to the phrase label. For example, in the RAEbased phrase reordering model for SMT (Li et al</context>
</contexts>
<marker>Socher, Manning, Ng, 2010</marker>
<rawString>Richard Socher, Christopher D Manning, and Andrew Y Ng. 2010. Learning continuous phrase representations and syntactic parsing with recursive neural networks. In Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Jeffrey Pennington</author>
<author>Eric H Huang</author>
<author>Andrew Y Ng</author>
<author>Christopher D Manning</author>
</authors>
<title>Semi-supervised recursive autoencoders for predicting sentiment distributions.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>151--161</pages>
<contexts>
<context position="2906" citStr="Socher et al. (2011)" startWordPosition="432" endWordPosition="435">sing word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or</context>
<context position="8988" citStr="Socher et al., 2011" startWordPosition="1393" endWordPosition="1396">fa 931VT France and Russia source phrase embedding ps target phrase embedding pt 112 ever, this kind of phrase embedding is hard to capture full semantics since the context of a phrase is limited. Furthermore, this method can only account for a very small part of phrases, since most of the phrases are compositional. In contrast, our method attempts to learn the semantic vector representation for any phrase. The third method views any phrase as the meaningful composition of its internal words. The recursive auto-encoder is typically adopted to learn the way of composition (Socher et al., 2010; Socher et al., 2011; Socher et al., 2013a; Socher et al., 2013b; Li et al., 2013). They pre-train the RAE with an unsupervised algorithm. And then, they fine-tune the RAE according to the label of the phrase, such as the syntactic category in parsing (Socher et al., 2013a), the polarity in sentiment analysis (Socher et al., 2011; Socher et al., 2013b), and the reordering pattern in SMT (Li et al., 2013). This kind of semi-supervised phrase embedding is in fact performing phrase clustering with respect to the phrase label. For example, in the RAEbased phrase reordering model for SMT (Li et al., 2013), the phrases</context>
<context position="10474" citStr="Socher et al., 2011" startWordPosition="1625" endWordPosition="1628"> Although we also follow the composition-based phrase embedding, we are the first to focus on the semantic meanings of the phrases and propose a bilingually-constrained model to induce the semantic information and learn transformation of the semantic space in one language to the other. 3 Bilingually-constrained Recursive Auto-encoders This section introduces the Bilinguallyconstrained Recursive Auto-encoders (BRAE), that is inspired by two observations. First, the recursive auto-encoder provides a reasonable composition mechanism to embed each phrase. And the semi-supervised phrase embedding (Socher et al., 2011; Socher et al., 2013a; Li et al., 2013) further indicates that phrase embedding can be tuned with respect to the label. Second, even though we have no correct semantic phrase representation as the gold label, the phrases sharing the same meaning provide an indirect but feasible way. Figure 2: A recursive auto-encoder for a fourword phrase. The empty nodes are the reconstructions of the input. We will first briefly present the unsupervised phrase embedding, and then describe the semisupervised framework. After that, we introduce the BRAE on the network structure, objective function and paramet</context>
<context position="12291" citStr="Socher et al., 2011" startWordPosition="1947" endWordPosition="1950"> to retrieve the word’s vector representation using a simple multiplication with a binary vector e which is zero in all positions except for the ith index: xi = Lei E Rn (1) Note that n is usually set empirically, such as n = 50,100, 200. Throughout this paper, n = 3 is used for better illustration as shown in Fig. 1. 3.1.2 RAE-based Phrase Embedding Assuming we are given a phrase w1w2 · · · wm, it is first projected into a list of vectors (x1,x2, · · · , xm) using Eq. 1. The RAE learns the vector representation of the phrase by recursively combining two children vectors in a bottomup manner (Socher et al., 2011). Fig. 2 illustrates an instance of a RAE applied to a binary tree, in y3=�(W1)[y2; x4]+b) y2=�(W1)[y1; x3]+b) y1=�(W1)[x1; x2]+b) x1 x2 x3 x4 113 which a standard auto-encoder (in box) is re-used at each node. The standard auto-encoder aims at learning an abstract representation of its input. For two children c1 = x1 and c2 = x2, the autoencoder computes the parent vector y1 as follows: p = f(W(1)[c1;c2] + b(1)) (2) Where we multiply the parameter matrix W (1) E Rnx2n by the concatenation of two children [c1; c2] E R2nx1. After adding a bias term b(1), we apply an element-wise activation func</context>
<context position="14214" citStr="Socher et al., 2011" startWordPosition="2284" endWordPosition="2287">ed ones during training: 1 Erec([c1; c2]) = 2||[c1; c2] − [c&apos;1; c&apos;2]||2 (4) Given y1 = p, we can use Eq. 2 again to compute y2 by setting the children to be [c1; c2] = [y1; x3]. The same auto-encoder is re-used until the vector of the whole phrase is generated. For unsupervised phrase embedding, the only objective is to minimize the sum of reconstruction errors at each node in the optimal binary tree: 1: RAEθ(x) = argmin Erec([c1; c2]s) (5) yEA(x) sEy Where x is the list of vectors of a phrase, and A(x) denotes all the possible binary trees that can be built from inputs x. A greedy algorithm (Socher et al., 2011) is used to generate the optimal binary tree y. The parameters 0 = (W, b) are optimized over all the phrases in the training data. 3.2 Semi-supervised Phrase Embedding The above RAE is completely unsupervised and can only induce general representations of the Reconstruction Error Prediction Error Figure 3: An illustration of a semi-supervised RAE unit. Red nodes show the label distribution. multi-word phrases. Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher </context>
<context position="22351" citStr="Socher et al., 2011" startWordPosition="3629" endWordPosition="3632"> (Mikolov et al., 2013) to pre-train the word embedding for the source and target languages. The word embeddings will be fine-tuned in our BRAE model to capture much more semantics. The partial gradient for one instance is computed as follows: ∂Js ∂E(s|t, θs) + λθs (15) ∂θs = ∂θs Where the source-side error given the target phrase representation includes reconstruction error and updated semantic error: E(s|t, θs) = αErec(s; θs) + (1 − α)E∗sem(s|t, θs) (16) Given the current θs, we first construct the binary tree (as illustrated in Fig. 2) for any source-side phrase using the greedy algorithm (Socher et al., 2011). Then, the derivatives for the parameters in the fixed binary tree will be calculated via backpropagation through structures (Goller and Kuchler, 1996). Finally, the parameters will be updated using Eq. 14 and a new θs is obtained. The target-side parameters θt can be optimized in the same way as long as the source-side phrase representation ps is available. It seems a paradox that updating θs needs pt while updating θt needs ps. To solve this problem, we propose an co-training style algorithm which includes three steps: 1. Pre-training: applying unsupervised phrase embedding with standard RA</context>
</contexts>
<marker>Socher, Pennington, Huang, Ng, Manning, 2011</marker>
<rawString>Richard Socher, Jeffrey Pennington, Eric H Huang, Andrew Y Ng, and Christopher D Manning. 2011. Semi-supervised recursive autoencoders for predicting sentiment distributions. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 151–161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>John Bauer</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
</authors>
<title>Parsing with compositional vector grammars.</title>
<date>2013</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="2988" citStr="Socher et al. (2013" startWordPosition="444" endWordPosition="447">syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully</context>
<context position="9009" citStr="Socher et al., 2013" startWordPosition="1397" endWordPosition="1400">ussia source phrase embedding ps target phrase embedding pt 112 ever, this kind of phrase embedding is hard to capture full semantics since the context of a phrase is limited. Furthermore, this method can only account for a very small part of phrases, since most of the phrases are compositional. In contrast, our method attempts to learn the semantic vector representation for any phrase. The third method views any phrase as the meaningful composition of its internal words. The recursive auto-encoder is typically adopted to learn the way of composition (Socher et al., 2010; Socher et al., 2011; Socher et al., 2013a; Socher et al., 2013b; Li et al., 2013). They pre-train the RAE with an unsupervised algorithm. And then, they fine-tune the RAE according to the label of the phrase, such as the syntactic category in parsing (Socher et al., 2013a), the polarity in sentiment analysis (Socher et al., 2011; Socher et al., 2013b), and the reordering pattern in SMT (Li et al., 2013). This kind of semi-supervised phrase embedding is in fact performing phrase clustering with respect to the phrase label. For example, in the RAEbased phrase reordering model for SMT (Li et al., 2013), the phrases with the similar reo</context>
<context position="10495" citStr="Socher et al., 2013" startWordPosition="1629" endWordPosition="1632">low the composition-based phrase embedding, we are the first to focus on the semantic meanings of the phrases and propose a bilingually-constrained model to induce the semantic information and learn transformation of the semantic space in one language to the other. 3 Bilingually-constrained Recursive Auto-encoders This section introduces the Bilinguallyconstrained Recursive Auto-encoders (BRAE), that is inspired by two observations. First, the recursive auto-encoder provides a reasonable composition mechanism to embed each phrase. And the semi-supervised phrase embedding (Socher et al., 2011; Socher et al., 2013a; Li et al., 2013) further indicates that phrase embedding can be tuned with respect to the label. Second, even though we have no correct semantic phrase representation as the gold label, the phrases sharing the same meaning provide an indirect but feasible way. Figure 2: A recursive auto-encoder for a fourword phrase. The empty nodes are the reconstructions of the input. We will first briefly present the unsupervised phrase embedding, and then describe the semisupervised framework. After that, we introduce the BRAE on the network structure, objective function and parameter inference. 3.1 Uns</context>
<context position="14879" citStr="Socher et al., 2013" startWordPosition="2387" endWordPosition="2390">. The parameters 0 = (W, b) are optimized over all the phrases in the training data. 3.2 Semi-supervised Phrase Embedding The above RAE is completely unsupervised and can only induce general representations of the Reconstruction Error Prediction Error Figure 3: An illustration of a semi-supervised RAE unit. Red nodes show the label distribution. multi-word phrases. Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al., 2011), syntactic category in parsing (Socher et al., 2013a) and phrase reordering pattern in SMT (Li et al., 2013). In the semi-supervised RAE for phrase embedding, the objective function over a (phrase, label) pair (x, t) includes the reconstruction error and the prediction error, as illustrated in Fig. 3. E(x, t; 0) = αErec(x, t; 0)+(1−α)Epred(x, t; 0) (6) Where the hyper-parameter α is used to balance the reconstruction and prediction error. For label prediction, the cross-entropy error is usually used to calculate Epred. By optimizing the above objective, the phrases in the vector embedding space will be grouped according to the labels. 3.3 The </context>
</contexts>
<marker>Socher, Bauer, Manning, Ng, 2013</marker>
<rawString>Richard Socher, John Bauer, Christopher D Manning, and Andrew Y Ng. 2013a. Parsing with compositional vector grammars. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Socher</author>
<author>Alex Perelygin</author>
<author>Jean Y Wu</author>
<author>Jason Chuang</author>
<author>Christopher D Manning</author>
<author>Andrew Y Ng</author>
<author>Christopher Potts</author>
</authors>
<title>Recursive deep models for semantic compositionality over a sentiment treebank.</title>
<date>2013</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2988" citStr="Socher et al. (2013" startWordPosition="444" endWordPosition="447">syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation process, such as modelling the decoding process, learning compact vector representations for the basic phrasal translation units is the essential and fundamental work. In this paper, we explore the phrase embedding, which represents a phrase (sequence of words) with a real-valued vector. In some previous works, phrase embedding has been discussed from different views. Socher et al. (2011) make the phrase embeddings capture the sentiment information. Socher et al. (2013a) enable the phrase embeddings to mainly capture the syntactic knowledge. Li et al. (2013) attempt to encode the reordering pattern in the phrase embeddings. Kalchbrenner and Blunsom (2013) utilize a simple convolution model to generate phrase embeddings from word embeddings. Mikolov et al. (2013) consider a phrase as an indivisible n-gram. Obviously, these methods of learning phrase embeddings either focus on some aspects of the phrase (e.g. reordering pattern), or impose strong assumptions (e.g. bagof-words or indivisible n-gram). Therefore, these phrase embeddings are not suitable to fully</context>
<context position="9009" citStr="Socher et al., 2013" startWordPosition="1397" endWordPosition="1400">ussia source phrase embedding ps target phrase embedding pt 112 ever, this kind of phrase embedding is hard to capture full semantics since the context of a phrase is limited. Furthermore, this method can only account for a very small part of phrases, since most of the phrases are compositional. In contrast, our method attempts to learn the semantic vector representation for any phrase. The third method views any phrase as the meaningful composition of its internal words. The recursive auto-encoder is typically adopted to learn the way of composition (Socher et al., 2010; Socher et al., 2011; Socher et al., 2013a; Socher et al., 2013b; Li et al., 2013). They pre-train the RAE with an unsupervised algorithm. And then, they fine-tune the RAE according to the label of the phrase, such as the syntactic category in parsing (Socher et al., 2013a), the polarity in sentiment analysis (Socher et al., 2011; Socher et al., 2013b), and the reordering pattern in SMT (Li et al., 2013). This kind of semi-supervised phrase embedding is in fact performing phrase clustering with respect to the phrase label. For example, in the RAEbased phrase reordering model for SMT (Li et al., 2013), the phrases with the similar reo</context>
<context position="10495" citStr="Socher et al., 2013" startWordPosition="1629" endWordPosition="1632">low the composition-based phrase embedding, we are the first to focus on the semantic meanings of the phrases and propose a bilingually-constrained model to induce the semantic information and learn transformation of the semantic space in one language to the other. 3 Bilingually-constrained Recursive Auto-encoders This section introduces the Bilinguallyconstrained Recursive Auto-encoders (BRAE), that is inspired by two observations. First, the recursive auto-encoder provides a reasonable composition mechanism to embed each phrase. And the semi-supervised phrase embedding (Socher et al., 2011; Socher et al., 2013a; Li et al., 2013) further indicates that phrase embedding can be tuned with respect to the label. Second, even though we have no correct semantic phrase representation as the gold label, the phrases sharing the same meaning provide an indirect but feasible way. Figure 2: A recursive auto-encoder for a fourword phrase. The empty nodes are the reconstructions of the input. We will first briefly present the unsupervised phrase embedding, and then describe the semisupervised framework. After that, we introduce the BRAE on the network structure, objective function and parameter inference. 3.1 Uns</context>
<context position="14879" citStr="Socher et al., 2013" startWordPosition="2387" endWordPosition="2390">. The parameters 0 = (W, b) are optimized over all the phrases in the training data. 3.2 Semi-supervised Phrase Embedding The above RAE is completely unsupervised and can only induce general representations of the Reconstruction Error Prediction Error Figure 3: An illustration of a semi-supervised RAE unit. Red nodes show the label distribution. multi-word phrases. Several researchers extend the original RAEs to a semi-supervised setting so that the induced phrase embedding can predict a target label, such as polarity in sentiment analysis (Socher et al., 2011), syntactic category in parsing (Socher et al., 2013a) and phrase reordering pattern in SMT (Li et al., 2013). In the semi-supervised RAE for phrase embedding, the objective function over a (phrase, label) pair (x, t) includes the reconstruction error and the prediction error, as illustrated in Fig. 3. E(x, t; 0) = αErec(x, t; 0)+(1−α)Epred(x, t; 0) (6) Where the hyper-parameter α is used to balance the reconstruction and prediction error. For label prediction, the cross-entropy error is usually used to calculate Epred. By optimizing the above objective, the phrases in the vector embedding space will be grouped according to the labels. 3.3 The </context>
</contexts>
<marker>Socher, Perelygin, Wu, Chuang, Manning, Ng, Potts, 2013</marker>
<rawString>Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. 2013b. Recursive deep models for semantic compositionality over a sentiment treebank. In Proceedings of the Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nadi Tomeh</author>
<author>Nicola Cancedda</author>
<author>Marc Dymetman</author>
</authors>
<title>Complexity-based phrase-table filtering for statistical machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of Summit XII,</booktitle>
<pages>144--151</pages>
<contexts>
<context position="26330" citStr="Tomeh et al., 2009" startWordPosition="4284" endWordPosition="4287">high-quality bilingual phrase pairs to train our BRAE model, we perform forced decoding for the bilingual training sentences and collect the phrase pairs used. After removing the duplicates, the remaining 1.12M bilingual phrase pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on translation quality is very important for translation especially in environments where memory and time constraints are imposed. Many algorithms have been proposed to deal with this problem, such as significance pruning (Johnson et al., 2007; Tomeh et al., 2009), relevance pruning (Eck et al., 2007) and entropy-based pruning 2LDC category numbers: LDC2000T50, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2005T34. (Ling et al., 2012; Zens et al., 2012). These algorithms are based on corpus statistics including cooccurrence statistics, phrase pair usage and composition information. For example, the significance pruning, which is proven to be a very effective algorithm, computes the probability named p-value, that tests whether a source phrase s and a target phrase t co-occur more frequently in a bilingual corpus than the</context>
</contexts>
<marker>Tomeh, Cancedda, Dymetman, 2009</marker>
<rawString>Nadi Tomeh, Nicola Cancedda, and Marc Dymetman. 2009. Complexity-based phrase-table filtering for statistical machine translation. In Proceedings of Summit XII, pages 144–151.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ashish Vaswani</author>
<author>Yinggong Zhao</author>
<author>Victoria Fossum</author>
<author>David Chiang</author>
</authors>
<title>Decoding with largescale neural language models improves translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1387--1392</pages>
<contexts>
<context position="1914" citStr="Vaswani et al., 2013" startWordPosition="275" endWordPosition="278">(multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings as the direct inputs to DNN cannot make full use of the whole syntactic and semantic information of the phrasal translation rules. Therefore, in order to successfully apply DNN to model the whole translation</context>
</contexts>
<marker>Vaswani, Zhao, Fossum, Chiang, 2013</marker>
<rawString>Ashish Vaswani, Yinggong Zhao, Victoria Fossum, and David Chiang. 2013. Decoding with largescale neural language models improves translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational linguistics,</journal>
<pages>23--3</pages>
<contexts>
<context position="24787" citStr="Wu, 1997" startWordPosition="4040" endWordPosition="4041">ht α in Eq. 10, λs in Eq. 11, and the learning rate η in Eq. 14. For the dimensionality n, we have tried three settings n = 50,100, 200 in our experiments. We (13) 116 empirically set the learning rate q = 0.01. We draw α from 0.05 to 0.5 with step 0.05, and As from {10−6,10−5,10−4,10−3,10−2}. The overall error of the BRAE model is employed to guide the search procedure. Finally, we choose α = 0.15, AL = 10−2, Arec = 10−3 and Asem = 10−3. 4.2 SMT Setup We have implemented a phrase-based translation system with a maximum entropy based reordering model using the bracketing transduction grammar (Wu, 1997; Xiong et al., 2006). The SMT evaluation is conducted on Chineseto-English translation. Accordingly, our BRAE model is trained on Chinese and English. The bilingual training data from LDC 2 contains 0.96M sentence pairs and 1.1M entity pairs with 27.7M Chinese words and 31.9M English words. A 5- gram language model is trained on the Xinhua portion of the English Gigaword corpus and the English part of bilingual training data. The NIST MT03 is used as the development data. NIST MT04-06 and MT08 (news data) are used as the test data. Case-insensitive BLEU is employed as the evaluation metric. T</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational linguistics, 23(3):377–403.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Deyi Xiong</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Maximum entropy based phrase reordering model for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of ACLCOLING,</booktitle>
<pages>505--512</pages>
<contexts>
<context position="24808" citStr="Xiong et al., 2006" startWordPosition="4042" endWordPosition="4045">. 10, λs in Eq. 11, and the learning rate η in Eq. 14. For the dimensionality n, we have tried three settings n = 50,100, 200 in our experiments. We (13) 116 empirically set the learning rate q = 0.01. We draw α from 0.05 to 0.5 with step 0.05, and As from {10−6,10−5,10−4,10−3,10−2}. The overall error of the BRAE model is employed to guide the search procedure. Finally, we choose α = 0.15, AL = 10−2, Arec = 10−3 and Asem = 10−3. 4.2 SMT Setup We have implemented a phrase-based translation system with a maximum entropy based reordering model using the bracketing transduction grammar (Wu, 1997; Xiong et al., 2006). The SMT evaluation is conducted on Chineseto-English translation. Accordingly, our BRAE model is trained on Chinese and English. The bilingual training data from LDC 2 contains 0.96M sentence pairs and 1.1M entity pairs with 27.7M Chinese words and 31.9M English words. A 5- gram language model is trained on the Xinhua portion of the English Gigaword corpus and the English part of bilingual training data. The NIST MT03 is used as the development data. NIST MT04-06 and MT08 (news data) are used as the test data. Case-insensitive BLEU is employed as the evaluation metric. The statistical signif</context>
</contexts>
<marker>Xiong, Liu, Lin, 2006</marker>
<rawString>Deyi Xiong, Qun Liu, and Shouxun Lin. 2006. Maximum entropy based phrase reordering model for statistical machine translation. In Proceedings of ACLCOLING, pages 505–512.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nan Yang</author>
<author>Shujie Liu</author>
<author>Mu Li</author>
<author>Ming Zhou</author>
<author>Nenghai Yu</author>
</authors>
<title>Word alignment modeling with context dependent deep neural network.</title>
<date>2013</date>
<booktitle>In 51st Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="1633" citStr="Yang et al., 2013" startWordPosition="230" endWordPosition="233">ties) which need to measure semantic similarity between a source phrase and its translation candidates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, </context>
</contexts>
<marker>Yang, Liu, Li, Zhou, Yu, 2013</marker>
<rawString>Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and Nenghai Yu. 2013. Word alignment modeling with context dependent deep neural network. In 51st Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Zens</author>
<author>Daisy Stanton</author>
<author>Peng Xu</author>
</authors>
<title>A systematic comparison of phrase table pruning techniques.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>972--983</pages>
<contexts>
<context position="26555" citStr="Zens et al., 2012" startWordPosition="4314" endWordPosition="4317">e pairs (length ranging from 1 to 7) are obtained. 4.3 Phrase Table Pruning Pruning most of the phrase table without much impact on translation quality is very important for translation especially in environments where memory and time constraints are imposed. Many algorithms have been proposed to deal with this problem, such as significance pruning (Johnson et al., 2007; Tomeh et al., 2009), relevance pruning (Eck et al., 2007) and entropy-based pruning 2LDC category numbers: LDC2000T50, LDC2002L27, LDC2003E07, LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2005T34. (Ling et al., 2012; Zens et al., 2012). These algorithms are based on corpus statistics including cooccurrence statistics, phrase pair usage and composition information. For example, the significance pruning, which is proven to be a very effective algorithm, computes the probability named p-value, that tests whether a source phrase s and a target phrase t co-occur more frequently in a bilingual corpus than they happen just by chance. The higher the p-value, the more likely of the phrase pair to be spurious. Our work has the same objective, but instead of using corpus statistics, we attempt to measure the quality of the phrase pair</context>
</contexts>
<marker>Zens, Stanton, Xu, 2012</marker>
<rawString>Richard Zens, Daisy Stanton, and Peng Xu. 2012. A systematic comparison of phrase table pruning techniques. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 972–983.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Will Y Zou</author>
<author>Richard Socher</author>
<author>Daniel Cer</author>
<author>Christopher D Manning</author>
</authors>
<title>Bilingual word embeddings for phrase-based machine translation.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1393--1398</pages>
<contexts>
<context position="1727" citStr="Zou et al., 2013" startWordPosition="246" endWordPosition="249">didates. Extensive experiments show that the BRAE is remarkably effective in these two tasks. 1 Introduction Due to the powerful capacity of feature learning and representation, Deep (multi-layer) Neural Networks (DNN) have achieved a great success in speech and image processing (Kavukcuoglu et al., 2010; Krizhevsky et al., 2012; Dahl et al., 2012). Recently, statistical machine translation (SMT) community has seen a strong interest in adapting and applying DNN to many tasks, such as word alignment (Yang et al., 2013), translation confidence estimation (Mikolov et al., 2010; Liu et al., 2013; Zou et al., 2013), phrase reordering prediction (Li et al., 2013), translation modelling (Auli et al., 2013; Kalchbrenner and Blunsom, 2013) and language modelling (Duh et al., 2013; Vaswani et al., 2013). Most of these works attempt to improve some components in SMT based on word embedding, which converts a word into a dense, low dimensional, real-valued vector representation (Bengio et al., 2003; Bengio et al., 2006; Collobert and Weston, 2008; Mikolov et al., 2013). However, in the conventional (phrase-based) SMT, phrases are the basic translation units. The models using word embeddings as the direct inputs</context>
</contexts>
<marker>Zou, Socher, Cer, Manning, 2013</marker>
<rawString>Will Y Zou, Richard Socher, Daniel Cer, and Christopher D Manning. 2013. Bilingual word embeddings for phrase-based machine translation. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1393–1398.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>