<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008077">
<title confidence="0.998705">
Online Statistics for a Unification-Based Dialogue Parser
</title>
<author confidence="0.99941">
Micha Elsner, Mary Swift, James Allen, and Daniel Gildea
</author>
<affiliation confidence="0.997984">
Department of Computer Science
University of Rochester
</affiliation>
<address confidence="0.874199">
Rochester, NY 14627
</address>
<email confidence="0.999695">
{melsner,swift,allen,gildea}@cs.rochester.edu
</email>
<sectionHeader confidence="0.998606" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999305">
We describe a method for augmenting
unification-based deep parsing with statis-
tical methods. We extend and adapt the
Bikel parser, which uses head-driven lex-
ical statistics, to dialogue. We show that
our augmented parser produces signifi-
cantly fewer constituents than the baseline
system and achieves comparable brack-
eting accuracy, even yielding slight im-
provements for longer sentences.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99995075">
Unification parsers have problems with efficiency
and selecting the best parse. Lexically-conditioned
statistics as used by Collins (1999) may provide a
solution. They have been used in three ways: as
a postprocess for parse selection (Toutanova et al.,
2005; Riezler et al., 2000; Riezler et al., 2002), a
preprocess to find more probable bracketing struc-
tures (Swift et al., 2004), and online to rank each
constituent produced, as in Tsuruoka et al. (2004)
and this experiment.
The TRIPS parser (Allen et al., 1996) is a unifi-
cation parser using an HPSG-inspired grammar and
hand-tuned weights for each rule. In our augmented
system (Aug-TRIPS), we replaced these weights
with a lexically-conditioned model based on the
adaptation of Collins used by Bikel (2002), allowing
more efficiency and (in some cases) better selection.
Aug-TRIPS retains the same grammar and lexicon
as TRIPS, but uses its statistical model to determine
the order in which unifications are attempted.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="introduction">
2 Experiments
</sectionHeader>
<bodyText confidence="0.999893916666667">
We tested bracketing accuracy on the Monroe cor-
pus (Stent, 2001), which contains collaborative
emergency-management dialogues. Aug-TRIPS is
comparable to TRIPS in accuracy, but produces
fewer constituents (Table 1). The Bikel parser has
slightly higher precision/recall than either TRIPS
or Aug-TRIPS, since it can choose any bracketing
structure regardless of semantic coherence, while
the TRIPS systems must find a legal pattern of fea-
ture unifications. Aug-TRIPS also has better preci-
sion/recall when parsing the longer sentences (Ta-
ble 2).
</bodyText>
<table confidence="0.9977874">
(training=9282) Bikel Aug-TRIPS TRIPS
Recall 79.40 76.09 76.77
Precision 79.40 77.08 78.20
Complete Match 42.00 46.00 65.00
% Constit. Reduction - 36.96 0.00
</table>
<tableCaption confidence="0.9397115">
Table 1: Bracketing accuracy for 100 random sen-
tences ≥ 2 words.
</tableCaption>
<table confidence="0.99975075">
&gt; 7 Aug-TRIPS &gt; 7 TRIPS
Recall 73.25 71.00
Precision 74.78 73.44
Complete Match 22.50 37.50
</table>
<tableCaption confidence="0.937592">
Table 2: Bracketing accuracy for the 40 sentences &gt;
7 words.
</tableCaption>
<bodyText confidence="0.997647666666667">
Since our motivation for unification parsing is to
reveal semantics as well as syntax, we next evalu-
ated Aug-TRIPS’s production of correct interpreta-
tions at the sentence level, which require complete
correctness not only of the bracketing structure but
of the sense chosen for each word and the thematic
</bodyText>
<page confidence="0.948036">
198
</page>
<bodyText confidence="0.906818695652174">
Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 198–199,
Vancouver, October 2005. c�2005 Association for Computational Linguistics
roles of each argument (Tetreault et al., 2004).
For this task, we modified the probability model
to condition on the senses in our lexicon rather than
words. For instance, the words “two thousand dol-
lars” are replaced with the senses “number number-
unit money-unit”. This allows us to model lexi-
cal disambiguation explicitly. The model generates
one or more senses from each word with probability
P(senselword, tag), and then uses sense statistics
rather than word statistics in all other calculations.
Similar but more complex models were used in the
PCFG-sem model of Toutanova et al. (2005) and us-
ing WordNet senses in Bikel (2000).
We used the Projector dialogues (835 sentences),
which concern purchasing video projectors. In this
domain, Aug-TRIPS makes about 10% more inter-
pretation errors than TRIPS (Table 3), but when
parsing sentences on which TRIPS itself makes er-
rors, it can correct about 10% (Table 4).
Table 3: Sentence-level accuracy on 75 random sen-
tences.
</bodyText>
<sectionHeader confidence="0.821772" genericHeader="references">
References
</sectionHeader>
<bodyText confidence="0.669683142857143">
James F. Allen, Bradford W. Miller, Eric K. Ringger, and
Teresa Sikorski. 1996. A robust system for natural
spoken dialogue. In Proceedings of the 1996 Annual
Meeting of the Association for Computational Linguis-
tics (ACL’96).
Daniel Bikel. 2000. A statistical model for parsing
and word-sense disambiguation. In Proceedings of
</bodyText>
<figureCaption confidence="0.894968105263158">
the Joint SIGDAT Conference on Empirical Methods
in Natural Language Processing and Very Large Cor-
pora, Hong Kong.
Daniel Bikel. 2002. Design of a multi-lingual, parallel-
processing statistical parsing engine. In Human Lan-
guage Technology Conference (HLT), San Diego.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Stefan Riezler, Detlef Prescher, Jonas Kuhn, and Mark
Johnson. 2000. Lexicalized stochastic modeling of
constraint-based grammars using log-linear measures
and EM training. In Proceedings of the 38th Annual
Meeting of the ACL, Hong Kong.
Stefan Riezler, Tracy H. King, Richard Crouch, and
John T. Maxwell. 2002. Parsing the Wall Street Jour-
nal using a Lexical-Functional Grammar and discrim-
inative estimation. In Proceedings of the 40th Annual
Meeting of the ACL, Philadelphia.
</figureCaption>
<figure confidence="0.981925583333333">
(training=310)
TRIPS
Aug-TRIPS
Correct
Incorrect
% Reduction in Constituents
26
49
0%
21
54
45%
</figure>
<tableCaption confidence="0.8101555">
Table 4: Sentence-level accuracy on 54 TRIPS error
sentences
</tableCaption>
<bodyText confidence="0.995303692307692">
Our parser makes substantially fewer constituents
than baseline TRIPS at only slightly lower accu-
racy. Tsuruoka et al. (2004) achieved a much higher
speedup (30 times) than we did; this is partly due to
their use of the Penn Treebank, which contains much
more data than our corpora. In addition, however,
their baseline system is a classic HPSG parser with
no efficiency features, while our baseline, TRIPS, is
designed as a real-time dialogue parser which uses
hand-tuned weights to guide its search and imposes
a maximum chart size.
Acknowledgements Our thanks to Will DeBeau-
mont and four anonymous reviewers.
</bodyText>
<reference confidence="0.995105904761905">
Amanda J. Stent. 2001. Dialogue Systems as Conversa-
tional Partners. Ph.D. thesis, University of Rochester.
Mary Swift, James Allen, and Daniel Gildea. 2004.
Skeletons in the parser: Using a shallow parser to im-
prove deep parsing. In Proceedings of the 20th In-
ternational Conference on Computational Linguistics
(COLING-04), Geneva, Switzerland, August.
Joel Tetreault, Mary Swift, Preethum Prithviraj, My-
roslava Dzikovska, and James Allen. 2004. Discourse
annotation in the Monroe corpus. In ACL workshop on
Discourse Annotation, Barcelona, Spain, July.
Kristina Toutanova, Christopher D. Manning, Dan
Flickinger, and Stephan Oepen. 2005. Stochastic
HPSG parse disambiguation using the Redwoods cor-
pus. Journal ofLogic and Computation.
Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii.
2004. Towards efficient probabilistic HPSG parsing:
Integrating semantic and syntactic preference to guide
the parsing. In Proceedings of IJCNLP-04 Workshop:
Beyond Shallow Analyses- Formalisms and Statistical
Modeling for Deep Analyses, Sanya City, China.
</reference>
<figure confidence="0.97015475">
(training=396)
TRIPS
Aug-TRIPS
Correct
Incorrect
% Reduction in Constituents
0
54
0%
8
46
46%
</figure>
<page confidence="0.971291">
199
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.930991">
<title confidence="0.998773">Online Statistics for a Unification-Based Dialogue Parser</title>
<author confidence="0.993044">Micha Elsner</author>
<author confidence="0.993044">Mary Swift</author>
<author confidence="0.993044">James Allen</author>
<author confidence="0.993044">Daniel</author>
<affiliation confidence="0.9997665">Department of Computer University of</affiliation>
<address confidence="0.96207">Rochester, NY</address>
<abstract confidence="0.997737636363636">We describe a method for augmenting unification-based deep parsing with statistical methods. We extend and adapt the Bikel parser, which uses head-driven lexical statistics, to dialogue. We show that our augmented parser produces significantly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Amanda J Stent</author>
</authors>
<title>Dialogue Systems as Conversational Partners.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Rochester.</institution>
<contexts>
<context position="1704" citStr="Stent, 2001" startWordPosition="251" endWordPosition="252">a et al. (2004) and this experiment. The TRIPS parser (Allen et al., 1996) is a unification parser using an HPSG-inspired grammar and hand-tuned weights for each rule. In our augmented system (Aug-TRIPS), we replaced these weights with a lexically-conditioned model based on the adaptation of Collins used by Bikel (2002), allowing more efficiency and (in some cases) better selection. Aug-TRIPS retains the same grammar and lexicon as TRIPS, but uses its statistical model to determine the order in which unifications are attempted. 2 Experiments We tested bracketing accuracy on the Monroe corpus (Stent, 2001), which contains collaborative emergency-management dialogues. Aug-TRIPS is comparable to TRIPS in accuracy, but produces fewer constituents (Table 1). The Bikel parser has slightly higher precision/recall than either TRIPS or Aug-TRIPS, since it can choose any bracketing structure regardless of semantic coherence, while the TRIPS systems must find a legal pattern of feature unifications. Aug-TRIPS also has better precision/recall when parsing the longer sentences (Table 2). (training=9282) Bikel Aug-TRIPS TRIPS Recall 79.40 76.09 76.77 Precision 79.40 77.08 78.20 Complete Match 42.00 46.00 65</context>
</contexts>
<marker>Stent, 2001</marker>
<rawString>Amanda J. Stent. 2001. Dialogue Systems as Conversational Partners. Ph.D. thesis, University of Rochester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mary Swift</author>
<author>James Allen</author>
<author>Daniel Gildea</author>
</authors>
<title>Skeletons in the parser: Using a shallow parser to improve deep parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04),</booktitle>
<location>Geneva, Switzerland,</location>
<contexts>
<context position="1031" citStr="Swift et al., 2004" startWordPosition="143" endWordPosition="146">stics, to dialogue. We show that our augmented parser produces significantly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences. 1 Introduction Unification parsers have problems with efficiency and selecting the best parse. Lexically-conditioned statistics as used by Collins (1999) may provide a solution. They have been used in three ways: as a postprocess for parse selection (Toutanova et al., 2005; Riezler et al., 2000; Riezler et al., 2002), a preprocess to find more probable bracketing structures (Swift et al., 2004), and online to rank each constituent produced, as in Tsuruoka et al. (2004) and this experiment. The TRIPS parser (Allen et al., 1996) is a unification parser using an HPSG-inspired grammar and hand-tuned weights for each rule. In our augmented system (Aug-TRIPS), we replaced these weights with a lexically-conditioned model based on the adaptation of Collins used by Bikel (2002), allowing more efficiency and (in some cases) better selection. Aug-TRIPS retains the same grammar and lexicon as TRIPS, but uses its statistical model to determine the order in which unifications are attempted. 2 Exp</context>
</contexts>
<marker>Swift, Allen, Gildea, 2004</marker>
<rawString>Mary Swift, James Allen, and Daniel Gildea. 2004. Skeletons in the parser: Using a shallow parser to improve deep parsing. In Proceedings of the 20th International Conference on Computational Linguistics (COLING-04), Geneva, Switzerland, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Mary Swift</author>
<author>Preethum Prithviraj</author>
<author>Myroslava Dzikovska</author>
<author>James Allen</author>
</authors>
<title>Discourse annotation in the Monroe corpus.</title>
<date>2004</date>
<booktitle>In ACL workshop on Discourse Annotation,</booktitle>
<location>Barcelona, Spain,</location>
<contexts>
<context position="3085" citStr="Tetreault et al., 2004" startWordPosition="457" endWordPosition="460">.78 73.44 Complete Match 22.50 37.50 Table 2: Bracketing accuracy for the 40 sentences &gt; 7 words. Since our motivation for unification parsing is to reveal semantics as well as syntax, we next evaluated Aug-TRIPS’s production of correct interpretations at the sentence level, which require complete correctness not only of the bracketing structure but of the sense chosen for each word and the thematic 198 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 198–199, Vancouver, October 2005. c�2005 Association for Computational Linguistics roles of each argument (Tetreault et al., 2004). For this task, we modified the probability model to condition on the senses in our lexicon rather than words. For instance, the words “two thousand dollars” are replaced with the senses “number numberunit money-unit”. This allows us to model lexical disambiguation explicitly. The model generates one or more senses from each word with probability P(senselword, tag), and then uses sense statistics rather than word statistics in all other calculations. Similar but more complex models were used in the PCFG-sem model of Toutanova et al. (2005) and using WordNet senses in Bikel (2000). We used the</context>
</contexts>
<marker>Tetreault, Swift, Prithviraj, Dzikovska, Allen, 2004</marker>
<rawString>Joel Tetreault, Mary Swift, Preethum Prithviraj, Myroslava Dzikovska, and James Allen. 2004. Discourse annotation in the Monroe corpus. In ACL workshop on Discourse Annotation, Barcelona, Spain, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kristina Toutanova</author>
<author>Christopher D Manning</author>
<author>Dan Flickinger</author>
<author>Stephan Oepen</author>
</authors>
<title>Stochastic HPSG parse disambiguation using the Redwoods corpus. Journal ofLogic and Computation.</title>
<date>2005</date>
<contexts>
<context position="907" citStr="Toutanova et al., 2005" startWordPosition="122" endWordPosition="125">ication-based deep parsing with statistical methods. We extend and adapt the Bikel parser, which uses head-driven lexical statistics, to dialogue. We show that our augmented parser produces significantly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences. 1 Introduction Unification parsers have problems with efficiency and selecting the best parse. Lexically-conditioned statistics as used by Collins (1999) may provide a solution. They have been used in three ways: as a postprocess for parse selection (Toutanova et al., 2005; Riezler et al., 2000; Riezler et al., 2002), a preprocess to find more probable bracketing structures (Swift et al., 2004), and online to rank each constituent produced, as in Tsuruoka et al. (2004) and this experiment. The TRIPS parser (Allen et al., 1996) is a unification parser using an HPSG-inspired grammar and hand-tuned weights for each rule. In our augmented system (Aug-TRIPS), we replaced these weights with a lexically-conditioned model based on the adaptation of Collins used by Bikel (2002), allowing more efficiency and (in some cases) better selection. Aug-TRIPS retains the same gr</context>
<context position="3631" citStr="Toutanova et al. (2005)" startWordPosition="545" endWordPosition="548">for Computational Linguistics roles of each argument (Tetreault et al., 2004). For this task, we modified the probability model to condition on the senses in our lexicon rather than words. For instance, the words “two thousand dollars” are replaced with the senses “number numberunit money-unit”. This allows us to model lexical disambiguation explicitly. The model generates one or more senses from each word with probability P(senselword, tag), and then uses sense statistics rather than word statistics in all other calculations. Similar but more complex models were used in the PCFG-sem model of Toutanova et al. (2005) and using WordNet senses in Bikel (2000). We used the Projector dialogues (835 sentences), which concern purchasing video projectors. In this domain, Aug-TRIPS makes about 10% more interpretation errors than TRIPS (Table 3), but when parsing sentences on which TRIPS itself makes errors, it can correct about 10% (Table 4). Table 3: Sentence-level accuracy on 75 random sentences. References James F. Allen, Bradford W. Miller, Eric K. Ringger, and Teresa Sikorski. 1996. A robust system for natural spoken dialogue. In Proceedings of the 1996 Annual Meeting of the Association for Computational Lin</context>
</contexts>
<marker>Toutanova, Manning, Flickinger, Oepen, 2005</marker>
<rawString>Kristina Toutanova, Christopher D. Manning, Dan Flickinger, and Stephan Oepen. 2005. Stochastic HPSG parse disambiguation using the Redwoods corpus. Journal ofLogic and Computation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoshimasa Tsuruoka</author>
<author>Yusuke Miyao</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>Towards efficient probabilistic HPSG parsing: Integrating semantic and syntactic preference to guide the parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of IJCNLP-04 Workshop: Beyond Shallow Analyses- Formalisms and Statistical Modeling for Deep Analyses,</booktitle>
<location>Sanya City, China.</location>
<contexts>
<context position="1107" citStr="Tsuruoka et al. (2004)" startWordPosition="156" endWordPosition="159">tly fewer constituents than the baseline system and achieves comparable bracketing accuracy, even yielding slight improvements for longer sentences. 1 Introduction Unification parsers have problems with efficiency and selecting the best parse. Lexically-conditioned statistics as used by Collins (1999) may provide a solution. They have been used in three ways: as a postprocess for parse selection (Toutanova et al., 2005; Riezler et al., 2000; Riezler et al., 2002), a preprocess to find more probable bracketing structures (Swift et al., 2004), and online to rank each constituent produced, as in Tsuruoka et al. (2004) and this experiment. The TRIPS parser (Allen et al., 1996) is a unification parser using an HPSG-inspired grammar and hand-tuned weights for each rule. In our augmented system (Aug-TRIPS), we replaced these weights with a lexically-conditioned model based on the adaptation of Collins used by Bikel (2002), allowing more efficiency and (in some cases) better selection. Aug-TRIPS retains the same grammar and lexicon as TRIPS, but uses its statistical model to determine the order in which unifications are attempted. 2 Experiments We tested bracketing accuracy on the Monroe corpus (Stent, 2001), w</context>
</contexts>
<marker>Tsuruoka, Miyao, Tsujii, 2004</marker>
<rawString>Yoshimasa Tsuruoka, Yusuke Miyao, and Jun’ichi Tsujii. 2004. Towards efficient probabilistic HPSG parsing: Integrating semantic and syntactic preference to guide the parsing. In Proceedings of IJCNLP-04 Workshop: Beyond Shallow Analyses- Formalisms and Statistical Modeling for Deep Analyses, Sanya City, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>