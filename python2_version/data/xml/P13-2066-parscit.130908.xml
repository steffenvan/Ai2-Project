<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000204">
<title confidence="0.945784">
A Novel Translation Framework Based on Rhetorical Structure Theory
</title>
<author confidence="0.982929">
Mei Tu Yu Zhou Chengqing Zong
</author>
<affiliation confidence="0.9695445">
National Laboratory of Pattern Recognition, Institute of Automation,
Chinese Academy of Sciences
</affiliation>
<email confidence="0.982014">
{mtu,yzhou,cqzong}@nlpr.ia.ac.cn
</email>
<sectionHeader confidence="0.997187" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993885470588235">
Rhetorical structure theory (RST) is widely
used for discourse understanding, which repre-
sents a discourse as a hierarchically semantic
structure. In this paper, we propose a novel
translation framework with the help of RST. In
our framework, the translation process mainly
includes three steps: 1) Source RST-tree ac-
quisition: a source sentence is parsed into an
RST tree; 2) Rule extraction: translation rules
are extracted from the source tree and the tar-
get string via bilingual word alignment; 3)
RST-based translation: the source RST-tree
is translated with translation rules. Experi-
ments on Chinese-to-English show that our
RST-based approach achieves improvements
of 2.3/0.77/1.43 BLEU points on
NIST04/NIST05/CWMT2008 respectively.
</bodyText>
<sectionHeader confidence="0.999503" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999203975">
For statistical machine translation (SMT), a cru-
cial issue is how to build a translation model to
extract as much accurate and generative transla-
tion knowledge as possible. The existing SMT
models have made much progress. However,
they still suffer from the bad performance of un-
natural or even unreadable translation, especially
when the sentences become complicated. We
think the deep reason is that those models only
extract translation information on lexical or syn-
tactic level, but fail to give an overall under-
standing of source sentences on semantic level of
discourse. In order to solve such problem, (Gong
et al., 2011; Xiao et al., 2011; Wong and Kit,
2012) build discourse-based translation models
to ensure the lexical coherence or consistency.
Although some lexicons can be translated better
by their models, the overall structure still re-
mains unnatural. Marcu et al. (2000) design a
discourse structure transferring module, but leave
much work to do, especially on how to integrate
this module into SMT and how to automatically
analyze the structures. Those reasons urge us to
seek a new translation framework under the idea
of “translation with overall understanding”.
Rhetorical structure theory (RST) (Mann and
Thompson, 1988) provides us with a good per-
spective and inspiration to build such a frame-
work. Generally, an RST tree can explicitly show
the minimal spans with semantic functional in-
tegrity, which are called elementary discourse
units (edus) (Marcu et al., 2000), and it also de-
picts the hierarchical relations among edus. Fur-
thermore, since different languages’ edus are
usually equivalent on semantic level, it is intui-
tive to create a new framework based on RST by
directly mapping the source edus to target ones.
Taking the Chinese-to-English translation as
an example, our translation framework works as
the following steps:
</bodyText>
<listItem confidence="0.9942275">
1) Source RST-tree acquisition: a source
sentence is parsed into an RST-tree;
2) Rule extraction: translation rules are ex-
tracted from the source tree and the target string
via bilingual word alignment;
3) RST-based translation: the source RST-
tree is translated into target sentence with ex-
tracted translation rules.
</listItem>
<bodyText confidence="0.99682">
Experiments on Chinese-to-English sentence-
level discourses demonstrate that this method
achieves significant improvements.
</bodyText>
<sectionHeader confidence="0.996433" genericHeader="method">
2 Chinese RST Parser
</sectionHeader>
<subsectionHeader confidence="0.999969">
2.1 Annotation of Chinese RST Tree
</subsectionHeader>
<bodyText confidence="0.997446">
Similar to (Soricut and Marcu, 2003), a node of
RST tree is represented as a tuple R-[s, m, e],
which means the relation R controls two seman-
tic spans U1 and U2 , U1 starts from word position
s and stops at word position m. U2 starts from
m+1 and ends with e. Under the guidance of def-
inition of RST, Yue (2008) defined 12 groups1 of
</bodyText>
<footnote confidence="0.956129333333333">
1They are Parallel, Alternative, Condition, Reason, Elabo-
ration, Means, Preparation, Enablement, Antithesis, Back-
ground, Evidences, Others.
</footnote>
<page confidence="0.934296">
370
</page>
<note confidence="0.535737">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 370–374,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<tableCaption confidence="0.4065066">
Example 1:
Antithesis
Cue-words pair matching set of cue words for span [0,9] and [10,21]:{PP*/Eh+,PP*/NULL,NULL/Eh+}
Cue-words pair matching set of cue words for span [10,13] and [14,21]:{Eh+/NULL}
RST-based Rules: Antithesis:: PP*[X]/[Y] _&gt; Although[X]/[Y] ; Reason::Eh+[X]/[Y] _&gt; [Y]/because of[X]
</tableCaption>
<figureCaption confidence="0.992954">
Figure 1: An example of Chinese RST tree and its word alignment of the corresponding English string.
</figureCaption>
<equation confidence="0.628879538461538">
Jíshǐ labù duì meǐyuán de míngyì huìlu xiàjiàng le ,
PP&amp; #;fff X Ac7G n �&amp;quot; 7C* TIS 7 �
0 1 2 3 4 5 6 7 8 9 yóuya gāo tōngzhàng ,
Eh+ ;E i&amp;Ar �
10 11 12 13
U1:[0,9]
U1:[10,13]
Reason
A &amp;MF n* P_ A ±3t n �
14 15 16 17 18 19 20 21
U2:[10,21]
U2:[14,21]
P(eI F91 ) = P(eI F47; F9) (2)
</equation>
<bodyText confidence="0.978101444444445">
Although the rupee&apos;s nominal rate against the dollar was held down , India&apos;s real exchange rate rose because of high inflation .
rhetorical relations for Chinese particularly, upon
which our Chinese RST parser is developed.
Figure 1 illustrates an example of Chinese
RST tree and its alignment to the English string.
There are two levels in this tree. The Antithesis
relation controls U1 from 0 to 9 and U2 from 10
to 21. Thus it is written as Antithesis-[0,9,21].
Different shadow blocks denote the alignments
of different edus. Links between source and tar-
get words are alignments of cue words. Cue
words are viewed as the strongest clues for rhe-
torical relation recognition and always found at
the beginning of text (Reitter, 2003), such as “RP
使(although), 由于(because of)”. With the cue
words included, the relations are much easier to
be analyzed. So we focus on the explicit relations
with cue words in this paper as our first try.
</bodyText>
<subsectionHeader confidence="0.994638">
2.2 Bayesian Method for Chinese RST Parser
</subsectionHeader>
<bodyText confidence="0.951465">
For Chinese RST parser, there are two tasks. One
is the segmentation of edu and the other is the
relation tagging between two semantic spans.
Feature Meaning
F1(F6) left(right) child is a syntactic sub-tree?
F2(F5) left(right) child ends with a punctuation?
F3(F4) cue words of left (right) child.
F7 left and right children are sibling nodes?
F8(F9) syntactic head symbol of left(right) child.
</bodyText>
<tableCaption confidence="0.980746">
Table 1: 9 features used in our Bayesian model
</tableCaption>
<bodyText confidence="0.999510181818182">
Inspired by the features used in English RST
parser (Soricut and Marcu, 2003; Reitter, 2003;
Duverle and Prendinger, 2009; Hernault et al.,
2010a), we design a Bayesian model to build a
joint parser for segmentation and tagging simul-
taneously. In this model, 9 features in Table 1 are
used. In the table, punctuations include comma,
semicolons, period and question mark. We view
explicit connectives as cue words in this paper.
Figure 2 illustrates the conditional independ-
ences of 9 features which are denoted with F1~F9.
</bodyText>
<equation confidence="0.998826">
P(mI F91 ) = P(mI Fi ; F8) (1)
</equation>
<figureCaption confidence="0.999685">
Figure 2: The graph for conditional independences
of 9 features.
</figureCaption>
<bodyText confidence="0.987966916666667">
qí shíjì huìlu yě shì shàngshēng de .
The segmentation and parsing conditional
probabilities are computed as follows:
where represents the feature , means
features from to . is short for relation. (1)
and (2) describe the conditional probabilities of
m and e. When using Formula (3) to predict the
relation, we search all the cue-words pair, as
shown in Figure 1, to get the best match. When
training, we use maximum likelihood estimation
to get all the associated probabilities. For decod-
ing, the pseudo codes are given as below.
</bodyText>
<listItem confidence="0.996284111111111">
1: Nodes={[]}
2: Parser(0,End)
3: Parser(s,e): // recursive parser function
4: if s &gt; e or e is -1: return -1;
5: m = GetMaxM(s,e) //compute m through Formu-
la(1);if no cue words found,
then m=-1;
6: e’ = GetMaxE(s,m,e) //compute e’ through F (2);
7: if m or e’ equals to -1: return -1;
8: Rel=GetRelation(s,m,e’) //compute relation by F
(3)
9: push [Rel,s,m,e’] into Nodes
10: Parser(s,m)
11: Parser(m+1,e’)
12: Parser(e’+1,e)
13: Rel=GetRelation(s,e’,e)
14: push [Rel,s,e’,e] into Nodes
15: return e
</listItem>
<equation confidence="0.987887333333333">
Ft F2 F8 F3 F4 F5 F6 F7 F9
P(RelI F91 ) = P(RelI F43 ) (3)
m Rel e
</equation>
<page confidence="0.977481">
371
</page>
<bodyText confidence="0.999935083333333">
For example in Figure 1, for the first iteration,
s=0 and m will be chosen from {1-20}. We get
m=9 through Formula (1). Then, similar with m,
we get e=21 through Formula (2). Finally, the
relation is figured out by Formula (3). Thus, a
node is generated. A complete RST tree con-
structs until the end of the iterative process for
this sentence. This method can run fast due to the
simple greedy algorithm. It is plausible in our
cases, because we only have a small scale of
manually-annotated Chinese RST corpus, which
prefers simple rather than complicated models.
</bodyText>
<sectionHeader confidence="0.999343" genericHeader="method">
3 Translation Model
</sectionHeader>
<subsectionHeader confidence="0.997013">
3.1 Rule Extraction
</subsectionHeader>
<bodyText confidence="0.999905571428571">
As shown in Figure 1, the RST tree-to-string
alignment provides us with two types of transla-
tion rules. One is common phrase-based rules,
which are just like those in phrase-based model
(Koehn et al., 2003). The other is RST tree-to-
string rule, and it’s defined as,
where the terminal characters α and Y represent
the cue words which are optimum match for
maximizing Formula (3). While the non-
terminals X and Y represent the rest of the se-
quence. Function tr(· ) means the translation
of ·. The operator — is an operator to indicate
that the order of tr(U1) and tr(U2) is monotone or
reverse. During rules’ extraction, if the mean
position of all the words in tr(U1) precedes that
in tr(U2), — is monotone. Otherwise, — is reverse.
For example in Figure 1, the Reason relation
controls U1:[10,13] and U2:[14,21]. Because the
mean position of tr(U2) is before that of tr(U1),
the reverse order is selected. We list the RST-
based rules for Example 1 in Figure 1.
</bodyText>
<subsectionHeader confidence="0.998224">
3.2 Probabilities Estimation
</subsectionHeader>
<bodyText confidence="0.9999248">
For the phrase-based translation rules, we use
four common probabilities and the probabilities’
estimation is the same with those in (Koehn et al.,
2003). While the probabilities of RST-based
translation rules are given as follows,
</bodyText>
<equation confidence="0.9926115">
(1) P(r_Irf,Rel) = Count (re,rf,relation,). `,,here re
Count(rf,relation) &apos;
</equation>
<bodyText confidence="0.912417">
is the target side of the rule, ignorance of the or-
der, i.e. U1(tr(a),tr(X)) — U2(tr(y),tr(Y)) with
two directions, rf is the source side, i.e.
</bodyText>
<equation confidence="0.9911455">
U1 (®, X)=U2 (°, Y), and means the relation
type.
(2) :
T E {monotone, . It is the conditional
</equation>
<bodyText confidence="0.552642">
probability of re-ordering.
</bodyText>
<sectionHeader confidence="0.995632" genericHeader="method">
4 Decoding
</sectionHeader>
<bodyText confidence="0.997818">
The decoding procedure of a discourse can be
derived from the original decoding formula
el = argmaxer P(ei I fi) . Given the rhetorical
structure of a source sentence and the corre-
sponding rule-table, the translating process is to
find an optimal path to get the highest score un-
der structure constrains, which is,
where is a source RST tree combined by a set
of node . es is the target string combined by
series of (translations of ). consists of
U1 and U2. and are translations of U1 and
U2 respectively. This global optimization prob-
lem is approximately simplified to local optimi-
zation to reduce the complexity,
In our paper, we have the following two ways
to factorize the above formula,
</bodyText>
<equation confidence="0.87448575">
Decoder 1:
P(eu1, eu2, T j fn)
= P(ecp, eX, eY , Tjfcp, fX, fY )
= P(ecpjfcp)P(Tjecp, fcp)P(eXjfX)P(eY jfY )
</equation>
<bodyText confidence="0.933205857142857">
= P(rejrf, Rel)P(Tjre, rf, Rel)P(eXjfX)P(eY jfY )
where eX, eY are the translation of non-terminal
parts. fep and ecp are cue-words pair of source
and target sides. The first and second factors are
just the probabilities introduced in Section 3.2.
After approximately simplified to local optimiza-
tion, the final formulae are re-written as,
</bodyText>
<equation confidence="0.998794666666667">
argmaxr{P(reIrf7Rel)P(¿Ire7rf7Rel)} (4)
argmaxeX {P(eX I fX )} (5)
argmaxeY {P(eY I fY )} (6)
</equation>
<bodyText confidence="0.9997821">
Taking the source sentence with its RST tree
in Figure 1 for instance, we adopt a bottom-up
manner to do translation recursively. Suppose the
best rules selected by (4) are just those written in
the figure, Then span [11,13] and [14,21] are
firstly translated by (5) and (6). Their translations
are then re-packaged by the rule of Reason-
[10,13,21]. Iteratively, the translations of span
[1,9] and [10,21] are re-packaged by the rule of
Antithesis-[0,9,21] to form the final translation.
</bodyText>
<page confidence="0.996607">
372
</page>
<bodyText confidence="0.94878225">
Decoder 2 : Suppose that the translating process
of two spans U1 and U2 are independent of each
other, we rewriteP(e,,,l,e,,,2,Tlfn)
as follows,
</bodyText>
<equation confidence="0.9798222">
P(eu1 ; eu2 ; T j fn)
= P(eu1; eu2; Tjfu1; fu2)
= P(eu1jfu1)P(eu2jfu2)P(Tjrf;Rel)
X= P(eu1jfu1)P(eu2jfu2) P(Tjre; rf; Rel)P(rejrf; Rel)
Te
</equation>
<bodyText confidence="0.805874">
after approximately simplified to local optimization,
the final formulae are re-written as below,
</bodyText>
<figure confidence="0.54172925">
argmaxe, fPr(eu1j fu1)g
argmaxe,JPr(eu2jfu2)g
XargmaxTf Pr(¿jre;rf;Rel)Pr(rejrf;Rel)g
e
</figure>
<bodyText confidence="0.999923277777778">
We also adopt the bottom-up manner similar
to Decoder 1. In Figure 1, U1 and U2 of Reason
node are firstly translated. Their translations are
then re-ordered. Then the translations of two
spans of Antithesis node are re-ordered and con-
structed into the final translation. In Decoder 2,
the minimal translation-unit is edu. While in De-
coder 1, an edu is further split into cue-word part
and the rest part to obtain the respective transla-
tion.
In our decoders, language model(LM) is used
for translating edus in Formula(5),(6),(7),(8), but
not for reordering the upper spans because with
the bottom-to-up combination, the spans become
longer and harder to be judged by a traditional
language model. So we only use RST rules to
guide the reordering. But LM will be properly
considered in our future work.
</bodyText>
<sectionHeader confidence="0.995672" genericHeader="evaluation">
5 Experiment
</sectionHeader>
<subsectionHeader confidence="0.97584">
5.1 Setup
</subsectionHeader>
<bodyText confidence="0.999896769230769">
In order to do Chinese RST parser, we annotated
over 1,000 complicated sentences on CTB (Xue
et al., 2005), among which 1,107 sentences are
used for training, and 500 sentences are used for
testing. Berkeley parser2 is used for getting the
syntactic trees.
The translation experiment is conducted on
Chinese-to-English direction. The bilingual train-
ing data is from the LDC corpus3. The training
corpus contains 2.1M sentence pairs. We obtain
the word alignment with the grow-diag-final-and
strategy by GIZA++4. A 5-gram language model
is trained on the Xinhua portion of the English
</bodyText>
<footnote confidence="0.9990828">
2 http://code.google.com/p/berkeleyparser/
3 LDC category number : LDC2000T50, LDC2002E18,
LDC2003E07, LDC2004T07, LDC2005T06, LDC2002L27,
LDC2005T10 and LDC2005T34
4 http://code.google.com/p/giza-pp/
</footnote>
<bodyText confidence="0.999247625">
Gigaword corpus. For tuning and testing, we use
NIST03 evaluation data as the development set,
and extract the relatively long and complicated
sentences from NIST04, NIST05 and CWMT085
evaluation data as the test set. The number and
average word-length of sentences are 511/36,
320/34, 590/38 respectively. We use case-
insensitive BLEU-4 with the shortest length pen-
alty for evaluation.
To create the baseline system, we use the
toolkit Moses6 to build a phrase-based translation
system. Meanwhile, considering that Xiong et al.
(2009) have presented good results by dividing
long and complicated sentences into sub-
sentences only by punctuations during decoding,
we re-implement their method for comparison.
</bodyText>
<subsectionHeader confidence="0.997356">
5.2 Results of Chinese RST Parser
</subsectionHeader>
<bodyText confidence="0.9998925">
Table 2 shows the results of RST parsing. On
average, our RS trees are 2 layers deep. The
parsing errors mostly result from the segmenta-
tion errors, which are mainly caused by syntactic
parsing errors. On the other hand, the polyse-
mous cue words, such as “而(but, and, thus)”
may lead ambiguity for relation recognition, be-
cause they can be clues for different relations.
</bodyText>
<table confidence="0.999273666666667">
Task Precision Recall F1
Segmentation 0.74 0.83 0.78
Labeling 0.71 0.78 0.75
</table>
<tableCaption confidence="0.998992">
Table 2: Segmentation and labeling result.
</tableCaption>
<subsectionHeader confidence="0.98662">
5.3 Results of Translation
</subsectionHeader>
<bodyText confidence="0.999972769230769">
Table 3 presents the translation comparison re-
sults. In this table, XD represents the method in
(Xiong et al., 2009). D1 stands for Decoder-1,
and D2 for Decoder-2. Values with boldface are
the highest scores in comparison. D2 performs
best on the test data with 2.3/0.77/1.43/1.16
points. Compared with XD, our results also out-
perform by 0.52 points on the whole test data.
Observing and comparing the translation re-
sults, we find that our translation results are more
readable by maintaining the semantic integrality
of the edus and by giving more appreciate reor-
ganization of the translated edus.
</bodyText>
<table confidence="0.9990078">
Testing Set Baseline XD D1 D2
NIST04 29.39 31.52 31.34 31.69
NIST05 29.86 29.80 30.28 30.63
CWMT08 24.31 25.24 25.74 25.74
ALL 27.85 28.49 28.66 29.01
</table>
<tableCaption confidence="0.999994">
Table 3: Comparison with related models.
</tableCaption>
<note confidence="0.650623">
5 China Workshop on Machine Translation 2008
</note>
<footnote confidence="0.93983">
6 www.statmt.org/moses/index.php?n=Main.HomePage
</footnote>
<page confidence="0.998592">
373
</page>
<sectionHeader confidence="0.996565" genericHeader="conclusions">
6 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999963235294118">
In this paper, we present an RST-based transla-
tion framework for modeling semantic structures
in translation model, so as to maintain the se-
mantically functional integrity and hierarchical
relations of edus during translating. With respect
to the existing models, we think our translation
framework works more similarly to what human
does, and we believe that this research is a cru-
cial step towards discourse-oriented translation.
In the next step, we will study on the implicit
discourse relations for Chinese and further modi-
fy the RST-based framework. Besides, we will
try to combine other current translation models
such as syntactic model and hierarchical model
into our framework. Furthermore, the more accu-
rate evaluation metric for discourse-oriented
translation will be further studied.
</bodyText>
<sectionHeader confidence="0.998377" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.994548857142857">
The research work has been funded by the Hi-Tech
Research and Development Program (“863” Pro-
gram) of China under Grant No. 2011AA01A207,
2012AA011101, and 2012AA011102 and also sup-
ported by the Key Project of Knowledge Innova-
tion Program of Chinese Academy of Sciences un-
der Grant No.KGZD-EW-501.
</bodyText>
<sectionHeader confidence="0.997663" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9952058375">
David A Duverle and Helmut Prendinger. 2009. A
novel discourse parser based on support vector ma-
chine classification. In Proceedings of the Joint
Conference of the 47th Annual Meeting of the ACL
and the 4th International Joint Conference on Iat-
ural Language Processing of the AFILP: Volume
2-Volume 2, pages 665–673. Association for Com-
putational Linguistics.
Zhengxian Gong, Min Zhang, and Guodong Zhou.
2011. Cache-based document-level statistical ma-
chine translation. In Proceedings of the Conference
on Empirical Methods in Iatural Language Pro-
cessing, pages 909–919. Association for Computa-
tional Linguistics.
Hugo Hernault, Danushka Bollegala, and Mitsuru
Ishizuka. 2010a. A sequential model for discourse
segmentation. Computational Linguistics and Intel-
ligent Text Processing, pages 315–326.
Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka,
et al. 2010b. Hilda: A discourse parser using sup-
port vector machine classification. Dialogue &amp;
Discourse, 1(3).
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the 2003 Conference of the Iorth
American Chapter of the Association for Computa-
tional Linguistics on Human Language Technology
Volume 1, pages 48–54. Association for Computa-
tional Linguistics.
William C Mann and Sandra A Thompson. 1986.
Rhetorical structure theory: Description and con-
struction of text structures. Technical report, DTIC
Document.
William C Mann and Sandra A Thompson. 1987.
Rhetorical structure theory: A framework for the
analysis of texts. Technical report, DTIC Docu-
ment.
William C Mann and Sandra A Thompson. 1988.
Rhetorical structure theory: Toward a functional
theory of text organization. Text, 8(3):243–281.
Daniel Marcu, Lynn Carlson, and Maki Watanabe.
2000. The automatic translation of discourse struc-
tures. In Proceedings of the 1st Iorth American
chapter of the Association for Computational Lin-
guistics conference, pages 9–17. Morgan Kauf-
mann Publishers Inc.
David Reitter. 2003. Simple signals for complex rhet-
orics: On rhetorical analysis with rich-feature sup-
port vector models. Language, 18:52.
Radu Soricut and Daniel Marcu. 2003. Sentence level
discourse parsing using syntactic and lexical in-
formation. In Proceedings of the 2003 Conference
of the Iorth American Chapter of the Association
for Computational Linguistics on Human Lan-
guage Technology-Volume 1, pages 149–156. As-
sociation for Computational Linguistics.
Billy TM Wong and Chunyu Kit. 2012. Extending
machine translation evaluation metrics with lexical
cohesion to document level. In Proceedings of the
2012 Joint Conference on Empirical Methods in
Iatural Language Processing and Computational
Iatural Language Learning, page 1060–1068. As-
sociation for Computational Linguistics.
Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.
2011. Document-level consistency verification in
machine translation. In Machine Translation Sum-
mit, volume 13, pages 131–138.
Hao Xiong, Wenwen Xu, Haitao Mi, Yang Liu, and
Qun Liu. 2009. Sub-sentence division for tree-
based machine translation. In Proceedings of the
ACL-IJCILP 2009 Conference Short Papers, pag-
es 137–140. Association for Computational Lin-
guistics.
Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta
Palmer. 2005. The Penn Chinese treebank: Phrase
structure annotation of a large corpus. Iatural
Language Engineering, 11(2):207.
Ming Yue. 2008. Rhetorical structure annotation of
Chinese news commentaries. Journal of Chinese
Information Processing, 4:002.
</reference>
<page confidence="0.999025">
374
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.575284">
<title confidence="0.999451">A Novel Translation Framework Based on Rhetorical Structure Theory</title>
<author confidence="0.991612">Mei Tu Yu Zhou Chengqing Zong</author>
<affiliation confidence="0.91697">National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences</affiliation>
<email confidence="0.814217">mtu@nlpr.ia.ac.cn</email>
<email confidence="0.814217">yzhou@nlpr.ia.ac.cn</email>
<email confidence="0.814217">cqzong@nlpr.ia.ac.cn</email>
<abstract confidence="0.985491444444444">Rhetorical structure theory (RST) is widely used for discourse understanding, which represents a discourse as a hierarchically semantic structure. In this paper, we propose a novel translation framework with the help of RST. In our framework, the translation process mainly three steps: 1) RST-tree aca source sentence is parsed into an tree; 2) translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) the source RST-tree is translated with translation rules. Experiments on Chinese-to-English show that our RST-based approach achieves improvements of 2.3/0.77/1.43 BLEU points on NIST04/NIST05/CWMT2008 respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>David A Duverle</author>
<author>Helmut Prendinger</author>
</authors>
<title>A novel discourse parser based on support vector machine classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Iatural Language Processing of the AFILP: Volume</booktitle>
<volume>2</volume>
<pages>665--673</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6240" citStr="Duverle and Prendinger, 2009" startWordPosition="992" endWordPosition="995">per as our first try. 2.2 Bayesian Method for Chinese RST Parser For Chinese RST parser, there are two tasks. One is the segmentation of edu and the other is the relation tagging between two semantic spans. Feature Meaning F1(F6) left(right) child is a syntactic sub-tree? F2(F5) left(right) child ends with a punctuation? F3(F4) cue words of left (right) child. F7 left and right children are sibling nodes? F8(F9) syntactic head symbol of left(right) child. Table 1: 9 features used in our Bayesian model Inspired by the features used in English RST parser (Soricut and Marcu, 2003; Reitter, 2003; Duverle and Prendinger, 2009; Hernault et al., 2010a), we design a Bayesian model to build a joint parser for segmentation and tagging simultaneously. In this model, 9 features in Table 1 are used. In the table, punctuations include comma, semicolons, period and question mark. We view explicit connectives as cue words in this paper. Figure 2 illustrates the conditional independences of 9 features which are denoted with F1~F9. P(mI F91 ) = P(mI Fi ; F8) (1) Figure 2: The graph for conditional independences of 9 features. qí shíjì huìlu yě shì shàngshēng de . The segmentation and parsing conditional probabilities are compu</context>
</contexts>
<marker>Duverle, Prendinger, 2009</marker>
<rawString>David A Duverle and Helmut Prendinger. 2009. A novel discourse parser based on support vector machine classification. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Iatural Language Processing of the AFILP: Volume 2-Volume 2, pages 665–673. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhengxian Gong</author>
<author>Min Zhang</author>
<author>Guodong Zhou</author>
</authors>
<title>Cache-based document-level statistical machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Iatural Language Processing,</booktitle>
<pages>909--919</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1619" citStr="Gong et al., 2011" startWordPosition="236" endWordPosition="239">tical machine translation (SMT), a crucial issue is how to build a translation model to extract as much accurate and generative translation knowledge as possible. The existing SMT models have made much progress. However, they still suffer from the bad performance of unnatural or even unreadable translation, especially when the sentences become complicated. We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of “translation with overall understanding”. Rhetorical structure theory (RST) (Mann an</context>
</contexts>
<marker>Gong, Zhang, Zhou, 2011</marker>
<rawString>Zhengxian Gong, Min Zhang, and Guodong Zhou. 2011. Cache-based document-level statistical machine translation. In Proceedings of the Conference on Empirical Methods in Iatural Language Processing, pages 909–919. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hugo Hernault</author>
<author>Danushka Bollegala</author>
<author>Mitsuru Ishizuka</author>
</authors>
<title>A sequential model for discourse segmentation.</title>
<date>2010</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<pages>315--326</pages>
<contexts>
<context position="6263" citStr="Hernault et al., 2010" startWordPosition="996" endWordPosition="999">sian Method for Chinese RST Parser For Chinese RST parser, there are two tasks. One is the segmentation of edu and the other is the relation tagging between two semantic spans. Feature Meaning F1(F6) left(right) child is a syntactic sub-tree? F2(F5) left(right) child ends with a punctuation? F3(F4) cue words of left (right) child. F7 left and right children are sibling nodes? F8(F9) syntactic head symbol of left(right) child. Table 1: 9 features used in our Bayesian model Inspired by the features used in English RST parser (Soricut and Marcu, 2003; Reitter, 2003; Duverle and Prendinger, 2009; Hernault et al., 2010a), we design a Bayesian model to build a joint parser for segmentation and tagging simultaneously. In this model, 9 features in Table 1 are used. In the table, punctuations include comma, semicolons, period and question mark. We view explicit connectives as cue words in this paper. Figure 2 illustrates the conditional independences of 9 features which are denoted with F1~F9. P(mI F91 ) = P(mI Fi ; F8) (1) Figure 2: The graph for conditional independences of 9 features. qí shíjì huìlu yě shì shàngshēng de . The segmentation and parsing conditional probabilities are computed as follows: where r</context>
</contexts>
<marker>Hernault, Bollegala, Ishizuka, 2010</marker>
<rawString>Hugo Hernault, Danushka Bollegala, and Mitsuru Ishizuka. 2010a. A sequential model for discourse segmentation. Computational Linguistics and Intelligent Text Processing, pages 315–326.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Hugo Hernault</author>
</authors>
<title>Helmut Prendinger, Mitsuru Ishizuka, et al. 2010b. Hilda: A discourse parser using support vector machine classification.</title>
<journal>Dialogue &amp; Discourse,</journal>
<volume>1</volume>
<issue>3</issue>
<marker>Hernault, </marker>
<rawString>Hugo Hernault, Helmut Prendinger, Mitsuru Ishizuka, et al. 2010b. Hilda: A discourse parser using support vector machine classification. Dialogue &amp; Discourse, 1(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the Iorth American Chapter of the Association for Computational Linguistics on Human Language Technology</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="8652" citStr="Koehn et al., 2003" startWordPosition="1411" endWordPosition="1414">tion is figured out by Formula (3). Thus, a node is generated. A complete RST tree constructs until the end of the iterative process for this sentence. This method can run fast due to the simple greedy algorithm. It is plausible in our cases, because we only have a small scale of manually-annotated Chinese RST corpus, which prefers simple rather than complicated models. 3 Translation Model 3.1 Rule Extraction As shown in Figure 1, the RST tree-to-string alignment provides us with two types of translation rules. One is common phrase-based rules, which are just like those in phrase-based model (Koehn et al., 2003). The other is RST tree-tostring rule, and it’s defined as, where the terminal characters α and Y represent the cue words which are optimum match for maximizing Formula (3). While the nonterminals X and Y represent the rest of the sequence. Function tr(· ) means the translation of ·. The operator — is an operator to indicate that the order of tr(U1) and tr(U2) is monotone or reverse. During rules’ extraction, if the mean position of all the words in tr(U1) precedes that in tr(U2), — is monotone. Otherwise, — is reverse. For example in Figure 1, the Reason relation controls U1:[10,13] and U2:[1</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the 2003 Conference of the Iorth American Chapter of the Association for Computational Linguistics on Human Language Technology Volume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Description and construction of text structures.</title>
<date>1986</date>
<tech>Technical report, DTIC Document.</tech>
<marker>Mann, Thompson, 1986</marker>
<rawString>William C Mann and Sandra A Thompson. 1986. Rhetorical structure theory: Description and construction of text structures. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: A framework for the analysis of texts.</title>
<date>1987</date>
<tech>Technical report, DTIC Document.</tech>
<marker>Mann, Thompson, 1987</marker>
<rawString>William C Mann and Sandra A Thompson. 1987. Rhetorical structure theory: A framework for the analysis of texts. Technical report, DTIC Document.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text, 8(3):243–281.</tech>
<contexts>
<context position="2236" citStr="Mann and Thompson, 1988" startWordPosition="330" endWordPosition="333">., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of “translation with overall understanding”. Rhetorical structure theory (RST) (Mann and Thompson, 1988) provides us with a good perspective and inspiration to build such a framework. Generally, an RST tree can explicitly show the minimal spans with semantic functional integrity, which are called elementary discourse units (edus) (Marcu et al., 2000), and it also depicts the hierarchical relations among edus. Furthermore, since different languages’ edus are usually equivalent on semantic level, it is intuitive to create a new framework based on RST by directly mapping the source edus to target ones. Taking the Chinese-to-English translation as an example, our translation framework works as the f</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C Mann and Sandra A Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243–281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Lynn Carlson</author>
<author>Maki Watanabe</author>
</authors>
<title>The automatic translation of discourse structures.</title>
<date>2000</date>
<booktitle>In Proceedings of the 1st Iorth American chapter of the Association for Computational Linguistics conference,</booktitle>
<pages>9--17</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<contexts>
<context position="1880" citStr="Marcu et al. (2000)" startWordPosition="276" endWordPosition="279">ance of unnatural or even unreadable translation, especially when the sentences become complicated. We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of “translation with overall understanding”. Rhetorical structure theory (RST) (Mann and Thompson, 1988) provides us with a good perspective and inspiration to build such a framework. Generally, an RST tree can explicitly show the minimal spans with semantic functional integrity, which are called elementary discourse units (edus) (Marcu et al., 2</context>
</contexts>
<marker>Marcu, Carlson, Watanabe, 2000</marker>
<rawString>Daniel Marcu, Lynn Carlson, and Maki Watanabe. 2000. The automatic translation of discourse structures. In Proceedings of the 1st Iorth American chapter of the Association for Computational Linguistics conference, pages 9–17. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Reitter</author>
</authors>
<title>Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models.</title>
<date>2003</date>
<journal>Language,</journal>
<pages>18--52</pages>
<contexts>
<context position="5430" citStr="Reitter, 2003" startWordPosition="860" endWordPosition="861">lation . rhetorical relations for Chinese particularly, upon which our Chinese RST parser is developed. Figure 1 illustrates an example of Chinese RST tree and its alignment to the English string. There are two levels in this tree. The Antithesis relation controls U1 from 0 to 9 and U2 from 10 to 21. Thus it is written as Antithesis-[0,9,21]. Different shadow blocks denote the alignments of different edus. Links between source and target words are alignments of cue words. Cue words are viewed as the strongest clues for rhetorical relation recognition and always found at the beginning of text (Reitter, 2003), such as “RP 使(although), 由于(because of)”. With the cue words included, the relations are much easier to be analyzed. So we focus on the explicit relations with cue words in this paper as our first try. 2.2 Bayesian Method for Chinese RST Parser For Chinese RST parser, there are two tasks. One is the segmentation of edu and the other is the relation tagging between two semantic spans. Feature Meaning F1(F6) left(right) child is a syntactic sub-tree? F2(F5) left(right) child ends with a punctuation? F3(F4) cue words of left (right) child. F7 left and right children are sibling nodes? F8(F9) sy</context>
</contexts>
<marker>Reitter, 2003</marker>
<rawString>David Reitter. 2003. Simple signals for complex rhetorics: On rhetorical analysis with rich-feature support vector models. Language, 18:52.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Radu Soricut</author>
<author>Daniel Marcu</author>
</authors>
<title>Sentence level discourse parsing using syntactic and lexical information.</title>
<date>2003</date>
<booktitle>In Proceedings of the 2003 Conference of the Iorth American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1,</booktitle>
<pages>149--156</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="3384" citStr="Soricut and Marcu, 2003" startWordPosition="507" endWordPosition="510">-English translation as an example, our translation framework works as the following steps: 1) Source RST-tree acquisition: a source sentence is parsed into an RST-tree; 2) Rule extraction: translation rules are extracted from the source tree and the target string via bilingual word alignment; 3) RST-based translation: the source RSTtree is translated into target sentence with extracted translation rules. Experiments on Chinese-to-English sentencelevel discourses demonstrate that this method achieves significant improvements. 2 Chinese RST Parser 2.1 Annotation of Chinese RST Tree Similar to (Soricut and Marcu, 2003), a node of RST tree is represented as a tuple R-[s, m, e], which means the relation R controls two semantic spans U1 and U2 , U1 starts from word position s and stops at word position m. U2 starts from m+1 and ends with e. Under the guidance of definition of RST, Yue (2008) defined 12 groups1 of 1They are Parallel, Alternative, Condition, Reason, Elaboration, Means, Preparation, Enablement, Antithesis, Background, Evidences, Others. 370 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 370–374, Sofia, Bulgaria, August 4-9 2013. c�2013 Association f</context>
<context position="6195" citStr="Soricut and Marcu, 2003" startWordPosition="986" endWordPosition="989">icit relations with cue words in this paper as our first try. 2.2 Bayesian Method for Chinese RST Parser For Chinese RST parser, there are two tasks. One is the segmentation of edu and the other is the relation tagging between two semantic spans. Feature Meaning F1(F6) left(right) child is a syntactic sub-tree? F2(F5) left(right) child ends with a punctuation? F3(F4) cue words of left (right) child. F7 left and right children are sibling nodes? F8(F9) syntactic head symbol of left(right) child. Table 1: 9 features used in our Bayesian model Inspired by the features used in English RST parser (Soricut and Marcu, 2003; Reitter, 2003; Duverle and Prendinger, 2009; Hernault et al., 2010a), we design a Bayesian model to build a joint parser for segmentation and tagging simultaneously. In this model, 9 features in Table 1 are used. In the table, punctuations include comma, semicolons, period and question mark. We view explicit connectives as cue words in this paper. Figure 2 illustrates the conditional independences of 9 features which are denoted with F1~F9. P(mI F91 ) = P(mI Fi ; F8) (1) Figure 2: The graph for conditional independences of 9 features. qí shíjì huìlu yě shì shàngshēng de . The segmentation an</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>Radu Soricut and Daniel Marcu. 2003. Sentence level discourse parsing using syntactic and lexical information. In Proceedings of the 2003 Conference of the Iorth American Chapter of the Association for Computational Linguistics on Human Language Technology-Volume 1, pages 149–156. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Billy TM Wong</author>
<author>Chunyu Kit</author>
</authors>
<title>Extending machine translation evaluation metrics with lexical cohesion to document level.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Iatural Language Processing and Computational Iatural Language Learning,</booktitle>
<pages>1060--1068</pages>
<institution>Association for Computational Linguistics.</institution>
<contexts>
<context position="1659" citStr="Wong and Kit, 2012" startWordPosition="244" endWordPosition="247">cial issue is how to build a translation model to extract as much accurate and generative translation knowledge as possible. The existing SMT models have made much progress. However, they still suffer from the bad performance of unnatural or even unreadable translation, especially when the sentences become complicated. We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of “translation with overall understanding”. Rhetorical structure theory (RST) (Mann and Thompson, 1988) provides us with a goo</context>
</contexts>
<marker>Wong, Kit, 2012</marker>
<rawString>Billy TM Wong and Chunyu Kit. 2012. Extending machine translation evaluation metrics with lexical cohesion to document level. In Proceedings of the 2012 Joint Conference on Empirical Methods in Iatural Language Processing and Computational Iatural Language Learning, page 1060–1068. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tong Xiao</author>
<author>Jingbo Zhu</author>
<author>Shujie Yao</author>
<author>Hao Zhang</author>
</authors>
<title>Document-level consistency verification in machine translation.</title>
<date>2011</date>
<booktitle>In Machine Translation Summit,</booktitle>
<volume>13</volume>
<pages>131--138</pages>
<contexts>
<context position="1638" citStr="Xiao et al., 2011" startWordPosition="240" endWordPosition="243">lation (SMT), a crucial issue is how to build a translation model to extract as much accurate and generative translation knowledge as possible. The existing SMT models have made much progress. However, they still suffer from the bad performance of unnatural or even unreadable translation, especially when the sentences become complicated. We think the deep reason is that those models only extract translation information on lexical or syntactic level, but fail to give an overall understanding of source sentences on semantic level of discourse. In order to solve such problem, (Gong et al., 2011; Xiao et al., 2011; Wong and Kit, 2012) build discourse-based translation models to ensure the lexical coherence or consistency. Although some lexicons can be translated better by their models, the overall structure still remains unnatural. Marcu et al. (2000) design a discourse structure transferring module, but leave much work to do, especially on how to integrate this module into SMT and how to automatically analyze the structures. Those reasons urge us to seek a new translation framework under the idea of “translation with overall understanding”. Rhetorical structure theory (RST) (Mann and Thompson, 1988) p</context>
</contexts>
<marker>Xiao, Zhu, Yao, Zhang, 2011</marker>
<rawString>Tong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang. 2011. Document-level consistency verification in machine translation. In Machine Translation Summit, volume 13, pages 131–138.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hao Xiong</author>
<author>Wenwen Xu</author>
<author>Haitao Mi</author>
<author>Yang Liu</author>
<author>Qun Liu</author>
</authors>
<title>Sub-sentence division for treebased machine translation.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACL-IJCILP 2009 Conference Short Papers,</booktitle>
<pages>137--140</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="14362" citStr="Xiong et al. (2009)" startWordPosition="2327" endWordPosition="2330">DC2005T06, LDC2002L27, LDC2005T10 and LDC2005T34 4 http://code.google.com/p/giza-pp/ Gigaword corpus. For tuning and testing, we use NIST03 evaluation data as the development set, and extract the relatively long and complicated sentences from NIST04, NIST05 and CWMT085 evaluation data as the test set. The number and average word-length of sentences are 511/36, 320/34, 590/38 respectively. We use caseinsensitive BLEU-4 with the shortest length penalty for evaluation. To create the baseline system, we use the toolkit Moses6 to build a phrase-based translation system. Meanwhile, considering that Xiong et al. (2009) have presented good results by dividing long and complicated sentences into subsentences only by punctuations during decoding, we re-implement their method for comparison. 5.2 Results of Chinese RST Parser Table 2 shows the results of RST parsing. On average, our RS trees are 2 layers deep. The parsing errors mostly result from the segmentation errors, which are mainly caused by syntactic parsing errors. On the other hand, the polysemous cue words, such as “而(but, and, thus)” may lead ambiguity for relation recognition, because they can be clues for different relations. Task Precision Recall </context>
</contexts>
<marker>Xiong, Xu, Mi, Liu, Liu, 2009</marker>
<rawString>Hao Xiong, Wenwen Xu, Haitao Mi, Yang Liu, and Qun Liu. 2009. Sub-sentence division for treebased machine translation. In Proceedings of the ACL-IJCILP 2009 Conference Short Papers, pages 137–140. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Naiwen Xue</author>
<author>Fei Xia</author>
<author>Fu-Dong Chiou</author>
<author>Marta Palmer</author>
</authors>
<title>The Penn Chinese treebank: Phrase structure annotation of a large corpus.</title>
<date>2005</date>
<journal>Iatural Language Engineering,</journal>
<volume>11</volume>
<issue>2</issue>
<contexts>
<context position="13149" citStr="Xue et al., 2005" startWordPosition="2154" endWordPosition="2157"> is edu. While in Decoder 1, an edu is further split into cue-word part and the rest part to obtain the respective translation. In our decoders, language model(LM) is used for translating edus in Formula(5),(6),(7),(8), but not for reordering the upper spans because with the bottom-to-up combination, the spans become longer and harder to be judged by a traditional language model. So we only use RST rules to guide the reordering. But LM will be properly considered in our future work. 5 Experiment 5.1 Setup In order to do Chinese RST parser, we annotated over 1,000 complicated sentences on CTB (Xue et al., 2005), among which 1,107 sentences are used for training, and 500 sentences are used for testing. Berkeley parser2 is used for getting the syntactic trees. The translation experiment is conducted on Chinese-to-English direction. The bilingual training data is from the LDC corpus3. The training corpus contains 2.1M sentence pairs. We obtain the word alignment with the grow-diag-final-and strategy by GIZA++4. A 5-gram language model is trained on the Xinhua portion of the English 2 http://code.google.com/p/berkeleyparser/ 3 LDC category number : LDC2000T50, LDC2002E18, LDC2003E07, LDC2004T07, LDC2005</context>
</contexts>
<marker>Xue, Xia, Chiou, Palmer, 2005</marker>
<rawString>Naiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta Palmer. 2005. The Penn Chinese treebank: Phrase structure annotation of a large corpus. Iatural Language Engineering, 11(2):207.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ming Yue</author>
</authors>
<title>Rhetorical structure annotation of Chinese news commentaries.</title>
<date>2008</date>
<journal>Journal of Chinese Information Processing,</journal>
<pages>4--002</pages>
<contexts>
<context position="3659" citStr="Yue (2008)" startWordPosition="566" endWordPosition="567">ment; 3) RST-based translation: the source RSTtree is translated into target sentence with extracted translation rules. Experiments on Chinese-to-English sentencelevel discourses demonstrate that this method achieves significant improvements. 2 Chinese RST Parser 2.1 Annotation of Chinese RST Tree Similar to (Soricut and Marcu, 2003), a node of RST tree is represented as a tuple R-[s, m, e], which means the relation R controls two semantic spans U1 and U2 , U1 starts from word position s and stops at word position m. U2 starts from m+1 and ends with e. Under the guidance of definition of RST, Yue (2008) defined 12 groups1 of 1They are Parallel, Alternative, Condition, Reason, Elaboration, Means, Preparation, Enablement, Antithesis, Background, Evidences, Others. 370 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 370–374, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Example 1: Antithesis Cue-words pair matching set of cue words for span [0,9] and [10,21]:{PP*/Eh+,PP*/NULL,NULL/Eh+} Cue-words pair matching set of cue words for span [10,13] and [14,21]:{Eh+/NULL} RST-based Rules: Antithesis:: PP*[X]/[Y] _&gt; Alt</context>
</contexts>
<marker>Yue, 2008</marker>
<rawString>Ming Yue. 2008. Rhetorical structure annotation of Chinese news commentaries. Journal of Chinese Information Processing, 4:002.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>