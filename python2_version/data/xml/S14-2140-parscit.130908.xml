<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.071861">
<title confidence="0.982037">
USF: Chunking for Aspect Term Identification &amp; Polarity Classification
</title>
<author confidence="0.990277">
Cindi Thompson
</author>
<affiliation confidence="0.993325">
University of San Francisco
</affiliation>
<address confidence="0.742988">
2130 Fulton St, HR 240 San Francisco, CA 94117
</address>
<email confidence="0.994137">
cathompson4@usfca.edu
</email>
<sectionHeader confidence="0.993781" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999942304347826">
This paper describes the systems submit-
ted by the University of San Francisco
(USF) to Semeval-2014 Task 4, Aspect
Based Sentiment Analysis (ABSA), which
provides labeled data in two domains, lap-
tops and restaurants. For the constrained
condition of both the aspect term extrac-
tion and aspect term polarity tasks, we take
a supervised machine learning approach
using a combination of lexical, syntactic,
and baseline sentiment features. Our ex-
traction approach is inspired by a chunk-
ing approach, based on its strong past re-
sults on related tasks. Our system per-
formed slightly below average compared
to other submissions, possibly because we
use a simpler classification model than
prior work. Our polarity labeling ap-
proach uses two baseline hand-built sen-
timent classifiers as features in addition
to lexical and syntactic features, and per-
formed in the top ten of other constrained
systems on both domains.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999122">
As stated in the call for participation for this Se-
meval task, sentiment analysis focusing on overall
polarity of a document, sentence, or similar con-
text has been well studied in recent years (Liu,
2010; Pang and Lee, 2008; Tsytsarau and Pal-
panas, 2012). However, there is less prior work
examining finer levels of granularity associated
with individual entities and their characteristics
or attributes, which the organizers for this task
call aspects. The aspect based sentiment analysis
</bodyText>
<footnote confidence="0.8515302">
This work is licenced under a Creative Commons At-
tribution 4.0 International License. Page numbers and pro-
ceedings footer are added by the organizers. License de-
tails: http://creativecommons.org/licenses/
by/4.0/
</footnote>
<bodyText confidence="0.999512939393939">
task (ABSA) has the goal of identifying aspects
of stated or implied target entities and the senti-
ment expressed towards each aspect. This prob-
lem has not been deeply studied in prior literature
due to the lack, until now, of a large gold standard
dataset. This Semeval task has provided two such
datasets, in the domains of laptops and restaurants.
A full description of the task and data is presented
with this volume (Pontiki et al., 2014).
In this paper, we discuss our approach to the
first two subtasks of the Semeval ABSA Task,
those of aspect term extraction and aspect term
polarity. In aspect term extraction the domain
(restaurants or laptops) is known and the goal is
to identify terms in a sentence that are features
commonly associated with that domain, such as
service and staff in the case of restaurants or size
and speed in the case of laptops. In the polarity
subtask, the aspect terms for a given sentence are
already identified and the sentiment polarity (pos-
itive, negative, conflict, or neutral) must be as-
signed.
We approach both subtasks using supervised
machine learning with background knowledge of
sentiment lexicons and syntax included in our fea-
ture set. Our goal was to investigate whether tech-
niques that have been successful in similar tasks
would perform well on this newly created data
set. We did not use additional corpus-based re-
sources, so qualified for the constrained (versus
unconstrained) version of the task. The remain-
der of the paper details related work, our approach,
and experiments and the results we obtained.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999770166666667">
We divide related work into two areas: research re-
lated to aspect and aspect term identification, and
research related to sentiment classification for as-
pect terms. We note that aspects have also been
called topics and features in prior work. Un-
til more recently, the community lacked a corpus
</bodyText>
<page confidence="0.95162">
790
</page>
<note confidence="0.7281995">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 790–795,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.999929054794521">
of gold-standard labeled data that focuses on as-
pect terms, rather than more general expressions
of subjectivity or other private states. Thus, early
work focused on learning or identifying aspects
in an unsupervised (Hu and Liu, 2004) or semi-
supervised setting (Moghaddam and Ester, 2010;
Zhai et al., 2011). The earliest work on aspect
detection focused on identifying frequently oc-
curring noun phrases using information extraction
(IE) techniques (Hu and Liu, 2004). Unsupervised
techniques include clustering (Fahrni and Klenner,
2008; Popescu and Etzioni, 2005) and topic mod-
els (Titov and McDonald, 2008).
The benchmark corpus for sentiment analysis
from Wiebe et al. (2005) inspired much work on
learning subjective phrases in a supervised set-
ting. The nature of the data and annotation differ
from the data for this Semeval task, as it focuses
on news articles and identifying an entire opinion
phrase, including the source of the opinion, and
only recently added aspect annotations. However,
the techniques used by others to learn to extract
this data and the associated polarity inspired our
own approach. These include extraction-like ap-
proaches, usually using sequence modeling (Breck
et al., 2007; Jin et al., 2009; Johansson and
Moschitti, 2013; Li et al., 2010; Mitchell et al.,
2013; Yang and Cardie, 2013) and semantic de-
pendency or semantic parsing approaches (Kim
and Hovy, 2006; Kobayashi et al., 2007; Wu et
al., 2009) sometimes using background knowl-
edge from sentiment lexicons (Zhang et al., 2009).
The main differences between our approach and
that of Breck et al. (2007) and Mitchell et al.
(2013) are the classifier used and some of the fea-
tures; they both use CRFs versus our Maximum
entropy classifier, and they used a wider range of
syntactic and dictionary-based features.
A second related corpus which includes more
aspect information is that developed by Kim and
Hovy (2006). This corpus also focuses on news
articles rather than reviews, but does expand the
types of aspects identified. The main focus of that
work is on the identification, using FrameNet role
labels, of the holder and target of an opinion, while
the opinion itself is provided to the system.
The restaurant reviews used in this Semeval task
are a 3000-plus sentence subset of those harvested
by Ganu et al. (2009), plus newly annotated sen-
tences used for test data. The original corpus con-
tains over 50,000 structured restaurant reviews in-
cluding restaurant information and a star rating.
The original star rating was not made available for
the Semeval tasks, and the aspect term annotations
and their associated sentiment were added for this
task; the original sentence-level sentiment annota-
tions were not provided. Most of the work explor-
ing this corpus to date uses unsupervised (Brody
and Elhadad, 2010) or semi-supervised (Mukher-
jee and Liu, 2012) approaches.
As there has been an explosion of research in
sentiment classification, it is impossible to review
all of the related work. See Tsytsarau and Palpanas
(2012) for a recent survey. We will note that our
approach follows a somewhat standard machine
learning approach inspired by that of Wilson et al.
(2005), but with a different feature set. We did
not thoroughly explore as many classifiers as this
work and others have done. Finally, we note that
some work has investigated the joint task of iden-
tifying opinion phrases or targets simultaneously
with polarity (Choi and Cardie, 2009; Johansson
and Moschitti, 2013; Mitchell et al., 2013).
</bodyText>
<sectionHeader confidence="0.995437" genericHeader="method">
3 Approach
</sectionHeader>
<bodyText confidence="0.999969814814815">
For both subtasks, we take a supervised ma-
chine learning approach, examining several classi-
fiers and their variants, and converging on feature
sets which performed best in small-scale cross-
validation experiments. After the official com-
petition ended, we continued to examine differ-
ent variants and discuss alternative approaches and
their accuracy in the experimental results section.
For all tasks we use the Maximum Entropy clas-
sifier, “iib” variant from the Natural Language
Toolkit (NLTK) in Python (Bird et al., 2009). We
experimented with several other classifiers from
NLTK and found that Maximum Entropy per-
formed best on a hold out set of data. We had orig-
inally planned to use a Conditional Random Field
(CRF) model (Lafferty et al., 2001) because of its
strong performance on similar tasks, but met with
time limitations when converting the data to the
appropriate format (there is no CRF provided with
NLTK at this time). We had also planned to try
classifiers from the scikit-learn toolkit (Pedregosa
et al., 2011), but again met with time constraints
due to the necessity to manually convert the fea-
tures to a binary representation.
We first preprocess the data using NLTK’s tok-
enization and part-of-speech tagging modules and
align the results with the aspect terms in the data,
</bodyText>
<page confidence="0.987019">
791
</page>
<bodyText confidence="0.99998775">
as detailed further below. The sentiment lexicon
we use as the basis of all sentiment features dis-
cussed below combines two standard lexicons (Liu
et al., 2005; Wilson et al., 2005).
</bodyText>
<subsectionHeader confidence="0.99491">
3.1 Aspect Term Extraction
</subsectionHeader>
<listItem confidence="0.90514175">
While it is difficult to give a precise definition of
aspect, it can be roughly thought of as a charac-
teristic of a target concept such as a restaurant or
laptop. Examples include the italicized terms in
the following:
• I liked the service and the staff, but not the
food.
• The hard disk is very noisy.
</listItem>
<bodyText confidence="0.9982645">
We use a sequence labeling approach, which
can also be thought of as a tagging or chunking
approach, to identify the aspect terms in each sen-
tence. Specifically, and similar to Breck et al.
(2007) and Mitchell et al. (2013), as the target
class for each token, we use the IOB2 sequence
labeling scheme (Tjong et al., 2000), where the
aspect terms are considered as the chunks to be la-
beled. Using this approach, each token is tagged
as either Beginning an aspect term, being In an as-
pect term, or being Outside an aspect term. We
also experimented with an IO labeling scheme as
discussed in the experimental results section, in
which each token is tagged as being either In or
Outside an aspect term. Here is an example of a
sentence with its IOB tags:
</bodyText>
<listItem confidence="0.91315">
• The-O pizza-B is-O the-O best-O if-O you-O
like-O thin-B crusted-I pizza-I .-O
</listItem>
<bodyText confidence="0.999814205882353">
Of course, unlike an HMM or CRF, a standard
classifier such as Maximum entropy does not la-
bel entire sequences. Therefore, each example
presented to our classifier represents a single to-
ken from the sentence being labeled, and the tar-
get label is the IOB tag of that token. Further,
we present the tokens of a given sentence in order
from the first word in the sentence to the last.
The features used for each token are derived
from the token, the prior token, and the next token
in the sentence (thus using a three-token window).
In addition, we include the IOB tag of the prior to-
ken, using the gold standard at training time and
the classifier’s output at testing time, even if it is
incorrect. For each token we extract the word,
its stem, its part-of-speech (POS) tag, its polar-
ity from the sentiment dictionary, and whether the
word is objective or subjective, from the same sen-
timent dictionary. We use dummy values for the
prior and next words of the first and last token in
a sentence, respectively. All feature-value pairs
are converted to binary features automatically by
NLTK.
Because we believed that the data would prove
to be sparse and that new words would appear in
the testing data, we also include an unknown word
feature, replacing the 50% least frequent words
in the training data with the “UNK” token, and
doing the same for both these words and unseen
words in the test set. However, we later found that
we should have used cross-validation to support
our hypothesis, and that using the full vocabulary
would have improved our results, as shown in the
experimental results section.
</bodyText>
<subsectionHeader confidence="0.993903">
3.2 Polarity
</subsectionHeader>
<bodyText confidence="0.999974625">
In the polarity subtask, the aspect terms are pro-
vided, and the goal is to classify them as posi-
tive, negative, conflict, or neutral. In this case,
we use a simple classification approach that in-
cludes features of the aspect term and surround-
ing tokens (again in a three-token window), and
also some simple baseline sentiment classification
features. First, we use similar features as for the
aspect term extraction task, with changes to incor-
porate the fact that aspect terms are occasionally
phrases, not single words. In fact, we hypothesize
that features of the words before and after an as-
pect phrase could be more useful than the words
prior to and after a particular word in the phrase.
Thus, instead of using features from the three-
token window including the current token, we use
features from the words on each side of the as-
pect phrase, and use the head of the aspect phrase
and its features as the middle of the window. This
approach is similar to that of Johansson and Mos-
chitti (2013), who use features from the words be-
fore and after opinion expressions. In our case,
these features are again the word, its POS tag, its
sentiment polarity and objectivity, and its IOB tag.
Note that in this case we use the IOB tag from all
terms in the window, since the aspect term extrac-
tion task is treated as a prerequisite to the polarity
classification task.
In addition to these word-based features, we
add four higher-level features. The first is an in-
dicator of the number of aspect terms in the en-
tire sentence, since this might indicate a more de-
</bodyText>
<page confidence="0.986785">
792
</page>
<bodyText confidence="0.999967866666667">
tailed sentence, and we believe that more specific
sentences might correlate with positive sentiment.
The other three features are baselines connected to
the estimated sentiment of the sentence or phrase.
First, we apply a hand-built sentence level senti-
ment classifier that follows a now standard base-
line approach (Zhu et al., 2009): using a senti-
ment lexicon (Liu’s), it counts the number of posi-
tive and negative sentiment words in the sentence,
flipping polarity when negation words are encoun-
tered, and discontinuing the polarity flip when
punctuation is encountered. This results in a “high
level sentiment” feature consisting of the number
of positive sentiment words minus the number of
negative sentiment words.
Our other two sentiment features provide finer
granularity information, based on the sentiment
of the “chunks” in which an aspect term appears.
First, we use the punctuation within the sentence
to divide it into punctuation-separated chunks.
Then, we calculate the number of positive and neg-
ative sentiment words within each chunk, again
flipping polarity after the presence of a negation
word. The positive and negative counts associated
with the chunk within which an aspect phrase ap-
pears are then used as features when classifying
the phrase. We also experimented with using con-
junctions (and, or, but, etc.) as chunk boundaries,
but preliminary results indicated that this resulted
in reduced accuracy.
</bodyText>
<sectionHeader confidence="0.998561" genericHeader="method">
4 Experimental Results &amp; Analysis
</sectionHeader>
<bodyText confidence="0.999960823529412">
In this section we report our results and some ad-
ditional analysis for the ABSA subtasks 1 and 2.
Please refer to Pontiki et al. (2014) for details on
the tasks, corpora, and evaluation criteria. We
chose the constrained condition, which allows the
use of sentiment lexicons in addition to the train-
ing data provided, but no additional data such as
other reviews.
Aspect term extraction is evaluated using Pre-
cision, Recall, and F-measure on an unseen set of
sentences. Table 1 shows our results1 on both do-
mains, the top results,2 and the mean score of all
constrained submissions (21 entries). Note that for
Restaurants, COMMIT-P1WP3 had the best Preci-
sion, at 0.909, but XRCE had the best F-measure,
so we show their three scores. Our results were
close to the mean for both corpora and quite a
</bodyText>
<footnote confidence="0.999257">
1Rank averaged over P, R, and F for USF
2We abbreviate IHS RD Belarus as Belarus.
</footnote>
<table confidence="0.999961111111111">
System P R F1 Rank
Lap Belarus 0.848 0.665 0.746 1
mean 0.760 0.503 0.562 11
USF 0.754 0.404 0.526 14.7
baseline 0.443 0.298 0.356
Rest XRCE 0.862 0.818 0.840 1
mean 0.770 0.649 0.693 11
USF 0.783 0.645 0.707 14.3
baseline 0.525 0.428 0.472
</table>
<tableCaption confidence="0.98069">
Table 1: Aspect Term Extraction Results, Con-
strained.
</tableCaption>
<table confidence="0.999985222222222">
Approach P R F1
Lap FV-No-Snt 0.724 0.622 0.669
Full Voc. 0.733 0.601 0.660
Original 0.715 0.493 0.583
IO 0.696 0.501 0.582
Rest Full Voc. 0.792 0.704 0.746
FV-No-Snt 0.784 0.710 0.745
Original 0.777 0.657 0.711
IO 0.769 0.660 0.710
</table>
<tableCaption confidence="0.904603">
Table 2: Aspect Term Extraction Cross-Validation
Results.
</tableCaption>
<bodyText confidence="0.999866730769231">
bit above the lowest scoring submissions and the
baseline provided by the organizers; the latter is
also shown in the Table.
After the submission deadline, we continued to
experiment with alternative approaches using 5-
fold cross validation on the training set, shown in
Table 2. We found that using the full vocabulary
was better than our original approach of only us-
ing the top 50% occurring words, even with 28%
unseen words in the restaurant test set and 21% in
laptops. We also found that leaving out the polar-
ity feature while using all vocabulary words (FV-
No-Snt) improved our F-measure score to 0.669
for laptops but reduced it slightly to 0.745 for
restaurants. Finally, using IO versus IOB tag-
ging did not influence the F-measure significantly.
About 25% of the aspect terms in the restaurant
training set have length greater than one, and 37%
of the laptop terms.
Aspect term polarity is evaluated on accuracy
over all labels: positive, negative, neutral, or con-
flict. Table 3 shows our results on both domains,
the top results, the mean score of all constrained
submissions (24 entries for laptops, 28 for restau-
rants), and the baseline accuracy. In this case our
scores are above average in all cases.
</bodyText>
<page confidence="0.996556">
793
</page>
<table confidence="0.999665111111111">
System Acc Rank
NRC-Canada 0.705 1
Lap USF 0.645 6
mean 0.604 12.5
baseline 0.514
DCU 0.810 1
Rest USF 0.732 9
mean 0.702 14.5
baseline 0.643
</table>
<tableCaption confidence="0.9733935">
Table 3: Aspect Term Polarity Results, Con-
strained.
</tableCaption>
<sectionHeader confidence="0.991824" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.9999939">
In conclusion, we show that a chunking approach
to supervised learning works fairly well in the
aspect term extraction task, and that local sen-
tence features and a baseline sentiment classifier
work well for aspect term polarity classification.
Our systems for both tasks performed reasonably
well considering the relatively simple classifica-
tion techniques and features incorporated. In fu-
ture work, we plan to apply more sophisticated
classifiers which have shown to be accurate on re-
lated tasks, including CRFs and Support Vector
Machines. We also would like to experiment with
variants of the features used here, such as the ex-
ploration of smaller or larger context windows, or
the usefulness of stemming compared to the orig-
inal tokens. We also believe that more sophisti-
cated syntactic or semantic features, or topic mod-
els, could improve results on one or both tasks.
We thank the organizers for the provision of this
interesting dataset.
</bodyText>
<sectionHeader confidence="0.970219" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999462333333333">
The author thanks her spring 2014 research assis-
tant, Hao Chen, for his help in preparing some of
the code used in the experiments.
</bodyText>
<sectionHeader confidence="0.992854" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997812507692308">
Steven Bird, Edward Loper, and Ewan Klein.
2009. Natural Language Processing with Python.
O’Reilly Media Inc.
Eric Breck, Yejin Choi, and Claire Cardie. 2007. Iden-
tifying expressions of opinion in context. In Twen-
tieth International Joint Conference on Artificial In-
telligence, pages 2683–2688.
Samuel Brody and Noemie Elhadad. 2010. An unsu-
pervised aspect-sentiment model for online reviews.
In Human Language Technologies: The 2010 An-
nual Conference of the North American Chapter
of the Association for Computational Linguistics,
pages 804–812.
Yejin Choi and Claire Cardie. 2009. Adapting a po-
larity lexicon using integer linear programming for
domain-specific sentiment classification. In Pro-
ceedings of the 2009 Conference on Empirical Meth-
ods in Natural Language Processing, pages 590–
598.
Angela Fahrni and Manfred Klenner. 2008. Old wine
or warm beer: Target-specific sentiment analysis of
adjectives. In Proc. of the Symposium on Affective
Language in Human and Machine, AISB, pages 60–
63.
Gayatree Ganu, Noemie Elhadad, and Amelie Marian.
2009. Beyond the stars: Improving rating predic-
tions using review text content. In 12th Interna-
tional Workshop on the Web and Databases, pages
1–6.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.
Wei Jin, Hung Hay Ho, and Rohini K Srihari. 2009. A
novel lexicalized HMM-based learning framework
for web opinion mining. In Proceedings of the
26th Annual International Conference on Machine
Learning, pages 465–472.
Richard Johansson and Alessandro Moschitti. 2013.
Relational features in fine-grained opinion analysis.
Computational Linguistics, 39(3):473–509.
Soo-min Kim and Eduard Hovy. 2006. Extracting
opinions, opinion holders, and topics expressed in
online news media text. In Workshop on Sentiment
and Subjectivity in Text, pages 1–8.
Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto.
2007. Extracting aspect-evaluation and aspect-of
relations in opinion mining. In EMNLP-CoNLL,
pages 1065–1074.
John Lafferty, Andrew McCallum, and Fernando
Pereira. 2001. Conditional random fields: Prob-
abilistic models for segmenting and labeling se-
quence data. In Proceedings of International Con-
ference on Machine Learning, pages 282–289.
Fangtao Li, Chao Han, Minlie Huang, and Xiaoyan
Zhu. 2010. Structure-aware review mining and
summarization. In Proceedings of the 23rd Inter-
national Conference on Computational Linguistics,
pages 653–661.
Bing Liu, Minqing Hu, and Junsheng Cheng. 2005.
Opinion observer: Analyzing and comparing opin-
ions on the web. In Proceedings of the 14th Interna-
tional World Wide Web conference, pages 342–351.
ACM.
</reference>
<page confidence="0.998885">
794
</page>
<bodyText confidence="0.955029714285714">
Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005. Annotating expressions of opinions and emo-
tions in language. Language Resources and Evalu-
ation, 39(2-3):165–210.
Bing Liu. 2010. Sentiment analysis and subjectiv-
ity. In Handbook of Natural Language Processing
2, pages 627–666. CRC Press.
</bodyText>
<reference confidence="0.981378135802469">
Margaret Mitchell, Jacqueline Aguilar, Theresa Wil-
son, and Benjamin Van Durme. 2013. Open domain
targeted sentiment. In EMNLP, pages 1643–1654.
Samaneh Moghaddam and Martin Ester. 2010. Opin-
ion digger: An unsupervised opinion miner from
unstructured product reviews. In Proceedings of
the 19th ACM international conference on Informa-
tion and knowledge management, pages 1825–1828.
ACM.
Arjun Mukherjee and Bing Liu. 2012. Aspect extrac-
tion through semi-supervised modeling. In Proceed-
ings of the 50th Annual Meeting of the Association
for Computational Linguistics, pages 339–348.
Bo Pang and Lillian Lee. 2008. Opinion mining and
sentiment analysis. Foundations and Trends in In-
formation Retrieval, 2(1-2):1–135.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron
Weiss, Vincent Dubourg, Jake Vanderplas, Alexan-
dre Passos, David Cournapeau, Matthieu Brucher,
Matthieu Perrot, and ´Edouard Duchesnay. 2011.
Scikit-learn: Machine learning in Python. Journal
of Machine Learning Research, 12:2825–2830.
Maria Pontiki, Dimitrios Galanis, John Pavlopou-
los, Harris Papageorgiou, Ion Androutsopoulos, and
Suresh Manandhar. 2014. Semeval-2014 task 4:
Aspect based sentiment analysis. In Proceedings of
the 8th International Workshop on Semantic Evalu-
ation (SemEval 2014).
Ana-maria Popescu and Oren Etzioni. 2005. Extract-
ing product features and opinions from reviews. In
Proceedings of Human Language Technology Con-
ference and Conference on Empirical Methods in
Natural Language Processing, pages 339–346.
Ivan Titov and Ryan McDonald. 2008. Modeling
online reviews with multi-grain topic models. In
Proceeding of the 17th international conference on
World Wide Web, pages 111–120, New York, New
York, USA. ACM.
Erik F Tjong, Kim Sang, Walter Daelemans, Rob
Koeling, Yuval Krymolowski, Vasin Punyakanok,
Dan Roth, Millers Yard, Mill Lane, and Ramat
Gan. 2000. Applying system combination to base
noun phrase identification. In Proceedings of the
18th conference on Computational linguistics, pages
857–863.
Mikalai Tsytsarau and Themis Palpanas. 2012. Survey
on mining subjective data on the web. Data Mining
and Knowledge Discovery, 24(3):478–514.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-
level sentiment analysis. In Proceedings of the con-
ference on Human Language Technology and Em-
pirical Methods in Natural Language Processing,
pages 347–354.
Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.
2009. Phrase dependency parsing for opinion min-
ing. In Proceedings of the 2009 Conference on Em-
pirical Methods in Natural Language Processing,
pages 1533–1541.
Bishan Yang and Claire Cardie. 2013. Joint inference
for fine-grained opinion extraction. In Proceedings
ofACL, pages 16550–1649.
Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011.
Clustering product features for opinion mining. In
Proceedings of the fourth ACM international con-
ference on Web search and data mining, pages 347–
354, New York, New York, USA. ACM.
Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara,
Joseph Johnson, and Xuanjing Huang. 2009. Min-
ing product reviews based on shallow dependency
parsing. In Proceedings of the 32nd international
ACM SIGIR conference on Research and develop-
ment in information retrieval, pages 726–727. ACM.
Jingbo Zhu, Muhua Zhu, Huizhen Wang, and Ben-
jamin Tsou. 2009. Aspect-based sentence segmen-
tation for sentiment summarization. In Proceedings
of the 1st international CIKM workshop on Topic-
sentiment analysis for mass opinion, pages 65–72.
ACM.
</reference>
<page confidence="0.998239">
795
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.925264">
<title confidence="0.999145">USF: Chunking for Aspect Term Identification &amp; Polarity Classification</title>
<author confidence="0.987817">Cindi</author>
<affiliation confidence="0.999918">University of San</affiliation>
<address confidence="0.999352">2130 Fulton St, HR 240 San Francisco, CA</address>
<email confidence="0.98712">cathompson4@usfca.edu</email>
<abstract confidence="0.995675166666667">This paper describes the systems submitted by the University of San Francisco (USF) to Semeval-2014 Task 4, Aspect Based Sentiment Analysis (ABSA), which provides labeled data in two domains, laptops and restaurants. For the constrained condition of both the aspect term extraction and aspect term polarity tasks, we take a supervised machine learning approach using a combination of lexical, syntactic, and baseline sentiment features. Our extraction approach is inspired by a chunking approach, based on its strong past results on related tasks. Our system performed slightly below average compared to other submissions, possibly because we use a simpler classification model than prior work. Our polarity labeling approach uses two baseline hand-built sentiment classifiers as features in addition to lexical and syntactic features, and performed in the top ten of other constrained systems on both domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Edward Loper</author>
<author>Ewan Klein</author>
</authors>
<date>2009</date>
<booktitle>Natural Language Processing with Python. O’Reilly Media Inc.</booktitle>
<contexts>
<context position="7873" citStr="Bird et al., 2009" startWordPosition="1255" endWordPosition="1258">arity (Choi and Cardie, 2009; Johansson and Moschitti, 2013; Mitchell et al., 2013). 3 Approach For both subtasks, we take a supervised machine learning approach, examining several classifiers and their variants, and converging on feature sets which performed best in small-scale crossvalidation experiments. After the official competition ended, we continued to examine different variants and discuss alternative approaches and their accuracy in the experimental results section. For all tasks we use the Maximum Entropy classifier, “iib” variant from the Natural Language Toolkit (NLTK) in Python (Bird et al., 2009). We experimented with several other classifiers from NLTK and found that Maximum Entropy performed best on a hold out set of data. We had originally planned to use a Conditional Random Field (CRF) model (Lafferty et al., 2001) because of its strong performance on similar tasks, but met with time limitations when converting the data to the appropriate format (there is no CRF provided with NLTK at this time). We had also planned to try classifiers from the scikit-learn toolkit (Pedregosa et al., 2011), but again met with time constraints due to the necessity to manually convert the features to </context>
</contexts>
<marker>Bird, Loper, Klein, 2009</marker>
<rawString>Steven Bird, Edward Loper, and Ewan Klein. 2009. Natural Language Processing with Python. O’Reilly Media Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Breck</author>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Identifying expressions of opinion in context.</title>
<date>2007</date>
<booktitle>In Twentieth International Joint Conference on Artificial Intelligence,</booktitle>
<pages>2683--2688</pages>
<contexts>
<context position="5041" citStr="Breck et al., 2007" startWordPosition="795" endWordPosition="798">and McDonald, 2008). The benchmark corpus for sentiment analysis from Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007; Jin et al., 2009; Johansson and Moschitti, 2013; Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2013) and semantic dependency or semantic parsing approaches (Kim and Hovy, 2006; Kobayashi et al., 2007; Wu et al., 2009) sometimes using background knowledge from sentiment lexicons (Zhang et al., 2009). The main differences between our approach and that of Breck et al. (2007) and Mitchell et al. (2013) are the classifier used and some of the features; they both use CRFs versus our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A sec</context>
<context position="9360" citStr="Breck et al. (2007)" startWordPosition="1514" endWordPosition="1517">es discussed below combines two standard lexicons (Liu et al., 2005; Wilson et al., 2005). 3.1 Aspect Term Extraction While it is difficult to give a precise definition of aspect, it can be roughly thought of as a characteristic of a target concept such as a restaurant or laptop. Examples include the italicized terms in the following: • I liked the service and the staff, but not the food. • The hard disk is very noisy. We use a sequence labeling approach, which can also be thought of as a tagging or chunking approach, to identify the aspect terms in each sentence. Specifically, and similar to Breck et al. (2007) and Mitchell et al. (2013), as the target class for each token, we use the IOB2 sequence labeling scheme (Tjong et al., 2000), where the aspect terms are considered as the chunks to be labeled. Using this approach, each token is tagged as either Beginning an aspect term, being In an aspect term, or being Outside an aspect term. We also experimented with an IO labeling scheme as discussed in the experimental results section, in which each token is tagged as being either In or Outside an aspect term. Here is an example of a sentence with its IOB tags: • The-O pizza-B is-O the-O best-O if-O you-</context>
</contexts>
<marker>Breck, Choi, Cardie, 2007</marker>
<rawString>Eric Breck, Yejin Choi, and Claire Cardie. 2007. Identifying expressions of opinion in context. In Twentieth International Joint Conference on Artificial Intelligence, pages 2683–2688.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samuel Brody</author>
<author>Noemie Elhadad</author>
</authors>
<title>An unsupervised aspect-sentiment model for online reviews.</title>
<date>2010</date>
<contexts>
<context position="6643" citStr="Brody and Elhadad, 2010" startWordPosition="1060" endWordPosition="1063">system. The restaurant reviews used in this Semeval task are a 3000-plus sentence subset of those harvested by Ganu et al. (2009), plus newly annotated sentences used for test data. The original corpus contains over 50,000 structured restaurant reviews including restaurant information and a star rating. The original star rating was not made available for the Semeval tasks, and the aspect term annotations and their associated sentiment were added for this task; the original sentence-level sentiment annotations were not provided. Most of the work exploring this corpus to date uses unsupervised (Brody and Elhadad, 2010) or semi-supervised (Mukherjee and Liu, 2012) approaches. As there has been an explosion of research in sentiment classification, it is impossible to review all of the related work. See Tsytsarau and Palpanas (2012) for a recent survey. We will note that our approach follows a somewhat standard machine learning approach inspired by that of Wilson et al. (2005), but with a different feature set. We did not thoroughly explore as many classifiers as this work and others have done. Finally, we note that some work has investigated the joint task of identifying opinion phrases or targets simultaneou</context>
</contexts>
<marker>Brody, Elhadad, 2010</marker>
<rawString>Samuel Brody and Noemie Elhadad. 2010. An unsupervised aspect-sentiment model for online reviews.</rawString>
</citation>
<citation valid="true">
<authors>
<author>In Human</author>
</authors>
<title>Language Technologies: The</title>
<date>2010</date>
<booktitle>Annual Conference of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<pages>804--812</pages>
<marker>Human, 2010</marker>
<rawString>In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pages 804–812.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yejin Choi</author>
<author>Claire Cardie</author>
</authors>
<title>Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>590--598</pages>
<contexts>
<context position="7283" citStr="Choi and Cardie, 2009" startWordPosition="1165" endWordPosition="1168"> (Mukherjee and Liu, 2012) approaches. As there has been an explosion of research in sentiment classification, it is impossible to review all of the related work. See Tsytsarau and Palpanas (2012) for a recent survey. We will note that our approach follows a somewhat standard machine learning approach inspired by that of Wilson et al. (2005), but with a different feature set. We did not thoroughly explore as many classifiers as this work and others have done. Finally, we note that some work has investigated the joint task of identifying opinion phrases or targets simultaneously with polarity (Choi and Cardie, 2009; Johansson and Moschitti, 2013; Mitchell et al., 2013). 3 Approach For both subtasks, we take a supervised machine learning approach, examining several classifiers and their variants, and converging on feature sets which performed best in small-scale crossvalidation experiments. After the official competition ended, we continued to examine different variants and discuss alternative approaches and their accuracy in the experimental results section. For all tasks we use the Maximum Entropy classifier, “iib” variant from the Natural Language Toolkit (NLTK) in Python (Bird et al., 2009). We exper</context>
</contexts>
<marker>Choi, Cardie, 2009</marker>
<rawString>Yejin Choi and Claire Cardie. 2009. Adapting a polarity lexicon using integer linear programming for domain-specific sentiment classification. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 590– 598.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Angela Fahrni</author>
<author>Manfred Klenner</author>
</authors>
<title>Old wine or warm beer: Target-specific sentiment analysis of adjectives.</title>
<date>2008</date>
<booktitle>In Proc. of the Symposium on Affective Language in Human and Machine, AISB,</booktitle>
<pages>60--63</pages>
<contexts>
<context position="4370" citStr="Fahrni and Klenner, 2008" startWordPosition="688" endWordPosition="691">mantic Evaluation (SemEval 2014), pages 790–795, Dublin, Ireland, August 23-24, 2014. of gold-standard labeled data that focuses on aspect terms, rather than more general expressions of subjectivity or other private states. Thus, early work focused on learning or identifying aspects in an unsupervised (Hu and Liu, 2004) or semisupervised setting (Moghaddam and Ester, 2010; Zhai et al., 2011). The earliest work on aspect detection focused on identifying frequently occurring noun phrases using information extraction (IE) techniques (Hu and Liu, 2004). Unsupervised techniques include clustering (Fahrni and Klenner, 2008; Popescu and Etzioni, 2005) and topic models (Titov and McDonald, 2008). The benchmark corpus for sentiment analysis from Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extracti</context>
</contexts>
<marker>Fahrni, Klenner, 2008</marker>
<rawString>Angela Fahrni and Manfred Klenner. 2008. Old wine or warm beer: Target-specific sentiment analysis of adjectives. In Proc. of the Symposium on Affective Language in Human and Machine, AISB, pages 60– 63.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gayatree Ganu</author>
<author>Noemie Elhadad</author>
<author>Amelie Marian</author>
</authors>
<title>Beyond the stars: Improving rating predictions using review text content.</title>
<date>2009</date>
<booktitle>In 12th International Workshop on the Web and Databases,</booktitle>
<pages>1--6</pages>
<contexts>
<context position="6148" citStr="Ganu et al. (2009)" startWordPosition="982" endWordPosition="985">s our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A second related corpus which includes more aspect information is that developed by Kim and Hovy (2006). This corpus also focuses on news articles rather than reviews, but does expand the types of aspects identified. The main focus of that work is on the identification, using FrameNet role labels, of the holder and target of an opinion, while the opinion itself is provided to the system. The restaurant reviews used in this Semeval task are a 3000-plus sentence subset of those harvested by Ganu et al. (2009), plus newly annotated sentences used for test data. The original corpus contains over 50,000 structured restaurant reviews including restaurant information and a star rating. The original star rating was not made available for the Semeval tasks, and the aspect term annotations and their associated sentiment were added for this task; the original sentence-level sentiment annotations were not provided. Most of the work exploring this corpus to date uses unsupervised (Brody and Elhadad, 2010) or semi-supervised (Mukherjee and Liu, 2012) approaches. As there has been an explosion of research in s</context>
</contexts>
<marker>Ganu, Elhadad, Marian, 2009</marker>
<rawString>Gayatree Ganu, Noemie Elhadad, and Amelie Marian. 2009. Beyond the stars: Improving rating predictions using review text content. In 12th International Workshop on the Web and Databases, pages 1–6.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4067" citStr="Hu and Liu, 2004" startWordPosition="645" endWordPosition="648">ed to aspect and aspect term identification, and research related to sentiment classification for aspect terms. We note that aspects have also been called topics and features in prior work. Until more recently, the community lacked a corpus 790 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 790–795, Dublin, Ireland, August 23-24, 2014. of gold-standard labeled data that focuses on aspect terms, rather than more general expressions of subjectivity or other private states. Thus, early work focused on learning or identifying aspects in an unsupervised (Hu and Liu, 2004) or semisupervised setting (Moghaddam and Ester, 2010; Zhai et al., 2011). The earliest work on aspect detection focused on identifying frequently occurring noun phrases using information extraction (IE) techniques (Hu and Liu, 2004). Unsupervised techniques include clustering (Fahrni and Klenner, 2008; Popescu and Etzioni, 2005) and topic models (Titov and McDonald, 2008). The benchmark corpus for sentiment analysis from Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval tas</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Jin</author>
<author>Hung Hay Ho</author>
<author>Rohini K Srihari</author>
</authors>
<title>A novel lexicalized HMM-based learning framework for web opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the 26th Annual International Conference on Machine Learning,</booktitle>
<pages>465--472</pages>
<contexts>
<context position="5059" citStr="Jin et al., 2009" startWordPosition="799" endWordPosition="802"> The benchmark corpus for sentiment analysis from Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007; Jin et al., 2009; Johansson and Moschitti, 2013; Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2013) and semantic dependency or semantic parsing approaches (Kim and Hovy, 2006; Kobayashi et al., 2007; Wu et al., 2009) sometimes using background knowledge from sentiment lexicons (Zhang et al., 2009). The main differences between our approach and that of Breck et al. (2007) and Mitchell et al. (2013) are the classifier used and some of the features; they both use CRFs versus our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A second related corpus</context>
</contexts>
<marker>Jin, Ho, Srihari, 2009</marker>
<rawString>Wei Jin, Hung Hay Ho, and Rohini K Srihari. 2009. A novel lexicalized HMM-based learning framework for web opinion mining. In Proceedings of the 26th Annual International Conference on Machine Learning, pages 465–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Relational features in fine-grained opinion analysis.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="5090" citStr="Johansson and Moschitti, 2013" startWordPosition="803" endWordPosition="806">pus for sentiment analysis from Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007; Jin et al., 2009; Johansson and Moschitti, 2013; Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2013) and semantic dependency or semantic parsing approaches (Kim and Hovy, 2006; Kobayashi et al., 2007; Wu et al., 2009) sometimes using background knowledge from sentiment lexicons (Zhang et al., 2009). The main differences between our approach and that of Breck et al. (2007) and Mitchell et al. (2013) are the classifier used and some of the features; they both use CRFs versus our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A second related corpus which includes more aspect inf</context>
<context position="7314" citStr="Johansson and Moschitti, 2013" startWordPosition="1169" endWordPosition="1172">12) approaches. As there has been an explosion of research in sentiment classification, it is impossible to review all of the related work. See Tsytsarau and Palpanas (2012) for a recent survey. We will note that our approach follows a somewhat standard machine learning approach inspired by that of Wilson et al. (2005), but with a different feature set. We did not thoroughly explore as many classifiers as this work and others have done. Finally, we note that some work has investigated the joint task of identifying opinion phrases or targets simultaneously with polarity (Choi and Cardie, 2009; Johansson and Moschitti, 2013; Mitchell et al., 2013). 3 Approach For both subtasks, we take a supervised machine learning approach, examining several classifiers and their variants, and converging on feature sets which performed best in small-scale crossvalidation experiments. After the official competition ended, we continued to examine different variants and discuss alternative approaches and their accuracy in the experimental results section. For all tasks we use the Maximum Entropy classifier, “iib” variant from the Natural Language Toolkit (NLTK) in Python (Bird et al., 2009). We experimented with several other clas</context>
<context position="12614" citStr="Johansson and Moschitti (2013)" startWordPosition="2087" endWordPosition="2091">ilar features as for the aspect term extraction task, with changes to incorporate the fact that aspect terms are occasionally phrases, not single words. In fact, we hypothesize that features of the words before and after an aspect phrase could be more useful than the words prior to and after a particular word in the phrase. Thus, instead of using features from the threetoken window including the current token, we use features from the words on each side of the aspect phrase, and use the head of the aspect phrase and its features as the middle of the window. This approach is similar to that of Johansson and Moschitti (2013), who use features from the words before and after opinion expressions. In our case, these features are again the word, its POS tag, its sentiment polarity and objectivity, and its IOB tag. Note that in this case we use the IOB tag from all terms in the window, since the aspect term extraction task is treated as a prerequisite to the polarity classification task. In addition to these word-based features, we add four higher-level features. The first is an indicator of the number of aspect terms in the entire sentence, since this might indicate a more de792 tailed sentence, and we believe that m</context>
</contexts>
<marker>Johansson, Moschitti, 2013</marker>
<rawString>Richard Johansson and Alessandro Moschitti. 2013. Relational features in fine-grained opinion analysis. Computational Linguistics, 39(3):473–509.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Soo-min Kim</author>
<author>Eduard Hovy</author>
</authors>
<title>Extracting opinions, opinion holders, and topics expressed in online news media text.</title>
<date>2006</date>
<booktitle>In Workshop on Sentiment and Subjectivity in Text,</booktitle>
<pages>1--8</pages>
<contexts>
<context position="5229" citStr="Kim and Hovy, 2006" startWordPosition="827" endWordPosition="830">and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007; Jin et al., 2009; Johansson and Moschitti, 2013; Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2013) and semantic dependency or semantic parsing approaches (Kim and Hovy, 2006; Kobayashi et al., 2007; Wu et al., 2009) sometimes using background knowledge from sentiment lexicons (Zhang et al., 2009). The main differences between our approach and that of Breck et al. (2007) and Mitchell et al. (2013) are the classifier used and some of the features; they both use CRFs versus our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A second related corpus which includes more aspect information is that developed by Kim and Hovy (2006). This corpus also focuses on news articles rather than reviews, but does expand the types</context>
</contexts>
<marker>Kim, Hovy, 2006</marker>
<rawString>Soo-min Kim and Eduard Hovy. 2006. Extracting opinions, opinion holders, and topics expressed in online news media text. In Workshop on Sentiment and Subjectivity in Text, pages 1–8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nozomi Kobayashi</author>
<author>Kentaro Inui</author>
<author>Yuji Matsumoto</author>
</authors>
<title>Extracting aspect-evaluation and aspect-of relations in opinion mining.</title>
<date>2007</date>
<booktitle>In EMNLP-CoNLL,</booktitle>
<pages>1065--1074</pages>
<contexts>
<context position="5253" citStr="Kobayashi et al., 2007" startWordPosition="831" endWordPosition="834">r from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007; Jin et al., 2009; Johansson and Moschitti, 2013; Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2013) and semantic dependency or semantic parsing approaches (Kim and Hovy, 2006; Kobayashi et al., 2007; Wu et al., 2009) sometimes using background knowledge from sentiment lexicons (Zhang et al., 2009). The main differences between our approach and that of Breck et al. (2007) and Mitchell et al. (2013) are the classifier used and some of the features; they both use CRFs versus our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A second related corpus which includes more aspect information is that developed by Kim and Hovy (2006). This corpus also focuses on news articles rather than reviews, but does expand the types of aspects identified. </context>
</contexts>
<marker>Kobayashi, Inui, Matsumoto, 2007</marker>
<rawString>Nozomi Kobayashi, Kentaro Inui, and Yuji Matsumoto. 2007. Extracting aspect-evaluation and aspect-of relations in opinion mining. In EMNLP-CoNLL, pages 1065–1074.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
<author>Fernando Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In</title>
<date>2001</date>
<booktitle>Proceedings of International Conference on Machine Learning,</booktitle>
<pages>282--289</pages>
<contexts>
<context position="8100" citStr="Lafferty et al., 2001" startWordPosition="1295" endWordPosition="1298">ging on feature sets which performed best in small-scale crossvalidation experiments. After the official competition ended, we continued to examine different variants and discuss alternative approaches and their accuracy in the experimental results section. For all tasks we use the Maximum Entropy classifier, “iib” variant from the Natural Language Toolkit (NLTK) in Python (Bird et al., 2009). We experimented with several other classifiers from NLTK and found that Maximum Entropy performed best on a hold out set of data. We had originally planned to use a Conditional Random Field (CRF) model (Lafferty et al., 2001) because of its strong performance on similar tasks, but met with time limitations when converting the data to the appropriate format (there is no CRF provided with NLTK at this time). We had also planned to try classifiers from the scikit-learn toolkit (Pedregosa et al., 2011), but again met with time constraints due to the necessity to manually convert the features to a binary representation. We first preprocess the data using NLTK’s tokenization and part-of-speech tagging modules and align the results with the aspect terms in the data, 791 as detailed further below. The sentiment lexicon we</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>John Lafferty, Andrew McCallum, and Fernando Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings of International Conference on Machine Learning, pages 282–289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fangtao Li</author>
<author>Chao Han</author>
<author>Minlie Huang</author>
<author>Xiaoyan Zhu</author>
</authors>
<title>Structure-aware review mining and summarization.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>653--661</pages>
<contexts>
<context position="5107" citStr="Li et al., 2010" startWordPosition="807" endWordPosition="810"> Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007; Jin et al., 2009; Johansson and Moschitti, 2013; Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2013) and semantic dependency or semantic parsing approaches (Kim and Hovy, 2006; Kobayashi et al., 2007; Wu et al., 2009) sometimes using background knowledge from sentiment lexicons (Zhang et al., 2009). The main differences between our approach and that of Breck et al. (2007) and Mitchell et al. (2013) are the classifier used and some of the features; they both use CRFs versus our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A second related corpus which includes more aspect information is that </context>
</contexts>
<marker>Li, Han, Huang, Zhu, 2010</marker>
<rawString>Fangtao Li, Chao Han, Minlie Huang, and Xiaoyan Zhu. 2010. Structure-aware review mining and summarization. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 653–661.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bing Liu</author>
<author>Minqing Hu</author>
<author>Junsheng Cheng</author>
</authors>
<title>Opinion observer: Analyzing and comparing opinions on the web.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th International World Wide Web conference,</booktitle>
<pages>342--351</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8808" citStr="Liu et al., 2005" startWordPosition="1413" endWordPosition="1416">erting the data to the appropriate format (there is no CRF provided with NLTK at this time). We had also planned to try classifiers from the scikit-learn toolkit (Pedregosa et al., 2011), but again met with time constraints due to the necessity to manually convert the features to a binary representation. We first preprocess the data using NLTK’s tokenization and part-of-speech tagging modules and align the results with the aspect terms in the data, 791 as detailed further below. The sentiment lexicon we use as the basis of all sentiment features discussed below combines two standard lexicons (Liu et al., 2005; Wilson et al., 2005). 3.1 Aspect Term Extraction While it is difficult to give a precise definition of aspect, it can be roughly thought of as a characteristic of a target concept such as a restaurant or laptop. Examples include the italicized terms in the following: • I liked the service and the staff, but not the food. • The hard disk is very noisy. We use a sequence labeling approach, which can also be thought of as a tagging or chunking approach, to identify the aspect terms in each sentence. Specifically, and similar to Breck et al. (2007) and Mitchell et al. (2013), as the target class</context>
</contexts>
<marker>Liu, Hu, Cheng, 2005</marker>
<rawString>Bing Liu, Minqing Hu, and Junsheng Cheng. 2005. Opinion observer: Analyzing and comparing opinions on the web. In Proceedings of the 14th International World Wide Web conference, pages 342–351. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Margaret Mitchell</author>
<author>Jacqueline Aguilar</author>
<author>Theresa Wilson</author>
<author>Benjamin Van Durme</author>
</authors>
<title>Open domain targeted sentiment.</title>
<date>2013</date>
<booktitle>In EMNLP,</booktitle>
<pages>1643--1654</pages>
<marker>Mitchell, Aguilar, Wilson, Van Durme, 2013</marker>
<rawString>Margaret Mitchell, Jacqueline Aguilar, Theresa Wilson, and Benjamin Van Durme. 2013. Open domain targeted sentiment. In EMNLP, pages 1643–1654.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Samaneh Moghaddam</author>
<author>Martin Ester</author>
</authors>
<title>Opinion digger: An unsupervised opinion miner from unstructured product reviews.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th ACM international conference on Information and knowledge management,</booktitle>
<pages>1825--1828</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="4120" citStr="Moghaddam and Ester, 2010" startWordPosition="653" endWordPosition="656">and research related to sentiment classification for aspect terms. We note that aspects have also been called topics and features in prior work. Until more recently, the community lacked a corpus 790 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 790–795, Dublin, Ireland, August 23-24, 2014. of gold-standard labeled data that focuses on aspect terms, rather than more general expressions of subjectivity or other private states. Thus, early work focused on learning or identifying aspects in an unsupervised (Hu and Liu, 2004) or semisupervised setting (Moghaddam and Ester, 2010; Zhai et al., 2011). The earliest work on aspect detection focused on identifying frequently occurring noun phrases using information extraction (IE) techniques (Hu and Liu, 2004). Unsupervised techniques include clustering (Fahrni and Klenner, 2008; Popescu and Etzioni, 2005) and topic models (Titov and McDonald, 2008). The benchmark corpus for sentiment analysis from Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an </context>
</contexts>
<marker>Moghaddam, Ester, 2010</marker>
<rawString>Samaneh Moghaddam and Martin Ester. 2010. Opinion digger: An unsupervised opinion miner from unstructured product reviews. In Proceedings of the 19th ACM international conference on Information and knowledge management, pages 1825–1828. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arjun Mukherjee</author>
<author>Bing Liu</author>
</authors>
<title>Aspect extraction through semi-supervised modeling.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>339--348</pages>
<contexts>
<context position="6688" citStr="Mukherjee and Liu, 2012" startWordPosition="1066" endWordPosition="1070">emeval task are a 3000-plus sentence subset of those harvested by Ganu et al. (2009), plus newly annotated sentences used for test data. The original corpus contains over 50,000 structured restaurant reviews including restaurant information and a star rating. The original star rating was not made available for the Semeval tasks, and the aspect term annotations and their associated sentiment were added for this task; the original sentence-level sentiment annotations were not provided. Most of the work exploring this corpus to date uses unsupervised (Brody and Elhadad, 2010) or semi-supervised (Mukherjee and Liu, 2012) approaches. As there has been an explosion of research in sentiment classification, it is impossible to review all of the related work. See Tsytsarau and Palpanas (2012) for a recent survey. We will note that our approach follows a somewhat standard machine learning approach inspired by that of Wilson et al. (2005), but with a different feature set. We did not thoroughly explore as many classifiers as this work and others have done. Finally, we note that some work has investigated the joint task of identifying opinion phrases or targets simultaneously with polarity (Choi and Cardie, 2009; Joh</context>
</contexts>
<marker>Mukherjee, Liu, 2012</marker>
<rawString>Arjun Mukherjee and Bing Liu. 2012. Aspect extraction through semi-supervised modeling. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 339–348.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
</authors>
<title>Opinion mining and sentiment analysis.</title>
<date>2008</date>
<booktitle>Foundations and Trends in Information Retrieval,</booktitle>
<pages>2--1</pages>
<contexts>
<context position="1341" citStr="Pang and Lee, 2008" startWordPosition="208" endWordPosition="211">n related tasks. Our system performed slightly below average compared to other submissions, possibly because we use a simpler classification model than prior work. Our polarity labeling approach uses two baseline hand-built sentiment classifiers as features in addition to lexical and syntactic features, and performed in the top ten of other constrained systems on both domains. 1 Introduction As stated in the call for participation for this Semeval task, sentiment analysis focusing on overall polarity of a document, sentence, or similar context has been well studied in recent years (Liu, 2010; Pang and Lee, 2008; Tsytsarau and Palpanas, 2012). However, there is less prior work examining finer levels of granularity associated with individual entities and their characteristics or attributes, which the organizers for this task call aspects. The aspect based sentiment analysis This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/ by/4.0/ task (ABSA) has the goal of identifying aspects of stated or implied target entities and the sentiment expressed towards</context>
</contexts>
<marker>Pang, Lee, 2008</marker>
<rawString>Bo Pang and Lillian Lee. 2008. Opinion mining and sentiment analysis. Foundations and Trends in Information Retrieval, 2(1-2):1–135.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand Thirion</author>
<author>Olivier Grisel</author>
<author>Mathieu Blondel</author>
<author>Peter Prettenhofer</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Thirion, Grisel, Blondel, Prettenhofer, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, Jake Vanderplas, Alexandre Passos, David Cournapeau, Matthieu Brucher, Matthieu Perrot, and ´Edouard Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maria Pontiki</author>
<author>Dimitrios Galanis</author>
<author>John Pavlopoulos</author>
<author>Harris Papageorgiou</author>
<author>Ion Androutsopoulos</author>
<author>Suresh Manandhar</author>
</authors>
<title>Semeval-2014 task 4: Aspect based sentiment analysis.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<contexts>
<context position="2263" citStr="Pontiki et al., 2014" startWordPosition="352" endWordPosition="355">tive Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/ by/4.0/ task (ABSA) has the goal of identifying aspects of stated or implied target entities and the sentiment expressed towards each aspect. This problem has not been deeply studied in prior literature due to the lack, until now, of a large gold standard dataset. This Semeval task has provided two such datasets, in the domains of laptops and restaurants. A full description of the task and data is presented with this volume (Pontiki et al., 2014). In this paper, we discuss our approach to the first two subtasks of the Semeval ABSA Task, those of aspect term extraction and aspect term polarity. In aspect term extraction the domain (restaurants or laptops) is known and the goal is to identify terms in a sentence that are features commonly associated with that domain, such as service and staff in the case of restaurants or size and speed in the case of laptops. In the polarity subtask, the aspect terms for a given sentence are already identified and the sentiment polarity (positive, negative, conflict, or neutral) must be assigned. We ap</context>
<context position="14775" citStr="Pontiki et al. (2014)" startWordPosition="2443" endWordPosition="2446">late the number of positive and negative sentiment words within each chunk, again flipping polarity after the presence of a negation word. The positive and negative counts associated with the chunk within which an aspect phrase appears are then used as features when classifying the phrase. We also experimented with using conjunctions (and, or, but, etc.) as chunk boundaries, but preliminary results indicated that this resulted in reduced accuracy. 4 Experimental Results &amp; Analysis In this section we report our results and some additional analysis for the ABSA subtasks 1 and 2. Please refer to Pontiki et al. (2014) for details on the tasks, corpora, and evaluation criteria. We chose the constrained condition, which allows the use of sentiment lexicons in addition to the training data provided, but no additional data such as other reviews. Aspect term extraction is evaluated using Precision, Recall, and F-measure on an unseen set of sentences. Table 1 shows our results1 on both domains, the top results,2 and the mean score of all constrained submissions (21 entries). Note that for Restaurants, COMMIT-P1WP3 had the best Precision, at 0.909, but XRCE had the best F-measure, so we show their three scores. O</context>
</contexts>
<marker>Pontiki, Galanis, Pavlopoulos, Papageorgiou, Androutsopoulos, Manandhar, 2014</marker>
<rawString>Maria Pontiki, Dimitrios Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. Semeval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ana-maria Popescu</author>
<author>Oren Etzioni</author>
</authors>
<title>Extracting product features and opinions from reviews.</title>
<date>2005</date>
<booktitle>In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>339--346</pages>
<contexts>
<context position="4398" citStr="Popescu and Etzioni, 2005" startWordPosition="692" endWordPosition="695"> 2014), pages 790–795, Dublin, Ireland, August 23-24, 2014. of gold-standard labeled data that focuses on aspect terms, rather than more general expressions of subjectivity or other private states. Thus, early work focused on learning or identifying aspects in an unsupervised (Hu and Liu, 2004) or semisupervised setting (Moghaddam and Ester, 2010; Zhai et al., 2011). The earliest work on aspect detection focused on identifying frequently occurring noun phrases using information extraction (IE) techniques (Hu and Liu, 2004). Unsupervised techniques include clustering (Fahrni and Klenner, 2008; Popescu and Etzioni, 2005) and topic models (Titov and McDonald, 2008). The benchmark corpus for sentiment analysis from Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually </context>
</contexts>
<marker>Popescu, Etzioni, 2005</marker>
<rawString>Ana-maria Popescu and Oren Etzioni. 2005. Extracting product features and opinions from reviews. In Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, pages 339–346.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ivan Titov</author>
<author>Ryan McDonald</author>
</authors>
<title>Modeling online reviews with multi-grain topic models.</title>
<date>2008</date>
<booktitle>In Proceeding of the 17th international conference on World Wide Web,</booktitle>
<pages>111--120</pages>
<publisher>ACM.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="4442" citStr="Titov and McDonald, 2008" startWordPosition="700" endWordPosition="703">t 23-24, 2014. of gold-standard labeled data that focuses on aspect terms, rather than more general expressions of subjectivity or other private states. Thus, early work focused on learning or identifying aspects in an unsupervised (Hu and Liu, 2004) or semisupervised setting (Moghaddam and Ester, 2010; Zhai et al., 2011). The earliest work on aspect detection focused on identifying frequently occurring noun phrases using information extraction (IE) techniques (Hu and Liu, 2004). Unsupervised techniques include clustering (Fahrni and Klenner, 2008; Popescu and Etzioni, 2005) and topic models (Titov and McDonald, 2008). The benchmark corpus for sentiment analysis from Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007;</context>
</contexts>
<marker>Titov, McDonald, 2008</marker>
<rawString>Ivan Titov and Ryan McDonald. 2008. Modeling online reviews with multi-grain topic models. In Proceeding of the 17th international conference on World Wide Web, pages 111–120, New York, New York, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Erik F Tjong</author>
<author>Kim Sang</author>
<author>Walter Daelemans</author>
<author>Rob Koeling</author>
<author>Yuval Krymolowski</author>
<author>Vasin Punyakanok</author>
<author>Dan Roth</author>
<author>Millers Yard</author>
<author>Mill Lane</author>
<author>Ramat Gan</author>
</authors>
<title>Applying system combination to base noun phrase identification.</title>
<date>2000</date>
<booktitle>In Proceedings of the 18th conference on Computational linguistics,</booktitle>
<pages>857--863</pages>
<contexts>
<context position="9486" citStr="Tjong et al., 2000" startWordPosition="1537" endWordPosition="1540"> is difficult to give a precise definition of aspect, it can be roughly thought of as a characteristic of a target concept such as a restaurant or laptop. Examples include the italicized terms in the following: • I liked the service and the staff, but not the food. • The hard disk is very noisy. We use a sequence labeling approach, which can also be thought of as a tagging or chunking approach, to identify the aspect terms in each sentence. Specifically, and similar to Breck et al. (2007) and Mitchell et al. (2013), as the target class for each token, we use the IOB2 sequence labeling scheme (Tjong et al., 2000), where the aspect terms are considered as the chunks to be labeled. Using this approach, each token is tagged as either Beginning an aspect term, being In an aspect term, or being Outside an aspect term. We also experimented with an IO labeling scheme as discussed in the experimental results section, in which each token is tagged as being either In or Outside an aspect term. Here is an example of a sentence with its IOB tags: • The-O pizza-B is-O the-O best-O if-O you-O like-O thin-B crusted-I pizza-I .-O Of course, unlike an HMM or CRF, a standard classifier such as Maximum entropy does not </context>
</contexts>
<marker>Tjong, Sang, Daelemans, Koeling, Krymolowski, Punyakanok, Roth, Yard, Lane, Gan, 2000</marker>
<rawString>Erik F Tjong, Kim Sang, Walter Daelemans, Rob Koeling, Yuval Krymolowski, Vasin Punyakanok, Dan Roth, Millers Yard, Mill Lane, and Ramat Gan. 2000. Applying system combination to base noun phrase identification. In Proceedings of the 18th conference on Computational linguistics, pages 857–863.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mikalai Tsytsarau</author>
<author>Themis Palpanas</author>
</authors>
<title>Survey on mining subjective data on the web. Data Mining and Knowledge Discovery,</title>
<date>2012</date>
<contexts>
<context position="1372" citStr="Tsytsarau and Palpanas, 2012" startWordPosition="212" endWordPosition="216"> system performed slightly below average compared to other submissions, possibly because we use a simpler classification model than prior work. Our polarity labeling approach uses two baseline hand-built sentiment classifiers as features in addition to lexical and syntactic features, and performed in the top ten of other constrained systems on both domains. 1 Introduction As stated in the call for participation for this Semeval task, sentiment analysis focusing on overall polarity of a document, sentence, or similar context has been well studied in recent years (Liu, 2010; Pang and Lee, 2008; Tsytsarau and Palpanas, 2012). However, there is less prior work examining finer levels of granularity associated with individual entities and their characteristics or attributes, which the organizers for this task call aspects. The aspect based sentiment analysis This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/ by/4.0/ task (ABSA) has the goal of identifying aspects of stated or implied target entities and the sentiment expressed towards each aspect. This problem has </context>
<context position="6858" citStr="Tsytsarau and Palpanas (2012)" startWordPosition="1094" endWordPosition="1097"> over 50,000 structured restaurant reviews including restaurant information and a star rating. The original star rating was not made available for the Semeval tasks, and the aspect term annotations and their associated sentiment were added for this task; the original sentence-level sentiment annotations were not provided. Most of the work exploring this corpus to date uses unsupervised (Brody and Elhadad, 2010) or semi-supervised (Mukherjee and Liu, 2012) approaches. As there has been an explosion of research in sentiment classification, it is impossible to review all of the related work. See Tsytsarau and Palpanas (2012) for a recent survey. We will note that our approach follows a somewhat standard machine learning approach inspired by that of Wilson et al. (2005), but with a different feature set. We did not thoroughly explore as many classifiers as this work and others have done. Finally, we note that some work has investigated the joint task of identifying opinion phrases or targets simultaneously with polarity (Choi and Cardie, 2009; Johansson and Moschitti, 2013; Mitchell et al., 2013). 3 Approach For both subtasks, we take a supervised machine learning approach, examining several classifiers and their </context>
</contexts>
<marker>Tsytsarau, Palpanas, 2012</marker>
<rawString>Mikalai Tsytsarau and Themis Palpanas. 2012. Survey on mining subjective data on the web. Data Mining and Knowledge Discovery, 24(3):478–514.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phraselevel sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing,</booktitle>
<pages>347--354</pages>
<contexts>
<context position="7005" citStr="Wilson et al. (2005)" startWordPosition="1119" endWordPosition="1122">al tasks, and the aspect term annotations and their associated sentiment were added for this task; the original sentence-level sentiment annotations were not provided. Most of the work exploring this corpus to date uses unsupervised (Brody and Elhadad, 2010) or semi-supervised (Mukherjee and Liu, 2012) approaches. As there has been an explosion of research in sentiment classification, it is impossible to review all of the related work. See Tsytsarau and Palpanas (2012) for a recent survey. We will note that our approach follows a somewhat standard machine learning approach inspired by that of Wilson et al. (2005), but with a different feature set. We did not thoroughly explore as many classifiers as this work and others have done. Finally, we note that some work has investigated the joint task of identifying opinion phrases or targets simultaneously with polarity (Choi and Cardie, 2009; Johansson and Moschitti, 2013; Mitchell et al., 2013). 3 Approach For both subtasks, we take a supervised machine learning approach, examining several classifiers and their variants, and converging on feature sets which performed best in small-scale crossvalidation experiments. After the official competition ended, we </context>
<context position="8830" citStr="Wilson et al., 2005" startWordPosition="1417" endWordPosition="1420"> the appropriate format (there is no CRF provided with NLTK at this time). We had also planned to try classifiers from the scikit-learn toolkit (Pedregosa et al., 2011), but again met with time constraints due to the necessity to manually convert the features to a binary representation. We first preprocess the data using NLTK’s tokenization and part-of-speech tagging modules and align the results with the aspect terms in the data, 791 as detailed further below. The sentiment lexicon we use as the basis of all sentiment features discussed below combines two standard lexicons (Liu et al., 2005; Wilson et al., 2005). 3.1 Aspect Term Extraction While it is difficult to give a precise definition of aspect, it can be roughly thought of as a characteristic of a target concept such as a restaurant or laptop. Examples include the italicized terms in the following: • I liked the service and the staff, but not the food. • The hard disk is very noisy. We use a sequence labeling approach, which can also be thought of as a tagging or chunking approach, to identify the aspect terms in each sentence. Specifically, and similar to Breck et al. (2007) and Mitchell et al. (2013), as the target class for each token, we us</context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phraselevel sentiment analysis. In Proceedings of the conference on Human Language Technology and Empirical Methods in Natural Language Processing, pages 347–354.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuanbin Wu</author>
<author>Qi Zhang</author>
<author>Xuanjing Huang</author>
<author>Lide Wu</author>
</authors>
<title>Phrase dependency parsing for opinion mining.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>1533--1541</pages>
<contexts>
<context position="5271" citStr="Wu et al., 2009" startWordPosition="835" endWordPosition="838"> Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007; Jin et al., 2009; Johansson and Moschitti, 2013; Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2013) and semantic dependency or semantic parsing approaches (Kim and Hovy, 2006; Kobayashi et al., 2007; Wu et al., 2009) sometimes using background knowledge from sentiment lexicons (Zhang et al., 2009). The main differences between our approach and that of Breck et al. (2007) and Mitchell et al. (2013) are the classifier used and some of the features; they both use CRFs versus our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A second related corpus which includes more aspect information is that developed by Kim and Hovy (2006). This corpus also focuses on news articles rather than reviews, but does expand the types of aspects identified. The main focus of </context>
</contexts>
<marker>Wu, Zhang, Huang, Wu, 2009</marker>
<rawString>Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu. 2009. Phrase dependency parsing for opinion mining. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1533–1541.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bishan Yang</author>
<author>Claire Cardie</author>
</authors>
<title>Joint inference for fine-grained opinion extraction.</title>
<date>2013</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>16550--1649</pages>
<contexts>
<context position="5154" citStr="Yang and Cardie, 2013" startWordPosition="815" endWordPosition="818">on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007; Jin et al., 2009; Johansson and Moschitti, 2013; Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2013) and semantic dependency or semantic parsing approaches (Kim and Hovy, 2006; Kobayashi et al., 2007; Wu et al., 2009) sometimes using background knowledge from sentiment lexicons (Zhang et al., 2009). The main differences between our approach and that of Breck et al. (2007) and Mitchell et al. (2013) are the classifier used and some of the features; they both use CRFs versus our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A second related corpus which includes more aspect information is that developed by Kim and Hovy (2006). This corpus a</context>
</contexts>
<marker>Yang, Cardie, 2013</marker>
<rawString>Bishan Yang and Claire Cardie. 2013. Joint inference for fine-grained opinion extraction. In Proceedings ofACL, pages 16550–1649.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhongwu Zhai</author>
<author>Bing Liu</author>
<author>Hua Xu</author>
<author>Peifa Jia</author>
</authors>
<title>Clustering product features for opinion mining.</title>
<date>2011</date>
<booktitle>In Proceedings of the fourth ACM international conference on Web search and data mining,</booktitle>
<pages>347--354</pages>
<publisher>ACM.</publisher>
<location>New York, New York, USA.</location>
<contexts>
<context position="4140" citStr="Zhai et al., 2011" startWordPosition="657" endWordPosition="660">timent classification for aspect terms. We note that aspects have also been called topics and features in prior work. Until more recently, the community lacked a corpus 790 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 790–795, Dublin, Ireland, August 23-24, 2014. of gold-standard labeled data that focuses on aspect terms, rather than more general expressions of subjectivity or other private states. Thus, early work focused on learning or identifying aspects in an unsupervised (Hu and Liu, 2004) or semisupervised setting (Moghaddam and Ester, 2010; Zhai et al., 2011). The earliest work on aspect detection focused on identifying frequently occurring noun phrases using information extraction (IE) techniques (Hu and Liu, 2004). Unsupervised techniques include clustering (Fahrni and Klenner, 2008; Popescu and Etzioni, 2005) and topic models (Titov and McDonald, 2008). The benchmark corpus for sentiment analysis from Wiebe et al. (2005) inspired much work on learning subjective phrases in a supervised setting. The nature of the data and annotation differ from the data for this Semeval task, as it focuses on news articles and identifying an entire opinion phras</context>
</contexts>
<marker>Zhai, Liu, Xu, Jia, 2011</marker>
<rawString>Zhongwu Zhai, Bing Liu, Hua Xu, and Peifa Jia. 2011. Clustering product features for opinion mining. In Proceedings of the fourth ACM international conference on Web search and data mining, pages 347– 354, New York, New York, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qi Zhang</author>
<author>Yuanbin Wu</author>
<author>Tao Li</author>
<author>Mitsunori Ogihara</author>
<author>Joseph Johnson</author>
<author>Xuanjing Huang</author>
</authors>
<title>Mining product reviews based on shallow dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>726--727</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5353" citStr="Zhang et al., 2009" startWordPosition="847" endWordPosition="850"> phrase, including the source of the opinion, and only recently added aspect annotations. However, the techniques used by others to learn to extract this data and the associated polarity inspired our own approach. These include extraction-like approaches, usually using sequence modeling (Breck et al., 2007; Jin et al., 2009; Johansson and Moschitti, 2013; Li et al., 2010; Mitchell et al., 2013; Yang and Cardie, 2013) and semantic dependency or semantic parsing approaches (Kim and Hovy, 2006; Kobayashi et al., 2007; Wu et al., 2009) sometimes using background knowledge from sentiment lexicons (Zhang et al., 2009). The main differences between our approach and that of Breck et al. (2007) and Mitchell et al. (2013) are the classifier used and some of the features; they both use CRFs versus our Maximum entropy classifier, and they used a wider range of syntactic and dictionary-based features. A second related corpus which includes more aspect information is that developed by Kim and Hovy (2006). This corpus also focuses on news articles rather than reviews, but does expand the types of aspects identified. The main focus of that work is on the identification, using FrameNet role labels, of the holder and </context>
</contexts>
<marker>Zhang, Wu, Li, Ogihara, Johnson, Huang, 2009</marker>
<rawString>Qi Zhang, Yuanbin Wu, Tao Li, Mitsunori Ogihara, Joseph Johnson, and Xuanjing Huang. 2009. Mining product reviews based on shallow dependency parsing. In Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 726–727. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jingbo Zhu</author>
<author>Muhua Zhu</author>
<author>Huizhen Wang</author>
<author>Benjamin Tsou</author>
</authors>
<title>Aspect-based sentence segmentation for sentiment summarization.</title>
<date>2009</date>
<booktitle>In Proceedings of the 1st international CIKM workshop on Topicsentiment analysis for mass opinion,</booktitle>
<pages>65--72</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13510" citStr="Zhu et al., 2009" startWordPosition="2242" endWordPosition="2245">term extraction task is treated as a prerequisite to the polarity classification task. In addition to these word-based features, we add four higher-level features. The first is an indicator of the number of aspect terms in the entire sentence, since this might indicate a more de792 tailed sentence, and we believe that more specific sentences might correlate with positive sentiment. The other three features are baselines connected to the estimated sentiment of the sentence or phrase. First, we apply a hand-built sentence level sentiment classifier that follows a now standard baseline approach (Zhu et al., 2009): using a sentiment lexicon (Liu’s), it counts the number of positive and negative sentiment words in the sentence, flipping polarity when negation words are encountered, and discontinuing the polarity flip when punctuation is encountered. This results in a “high level sentiment” feature consisting of the number of positive sentiment words minus the number of negative sentiment words. Our other two sentiment features provide finer granularity information, based on the sentiment of the “chunks” in which an aspect term appears. First, we use the punctuation within the sentence to divide it into </context>
</contexts>
<marker>Zhu, Zhu, Wang, Tsou, 2009</marker>
<rawString>Jingbo Zhu, Muhua Zhu, Huizhen Wang, and Benjamin Tsou. 2009. Aspect-based sentence segmentation for sentiment summarization. In Proceedings of the 1st international CIKM workshop on Topicsentiment analysis for mass opinion, pages 65–72. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>