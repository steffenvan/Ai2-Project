<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001715">
<title confidence="0.8390195">
The Meaning Factory: Formal Semantics for Recognizing Textual
Entailment and Determining Semantic Similarity
</title>
<author confidence="0.986389">
Johannes Bjerva Johan Bos Rob van der Goot Malvina Nissim
</author>
<affiliation confidence="0.98941">
Univ. of Groningen Univ. of Groningen Univ. of Groningen Univ. of Bologna
</affiliation>
<email confidence="0.946616">
j.bjerva@rug.nl johan.bos@rug.nl r.van.der.goot@rug.nl malvina.nissim@unibo.it
</email>
<sectionHeader confidence="0.996673" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999459375">
Shared Task 1 of SemEval-2014 com-
prised two subtasks on the same dataset
of sentence pairs: recognizing textual en-
tailment and determining textual similar-
ity. We used an existing system based on
formal semantics and logical inference to
participate in the first subtask, reaching
an accuracy of 82%, ranking in the top
5 of more than twenty participating sys-
tems. For determining semantic similar-
ity we took a supervised approach using a
variety of features, the majority of which
was produced by our system for recogniz-
ing textual entailment. In this subtask our
system achieved a mean squared error of
0.322, the best of all participating systems.
</bodyText>
<sectionHeader confidence="0.999394" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999890238095238">
The recent popularity of employing distributional
approaches to semantic interpretation has also lead
to interesting questions about the relationship be-
tween classic formal semantics (including its com-
putational adaptations) and statistical semantics.
A promising way to provide insight into these
questions was brought forward as Shared Task 1 in
the SemEval-2014 campaign for semantic evalua-
tion (Marelli et al., 2014). In this task, a system is
given a set of sentence pairs, and has to predict for
each pair whether the sentences are somehow re-
lated in meaning. Interestingly, this is done using
two different metrics: the first stemming from the
formal tradition (contradiction, entailed, neutral),
and the second in a distributional fashion (a simi-
larity score between 1 and 5). We participated in
this shared task with a system rooted in formal se-
mantics. In particular, we were interested in find-
ing out whether paraphrasing techniques could in-
crease the accuracy of our system, whether mean-
ing representations used for textual entailment are
</bodyText>
<footnote confidence="0.9054245">
This work is licensed under a Creative Commons Attribution
4.0 International Licence. Page numbers and proceedings
footer are added by the organisers. Licence details: http:
//creativecommons.org/licenses/by/4.0/
</footnote>
<bodyText confidence="0.999213833333333">
useful for predicting semantic similarity, and con-
versely, whether similarity features could be used
to boost accuracy of recognizing textual entail-
ment. In this paper we outline our method and
present the results for both the textual entailment
and the semantic similarity task.1
</bodyText>
<sectionHeader confidence="0.98889" genericHeader="method">
2 Recognizing Textual Entailment
</sectionHeader>
<subsectionHeader confidence="0.645588">
2.1 Overview
</subsectionHeader>
<bodyText confidence="0.999938">
The core of our system for recognizing textual en-
tailment works as follows: (i) produce a formal se-
mantic representation for each sentence for a given
sentence pair; (ii) translate these semantic repre-
sentations into first-order logic; (iii) use off-the-
shelf theorem provers and model builders to check
whether the first sentence entails the second, or
whether the sentences are contradictory. This is
essentially an improved version of the framework
introduced by Bos &amp; Markert (2006).
To generate background knowledge that could
assist in finding a proof we used the lexical
database WordNet (Fellbaum, 1998). We also
used a large database of paraphrases (Ganitkevitch
et al., 2013) to alter the second sentence in case no
proof was found at the first attempt, inspired by
Bosma &amp; Callison-Burch (2006). The core sys-
tem reached high precision on entailment and con-
tradiction. To increase recall, we used a classifier
trained on the output from our similarity task sys-
tem (see Section 3) to reclassify the “neutrals” into
possible entailments.
</bodyText>
<subsectionHeader confidence="0.997043">
2.2 Technicalities
</subsectionHeader>
<bodyText confidence="0.97694025">
The semantic parser that we used is Boxer (Bos,
2008). It is the last component in the pipeline of
the C&amp;C tools (Curran et al., 2007), comprising
a tokenizer, POS-tagger, lemmatizer (Minnen et
</bodyText>
<footnote confidence="0.999829857142857">
1To reproduce these results in a linux environment (with
SWI Prolog) one needs to install the C&amp;C tools (this in-
cludes Boxer and the RTE system), the Vampire theorem
prover, the two model builders Paradox and Mace-2, and the
PPDB-1.0 XL database. Detailed instructions can be found in
the src/scripts/boxer/sick/README folder of the
C&amp;C tools.
</footnote>
<page confidence="0.904937">
642
</page>
<note confidence="0.7375495">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642–646,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.9999160625">
al., 2001), and a robust parser for CCG (Steed-
man, 2001). Boxer produces semantic represen-
tations based on Discourse Representation Theory
(Kamp and Reyle, 1993). We used the standard
translation from Discourse Representation Struc-
tures to first-order logic, rather than the one based
on modal first-order logic (Bos, 2004), since the
shared task data did not contain any sentences with
propositional argument verbs.
After conversion to first-order logic, we
checked with the theorem prover Vampire (Ri-
azanov and Voronkov, 2002) whether a proof
could be found for the first sentence entailing the
second, and whether a contradiction could be de-
tected for the conjunction of both sentences trans-
lated into first-order logic. If neither a proof nor
a contradiction could be found within 30 seconds,
we used the model builder Paradox (Claessen and
S¨orensson, 2003) to produce a model of the two
sentences separately, and one of the two sentences
together. However, even though Paradox is an ef-
ficient piece of software, it does not always return
minimal models with respect to the extensions of
the non-logical symbols. Therefore, in a second
step, we asked the model builder Mace-2 (Mc-
Cune, 1998) to construct a minimal model for the
domain size established by Paradox. These mod-
els are used as features in the similarity task (Sec-
tion 3).
Background knowledge is important to increase
recall of the theorem prover, but hard to acquire
automatically (Bos, 2013). Besides translating hy-
pernym relations of WordNet to first-order logic
axioms, we also reasoned that it would be benefi-
cial to have a way of dealing with multi-word ex-
pressions. But instead of translating paraphrases
into axioms, we used them to rephrase the input
sentence in case no proof or contradiction was
found for the original sentence pair. Given a para-
phrase SRCHTGT, we rephrased the first sen-
tence of a pair only if SRC matches with up to
four words, no words of TGT were already in the
first sentence, and every word of TGT appeared in
the second sentence. The paraphrases themselves
were taken from PPDB-1.0 (Ganitkevitch et al.,
2013). In the training phrase we found that the XL
version (comprising o2m, m2o, phrasal, lexical)
gave the best results (using a larger version caused
a strong decrease in precision, while smaller ver-
sions lead to a decrease in recall).
We trained a separate classifier in order to re-
classify items judged by our RTE system as be-
ing neutral. This classifier uses a single feature,
namely the relatedness score for each sentence
pair. As training material, we used the gold relat-
edness scores from the training and trial sets. For
classification of the test set, we used the related-
ness scores obtained from our Semantic Similarity
system (see Section 3). The classifier is a Support
Vector Machine classifier, in the implementation
provided by Scikit-Learn (Pedregosa et al., 2011),
based on the commonly used implementation LIB-
SVM (Chang and Lin, 2011). We used the imple-
mentation’s standard parameters.
</bodyText>
<subsectionHeader confidence="0.874199">
2.3 Results
</subsectionHeader>
<bodyText confidence="0.999793545454545">
We submitted two runs. The first (primary) run
was produced by a configuration that included re-
classifying the ’neutrals’. The second run is with-
out the reclassification of the neutrals. After sub-
mission we ran a system that did not use the para-
phrasing technique in order to measure what in-
fluence the PPDB had on our performance. The
results are summarized in Table 1. In the train-
ing phase we got the best results for the configu-
ration using the PPDB and reclassication, which
was submitted as our primary run.
</bodyText>
<tableCaption confidence="0.92141">
Table 1: Results on the entailment task for various
system configurations.
</tableCaption>
<subsectionHeader confidence="0.957619">
System Configuration Accuracy
</subsectionHeader>
<bodyText confidence="0.9486245">
most frequent class baseline
−PPDB, −reclassification
+PPDB, −reclassification
+PPDB, +reclassification
In sum, our system for recognizing entailment
performed well reaching 82% accuracy and by
far outperforming the most-frequent class baseline
(Table 1). We show some selected examples illus-
trating the strengths of our system below.
Example 1627 (ENTAILMENT)
A man is mixing a few ingredients in a bowl
Some ingredients are being mixed in a bowl by a person
Example 2709 (CONTRADICTION)
There is no person boiling noodles
A woman is boiling noodles in water
Example 9051 (ENTAILMENT)
A pair of kids are sticking out blue and green colored tongues
Two kids are sticking out blue and green colored tongues
A proof for entailment is found for Ex. 1627,
because for passive sentences Boxer produces
a meaning representation equivalent to their ac-
tive variants. A contradiction is detected for
Ex. 2709 because of the way negation is han-
dled by Boxer. Both examples trigger background
knowledge from WordNet hyperonyms (man —*
person; woman —* person) that is used in the
</bodyText>
<figure confidence="0.877269">
56.7
77.6
79.6
81.6
</figure>
<page confidence="0.995127">
643
</page>
<bodyText confidence="0.989685">
proofs.2 Ex. 9051 shows how paraphrasing helps,
here “a pair of” H “two”.
</bodyText>
<sectionHeader confidence="0.975286" genericHeader="method">
3 Determining Semantic Similarity
</sectionHeader>
<subsectionHeader confidence="0.965458">
3.1 Overview
</subsectionHeader>
<bodyText confidence="0.9999268">
The Semantic Similarity system follows a super-
vised approach to solving the regression problem
of determining the similarity between each given
sentence pair. The system uses a variety of fea-
tures, ranging from simpler ones such as word
overlap, to more complex ones in the form of
deep semantic features and features derived from a
compositional distributional semantic model. The
majority of these features are derived from the
models from our RTE system (see Section 2).
</bodyText>
<sectionHeader confidence="0.5628745" genericHeader="method">
3.2 Technicalities
3.2.1 Regressor
</sectionHeader>
<bodyText confidence="0.999994789473684">
The regressor used is a Random Forest Regressor
in the implementation provided by Scikit-Learn
(Pedregosa et al., 2011). Random forests are ro-
bust with respect to noise and do not overfit easily
(Breiman, 2001). These two factors make them a
highly suitable choice for our approach, since we
are dealing with a relatively large number of weak
features, i.e., features which may be seen as indi-
vidually containing a rather small amount of infor-
mation for the problem at hand.
Our parameter settings for the regressor is fol-
lows. We used a total of 1000 trees, with a maxi-
mum tree depth of 20. At each node in a tree the
regressor looked at maximum 3 features in order
to decide on the split. The quality of each such
split is determined using mean squared error as
measure. These parameter values were optimised
when training on the training set, with regards to
performance on the trial set.
</bodyText>
<subsectionHeader confidence="0.897326">
3.2.2 Feature overview
</subsectionHeader>
<bodyText confidence="0.999996444444445">
We used a total of 32 features for our regres-
sor. Due to space constraints, we have sub-divided
our features into groups by the model/method in-
volved. For all features we compared the outcome
of the original sentence pair with the outcome of
the paraphrased sentence pairs (see Section 2.2)3.
If the paraphrased sentence pair yielded a higher
feature overlap score than the original sentence
pair, we utilized the former. In other words, we
</bodyText>
<footnote confidence="0.680217333333333">
2In the training data around 20% of the proofs for entail-
ment were established with the help of WordNet, but only 4%
for detecting contradictions.
3In addition to the PPDB we added handling of negations,
by removing some negations {not, n’t} and substituting oth-
ers {no:a, none:some, nobody:somebody}.
</footnote>
<bodyText confidence="0.986663363636363">
assume that the sentence pair generated with para-
phrases is a good representation of the original
pair, and that similarities found here are an im-
provement on the original score.
Logical model We used the logical models cre-
ated by Paradox and Mace for the two sentences
separately, as well as a combined model (see Sec-
tion 2.2). The features extracted from this model
are the proportion of overlap between the in-
stances in the domain, and the proportion of over-
lap between the relations in the model.
Noun/verb overlap We first extracted and lem-
matised all nouns and verbs from the sentence
pairs. With these lemmas we calculated two new
separate features, the overlap of the noun lemmas
and the overlap of the verb lemmas.
Discourse Representation Structure (DRS)
The two most interesting pieces of information
which easily can be extracted from the DRS mod-
els are the agents and patients. We first extracted
the agents for both sentences in a sentence pair,
and then computed the overlap between the two
lists of agents. Secondly, since all sentences in the
corpus have exactly one patient, we extracted the
patient of each sentence and used this overlap as a
binary feature.
Wordnet novelty We build one tree containing
all WordNet concepts included in the first sen-
tence, and one containing all WordNet concepts
of both sentences together. The difference in size
between these two trees is used as a feature.
RTE The result from our RTE system (entail-
ment, neutral or contradiction) is used as a feature.
</bodyText>
<sectionHeader confidence="0.443372" genericHeader="method">
Compositional Distributional Semantic Model
</sectionHeader>
<bodyText confidence="0.999937071428571">
Our CDSM feature is based on word vectors de-
rived using a Skip-Gram model (Mikolov et al.,
2013a; Mikolov et al., 2013b). We used the pub-
licly available word2vec4 tool to calculate these
vectors. We trained the tool on a data set con-
sisting of the first billion characters of Wikipedia5
and the English part of the French-English 109
corpus used in the wmt11 translation task6. The
Wikipedia section of the data was pre-processed
using a script7 which made the text lower case, re-
moved tables etc. The second section of the data
was also converted to lower case prior to training.
We trained the vectors using the following pa-
rameter settings. Vector dimensionality was set
</bodyText>
<footnote confidence="0.99907775">
4code.google.com/p/word2vec/
5mattmahoney.net/dc/enwik9.zip
6statmt.org/wmt11/translation-task.html#download
7mattmahoney.net/dc/textdata.html
</footnote>
<page confidence="0.998006">
644
</page>
<tableCaption confidence="0.999534">
Table 2: Pearson correlation and MSE obtained on the test set for each feature group in isolation.
</tableCaption>
<table confidence="0.999090769230769">
Feature group p [−PPDB] p [+PPDB] MSE [−PPDB] MSE [+PPDB]
Logical model 0.649 0.737 0.590 0.476
Noun/verb overlap 0.647 0.676 0.592 0.553
DRS 0.634 0.667 0.610 0.569
Wordnet novelty 0.652 0.651 0.590 0.591
RTE 0.621 0.620 0.626 0.627
CDSM 0.608 0.609 0.681 0.679
IDs 0.493 0.493 0.807 0.807
Synset 0.414 0.417 0.891 0.889
Word overlap 0.271 0.340 0.944 0.902
Sentence length 0.227 0.228 0.971 0.971
All with IDs 0.836 0.842 0.308 0.297
All without IDs 0.819 0.827 0.336 0.322
</table>
<bodyText confidence="0.999344193548387">
to 1600 with a context window of 10 words. The
skip-gram model with hierarchical softmax, and a
negative sampling of 1e-3 was used.
To arrive at the feature used for our regressor,
we first calculated the element-wise sum of the
vectors of each word in the given sentences. We
then calculated the cosine distance between the
sentences in the sentence pair.
IDs One surprisingly helpful feature was each
sentence pair’s ID in the corpus.8 Since this
feature clearly is not representative of what one
would have access to in a real-world scenario, it
was not included in the primary run.
Synset Overlap We built one set for each sen-
tence pair consisting of each possible lemma form
of all possible noun synsets for each word. The
proportion of overlap between the two resulting
sets was then used as a feature. Given cases where
relatively synonymous words are used (e.g. kid
and child), these will often belong to the same
synset, thus resulting in a high overlap score.
Synset Distance We first generated each possi-
ble word pair consisting of one word from each
sentence. Using these pairings, we calculated
the maximum path similarity between the noun
synsets available for these words. This calculation
is restricted so that each word in the first sentence
in each pair is only used once.
Word overlap Our word overlap feature was
calculated by first creating one set per sentence,
containing each word occurring in that sentence.
</bodyText>
<footnote confidence="0.683011285714286">
8We discovered that the ordering of the entire data set was
informative for the prediction of sentence relatedness. We
have illustrated this by using the ordering of the sentences
(i.e. the sentence IDs) as a feature in our model, and thereby
obtaining better results. Relying on such a non-natural order-
ing of the sentences would be methodologically flawed, and
therefore this feature was not used in our primary run.
</footnote>
<bodyText confidence="0.999767444444444">
The four most common words in the corpus were
used as a stop list, and removed from each set. The
proportion of overlap between the two sets was
then used as our word overlap feature.
Sentence Lengths The difference in length be-
tween the sentence pairs proved to be a somewhat
useful feature. Although mildly useful for this par-
ticular data set, we do not expect this to be a par-
ticularly helpful feature in real world applications.
</bodyText>
<subsectionHeader confidence="0.600143">
3.3 Results
</subsectionHeader>
<bodyText confidence="0.9999962">
We trained our system on 5000 sentence pairs, and
evaluated it on 4927 sentence pairs. Table 2 con-
tains our scores for the evaluation, broken up per
feature group. Our relatedness system yielded the
highest scores compared to all other systems in
this shared task, as measured by MSE and Spear-
man correlation scores. Although our system per-
formed slightly worse as measured by Pearson
correlation, there is no significant difference to the
scores obtained by the two higher ranked systems.
</bodyText>
<sectionHeader confidence="0.999492" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999987142857143">
Our work shows that paraphrasing techniques can
be used to improve the results of a textual entail-
ment system. Additionally, the scores from our
semantic similarity measure could be used to im-
prove the scores of the textual entailment system.
Our work also shows that deep semantic features
can be used to predict semantic relatedness.
</bodyText>
<sectionHeader confidence="0.997882" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.82186425">
We thank Chris Callison-Burch, Juri Ganitkevitch and Ellie
Pavlick for getting the most out of PPDB. We also thank our
colleagues Valerio Basile, Harm Brouwer, Kilian Evang and
Noortje Venhuizen for valuable comments and feedback.
</bodyText>
<page confidence="0.998672">
645
</page>
<sectionHeader confidence="0.996291" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999803840909091">
Johan Bos and Katja Markert. 2006. Recognising
textual entailment with robust logical inference. In
Joaquin Quinonero-Candela, Ido Dagan, Bernardo
Magnini, and Florence d’Alch´e Buc, editors, Ma-
chine Learning Challenges, MLCW 2005, volume
3944 of LNAI, pages 404–426.
Johan Bos. 2004. Computational Semantics in Dis-
course: Underspecification, Resolution, and Infer-
ence. Journal of Logic, Language and Information,
13(2):139–157.
Johan Bos. 2008. Wide-Coverage Semantic Analy-
sis with Boxer. In J. Bos and R. Delmonte, editors,
Semantics in Text Processing. STEP 2008 Confer-
ence Proceedings, volume 1 of Research in Compu-
tational Semantics, pages 277–286. College Publi-
cations.
Johan Bos. 2013. Is there a place for logic in rec-
ognizing textual entailment? Linguistic Issues in
Language Technology, 9(3):1–18.
Wauter Bosma and Chris Callison-Burch. 2006. Para-
phrase substitution for recognizing textual entail-
ment. In Proceedings of CLEF.
Leo Breiman. 2001. Random forests. Machine learn-
ing, 45(1):5–32.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: a library for support vector machines. ACM
Transactions on Intelligent Systems and Technology
(TIST), 2(3):27.
K. Claessen and N. S¨orensson. 2003. New techniques
that improve mace-style model finding. In P. Baum-
gartner and C. Fermiller, editors, Model Computa-
tion – Principles, Algorithms, Applications (Cade-
19 Workshop), pages 11–27, Miami, Florida, USA.
James Curran, Stephen Clark, and Johan Bos. 2007.
Linguistically Motivated Large-Scale NLP with
C&amp;C and Boxer. In Proceedings of the 45th An-
nual Meeting of the Association for Computational
Linguistics Companion Volume Proceedings of the
Demo and Poster Sessions, pages 33–36, Prague,
Czech Republic.
Christiane Fellbaum, editor. 1998. WordNet. An Elec-
tronic Lexical Database. The MIT Press.
Juri Ganitkevitch, Benjamin VanDurme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In Proceedings of the 2013 Conference of
the North American Chapter of the Association for
Computational Linguistics (NAACL 2013), Atlanta,
Georgia, June. Association for Computational Lin-
guistics.
Hans Kamp and Uwe Reyle. 1993. From Discourse
to Logic; An Introduction to Modeltheoretic Seman-
tics of Natural Language, Formal Logic and DRT.
Kluwer, Dordrecht.
M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi,
S. Menini, and R. Zamparelli. 2014. Semeval-2014
task 1: Evaluation of compositional distributional
semantic models on full sentences through seman-
tic relatedness and textual entailment. In Proceed-
ings of SemEval 2014: International Workshop on
Semantic Evaluation.
W. McCune. 1998. Automatic Proofs and Counterex-
amples for Some Ortholattice Identities. Informa-
tion Processing Letters, 65(6):285–291.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-
frey Dean. 2013a. Efficient estimation of word
representations in vector space. arXiv preprint
arXiv:1301.3781.
Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-
rado, and Jeff Dean. 2013b. Distributed representa-
tions of words and phrases and their compositional-
ity. In Advances in Neural Information Processing
Systems, pages 3111–3119.
Guido Minnen, John Carroll, and Darren Pearce. 2001.
Applied morphological processing of english. Jour-
nal of Natural Language Engineering, 7(3):207–
223.
F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,
B. Thirion, O. Grisel, M. Blondel, P. Pretten-
hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-
sos, D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. 2011. Scikit-learn: Machine learn-
ing in Python. Journal of Machine Learning Re-
search, 12:2825–2830.
A. Riazanov and A. Voronkov. 2002. The Design and
Implementation of Vampire. AI Communications,
15(2–3):91–110.
Mark Steedman. 2001. The Syntactic Process. The
MIT Press.
</reference>
<page confidence="0.998863">
646
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.965201">
<title confidence="0.9965555">The Meaning Factory: Formal Semantics for Recognizing Entailment and Determining Semantic Similarity</title>
<author confidence="0.996974">Johannes Bjerva Johan Bos Rob van_der_Goot Malvina Nissim</author>
<affiliation confidence="0.99741">Univ. of Groningen Univ. of Groningen Univ. of Groningen Univ. of Bologna</affiliation>
<email confidence="0.978197">j.bjerva@rug.nljohan.bos@rug.nlr.van.der.goot@rug.nlmalvina.nissim@unibo.it</email>
<abstract confidence="0.999847235294118">Shared Task 1 of SemEval-2014 comprised two subtasks on the same dataset of sentence pairs: recognizing textual entailment and determining textual similarity. We used an existing system based on formal semantics and logical inference to participate in the first subtask, reaching an accuracy of 82%, ranking in the top 5 of more than twenty participating systems. For determining semantic similarity we took a supervised approach using a variety of features, the majority of which was produced by our system for recognizing textual entailment. In this subtask our system achieved a mean squared error of 0.322, the best of all participating systems.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Johan Bos</author>
<author>Katja Markert</author>
</authors>
<title>Recognising textual entailment with robust logical inference.</title>
<date>2006</date>
<booktitle>Machine Learning Challenges, MLCW 2005,</booktitle>
<volume>3944</volume>
<pages>404--426</pages>
<editor>In Joaquin Quinonero-Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch´e Buc, editors,</editor>
<contexts>
<context position="3070" citStr="Bos &amp; Markert (2006)" startWordPosition="460" endWordPosition="463">d present the results for both the textual entailment and the semantic similarity task.1 2 Recognizing Textual Entailment 2.1 Overview The core of our system for recognizing textual entailment works as follows: (i) produce a formal semantic representation for each sentence for a given sentence pair; (ii) translate these semantic representations into first-order logic; (iii) use off-theshelf theorem provers and model builders to check whether the first sentence entails the second, or whether the sentences are contradictory. This is essentially an improved version of the framework introduced by Bos &amp; Markert (2006). To generate background knowledge that could assist in finding a proof we used the lexical database WordNet (Fellbaum, 1998). We also used a large database of paraphrases (Ganitkevitch et al., 2013) to alter the second sentence in case no proof was found at the first attempt, inspired by Bosma &amp; Callison-Burch (2006). The core system reached high precision on entailment and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. 2.2 Technicalities The semantic parser t</context>
</contexts>
<marker>Bos, Markert, 2006</marker>
<rawString>Johan Bos and Katja Markert. 2006. Recognising textual entailment with robust logical inference. In Joaquin Quinonero-Candela, Ido Dagan, Bernardo Magnini, and Florence d’Alch´e Buc, editors, Machine Learning Challenges, MLCW 2005, volume 3944 of LNAI, pages 404–426.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Computational Semantics in Discourse: Underspecification, Resolution, and Inference.</title>
<date>2004</date>
<journal>Journal of Logic, Language and Information,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="4651" citStr="Bos, 2004" startWordPosition="711" endWordPosition="712">aradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&amp;C tools. 642 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642–646, Dublin, Ireland, August 23-24, 2014. al., 2001), and a robust parser for CCG (Steedman, 2001). Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993). We used the standard translation from Discourse Representation Structures to first-order logic, rather than the one based on modal first-order logic (Bos, 2004), since the shared task data did not contain any sentences with propositional argument verbs. After conversion to first-order logic, we checked with the theorem prover Vampire (Riazanov and Voronkov, 2002) whether a proof could be found for the first sentence entailing the second, and whether a contradiction could be detected for the conjunction of both sentences translated into first-order logic. If neither a proof nor a contradiction could be found within 30 seconds, we used the model builder Paradox (Claessen and S¨orensson, 2003) to produce a model of the two sentences separately, and one </context>
</contexts>
<marker>Bos, 2004</marker>
<rawString>Johan Bos. 2004. Computational Semantics in Discourse: Underspecification, Resolution, and Inference. Journal of Logic, Language and Information, 13(2):139–157.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Wide-Coverage Semantic Analysis with Boxer. In</title>
<date>2008</date>
<booktitle>Semantics in Text Processing. STEP 2008 Conference Proceedings,</booktitle>
<volume>1</volume>
<pages>277--286</pages>
<editor>J. Bos and R. Delmonte, editors,</editor>
<publisher>College Publications.</publisher>
<contexts>
<context position="3702" citStr="Bos, 2008" startWordPosition="565" endWordPosition="566">nd knowledge that could assist in finding a proof we used the lexical database WordNet (Fellbaum, 1998). We also used a large database of paraphrases (Ganitkevitch et al., 2013) to alter the second sentence in case no proof was found at the first attempt, inspired by Bosma &amp; Callison-Burch (2006). The core system reached high precision on entailment and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&amp;C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&amp;C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&amp;C tools. 642 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642–646, Dublin, Ire</context>
</contexts>
<marker>Bos, 2008</marker>
<rawString>Johan Bos. 2008. Wide-Coverage Semantic Analysis with Boxer. In J. Bos and R. Delmonte, editors, Semantics in Text Processing. STEP 2008 Conference Proceedings, volume 1 of Research in Computational Semantics, pages 277–286. College Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johan Bos</author>
</authors>
<title>Is there a place for logic in recognizing textual entailment?</title>
<date>2013</date>
<journal>Linguistic Issues in Language Technology,</journal>
<volume>9</volume>
<issue>3</issue>
<contexts>
<context position="5788" citStr="Bos, 2013" startWordPosition="898" endWordPosition="899">nsson, 2003) to produce a model of the two sentences separately, and one of the two sentences together. However, even though Paradox is an efficient piece of software, it does not always return minimal models with respect to the extensions of the non-logical symbols. Therefore, in a second step, we asked the model builder Mace-2 (McCune, 1998) to construct a minimal model for the domain size established by Paradox. These models are used as features in the similarity task (Section 3). Background knowledge is important to increase recall of the theorem prover, but hard to acquire automatically (Bos, 2013). Besides translating hypernym relations of WordNet to first-order logic axioms, we also reasoned that it would be beneficial to have a way of dealing with multi-word expressions. But instead of translating paraphrases into axioms, we used them to rephrase the input sentence in case no proof or contradiction was found for the original sentence pair. Given a paraphrase SRCHTGT, we rephrased the first sentence of a pair only if SRC matches with up to four words, no words of TGT were already in the first sentence, and every word of TGT appeared in the second sentence. The paraphrases themselves w</context>
</contexts>
<marker>Bos, 2013</marker>
<rawString>Johan Bos. 2013. Is there a place for logic in recognizing textual entailment? Linguistic Issues in Language Technology, 9(3):1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wauter Bosma</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrase substitution for recognizing textual entailment.</title>
<date>2006</date>
<booktitle>In Proceedings of CLEF.</booktitle>
<contexts>
<context position="3389" citStr="Bosma &amp; Callison-Burch (2006)" startWordPosition="512" endWordPosition="515">slate these semantic representations into first-order logic; (iii) use off-theshelf theorem provers and model builders to check whether the first sentence entails the second, or whether the sentences are contradictory. This is essentially an improved version of the framework introduced by Bos &amp; Markert (2006). To generate background knowledge that could assist in finding a proof we used the lexical database WordNet (Fellbaum, 1998). We also used a large database of paraphrases (Ganitkevitch et al., 2013) to alter the second sentence in case no proof was found at the first attempt, inspired by Bosma &amp; Callison-Burch (2006). The core system reached high precision on entailment and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&amp;C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&amp;C tools (this includes Boxer and the RTE system), </context>
</contexts>
<marker>Bosma, Callison-Burch, 2006</marker>
<rawString>Wauter Bosma and Chris Callison-Burch. 2006. Paraphrase substitution for recognizing textual entailment. In Proceedings of CLEF.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Leo Breiman</author>
</authors>
<title>Random forests.</title>
<date>2001</date>
<booktitle>Machine learning,</booktitle>
<pages>45--1</pages>
<contexts>
<context position="9904" citStr="Breiman, 2001" startWordPosition="1572" endWordPosition="1573">ning the similarity between each given sentence pair. The system uses a variety of features, ranging from simpler ones such as word overlap, to more complex ones in the form of deep semantic features and features derived from a compositional distributional semantic model. The majority of these features are derived from the models from our RTE system (see Section 2). 3.2 Technicalities 3.2.1 Regressor The regressor used is a Random Forest Regressor in the implementation provided by Scikit-Learn (Pedregosa et al., 2011). Random forests are robust with respect to noise and do not overfit easily (Breiman, 2001). These two factors make them a highly suitable choice for our approach, since we are dealing with a relatively large number of weak features, i.e., features which may be seen as individually containing a rather small amount of information for the problem at hand. Our parameter settings for the regressor is follows. We used a total of 1000 trees, with a maximum tree depth of 20. At each node in a tree the regressor looked at maximum 3 features in order to decide on the split. The quality of each such split is determined using mean squared error as measure. These parameter values were optimised</context>
</contexts>
<marker>Breiman, 2001</marker>
<rawString>Leo Breiman. 2001. Random forests. Machine learning, 45(1):5–32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: a library for support vector machines.</title>
<date>2011</date>
<booktitle>ACM Transactions on Intelligent Systems and Technology (TIST),</booktitle>
<pages>2--3</pages>
<contexts>
<context position="7288" citStr="Chang and Lin, 2011" startWordPosition="1148" endWordPosition="1151">l). We trained a separate classifier in order to reclassify items judged by our RTE system as being neutral. This classifier uses a single feature, namely the relatedness score for each sentence pair. As training material, we used the gold relatedness scores from the training and trial sets. For classification of the test set, we used the relatedness scores obtained from our Semantic Similarity system (see Section 3). The classifier is a Support Vector Machine classifier, in the implementation provided by Scikit-Learn (Pedregosa et al., 2011), based on the commonly used implementation LIBSVM (Chang and Lin, 2011). We used the implementation’s standard parameters. 2.3 Results We submitted two runs. The first (primary) run was produced by a configuration that included reclassifying the ’neutrals’. The second run is without the reclassification of the neutrals. After submission we ran a system that did not use the paraphrasing technique in order to measure what influence the PPDB had on our performance. The results are summarized in Table 1. In the training phase we got the best results for the configuration using the PPDB and reclassication, which was submitted as our primary run. Table 1: Results on th</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: a library for support vector machines. ACM Transactions on Intelligent Systems and Technology (TIST), 2(3):27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Claessen</author>
<author>N S¨orensson</author>
</authors>
<title>New techniques that improve mace-style model finding.</title>
<date>2003</date>
<booktitle>Model Computation – Principles, Algorithms, Applications (Cade19 Workshop),</booktitle>
<pages>11--27</pages>
<editor>In P. Baumgartner and C. Fermiller, editors,</editor>
<location>Miami, Florida, USA.</location>
<marker>Claessen, S¨orensson, 2003</marker>
<rawString>K. Claessen and N. S¨orensson. 2003. New techniques that improve mace-style model finding. In P. Baumgartner and C. Fermiller, editors, Model Computation – Principles, Algorithms, Applications (Cade19 Workshop), pages 11–27, Miami, Florida, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James Curran</author>
<author>Stephen Clark</author>
<author>Johan Bos</author>
</authors>
<title>Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer.</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,</booktitle>
<pages>33--36</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="3783" citStr="Curran et al., 2007" startWordPosition="579" endWordPosition="582">atabase WordNet (Fellbaum, 1998). We also used a large database of paraphrases (Ganitkevitch et al., 2013) to alter the second sentence in case no proof was found at the first attempt, inspired by Bosma &amp; Callison-Burch (2006). The core system reached high precision on entailment and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&amp;C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&amp;C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&amp;C tools. 642 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642–646, Dublin, Ireland, August 23-24, 2014. al., 2001), and a robust parser for CCG (Steedman, 2001</context>
</contexts>
<marker>Curran, Clark, Bos, 2007</marker>
<rawString>James Curran, Stephen Clark, and Johan Bos. 2007. Linguistically Motivated Large-Scale NLP with C&amp;C and Boxer. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 33–36, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<title>WordNet. An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. WordNet. An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin VanDurme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013),</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia,</location>
<contexts>
<context position="3269" citStr="Ganitkevitch et al., 2013" startWordPosition="491" endWordPosition="494">works as follows: (i) produce a formal semantic representation for each sentence for a given sentence pair; (ii) translate these semantic representations into first-order logic; (iii) use off-theshelf theorem provers and model builders to check whether the first sentence entails the second, or whether the sentences are contradictory. This is essentially an improved version of the framework introduced by Bos &amp; Markert (2006). To generate background knowledge that could assist in finding a proof we used the lexical database WordNet (Fellbaum, 1998). We also used a large database of paraphrases (Ganitkevitch et al., 2013) to alter the second sentence in case no proof was found at the first attempt, inspired by Bosma &amp; Callison-Burch (2006). The core system reached high precision on entailment and contradiction. To increase recall, we used a classifier trained on the output from our similarity task system (see Section 3) to reclassify the “neutrals” into possible entailments. 2.2 Technicalities The semantic parser that we used is Boxer (Bos, 2008). It is the last component in the pipeline of the C&amp;C tools (Curran et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1To reproduce these result</context>
<context position="6439" citStr="Ganitkevitch et al., 2013" startWordPosition="1008" endWordPosition="1011">ernym relations of WordNet to first-order logic axioms, we also reasoned that it would be beneficial to have a way of dealing with multi-word expressions. But instead of translating paraphrases into axioms, we used them to rephrase the input sentence in case no proof or contradiction was found for the original sentence pair. Given a paraphrase SRCHTGT, we rephrased the first sentence of a pair only if SRC matches with up to four words, no words of TGT were already in the first sentence, and every word of TGT appeared in the second sentence. The paraphrases themselves were taken from PPDB-1.0 (Ganitkevitch et al., 2013). In the training phrase we found that the XL version (comprising o2m, m2o, phrasal, lexical) gave the best results (using a larger version caused a strong decrease in precision, while smaller versions lead to a decrease in recall). We trained a separate classifier in order to reclassify items judged by our RTE system as being neutral. This classifier uses a single feature, namely the relatedness score for each sentence pair. As training material, we used the gold relatedness scores from the training and trial sets. For classification of the test set, we used the relatedness scores obtained fr</context>
</contexts>
<marker>Ganitkevitch, VanDurme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin VanDurme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL 2013), Atlanta, Georgia, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hans Kamp</author>
<author>Uwe Reyle</author>
</authors>
<title>From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT.</title>
<date>1993</date>
<publisher>Kluwer,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="4489" citStr="Kamp and Reyle, 1993" startWordPosition="685" endWordPosition="688">ts in a linux environment (with SWI Prolog) one needs to install the C&amp;C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&amp;C tools. 642 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642–646, Dublin, Ireland, August 23-24, 2014. al., 2001), and a robust parser for CCG (Steedman, 2001). Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993). We used the standard translation from Discourse Representation Structures to first-order logic, rather than the one based on modal first-order logic (Bos, 2004), since the shared task data did not contain any sentences with propositional argument verbs. After conversion to first-order logic, we checked with the theorem prover Vampire (Riazanov and Voronkov, 2002) whether a proof could be found for the first sentence entailing the second, and whether a contradiction could be detected for the conjunction of both sentences translated into first-order logic. If neither a proof nor a contradictio</context>
</contexts>
<marker>Kamp, Reyle, 1993</marker>
<rawString>Hans Kamp and Uwe Reyle. 1993. From Discourse to Logic; An Introduction to Modeltheoretic Semantics of Natural Language, Formal Logic and DRT. Kluwer, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marelli</author>
<author>L Bentivogli</author>
<author>M Baroni</author>
<author>R Bernardi</author>
<author>S Menini</author>
<author>R Zamparelli</author>
</authors>
<title>Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment.</title>
<date>2014</date>
<booktitle>In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</booktitle>
<contexts>
<context position="1414" citStr="Marelli et al., 2014" startWordPosition="205" endWordPosition="208">ajority of which was produced by our system for recognizing textual entailment. In this subtask our system achieved a mean squared error of 0.322, the best of all participating systems. 1 Introduction The recent popularity of employing distributional approaches to semantic interpretation has also lead to interesting questions about the relationship between classic formal semantics (including its computational adaptations) and statistical semantics. A promising way to provide insight into these questions was brought forward as Shared Task 1 in the SemEval-2014 campaign for semantic evaluation (Marelli et al., 2014). In this task, a system is given a set of sentence pairs, and has to predict for each pair whether the sentences are somehow related in meaning. Interestingly, this is done using two different metrics: the first stemming from the formal tradition (contradiction, entailed, neutral), and the second in a distributional fashion (a similarity score between 1 and 5). We participated in this shared task with a system rooted in formal semantics. In particular, we were interested in finding out whether paraphrasing techniques could increase the accuracy of our system, whether meaning representations u</context>
</contexts>
<marker>Marelli, Bentivogli, Baroni, Bernardi, Menini, Zamparelli, 2014</marker>
<rawString>M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi, S. Menini, and R. Zamparelli. 2014. Semeval-2014 task 1: Evaluation of compositional distributional semantic models on full sentences through semantic relatedness and textual entailment. In Proceedings of SemEval 2014: International Workshop on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W McCune</author>
</authors>
<title>Automatic Proofs and Counterexamples for Some Ortholattice Identities.</title>
<date>1998</date>
<journal>Information Processing Letters,</journal>
<volume>65</volume>
<issue>6</issue>
<contexts>
<context position="5523" citStr="McCune, 1998" startWordPosition="853" endWordPosition="855"> entailing the second, and whether a contradiction could be detected for the conjunction of both sentences translated into first-order logic. If neither a proof nor a contradiction could be found within 30 seconds, we used the model builder Paradox (Claessen and S¨orensson, 2003) to produce a model of the two sentences separately, and one of the two sentences together. However, even though Paradox is an efficient piece of software, it does not always return minimal models with respect to the extensions of the non-logical symbols. Therefore, in a second step, we asked the model builder Mace-2 (McCune, 1998) to construct a minimal model for the domain size established by Paradox. These models are used as features in the similarity task (Section 3). Background knowledge is important to increase recall of the theorem prover, but hard to acquire automatically (Bos, 2013). Besides translating hypernym relations of WordNet to first-order logic axioms, we also reasoned that it would be beneficial to have a way of dealing with multi-word expressions. But instead of translating paraphrases into axioms, we used them to rephrase the input sentence in case no proof or contradiction was found for the origina</context>
</contexts>
<marker>McCune, 1998</marker>
<rawString>W. McCune. 1998. Automatic Proofs and Counterexamples for Some Ortholattice Identities. Information Processing Letters, 65(6):285–291.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</title>
<date>2013</date>
<contexts>
<context position="13000" citStr="Mikolov et al., 2013" startWordPosition="2097" endWordPosition="2100">dly, since all sentences in the corpus have exactly one patient, we extracted the patient of each sentence and used this overlap as a binary feature. Wordnet novelty We build one tree containing all WordNet concepts included in the first sentence, and one containing all WordNet concepts of both sentences together. The difference in size between these two trees is used as a feature. RTE The result from our RTE system (entailment, neutral or contradiction) is used as a feature. Compositional Distributional Semantic Model Our CDSM feature is based on word vectors derived using a Skip-Gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). We used the publicly available word2vec4 tool to calculate these vectors. We trained the tool on a data set consisting of the first billion characters of Wikipedia5 and the English part of the French-English 109 corpus used in the wmt11 translation task6. The Wikipedia section of the data was pre-processed using a script7 which made the text lower case, removed tables etc. The second section of the data was also converted to lower case prior to training. We trained the vectors using the following parameter settings. Vector dimensionality was set 4code.google.com/p/wo</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Ilya Sutskever</author>
<author>Kai Chen</author>
<author>Greg S Corrado</author>
<author>Jeff Dean</author>
</authors>
<title>Distributed representations of words and phrases and their compositionality.</title>
<date>2013</date>
<booktitle>In Advances in Neural Information Processing Systems,</booktitle>
<pages>3111--3119</pages>
<contexts>
<context position="13000" citStr="Mikolov et al., 2013" startWordPosition="2097" endWordPosition="2100">dly, since all sentences in the corpus have exactly one patient, we extracted the patient of each sentence and used this overlap as a binary feature. Wordnet novelty We build one tree containing all WordNet concepts included in the first sentence, and one containing all WordNet concepts of both sentences together. The difference in size between these two trees is used as a feature. RTE The result from our RTE system (entailment, neutral or contradiction) is used as a feature. Compositional Distributional Semantic Model Our CDSM feature is based on word vectors derived using a Skip-Gram model (Mikolov et al., 2013a; Mikolov et al., 2013b). We used the publicly available word2vec4 tool to calculate these vectors. We trained the tool on a data set consisting of the first billion characters of Wikipedia5 and the English part of the French-English 109 corpus used in the wmt11 translation task6. The Wikipedia section of the data was pre-processed using a script7 which made the text lower case, removed tables etc. The second section of the data was also converted to lower case prior to training. We trained the vectors using the following parameter settings. Vector dimensionality was set 4code.google.com/p/wo</context>
</contexts>
<marker>Mikolov, Sutskever, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013b. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems, pages 3111–3119.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Guido Minnen</author>
<author>John Carroll</author>
<author>Darren Pearce</author>
</authors>
<title>Applied morphological processing of english.</title>
<date>2001</date>
<journal>Journal of Natural Language Engineering,</journal>
<volume>7</volume>
<issue>3</issue>
<pages>223</pages>
<marker>Minnen, Carroll, Pearce, 2001</marker>
<rawString>Guido Minnen, John Carroll, and Darren Pearce. 2001. Applied morphological processing of english. Journal of Natural Language Engineering, 7(3):207– 223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Pedregosa</author>
<author>G Varoquaux</author>
<author>A Gramfort</author>
<author>V Michel</author>
<author>B Thirion</author>
<author>O Grisel</author>
<author>M Blondel</author>
<author>P Prettenhofer</author>
<author>R Weiss</author>
<author>V Dubourg</author>
<author>J Vanderplas</author>
<author>A Passos</author>
<author>D Cournapeau</author>
<author>M Brucher</author>
<author>M Perrot</author>
<author>E Duchesnay</author>
</authors>
<title>Scikit-learn: Machine learning in Python.</title>
<date>2011</date>
<journal>Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<contexts>
<context position="7216" citStr="Pedregosa et al., 2011" startWordPosition="1136" endWordPosition="1139">g decrease in precision, while smaller versions lead to a decrease in recall). We trained a separate classifier in order to reclassify items judged by our RTE system as being neutral. This classifier uses a single feature, namely the relatedness score for each sentence pair. As training material, we used the gold relatedness scores from the training and trial sets. For classification of the test set, we used the relatedness scores obtained from our Semantic Similarity system (see Section 3). The classifier is a Support Vector Machine classifier, in the implementation provided by Scikit-Learn (Pedregosa et al., 2011), based on the commonly used implementation LIBSVM (Chang and Lin, 2011). We used the implementation’s standard parameters. 2.3 Results We submitted two runs. The first (primary) run was produced by a configuration that included reclassifying the ’neutrals’. The second run is without the reclassification of the neutrals. After submission we ran a system that did not use the paraphrasing technique in order to measure what influence the PPDB had on our performance. The results are summarized in Table 1. In the training phase we got the best results for the configuration using the PPDB and reclas</context>
<context position="9813" citStr="Pedregosa et al., 2011" startWordPosition="1554" endWordPosition="1557">emantic Similarity system follows a supervised approach to solving the regression problem of determining the similarity between each given sentence pair. The system uses a variety of features, ranging from simpler ones such as word overlap, to more complex ones in the form of deep semantic features and features derived from a compositional distributional semantic model. The majority of these features are derived from the models from our RTE system (see Section 2). 3.2 Technicalities 3.2.1 Regressor The regressor used is a Random Forest Regressor in the implementation provided by Scikit-Learn (Pedregosa et al., 2011). Random forests are robust with respect to noise and do not overfit easily (Breiman, 2001). These two factors make them a highly suitable choice for our approach, since we are dealing with a relatively large number of weak features, i.e., features which may be seen as individually containing a rather small amount of information for the problem at hand. Our parameter settings for the regressor is follows. We used a total of 1000 trees, with a maximum tree depth of 20. At each node in a tree the regressor looked at maximum 3 features in order to decide on the split. The quality of each such spl</context>
</contexts>
<marker>Pedregosa, Varoquaux, Gramfort, Michel, Thirion, Grisel, Blondel, Prettenhofer, Weiss, Dubourg, Vanderplas, Passos, Cournapeau, Brucher, Perrot, Duchesnay, 2011</marker>
<rawString>F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay. 2011. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Riazanov</author>
<author>A Voronkov</author>
</authors>
<date>2002</date>
<booktitle>The Design and Implementation of Vampire. AI Communications,</booktitle>
<pages>15--2</pages>
<contexts>
<context position="4856" citStr="Riazanov and Voronkov, 2002" startWordPosition="739" endWordPosition="743">Workshop on Semantic Evaluation (SemEval 2014), pages 642–646, Dublin, Ireland, August 23-24, 2014. al., 2001), and a robust parser for CCG (Steedman, 2001). Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993). We used the standard translation from Discourse Representation Structures to first-order logic, rather than the one based on modal first-order logic (Bos, 2004), since the shared task data did not contain any sentences with propositional argument verbs. After conversion to first-order logic, we checked with the theorem prover Vampire (Riazanov and Voronkov, 2002) whether a proof could be found for the first sentence entailing the second, and whether a contradiction could be detected for the conjunction of both sentences translated into first-order logic. If neither a proof nor a contradiction could be found within 30 seconds, we used the model builder Paradox (Claessen and S¨orensson, 2003) to produce a model of the two sentences separately, and one of the two sentences together. However, even though Paradox is an efficient piece of software, it does not always return minimal models with respect to the extensions of the non-logical symbols. Therefore,</context>
</contexts>
<marker>Riazanov, Voronkov, 2002</marker>
<rawString>A. Riazanov and A. Voronkov. 2002. The Design and Implementation of Vampire. AI Communications, 15(2–3):91–110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Steedman</author>
</authors>
<title>The Syntactic Process.</title>
<date>2001</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="4384" citStr="Steedman, 2001" startWordPosition="672" endWordPosition="674"> et al., 2007), comprising a tokenizer, POS-tagger, lemmatizer (Minnen et 1To reproduce these results in a linux environment (with SWI Prolog) one needs to install the C&amp;C tools (this includes Boxer and the RTE system), the Vampire theorem prover, the two model builders Paradox and Mace-2, and the PPDB-1.0 XL database. Detailed instructions can be found in the src/scripts/boxer/sick/README folder of the C&amp;C tools. 642 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 642–646, Dublin, Ireland, August 23-24, 2014. al., 2001), and a robust parser for CCG (Steedman, 2001). Boxer produces semantic representations based on Discourse Representation Theory (Kamp and Reyle, 1993). We used the standard translation from Discourse Representation Structures to first-order logic, rather than the one based on modal first-order logic (Bos, 2004), since the shared task data did not contain any sentences with propositional argument verbs. After conversion to first-order logic, we checked with the theorem prover Vampire (Riazanov and Voronkov, 2002) whether a proof could be found for the first sentence entailing the second, and whether a contradiction could be detected for t</context>
</contexts>
<marker>Steedman, 2001</marker>
<rawString>Mark Steedman. 2001. The Syntactic Process. The MIT Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>