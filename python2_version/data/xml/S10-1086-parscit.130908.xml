<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000063">
<title confidence="0.822192">
MSS: Investigating the Effectiveness of Domain Combinations and
Topic Features for Word Sense Disambiguation
</title>
<author confidence="0.702612">
Sanae Fujita Kevin Duh Akinori Fujino Hirotoshi Taira Hiroyuki Shindo
</author>
<affiliation confidence="0.530238">
NTT Communication Science Laboratories
</affiliation>
<email confidence="0.781401">
{sanae, kevinduh, taira, a.fujino, shindo}@cslab.kecl.ntt.co.jp
</email>
<sectionHeader confidence="0.996254" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9990593">
We participated in the SemEval-2010
Japanese Word Sense Disambiguation
(WSD) task (Task 16) and focused on
the following: (1) investigating domain
differences, (2) incorporating topic fea-
tures, and (3) predicting new unknown
senses. We experimented with Support
Vector Machines (SVM) and Maximum
Entropy (MEM) classifiers. We achieved
80.1% accuracy in our experiments.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99934484">
We participated in the SemEval-2010 Japanese
Word Sense Disambiguation (WSD) task (Task 16
(Okumura et al., 2010)), which has two new char-
acteristics: (1) Both training and test data across
3 or 4 domains. The training data include books
or magazines (called PB), newspaper articles (PN),
and white papers (OW). The test data also include
documents from a Q&amp;A site on the WWW (OC);
(2) Test data include new senses (called X) that are
not defined in dictionary.
There is much previous research on WSD. In
the case of Japanese, unsupervised approaches
such as extended Lesk have performed well (Bald-
win et al., 2010), although they are outperformed
by supervised approaches (Tanaka et al., 2007;
Murata et al., 2003). Therefore, we selected a su-
pervised approach and constructed Support Vector
Machines (SVM) and Maximum Entropy (MEM)
classifiers using common features and topic fea-
tures. We performed extensive experiments to in-
vestigate the best combinations of domains for
training.
We describe the data in Section 2, and our sys-
tem in Section 3. Then in Section 4, we show the
results and provide some discussion.
</bodyText>
<sectionHeader confidence="0.990359" genericHeader="method">
2 Data Description
</sectionHeader>
<subsectionHeader confidence="0.961088">
2.1 Given Data
</subsectionHeader>
<bodyText confidence="0.999714777777778">
We show an example of Iwanami Kokugo Jiten
(Nishio et al., 1994), which is a dictionary used as
a sense inventory. As shown in Figure 1, each en-
try has POS information and definition sentences
including example sentences.
We show an example of the given training data
in (1). The given data are morphologically ana-
lyzed and partly tagged with Iwanami’s sense IDs,
such as&amp;quot;37713-0-0-1-1&amp;quot;in (1).
</bodyText>
<listItem confidence="0.5395735">
(1) &lt;mor pos=&apos;��-- rd=&apos;Ÿš(bfm=&apos;Ÿ
Â(sense=&apos;37713-0-0-1-1(&gt;R&lt;&lt;/mor&gt;
</listItem>
<bodyText confidence="0.999907875">
This task includes 50 target words that were
split into 219 senses in Iwanami; among them, 143
senses including two Xs that were not defined in
Iwanami, appear in the training data. In the test
data, 150 senses including eight Xs appear. The
training and test data share 135 senses including
two Xs; that is, 15 senses including six Xs in the
test data are unseen in the training data.
</bodyText>
<subsectionHeader confidence="0.995499">
2.2 Data Pre-processing
</subsectionHeader>
<bodyText confidence="0.999860666666667">
We performed two preliminary pre-processing
steps. First, we restored the base forms because
the given training and test data have no informa-
tion about the base forms. (1) shows an example
of the original morphological data, and then we
added the base form (lemma), as shown in (2).
</bodyText>
<listItem confidence="0.780006333333333">
(2) &lt;mor pos=&apos;}j�-- rd=&apos;Ÿš(
bfm=&apos;ŸÂ(sense=&apos;37713-0-0-1-1(
lemma=&apos;�kd(&gt;�k&lt;&lt;/mor&gt;
</listItem>
<bodyText confidence="0.998045">
Secondly, we extracted example sentences from
Iwanami, which is used as a sense inventory. To
compensate for the lack of training data, we an-
alyzed examples with a morphological analyzer,
Mecab1 UniDic version, because the training and
test data were tagged with POS based on UniDic.
</bodyText>
<footnote confidence="0.993712">
1http://mecab.sourceforge.net/
</footnote>
<page confidence="0.981788">
383
</page>
<bodyText confidence="0.4969975">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 383–386,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</bodyText>
<equation confidence="0.983389777777778">
� �
HEADWORD Ad91dAIZd-*Ad-11d:take (EftTransitive Verb)
� �
37713-0-0-1-0 [&lt;1&gt;��? 8[GCBk3D#*=oto get something left into one’s hand �
� �
� �
�&lt;y&gt;3�Ǒ��=�53k-&lt;?�(6
� �
37713-0-0-1-1
</equation>
<bodyText confidence="0.591282">
take and hold by hand.&apos;to lead someone by the hand(�
</bodyText>
<figureCaption confidence="0.998219">
Figure 1: Simplified Entry for Iwanami Kokugo Jiten:L7Dtake
</figureCaption>
<bodyText confidence="0.997728666666667">
For example, from the entry forL7Dtake, as
shown in Figure 1, we extracted an example sen-
tence and morphologically analyzed it, as shown
in (3)2, for the second sense, 37713-0-0-1-1. In
(3), the underlined part is the headword and is
tagged with 37713-0-0-1-1.
</bodyText>
<equation confidence="0.772915">
(3)3
hand
</equation>
<bodyText confidence="0.582479">
“(I) take someone’s hand and lead him/her”
</bodyText>
<sectionHeader confidence="0.985129" genericHeader="method">
3 System Description
</sectionHeader>
<subsectionHeader confidence="0.882473">
3.1 Features
</subsectionHeader>
<bodyText confidence="0.998024">
In this section, we describe the features we gener-
ated.
</bodyText>
<subsectionHeader confidence="0.79198">
3.1.1 Baseline Features
</subsectionHeader>
<bodyText confidence="0.999966111111111">
For each target word w, we used the surface form,
the base form, the POS tag, and the top POS cat-
egories, such as nouns, verbs, and adjectives of
w. Here the target is the ith word, so we also
used the same information of i − 2, i − 1, i + 1, and
i+2th words. We used bigrams, trigrams, and skip-
bigrams back and forth within three words. We re-
fer to the model that uses these baseline features
as bl.
</bodyText>
<subsectionHeader confidence="0.930619">
3.1.2 Bag-of-Words
</subsectionHeader>
<bodyText confidence="0.99995">
For each target word w, we got all base forms of
the content words within the same document or
within the same article for newspapers (PN). We
refer to the model that uses these baseline features
as bow.
</bodyText>
<subsectionHeader confidence="0.788141">
3.1.3 Topic Features
</subsectionHeader>
<bodyText confidence="0.983693138888889">
In the SemEval-2007 English WSD tasks, a sys-
tem incorporating topic features achieved the
highest accuracy (Cai et al., 2007). Inspired by
(Cai et al., 2007), we also used topic features.
Their approach uses Bayesian topic models (La-
tent Dirichlet Allocation: LDA) to infer topics in
an unsupervised fashion. Then the inferred topics
2We use ACC as an abbreviation of accusative
postposition.
are added as features to reduce the sparsity prob-
lem with word-only features.
In our proposed approach, we use the inferred
topics to find&amp;quot;related’&apos;words and directly add
these word counts to the bag-of-words representa-
tion.
We applied gibbslda++3 to the training and test
data to obtain multiple topic classification per doc-
ument or article for newspapers (PN). We used the
document or article topics for newspapers (PN) in-
cluding the target word. We refer to the model
that uses these topic features as tpX, where X is
the number of topics and tpdistX with the topics
weighted by distributions. In particular, the topic
distribution of each document/article is inferred by
the LDA topic model using standard Gibbs sam-
pling.
We also add the most typical words in the topic
as a bag-of-words. For example, one topic might
includer*i city, A)j: Tokyo, �V train line, C ward
and so on. A second topic might include ǑAll dis-
section, after, [�? medicine, 9 grave and so
on. If a document is inferred to contain the first
topic, then the words (r4 city, A)j: Tokyo, �V train
line, ...) are added to the bag-of-words feature. We
refer to these features as twdY, including the most
typical Y words as bag-of-words.
</bodyText>
<subsectionHeader confidence="0.999298">
3.2 Investigation between Domains
</subsectionHeader>
<bodyText confidence="0.999076416666667">
In preliminary experiments, we used both SVM4
and MEM (Nigam et al., 1999), with optimization
method L-BFGS (Liu and Nocedal, 1989) to train
the WSD model.
First, we investigated the effect between do-
mains (PN, PB, and OW). For training data, we se-
lected words that occur in more than 50 sentences,
separated the training data by domain, and tested
different domain combinations.
Table 1 shows the SVM results of the domain
combinations. For Table 1, we did a 5-fold cross
validation for the self domain and for comparison
</bodyText>
<footnote confidence="0.989315">
3http://gibbslda.sourceforge.net/
4http://www.csie.ntu.edu.tw/˜cjlin/
libsvm/
</footnote>
<table confidence="0.8800375">
k 1&lt; ? �(
ACC take and lead
</table>
<page confidence="0.931639">
384
</page>
<tableCaption confidence="0.982791">
Table 1: Investigation of Domain Combinations
on Training data (features: bl + bow, SVM)
</tableCaption>
<table confidence="0.979086785714286">
Target Words 77, No. of Instances &gt; 50
Domain Acc.(%) Diff. Comment
PN 78.7 - 63 words,
PN +OW 79.25 0.55 1094 instances
PN +PB 79.43 0.73
PN +ALL 79.34 0.64
PB 79.29 - 75 words,
PB +PN 78.85 -0.45 2463 instances
PB +OW 78.56 -0.73
PB +ALL 78.4 -0.89
OW 87.91 - 42 words,
OW +PN 89.05 1.14 703 instances
OW +PB 88.34 0.43
OW +ALL 89.05 1.14
</table>
<bodyText confidence="0.9999215">
with the results after adding the other domain data.
In Table 1, Diff. shows the differences to the self
domain.
As shown in Table 1, for PN and OW, using other
domains improved the results, but for PB, other do-
mains degraded the results. So we decided to se-
lect the domains for each target word.
In the formal run, for each pair of domain and
target words, we selected the combination of do-
main and dictionary examples that got the best
cross-validation result in the training data. Note
that in the case of no training data for the test data
domain, for example, since no OCs have training
data, we used all training data and dictionary ex-
amples.
We show the number of selected domain combi-
nations for each target domain in Table 2. Because
the distribution of target words is very unbalanced
in domains, not all types of target words appear in
every domain, as shown in Table 2.
</bodyText>
<subsectionHeader confidence="0.996988">
3.3 Method for Predicting New Senses
</subsectionHeader>
<bodyText confidence="0.943420785714286">
We also tried to predict new senses (X) that didn’t
appear in the training data by calculating the en-
tropy for each target given in the MEM. We as-
sumed that high entropy (when the probabilities
of classes are uniformly dispersed) was indicative
of X; i.e., if [entropy &gt; threshold] =&gt; predict X;
else =&gt; predict with MEM’s output sense tag.
Note that we used the words that were tagged
with Xs in the training data, except for the target
words. We compared the entropies of X and not
X of the words and heuristically tuned the thresh-
old based on the differences among entropies. Our
three official submissions correspond to different
thresholds.
</bodyText>
<tableCaption confidence="0.976976">
Table 2: Used Domain Combinations
</tableCaption>
<table confidence="0.998740958333333">
Used MEM SVM
Domain No. (%) No. (%)
Target: PB (48 types of target words)
ALL +EX 26 54.2 23 47.9
ALL 4 8.3 6 12.5
PB 11 22.9 8 16.7
PB+EX 1 2.1 1 2.1
PB +OW 1 2.1 3 6.3
PB +PN 5 10.4 7 14.6
Target: PN (46 types of target words)
ALL +EX 30 65.2 30 65.2
ALL 4 8.7 4 8.7
PN 4 8.7 1 2.2
PN +EX 0 0 1 2.2
PN +OW 2 4.3 2 4.3
PN +PB 6 13 8 17.4
Target: OW (16 types of target words)
ALL +EX 5 31.3 5 31.3
ALL 2 12.5 1 6.3
OW 6 37.5 3 18.8
OW +PB 3 18.8 3 18.8
OW +PN 0 0 4 25.0
Target: OC (46 types of target words)
ALL +EX 46 100 46 100
</table>
<sectionHeader confidence="0.999369" genericHeader="evaluation">
4 Results and Discussions
</sectionHeader>
<bodyText confidence="0.999675206896552">
Our cross-validation experiments on the training
set showed that selecting data by domain combi-
nations works well, but unfortunately this failed
to achieve optimal results on the formal run. In
this section, we show the results using all of the
training data with no domain selections (also after
fixing some bugs).
Table 3 shows the results for the combination
of features on the test data. MEM greatly outper-
formed SVM. Its effective features are also quite
different. In the case of MEM, baseline features
(bl) almost gave the best result, and the topic fea-
tures improved the accuracy, especially when di-
vided into 200 topics. But for SVM, the topic
features are not so effective, and the bag-of-words
features improved accuracy.
For MEM with bl +tp200, which produced the
best result, the following are the best words:9F
outside (accuracy is 100%),ff&amp;economy (98%),
t z,6think (98%), J�,- � o big (98%), and �Z4L
culture (98%). On the other hand, the following
are the worst words: V,6 take (36%), Q o good
(48%), 1 d-} ,6 raise (48%), put out (50%),
and 1 -D stand up (54%).
In Table 4, we show the results for each POS (bl
+tp200, MEM). The results for the verbs are com-
parably lower than the others. In future work, we
will consider adding syntactic features that may
improve the results.
</bodyText>
<page confidence="0.999292">
385
</page>
<tableCaption confidence="0.999812">
Table 3: Comparisons among Features and Test data
</tableCaption>
<table confidence="0.974124384615385">
TYPE Precision (%) Explain
MEM SVM
Base Line 68.96 68.96 Most Frequent Sense
bl 79.3 69.6 Base Line Features
bl +bow 77.0 70.8 + Bag-of-Words (BOW)
bl +bow +tp100 76.4 70.7 +BOW + Topics (100)
bl +bow +tp200 77.0 70.7 +BOW + Topics (200)
bl +bow +tp300 77.4 70.7 +BOW + Topics (300)
bl +bow +tp400 76.8 70.7 +BOW + Topics (400)
bl +bow +tpdist300 77.0 70.8 +BOW + Topics (300)*distribution
bl +bow +tp300 +twd100 76.2 70.8 + Topics (300) with 100 topic words
bl +bow +tp300 +twd200 76.0 70.8 + Topics (300) with 200 topic words
bl +bow +tp300 +twd300 75.9 70.8 + Topics (300) with 300 topic words
</table>
<figure confidence="0.990725">
without bow
69.6
69.6
69.6
69.6
69.6
69.6
69.6
69.6
69.4
69.3
69.2
69.6
69.6
69.6
69.6
69.6
69.6
79.3
80.1
79.6
79.6
79.3
79.3
79.3
74.6
74.4
75.2
74.8
74.6
75.0
74.1
79.3
79.3
79.3
</figure>
<table confidence="0.981459">
+ Topics (100)
+ Topics (200)
+ Topics (300)
+ Topics (400)
+ Topics (100)*distribution
+ Topics (200)*distribution
+ Topics (300)*distribution
+ Topics (200) with 100 topic words
+ Topics (300) with 10 topic words
+ Topics (300) with 20 topic words
+ Topics (300) with 50 topic words
+ Topics (300) with 200 topic words
+ Topics (300) with 300 topic words
+ Topics (400) with 100 topic words
+ Topics (100)*distribution with 20 topic words
+ Topics (200)*distribution with 20 topic words
+ Topics (400)*distribution with 20 topic words
</table>
<figure confidence="0.992262294117647">
bl +tp100
bl +tp200
bl +tp300
bl +tp400
bl +tpdist100
bl +tpdist200
bl +tpdist300
bl +tp200 +twd100
bl +tp300 +twd10
bl +tp300 +twd20
bl +tp300 +twd50
bl +tp300 +twd200
bl +tp300 +twd300
bl +tp400 +twd100
bl+tpdist100 +twd20
bl+tpdist200 +twd20
bl+tpdist400 +twd20
</figure>
<tableCaption confidence="0.950707">
Table 4: Results for each POS (bl +tp200, MEM)
</tableCaption>
<note confidence="0.606768">
POS No. of Types Acc. (%)
</note>
<reference confidence="0.428833666666667">
Jun Fu Cai, Wee Sun Lee, and YW Teh. 2007. Improv-
ing Word Sense Disambiguation using Topic Features. In
Proceedings ofEMNLP-CoNLL-2007, pp. 1015–1023.
</reference>
<figure confidence="0.958772933333333">
22
5
15
8
50
Nouns
Adjectives
Transitive Verbs
Intransitive Verbs
Total
Dong C. Liu and Jorge Nocedal. 1989. On the Limited Mem-
ory BFGS Method for Large Scale Optimization. Math.
76.9 Programming, 45(3, (Ser. B)):503–528.
85.5
79.2
</figure>
<bodyText confidence="0.999832777777778">
In the formal run, we selected training data
for each pair of domain and target words and
used entropy to predict new unknown senses. Al-
though these two methods worked well in our
cross-validation experiments, they did not perform
well for the test data, probably due to domain mis-
match.
Finally, we also experimented with SVM and
MEM, and MEM gave better results.
</bodyText>
<sectionHeader confidence="0.999644" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999775">
Timothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fu-
jita, David Martinez, and Takaaki Tanaka. 2010. A Re-
examination of MRD-based Word Sense Disambiguation.
Transactions on Asian Language Information Process, As-
sociation for Computing Machinery (ACM), 9(4):1–21.
Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing
Ma, and Hitoshi Isahara. 2003. CRL at Japanese
dictionary-based task of SENSEVAL-2. Journal of Nat-
ural Language Processing, 10(3):115–143. (in Japanese).
Kamal Nigam, John Lafferty, and Andrew McCallum. 1999.
Using Maximum Entropy for Text Classification. In
IJCAI-99 Workshop on Machine Learningfor Information
Filtering, pp. 61–67.
Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.
1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami
Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo.
(in Japanese).
Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and
Hikaru Yokono. 2010. SemEval-2010 Task: Japanese
WSD. In SemEval-2: Evaluation Exercises on Semantic
Evaluation.
Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae Fu-
jita, and Chikara Hashimoto. 2007. Word Sense Disam-
biguation Incorporating Lexical and Structural Semantic
Information. In Proceedings ofEMNLP-CoNLL-2007, pp.
477–485.
</reference>
<page confidence="0.673679666666667">
71.8
80.1
386
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.470293">
<title confidence="0.9989485">MSS: Investigating the Effectiveness of Domain Combinations and Topic Features for Word Sense Disambiguation</title>
<author confidence="0.996443">Sanae Fujita Kevin Duh Akinori Fujino Hirotoshi Taira Hiroyuki Shindo</author>
<affiliation confidence="0.997726">NTT Communication Science Laboratories</affiliation>
<email confidence="0.647725">kevinduh,taira,a.fujino,</email>
<abstract confidence="0.889050727272727">We participated in the SemEval-2010 Japanese Word Sense Disambiguation (WSD) task (Task 16) and focused on the following: (1) investigating domain differences, (2) incorporating topic features, and (3) predicting new unknown senses. We experimented with Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers. We achieved 80.1% accuracy in our experiments.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jun Fu Cai</author>
<author>Wee Sun Lee</author>
<author>YW Teh</author>
</authors>
<title>Improving Word Sense Disambiguation using Topic Features.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL-2007,</booktitle>
<pages>1015--1023</pages>
<contexts>
<context position="5050" citStr="Cai et al., 2007" startWordPosition="803" endWordPosition="806"> the target is the ith word, so we also used the same information of i − 2, i − 1, i + 1, and i+2th words. We used bigrams, trigrams, and skipbigrams back and forth within three words. We refer to the model that uses these baseline features as bl. 3.1.2 Bag-of-Words For each target word w, we got all base forms of the content words within the same document or within the same article for newspapers (PN). We refer to the model that uses these baseline features as bow. 3.1.3 Topic Features In the SemEval-2007 English WSD tasks, a system incorporating topic features achieved the highest accuracy (Cai et al., 2007). Inspired by (Cai et al., 2007), we also used topic features. Their approach uses Bayesian topic models (Latent Dirichlet Allocation: LDA) to infer topics in an unsupervised fashion. Then the inferred topics 2We use ACC as an abbreviation of accusative postposition. are added as features to reduce the sparsity problem with word-only features. In our proposed approach, we use the inferred topics to find&amp;quot;related’&apos;words and directly add these word counts to the bag-of-words representation. We applied gibbslda++3 to the training and test data to obtain multiple topic classification per document o</context>
</contexts>
<marker>Cai, Lee, Teh, 2007</marker>
<rawString>Jun Fu Cai, Wee Sun Lee, and YW Teh. 2007. Improving Word Sense Disambiguation using Topic Features. In Proceedings ofEMNLP-CoNLL-2007, pp. 1015–1023.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Timothy Baldwin</author>
<author>Su Nam Kim</author>
<author>Francis Bond</author>
<author>Sanae Fujita</author>
<author>David Martinez</author>
<author>Takaaki Tanaka</author>
</authors>
<title>A Reexamination of MRD-based Word Sense Disambiguation.</title>
<date>2010</date>
<journal>Transactions on Asian Language Information Process, Association for Computing Machinery (ACM),</journal>
<volume>9</volume>
<issue>4</issue>
<contexts>
<context position="1291" citStr="Baldwin et al., 2010" startWordPosition="187" endWordPosition="191">ction We participated in the SemEval-2010 Japanese Word Sense Disambiguation (WSD) task (Task 16 (Okumura et al., 2010)), which has two new characteristics: (1) Both training and test data across 3 or 4 domains. The training data include books or magazines (called PB), newspaper articles (PN), and white papers (OW). The test data also include documents from a Q&amp;A site on the WWW (OC); (2) Test data include new senses (called X) that are not defined in dictionary. There is much previous research on WSD. In the case of Japanese, unsupervised approaches such as extended Lesk have performed well (Baldwin et al., 2010), although they are outperformed by supervised approaches (Tanaka et al., 2007; Murata et al., 2003). Therefore, we selected a supervised approach and constructed Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers using common features and topic features. We performed extensive experiments to investigate the best combinations of domains for training. We describe the data in Section 2, and our system in Section 3. Then in Section 4, we show the results and provide some discussion. 2 Data Description 2.1 Given Data We show an example of Iwanami Kokugo Jiten (Nishio et al., 1994)</context>
</contexts>
<marker>Baldwin, Kim, Bond, Fujita, Martinez, Tanaka, 2010</marker>
<rawString>Timothy Baldwin, Su Nam Kim, Francis Bond, Sanae Fujita, David Martinez, and Takaaki Tanaka. 2010. A Reexamination of MRD-based Word Sense Disambiguation. Transactions on Asian Language Information Process, Association for Computing Machinery (ACM), 9(4):1–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Masaaki Murata</author>
<author>Masao Utiyama</author>
<author>Kiyotaka Uchimoto</author>
<author>Qing Ma</author>
<author>Hitoshi Isahara</author>
</authors>
<title>CRL at Japanese dictionary-based task of SENSEVAL-2.</title>
<date>2003</date>
<journal>Journal of Natural Language Processing,</journal>
<volume>10</volume>
<issue>3</issue>
<note>(in Japanese).</note>
<contexts>
<context position="1391" citStr="Murata et al., 2003" startWordPosition="203" endWordPosition="206">mura et al., 2010)), which has two new characteristics: (1) Both training and test data across 3 or 4 domains. The training data include books or magazines (called PB), newspaper articles (PN), and white papers (OW). The test data also include documents from a Q&amp;A site on the WWW (OC); (2) Test data include new senses (called X) that are not defined in dictionary. There is much previous research on WSD. In the case of Japanese, unsupervised approaches such as extended Lesk have performed well (Baldwin et al., 2010), although they are outperformed by supervised approaches (Tanaka et al., 2007; Murata et al., 2003). Therefore, we selected a supervised approach and constructed Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers using common features and topic features. We performed extensive experiments to investigate the best combinations of domains for training. We describe the data in Section 2, and our system in Section 3. Then in Section 4, we show the results and provide some discussion. 2 Data Description 2.1 Given Data We show an example of Iwanami Kokugo Jiten (Nishio et al., 1994), which is a dictionary used as a sense inventory. As shown in Figure 1, each entry has POS informat</context>
</contexts>
<marker>Murata, Utiyama, Uchimoto, Ma, Isahara, 2003</marker>
<rawString>Masaaki Murata, Masao Utiyama, Kiyotaka Uchimoto, Qing Ma, and Hitoshi Isahara. 2003. CRL at Japanese dictionary-based task of SENSEVAL-2. Journal of Natural Language Processing, 10(3):115–143. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kamal Nigam</author>
<author>John Lafferty</author>
<author>Andrew McCallum</author>
</authors>
<title>Using Maximum Entropy for Text Classification.</title>
<date>1999</date>
<booktitle>In IJCAI-99 Workshop on Machine Learningfor Information Filtering,</booktitle>
<pages>61--67</pages>
<contexts>
<context position="6634" citStr="Nigam et al., 1999" startWordPosition="1069" endWordPosition="1072">ing standard Gibbs sampling. We also add the most typical words in the topic as a bag-of-words. For example, one topic might includer*i city, A)j: Tokyo, �V train line, C ward and so on. A second topic might include ǑAll dissection, after, [�? medicine, 9 grave and so on. If a document is inferred to contain the first topic, then the words (r4 city, A)j: Tokyo, �V train line, ...) are added to the bag-of-words feature. We refer to these features as twdY, including the most typical Y words as bag-of-words. 3.2 Investigation between Domains In preliminary experiments, we used both SVM4 and MEM (Nigam et al., 1999), with optimization method L-BFGS (Liu and Nocedal, 1989) to train the WSD model. First, we investigated the effect between domains (PN, PB, and OW). For training data, we selected words that occur in more than 50 sentences, separated the training data by domain, and tested different domain combinations. Table 1 shows the SVM results of the domain combinations. For Table 1, we did a 5-fold cross validation for the self domain and for comparison 3http://gibbslda.sourceforge.net/ 4http://www.csie.ntu.edu.tw/˜cjlin/ libsvm/ k 1&lt; ? �( ACC take and lead 384 Table 1: Investigation of Domain Combinat</context>
</contexts>
<marker>Nigam, Lafferty, McCallum, 1999</marker>
<rawString>Kamal Nigam, John Lafferty, and Andrew McCallum. 1999. Using Maximum Entropy for Text Classification. In IJCAI-99 Workshop on Machine Learningfor Information Filtering, pp. 61–67.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minoru Nishio</author>
<author>Etsutaro Iwabuchi</author>
<author>Shizuo Mizutani</author>
</authors>
<date>1994</date>
<booktitle>Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dictionary Edition 5]. Iwanami Shoten,</booktitle>
<location>Tokyo.</location>
<note>(in Japanese).</note>
<contexts>
<context position="1891" citStr="Nishio et al., 1994" startWordPosition="286" endWordPosition="289">aldwin et al., 2010), although they are outperformed by supervised approaches (Tanaka et al., 2007; Murata et al., 2003). Therefore, we selected a supervised approach and constructed Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers using common features and topic features. We performed extensive experiments to investigate the best combinations of domains for training. We describe the data in Section 2, and our system in Section 3. Then in Section 4, we show the results and provide some discussion. 2 Data Description 2.1 Given Data We show an example of Iwanami Kokugo Jiten (Nishio et al., 1994), which is a dictionary used as a sense inventory. As shown in Figure 1, each entry has POS information and definition sentences including example sentences. We show an example of the given training data in (1). The given data are morphologically analyzed and partly tagged with Iwanami’s sense IDs, such as&amp;quot;37713-0-0-1-1&amp;quot;in (1). (1) &lt;mor pos=&apos;��-- rd=&apos;Ÿš(bfm=&apos;Ÿ Â(sense=&apos;37713-0-0-1-1(&gt;R&lt;&lt;/mor&gt; This task includes 50 target words that were split into 219 senses in Iwanami; among them, 143 senses including two Xs that were not defined in Iwanami, appear in the training data. In the test data, 150 </context>
</contexts>
<marker>Nishio, Iwabuchi, Mizutani, 1994</marker>
<rawString>Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani. 1994. Iwanami Kokugo Jiten Dai Go Han [Iwanami Japanese Dictionary Edition 5]. Iwanami Shoten, Tokyo. (in Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Manabu Okumura</author>
<author>Kiyoaki Shirai</author>
<author>Kanako Komiya</author>
<author>Hikaru Yokono</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task: Japanese WSD. In SemEval-2: Evaluation Exercises on Semantic Evaluation.</booktitle>
<contexts>
<context position="789" citStr="Okumura et al., 2010" startWordPosition="101" endWordPosition="104">uki Shindo NTT Communication Science Laboratories {sanae, kevinduh, taira, a.fujino, shindo}@cslab.kecl.ntt.co.jp Abstract We participated in the SemEval-2010 Japanese Word Sense Disambiguation (WSD) task (Task 16) and focused on the following: (1) investigating domain differences, (2) incorporating topic features, and (3) predicting new unknown senses. We experimented with Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers. We achieved 80.1% accuracy in our experiments. 1 Introduction We participated in the SemEval-2010 Japanese Word Sense Disambiguation (WSD) task (Task 16 (Okumura et al., 2010)), which has two new characteristics: (1) Both training and test data across 3 or 4 domains. The training data include books or magazines (called PB), newspaper articles (PN), and white papers (OW). The test data also include documents from a Q&amp;A site on the WWW (OC); (2) Test data include new senses (called X) that are not defined in dictionary. There is much previous research on WSD. In the case of Japanese, unsupervised approaches such as extended Lesk have performed well (Baldwin et al., 2010), although they are outperformed by supervised approaches (Tanaka et al., 2007; Murata et al., 200</context>
</contexts>
<marker>Okumura, Shirai, Komiya, Yokono, 2010</marker>
<rawString>Manabu Okumura, Kiyoaki Shirai, Kanako Komiya, and Hikaru Yokono. 2010. SemEval-2010 Task: Japanese WSD. In SemEval-2: Evaluation Exercises on Semantic Evaluation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Takaaki Tanaka</author>
<author>Francis Bond</author>
<author>Timothy Baldwin</author>
<author>Sanae Fujita</author>
<author>Chikara Hashimoto</author>
</authors>
<title>Word Sense Disambiguation Incorporating Lexical and Structural Semantic Information.</title>
<date>2007</date>
<booktitle>In Proceedings ofEMNLP-CoNLL-2007,</booktitle>
<pages>477--485</pages>
<contexts>
<context position="1369" citStr="Tanaka et al., 2007" startWordPosition="199" endWordPosition="202">D) task (Task 16 (Okumura et al., 2010)), which has two new characteristics: (1) Both training and test data across 3 or 4 domains. The training data include books or magazines (called PB), newspaper articles (PN), and white papers (OW). The test data also include documents from a Q&amp;A site on the WWW (OC); (2) Test data include new senses (called X) that are not defined in dictionary. There is much previous research on WSD. In the case of Japanese, unsupervised approaches such as extended Lesk have performed well (Baldwin et al., 2010), although they are outperformed by supervised approaches (Tanaka et al., 2007; Murata et al., 2003). Therefore, we selected a supervised approach and constructed Support Vector Machines (SVM) and Maximum Entropy (MEM) classifiers using common features and topic features. We performed extensive experiments to investigate the best combinations of domains for training. We describe the data in Section 2, and our system in Section 3. Then in Section 4, we show the results and provide some discussion. 2 Data Description 2.1 Given Data We show an example of Iwanami Kokugo Jiten (Nishio et al., 1994), which is a dictionary used as a sense inventory. As shown in Figure 1, each </context>
</contexts>
<marker>Tanaka, Bond, Baldwin, Fujita, Hashimoto, 2007</marker>
<rawString>Takaaki Tanaka, Francis Bond, Timothy Baldwin, Sanae Fujita, and Chikara Hashimoto. 2007. Word Sense Disambiguation Incorporating Lexical and Structural Semantic Information. In Proceedings ofEMNLP-CoNLL-2007, pp. 477–485.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>