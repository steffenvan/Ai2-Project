<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.002365">
<title confidence="0.996186">
Scalable Discriminative Parsing for German
</title>
<author confidence="0.788364">
Yannick Versley Ines Rehbein
</author>
<address confidence="0.3858995">
SFB 833 Dep. of Computational Linguistics
Universit¨at T¨ubingen Universit¨at des Saarlandes
</address>
<email confidence="0.968932">
versley@sfs.uni-tuebingen.de rehbein@coli.uni-sb.de
</email>
<sectionHeader confidence="0.992979" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999966869565218">
Generative lexicalized parsing models,
which are the mainstay for probabilistic
parsing of English, do not perform as well
when applied to languages with differ-
ent language-specific properties such as
free(r) word order or rich morphology. For
German and other non-English languages,
linguistically motivated complex treebank
transformations have been shown to im-
prove performance within the framework
of PCFG parsing, while generative lexical-
ized models do not seem to be as easily
adaptable to these languages.
In this paper, we show a practical way
to use grammatical functions as first-class
citizens in a discriminative model that al-
lows to extend annotated treebank gram-
mars with rich feature sets without hav-
ing to suffer from sparse data problems.
We demonstrate the flexibility of the ap-
proach by integrating unsupervised PP at-
tachment and POS-based word clusters
into the parser.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999840022727273">
To capture the semantic relations inherent in a
text, parsing has to recover both structural infor-
mation and grammatical functions, which com-
monly coincide in English, but not in freer
word order languages such as German. In-
stead one has to make use of morphological fea-
tures in addition to exploiting ordering preferences
such as the (violatable) default ordering of (sub-
ject&lt;)dative&lt;accusative.
Because of this fact, many successful ap-
proaches for German PCFG parsing (Schiehlen,
2004; Dubey, 2005; Versley, 2005) use annotated
treebank grammars where the constituent trees
from the treebank are enriched with further lin-
guistic information that allows an adequate recon-
struction of syntactic relationships, suggesting that
probabilistic context-free grammars are an ade-
quate tool for parsing these languages.
In the ACL 2008 workshop on Parsing Ger-
man (K¨ubler, 2008), Rafferty and Manning (2008)
used a lexicalized PCFG parser using markoviza-
tion and parent annotation, but no linguistically in-
spired transformations; Rafferty and Manning did
quite well on constituents, but were not success-
ful in reconstructing grammatical functions, with
results considerably worse than for other submis-
sions in the shared task.
The framework we present in this paper – an-
notated treebank grammars with a discriminative
model that allows lexicalization based on gram-
matical function assignment, as well as the ad-
dition of features based on unsupervised learn-
ing, including PP attachment and word clusters –
shows that it is possible to achieve good improve-
ments over generative lexicalized models by using
the additional flexibility gained over standard lex-
icalized PCFG models. Our approach offers more
flexibility than generative PCFG models, while
computational costs for development and practi-
cal use are still acceptable. While we only present
results for German, we are confident that the re-
sults carry over to other languages where anno-
tated treebank grammars have been used success-
fully.
</bodyText>
<sectionHeader confidence="0.9425125" genericHeader="method">
2 Parsing German with Morphology and
Valence Information
</sectionHeader>
<bodyText confidence="0.999992">
As a base parser, we use BitPar (Schmid, 2004),
a fast unlexicalized PCFG parser based on a first
pass where non-probabilistic bottom-up parsing
and top-down filtering is carried out efficiently by
storing the chart in bit vectors, and construct the
probabilistic chart only after top-down filtering.
We use an annotated treebank PCFG that is de-
</bodyText>
<page confidence="0.982026">
134
</page>
<bodyText confidence="0.988173086956522">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134–137,
Paris, October 2009. c�2009 Association for Computational Linguistics
rived from the Tiger treebank and largely inspired
by earlier work on annotated treebank grammars
for German (Schiehlen, 2004; Dubey, 2005; Vers-
ley, 2005).
Subcategorization With respect to the treebank
grammar, we refine the node labels with linguisti-
cally important information that is only implicit in
the treebank but would be tedious (and pointless)
to annotate by hand:
Firstly, we annotate NPs by case; clause nodes
(S and VP) are subcategorized by the clause type
(fin,inf,izu,rel), and NPs and PPs with a relative
pronoun are marked. Comparative phrases (e.g.,
bigger [than a house], marked as NP in Tiger and
T¨uBa-D/Z) are marked by adding a “CC” ending
to the node label. Finally, auxiliaries are split ac-
cording to their verb lemma into sein (be), haben
(have), werden (become).
To aid the identification of noun phrase case,
we add information related to case/number/gender
syncretism to the preterminal labels of determin-
ers, nouns, and adjectives (for details, see Versley,
2005) that allows to accurately determine the set of
possible cases while keeping the size of the tagset
relatively small.
Verb Valence We use information from the lex-
icon of the WCDG parser for German (Foth and
Menzel, 2006) to mark verbs according to the ar-
guments that they can take. While the WCDG
lexicon contains more information, we only en-
code the possibility of accusative and dative com-
plements, ignoring entries for genitive or clausal
complements.
Markovization with Argument Marking It
has been noted consistently (Klein and Manning,
2003; Schiehlen, 2004) that using markovization
- replacing the original treebank rules by an ap-
proximation that only considers a limited context
window of one or two siblings - improves re-
sults at least for a constituency-based evaluation.
However, in some cases this simple markoviza-
tion scheme leads to undesirable results includ-
ing sentences with multiple subjects, as predica-
tive arguments also have nominative case. To
avoid this, we additionally mark which arguments
have already been seen, yielding node labels such
as S fin&lt;VVFIN a&lt;RNP a&lt;sa in the case of a
partial constituent for a finite sentence (S fin)
expanding to the right (&lt;R) where both subject (s)
and accusative object (a) have already been seen.
Unknown Words For the base PCFG parse, we
use a decision tree with 43 regular expressions as
features, five of which are tailored towards rec-
ognizing the past and zu-infinitive form of sep-
arable prefix verbs (abarbeiten ⇒ abgearbeitet,
abzuarbeiten), which cannot be recognized by
considering suffixes only. The extended part of
speech tags for verbs (which contain valency in-
formation) are interpolated between the distribu-
tion at the concrete leaf of the decision tree and the
global valency distribution for the (coarse) part-of-
speech tag.
Additionally, we use SMOR (Schmid et al.,
2004) in conjunction with the verb lexicon and
a gazetteer list containing person and location
names to determine possible fine-grained part-of-
speech tags for unknown words.
</bodyText>
<subsectionHeader confidence="0.482423">
Restoring Grammatical Functions Adding
</subsectionHeader>
<bodyText confidence="0.999994869565217">
edge labels to the nodes in PCFG parsing easily
creates sparse data problems, as reported by
Rafferty and Manning (2008), who witness a drop
in constituent F-measure (excluding grammatical
function labels) when they include function labels
in the symbols of their PCFG. On the other hand,
the informativity of grammatical function labels
for the contents of the node does not always
justify their cost in terms of data sparseness.
Thus, we chose an approach where we include
linguistically relevant information in the node
labels (see above), and use the finer categoriza-
tion to restore the grammatical function labels
automatically: Using the most frequent function
label sequence associated with a rule yields good
results even in the presence of markovization,
where some of the surrounding context is lost.
Furthermore, this approach allows us to use the
grammatical function label assignments in the
subsequent discriminative model, thus yielding
typed dependencies rather than the unlabeled
dependencies that are used in the lexicalization
model of the Stanford parser.
</bodyText>
<sectionHeader confidence="0.99873" genericHeader="method">
3 Discriminative Parsing
</sectionHeader>
<bodyText confidence="0.999379375">
Generative parsing models are based on few dis-
tributions that use different feature combinations
based on smoothing; incorporating additional fea-
tures into these is very difficult at best.
As a result, the use of external preferences in
such parsers is usually limited to approaches that
reattach dependents in the output of the parser
rather than integrating them in the parsing process.
</bodyText>
<page confidence="0.983603">
135
</page>
<table confidence="0.612258666666667">
no GFs with GFs
77.40 NA
72.09 60.48
74.66 62.47
73.94 61.63
75.00 63.58
77.68 66.05
77.55 66.69
78.43 67.90
Settings
Rafferty and Manning (2008)
—, training with GFs
</table>
<figure confidence="0.923148066666667">
markov[unlex]
markov+parent[unlex]
markovGF[unlex]
markov[lex]
markovGF[lex]
markovGF[+pp]
fW-w-pos, CW-w-pos word form, cluster
f-sp, fS-sp-size node label, node size(1)
f-sp-RHS rule expansion
LDdir-sp-sd-hsd daughter attachment
LH-sp-sd-hsd-hld head projection
Lddir-hsp-hsd dependency (pos-pos)
Lddir-hsp-hsd-dist attachment length(1)
Ledir-hsp-hsd-hld dependency (pos-lemma)
Lfdir-hsp-hlp-hsd dependency (pos-lemma)
</figure>
<table confidence="0.81968525">
Lfdir-hsp-hlp-hsd-GF typed dep. (lemma-pos)
LhGF-hcp-hlp-hcd-hld typed dep. (lemma-lemma)
MIpp-prep, MIpp0-prep PP attach (noun)
MIppV-prep, MIppV0-prep PP attach (verb)
</table>
<tableCaption confidence="0.9117215">
Table 1: Evaluation results: PARSEVAL F1 on
PaGe development set
</tableCaption>
<bodyText confidence="0.998578868421053">
Discriminative parsing for unification-based
grammar commonly uses the conditional random
field formulation introduced by Miyao and Tsu-
jii (2002) and Geman and Johnson (2002), which
uses local features to select a parse from a packed
forest. The much larger cost in terms of mem-
ory and time compared to generative models has
until recently made this approach largely unattrac-
tive (but see Finkel et al., 2008, who distributes the
learning process over several powerful machines).
An alternative use of discriminative models
has been to incorporate global features, either by
reranking (e.g. Charniak and Johnson, 2005, or
K¨ubler et al., 2009 for German) or by beam search
over a pruned parse forest (Huang, 2008). How-
ever, Huang shows that a discriminative model us-
ing only local features reaps most of the benefits
of the global model and performs at a similar level
than earlier reranking-based approaches, pointing
to the fact that local ambiguities often result in the
n-best list not containing the correct parse.
The model we propose here extracts a pruned
parse forest from a simple unlexicalized parser and
then uses a factored discriminative model to apply
a rich set of features using the lexicalized parse
tree and its typed dependencies.
CRF parsing on pruned forests We extract a
pruned forest that contains exactly those nodes and
edges that can occur in trees that have a probabil-
ity ≥ pbest · t, where in practice a threshold of
t = 10−3 ensures that no good parse is pruned
away while at the same time, the resulting forest
has only few nodes and edges.
For training, we extract an oracle tree, which is
selected according to a combination of correct (an-
notated grammar) constituents, the absence of in-
correct constituents, and the likelihood of the tree,
to account for the fact that the forest does not al-
</bodyText>
<tableCaption confidence="0.7364222">
1) node sizes and attachment distances are discretized.
dir: one of H(head), L/R(head dep), B/I/E(nonheaded dep)
sp/d constituent symbol (parent/dep), hsp/d head cat, hc
head cat (coarse), hl head lemma
Table 2: List of Features
</tableCaption>
<bodyText confidence="0.999962857142857">
ways contain the exact gold tree. We then use
the AMIS maximum entropy learner of Miyao and
Tsujii (2002) to learn the discriminative model by
creating a forest from a grammar learned on the
remaining 4/5 of the training data.
Efficiency Parsing using the discriminative
model is quite efficient, with a memory con-
sumption for the whole system at about 270MB,
including the data used to determine the corpus
derived features (word clusters, mutual informa-
tion statistics, semantic role clusters). Parsing
speed is at 1.65sec./sentence on a 1.5GHz Pen-
tium M, against 1.84sec./sent for BitPar alone
when not using the tag filter for unknown words.
The time needed for learning can be reduced
by keeping the pruned parse charts and only re-
running the part of lexicalization and discrimina-
tive feature extraction; when reusing the old pa-
rameters as a starting point for AMIS’ model esti-
mation, the turn-around time including feature ex-
traction is below two hours.
</bodyText>
<subsectionHeader confidence="0.997974">
3.1 Clustering for unknown words
</subsectionHeader>
<bodyText confidence="0.99995325">
To improve the behaviour on unknown words
where morphological analyzer and regular expres-
sions do not yield informative preferences, we ex-
ploit a large, part-of-speech-tagged corpus to in-
duce clusters which provide robust information
that is useful even in our case where preterminals
in the PCFG are finer than standard POS tags.
The following features were gathered and used
by weighting by the pointwise mutual information
between the word and feature occurrences:
The context feature retrieves windows of high-
frequent words surrounding the word in question
</bodyText>
<page confidence="0.997466">
136
</page>
<bodyText confidence="0.9992755">
(e.g. der mit for ‘der Mann mit den Blumen’).
The context2 feature retrieves windows of one
high-frequent word and one part-of-speech tag
surrounding the word in question (e.g. der NN for
‘der sch¨one Mann’).
The postag feature simply retrieves the part-of-
speech tag that is assigned to the word.
The result of using the repeated bisecting
k-means implementation of CLUTO (Steinbach
et al., 2000) on the resulting features yields syntac-
tically sensible clusters containing years, money
sums, last names, or place names.
</bodyText>
<subsectionHeader confidence="0.9491685">
3.2 Unsupervised PP Attachment and
Subject-Object preferences
</subsectionHeader>
<bodyText confidence="0.9998931">
We used simple part-of-speech tag patterns to
gather statistics on the association between nouns
and immediately following prepositions, as well
as between prepositions and closely following
verbs on the DE-WaC corpus (Baroni and Kilgar-
iff, 2006), an 1.7G words sample of the German-
language WWW. The mutual information values
for PP attachment are made available to the parser
as features that are weighted by the mutual infor-
mation value.
</bodyText>
<sectionHeader confidence="0.995196" genericHeader="conclusions">
4 Evaluation and Discussion
</sectionHeader>
<bodyText confidence="0.999980485714286">
To evaluate our approach, we use the dataset
used for the ACL-2008 Parsing German Workshop
(K¨ubler, 2008) that contains 26,116 sentences of
the TIGER treebank (Brants et al., 2002), in a 8:1:1
split of training, testing, and evaluation data, and
validate our approach on the development data,
where the results published by Rafferty and Man-
ning (2008) provide a useful comparison. All our
experiments are done using tags automatically as-
signed by the parser, which reaches a tagging ac-
curacy of about 97.5% according to the EVALB
output.
We find that our final model, combining aug-
menting the treebank labels with lingustic infor-
mation in addition to lexicalization and unsuper-
vised PP attachment works better than the best-
performing models of Rafferty and Manning, with
a very large improvement in grammatical func-
tions that is only surpassed by the Berkeley Parser
(Petrov and Klein, 2008), showing that our combi-
nation of annotated treebank grammars with a fac-
tored discriminative model not only allows great
control and flexibility for experimenting with the
inclusion of novel features, but also yields very
good results compared with the state of the art
for German (see table 1 for results on the Tiger
treebank). Preliminary results on T¨uBa-D/Z with
a subset of the transformations of Versley (2005)
show the same tendency as the results for Tiger,
with 91.3% for constituents only, and 80.1% in-
cluding function labels (compared to 88.9% and
77.2% for the Stanford parser).
Future work will investigate the impact of in-
cluding additional features into the discriminative
parsing model.
</bodyText>
<sectionHeader confidence="0.998367" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999364098039216">
Baroni, M. and Kilgariff, A. (2006). Large linguistically-
processed web corpora for multiple languages. In EACL
2006.
Brants, S., Dipper, S., Hansen, S., Lezius, W., and Smith, G.
(2002). The TIGER treebank. In Proc. TLT 2002.
Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best
parsing and maxent discriminative reranking. In Proc.
ACL 2005.
Dubey, A. (2005). What to do when lexicalization fails: pars-
ing German with suffix analysis and smoothing. In ACL-
2005.
Finkel, J. R., Kleeman, A., and Manning, C. D. (2008). Effi-
cient, feature-based, conditional random field parsing. In
ACL/HLT-2008.
Foth, K. and Menzel, W. (2006). Hybrid parsing: Using prob-
abilistic models as predictors for a symbolic parser. In
ACL 2006.
Geman, S. and Johnson, M. (2002). Dynamic programming
for parsing and estimation of stochastic unification-based
grammars. In ACL 2002.
Huang, L. (2008). Forest reranking: Discriminative parsing
with non-local features. In HLT/ACL 2008.
Klein, D. and Manning, C. D. (2003). Accurate unlexicalized
parsing. In ACL 2003.
K¨ubler, S. (2008). The PaGe 2008 shared task on parsing
German. In Proceedings of the ACL-2008 Workshop on
Parsing German.
K¨ubler, S., Hinrichs, E., Maier, W., and Klett, E. (2009). Pars-
ing coordinations. In EACL 2009.
Miyao, Y. and Tsujii, J. (2002). Maximum entropy estimation
for feature forests. In HLT 2002.
Petrov, S. and Klein, D. (2008). Parsing German with latent
variable grammars. In Parsing German Workshop at ACL-
HLT 2008.
Rafferty, A. and Manning, C. D. (2008). Parsing three Ger-
man treebanks: Lexicalized and unlexicalized baselines.
In ACL’08 workshop on Parsing German.
Schiehlen, M. (2004). Annotation strategies for probabilistic
parsing in German. In Proc. Coling 2004.
Schmid, H. (2004). Efficient parsing of highly ambiguous
context-free grammars with bit vectors. In Proc. Coling
2004.
Schmid, H., Fitschen, A., and Heid, U. (2004). SMOR: A
German computational morphology covering derivation,
composition and inflection. In Proceedings of LREC 2004.
Steinbach, M., Karypis, G., and Kumar, V. (2000). A com-
parison of document clustering techniques. In KDD Work-
shop on Text Mining.
Versley, Y. (2005). Parser evaluation across text types. In
Proceedings of the Fourth Workshop on Treebanks and
Linguistic Theories (TLT 2005).
</reference>
<page confidence="0.997804">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.134984">
<title confidence="0.999181">Scalable Discriminative Parsing for German</title>
<author confidence="0.916705">Yannick Versley Ines Rehbein</author>
<affiliation confidence="0.465197">SFB 833 Dep. of Computational Linguistics</affiliation>
<abstract confidence="0.956449423076923">Universit¨at T¨ubingen Universit¨at des Saarlandes versley@sfs.uni-tuebingen.de rehbein@coli.uni-sb.de Abstract Generative lexicalized parsing models, which are the mainstay for probabilistic parsing of English, do not perform as well when applied to languages with different language-specific properties such as free(r) word order or rich morphology. For German and other non-English languages, linguistically motivated complex treebank transformations have been shown to improve performance within the framework of PCFG parsing, while generative lexicalized models do not seem to be as easily adaptable to these languages. In this paper, we show a practical way to use grammatical functions as first-class citizens in a discriminative model that allows to extend annotated treebank grammars with rich feature sets without having to suffer from sparse data problems. We demonstrate the flexibility of the approach by integrating unsupervised PP attachment and POS-based word clusters into the parser.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Baroni</author>
<author>A Kilgariff</author>
</authors>
<title>Large linguisticallyprocessed web corpora for multiple languages. In</title>
<date>2006</date>
<booktitle>EACL</booktitle>
<contexts>
<context position="13499" citStr="Baroni and Kilgariff, 2006" startWordPosition="2070" endWordPosition="2074">Mann’). The postag feature simply retrieves the part-ofspeech tag that is assigned to the word. The result of using the repeated bisecting k-means implementation of CLUTO (Steinbach et al., 2000) on the resulting features yields syntactically sensible clusters containing years, money sums, last names, or place names. 3.2 Unsupervised PP Attachment and Subject-Object preferences We used simple part-of-speech tag patterns to gather statistics on the association between nouns and immediately following prepositions, as well as between prepositions and closely following verbs on the DE-WaC corpus (Baroni and Kilgariff, 2006), an 1.7G words sample of the Germanlanguage WWW. The mutual information values for PP attachment are made available to the parser as features that are weighted by the mutual information value. 4 Evaluation and Discussion To evaluate our approach, we use the dataset used for the ACL-2008 Parsing German Workshop (K¨ubler, 2008) that contains 26,116 sentences of the TIGER treebank (Brants et al., 2002), in a 8:1:1 split of training, testing, and evaluation data, and validate our approach on the development data, where the results published by Rafferty and Manning (2008) provide a useful comparis</context>
</contexts>
<marker>Baroni, Kilgariff, 2006</marker>
<rawString>Baroni, M. and Kilgariff, A. (2006). Large linguisticallyprocessed web corpora for multiple languages. In EACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Brants</author>
<author>S Dipper</author>
<author>S Hansen</author>
<author>W Lezius</author>
<author>G Smith</author>
</authors>
<title>The TIGER treebank.</title>
<date>2002</date>
<booktitle>In Proc. TLT</booktitle>
<contexts>
<context position="13902" citStr="Brants et al., 2002" startWordPosition="2137" endWordPosition="2140"> tag patterns to gather statistics on the association between nouns and immediately following prepositions, as well as between prepositions and closely following verbs on the DE-WaC corpus (Baroni and Kilgariff, 2006), an 1.7G words sample of the Germanlanguage WWW. The mutual information values for PP attachment are made available to the parser as features that are weighted by the mutual information value. 4 Evaluation and Discussion To evaluate our approach, we use the dataset used for the ACL-2008 Parsing German Workshop (K¨ubler, 2008) that contains 26,116 sentences of the TIGER treebank (Brants et al., 2002), in a 8:1:1 split of training, testing, and evaluation data, and validate our approach on the development data, where the results published by Rafferty and Manning (2008) provide a useful comparison. All our experiments are done using tags automatically assigned by the parser, which reaches a tagging accuracy of about 97.5% according to the EVALB output. We find that our final model, combining augmenting the treebank labels with lingustic information in addition to lexicalization and unsupervised PP attachment works better than the bestperforming models of Rafferty and Manning, with a very la</context>
</contexts>
<marker>Brants, Dipper, Hansen, Lezius, Smith, 2002</marker>
<rawString>Brants, S., Dipper, S., Hansen, S., Lezius, W., and Smith, G. (2002). The TIGER treebank. In Proc. TLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Charniak</author>
<author>M Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and maxent discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. ACL</booktitle>
<contexts>
<context position="9676" citStr="Charniak and Johnson, 2005" startWordPosition="1450" endWordPosition="1453">ment set Discriminative parsing for unification-based grammar commonly uses the conditional random field formulation introduced by Miyao and Tsujii (2002) and Geman and Johnson (2002), which uses local features to select a parse from a packed forest. The much larger cost in terms of memory and time compared to generative models has until recently made this approach largely unattractive (but see Finkel et al., 2008, who distributes the learning process over several powerful machines). An alternative use of discriminative models has been to incorporate global features, either by reranking (e.g. Charniak and Johnson, 2005, or K¨ubler et al., 2009 for German) or by beam search over a pruned parse forest (Huang, 2008). However, Huang shows that a discriminative model using only local features reaps most of the benefits of the global model and performs at a similar level than earlier reranking-based approaches, pointing to the fact that local ambiguities often result in the n-best list not containing the correct parse. The model we propose here extracts a pruned parse forest from a simple unlexicalized parser and then uses a factored discriminative model to apply a rich set of features using the lexicalized parse</context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Charniak, E. and Johnson, M. (2005). Coarse-to-fine n-best parsing and maxent discriminative reranking. In Proc. ACL 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Dubey</author>
</authors>
<title>What to do when lexicalization fails: parsing German with suffix analysis and smoothing.</title>
<date>2005</date>
<booktitle>In ACL2005.</booktitle>
<contexts>
<context position="1630" citStr="Dubey, 2005" startWordPosition="238" endWordPosition="239"> approach by integrating unsupervised PP attachment and POS-based word clusters into the parser. 1 Introduction To capture the semantic relations inherent in a text, parsing has to recover both structural information and grammatical functions, which commonly coincide in English, but not in freer word order languages such as German. Instead one has to make use of morphological features in addition to exploiting ordering preferences such as the (violatable) default ordering of (subject&lt;)dative&lt;accusative. Because of this fact, many successful approaches for German PCFG parsing (Schiehlen, 2004; Dubey, 2005; Versley, 2005) use annotated treebank grammars where the constituent trees from the treebank are enriched with further linguistic information that allows an adequate reconstruction of syntactic relationships, suggesting that probabilistic context-free grammars are an adequate tool for parsing these languages. In the ACL 2008 workshop on Parsing German (K¨ubler, 2008), Rafferty and Manning (2008) used a lexicalized PCFG parser using markovization and parent annotation, but no linguistically inspired transformations; Rafferty and Manning did quite well on constituents, but were not successful </context>
<context position="3827" citStr="Dubey, 2005" startWordPosition="569" endWordPosition="570">004), a fast unlexicalized PCFG parser based on a first pass where non-probabilistic bottom-up parsing and top-down filtering is carried out efficiently by storing the chart in bit vectors, and construct the probabilistic chart only after top-down filtering. We use an annotated treebank PCFG that is de134 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134–137, Paris, October 2009. c�2009 Association for Computational Linguistics rived from the Tiger treebank and largely inspired by earlier work on annotated treebank grammars for German (Schiehlen, 2004; Dubey, 2005; Versley, 2005). Subcategorization With respect to the treebank grammar, we refine the node labels with linguistically important information that is only implicit in the treebank but would be tedious (and pointless) to annotate by hand: Firstly, we annotate NPs by case; clause nodes (S and VP) are subcategorized by the clause type (fin,inf,izu,rel), and NPs and PPs with a relative pronoun are marked. Comparative phrases (e.g., bigger [than a house], marked as NP in Tiger and T¨uBa-D/Z) are marked by adding a “CC” ending to the node label. Finally, auxiliaries are split according to their verb</context>
</contexts>
<marker>Dubey, 2005</marker>
<rawString>Dubey, A. (2005). What to do when lexicalization fails: parsing German with suffix analysis and smoothing. In ACL2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Finkel</author>
<author>A Kleeman</author>
<author>C D Manning</author>
</authors>
<title>Efficient, feature-based, conditional random field parsing.</title>
<date>2008</date>
<booktitle>In ACL/HLT-2008.</booktitle>
<contexts>
<context position="9467" citStr="Finkel et al., 2008" startWordPosition="1421" endWordPosition="1424">ed dep. (lemma-pos) LhGF-hcp-hlp-hcd-hld typed dep. (lemma-lemma) MIpp-prep, MIpp0-prep PP attach (noun) MIppV-prep, MIppV0-prep PP attach (verb) Table 1: Evaluation results: PARSEVAL F1 on PaGe development set Discriminative parsing for unification-based grammar commonly uses the conditional random field formulation introduced by Miyao and Tsujii (2002) and Geman and Johnson (2002), which uses local features to select a parse from a packed forest. The much larger cost in terms of memory and time compared to generative models has until recently made this approach largely unattractive (but see Finkel et al., 2008, who distributes the learning process over several powerful machines). An alternative use of discriminative models has been to incorporate global features, either by reranking (e.g. Charniak and Johnson, 2005, or K¨ubler et al., 2009 for German) or by beam search over a pruned parse forest (Huang, 2008). However, Huang shows that a discriminative model using only local features reaps most of the benefits of the global model and performs at a similar level than earlier reranking-based approaches, pointing to the fact that local ambiguities often result in the n-best list not containing the cor</context>
</contexts>
<marker>Finkel, Kleeman, Manning, 2008</marker>
<rawString>Finkel, J. R., Kleeman, A., and Manning, C. D. (2008). Efficient, feature-based, conditional random field parsing. In ACL/HLT-2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Foth</author>
<author>W Menzel</author>
</authors>
<title>Hybrid parsing: Using probabilistic models as predictors for a symbolic parser.</title>
<date>2006</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="4904" citStr="Foth and Menzel, 2006" startWordPosition="742" endWordPosition="745">marked as NP in Tiger and T¨uBa-D/Z) are marked by adding a “CC” ending to the node label. Finally, auxiliaries are split according to their verb lemma into sein (be), haben (have), werden (become). To aid the identification of noun phrase case, we add information related to case/number/gender syncretism to the preterminal labels of determiners, nouns, and adjectives (for details, see Versley, 2005) that allows to accurately determine the set of possible cases while keeping the size of the tagset relatively small. Verb Valence We use information from the lexicon of the WCDG parser for German (Foth and Menzel, 2006) to mark verbs according to the arguments that they can take. While the WCDG lexicon contains more information, we only encode the possibility of accusative and dative complements, ignoring entries for genitive or clausal complements. Markovization with Argument Marking It has been noted consistently (Klein and Manning, 2003; Schiehlen, 2004) that using markovization - replacing the original treebank rules by an approximation that only considers a limited context window of one or two siblings - improves results at least for a constituency-based evaluation. However, in some cases this simple ma</context>
</contexts>
<marker>Foth, Menzel, 2006</marker>
<rawString>Foth, K. and Menzel, W. (2006). Hybrid parsing: Using probabilistic models as predictors for a symbolic parser. In ACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Geman</author>
<author>M Johnson</author>
</authors>
<title>Dynamic programming for parsing and estimation of stochastic unification-based grammars.</title>
<date>2002</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="9233" citStr="Geman and Johnson (2002)" startWordPosition="1379" endWordPosition="1382">-hsd daughter attachment LH-sp-sd-hsd-hld head projection Lddir-hsp-hsd dependency (pos-pos) Lddir-hsp-hsd-dist attachment length(1) Ledir-hsp-hsd-hld dependency (pos-lemma) Lfdir-hsp-hlp-hsd dependency (pos-lemma) Lfdir-hsp-hlp-hsd-GF typed dep. (lemma-pos) LhGF-hcp-hlp-hcd-hld typed dep. (lemma-lemma) MIpp-prep, MIpp0-prep PP attach (noun) MIppV-prep, MIppV0-prep PP attach (verb) Table 1: Evaluation results: PARSEVAL F1 on PaGe development set Discriminative parsing for unification-based grammar commonly uses the conditional random field formulation introduced by Miyao and Tsujii (2002) and Geman and Johnson (2002), which uses local features to select a parse from a packed forest. The much larger cost in terms of memory and time compared to generative models has until recently made this approach largely unattractive (but see Finkel et al., 2008, who distributes the learning process over several powerful machines). An alternative use of discriminative models has been to incorporate global features, either by reranking (e.g. Charniak and Johnson, 2005, or K¨ubler et al., 2009 for German) or by beam search over a pruned parse forest (Huang, 2008). However, Huang shows that a discriminative model using only</context>
</contexts>
<marker>Geman, Johnson, 2002</marker>
<rawString>Geman, S. and Johnson, M. (2002). Dynamic programming for parsing and estimation of stochastic unification-based grammars. In ACL 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Huang</author>
</authors>
<title>Forest reranking: Discriminative parsing with non-local features.</title>
<date>2008</date>
<booktitle>In HLT/ACL</booktitle>
<contexts>
<context position="9772" citStr="Huang, 2008" startWordPosition="1470" endWordPosition="1471">lation introduced by Miyao and Tsujii (2002) and Geman and Johnson (2002), which uses local features to select a parse from a packed forest. The much larger cost in terms of memory and time compared to generative models has until recently made this approach largely unattractive (but see Finkel et al., 2008, who distributes the learning process over several powerful machines). An alternative use of discriminative models has been to incorporate global features, either by reranking (e.g. Charniak and Johnson, 2005, or K¨ubler et al., 2009 for German) or by beam search over a pruned parse forest (Huang, 2008). However, Huang shows that a discriminative model using only local features reaps most of the benefits of the global model and performs at a similar level than earlier reranking-based approaches, pointing to the fact that local ambiguities often result in the n-best list not containing the correct parse. The model we propose here extracts a pruned parse forest from a simple unlexicalized parser and then uses a factored discriminative model to apply a rich set of features using the lexicalized parse tree and its typed dependencies. CRF parsing on pruned forests We extract a pruned forest that </context>
</contexts>
<marker>Huang, 2008</marker>
<rawString>Huang, L. (2008). Forest reranking: Discriminative parsing with non-local features. In HLT/ACL 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C D Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL</booktitle>
<contexts>
<context position="5230" citStr="Klein and Manning, 2003" startWordPosition="793" endWordPosition="796">labels of determiners, nouns, and adjectives (for details, see Versley, 2005) that allows to accurately determine the set of possible cases while keeping the size of the tagset relatively small. Verb Valence We use information from the lexicon of the WCDG parser for German (Foth and Menzel, 2006) to mark verbs according to the arguments that they can take. While the WCDG lexicon contains more information, we only encode the possibility of accusative and dative complements, ignoring entries for genitive or clausal complements. Markovization with Argument Marking It has been noted consistently (Klein and Manning, 2003; Schiehlen, 2004) that using markovization - replacing the original treebank rules by an approximation that only considers a limited context window of one or two siblings - improves results at least for a constituency-based evaluation. However, in some cases this simple markovization scheme leads to undesirable results including sentences with multiple subjects, as predicative arguments also have nominative case. To avoid this, we additionally mark which arguments have already been seen, yielding node labels such as S fin&lt;VVFIN a&lt;RNP a&lt;sa in the case of a partial constituent for a finite sent</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>Klein, D. and Manning, C. D. (2003). Accurate unlexicalized parsing. In ACL 2003.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K¨ubler</author>
</authors>
<title>The PaGe</title>
<date>2008</date>
<booktitle>In Proceedings of the ACL-2008 Workshop on Parsing German.</booktitle>
<marker>K¨ubler, 2008</marker>
<rawString>K¨ubler, S. (2008). The PaGe 2008 shared task on parsing German. In Proceedings of the ACL-2008 Workshop on Parsing German.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S K¨ubler</author>
<author>E Hinrichs</author>
<author>W Maier</author>
<author>E Klett</author>
</authors>
<title>Parsing coordinations.</title>
<date>2009</date>
<booktitle>In EACL</booktitle>
<marker>K¨ubler, Hinrichs, Maier, Klett, 2009</marker>
<rawString>K¨ubler, S., Hinrichs, E., Maier, W., and Klett, E. (2009). Parsing coordinations. In EACL 2009.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Miyao</author>
<author>J Tsujii</author>
</authors>
<title>Maximum entropy estimation for feature forests.</title>
<date>2002</date>
<booktitle>In HLT</booktitle>
<contexts>
<context position="9204" citStr="Miyao and Tsujii (2002)" startWordPosition="1373" endWordPosition="1377">S rule expansion LDdir-sp-sd-hsd daughter attachment LH-sp-sd-hsd-hld head projection Lddir-hsp-hsd dependency (pos-pos) Lddir-hsp-hsd-dist attachment length(1) Ledir-hsp-hsd-hld dependency (pos-lemma) Lfdir-hsp-hlp-hsd dependency (pos-lemma) Lfdir-hsp-hlp-hsd-GF typed dep. (lemma-pos) LhGF-hcp-hlp-hcd-hld typed dep. (lemma-lemma) MIpp-prep, MIpp0-prep PP attach (noun) MIppV-prep, MIppV0-prep PP attach (verb) Table 1: Evaluation results: PARSEVAL F1 on PaGe development set Discriminative parsing for unification-based grammar commonly uses the conditional random field formulation introduced by Miyao and Tsujii (2002) and Geman and Johnson (2002), which uses local features to select a parse from a packed forest. The much larger cost in terms of memory and time compared to generative models has until recently made this approach largely unattractive (but see Finkel et al., 2008, who distributes the learning process over several powerful machines). An alternative use of discriminative models has been to incorporate global features, either by reranking (e.g. Charniak and Johnson, 2005, or K¨ubler et al., 2009 for German) or by beam search over a pruned parse forest (Huang, 2008). However, Huang shows that a di</context>
<context position="11218" citStr="Miyao and Tsujii (2002)" startWordPosition="1715" endWordPosition="1718">orest has only few nodes and edges. For training, we extract an oracle tree, which is selected according to a combination of correct (annotated grammar) constituents, the absence of incorrect constituents, and the likelihood of the tree, to account for the fact that the forest does not al1) node sizes and attachment distances are discretized. dir: one of H(head), L/R(head dep), B/I/E(nonheaded dep) sp/d constituent symbol (parent/dep), hsp/d head cat, hc head cat (coarse), hl head lemma Table 2: List of Features ways contain the exact gold tree. We then use the AMIS maximum entropy learner of Miyao and Tsujii (2002) to learn the discriminative model by creating a forest from a grammar learned on the remaining 4/5 of the training data. Efficiency Parsing using the discriminative model is quite efficient, with a memory consumption for the whole system at about 270MB, including the data used to determine the corpus derived features (word clusters, mutual information statistics, semantic role clusters). Parsing speed is at 1.65sec./sentence on a 1.5GHz Pentium M, against 1.84sec./sent for BitPar alone when not using the tag filter for unknown words. The time needed for learning can be reduced by keeping the </context>
</contexts>
<marker>Miyao, Tsujii, 2002</marker>
<rawString>Miyao, Y. and Tsujii, J. (2002). Maximum entropy estimation for feature forests. In HLT 2002.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Petrov</author>
<author>D Klein</author>
</authors>
<title>Parsing German with latent variable grammars.</title>
<date>2008</date>
<booktitle>In Parsing German Workshop at ACLHLT</booktitle>
<contexts>
<context position="14613" citStr="Petrov and Klein, 2008" startWordPosition="2253" endWordPosition="2256">on the development data, where the results published by Rafferty and Manning (2008) provide a useful comparison. All our experiments are done using tags automatically assigned by the parser, which reaches a tagging accuracy of about 97.5% according to the EVALB output. We find that our final model, combining augmenting the treebank labels with lingustic information in addition to lexicalization and unsupervised PP attachment works better than the bestperforming models of Rafferty and Manning, with a very large improvement in grammatical functions that is only surpassed by the Berkeley Parser (Petrov and Klein, 2008), showing that our combination of annotated treebank grammars with a factored discriminative model not only allows great control and flexibility for experimenting with the inclusion of novel features, but also yields very good results compared with the state of the art for German (see table 1 for results on the Tiger treebank). Preliminary results on T¨uBa-D/Z with a subset of the transformations of Versley (2005) show the same tendency as the results for Tiger, with 91.3% for constituents only, and 80.1% including function labels (compared to 88.9% and 77.2% for the Stanford parser). Future w</context>
</contexts>
<marker>Petrov, Klein, 2008</marker>
<rawString>Petrov, S. and Klein, D. (2008). Parsing German with latent variable grammars. In Parsing German Workshop at ACLHLT 2008.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Rafferty</author>
<author>C D Manning</author>
</authors>
<title>Parsing three German treebanks: Lexicalized and unlexicalized baselines.</title>
<date>2008</date>
<booktitle>In ACL’08 workshop on Parsing German.</booktitle>
<contexts>
<context position="2030" citStr="Rafferty and Manning (2008)" startWordPosition="294" endWordPosition="297">res in addition to exploiting ordering preferences such as the (violatable) default ordering of (subject&lt;)dative&lt;accusative. Because of this fact, many successful approaches for German PCFG parsing (Schiehlen, 2004; Dubey, 2005; Versley, 2005) use annotated treebank grammars where the constituent trees from the treebank are enriched with further linguistic information that allows an adequate reconstruction of syntactic relationships, suggesting that probabilistic context-free grammars are an adequate tool for parsing these languages. In the ACL 2008 workshop on Parsing German (K¨ubler, 2008), Rafferty and Manning (2008) used a lexicalized PCFG parser using markovization and parent annotation, but no linguistically inspired transformations; Rafferty and Manning did quite well on constituents, but were not successful in reconstructing grammatical functions, with results considerably worse than for other submissions in the shared task. The framework we present in this paper – annotated treebank grammars with a discriminative model that allows lexicalization based on grammatical function assignment, as well as the addition of features based on unsupervised learning, including PP attachment and word clusters – sh</context>
<context position="6862" citStr="Rafferty and Manning (2008)" startWordPosition="1051" endWordPosition="1054">g suffixes only. The extended part of speech tags for verbs (which contain valency information) are interpolated between the distribution at the concrete leaf of the decision tree and the global valency distribution for the (coarse) part-ofspeech tag. Additionally, we use SMOR (Schmid et al., 2004) in conjunction with the verb lexicon and a gazetteer list containing person and location names to determine possible fine-grained part-ofspeech tags for unknown words. Restoring Grammatical Functions Adding edge labels to the nodes in PCFG parsing easily creates sparse data problems, as reported by Rafferty and Manning (2008), who witness a drop in constituent F-measure (excluding grammatical function labels) when they include function labels in the symbols of their PCFG. On the other hand, the informativity of grammatical function labels for the contents of the node does not always justify their cost in terms of data sparseness. Thus, we chose an approach where we include linguistically relevant information in the node labels (see above), and use the finer categorization to restore the grammatical function labels automatically: Using the most frequent function label sequence associated with a rule yields good res</context>
<context position="8381" citStr="Rafferty and Manning (2008)" startWordPosition="1285" endWordPosition="1288">at are used in the lexicalization model of the Stanford parser. 3 Discriminative Parsing Generative parsing models are based on few distributions that use different feature combinations based on smoothing; incorporating additional features into these is very difficult at best. As a result, the use of external preferences in such parsers is usually limited to approaches that reattach dependents in the output of the parser rather than integrating them in the parsing process. 135 no GFs with GFs 77.40 NA 72.09 60.48 74.66 62.47 73.94 61.63 75.00 63.58 77.68 66.05 77.55 66.69 78.43 67.90 Settings Rafferty and Manning (2008) —, training with GFs markov[unlex] markov+parent[unlex] markovGF[unlex] markov[lex] markovGF[lex] markovGF[+pp] fW-w-pos, CW-w-pos word form, cluster f-sp, fS-sp-size node label, node size(1) f-sp-RHS rule expansion LDdir-sp-sd-hsd daughter attachment LH-sp-sd-hsd-hld head projection Lddir-hsp-hsd dependency (pos-pos) Lddir-hsp-hsd-dist attachment length(1) Ledir-hsp-hsd-hld dependency (pos-lemma) Lfdir-hsp-hlp-hsd dependency (pos-lemma) Lfdir-hsp-hlp-hsd-GF typed dep. (lemma-pos) LhGF-hcp-hlp-hcd-hld typed dep. (lemma-lemma) MIpp-prep, MIpp0-prep PP attach (noun) MIppV-prep, MIppV0-prep PP a</context>
<context position="14073" citStr="Rafferty and Manning (2008)" startWordPosition="2164" endWordPosition="2168">rbs on the DE-WaC corpus (Baroni and Kilgariff, 2006), an 1.7G words sample of the Germanlanguage WWW. The mutual information values for PP attachment are made available to the parser as features that are weighted by the mutual information value. 4 Evaluation and Discussion To evaluate our approach, we use the dataset used for the ACL-2008 Parsing German Workshop (K¨ubler, 2008) that contains 26,116 sentences of the TIGER treebank (Brants et al., 2002), in a 8:1:1 split of training, testing, and evaluation data, and validate our approach on the development data, where the results published by Rafferty and Manning (2008) provide a useful comparison. All our experiments are done using tags automatically assigned by the parser, which reaches a tagging accuracy of about 97.5% according to the EVALB output. We find that our final model, combining augmenting the treebank labels with lingustic information in addition to lexicalization and unsupervised PP attachment works better than the bestperforming models of Rafferty and Manning, with a very large improvement in grammatical functions that is only surpassed by the Berkeley Parser (Petrov and Klein, 2008), showing that our combination of annotated treebank grammar</context>
</contexts>
<marker>Rafferty, Manning, 2008</marker>
<rawString>Rafferty, A. and Manning, C. D. (2008). Parsing three German treebanks: Lexicalized and unlexicalized baselines. In ACL’08 workshop on Parsing German.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Schiehlen</author>
</authors>
<title>Annotation strategies for probabilistic parsing in German.</title>
<date>2004</date>
<booktitle>In Proc. Coling</booktitle>
<contexts>
<context position="1617" citStr="Schiehlen, 2004" startWordPosition="236" endWordPosition="237">lexibility of the approach by integrating unsupervised PP attachment and POS-based word clusters into the parser. 1 Introduction To capture the semantic relations inherent in a text, parsing has to recover both structural information and grammatical functions, which commonly coincide in English, but not in freer word order languages such as German. Instead one has to make use of morphological features in addition to exploiting ordering preferences such as the (violatable) default ordering of (subject&lt;)dative&lt;accusative. Because of this fact, many successful approaches for German PCFG parsing (Schiehlen, 2004; Dubey, 2005; Versley, 2005) use annotated treebank grammars where the constituent trees from the treebank are enriched with further linguistic information that allows an adequate reconstruction of syntactic relationships, suggesting that probabilistic context-free grammars are an adequate tool for parsing these languages. In the ACL 2008 workshop on Parsing German (K¨ubler, 2008), Rafferty and Manning (2008) used a lexicalized PCFG parser using markovization and parent annotation, but no linguistically inspired transformations; Rafferty and Manning did quite well on constituents, but were no</context>
<context position="3814" citStr="Schiehlen, 2004" startWordPosition="567" endWordPosition="568">BitPar (Schmid, 2004), a fast unlexicalized PCFG parser based on a first pass where non-probabilistic bottom-up parsing and top-down filtering is carried out efficiently by storing the chart in bit vectors, and construct the probabilistic chart only after top-down filtering. We use an annotated treebank PCFG that is de134 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134–137, Paris, October 2009. c�2009 Association for Computational Linguistics rived from the Tiger treebank and largely inspired by earlier work on annotated treebank grammars for German (Schiehlen, 2004; Dubey, 2005; Versley, 2005). Subcategorization With respect to the treebank grammar, we refine the node labels with linguistically important information that is only implicit in the treebank but would be tedious (and pointless) to annotate by hand: Firstly, we annotate NPs by case; clause nodes (S and VP) are subcategorized by the clause type (fin,inf,izu,rel), and NPs and PPs with a relative pronoun are marked. Comparative phrases (e.g., bigger [than a house], marked as NP in Tiger and T¨uBa-D/Z) are marked by adding a “CC” ending to the node label. Finally, auxiliaries are split according </context>
<context position="5248" citStr="Schiehlen, 2004" startWordPosition="797" endWordPosition="798">uns, and adjectives (for details, see Versley, 2005) that allows to accurately determine the set of possible cases while keeping the size of the tagset relatively small. Verb Valence We use information from the lexicon of the WCDG parser for German (Foth and Menzel, 2006) to mark verbs according to the arguments that they can take. While the WCDG lexicon contains more information, we only encode the possibility of accusative and dative complements, ignoring entries for genitive or clausal complements. Markovization with Argument Marking It has been noted consistently (Klein and Manning, 2003; Schiehlen, 2004) that using markovization - replacing the original treebank rules by an approximation that only considers a limited context window of one or two siblings - improves results at least for a constituency-based evaluation. However, in some cases this simple markovization scheme leads to undesirable results including sentences with multiple subjects, as predicative arguments also have nominative case. To avoid this, we additionally mark which arguments have already been seen, yielding node labels such as S fin&lt;VVFIN a&lt;RNP a&lt;sa in the case of a partial constituent for a finite sentence (S fin) expan</context>
</contexts>
<marker>Schiehlen, 2004</marker>
<rawString>Schiehlen, M. (2004). Annotation strategies for probabilistic parsing in German. In Proc. Coling 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Efficient parsing of highly ambiguous context-free grammars with bit vectors.</title>
<date>2004</date>
<booktitle>In Proc. Coling</booktitle>
<contexts>
<context position="3220" citStr="Schmid, 2004" startWordPosition="481" endWordPosition="482"> and word clusters – shows that it is possible to achieve good improvements over generative lexicalized models by using the additional flexibility gained over standard lexicalized PCFG models. Our approach offers more flexibility than generative PCFG models, while computational costs for development and practical use are still acceptable. While we only present results for German, we are confident that the results carry over to other languages where annotated treebank grammars have been used successfully. 2 Parsing German with Morphology and Valence Information As a base parser, we use BitPar (Schmid, 2004), a fast unlexicalized PCFG parser based on a first pass where non-probabilistic bottom-up parsing and top-down filtering is carried out efficiently by storing the chart in bit vectors, and construct the probabilistic chart only after top-down filtering. We use an annotated treebank PCFG that is de134 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134–137, Paris, October 2009. c�2009 Association for Computational Linguistics rived from the Tiger treebank and largely inspired by earlier work on annotated treebank grammars for German (Schiehlen, 2004; Dube</context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>Schmid, H. (2004). Efficient parsing of highly ambiguous context-free grammars with bit vectors. In Proc. Coling 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
<author>A Fitschen</author>
<author>U Heid</author>
</authors>
<title>SMOR: A German computational morphology covering derivation, composition and inflection.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC</booktitle>
<contexts>
<context position="6534" citStr="Schmid et al., 2004" startWordPosition="1002" endWordPosition="1005">t (a) have already been seen. Unknown Words For the base PCFG parse, we use a decision tree with 43 regular expressions as features, five of which are tailored towards recognizing the past and zu-infinitive form of separable prefix verbs (abarbeiten ⇒ abgearbeitet, abzuarbeiten), which cannot be recognized by considering suffixes only. The extended part of speech tags for verbs (which contain valency information) are interpolated between the distribution at the concrete leaf of the decision tree and the global valency distribution for the (coarse) part-ofspeech tag. Additionally, we use SMOR (Schmid et al., 2004) in conjunction with the verb lexicon and a gazetteer list containing person and location names to determine possible fine-grained part-ofspeech tags for unknown words. Restoring Grammatical Functions Adding edge labels to the nodes in PCFG parsing easily creates sparse data problems, as reported by Rafferty and Manning (2008), who witness a drop in constituent F-measure (excluding grammatical function labels) when they include function labels in the symbols of their PCFG. On the other hand, the informativity of grammatical function labels for the contents of the node does not always justify t</context>
</contexts>
<marker>Schmid, Fitschen, Heid, 2004</marker>
<rawString>Schmid, H., Fitschen, A., and Heid, U. (2004). SMOR: A German computational morphology covering derivation, composition and inflection. In Proceedings of LREC 2004.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Steinbach</author>
<author>G Karypis</author>
<author>V Kumar</author>
</authors>
<title>A comparison of document clustering techniques.</title>
<date>2000</date>
<booktitle>In KDD Workshop on Text Mining.</booktitle>
<contexts>
<context position="13067" citStr="Steinbach et al., 2000" startWordPosition="2010" endWordPosition="2013">lowing features were gathered and used by weighting by the pointwise mutual information between the word and feature occurrences: The context feature retrieves windows of highfrequent words surrounding the word in question 136 (e.g. der mit for ‘der Mann mit den Blumen’). The context2 feature retrieves windows of one high-frequent word and one part-of-speech tag surrounding the word in question (e.g. der NN for ‘der sch¨one Mann’). The postag feature simply retrieves the part-ofspeech tag that is assigned to the word. The result of using the repeated bisecting k-means implementation of CLUTO (Steinbach et al., 2000) on the resulting features yields syntactically sensible clusters containing years, money sums, last names, or place names. 3.2 Unsupervised PP Attachment and Subject-Object preferences We used simple part-of-speech tag patterns to gather statistics on the association between nouns and immediately following prepositions, as well as between prepositions and closely following verbs on the DE-WaC corpus (Baroni and Kilgariff, 2006), an 1.7G words sample of the Germanlanguage WWW. The mutual information values for PP attachment are made available to the parser as features that are weighted by the </context>
</contexts>
<marker>Steinbach, Karypis, Kumar, 2000</marker>
<rawString>Steinbach, M., Karypis, G., and Kumar, V. (2000). A comparison of document clustering techniques. In KDD Workshop on Text Mining.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Versley</author>
</authors>
<title>Parser evaluation across text types.</title>
<date>2005</date>
<booktitle>In Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT</booktitle>
<contexts>
<context position="1646" citStr="Versley, 2005" startWordPosition="240" endWordPosition="241">integrating unsupervised PP attachment and POS-based word clusters into the parser. 1 Introduction To capture the semantic relations inherent in a text, parsing has to recover both structural information and grammatical functions, which commonly coincide in English, but not in freer word order languages such as German. Instead one has to make use of morphological features in addition to exploiting ordering preferences such as the (violatable) default ordering of (subject&lt;)dative&lt;accusative. Because of this fact, many successful approaches for German PCFG parsing (Schiehlen, 2004; Dubey, 2005; Versley, 2005) use annotated treebank grammars where the constituent trees from the treebank are enriched with further linguistic information that allows an adequate reconstruction of syntactic relationships, suggesting that probabilistic context-free grammars are an adequate tool for parsing these languages. In the ACL 2008 workshop on Parsing German (K¨ubler, 2008), Rafferty and Manning (2008) used a lexicalized PCFG parser using markovization and parent annotation, but no linguistically inspired transformations; Rafferty and Manning did quite well on constituents, but were not successful in reconstructin</context>
<context position="3843" citStr="Versley, 2005" startWordPosition="571" endWordPosition="573">unlexicalized PCFG parser based on a first pass where non-probabilistic bottom-up parsing and top-down filtering is carried out efficiently by storing the chart in bit vectors, and construct the probabilistic chart only after top-down filtering. We use an annotated treebank PCFG that is de134 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 134–137, Paris, October 2009. c�2009 Association for Computational Linguistics rived from the Tiger treebank and largely inspired by earlier work on annotated treebank grammars for German (Schiehlen, 2004; Dubey, 2005; Versley, 2005). Subcategorization With respect to the treebank grammar, we refine the node labels with linguistically important information that is only implicit in the treebank but would be tedious (and pointless) to annotate by hand: Firstly, we annotate NPs by case; clause nodes (S and VP) are subcategorized by the clause type (fin,inf,izu,rel), and NPs and PPs with a relative pronoun are marked. Comparative phrases (e.g., bigger [than a house], marked as NP in Tiger and T¨uBa-D/Z) are marked by adding a “CC” ending to the node label. Finally, auxiliaries are split according to their verb lemma into sein</context>
</contexts>
<marker>Versley, 2005</marker>
<rawString>Versley, Y. (2005). Parser evaluation across text types. In Proceedings of the Fourth Workshop on Treebanks and Linguistic Theories (TLT 2005).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>