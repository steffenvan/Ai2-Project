<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000012">
<title confidence="0.998434">
A Multi-Neuro Tagger Using Variable Lengths of Contexts
</title>
<author confidence="0.994438">
Qing Ma and Hitoshi Isahara
</author>
<affiliation confidence="0.9901605">
Communications Research Laboratory
Ministry of Posts and Telecommunications
</affiliation>
<address confidence="0.846139">
588-2, Iwaoka, Nishi-ku, Kobe, 651-2401, Japan
</address>
<email confidence="0.777569">
{qma, isahara}Ocrl.go.jp
</email>
<sectionHeader confidence="0.983198" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999855846153846">
This paper presents a multi-neuro tagger that
uses variable lengths of contexts and weighted
inputs (with information gains) for part of
speech tagging. Computer experiments show
that it has a correct rate of over 94% for tag-
ging ambiguous words when a small Thai corpus
with 22,311 ambiguous words is used for train-
ing. This result is better than any of the results
obtained using the single-neuro taggers with
fixed but different lengths of contexts, which
indicates that the multi-neuro tagger can dy-
namically find a suitable length of contexts in
tagging.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999965360655738">
Words are often ambiguous in terms of their
part of speech (POS). POS tagging disam-
biguates them, i.e., it assigns to each word the
correct POS in the context of the sentence.
Several kinds of POS taggers using rule-based
(e.g., Brill et al., 1990), statistical (e.g., Meri-
aldo, 1994), memory-based (e.g., Daelemans,
1996), and neural network (e.g., Schmid, 1994)
models have been proposed for some languages.
The correct rate of tagging of these models
has reached 95%, in part by using a very large
amount of training data (e.g., 1,000,000 words
in Schmid, 1994). For many other languages
(e.g., Thai, which we deal with in this paper),
however, the corpora have not been prepared
and there is not a large amount of training data
available. It is therefore important to construct
a practical tagger using as few training data as
possible.
In most of the statistical and neural network
models proposed so far, the length of the con-
texts used for tagging is fixed and has to be
selected empirically. In addition, all words in
the input are regarded to have the same rele-
vance in tagging. An ideal model would be one
in which the length of the contexts can be au-
tomatically selected as needed in tagging and
the words used in tagging can be given different
relevances. A simple but effective solution is to
introduce a multi-module tagger composed of
multiple modules (basic taggers) with fixed but
different lengths of contexts in the input and
a selector (a selecting rule) to obtain the final
answer. The tagger should also have a set of
weights reflecting the different relevances of the
input elements. If we construct such a multi-
module tagger with statistical methods (e.g., n-
gram models), however, the size of the n-gram
table would be extremely large, as mentioned in
Sec. 4.4. On the other hand, in memory-based
models such as IGtree (Daelemans, 1996), the
number of features used in tagging is actually
variable, within the maximum length (i.e., the
number of features spanning the tree), and the
different relevances of the different features are
taken into account in tagging. Tagging by this
approach, however, may be computationally ex-
pensive if the maximum length is large. Actu-
ally, the maximum length was set at 4 in Daele-
mans&apos;s model, which can therefore be regarded
as one using fixed length of contexts.
This paper presents a multi-neuro tagger
that is constructed using multiple neural net-
works, all of which can be regarded as single-
neuro taggers with fixed but different lengths of
contexts in inputs. The tagger performs POS
tagging in different lengths of contexts based on
longest context priority. Given that the target
word is more relevant than any of the words
in its context and that the words in context
may have different relevances in tagging, each
</bodyText>
<page confidence="0.99626">
802
</page>
<bodyText confidence="0.999844090909091">
element of the input is weighted with informa-
tion gains, i.e., numbers expressing the average
amount of reduction of training set informa-
tion entropy when the POSs of the element are
known (Quinlan 1993). By using the trained re-
sults (weights) of the single-neuro taggers with
short inputs as initial weights of those with long
inputs, the training time for the latter ones can
be greatly reduced and the cost to train a multi-
neuro tagger is almost the same as that to train
a single-neuro tagger.
</bodyText>
<sectionHeader confidence="0.970551" genericHeader="method">
2 POS Tagging Problems
</sectionHeader>
<bodyText confidence="0.999711333333333">
Since each input Thai text can be segmented
into individual words that can be further tagged
with all possible POSs using an electronic Thai
dictionary, the POS tagging tasks can be re-
garded as a kind of POS disambiguation prob-
lem using contexts as follows:
</bodyText>
<equation confidence="0.9997895">
I PT : (ipt_li, • • • ,ipt_li,ipt_t, , • • ,ipt_rr)
OPT : POS_t, (1)
</equation>
<bodyText confidence="0.873204428571429">
where ipt _t is the element related to the possible
POSs of the target word, • • ,ipt_li) and
, • • - ,ipt_rr) are the elements related to
the contexts, i.e., the POSs of the words to the
left and right of the target word, respectively,
and PO S _t is the correct POS of the target word
in the contexts.
</bodyText>
<sectionHeader confidence="0.998448" genericHeader="method">
3 Information Gain
</sectionHeader>
<bodyText confidence="0.997662888888889">
Suppose each element, ipt_x (x = li,t, or ri),
in (1) has a weight, w_x, which can be obtained
using information theory as follows. Let S be
the training set and Ci be the ith class, i.e.,
the ith POS (i = 1, ,n, where n is the total
number of POSs). The entropy of the set S,
i.e., the average amount of information needed
to identify the class (the POS) of an example in
5, is
</bodyText>
<equation confidence="0.715438">
n freg(Ci, S) f req(Ci, S)
x In(
in f o(S) = — E , ),
151 Is&apos;
</equation>
<bodyText confidence="0.994751142857143">
(2)
where ISI is the number of examples in S and
f reg(Ci, S) is the number of examples belong-
ing to class C. When S has been partitioned
to h subset Si (i = 1,- ,h) according to the
element ipt_x , the new entropy can be found as
the weighted sum over these subsets, or
</bodyText>
<equation confidence="0.5041785">
in fox(S) = —ISil x in f o(Si). (3)
i=i ISI
</equation>
<bodyText confidence="0.998828333333333">
Thus, the quantity of information gained by this
partitioning, or by knowing the POSs of element
ipt.x, can be obtained by
</bodyText>
<equation confidence="0.989647666666667">
gain(x) = in fo(S) — in f ox(S), (4)
which is used as the weight, W_X, i.e.,
w_x = gain(x). (5)
</equation>
<sectionHeader confidence="0.991571" genericHeader="method">
4 Multi-Neuro Tagger
</sectionHeader>
<subsectionHeader confidence="0.845468">
4.1 Single-Neuro Tagger
</subsectionHeader>
<bodyText confidence="0.997369285714286">
Figure 1 shows a single-neuro tagger (SNT)
which consists of a 3-layer feedforward neural
network. The SNT can disambiguate the POS
of each word using a fixed length of the con-
text by training it in a supervised manner with
a well-known error back-propagation algorithm
(for details see e.g., Haykin, 1994).
</bodyText>
<figure confidence="0.885204">
OPT
ipt_11 ipt_l, ipt_t ipt_rr
IPT
</figure>
<figureCaption confidence="0.999575">
Fig. 1. The single-neuro tagger (SNT).
</figureCaption>
<bodyText confidence="0.999704">
When word x is given in position y (y = t,
or ri), element ipt_y of input I PT is a weighted
pattern defined as
</bodyText>
<equation confidence="0.902507">
ipt _y = w_y • (exi , er2, • • lexn),
= (.41,1s2, • • &apos; Ixn) (6)
</equation>
<bodyText confidence="0.999653">
where w_y is the weight obtained in (5), n is
the total number of POSs defined in Thai, and
</bodyText>
<page confidence="0.946903">
1=1
803
</page>
<equation confidence="0.505865">
&apos;xi = w_y • esi (i = 1, • , n). If x is a known
word, i.e., it appears in the training data, each
bit exi is obtained as follows:
= Prob(POSilx). (7)
</equation>
<bodyText confidence="0.985381333333333">
Here the Prob(POSilx) is the prior probability
of POS i that the word x can be and is estimated
from the training data as
</bodyText>
<equation confidence="0.975908666666667">
IPOSi,x1
Prob(POSilx) = (8)
Ix&apos;
</equation>
<bodyText confidence="0.9984908">
where IPOSi,s1 is the number of times both
POS i and x appear and lx1 is the number of
times x appears in all the training data. If x is
an unknown word, i.e., it does not appear in the
training data, each bit ex.i is obtained as follows:
</bodyText>
<equation confidence="0.973302333333333">
-1—, if POSi is a candidate
exi = (9)
0, otherwise,
</equation>
<bodyText confidence="0.99831825">
where nx is the number of POSs that the word
x can be (this number can be simply obtained
from an electronic Thai dictionary). The OPT
is a pattern defined as follows:
</bodyText>
<equation confidence="0.995324">
OPT = (01,02,- • • ,°n). (10)
</equation>
<bodyText confidence="0.869972">
The OPT is decoded to obtain a final result
RST for the POS of the target word as follows:
RST ={POSi, if Oi = 1 Oi = 0 for j
Unknown. otherwise
</bodyText>
<equation confidence="0.554914">
(11)
</equation>
<bodyText confidence="0.9950978">
There is more information available for con-
structing the input for the words on the left be-
cause they have already been tagged. In the
tagging phase, instead of using (6)-(9), the in-
put may be constructed simply as follows:
</bodyText>
<equation confidence="0.999177">
ipt_li(t) = w_li • OPT(t — i), (12)
</equation>
<bodyText confidence="0.999557571428571">
where t is the position of the target word in a
sentence and i = 1,2, • • • ,l for t — i &gt; 0. How-
ever, in the training process the output of the
tagger is not correct and cannot be fed back to
the inputs directly. Instead, a weighted average
of the actual output and the desired output is
used as follows:
</bodyText>
<equation confidence="0.941875">
ipt_li(t)= w-li.(wopT•OPT(t—i)-EwDEs•DES),
(13)
</equation>
<bodyText confidence="0.341358">
where DES is the desired output
</bodyText>
<figure confidence="0.6114735">
DES = (Di, D2,- • ,Dn), (14)
whose bits are defined as follows:
=
1 1 if POS i is a desired answer
0. otherwise
(15)
and WopT and WDEs are respectively defined as
EOBJ
WOPT= (16)
E,AcT
and
WDES = 1 — WOPT, (17)
</figure>
<bodyText confidence="0.9690598">
where EcoEj and EAcT are the objective and
actual errors, respectively. Thus, the weighting
of the desired output is large at the beginning of
the training, and decreases to zero during train-
ing.
</bodyText>
<subsectionHeader confidence="0.897549">
4.2 Multi-Neuro Tagger
</subsectionHeader>
<figureCaption confidence="0.855311">
Figure 2 shows the structure of the multi-neuro
tagger. The individual SNT i has input IPTi
with length (the number of input elements: 1+
1+ r)1(IPTi), for which the following relations
hold: 1(IPTi) &lt;1(IPTi) for i &lt;j.
</figureCaption>
<table confidence="0.840344266666667">
IPT, OPT, RST,
IP T2 &gt;
IPT„, OPT2
OPT
tTl SNT cra
0 C.
1-1 0
,44
RST2
SNT2
1— RST„,
.4
1-.
I
SNT
</table>
<figureCaption confidence="0.996691">
Fig. 2. The multi-neuro tagger.
</figureCaption>
<bodyText confidence="0.99433625">
When a sequence of words (word_li, • • •,
word_li, word_t, word_ri, • • word_rr), which
has a target word word_t in the center and a
maximum length /(/PTrn ), is inputed, its subse-
quence of words, which also has the target word
word_t in the center and length 1(IPTi), will be
encoded into IPTi in the same way as described
in the previous section. The outputs OPTS (for
</bodyText>
<page confidence="0.993773">
804
</page>
<bodyText confidence="0.98297775">
i = 1, • , m) of the single-neuro taggers are de-
coded into RST i by (11). The RSTi are next
inputed into the longest-context-priority selec-
tor which obtains the final result as follows:
</bodyText>
<equation confidence="0.717323833333333">
RST, if RST, = Unknown
(for j &gt; i)
POSi ={
and RSTi 0 Unknown
Unknown. otherwise
(18)
</equation>
<bodyText confidence="0.99657475">
This means that the output of the single-neuro
tagger that gives a result being not unknown
and has the largest length of input is regarded
as a final answer.
</bodyText>
<subsectionHeader confidence="0.999776">
4.3 Training
</subsectionHeader>
<bodyText confidence="0.997268583333333">
If we use the weights trained by the single-neuro
taggers with short inputs as the initial values of
those with long inputs, the training time for the
latter ones can be greatly reduced and the cost
to train multi-neuro taggers would be almost
the same as that to train the single-neuro tag-
gers. Figure 3 shows an example of training a
tagger with four input elements. The trained
weights, w1 and w2, of the tagger with three
input elements are copied to the corresponding
part of the tagger and used as initial values for
its training.
</bodyText>
<figureCaption confidence="0.683693">
Fig. 3. How to train single-neuro tagger.
</figureCaption>
<subsectionHeader confidence="0.993929">
4.4 Features
</subsectionHeader>
<bodyText confidence="0.99655505882353">
Suppose that at most seven elements are
adopted in the inputs for tagging and that there
are 50 POSs. The n-gram models must es-
timate 507 = 7.8e + 11 n-grams, while the
single-neuro tagger with the longest input uses
only 70,000 weights, which can be calculated
nipt,
by nipt• nhid nh where id • nopt nhid, and
nopt are, respectively, the number of units in
the input, the hidden, and the output layers,
and nhid is set to be n2pt/2. That neuro models
require few parameters may offer another ad-
vantage: their performance is less affected by a
small amount of training data than that of the
statistical methods (Schmid, 1994). Neuro tag-
gers also offer fast tagging compared to other
models, although its training stage is longer.
</bodyText>
<sectionHeader confidence="0.988078" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999625628571429">
The Thai corpus used in the computer experi-
ments contains 10,452 sentences that are ran-
domly divided into two sets: one with 8,322
sentences for training and another with 2,130
sentences for testing. The training and test-
ing sets contain, respectively, 22,311 and 6,717
ambiguous words that serve as more than one
POS and were used for training and testing.
Because there are 47 types of POSs in Thai
(Charoenporn et al., 1997), n in (6), (10), and
(14) was set at 47. The single neuro-taggers
are 3-layer neural networks whose input length,
1(IPT) (.1+ 1+ r), is set to 3-7 and whose size
is p x x n, where p = n x 1(I PT). The multi-
neuro tagger is constructed by five (i.e., m = 5)
single-neuro taggers, SNTi (i = 1, • • • ,5), in
which 1(IPTi) = 2 -I- i.
Table 1 shows that no matter whether the
information gain (IG) was used or not, the
multi-neuro tagger has a correct rate of over
94%, which is higher than that of any of the
single-neuro taggers. This indicates that by us-
ing the multi-neuro tagger the length of the con-
text need not be chosen empirically; it can be
selected dynamically instead. If we focus on the
single-neuro taggers with inputs greater than
four, we can see that the taggers with informa-
tion gain are superior to those without informa-
tion gain. Note that the correct rates shown in
the table were obtained when only counting the
ambiguous words in the testing set. The correct
rate of the multi-neuro tagger is 98.9% if all the
words in the testing set (the ratio of ambigu-
ous words was 0.19) are counted. Moreover, al-
though the overall performance is not improved
</bodyText>
<page confidence="0.999038">
805
</page>
<tableCaption confidence="0.999755">
Table 1. Results of POS Tagging for Testing Data
</tableCaption>
<table confidence="0.82231">
Taggers &amp;quot;single-neuro&amp;quot; &amp;quot;multi-neuro&amp;quot;
l(IPTi) 3 4 5 6 7
with IG 0.915 0.920 0.929 0.930 0.933 0.943
without IG 0.924 0.927 0.922 0.926 0.926 0.941
</table>
<bodyText confidence="0.991671">
much by adopting the information gains, the
training can be greatly speeded up. It takes
1024 steps to train the first tagger, SNTI, when
the information gains are not, used and only 664
steps to train the same tagger when the infor-
mation gains are used.
Figure 4 shows learning (training) curves in
different cases for the single-neuro tagger with
six input elements. Thick line shows the case
in which the tagger is trained by using trained
weights of the tagger with five input elements as
initial values. The thin line shows the case in
which the tagger is trained independently. The
dashed line shows the case in which the tagger
is trained independently and does not use the
information gain. From this figure, we know
that the training time can be greatly reduced
by using the previous result and the information
gain.
</bodyText>
<figure confidence="0.987425714285714">
0.025
0.02
0.015
0.01
0.005
0 10 20 30 40 50 60 70 80 90 100
Number of learning steps
</figure>
<figureCaption confidence="0.999029">
Fig. 4. Learning curves.
</figureCaption>
<sectionHeader confidence="0.998813" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999967666666667">
This paper described a multi-neuro tagger that
uses variable lengths of contexts and weighted
inputs for part of speech tagging. Computer ex-
periments showed that the multi-neuro tagger
has a correct rate of over 94% for tagging am-
biguous words when a small Thai corpus with
22,311 ambiguous words is used for training.
This result is better than any of the results ob-
tamed by the single-neuro taggers, which indi-
cates that that the multi-neuro tagger can dy-
namically find suitable lengths of contexts for
tagging. The cost to train a multi-neuro tag-
ger was almost the same as that to train a
single-neuro tagger using new learning methods
in which the trained results (weights) of the pre-
vious taggers are used as initial weights for the
latter ones. It was also shown that while the
performance of tagging can be improved only
slightly, the training time can be greatly re-
duced by using information gain to weight input
elements.
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998132192307693">
Brill, E., Magerman, D., and Santorini, B.: De-
ducing linguistic structure from the statis-
tics of large corpora, Proc. DARPA Speech
and Natural Language Workshop, Hidden
Valley PA, pp. 275-282, 1990.
Charoenporn, T., Sornlertlamvanich, V., and
Isahara, H.: Building a large Thai text cor-
pus - part of speech tagged corpus: OR-
CHID, Proc. Natural Language Process-
ing Pacific Rim Symposium 1997, Thailand,
1997.
Daelemans, W., Zavrel, J., Berck, P., and Gillis,
S.: MBT: A memory-based part of speech
tagger-generator, Proc. 4th Workshop on
Very Large Corpora, Denmark, 1996.
Haykin, S.: Neural Networks, Macmillan Col-
lege Publishing Company, Inc., 1994.
Merialdo, B.: Tagging English text with a prob-
abilistic model, Computational Linguistics,
vol. 20, No. 2, pp. 155-171, 1994.
Quinlan, J.: C.4.5: Programs for Machine
Learning, San Mateo, CA: Morgan Kauf-
mann, 1993.
Schmid, H.: Part-of-speech tagging with neural
networks, Proc. Int. Conf. on Computa-
tional Linguistics, Japan, pp. 172-176, 1994.
</reference>
<bodyText confidence="0.459616">
—Learning using previous result
— Learning with IG
Learning without IG
</bodyText>
<page confidence="0.986069">
806
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.882151">
<title confidence="0.999176">A Multi-Neuro Tagger Using Variable Lengths of Contexts</title>
<author confidence="0.993252">Ma Isahara</author>
<affiliation confidence="0.9973515">Communications Research Laboratory Ministry of Posts and Telecommunications</affiliation>
<address confidence="0.99551">588-2, Iwaoka, Nishi-ku, Kobe, 651-2401, Japan</address>
<email confidence="0.981316">qmaOcrl.go.jp</email>
<email confidence="0.981316">isaharaOcrl.go.jp</email>
<abstract confidence="0.993552785714286">This paper presents a multi-neuro tagger that uses variable lengths of contexts and weighted inputs (with information gains) for part of speech tagging. Computer experiments show it has a correct rate of over tagging ambiguous words when a small Thai corpus with 22,311 ambiguous words is used for training. This result is better than any of the results obtained using the single-neuro taggers with fixed but different lengths of contexts, which indicates that the multi-neuro tagger can dynamically find a suitable length of contexts in tagging.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>E Brill</author>
<author>D Magerman</author>
<author>B Santorini</author>
</authors>
<title>Deducing linguistic structure from the statistics of large corpora,</title>
<date>1990</date>
<booktitle>Proc. DARPA Speech and Natural Language Workshop,</booktitle>
<pages>275--282</pages>
<location>Hidden Valley PA,</location>
<contexts>
<context position="1064" citStr="Brill et al., 1990" startWordPosition="164" endWordPosition="167">e of over 94% for tagging ambiguous words when a small Thai corpus with 22,311 ambiguous words is used for training. This result is better than any of the results obtained using the single-neuro taggers with fixed but different lengths of contexts, which indicates that the multi-neuro tagger can dynamically find a suitable length of contexts in tagging. 1 Introduction Words are often ambiguous in terms of their part of speech (POS). POS tagging disambiguates them, i.e., it assigns to each word the correct POS in the context of the sentence. Several kinds of POS taggers using rule-based (e.g., Brill et al., 1990), statistical (e.g., Merialdo, 1994), memory-based (e.g., Daelemans, 1996), and neural network (e.g., Schmid, 1994) models have been proposed for some languages. The correct rate of tagging of these models has reached 95%, in part by using a very large amount of training data (e.g., 1,000,000 words in Schmid, 1994). For many other languages (e.g., Thai, which we deal with in this paper), however, the corpora have not been prepared and there is not a large amount of training data available. It is therefore important to construct a practical tagger using as few training data as possible. In most</context>
</contexts>
<marker>Brill, Magerman, Santorini, 1990</marker>
<rawString>Brill, E., Magerman, D., and Santorini, B.: Deducing linguistic structure from the statistics of large corpora, Proc. DARPA Speech and Natural Language Workshop, Hidden Valley PA, pp. 275-282, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Charoenporn</author>
<author>V Sornlertlamvanich</author>
<author>H Isahara</author>
</authors>
<title>Building a large Thai text corpus - part of speech tagged corpus: ORCHID,</title>
<date>1997</date>
<booktitle>Proc. Natural Language Processing Pacific Rim Symposium</booktitle>
<contexts>
<context position="11411" citStr="Charoenporn et al., 1997" startWordPosition="2077" endWordPosition="2080">t of training data than that of the statistical methods (Schmid, 1994). Neuro taggers also offer fast tagging compared to other models, although its training stage is longer. 5 Experimental Results The Thai corpus used in the computer experiments contains 10,452 sentences that are randomly divided into two sets: one with 8,322 sentences for training and another with 2,130 sentences for testing. The training and testing sets contain, respectively, 22,311 and 6,717 ambiguous words that serve as more than one POS and were used for training and testing. Because there are 47 types of POSs in Thai (Charoenporn et al., 1997), n in (6), (10), and (14) was set at 47. The single neuro-taggers are 3-layer neural networks whose input length, 1(IPT) (.1+ 1+ r), is set to 3-7 and whose size is p x x n, where p = n x 1(I PT). The multineuro tagger is constructed by five (i.e., m = 5) single-neuro taggers, SNTi (i = 1, • • • ,5), in which 1(IPTi) = 2 -I- i. Table 1 shows that no matter whether the information gain (IG) was used or not, the multi-neuro tagger has a correct rate of over 94%, which is higher than that of any of the single-neuro taggers. This indicates that by using the multi-neuro tagger the length of the co</context>
</contexts>
<marker>Charoenporn, Sornlertlamvanich, Isahara, 1997</marker>
<rawString>Charoenporn, T., Sornlertlamvanich, V., and Isahara, H.: Building a large Thai text corpus - part of speech tagged corpus: ORCHID, Proc. Natural Language Processing Pacific Rim Symposium 1997, Thailand, 1997.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A memory-based part of speech tagger-generator,</title>
<date>1996</date>
<booktitle>Proc. 4th Workshop on Very Large Corpora,</booktitle>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>Daelemans, W., Zavrel, J., Berck, P., and Gillis, S.: MBT: A memory-based part of speech tagger-generator, Proc. 4th Workshop on Very Large Corpora, Denmark, 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Haykin</author>
</authors>
<title>Neural Networks,</title>
<date>1994</date>
<publisher>Macmillan College Publishing Company, Inc.,</publisher>
<contexts>
<context position="6100" citStr="Haykin, 1994" startWordPosition="1074" endWordPosition="1075">n fox(S) = —ISil x in f o(Si). (3) i=i ISI Thus, the quantity of information gained by this partitioning, or by knowing the POSs of element ipt.x, can be obtained by gain(x) = in fo(S) — in f ox(S), (4) which is used as the weight, W_X, i.e., w_x = gain(x). (5) 4 Multi-Neuro Tagger 4.1 Single-Neuro Tagger Figure 1 shows a single-neuro tagger (SNT) which consists of a 3-layer feedforward neural network. The SNT can disambiguate the POS of each word using a fixed length of the context by training it in a supervised manner with a well-known error back-propagation algorithm (for details see e.g., Haykin, 1994). OPT ipt_11 ipt_l, ipt_t ipt_rr IPT Fig. 1. The single-neuro tagger (SNT). When word x is given in position y (y = t, or ri), element ipt_y of input I PT is a weighted pattern defined as ipt _y = w_y • (exi , er2, • • lexn), = (.41,1s2, • • &apos; Ixn) (6) where w_y is the weight obtained in (5), n is the total number of POSs defined in Thai, and 1=1 803 &apos;xi = w_y • esi (i = 1, • , n). If x is a known word, i.e., it appears in the training data, each bit exi is obtained as follows: = Prob(POSilx). (7) Here the Prob(POSilx) is the prior probability of POS i that the word x can be and is estimated f</context>
</contexts>
<marker>Haykin, 1994</marker>
<rawString>Haykin, S.: Neural Networks, Macmillan College Publishing Company, Inc., 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English text with a probabilistic model,</title>
<date>1994</date>
<journal>Computational Linguistics,</journal>
<volume>20</volume>
<pages>155--171</pages>
<contexts>
<context position="1100" citStr="Merialdo, 1994" startWordPosition="170" endWordPosition="172">s when a small Thai corpus with 22,311 ambiguous words is used for training. This result is better than any of the results obtained using the single-neuro taggers with fixed but different lengths of contexts, which indicates that the multi-neuro tagger can dynamically find a suitable length of contexts in tagging. 1 Introduction Words are often ambiguous in terms of their part of speech (POS). POS tagging disambiguates them, i.e., it assigns to each word the correct POS in the context of the sentence. Several kinds of POS taggers using rule-based (e.g., Brill et al., 1990), statistical (e.g., Merialdo, 1994), memory-based (e.g., Daelemans, 1996), and neural network (e.g., Schmid, 1994) models have been proposed for some languages. The correct rate of tagging of these models has reached 95%, in part by using a very large amount of training data (e.g., 1,000,000 words in Schmid, 1994). For many other languages (e.g., Thai, which we deal with in this paper), however, the corpora have not been prepared and there is not a large amount of training data available. It is therefore important to construct a practical tagger using as few training data as possible. In most of the statistical and neural netwo</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>Merialdo, B.: Tagging English text with a probabilistic model, Computational Linguistics, vol. 20, No. 2, pp. 155-171, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Quinlan</author>
</authors>
<title>C.4.5: Programs for Machine Learning,</title>
<date>1993</date>
<publisher>Morgan Kaufmann,</publisher>
<location>San Mateo, CA:</location>
<contexts>
<context position="3806" citStr="Quinlan 1993" startWordPosition="629" endWordPosition="630">tructed using multiple neural networks, all of which can be regarded as singleneuro taggers with fixed but different lengths of contexts in inputs. The tagger performs POS tagging in different lengths of contexts based on longest context priority. Given that the target word is more relevant than any of the words in its context and that the words in context may have different relevances in tagging, each 802 element of the input is weighted with information gains, i.e., numbers expressing the average amount of reduction of training set information entropy when the POSs of the element are known (Quinlan 1993). By using the trained results (weights) of the single-neuro taggers with short inputs as initial weights of those with long inputs, the training time for the latter ones can be greatly reduced and the cost to train a multineuro tagger is almost the same as that to train a single-neuro tagger. 2 POS Tagging Problems Since each input Thai text can be segmented into individual words that can be further tagged with all possible POSs using an electronic Thai dictionary, the POS tagging tasks can be regarded as a kind of POS disambiguation problem using contexts as follows: I PT : (ipt_li, • • • ,i</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>Quinlan, J.: C.4.5: Programs for Machine Learning, San Mateo, CA: Morgan Kaufmann, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Part-of-speech tagging with neural networks,</title>
<date>1994</date>
<booktitle>Proc. Int. Conf. on Computational Linguistics, Japan,</booktitle>
<pages>172--176</pages>
<contexts>
<context position="1179" citStr="Schmid, 1994" startWordPosition="181" endWordPosition="182"> result is better than any of the results obtained using the single-neuro taggers with fixed but different lengths of contexts, which indicates that the multi-neuro tagger can dynamically find a suitable length of contexts in tagging. 1 Introduction Words are often ambiguous in terms of their part of speech (POS). POS tagging disambiguates them, i.e., it assigns to each word the correct POS in the context of the sentence. Several kinds of POS taggers using rule-based (e.g., Brill et al., 1990), statistical (e.g., Merialdo, 1994), memory-based (e.g., Daelemans, 1996), and neural network (e.g., Schmid, 1994) models have been proposed for some languages. The correct rate of tagging of these models has reached 95%, in part by using a very large amount of training data (e.g., 1,000,000 words in Schmid, 1994). For many other languages (e.g., Thai, which we deal with in this paper), however, the corpora have not been prepared and there is not a large amount of training data available. It is therefore important to construct a practical tagger using as few training data as possible. In most of the statistical and neural network models proposed so far, the length of the contexts used for tagging is fixed</context>
<context position="10856" citStr="Schmid, 1994" startWordPosition="1986" endWordPosition="1987"> at most seven elements are adopted in the inputs for tagging and that there are 50 POSs. The n-gram models must estimate 507 = 7.8e + 11 n-grams, while the single-neuro tagger with the longest input uses only 70,000 weights, which can be calculated nipt, by nipt• nhid nh where id • nopt nhid, and nopt are, respectively, the number of units in the input, the hidden, and the output layers, and nhid is set to be n2pt/2. That neuro models require few parameters may offer another advantage: their performance is less affected by a small amount of training data than that of the statistical methods (Schmid, 1994). Neuro taggers also offer fast tagging compared to other models, although its training stage is longer. 5 Experimental Results The Thai corpus used in the computer experiments contains 10,452 sentences that are randomly divided into two sets: one with 8,322 sentences for training and another with 2,130 sentences for testing. The training and testing sets contain, respectively, 22,311 and 6,717 ambiguous words that serve as more than one POS and were used for training and testing. Because there are 47 types of POSs in Thai (Charoenporn et al., 1997), n in (6), (10), and (14) was set at 47. The</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Schmid, H.: Part-of-speech tagging with neural networks, Proc. Int. Conf. on Computational Linguistics, Japan, pp. 172-176, 1994.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>