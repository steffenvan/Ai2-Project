<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.952266">
Efficient, correct, unsupervised learning of context-sensitive languages
</title>
<author confidence="0.989629">
Alexander Clark
</author>
<affiliation confidence="0.9837975">
Department of Computer Science
Royal Holloway, University of London
</affiliation>
<email confidence="0.986456">
alexc@cs.rhul.ac.uk
</email>
<sectionHeader confidence="0.994541" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999951625">
A central problem for NLP is grammar in-
duction: the development of unsupervised
learning algorithms for syntax. In this pa-
per we present a lattice-theoretic represen-
tation for natural language syntax, called
Distributional Lattice Grammars. These
representations are objective or empiri-
cist, based on a generalisation of distribu-
tional learning, and are capable of repre-
senting all regular languages, some but not
all context-free languages and some non-
context-free languages. We present a sim-
ple algorithm for learning these grammars
together with a complete self-contained
proof of the correctness and efficiency of
the algorithm.
</bodyText>
<sectionHeader confidence="0.998783" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.99993820967742">
Grammar induction, or unsupervised learning of
syntax, no longer requires extensive justification
and motivation. Both from engineering and cog-
nitive/linguistic angles, it is a central challenge
for computational linguistics. However good al-
gorithms for this task are thin on the ground.
There are numerous heuristic algorithms, some of
which have had significant success in inducing
constituent structure (Klein and Manning, 2004).
There are algorithms with theoretical guarantees
as to their correctness – such as for example
Bayesian algorithms for inducing PCFGs (John-
son, 2008), but such algorithms are inefficient: an
exponential search algorithm is hidden in the con-
vergence of the MCMC samplers. The efficient
algorithms that are actually used are heuristic ap-
proximations to the true posteriors. There are al-
gorithms like the Inside-Outside algorithm (Lari
and Young, 1990) which are guaranteed to con-
verge efficiently, but not necessarily to the right
answer: they converge to a local optimum that
may be, and in practice nearly always is very far
from the optimum. There are naive enumerative
algorithms that are correct, but involve exhaus-
tively enumerating all representations below a cer-
tain size (Horning, 1969). There are no correct
and efficient algorithms, as there are for parsing,
for example.
There is a reason for this: from a formal point
of view, the problem is intractably hard for the
standard representations in the Chomsky hierar-
chy. Abe and Warmuth (1992) showed that train-
ing stochastic regular grammars is hard; Angluin
and Kharitonov (1995) showed that regular gram-
mars cannot be learned even using queries; these
results obviously apply also to PCFGs and CFGs
as well as to the more complex representations
built by extending CFGs, such as TAGs and so
on. However, these results do not necessarily ap-
ply to other representations. Regular grammars
are not learnable, but deterministic finite automata
are learnable under various paradigms (Angluin,
1987). Thus it is possible to learn by changing to
representations that have better properties: in par-
ticular DFAs are learnable because they are “ob-
jective”; there is a correspondence between the
structure of the language, (the residual languages)
and the representational primitives of the formal-
ism (the states) which is expressed by the Myhill-
Nerode theorem.
In this paper we study the learnability of a class
of representations that we call distributional lat-
tice grammars (DLGs). Lattice-based formalisms
were introduced by Clark et al. (2008) and Clark
(2009) as context sensitive formalisms that are po-
tentially learnable. Clark et al. (2008) established
a similar learnability result for a limited class of
context free languages. In Clark (2009), the ap-
proach was extended to a significantly larger class
but without an explicit learning algorithm. Most of
the building blocks are however in place, though
we need to make several modifications and ex-
</bodyText>
<page confidence="0.983055">
28
</page>
<note confidence="0.9552385">
Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 28–37,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.99986404">
tensions to get a clean result. Most importantly,
we need to replace the representation used there,
which naively could be exponential, with a lazy,
exemplar based model.
In this paper we present a simple algorithm
for the inference of these representations and
prove its correctness under the following learning
paradigm: we assume that as normal there is a sup-
ply of positive examples, and additionally that the
learner can query whether a string is in the lan-
guage or not (an oracle for membership queries).
We also prove that the algorithm is efficient in
the sense that it will use a polynomial amount of
computation and makes a polynomial number of
queries at each step.
The contributions of this paper are as follows:
after some basic discussion of distributional learn-
ing in Section 2, we define in Section 3 an
exemplar-based grammatical formalism which we
call Distributional Lattice Grammars. We then
give a learning algorithm under a reasonable learn-
ing paradigm, together with a self contained proof
in elementary terms (not presupposing any exten-
sive knowledge of lattice theory), of the correct-
ness of this algorithm.
</bodyText>
<sectionHeader confidence="0.969451" genericHeader="method">
2 Basic definitions
</sectionHeader>
<bodyText confidence="0.997920745098039">
We now define our notation; we have a finite al-
phabet E; let E* be the set of all strings (the free
monoid) over E, with A the empty string. A (for-
mal) language is a subset of E*. We can concate-
nate two languages A and B to get AB = {uv|u E
A, b E B}.
A context or environment, as it is called in struc-
turalist linguistics, is just an ordered pair of strings
that we write (l, r) where l and r refer to left and
right; l and r can be of any length. We can com-
bine a context (l, r) with a string u with a wrap-
ping operation that we write O: so (l, r) O u is
defined to be lur. We will sometimes write f for
a context (l, r). There is a special context (A, A):
(A, A) O w = w. We will extend this to sets of
contexts and sets of strings in the natural way. We
will write Sub(w) = {u|](l, r) : lur = w} for
the set of substrings of a string, and Con(w) =
{(l,r)|]u E E* : lur = w}.
For a given string w we can define the distribu-
tion of that string to be the set of all contexts that it
can appear in: CL(w) = {(l, r)|lwr E L}, equiv-
alently {f|f O w E L}. Clearly (A, A) E CL(w)
iff w E L.
Distributional learning (Harris, 1954) as a tech-
nical term refers to learning techniques which
model directly or indirectly properties of the dis-
tribution of strings or words in a corpus or a lan-
guage. There are a number of reasons to take
distributional learning seriously: first, historically,
CFGs and other PSG formalisms were intended
to be learnable by distributional means. Chomsky
(2006) says (p. 172, footnote 15):
The concept of “phrase structure
grammar” was explicitly designed to ex-
press the richest system that could rea-
sonably be expected to result from the
application of Harris-type procedures to
a corpus.
Second, empirically we know they work well at
least for lexical induction, (Sch¨utze, 1993; Cur-
ran, 2003) and are a component of some imple-
mented unsupervised learning systems (Klein and
Manning, 2001). Linguists use them as one of the
key tests for constituent structure (Carnie, 2008),
and finally there is some psycholinguistic evidence
that children are sensitive to distributional struc-
ture, at least in artificial grammar learning tasks
(Saffran et al., 1996). These arguments together
suggest that distributional learning has a some-
what privileged status.
</bodyText>
<sectionHeader confidence="0.967106" genericHeader="method">
3 Lattice grammars
</sectionHeader>
<bodyText confidence="0.9994535">
Clark (2009) presents the theory of lattice based
formalisms starting algebraically from the theory
of residuated lattices. Here we will largely ig-
nore this, and start from a straightforward com-
putational treatment. We start by defining the rep-
resentation.
</bodyText>
<construct confidence="0.902336166666667">
Definition 1. Given a non-empty finite alphabet,
E, a distributional lattice grammar (DLG) is a 3-
tuple consisting of (K, D, F), where F is a finite
subset of E* x E*, such that (A, A) E F, K is a
finite subset of E* which contains A and E, and D
is a subset of (F O KK).
</construct>
<bodyText confidence="0.998979222222222">
K here can be thought of as a finite set of exem-
plars, which correspond to substrings or fragments
of the language. F is a set of contexts or fea-
tures, that we will use to define the distributional
properties of these exemplars; finally D is a set
of grammatical strings, the data; a finite subset of
the language. F OKK using the notation above is
{luvr|u, v E K, (l, r) E F}. This is the finite part
of the language that we examine. If the language
</bodyText>
<page confidence="0.995946">
29
</page>
<bodyText confidence="0.966125478260869">
we are modeling is L, then D = L ∩ (F (D KK).
Since λ ∈ K, K ⊆ KK.
We define a concept to be an ordered pair hS, Ci
where S ⊆ K and C ⊆ F, which satisfies the fol-
lowing two conditions: first C O S ⊆ D; that is
to say every string in S can be combined with any
context in C to give a grammatical string, and sec-
ondly they are maximal in that neither K nor F
can be increased without violating the first condi-
tion.
We define B(K, D, F) to be the set of all such
concepts. We use the B symbol (Begriff) to bring
out the links to Formal Concept Analysis (Ganter
and Wille, 1997; Davey and Priestley, 2002). This
lattice may contain exponentially many concepts,
but it is clearly finite, as the number of concepts is
less than min(2|F|, 2|K|).
There is an obvious partial order defined by
hS1, C1i ≤ hS2, C2i iff S1 ⊆ S2, Note that
S1 ⊆ S2 iff C2 ⊆ C1.
Given a set of strings S we can define a set of
contexts S0 to be the set of contexts that appear
with every element of S.
</bodyText>
<equation confidence="0.70885">
S0 = {(l,r) ∈ F : ∀w ∈ S, lwr ∈ D}
Dually we can define for a set of contexts C the
set of strings C0 that occur with all of the elements
of C:
C0 = {w ∈ K : ∀(l,r) ∈ C, lwr ∈ D}
</equation>
<bodyText confidence="0.984437">
The concepts hS, Ci are just the pairs that sat-
isfy S0 = C and C0 = S; the two maps denoted
by 0 are called the polar maps. For any S ⊆ K,
S000 = S0 and for any C ⊆ F, C000 = C0. Thus we
can form a concept from any set of strings S ⊆ K
by taking hS00, S0i; this is a concept as S000 = S0.
We will write this as C(S), and for any C ⊆ F,
we will write C(C) = hC0, C00i.
If S ⊆ T then T0 ⊆ S0, and S00 ⊆ T00. For any
set of strings S ⊆ K, S ⊆ S00.
One crucial concept here is the concept de-
fined by (λ, λ) or equivalently by the set K ∩ D
which corresponds to all of the elements in the
language. We will denote this concept by L =
C({(λ, λ)}) = C(K ∩ D).
We also define a meet operation by
hS1, C1i ∧ hS2, C2i = hS1 ∩ S2, (S1 ∩ S2)0i
This is the greatest lower bound of the two con-
cepts; this is a concept since if S001 = S1 and
S002 = S2 then (S1 ∩ S2)00 = (S1 ∩ S2). Note that
this operation is associative and commutative. We
can also define a join operation dually; with these
operations B(K, D, D) is a complete lattice.
So far we have only used strings in F O K; we
now define a concatenation operation as follows.
hS1, C1i ◦ hS2, C2i = h(S1S2)00, (S1S2)000i
Since S1 and S2 are subsets of K, S1S2 is a sub-
set of KK, but not necessarily of K. (S1S2)0 is
the set of contexts shared by all elements of S1S2
and (S1S2)00 is the subset of K, not KK, that
has all of the contexts of (S1S2)0. (S1S2)000 might
be larger than (S1S2)0. We can also write this as
C((S1S2)0).
Both ∧ and ◦ are monotonic in the sense that if
X ≤ Y then X ◦ Z ≤ Y ◦ Z, Z ◦ X ≤ Z ◦ Y
and X ∧ Z ≤ Y ∧ Z. Note that all of these oper-
ations can be computed efficiently; using a perfect
hash, and a naive algorithm, we can do the polar
maps and ∧ operations in time O(|K||F |), and the
concatenation in time O(|K|2|F|).
We now define the notion of derivation in this
representation. Given a string w we recursively
compute a concept for every substring of w; this
concept will approximate the distribution of the
string. We define φG as a function from E∗ to
B(K, D, F); we define it recursively:
</bodyText>
<listItem confidence="0.999918">
• If |w |≤ 1, then φG(w) = h{w}00, {w}0i
• If |w |&gt; 1 then
</listItem>
<equation confidence="0.936434">
φG(w) = / \u,v∈E+:uv=w φG(u) ◦ φG(v)
</equation>
<bodyText confidence="0.999830833333333">
The first step is well defined because all of the
strings of length at most 1 are already in K so
we can look them up directly. To clarify the sec-
ond step, if w = abc then φG(abc) = φG(a) ◦
φG(bc) ∧ φG(ab) ◦ φG(c); we compute the string
from all possible non-trivial splits of the string
into a prefix and a suffix. By using a dynamic
programming table that stores the values of φ(u)
for all u ∈ Sub(w) we can compute this in time
O(|K|2|F||w|3); this is just an elementary variant
of the CKY algorithm. We define the language de-
fined by the DLG G to be
</bodyText>
<equation confidence="0.975071">
L(G) = {w|φG(w) ≤ C({(λ, λ)})}
</equation>
<bodyText confidence="0.998768333333333">
That is to say, a string is in the language if we
predict that a string has the context (λ, λ). We now
consider a trivial example: the Dyck language.
</bodyText>
<page confidence="0.996212">
30
</page>
<construct confidence="0.9827895">
Example 1. Let L be the Dyck language (matched
parenthesis language) over E = {a, b}, where
a corresponds to open bracket, and b to close
bracket. Define
</construct>
<listItem confidence="0.997212">
• K = {A, a, b, ab}
• F = {(A, A), (A, b), (a, A)}.
• D = {A, ab, abab, aabb}
</listItem>
<bodyText confidence="0.795301">
G = hK, D, Fi is a DLG. We will now write down
the 5 elements of the lattice:
</bodyText>
<listItem confidence="0.851644666666667">
• &gt; = hK,∅i
• ⊥ = h∅, Fi
• L = h{A, ab}, {(A, A)}i
•
A = h{a}, {(A, b)}i
• B = h{b}, {(a, A)}i
</listItem>
<bodyText confidence="0.917873166666667">
To compute the concatenation A ◦ B we first
compute {a}{b} = {ab}; we then compute {ab}&apos;
which is {(A, A)}, and {(A, A)}&apos; = {A, ab}, so
A ◦ B = L. Similarly to compute L ◦ L, we first
take {A, ab}{A, ab} = {A, ab, abab}. These all
have the context (A, A), so the result is the con-
cept L. If we compute A ◦ A we get {a}{a} which
is {aa} which has no contexts so the result is &gt;.
We have OG(A) = L, OG(a) = A, OG(b) = B.
Applying the recursive computation we can verify
that OG(w) = L iff w ∈ L and so L(G) = L. We
can also see that D = L ∩ (F (D KK).
</bodyText>
<sectionHeader confidence="0.985224" genericHeader="method">
4 Search
</sectionHeader>
<bodyText confidence="0.99992780952381">
In order to learn these grammars we need to find a
suitable set of contexts F, a suitable set of strings
K, and then work out which elements of F O KK
are grammatical. So given a choice for K and F
it is easy to learn these models under a suitable
regime: the details of how we collect information
about D depend on the learning model.
The question is therefore whether it is easy
to find suitable sets, K and F. Because of the
way the formalism is designed, it transpires that
the search problem is entirely tractable. In or-
der to analyse the search space, we define two
maps between the lattices as K and F are in-
creased. We are going to augment our notation
slightly; we will write B(K, L, F) for B(K, L ∩
(F(DKK), F) and similarly hK, L, Fi for hK, L∩
(F O KK), Fi. When we use the two polar maps
(such as C&apos;, S&apos;), though we are dealing with more
than one lattice, there is no ambiguity as the maps
agree; we will when necessary explicitly restrict
the output (e.g. C&apos; ∩ J) to avoid confusion.
</bodyText>
<construct confidence="0.886028125">
Definition 2. For any language L and any set of
contexts F ⊆ G, and any sets of strings J ⊆
K ⊆ E*. We define a map g from B(J, L, F) to
B(K, L, F) (from the smaller lattice to the larger
lattice) as g(hS, Ci) = hC&apos;, Ci.
We also define a map f from B(K, L, G)
to B(K, L, F), (from larger to smaller) as
f(hS, Ci) = h(C ∩ F)&apos;, C ∩ Fi.
</construct>
<bodyText confidence="0.9997975">
These two maps are defined in opposite direc-
tions: this is because of the duality of the lattice.
By defining them in this way, as we will see, we
can prove that these two maps have very similar
properties. We can verify that the outputs of these
maps are in fact concepts.
We now need to define two monotonicity lem-
mas: these lemmas are crucial to the success of
the formalism. We show that as we increase K
the language defined by the formalism decreases
monotonically, and that as we increase F the lan-
guage increases monotonically. There is some du-
plication in the proofs of the two lemmas; we
could prove them both from more abstract prop-
erties of the maps f, g which are what are called
residual maps, but we will do it directly.
</bodyText>
<construct confidence="0.997732">
Lemma 1. Given two lattices B(K, L, F) and
B(K, L, G) where F ⊆ G; For all X, Y ∈
B(K, L, G) we have that
</construct>
<listItem confidence="0.9950125">
1. f(X) ◦ f(Y ) ≥ f(X ◦ Y )
2. f(X) ∧ f(Y ) ≥ f(X ∧ Y )
</listItem>
<bodyText confidence="0.840991411764706">
Proof. The proof is elementary but difficult to
read. We write X = hSX, CXi and similarly for
Y . For part 1 of the lemma: Clearly (S&apos;X ∩ F) ⊆
S&apos;X, so (S&apos;X ∩ F)&apos; ⊇ S&apos;&apos;X = SX and the same for
SY . So (S&apos;X ∩ F)&apos;(S&apos;Y ∩ F)&apos; ⊇ SXSY (as subsets
of KK). So ((S&apos;X ∩F)&apos;(S&apos;Y ∩F)&apos;)&apos; ⊆ (SXSY)&apos; ⊆
(SXSY )&apos;&apos;&apos;.
C(Z) where Z = ((S&apos;X ∩ F)&apos;(S&apos;Y ∩ F)&apos;)&apos; ∩ F and
f(X ◦ Y ) has the set of contexts ((SXSY )&apos;&apos;&apos; ∩ F).
Therefore f(X ◦ Y ) has a bigger set of contexts
than f(X) ◦ f(Y ) and is thus a smaller concept.
For part 2: by definition f(X ∧ Y ) = h((SX ∩
Sy)&apos; ∩ F)&apos;, (SX ∩ Sy)&apos; ∩ Fi and f(X) ∧ f(Y ) =
h(S&apos;X∩F)&apos;∩(S&apos;y∩F)&apos;, ((S&apos;X∩F)&apos;∩(S&apos;y∩F)&apos;)&apos;∩Fi
Now S&apos;X ∩ F ⊆ S&apos;X, so (since S&apos;&apos;X = SX) SX ⊆
(S&apos;X∩F)&apos;, and so SX∩Sy ⊆ (S&apos;X∩F)&apos;∩(S&apos;y∩F)&apos;.
Now by definition, f(X) ◦ f(Y ) is
</bodyText>
<page confidence="0.998943">
31
</page>
<bodyText confidence="0.975573">
So (SX ∩ Sy)&apos; ⊇ ((S&apos;X ∩ F)&apos; ∩ (S&apos;y ∩ F)&apos;) which
gives the result by comparing the context sets of
the two sides of the inequality.
</bodyText>
<construct confidence="0.865275">
Lemma 2. For any language L, and two sets of
contexts F ⊆ G, and any K, if we have two DLGs
hK, L, Fi with map φF : E* → B(K, L, F) and
hK, L, Gi with map φG : E* → B(K, L, G) then
for all w, f(φG(w)) ≤ φF(w).
Proof. By induction on the length of w; clearly
if |w |≤ 1, f(φG(w)) = φF(w). We now take
</construct>
<bodyText confidence="0.6594495">
the inductive step; by definition, (suppressing the
definition of u, v in the meet)
</bodyText>
<equation confidence="0.988873">
f(φG(w)) = f( ^ φG(u) ◦ φG(v))
u,v
By Lemma 1, part 2:
f(φG(w)) ≤ ^ f(φG(u) ◦ φG(v))
u,v
By Lemma 1, part 1:
f(φG(w)) ≤ ^ f(φG(u)) ◦ f(φG(v))
u,v
</equation>
<bodyText confidence="0.87607104">
Proof. For the first part: Write X = hSX, CXi as
before. Note that SX = C&apos;X ∩ J . SX ⊆ C&apos;X, so
SXSY ⊆ C&apos;XC&apos;Y , and so (SXSY )&apos;&apos; ⊆ (C&apos;XC&apos;Y )&apos;&apos;,
and ((SXSY )&apos;&apos; ∩ J)&apos; ⊇ (C&apos;XC&apos;Y )&apos;&apos;&apos;. By calcu-
lation g(X) ◦ g(Y ) = h(C&apos;XC&apos;Y )&apos;&apos;, (C&apos;XC&apos;Y )&apos;&apos;&apos;i
On the other hand, g(X ◦ Y ) = h((SXSY )&apos;&apos; ∩
J)&apos;&apos;, ((SXSY )&apos;&apos; ∩ J)&apos;i and so g(X ◦ Y ) is smaller
as it has a larger set of contexts.
For the second part: g(X) ∧ g(Y ) = hC&apos;X ∩
C&apos;Y , (C&apos;X ∩ C&apos;Y )&apos;i and g(X ∧ Y ) = h(SX ∩
SY )&apos;&apos;, (SX ∩ SY )&apos;i. Since SX = C&apos;X ∩ J, SX ⊆
C&apos;X, so (SX ∩ SY ) ⊆ C&apos;X ∩ C&apos;Y , and therefore
(SX ∩ SY )&apos;&apos; ⊆ (C&apos;X ∩ C&apos;Y )&apos;&apos; = C&apos;X ∩ C&apos;Y .
We now state and prove the monotonicity
lemma for g.
Lemma 5. For all J ⊆ K ⊆ E* × E*, and for all
strings w; we have that g(φJ(w)) ≤ φK(w).
Proof. By induction on length of w. Both J and
K include the basic elements of E and λ. First
suppose |w |≤ 1, then φJ(w) = h(CL(w) ∩
F)&apos;∩J,CL(w)∩Fi, and g(φJ(w)) = h(CL(w)∩
F)&apos;, CL(w) ∩ Fi which is equal to φK(w).
Now suppose true for all w of length at most k,
and take some w of length k + 1. By definition of
φJ:
</bodyText>
<table confidence="0.536699428571429">
By the inductive hypothesis we have f(φG(u)) ≤ g(φJ(w)) = g ^ φJ(u) ◦ φJ(v)!
φF(u) and similarly for v and so by the mono- u,v
tonicity of ∧ and ◦: Next by Lemma 4, Part 2
^ φF (u) ◦ φF (v) ^ g(φJ(u) ◦ φJ(v))
f(φG(w)) ≤ g(φJ(w)) ≤
u,v
u,v
</table>
<bodyText confidence="0.94999075">
Since the right hand side is equal to φF (w), the
proof is done.
It is then immediate that
By Lemma 4, Part 1
</bodyText>
<equation confidence="0.866067333333333">
^
g(φJ(w)) ≤
u,v
g(φJ(u)) ◦ g(φJ(v))
Lemma 3. If F ⊆ G then L(hK, L, Fi) ⊆
L(hK, L, Gi),
</equation>
<bodyText confidence="0.4867508">
By the inductive hypothesis and monotonicity of ◦
and ∧:
Proof. If w ∈ L(hK, L, Fi), then φF (w) ≤ L,
and so f(φG(w)) ≤ L and so φG(w) has the con-
text (λ, λ) and is thus in L(hK, L, Gi).
</bodyText>
<equation confidence="0.400416">
^ φK(u) ◦ φK(v) = φK(w)
g(φJ(w)) ≤
u,v
</equation>
<bodyText confidence="0.626484333333333">
We now prove the corresponding facts about g.
Lemma 4. For any J ⊆ K and any concepts X, Y
in B(J, L, F), we have that
</bodyText>
<listItem confidence="0.9926265">
1. g(X) ◦ g(Y ) ≥ g(X ◦ Y )
2. g(X) ∧ g(Y ) ≥ g(X ∧ Y )
</listItem>
<construct confidence="0.85281375">
Lemma 6. If J ⊆ K then L(hJ, L, Fi) ⊇
L(hK, L, Fi)
Proof. Suppose w ∈ L(hK, L, Fi). this means
that φK(w) ≤ LK. therefore g(φJ(w)) ≤
</construct>
<bodyText confidence="0.695394666666667">
Lk; which means that (λ, λ) is in the concept
g(φJ(w)), which means it is in the concept φJ(w),
and therefore w ∈ L(hJ, L, Fi).
</bodyText>
<page confidence="0.996925">
32
</page>
<bodyText confidence="0.961564875">
Given these two lemmas we can make the fol-
lowing observations. First, if we have a fixed L
and F, then as we increase K, the language will
decrease until it reaches a limit, which it will at-
tain after a finite limit.
Lemma 7. For all L, and finite context sets F,
there is a finite K such that for all K2, K C K2,
L((K,L,F)) = L((K2,L,F)).
Proof. We can define the lattice B(E*, L, F). De-
fine the following equivalence relation between
pairs of strings, where (u1, v1) — (u2, v2) iff
C(u1) = C(u2) and C(v1) = C(v2) and C(u1v1) =
C(u2v2). The number of equivalence classes is
clearly finite. If K is sufficiently large that there is
a pair of strings (u, v) in K for each equivalence
class, then clearly the lattice defined by this K will
be isomorphic to B(E*, L, F). Any superset of K
will not change this lattice.
Moreover this language is unique for each L, F.
We will call this the limit language of L, F, and we
will write it as L((E*, L, F)).
If F C_ G, then L((E*, L, F)) C_
L((E*, L, G)). Finally, we will show that
the limit languages never overgeneralise.
</bodyText>
<construct confidence="0.9786565">
Lemma 8. For any L, and for any F,
L((E*, L, F)) C_ L.
</construct>
<subsubsectionHeader confidence="0.602196">
Proof. Recall that C(w) = ({w}&apos;&apos;, {w}&apos;) is the
</subsubsectionHeader>
<bodyText confidence="0.975117666666667">
real concept. If G is a limit grammar, we can
show that we always have OG(w) &gt; C(w), which
will give us the result immediately. First note
that C(u) o C(v) &gt; C(uv), which is immedi-
ate by the definition of o. We proceed, again,
by induction on the length of w. For |w |G 1,
</bodyText>
<equation confidence="0.74756275">
OG(w) = C(w). For the inductive step we have
OG(w) = Au,v OG(u) o OG(v); by inductive hy-
pothesis we have that this must be more than
Au,v C(u) o C(v) &gt; Au,v
</equation>
<sectionHeader confidence="0.917733" genericHeader="method">
5 Weak generative power
</sectionHeader>
<bodyText confidence="0.99945325">
First we make the following observation: if we
consider an infinite variant of this, where we set
K = E* and F = E* x E* and D = L, we
can prove easily that, allowing infinite “represen-
tations”, for any L, L((K, D, F)) = L. In this
infinite data limit, o becomes associative, and the
structure of B(K, D, F) becomes a residuated lat-
tice, called the syntactic concept lattice of the lan-
guage L, B(L). This lattice is finite iff the lan-
guage is regular. The fact that this lattice now has
residuation operations suggest interesting links to
the theory of categorial grammar. It is the finite
case that interests us.
We will use LDLG to refer to the class of lan-
guages that are limit languages in the sense de-
fined above.
</bodyText>
<equation confidence="0.89829">
LDLG = {L|]F, L((E*, L, F)) = L}
</equation>
<bodyText confidence="0.994102318181818">
Our focus in this paper is not on the language
theory: we present the following propositions.
First LDLG properly contains the class of regular
languages. Secondly LDLG contains some non-
context-free languages (Clark, 2009). Thirdly it
does not contain all context-free languages.
A natural question to ask is how to convert a
CFG into a DLG. This is in our view the wrong
question, as we are not interested in modeling
CFGs but modeling natural languages, but given
the status of CFGs as a default model for syn-
tactic structure, it will help to give a few exam-
ples, and a general mechanism. Consider a non-
terminal N in a CFG with start symbol S. We
can define QN) = {(l,r)|S =*=&gt;. lNr} and the
yield Y(N) = {w|N =*=&gt;. w}. Clearly QN) (D
Y (N) C_ L, but these are not necessarily maxi-
mal, and thus (QN), Y (N)) is not necessarily a
concept. Nonetheless in most cases, we can con-
struct a grammar where the non-terminals will cor-
respond to concepts, in this way.
The basic approach is this: for each non-
terminal, we identify a finite set of contexts that
will pick out only the set of strings generated
from that non-terminal: we find some set of con-
texts FN typically a subset of QN) such that
Y (N) = {w|b(l, r) E FN, lwr E L}. We say
that we can contextually define this non-terminal
if there is such a finite set of contexts FN. If a
CFG in Chomsky normal form is such that every
non-terminal can be contextually defined then the
language defined by that grammar is in LDLG. If
we can do that, then the rest is trivial. We take
any set of features F that includes all of these FN;
probably just F = UN FN; we then pick a set of
strings K that is sufficiently large to rule out all
incorrect generalisations, and then define D to be
L n (F ( DKK).
Consider the language L = {anbncm|n, m &gt;
0} U {ambncn|n,m &gt; 0}. L is a classic ex-
ample of an inherently ambiguous and thus non-
deterministic language.
The natural CFG in CNF for L has
non-terminals that generate the following
</bodyText>
<equation confidence="0.992346">
C(uv) = C(w)
</equation>
<page confidence="0.724781">
33
</page>
<bodyText confidence="0.984071246153847">
sets: {anbn|n ≥ 0}, {an+1bn|n ≥ 0},
{bncn|n ≥ 0}, {bncn+1|n ≥ 0}, {al
and {c*}. We note that the six contexts
(aa, bbc), (aa, bbbc), (abb, cc)(abbb, cc), (A, a)
and (c, A) will define exactly these sets, in
the sense that the set of strings that oc-
cur in each context will be exactly the
corresponding set. We can also pick out
A, a, b, c with individual contexts. Let F =
{(A, A), (aaabb, bccc), (aaabbc, A), (A, abbccc),
(aaab, bccc), (aa, bbc), (aa, bbbc), (abb, cc),
(abbb, cc), (A, a), (c, A)}. If we take a sufficiently
large set K, say A, a, b, c, ab, aab, bc, bcc, abc, and
set D = L ∩ F O KK, then we will have a DLG
for the language L. In this example, it is sufficient
to have one context per non-terminal. This is not
in general the case.
Consider L = {anbn|n ≥ 0} ∪ {anb2n|n ≥
0}. Here we clearly need to identify sets of strings
corresponding to the two parts of this language,
but it is easy to see that no one context will suffice.
However, note that the first part is defined by the
two contexts (A, A), (a, b) and the second by the
two contexts (A, A), (a, bb). Thus it is sufficient to
have a set F that includes these four contexts, as
well as similar pairs for the other non-terminals in
the grammar, and some contexts to define a and b.
We can see that we will not always be able to do
this for every CFG. One fixable problem is if the
CFG has two separate non-terminals, M, N such
that C(M) ⊇ C(N). If this is the case, then we
must have that Y (N) ⊇ Y (M), If we pick a set
of contexts to define Y (N), then clearly any string
in Y (M) will also be picked out by the same con-
texts. If this is not the case, then we can clearly try
to rectify it by adding a rule N → M which will
not change the language defined.
However, we cannot always pick out the non-
terminals with a finite set of contexts. Consider
the language L = {anb|n &gt; 0} ∪ {anc,|M &gt;
n &gt; 0} defined in Clark et al. (2008). Sup-
pose wlog that F contains no context (l, r) such
that |l |+ |r |≥ k. Then it is clear that we will
not be able to pick out b without also picking out
ck+1, since CL(ck+1) ∩ F ⊇ CL(b) ∩ F. Thus
L, which is clearly context-free, is not in LDLG.
Luckily, this example is highly artificial and does
not correspond to any phenomena we are aware of
in linguistics.
In terms of representing natural languages, we
clearly will in many cases need more than one
context to pick out syntactically relevant groups
of strings. Using a very simplified example from
English, if we want to identify say singular noun
phrases, a context like (that is, A) will not be suf-
ficient since as well as noun phrases we will also
have some adjective phrases. However if we in-
clude multiple contexts such as (A, is over there)
and so on, eventually we will be able to pick out
exactly the relevant set of strings. One of the
reasons we need to use a context sensitive repre-
sentation, is so that we can consider every possi-
ble combination of contexts simultaneously: this
would require an exponentially large context free
grammar.
</bodyText>
<sectionHeader confidence="0.995004" genericHeader="method">
6 Learning Model
</sectionHeader>
<bodyText confidence="0.999986971428572">
In order to prove correctness of the learning algo-
rithm we will use a variant of Gold-style inductive
inference (Gold, 1967). Our choice of this rather
old-fashioned model requires justification. There
are two problems with learning – the information
theoretic problems studied under VC-dimension
etc., and the computational complexity issues of
constructing a hypothesis from the data. In our
view, the latter problems are the key ones. Ac-
cordingly, we focus entirely on the efficiency is-
sue, and allow ourself a slightly unrealistic model;
see (Clark and Lappin, 2009) for arguments that
this is a plausible model.
We assume that we have a sequence of posi-
tive examples, and that we can query examples for
membership. Given a language L a presentation
for L is an infinite sequence of strings w1, w2, .. .
such that {wZ|i ∈ N} = L. An algorithm receives
a sequence T and an oracle, and must produce a
hypothesis H at every step, using only a polyno-
mial number of queries to the membership oracle
– polynomial in the total size of the presentation.
It identifies in the limit the language L iff for ev-
ery presentation T of L there is a N such that for
all n &gt; N Hn = HN, and L(HN) = L. We say
it identifies in the limit a class of languages L iff
it identifies in the limit all L in L. We say that it
identifies the class in polynomial update time iff
there is a polynomial p, such that at each step the
model uses an amount of computation (and thus
also a number of queries) that is less than p(n, l),
where n is the number of strings and l is the max-
imum length of a string in the observed data. We
note that this is slightly too weak. It is possible
to produce vacuous enumerative algorithms that
</bodyText>
<page confidence="0.997622">
34
</page>
<bodyText confidence="0.9937625">
can learn anything by only processing a logarith-
mically small prefix of the string (Pitt, 1989).
</bodyText>
<sectionHeader confidence="0.806127" genericHeader="method">
7 Learning Algorithm
</sectionHeader>
<bodyText confidence="0.992999333333333">
We now define a simple learning algorithm, that
establishes learnability under this paradigm.
There is one minor technical detail we need to
deal with. We need to be able to tell when adding
a string to a lazy DLG will leave the grammar un-
changed. We use a slightly weaker test. Given
G1 = (K, D, F) we define as before the equiva-
lence relation between pairs of strings of K, where
(u1, v1) —G1 (u2, v2) iff CD(u1) = CD(u2) and
CD(v1) = CD(v2) and CD(u1v1) = CD(u2v2).
Note that CD(u) = {(l, r)|lur E D}.
Given two grammars G1 = (K, D, F) and
G2 = (K2, D2, F) where K C_ K2 and D C_ D2
but F is unchanged, we say that these two are
indistinguishable iff the number of equivalence
classes of K x K under —G1 is equal to the num-
ber of equivalence classes of K2 x K2 under —G2.
This can clearly be computed efficiently using a
union-find algorithm, in time polynomial in |K|
and |F|. If they are indistinguishable then they de-
fine the same language.
</bodyText>
<subsectionHeader confidence="0.993708">
7.1 Algorithm
</subsectionHeader>
<bodyText confidence="0.999867333333333">
Algorithm 1 presents the basic algorithm. At var-
ious points we compute sets of strings like (F (D
KK) n L; these can be computed using the mem-
bership oracle.
First we prove that the program is efficient in
the sense that it runs in polynomial update time.
</bodyText>
<construct confidence="0.61393075">
Lemma 9. There is a polynomial p, such that Al-
gorithm 1, for each wn, runs in time bounded by
p(n, l) where l is the maximum length of a string
in w1, ... wn.
</construct>
<bodyText confidence="0.999740769230769">
Proof. First we note that K, K2 and F are always
subsets of Sub(E)UE and Con(E), and thus both
|K |and |F |are bounded by nl(l + 1)/2 + |E |+ 1.
Computing D is efficient as |F (DKK |is bounded
by |K|2|F |. We can compute φG as mentioned
above in time |K|2|F|l3; distinguishability is as
observed earlier also polynomial.
Before we prove the correctness of the algo-
rithm we make some informal points. First, we
are learning under a rather pessimistic model – the
positive examples may be chosen to confuse us,
so we cannot make any assumptions. Accordingly
we have to very crudely add all substrings and all
</bodyText>
<table confidence="0.4368915">
Algorithm 1: DLG learning algorithm
Data: Input strings S = {w1, w2 ... , },
membership oracle O
Result: A sequence of DLGs G1, G2,...
</table>
<equation confidence="0.960289">
K +— E U {λ}, K2 = K ;
F +— {(λ,λ)}, E = {} ;
D = (F (D KK) n L ;
G = (K, D, F) ;
for wz do
E+—EU{wz};
K2 +— K2 U Sub(wz) ;
if there is some w E E that is not in
L(G) then
F +— Con(E) ;
K +— K2 ;
D = (F (D KK) n L ;
G = (K, D, F) ;
end
else
D2+—(F(DK2K2)nL;
if (K2, D2, F) not indistinguishable
from (K, D, F) then
K +— K2 ;
D = (F (D KK) n L ;
G = (K, D, F) ;
end
end
Output G;
end
</equation>
<bodyText confidence="0.96162905">
contexts, rather than using sensible heuristics to
select frequent or likely ones.
Intuitively the algorithm works as follows: if we
observe a string not in our current hypothesis, then
we increase the set of contexts which will increase
the language defined. Since we only see positive
examples, we will never explicitly find out that our
hypothesis overgenerates, accordingly we always
add strings to a tester set K2 and see if this gives
us a more refined model. If this seems like it might
give a tighter hypothesis, then we increase K.
In what follows we will say that the hypothesis
at step n, Gn = (Kn, Dn, Fn), and the language
defined is Ln. We will assume that the target lan-
guage is some L E LDLG and w1, ... is a presen-
tation of L.
Lemma 10. Then there is a point n, and a finite set
of contexts F such that for all N &gt; n, FN = F.,
and L((E*, L, F)) = L.
Proof. Since L E LDLG there is some set of con-
</bodyText>
<page confidence="0.997756">
35
</page>
<bodyText confidence="0.940409454545455">
texts G C Con(L), such that L = L((E*, L, G)).
Any superset of G will define the correct limit lan-
guage. Let n be the smallest n such that G is a
subset of Con({wi, ... , wn}). Consider Fn. If
Fn defines the correct limit language, then we will
never change F as the hypothesis will be a super-
set of the target. Otherwise it must define a subset
of the correct language. Then either there is some
N &gt; n at which it has converged to the limit lan-
guage which will cause the first condition in the
loop to be satisfied and F will be increased to a
superset of G, or F will be increased before it con-
verges, and thus the result holds.
Lemma 11. After F converges according to the
previous lemma, there is some n, such that for all
N &gt; n, KN = Kn and L((Kn, L, Fn)) = L.
Proof. let no be the convergence point of F; for
all n &gt; no the hypothesis will be a superset of
the target language; therefore the only change that
can happen is that K will increase. By definition
of the limit language, it must converge after a finite
number of examples.
</bodyText>
<construct confidence="0.752492333333333">
Theorem 1. For every language L E LDLG, and
every presentation of L, Algorithm 1 will converge
to a grammar G such that L(G) = L.
</construct>
<bodyText confidence="0.991493">
This result is immediate by the two preceding
lemmas.
</bodyText>
<sectionHeader confidence="0.99764" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.998410209677419">
We have presented an efficient, correct learning al-
gorithm for an interesting class of languages; this
is the first such learning result for a class of lan-
guages that is potentially large enough to describe
natural language.
The results presented here lack a couple of tech-
nical details to be completely convincing. In par-
ticular we would like to show that given a repre-
sentation of size n, we can learn once we have seen
a set of examples that is polynomially bounded by
n. This will be challenging, as the size of the K
we need to converge can be exponentially large
in F. We can construct DFAs where the num-
ber of congruence classes of the language is an
exponential function of the number of states. In
order to learn languages like this, we will need
to use a more efficient algorithm that can learn
even with “insufficient” K: that is to say when
the lattice 93(K, L, F) has fewer elements that
93(KK, L, F).
This algorithm can be implemented directly and
functions as expected on synthetic examples, but
would need modification to run efficiently on nat-
ural languages. In particular rather than consider-
ing whole contexts of the form (l, r) it would be
natural to restrict them just to a narrow window
of one or two words or tags on each side. Rather
than using a membership oracle, we could prob-
abilistically cluster the data in the table of counts
of strings in F O K. In practice we will have a
limited amount of data to work with and we can
control over-fitting in a principled way by control-
ling the relative size of K and F.
This formalism represents a process of anal-
ogy from stored examples, based on distributional
learning – this is very plausible in terms of what
we know about cognitive processes, and is com-
patible with much non-Chomskyan theorizing in
linguistics (Blevins and Blevins, 2009). The class
of languages is a good fit to the class of natural
languages; it contains, as far as we can tell, all
standard examples of context free grammars, and
includes non-deterministic and inherently ambigu-
ous grammars. It is hard to say whether the class
is in fact large enough to represent natural lan-
guages; but then we don’t know that about any for-
malism, context-free or context-sensitive. All we
can say is that there are no phenomena that we are
aware of that don’t fit. Only large scale empirical
work can answer this question.
Ideologically these models are empiricist – the
structure of the representation is based on the
structure of the data: this has to be a good thing
for computational modeling. By minimizing the
amount of hidden, unobservable structure, we can
improve learnability. Languages are enormously
complex, and it would be simplistic to try to re-
duce their acquisition to a few pages of mathe-
matics; nonetheless, we feel that the representa-
tions and grammar induction algorithms presented
in this paper could be a significant piece of the
puzzle.
</bodyText>
<page confidence="0.997475">
36
</page>
<sectionHeader confidence="0.993895" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99989468115942">
N. Abe and M. K. Warmuth. 1992. On the computa-
tional complexity of approximating distributions by
probabilistic automata. Machine Learning, 9:205–
260.
D. Angluin and M. Kharitonov. 1995. When won’t
membership queries help? J. Comput. Syst. Sci.,
50:336–355.
D. Angluin. 1987. Learning regular sets from queries
and counterexamples. Information and Computa-
tion, 75(2):87–106.
James P. Blevins and Juliette Blevins. 2009. Analogy
in grammar: Form and acquisition. Oxford Univer-
sity Press.
A. Carnie. 2008. Constituent structure. Oxford Uni-
versity Press, USA.
Noam Chomsky. 2006. Language and mind. Cam-
bridge University Press, 3rd edition.
Alexander Clark and Shalom Lappin. 2009. Another
look at indirect negative evidence. In Proceedings of
the EACL Workshop on Cognitive Aspects of Com-
putational Language Acquisition, Athens, March.
Alexander Clark, R´emi Eyraud, and Amaury Habrard.
2008. A polynomial algorithm for the inference of
context free languages. In Proceedings of Interna-
tional Colloquium on Grammatical Inference, pages
29–42. Springer, September.
Alexander Clark. 2009. A learnable representation
for syntax using residuated lattices. In Proceedings
of the 14th Conference on Formal Grammar, Bor-
deaux, France.
J.R. Curran. 2003. From distributional to semantic
similarity. Ph.D. thesis, University of Edinburgh.
B. A. Davey and H. A. Priestley. 2002. Introduction to
Lattices and Order. Cambridge University Press.
B. Ganter and R. Wille. 1997. Formal Concept Analy-
sis: Mathematical Foundations. Springer-Verlag.
E. M. Gold. 1967. Language identification in the limit.
Information and control, 10(5):447 – 474.
Zellig Harris. 1954. Distributional structure. Word,
10(2-3):146–62.
J. J. Horning. 1969. A Study of Grammatical Infer-
ence. Ph.D. thesis, Stanford University, Computer
Science Department, California.
M. Johnson. 2008. Using adaptor grammars to identify
synergies in the unsupervised acquisition of linguis-
tic structure. In 46th Annual Meeting of the ACL,
pages 398–406.
Dan Klein and Chris Manning. 2001. Distribu-
tional phrase structure induction. In Proceedings of
CoNLL 2001, pages 113–121.
Dan Klein and Chris Manning. 2004. Corpus-based
induction of syntactic structure: Models of depen-
dency and constituency. In Proceedings of the 42nd
Annual Meeting of the ACL.
K. Lari and S. J. Young. 1990. The estimation of
stochastic context-free grammars using the inside-
outside algorithm. Computer Speech and Language,
4:35–56.
L. Pitt. 1989. Inductive inference, dfa’s, and computa-
tional complexity. In K. P. Jantke, editor, Analogical
and Inductive Inference, number 397 in LNAI, pages
18–44. Springer-Verglag.
J. R. Saffran, R. N. Aslin, and E. L. Newport. 1996.
Statistical learning by eight month old infants. Sci-
ence, 274:1926–1928.
Hinrich Sch¨utze. 1993. Part of speech induction from
scratch. In Proceedings of the 31st annual meet-
ing of the Association for Computational Linguis-
tics, pages 251–258.
</reference>
<page confidence="0.99961">
37
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.471419">
<title confidence="0.99761">Efficient, correct, unsupervised learning of context-sensitive languages</title>
<author confidence="0.998359">Alexander</author>
<affiliation confidence="0.7446625">Department of Computer Royal Holloway, University of</affiliation>
<email confidence="0.98649">alexc@cs.rhul.ac.uk</email>
<abstract confidence="0.998508882352941">A central problem for NLP is grammar induction: the development of unsupervised learning algorithms for syntax. In this paper we present a lattice-theoretic representation for natural language syntax, called Distributional Lattice Grammars. These representations are objective or empiricist, based on a generalisation of distributional learning, and are capable of representing all regular languages, some but not all context-free languages and some noncontext-free languages. We present a simple algorithm for learning these grammars together with a complete self-contained proof of the correctness and efficiency of the algorithm.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>N Abe</author>
<author>M K Warmuth</author>
</authors>
<title>On the computational complexity of approximating distributions by probabilistic automata.</title>
<date>1992</date>
<booktitle>Machine Learning,</booktitle>
<pages>9--205</pages>
<contexts>
<context position="2318" citStr="Abe and Warmuth (1992)" startWordPosition="342" endWordPosition="345">ari and Young, 1990) which are guaranteed to converge efficiently, but not necessarily to the right answer: they converge to a local optimum that may be, and in practice nearly always is very far from the optimum. There are naive enumerative algorithms that are correct, but involve exhaustively enumerating all representations below a certain size (Horning, 1969). There are no correct and efficient algorithms, as there are for parsing, for example. There is a reason for this: from a formal point of view, the problem is intractably hard for the standard representations in the Chomsky hierarchy. Abe and Warmuth (1992) showed that training stochastic regular grammars is hard; Angluin and Kharitonov (1995) showed that regular grammars cannot be learned even using queries; these results obviously apply also to PCFGs and CFGs as well as to the more complex representations built by extending CFGs, such as TAGs and so on. However, these results do not necessarily apply to other representations. Regular grammars are not learnable, but deterministic finite automata are learnable under various paradigms (Angluin, 1987). Thus it is possible to learn by changing to representations that have better properties: in part</context>
</contexts>
<marker>Abe, Warmuth, 1992</marker>
<rawString>N. Abe and M. K. Warmuth. 1992. On the computational complexity of approximating distributions by probabilistic automata. Machine Learning, 9:205– 260.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Angluin</author>
<author>M Kharitonov</author>
</authors>
<title>When won’t membership queries help?</title>
<date>1995</date>
<journal>J. Comput. Syst. Sci.,</journal>
<pages>50--336</pages>
<contexts>
<context position="2406" citStr="Angluin and Kharitonov (1995)" startWordPosition="355" endWordPosition="358">arily to the right answer: they converge to a local optimum that may be, and in practice nearly always is very far from the optimum. There are naive enumerative algorithms that are correct, but involve exhaustively enumerating all representations below a certain size (Horning, 1969). There are no correct and efficient algorithms, as there are for parsing, for example. There is a reason for this: from a formal point of view, the problem is intractably hard for the standard representations in the Chomsky hierarchy. Abe and Warmuth (1992) showed that training stochastic regular grammars is hard; Angluin and Kharitonov (1995) showed that regular grammars cannot be learned even using queries; these results obviously apply also to PCFGs and CFGs as well as to the more complex representations built by extending CFGs, such as TAGs and so on. However, these results do not necessarily apply to other representations. Regular grammars are not learnable, but deterministic finite automata are learnable under various paradigms (Angluin, 1987). Thus it is possible to learn by changing to representations that have better properties: in particular DFAs are learnable because they are “objective”; there is a correspondence betwee</context>
</contexts>
<marker>Angluin, Kharitonov, 1995</marker>
<rawString>D. Angluin and M. Kharitonov. 1995. When won’t membership queries help? J. Comput. Syst. Sci., 50:336–355.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Angluin</author>
</authors>
<title>Learning regular sets from queries and counterexamples.</title>
<date>1987</date>
<journal>Information and Computation,</journal>
<volume>75</volume>
<issue>2</issue>
<contexts>
<context position="2820" citStr="Angluin, 1987" startWordPosition="422" endWordPosition="423">he problem is intractably hard for the standard representations in the Chomsky hierarchy. Abe and Warmuth (1992) showed that training stochastic regular grammars is hard; Angluin and Kharitonov (1995) showed that regular grammars cannot be learned even using queries; these results obviously apply also to PCFGs and CFGs as well as to the more complex representations built by extending CFGs, such as TAGs and so on. However, these results do not necessarily apply to other representations. Regular grammars are not learnable, but deterministic finite automata are learnable under various paradigms (Angluin, 1987). Thus it is possible to learn by changing to representations that have better properties: in particular DFAs are learnable because they are “objective”; there is a correspondence between the structure of the language, (the residual languages) and the representational primitives of the formalism (the states) which is expressed by the MyhillNerode theorem. In this paper we study the learnability of a class of representations that we call distributional lattice grammars (DLGs). Lattice-based formalisms were introduced by Clark et al. (2008) and Clark (2009) as context sensitive formalisms that a</context>
</contexts>
<marker>Angluin, 1987</marker>
<rawString>D. Angluin. 1987. Learning regular sets from queries and counterexamples. Information and Computation, 75(2):87–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>James P Blevins</author>
<author>Juliette Blevins</author>
</authors>
<title>Analogy in grammar: Form and acquisition.</title>
<date>2009</date>
<publisher>Oxford University Press.</publisher>
<contexts>
<context position="34890" citStr="Blevins and Blevins, 2009" startWordPosition="6836" endWordPosition="6839">ust to a narrow window of one or two words or tags on each side. Rather than using a membership oracle, we could probabilistically cluster the data in the table of counts of strings in F O K. In practice we will have a limited amount of data to work with and we can control over-fitting in a principled way by controlling the relative size of K and F. This formalism represents a process of analogy from stored examples, based on distributional learning – this is very plausible in terms of what we know about cognitive processes, and is compatible with much non-Chomskyan theorizing in linguistics (Blevins and Blevins, 2009). The class of languages is a good fit to the class of natural languages; it contains, as far as we can tell, all standard examples of context free grammars, and includes non-deterministic and inherently ambiguous grammars. It is hard to say whether the class is in fact large enough to represent natural languages; but then we don’t know that about any formalism, context-free or context-sensitive. All we can say is that there are no phenomena that we are aware of that don’t fit. Only large scale empirical work can answer this question. Ideologically these models are empiricist – the structure o</context>
</contexts>
<marker>Blevins, Blevins, 2009</marker>
<rawString>James P. Blevins and Juliette Blevins. 2009. Analogy in grammar: Form and acquisition. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Carnie</author>
</authors>
<title>Constituent structure.</title>
<date>2008</date>
<publisher>Oxford University Press, USA.</publisher>
<contexts>
<context position="7109" citStr="Carnie, 2008" startWordPosition="1176" endWordPosition="1177">FGs and other PSG formalisms were intended to be learnable by distributional means. Chomsky (2006) says (p. 172, footnote 15): The concept of “phrase structure grammar” was explicitly designed to express the richest system that could reasonably be expected to result from the application of Harris-type procedures to a corpus. Second, empirically we know they work well at least for lexical induction, (Sch¨utze, 1993; Curran, 2003) and are a component of some implemented unsupervised learning systems (Klein and Manning, 2001). Linguists use them as one of the key tests for constituent structure (Carnie, 2008), and finally there is some psycholinguistic evidence that children are sensitive to distributional structure, at least in artificial grammar learning tasks (Saffran et al., 1996). These arguments together suggest that distributional learning has a somewhat privileged status. 3 Lattice grammars Clark (2009) presents the theory of lattice based formalisms starting algebraically from the theory of residuated lattices. Here we will largely ignore this, and start from a straightforward computational treatment. We start by defining the representation. Definition 1. Given a non-empty finite alphabet</context>
</contexts>
<marker>Carnie, 2008</marker>
<rawString>A. Carnie. 2008. Constituent structure. Oxford University Press, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noam Chomsky</author>
</authors>
<title>Language and mind.</title>
<date>2006</date>
<publisher>Cambridge University Press,</publisher>
<note>3rd edition.</note>
<contexts>
<context position="6594" citStr="Chomsky (2006)" startWordPosition="1093" endWordPosition="1094"> lur = w}. For a given string w we can define the distribution of that string to be the set of all contexts that it can appear in: CL(w) = {(l, r)|lwr E L}, equivalently {f|f O w E L}. Clearly (A, A) E CL(w) iff w E L. Distributional learning (Harris, 1954) as a technical term refers to learning techniques which model directly or indirectly properties of the distribution of strings or words in a corpus or a language. There are a number of reasons to take distributional learning seriously: first, historically, CFGs and other PSG formalisms were intended to be learnable by distributional means. Chomsky (2006) says (p. 172, footnote 15): The concept of “phrase structure grammar” was explicitly designed to express the richest system that could reasonably be expected to result from the application of Harris-type procedures to a corpus. Second, empirically we know they work well at least for lexical induction, (Sch¨utze, 1993; Curran, 2003) and are a component of some implemented unsupervised learning systems (Klein and Manning, 2001). Linguists use them as one of the key tests for constituent structure (Carnie, 2008), and finally there is some psycholinguistic evidence that children are sensitive to </context>
</contexts>
<marker>Chomsky, 2006</marker>
<rawString>Noam Chomsky. 2006. Language and mind. Cambridge University Press, 3rd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>Shalom Lappin</author>
</authors>
<title>Another look at indirect negative evidence.</title>
<date>2009</date>
<booktitle>In Proceedings of the EACL Workshop on Cognitive Aspects of Computational Language Acquisition,</booktitle>
<location>Athens,</location>
<contexts>
<context position="27172" citStr="Clark and Lappin, 2009" startWordPosition="5319" endWordPosition="5322">ntially large context free grammar. 6 Learning Model In order to prove correctness of the learning algorithm we will use a variant of Gold-style inductive inference (Gold, 1967). Our choice of this rather old-fashioned model requires justification. There are two problems with learning – the information theoretic problems studied under VC-dimension etc., and the computational complexity issues of constructing a hypothesis from the data. In our view, the latter problems are the key ones. Accordingly, we focus entirely on the efficiency issue, and allow ourself a slightly unrealistic model; see (Clark and Lappin, 2009) for arguments that this is a plausible model. We assume that we have a sequence of positive examples, and that we can query examples for membership. Given a language L a presentation for L is an infinite sequence of strings w1, w2, .. . such that {wZ|i ∈ N} = L. An algorithm receives a sequence T and an oracle, and must produce a hypothesis H at every step, using only a polynomial number of queries to the membership oracle – polynomial in the total size of the presentation. It identifies in the limit the language L iff for every presentation T of L there is a N such that for all n &gt; N Hn = HN</context>
</contexts>
<marker>Clark, Lappin, 2009</marker>
<rawString>Alexander Clark and Shalom Lappin. 2009. Another look at indirect negative evidence. In Proceedings of the EACL Workshop on Cognitive Aspects of Computational Language Acquisition, Athens, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
<author>R´emi Eyraud</author>
<author>Amaury Habrard</author>
</authors>
<title>A polynomial algorithm for the inference of context free languages.</title>
<date>2008</date>
<booktitle>In Proceedings of International Colloquium on Grammatical Inference,</booktitle>
<pages>29--42</pages>
<publisher>Springer,</publisher>
<contexts>
<context position="3364" citStr="Clark et al. (2008)" startWordPosition="505" endWordPosition="508">istic finite automata are learnable under various paradigms (Angluin, 1987). Thus it is possible to learn by changing to representations that have better properties: in particular DFAs are learnable because they are “objective”; there is a correspondence between the structure of the language, (the residual languages) and the representational primitives of the formalism (the states) which is expressed by the MyhillNerode theorem. In this paper we study the learnability of a class of representations that we call distributional lattice grammars (DLGs). Lattice-based formalisms were introduced by Clark et al. (2008) and Clark (2009) as context sensitive formalisms that are potentially learnable. Clark et al. (2008) established a similar learnability result for a limited class of context free languages. In Clark (2009), the approach was extended to a significantly larger class but without an explicit learning algorithm. Most of the building blocks are however in place, though we need to make several modifications and ex28 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 28–37, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics tensi</context>
<context position="25477" citStr="Clark et al. (2008)" startWordPosition="5024" endWordPosition="5027">always be able to do this for every CFG. One fixable problem is if the CFG has two separate non-terminals, M, N such that C(M) ⊇ C(N). If this is the case, then we must have that Y (N) ⊇ Y (M), If we pick a set of contexts to define Y (N), then clearly any string in Y (M) will also be picked out by the same contexts. If this is not the case, then we can clearly try to rectify it by adding a rule N → M which will not change the language defined. However, we cannot always pick out the nonterminals with a finite set of contexts. Consider the language L = {anb|n &gt; 0} ∪ {anc,|M &gt; n &gt; 0} defined in Clark et al. (2008). Suppose wlog that F contains no context (l, r) such that |l |+ |r |≥ k. Then it is clear that we will not be able to pick out b without also picking out ck+1, since CL(ck+1) ∩ F ⊇ CL(b) ∩ F. Thus L, which is clearly context-free, is not in LDLG. Luckily, this example is highly artificial and does not correspond to any phenomena we are aware of in linguistics. In terms of representing natural languages, we clearly will in many cases need more than one context to pick out syntactically relevant groups of strings. Using a very simplified example from English, if we want to identify say singular</context>
</contexts>
<marker>Clark, Eyraud, Habrard, 2008</marker>
<rawString>Alexander Clark, R´emi Eyraud, and Amaury Habrard. 2008. A polynomial algorithm for the inference of context free languages. In Proceedings of International Colloquium on Grammatical Inference, pages 29–42. Springer, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Clark</author>
</authors>
<title>A learnable representation for syntax using residuated lattices.</title>
<date>2009</date>
<booktitle>In Proceedings of the 14th Conference on Formal Grammar,</booktitle>
<location>Bordeaux, France.</location>
<contexts>
<context position="3381" citStr="Clark (2009)" startWordPosition="510" endWordPosition="511">e learnable under various paradigms (Angluin, 1987). Thus it is possible to learn by changing to representations that have better properties: in particular DFAs are learnable because they are “objective”; there is a correspondence between the structure of the language, (the residual languages) and the representational primitives of the formalism (the states) which is expressed by the MyhillNerode theorem. In this paper we study the learnability of a class of representations that we call distributional lattice grammars (DLGs). Lattice-based formalisms were introduced by Clark et al. (2008) and Clark (2009) as context sensitive formalisms that are potentially learnable. Clark et al. (2008) established a similar learnability result for a limited class of context free languages. In Clark (2009), the approach was extended to a significantly larger class but without an explicit learning algorithm. Most of the building blocks are however in place, though we need to make several modifications and ex28 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 28–37, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics tensions to get a clea</context>
<context position="7417" citStr="Clark (2009)" startWordPosition="1220" endWordPosition="1221">ures to a corpus. Second, empirically we know they work well at least for lexical induction, (Sch¨utze, 1993; Curran, 2003) and are a component of some implemented unsupervised learning systems (Klein and Manning, 2001). Linguists use them as one of the key tests for constituent structure (Carnie, 2008), and finally there is some psycholinguistic evidence that children are sensitive to distributional structure, at least in artificial grammar learning tasks (Saffran et al., 1996). These arguments together suggest that distributional learning has a somewhat privileged status. 3 Lattice grammars Clark (2009) presents the theory of lattice based formalisms starting algebraically from the theory of residuated lattices. Here we will largely ignore this, and start from a straightforward computational treatment. We start by defining the representation. Definition 1. Given a non-empty finite alphabet, E, a distributional lattice grammar (DLG) is a 3- tuple consisting of (K, D, F), where F is a finite subset of E* x E*, such that (A, A) E F, K is a finite subset of E* which contains A and E, and D is a subset of (F O KK). K here can be thought of as a finite set of exemplars, which correspond to substri</context>
<context position="21804" citStr="Clark, 2009" startWordPosition="4290" endWordPosition="4291">cept lattice of the language L, B(L). This lattice is finite iff the language is regular. The fact that this lattice now has residuation operations suggest interesting links to the theory of categorial grammar. It is the finite case that interests us. We will use LDLG to refer to the class of languages that are limit languages in the sense defined above. LDLG = {L|]F, L((E*, L, F)) = L} Our focus in this paper is not on the language theory: we present the following propositions. First LDLG properly contains the class of regular languages. Secondly LDLG contains some noncontext-free languages (Clark, 2009). Thirdly it does not contain all context-free languages. A natural question to ask is how to convert a CFG into a DLG. This is in our view the wrong question, as we are not interested in modeling CFGs but modeling natural languages, but given the status of CFGs as a default model for syntactic structure, it will help to give a few examples, and a general mechanism. Consider a nonterminal N in a CFG with start symbol S. We can define QN) = {(l,r)|S =*=&gt;. lNr} and the yield Y(N) = {w|N =*=&gt;. w}. Clearly QN) (D Y (N) C_ L, but these are not necessarily maximal, and thus (QN), Y (N)) is not neces</context>
</contexts>
<marker>Clark, 2009</marker>
<rawString>Alexander Clark. 2009. A learnable representation for syntax using residuated lattices. In Proceedings of the 14th Conference on Formal Grammar, Bordeaux, France.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Curran</author>
</authors>
<title>From distributional to semantic similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Edinburgh.</institution>
<contexts>
<context position="6928" citStr="Curran, 2003" startWordPosition="1146" endWordPosition="1148">irectly properties of the distribution of strings or words in a corpus or a language. There are a number of reasons to take distributional learning seriously: first, historically, CFGs and other PSG formalisms were intended to be learnable by distributional means. Chomsky (2006) says (p. 172, footnote 15): The concept of “phrase structure grammar” was explicitly designed to express the richest system that could reasonably be expected to result from the application of Harris-type procedures to a corpus. Second, empirically we know they work well at least for lexical induction, (Sch¨utze, 1993; Curran, 2003) and are a component of some implemented unsupervised learning systems (Klein and Manning, 2001). Linguists use them as one of the key tests for constituent structure (Carnie, 2008), and finally there is some psycholinguistic evidence that children are sensitive to distributional structure, at least in artificial grammar learning tasks (Saffran et al., 1996). These arguments together suggest that distributional learning has a somewhat privileged status. 3 Lattice grammars Clark (2009) presents the theory of lattice based formalisms starting algebraically from the theory of residuated lattices.</context>
</contexts>
<marker>Curran, 2003</marker>
<rawString>J.R. Curran. 2003. From distributional to semantic similarity. Ph.D. thesis, University of Edinburgh.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B A Davey</author>
<author>H A Priestley</author>
</authors>
<title>Introduction to Lattices and Order.</title>
<date>2002</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="8990" citStr="Davey and Priestley, 2002" startWordPosition="1533" endWordPosition="1536">mine. If the language 29 we are modeling is L, then D = L ∩ (F (D KK). Since λ ∈ K, K ⊆ KK. We define a concept to be an ordered pair hS, Ci where S ⊆ K and C ⊆ F, which satisfies the following two conditions: first C O S ⊆ D; that is to say every string in S can be combined with any context in C to give a grammatical string, and secondly they are maximal in that neither K nor F can be increased without violating the first condition. We define B(K, D, F) to be the set of all such concepts. We use the B symbol (Begriff) to bring out the links to Formal Concept Analysis (Ganter and Wille, 1997; Davey and Priestley, 2002). This lattice may contain exponentially many concepts, but it is clearly finite, as the number of concepts is less than min(2|F|, 2|K|). There is an obvious partial order defined by hS1, C1i ≤ hS2, C2i iff S1 ⊆ S2, Note that S1 ⊆ S2 iff C2 ⊆ C1. Given a set of strings S we can define a set of contexts S0 to be the set of contexts that appear with every element of S. S0 = {(l,r) ∈ F : ∀w ∈ S, lwr ∈ D} Dually we can define for a set of contexts C the set of strings C0 that occur with all of the elements of C: C0 = {w ∈ K : ∀(l,r) ∈ C, lwr ∈ D} The concepts hS, Ci are just the pairs that satisfy</context>
</contexts>
<marker>Davey, Priestley, 2002</marker>
<rawString>B. A. Davey and H. A. Priestley. 2002. Introduction to Lattices and Order. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Ganter</author>
<author>R Wille</author>
</authors>
<title>Formal Concept Analysis: Mathematical Foundations.</title>
<date>1997</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="8962" citStr="Ganter and Wille, 1997" startWordPosition="1529" endWordPosition="1532">the language that we examine. If the language 29 we are modeling is L, then D = L ∩ (F (D KK). Since λ ∈ K, K ⊆ KK. We define a concept to be an ordered pair hS, Ci where S ⊆ K and C ⊆ F, which satisfies the following two conditions: first C O S ⊆ D; that is to say every string in S can be combined with any context in C to give a grammatical string, and secondly they are maximal in that neither K nor F can be increased without violating the first condition. We define B(K, D, F) to be the set of all such concepts. We use the B symbol (Begriff) to bring out the links to Formal Concept Analysis (Ganter and Wille, 1997; Davey and Priestley, 2002). This lattice may contain exponentially many concepts, but it is clearly finite, as the number of concepts is less than min(2|F|, 2|K|). There is an obvious partial order defined by hS1, C1i ≤ hS2, C2i iff S1 ⊆ S2, Note that S1 ⊆ S2 iff C2 ⊆ C1. Given a set of strings S we can define a set of contexts S0 to be the set of contexts that appear with every element of S. S0 = {(l,r) ∈ F : ∀w ∈ S, lwr ∈ D} Dually we can define for a set of contexts C the set of strings C0 that occur with all of the elements of C: C0 = {w ∈ K : ∀(l,r) ∈ C, lwr ∈ D} The concepts hS, Ci are</context>
</contexts>
<marker>Ganter, Wille, 1997</marker>
<rawString>B. Ganter and R. Wille. 1997. Formal Concept Analysis: Mathematical Foundations. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E M Gold</author>
</authors>
<title>Language identification in the limit.</title>
<date>1967</date>
<journal>Information and control,</journal>
<volume>10</volume>
<issue>5</issue>
<pages>474</pages>
<contexts>
<context position="26726" citStr="Gold, 1967" startWordPosition="5253" endWordPosition="5254">, A) will not be sufficient since as well as noun phrases we will also have some adjective phrases. However if we include multiple contexts such as (A, is over there) and so on, eventually we will be able to pick out exactly the relevant set of strings. One of the reasons we need to use a context sensitive representation, is so that we can consider every possible combination of contexts simultaneously: this would require an exponentially large context free grammar. 6 Learning Model In order to prove correctness of the learning algorithm we will use a variant of Gold-style inductive inference (Gold, 1967). Our choice of this rather old-fashioned model requires justification. There are two problems with learning – the information theoretic problems studied under VC-dimension etc., and the computational complexity issues of constructing a hypothesis from the data. In our view, the latter problems are the key ones. Accordingly, we focus entirely on the efficiency issue, and allow ourself a slightly unrealistic model; see (Clark and Lappin, 2009) for arguments that this is a plausible model. We assume that we have a sequence of positive examples, and that we can query examples for membership. Give</context>
</contexts>
<marker>Gold, 1967</marker>
<rawString>E. M. Gold. 1967. Language identification in the limit. Information and control, 10(5):447 – 474.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zellig Harris</author>
</authors>
<date>1954</date>
<booktitle>Distributional structure. Word,</booktitle>
<pages>10--2</pages>
<contexts>
<context position="6237" citStr="Harris, 1954" startWordPosition="1035" endWordPosition="1036"> operation that we write O: so (l, r) O u is defined to be lur. We will sometimes write f for a context (l, r). There is a special context (A, A): (A, A) O w = w. We will extend this to sets of contexts and sets of strings in the natural way. We will write Sub(w) = {u|](l, r) : lur = w} for the set of substrings of a string, and Con(w) = {(l,r)|]u E E* : lur = w}. For a given string w we can define the distribution of that string to be the set of all contexts that it can appear in: CL(w) = {(l, r)|lwr E L}, equivalently {f|f O w E L}. Clearly (A, A) E CL(w) iff w E L. Distributional learning (Harris, 1954) as a technical term refers to learning techniques which model directly or indirectly properties of the distribution of strings or words in a corpus or a language. There are a number of reasons to take distributional learning seriously: first, historically, CFGs and other PSG formalisms were intended to be learnable by distributional means. Chomsky (2006) says (p. 172, footnote 15): The concept of “phrase structure grammar” was explicitly designed to express the richest system that could reasonably be expected to result from the application of Harris-type procedures to a corpus. Second, empiri</context>
</contexts>
<marker>Harris, 1954</marker>
<rawString>Zellig Harris. 1954. Distributional structure. Word, 10(2-3):146–62.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Horning</author>
</authors>
<title>A Study of Grammatical Inference.</title>
<date>1969</date>
<tech>Ph.D. thesis,</tech>
<institution>Stanford University, Computer Science Department,</institution>
<location>California.</location>
<contexts>
<context position="2060" citStr="Horning, 1969" startWordPosition="300" endWordPosition="301">ficient: an exponential search algorithm is hidden in the convergence of the MCMC samplers. The efficient algorithms that are actually used are heuristic approximations to the true posteriors. There are algorithms like the Inside-Outside algorithm (Lari and Young, 1990) which are guaranteed to converge efficiently, but not necessarily to the right answer: they converge to a local optimum that may be, and in practice nearly always is very far from the optimum. There are naive enumerative algorithms that are correct, but involve exhaustively enumerating all representations below a certain size (Horning, 1969). There are no correct and efficient algorithms, as there are for parsing, for example. There is a reason for this: from a formal point of view, the problem is intractably hard for the standard representations in the Chomsky hierarchy. Abe and Warmuth (1992) showed that training stochastic regular grammars is hard; Angluin and Kharitonov (1995) showed that regular grammars cannot be learned even using queries; these results obviously apply also to PCFGs and CFGs as well as to the more complex representations built by extending CFGs, such as TAGs and so on. However, these results do not necessa</context>
</contexts>
<marker>Horning, 1969</marker>
<rawString>J. J. Horning. 1969. A Study of Grammatical Inference. Ph.D. thesis, Stanford University, Computer Science Department, California.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure.</title>
<date>2008</date>
<booktitle>In 46th Annual Meeting of the ACL,</booktitle>
<pages>398--406</pages>
<contexts>
<context position="1416" citStr="Johnson, 2008" startWordPosition="197" endWordPosition="199">f the algorithm. 1 Introduction Grammar induction, or unsupervised learning of syntax, no longer requires extensive justification and motivation. Both from engineering and cognitive/linguistic angles, it is a central challenge for computational linguistics. However good algorithms for this task are thin on the ground. There are numerous heuristic algorithms, some of which have had significant success in inducing constituent structure (Klein and Manning, 2004). There are algorithms with theoretical guarantees as to their correctness – such as for example Bayesian algorithms for inducing PCFGs (Johnson, 2008), but such algorithms are inefficient: an exponential search algorithm is hidden in the convergence of the MCMC samplers. The efficient algorithms that are actually used are heuristic approximations to the true posteriors. There are algorithms like the Inside-Outside algorithm (Lari and Young, 1990) which are guaranteed to converge efficiently, but not necessarily to the right answer: they converge to a local optimum that may be, and in practice nearly always is very far from the optimum. There are naive enumerative algorithms that are correct, but involve exhaustively enumerating all represen</context>
</contexts>
<marker>Johnson, 2008</marker>
<rawString>M. Johnson. 2008. Using adaptor grammars to identify synergies in the unsupervised acquisition of linguistic structure. In 46th Annual Meeting of the ACL, pages 398–406.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Distributional phrase structure induction.</title>
<date>2001</date>
<booktitle>In Proceedings of CoNLL</booktitle>
<pages>113--121</pages>
<contexts>
<context position="7024" citStr="Klein and Manning, 2001" startWordPosition="1160" endWordPosition="1163"> There are a number of reasons to take distributional learning seriously: first, historically, CFGs and other PSG formalisms were intended to be learnable by distributional means. Chomsky (2006) says (p. 172, footnote 15): The concept of “phrase structure grammar” was explicitly designed to express the richest system that could reasonably be expected to result from the application of Harris-type procedures to a corpus. Second, empirically we know they work well at least for lexical induction, (Sch¨utze, 1993; Curran, 2003) and are a component of some implemented unsupervised learning systems (Klein and Manning, 2001). Linguists use them as one of the key tests for constituent structure (Carnie, 2008), and finally there is some psycholinguistic evidence that children are sensitive to distributional structure, at least in artificial grammar learning tasks (Saffran et al., 1996). These arguments together suggest that distributional learning has a somewhat privileged status. 3 Lattice grammars Clark (2009) presents the theory of lattice based formalisms starting algebraically from the theory of residuated lattices. Here we will largely ignore this, and start from a straightforward computational treatment. We </context>
</contexts>
<marker>Klein, Manning, 2001</marker>
<rawString>Dan Klein and Chris Manning. 2001. Distributional phrase structure induction. In Proceedings of CoNLL 2001, pages 113–121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Klein</author>
<author>Chris Manning</author>
</authors>
<title>Corpus-based induction of syntactic structure: Models of dependency and constituency.</title>
<date>2004</date>
<booktitle>In Proceedings of the 42nd Annual Meeting of the ACL.</booktitle>
<contexts>
<context position="1265" citStr="Klein and Manning, 2004" startWordPosition="173" endWordPosition="176">ntext-free languages. We present a simple algorithm for learning these grammars together with a complete self-contained proof of the correctness and efficiency of the algorithm. 1 Introduction Grammar induction, or unsupervised learning of syntax, no longer requires extensive justification and motivation. Both from engineering and cognitive/linguistic angles, it is a central challenge for computational linguistics. However good algorithms for this task are thin on the ground. There are numerous heuristic algorithms, some of which have had significant success in inducing constituent structure (Klein and Manning, 2004). There are algorithms with theoretical guarantees as to their correctness – such as for example Bayesian algorithms for inducing PCFGs (Johnson, 2008), but such algorithms are inefficient: an exponential search algorithm is hidden in the convergence of the MCMC samplers. The efficient algorithms that are actually used are heuristic approximations to the true posteriors. There are algorithms like the Inside-Outside algorithm (Lari and Young, 1990) which are guaranteed to converge efficiently, but not necessarily to the right answer: they converge to a local optimum that may be, and in practice</context>
</contexts>
<marker>Klein, Manning, 2004</marker>
<rawString>Dan Klein and Chris Manning. 2004. Corpus-based induction of syntactic structure: Models of dependency and constituency. In Proceedings of the 42nd Annual Meeting of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="1716" citStr="Lari and Young, 1990" startWordPosition="242" endWordPosition="245">sk are thin on the ground. There are numerous heuristic algorithms, some of which have had significant success in inducing constituent structure (Klein and Manning, 2004). There are algorithms with theoretical guarantees as to their correctness – such as for example Bayesian algorithms for inducing PCFGs (Johnson, 2008), but such algorithms are inefficient: an exponential search algorithm is hidden in the convergence of the MCMC samplers. The efficient algorithms that are actually used are heuristic approximations to the true posteriors. There are algorithms like the Inside-Outside algorithm (Lari and Young, 1990) which are guaranteed to converge efficiently, but not necessarily to the right answer: they converge to a local optimum that may be, and in practice nearly always is very far from the optimum. There are naive enumerative algorithms that are correct, but involve exhaustively enumerating all representations below a certain size (Horning, 1969). There are no correct and efficient algorithms, as there are for parsing, for example. There is a reason for this: from a formal point of view, the problem is intractably hard for the standard representations in the Chomsky hierarchy. Abe and Warmuth (199</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the insideoutside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Pitt</author>
</authors>
<title>Inductive inference, dfa’s, and computational complexity. In</title>
<date>1989</date>
<booktitle>Analogical and Inductive Inference, number 397 in LNAI,</booktitle>
<pages>18--44</pages>
<editor>K. P. Jantke, editor,</editor>
<publisher>Springer-Verglag.</publisher>
<contexts>
<context position="28401" citStr="Pitt, 1989" startWordPosition="5565" endWordPosition="5566">e say it identifies in the limit a class of languages L iff it identifies in the limit all L in L. We say that it identifies the class in polynomial update time iff there is a polynomial p, such that at each step the model uses an amount of computation (and thus also a number of queries) that is less than p(n, l), where n is the number of strings and l is the maximum length of a string in the observed data. We note that this is slightly too weak. It is possible to produce vacuous enumerative algorithms that 34 can learn anything by only processing a logarithmically small prefix of the string (Pitt, 1989). 7 Learning Algorithm We now define a simple learning algorithm, that establishes learnability under this paradigm. There is one minor technical detail we need to deal with. We need to be able to tell when adding a string to a lazy DLG will leave the grammar unchanged. We use a slightly weaker test. Given G1 = (K, D, F) we define as before the equivalence relation between pairs of strings of K, where (u1, v1) —G1 (u2, v2) iff CD(u1) = CD(u2) and CD(v1) = CD(v2) and CD(u1v1) = CD(u2v2). Note that CD(u) = {(l, r)|lur E D}. Given two grammars G1 = (K, D, F) and G2 = (K2, D2, F) where K C_ K2 and</context>
</contexts>
<marker>Pitt, 1989</marker>
<rawString>L. Pitt. 1989. Inductive inference, dfa’s, and computational complexity. In K. P. Jantke, editor, Analogical and Inductive Inference, number 397 in LNAI, pages 18–44. Springer-Verglag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J R Saffran</author>
<author>R N Aslin</author>
<author>E L Newport</author>
</authors>
<title>Statistical learning by eight month old infants.</title>
<date>1996</date>
<journal>Science,</journal>
<pages>274--1926</pages>
<contexts>
<context position="7288" citStr="Saffran et al., 1996" startWordPosition="1200" endWordPosition="1203">s explicitly designed to express the richest system that could reasonably be expected to result from the application of Harris-type procedures to a corpus. Second, empirically we know they work well at least for lexical induction, (Sch¨utze, 1993; Curran, 2003) and are a component of some implemented unsupervised learning systems (Klein and Manning, 2001). Linguists use them as one of the key tests for constituent structure (Carnie, 2008), and finally there is some psycholinguistic evidence that children are sensitive to distributional structure, at least in artificial grammar learning tasks (Saffran et al., 1996). These arguments together suggest that distributional learning has a somewhat privileged status. 3 Lattice grammars Clark (2009) presents the theory of lattice based formalisms starting algebraically from the theory of residuated lattices. Here we will largely ignore this, and start from a straightforward computational treatment. We start by defining the representation. Definition 1. Given a non-empty finite alphabet, E, a distributional lattice grammar (DLG) is a 3- tuple consisting of (K, D, F), where F is a finite subset of E* x E*, such that (A, A) E F, K is a finite subset of E* which co</context>
</contexts>
<marker>Saffran, Aslin, Newport, 1996</marker>
<rawString>J. R. Saffran, R. N. Aslin, and E. L. Newport. 1996. Statistical learning by eight month old infants. Science, 274:1926–1928.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Sch¨utze</author>
</authors>
<title>Part of speech induction from scratch.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st annual meeting of the Association for Computational Linguistics,</booktitle>
<pages>251--258</pages>
<marker>Sch¨utze, 1993</marker>
<rawString>Hinrich Sch¨utze. 1993. Part of speech induction from scratch. In Proceedings of the 31st annual meeting of the Association for Computational Linguistics, pages 251–258.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>