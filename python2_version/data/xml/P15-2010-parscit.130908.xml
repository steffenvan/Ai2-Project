<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.007178">
<title confidence="0.983716">
Domain-Specific Paraphrase Extraction
</title>
<author confidence="0.984573">
Ellie Pavlick1 Juri Ganitkevitch2 Tsz Ping Chan3 Xuchen Yao4
Benjamin Van Durme2,5 Chris Callison-Burch1
</author>
<affiliation confidence="0.9007544">
1Computer and Information Science Department, University of Pennsylvania
2Center for Language and Speech Processing, Johns Hopkins University
3Bloomberg L.P., New York, NY
4kitt.ai∗, Seattle, WA
5Human Language Technology Center of Excellence, Johns Hopkins University
</affiliation>
<sectionHeader confidence="0.978367" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999861818181818">
The validity of applying paraphrase rules
depends on the domain of the text that
they are being applied to. We develop
a novel method for extracting domain-
specific paraphrases. We adapt the bilin-
gual pivoting paraphrase method to bias
the training data to be more like our tar-
get domain of biology. Our best model
results in higher precision while retaining
complete recall, giving a 10% relative im-
provement in AUC.
</bodyText>
<sectionHeader confidence="0.998797" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999843083333333">
Many data-driven paraphrase extraction algo-
rithms have been developed in recent years
(Madnani and Dorr, 2010; Androutsopoulos and
Malakasiotis, 2010). These algorithms attempt
to learn paraphrase rules, where one phrase can
be replaced with another phrase which has equiv-
alent meaning in at least some context. Deter-
mining whether a paraphrase is appropriate for
a specific context is a difficult problem (Bhagat
and Hovy, 2013), encompassing issues of syntax
(Callison-Burch, 2008), word sense (Apidianaki et
al., 2014), and style (Xu et al., 2012; Pavlick and
Nenkova, 2015). To date, the question of how do-
main effects paraphrase has been left unexplored.
Although most paraphrase extraction algo-
rithms attempt to estimate a confidence with which
a paraphrase rule might apply, these scores are
not differentiated by domain, and instead corre-
spond to the general domain represented by the
model’s training data. As illustrated by Table 1,
paraphrases that are highly probable in the gen-
eral domain (e.g. hot = sexy) can be extremely
improbable in more specialized domains like biol-
ogy. Dominant word senses change depending on
</bodyText>
<footnote confidence="0.504807">
∗Incubated by the Allen Institute for Artificial Intelli-
gence.
</footnote>
<table confidence="0.26101675">
General Biology
hot warm, sexy, exciting heated, warm, thermal
treat address, handle, buy cure, fight, kill
head leader, boss, mind skull, brain, cranium
</table>
<tableCaption confidence="0.95307675">
Table 1: Examples of domain-sensitive paraphrases. Most
paraphrase extraction techniques learn paraphrases for a mix
of senses that work well in general. But in specific domains,
paraphrasing should be sensitive to specialized language use.
</tableCaption>
<bodyText confidence="0.999737285714286">
domain: the verb treat is used in expressions like
treat you to dinner in conversational domains ver-
sus treat an infection in biology. This domain shift
changes the acceptability of its paraphrases.
We address the problem of customizing para-
phrase models to specific target domains. We ex-
plore the following ideas:
</bodyText>
<listItem confidence="0.98616025">
1. We sort sentences in the training corpus
based on how well they represent the target
domain, and then extract paraphrases from a
subsample of the most domain-like data.
2. We improve our domain-specific paraphrases
by weighting each training example based on
its domain score, instead of treating each ex-
ample equally.
3. We dramatically improve recall while main-
taining precision by combining the subsam-
pled in-domain paraphrase scores with the
general-domain paraphrase scores.
</listItem>
<sectionHeader confidence="0.971333" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999826875">
The paraphrase extraction algorithm that we cus-
tomize is the bilingual pivoting method (Bannard
and Callison-Burch, 2005) that was used to create
PPDB, the paraphrase database (Ganitkevitch et
al., 2013). To perform the subsampling, we adapt
and improve the method that Moore and Lewis
(2010) originally developed for domain-specific
language models in machine translation.
</bodyText>
<page confidence="0.955771">
57
</page>
<bodyText confidence="0.433997">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57–62,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</bodyText>
<subsectionHeader confidence="0.993176">
2.1 Paraphrase extraction
</subsectionHeader>
<bodyText confidence="0.9996364">
Paraphrases can be extracted via bilingual pivot-
ing. Intuitively, if two English phrases e1 and e2
translate to the same foreign phrase f, we can as-
sume that e1 and e2 have similar meaning, and
thus we can “pivot” over f and extract (e1, e2) as a
paraphrase pair. Since many possible paraphrases
are extracted in this way, and since they vary in
quality (in PPDB, the verb treat has 1,160 poten-
tial paraphrases, including address, handle, deal
with, care for, cure him, ’m paying, and ’s on the
house), it is necessary to assign some measure of
confidence to each paraphrase rule. Bannard and
Callison-Burch (2005) defined a conditional para-
phrase probability p(e2|e1) by marginalizing over
all shared foreign-language translations f:
</bodyText>
<equation confidence="0.988419">
�p(e2|e1) ^ p(e2|f)p(f|e1) (1)
f
</equation>
<bodyText confidence="0.9999895">
where p(e2|f) and p(f|e1) are translation model
probabilities estimated from the bilingual data.
Equation 1 approximates the probability with
which e1 can paraphrase as e2, but its estimate in-
evitably reflects the domain and style of the bilin-
gual training text. If e1 is a polysemous word,
the highest probabilities will be assigned to para-
phrases of the most frequently occurring sense of
e1, and lower probabilities to less frequent senses.
This results in inaccurate probability estimates
when moving to a domain with different sense dis-
tributions compared to the training corpus.
</bodyText>
<subsectionHeader confidence="0.999243">
2.2 Sorting by domain specificity
</subsectionHeader>
<bodyText confidence="0.9999959">
The crux of our method is to train a paraphrase
model on data from the same domain as the one in
which the paraphrases will be used. In practice, it
is unrealistic that we will be able to find bilingual
parallel corpora precompiled for each domain of
interest. We instead subsample from a large bitext,
biasing the sample towards the target domain.
We adapt and extend a method developed by
Moore and Lewis (2010) (henceforth M-L), which
builds a domain-specific sub-corpus from a large,
general-domain corpus. The M-L method assigns
a score to each sentence in the large corpus based
on two language models, one trained on a sam-
ple of target domain text and one trained on the
general domain. We want to identify sentences
which are similar to our target domain and dissim-
ilar from the general domain. M-L captures this
notion using the difference in the cross-entropies
according to each language model (LM). That is,
for a sentence si, we compute
</bodyText>
<equation confidence="0.984991">
ai = Htgt(si) − Hgen(si) (2)
</equation>
<bodyText confidence="0.999982666666667">
where Htgt is the cross-entropy under the in-
domain language model and Hgen is the cross-
entropy under the general domain LM. Cross-
entropy is monotonically equivalent to LM per-
plexity, in which lower scores imply a better fit.
Lower ai signifies greater domain-specificity.
</bodyText>
<sectionHeader confidence="0.998761" genericHeader="method">
3 Domain-Specific Paraphrases
</sectionHeader>
<bodyText confidence="0.996288461538462">
To apply the M-L method to paraphrasing, we
need a sample of in-domain monolingual text.
This data is not directly used to extract para-
phrases, but instead to train an n-gram LM for the
target domain. We compute ai for the English side
of every sentence pair in our bilingual data, using
the target domain LM and the general domain LM.
We sort the entire bilingual training corpus so that
the closer a sentence pair is to the top of the list,
the more specific it is to our target domain.
We can apply Bannard and Callison-Burch
(2005)’s bilingual pivoting paraphrase extraction
algorithm to this sorted bitext in several ways:
</bodyText>
<listItem confidence="0.950691866666666">
1. By choosing a threshold value for ai and dis-
carding all sentence pairs that fall outside
of that threshold, we can extract paraphrases
from a subsampled bitext that approximates
the target domain.
2. Instead of simply extracting from a subsam-
pled corpus (where each training example is
equally weighted), we can weight each train-
ing example proportional to ai when comput-
ing the paraphrase scores.
3. We can combine multiple paraphrase scores:
one derived from the original corpus and one
from the subsample. This has the advantage
of producing the full set of paraphrases that
can be extracted from the entire bitext.
</listItem>
<sectionHeader confidence="0.99879" genericHeader="method">
4 Experimental Conditions
</sectionHeader>
<bodyText confidence="0.992419857142857">
Domain data We evaluate our domain-specific
paraphrasing model in the target domain of biol-
ogy. Our monolingual in-domain data is a com-
bination of text from the GENIA database (Kim
et al., 2003) and text from an introductory biology
textbook. Our bilingual general-domain data is the
109 word parallel corpus (Callison-Burch et al.,
</bodyText>
<page confidence="0.991538">
58
</page>
<bodyText confidence="0.999864760869565">
2009), a collection of French-English parallel data
covering a mix of genres from legal text (Stein-
berger et al., 2006) to movie subtitles (Tiedemann,
2012). We use 5-gram language models with
Kneser-Ney discounting (Heafield et al., 2013).
Evaluation We measure the precision and recall
of paraphrase pairs produced by each of our mod-
els by collecting human judgments of what para-
phrases are acceptable in sentences drawn from
the target domain and in sentences drawn from the
general domain. We sample 15K sentences from
our biology data, and 10K general-domain sen-
tences from Wikipedia. We select a phrase from
each sentence, and show the list of candidate para-
phrases1 to 5 human judges. Judges make a binary
decision about whether each paraphrase is appro-
priate given the domain-specific context. We con-
sider a paraphrase rule to be good in the domain if
it is judged to be good in least one context by the
majority of judges. See Supplementary Materials
for a detailed description of our methodology.
Baseline We run normal paraphrase extraction
over the entire 109 word parallel corpus (which
has 828M words on the English side) without any
attempt to bias it toward the target domain. We
refer this system as General.
Subsampling After sorting the 109 word paral-
lel corpus by Equation 2, we chose several thresh-
old values for subsampling, keeping only top-
ranked T words of the bitext. We train models on
for several values of T (1.5M, 7M, 35M, and 166M
words). We refer to these model as M-L,T=T.
M-L Change Point We test a model where T is
set at the point where ai switches from negative
to positive. This includes all sentences which look
more like the target domain than the general. This
threshold is equivalent to sampling 20M words.
Weighted Counts Instead of weighting each
subsampled sentence equally, we test a novel ex-
tension of M-L in which we weight each sentence
proportional to ai when computing p(e2|e1).
Combined Models We combine the subsam-
pled models with the general model, using binary
logistic regression to combine the p(e2|e1) esti-
mate of the general model and that of the domain-
specific model. We use 1,000 labeled pairs from
</bodyText>
<footnote confidence="0.809335">
1The candidates paraphrases constitute the full set of para-
phrases that can be extracted from our training corpus.
</footnote>
<figureCaption confidence="0.997809">
Figure 1: Precision-recall curves for paraphrase pairs ex-
tracted by models trained on data from each of the described
subsampling methods. These curves are generated using the
15k manually annotated sentences in the biology domain.
</figureCaption>
<bodyText confidence="0.9995675">
the target domain to set the regression weights.
This tuning set is disjoint from the test set.
</bodyText>
<sectionHeader confidence="0.996984" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.997503357142857">
What is the effect of subsampling? Figure 1
compares the precision and recall of the differ-
ent subsampling methods against the baseline of
training on everything, when they are evaluated
on manually labeled test paraphrases from the bi-
ology domain. All of subsampled models have a
higher precision than the baseline General model,
except for the largest of the subsampled models
(which was trained on sentence pairs with 166M
words - many of which are more like the general
domain than the biology domain).
The subsampled models have reduced recall
since many of the paraphrases that occur in the full
109 word bilingual training corpus do not occur in
the subsamples. As we increase T we improve re-
call at the expense of precision, since we are in-
cluding training data that is less and less like our
target domain. The highest precision model based
on the vanilla M-L method is M-L Change Point,
which sets the subsample size to include exactly
those sentence pairs that look more like the target
domain than the general domain.
Our novel extension of the M-L model (M-L
Weighted) provides further improvements. Here,
we weight each sentence pair in the bilingual train-
ing corpus proportional to ai when computing
the paraphrase scores. Specifically, we weight
the counting during the bilingual pivoting so that
</bodyText>
<page confidence="0.993731">
59
</page>
<figure confidence="0.989239">
(a) Biology domain (b) General domain
</figure>
<figureCaption confidence="0.999065">
Figure 2: Performance of models build by combining small domain-specific models trained on subsampled data with general
domain models trained on all the data. Performance in the general domain are shown as a control.
</figureCaption>
<figure confidence="0.88162725">
general / bio-spec. general / bio-spec.
air aerial / atmosphere fruit result / fruiting
balance pay / equilibrate heated lively / hot
breaks pauses / ruptures motion proposal / movement
</figure>
<bodyText confidence="0.996671176470588">
rather than each occurrence counting as 1, each
occurrence counts as the ratio of the sentence’s
cross-entropies: H���
H��� . The top-ranked sentence
pairs receive an exaggerated count of 52, while
the bottom ones receive a tiny factional count of
0.0068. Thus, paraphrases extracted from sen-
tence pairs that are unlike the biology domain re-
ceive very low scores. This allows us to achieve
higher recall by incorporating more training data,
while also improving the precision.
What is the benefit of combining models? We
have demonstrated that extracting paraphrases
from subsampled data results in higher precision
domain-specific paraphrases. But these models
extract only a fraction of the paraphrases that are
extracted by a general model trained on the full
bitext, resulting in a lower recall.
We dramatically improve the recall of our
domain-specific models by combining the small
subsampled models with the large general-domain
model. We use binary logistic regression to com-
bine the p(e2|e1) estimate of the general model
with that of each domain-specific model. Figure
2(a) shows that we are able to extend the recall
of our domain-specific models to match the recall
of the full general-domain model. The precision
scores remain higher for the domain-specific mod-
els. Our novel M-L Weighted model performs the
best. Table 3 gives the area under the curve (AUC).
The best combination improves AUC by more
than 4 points absolute (&gt;10 points relative) in the
biology domain. Table 2 provides examples of
paraphrases extracted using our domain-specific
</bodyText>
<tableCaption confidence="0.99459175">
Table 2: Top paraphrase under the general and the best
domain-specific model, General+M-L Weighted.
Table 3: AUC (× 100) for each model in the biology domain
from Figure 2(a).
</tableCaption>
<bodyText confidence="0.746189">
model for biology versus the baseline model.
</bodyText>
<sectionHeader confidence="0.999736" genericHeader="related work">
6 Related Work
</sectionHeader>
<bodyText confidence="0.980039461538462">
Domain-specific paraphrasing has not received
previous attention, but there is relevant prior work
on domain-specific machine translation (MT). We
build on the Moore-Lewis method, which has
been used for language models (Moore and Lewis,
2010) and translation models (Axelrod et al.,
2011). Similar methods use LM perplexity to
rank sentences (Gao et al., 2002; Yasuda et al.,
2008), rather than the difference in cross-entropy.
Within MT, Foster and Kuhn (2007) used log-
linear weightings of translation probabilities to
combine models trained in different domains, as
we do here. Relevant to our proposed method of
</bodyText>
<table confidence="0.995223875">
General AUC Aabsolute Arelative
39.5 – –
Gen.+M-L,T=1 40.8 +1.3 +3.3
Gen.+M-L,T=145 40.8 +1.3 +3.3
Gen.+M-L,T=29 41.2 +1.7 +4.3
Gen.+M-L CP 41.9 +2.4 +6.1
Gen.+M-L,T=6 42.3 +2.8 +7.1
Gen.+M-L Weighted 43.7 +4.2 +10.6
</table>
<page confidence="0.998188">
60
</page>
<bodyText confidence="0.999689666666667">
fractional counting, (Madnani et al., 2007) used
introduced a count-centric approach to paraphrase
probability estimation. Matsoukas et al. (2009)
and Foster et al. (2010) explored weighted training
sentences for MT, but set weights discriminatively
based on sentence-level features.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999980576923077">
We have discussed the new problem of extracting
domain-specific paraphrases. We adapt a method
from machine translation to the task of learn-
ing domain-biased paraphrases from bilingual cor-
pora. We introduce two novel extensions to this
method. Our best domain-specific model dramat-
ically improves paraphrase quality for the target
domain.
Acknowledgements This research was sup-
ported by the Allen Institute for Artificial Intel-
ligence (AI2), the Human Language Technology
Center of Excellence (HLTCOE), and by gifts
from the Alfred P. Sloan Foundation, Google, and
Facebook. This material is based in part on re-
search sponsored by the NSF under grant IIS-
1249516 and DARPA under agreement number
FA8750-13-2-0017 (the DEFT program). The
U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes.
The views and conclusions contained in this pub-
lication are those of the authors and should not be
interpreted as representing official policies or en-
dorsements of DARPA or the U.S. Government.
We would like to thank Luke Orland for his con-
tributions to this research, and to thank the anony-
mous reviewers for their thoughtful comments.
</bodyText>
<sectionHeader confidence="0.998468" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999184940298507">
Ion Androutsopoulos and Prodromos Malakasiotis.
2010. A survey of paraphrasing and textual entail-
ment methods. JAIR, pages 135–187.
Marianna Apidianaki, Emilia Verzeni, and Diana Mc-
Carthy. 2014. Semantic clustering of pivot para-
phrases. In LREC.
Amittai Axelrod, Xiaodong He, and Jianfeng Gao.
2011. Domain adaptation via pseudo in-domain data
selection. In EMNLP, pages 355–362.
Colin Bannard and Chris Callison-Burch. 2005. Para-
phrasing with bilingual parallel corpora. In ACL,
pages 597–604.
Rahul Bhagat and Eduard Hovy. 2013. What is a para-
phrase? Computational Linguistics, 39(3):463–472.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the Fourth Workshop on Statistical
Machine Translation.
Chris Callison-Burch. 2008. Syntactic constraints
on paraphrases extracted from parallel corpora. In
EMNLP, pages 196–205. Association for Computa-
tional Linguistics.
George Foster and Roland Kuhn. 2007. Mixture-
model adaptation for smt.
George Foster, Cyril Goutte, and Roland Kuhn. 2010.
Discriminative instance weighting for domain adap-
tation in statistical machine translation. In EMNLP,
pages 451–459.
Juri Ganitkevitch, Benjamin Van Durme, and Chris
Callison-Burch. 2013. PPDB: The paraphrase
database. In NAACL-HLT, pages 758–764, Atlanta,
Georgia, June.
Jianfeng Gao, Joshua Goodman, Mingjing Li, and Kai-
Fu Lee. 2002. Toward a unified approach to sta-
tistical language modeling for chinese. ACM Trans-
actions on Asian Language Information Processing
(TALIP), 1(1):3–33.
Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H
Clark, and Philipp Koehn. 2013. Scalable modified
kneser-ney language model estimation. In ACL.
J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsu-
jii. 2003. Genia corpusa semantically annotated
corpus for bio-textmining. Bioinformatics, 19(suppl
1):i180–i182.
Nitin Madnani and Bonnie J. Dorr. 2010. Generating
phrasal and sentential paraphrases: A survey of data-
driven methods. Computational Linguistics, 36.
Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and
Bonnie Dorr. 2007. Using paraphrases for parame-
ter tuning in statistical machine translation. In Pro-
ceedings of the Workshop on Machine Translation.
Spyros Matsoukas, Antti-Veikko I Rosti, and Bing
Zhang. 2009. Discriminative corpus weight esti-
mation for machine translation. In EMNLP, pages
708–717.
Robert C Moore and William Lewis. 2010. Intelligent
selection of language model training data. In ACL,
pages 220–224.
Ellie Pavlick and Ani Nenkova. 2015. Inducing lexical
style properties for paraphrase and genre differenti-
ation. In NAACL.
Ralf Steinberger, Bruno Pouliquen, Anna Widiger,
Camelia Ignat, Tomaz Erjavec, Dan Tufis, and
D´aniel Varga. 2006. The jrc-acquis: A multilingual
aligned parallel corpus with 20+ languages. arXiv
preprint.
</reference>
<page confidence="0.983271">
61
</page>
<reference confidence="0.999599">
J¨org Tiedemann. 2012. Parallel data, tools and inter-
faces in opus. In LREC, pages 2214–2218.
Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and
Colin Cherry. 2012. Paraphrasing for style. In
COLING, pages 2899–2914.
Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto,
and Eiichiro Sumita. 2008. Method of selecting
training data to build a compact and efficient trans-
lation model. In IJCNLP, pages 655–660.
</reference>
<page confidence="0.999189">
62
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.250159">
<title confidence="0.83293">Domain-Specific Paraphrase Extraction Juri Tsz Ping Xuchen</title>
<author confidence="0.999486">Van_Chris</author>
<affiliation confidence="0.81214">and Information Science Department, University of for Language and Speech Processing, Johns Hopkins L.P., New York, Seattle, Language Technology Center of Excellence, Johns Hopkins University</affiliation>
<abstract confidence="0.993219583333333">The validity of applying paraphrase rules depends on the domain of the text that they are being applied to. We develop a novel method for extracting domainspecific paraphrases. We adapt the bilingual pivoting paraphrase method to bias the training data to be more like our target domain of biology. Our best model results in higher precision while retaining complete recall, giving a 10% relative improvement in AUC.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Ion Androutsopoulos</author>
<author>Prodromos Malakasiotis</author>
</authors>
<title>A survey of paraphrasing and textual entailment methods.</title>
<date>2010</date>
<journal>JAIR,</journal>
<pages>135--187</pages>
<contexts>
<context position="1003" citStr="Androutsopoulos and Malakasiotis, 2010" startWordPosition="141" endWordPosition="144"> Center of Excellence, Johns Hopkins University Abstract The validity of applying paraphrase rules depends on the domain of the text that they are being applied to. We develop a novel method for extracting domainspecific paraphrases. We adapt the bilingual pivoting paraphrase method to bias the training data to be more like our target domain of biology. Our best model results in higher precision while retaining complete recall, giving a 10% relative improvement in AUC. 1 Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which</context>
</contexts>
<marker>Androutsopoulos, Malakasiotis, 2010</marker>
<rawString>Ion Androutsopoulos and Prodromos Malakasiotis. 2010. A survey of paraphrasing and textual entailment methods. JAIR, pages 135–187.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marianna Apidianaki</author>
<author>Emilia Verzeni</author>
<author>Diana McCarthy</author>
</authors>
<title>Semantic clustering of pivot paraphrases.</title>
<date>2014</date>
<booktitle>In LREC.</booktitle>
<contexts>
<context position="1374" citStr="Apidianaki et al., 2014" startWordPosition="198" endWordPosition="201">er precision while retaining complete recall, giving a 10% relative improvement in AUC. 1 Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change dep</context>
</contexts>
<marker>Apidianaki, Verzeni, McCarthy, 2014</marker>
<rawString>Marianna Apidianaki, Emilia Verzeni, and Diana McCarthy. 2014. Semantic clustering of pivot paraphrases. In LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Amittai Axelrod</author>
<author>Xiaodong He</author>
<author>Jianfeng Gao</author>
</authors>
<title>Domain adaptation via pseudo in-domain data selection.</title>
<date>2011</date>
<booktitle>In EMNLP,</booktitle>
<pages>355--362</pages>
<contexts>
<context position="14677" citStr="Axelrod et al., 2011" startWordPosition="2350" endWordPosition="2353">he biology domain. Table 2 provides examples of paraphrases extracted using our domain-specific Table 2: Top paraphrase under the general and the best domain-specific model, General+M-L Weighted. Table 3: AUC (× 100) for each model in the biology domain from Figure 2(a). model for biology versus the baseline model. 6 Related Work Domain-specific paraphrasing has not received previous attention, but there is relevant prior work on domain-specific machine translation (MT). We build on the Moore-Lewis method, which has been used for language models (Moore and Lewis, 2010) and translation models (Axelrod et al., 2011). Similar methods use LM perplexity to rank sentences (Gao et al., 2002; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of General AUC Aabsolute Arelative 39.5 – – Gen.+M-L,T=1 40.8 +1.3 +3.3 Gen.+M-L,T=145 40.8 +1.3 +3.3 Gen.+M-L,T=29 41.2 +1.7 +4.3 Gen.+M-L CP 41.9 +2.4 +6.1 Gen.+M-L,T=6 42.3 +2.8 +7.1 Gen.+M-L Weighted 43.7 +4.2 +10.6 60 fractional counting, (Madnani et al., 2007) used intr</context>
</contexts>
<marker>Axelrod, He, Gao, 2011</marker>
<rawString>Amittai Axelrod, Xiaodong He, and Jianfeng Gao. 2011. Domain adaptation via pseudo in-domain data selection. In EMNLP, pages 355–362.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Bannard</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Paraphrasing with bilingual parallel corpora.</title>
<date>2005</date>
<booktitle>In ACL,</booktitle>
<pages>597--604</pages>
<contexts>
<context position="3374" citStr="Bannard and Callison-Burch, 2005" startWordPosition="507" endWordPosition="510">ollowing ideas: 1. We sort sentences in the training corpus based on how well they represent the target domain, and then extract paraphrases from a subsample of the most domain-like data. 2. We improve our domain-specific paraphrases by weighting each training example based on its domain score, instead of treating each example equally. 3. We dramatically improve recall while maintaining precision by combining the subsampled in-domain paraphrase scores with the general-domain paraphrase scores. 2 Background The paraphrase extraction algorithm that we customize is the bilingual pivoting method (Bannard and Callison-Burch, 2005) that was used to create PPDB, the paraphrase database (Ganitkevitch et al., 2013). To perform the subsampling, we adapt and improve the method that Moore and Lewis (2010) originally developed for domain-specific language models in machine translation. 57 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57–62, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2.1 Paraphrase extraction Paraphrases can be extracted via bilingual p</context>
<context position="7133" citStr="Bannard and Callison-Burch (2005)" startWordPosition="1123" endWordPosition="1126"> imply a better fit. Lower ai signifies greater domain-specificity. 3 Domain-Specific Paraphrases To apply the M-L method to paraphrasing, we need a sample of in-domain monolingual text. This data is not directly used to extract paraphrases, but instead to train an n-gram LM for the target domain. We compute ai for the English side of every sentence pair in our bilingual data, using the target domain LM and the general domain LM. We sort the entire bilingual training corpus so that the closer a sentence pair is to the top of the list, the more specific it is to our target domain. We can apply Bannard and Callison-Burch (2005)’s bilingual pivoting paraphrase extraction algorithm to this sorted bitext in several ways: 1. By choosing a threshold value for ai and discarding all sentence pairs that fall outside of that threshold, we can extract paraphrases from a subsampled bitext that approximates the target domain. 2. Instead of simply extracting from a subsampled corpus (where each training example is equally weighted), we can weight each training example proportional to ai when computing the paraphrase scores. 3. We can combine multiple paraphrase scores: one derived from the original corpus and one from the subsam</context>
</contexts>
<marker>Bannard, Callison-Burch, 2005</marker>
<rawString>Colin Bannard and Chris Callison-Burch. 2005. Paraphrasing with bilingual parallel corpora. In ACL, pages 597–604.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rahul Bhagat</author>
<author>Eduard Hovy</author>
</authors>
<title>What is a paraphrase?</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>3</issue>
<contexts>
<context position="1282" citStr="Bhagat and Hovy, 2013" startWordPosition="186" endWordPosition="189">training data to be more like our target domain of biology. Our best model results in higher precision while retaining complete recall, giving a 10% relative improvement in AUC. 1 Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be ex</context>
</contexts>
<marker>Bhagat, Hovy, 2013</marker>
<rawString>Rahul Bhagat and Eduard Hovy. 2013. What is a paraphrase? Computational Linguistics, 39(3):463–472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation.</booktitle>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Syntactic constraints on paraphrases extracted from parallel corpora.</title>
<date>2008</date>
<booktitle>In EMNLP,</booktitle>
<pages>196--205</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1336" citStr="Callison-Burch, 2008" startWordPosition="194" endWordPosition="195">ogy. Our best model results in higher precision while retaining complete recall, giving a 10% relative improvement in AUC. 1 Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like bi</context>
</contexts>
<marker>Callison-Burch, 2008</marker>
<rawString>Chris Callison-Burch. 2008. Syntactic constraints on paraphrases extracted from parallel corpora. In EMNLP, pages 196–205. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Mixturemodel adaptation for smt.</title>
<date>2007</date>
<contexts>
<context position="14850" citStr="Foster and Kuhn (2007)" startWordPosition="2378" endWordPosition="2381">, General+M-L Weighted. Table 3: AUC (× 100) for each model in the biology domain from Figure 2(a). model for biology versus the baseline model. 6 Related Work Domain-specific paraphrasing has not received previous attention, but there is relevant prior work on domain-specific machine translation (MT). We build on the Moore-Lewis method, which has been used for language models (Moore and Lewis, 2010) and translation models (Axelrod et al., 2011). Similar methods use LM perplexity to rank sentences (Gao et al., 2002; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of General AUC Aabsolute Arelative 39.5 – – Gen.+M-L,T=1 40.8 +1.3 +3.3 Gen.+M-L,T=145 40.8 +1.3 +3.3 Gen.+M-L,T=29 41.2 +1.7 +4.3 Gen.+M-L CP 41.9 +2.4 +6.1 Gen.+M-L,T=6 42.3 +2.8 +7.1 Gen.+M-L Weighted 43.7 +4.2 +10.6 60 fractional counting, (Madnani et al., 2007) used introduced a count-centric approach to paraphrase probability estimation. Matsoukas et al. (2009) and Foster et al. (2010) explored weighted training sentences for MT, but set w</context>
</contexts>
<marker>Foster, Kuhn, 2007</marker>
<rawString>George Foster and Roland Kuhn. 2007. Mixturemodel adaptation for smt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Foster</author>
<author>Cyril Goutte</author>
<author>Roland Kuhn</author>
</authors>
<title>Discriminative instance weighting for domain adaptation in statistical machine translation.</title>
<date>2010</date>
<booktitle>In EMNLP,</booktitle>
<pages>451--459</pages>
<contexts>
<context position="15395" citStr="Foster et al. (2010)" startWordPosition="2460" endWordPosition="2463">her than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of General AUC Aabsolute Arelative 39.5 – – Gen.+M-L,T=1 40.8 +1.3 +3.3 Gen.+M-L,T=145 40.8 +1.3 +3.3 Gen.+M-L,T=29 41.2 +1.7 +4.3 Gen.+M-L CP 41.9 +2.4 +6.1 Gen.+M-L,T=6 42.3 +2.8 +7.1 Gen.+M-L Weighted 43.7 +4.2 +10.6 60 fractional counting, (Madnani et al., 2007) used introduced a count-centric approach to paraphrase probability estimation. Matsoukas et al. (2009) and Foster et al. (2010) explored weighted training sentences for MT, but set weights discriminatively based on sentence-level features. 7 Conclusion We have discussed the new problem of extracting domain-specific paraphrases. We adapt a method from machine translation to the task of learning domain-biased paraphrases from bilingual corpora. We introduce two novel extensions to this method. Our best domain-specific model dramatically improves paraphrase quality for the target domain. Acknowledgements This research was supported by the Allen Institute for Artificial Intelligence (AI2), the Human Language Technology Ce</context>
</contexts>
<marker>Foster, Goutte, Kuhn, 2010</marker>
<rawString>George Foster, Cyril Goutte, and Roland Kuhn. 2010. Discriminative instance weighting for domain adaptation in statistical machine translation. In EMNLP, pages 451–459.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Benjamin Van Durme</author>
<author>Chris Callison-Burch</author>
</authors>
<title>PPDB: The paraphrase database.</title>
<date>2013</date>
<booktitle>In NAACL-HLT,</booktitle>
<pages>758--764</pages>
<location>Atlanta, Georgia,</location>
<marker>Ganitkevitch, Van Durme, Callison-Burch, 2013</marker>
<rawString>Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch. 2013. PPDB: The paraphrase database. In NAACL-HLT, pages 758–764, Atlanta, Georgia, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jianfeng Gao</author>
<author>Joshua Goodman</author>
<author>Mingjing Li</author>
<author>KaiFu Lee</author>
</authors>
<title>Toward a unified approach to statistical language modeling for chinese.</title>
<date>2002</date>
<journal>ACM Transactions on Asian Language Information Processing (TALIP),</journal>
<volume>1</volume>
<issue>1</issue>
<contexts>
<context position="14748" citStr="Gao et al., 2002" startWordPosition="2362" endWordPosition="2365">our domain-specific Table 2: Top paraphrase under the general and the best domain-specific model, General+M-L Weighted. Table 3: AUC (× 100) for each model in the biology domain from Figure 2(a). model for biology versus the baseline model. 6 Related Work Domain-specific paraphrasing has not received previous attention, but there is relevant prior work on domain-specific machine translation (MT). We build on the Moore-Lewis method, which has been used for language models (Moore and Lewis, 2010) and translation models (Axelrod et al., 2011). Similar methods use LM perplexity to rank sentences (Gao et al., 2002; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of General AUC Aabsolute Arelative 39.5 – – Gen.+M-L,T=1 40.8 +1.3 +3.3 Gen.+M-L,T=145 40.8 +1.3 +3.3 Gen.+M-L,T=29 41.2 +1.7 +4.3 Gen.+M-L CP 41.9 +2.4 +6.1 Gen.+M-L,T=6 42.3 +2.8 +7.1 Gen.+M-L Weighted 43.7 +4.2 +10.6 60 fractional counting, (Madnani et al., 2007) used introduced a count-centric approach to paraphrase probability estimation. M</context>
</contexts>
<marker>Gao, Goodman, Li, Lee, 2002</marker>
<rawString>Jianfeng Gao, Joshua Goodman, Mingjing Li, and KaiFu Lee. 2002. Toward a unified approach to statistical language modeling for chinese. ACM Transactions on Asian Language Information Processing (TALIP), 1(1):3–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenneth Heafield</author>
<author>Ivan Pouzyrevsky</author>
<author>Jonathan H Clark</author>
<author>Philipp Koehn</author>
</authors>
<title>Scalable modified kneser-ney language model estimation.</title>
<date>2013</date>
<booktitle>In ACL.</booktitle>
<contexts>
<context position="8449" citStr="Heafield et al., 2013" startWordPosition="1332" endWordPosition="1335">m the entire bitext. 4 Experimental Conditions Domain data We evaluate our domain-specific paraphrasing model in the target domain of biology. Our monolingual in-domain data is a combination of text from the GENIA database (Kim et al., 2003) and text from an introductory biology textbook. Our bilingual general-domain data is the 109 word parallel corpus (Callison-Burch et al., 58 2009), a collection of French-English parallel data covering a mix of genres from legal text (Steinberger et al., 2006) to movie subtitles (Tiedemann, 2012). We use 5-gram language models with Kneser-Ney discounting (Heafield et al., 2013). Evaluation We measure the precision and recall of paraphrase pairs produced by each of our models by collecting human judgments of what paraphrases are acceptable in sentences drawn from the target domain and in sentences drawn from the general domain. We sample 15K sentences from our biology data, and 10K general-domain sentences from Wikipedia. We select a phrase from each sentence, and show the list of candidate paraphrases1 to 5 human judges. Judges make a binary decision about whether each paraphrase is appropriate given the domain-specific context. We consider a paraphrase rule to be g</context>
</contexts>
<marker>Heafield, Pouzyrevsky, Clark, Koehn, 2013</marker>
<rawString>Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H Clark, and Philipp Koehn. 2013. Scalable modified kneser-ney language model estimation. In ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J-D Kim</author>
<author>Tomoko Ohta</author>
<author>Yuka Tateisi</author>
<author>Junichi Tsujii</author>
</authors>
<title>Genia corpusa semantically annotated corpus for bio-textmining. Bioinformatics,</title>
<date>2003</date>
<pages>1--180</pages>
<contexts>
<context position="8068" citStr="Kim et al., 2003" startWordPosition="1274" endWordPosition="1277">acting from a subsampled corpus (where each training example is equally weighted), we can weight each training example proportional to ai when computing the paraphrase scores. 3. We can combine multiple paraphrase scores: one derived from the original corpus and one from the subsample. This has the advantage of producing the full set of paraphrases that can be extracted from the entire bitext. 4 Experimental Conditions Domain data We evaluate our domain-specific paraphrasing model in the target domain of biology. Our monolingual in-domain data is a combination of text from the GENIA database (Kim et al., 2003) and text from an introductory biology textbook. Our bilingual general-domain data is the 109 word parallel corpus (Callison-Burch et al., 58 2009), a collection of French-English parallel data covering a mix of genres from legal text (Steinberger et al., 2006) to movie subtitles (Tiedemann, 2012). We use 5-gram language models with Kneser-Ney discounting (Heafield et al., 2013). Evaluation We measure the precision and recall of paraphrase pairs produced by each of our models by collecting human judgments of what paraphrases are acceptable in sentences drawn from the target domain and in sente</context>
</contexts>
<marker>Kim, Ohta, Tateisi, Tsujii, 2003</marker>
<rawString>J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsujii. 2003. Genia corpusa semantically annotated corpus for bio-textmining. Bioinformatics, 19(suppl 1):i180–i182.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Bonnie J Dorr</author>
</authors>
<title>Generating phrasal and sentential paraphrases: A survey of datadriven methods.</title>
<date>2010</date>
<journal>Computational Linguistics,</journal>
<volume>36</volume>
<contexts>
<context position="962" citStr="Madnani and Dorr, 2010" startWordPosition="137" endWordPosition="140">uman Language Technology Center of Excellence, Johns Hopkins University Abstract The validity of applying paraphrase rules depends on the domain of the text that they are being applied to. We develop a novel method for extracting domainspecific paraphrases. We adapt the bilingual pivoting paraphrase method to bias the training data to be more like our target domain of biology. Our best model results in higher precision while retaining complete recall, giving a 10% relative improvement in AUC. 1 Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms at</context>
</contexts>
<marker>Madnani, Dorr, 2010</marker>
<rawString>Nitin Madnani and Bonnie J. Dorr. 2010. Generating phrasal and sentential paraphrases: A survey of datadriven methods. Computational Linguistics, 36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Nitin Madnani</author>
<author>Necip Fazil Ayan</author>
<author>Philip Resnik</author>
<author>Bonnie Dorr</author>
</authors>
<title>Using paraphrases for parameter tuning in statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the Workshop on Machine Translation.</booktitle>
<contexts>
<context position="15267" citStr="Madnani et al., 2007" startWordPosition="2442" endWordPosition="2445">n models (Axelrod et al., 2011). Similar methods use LM perplexity to rank sentences (Gao et al., 2002; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of General AUC Aabsolute Arelative 39.5 – – Gen.+M-L,T=1 40.8 +1.3 +3.3 Gen.+M-L,T=145 40.8 +1.3 +3.3 Gen.+M-L,T=29 41.2 +1.7 +4.3 Gen.+M-L CP 41.9 +2.4 +6.1 Gen.+M-L,T=6 42.3 +2.8 +7.1 Gen.+M-L Weighted 43.7 +4.2 +10.6 60 fractional counting, (Madnani et al., 2007) used introduced a count-centric approach to paraphrase probability estimation. Matsoukas et al. (2009) and Foster et al. (2010) explored weighted training sentences for MT, but set weights discriminatively based on sentence-level features. 7 Conclusion We have discussed the new problem of extracting domain-specific paraphrases. We adapt a method from machine translation to the task of learning domain-biased paraphrases from bilingual corpora. We introduce two novel extensions to this method. Our best domain-specific model dramatically improves paraphrase quality for the target domain. Acknowl</context>
</contexts>
<marker>Madnani, Ayan, Resnik, Dorr, 2007</marker>
<rawString>Nitin Madnani, Necip Fazil Ayan, Philip Resnik, and Bonnie Dorr. 2007. Using paraphrases for parameter tuning in statistical machine translation. In Proceedings of the Workshop on Machine Translation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spyros Matsoukas</author>
<author>Antti-Veikko I Rosti</author>
<author>Bing Zhang</author>
</authors>
<title>Discriminative corpus weight estimation for machine translation. In</title>
<date>2009</date>
<booktitle>EMNLP,</booktitle>
<pages>708--717</pages>
<contexts>
<context position="15370" citStr="Matsoukas et al. (2009)" startWordPosition="2455" endWordPosition="2458">2; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of General AUC Aabsolute Arelative 39.5 – – Gen.+M-L,T=1 40.8 +1.3 +3.3 Gen.+M-L,T=145 40.8 +1.3 +3.3 Gen.+M-L,T=29 41.2 +1.7 +4.3 Gen.+M-L CP 41.9 +2.4 +6.1 Gen.+M-L,T=6 42.3 +2.8 +7.1 Gen.+M-L Weighted 43.7 +4.2 +10.6 60 fractional counting, (Madnani et al., 2007) used introduced a count-centric approach to paraphrase probability estimation. Matsoukas et al. (2009) and Foster et al. (2010) explored weighted training sentences for MT, but set weights discriminatively based on sentence-level features. 7 Conclusion We have discussed the new problem of extracting domain-specific paraphrases. We adapt a method from machine translation to the task of learning domain-biased paraphrases from bilingual corpora. We introduce two novel extensions to this method. Our best domain-specific model dramatically improves paraphrase quality for the target domain. Acknowledgements This research was supported by the Allen Institute for Artificial Intelligence (AI2), the Hum</context>
</contexts>
<marker>Matsoukas, Rosti, Zhang, 2009</marker>
<rawString>Spyros Matsoukas, Antti-Veikko I Rosti, and Bing Zhang. 2009. Discriminative corpus weight estimation for machine translation. In EMNLP, pages 708–717.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Robert C Moore</author>
<author>William Lewis</author>
</authors>
<title>Intelligent selection of language model training data. In</title>
<date>2010</date>
<booktitle>ACL,</booktitle>
<pages>220--224</pages>
<contexts>
<context position="3545" citStr="Moore and Lewis (2010)" startWordPosition="535" endWordPosition="538">data. 2. We improve our domain-specific paraphrases by weighting each training example based on its domain score, instead of treating each example equally. 3. We dramatically improve recall while maintaining precision by combining the subsampled in-domain paraphrase scores with the general-domain paraphrase scores. 2 Background The paraphrase extraction algorithm that we customize is the bilingual pivoting method (Bannard and Callison-Burch, 2005) that was used to create PPDB, the paraphrase database (Ganitkevitch et al., 2013). To perform the subsampling, we adapt and improve the method that Moore and Lewis (2010) originally developed for domain-specific language models in machine translation. 57 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 57–62, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2.1 Paraphrase extraction Paraphrases can be extracted via bilingual pivoting. Intuitively, if two English phrases e1 and e2 translate to the same foreign phrase f, we can assume that e1 and e2 have similar meaning, and thus we can “pivot” o</context>
<context position="5730" citStr="Moore and Lewis (2010)" startWordPosition="882" endWordPosition="885">bilities to less frequent senses. This results in inaccurate probability estimates when moving to a domain with different sense distributions compared to the training corpus. 2.2 Sorting by domain specificity The crux of our method is to train a paraphrase model on data from the same domain as the one in which the paraphrases will be used. In practice, it is unrealistic that we will be able to find bilingual parallel corpora precompiled for each domain of interest. We instead subsample from a large bitext, biasing the sample towards the target domain. We adapt and extend a method developed by Moore and Lewis (2010) (henceforth M-L), which builds a domain-specific sub-corpus from a large, general-domain corpus. The M-L method assigns a score to each sentence in the large corpus based on two language models, one trained on a sample of target domain text and one trained on the general domain. We want to identify sentences which are similar to our target domain and dissimilar from the general domain. M-L captures this notion using the difference in the cross-entropies according to each language model (LM). That is, for a sentence si, we compute ai = Htgt(si) − Hgen(si) (2) where Htgt is the cross-entropy un</context>
<context position="14631" citStr="Moore and Lewis, 2010" startWordPosition="2343" endWordPosition="2346">an 4 points absolute (&gt;10 points relative) in the biology domain. Table 2 provides examples of paraphrases extracted using our domain-specific Table 2: Top paraphrase under the general and the best domain-specific model, General+M-L Weighted. Table 3: AUC (× 100) for each model in the biology domain from Figure 2(a). model for biology versus the baseline model. 6 Related Work Domain-specific paraphrasing has not received previous attention, but there is relevant prior work on domain-specific machine translation (MT). We build on the Moore-Lewis method, which has been used for language models (Moore and Lewis, 2010) and translation models (Axelrod et al., 2011). Similar methods use LM perplexity to rank sentences (Gao et al., 2002; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of General AUC Aabsolute Arelative 39.5 – – Gen.+M-L,T=1 40.8 +1.3 +3.3 Gen.+M-L,T=145 40.8 +1.3 +3.3 Gen.+M-L,T=29 41.2 +1.7 +4.3 Gen.+M-L CP 41.9 +2.4 +6.1 Gen.+M-L,T=6 42.3 +2.8 +7.1 Gen.+M-L Weighted 43.7 +4.2 +10.6 60 fractio</context>
</contexts>
<marker>Moore, Lewis, 2010</marker>
<rawString>Robert C Moore and William Lewis. 2010. Intelligent selection of language model training data. In ACL, pages 220–224.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ellie Pavlick</author>
<author>Ani Nenkova</author>
</authors>
<title>Inducing lexical style properties for paraphrase and genre differentiation.</title>
<date>2015</date>
<booktitle>In NAACL.</booktitle>
<contexts>
<context position="1430" citStr="Pavlick and Nenkova, 2015" startWordPosition="208" endWordPosition="211"> 10% relative improvement in AUC. 1 Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change depending on ∗Incubated by the Allen Institute for Artifici</context>
</contexts>
<marker>Pavlick, Nenkova, 2015</marker>
<rawString>Ellie Pavlick and Ani Nenkova. 2015. Inducing lexical style properties for paraphrase and genre differentiation. In NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralf Steinberger</author>
</authors>
<title>Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis,</title>
<date>2006</date>
<location>and</location>
<marker>Steinberger, 2006</marker>
<rawString>Ralf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Tomaz Erjavec, Dan Tufis, and D´aniel Varga. 2006. The jrc-acquis: A multilingual aligned parallel corpus with 20+ languages. arXiv preprint.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J¨org Tiedemann</author>
</authors>
<title>Parallel data, tools and interfaces in opus.</title>
<date>2012</date>
<booktitle>In LREC,</booktitle>
<pages>2214--2218</pages>
<contexts>
<context position="8366" citStr="Tiedemann, 2012" startWordPosition="1322" endWordPosition="1323"> advantage of producing the full set of paraphrases that can be extracted from the entire bitext. 4 Experimental Conditions Domain data We evaluate our domain-specific paraphrasing model in the target domain of biology. Our monolingual in-domain data is a combination of text from the GENIA database (Kim et al., 2003) and text from an introductory biology textbook. Our bilingual general-domain data is the 109 word parallel corpus (Callison-Burch et al., 58 2009), a collection of French-English parallel data covering a mix of genres from legal text (Steinberger et al., 2006) to movie subtitles (Tiedemann, 2012). We use 5-gram language models with Kneser-Ney discounting (Heafield et al., 2013). Evaluation We measure the precision and recall of paraphrase pairs produced by each of our models by collecting human judgments of what paraphrases are acceptable in sentences drawn from the target domain and in sentences drawn from the general domain. We sample 15K sentences from our biology data, and 10K general-domain sentences from Wikipedia. We select a phrase from each sentence, and show the list of candidate paraphrases1 to 5 human judges. Judges make a binary decision about whether each paraphrase is a</context>
</contexts>
<marker>Tiedemann, 2012</marker>
<rawString>J¨org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In LREC, pages 2214–2218.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wei Xu</author>
<author>Alan Ritter</author>
<author>Bill Dolan</author>
<author>Ralph Grishman</author>
<author>Colin Cherry</author>
</authors>
<title>Paraphrasing for style.</title>
<date>2012</date>
<booktitle>In COLING,</booktitle>
<pages>2899--2914</pages>
<contexts>
<context position="1402" citStr="Xu et al., 2012" startWordPosition="204" endWordPosition="207"> recall, giving a 10% relative improvement in AUC. 1 Introduction Many data-driven paraphrase extraction algorithms have been developed in recent years (Madnani and Dorr, 2010; Androutsopoulos and Malakasiotis, 2010). These algorithms attempt to learn paraphrase rules, where one phrase can be replaced with another phrase which has equivalent meaning in at least some context. Determining whether a paraphrase is appropriate for a specific context is a difficult problem (Bhagat and Hovy, 2013), encompassing issues of syntax (Callison-Burch, 2008), word sense (Apidianaki et al., 2014), and style (Xu et al., 2012; Pavlick and Nenkova, 2015). To date, the question of how domain effects paraphrase has been left unexplored. Although most paraphrase extraction algorithms attempt to estimate a confidence with which a paraphrase rule might apply, these scores are not differentiated by domain, and instead correspond to the general domain represented by the model’s training data. As illustrated by Table 1, paraphrases that are highly probable in the general domain (e.g. hot = sexy) can be extremely improbable in more specialized domains like biology. Dominant word senses change depending on ∗Incubated by the </context>
</contexts>
<marker>Xu, Ritter, Dolan, Grishman, Cherry, 2012</marker>
<rawString>Wei Xu, Alan Ritter, Bill Dolan, Ralph Grishman, and Colin Cherry. 2012. Paraphrasing for style. In COLING, pages 2899–2914.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Keiji Yasuda</author>
<author>Ruiqiang Zhang</author>
<author>Hirofumi Yamamoto</author>
<author>Eiichiro Sumita</author>
</authors>
<title>Method of selecting training data to build a compact and efficient translation model.</title>
<date>2008</date>
<booktitle>In IJCNLP,</booktitle>
<pages>655--660</pages>
<contexts>
<context position="14770" citStr="Yasuda et al., 2008" startWordPosition="2366" endWordPosition="2369">c Table 2: Top paraphrase under the general and the best domain-specific model, General+M-L Weighted. Table 3: AUC (× 100) for each model in the biology domain from Figure 2(a). model for biology versus the baseline model. 6 Related Work Domain-specific paraphrasing has not received previous attention, but there is relevant prior work on domain-specific machine translation (MT). We build on the Moore-Lewis method, which has been used for language models (Moore and Lewis, 2010) and translation models (Axelrod et al., 2011). Similar methods use LM perplexity to rank sentences (Gao et al., 2002; Yasuda et al., 2008), rather than the difference in cross-entropy. Within MT, Foster and Kuhn (2007) used loglinear weightings of translation probabilities to combine models trained in different domains, as we do here. Relevant to our proposed method of General AUC Aabsolute Arelative 39.5 – – Gen.+M-L,T=1 40.8 +1.3 +3.3 Gen.+M-L,T=145 40.8 +1.3 +3.3 Gen.+M-L,T=29 41.2 +1.7 +4.3 Gen.+M-L CP 41.9 +2.4 +6.1 Gen.+M-L,T=6 42.3 +2.8 +7.1 Gen.+M-L Weighted 43.7 +4.2 +10.6 60 fractional counting, (Madnani et al., 2007) used introduced a count-centric approach to paraphrase probability estimation. Matsoukas et al. (2009)</context>
</contexts>
<marker>Yasuda, Zhang, Yamamoto, Sumita, 2008</marker>
<rawString>Keiji Yasuda, Ruiqiang Zhang, Hirofumi Yamamoto, and Eiichiro Sumita. 2008. Method of selecting training data to build a compact and efficient translation model. In IJCNLP, pages 655–660.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>