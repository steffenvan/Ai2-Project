<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001734">
<title confidence="0.996799">
Error Measures and Bayes Decision Rules Revisited
with Applications to POS Tagging
</title>
<author confidence="0.998995">
Hermann Ney, Maja Popovi´c, David S¨undermann
</author>
<affiliation confidence="0.9790665">
Lehrstuhl f¨ur Informatik VI - Computer Science Department
RWTH Aachen University
</affiliation>
<address confidence="0.914384">
Ahornstrasse 55
52056 Aachen, Germany
</address>
<email confidence="0.999429">
{popovic,ney}@informatik.rwth-aachen.de
</email>
<sectionHeader confidence="0.997396" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999986833333333">
Starting from first principles, we re-visit the
statistical approach and study two forms of
the Bayes decision rule: the common rule for
minimizing the number of string errors and a
novel rule for minimizing the number of symbols
errors. The Bayes decision rule for minimizing
the number of string errors is widely used, e.g.
in speech recognition, POS tagging and machine
translation, but its justification is rarely questioned.
To minimize the number of symbol errors as is
more suitable for a task like POS tagging, we show
that another form of the Bayes decision rule can
be derived. The major purpose of this paper is to
show that the form of the Bayes decision rule should
not be taken for granted (as it is done in virtually
all statistical NLP work), but should be adapted
to the error measure being used. We present first
experimental results for POS tagging tasks.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999811428571429">
Meanwhile, the statistical approach to natural
language processing (NLP) tasks like speech
recognition, POS tagging and machine translation
has found widespread use. There are three
ingredients to any statistical approach to NLP,
namely the Bayes decision rule, the probability
models (like trigram model, HMM, ...) and the
training criterion (like maximum likelihood, mutual
information, ...).
The topic of this paper is to re-consider the form
of the Bayes decision rule. In virtually all NLP
tasks, the specific form of the Bayes decision rule
is never questioned, and the decision rule is adapted
from speech recognition. In speech recognition, the
typical decision rule is to maximize the sentence
probability over all possible sentences. However,
this decision rule is optimal for the sentence error
rate and not for the word error rate. This difference
is rarely studied in the literature.
As a specific NLP task, we will consider part-
of-speech (POS) tagging. However, the problem
addressed comes up in any NLP task which is
tackled by the statistical approach and which makes
use of a Bayes decision rule. Other prominent
examples are speech recognition and machine
translation. The advantage of the POS tagging
task is that it will be easier to handle from the
mathematical point of view and will result in closed-
form solutions for the decision rules. From this
point-of-view, the POS tagging task serves as a
good opportunity to illustrate the key concepts of
the statistical approach to NLP.
Related Work: For the task of POS tagging,
statistical approaches were proposed already in the
60’s and 70’s (Stolz et al., 1965; Bahl and Mercer,
1976), before they started to find widespread use
in the 80’s (Beale, 1985; DeRose, 1989; Church,
1989).
To the best of our knowledge, the ’standard’
version of the Bayes decision rule, which minimizes
the number of string errors, is used in virtually all
approaches to POS tagging and other NLP tasks.
There are only two research groups that do not take
this type of decision rule for granted:
(Merialdo, 1994): In the context of POS tagging,
the author introduces a method that he calls
maximum likelihood tagging. The spirit of this
method is similar to that of this work. However, this
method is mentioned as an aside and its implications
for the Bayes decision rule and the statistical
approach are not addressed. Part of this work
goes back to (Bahl et al., 1974) who considered
a problem in coding theory.
(Goel and Byrne, 2003): The error measure
considered by the authors is the word error rate in
speech recognition, i.e. the edit distance. Due to
the mathematical complexity of this error measure,
the authors resort to numeric approximations
to compute the Bayes risk (see next section).
Since this approach does not results in explicit
closed-form equations and involves many numeric
approximations, it is not easy to draw conclusions
from this work.
</bodyText>
<sectionHeader confidence="0.995689" genericHeader="introduction">
2 Bayes Decision Rule for Minimum Error
Rate
</sectionHeader>
<subsectionHeader confidence="0.998893">
2.1 The Bayes Posterior Risk
</subsectionHeader>
<bodyText confidence="0.999992">
Knowing that any task in NLP tasks is a difficult
one, we want to keep the number of wrong
decisions as small as possible. This point-of-view
has been used already for more than 40 years in
pattern classification as the starting point for many
techniques in pattern classification. To classify an
observation vector y into one out of several classes
c, we resort to the so-called statistical decision
theory and try to minimize the average risk or loss
in taking a decision. The result is known as Bayes
decision rule (Chapter 2 in (Duda and Hart, 1973)):
</bodyText>
<equation confidence="0.976268">
Pr(c|y) · L[c, c] I
</equation>
<bodyText confidence="0.999969083333333">
where L[c, ˜c] is the so-called loss function or error
measure, i.e. the loss we incur in making decision c
when the true class is ˜c.
In the following, we will consider two specific
forms of the loss function or error measure L[c, ˜c].
The first will be the measure for string errors,
which is the typical loss function used in virtually
all statistical approaches. The second is the
measure for symbol errors, which is the more
appropriate measure for POS tagging and also
speech recognition with no insertion and deletion
errors (such as isolated word recognition).
</bodyText>
<subsectionHeader confidence="0.999912">
2.2 String Error
</subsectionHeader>
<bodyText confidence="0.99988025">
For POS tagging, the starting point is the observed
sequence of words y = w1N = w1...wN, i.e. the
sequence of words for which the POS tag sequence
has c = gN1 = g1...gN has to be determined.
The first error measure we consider is the string
error: the error is equal to zero only if the POS
symbols of the two strings are identical at each
position. In this case, the loss function is:
</bodyText>
<equation confidence="0.962429">
N
L[gN1 , ˜gN1 ] = 1 − H δ(gn, ˜gn)
n=1
</equation>
<bodyText confidence="0.9994365">
with the Kronecker delta δ(c, ˜c). In other words,
the errors are counted at the string level and not
at the level of single symbols. Inserting this cost
function into the Bayes risk (see Section 2.1), we
immediately obtain the following form of Bayes
decision rule for minimum string error:
</bodyText>
<equation confidence="0.9750555">
{ }
Pr(gN 1 |wN 1 )
{ }
P r(gN 1 , wN 1 )
</equation>
<bodyText confidence="0.999947">
This is the starting point for virtually all statistical
approaches in NLP like speech recognition and
machine translation. However, this decision rule is
only optimal when we consider string errors, e.g.
sentence error rate in POS tagging and in speech
recognition. In practice, however, the empirical
errors are counted at the symbol level. Apart
from (Goel and Byrne, 2003), this inconsistency of
decision rule and error measure is never addressed
in the literature.
</bodyText>
<subsectionHeader confidence="0.999681">
2.3 Symbol Error
</subsectionHeader>
<bodyText confidence="0.85816375">
Instead of the string error rate, we can also consider
the error rate of single POS tag symbols (Bahl et
al., 1974; Merialdo, 1994).
This error measure is defined by the loss function:
</bodyText>
<equation confidence="0.987759333333333">
N
L[gN1 , ˜gN1 ] = E [1 − δ(gn, ˜gn)]
n=1
</equation>
<bodyText confidence="0.980934">
This loss function has to be inserted into the Bayes
decision rule in Section 2.1. The computation of the
expected loss, i.e. the averaging over all classes c˜ =
˜gN1 , can be performed in a closed form. We omit
the details of the straightforward calculations and
state only the result. It turns out that we will need
the marginal (and posterior) probability distribution
Prm(g|wN1 ) at positions m = 1, ..., N:
</bodyText>
<equation confidence="0.968755">
EPrm(g|wN1 ) := Pr(gN1 |wN1 )
gN1 : gm=g
</equation>
<bodyText confidence="0.999958714285714">
where the sum is carried out over all POS tag strings
gN1 with gm = g, i.e. the tag gm at position m is
fixed at gm = g. The question of how to perform
this summation efficiently will be considered later
after we have introduced the model distributions.
Thus we have obtained the Bayes decision rule
for minimum symbol error at position m = 1, ..., N:
</bodyText>
<equation confidence="0.945755">
{ }
(wN 1 , m) → ˆgm = arg max P rm(g|wN 1 )
g
=arg max
g
</equation>
<bodyText confidence="0.999875">
By construction this decision rule has the special
property that it does not put direct emphasis on
local coherency of the POS tags produced. In other
words, this decision rule may produce a POS tag
string which is linguistically less likely.
</bodyText>
<sectionHeader confidence="0.9452995" genericHeader="method">
3 The Modelling Approaches to POS
Tagging
</sectionHeader>
<bodyText confidence="0.98978175">
The derivation of the Bayes decision rule assumes
that the probability distribution Pr(gN1 , wN1 ) (or
Pr(gN1 |wN1 )) is known. Unfortunately, this is not
the case in practice. Therefore, the usual approach
</bodyText>
<equation confidence="0.997713166666666">
cˆ = arg min
c l {
Y6-
y →
wN1 → ˆgN1 = arg max
gN
1
=arg max
gN
1
{ }
Prm(g, wN 1 )
</equation>
<bodyText confidence="0.999787428571429">
is to approximate the true but unknown distribution
by a model distribution p(gN1 , wN1 ) (or p(gN1 |wN1 )).
We will review two popular modelling approaches,
namely the generative model and the direct model,
and consider the associated Bayes decision rules for
both minimum string error and minimum symbol
error.
</bodyText>
<subsectionHeader confidence="0.934227">
3.1 Generative Model: Trigram Model
</subsectionHeader>
<bodyText confidence="0.992084">
We replace the true but unknown joint distribution
Pr(gN1 , wN1 ) by a model-based probability distribu-
tion p(gN1 , wN1 ):
</bodyText>
<equation confidence="0.720346">
Pr(gN1 , wN1 ) → p(gN1 , wN1 ) = p(gN1 ) · p(wN1 |gN1 )
</equation>
<bodyText confidence="0.977856956521739">
We apply the so-called chain rule to factorize each
of the distributions p(gN1 ) and p(wN1 |gN1 ) into a
product of conditional probabilities using specific
dependence assumptions:
[p(gn|gn−1
n−2) · p(wn |gn)]
with suitable definitions for the case n = 1.
Here, the specific dependence assumptions are that
the conditional probabilities can be represented
by a POS trigram model p(gn|gn−1
n−2) and a word
membership model p(wn|gn). Thus we obtain
a probability model whose structure fits into
the mathematical framework of so-called Hidden
Markov Model (HMM). Therefore, this approach is
often also referred to as HMM-based POS tagging.
However, this terminology is misleading: The POS
tag sequence is observable whereas in the Hidden
Markov Model the state sequence is always hidden
and cannot be observed. In the experiments, we will
use a 7-gram POS model. It is clear how to extend
the equations from the trigram case to the 7-gram
case.
</bodyText>
<subsectionHeader confidence="0.9818">
3.1.1 String Error
</subsectionHeader>
<bodyText confidence="0.9999855">
Using the above model distribution, we directly
obtain the decision rule for minimum string error:
</bodyText>
<equation confidence="0.98731425">
{ }
wN 1 → ˆgN 1 = arg max p(gN 1 ,wN 1 )
gN
1
</equation>
<bodyText confidence="0.999949333333333">
Since the model distribution is a basically a second-
order model (or trigram model), there is an efficient
algorithm for finding the most probable POS tag
string. This is achieved by a suitable dynamic
programming algorithm, which is often referred to
as Viterbi algorithm in the literature.
</bodyText>
<subsectionHeader confidence="0.561075">
3.1.2 Symbol Error
</subsectionHeader>
<bodyText confidence="0.9961195">
To apply the Bayes decision rule for minimum
symbol error rate, we first compute the marginal
</bodyText>
<equation confidence="0.988925571428571">
probability pm(g, wN1 ):
E
N
pm(g, w1 ) =
[p(gngN1 : gm=g
|gn−1
n−2) · p(wn |gn)]
</equation>
<bodyText confidence="0.999751857142857">
Again, since the model is a second-order model,
the sum over all possible POS tag strings gN1
(with gm = g) can be computed efficiently
using a suitable extension of the forward-backward
algorithm (Bahl et al., 1974).
Thus we obtain the decision rule for minimum
symbol error at positions m = 1, ..., N:
</bodyText>
<equation confidence="0.980323">
{ }
(wN 1 , m) → ˆgm = arg max pm(g, wN 1 )
g
</equation>
<bodyText confidence="0.999766666666667">
Here, after the the marginal probability pm(g, wN1 )
has been computed, the task of finding the most
probable POS tag at position m is computationally
easy. Instead, the lion’s share for the computational
effort is required to compute the marginal probabil-
ity pm(g, wN1 ).
</bodyText>
<subsectionHeader confidence="0.989188">
3.2 Direct Model: Maximum Entropy
</subsectionHeader>
<bodyText confidence="0.985154333333333">
We replace the true but unknown posterior distri-
bution Pr(gN1 |wN1 ) by a model-based probability
distribution p(gN1 |wN1 ):
</bodyText>
<equation confidence="0.978878">
Pr(gN1 |wN1 ) → p(gN1 |wN1 )
</equation>
<bodyText confidence="0.991725">
and apply the chain rule:
</bodyText>
<equation confidence="0.9686065">
p(gn|gn−1
1 , wN1 )
n−1 n+2
p(gn |gn−2 , wn−2 )
</equation>
<bodyText confidence="0.9994768">
As for the generative model, we have made specific
assumptions: There is a second-order dependence
for the tags gn1, and the dependence on the words
wN1 is limited to a window wn+2
n−2 around position
n. The resulting model is still rather complex
and requires further specifications. The typical
procedure is to resort to log-linear modelling, which
is also referred to as maximum entropy modelling
(Ratnaparkhi, 1996; Berger et al., 1996).
</bodyText>
<subsectionHeader confidence="0.849986">
3.2.1 String Error
</subsectionHeader>
<bodyText confidence="0.974356">
For the minimum string error, we obtain the
decision rule:
</bodyText>
<equation confidence="0.9966855">
{ }
wN 1 → ˆgN 1 = arg max p(gN 1 |wN 1 )
gN
1
N
p(gN1 , wN1 ) = H
n=1
p(gN1 , wN1 )
E= H
gN1 : gm=g n
N
p(gN1 |wN1 ) = H
n=1
N
= H
n=1
</equation>
<bodyText confidence="0.997394">
Since this is still a second-order model, we can use
dynamic programming to compute the most likely
POS string.
</bodyText>
<subsectionHeader confidence="0.735573">
3.2.2 Symbol Error
</subsectionHeader>
<bodyText confidence="0.95725">
For the minimum symbol error, the marginal
(and posterior) probability pm(g|wN1 ) has to be
computed:
</bodyText>
<equation confidence="0.991502">
pm(g|wN1 ) = E Pr(gN1 |wN1 )
gN1 : gm=g
n−1 n+2
p(gn |gn−2 , wn−2 )
gN1 : gm=g
</equation>
<bodyText confidence="0.825366857142857">
which, due to the specific structure of the model
p(gnn−1 wn+2) can be calculated efficientl
gn−2 n−2,y
using only a forward algorithm (without a
’backward’ part).
Thus we obtain the decision rule for minimum
symbol error at positions m = 1, ..., N:
</bodyText>
<equation confidence="0.948775">
(wN1 ,m) → ˆgm = arg mgax{pm(g|wN1 )}
</equation>
<bodyText confidence="0.9999375">
As in the case of the generative model, the
computational effort is to compute the posterior
probability pm(g|wN1 ) rather than to find the most
probable tag at position m.
</bodyText>
<sectionHeader confidence="0.991091" genericHeader="method">
4 The Training Procedure
</sectionHeader>
<bodyText confidence="0.99996925">
So far, we have said nothing about how we train
the free parameters of the model distributions. We
use fairly conventional training procedures that we
mention only for the sake of completeness.
</bodyText>
<sectionHeader confidence="0.571083" genericHeader="method">
4.1 Generative Model
</sectionHeader>
<bodyText confidence="0.999990933333333">
We consider the trigram-based model. The free
parameters here are the entries of the POS trigram
distribution p(g|g&apos;&apos;, g&apos;) and of the word membership
distribution p(w|g). These unknown parameters are
computed from a labelled training corpus, i.e. a
collection of sentences where for each word the
associated POS tag is given.
In principle, the free parameters of the models
are estimated as relative frequencies. For the test
data, we have to allow for both POS trigrams (or n-
grams) and (single) words that were not seen in the
training data. This problem is tackled by applying
smoothing methods that were originally designed
for language modelling in speech recognition (Ney
et al., 1997).
</bodyText>
<subsectionHeader confidence="0.610789">
4.2 Direct Model
</subsectionHeader>
<bodyText confidence="0.9919118">
For the maximum entropy model, the free param-
eters are the so-called λi or feature parameters
(Berger et al., 1996; Ratnaparkhi, 1996). The
training criterion is to optimize the logarithm
of the model probabilities p(gn|gn−2
</bodyText>
<equation confidence="0.960873">
n−1, wn+2
n−2) over
</equation>
<bodyText confidence="0.999113666666667">
all positions n in the training corpus. The
corresponding algorithm is referred to as GIS
algorithm (Berger et al., 1996). As usual
with maximum entropy models, the problem of
smoothing does not seem to be critical and is not
addressed explicitly.
</bodyText>
<sectionHeader confidence="0.996513" genericHeader="evaluation">
5 Experimental Results
</sectionHeader>
<bodyText confidence="0.999976888888889">
Of course, there have already been many papers
about POS tagging using statistical methods. The
goal of the experiments is to compare the two
decision rules and to analyze the differences in
performance. As the results for the WSJ corpus will
show, both the trigram method and the maximum
entropy method have an tagging error rate of 3.0%
to 3.5% and are thus comparable to the best results
reported in the literature, e.g. (Ratnaparkhi, 1996).
</bodyText>
<subsectionHeader confidence="0.982202">
5.1 Task and Corpus
</subsectionHeader>
<bodyText confidence="0.999789571428572">
The experiments are performed on the Wall Street
Journal (WSJ) English corpus and on the M¨unster
Tagging Project (MTP) German corpus.
The POS tagging part of The WSJ corpus
(Table 1) was compiled by the University of
Pennsylvania and consists of about one million
English words with manually annotated POS tags.
</bodyText>
<table confidence="0.999164888888889">
Text POS
Train Sentences 43508
Words+PMs 1061772
Singletons 21522 0
Word Vocabulary 46806 45
PM Vocabulary 25 9
Test Sentences 4478
Words+PMs 111220
OOVs 2879 0
</table>
<tableCaption confidence="0.999538">
Table 1: WSJ corpus statistics.
</tableCaption>
<bodyText confidence="0.999864375">
The MTP corpus (Table 2) was compiled at the
University of M¨unster and contains tagged German
words from articles of the newspapers Die Zeit
and Frankfurter Allgemeine Zeitung (Kinscher and
Steiner, 1995).
For the corpus statistics, it is helpful to
distinguish between the true words and the
punctuation marks (see Table 1 and Table 2). This
distinction is made for both the text and the POS
corpus. In addition, the tables show the vocabulary
size (number of different tokens) for the words and
for the punctuation marks.
Punctuation marks (PMs) are all tokens which
do not contain letters or digits. The total number
of running tokens is indicated as Words+PMs.
Singletons are the tokens which occur only once in
</bodyText>
<table confidence="0.984728083333333">
=
H
n
Text POS
Train Sentences 19845
Words+PMs 349699
Singletons 32678 11
Word Vocabulary 51491 68
PM Vocabulary 27 5
Test Sentences 2206
Words+PMs 39052
OOVs 3584 2
</table>
<tableCaption confidence="0.999337">
Table 2: MTP corpus statistics.
</tableCaption>
<bodyText confidence="0.99854">
the training data. Out-of-Vocabulary words (OOVs)
are the words in the test data that did not not occur
in the training corpus.
</bodyText>
<subsectionHeader confidence="0.997638">
5.2 POS Tagging Results
</subsectionHeader>
<bodyText confidence="0.999974766666667">
The tagging experiments were performed for both
types of models, each of them with both types of
the decision rules. The generative model is based on
the approach described in (S¨undermann and Ney,
2003). Here the optimal value of the n-gram order
is determined from the corpus statistics and has a
maximum of n = 7. The experiments for the direct
model were performed using the maximum entropy
tagger described in (Ratnaparkhi, 1996).
The tagging error rates are showed in Table 3 and
Table 4. In addition to the overall tagging error rate
(Overall), the tables show the tagging error rates for
the Out-of-Vocabulary words (OOVs) and for the
punctuation marks (PMs).
For the generative model, both decision rules
yield similar results. For the direct model, the
overall tagging error rate increases on each of the
two tasks (from 3.0 % to 3.3 % on WSJ and from
5.4 % to 5.6 % on MTP) when we use the symbol
decision rule instead of the string decision rule. In
particular, for OOVs, the error rate goes up clearly.
Right now, we do not have a clear explanation
for this difference between the generative model
and the direct model. It might be related to the
’forward’ structure of the direct model as opposed to
the ’forward-backward’ structure of the generative
model. Anyway, the refined bootstrap method
(Bisani and Ney, 2004) has shown that differences
in the overall tagging error rate are statistically not
significant.
</bodyText>
<subsectionHeader confidence="0.993167">
5.3 Examples
</subsectionHeader>
<bodyText confidence="0.999918285714286">
A detailed analysis of the tagging results showed
that for both models there are sentences where the
one decision rule is more efficient and sentences
where the other decision rule is better.
For the generative model, these differences seem
to occur at random, but for the direct model, some
distinct tendencies can be observed. For example,
</bodyText>
<table confidence="0.995779">
WSJ Task Decision Overall OOVs PMs
Rule
Generative string 3.5 16.9 0
Model
symbol 3.5 16.7 0
Direct string 3.0 15.4 0.08
Model
symbol 3.3 16.6 0.1
</table>
<tableCaption confidence="0.993265">
Table 3: POS tagging error rates [%] for WSJ task.
</tableCaption>
<table confidence="0.9998025">
MTP Task Decision Overall OOVs PMs
Rule
Generative string 5.4 13.4 3.6
Model
symbol 5.4 13.4 3.6
Direct string 5.4 12.7 3.8
Model
symbol 5.6 13.4 3.7
</table>
<tableCaption confidence="0.998701">
Table 4: POS tagging error rates [%] for MTP task.
</tableCaption>
<bodyText confidence="0.999970739130435">
for the WSJ corpus, the string decision rule is
significantly better for the present and past tense of
verbs (VBP, VBN), and the symbol decision rule
is better for adverb (RB) and verb past participle
(VBN). Typical errors generated by the symbol
decision rule are tagging present tense as infinitive
(VB) and past tense as past participle (VBN), and
for string decision rule, adverbs are often tagged as
preposition (IN) or adjective (JJ) and past participle
as past tense (VBD).
For the German corpus, the string decision
rule better handles demonstrative determiners
(Rr) and subordinate conjunctions (Cs) whereas
symbol decision rule is better for definite articles
(Db). The symbol decision rule typically tags
the demonstrative determiner as definite article
(Db) and subordinate conjunctions as interrogative
adverbs (Bi), and the string decision rule tends to
assign the demonstrative determiner tag to definite
articles.
These typical errors for the symbol decision rule
are shown in Table 5, and for the string decision rule
in Table 6.
</bodyText>
<sectionHeader confidence="0.999596" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.998505">
So far, the experimental tests have shown no
improvement when we use the Bayes decision rule
for minimizing the number of symbol errors rather
than the number of string errors. However, the
important result is that the new approach results in
comparable performance. More work is needed to
contrast the two approaches.
The main purpose of this paper has been to show
that, in addition to the widely used decision rule for
minimizing the string errors, it is possible to derive a
decision rule for minimizing the number of symbol
errors and to build up the associated mathematical
framework.
There are a number of open questions for future
work:
1) The error rates for the two decision rules are
comparable. Is that an experimental coincidence?
Are there situations for which we must expect a
significance difference between the two decision
rules? We speculate that the two decision rules
could always have similar performance if the error
rates are small.
2) Ideally, the training criterion should be closely
related to the error measure used in the decision
rule. Right now, we have used the training criteria
that had been developed in the past and that had
been (more or less) designed for the string error rate
as error measure. Can we come up with a training
criterion tailored to the symbol error rate?
3) In speech recognition and machine translation,
more complicated error measures such as the edit
distance and the BLEU measure are used. Is it
possible to derive closed-form Bayes decision rules
(or suitable analytic approximations) for these error
measures? What are the implications?
</bodyText>
<sectionHeader confidence="0.998935" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995233253968254">
L. Bahl, J. Cocke, F. Jelinek and J. Raviv.
1974. Optimal Decoding of Linear Codes for
Minimizing Symbol Error Rate. IEEE Trans. on
Information Theory, No. 20, pages 284–287
L. Bahl and L. R. Mercer. 1976. Part of Speech
Assignment by a Statistical Decision Algorithm.
In IEEE Symposium on Information Theory,
abstract, pages 88–89, Ronneby, Sweden.
A. D. Beale. 1985. A Probabilistic Approach
to Grammatical Analysis of Written English by
Computer. In 2nd Conf. of the European Chapter
of the ACL, pages 159–169, Geneva, Switzerland.
A. L. Berger, S. Della Pietra and V. Della Pietra.
1996. A Maximum Entropy Approach to
Natural Language Processing. Computational
Linguistics, No. 22, Vol. 1, pages 39–71.
M. Bisani and H. Ney. 2004. Bootstrap Estimates
for Confidence Intervals in ASR Performance
Evaluation. In IEEE Int. Conf. on Acoustics,
Speech and Signal Processing, pages 409–412,
Montreal, Canada.
K. W. Church. 1989. A Stochastic Parts Program
Noun Phrase Parser for Unrestricted Text. In
IEEE Int. Conf. on Acoustics, Speech and Signal
Processing, pages 695–698, Glasgow, Scotland.
S. DeRose. 1989. Grammatical Category Disam-
biguation by Statistical Optimization. Computa-
tional Linguistics, No. 14, Vol. 1, pages 31–39
R. O. Duda and P. E. Hart. 1973. Pattern
Classification and Scene Analysis. John Wiley &amp;
Sons, New York.
V. Goel and W. Byrne. 2003. Minimum Bayes-
risk Automatic Speech Recognition. In W. Chou
and B. H. Juang (editors): Pattern Recognition
in Speech and Language Processing. CRC Press,
Boca Rota, Florida.
J. Kinscher and P. Steiner. 1995. M¨unster Tagging
Project (MTP). Handout for the 4th Northern
German Linguistic Colloquium, University of
M¨unster, Internal report.
B. Merialdo. 1994. Tagging English Text with a
Probabilistic Model. Computational Linguistics,
No. 20, Vol. 2, pages 155–168.
H. Ney, S. Martin and F. Wessel. 1997.
Statistical Language Modelling by Leaving-One-
Out. In G. Bloothooft and S. Young (editors):
Corpus-Based Methods in Speech and Language,
pages 174–207. Kluwer Academic Publishers,
Dordrecht.
A. Ratnaparkhi. 1996. A Maximum Entropy
Model for Part-of-Speech Tagging. In Conf.
on Empirical Methods in Natural Language
Processing and Very Large Corpora , pages 133–
142, Sommerset, NJ.
W. S. Stolz, P. H. Tannenbaum and F. V. Carstensen.
1965. Stochastic Approach to the Grammatical
Coding of English. Communications of the ACM,
No. 8, pages 399–405.
D. S¨undermann and H. Ney. 2003. SYNTHER
- a New m-gram POS Tagger. In Proc. of
the Int. Conf. on Natural Language Processing
and Knowledge Engineering, pages 628–633,
Beijing, China.
</reference>
<table confidence="0.99805108">
VBP —* VB
reference ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...
string ... investors/NNS already/RB have/VBP sharply/RB scaled/VBN ...
symbol ... investors/NNS already/RB have/VB sharply/RB scaled/VBN ...
reference We/PRP basically/RB think/VBP that/IN ...
string We/PRP basically/RB think/VBP that/IN ...
symbol We/PRP basically/RB think/VB that/IN ...
VBD —* VBN
reference ... plant-expansion/JJ program/NN started/VBD this/DT year/NN ...
string ... plant-expansion/NN program/NN started/VBD this/DT year/NN ...
symbol ... plant-expansion/NN program/NN started/VBN this/DT year/NN ...
reference ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...
string ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBD agreements/NNS ...
symbol ... countries/NNS have/VBP in/IN recent/JJ years/NNS made/VBN agreements/NNS ...
Rr —* Db
reference Das/Db Sandm¨annchen/Ne ,/Fi das/Rr uns/Rp der/Db NDR/Ab pr¨asentiert/Vf ...
string Das/Db Sandm¨annchen/Ng ,/Fi das/Rr uns/Rp der/Db NDR/Ab pr¨asentiert/Vf ...
symbol Das/Db Sandm¨annchen/Ng ,/Fi das/Db uns/Rp der/Db NDR/Ab pr¨asentiert/Vf ...
reference ... f¨ur/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...
string ... f¨ur/Po Leute/Ng ,/Fi die/Rr glauben/Vf ...
symbol ... f¨ur/Po Leute/Ng ,/Fi die/Db glauben/Vf ...
Cs —* Bi
reference Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gef¨allt/Vf ...
string Denke/Vf ich/Rp nach/Qv ,/Fi warum/Cs mir/Rp die/Db Geschichte/Ng gef¨allt/Vf ...
symbol Denke/Vf ich/Rp nach/Qv ,/Fi warum/Bi mir/Rp die/Db Geschichte/Ng gef¨allt/Vf ...
</table>
<tableCaption confidence="0.994977">
Table 5: Examples of tagging errors for the symbol decision rule (direct model)
</tableCaption>
<table confidence="0.999181466666667">
RB —* IN, JJ
reference The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...
string The/DT negotiations/NNS allocate/VBP about/IN 15/CD %/NN ...
symbol The/DT negotiations/NNS allocate/VBP about/RB 15/CD %/NN ...
reference ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...
string ... will/MD lead/VB to/TO a/DT much/JJ stronger/JJR performance/NN ...
symbol ... will/MD lead/VB to/TO a/DT much/RB stronger/JJR performance/NN ...
VBN —* VBD
reference ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...
string ... by/IN a/DT police/NN officer/NN named/VBD John/NNP Klute/NNP ...
symbol ... by/IN a/DT police/NN officer/NN named/VBN John/NNP Klute/NNP ...
Db —* Rr
reference er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...
string er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Rr Emotionen/Ng zu/Qi kanalisieren/Vi ...
symbol er/Rp kam/Vf auf/Po die/Db Idee/Ng ,/Fi die/Db Emotionen/Ng zu/Qi kanalisieren/Vi ...
</table>
<tableCaption confidence="0.999001">
Table 6: Examples of tagging errors for the string decision rule (direct model)
</tableCaption>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.429615">
<title confidence="0.9995305">Error Measures and Bayes Decision Rules with Applications to POS Tagging</title>
<author confidence="0.995606">Hermann Ney</author>
<author confidence="0.995606">Maja Popovi´c</author>
<author confidence="0.995606">David</author>
<affiliation confidence="0.9174505">Lehrstuhl f¨ur Informatik VI - Computer Science RWTH Aachen</affiliation>
<address confidence="0.745925">Ahornstrasse 52056 Aachen,</address>
<abstract confidence="0.998772947368421">Starting from first principles, we re-visit the statistical approach and study two forms of the Bayes decision rule: the common rule for minimizing the number of string errors and a novel rule for minimizing the number of symbols errors. The Bayes decision rule for minimizing the number of string errors is widely used, e.g. in speech recognition, POS tagging and machine translation, but its justification is rarely questioned. To minimize the number of symbol errors as is more suitable for a task like POS tagging, we show that another form of the Bayes decision rule can be derived. The major purpose of this paper is to show that the form of the Bayes decision rule should not be taken for granted (as it is done in virtually all statistical NLP work), but should be adapted to the error measure being used. We present first experimental results for POS tagging tasks.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>J Cocke</author>
<author>F Jelinek</author>
<author>J Raviv</author>
</authors>
<title>Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate.</title>
<date>1974</date>
<journal>IEEE Trans. on Information Theory,</journal>
<volume>20</volume>
<pages>284--287</pages>
<contexts>
<context position="3607" citStr="Bahl et al., 1974" startWordPosition="582" endWordPosition="585">ersion of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. However, this method is mentioned as an aside and its implications for the Bayes decision rule and the statistical approach are not addressed. Part of this work goes back to (Bahl et al., 1974) who considered a problem in coding theory. (Goel and Byrne, 2003): The error measure considered by the authors is the word error rate in speech recognition, i.e. the edit distance. Due to the mathematical complexity of this error measure, the authors resort to numeric approximations to compute the Bayes risk (see next section). Since this approach does not results in explicit closed-form equations and involves many numeric approximations, it is not easy to draw conclusions from this work. 2 Bayes Decision Rule for Minimum Error Rate 2.1 The Bayes Posterior Risk Knowing that any task in NLP ta</context>
<context position="6699" citStr="Bahl et al., 1974" startWordPosition="1126" endWordPosition="1129">r(gN 1 , wN 1 ) This is the starting point for virtually all statistical approaches in NLP like speech recognition and machine translation. However, this decision rule is only optimal when we consider string errors, e.g. sentence error rate in POS tagging and in speech recognition. In practice, however, the empirical errors are counted at the symbol level. Apart from (Goel and Byrne, 2003), this inconsistency of decision rule and error measure is never addressed in the literature. 2.3 Symbol Error Instead of the string error rate, we can also consider the error rate of single POS tag symbols (Bahl et al., 1974; Merialdo, 1994). This error measure is defined by the loss function: N L[gN1 , ˜gN1 ] = E [1 − δ(gn, ˜gn)] n=1 This loss function has to be inserted into the Bayes decision rule in Section 2.1. The computation of the expected loss, i.e. the averaging over all classes c˜ = ˜gN1 , can be performed in a closed form. We omit the details of the straightforward calculations and state only the result. It turns out that we will need the marginal (and posterior) probability distribution Prm(g|wN1 ) at positions m = 1, ..., N: EPrm(g|wN1 ) := Pr(gN1 |wN1 ) gN1 : gm=g where the sum is carried out over </context>
<context position="10589" citStr="Bahl et al., 1974" startWordPosition="1830" endWordPosition="1833">there is an efficient algorithm for finding the most probable POS tag string. This is achieved by a suitable dynamic programming algorithm, which is often referred to as Viterbi algorithm in the literature. 3.1.2 Symbol Error To apply the Bayes decision rule for minimum symbol error rate, we first compute the marginal probability pm(g, wN1 ): E N pm(g, w1 ) = [p(gngN1 : gm=g |gn−1 n−2) · p(wn |gn)] Again, since the model is a second-order model, the sum over all possible POS tag strings gN1 (with gm = g) can be computed efficiently using a suitable extension of the forward-backward algorithm (Bahl et al., 1974). Thus we obtain the decision rule for minimum symbol error at positions m = 1, ..., N: { } (wN 1 , m) → ˆgm = arg max pm(g, wN 1 ) g Here, after the the marginal probability pm(g, wN1 ) has been computed, the task of finding the most probable POS tag at position m is computationally easy. Instead, the lion’s share for the computational effort is required to compute the marginal probability pm(g, wN1 ). 3.2 Direct Model: Maximum Entropy We replace the true but unknown posterior distribution Pr(gN1 |wN1 ) by a model-based probability distribution p(gN1 |wN1 ): Pr(gN1 |wN1 ) → p(gN1 |wN1 ) and a</context>
</contexts>
<marker>Bahl, Cocke, Jelinek, Raviv, 1974</marker>
<rawString>L. Bahl, J. Cocke, F. Jelinek and J. Raviv. 1974. Optimal Decoding of Linear Codes for Minimizing Symbol Error Rate. IEEE Trans. on Information Theory, No. 20, pages 284–287</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Bahl</author>
<author>L R Mercer</author>
</authors>
<title>Part of Speech Assignment by a Statistical Decision Algorithm.</title>
<date>1976</date>
<booktitle>In IEEE Symposium on Information Theory, abstract,</booktitle>
<pages>88--89</pages>
<location>Ronneby,</location>
<contexts>
<context position="2843" citStr="Bahl and Mercer, 1976" startWordPosition="450" endWordPosition="453">ackled by the statistical approach and which makes use of a Bayes decision rule. Other prominent examples are speech recognition and machine translation. The advantage of the POS tagging task is that it will be easier to handle from the mathematical point of view and will result in closedform solutions for the decision rules. From this point-of-view, the POS tagging task serves as a good opportunity to illustrate the key concepts of the statistical approach to NLP. Related Work: For the task of POS tagging, statistical approaches were proposed already in the 60’s and 70’s (Stolz et al., 1965; Bahl and Mercer, 1976), before they started to find widespread use in the 80’s (Beale, 1985; DeRose, 1989; Church, 1989). To the best of our knowledge, the ’standard’ version of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. However, this method is menti</context>
</contexts>
<marker>Bahl, Mercer, 1976</marker>
<rawString>L. Bahl and L. R. Mercer. 1976. Part of Speech Assignment by a Statistical Decision Algorithm. In IEEE Symposium on Information Theory, abstract, pages 88–89, Ronneby, Sweden.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A D Beale</author>
</authors>
<title>A Probabilistic Approach to Grammatical Analysis of Written English by Computer.</title>
<date>1985</date>
<booktitle>In 2nd Conf. of the European Chapter of the ACL,</booktitle>
<pages>159--169</pages>
<location>Geneva, Switzerland.</location>
<contexts>
<context position="2912" citStr="Beale, 1985" startWordPosition="464" endWordPosition="465"> Other prominent examples are speech recognition and machine translation. The advantage of the POS tagging task is that it will be easier to handle from the mathematical point of view and will result in closedform solutions for the decision rules. From this point-of-view, the POS tagging task serves as a good opportunity to illustrate the key concepts of the statistical approach to NLP. Related Work: For the task of POS tagging, statistical approaches were proposed already in the 60’s and 70’s (Stolz et al., 1965; Bahl and Mercer, 1976), before they started to find widespread use in the 80’s (Beale, 1985; DeRose, 1989; Church, 1989). To the best of our knowledge, the ’standard’ version of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. However, this method is mentioned as an aside and its implications for the Bayes decision rule and</context>
</contexts>
<marker>Beale, 1985</marker>
<rawString>A. D. Beale. 1985. A Probabilistic Approach to Grammatical Analysis of Written English by Computer. In 2nd Conf. of the European Chapter of the ACL, pages 159–169, Geneva, Switzerland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics, No.</journal>
<volume>22</volume>
<pages>39--71</pages>
<contexts>
<context position="11698" citStr="Berger et al., 1996" startWordPosition="2029" endWordPosition="2032">bution Pr(gN1 |wN1 ) by a model-based probability distribution p(gN1 |wN1 ): Pr(gN1 |wN1 ) → p(gN1 |wN1 ) and apply the chain rule: p(gn|gn−1 1 , wN1 ) n−1 n+2 p(gn |gn−2 , wn−2 ) As for the generative model, we have made specific assumptions: There is a second-order dependence for the tags gn1, and the dependence on the words wN1 is limited to a window wn+2 n−2 around position n. The resulting model is still rather complex and requires further specifications. The typical procedure is to resort to log-linear modelling, which is also referred to as maximum entropy modelling (Ratnaparkhi, 1996; Berger et al., 1996). 3.2.1 String Error For the minimum string error, we obtain the decision rule: { } wN 1 → ˆgN 1 = arg max p(gN 1 |wN 1 ) gN 1 N p(gN1 , wN1 ) = H n=1 p(gN1 , wN1 ) E= H gN1 : gm=g n N p(gN1 |wN1 ) = H n=1 N = H n=1 Since this is still a second-order model, we can use dynamic programming to compute the most likely POS string. 3.2.2 Symbol Error For the minimum symbol error, the marginal (and posterior) probability pm(g|wN1 ) has to be computed: pm(g|wN1 ) = E Pr(gN1 |wN1 ) gN1 : gm=g n−1 n+2 p(gn |gn−2 , wn−2 ) gN1 : gm=g which, due to the specific structure of the model p(gnn−1 wn+2) can be c</context>
<context position="13750" citStr="Berger et al., 1996" startWordPosition="2401" endWordPosition="2404">uted from a labelled training corpus, i.e. a collection of sentences where for each word the associated POS tag is given. In principle, the free parameters of the models are estimated as relative frequencies. For the test data, we have to allow for both POS trigrams (or ngrams) and (single) words that were not seen in the training data. This problem is tackled by applying smoothing methods that were originally designed for language modelling in speech recognition (Ney et al., 1997). 4.2 Direct Model For the maximum entropy model, the free parameters are the so-called λi or feature parameters (Berger et al., 1996; Ratnaparkhi, 1996). The training criterion is to optimize the logarithm of the model probabilities p(gn|gn−2 n−1, wn+2 n−2) over all positions n in the training corpus. The corresponding algorithm is referred to as GIS algorithm (Berger et al., 1996). As usual with maximum entropy models, the problem of smoothing does not seem to be critical and is not addressed explicitly. 5 Experimental Results Of course, there have already been many papers about POS tagging using statistical methods. The goal of the experiments is to compare the two decision rules and to analyze the differences in perform</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. L. Berger, S. Della Pietra and V. Della Pietra. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, No. 22, Vol. 1, pages 39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bisani</author>
<author>H Ney</author>
</authors>
<title>Bootstrap Estimates for Confidence Intervals in ASR Performance Evaluation.</title>
<date>2004</date>
<booktitle>In IEEE Int. Conf. on Acoustics, Speech and Signal Processing,</booktitle>
<pages>409--412</pages>
<location>Montreal, Canada.</location>
<contexts>
<context position="17521" citStr="Bisani and Ney, 2004" startWordPosition="3033" endWordPosition="3036">les yield similar results. For the direct model, the overall tagging error rate increases on each of the two tasks (from 3.0 % to 3.3 % on WSJ and from 5.4 % to 5.6 % on MTP) when we use the symbol decision rule instead of the string decision rule. In particular, for OOVs, the error rate goes up clearly. Right now, we do not have a clear explanation for this difference between the generative model and the direct model. It might be related to the ’forward’ structure of the direct model as opposed to the ’forward-backward’ structure of the generative model. Anyway, the refined bootstrap method (Bisani and Ney, 2004) has shown that differences in the overall tagging error rate are statistically not significant. 5.3 Examples A detailed analysis of the tagging results showed that for both models there are sentences where the one decision rule is more efficient and sentences where the other decision rule is better. For the generative model, these differences seem to occur at random, but for the direct model, some distinct tendencies can be observed. For example, WSJ Task Decision Overall OOVs PMs Rule Generative string 3.5 16.9 0 Model symbol 3.5 16.7 0 Direct string 3.0 15.4 0.08 Model symbol 3.3 16.6 0.1 T</context>
</contexts>
<marker>Bisani, Ney, 2004</marker>
<rawString>M. Bisani and H. Ney. 2004. Bootstrap Estimates for Confidence Intervals in ASR Performance Evaluation. In IEEE Int. Conf. on Acoustics, Speech and Signal Processing, pages 409–412, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A Stochastic Parts Program Noun Phrase Parser for Unrestricted Text.</title>
<date>1989</date>
<booktitle>In IEEE Int. Conf. on Acoustics, Speech and Signal Processing,</booktitle>
<pages>695--698</pages>
<location>Glasgow, Scotland.</location>
<contexts>
<context position="2941" citStr="Church, 1989" startWordPosition="468" endWordPosition="469">re speech recognition and machine translation. The advantage of the POS tagging task is that it will be easier to handle from the mathematical point of view and will result in closedform solutions for the decision rules. From this point-of-view, the POS tagging task serves as a good opportunity to illustrate the key concepts of the statistical approach to NLP. Related Work: For the task of POS tagging, statistical approaches were proposed already in the 60’s and 70’s (Stolz et al., 1965; Bahl and Mercer, 1976), before they started to find widespread use in the 80’s (Beale, 1985; DeRose, 1989; Church, 1989). To the best of our knowledge, the ’standard’ version of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. However, this method is mentioned as an aside and its implications for the Bayes decision rule and the statistical approach are</context>
</contexts>
<marker>Church, 1989</marker>
<rawString>K. W. Church. 1989. A Stochastic Parts Program Noun Phrase Parser for Unrestricted Text. In IEEE Int. Conf. on Acoustics, Speech and Signal Processing, pages 695–698, Glasgow, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S DeRose</author>
</authors>
<title>Grammatical Category Disambiguation by Statistical Optimization.</title>
<date>1989</date>
<journal>Computational Linguistics, No.</journal>
<volume>14</volume>
<pages>31--39</pages>
<contexts>
<context position="2926" citStr="DeRose, 1989" startWordPosition="466" endWordPosition="467">ent examples are speech recognition and machine translation. The advantage of the POS tagging task is that it will be easier to handle from the mathematical point of view and will result in closedform solutions for the decision rules. From this point-of-view, the POS tagging task serves as a good opportunity to illustrate the key concepts of the statistical approach to NLP. Related Work: For the task of POS tagging, statistical approaches were proposed already in the 60’s and 70’s (Stolz et al., 1965; Bahl and Mercer, 1976), before they started to find widespread use in the 80’s (Beale, 1985; DeRose, 1989; Church, 1989). To the best of our knowledge, the ’standard’ version of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. However, this method is mentioned as an aside and its implications for the Bayes decision rule and the statistic</context>
</contexts>
<marker>DeRose, 1989</marker>
<rawString>S. DeRose. 1989. Grammatical Category Disambiguation by Statistical Optimization. Computational Linguistics, No. 14, Vol. 1, pages 31–39</rawString>
</citation>
<citation valid="true">
<authors>
<author>R O Duda</author>
<author>P E Hart</author>
</authors>
<title>Pattern Classification and Scene Analysis.</title>
<date>1973</date>
<publisher>John Wiley &amp; Sons,</publisher>
<location>New York.</location>
<contexts>
<context position="4729" citStr="Duda and Hart, 1973" startWordPosition="770" endWordPosition="773"> Decision Rule for Minimum Error Rate 2.1 The Bayes Posterior Risk Knowing that any task in NLP tasks is a difficult one, we want to keep the number of wrong decisions as small as possible. This point-of-view has been used already for more than 40 years in pattern classification as the starting point for many techniques in pattern classification. To classify an observation vector y into one out of several classes c, we resort to the so-called statistical decision theory and try to minimize the average risk or loss in taking a decision. The result is known as Bayes decision rule (Chapter 2 in (Duda and Hart, 1973)): Pr(c|y) · L[c, c] I where L[c, ˜c] is the so-called loss function or error measure, i.e. the loss we incur in making decision c when the true class is ˜c. In the following, we will consider two specific forms of the loss function or error measure L[c, ˜c]. The first will be the measure for string errors, which is the typical loss function used in virtually all statistical approaches. The second is the measure for symbol errors, which is the more appropriate measure for POS tagging and also speech recognition with no insertion and deletion errors (such as isolated word recognition). 2.2 Stri</context>
</contexts>
<marker>Duda, Hart, 1973</marker>
<rawString>R. O. Duda and P. E. Hart. 1973. Pattern Classification and Scene Analysis. John Wiley &amp; Sons, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Goel</author>
<author>W Byrne</author>
</authors>
<title>Minimum Bayesrisk Automatic Speech Recognition.</title>
<date>2003</date>
<booktitle>Pattern Recognition in Speech and Language Processing.</booktitle>
<editor>In W. Chou and B. H. Juang (editors):</editor>
<publisher>CRC Press,</publisher>
<location>Boca Rota, Florida.</location>
<contexts>
<context position="3673" citStr="Goel and Byrne, 2003" startWordPosition="593" endWordPosition="596">f string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. However, this method is mentioned as an aside and its implications for the Bayes decision rule and the statistical approach are not addressed. Part of this work goes back to (Bahl et al., 1974) who considered a problem in coding theory. (Goel and Byrne, 2003): The error measure considered by the authors is the word error rate in speech recognition, i.e. the edit distance. Due to the mathematical complexity of this error measure, the authors resort to numeric approximations to compute the Bayes risk (see next section). Since this approach does not results in explicit closed-form equations and involves many numeric approximations, it is not easy to draw conclusions from this work. 2 Bayes Decision Rule for Minimum Error Rate 2.1 The Bayes Posterior Risk Knowing that any task in NLP tasks is a difficult one, we want to keep the number of wrong decisi</context>
<context position="6474" citStr="Goel and Byrne, 2003" startWordPosition="1087" endWordPosition="1090">evel and not at the level of single symbols. Inserting this cost function into the Bayes risk (see Section 2.1), we immediately obtain the following form of Bayes decision rule for minimum string error: { } Pr(gN 1 |wN 1 ) { } P r(gN 1 , wN 1 ) This is the starting point for virtually all statistical approaches in NLP like speech recognition and machine translation. However, this decision rule is only optimal when we consider string errors, e.g. sentence error rate in POS tagging and in speech recognition. In practice, however, the empirical errors are counted at the symbol level. Apart from (Goel and Byrne, 2003), this inconsistency of decision rule and error measure is never addressed in the literature. 2.3 Symbol Error Instead of the string error rate, we can also consider the error rate of single POS tag symbols (Bahl et al., 1974; Merialdo, 1994). This error measure is defined by the loss function: N L[gN1 , ˜gN1 ] = E [1 − δ(gn, ˜gn)] n=1 This loss function has to be inserted into the Bayes decision rule in Section 2.1. The computation of the expected loss, i.e. the averaging over all classes c˜ = ˜gN1 , can be performed in a closed form. We omit the details of the straightforward calculations an</context>
</contexts>
<marker>Goel, Byrne, 2003</marker>
<rawString>V. Goel and W. Byrne. 2003. Minimum Bayesrisk Automatic Speech Recognition. In W. Chou and B. H. Juang (editors): Pattern Recognition in Speech and Language Processing. CRC Press, Boca Rota, Florida.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kinscher</author>
<author>P Steiner</author>
</authors>
<title>M¨unster Tagging Project (MTP). Handout for the 4th Northern German Linguistic Colloquium,</title>
<date>1995</date>
<institution>University of M¨unster, Internal report.</institution>
<contexts>
<context position="15328" citStr="Kinscher and Steiner, 1995" startWordPosition="2657" endWordPosition="2660"> the M¨unster Tagging Project (MTP) German corpus. The POS tagging part of The WSJ corpus (Table 1) was compiled by the University of Pennsylvania and consists of about one million English words with manually annotated POS tags. Text POS Train Sentences 43508 Words+PMs 1061772 Singletons 21522 0 Word Vocabulary 46806 45 PM Vocabulary 25 9 Test Sentences 4478 Words+PMs 111220 OOVs 2879 0 Table 1: WSJ corpus statistics. The MTP corpus (Table 2) was compiled at the University of M¨unster and contains tagged German words from articles of the newspapers Die Zeit and Frankfurter Allgemeine Zeitung (Kinscher and Steiner, 1995). For the corpus statistics, it is helpful to distinguish between the true words and the punctuation marks (see Table 1 and Table 2). This distinction is made for both the text and the POS corpus. In addition, the tables show the vocabulary size (number of different tokens) for the words and for the punctuation marks. Punctuation marks (PMs) are all tokens which do not contain letters or digits. The total number of running tokens is indicated as Words+PMs. Singletons are the tokens which occur only once in = H n Text POS Train Sentences 19845 Words+PMs 349699 Singletons 32678 11 Word Vocabular</context>
</contexts>
<marker>Kinscher, Steiner, 1995</marker>
<rawString>J. Kinscher and P. Steiner. 1995. M¨unster Tagging Project (MTP). Handout for the 4th Northern German Linguistic Colloquium, University of M¨unster, Internal report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Merialdo</author>
</authors>
<title>Tagging English Text with a Probabilistic Model.</title>
<date>1994</date>
<journal>Computational Linguistics, No.</journal>
<volume>20</volume>
<pages>155--168</pages>
<contexts>
<context position="3249" citStr="Merialdo, 1994" startWordPosition="521" endWordPosition="522">ate the key concepts of the statistical approach to NLP. Related Work: For the task of POS tagging, statistical approaches were proposed already in the 60’s and 70’s (Stolz et al., 1965; Bahl and Mercer, 1976), before they started to find widespread use in the 80’s (Beale, 1985; DeRose, 1989; Church, 1989). To the best of our knowledge, the ’standard’ version of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. However, this method is mentioned as an aside and its implications for the Bayes decision rule and the statistical approach are not addressed. Part of this work goes back to (Bahl et al., 1974) who considered a problem in coding theory. (Goel and Byrne, 2003): The error measure considered by the authors is the word error rate in speech recognition, i.e. the edit distance. Due to the mathematical complexity of this error measure, th</context>
<context position="6716" citStr="Merialdo, 1994" startWordPosition="1130" endWordPosition="1131">s is the starting point for virtually all statistical approaches in NLP like speech recognition and machine translation. However, this decision rule is only optimal when we consider string errors, e.g. sentence error rate in POS tagging and in speech recognition. In practice, however, the empirical errors are counted at the symbol level. Apart from (Goel and Byrne, 2003), this inconsistency of decision rule and error measure is never addressed in the literature. 2.3 Symbol Error Instead of the string error rate, we can also consider the error rate of single POS tag symbols (Bahl et al., 1974; Merialdo, 1994). This error measure is defined by the loss function: N L[gN1 , ˜gN1 ] = E [1 − δ(gn, ˜gn)] n=1 This loss function has to be inserted into the Bayes decision rule in Section 2.1. The computation of the expected loss, i.e. the averaging over all classes c˜ = ˜gN1 , can be performed in a closed form. We omit the details of the straightforward calculations and state only the result. It turns out that we will need the marginal (and posterior) probability distribution Prm(g|wN1 ) at positions m = 1, ..., N: EPrm(g|wN1 ) := Pr(gN1 |wN1 ) gN1 : gm=g where the sum is carried out over all POS tag strin</context>
</contexts>
<marker>Merialdo, 1994</marker>
<rawString>B. Merialdo. 1994. Tagging English Text with a Probabilistic Model. Computational Linguistics, No. 20, Vol. 2, pages 155–168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Ney</author>
<author>S Martin</author>
<author>F Wessel</author>
</authors>
<title>Statistical Language Modelling by Leaving-OneOut.</title>
<date>1997</date>
<booktitle>Corpus-Based Methods in Speech and Language,</booktitle>
<pages>174--207</pages>
<editor>In G. Bloothooft and S. Young (editors):</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<location>Dordrecht.</location>
<contexts>
<context position="13617" citStr="Ney et al., 1997" startWordPosition="2378" endWordPosition="2381">ries of the POS trigram distribution p(g|g&apos;&apos;, g&apos;) and of the word membership distribution p(w|g). These unknown parameters are computed from a labelled training corpus, i.e. a collection of sentences where for each word the associated POS tag is given. In principle, the free parameters of the models are estimated as relative frequencies. For the test data, we have to allow for both POS trigrams (or ngrams) and (single) words that were not seen in the training data. This problem is tackled by applying smoothing methods that were originally designed for language modelling in speech recognition (Ney et al., 1997). 4.2 Direct Model For the maximum entropy model, the free parameters are the so-called λi or feature parameters (Berger et al., 1996; Ratnaparkhi, 1996). The training criterion is to optimize the logarithm of the model probabilities p(gn|gn−2 n−1, wn+2 n−2) over all positions n in the training corpus. The corresponding algorithm is referred to as GIS algorithm (Berger et al., 1996). As usual with maximum entropy models, the problem of smoothing does not seem to be critical and is not addressed explicitly. 5 Experimental Results Of course, there have already been many papers about POS tagging </context>
</contexts>
<marker>Ney, Martin, Wessel, 1997</marker>
<rawString>H. Ney, S. Martin and F. Wessel. 1997. Statistical Language Modelling by Leaving-OneOut. In G. Bloothooft and S. Young (editors): Corpus-Based Methods in Speech and Language, pages 174–207. Kluwer Academic Publishers, Dordrecht.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Model for Part-of-Speech Tagging.</title>
<date>1996</date>
<booktitle>In Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora ,</booktitle>
<pages>133--142</pages>
<location>Sommerset, NJ.</location>
<contexts>
<context position="11676" citStr="Ratnaparkhi, 1996" startWordPosition="2027" endWordPosition="2028">wn posterior distribution Pr(gN1 |wN1 ) by a model-based probability distribution p(gN1 |wN1 ): Pr(gN1 |wN1 ) → p(gN1 |wN1 ) and apply the chain rule: p(gn|gn−1 1 , wN1 ) n−1 n+2 p(gn |gn−2 , wn−2 ) As for the generative model, we have made specific assumptions: There is a second-order dependence for the tags gn1, and the dependence on the words wN1 is limited to a window wn+2 n−2 around position n. The resulting model is still rather complex and requires further specifications. The typical procedure is to resort to log-linear modelling, which is also referred to as maximum entropy modelling (Ratnaparkhi, 1996; Berger et al., 1996). 3.2.1 String Error For the minimum string error, we obtain the decision rule: { } wN 1 → ˆgN 1 = arg max p(gN 1 |wN 1 ) gN 1 N p(gN1 , wN1 ) = H n=1 p(gN1 , wN1 ) E= H gN1 : gm=g n N p(gN1 |wN1 ) = H n=1 N = H n=1 Since this is still a second-order model, we can use dynamic programming to compute the most likely POS string. 3.2.2 Symbol Error For the minimum symbol error, the marginal (and posterior) probability pm(g|wN1 ) has to be computed: pm(g|wN1 ) = E Pr(gN1 |wN1 ) gN1 : gm=g n−1 n+2 p(gn |gn−2 , wn−2 ) gN1 : gm=g which, due to the specific structure of the model </context>
<context position="13770" citStr="Ratnaparkhi, 1996" startWordPosition="2405" endWordPosition="2406">training corpus, i.e. a collection of sentences where for each word the associated POS tag is given. In principle, the free parameters of the models are estimated as relative frequencies. For the test data, we have to allow for both POS trigrams (or ngrams) and (single) words that were not seen in the training data. This problem is tackled by applying smoothing methods that were originally designed for language modelling in speech recognition (Ney et al., 1997). 4.2 Direct Model For the maximum entropy model, the free parameters are the so-called λi or feature parameters (Berger et al., 1996; Ratnaparkhi, 1996). The training criterion is to optimize the logarithm of the model probabilities p(gn|gn−2 n−1, wn+2 n−2) over all positions n in the training corpus. The corresponding algorithm is referred to as GIS algorithm (Berger et al., 1996). As usual with maximum entropy models, the problem of smoothing does not seem to be critical and is not addressed explicitly. 5 Experimental Results Of course, there have already been many papers about POS tagging using statistical methods. The goal of the experiments is to compare the two decision rules and to analyze the differences in performance. As the results</context>
<context position="16624" citStr="Ratnaparkhi, 1996" startWordPosition="2879" endWordPosition="2880">Table 2: MTP corpus statistics. the training data. Out-of-Vocabulary words (OOVs) are the words in the test data that did not not occur in the training corpus. 5.2 POS Tagging Results The tagging experiments were performed for both types of models, each of them with both types of the decision rules. The generative model is based on the approach described in (S¨undermann and Ney, 2003). Here the optimal value of the n-gram order is determined from the corpus statistics and has a maximum of n = 7. The experiments for the direct model were performed using the maximum entropy tagger described in (Ratnaparkhi, 1996). The tagging error rates are showed in Table 3 and Table 4. In addition to the overall tagging error rate (Overall), the tables show the tagging error rates for the Out-of-Vocabulary words (OOVs) and for the punctuation marks (PMs). For the generative model, both decision rules yield similar results. For the direct model, the overall tagging error rate increases on each of the two tasks (from 3.0 % to 3.3 % on WSJ and from 5.4 % to 5.6 % on MTP) when we use the symbol decision rule instead of the string decision rule. In particular, for OOVs, the error rate goes up clearly. Right now, we do n</context>
</contexts>
<marker>Ratnaparkhi, 1996</marker>
<rawString>A. Ratnaparkhi. 1996. A Maximum Entropy Model for Part-of-Speech Tagging. In Conf. on Empirical Methods in Natural Language Processing and Very Large Corpora , pages 133– 142, Sommerset, NJ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W S Stolz</author>
<author>P H Tannenbaum</author>
<author>F V Carstensen</author>
</authors>
<title>Stochastic Approach to the Grammatical Coding of English.</title>
<date>1965</date>
<journal>Communications of the ACM,</journal>
<volume>8</volume>
<pages>399--405</pages>
<contexts>
<context position="2819" citStr="Stolz et al., 1965" startWordPosition="446" endWordPosition="449"> NLP task which is tackled by the statistical approach and which makes use of a Bayes decision rule. Other prominent examples are speech recognition and machine translation. The advantage of the POS tagging task is that it will be easier to handle from the mathematical point of view and will result in closedform solutions for the decision rules. From this point-of-view, the POS tagging task serves as a good opportunity to illustrate the key concepts of the statistical approach to NLP. Related Work: For the task of POS tagging, statistical approaches were proposed already in the 60’s and 70’s (Stolz et al., 1965; Bahl and Mercer, 1976), before they started to find widespread use in the 80’s (Beale, 1985; DeRose, 1989; Church, 1989). To the best of our knowledge, the ’standard’ version of the Bayes decision rule, which minimizes the number of string errors, is used in virtually all approaches to POS tagging and other NLP tasks. There are only two research groups that do not take this type of decision rule for granted: (Merialdo, 1994): In the context of POS tagging, the author introduces a method that he calls maximum likelihood tagging. The spirit of this method is similar to that of this work. Howev</context>
</contexts>
<marker>Stolz, Tannenbaum, Carstensen, 1965</marker>
<rawString>W. S. Stolz, P. H. Tannenbaum and F. V. Carstensen. 1965. Stochastic Approach to the Grammatical Coding of English. Communications of the ACM, No. 8, pages 399–405.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D S¨undermann</author>
<author>H Ney</author>
</authors>
<title>SYNTHER - a New m-gram POS Tagger.</title>
<date>2003</date>
<booktitle>In Proc. of the Int. Conf. on Natural Language Processing and Knowledge Engineering,</booktitle>
<pages>628--633</pages>
<location>Beijing, China.</location>
<marker>S¨undermann, Ney, 2003</marker>
<rawString>D. S¨undermann and H. Ney. 2003. SYNTHER - a New m-gram POS Tagger. In Proc. of the Int. Conf. on Natural Language Processing and Knowledge Engineering, pages 628–633, Beijing, China.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>