<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000182">
<title confidence="0.999351">
A Deeper Exploration of the Standard PB-SMT Approach
to Text Simplification and its Evaluation
</title>
<author confidence="0.977111">
Sanja ˇStajner1 and Hannah B´echara1 and Horacio Saggion2
</author>
<affiliation confidence="0.9049595">
1Research Group in Computational Linguistics, University of Wolverhampton, UK
2TALN Research Group, Universitat Pompeu Fabra, Spain
</affiliation>
<email confidence="0.997532">
{SanjaStajner,Hanna.Bechara}@wlv.ac.uk, horacio.saggion@upf.edu
</email>
<sectionHeader confidence="0.993861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999946884615385">
In the last few years, there has been a
growing number of studies addressing the
Text Simplification (TS) task as a mono-
lingual machine translation (MT) problem
which translates from ‘original’ to ‘sim-
ple’ language. Motivated by those re-
sults, we investigate the influence of qual-
ity vs quantity of the training data on the
effectiveness of such a MT approach to
text simplification. We conduct 40 ex-
periments on the aligned sentences from
English Wikipedia and Simple English
Wikipedia, controlling for: (1) the sim-
ilarity between the original and simpli-
fied sentences in the training and develop-
ment datasets, and (2) the sizes of those
datasets. The results suggest that in the
standard PB-SMT approach to text simpli-
fication the quality of the datasets has a
greater impact on the system performance.
Additionally, we point out several impor-
tant differences between cross-lingual MT
and monolingual MT used in text sim-
plification, and show that BLEU is not a
good measure of system performance in
text simplification task.
</bodyText>
<sectionHeader confidence="0.999134" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999795703703703">
In the last few years, a growing number of studies
have addressed the text simplification (TS) task as
a monolingual machine translation (MT) problem
of translating sentences from ‘original’ to ‘sim-
ple’ language. Several studies reported promising
results using standard phrase-based statistical ma-
chine translation (PB-SMT) for this task (Specia,
2010; Coster and Kauchak, 2011a; Wubben et al.,
2012), but made no attempt to explain the reasons
behind the success of their systems. Specia (2010)
obtained reasonably good results (BLEU = 60.75)
despite the small size of the datasets used (4,483
original sentences and their corresponding simpli-
fications). Her results indicated that in this spe-
cific monolingual MT task, we do not need such
large datasets (as in cross-lingual MT) in order to
achieve good results.
At the moment, the scarcity and very limited
sizes of the available TS datasets (usually only
up to 1,000 sentence pairs) are the main factors
which impede the use of data-driven approaches to
text simplification for all languages except English
(for which English Wikipedia and Simple English
Wikipedia offer a large comparable TS dataset).
Therefore, in this paper, we decided to investigate
several important issues in MT-based text simpli-
fication:
</bodyText>
<listItem confidence="0.988725375">
1. The impact of the size of the training and de-
velopment datasets;
2. The impact of the similarity between the orig-
inal and simplified sentences in the training
and development datasets; and
3. The suitability of using the BLEU score for
the automatic evaluation of system’s perfor-
mance.
</listItem>
<bodyText confidence="0.999903846153846">
To the best of our knowledge, there have been no
studies which address those important questions.
In order to explore the first two issues, we con-
duct 40 translation experiments using the aligned
sentence pairs from the largest existing TS cor-
pus (Wikipedia TS corpus), controlling the train-
ing and development datasets for: (1) sentence
similarity (in terms of the S-BLEU score), and (2)
size. Our results indicate that only the former can
influence the MT output significantly. In order to
explore the last issue, we test our models on two
different test sets and perform human evaluation
of the output of several systems.
</bodyText>
<page confidence="0.937727">
823
</page>
<bodyText confidence="0.299455">
Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics
and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 823–828,
Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics
</bodyText>
<sectionHeader confidence="0.998936" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.9999838">
Specia (2010) used the standard PB-SMT model
provided by the Moses toolkit (Koehn et al., 2007)
to translate from ‘original’ to ‘simple’ sentences
in Brazilian Portuguese. The dataset contained
manual simplifications aimed at people with low
literacy levels. The most commonly used sim-
plifications (by human editors) were lexical sub-
stitutions and splitting sentences (Gasperin et al.,
2009). In terms of the automatic BLEU evalua-
tion (Papineni et al., 2002), the results were rea-
sonably good (BLEU = 60.75) despite the small
size of the corpora (4,483 original sentences and
their corresponding simplifications). However, the
TS system was overcautious in performing simpli-
fications, i.e. the simplifications produced by the
systems were closer to the source than to the ref-
erence segments (Specia, 2010).
Coster and Kauchak (2011a) used the same
approach for English. Additionally, they ex-
tended the PB-SMT system by adding phrasal
deletion to the probabilistic translation model
in order to better cover deletion, which is a
frequent phenomenon in TS. The system was
trained on 124,000 aligned sentences from En-
glish Wikipedia and Simple English Wikipedia.
The analysis of the Wikipedia TS corpus (Coster
and Kauchak, 2011b) reported that rewordings
(1–1 lexical substitutions) are the most common
simplification operation (65%). The system with
added phrasal deletion achieved the BLEU score
of 60.46, while the the standard model with-
out phrasal deletion achieved the BLEU score
of 59.87. However, the baseline (BLEU score
when the system does not perform any simplifica-
tion on the original sentence) was 59.37, indicat-
ing that the systems often leave the original sen-
tences unchanged. In order to address that prob-
lem, Wubben et al. (2012) performed post-hoc re-
ranking on the Moses’ output (simplification hy-
potheses) based on their dissimilarity to the input
(original sentences), while at the same time con-
trolling for its adequacy and fluency.
ˇStajner (2014) applied the same PB-SMT model
to two different TS corpora in Spanish, which con-
tained different levels of simplification. The re-
sults, which should be regarded only as prelim-
inary as both corpora have fewer than 1,000 sen-
tence pairs, imply that the level of simplification in
the training datasets has a greater impact than the
size of the datasets on the system’s performance.
</bodyText>
<sectionHeader confidence="0.997497" genericHeader="method">
3 Methodology
</sectionHeader>
<bodyText confidence="0.9999495">
We focus on the two TS corpora available for En-
glish (Wikipedia and EncBrit) and train a series
of translation models on training and development
datasets of varying size and quality.
</bodyText>
<subsectionHeader confidence="0.99669">
3.1 Corpora
</subsectionHeader>
<bodyText confidence="0.999959">
Wikipedia is a comparable TS corpus of 137,000
automatically aligned sentence pairs from English
Wikipedia and Simple English Wikipedia1, previ-
ously used by Coster and Kauchak (2011a). We
use a small portion of this corpus (240 sentence
pairs) to build the first test set (WikiTest), and
88,000 sentence pairs from the remaining sentence
pairs to build translation models.
EncBrit is a comparable TS corpus of original
sentences from Encyclopedia Britannica and their
manually simplified versions for children (Barzi-
lay and Elhadad, 2003).2 Given its small size (601
sentence pairs) this dataset is not used in the trans-
lation experiments. It is only used as the second
test set (EncBritTest).
</bodyText>
<subsectionHeader confidence="0.998072">
3.2 Experimental Setup
</subsectionHeader>
<bodyText confidence="0.999520692307692">
In all experiments, we use the same standard PB-
SMT model (Koehn et al., 2007), the GIZA++
implementation of IBM word alignment model
4 (Och and Ney, 2003), and the refinement and
phrase-extraction heuristics described further by
Koehn et al. (2003). We tune the systems using
minimum error rate training (MERT) (Och, 2003).
For the language model (LM) we use the corpus
of 60,000 Simple English Wikipedia articles3 and
build a 3-gram language model with Kneser-Ney
smoothing trained with SRILM (Stolcke, 2002).
We limit our stack size to 500 hypotheses during
decoding.
</bodyText>
<subsectionHeader confidence="0.998061">
3.3 Training and development datasets
</subsectionHeader>
<bodyText confidence="0.999903">
We tokenise and shuffle the initial dataset of
167,689 aligned sentences from the Wikipedia
dataset.4 Using the simplified sentences as ref-
erences and the original sentences as hypotheses,
</bodyText>
<footnote confidence="0.9931743">
1http://www.cs.middlebury.edu/
-dkauchak/simplification/
2http://www.cs.columbia.edu/-noemie/
alignment/
3Version 2.0 document-aligned data, available at:
http://www.cs.middlebury.edu/-dkauchak/
simplification/
4Version 2.0 sentence-aligned data, available at:
http://www.cs.middlebury.edu/-dkauchak/
simplification/
</footnote>
<page confidence="0.998447">
824
</page>
<tableCaption confidence="0.999643">
Table 1: Examples of sentences pairs with various S-BLEU scores from the training sets
</tableCaption>
<table confidence="0.466237">
S-BLEU Original sentence Simpler version
0.08 In women, the larger mammary glands within the
</table>
<bodyText confidence="0.9789986875">
breast produce the milk.
0.38 Built as a double-track railroad bridge, it was com-
pleted on January 1, 1889, and went out of service on
May 8, 1974.
0.55 In 2000, the series sold its naming rights to Internet
search engine Northern Light for five seasons, and
the series was named the Indy Racing Northern Light
Series.
0.63 Wildlife which eat acorns as an important part of
their diets include birds, such as jays, pigeons, some
ducks, and several species of woodpeckers.
0.77 It was discovered by Brett J. Gladman in 2000, and
given the temporary designation S2000 S 5.
0.87 Austen was not well known in Russia and the first
Russian translation of an Austen novel did not appear
until 1967.
The breast contains mammary glands.
It was built for trains and was completed on January
1, 1889. It closed down on May 8, 1974 after a bad
fire.
In 2000, the series sponsor became the Internet
search engine Northern Light. The series was named
the Indy Racing Northern Light Series.
Creatures that make acorns an important part of their
diet include birds, such as jays, pigeons, some ducks
and several species of woodpeckers.
It was found by Brett J. Gladman in 2000, and given
the designation S2000 S 5.
Austen was not well known in Russia. The first Rus-
sian translation of an Austen novel did not appear un-
til 1967.
we rank each sentence pair by its sentence-wise
BLEU (S-BLEU) score and categorise the sen-
tence pairs into eight different sets depending on
the interval in which their S-BLEU scores lie ((0,
0.3], (0.3, 0.4], (0.4, 0.5], (0.5, 0.6], (0.6, 0.7],
(0.7, 0.8], (0.8, 0.9], (0.9, 1]). With each of the
eight sets, we train five translation models, vary-
ing the number of sentences used for training and
tuning (2,000, 4,000, 6,000, 8,000, and 10,000 for
training and 200, 400, 600, 800, and 1,000 for
tuning, respectively). That leads to a total of 40
translation models varying by number of sentence
pairs and similarity between original and simpli-
fied sentences (in terms of the S-BLEU score)
in the datasets used for their training and tuning.
Several examples of sentence pairs with various
S-BLEU scores are presented in Table 1.
</bodyText>
<subsectionHeader confidence="0.998018">
3.4 Test datasets
</subsectionHeader>
<bodyText confidence="0.999865">
We test our models on two different test sets:
</bodyText>
<listItem confidence="0.846248">
1. The WikiTest which contains a total of 240
sentence pairs, with 30 sentence pairs from
each of the eight categories with different
intervals for the S-BLEU scores ([0,0.3],
(0.3,0.4], ... , (0.9,1]);
2. The EncBritTest which contains all 601
</listItem>
<bodyText confidence="0.678729666666667">
sentence pairs present in the EncBrit cor-
pus (with an unbalanced number of sentence
pairs from each of the eight S-BLEU inter-
vals).
The sizes of both test sets and their BLEU
scores (calculated using the original sentences as
</bodyText>
<tableCaption confidence="0.7538095">
Table 2: Test sets for all translation experiments
Test set Size BLEU
</tableCaption>
<bodyText confidence="0.8985566">
WikiTest 240 62.27
EncBritTest 601 12.40
simplification/translation hypotheses and the cor-
responding manually simplified sentences as sim-
plification/translation references) are given in Ta-
ble 2. Note that those BLEU scores can be re-
garded as the baselines for the translation exper-
iments, as they correspond to the BLEU score
obtained when the systems do not perform any
changes to the input.
</bodyText>
<sectionHeader confidence="0.996067" genericHeader="method">
4 Automatic Evaluation
</sectionHeader>
<bodyText confidence="0.999979875">
The BLEU scores for all 40 experiments tested
on the WikiTest dataset, are presented in Table 3.
The baseline BLEU score (when no simplification
is performed) for this test set is 62.27 (Table 2).
As shown in Table 3, none of the 40 experiments
have even reached that baseline. We compare S-
BLEU scores for each pair of experiments (240
reference sentences in the test set and their corre-
sponding automatically simplified sentences) us-
ing the paired t-test in SPSS in order to check
whether the differences in the obtained results are
significant. The only results that are significantly
lower than the rest are those obtained for the ex-
periments in which the training and development
datasets consist only of the sentence pairs with S-
BLEU scores between 0 and 0.3. The results sug-
</bodyText>
<page confidence="0.999491">
825
</page>
<tableCaption confidence="0.982251">
Table 3: BLEU scores on the WikiTest dataset Table 4: BLEU scores on the EncBritTest dataset
</tableCaption>
<table confidence="0.5619852">
S-BLEU Size of the training set
2,000 4,000 6,000 8,000 10,000
[0, 0.3] 56.38 56.38 56.15 57.75 57.89
(0.3, 0.4] 60.89 61.35 61.76 61.52 61.37
(0.4, 0.5] 61.27 61.36 61.74 61.55 62.11
(0.5, 0.6] 60.96 61.30 61.52 61.77 61.98
(0.6, 0.7] 60.96 61.30 61.60 61.69 61.80
(0.7, 0.8] 61.56 61.38 61.67 61.77 61.89
(0.8, 0.9] 61.54 61.49 61.51 61.57 61.61
(0.9, 1] 61.57 61.57 61.59 61.55 61.55
</table>
<bodyText confidence="0.989249342105263">
The rows represent intervals of the S-BLEU scores on the
training and development datasets, while the columns repre-
sent the number of the sentence pairs used for training. The
highest score is presented in bold; the baseline (no simplifi-
cation performed) is 62.27.
S-BLEU Size of the training set
2,000 4,000 6,000 8,000 10,000
[0, 0.3] 13.84 13.84 13.87 13.68 13.59
(0.3, 0.4] 14.05 13.95 14.08 14.06 14.01
(0.4, 0.5] 14.02 14.09 14.17 14.15 14.12
(0.5, 0.6] 14.09 14.22 14.27 14.16 14.13
(0.6, 0.7] 14.25 14.30 14.35 14.35 14.32
(0.7, 0.8] 14.30 14.29 14.30 14.30 14.28
(0.8, 0.9] 14.38 14.40 14.40 14.40 14.41
(0.9, 1] 12.71 12.52 12.46 12.39 12.54
The rows represent intervals of the S-BLEU scores on the
training and development datasets, while the columns repre-
sent the number of the sentence pairs used for training. The
highest score is presented in bold; the baseline (no simplifi-
cation performed) is 12.40.
gest that the sizes of the training and development
datasets do not influence the translation results sig-
nificantly on any type of sentence pairs used.
The results of the experiments tested on
EncBritTest (Table 4) again show that the quan-
tity of the training data does not influence sys-
tem performance. There are no statistically sig-
nificant differences (measured by the paired t-test
on S-BLEU scores on all 601 reference sentences
and the corresponding automatic simplifications)
among experiments which differ only in the size
of the training and development datasets. How-
ever, the models trained and tuned on the datasets
consisting of the sentence pairs with the high-
est and the lowest S-BLEU scores ([0,0.3] and
(0.9,1]) perform significantly worse than the mod-
els trained and tuned on the sentence pairs with
S-BLEU scores belonging to other intervals.
</bodyText>
<sectionHeader confidence="0.983934" genericHeader="method">
5 Human Evaluation
</sectionHeader>
<bodyText confidence="0.999919571428571">
The results presented in Tables 3 and 4 indicate
that the BLEU score, in MT-based text simplifica-
tion, mostly reflects the surface similarity of the
original and simplified sentences in the test set
and does not give an informative evaluation of the
systems. Therefore, we conducted a human as-
sessment of the generated sentences. Following
the standard procedure for human evaluation of
TS systems used in previous studies (Coster and
Kauchak, 2011a; Drndarevi´c et al., 2013; Wubben
et al., 2012; Saggion et al., 2015), three human
evaluators were asked to assess the generated sen-
tences on a 1–5 scale (where the higher mark al-
ways denotes better output) according to three cri-
</bodyText>
<tableCaption confidence="0.989869">
Table 5: Systems used in human evaluation
</tableCaption>
<figure confidence="0.638857857142857">
System Training size Dev. size S-BLEU
S-03-200 2,000 200 [0,0.3]
S-03-1000 10,000 1,000 [0,0.3]
S-06-200 2,000 200 (0.5,0.6]
S-06-1000 10,000 1,000 (0.5,0.6]
S-10-200 2,000 200 [0.9,1]
S-10-1000 10,000 1,000 [0.9,1]
</figure>
<bodyText confidence="0.995163190476191">
teria: grammaticality (G), meaning preservation
(M), and simplicity (S).
We decided that the same person has to rate
all simplified versions of the same original sen-
tence (shown always in a random order), in order
to make a fairer comparison among the systems.
That decision, however, limited the number of sys-
tems we can evaluate. Therefore, we focused only
on six out of 40 trained systems (Table 5). Sev-
eral examples of the automatically simplified sen-
tences and their scores are presented in Table 6.
The results of the human evaluation are given
in Table 7. It seems that the use of the sentence
pairs with the S-BLEU score between 0.5 and 0.6
leads to the best system performances in terms of
grammaticality and meaning preservation, while
at the same time improving the simplicity of the
sentences.5 Furthermore, the differences in human
scores between the systems differing only in size
of the datasets used were not statistically signifi-
cant. At the same time, the differences in human
</bodyText>
<footnote confidence="0.9967825">
5The details of the human evaluation and examples can be
found in (ˇStajner, 2015).
</footnote>
<page confidence="0.998008">
826
</page>
<tableCaption confidence="0.995262">
Table 6: Outputs of different systems and their human evaluation scores
</tableCaption>
<figure confidence="0.9922303">
G M S
5 / 4
4 4 4
5 5 5
3 3 3
5 / 2
5 4 3
5 5 2
3 3 2
2 3 2
</figure>
<table confidence="0.4665121">
System Sentence
Original Madrid was occupied by French troops during the Napoleonic Wars, and Napoleon’s
brother Joseph was installed on the throne.
S-03-200 Madrid was occupied by French his soldiers during the Napoleonic Wars, and Napoleon’s
brother Joseph was installed on the throne.
S-03-1000 Madrid was occupied by French troops during the Napoleonic Wars, and Napoleon’s
brother Joseph was put on the throne.
S-10-1000 Madrid was occupied by French troops during the Napoleonic Wars, and Napoleon’s
brother Joseph was -RRB- installed on them on the throne.
Original Although largely of postwar construction, this central area retains its old street pattern,
</table>
<bodyText confidence="0.71921275">
and most of the surviving historical and architectural monuments are located there.
S-03-200/1000 Although largely of postwar construction, this central area retains its old street pattern,
and most of the surviving and architectural historical monuments are located there.
S-06-1000 Although mostly of postwar construction, this central area retains its old street pattern, and
most of the surviving historical and architectural monuments are located there.
S-10-200 Although largely of postwar construction, this central area retains its old street pattern,
and most of the surviving historical monuments and architectural are a instead.
S-10-1000 As of the postwar construction, in this central area uses its old street pattern, and most of
the historical monuments and and architectural are located there.
The columns G, M, S contain the mean value of the human scores for grammaticality, meaning preservation, and simplicity,
respectively. Differences to the original versions are shown in italics. Systems which are not presented did not make any
changes to these two original sentences.
</bodyText>
<tableCaption confidence="0.99825">
Table 7: Results of the human evaluation
</tableCaption>
<table confidence="0.999119">
System G M S
Original 4.85 / 2.60
S-03-200 4.03 3.95 2.57
S-03-1000 4.20 4.03 2.85
S-06-200 4.50 4.45 2.68
S-06-1000 4.43 4.48 2.72
S-10-200 3.25 2.92 2.45
S-10-1000 2.92 2.95 2.53
</table>
<bodyText confidence="0.992267142857143">
The mean value of the human scores for grammaticality (G),
meaning preservation (M), and simplicity (S). The highest
achieved scores (excluding the scores for original sentences)
on each aspect (G, M, and S) are presented in bold.
scores between the systems differing only in sim-
ilarity of the sentence pairs (the interval of the S-
BLEU score) used were statistically significant.
</bodyText>
<sectionHeader confidence="0.999498" genericHeader="conclusions">
6 Conclusions
</sectionHeader>
<bodyText confidence="0.999978448275862">
Recently, there have been several attempts at ad-
dressing the TS task as a monolingual translation
problem, translating from ‘original’ to ‘simple’
sentences. However, they did not try to seek rea-
sons for the success or the failure of their systems.
Our experiments, conducted on 40 different,
carefully designed datasets from the largest avail-
able sentence-aligned TS corpus (Wikipedia TS
corpus), provide valuable insights into how much
of an effect the size and the quality of the training
data have on the performance of the PB-SMT sys-
tem which tries to learn to translate from ‘original’
to ‘simple’ sentences. The results indicate that us-
ing the sentence pairs with low S-BLEU scores
for training and tuning of PB-SMT models for TS
tend to cause the fluency to deteriorate and even
change the meaning of the output. Furthermore, it
seems that the sizes of the training and develop-
ment datasets do not play a significant role in how
successful the model is. It appears that carefully
selected sentence pairs in the training and develop-
ment datasets (i.e. sentence pairs with a moderate
similarity) lead to best performances of PB-SMT
systems regardless of the size of the datasets.
Our results open up new directions for enhanc-
ing the current PB-SMT models for TS, indicat-
ing that their performance can be significantly im-
proved by carefully filtering sentence pairs used
for training and tuning.
</bodyText>
<sectionHeader confidence="0.99495" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999943727272727">
The research described in this paper was par-
tially funded by the project SKATER-UPF-
TALN (TIN2012-38584-C06-03), Ministerio de
Economia y Competitividad, Secretaria de Estado
de Investigaci´on, Desarrollo e Innovaci´on, Spain,
and the project ABLE-TO-INCLUDE (CIP-ICT-
PSP-2013-7/621055). Hannah B´echara is sup-
ported by the People Programme (Marie Curie Ac-
tions) of the European Union’s Seventh Frame-
work Programme FP7/2007-2013/ under REA
grant agreement no. 31747.
</bodyText>
<page confidence="0.995896">
827
</page>
<sectionHeader confidence="0.990046" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999738988764046">
Regina Barzilay and Noemie Elhadad. 2003. Sen-
tence alignment for monolingual comparable cor-
pora. In Proceedings of the Conference on Em-
pirical Methods in Natural Language Processing
(EMNLP), pages 25–32. Association for Computa-
tional Linguistics.
William Coster and David Kauchak. 2011a. Learn-
ing to Simplify Sentences Using Wikipedia. In Pro-
ceedings of the 49th Annual Meeting of the Asso-
ciation for Computational Linguistics (ACL), pages
1–9. Association for Computational Linguistics.
William Coster and David Kauchak. 2011b. Simple
English Wikipedia: a new text simplification task.
In Proceedings of the 49th Annual Meeting of the
Association for Computational Linguistics: Human
Language Technologies (ACL&amp;HLT), pages 665–
669. Association for Computational Linguistics.
Biljana Drndarevi´c, Sanja ˇStajner, Stefan Bott, Susana
Bautista, and Horacio Saggion. 2013. Automatic
Text Simplication in Spanish: A Comparative Eval-
uation of Complementing Components. In Proceed-
ings of the 12th International Conference on Intelli-
gent Text Processing and Computational Linguistics
(CICLing), volume 7817 of Lecture Notes in Com-
puter Science, pages 488–500. Springer Berlin Hei-
delberg.
Caroline Gasperin, Lucia Specia, Tiago F. Pereira, and
Sandra M. Aluisio. 2009. Learning When to Sim-
plify Sentences for Natural Text Simplification. In
Proceedings of the Encontro Nacional de Intelign-
cia Artificial (ENIA), Bento Gonalves, Brazil, pages
809–818.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings of the Conference of the North American
Chapter of the Association for Computational Lin-
guistics on Human Language Technology - Volume
1, pages 48–54. Association for Computational Lin-
guistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran,
Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra
Constantin, and Evan Herbst. 2007. Moses: Open
source toolkit for statistical machine translation. In
Proceedings of the 45th Annual Meeting of the As-
sociation for Computational Linguistics (ACL). As-
sociation for Computational Linguistics.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignment
models. Computational Linguistics, 29(1):19–51.
Franz Och. 2003. Minimum Error Rate Training in
Statistical Machine Translation. In Proceedings of
the 41st Annual Meeting of the Association for Com-
putational Linguistics (ACL), pages 160–167. Asso-
ciation for Computational Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. BLEU: a method for automatic
evaluation of machine translation. In Proceedings
ofACL.
Horacio Saggion, Sanja ˇStajner, Stefan Bott, Simon
Mille, Luz Rello, and Biljana Drndarevic. 2015.
Making It Simplext: Implementation and Evaluation
of a Text Simplification System for Spanish. ACM
Transactions on Accessible Computing, 6(4):14:1–
14:36.
Lucia Specia. 2010. Translating from complex to sim-
plified sentences. In Proceedings of the 9th interna-
tional conference on Computational Processing of
the Portuguese Language (PROPOR), volume 6001
of Lecture Notes in Computer Science, pages 30–39.
Springer Berlin Heidelberg.
Andreas Stolcke. 2002. SRILM - an Extensible Lan-
guage Modeling Toolkit. In Proceedings of the In-
ternational Conference on Spoken Language Pro-
cessing (ICSLP), pages 901–904.
Sanja ˇStajner. 2014. Translating sentences from ‘orig-
inal’ to ‘simplified’ spanish. Procesamiento del
Lenguaje Natural, 53:61–68.
Sanja ˇStajner. 2015. New Data-Driven Approaches
to Text Simplification. Ph.D. thesis, University of
Wolverhampton, UK.
Sander Wubben, Antal van den Bosch, and Emiel
Krahmer. 2012. Sentence simplification by mono-
lingual machine translation. In Proceedings of the
50th Annual Meeting of the Association for Com-
putational Linguistics (ACL): Long Papers - Volume
1, pages 1015–1024. Association for Computational
Linguistics.
</reference>
<page confidence="0.997607">
828
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.307556">
<title confidence="0.9748375">A Deeper Exploration of the Standard PB-SMT to Text Simplification and its Evaluation</title>
<affiliation confidence="0.646239">Group in Computational Linguistics, University of Wolverhampton, Research Group, Universitat Pompeu Fabra,</affiliation>
<email confidence="0.999257">horacio.saggion@upf.edu</email>
<abstract confidence="0.998801111111111">In the last few years, there has been a growing number of studies addressing the Text Simplification (TS) task as a monolingual machine translation (MT) problem which translates from ‘original’ to ‘simple’ language. Motivated by those results, we investigate the influence of quality vs quantity of the training data on the effectiveness of such a MT approach to text simplification. We conduct 40 experiments on the aligned sentences from English Wikipedia and Simple English Wikipedia, controlling for: (1) the similarity between the original and simplified sentences in the training and development datasets, and (2) the sizes of those datasets. The results suggest that in the standard PB-SMT approach to text simplification the quality of the datasets has a greater impact on the system performance. Additionally, we point out several important differences between cross-lingual MT and monolingual MT used in text simplification, and show that BLEU is not a good measure of system performance in text simplification task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Regina Barzilay</author>
<author>Noemie Elhadad</author>
</authors>
<title>Sentence alignment for monolingual comparable corpora.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP),</booktitle>
<pages>25--32</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="6972" citStr="Barzilay and Elhadad, 2003" startWordPosition="1082" endWordPosition="1086">ion models on training and development datasets of varying size and quality. 3.1 Corpora Wikipedia is a comparable TS corpus of 137,000 automatically aligned sentence pairs from English Wikipedia and Simple English Wikipedia1, previously used by Coster and Kauchak (2011a). We use a small portion of this corpus (240 sentence pairs) to build the first test set (WikiTest), and 88,000 sentence pairs from the remaining sentence pairs to build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and bu</context>
</contexts>
<marker>Barzilay, Elhadad, 2003</marker>
<rawString>Regina Barzilay and Noemie Elhadad. 2003. Sentence alignment for monolingual comparable corpora. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 25–32. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Coster</author>
<author>David Kauchak</author>
</authors>
<title>Learning to Simplify Sentences Using Wikipedia.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>1--9</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1778" citStr="Coster and Kauchak, 2011" startWordPosition="262" endWordPosition="265">ormance. Additionally, we point out several important differences between cross-lingual MT and monolingual MT used in text simplification, and show that BLEU is not a good measure of system performance in text simplification task. 1 Introduction In the last few years, a growing number of studies have addressed the text simplification (TS) task as a monolingual machine translation (MT) problem of translating sentences from ‘original’ to ‘simple’ language. Several studies reported promising results using standard phrase-based statistical machine translation (PB-SMT) for this task (Specia, 2010; Coster and Kauchak, 2011a; Wubben et al., 2012), but made no attempt to explain the reasons behind the success of their systems. Specia (2010) obtained reasonably good results (BLEU = 60.75) despite the small size of the datasets used (4,483 original sentences and their corresponding simplifications). Her results indicated that in this specific monolingual MT task, we do not need such large datasets (as in cross-lingual MT) in order to achieve good results. At the moment, the scarcity and very limited sizes of the available TS datasets (usually only up to 1,000 sentence pairs) are the main factors which impede the us</context>
<context position="4708" citStr="Coster and Kauchak (2011" startWordPosition="723" endWordPosition="726">fications aimed at people with low literacy levels. The most commonly used simplifications (by human editors) were lexical substitutions and splitting sentences (Gasperin et al., 2009). In terms of the automatic BLEU evaluation (Papineni et al., 2002), the results were reasonably good (BLEU = 60.75) despite the small size of the corpora (4,483 original sentences and their corresponding simplifications). However, the TS system was overcautious in performing simplifications, i.e. the simplifications produced by the systems were closer to the source than to the reference segments (Specia, 2010). Coster and Kauchak (2011a) used the same approach for English. Additionally, they extended the PB-SMT system by adding phrasal deletion to the probabilistic translation model in order to better cover deletion, which is a frequent phenomenon in TS. The system was trained on 124,000 aligned sentences from English Wikipedia and Simple English Wikipedia. The analysis of the Wikipedia TS corpus (Coster and Kauchak, 2011b) reported that rewordings (1–1 lexical substitutions) are the most common simplification operation (65%). The system with added phrasal deletion achieved the BLEU score of 60.46, while the the standard mo</context>
<context position="6615" citStr="Coster and Kauchak (2011" startWordPosition="1028" endWordPosition="1031">ld be regarded only as preliminary as both corpora have fewer than 1,000 sentence pairs, imply that the level of simplification in the training datasets has a greater impact than the size of the datasets on the system’s performance. 3 Methodology We focus on the two TS corpora available for English (Wikipedia and EncBrit) and train a series of translation models on training and development datasets of varying size and quality. 3.1 Corpora Wikipedia is a comparable TS corpus of 137,000 automatically aligned sentence pairs from English Wikipedia and Simple English Wikipedia1, previously used by Coster and Kauchak (2011a). We use a small portion of this corpus (240 sentence pairs) to build the first test set (WikiTest), and 88,000 sentence pairs from the remaining sentence pairs to build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn </context>
<context position="15122" citStr="Coster and Kauchak, 2011" startWordPosition="2410" endWordPosition="2413">s ([0,0.3] and (0.9,1]) perform significantly worse than the models trained and tuned on the sentence pairs with S-BLEU scores belonging to other intervals. 5 Human Evaluation The results presented in Tables 3 and 4 indicate that the BLEU score, in MT-based text simplification, mostly reflects the surface similarity of the original and simplified sentences in the test set and does not give an informative evaluation of the systems. Therefore, we conducted a human assessment of the generated sentences. Following the standard procedure for human evaluation of TS systems used in previous studies (Coster and Kauchak, 2011a; Drndarevi´c et al., 2013; Wubben et al., 2012; Saggion et al., 2015), three human evaluators were asked to assess the generated sentences on a 1–5 scale (where the higher mark always denotes better output) according to three criTable 5: Systems used in human evaluation System Training size Dev. size S-BLEU S-03-200 2,000 200 [0,0.3] S-03-1000 10,000 1,000 [0,0.3] S-06-200 2,000 200 (0.5,0.6] S-06-1000 10,000 1,000 (0.5,0.6] S-10-200 2,000 200 [0.9,1] S-10-1000 10,000 1,000 [0.9,1] teria: grammaticality (G), meaning preservation (M), and simplicity (S). We decided that the same person has to</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>William Coster and David Kauchak. 2011a. Learning to Simplify Sentences Using Wikipedia. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL), pages 1–9. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Coster</author>
<author>David Kauchak</author>
</authors>
<title>Simple English Wikipedia: a new text simplification task.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL&amp;HLT),</booktitle>
<pages>665--669</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1778" citStr="Coster and Kauchak, 2011" startWordPosition="262" endWordPosition="265">ormance. Additionally, we point out several important differences between cross-lingual MT and monolingual MT used in text simplification, and show that BLEU is not a good measure of system performance in text simplification task. 1 Introduction In the last few years, a growing number of studies have addressed the text simplification (TS) task as a monolingual machine translation (MT) problem of translating sentences from ‘original’ to ‘simple’ language. Several studies reported promising results using standard phrase-based statistical machine translation (PB-SMT) for this task (Specia, 2010; Coster and Kauchak, 2011a; Wubben et al., 2012), but made no attempt to explain the reasons behind the success of their systems. Specia (2010) obtained reasonably good results (BLEU = 60.75) despite the small size of the datasets used (4,483 original sentences and their corresponding simplifications). Her results indicated that in this specific monolingual MT task, we do not need such large datasets (as in cross-lingual MT) in order to achieve good results. At the moment, the scarcity and very limited sizes of the available TS datasets (usually only up to 1,000 sentence pairs) are the main factors which impede the us</context>
<context position="4708" citStr="Coster and Kauchak (2011" startWordPosition="723" endWordPosition="726">fications aimed at people with low literacy levels. The most commonly used simplifications (by human editors) were lexical substitutions and splitting sentences (Gasperin et al., 2009). In terms of the automatic BLEU evaluation (Papineni et al., 2002), the results were reasonably good (BLEU = 60.75) despite the small size of the corpora (4,483 original sentences and their corresponding simplifications). However, the TS system was overcautious in performing simplifications, i.e. the simplifications produced by the systems were closer to the source than to the reference segments (Specia, 2010). Coster and Kauchak (2011a) used the same approach for English. Additionally, they extended the PB-SMT system by adding phrasal deletion to the probabilistic translation model in order to better cover deletion, which is a frequent phenomenon in TS. The system was trained on 124,000 aligned sentences from English Wikipedia and Simple English Wikipedia. The analysis of the Wikipedia TS corpus (Coster and Kauchak, 2011b) reported that rewordings (1–1 lexical substitutions) are the most common simplification operation (65%). The system with added phrasal deletion achieved the BLEU score of 60.46, while the the standard mo</context>
<context position="6615" citStr="Coster and Kauchak (2011" startWordPosition="1028" endWordPosition="1031">ld be regarded only as preliminary as both corpora have fewer than 1,000 sentence pairs, imply that the level of simplification in the training datasets has a greater impact than the size of the datasets on the system’s performance. 3 Methodology We focus on the two TS corpora available for English (Wikipedia and EncBrit) and train a series of translation models on training and development datasets of varying size and quality. 3.1 Corpora Wikipedia is a comparable TS corpus of 137,000 automatically aligned sentence pairs from English Wikipedia and Simple English Wikipedia1, previously used by Coster and Kauchak (2011a). We use a small portion of this corpus (240 sentence pairs) to build the first test set (WikiTest), and 88,000 sentence pairs from the remaining sentence pairs to build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn </context>
<context position="15122" citStr="Coster and Kauchak, 2011" startWordPosition="2410" endWordPosition="2413">s ([0,0.3] and (0.9,1]) perform significantly worse than the models trained and tuned on the sentence pairs with S-BLEU scores belonging to other intervals. 5 Human Evaluation The results presented in Tables 3 and 4 indicate that the BLEU score, in MT-based text simplification, mostly reflects the surface similarity of the original and simplified sentences in the test set and does not give an informative evaluation of the systems. Therefore, we conducted a human assessment of the generated sentences. Following the standard procedure for human evaluation of TS systems used in previous studies (Coster and Kauchak, 2011a; Drndarevi´c et al., 2013; Wubben et al., 2012; Saggion et al., 2015), three human evaluators were asked to assess the generated sentences on a 1–5 scale (where the higher mark always denotes better output) according to three criTable 5: Systems used in human evaluation System Training size Dev. size S-BLEU S-03-200 2,000 200 [0,0.3] S-03-1000 10,000 1,000 [0,0.3] S-06-200 2,000 200 (0.5,0.6] S-06-1000 10,000 1,000 (0.5,0.6] S-10-200 2,000 200 [0.9,1] S-10-1000 10,000 1,000 [0.9,1] teria: grammaticality (G), meaning preservation (M), and simplicity (S). We decided that the same person has to</context>
</contexts>
<marker>Coster, Kauchak, 2011</marker>
<rawString>William Coster and David Kauchak. 2011b. Simple English Wikipedia: a new text simplification task. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies (ACL&amp;HLT), pages 665– 669. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Biljana Drndarevi´c</author>
<author>Sanja ˇStajner</author>
<author>Stefan Bott</author>
<author>Susana Bautista</author>
<author>Horacio Saggion</author>
</authors>
<title>Automatic Text Simplication in Spanish: A Comparative Evaluation of Complementing Components.</title>
<date>2013</date>
<booktitle>In Proceedings of the 12th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing),</booktitle>
<volume>7817</volume>
<pages>488--500</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<marker>Drndarevi´c, ˇStajner, Bott, Bautista, Saggion, 2013</marker>
<rawString>Biljana Drndarevi´c, Sanja ˇStajner, Stefan Bott, Susana Bautista, and Horacio Saggion. 2013. Automatic Text Simplication in Spanish: A Comparative Evaluation of Complementing Components. In Proceedings of the 12th International Conference on Intelligent Text Processing and Computational Linguistics (CICLing), volume 7817 of Lecture Notes in Computer Science, pages 488–500. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Gasperin</author>
<author>Lucia Specia</author>
<author>Tiago F Pereira</author>
<author>Sandra M Aluisio</author>
</authors>
<title>Learning When to Simplify Sentences for Natural Text Simplification.</title>
<date>2009</date>
<booktitle>In Proceedings of the Encontro Nacional de Inteligncia Artificial (ENIA), Bento Gonalves, Brazil,</booktitle>
<pages>809--818</pages>
<contexts>
<context position="4268" citStr="Gasperin et al., 2009" startWordPosition="654" endWordPosition="657">r Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 823–828, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Specia (2010) used the standard PB-SMT model provided by the Moses toolkit (Koehn et al., 2007) to translate from ‘original’ to ‘simple’ sentences in Brazilian Portuguese. The dataset contained manual simplifications aimed at people with low literacy levels. The most commonly used simplifications (by human editors) were lexical substitutions and splitting sentences (Gasperin et al., 2009). In terms of the automatic BLEU evaluation (Papineni et al., 2002), the results were reasonably good (BLEU = 60.75) despite the small size of the corpora (4,483 original sentences and their corresponding simplifications). However, the TS system was overcautious in performing simplifications, i.e. the simplifications produced by the systems were closer to the source than to the reference segments (Specia, 2010). Coster and Kauchak (2011a) used the same approach for English. Additionally, they extended the PB-SMT system by adding phrasal deletion to the probabilistic translation model in order </context>
</contexts>
<marker>Gasperin, Specia, Pereira, Aluisio, 2009</marker>
<rawString>Caroline Gasperin, Lucia Specia, Tiago F. Pereira, and Sandra M. Aluisio. 2009. Learning When to Simplify Sentences for Natural Text Simplification. In Proceedings of the Encontro Nacional de Inteligncia Artificial (ENIA), Bento Gonalves, Brazil, pages 809–818.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology -</booktitle>
<volume>1</volume>
<pages>48--54</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7399" citStr="Koehn et al. (2003)" startWordPosition="1154" endWordPosition="1157"> build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references and the original sentences as hypotheses, 1http://www.cs.middlebury.edu/ -dkauchak/simplifica</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings of the Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technology - Volume 1, pages 48–54. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
<author>Christine Moran</author>
<author>Richard Zens</author>
</authors>
<title>Chris Dyer, Ondrej Bojar,</title>
<date>2007</date>
<booktitle>In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics.</booktitle>
<location>Alexandra</location>
<contexts>
<context position="3972" citStr="Koehn et al., 2007" startWordPosition="612" endWordPosition="615">ur results indicate that only the former can influence the MT output significantly. In order to explore the last issue, we test our models on two different test sets and perform human evaluation of the output of several systems. 823 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 823–828, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Specia (2010) used the standard PB-SMT model provided by the Moses toolkit (Koehn et al., 2007) to translate from ‘original’ to ‘simple’ sentences in Brazilian Portuguese. The dataset contained manual simplifications aimed at people with low literacy levels. The most commonly used simplifications (by human editors) were lexical substitutions and splitting sentences (Gasperin et al., 2009). In terms of the automatic BLEU evaluation (Papineni et al., 2002), the results were reasonably good (BLEU = 60.75) despite the small size of the corpora (4,483 original sentences and their corresponding simplifications). However, the TS system was overcautious in performing simplifications, i.e. the s</context>
<context position="7228" citStr="Koehn et al., 2007" startWordPosition="1128" endWordPosition="1131"> (2011a). We use a small portion of this corpus (240 sentence pairs) to build the first test set (WikiTest), and 88,000 sentence pairs from the remaining sentence pairs to build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned senten</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, Moran, Zens, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignment models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="7305" citStr="Och and Ney, 2003" startWordPosition="1141" endWordPosition="1144">the first test set (WikiTest), and 88,000 sentence pairs from the remaining sentence pairs to build translation models. EncBrit is a comparable TS corpus of original sentences from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references</context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignment models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum Error Rate Training in Statistical Machine Translation.</title>
<date>2003</date>
<booktitle>In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>160--167</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7473" citStr="Och, 2003" startWordPosition="1168" endWordPosition="1169">from Encyclopedia Britannica and their manually simplified versions for children (Barzilay and Elhadad, 2003).2 Given its small size (601 sentence pairs) this dataset is not used in the translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references and the original sentences as hypotheses, 1http://www.cs.middlebury.edu/ -dkauchak/simplification/ 2http://www.cs.columbia.edu/-noemie/ alignment/ 3Version 2.0 documen</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum Error Rate Training in Statistical Machine Translation. In Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics (ACL), pages 160–167. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>BLEU: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="4335" citStr="Papineni et al., 2002" startWordPosition="666" endWordPosition="669">nce on Natural Language Processing (Short Papers), pages 823–828, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Specia (2010) used the standard PB-SMT model provided by the Moses toolkit (Koehn et al., 2007) to translate from ‘original’ to ‘simple’ sentences in Brazilian Portuguese. The dataset contained manual simplifications aimed at people with low literacy levels. The most commonly used simplifications (by human editors) were lexical substitutions and splitting sentences (Gasperin et al., 2009). In terms of the automatic BLEU evaluation (Papineni et al., 2002), the results were reasonably good (BLEU = 60.75) despite the small size of the corpora (4,483 original sentences and their corresponding simplifications). However, the TS system was overcautious in performing simplifications, i.e. the simplifications produced by the systems were closer to the source than to the reference segments (Specia, 2010). Coster and Kauchak (2011a) used the same approach for English. Additionally, they extended the PB-SMT system by adding phrasal deletion to the probabilistic translation model in order to better cover deletion, which is a frequent phenomenon in TS. The</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Horacio Saggion</author>
<author>Sanja ˇStajner</author>
<author>Stefan Bott</author>
<author>Simon Mille</author>
<author>Luz Rello</author>
<author>Biljana Drndarevic</author>
</authors>
<title>Making It Simplext: Implementation and Evaluation of a Text Simplification System for Spanish.</title>
<date>2015</date>
<journal>ACM Transactions on Accessible Computing,</journal>
<volume>6</volume>
<issue>4</issue>
<pages>14--36</pages>
<marker>Saggion, ˇStajner, Bott, Mille, Rello, Drndarevic, 2015</marker>
<rawString>Horacio Saggion, Sanja ˇStajner, Stefan Bott, Simon Mille, Luz Rello, and Biljana Drndarevic. 2015. Making It Simplext: Implementation and Evaluation of a Text Simplification System for Spanish. ACM Transactions on Accessible Computing, 6(4):14:1– 14:36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucia Specia</author>
</authors>
<title>Translating from complex to simplified sentences.</title>
<date>2010</date>
<booktitle>In Proceedings of the 9th international conference on Computational Processing of the Portuguese Language (PROPOR),</booktitle>
<volume>6001</volume>
<pages>30--39</pages>
<publisher>Springer</publisher>
<location>Berlin Heidelberg.</location>
<contexts>
<context position="1752" citStr="Specia, 2010" startWordPosition="260" endWordPosition="261">he system performance. Additionally, we point out several important differences between cross-lingual MT and monolingual MT used in text simplification, and show that BLEU is not a good measure of system performance in text simplification task. 1 Introduction In the last few years, a growing number of studies have addressed the text simplification (TS) task as a monolingual machine translation (MT) problem of translating sentences from ‘original’ to ‘simple’ language. Several studies reported promising results using standard phrase-based statistical machine translation (PB-SMT) for this task (Specia, 2010; Coster and Kauchak, 2011a; Wubben et al., 2012), but made no attempt to explain the reasons behind the success of their systems. Specia (2010) obtained reasonably good results (BLEU = 60.75) despite the small size of the datasets used (4,483 original sentences and their corresponding simplifications). Her results indicated that in this specific monolingual MT task, we do not need such large datasets (as in cross-lingual MT) in order to achieve good results. At the moment, the scarcity and very limited sizes of the available TS datasets (usually only up to 1,000 sentence pairs) are the main f</context>
<context position="3890" citStr="Specia (2010)" startWordPosition="600" endWordPosition="601">for: (1) sentence similarity (in terms of the S-BLEU score), and (2) size. Our results indicate that only the former can influence the MT output significantly. In order to explore the last issue, we test our models on two different test sets and perform human evaluation of the output of several systems. 823 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 823–828, Beijing, China, July 26-31, 2015. c�2015 Association for Computational Linguistics 2 Related Work Specia (2010) used the standard PB-SMT model provided by the Moses toolkit (Koehn et al., 2007) to translate from ‘original’ to ‘simple’ sentences in Brazilian Portuguese. The dataset contained manual simplifications aimed at people with low literacy levels. The most commonly used simplifications (by human editors) were lexical substitutions and splitting sentences (Gasperin et al., 2009). In terms of the automatic BLEU evaluation (Papineni et al., 2002), the results were reasonably good (BLEU = 60.75) despite the small size of the corpora (4,483 original sentences and their corresponding simplifications).</context>
</contexts>
<marker>Specia, 2010</marker>
<rawString>Lucia Specia. 2010. Translating from complex to simplified sentences. In Proceedings of the 9th international conference on Computational Processing of the Portuguese Language (PROPOR), volume 6001 of Lecture Notes in Computer Science, pages 30–39. Springer Berlin Heidelberg.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Stolcke</author>
</authors>
<title>SRILM - an Extensible Language Modeling Toolkit.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing (ICSLP),</booktitle>
<pages>901--904</pages>
<contexts>
<context position="7660" citStr="Stolcke, 2002" startWordPosition="1197" endWordPosition="1198">he translation experiments. It is only used as the second test set (EncBritTest). 3.2 Experimental Setup In all experiments, we use the same standard PBSMT model (Koehn et al., 2007), the GIZA++ implementation of IBM word alignment model 4 (Och and Ney, 2003), and the refinement and phrase-extraction heuristics described further by Koehn et al. (2003). We tune the systems using minimum error rate training (MERT) (Och, 2003). For the language model (LM) we use the corpus of 60,000 Simple English Wikipedia articles3 and build a 3-gram language model with Kneser-Ney smoothing trained with SRILM (Stolcke, 2002). We limit our stack size to 500 hypotheses during decoding. 3.3 Training and development datasets We tokenise and shuffle the initial dataset of 167,689 aligned sentences from the Wikipedia dataset.4 Using the simplified sentences as references and the original sentences as hypotheses, 1http://www.cs.middlebury.edu/ -dkauchak/simplification/ 2http://www.cs.columbia.edu/-noemie/ alignment/ 3Version 2.0 document-aligned data, available at: http://www.cs.middlebury.edu/-dkauchak/ simplification/ 4Version 2.0 sentence-aligned data, available at: http://www.cs.middlebury.edu/-dkauchak/ simplificat</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andreas Stolcke. 2002. SRILM - an Extensible Language Modeling Toolkit. In Proceedings of the International Conference on Spoken Language Processing (ICSLP), pages 901–904.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanja ˇStajner</author>
</authors>
<title>Translating sentences from ‘original’ to ‘simplified’ spanish.</title>
<date>2014</date>
<booktitle>Procesamiento del Lenguaje Natural,</booktitle>
<pages>53--61</pages>
<marker>ˇStajner, 2014</marker>
<rawString>Sanja ˇStajner. 2014. Translating sentences from ‘original’ to ‘simplified’ spanish. Procesamiento del Lenguaje Natural, 53:61–68.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sanja ˇStajner</author>
</authors>
<title>New Data-Driven Approaches to Text Simplification.</title>
<date>2015</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Wolverhampton, UK.</institution>
<marker>ˇStajner, 2015</marker>
<rawString>Sanja ˇStajner. 2015. New Data-Driven Approaches to Text Simplification. Ph.D. thesis, University of Wolverhampton, UK.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sander Wubben</author>
<author>Antal van den Bosch</author>
<author>Emiel Krahmer</author>
</authors>
<title>Sentence simplification by monolingual machine translation.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL): Long Papers -</booktitle>
<volume>1</volume>
<pages>1015--1024</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Wubben, van den Bosch, Krahmer, 2012</marker>
<rawString>Sander Wubben, Antal van den Bosch, and Emiel Krahmer. 2012. Sentence simplification by monolingual machine translation. In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (ACL): Long Papers - Volume 1, pages 1015–1024. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>