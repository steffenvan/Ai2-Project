<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.9984885">
Error-driven HMM-based Chunk Tagger
with Context-dependent Lexicon
</title>
<author confidence="0.954545">
GuoDong ZHOU
</author>
<affiliation confidence="0.774351">
Kent Ridge Digital Labs
</affiliation>
<address confidence="0.970048">
21 Heng Hui Keng Terrace
Singapore 119613
</address>
<email confidence="0.966313">
zhougd@krdl.ora.sg
</email>
<author confidence="0.756912">
Jian SU
</author>
<affiliation confidence="0.644205">
Kent Ridge Digital Labs
</affiliation>
<address confidence="0.975207">
21 Heng Hui Keng Terrace
Singapore 119613
</address>
<email confidence="0.994566">
sujian@krdl.org.sz
</email>
<sectionHeader confidence="0.996587" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999899954545455">
This paper proposes a new error-driven 11M3/1-
based text chunk tagger with context-dependent
lexicon. Compared with standard HMM-based
tagger, this tagger uses a new Hidden Markov
Modelling approach which incorporates more
contextual information into a lexical entry.
Moreover, an error-driven learning approach is
adopted to decrease the memory requirement by
keeping only positive lexical entries and makes
it possible to further incorporate more context-
dependent lexical entries. Experiments show
that this technique achieves overall precision
and recall rates of 93.40% and 93.95% for all
chunk types, 93.60% and 94.64% for noun
phrases, and 94.64% and 94.75% for verb
phrases when trained on PENN WSJ TreeBank
section 00-19 and tested on section 20-24, while
25-fold validation experiments of PENN WSJ
TreeBank show overall precision and recall
rates of 96.40% and 96.47% for all chunk types,
96.49% and 96.99% for noun phrases, and
97.13% and 97.36% for verb phrases.
</bodyText>
<sectionHeader confidence="0.95505" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999987807692308">
Text chunking is to divide sentences into non-
overlapping segments on the basis of fairly
superficial analysis. Abney(1991) proposed this
as a useful and relatively tractable precursor to
full parsing, since it provides a foundation for
further levels of analysis, while still allowing
more complex attachment decisions to be
postponed to a later phase.
Text chunking typically relies on fairly
simple and efficient processing algorithms.
Recently, many researchers have looked at text
chunking in two different ways: Some
researchers have applied rule-based methods,
combining lexical data with finite state or other
rule constraints, while others have worked on
inducing statistical models either directly from
the words and/or from automatically assigned
part-of-speech classes. On the statistics-based
approaches, Skut and Brants(1998) proposed a
HMM-based approach to recognise the syntactic
structures of limited length. Buchholz, Veenstra
and Daelemans(1999), and Veenstra(1999)
explored memory-based learning method to fmd
labelled chunks. Ratnaparkhi(1998) used
maximum entropy to recognise arbitrary chunk
as part of a tagging task. On the rule-based
approaches, Bourigaut(1992) used some
heuristics and a grammar to extract
&amp;quot;terminology noun phrases&amp;quot; from French text.
Voutilainen(1993) used similar method to detect
English noun phrases. Kupiec(1993) applied
finite state transducer in his noun phrases
recogniser for both English and French.
Ramshaw and Marcus(1995) used
transformation-based learning, an error-driven
learning technique introduced by Eric
Brill(1993), to locate chunks in the tagged
corpus. Grefenstette(1996) applied finite state
transducers to fmd noun phrases and verb
phrases.
In this paper, we will focus on statistics-
based methods. The structure of this paper is as
follows: In section 1, we will briefly describe
the new error-driven HMM-based chunk tagger
with context-dependent lexicon in principle. In
section 2, a baseline system which only includes
the current part-of-speech in the lexicon is
given. In section 3, several extended systems
with different context-dependent lexicons are
described. In section 4, an error-driven learning
method is used to decrease memory requirement
of the lexicon by keeping only positive lexical
</bodyText>
<page confidence="0.997406">
71
</page>
<bodyText confidence="0.997914952380952">
entries and make it possible to further improve
the accuracy by merging different context-
dependent lexicons into one after automatic
analysis of the chunking errors. Finally, the
conclusion is given.
The data used for all our experiments is
extracted from the PENN WSJ Treebank
(Marcus et al. 1993) by the program provided
by Sabine Buchholz from Tilbug University.
We use sections 00-19 as the training data and
20-24 as test data. Therefore, the performance is
on large scale task instead of small scale task on
CoNLL-2000 with the same evaluation
program.
For evaluation of our results, we use the
precision and recall measures. Precision is the
percentage of predicted chunks that are actually
correct while the recall is the percentage of
correct chunks that are actually found. For
convenient comparisons of only one value, we
also list the Ft1=1 value(Rijsbergen 1979) :
</bodyText>
<equation confidence="0.991016">
(0 2 + 1) • precision recall , with = 1.
f3 2 • precision + recall
</equation>
<sectionHeader confidence="0.561009" genericHeader="method">
1 HM:M-based Chunk Tagger
</sectionHeader>
<bodyText confidence="0.99990825">
The idea of using statistics for chunking goes
back to Church(1988), who used corpus
frequencies to determine the boundaries of
simple non-recursive noun phrases. Skut and
Brants(1998) modified Church&apos;s approach in a
way permitting efficient and reliable recognition
of structures of limited depth and encoded the
structure in such a way that it can be recognised
by a Viterbi tagger. This makes the process run
in time linear to the length of the input string.
Our approach follows Skut and Brants&apos; way
by employing HMM-based tagging method to
model the chunking process.
Given a token sequence Gin = gig2 •-• gn ,
the goal is to find a stochastic optimal tag
sequence TIn = t1t2 •tn which maximizes
</bodyText>
<equation confidence="0.810855">
log P(Tin ) :
</equation>
<bodyText confidence="0.939756833333333">
The second item in the above equation is the
mutual information between the tag sequence
Tin and the given token sequence Gin . By
assuming that the mutual information between
Gin and Ti&amp;quot; is equal to the summation of mutual
information between Gin and the individual tag
</bodyText>
<equation confidence="0.993229615384615">
t1(15_i:5n) :
P(Tin ,G) PO; , )
log = E log
P(771) • P(G;z ) P(t i) • P(G11)
or
MI(Tin ,G )= E mi(ti , G;&apos; ) ,
we have:
log P(T I qz)
n P(t.,q)
= log P(Tin ) + E log
P(t1) • P(G)
= log P(7-in ) — log P(ti)+ log P(t
i=i
</equation>
<bodyText confidence="0.999463115384615">
The first item of above equation can be
solved by using chain rules. Normally, each tag
is assumed to be probabilistic dependent on the
N-1 previous tags. Here, backoff bigram(N=2)
model is used. The second item is the
summation of log probabilities of all the tags.
Both the first item and second item correspond
to the language model component of the tagger
while the third item corresponds to the lexicon
component of the tagger. Ideally the third item
can be estimated by using the forward-backward
algorithm(Rabiner 1989) recursively for the
first-order(Rabiner 1989) or second-order
HMMs(Watson and Chunk 1992). However,
several approximations on it will be attempted
later in this paper instead. The stochastic
optimal tag sequence can be found by
maxmizing the above equation over all the
possible tag sequences. This is implemented by
the Viterbi algorithm.
The main difference between our tagger and
other standard taggers lies in our tagger has a
context-dependent lexicon while others use a
context-independent lexicon.
For chunk tagger, we have g1 = piwi where
Win = w1v2 wn is the word - Sequence and
</bodyText>
<equation confidence="0.9975085">
pin = P &apos; p,, is the part-of-speech
P(7&apos;In ,q`)
log P(Tin I ) = log P(Tin ) + log
P(Tr). P(Gi)
</equation>
<page confidence="0.957242">
72
</page>
<bodyText confidence="0.986175594594595">
sequence. Here, we use structural tags to
representing chunIcing(bracketing and labelling)
structure. The basic idea of representing the
structural tags is similar to Skut and
Brants(1998) and the structural tag consists of
three parts:
1) Structural relation. The basic idea is simple:
structures of limited depth are encoded using a
finite number of flags. Given a sequence of
input tokens(here, the word and part-of-speech
pairs), we consider the structural relation
between the previous input token and the
current one. For the recognition of chunks, it is
sufficient to distinguish the following four
different structural relations which uniquely
identify the sub-structures of depth 1 (Skut and
Brants used seven different structural relations
to identify the sub-structures of depth 2).
00 the current input token and the previous one
have the same parent
90 one ancestor of the current input token and
the previous input token have the same parent
09 the current input token and one ancestor of
the previous input token have the same parent
99 one ancestor of the current input token and
one ancestor of the previous input token have
the same parent
For example, in the following chunk tagged
sentence(NULL represents the beginning and
end of the sentence):
NULL [NP He/PRP] [VP reckons/VBZ] [ NP
the/DT current/JJ account/NN deficit/NN] [VP
will/MD narrow/VB] [PP to/TO] [NP only/RB
#/# 1.8/CD billion/CD] [PP in/IN] [NP
September/NNP] [0 J.] NULL
the corresponding structural relations between
two adjacent input tokens are:
</bodyText>
<equation confidence="0.991409117647059">
90(N&apos;ULL He/PRP)
99(He/PRP reckons/VBZ)
99(reckons/VBZ the/DT)
00(the/DT current/11)
00(current/JJ account/NN)
00(account/NN deficit/NN)
99(deficit/NN will/MD)
00(will/MD narrow/VB)
99(narrowNB to/TO)
99(to/TO only/RB)
00(only/RB #/#)
00(#/# 1.8/CD)
00(1.8/CD billion/CD)
99(billion/CD in/IN)
99(in/IN september/NNP)
99(september/NNP ./.)
09(./. NULL)
</equation>
<bodyText confidence="0.997164529411765">
Compared with the B-Chunk and I-Chunk
used in Ramshaw and Marcus(1995), structural
relations 99 and 90 correspond to B-Chunk
which represents the first word of the chunk,
and structural relations 00 and 09 correspond to
I-Chunk which represnts each other in the chunk
while 90 also means the beginning of the
sentence and 09 means the end of the sentence.
2)Phrase category. This is used to identify the
phrase categories of input tokens.
3)Part-of-speech. Because of the limited
number of structural relations and phrase
categories, the part-of-speech is added into the
structural tag to represent more accurate models.
For the above chunk tagged sentence, the
structural tags for all the corresponding input
tokens are:
</bodyText>
<equation confidence="0.9986446875">
90_PRP_NP(He/PRP)
99_VBZ_VP(reckons/VBZ)
99_DT_NP(the/DT)
00_H_NP(current/JJ)
00_NN_NP(account/NN)
00_NN_NP(deficit/NN)
99_MD_VP(will/MD)
00_VB_VP(narrow/VB)
99_TO_PP(to/TO)
99_RB_NP(only/RB)
00_#_NP(#/#)
00_CD_NP(1.8/CD)
00_CD_NP(blllion/CD)
99_IN_PP(in/IN)
99_NNP_NP(september/NNP)
99_._0(./.)
</equation>
<sectionHeader confidence="0.815139" genericHeader="method">
2 The Baseline System
</sectionHeader>
<bodyText confidence="0.9719124">
As the baseline system, we assume
P(ti I Gin)= P(ti I pi). That is to say, only the
current part-of-speech is used as a lexical entry
to determine the current structural chunk tag.
Here, we define:
</bodyText>
<listItem confidence="0.9919565">
• (I) is the list of lexical entries in the
chunking lexicon,
</listItem>
<page confidence="0.758758">
73
</page>
<listItem confidence="0.989773375">
• 1431 is the number of lexical entries(the size
of the chunking lexicon)
• C is the training data.
For the baseline system, we have:
• ={pi, pi3C} , where p, is a part-of-
speech existing in the training data C
• 1401=48 (the number of part-of-speech tags
in the training data).
</listItem>
<bodyText confidence="0.9920058">
Table 1 gives an overview of the results of
the chunking experiments. For convenience,
precision, recall and Fp.„ values are given
seperately for the chunk types NP, VP, ADJP,
ADVP and PP.
</bodyText>
<table confidence="0.998304875">
Type Precision Recall F
P=1
Overall 87.01 89.68 88.32
NP 90.02 90.50 90.26
VP 89.86 93.14 91.47
ADJP 70.94 63.84 67.20
ADVP 57.98 80.33 67.35
PP 85.95 96.62 90.97
</table>
<tableCaption confidence="0.878951">
Table 1: Results of chunking experiments with
the lexical entry list: 0= fp i , pi3C1
</tableCaption>
<sectionHeader confidence="0.993948" genericHeader="method">
3 Context-dependent Lexicons
</sectionHeader>
<bodyText confidence="0.999962555555556">
In the last section, we only use current part-of-
speech as a lexical entry. In this section, we will
attempt to add more contextual information to
approximate P(ti I G;1). This can be done by
adding lexical entries with more contextual
information into the lexicon 43. In the
following, we will discuss five context-
dependent lexicons which consider different
contextual information.
</bodyText>
<subsectionHeader confidence="0.953329">
3.1 Context of current part-of-speech and
current word
</subsectionHeader>
<bodyText confidence="0.999940818181818">
In this case, the current part-of-speech and
word pair is also used as a lexical entry to
determine the current structural chunk tag and
we have a total of about 49563 lexical
entries( 4)1=49563). Actually, the lexicon used
here can be regarded as context-independent.
The reason we discuss it in this section is to
distinguish it from the context-independent
lexicon used in the baseline system. Table 2
give an overview of the results of the chunking
experiments on the test data.
</bodyText>
<table confidence="0.9941447">
Type Precision Recall F_1
Overall 90.32 92.18 91.24
NP 90.75 92.14 91.44
VP 90.88 92.78 91.82
ADJP 76.01 70.00 72.88
ADVP 72.67 88.33 79.74
PP 94.96 96.48 95.71
Table 2 : Results of chunking experiments with
the lexical entry list:
43={P1w1,Piwi3C}+{PoP13C)
</table>
<bodyText confidence="0.913491625">
Table 2 shows that incorporation of current
word information improves the overall Ffr4
value by 2.9%(especially for the ADJP, ADVP
and PP chunks), compared with Table 1 of the
baseline system which only uses current part-of-
speech information. This result suggests that .
current word information plays a very important
role in determining the current chunk tag.
</bodyText>
<subsectionHeader confidence="0.7568105">
3.2 Context of previous part-of-speech and
current part-of-speech
</subsectionHeader>
<bodyText confidence="0.909529">
Here, we assume:
</bodyText>
<equation confidence="0.984996">
P(ti IG;)
{
P(ti I P
P(ti I pi)
</equation>
<bodyText confidence="0.93700025">
where
where
(I3={p1w1,p1w13C}+{pop13C} and piwi is a
part-of-speech and word pair existing in the
training data C.
0={Pi-1PoPi-IPi3C}+{PoPi3C} and Pi4Pi
is a pair of previous part-of-speech and current
part-of-speech existing in the training data C.
In this case, the previous part-of-speech and
current part-of-speech pair is also used as a
lexical entry to determine the current structural
chunk tag and we have a total of about 1411
lexical entries(&apos; 4)1=1414 Table 3 give an
overview of the results of the chunking
experiments.
Here, we assume:
</bodyText>
<equation confidence="0.989762">
P(ti 1G1)= rti I Piwi) Piwi e
P(ti I Pi) p1w,cI•
</equation>
<page confidence="0.988724">
74
</page>
<table confidence="0.9897828">
Type Precision Recall F1
Overall 88.63 89.00 88.82
NP 90.77 91.18 90.97
VP 92.46 92.98 92.72
ADJP 74.93 60.13 66.72
ADVP 71.65 73.21 72.42
PP 87.28 91.80 89.49
Table 3: Results of chunking experiments with
the lexical entry list: (I) =
{ P i-IP 0 P i-IP i3C} + {Pi, Pi3C}
</table>
<bodyText confidence="0.982556916666667">
Compared with Table 1 of the baseline
system, Table 3 shows that additional contextual
information of previous part-of-speech improves
the overall Fs.1 value by 0.5%. Especially,
Fi3=1 value for VP improves by 1.25%, which
indicates that previous part-of-speech
information has a important role in determining
the chunk type VP. Table 3 also shows that the
recall rate for chunk type ADJP decrease by
3.7%. It indicates that additional previous part-
of-speech information makes ADJP chunks
easier to merge with neibghbouring chunks.
</bodyText>
<subsectionHeader confidence="0.955837">
3.3 Context of previous part-of-speech,
</subsectionHeader>
<bodyText confidence="0.736959">
previous word and current part-of-speech
Here, we assume:
</bodyText>
<equation confidence="0.830927333333333">
P(t, I G&apos;)
{PO i I 6 (13
P(t, I p,)
</equation>
<bodyText confidence="0.998130722222222">
where
where is a triple pattern existing in
the training corpus.
In this case, the previous part-of-speech,
previous word and current part-of-speech triple
is also used as a lexical entry to determine the
current structural chunk tag and 1(131=136164.
Table 4 gives the results of the chunking
experiments. Compared with Table 1 of the
baseline system, Table 4 shows that additional
136116 new lexical entries of format
improves the overall Fp=1 value by
3.3%. Compared with Table 3 of the extended
system 2.2 which uses previous part-of-speech
and current part-of-speech as a lexical entry,
Table 4 shows that additional contextual
information of previous word improves the
overall F,1 value by 2.8%.
</bodyText>
<table confidence="0.990549090909091">
Type Precision Recall F
P-4
Overall 91.23 92.03 91.63
NP 92.89 93.85 93.37
VP 94.10 94.23 94.16
ADJP 79.83 69.01 74.03
ADVP 76.91 &apos; 80.53 78.68
PP 90.41 94.77 92.53
Table 4 : Results of chunking experiments with
the lexical entry list:
43 ={Pi_iwi-IPi, Pi-1w1-1Pi3C} i-{Pi,P,3C}
</table>
<tableCaption confidence="0.4993885">
3.4 Context of previous part-of-speech, current
part-of-speech and current word
</tableCaption>
<bodyText confidence="0.974149">
Here, we assume:
</bodyText>
<equation confidence="0.997736333333333">
P(t, I G)
{P(ti I Pi-iP,wi)
P(t, I pi)
</equation>
<bodyText confidence="0.907269833333333">
where
P wi3C)-F {Pi P13C},
where is a triple pattern existing in
the training and 14)1=131416.
Table 5 gives the results of the chunking
experiments.
</bodyText>
<table confidence="0.998815125">
Type Precision Recall F..,
Overall 92.67 93.43 93.05
Overall
NP 93.35 94.10 93.73
VP 93.05 94.30 93.67
ADJP 80.65 72.27 76.23
ADVP 78.92 84.48 81.60
PP 95.30 96.67 95.98
</table>
<tableCaption confidence="0.557815333333333">
Table 5: Results of chunking experiments with
the lexical entry list:
1:1)={Pi-1P1w1,Pi-IPiw13C}+{P1, Pi3C}
</tableCaption>
<bodyText confidence="0.997827">
Compared with Table 2 of the extended
system which uses current part-of-speech and
current word as a lexical entry, Table 5 shows
that additional contextual information of
previous part-of-speech improves the overall
Fp--=1 value by 1.8%.
</bodyText>
<subsectionHeader confidence="0.901344">
3.5 Context of previous part-of-speech,
</subsectionHeader>
<bodyText confidence="0.9880216">
previous word, current part-of-speech and
current word
Here, the context of previous part-of-speech,
current part-of-speech and current word is used
as a lexical entry to determine the current
</bodyText>
<figure confidence="0.4909315">
p11p1w1 E (I)
P1-1 Pi wi (13
</figure>
<page confidence="0.988825">
75
</page>
<bodyText confidence="0.950665625">
structural chunk tag and (I) =
Pi3C1 ,
where p. w1is a pattern existing in the
training corpus. Due to memory limitation, only
lexical entries which occurs more than 1 times
are kept. Out of 364365 possible lexical entries
existing in the training data, 98489 are kept(
101=98489).
</bodyText>
<sectionHeader confidence="0.999018" genericHeader="method">
4 Error-driven Learning
</sectionHeader>
<bodyText confidence="0.999434666666667">
In section 2, we implement a baseline system
which only considers current part-of-speech as a
lexical entry to deterrmine the current chunk tag
while in section 3, we implement several
extended systems which take more contextual
information into consideration.
</bodyText>
<table confidence="0.9952212">
P(t /G) Here, we will examine the effectiveness of
113(ti I Pi-11411-1Piwii) lexical entries to reduce the size of lexicon and
P(ti I Pi) make it possible to further improve the
chunking accuracy by merging several context-
dependent lexicons in a single lexicon.
</table>
<tableCaption confidence="0.9700495">
Table 6 gives the results of the chunking
experiments.
</tableCaption>
<table confidence="0.9857877">
Type Precision Recall Fs.,
Overall 92.28 93.04 92.66
NP 93.50 93.53 93.52
VP 92.62 94.07 93.35
ADJP 81.39 72.17 76.50
AD&apos;VP 75.09 86.23 80.27
PP 94.12 97.12 95.59
Table 6: Results of chunking experiments with
the lexical entry list: 04:0=
{Pi-1111,-IPiwi,Pi-1wi-1P1wi3C1+ IP; ,Pi3C)
</table>
<bodyText confidence="0.997817571428571">
Compared with Table 2 of the extended
system which uses current part-of-speech and
current word as a lexical entry, Table 6 shows
that additional contextual information of
previous part-of-speech improves the overall
F1 value by 1.8%.
fi=
</bodyText>
<subsectionHeader confidence="0.940401">
3.6 Conclusion
</subsectionHeader>
<bodyText confidence="0.997947230769231">
Above experiments shows that adding more
contextual information into lexicon significantly
improves the chunking accuracy. However, this
improvement is gained at the expense of a very
large lexicon and we fmd it difficult to merge all
the above context-dependent lexicons in a single
lexicon to further improve the chunking
accurracy because of memory limitation. In
order to reduce the size of lexicon effectively,
an error-driven learning approach is adopted to
examine the effectiveness of lexical entries and
make it possible to further improve the
chunking accuracy by merging all the above
context-dependent lexicons in a single lexicon.
This will be discussed in the next section.
For a new lexical entry ei, the effectiveness
(e,) is measured by the reduction in error
which results from adding the lexical entry to
the lexicon: F (e,)= F:TOr (ei)_F:zoor (e1).
Here, FoError(e1) is the chunking error number
of the lexical entry e, for the old lexicon
and Fe,EZ;(e1) is the chunking error number of
the lexical entry e, for the new lexicon
+ AO where e, E (Ad is the list of
new lexical entries added to the old lexicon d)).
If Fo(e1)&gt; 0, we define the lexical entry e, as
positive for lexicon ED . Otherwise, the lexical
entry e, is negative for lexicon .
Tables 7 and 8 give an overview of the
effectiveness distributions for different lexicons
applied in the extended systems, compared with
the lexicon applied in the baseline system, on
the test data and the training data, respectively.
Tables 7 and 8 show that only a minority of
lexical entries are positive. This indicates that
discarding non-positive lexical entries will
largely decrease the lexicon memory
requirement while keeping the chunking
accurracy.
</bodyText>
<table confidence="0.975423235294118">
Context Positive Negative Total
—...., 1800 314 49515
209 136 1363
_... ... ,.„.. 2876 229 136116
2895 193 131368
„— ..., —...., 4083 155 98441
Table 7 : The effectiveness of lexical entries on
the test data
76
Context Positive Negative Total
POS,W, 6724 719 49515
POSPOS„ 357 196 1363
POS,_,W,_,POS„ 13205 582 136116
POSPOSI,W, 14186 325 131368
15516 144 98441
Table 8 : The effectiveness of lexical entries on
the training data
</table>
<bodyText confidence="0.994811615384615">
Tables 9-13 give the performances of the
five error-driven systems which discard all the
non-positive lexical enrties on the training data.
Here, V is the lexicon used in the baseline
system. cto = {pi , pi3C} and 60:13 =CI . It
is found that Fi3=1 values of error driven
systems for context of current part-of-speech
and word pair and for context of previous part-
of-speech and current part-of-speech increase by
1.2% and 0.6%. Although Fi3=1 values for other
three cases slightly decrease by 0.02%, 0.02%
and 0.19%, the sizes of lexicons have been
greatly reduced by 85% to 97%.
</bodyText>
<table confidence="0.985776236363636">
Type Precision Recall Ff3.1
Overall 91.69 93.28 92.48
NP 92.64 93.48 93.06
VP 92.16 93.66 92.90
ADJP 78.39 71.69 74.89
ADVP 73.66 87.80 80.11
PP 95.18 97.38 96.27
Table 9 : Results of chunking experiments with
error-driven lexicon: 43=
{piwi, ptwi3C &amp; Fo.(piwi)&gt; 0} +{pi, piaC}
Type Precision Recall Fi3=1
Overall 88.68 90.28 89.47
NP 90.61 91.57 91.08
VP 91.80 94.08 92.90
ADJP 72.20 62.72 67.13
ADVP 70.53 78.90 74.48
PP 86.55 96.34 91.19
Table 10: Results of chunking experiments
with error-driven lexicon: (I)=
/20 Pi-IPi3C &amp; F..(19H121)&gt; 13)
+{PoPi3C}
Type Precision Recall Fe=1
_
Overall 91.02 92.21 91.61
NP 92.36 93.69 93.02
VP 93.68 94.94 94.30
ADJP 78.28 71.46 74.71
ADVP 76.77 81.79 79.20
PP 90.67 95.37 92.96
Table 11: Results of chunking experiments
with error-driven lexicon: 43=
IP i-1w,-1Pi, Pi-1w,-1P13C &amp; F(b. (Pi-iw,-1Pi)&gt; 0}
4-{PoPi3C}
Type Precision Recall F,
Overall 92.84 93.21 93.03
NP 93.35 93.65 93.50
VP 93.97 94.67 94.32
ADJP 79.49 72.94 76.07
ADVP 79.47 85.91 82.57
PP 95.19 96.29 95.74
Table 12: Results of chunking experiments
with error-driven lexicon: (II=
{P,-1Piwi,/),-IPiwi3C&amp; Fo.(Pi-IPiwi)&gt; 0}
4-{PoPi3C)
Type Precision Recall F,
Overall 91.99 92.95 92.47
NP 93.35 93.39 93.37
VP 92.89 94.36 93.62
ADJP 80.01 71.70 75.63
ADVP 73.40 87.32 79.76
PP 93.42 97.33 95.33
Table 13: Results of chunking experiments
with error-driven lexicon: 0:13=
Pi-ilvi-iPiwiaC+{pi, pi3C}
&amp;F..(Pi-1lv1-1Piw1)&gt;01
</table>
<construct confidence="0.8970335">
After discussing the five context-dependent
lexicons separately, now we explore the
merging of context-dependent lexicons by
assuming:
=
{ 17 t-i
. p.w. p • w. p.w.3C
&amp;
&amp; Fo.(PfriPiwd&gt;
P Po &amp; F,. (P Pi) &gt;
+(Pi-i Pi, p..1p3C &amp; F4).(P1-1 Pd.&gt; 0}
+{piwi, p1wi3C &amp; F4,.(p iw d&gt; 0) + {p JEW}
</construct>
<page confidence="0.991066">
77
</page>
<figure confidence="0.974132454545454">
and P(ti I G;&apos;) is approximated by the following
order:
1. if p iwi E CD ,
P(ti IGin)=P(ti I pi_ovi_ipiwi)
2. if pi_ipiwi E ,
P(ti IG;&apos;)=P(ti I
3. if E CI)
P(ti /G1&apos;)= P(ti I pi_iwi_j,pi)
4. if piwi E , P(ti I Gin )=: P(ti I piwi)
5. if E , P(t I ) P(t I pi_ipi)
6. P(ti I Gr ). P(ti 1 pi_lpi)
</figure>
<bodyText confidence="0.975345555555556">
Table 14 gives an overview of the chunking
experiments using the above assumption. It
shows that the F0=1 value for the merged
context-dependent lexicon inreases to 93.68%.
For a comparison, the Fp=1 value is 93.30%
when all the possible lexical entries are included
in (1) (Due to memory limitation, only the top
150000 mostly occurred lexical entries are
included).
</bodyText>
<table confidence="0.998754625">
Type Precision Recall F
fi=1
Overall 93.40 93.95 93.68
NP 93.60 94.64 94.12
VP 94.64 94.75 94.70
ADJP 77.12 74.55 75.81
ADVP 82.39 83.80 83.09
PP 96.61 96.63 96.62
</table>
<tableCaption confidence="0.912187">
Table 14: Results of chunking experiments
with the merged context-dependent lexicon
</tableCaption>
<bodyText confidence="0.9996628">
For the relationship between the training
corpus size and error driven learning
performance, Table 15 shows that the
performance of error-driven learning improves
stably when the training corpus size increases.
</bodyText>
<table confidence="0.997599545454545">
Training Sections I 0 I Accuracy FB1
0-1 14384 94.78% 91.95
0-3 24507 95.19% 92.51
0-5 32316 95.28% 92.77
0-7 38286 95.41% 93.00
0-9 39876 95.53% 93.12
0-11 43372 95.65% 93.31
0-13 46029 95.62% 93.29
0-15 47901 95.66% 93.34
0-17 48813 95.74% 93.41
0-19 49988 95.92% 93.68
</table>
<tableCaption confidence="0.9658295">
Table 15: The performance of error-driven
learning with different training corpus size
</tableCaption>
<bodyText confidence="0.996808166666667">
For comparison with other chunk taggers,
we also evaluate our chunk tagger with the
merged context-dependent lexicon by cross-
validation on all 25 partitions of the PENN WSJ
TreeBank. Table 16 gives an overview of such
chunking experiments.
</bodyText>
<table confidence="0.9226359">
Type Precision Recall Ffl.1
Overall 96.40 96.47 96.44
NP 96.49 96.99 96.74
VP 97.13 97.36 97.25
ADJP 89.92 88.15 89.03
ADVP 91.52 87.57 89.50
PP 97.13 97.36 97.25
Table 16: Results of 25-fold cross-validation
chunking experiments with the merged
context-dependent lexicon
</table>
<bodyText confidence="0.997115125">
Tables 14 and 16 shows that our new chunk
tagger greatly outperforms other reported chunk
taggers on the same training data and test data
by 2%-3%.(Buchholz S., Veenstra J. and
Daelmans W.(1999), Ramshaw L.A. and
Marcus M.P.(1995), Daelemans W., Buchholz
S. and Veenstra J.(1999), and Veenstra
J.(1999)).
</bodyText>
<sectionHeader confidence="0.951448" genericHeader="conclusions">
Conclusion
</sectionHeader>
<bodyText confidence="0.969931727272727">
This paper proposes a new error-driven HMM-
based chunk tagger with context-dependent
lexicon. Compared with standard HM:M-based
tagger, this new tagger uses a new Hidden
Markov Modelling approach which incorporates
more contextual information into a lexical entry
by assuming M/(Tin ,G:1)= Emicti, Gin .
i=1
Moreover, an error-driven learning approach is
adopted to drease the memeory requirement and
further improve the accuracy by including more
context-dependent information into lexicon.
It is found that our new chunk tagger
singnificantly outperforms other reported chunk
taggers on the same training data and test data.
For future work, we will explore the
effectivessness of considering even more
contextual information on approximation of
P(Tln Gi) by using the forward-backward
algoritlun(Rabiner 1989) while currently we
only consider the contextual information of
current location and previous location.
</bodyText>
<page confidence="0.997328">
78
</page>
<sectionHeader confidence="0.992148" genericHeader="acknowledgments">
Acknowledgement
</sectionHeader>
<bodyText confidence="0.99992625">
We wish to thank Sabine Buchholz
from Tilbug University for kindly providing us
her program which is also used to
extact data for Con11-2000 share task.
</bodyText>
<sectionHeader confidence="0.998445" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.99977647368421">
Abney S. &amp;quot;Parsing by chunks &amp;quot;. Principle-Based
Parsing edited by Berwick, Abney and Tenny.
Kluwer Academic Publishers.
Argamon S., Dagan I. and Krymolowski Y. &amp;quot;A
memory-based approach to learning shallow
natural language patterns.&amp;quot; COLING/ACL-
1998. Pp.67-73. Montreal, Canada. 1998.
Bod R. &amp;quot;A computational model of language
performance: Data-oriented parsing.&amp;quot;
COLING-1992. Pp.855-859. Nantes, France.
1992.
Bourigault D. &amp;quot;Surface grammatical analysis for
the extraction of terminological noun
phrases&amp;quot;. COLING-92. Pp.977-981. 1992.
Brill Eric. &amp;quot;A corpus-based approach to
language learning&amp;quot;. Ph.D thesis. Univ. of
Penn. 1993
Buchholz S., Veenstra J. and Daehnans W.
&amp;quot;Cascaded grammatical relation assignment.&amp;quot;
Proceeding of EMNLP/VLC-99, at ACL&apos;99.
1999
Cardie C. &amp;quot;A case-based approach to knowledge
acquisition for domain-specific sentence
analysis.&amp;quot; Proceeding of the 11th National
Conference on Artificial Intelligence. Pp.798-
803. Menlo Park, CA, USA. AAAI Press.
1993.
Church K.W. &amp;quot;A stochastic parts program and
noun phrase parser for unrestricted Text.&amp;quot;
Proceeding of Second Conference on Applied
Natural Language Processing. Pp.136-143.
Austin, Texas, USA. 1988.
Daelemans W., Buchholz S. and Veenstra J.
&amp;quot;Memory-based shallow parsing.&amp;quot; CoNLL-
1999. Pp.53-60. Bergen, Norway. 1999.
Daelemans W., Zavrel J., Berck P. and Gillis S.
&amp;quot;MBT: A memory-based part-of-speech
tagger generator.&amp;quot; Proceeding of the Fourth
Workshop on Large Scale Corpora. Pp.14-27.
ACL SIGDAT. 1996.
Grefenstette G. &amp;quot;Light parsing as finite-state
filtering&amp;quot;. Workshop on Extended Finite State
Models of Language at ECAI&apos;96. Budapest,
Hungary. 1996.
Kupiec J. &amp;quot; An algorithm for finding noun
phrase correspondences in bilingual corpora&amp;quot;.
ACL&apos;93. Pp17-22. 1993.
Marcus M., Santorini B. and Marcinkiewicz
M.A. &amp;quot;Buliding a large annotated corpus of
English: The Penn Treebank&amp;quot;. Computational
Linguistics. 19(2):313-330. 1993.
Rabiner L. &amp;quot;A tutorial on Hidden Markov
Models and selected applications in speech
recognition&amp;quot;. IEEE 77(2), pp.257-285. 1989.
Ramshaw L.A. and Marcus M.P.
&amp;quot;Transformation-based Learning&amp;quot;.
Proceeding of 3th ACL Workshop on Very
Large Corpora at ACL&apos;95. 1995.
Rijsbergen C.J.van. Information Retrieval.
Buttersworth, London. 1979.
Skut W. and Brants T. &amp;quot;Chunk tagger: statistical
recognition of noun phrases.&amp;quot; ESSLLI-1998
Workshop on Automated Acquisition of Syntax
and Parsing. Saarbruucken, Germany. 1998.
Veenstra J. &amp;quot;Memory-based text chunking&amp;quot;.
Workshop on machine learning in human
language technology at ACAI&apos;99. 1999.
Voutilainen A. &amp;quot;Nptool: a detector of English
phrases&amp;quot;. Proceeding of the Workshop on
Very Large Corpora. Pp48-57. ACL&apos; 93.
1993
Watson B. and Chunk Tsoi A. &amp;quot;Second order
Hidden Markov Models for speech
recognition&amp;quot;. Proceeding of 4th Australian
International Conference on Speech Science
and Technology. Pp.146-151. 1992.
</reference>
<page confidence="0.999045">
79
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.299805">
<title confidence="0.9966335">Error-driven HMM-based Chunk with Context-dependent Lexicon</title>
<author confidence="0.767795">GuoDong ZHOU</author>
<affiliation confidence="0.835292">Kent Ridge Digital Labs</affiliation>
<address confidence="0.9144615">21 Heng Hui Keng Terrace Singapore 119613</address>
<email confidence="0.962105">zhougd@krdl.ora.sg</email>
<affiliation confidence="0.7150425">Jian Kent Ridge Digital</affiliation>
<address confidence="0.9647225">21 Heng Hui Keng Singapore</address>
<email confidence="0.993588">sujian@krdl.org.sz</email>
<abstract confidence="0.997000608695652">paper proposes a new error-driven based text chunk tagger with context-dependent lexicon. Compared with standard HMM-based tagger, this tagger uses a new Hidden Markov Modelling approach which incorporates more contextual information into a lexical entry. Moreover, an error-driven learning approach is adopted to decrease the memory requirement by keeping only positive lexical entries and makes it possible to further incorporate more contextdependent lexical entries. Experiments show that this technique achieves overall precision and recall rates of 93.40% and 93.95% for all chunk types, 93.60% and 94.64% for noun phrases, and 94.64% and 94.75% for verb phrases when trained on PENN WSJ TreeBank section 00-19 and tested on section 20-24, while 25-fold validation experiments of PENN WSJ TreeBank show overall precision and recall rates of 96.40% and 96.47% for all chunk types, 96.49% and 96.99% for noun phrases, and 97.13% and 97.36% for verb phrases.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>S Abney</author>
</authors>
<title>Parsing by chunks &amp;quot;. Principle-Based Parsing edited by Berwick, Abney and Tenny.</title>
<publisher>Kluwer Academic Publishers.</publisher>
<marker>Abney, </marker>
<rawString>Abney S. &amp;quot;Parsing by chunks &amp;quot;. Principle-Based Parsing edited by Berwick, Abney and Tenny. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Argamon</author>
<author>I Dagan</author>
<author>Y Krymolowski</author>
</authors>
<title>A memory-based approach to learning shallow natural language patterns.&amp;quot;</title>
<date>1998</date>
<booktitle>COLING/ACL1998. Pp.67-73.</booktitle>
<location>Montreal,</location>
<marker>Argamon, Dagan, Krymolowski, 1998</marker>
<rawString>Argamon S., Dagan I. and Krymolowski Y. &amp;quot;A memory-based approach to learning shallow natural language patterns.&amp;quot; COLING/ACL1998. Pp.67-73. Montreal, Canada. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Bod</author>
</authors>
<title>A computational model of language performance: Data-oriented parsing.&amp;quot;</title>
<date>1992</date>
<pages>1992--855</pages>
<location>Nantes,</location>
<marker>Bod, 1992</marker>
<rawString>Bod R. &amp;quot;A computational model of language performance: Data-oriented parsing.&amp;quot; COLING-1992. Pp.855-859. Nantes, France. 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Bourigault</author>
</authors>
<title>Surface grammatical analysis for the extraction of terminological noun phrases&amp;quot;.</title>
<date>1992</date>
<pages>92--977</pages>
<marker>Bourigault, 1992</marker>
<rawString>Bourigault D. &amp;quot;Surface grammatical analysis for the extraction of terminological noun phrases&amp;quot;. COLING-92. Pp.977-981. 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brill Eric</author>
</authors>
<title>A corpus-based approach to language learning&amp;quot;.</title>
<date>1993</date>
<journal>Ph.D thesis. Univ. of</journal>
<marker>Eric, 1993</marker>
<rawString>Brill Eric. &amp;quot;A corpus-based approach to language learning&amp;quot;. Ph.D thesis. Univ. of Penn. 1993</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Buchholz</author>
<author>J Veenstra</author>
<author>W Daehnans</author>
</authors>
<title>Cascaded grammatical relation assignment.&amp;quot; Proceeding of EMNLP/VLC-99, at ACL&apos;99.</title>
<date>1999</date>
<marker>Buchholz, Veenstra, Daehnans, 1999</marker>
<rawString>Buchholz S., Veenstra J. and Daehnans W. &amp;quot;Cascaded grammatical relation assignment.&amp;quot; Proceeding of EMNLP/VLC-99, at ACL&apos;99. 1999</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Cardie</author>
</authors>
<title>A case-based approach to knowledge acquisition for domain-specific sentence analysis.&amp;quot;</title>
<date>1993</date>
<booktitle>Proceeding of the 11th National Conference on Artificial Intelligence. Pp.798-803.</booktitle>
<publisher>AAAI Press.</publisher>
<location>Menlo Park, CA, USA.</location>
<marker>Cardie, 1993</marker>
<rawString>Cardie C. &amp;quot;A case-based approach to knowledge acquisition for domain-specific sentence analysis.&amp;quot; Proceeding of the 11th National Conference on Artificial Intelligence. Pp.798-803. Menlo Park, CA, USA. AAAI Press. 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A stochastic parts program and noun phrase parser for unrestricted Text.&amp;quot;</title>
<date>1988</date>
<booktitle>Proceeding of Second Conference on Applied Natural Language Processing. Pp.136-143.</booktitle>
<location>Austin, Texas, USA.</location>
<marker>Church, 1988</marker>
<rawString>Church K.W. &amp;quot;A stochastic parts program and noun phrase parser for unrestricted Text.&amp;quot; Proceeding of Second Conference on Applied Natural Language Processing. Pp.136-143. Austin, Texas, USA. 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>S Buchholz</author>
<author>J Veenstra</author>
</authors>
<title>Memory-based shallow parsing.&amp;quot;</title>
<date>1999</date>
<booktitle>CoNLL1999. Pp.53-60.</booktitle>
<location>Bergen,</location>
<marker>Daelemans, Buchholz, Veenstra, 1999</marker>
<rawString>Daelemans W., Buchholz S. and Veenstra J. &amp;quot;Memory-based shallow parsing.&amp;quot; CoNLL1999. Pp.53-60. Bergen, Norway. 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Daelemans</author>
<author>J Zavrel</author>
<author>P Berck</author>
<author>S Gillis</author>
</authors>
<title>MBT: A memory-based part-of-speech tagger generator.&amp;quot;</title>
<date>1996</date>
<booktitle>Proceeding of the Fourth Workshop on Large Scale Corpora. Pp.14-27. ACL SIGDAT.</booktitle>
<marker>Daelemans, Zavrel, Berck, Gillis, 1996</marker>
<rawString>Daelemans W., Zavrel J., Berck P. and Gillis S. &amp;quot;MBT: A memory-based part-of-speech tagger generator.&amp;quot; Proceeding of the Fourth Workshop on Large Scale Corpora. Pp.14-27. ACL SIGDAT. 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Grefenstette</author>
</authors>
<title>Light parsing as finite-state filtering&amp;quot;.</title>
<date>1996</date>
<booktitle>Workshop on Extended Finite State Models of Language at ECAI&apos;96.</booktitle>
<location>Budapest,</location>
<marker>Grefenstette, 1996</marker>
<rawString>Grefenstette G. &amp;quot;Light parsing as finite-state filtering&amp;quot;. Workshop on Extended Finite State Models of Language at ECAI&apos;96. Budapest, Hungary. 1996.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kupiec</author>
</authors>
<title>An algorithm for finding noun phrase correspondences in bilingual corpora&amp;quot;.</title>
<date>1993</date>
<pages>93--17</pages>
<marker>Kupiec, 1993</marker>
<rawString>Kupiec J. &amp;quot; An algorithm for finding noun phrase correspondences in bilingual corpora&amp;quot;. ACL&apos;93. Pp17-22. 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Buliding a large annotated corpus of English: The Penn Treebank&amp;quot;. Computational Linguistics.</title>
<date>1993</date>
<contexts>
<context position="3828" citStr="Marcus et al. 1993" startWordPosition="550" endWordPosition="553"> baseline system which only includes the current part-of-speech in the lexicon is given. In section 3, several extended systems with different context-dependent lexicons are described. In section 4, an error-driven learning method is used to decrease memory requirement of the lexicon by keeping only positive lexical 71 entries and make it possible to further improve the accuracy by merging different contextdependent lexicons into one after automatic analysis of the chunking errors. Finally, the conclusion is given. The data used for all our experiments is extracted from the PENN WSJ Treebank (Marcus et al. 1993) by the program provided by Sabine Buchholz from Tilbug University. We use sections 00-19 as the training data and 20-24 as test data. Therefore, the performance is on large scale task instead of small scale task on CoNLL-2000 with the same evaluation program. For evaluation of our results, we use the precision and recall measures. Precision is the percentage of predicted chunks that are actually correct while the recall is the percentage of correct chunks that are actually found. For convenient comparisons of only one value, we also list the Ft1=1 value(Rijsbergen 1979) : (0 2 + 1) • precisio</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>Marcus M., Santorini B. and Marcinkiewicz M.A. &amp;quot;Buliding a large annotated corpus of English: The Penn Treebank&amp;quot;. Computational Linguistics. 19(2):313-330. 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Rabiner</author>
</authors>
<title>A tutorial on Hidden Markov Models and selected applications in speech recognition&amp;quot;.</title>
<date>1989</date>
<journal>IEEE</journal>
<volume>77</volume>
<issue>2</issue>
<pages>257--285</pages>
<contexts>
<context position="6232" citStr="Rabiner 1989" startWordPosition="980" endWordPosition="981">) n P(t.,q) = log P(Tin ) + E log P(t1) • P(G) = log P(7-in ) — log P(ti)+ log P(t i=i The first item of above equation can be solved by using chain rules. Normally, each tag is assumed to be probabilistic dependent on the N-1 previous tags. Here, backoff bigram(N=2) model is used. The second item is the summation of log probabilities of all the tags. Both the first item and second item correspond to the language model component of the tagger while the third item corresponds to the lexicon component of the tagger. Ideally the third item can be estimated by using the forward-backward algorithm(Rabiner 1989) recursively for the first-order(Rabiner 1989) or second-order HMMs(Watson and Chunk 1992). However, several approximations on it will be attempted later in this paper instead. The stochastic optimal tag sequence can be found by maxmizing the above equation over all the possible tag sequences. This is implemented by the Viterbi algorithm. The main difference between our tagger and other standard taggers lies in our tagger has a context-dependent lexicon while others use a context-independent lexicon. For chunk tagger, we have g1 = piwi where Win = w1v2 wn is the word - Sequence and pin = P &apos; p</context>
</contexts>
<marker>Rabiner, 1989</marker>
<rawString>Rabiner L. &amp;quot;A tutorial on Hidden Markov Models and selected applications in speech recognition&amp;quot;. IEEE 77(2), pp.257-285. 1989. Ramshaw L.A. and Marcus M.P.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Transformation-based Learning</author>
</authors>
<date>1995</date>
<booktitle>Proceeding of 3th ACL Workshop on Very Large Corpora at ACL&apos;95.</booktitle>
<marker>Learning, 1995</marker>
<rawString>&amp;quot;Transformation-based Learning&amp;quot;. Proceeding of 3th ACL Workshop on Very Large Corpora at ACL&apos;95. 1995.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rijsbergen C J van</author>
</authors>
<title>Information Retrieval.</title>
<date>1979</date>
<publisher>Buttersworth,</publisher>
<location>London.</location>
<marker>van, 1979</marker>
<rawString>Rijsbergen C.J.van. Information Retrieval. Buttersworth, London. 1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Skut</author>
<author>T Brants</author>
</authors>
<title>Chunk tagger: statistical recognition of noun phrases.&amp;quot;</title>
<date>1998</date>
<booktitle>ESSLLI-1998 Workshop on Automated Acquisition of Syntax and Parsing. Saarbruucken,</booktitle>
<marker>Skut, Brants, 1998</marker>
<rawString>Skut W. and Brants T. &amp;quot;Chunk tagger: statistical recognition of noun phrases.&amp;quot; ESSLLI-1998 Workshop on Automated Acquisition of Syntax and Parsing. Saarbruucken, Germany. 1998.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Veenstra</author>
</authors>
<title>Memory-based text chunking&amp;quot;. Workshop on machine learning in human language technology at ACAI&apos;99.</title>
<date>1999</date>
<marker>Veenstra, 1999</marker>
<rawString>Veenstra J. &amp;quot;Memory-based text chunking&amp;quot;. Workshop on machine learning in human language technology at ACAI&apos;99. 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Voutilainen</author>
</authors>
<title>Nptool: a detector of English phrases&amp;quot;.</title>
<date>1993</date>
<booktitle>Proceeding of the Workshop on Very Large Corpora. Pp48-57. ACL&apos; 93.</booktitle>
<marker>Voutilainen, 1993</marker>
<rawString>Voutilainen A. &amp;quot;Nptool: a detector of English phrases&amp;quot;. Proceeding of the Workshop on Very Large Corpora. Pp48-57. ACL&apos; 93. 1993</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Watson</author>
<author>Chunk Tsoi A</author>
</authors>
<title>Second order Hidden Markov Models for speech recognition&amp;quot;.</title>
<date>1992</date>
<booktitle>Proceeding of 4th Australian International Conference on Speech Science and Technology. Pp.146-151.</booktitle>
<marker>Watson, A, 1992</marker>
<rawString>Watson B. and Chunk Tsoi A. &amp;quot;Second order Hidden Markov Models for speech recognition&amp;quot;. Proceeding of 4th Australian International Conference on Speech Science and Technology. Pp.146-151. 1992.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>