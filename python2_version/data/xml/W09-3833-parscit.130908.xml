<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003185">
<title confidence="0.986556">
Smoothing fine-grained PCFG lexicons
</title>
<author confidence="0.973121">
Tejaswini Deoskar Mats Rooth Khalil Sima’an
</author>
<affiliation confidence="0.9767055">
ILLC Dept. of Linguistics and CIS ILLC
University of Amsterdam Cornell University University of Amsterdam
</affiliation>
<email confidence="0.993712">
t.deoskar@uva.nl mr249@cornell.edu k.simaan@uva.nl
</email>
<sectionHeader confidence="0.993735" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926153846154">
We present an approach for smoothing
treebank-PCFG lexicons by interpolating
treebank lexical parameter estimates with
estimates obtained from unannotated data
via the Inside-outside algorithm. The
PCFG has complex lexical categories,
making relative-frequency estimates from
a treebank very sparse. This kind of
smoothing for complex lexical categories
results in improved parsing performance,
with a particular advantage in identify-
ing obligatory arguments subcategorized
by verbs unseen in the treebank.
</bodyText>
<sectionHeader confidence="0.998799" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999978826086957">
Lexical scarcity is a problem faced by all sta-
tistical NLP applications that depend on anno-
tated training data, including parsing. One way
of alleviating this problem is to supplement super-
vised models with lexical information from unla-
beled data. In this paper, we present an approach
for smoothing the lexicon of a treebank PCFG
with frequencies estimated from unannotated data
with Inside-outside estimation (Lari and Young,
1990). The PCFG is an unlexicalised PCFG, but
contains complex lexical categories (akin to su-
pertags in LTAG (Bangalore and Joshi, 1999) or
CCG (Clark and Curran, 2004)) encoding struc-
tural preferences of words, like subcategorization.
The idea behind unlexicalised parsing is that the
syntax and lexicon of a language are largely inde-
pendent, being mediated by “selectional” proper-
ties of open-class words. This is the intuition be-
hind lexicalised formalisms like CCG: here lexical
categories are fine-grained and syntactic in nature.
Once a word is assigned a lexical category, the
word itself is not taken into consideration further
in the syntactic analysis. Fine-grained categories
imply that lexicons estimated from treebanks will
be extremely sparse, even for a language like En-
glish with a large treebank resource like the Penn
Treebank (PTB) (Marcus et al., 1993). Smoothing
a treebank lexicon with an external wide-coverage
lexicon is problematic due to their respective rep-
resentations being incompatible and without an
obvious mapping, assuming that the external lexi-
con is probabilistic to begin with. In this paper, we
start with a treebank PCFG with fine-grained lex-
ical categories and re-estimate its parameters on a
large corpus of unlabeled data. We then use re-
estimates of lexical parameters (i.e. pre-terminal
to terminal rule probabilities) to smooth the orig-
inal treebank lexical parameters by interpolation
between the two. Since the treebank PCFG itself is
used to propose analyses of new data, the mapping
problem is inherently taken care of. The smooth-
ing procedure takes into account the fact that unsu-
pervised estimation has benefits for unseen or low-
frequency lexical items, but the treebank relative-
frequency estimates are more reliable in the case
of high-frequency items.
</bodyText>
<sectionHeader confidence="0.983288" genericHeader="introduction">
2 Treebank PCFG
</sectionHeader>
<bodyText confidence="0.999919529411765">
In order to have fine-grained and linguistic lexi-
cal categories (like CCG) within a simple formal-
ism with well-understood estimation methods, we
first build a PCFG containing such categories from
the PTB. The PCFG is unlexicalised (with lim-
ited lexicalization of certain function words, like
in Klein and Manning (2003)). It is created by
first transforming the PTB (Johnson, 1998) in an
appropriate way and then extracting a PCFG from
the transformed trees (Deoskar and Rooth, 2008).
All functional tags in the PTB (such as NP-SBJ,
PP-TMP, etc.) are maintained, as are all empty
categories, making long-distance dependencies re-
coverable. The PCFG is trained on the standard
training sections of the PTB and performs at the
state-of-the-art level for unlexicalised PCFGs, giv-
ing 86.6% f-score on Sec. 23.
</bodyText>
<page confidence="0.984733">
214
</page>
<note confidence="0.368895">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 214–217,
Paris, October 2009. c�2009 Association for Computational Linguistics
</note>
<figure confidence="0.992416818181818">
the consumer
is right
(c) An SBAR frame: +C+ is the
empty complementizer.
VP
+C+ S
Vb.sb
think
SBAR
VP
(a) An NP PP subcategorization frame marked on the
</figure>
<figureCaption confidence="0.4397025">
verb “add” as np. Note that the arguments NP and PP-
CLR are part of the subcategorization frame and are
represented locally on the verb but the adjunct PP-
TMP is not.
</figureCaption>
<figure confidence="0.996958739130435">
VP
(b) An S frame on the verb “seeking”: +E-
NP+ represents the empty subject of the
S. Note that structure internal to S is also
marked on the verb.
NP
four more
Boeings
PP-TMP
by 1994
PP-CLR
to the
two units.
VB.np
add
S.e.to
+E-NP+ VP.to
TO
VBG.s.e.to
seeking
VP
to
avoid..
</figure>
<figureCaption confidence="0.999986">
Figure 1: Subcategorized structures are marked as features on the verbal POS category.
</figureCaption>
<bodyText confidence="0.999957772727273">
An important feature of our PCFG is that pre-
terminal categories for open-class items like verbs,
nouns and adverbs are more complex than PTB
POS tags. They encode information about the
structure selected by the lexical item, in effect,
its subcategorization frame. A pre-terminal in our
PCFG consists of the standard PTB POS tag, fol-
lowed by a sequence of features incorporated into
it. Thus, each PTB POS tag can be considered to
be divided into multiple finer-grained “supertags”
by the incorporated features. These features en-
code the structure selected by the words. We fo-
cus on verbs in this paper, as they are important
structural determiners. A sequence of one or more
features forms the “subcategorization frame” of a
verb: three examples are shown in Figure 1. The
features are determined by a fully automated pro-
cess based on PTB tree structure and node labels.
There are 81 distinct subcategorization frames for
verbal categories. The process can be repeated for
other languages with a treebank annotated in the
PTB style which marks arguments like the PTB.
</bodyText>
<sectionHeader confidence="0.996564" genericHeader="method">
3 Unsupervised Re-estimation
</sectionHeader>
<bodyText confidence="0.999973459459459">
Inside-outside (henceforth I-O) (Lari and Young,
1990), an instance of EM, is an iterative estima-
tion method for PCFGs that, given an initial model
and a corpus of unannotated data, produces mod-
els that assign increasingly higher likelihood to
the corpus at each iteration. I-O often leads to
sub-optimal grammars, being subject to the well-
known problem of local maxima, and dependence
on initial conditions (de Marcken, 1995) (although
there have been positive results using I-O as well,
for e.g. Beil et al. (1999)). More recently, Deoskar
(2008) re-estimated an unlexicalised PTB PCFG
using unlabeled Wall Street Journal data. They
compared models for which all PCFG parameters
were re-estimated from raw data to models for
which only lexical parameters were re-estimated,
and found that the latter had better parsing results.
While it is common to constrain EM either by
good initial conditions or by heuristic constraints,
their approach used syntactic parameters from a
treebank model to constrain re-estimation of lex-
ical parameters. Syntactic parameters are rela-
tively well-estimated from a treebank, not being as
sparse as lexical parameters. At each iteration, the
re-estimated lexicon was interpolated with a tree-
bank lexicon, ensuring that re-estimated lexicons
did not drift away from the treebank lexicon.
We follow their methodology of constrained
EM re-estimation. Using the PCFG with fine
lexical categories (as described in §2) as the ini-
tial model, we re-estimate its parameters from an
unannotated corpus. The lexical parameters of
the re-estimated PCFG form its probabilistic “lex-
icon”, containing the same fine-grained categories
as the original treebank PCFG. We use this re-
estimated “lexicon” to smooth the lexical proba-
bilities in the treebank PCFG.
</bodyText>
<sectionHeader confidence="0.5488955" genericHeader="method">
4 Smoothing based on a POS tagger : the
initial model.
</sectionHeader>
<bodyText confidence="0.9996977">
In order to use the treebank PCFG as an initial
model for unsupervised estimation, new words
from the unannotated training corpus must be in-
cluded in it – if not, parameter values for new
words will never be induced. Since the treebank
model contains no information regarding correct
feature sequences for unseen words, we assign all
possible sequences that have occurred in the tree-
bank model with the POS tag of the word. We
assign all possible sequences to seen words as
</bodyText>
<page confidence="0.995">
215
</page>
<bodyText confidence="0.999654363636364">
well – although the word is seen, the correct fea-
ture sequence for a structure in a training sentence
might still be unseen with that word. This is done
as follows: a standard POS-tagger (TreeTagger,
(Schmid, 1994)) is used to tag the unlabeled cor-
pus. A frequency table cpos(w, τ) consisting of
words and POS-tags is extracted from the result-
ing corpus, where w is the word and τ its POS
tag. The frequency cpos(w, τ) is split amongst all
possible feature sequences ι for that POS tag in
proportion to treebank marginals t(τ, ι) and t(τ)
</bodyText>
<equation confidence="0.9736815">
t(τ, ι)
cpos(w,τ,ι) = t(τ) cpos(w,τ) (1)
</equation>
<bodyText confidence="0.984500076923077">
Then the treebank frequency t(w, τ, ι) and the
scaled corpus frequency are interpolated to get a
smoothed model tpos. We use λ=0.001, giving a
small weight initially to the unlabeled corpus.
tpos(w, τ, ι) = (1 − λ)t(w, τ, ι) + λcpos(w, τ, ι)
(2)
The first term will be zero for words unseen in the
treebank: their distribution in the smoothed model
will be the average treebank distribution over all
possible feature sequences for a POS tag. For
seen words, the treebank distribution over feature
sequence is largely maintained, but a small fre-
quency is assigned to unseen sequences.
</bodyText>
<sectionHeader confidence="0.855585" genericHeader="method">
5 Smoothing based on EM re-estimation
</sectionHeader>
<bodyText confidence="0.996504833333333">
After each iteration i of I-O, the expected counts
cemi(w, τ, ι) under the model instance at itera-
tion (i − 1) are obtained. A smoothed treebank
lexicon temi is obtained by linearly interpolating
the smoothed treebank lexicon tpos(w, τ, ι) and a
scaled re-estimated lexicon ¯cemi(w, τ, ι).
</bodyText>
<equation confidence="0.956203">
temi(w, τ, ι) = (1−λ)tpos(w, τ, ι)+λ¯cemi(w, τ, ι)
(3)
</equation>
<bodyText confidence="0.999728">
where 0 &lt; λ &lt; 1. The term ¯cemi(w, τ, ι) is ob-
tained by scaling the frequencies cemi(w, τ, ι) ob-
tained by I-O, ensuring that the treebank lexicon is
not swamped with the large t1lraining corpus1 .
</bodyText>
<equation confidence="0.966621">
cemi (w, τ, ι) ,() , r� t) Cemi (w, τ, ι)
Ew cemi
(4)
</equation>
<bodyText confidence="0.99811">
λ determines the relative weights given to the
treebank and re-estimated model for a word. Since
parameters of high-frequency words are likely
to be more accurate in the treebank model, we
parametrize λ as λf according to the treebank fre-
quency f = t(w,τ).
</bodyText>
<footnote confidence="0.598733">
1Note that in Eq. 4, the ratio of the two terms involving
</footnote>
<bodyText confidence="0.700037">
c,., is the conditional, lexical probability P,.,(w|τ, t).
</bodyText>
<sectionHeader confidence="0.998157" genericHeader="method">
6 Experiments
</sectionHeader>
<bodyText confidence="0.999886181818182">
The treebank PCFG is trained on sections 0-22 of
the PTB, with 5000 sentences held-out for evalu-
ation. We conducted unsupervised estimation us-
ing Bitpar (Schmid, 2004) with unannotated Wall
Street Journal data of 4, 8 and 12 million words,
with sentence length &lt;25 words. The treebank
and re-estimated models are interpolated with λ =
0.5 (in Eq. 3). We also parametrize λ for treebank
frequency of words – optimizing over a develop-
ment set gives us the following values of λf for
different ranges of treebank word frequencies.
</bodyText>
<construct confidence="0.998279">
if t(w, τ) &lt;= 5 , λf = 0.5
if 5 &lt; t(w, τ) &lt;= 15 , λf = 0.25
if 15 &lt; t(w, τ) &lt;= 50 , λf = 0.05
if t(w, τ) &gt; 50 , λf = 0.005
</construct>
<bodyText confidence="0.999283090909091">
Evaluations are on held-out data from the PTB
by stripping all PTB annotation and obtaining
Viterbi parses with the parser Bitpar. In addition
to standard PARSEVAL measures, we also eval-
uate parses by another measure specific to sub-
categorization2: the POS-tag+feature sequence on
verbs in the Viterbi parse is compared against the
corresponding tag+feature sequence on the trans-
formed PTB gold tree, and errors are counted. The
tag-feature sequence correlates to the structure se-
lected by the verb, as exemplified in Fig. 1.
</bodyText>
<sectionHeader confidence="0.999761" genericHeader="evaluation">
7 Results
</sectionHeader>
<bodyText confidence="0.999987666666667">
There is a statistically significant improvement3
in labeled bracketing f-score on Sec. 23 when
the treebank lexicon is smoothed with an EM-re-
estimated lexicon. In Table 1, tt refers to the base-
line treebank model, smoothed using the POS-
tag smoothing method (from §4) on the test data
(Sec. 23) in order to incorporate new words from
the test data4. tpos refers to the initial model for
re-estimation, obtained by smoothed the treebank
model with the POS-tag smoothing method with
the large unannotated corpus (4 million words).
This model understandably does not improve over
tt for parsing Sec. 23. tem1,λ=0.5 is the model
obtained by smoothing with an EM-re-estimated
model with a constant interpolation factor λ =
0.5. This model gives a statistically significant im-
provement in f-score over both tt and tpos. The
last model tem1,λf is obtained by smoothing with
</bodyText>
<footnote confidence="0.866924">
2PARSEVAL measures are known to be insensitive to sub-
categorization (Carroll et al., 1998).
3A randomized version of a paired-sample t-test is used.
4This is always done before parsing test data.
(5)
</footnote>
<page confidence="0.984715">
216
</page>
<table confidence="0.9998075">
tt tpos teml,A=0.5 teml,Af
Recall 86.48 86.48 86.72 87.44
Precision 86.61 86.63 86.95 87.15
f-score 86.55 86.56 *86.83 *87.29
</table>
<tableCaption confidence="0.999899">
Table 1: Labeled bracketing F-score on section 23.
</tableCaption>
<bodyText confidence="0.9995493125">
an interpolation factor as in Eq. 5 : this is the best
model with a statistically significant improvement
in f-score over tt, tpos and tem,,A=0.5.
Since we expect that smoothing will be advanta-
geous for unseen or low-frequency words, we per-
form an evaluation targeted at identifying struc-
tures subcategorized by unseen verbs. Table 2
shows the error reduction in identifying subcat.
frames in Viterbi parses, of unseen verbs and also
of all verbs (seen and unseen) in the testset. A
breakup of error by frame type for unseen verbs is
also shown (here, only frames with &gt;10 token oc-
currences in the test data are shown). In all cases
(unseen verbs and all verbs) we see a substantial
error reduction. The error reduction improves with
larger amounts of unannotated training data.
</bodyText>
<sectionHeader confidence="0.989211" genericHeader="conclusions">
8 Discussion and Conclusions
</sectionHeader>
<bodyText confidence="0.999874769230769">
We have shown that lexicons re-estimated with I-
O can be used to smooth unlexicalised treebank
PCFGs, with a significant increase in f-score even
in the case of English with a large treebank re-
source. We expect this method to have more
impact for languages with a smaller treebank or
richer tag-set. An interesting aspect is the substan-
tial reduction in subcategorization error for un-
seen verbs for which no word-specific information
about subcategorization exists in the unsmoothed
or POS-tag-smoothed lexicon. The error reduction
in identifying subcat. frames implies that some
constituents (such as PPs) are not only attached
correctly but also identified correctly as arguments
(such as PP-CLR) rather than as adjuncts.
There have been previous attempts to use POS-
tagging technologies (such as HMM or maximum-
entropy based taggers) to enhance treebank-
trained grammars (Goldberg et al. (2009) for He-
brew, (Clark and Curran, 2004) for CCG). The re-
estimation method we use builds full parse-trees,
rather than use local features like taggers do, and
hence might have a benefit over such methods. An
interesting option would be to train a “supertag-
ger” on fine-grained tags from the PTB and to su-
pertag a large corpus to harvest lexical frequen-
</bodyText>
<table confidence="0.99947325">
Frame # tokens %Error %Error %Error
(test) tpos tem, Reduc.
All unseen (4M words) 1258 33.47 22.81 31.84
All unseen (8M words) 1258 33.47 22.26 33.49
All unseen (12M words) 1258 33.47 21.86 34.68
transitive 662 23.87 18.73 21.52
intransitive 115 38.26 33.91 11.36
NP PP-CLR 121 34.71 32.23 7.14
PP-CLR 73 27.4 20.55 25
SBAR 124 12.1 12.1 0
S 12 83.33 58.33 30
NP NP 10 90 80 11.11
PRT NP 21 38.1 33.33 12.5
s.e.to (see Fig.1b) 50 16 12 25
NP PP-DIR 11 63.64 54.55 14.28
All verbs (4M) 11710 18.5 16.84 8.97
</table>
<tableCaption confidence="0.999398">
Table 2: Subcat. error for verbs in Viterbi parses.
</tableCaption>
<bodyText confidence="0.999387">
cies. This would form another (possibly higher)
baseline for the I-O re-estimation approach pre-
sented here and is the focus of our future work.
</bodyText>
<sectionHeader confidence="0.999115" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999894972972973">
S. Bangalore and A. K. Joshi. 1999. Supertagging: An Ap-
proach to Almost Parsing. Computational Linguistics,
25:237–265.
F. Beil, G. Carroll, D. Prescher, S. Riezler, and M. Rooth.
1999. Inside-outside estimation of a lexicalized PCFG for
German. In ACL 37.
J. Carroll, G. Minnen, and E. Briscoe. 1998. Can subcate-
gorization probabilities help parsing. In 6th ACL/SIGDAT
Workshop on Very Large Corpora.
S. Clark and J. R. Curran. 2004. The Importance of Supertag-
ging for Wide-Coverage CCG Parsing. In 22nd COLING.
Carl de Marcken. 1995. On the unsupervised induction of
Phrase Structure grammars. In Proceedings of the 3rd
Workshop on Very Large Corpora.
T. Deoskar. 2008. Re-estimation of Lexical Parameters for
Treebank PCFGs. In 22nd COLING.
Tejaswini Deoskar and Mats Rooth. 2008. Induction of
Treebank-Aligned Lexical Resources. In 6th LREC.
Y. Goldberg, R. Tsarfaty, M. Adler, and M. Elhadad. 2009.
Enhancing Unlexicalized Parsing Performance using a
Wide Coverage Lexicon, Fuzzy Tag-set Mapping, and
EM-HMM-based Lexical Probabilities. In EACL-09.
M. Johnson. 1998. PCFG models of linguistic tree represen-
tations. Computational Linguistics, 24(4).
D. Klein and C. Manning. 2003. Accurate unlexicalized pars-
ing. In ACL 41.
K. Lari and S. J. Young. 1990. The estimation of stochas-
tic context-free grammars using the Inside-Outside algo-
rithm. Computer Speech and Language, 4:35–56.
M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993.
Building a Large Annotated Corpus of English: The Penn
Treebank. Computational Linguistics, 19(2):313–330.
H. Schmid. 1994. Probabilistic Part-of-Speech Tagging Us-
ing Decision Trees. In International Conference on New
Methods in Language Processing.
H. Schmid. 2004. Efficient Parsing of Highly Ambiguous
CFGs with Bit Vectors. In 20th COLING.
</reference>
<page confidence="0.998398">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.971575">
<title confidence="0.999691">Smoothing fine-grained PCFG lexicons</title>
<author confidence="0.999287">Tejaswini Deoskar Mats Rooth Khalil Sima’an</author>
<affiliation confidence="0.9990895">ILLC Dept. of Linguistics and CIS ILLC University of Amsterdam Cornell University University of Amsterdam</affiliation>
<email confidence="0.9837">t.deoskar@uva.nlmr249@cornell.eduk.simaan@uva.nl</email>
<abstract confidence="0.999272428571429">We present an approach for smoothing treebank-PCFG lexicons by interpolating treebank lexical parameter estimates with estimates obtained from unannotated data via the Inside-outside algorithm. The PCFG has complex lexical categories, making relative-frequency estimates from a treebank very sparse. This kind of smoothing for complex lexical categories results in improved parsing performance, with a particular advantage in identifying obligatory arguments subcategorized by verbs unseen in the treebank.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>S Bangalore</author>
<author>A K Joshi</author>
</authors>
<title>Supertagging: An Approach to Almost Parsing. Computational Linguistics,</title>
<date>1999</date>
<pages>25--237</pages>
<contexts>
<context position="1333" citStr="Bangalore and Joshi, 1999" startWordPosition="183" endWordPosition="186">ubcategorized by verbs unseen in the treebank. 1 Introduction Lexical scarcity is a problem faced by all statistical NLP applications that depend on annotated training data, including parsing. One way of alleviating this problem is to supplement supervised models with lexical information from unlabeled data. In this paper, we present an approach for smoothing the lexicon of a treebank PCFG with frequencies estimated from unannotated data with Inside-outside estimation (Lari and Young, 1990). The PCFG is an unlexicalised PCFG, but contains complex lexical categories (akin to supertags in LTAG (Bangalore and Joshi, 1999) or CCG (Clark and Curran, 2004)) encoding structural preferences of words, like subcategorization. The idea behind unlexicalised parsing is that the syntax and lexicon of a language are largely independent, being mediated by “selectional” properties of open-class words. This is the intuition behind lexicalised formalisms like CCG: here lexical categories are fine-grained and syntactic in nature. Once a word is assigned a lexical category, the word itself is not taken into consideration further in the syntactic analysis. Fine-grained categories imply that lexicons estimated from treebanks will</context>
</contexts>
<marker>Bangalore, Joshi, 1999</marker>
<rawString>S. Bangalore and A. K. Joshi. 1999. Supertagging: An Approach to Almost Parsing. Computational Linguistics, 25:237–265.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Beil</author>
<author>G Carroll</author>
<author>D Prescher</author>
<author>S Riezler</author>
<author>M Rooth</author>
</authors>
<title>Inside-outside estimation of a lexicalized PCFG for German. In</title>
<date>1999</date>
<journal>ACL</journal>
<volume>37</volume>
<contexts>
<context position="6287" citStr="Beil et al. (1999)" startWordPosition="985" endWordPosition="988"> treebank annotated in the PTB style which marks arguments like the PTB. 3 Unsupervised Re-estimation Inside-outside (henceforth I-O) (Lari and Young, 1990), an instance of EM, is an iterative estimation method for PCFGs that, given an initial model and a corpus of unannotated data, produces models that assign increasingly higher likelihood to the corpus at each iteration. I-O often leads to sub-optimal grammars, being subject to the wellknown problem of local maxima, and dependence on initial conditions (de Marcken, 1995) (although there have been positive results using I-O as well, for e.g. Beil et al. (1999)). More recently, Deoskar (2008) re-estimated an unlexicalised PTB PCFG using unlabeled Wall Street Journal data. They compared models for which all PCFG parameters were re-estimated from raw data to models for which only lexical parameters were re-estimated, and found that the latter had better parsing results. While it is common to constrain EM either by good initial conditions or by heuristic constraints, their approach used syntactic parameters from a treebank model to constrain re-estimation of lexical parameters. Syntactic parameters are relatively well-estimated from a treebank, not bei</context>
</contexts>
<marker>Beil, Carroll, Prescher, Riezler, Rooth, 1999</marker>
<rawString>F. Beil, G. Carroll, D. Prescher, S. Riezler, and M. Rooth. 1999. Inside-outside estimation of a lexicalized PCFG for German. In ACL 37.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>G Minnen</author>
<author>E Briscoe</author>
</authors>
<title>Can subcategorization probabilities help parsing.</title>
<date>1998</date>
<booktitle>In 6th ACL/SIGDAT Workshop on Very Large Corpora.</booktitle>
<contexts>
<context position="12409" citStr="Carroll et al., 1998" startWordPosition="2028" endWordPosition="2031">e test data4. tpos refers to the initial model for re-estimation, obtained by smoothed the treebank model with the POS-tag smoothing method with the large unannotated corpus (4 million words). This model understandably does not improve over tt for parsing Sec. 23. tem1,λ=0.5 is the model obtained by smoothing with an EM-re-estimated model with a constant interpolation factor λ = 0.5. This model gives a statistically significant improvement in f-score over both tt and tpos. The last model tem1,λf is obtained by smoothing with 2PARSEVAL measures are known to be insensitive to subcategorization (Carroll et al., 1998). 3A randomized version of a paired-sample t-test is used. 4This is always done before parsing test data. (5) 216 tt tpos teml,A=0.5 teml,Af Recall 86.48 86.48 86.72 87.44 Precision 86.61 86.63 86.95 87.15 f-score 86.55 86.56 *86.83 *87.29 Table 1: Labeled bracketing F-score on section 23. an interpolation factor as in Eq. 5 : this is the best model with a statistically significant improvement in f-score over tt, tpos and tem,,A=0.5. Since we expect that smoothing will be advantageous for unseen or low-frequency words, we perform an evaluation targeted at identifying structures subcategorized </context>
</contexts>
<marker>Carroll, Minnen, Briscoe, 1998</marker>
<rawString>J. Carroll, G. Minnen, and E. Briscoe. 1998. Can subcategorization probabilities help parsing. In 6th ACL/SIGDAT Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Clark</author>
<author>J R Curran</author>
</authors>
<title>The Importance of Supertagging for Wide-Coverage CCG Parsing.</title>
<date>2004</date>
<booktitle>In 22nd COLING.</booktitle>
<contexts>
<context position="1365" citStr="Clark and Curran, 2004" startWordPosition="189" endWordPosition="192">e treebank. 1 Introduction Lexical scarcity is a problem faced by all statistical NLP applications that depend on annotated training data, including parsing. One way of alleviating this problem is to supplement supervised models with lexical information from unlabeled data. In this paper, we present an approach for smoothing the lexicon of a treebank PCFG with frequencies estimated from unannotated data with Inside-outside estimation (Lari and Young, 1990). The PCFG is an unlexicalised PCFG, but contains complex lexical categories (akin to supertags in LTAG (Bangalore and Joshi, 1999) or CCG (Clark and Curran, 2004)) encoding structural preferences of words, like subcategorization. The idea behind unlexicalised parsing is that the syntax and lexicon of a language are largely independent, being mediated by “selectional” properties of open-class words. This is the intuition behind lexicalised formalisms like CCG: here lexical categories are fine-grained and syntactic in nature. Once a word is assigned a lexical category, the word itself is not taken into consideration further in the syntactic analysis. Fine-grained categories imply that lexicons estimated from treebanks will be extremely sparse, even for a</context>
<context position="14438" citStr="Clark and Curran, 2004" startWordPosition="2357" endWordPosition="2360">esting aspect is the substantial reduction in subcategorization error for unseen verbs for which no word-specific information about subcategorization exists in the unsmoothed or POS-tag-smoothed lexicon. The error reduction in identifying subcat. frames implies that some constituents (such as PPs) are not only attached correctly but also identified correctly as arguments (such as PP-CLR) rather than as adjuncts. There have been previous attempts to use POStagging technologies (such as HMM or maximumentropy based taggers) to enhance treebanktrained grammars (Goldberg et al. (2009) for Hebrew, (Clark and Curran, 2004) for CCG). The reestimation method we use builds full parse-trees, rather than use local features like taggers do, and hence might have a benefit over such methods. An interesting option would be to train a “supertagger” on fine-grained tags from the PTB and to supertag a large corpus to harvest lexical frequenFrame # tokens %Error %Error %Error (test) tpos tem, Reduc. All unseen (4M words) 1258 33.47 22.81 31.84 All unseen (8M words) 1258 33.47 22.26 33.49 All unseen (12M words) 1258 33.47 21.86 34.68 transitive 662 23.87 18.73 21.52 intransitive 115 38.26 33.91 11.36 NP PP-CLR 121 34.71 32.2</context>
</contexts>
<marker>Clark, Curran, 2004</marker>
<rawString>S. Clark and J. R. Curran. 2004. The Importance of Supertagging for Wide-Coverage CCG Parsing. In 22nd COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl de Marcken</author>
</authors>
<title>On the unsupervised induction of Phrase Structure grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of the 3rd Workshop on Very Large Corpora.</booktitle>
<marker>de Marcken, 1995</marker>
<rawString>Carl de Marcken. 1995. On the unsupervised induction of Phrase Structure grammars. In Proceedings of the 3rd Workshop on Very Large Corpora.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Deoskar</author>
</authors>
<title>Re-estimation of Lexical Parameters for Treebank PCFGs.</title>
<date>2008</date>
<booktitle>In 22nd COLING.</booktitle>
<contexts>
<context position="6319" citStr="Deoskar (2008)" startWordPosition="991" endWordPosition="992"> which marks arguments like the PTB. 3 Unsupervised Re-estimation Inside-outside (henceforth I-O) (Lari and Young, 1990), an instance of EM, is an iterative estimation method for PCFGs that, given an initial model and a corpus of unannotated data, produces models that assign increasingly higher likelihood to the corpus at each iteration. I-O often leads to sub-optimal grammars, being subject to the wellknown problem of local maxima, and dependence on initial conditions (de Marcken, 1995) (although there have been positive results using I-O as well, for e.g. Beil et al. (1999)). More recently, Deoskar (2008) re-estimated an unlexicalised PTB PCFG using unlabeled Wall Street Journal data. They compared models for which all PCFG parameters were re-estimated from raw data to models for which only lexical parameters were re-estimated, and found that the latter had better parsing results. While it is common to constrain EM either by good initial conditions or by heuristic constraints, their approach used syntactic parameters from a treebank model to constrain re-estimation of lexical parameters. Syntactic parameters are relatively well-estimated from a treebank, not being as sparse as lexical paramete</context>
</contexts>
<marker>Deoskar, 2008</marker>
<rawString>T. Deoskar. 2008. Re-estimation of Lexical Parameters for Treebank PCFGs. In 22nd COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tejaswini Deoskar</author>
<author>Mats Rooth</author>
</authors>
<title>Induction of Treebank-Aligned Lexical Resources.</title>
<date>2008</date>
<booktitle>In 6th LREC.</booktitle>
<contexts>
<context position="3500" citStr="Deoskar and Rooth, 2008" startWordPosition="522" endWordPosition="525">frequency lexical items, but the treebank relativefrequency estimates are more reliable in the case of high-frequency items. 2 Treebank PCFG In order to have fine-grained and linguistic lexical categories (like CCG) within a simple formalism with well-understood estimation methods, we first build a PCFG containing such categories from the PTB. The PCFG is unlexicalised (with limited lexicalization of certain function words, like in Klein and Manning (2003)). It is created by first transforming the PTB (Johnson, 1998) in an appropriate way and then extracting a PCFG from the transformed trees (Deoskar and Rooth, 2008). All functional tags in the PTB (such as NP-SBJ, PP-TMP, etc.) are maintained, as are all empty categories, making long-distance dependencies recoverable. The PCFG is trained on the standard training sections of the PTB and performs at the state-of-the-art level for unlexicalised PCFGs, giving 86.6% f-score on Sec. 23. 214 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 214–217, Paris, October 2009. c�2009 Association for Computational Linguistics the consumer is right (c) An SBAR frame: +C+ is the empty complementizer. VP +C+ S Vb.sb think SBAR VP (a) A</context>
</contexts>
<marker>Deoskar, Rooth, 2008</marker>
<rawString>Tejaswini Deoskar and Mats Rooth. 2008. Induction of Treebank-Aligned Lexical Resources. In 6th LREC.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Y Goldberg</author>
<author>R Tsarfaty</author>
<author>M Adler</author>
<author>M Elhadad</author>
</authors>
<title>Enhancing Unlexicalized Parsing Performance using a Wide Coverage Lexicon, Fuzzy Tag-set Mapping, and EM-HMM-based Lexical Probabilities.</title>
<date>2009</date>
<booktitle>In EACL-09.</booktitle>
<contexts>
<context position="14401" citStr="Goldberg et al. (2009)" startWordPosition="2350" endWordPosition="2353">treebank or richer tag-set. An interesting aspect is the substantial reduction in subcategorization error for unseen verbs for which no word-specific information about subcategorization exists in the unsmoothed or POS-tag-smoothed lexicon. The error reduction in identifying subcat. frames implies that some constituents (such as PPs) are not only attached correctly but also identified correctly as arguments (such as PP-CLR) rather than as adjuncts. There have been previous attempts to use POStagging technologies (such as HMM or maximumentropy based taggers) to enhance treebanktrained grammars (Goldberg et al. (2009) for Hebrew, (Clark and Curran, 2004) for CCG). The reestimation method we use builds full parse-trees, rather than use local features like taggers do, and hence might have a benefit over such methods. An interesting option would be to train a “supertagger” on fine-grained tags from the PTB and to supertag a large corpus to harvest lexical frequenFrame # tokens %Error %Error %Error (test) tpos tem, Reduc. All unseen (4M words) 1258 33.47 22.81 31.84 All unseen (8M words) 1258 33.47 22.26 33.49 All unseen (12M words) 1258 33.47 21.86 34.68 transitive 662 23.87 18.73 21.52 intransitive 115 38.26</context>
</contexts>
<marker>Goldberg, Tsarfaty, Adler, Elhadad, 2009</marker>
<rawString>Y. Goldberg, R. Tsarfaty, M. Adler, and M. Elhadad. 2009. Enhancing Unlexicalized Parsing Performance using a Wide Coverage Lexicon, Fuzzy Tag-set Mapping, and EM-HMM-based Lexical Probabilities. In EACL-09.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Johnson</author>
</authors>
<title>PCFG models of linguistic tree representations.</title>
<date>1998</date>
<journal>Computational Linguistics,</journal>
<volume>24</volume>
<issue>4</issue>
<contexts>
<context position="3398" citStr="Johnson, 1998" startWordPosition="507" endWordPosition="508">dure takes into account the fact that unsupervised estimation has benefits for unseen or lowfrequency lexical items, but the treebank relativefrequency estimates are more reliable in the case of high-frequency items. 2 Treebank PCFG In order to have fine-grained and linguistic lexical categories (like CCG) within a simple formalism with well-understood estimation methods, we first build a PCFG containing such categories from the PTB. The PCFG is unlexicalised (with limited lexicalization of certain function words, like in Klein and Manning (2003)). It is created by first transforming the PTB (Johnson, 1998) in an appropriate way and then extracting a PCFG from the transformed trees (Deoskar and Rooth, 2008). All functional tags in the PTB (such as NP-SBJ, PP-TMP, etc.) are maintained, as are all empty categories, making long-distance dependencies recoverable. The PCFG is trained on the standard training sections of the PTB and performs at the state-of-the-art level for unlexicalised PCFGs, giving 86.6% f-score on Sec. 23. 214 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 214–217, Paris, October 2009. c�2009 Association for Computational Linguistics the co</context>
</contexts>
<marker>Johnson, 1998</marker>
<rawString>M. Johnson. 1998. PCFG models of linguistic tree representations. Computational Linguistics, 24(4).</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Klein</author>
<author>C Manning</author>
</authors>
<title>Accurate unlexicalized parsing.</title>
<date>2003</date>
<booktitle>In ACL 41.</booktitle>
<contexts>
<context position="3336" citStr="Klein and Manning (2003)" startWordPosition="495" endWordPosition="498">ta, the mapping problem is inherently taken care of. The smoothing procedure takes into account the fact that unsupervised estimation has benefits for unseen or lowfrequency lexical items, but the treebank relativefrequency estimates are more reliable in the case of high-frequency items. 2 Treebank PCFG In order to have fine-grained and linguistic lexical categories (like CCG) within a simple formalism with well-understood estimation methods, we first build a PCFG containing such categories from the PTB. The PCFG is unlexicalised (with limited lexicalization of certain function words, like in Klein and Manning (2003)). It is created by first transforming the PTB (Johnson, 1998) in an appropriate way and then extracting a PCFG from the transformed trees (Deoskar and Rooth, 2008). All functional tags in the PTB (such as NP-SBJ, PP-TMP, etc.) are maintained, as are all empty categories, making long-distance dependencies recoverable. The PCFG is trained on the standard training sections of the PTB and performs at the state-of-the-art level for unlexicalised PCFGs, giving 86.6% f-score on Sec. 23. 214 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 214–217, Paris, October</context>
</contexts>
<marker>Klein, Manning, 2003</marker>
<rawString>D. Klein and C. Manning. 2003. Accurate unlexicalized parsing. In ACL 41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Lari</author>
<author>S J Young</author>
</authors>
<title>The estimation of stochastic context-free grammars using the Inside-Outside algorithm. Computer Speech and Language,</title>
<date>1990</date>
<pages>4--35</pages>
<contexts>
<context position="1202" citStr="Lari and Young, 1990" startWordPosition="162" endWordPosition="165"> lexical categories results in improved parsing performance, with a particular advantage in identifying obligatory arguments subcategorized by verbs unseen in the treebank. 1 Introduction Lexical scarcity is a problem faced by all statistical NLP applications that depend on annotated training data, including parsing. One way of alleviating this problem is to supplement supervised models with lexical information from unlabeled data. In this paper, we present an approach for smoothing the lexicon of a treebank PCFG with frequencies estimated from unannotated data with Inside-outside estimation (Lari and Young, 1990). The PCFG is an unlexicalised PCFG, but contains complex lexical categories (akin to supertags in LTAG (Bangalore and Joshi, 1999) or CCG (Clark and Curran, 2004)) encoding structural preferences of words, like subcategorization. The idea behind unlexicalised parsing is that the syntax and lexicon of a language are largely independent, being mediated by “selectional” properties of open-class words. This is the intuition behind lexicalised formalisms like CCG: here lexical categories are fine-grained and syntactic in nature. Once a word is assigned a lexical category, the word itself is not ta</context>
<context position="5825" citStr="Lari and Young, 1990" startWordPosition="908" endWordPosition="911">de the structure selected by the words. We focus on verbs in this paper, as they are important structural determiners. A sequence of one or more features forms the “subcategorization frame” of a verb: three examples are shown in Figure 1. The features are determined by a fully automated process based on PTB tree structure and node labels. There are 81 distinct subcategorization frames for verbal categories. The process can be repeated for other languages with a treebank annotated in the PTB style which marks arguments like the PTB. 3 Unsupervised Re-estimation Inside-outside (henceforth I-O) (Lari and Young, 1990), an instance of EM, is an iterative estimation method for PCFGs that, given an initial model and a corpus of unannotated data, produces models that assign increasingly higher likelihood to the corpus at each iteration. I-O often leads to sub-optimal grammars, being subject to the wellknown problem of local maxima, and dependence on initial conditions (de Marcken, 1995) (although there have been positive results using I-O as well, for e.g. Beil et al. (1999)). More recently, Deoskar (2008) re-estimated an unlexicalised PTB PCFG using unlabeled Wall Street Journal data. They compared models for</context>
</contexts>
<marker>Lari, Young, 1990</marker>
<rawString>K. Lari and S. J. Young. 1990. The estimation of stochastic context-free grammars using the Inside-Outside algorithm. Computer Speech and Language, 4:35–56.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M P Marcus</author>
<author>B Santorini</author>
<author>M A Marcinkiewicz</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="2069" citStr="Marcus et al., 1993" startWordPosition="297" endWordPosition="300">unlexicalised parsing is that the syntax and lexicon of a language are largely independent, being mediated by “selectional” properties of open-class words. This is the intuition behind lexicalised formalisms like CCG: here lexical categories are fine-grained and syntactic in nature. Once a word is assigned a lexical category, the word itself is not taken into consideration further in the syntactic analysis. Fine-grained categories imply that lexicons estimated from treebanks will be extremely sparse, even for a language like English with a large treebank resource like the Penn Treebank (PTB) (Marcus et al., 1993). Smoothing a treebank lexicon with an external wide-coverage lexicon is problematic due to their respective representations being incompatible and without an obvious mapping, assuming that the external lexicon is probabilistic to begin with. In this paper, we start with a treebank PCFG with fine-grained lexical categories and re-estimate its parameters on a large corpus of unlabeled data. We then use reestimates of lexical parameters (i.e. pre-terminal to terminal rule probabilities) to smooth the original treebank lexical parameters by interpolation between the two. Since the treebank PCFG i</context>
</contexts>
<marker>Marcus, Santorini, Marcinkiewicz, 1993</marker>
<rawString>M. P. Marcus, B. Santorini, and M. A. Marcinkiewicz. 1993. Building a Large Annotated Corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees.</title>
<date>1994</date>
<booktitle>In International Conference on New Methods in Language Processing.</booktitle>
<contexts>
<context position="8293" citStr="Schmid, 1994" startWordPosition="1307" endWordPosition="1308">on, new words from the unannotated training corpus must be included in it – if not, parameter values for new words will never be induced. Since the treebank model contains no information regarding correct feature sequences for unseen words, we assign all possible sequences that have occurred in the treebank model with the POS tag of the word. We assign all possible sequences to seen words as 215 well – although the word is seen, the correct feature sequence for a structure in a training sentence might still be unseen with that word. This is done as follows: a standard POS-tagger (TreeTagger, (Schmid, 1994)) is used to tag the unlabeled corpus. A frequency table cpos(w, τ) consisting of words and POS-tags is extracted from the resulting corpus, where w is the word and τ its POS tag. The frequency cpos(w, τ) is split amongst all possible feature sequences ι for that POS tag in proportion to treebank marginals t(τ, ι) and t(τ) t(τ, ι) cpos(w,τ,ι) = t(τ) cpos(w,τ) (1) Then the treebank frequency t(w, τ, ι) and the scaled corpus frequency are interpolated to get a smoothed model tpos. We use λ=0.001, giving a small weight initially to the unlabeled corpus. tpos(w, τ, ι) = (1 − λ)t(w, τ, ι) + λcpos(w</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid. 1994. Probabilistic Part-of-Speech Tagging Using Decision Trees. In International Conference on New Methods in Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Efficient Parsing of Highly Ambiguous CFGs with Bit Vectors.</title>
<date>2004</date>
<booktitle>In 20th COLING.</booktitle>
<contexts>
<context position="10432" citStr="Schmid, 2004" startWordPosition="1687" endWordPosition="1688">1 . cemi (w, τ, ι) ,() , r� t) Cemi (w, τ, ι) Ew cemi (4) λ determines the relative weights given to the treebank and re-estimated model for a word. Since parameters of high-frequency words are likely to be more accurate in the treebank model, we parametrize λ as λf according to the treebank frequency f = t(w,τ). 1Note that in Eq. 4, the ratio of the two terms involving c,., is the conditional, lexical probability P,.,(w|τ, t). 6 Experiments The treebank PCFG is trained on sections 0-22 of the PTB, with 5000 sentences held-out for evaluation. We conducted unsupervised estimation using Bitpar (Schmid, 2004) with unannotated Wall Street Journal data of 4, 8 and 12 million words, with sentence length &lt;25 words. The treebank and re-estimated models are interpolated with λ = 0.5 (in Eq. 3). We also parametrize λ for treebank frequency of words – optimizing over a development set gives us the following values of λf for different ranges of treebank word frequencies. if t(w, τ) &lt;= 5 , λf = 0.5 if 5 &lt; t(w, τ) &lt;= 15 , λf = 0.25 if 15 &lt; t(w, τ) &lt;= 50 , λf = 0.05 if t(w, τ) &gt; 50 , λf = 0.005 Evaluations are on held-out data from the PTB by stripping all PTB annotation and obtaining Viterbi parses with the </context>
</contexts>
<marker>Schmid, 2004</marker>
<rawString>H. Schmid. 2004. Efficient Parsing of Highly Ambiguous CFGs with Bit Vectors. In 20th COLING.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>