<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.011511">
<title confidence="0.98962">
Unsupervised joke generation from big data
</title>
<author confidence="0.996099">
Saˇsa Petrovi´c
</author>
<affiliation confidence="0.9979225">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.986062">
sasa.petrovic@ed.ac.uk
</email>
<author confidence="0.995419">
David Matthews
</author>
<affiliation confidence="0.9979945">
School of Informatics
University of Edinburgh
</affiliation>
<email confidence="0.988672">
dave.matthews@ed.ac.uk
</email>
<sectionHeader confidence="0.993669" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999887842105263">
Humor generation is a very hard problem.
It is difficult to say exactly what makes a
joke funny, and solving this problem al-
gorithmically is assumed to require deep
semantic understanding, as well as cul-
tural and other contextual cues. We depart
from previous work that tries to model this
knowledge using ad-hoc manually created
databases and labeled training examples.
Instead we present a model that uses large
amounts of unannotated data to generate I
like my X like I like my Y, Z jokes, where
X, Y, and Z are variables to be filled in.
This is, to the best of our knowledge, the
first fully unsupervised humor generation
system. Our model significantly outper-
forms a competitive baseline and gener-
ates funny jokes 16% of the time, com-
pared to 33% for human-generated jokes.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999956181818182">
Generating jokes is typically considered to be a
very hard natural language problem, as it implies
a deep semantic and often cultural understanding
of text. We deal with generating a particular type
of joke – I like my X like I like my Y, Z – where X
and Y are nouns and Z is typically an attribute that
describes X and Y. An example of such a joke is
I like my men like I like my tea, hot and British –
these jokes are very popular online.
While this particular type of joke is not interest-
ing from a purely generational point of view (the
syntactic structure is fixed), the content selection
problem is very challenging. Indeed, most of the
X, Y, and Z triples, when used in the context of
this joke, will not be considered funny. Thus, the
main challenge in this work is to “fill in” the slots
in the joke template in a way that the whole phrase
is considered funny.
Unlike the previous work in humor generation,
we do not rely on labeled training data or hand-
coded rules, but instead on large quantities of
unannotated data. We present a machine learning
model that expresses our assumptions about what
makes these types of jokes funny and show that by
using this fairly simple model and large quantities
of data, we are able to generate jokes that are con-
sidered funny by human raters in 16% of cases.
The main contribution of this paper is, to the
best of our knowledge, the first fully unsupervised
joke generation system. We rely only on large
quantities of unlabeled data, suggesting that gener-
ating jokes does not always require deep semantic
understanding, as usually thought.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999504458333333">
Related work on computational humor can be di-
vided into two classes: humor recognition and hu-
mor generation. Humor recognition includes dou-
ble entendre identification in the form of That’s
what she said jokes (Kiddon and Brun, 2011),
sarcastic sentence identification (Davidov et al.,
2010), and one-liner joke recognition (Mihalcea
and Strapparava, 2005). All this previous work
uses labeled training data. Kiddon and Brun
(2011) use a supervised classifier (SVM) trained
on 4,000 labeled examples, while Davidov et al.
(2010) and Mihalcea and Strapparava (2005) both
use a small amount of training data followed by a
bootstrapping step to gather more.
Examples of work on humor generation include
dirty joke telling robots (Sj¨obergh and Araki,
2008), a generative model of two-liner jokes (Lab-
utov and Lipson, 2012), and a model of punning
riddles (Binsted and Ritchie, 1994). Again, all this
work uses supervision in some form: Sj¨obergh and
Araki (2008) use only human jokes collected from
various sources, Labutov and Lipson (2012) use a
supervised approach to learn feasible circuits that
connect two concepts in a semantic network, and
</bodyText>
<page confidence="0.965203">
228
</page>
<note confidence="0.6261185">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228–232,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figureCaption confidence="0.992266">
Figure 1: Our model presented as a factor graph.
</figureCaption>
<bodyText confidence="0.9574885">
Binsted and Ritchie (1994) have a set of six hard-
coded rules for generating puns.
</bodyText>
<sectionHeader confidence="0.990735" genericHeader="method">
3 Generating jokes
</sectionHeader>
<bodyText confidence="0.999288666666667">
We generate jokes of the form I like my X like I like
my Y, Z, and we assume that X and Y are nouns,
and that Z is an adjective.
</bodyText>
<subsectionHeader confidence="0.998933">
3.1 Model
</subsectionHeader>
<bodyText confidence="0.972065071428571">
Our model encodes four main assumptions about
I like my jokes: i) a joke is funnier the more often
the attribute is used to describe both nouns, ii) a
joke is funnier the less common the attribute is, iii)
a joke is funnier the more ambiguous the attribute
is, and iv) a joke is funnier the more dissimilar
the two nouns are. A graphical representation of
our model in the form of a factor graph is shown
in Figure 1. Variables, denoted by circles, and fac-
tors, denoted by squares, define potential functions
involving the variables they are connected to.
Assumption i) is the most straightforward, and
is expressed through φ(X, Z) and φ(Y, Z) factors.
Mathematically, this assumption is expressed as:
</bodyText>
<equation confidence="0.977717">
φ(x, z) = p(x, z) = f(x, z) (1)
Ex,z f (x, z)
</equation>
<bodyText confidence="0.999884142857143">
where f(x, z)1 is a function that measures the co-
occurrence between x and z. In this work we sim-
ply use frequency of co-occurrence of x and z in
some large corpus, but other functions, e.g., TF-
IDF weighted frequency, could also be used. The
same formula is used for φ(Y, Z), only with dif-
ferent variables. Because this factor measures the
</bodyText>
<footnote confidence="0.5030895">
1We use uppercase to denote random variables, and low-
ercase to denote random variables taking on a specific value.
</footnote>
<bodyText confidence="0.903537666666667">
similarity between nouns and attributes, we will
also refer to it as noun-attribute similarity.
Assumption ii) says that jokes are funnier if the
attribute used is less common. For example, there
are a few attributes that are very common and can
be used to describe almost anything (e.g., new,
free, good), but using them would probably lead
to bad jokes. We posit that the less common the
attribute Z is, the more likely it is to lead to sur-
prisal, which is known to contribute to the funni-
ness of jokes. We express this assumption in the
factor φ1(Z):
</bodyText>
<equation confidence="0.999676">
φ1(z) = 1/f(z) (2)
</equation>
<bodyText confidence="0.9876214">
where f(z) is the number of times attribute z ap-
pears in some external corpus. We will refer to this
factor as attribute surprisal.
Assumption iii) says that more ambiguous at-
tributes lead to funnier jokes. This is based on the
observation that the humor often stems from the
fact that the attribute is used in one sense when
describing noun x, and in a different sense when
describing noun y. This assumption is expressed
in φ2(Z) as:
</bodyText>
<equation confidence="0.999718">
φ2(z) = 1/senses(z) (3)
</equation>
<bodyText confidence="0.999968866666667">
where senses(z) is the number of different senses
that attribute z has. Note that this does not exactly
capture the fact that z should be used in different
senses for the different nouns, but it is a reason-
able first approximation. We refer to this factor as
attribute ambiguity.
Finally, assumption iv) says that dissimilar
nouns lead to funnier jokes. For example, if the
two nouns are girls and boys, we could easily find
many attributes that both nouns share. However,
since the two nouns are very similar, the effect of
surprisal would diminish as the observer would ex-
pect us to find an attribute that can describe both
nouns well. We therefore use φ(X, Y ) to encour-
age dissimilarity between the two nouns:
</bodyText>
<equation confidence="0.938248">
φ(x, y) = 1/sim(x, y), (4)
</equation>
<bodyText confidence="0.999956166666667">
where sim is a similarity function that measures
how similar nouns x and y are. We call this fac-
tor noun dissimilarity. There are many similar-
ity functions proposed in the literature, see e.g.,
Weeds et al. (2004); we use the cosine between
the distributional representation of the nouns:
</bodyText>
<equation confidence="0.9957146">
Ez p(z|x)p(z|y)
sim(x, y) = (5)
V/ Ez p(z |x)2 ∗ Ez p(z |y)2
02(Z) 01(Z)
0(X, Z)
X
0(X, Y)
Z
0(Y, Z)
Y
</equation>
<page confidence="0.992466">
229
</page>
<bodyText confidence="0.99993575">
Equation 5 computes the similarity between the
nouns by representing them in the space of all at-
tributes used to describe them, and then taking the
cosine of the angle between the noun vectors in
this representation.
To obtain the joint probability for an (x, y, z)
triple we simply multiply all the factors and nor-
malize over all the triples.
</bodyText>
<sectionHeader confidence="0.998426" genericHeader="method">
4 Data
</sectionHeader>
<bodyText confidence="0.999940344827586">
For estimating f(x, y) and f(z), we use Google
n-gram data (Michel et al., 2010), in particular the
Google 2-grams. We tag each word in the 2-grams
with the part-of-speech (POS) tag that corresponds
to the most common POS tag associated with that
word in Wordnet (Fellbaum, 1998). Once we have
the POS-tagged Google 2-gram data, we extract
all (noun, adjective) pairs and use their counts to
estimate both f(x, z) and f(y, z). We discard 2-
grams whose count in the Google data is less than
1000. After filtering we are left with 2 million
(noun, adjective) pairs. We estimate f(z) by sum-
ming the counts of all Google 2-grams that con-
tain that particular z. We obtain senses(z) from
Wordnet, which contains the number of senses for
all common words.
It is important to emphasize here that, while we
do use Wordnet in our work, our approach does not
crucially rely on it, and we use it to obtain only
very shallow information. In particular, we use
Wordnet to obtain i) POS tags for Google 2-grams,
and ii) number of senses for adjectives. POS tag-
ging could be easily done using any one of the
readily available POS taggers, but we chose this
approach for its simplicity and speed. The number
of different word senses for adjectives is harder to
obtain without Wordnet, but this is only one of the
four factors in our model, and we do not depend
crucially on it.
</bodyText>
<sectionHeader confidence="0.999611" genericHeader="evaluation">
5 Experiments
</sectionHeader>
<bodyText confidence="0.99991575">
We evaluate our model in two stages. Firstly, using
automatic evaluation with a set of jokes collected
from Twitter, and secondly, by comparing our ap-
proach to human-generated jokes.
</bodyText>
<subsectionHeader confidence="0.609489">
5.1 Inference
</subsectionHeader>
<bodyText confidence="0.999954846153846">
As the focus of this paper is on the model, not the
inference methods, we use exact inference. While
this is too expensive for estimating the true proba-
bility of any (x, y, z) triple, it is feasible if we fix
one of the nouns, i.e., if we deal with P(Y, Z|X =
x). Note that this is only a limitation of our infer-
ence procedure, not the model, and future work
will look at other ways (e.g., Gibbs sampling) to
perform inference. However, generating Y and
Z given X, such that the joke is funny, is still a
formidable challenge that a lot of humans are not
able to perform successfully (cf. performance of
human-generated jokes in Table 2).
</bodyText>
<subsectionHeader confidence="0.999461">
5.2 Automatic evaluation
</subsectionHeader>
<bodyText confidence="0.991664314285714">
In the automatic evaluation we measure the effect
of the different factors in the model, as laid out in
Section 3.1. We use two metrics for this evalua-
tion. The first is similar to log-likelihood, i.e., the
log of the probability that our model assigns to a
triple. However, because we do not compute it on
all the data, just on the data that contains the Xs
from our development set, it is not exactly equal
to the log-likelihood. It is a local approximation
to log-likelihood, and we therefore dub it LOcal
Log-likelihood, or LOL-likelihood for short. Our
second metric computes the rank of the human-
generated jokes in the distribution of all possible
jokes sorted decreasingly by their LOL-likelihood.
This Rank OF Likelihood (ROFL) is computed
relative to the number of all possible jokes, and
like LOL-likelihood is averaged over all the jokes
in our development data. One advantage of ROFL
is that it is designed with the way we generate
jokes in mind (cf. Section 5.3), and thus more di-
rectly measures the quality of generated jokes than
LOL-likelihood. For measuring LOL-likelihood
and ROFL we use a set of 48 jokes randomly sam-
pled from Twitter that fit the I like my X like I like
my Y, Z pattern.
Table 1 shows the effect of the different fac-
tors on the two metrics. We use a model with
only noun-attribute similarity (factors O(X, Z)
and O(Y, Z)) as the baseline. We see that the sin-
gle biggest improvement comes from the attribute
surprisal factor, i.e., from using rarer attributes.
The best combination of the factors, according to
automatic metrics, is using all factors except for
the noun similarity (Model 1), while using all the
factors is the second best combination (Model 2).
</bodyText>
<subsectionHeader confidence="0.998844">
5.3 Human evaluation
</subsectionHeader>
<bodyText confidence="0.99996425">
The main evaluation of our model is in terms of
human ratings, put simply: do humans find the
jokes generated by our model funny? We compare
four models: the two best models from Section 5.2
</bodyText>
<page confidence="0.994273">
230
</page>
<table confidence="0.978020285714286">
Model LOL-likelihood ROFL
Baseline -225.3 0.1909
Baseline + 0(X, Y ) -227.1 0.2431
Baseline + 01(Z) -204.9 0.1467
Baseline + 02(Z) -224.6 0.1625
Baseline + 01(Z) + 02(Z) (Model 1) -198.6 0.1002
All factors (Model 2) -203.7 0.1267
</table>
<tableCaption confidence="0.999798">
Table 1: Effect of different factors.
</tableCaption>
<bodyText confidence="0.999939463414634">
(one that uses all the factors (Model 2), and one
that uses all factors except for the noun dissimilar-
ity (Model 1)), a baseline model that uses only the
noun-attribute similarity, and jokes generated by
humans, collected from Twitter. We sample a fur-
ther 32 jokes from Twitter, making sure that there
was no overlap with the development set.
To generate a joke for a particular x we keep the
top n most probable jokes according to the model,
renormalize their probabilities so they sum to one,
and sample from this reduced distribution. This al-
lows our model to focus on the jokes that it consid-
ers “funny”. In our experiments, we use n = 30,
which ensures that we can still generate a variety
of jokes for any given x.
In our experiments we showed five native En-
glish speakers the jokes from all the systems in a
random, per rater, order. The raters were asked
to score each joke on a 3-point Likert scale: 1
(funny), 2 (somewhat funny), and 3 (not funny).
Naturally, the raters did not know which approach
each joke was coming from. Our model was used
to sample Y and Z variables, given the same Xs
used in the jokes collected from Twitter.
Results are shown in Table 2. The second col-
umn shows the inter-rater agreement (Randolph,
2005), and we can see that it is generally good, but
that it is lower on the set of human jokes. We in-
spected the human-generated jokes with high dis-
agreement and found that the disagreement may
be partly explained by raters missing cultural ref-
erences in the jokes (e.g., a sonic screwdriver is
Doctor Who’s tool of choice, which might be lost
on those who are not familiar with the show).
We do not explicitly model cultural references,
and are thus less likely to generate such jokes,
leading to higher agreement. The third column
shows the mean joke score (lower is better), and
we can see that human-generated jokes were rated
the funniest, jokes from the baseline model the
least funny, and that the model which uses all the
</bodyText>
<table confidence="0.9990178">
Model κ Mean % funny jokes
Human jokes 0.31 2.09 33.1
Baseline 0.58 2.78 3.7
Model 1 0.52 2.71 6.3
Model 2 0.58 2.56 16.3
</table>
<tableCaption confidence="0.971182">
Table 2: Comparison of different models on the
task of generating Y and Z given X.
</tableCaption>
<bodyText confidence="0.999268222222222">
factors (Model 2) outperforms the model that was
best according to the automatic evaluation (Model
1). Finally, the last column shows the percentage
of jokes the raters scored as funny (i.e., the num-
ber of funny scores divided by the total number of
scores). This is a metric that we are ultimately
interested in – telling a joke that is somewhat
funny is not useful, and we should only reward
generating a joke that is found genuinely funny
by humans. The last column shows that human-
generated jokes are considered funnier than the
machine-generated ones, but also that our model
with all the factors does much better than the other
two models. Model 2 is significantly better than
the baseline at p = 0.05 using a sign test, and
human jokes are significantly better than all three
models at p = 0.05 (because we were testing mul-
tiple hypotheses, we employed Holm-Bonferroni
correction (Holm, 1979)). In the end, our best
model generated jokes that were found funny by
humans in 16% of cases, compared to 33% ob-
tained by human-generated jokes.
Finally, we note that the funny jokes generated
by our system are not simply repeats of the human
jokes, but entirely new ones that we were not able
to find anywhere online. Examples of the funny
jokes generated by Model 2 are shown in Table 3.
</bodyText>
<sectionHeader confidence="0.999486" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999522">
We have presented a fully unsupervised humor
generation system for generating jokes of the type
</bodyText>
<page confidence="0.991696">
231
</page>
<bodyText confidence="0.401114666666667">
I like my relationships like I like my source, open
I like my coffee like I like my war, cold
I like my boys like I like my sectors, bad
</bodyText>
<tableCaption confidence="0.995494">
Table 3: Example jokes generated by Model 2.
</tableCaption>
<bodyText confidence="0.997777857142857">
I like my X like I like my Y, Z, where X, Y, and Z are
slots to be filled in. To the best of our knowledge,
this is the first humor generation system that does
not require any labeled data or hard-coded rules.
We express our assumptions about what makes a
joke funny as a machine learning model and show
that by estimating its parameters on large quanti-
ties of unlabeled data we can generate jokes that
are found funny by humans. While our experi-
ments show that human-generated jokes are fun-
nier more of the time, our model significantly im-
proves upon a non-trivial baseline, and we believe
that the fact that humans found jokes generated by
our model funny 16% of the time is encouraging.
</bodyText>
<sectionHeader confidence="0.994727" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999950285714286">
The authors would like to thank the raters for their
help and patience in labeling the (often not so
funny) jokes. We would also like to thank Micha
Elsner for this helpful comments. Finally, we
thank the inhabitants of offices 3.48 and 3.38 for
putting up with our sniggering every Friday after-
noon.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999599698113208">
Kim Binsted and Graeme Ritchie. 1994. An imple-
mented model of punning riddles. In Proceedings
of the twelfth national conference on Artificial intel-
ligence (vol. 1), AAAI ’94, pages 633–638, Menlo
Park, CA, USA. American Association for Artificial
Intelligence.
Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010.
Semi-supervised recognition of sarcastic sentences
in twitter and amazon. In Proceedings of the Four-
teenth Conference on Computational Natural Lan-
guage Learning, CoNLL ’10, pages 107–116.
Christiane Fellbaum. 1998. Wordnet: an electronic
lexical database. MIT Press.
Sture Holm. 1979. A simple sequentially rejective
multiple test procedure. Scandinavian journal of
statistics, pages 65–70.
Chlo´e Kiddon and Yuriy Brun. 2011. That’s what she
said: double entendre identification. In Proceedings
of the 49th Annual Meeting of the ACL: Human Lan-
guage Technologies: short papers - Volume 2, pages
89–94.
Igor Labutov and Hod Lipson. 2012. Humor as cir-
cuits in semantic networks. In Proceedings of the
50th Annual Meeting of the ACL (Volume 2: Short
Papers), pages 150–155, July.
Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser
Aiden, Adrian Veres, Matthew K. Gray, The
Google Books Team, Joseph P. Pickett, Dale Hol-
berg, Dan Clancy, Peter Norvig, Jon Orwant, Steven
Pinker, Martin A. Nowak, and Erez Lieberman
Aiden. 2010. Quantitative analysis of culture using
millions of digitized books. Science.
Rada Mihalcea and Carlo Strapparava. 2005. Making
computers laugh: investigations in automatic humor
recognition. In Proceedings of the conference on
Human Language Technology and EMNLP, pages
531–538.
Justus J. Randolph. 2005. Free-marginal multi-
rater kappa (multirater free): An alternative to fleiss
fixed- marginal multirater kappa. In Joensuu Uni-
versity Learning and Instruction Symposium.
Jonas Sj¨obergh and Kenji Araki. 2008. A com-
plete and modestly funny system for generating and
performing japanese stand-up comedy. In Coling
2008: Companion volume: Posters, pages 111–114,
Manchester, UK, August. Coling 2008 Organizing
Committee.
Julie Weeds, David Weir, and Diana McCarthy. 2004.
Characterising measures of lexical distributional
similarity. In Proceedings of the 20th international
conference on Computational Linguistics, COLING
’04, Stroudsburg, PA, USA. Association for Compu-
tational Linguistics.
</reference>
<page confidence="0.995172">
232
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.320643">
<title confidence="0.992255">Unsupervised joke generation from big data</title>
<author confidence="0.961818">Saˇsa</author>
<affiliation confidence="0.9985715">School of University of</affiliation>
<email confidence="0.949429">sasa.petrovic@ed.ac.uk</email>
<author confidence="0.907401">David</author>
<affiliation confidence="0.9979655">School of University of</affiliation>
<email confidence="0.99372">dave.matthews@ed.ac.uk</email>
<abstract confidence="0.95871845">Humor generation is a very hard problem. It is difficult to say exactly what makes a joke funny, and solving this problem algorithmically is assumed to require deep semantic understanding, as well as cultural and other contextual cues. We depart from previous work that tries to model this knowledge using ad-hoc manually created databases and labeled training examples. Instead we present a model that uses large of unannotated data to generate my X like I like my Y, Z where X, Y, and Z are variables to be filled in. This is, to the best of our knowledge, the first fully unsupervised humor generation system. Our model significantly outperforms a competitive baseline and generates funny jokes 16% of the time, compared to 33% for human-generated jokes.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Kim Binsted</author>
<author>Graeme Ritchie</author>
</authors>
<title>An implemented model of punning riddles.</title>
<date>1994</date>
<journal>Artificial Intelligence.</journal>
<booktitle>In Proceedings of the twelfth national conference on Artificial intelligence</booktitle>
<volume>1</volume>
<pages>633--638</pages>
<publisher>American Association for</publisher>
<location>Menlo Park, CA, USA.</location>
<contexts>
<context position="3497" citStr="Binsted and Ritchie, 1994" startWordPosition="586" endWordPosition="589"> identification (Davidov et al., 2010), and one-liner joke recognition (Mihalcea and Strapparava, 2005). All this previous work uses labeled training data. Kiddon and Brun (2011) use a supervised classifier (SVM) trained on 4,000 labeled examples, while Davidov et al. (2010) and Mihalcea and Strapparava (2005) both use a small amount of training data followed by a bootstrapping step to gather more. Examples of work on humor generation include dirty joke telling robots (Sj¨obergh and Araki, 2008), a generative model of two-liner jokes (Labutov and Lipson, 2012), and a model of punning riddles (Binsted and Ritchie, 1994). Again, all this work uses supervision in some form: Sj¨obergh and Araki (2008) use only human jokes collected from various sources, Labutov and Lipson (2012) use a supervised approach to learn feasible circuits that connect two concepts in a semantic network, and 228 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228–232, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 1: Our model presented as a factor graph. Binsted and Ritchie (1994) have a set of six hardcoded rules for generating puns. 3 Generating</context>
</contexts>
<marker>Binsted, Ritchie, 1994</marker>
<rawString>Kim Binsted and Graeme Ritchie. 1994. An implemented model of punning riddles. In Proceedings of the twelfth national conference on Artificial intelligence (vol. 1), AAAI ’94, pages 633–638, Menlo Park, CA, USA. American Association for Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitry Davidov</author>
<author>Oren Tsur</author>
<author>Ari Rappoport</author>
</authors>
<title>Semi-supervised recognition of sarcastic sentences in twitter and amazon.</title>
<date>2010</date>
<booktitle>In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10,</booktitle>
<pages>107--116</pages>
<contexts>
<context position="2909" citStr="Davidov et al., 2010" startWordPosition="494" endWordPosition="497">y by human raters in 16% of cases. The main contribution of this paper is, to the best of our knowledge, the first fully unsupervised joke generation system. We rely only on large quantities of unlabeled data, suggesting that generating jokes does not always require deep semantic understanding, as usually thought. 2 Related Work Related work on computational humor can be divided into two classes: humor recognition and humor generation. Humor recognition includes double entendre identification in the form of That’s what she said jokes (Kiddon and Brun, 2011), sarcastic sentence identification (Davidov et al., 2010), and one-liner joke recognition (Mihalcea and Strapparava, 2005). All this previous work uses labeled training data. Kiddon and Brun (2011) use a supervised classifier (SVM) trained on 4,000 labeled examples, while Davidov et al. (2010) and Mihalcea and Strapparava (2005) both use a small amount of training data followed by a bootstrapping step to gather more. Examples of work on humor generation include dirty joke telling robots (Sj¨obergh and Araki, 2008), a generative model of two-liner jokes (Labutov and Lipson, 2012), and a model of punning riddles (Binsted and Ritchie, 1994). Again, all</context>
</contexts>
<marker>Davidov, Tsur, Rappoport, 2010</marker>
<rawString>Dmitry Davidov, Oren Tsur, and Ari Rappoport. 2010. Semi-supervised recognition of sarcastic sentences in twitter and amazon. In Proceedings of the Fourteenth Conference on Computational Natural Language Learning, CoNLL ’10, pages 107–116.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>Wordnet: an electronic lexical database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="8245" citStr="Fellbaum, 1998" startWordPosition="1434" endWordPosition="1435">utes the similarity between the nouns by representing them in the space of all attributes used to describe them, and then taking the cosine of the angle between the noun vectors in this representation. To obtain the joint probability for an (x, y, z) triple we simply multiply all the factors and normalize over all the triples. 4 Data For estimating f(x, y) and f(z), we use Google n-gram data (Michel et al., 2010), in particular the Google 2-grams. We tag each word in the 2-grams with the part-of-speech (POS) tag that corresponds to the most common POS tag associated with that word in Wordnet (Fellbaum, 1998). Once we have the POS-tagged Google 2-gram data, we extract all (noun, adjective) pairs and use their counts to estimate both f(x, z) and f(y, z). We discard 2- grams whose count in the Google data is less than 1000. After filtering we are left with 2 million (noun, adjective) pairs. We estimate f(z) by summing the counts of all Google 2-grams that contain that particular z. We obtain senses(z) from Wordnet, which contains the number of senses for all common words. It is important to emphasize here that, while we do use Wordnet in our work, our approach does not crucially rely on it, and we u</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. Wordnet: an electronic lexical database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sture Holm</author>
</authors>
<title>A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics,</title>
<date>1979</date>
<pages>65--70</pages>
<contexts>
<context position="15456" citStr="Holm, 1979" startWordPosition="2715" endWordPosition="2716">ultimately interested in – telling a joke that is somewhat funny is not useful, and we should only reward generating a joke that is found genuinely funny by humans. The last column shows that humangenerated jokes are considered funnier than the machine-generated ones, but also that our model with all the factors does much better than the other two models. Model 2 is significantly better than the baseline at p = 0.05 using a sign test, and human jokes are significantly better than all three models at p = 0.05 (because we were testing multiple hypotheses, we employed Holm-Bonferroni correction (Holm, 1979)). In the end, our best model generated jokes that were found funny by humans in 16% of cases, compared to 33% obtained by human-generated jokes. Finally, we note that the funny jokes generated by our system are not simply repeats of the human jokes, but entirely new ones that we were not able to find anywhere online. Examples of the funny jokes generated by Model 2 are shown in Table 3. 6 Conclusion We have presented a fully unsupervised humor generation system for generating jokes of the type 231 I like my relationships like I like my source, open I like my coffee like I like my war, cold I </context>
</contexts>
<marker>Holm, 1979</marker>
<rawString>Sture Holm. 1979. A simple sequentially rejective multiple test procedure. Scandinavian journal of statistics, pages 65–70.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chlo´e Kiddon</author>
<author>Yuriy Brun</author>
</authors>
<title>That’s what she said: double entendre identification.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies: short papers -</booktitle>
<volume>2</volume>
<pages>89--94</pages>
<contexts>
<context position="2851" citStr="Kiddon and Brun, 2011" startWordPosition="487" endWordPosition="490">ata, we are able to generate jokes that are considered funny by human raters in 16% of cases. The main contribution of this paper is, to the best of our knowledge, the first fully unsupervised joke generation system. We rely only on large quantities of unlabeled data, suggesting that generating jokes does not always require deep semantic understanding, as usually thought. 2 Related Work Related work on computational humor can be divided into two classes: humor recognition and humor generation. Humor recognition includes double entendre identification in the form of That’s what she said jokes (Kiddon and Brun, 2011), sarcastic sentence identification (Davidov et al., 2010), and one-liner joke recognition (Mihalcea and Strapparava, 2005). All this previous work uses labeled training data. Kiddon and Brun (2011) use a supervised classifier (SVM) trained on 4,000 labeled examples, while Davidov et al. (2010) and Mihalcea and Strapparava (2005) both use a small amount of training data followed by a bootstrapping step to gather more. Examples of work on humor generation include dirty joke telling robots (Sj¨obergh and Araki, 2008), a generative model of two-liner jokes (Labutov and Lipson, 2012), and a model </context>
</contexts>
<marker>Kiddon, Brun, 2011</marker>
<rawString>Chlo´e Kiddon and Yuriy Brun. 2011. That’s what she said: double entendre identification. In Proceedings of the 49th Annual Meeting of the ACL: Human Language Technologies: short papers - Volume 2, pages 89–94.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Labutov</author>
<author>Hod Lipson</author>
</authors>
<title>Humor as circuits in semantic networks.</title>
<date>2012</date>
<booktitle>In Proceedings of the 50th Annual Meeting of the ACL (Volume 2: Short Papers),</booktitle>
<pages>150--155</pages>
<contexts>
<context position="3437" citStr="Labutov and Lipson, 2012" startWordPosition="575" endWordPosition="579"> she said jokes (Kiddon and Brun, 2011), sarcastic sentence identification (Davidov et al., 2010), and one-liner joke recognition (Mihalcea and Strapparava, 2005). All this previous work uses labeled training data. Kiddon and Brun (2011) use a supervised classifier (SVM) trained on 4,000 labeled examples, while Davidov et al. (2010) and Mihalcea and Strapparava (2005) both use a small amount of training data followed by a bootstrapping step to gather more. Examples of work on humor generation include dirty joke telling robots (Sj¨obergh and Araki, 2008), a generative model of two-liner jokes (Labutov and Lipson, 2012), and a model of punning riddles (Binsted and Ritchie, 1994). Again, all this work uses supervision in some form: Sj¨obergh and Araki (2008) use only human jokes collected from various sources, Labutov and Lipson (2012) use a supervised approach to learn feasible circuits that connect two concepts in a semantic network, and 228 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 228–232, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics Figure 1: Our model presented as a factor graph. Binsted and Ritchie (1994) have a </context>
</contexts>
<marker>Labutov, Lipson, 2012</marker>
<rawString>Igor Labutov and Hod Lipson. 2012. Humor as circuits in semantic networks. In Proceedings of the 50th Annual Meeting of the ACL (Volume 2: Short Papers), pages 150–155, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean-Baptiste Michel</author>
<author>Yuan Kui Shen</author>
<author>Aviva Presser Aiden</author>
<author>Adrian Veres</author>
<author>Matthew K Gray</author>
</authors>
<title>The Google Books Team,</title>
<date>2010</date>
<journal>Joseph</journal>
<publisher>Science.</publisher>
<contexts>
<context position="8046" citStr="Michel et al., 2010" startWordPosition="1399" endWordPosition="1402">(2004); we use the cosine between the distributional representation of the nouns: Ez p(z|x)p(z|y) sim(x, y) = (5) V/ Ez p(z |x)2 ∗ Ez p(z |y)2 02(Z) 01(Z) 0(X, Z) X 0(X, Y) Z 0(Y, Z) Y 229 Equation 5 computes the similarity between the nouns by representing them in the space of all attributes used to describe them, and then taking the cosine of the angle between the noun vectors in this representation. To obtain the joint probability for an (x, y, z) triple we simply multiply all the factors and normalize over all the triples. 4 Data For estimating f(x, y) and f(z), we use Google n-gram data (Michel et al., 2010), in particular the Google 2-grams. We tag each word in the 2-grams with the part-of-speech (POS) tag that corresponds to the most common POS tag associated with that word in Wordnet (Fellbaum, 1998). Once we have the POS-tagged Google 2-gram data, we extract all (noun, adjective) pairs and use their counts to estimate both f(x, z) and f(y, z). We discard 2- grams whose count in the Google data is less than 1000. After filtering we are left with 2 million (noun, adjective) pairs. We estimate f(z) by summing the counts of all Google 2-grams that contain that particular z. We obtain senses(z) fr</context>
</contexts>
<marker>Michel, Shen, Aiden, Veres, Gray, 2010</marker>
<rawString>Jean-Baptiste Michel, Yuan Kui Shen, Aviva Presser Aiden, Adrian Veres, Matthew K. Gray, The Google Books Team, Joseph P. Pickett, Dale Holberg, Dan Clancy, Peter Norvig, Jon Orwant, Steven Pinker, Martin A. Nowak, and Erez Lieberman Aiden. 2010. Quantitative analysis of culture using millions of digitized books. Science.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Carlo Strapparava</author>
</authors>
<title>Making computers laugh: investigations in automatic humor recognition.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on Human Language Technology and EMNLP,</booktitle>
<pages>531--538</pages>
<contexts>
<context position="2974" citStr="Mihalcea and Strapparava, 2005" startWordPosition="502" endWordPosition="505">n of this paper is, to the best of our knowledge, the first fully unsupervised joke generation system. We rely only on large quantities of unlabeled data, suggesting that generating jokes does not always require deep semantic understanding, as usually thought. 2 Related Work Related work on computational humor can be divided into two classes: humor recognition and humor generation. Humor recognition includes double entendre identification in the form of That’s what she said jokes (Kiddon and Brun, 2011), sarcastic sentence identification (Davidov et al., 2010), and one-liner joke recognition (Mihalcea and Strapparava, 2005). All this previous work uses labeled training data. Kiddon and Brun (2011) use a supervised classifier (SVM) trained on 4,000 labeled examples, while Davidov et al. (2010) and Mihalcea and Strapparava (2005) both use a small amount of training data followed by a bootstrapping step to gather more. Examples of work on humor generation include dirty joke telling robots (Sj¨obergh and Araki, 2008), a generative model of two-liner jokes (Labutov and Lipson, 2012), and a model of punning riddles (Binsted and Ritchie, 1994). Again, all this work uses supervision in some form: Sj¨obergh and Araki (20</context>
</contexts>
<marker>Mihalcea, Strapparava, 2005</marker>
<rawString>Rada Mihalcea and Carlo Strapparava. 2005. Making computers laugh: investigations in automatic humor recognition. In Proceedings of the conference on Human Language Technology and EMNLP, pages 531–538.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Justus J Randolph</author>
</authors>
<title>Free-marginal multirater kappa (multirater free): An alternative to fleiss fixed- marginal multirater kappa.</title>
<date>2005</date>
<institution>In Joensuu University Learning and Instruction Symposium.</institution>
<contexts>
<context position="13626" citStr="Randolph, 2005" startWordPosition="2390" endWordPosition="2391">ts, we use n = 30, which ensures that we can still generate a variety of jokes for any given x. In our experiments we showed five native English speakers the jokes from all the systems in a random, per rater, order. The raters were asked to score each joke on a 3-point Likert scale: 1 (funny), 2 (somewhat funny), and 3 (not funny). Naturally, the raters did not know which approach each joke was coming from. Our model was used to sample Y and Z variables, given the same Xs used in the jokes collected from Twitter. Results are shown in Table 2. The second column shows the inter-rater agreement (Randolph, 2005), and we can see that it is generally good, but that it is lower on the set of human jokes. We inspected the human-generated jokes with high disagreement and found that the disagreement may be partly explained by raters missing cultural references in the jokes (e.g., a sonic screwdriver is Doctor Who’s tool of choice, which might be lost on those who are not familiar with the show). We do not explicitly model cultural references, and are thus less likely to generate such jokes, leading to higher agreement. The third column shows the mean joke score (lower is better), and we can see that human-</context>
</contexts>
<marker>Randolph, 2005</marker>
<rawString>Justus J. Randolph. 2005. Free-marginal multirater kappa (multirater free): An alternative to fleiss fixed- marginal multirater kappa. In Joensuu University Learning and Instruction Symposium.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonas Sj¨obergh</author>
<author>Kenji Araki</author>
</authors>
<title>A complete and modestly funny system for generating and performing japanese stand-up comedy.</title>
<date>2008</date>
<journal>Organizing Committee.</journal>
<booktitle>In Coling 2008: Companion volume: Posters,</booktitle>
<pages>111--114</pages>
<location>Manchester, UK,</location>
<marker>Sj¨obergh, Araki, 2008</marker>
<rawString>Jonas Sj¨obergh and Kenji Araki. 2008. A complete and modestly funny system for generating and performing japanese stand-up comedy. In Coling 2008: Companion volume: Posters, pages 111–114, Manchester, UK, August. Coling 2008 Organizing Committee.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Weeds</author>
<author>David Weir</author>
<author>Diana McCarthy</author>
</authors>
<title>Characterising measures of lexical distributional similarity.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="7432" citStr="Weeds et al. (2004)" startWordPosition="1284" endWordPosition="1287"> to funnier jokes. For example, if the two nouns are girls and boys, we could easily find many attributes that both nouns share. However, since the two nouns are very similar, the effect of surprisal would diminish as the observer would expect us to find an attribute that can describe both nouns well. We therefore use φ(X, Y ) to encourage dissimilarity between the two nouns: φ(x, y) = 1/sim(x, y), (4) where sim is a similarity function that measures how similar nouns x and y are. We call this factor noun dissimilarity. There are many similarity functions proposed in the literature, see e.g., Weeds et al. (2004); we use the cosine between the distributional representation of the nouns: Ez p(z|x)p(z|y) sim(x, y) = (5) V/ Ez p(z |x)2 ∗ Ez p(z |y)2 02(Z) 01(Z) 0(X, Z) X 0(X, Y) Z 0(Y, Z) Y 229 Equation 5 computes the similarity between the nouns by representing them in the space of all attributes used to describe them, and then taking the cosine of the angle between the noun vectors in this representation. To obtain the joint probability for an (x, y, z) triple we simply multiply all the factors and normalize over all the triples. 4 Data For estimating f(x, y) and f(z), we use Google n-gram data (Michel</context>
</contexts>
<marker>Weeds, Weir, McCarthy, 2004</marker>
<rawString>Julie Weeds, David Weir, and Diana McCarthy. 2004. Characterising measures of lexical distributional similarity. In Proceedings of the 20th international conference on Computational Linguistics, COLING ’04, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>