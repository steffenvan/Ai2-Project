<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.004146">
<note confidence="0.848935">
In: Proceedings of CoNLL-2000 and LLL-2000, pages 160-162, Lisbon, Portugal, 2000.
</note>
<title confidence="0.948001">
Phrase Parsing with Rule Sequence Processors:
an Application to the Shared CoNLL Task
</title>
<note confidence="0.867650666666667">
Marc Vilain and David Day
The MITRE Corporation
Bedford, MA 01730, USA
</note>
<bodyText confidence="0.987239962962963">
daylOmitre.org
For several years, chunking has been an inte-
gral part of MITRE&apos;s approach to information
extraction. Our work exploits chunking in two
principal ways. First, as part of our extraction
system (Alembic) (Aberdeen et al., 1995), the
chunker delineates descriptor phrases for entity
extraction. Second, as part of our ongoing re-
search in parsing, chunks provide the first level
of a stratified approach to syntax — the second
level is defined by grammatical relations, much
as in the SPARKLE effort (Carroll et al., 1997).
Because of our ongoing work with chunking,
we were naturally interested in evaluating our
approach on the common CoNLL task. In this
note, we thus present three different evaluations
of our work on phrase-level parsing. The first
is a baseline of sorts, our own version of the
&amp;quot;chunking as tagging&amp;quot; approach introduced by
Ramshaw and Marcus (Ramshaw and Marcus,
1995). The second set of results reports the
performance of a trainable rule-based system,
the Alembic phrase rule parser. As a point of
comparison, we also include a third set of mea-
sures produced by running the standard Alem-
bic chunker on the common task with little or
no adaptation.
</bodyText>
<sectionHeader confidence="0.536312" genericHeader="abstract">
1 Chunking as Tagging
</sectionHeader>
<bodyText confidence="0.999648272727273">
For this first experiment, we coerced our part-
of-speech tagger to generate chunk labels. We
did so in what can only count as the most rudi-
mentary way: by training the tagger to map
part-of-speech labels to the chunk labels of the
common task. The learning procedure is a re-
implementation of Brill&apos;s transformation-based
approach (Brill, 1993), extended to cover ap-
proximately an order of magnitude more rule
schemata. As input, the training corpus was
tagged with the parts-of-speech from the com-
</bodyText>
<table confidence="0.99837075">
N accuracy precision recall Fo=1
1000 86 77 77 77
2000 89 82 82 82
4000 89 81 81 81
</table>
<tableCaption confidence="0.866371">
Table 1: Performance of the brute-force re-
tagging approach
</tableCaption>
<bodyText confidence="0.999736741935484">
mon data set: these provided an initial labeling
of the data which was then directly converted
to chunk labels through the action of transfor-
mation rules (Brill&apos;s so-called contextual rules).
Because the learning procedure is none the
swiftest, we restricted ourselves to subsets of
the training data, acquiring rules from the first
1000, 2000, and 4000 sentences of the training
set. In each case, we acquired 500 transforma-
tion rules. We measured the following perfor-
mance of these rules on the test set.
These results are hardly stellar, falling some
10 points of F below the performance of previ-
ous approaches to noun group detection. To be
sure, the chunking task is more demanding than
the simple identification of noun group bound-
aries, so one would expect lower performance on
the harder problem. But the rudimentary way
in which we implemented the approach is likely
also to blame.
There are a number of clear-cut ways in which
we could attempt to improve our performance
using this approach. In particular, we would ex-
pect to obtain better results if we did not oblit-
erate the part-of-speech of a lexeme in the pro-
cess of tagging it with a chunk label. Indeed,
in our experiments, the learning procedure ac-
quired transformations that simply replaced the
part-of-speech tag with a chunking tag, thereby
inhibiting potentially useful downstream rules
for accessing the part-of-speech information of
</bodyText>
<page confidence="0.976953">
160
</page>
<bodyText confidence="0.977334476190476">
a chunk-tagged word.
2 Chunking with the Phrase Rule
Parser
Our main interest in this common evaluation,
however, was not to set new high-water marks
with the approach of Ramshaw and Marcus, but
to exercise our phrase rule parser.
The Alembic phrase rule parser (Vilain and
Day, 1996) provides the core of the system&apos;s
syntactic processing. In our extraction appli-
cations, the phraser (as we call it) initially tags
named entities and other fixed-class constructs
(like titles). The phraser also treats as atomic
units the stereotypical combinations of named
entities that one finds in newswire text, e.g.,
the person-title-organization apposition &amp;quot;U.N.
secretary general Kofi Anan&amp;quot;. The three com-
ponents of the apposition are initially parsed
as fixed-class entities, and are then combined
to form a single person-denoting phrase. These
preliminary parsing steps provide part of the in-
put to the chunker, which is itself implemented
as a phrase rule parser.
The architecture of the parser is based on
Brill&apos;s approach. The parser follows a sequence
of rules in order to build phrases out of parse is-
lands. These islands are initially introduced by
instantiating partial phrases around individual
lexemes (useful for name tagging), or around
runs of certain parts of speech (useful for both
name tagging and chunking). It is the job of the
phrase parsing rules to grow the boundaries of
these phrases to the left or right, and to assign
them a type, e.g., a name tag or a chunk la-
bel. As with other rule sequence processors, the
phraser proceeds in sequence through its cata-
logue of rules, applying each in turn wherever it
matches, and then discarding it to proceed on
to the next rule in the sequence.
For example, in name tagging, we seed
initial phrases around runs of capitalized
words. A phrase such as &amp;quot;meetings in Paris
and Rome&amp;quot; would produce an initial phrase
analysis of &amp;quot;meetings in &lt;?&gt;Paris&lt;/?&gt; and
&lt;?&gt;Rome&lt;/?&gt;&amp;quot;, where the&amp;quot;?&amp;quot; on the phrases
are initial labels that indicate the phrase has
not received a type.
The patterns that are implemented by phrase
parsing rules are similar to those in Brill&apos;s
transformation-based p-o-s tagger. A rule can
test for the presence of a given part of speech, of
a lexeme, of a list of lexemes, and so on. These
tests are themselves anchored to a specific locus
(a phrase or lexeme) and are performed relative
to that locus. As actions, the rules can grow the
boundaries of a phrase, and set or modify its la-
bel. For example, a typical name tagging rule
would assign a LOCATION tag to any phrase
preceded by the preposition &amp;quot;in&amp;quot;. And indeed,
this very rule tends to emerge as the very first
rule acquired in training a phraser-based name
tagger. We show it here with no further com-
ment, trusting that its syntax is self-evident.
</bodyText>
<equation confidence="0.710967666666667">
(def-phraser-rule
:conditions (:left-1 :lex &amp;quot;in&amp;quot;)
:actions (:set-label :LOCATION))
</equation>
<bodyText confidence="0.996973612903226">
In our particular example (&amp;quot;meetings in
&lt;?&gt;Paris&lt;/?&gt; and &lt;?&gt;Rome&lt;/?&gt;&amp;quot;), this
rule would re-label the &lt;?&gt; phrase around Paris
with the LOCATION tag. A subsequent rule
might then exploit the coordination to infer
that &amp;quot;Rome&amp;quot; is a location as well, implement-
ing the transformation &amp;quot;LOCATION and &lt;?&gt;&amp;quot;
—&gt; &amp;quot;LOCATION and LOCATION&amp;quot;. This incre-
mental patching of errors is the hallmark of
Brill&apos;s approach.
An interesting property of this rule language
is that the phraser can be operated either as
a trainable procedure, using standard error-
driven transformation learning, or as a hand-
engineered system. For the purpose of the com-
mon CoNLL task, let us first present our results
for the trainable case.
We again approached the task in a relatively
rudimentary way, in this case by applying the
phrase rule learning procedure with no partic-
ular adaptation to the task. Indeed, the proce-
dure can be parameterized by word lists which
it can then exploit to improve its performance.
Since our main interest here was to see our base-
line performance on the task, we did not harvest
such word lists from the training data (there is
an automated way to do this). We ran a num-
ber of training runs based on different partitions
of the training data, with the following overall
performance on test data, averaged across runs.
accuracy precision recall Fp=1
</bodyText>
<page confidence="0.7534085">
89 89 83 86
161
</page>
<bodyText confidence="0.999952428571428">
The constituents that were most accurately
recognized were noun groups (F=88), with verb
groups a close second (F=87). These were
followed by the ostensibly easy cases of PP&apos;s
(F=86), SBAR&apos;s (F=79), and ADVF&amp;quot;s (F=75).
Our lowest performing constituent for which
the learning procedure actually generated rules
was ADJP&apos;s (F=37), with no rules generated
to identify CONJP&apos;s, INTJ&apos;s, LST&apos;s, or PRT&apos;s
(F=0 in all these cases).
In general, precision, tended to be several
points of F higher than recall, and in the case
of ADJP&apos;s average precision was 76 compared
to average recall of 24!
</bodyText>
<sectionHeader confidence="0.664648" genericHeader="categories and subject descriptors">
3 Chunking with the
</sectionHeader>
<subsectionHeader confidence="0.397152">
Hand-Engineered System
</subsectionHeader>
<bodyText confidence="0.999881878787879">
As a point of comparison, we also applied our
hand-engineered chunker to the CoNLL task.
We expected that it would not perform at its
best on this task, since it was designed with
a significantly different model of chunking in
mind, and indeed, unmodified, it produced dis-
appointing results:
accuracy precision recall F;=1
84 80 75 77
The magnitude of our error term was some-
thing of a surprise. With production runs
on standard newswire stories (several hundred
words in lengths) the chunker typically produces
fewer errors per story than one can count on one
hand. The discrepancy with the results mea-
sured on the CoNLL task is of course due to
the fact that our manually engineered parser
was designed to produce chunks to a different
standard.
The standard was carefully defined so as to
be maximally informative to downstream pro-
cessing. Generally speaking, this means that it
tends to make distinctions that are not made in
the CoNLL data, e.g., splitting verbal runs such
as &amp;quot;failed to realize&amp;quot; into individual verb groups
when more than one event is denoted.
Our curiosity about these discrepancies is
now piqued. As a point of further investiga-
tion, we intend to apply the phraser&apos;s training
procedure to adapt the manual chunker to the
CoNLL task. With transformation-based rule
sequences, this is easy to do: one merely trains
the procedure to transform the output required
</bodyText>
<table confidence="0.999889583333333">
test data precision recall F0=1
ADJP 75.89% 24.43% 36.96
ADVP 80.64% 70.21% 75.06
CONJP 0.00% 0.00% 0.00
INTJ 0.00% 0.00% 0.00
LST 0.00% 0.00% 0.00
NP 87.85% 87.77% 87.81
PP 91.77% 80.42% 85.72
PRT 0.00% 0.00% 0.00
SBAR 91.36% 69.16% 78.72
VP 90.34% 84.13% 87.13
all 88.82% 82.91% 85.76
</table>
<tableCaption confidence="0.999601">
Table 2: The results of the phrase rule parser.
</tableCaption>
<bodyText confidence="0.999984333333333">
for the one task into that required for the other.
The rules acquired in this way are then sim-
ply tacked on to the end of the original rule
sequence (a half dozen such rules written by
hand bring the performance of the chunker up
to F=82, for example).
A more interesting point of investigation,
however, would be to analyze the discrepan-
cies between current chunk standards from the
standpoint of syntactic and semantic criteria.
We look forward to reporting on this at some
future point.
</bodyText>
<sectionHeader confidence="0.998298" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999634608695652">
J. Aberdeen, J. Burger, D. Day, L. Hirschman,
P. Robinson, and M. Vilain. 1995. Mitre: De-
scription of the alembic system used for muc-6.
In Proc. 6th Message Understanding Conference
(MUC-6). Defense Advanced Research Projects
Agency, November.
E. Brill. 1993. A Corpus-based Approach to Lan-
guage Learning. Ph.D. thesis, U. Pennsylvania.
J. Carroll, T. Briscoe, N. Calzolari, S. Fed-
erici, S. Montemagni, V. Pirrelli, G. Grefen-
stette, A. Sanfilippo, G. Carroll, and M. Rooth.
1997. SPARKLE work package 1, specifica-
tion of phrasal parsing, final report. Avail-
able at http: //www. ilc . pi . cnr. it/sparkle/-
sparkle .htm, November.
L. Ramshaw and M. Marcus. 1995. Text chunking
using transformation-based learning. In Proc. of
the 3rd Workshop on Very Large Corpora, pages
82-94, Cambridge, MA, USA.
M. Vilain and D. Day. 1996. Finite-state phrase
parsing by rule sequences. In Proceedings of the
16th Intl. Conference on Computational Linguis-
tics (COLING-96).
</reference>
<page confidence="0.997778">
162
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.004324">
<note confidence="0.965363">of CoNLL-2000 and LLL-2000, 160-162, Lisbon, Portugal, 2000.</note>
<title confidence="0.9888385">Phrase Parsing with Rule Sequence an Application to the Shared CoNLL Task</title>
<author confidence="0.999989">Marc Vilain</author>
<author confidence="0.999989">David Day</author>
<affiliation confidence="0.999741">The MITRE Corporation</affiliation>
<address confidence="0.999962">Bedford, MA 01730, USA</address>
<email confidence="0.920808">daylOmitre.org</email>
<abstract confidence="0.998767421052632">For several years, chunking has been an integral part of MITRE&apos;s approach to information extraction. Our work exploits chunking in two principal ways. First, as part of our extraction system (Alembic) (Aberdeen et al., 1995), the chunker delineates descriptor phrases for entity extraction. Second, as part of our ongoing research in parsing, chunks provide the first level of a stratified approach to syntax — the second level is defined by grammatical relations, much as in the SPARKLE effort (Carroll et al., 1997). Because of our ongoing work with chunking, we were naturally interested in evaluating our approach on the common CoNLL task. In this note, we thus present three different evaluations of our work on phrase-level parsing. The first is a baseline of sorts, our own version of the &amp;quot;chunking as tagging&amp;quot; approach introduced by Ramshaw and Marcus (Ramshaw and Marcus, 1995). The second set of results reports the performance of a trainable rule-based system, the Alembic phrase rule parser. As a point of comparison, we also include a third set of measures produced by running the standard Alembic chunker on the common task with little or no adaptation. 1 Chunking as Tagging For this first experiment, we coerced our partof-speech tagger to generate chunk labels. We did so in what can only count as the most rudimentary way: by training the tagger to map part-of-speech labels to the chunk labels of the common task. The learning procedure is a reimplementation of Brill&apos;s transformation-based approach (Brill, 1993), extended to cover approximately an order of magnitude more rule schemata. As input, the training corpus was with the parts-of-speech from the com-</abstract>
<note confidence="0.627434">N accuracy precision recall Fo=1</note>
<phone confidence="0.648146666666667">1000 86 77 77 77 2000 89 82 82 82 4000 89 81 81 81</phone>
<abstract confidence="0.988899584541063">Table 1: Performance of the brute-force retagging approach mon data set: these provided an initial labeling of the data which was then directly converted to chunk labels through the action of transformation rules (Brill&apos;s so-called contextual rules). Because the learning procedure is none the swiftest, we restricted ourselves to subsets of the training data, acquiring rules from the first 1000, 2000, and 4000 sentences of the training set. In each case, we acquired 500 transformation rules. We measured the following performance of these rules on the test set. These results are hardly stellar, falling some 10 points of F below the performance of previous approaches to noun group detection. To be sure, the chunking task is more demanding than the simple identification of noun group boundaries, so one would expect lower performance on the harder problem. But the rudimentary way in which we implemented the approach is likely also to blame. There are a number of clear-cut ways in which we could attempt to improve our performance using this approach. In particular, we would expect to obtain better results if we did not obliterate the part-of-speech of a lexeme in the process of tagging it with a chunk label. Indeed, in our experiments, the learning procedure acquired transformations that simply replaced the part-of-speech tag with a chunking tag, thereby inhibiting potentially useful downstream rules for accessing the part-of-speech information of 160 a chunk-tagged word. 2 Chunking with the Phrase Rule Parser Our main interest in this common evaluation, however, was not to set new high-water marks with the approach of Ramshaw and Marcus, but to exercise our phrase rule parser. The Alembic phrase rule parser (Vilain and Day, 1996) provides the core of the system&apos;s syntactic processing. In our extraction applications, the phraser (as we call it) initially tags named entities and other fixed-class constructs (like titles). The phraser also treats as atomic units the stereotypical combinations of named entities that one finds in newswire text, e.g., the person-title-organization apposition &amp;quot;U.N. secretary general Kofi Anan&amp;quot;. The three components of the apposition are initially parsed as fixed-class entities, and are then combined to form a single person-denoting phrase. These preliminary parsing steps provide part of the input to the chunker, which is itself implemented as a phrase rule parser. The architecture of the parser is based on Brill&apos;s approach. The parser follows a sequence of rules in order to build phrases out of parse islands. These islands are initially introduced by instantiating partial phrases around individual lexemes (useful for name tagging), or around runs of certain parts of speech (useful for both name tagging and chunking). It is the job of the phrase parsing rules to grow the boundaries of these phrases to the left or right, and to assign them a type, e.g., a name tag or a chunk label. As with other rule sequence processors, the phraser proceeds in sequence through its catalogue of rules, applying each in turn wherever it matches, and then discarding it to proceed on to the next rule in the sequence. For example, in name tagging, we seed initial phrases around runs of capitalized words. A phrase such as &amp;quot;meetings in Paris and Rome&amp;quot; would produce an initial phrase analysis of &amp;quot;meetings in &lt;?&gt;Paris&lt;/?&gt; and &lt;?&gt;Rome&lt;/?&gt;&amp;quot;, where the&amp;quot;?&amp;quot; on the phrases are initial labels that indicate the phrase has not received a type. The patterns that are implemented by phrase parsing rules are similar to those in Brill&apos;s transformation-based p-o-s tagger. A rule can test for the presence of a given part of speech, of a lexeme, of a list of lexemes, and so on. These tests are themselves anchored to a specific locus (a phrase or lexeme) and are performed relative to that locus. As actions, the rules can grow the boundaries of a phrase, and set or modify its label. For example, a typical name tagging rule would assign a LOCATION tag to any phrase preceded by the preposition &amp;quot;in&amp;quot;. And indeed, this very rule tends to emerge as the very first rule acquired in training a phraser-based name tagger. We show it here with no further comment, trusting that its syntax is self-evident. (def-phraser-rule :conditions (:left-1 :lex &amp;quot;in&amp;quot;) :actions (:set-label :LOCATION)) In our particular example (&amp;quot;meetings in &lt;?&gt;Paris&lt;/?&gt; and &lt;?&gt;Rome&lt;/?&gt;&amp;quot;), this rule would re-label the &lt;?&gt; phrase around Paris with the LOCATION tag. A subsequent rule might then exploit the coordination to infer that &amp;quot;Rome&amp;quot; is a location as well, implementing the transformation &amp;quot;LOCATION and &lt;?&gt;&amp;quot; —&gt; &amp;quot;LOCATION and LOCATION&amp;quot;. This incremental patching of errors is the hallmark of Brill&apos;s approach. An interesting property of this rule language is that the phraser can be operated either as a trainable procedure, using standard errordriven transformation learning, or as a handengineered system. For the purpose of the common CoNLL task, let us first present our results for the trainable case. We again approached the task in a relatively rudimentary way, in this case by applying the phrase rule learning procedure with no particular adaptation to the task. Indeed, the procedure can be parameterized by word lists which it can then exploit to improve its performance. Since our main interest here was to see our baseline performance on the task, we did not harvest such word lists from the training data (there is an automated way to do this). We ran a number of training runs based on different partitions of the training data, with the following overall performance on test data, averaged across runs. accuracy precision recall 89 89 83 86 161 The constituents that were most accurately recognized were noun groups (F=88), with verb groups a close second (F=87). These were followed by the ostensibly easy cases of PP&apos;s (F=86), SBAR&apos;s (F=79), and ADVF&amp;quot;s (F=75). Our lowest performing constituent for which the learning procedure actually generated rules was ADJP&apos;s (F=37), with no rules generated to identify CONJP&apos;s, INTJ&apos;s, LST&apos;s, or PRT&apos;s (F=0 in all these cases). In general, precision, tended to be several points of F higher than recall, and in the case of ADJP&apos;s average precision was 76 compared to average recall of 24! 3 Chunking with the Hand-Engineered System As a point of comparison, we also applied our hand-engineered chunker to the CoNLL task. We expected that it would not perform at its best on this task, since it was designed with a significantly different model of chunking in mind, and indeed, unmodified, it produced disappointing results: accuracy precision recall F;=1 84 80 75 77 The magnitude of our error term was something of a surprise. With production runs standard newswire stories (several words in lengths) the chunker typically produces fewer errors per story than one can count on one hand. The discrepancy with the results measured on the CoNLL task is of course due to the fact that our manually engineered parser was designed to produce chunks to a different standard. The standard was carefully defined so as to be maximally informative to downstream processing. Generally speaking, this means that it tends to make distinctions that are not made in the CoNLL data, e.g., splitting verbal runs such as &amp;quot;failed to realize&amp;quot; into individual verb groups when more than one event is denoted. Our curiosity about these discrepancies is now piqued. As a point of further investigation, we intend to apply the phraser&apos;s training procedure to adapt the manual chunker to the CoNLL task. With transformation-based rule sequences, this is easy to do: one merely trains the procedure to transform the output required test data precision recall F0=1 ADJP 75.89% 24.43% 36.96 ADVP 80.64% 70.21% 75.06 CONJP 0.00% 0.00% 0.00 INTJ 0.00% 0.00% 0.00 LST 0.00% 0.00% 0.00 NP 87.85% 87.77% 87.81 PP 91.77% 80.42% 85.72 PRT 0.00% 0.00% 0.00 SBAR 91.36% 69.16% 78.72 VP 90.34% 84.13% 87.13 all 88.82% 82.91% 85.76 Table 2: The results of the phrase rule parser. for the one task into that required for the other. The rules acquired in this way are then simply tacked on to the end of the original rule sequence (a half dozen such rules written by hand bring the performance of the chunker up to F=82, for example). A more interesting point of investigation, however, would be to analyze the discrepancies between current chunk standards from the standpoint of syntactic and semantic criteria. We look forward to reporting on this at some future point.</abstract>
<note confidence="0.921539777777778">References J. Aberdeen, J. Burger, D. Day, L. Hirschman, P. Robinson, and M. Vilain. 1995. Mitre: Description of the alembic system used for muc-6. 6th Message Understanding Conference Advanced Research Projects Agency, November. Brill. 1993. Corpus-based Approach to Lan- Learning. thesis, U. Pennsylvania.</note>
<author confidence="0.461148">T Briscoe</author>
<author confidence="0.461148">N Calzolari</author>
<author confidence="0.461148">S Federici</author>
<author confidence="0.461148">S Montemagni</author>
<author confidence="0.461148">V Pirrelli</author>
<author confidence="0.461148">G Grefen-</author>
<abstract confidence="0.961554125">stette, A. Sanfilippo, G. Carroll, and M. Rooth. package 1, specificaof parsing, final report. Availat //www. ilc . pi . cnr. it/sparkle/- .htm, L. Ramshaw and M. Marcus. 1995. Text chunking transformation-based learning. In of 3rd Workshop on Very Large Corpora,</abstract>
<address confidence="0.991853">82-94, Cambridge, MA, USA.</address>
<note confidence="0.591598">M. Vilain and D. Day. 1996. Finite-state phrase by rule sequences. In of the 16th Intl. Conference on Computational Linguistics (COLING-96). 162</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Aberdeen</author>
<author>J Burger</author>
<author>D Day</author>
<author>L Hirschman</author>
<author>P Robinson</author>
<author>M Vilain</author>
</authors>
<title>Mitre: Description of the alembic system used for muc-6.</title>
<date>1995</date>
<booktitle>In Proc. 6th Message Understanding Conference (MUC-6). Defense Advanced Research Projects Agency,</booktitle>
<marker>Aberdeen, Burger, Day, Hirschman, Robinson, Vilain, 1995</marker>
<rawString>J. Aberdeen, J. Burger, D. Day, L. Hirschman, P. Robinson, and M. Vilain. 1995. Mitre: Description of the alembic system used for muc-6. In Proc. 6th Message Understanding Conference (MUC-6). Defense Advanced Research Projects Agency, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Brill</author>
</authors>
<title>A Corpus-based Approach to Language Learning.</title>
<date>1993</date>
<tech>Ph.D. thesis, U. Pennsylvania.</tech>
<contexts>
<context position="1787" citStr="Brill, 1993" startWordPosition="289" endWordPosition="290">s the performance of a trainable rule-based system, the Alembic phrase rule parser. As a point of comparison, we also include a third set of measures produced by running the standard Alembic chunker on the common task with little or no adaptation. 1 Chunking as Tagging For this first experiment, we coerced our partof-speech tagger to generate chunk labels. We did so in what can only count as the most rudimentary way: by training the tagger to map part-of-speech labels to the chunk labels of the common task. The learning procedure is a reimplementation of Brill&apos;s transformation-based approach (Brill, 1993), extended to cover approximately an order of magnitude more rule schemata. As input, the training corpus was tagged with the parts-of-speech from the comN accuracy precision recall Fo=1 1000 86 77 77 77 2000 89 82 82 82 4000 89 81 81 81 Table 1: Performance of the brute-force retagging approach mon data set: these provided an initial labeling of the data which was then directly converted to chunk labels through the action of transformation rules (Brill&apos;s so-called contextual rules). Because the learning procedure is none the swiftest, we restricted ourselves to subsets of the training data, a</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>E. Brill. 1993. A Corpus-based Approach to Language Learning. Ph.D. thesis, U. Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Carroll</author>
<author>T Briscoe</author>
<author>N Calzolari</author>
<author>S Federici</author>
<author>S Montemagni</author>
<author>V Pirrelli</author>
<author>G Grefenstette</author>
<author>A Sanfilippo</author>
<author>G Carroll</author>
<author>M Rooth</author>
</authors>
<title>SPARKLE work package 1, specification of phrasal parsing, final report. Available at http: //www. ilc . pi . cnr. it/sparkle/-sparkle .htm,</title>
<date>1997</date>
<contexts>
<context position="772" citStr="Carroll et al., 1997" startWordPosition="117" endWordPosition="120">Shared CoNLL Task Marc Vilain and David Day The MITRE Corporation Bedford, MA 01730, USA daylOmitre.org For several years, chunking has been an integral part of MITRE&apos;s approach to information extraction. Our work exploits chunking in two principal ways. First, as part of our extraction system (Alembic) (Aberdeen et al., 1995), the chunker delineates descriptor phrases for entity extraction. Second, as part of our ongoing research in parsing, chunks provide the first level of a stratified approach to syntax — the second level is defined by grammatical relations, much as in the SPARKLE effort (Carroll et al., 1997). Because of our ongoing work with chunking, we were naturally interested in evaluating our approach on the common CoNLL task. In this note, we thus present three different evaluations of our work on phrase-level parsing. The first is a baseline of sorts, our own version of the &amp;quot;chunking as tagging&amp;quot; approach introduced by Ramshaw and Marcus (Ramshaw and Marcus, 1995). The second set of results reports the performance of a trainable rule-based system, the Alembic phrase rule parser. As a point of comparison, we also include a third set of measures produced by running the standard Alembic chunke</context>
</contexts>
<marker>Carroll, Briscoe, Calzolari, Federici, Montemagni, Pirrelli, Grefenstette, Sanfilippo, Carroll, Rooth, 1997</marker>
<rawString>J. Carroll, T. Briscoe, N. Calzolari, S. Federici, S. Montemagni, V. Pirrelli, G. Grefenstette, A. Sanfilippo, G. Carroll, and M. Rooth. 1997. SPARKLE work package 1, specification of phrasal parsing, final report. Available at http: //www. ilc . pi . cnr. it/sparkle/-sparkle .htm, November.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Ramshaw</author>
<author>M Marcus</author>
</authors>
<title>Text chunking using transformation-based learning.</title>
<date>1995</date>
<booktitle>In Proc. of the 3rd Workshop on Very Large Corpora,</booktitle>
<pages>82--94</pages>
<location>Cambridge, MA, USA.</location>
<contexts>
<context position="1141" citStr="Ramshaw and Marcus, 1995" startWordPosition="177" endWordPosition="180">hrases for entity extraction. Second, as part of our ongoing research in parsing, chunks provide the first level of a stratified approach to syntax — the second level is defined by grammatical relations, much as in the SPARKLE effort (Carroll et al., 1997). Because of our ongoing work with chunking, we were naturally interested in evaluating our approach on the common CoNLL task. In this note, we thus present three different evaluations of our work on phrase-level parsing. The first is a baseline of sorts, our own version of the &amp;quot;chunking as tagging&amp;quot; approach introduced by Ramshaw and Marcus (Ramshaw and Marcus, 1995). The second set of results reports the performance of a trainable rule-based system, the Alembic phrase rule parser. As a point of comparison, we also include a third set of measures produced by running the standard Alembic chunker on the common task with little or no adaptation. 1 Chunking as Tagging For this first experiment, we coerced our partof-speech tagger to generate chunk labels. We did so in what can only count as the most rudimentary way: by training the tagger to map part-of-speech labels to the chunk labels of the common task. The learning procedure is a reimplementation of Brill</context>
</contexts>
<marker>Ramshaw, Marcus, 1995</marker>
<rawString>L. Ramshaw and M. Marcus. 1995. Text chunking using transformation-based learning. In Proc. of the 3rd Workshop on Very Large Corpora, pages 82-94, Cambridge, MA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Vilain</author>
<author>D Day</author>
</authors>
<title>Finite-state phrase parsing by rule sequences.</title>
<date>1996</date>
<booktitle>In Proceedings of the 16th Intl. Conference on Computational Linguistics (COLING-96).</booktitle>
<contexts>
<context position="3779" citStr="Vilain and Day, 1996" startWordPosition="621" endWordPosition="624">literate the part-of-speech of a lexeme in the process of tagging it with a chunk label. Indeed, in our experiments, the learning procedure acquired transformations that simply replaced the part-of-speech tag with a chunking tag, thereby inhibiting potentially useful downstream rules for accessing the part-of-speech information of 160 a chunk-tagged word. 2 Chunking with the Phrase Rule Parser Our main interest in this common evaluation, however, was not to set new high-water marks with the approach of Ramshaw and Marcus, but to exercise our phrase rule parser. The Alembic phrase rule parser (Vilain and Day, 1996) provides the core of the system&apos;s syntactic processing. In our extraction applications, the phraser (as we call it) initially tags named entities and other fixed-class constructs (like titles). The phraser also treats as atomic units the stereotypical combinations of named entities that one finds in newswire text, e.g., the person-title-organization apposition &amp;quot;U.N. secretary general Kofi Anan&amp;quot;. The three components of the apposition are initially parsed as fixed-class entities, and are then combined to form a single person-denoting phrase. These preliminary parsing steps provide part of the </context>
</contexts>
<marker>Vilain, Day, 1996</marker>
<rawString>M. Vilain and D. Day. 1996. Finite-state phrase parsing by rule sequences. In Proceedings of the 16th Intl. Conference on Computational Linguistics (COLING-96).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>