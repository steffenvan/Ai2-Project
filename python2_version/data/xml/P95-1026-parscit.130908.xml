<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<sectionHeader confidence="0.7271935" genericHeader="abstract">
UNSUPERVISED WORD SENSE DISAMBIGUATION
RIVALING SUPERVISED METHODS
</sectionHeader>
<author confidence="0.988293">
David Yarowsky
</author>
<affiliation confidence="0.9989295">
Department of Computer and Information Science
University of Pennsylvania
</affiliation>
<address confidence="0.508931">
Philadelphia, PA 19104, USA
</address>
<email confidence="0.957462">
yarowskyOunagi.cis.upenn.edu
</email>
<sectionHeader confidence="0.994414" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999934090909091">
This paper presents an unsupervised learn-
ing algorithm for sense disambiguation
that, when trained on unannotated English
text, rivals the performance of supervised
techniques that require time-consuming
hand annotations. The algorithm is based
on two powerful constraints — that words
tend to have one sense per discourse and
one sense per collocation — exploited in an
iterative bootstrapping procedure. Tested
accuracy exceeds 96%.
</bodyText>
<sectionHeader confidence="0.998801" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999864">
This paper presents an unsupervised algorithm that
can accurately disambiguate word senses in a large,
completely untagged corpus.1 The algorithm avoids
the need for costly hand-tagged training data by ex-
ploiting two powerful properties of human language:
</bodyText>
<listItem confidence="0.988504571428572">
1. One sense per collocation:2 Nearby words
provide strong and consistent clues to the sense
of a target word, conditional on relative dis-
tance, order and syntactic relationship.
2. One sense per discourse: The sense of a tar-
get word is highly consistent within any given
document.
</listItem>
<bodyText confidence="0.988441894736842">
Moreover, language is highly redundant, so that
the sense of a word is effectively overdetermined by
(1) and (2) above. The algorithm uses these prop-
erties to incrementally identify collocations for tar-
get senses of a word, given a few seed collocations
&apos;Note that the problem here is sense disambiguation:
assigning each instance of a word to established sense
definitions (such as in a dictionary). This differs from
sense induction: using distributional similarity to parti-
tion word instances into clusters that may have no rela-
tion to standard sense partitions.
&apos;Here I use the traditional dictionary definition of
collocation — &amp;quot;appearing in the same location; a juxta-
position of words&amp;quot;. No idiomatic or non-compositional
interpretation is implied.
for each sense, This procedure is robust and self-
correcting, and exhibits many strengths of super-
vised approaches, including sensitivity to word-order
information lost in earlier unsupervised algorithms.
</bodyText>
<sectionHeader confidence="0.962062" genericHeader="method">
2 One Sense Per Discourse
</sectionHeader>
<bodyText confidence="0.99992765">
The observation that words strongly tend to exhibit
only one sense in a given discourse or document was
stated and quantified in Gale, Church and Yarowsky
(1992). Yet to date, the full power of this property
has not been exploited for sense disambiguation.
The work reported here is the first to take advan-
tage of this regularity in conjunction with separate
models of local context for each word. Importantly,
I do not use one-sense-per-discourse as a hard con-
straint; it affects the classification probabilistically
and can be overridden when local evidence is strong.
In this current work, the one-sense-per-discourse
hypothesis was tested on a set of 37,232 examples
(hand-tagged over a period of 3 years), the same
data studied in the disambiguation experiments. For
these words, the table below measures the claim&apos;s
accuracy (when the word occurs more than once in
a discourse, how often it takes on the majority sense
for the discourse) and applicability (how often the
word does occur more than once in a discourse).
</bodyText>
<note confidence="0.382318">
The one-sense-per-discourse hypothesis:
</note>
<table confidence="0.982397416666667">
Word Senses Accuracy Applicblty
plant living/factory 99.8 % 72.8 %
tank vehicle/contnr 99.6 % 50.5 %
poach steal/boil 100.0 % 44.4 %
palm tree/hand 99.8 % 38.5 %
axes grid/tools 100.0 % 35.5 %
sake benefit/drink 100.0 % 33.7 %
bass fish/music 100.0 % 58.8 %
space volume/outer 99.2 % 67.7 %
motion legal/physical 99.9 % 49.8 %
crane bird/machine 100.0 % 49.1 %
Average 99.8 % 50.1 %
</table>
<bodyText confidence="0.959621">
Clearly, the claim holds with very high reliability
for these words, and may be confidently exploited
</bodyText>
<page confidence="0.998058">
189
</page>
<bodyText confidence="0.971347">
as another source of evidence in sense tagging.&apos;
</bodyText>
<sectionHeader confidence="0.976418" genericHeader="method">
3 One Sense Per Collocation
</sectionHeader>
<bodyText confidence="0.986305698412698">
The strong tendency for words to exhibit only one
sense in a given collocation was observed and quan-
tified in (Yarowsky, 1993). This effect varies de-
pending on the type of collocation. It is strongest
for immediately adjacent collocations, and weakens
with distance. It is much stronger for words in a
predicate-argument relationship than for arbitrary
associations at equivalent distance. It is very much
stronger for collocations with content words than
those with function words.&apos; In general, the high reli-
ability of this behavior (in excess of 97% for adjacent
content words, for example) makes it an extremely
useful property for sense disambiguation.
A supervised algorithm based on this property is
given in (Yarowsky, 1994). Using a decision list
control structure based on (Rivest, 1987), this al-
gorithm integrates a wide diversity of potential ev-
idence sources (lemmas, inflected forms, parts of
speech and arbitrary word classes) in a wide di-
versity of positional relationships (including local
and distant collocations, trigram sequences, and
predicate-argument association). The training pro-
cedure computes the word-sense probability distri-
butions for all such collocations, and orders them by
the log-likelihood ratio Log( r4S enseAlCollocation,)\ 5
Sensealeollocatton,)),
with optional steps for interpolation and pruning.
New data are classified by using the single most
predictive piece of disambiguating evidence that ap-
pears in the target context. By not combining prob-
abilities, this decision-list approach avoids the prob-
lematic complex modeling of statistical dependencies
&apos;It is interesting to speculate on the reasons for this
phenomenon. Most of the tendency is statistical: two
distinct arbitrary terms of moderate corpus frequency
are quite unlikely to co-occur in the same discourse
whether they are homographs or not. This is particu-
larly true for content words, which exhibit a &amp;quot;bursty&amp;quot;
distribution. However, it appears that human writers
also have some active tendency to avoid mixing senses
within a discourse. In a small study, homograph pairs
were observed to co-occur roughly 5 times less often than
arbitrary word pairs of comparable frequency. Regard-
less of origin, this phenomenon is strong enough to be
of significant practical use as an additional probabilistic
disambiguation constraint.
4This latter effect is actually a continuous function
conditional on the burstiness of the word (the tendency
of a word to deviate from a constant Poisson distribution
in a corpus).
&apos;As most ratios involve a 0 for some observed value,
smoothing is crucial. The process employed here is sen-
sitive to variables including the type of collocation (ad-
jacent bigrams or wider context), collocational distance,
type of word (content word vs. function word) and the
expected amount of noise in the training data. Details
are provided in (Yarowsky, to appear).
encountered in other frameworks. The algorithm is
especially well suited for utilizing a large set of highly
non-independent evidence such as found here. In
general, the decision-list algorithm is well suited for
the task of sense disambiguation and will be used as
a component of the unsupervised algorithm below.
</bodyText>
<sectionHeader confidence="0.976639" genericHeader="method">
4 Unsupervised Learning Algorithm
</sectionHeader>
<bodyText confidence="0.985434157894737">
Words not only tend to occur in collocations that
reliably indicate their sense, they tend to occur in
multiple such collocations. This provides a mecha-
nism for bootstrapping a sense tagger. If one begins
with a small set of seed examples representative of
two senses of a word, one can incrementally aug-
ment these seed examples with additional examples
of each sense, using a combination of the one-sense-
per-collocation and one-sense-per-discourse tenden-
cies.
Although several algorithms can accomplish sim-
ilar ends,6 the following approach has the advan-
tages of simplicity and the ability to build on an
existing supervised classification algorithm without
modification.&apos; As shown empirically, it also exhibits
considerable effectiveness.
The algorithm will be illustrated by the disam-
biguation of 7538 instances of the polysemous word
plant in a previously untagged corpus.
</bodyText>
<sectionHeader confidence="0.912122" genericHeader="method">
STEP 1:
</sectionHeader>
<bodyText confidence="0.999532333333333">
In a large corpus, identify all examples of the given
polysemous word, storing their contexts as lines in
an initially untagged training set. For example:
</bodyText>
<listItem confidence="0.971535148148148">
Sense Training Examples (Keyword in Context)
? ... company said the plant is still operating
? Although thousands of plant and animal species
? ... zonal distribution of plant life . ...
? ... to strain microscopic plant life from the ...
? vinyl chloride monomer plant, which is ...
? and Golgi apparatus of plant and animal cells
? ... computer disk drive plant located in ...
? ... divide life into plant and animal kingdom
? ... close-up studies of plant life and natural
? ... Nissan car and truck plant in Japan is ...
? ... keep a manufacturing plant profitable without
? ... molecules found in plant and animal tissue
? ... union responses to plant closures . ...
? ... animal rather than plant tissues can be
? ... many dangers to plant and animal life
? company manufacturing plant is in Orlando ...
? ... growth of aquatic plant life in water ...
? automated manufacturing plant in Fremont ,
? ... Animal and plant life are delicately
? discovered at a St. Louis plant manufacturing
? computer manufacturing plant and adjacent ...
? ... the proliferation of plant and animal life
? ... ...
&apos;Including variants of the EM algorithm (Baum,
1972; Dempster et al., 1977), especially as applied in
Gale, Church and Yarowsky (1994).
</listItem>
<bodyText confidence="0.8466446">
TIndeed, any supervised classification algorithm that
returns probabilities with its classifications may poten-
tially be used here. These include Bayesian classifiers
(Mosteller and Wallace, 1964) and some implementa-
tions of neural nets, but not Brill rules (Brill, 1993).
</bodyText>
<page confidence="0.979158">
190
</page>
<figure confidence="0.986892971428571">
7 ? 7 ? 7 ? 7 7777 7 ? , ? 97 ? 7 7? 7 7 7 77 77 7 7 7
AAAA
AAAA A
A AAA AAA A AAAA AAA
AAAAA AA A AA AA 7 ? 7 7 ? ? 7 7 7 9 7 7 7 ? 7 7
7 7 7 7 7 7 7 - 7
? 7 ? 7 7 7 7
7 ?
7777 7 ? 7 7 77 7 7 7 7 7 7 , ? 7 7 7 7 7 7 7 7 9 7?
????7&amp;quot;9777 ? 779, 77;7777;7 &amp;quot;777777 77
7 7 ?? 7 7 77 77 7 7 7? 77 7 7 7 7 7 1 7? : 7 17 7 77 9 ? 9 ? ? 77 79 : 7?
7 ? 7? 777 7 ? 7 7 7 , 7 &amp;quot;7 ? ; ; 77 7 7 7 7 7 7
7 7 7 ? 77 7 ? 7 7 7 7 7 7 7 ? 7 7 7 77 ?
, 77 7 7 7,. _ 1 7 7
? 7 7 &amp;quot; 77 77797777977797 7?
? 7 1 7 7 ? 7 977797999779 7 7 ? 7 7 7
o 1
7 7 7 ? 7 7 7 7 9 7 777 7 7 7 ? 7 7 7 7 7 7 ? 7
?
777 7979 7777 79 7997; 797777777 :97777 9; ??? 7: 779 ;797:: 977:777 777:7777:: 7777? ??777??? 7;??7???
7 7 7 ? ? 7 7 97 7 77 ? 7 77 7 7977 7 ? 77 97? 99 7? 7 71 9
? ? 9 ? 71 , 7 1 7 ? 7 7 7 7 7977797777
7? 7 , 7 . 7 7 7 7 7 7 7? 7 7
? • ? , 7 79 7 7 7 77 ? 7 , 7 ? ? B B BB 7777
8 L BBB: 88 IIBB,4%
BB Bo :8 B B Bil:BBBSB
7 7 7 T 7 7
7 7 ? 7 7
77 ? ir? 77 ; 79 ? ? 7 77 7 7 ? ? ? ? 7 7
? 7 7 7 7 7 7 7777 ? 7 197 77 7
77
7 7 ? 7 77 7 7 7?
77777777 7 ? 7 7 ? 77 7 7 7
Life
STEP 2:
</figure>
<bodyText confidence="0.997161088888889">
For each possible sense of the word, identify a rel-
atively small number of training examples represen-
tative of that sense.&apos; This could be accomplished
by hand tagging a subset of the training sentences.
However, I avoid this laborious procedure by iden-
tifying a small number of seed collocations repre-
sentative of each sense and then tagging all train-
ing examples containing the seed collocates with the
seed&apos;s sense label. The remainder of the examples
(typically 85-98%) constitute an untagged residual.
Several strategies for identifying seeds that require
minimal or no human participation are discussed in
Section 5.
In the example below, the words life and manufac-
turing are used as seed collocations for the two major
senses of plant (labeled A and B respectively). This
partitions the training set into 82 examples of living
plants (1%), 106 examples of manufacturing plants
(1%), and 7350 residual examples (98%).
Sense Training Examples (Keyword in Context)
A A...
A used to strain microscopic plant life from the ...
A ... zonal distribution of plant life . ...
A close-up studies of plant life and natural ...
A too rapid growth of aquatic plant life in water ...
A ... the proliferation of plant and animal life ...
A establishment phase of the plant virus life cycle ...
A ... that divide life into plant and animal kingdom
A ... many dangers to plant and animal life ...
A mammals . Animal and plant life are delicately
A beds too salty to support plant life . River ...
heavy seas, damage , and plant life growing on ...
...
? ... vinyl chloride monomer plant, which is ...
? ... molecules found in plant and animal tissue
? ... Nissan car and truck plant in Japan is ...
? ... and Golgi apparatus of plant and animal cells ...
? ... union responses to plant closures . ...
? ... ...
? ... ...
? ... cell types found in the plant kingdom are ...
? ... company said the plant is still operating ...
? ... Although thousands of plant and animal species
? ... animal rather than plant tissues can be ...
? ... computer disk drive plant located in ...
</bodyText>
<figure confidence="0.979502090909091">
B B...
B ...
B automated manufacturing plant in Fremont ...
B ... vast manufacturing plant and distribution ...
B chemical manufacturing plant, producing viscose
B ... keep a manufacturing plant profitable without
B computer manufacturing plant and adjacent ...
B discovered at a St. Louis plant manufacturing
B ... copper manufacturing plant found that they
B copper wire manufacturing plant, for example ...
B &apos;s cement manufacturing plant in Alpena ...
</figure>
<bodyText confidence="0.932885363636364">
polystyrene manufacturing plant at its Dow ...
company manufacturing plant is in Orlando ...
It is useful to visualize the process of seed de-
velopment graphically. The following figure illus-
trates this sample initial state. Circled regions are
the training examples that contain either an A or B
seed collocate. The bulk of the sample points &amp;quot;?&amp;quot;
constitute the untagged residual.
8 For the purposes of exposition, I will assume a binary
sense partition. It is straightforward to extend this to k
senses using k sets of seeds.
</bodyText>
<figureCaption confidence="0.993424">
Figure 1: Sample Initial State
</figureCaption>
<equation confidence="0.960046">
A = SENSE-A training example
B = SENSE-B training. example
? = currently unclassified training example
</equation>
<bodyText confidence="0.997346">
= Set of training examples containing the
collocation &amp;quot;life&amp;quot;.
</bodyText>
<sectionHeader confidence="0.510431" genericHeader="method">
STEP 3a:
</sectionHeader>
<bodyText confidence="0.999833571428572">
Train the supervised classification algorithm on
the SENSE-A/SENSE-B seed sets. The decision-list al-
gorithm used here (Yarowsky, 1994) identifies other
collocations that reliably partition the seed training
data, ranked by the purity of the distribution. Be-
low is an abbreviated example of the decision list
trained on the plant seed data.9
</bodyText>
<table confidence="0.9985294">
Initial decision list for plant (abbreviated)
LogL Collocation Sense
8.10 plant life A
7.58 manufacturing plant B
7.39 life (within ±2-10 words) A
7.20 manufacturing (in ±2-10 words) B
6.27 animal (within ±2-10 words) = A
4.70 equipment (within ±2-10 words) B
4.39 employee (within ±2-10 words) B
4.30 assembly plant B
4.10 plant closure B
3.52 plant species A
3.48 automate (within ±2-10 words) B
3.45 microscopic plant A
...
</table>
<footnote confidence="0.472813125">
9Note that a given collocate such as life may appear
multiple times in the list in different collocational re-
lationships, including left-adjacent, right-adjacent, co-
occurrence at other positions in a ±k-word window and
various other syntactic associations. Different positions
often yield substantially different likelihood ratios and in
cases such as pesticide plant vs. plant pesticide indicate
entirely different classifications.
</footnote>
<page confidence="0.987953">
191
</page>
<bodyText confidence="0.949022454545455">
STEP 3h:
Apply the resulting classifier to the entire sam-
ple set. Take those members in the residual that
are tagged as SENSE-A or SENSE-B with proba-
bility above a certain threshold, and add those
examples to the growing seed sets. Using the
decision-list algorithm, these additions will contain
newly-learned collocations that are reliably indica-
tive of the previously-trained seed sets. The acquisi-
tion of additional partitioning collocations from co-
occurrence with previously-identified ones is illus-
trated in the lower portion of Figure 2.
STEP 3c:
Optionally, the one-sense-per-discourse constraint
is then used both to filter and augment this addition.
The details of this process are discussed in Section 7.
In brief, if several instances of the polysemous word
in a discourse have already been assigned SENSE-A,
this sense tag may be extended to all examples in
the discourse, conditional on the relative numbers
and the probabilities associated with the tagged ex-
amples.
</bodyText>
<table confidence="0.954570818181818">
Labeling previously untagged contexts
using the one-sense-per-discourse property
Change Disc. Training Examples (from same discourse)
in tag Numb.
A --. A 724 ... the existence of plant and animal life ...
A --. A 724 ... classified as either plant or animal ...
? ---• A 724 Although bacterial and plant cells are enclosed
A --. A 348 .. the life of the plant , producing stem
A --. A 348 ... an aspect of plant life , for example
? —. A 348 ... tissues ; because plant egg cells have
A 348 photosynthesis, and so plant growth is attuned
</table>
<bodyText confidence="0.999667666666667">
This augmentation of the training data can often
form a bridge to new collocations that may not oth-
erwise co-occur in the same nearby context with pre-
viously identified collocations. Such a bridge to the
SENSE-A collocate &amp;quot;cell&amp;quot; is illustrated graphically in
the upper half of Figure 2.
Similarly, the one-sense-per-discourse constraint
may also be used to correct erroneously labeled ex-
amples. For example:
</bodyText>
<table confidence="0.724264571428571">
Error Correction using the one-sense-per-discourse property
Change Disc. Training Examples (from same discourse)
in tag Numb.
A --. A 525 contains a varied plant and animal life
A --. A 525 the most common plant life , the ...
A -. A 525 slight within Arctic plant species ...
B —. A 525 are protected by plant parts remaining from
</table>
<sectionHeader confidence="0.643845" genericHeader="method">
STEP 3d:
</sectionHeader>
<bodyText confidence="0.9778968">
Repeat Step 3 iteratively. The training sets (e.g.
SENSE-A seeds plus newly added examples) will tend
to grow, while the residual will tend to shrink. Addi-
tional details aimed at correcting and avoiding mis-
classifications will be discussed in Section 6.
</bodyText>
<figure confidence="0.997231692307692">
? 7 77 7 777 ? 7. 7 ? 7 7 7 ? 7 ?
7. ,
7 7? 7 77 7 7 7 77 ? 7 7 7 7 ? 77 ? 7
; 7 7 7 7 7 7 7 ? 7 7 7. 7 7 7 , 7, 7.. 7 ,7 ?9
7
&apos; ? ; ? 77? 7 i7 7?` 137177 ? r Automate
7 7 79 7 9 77 7 7 77 7 7 7
977 7 7 7k,? 7? 7 7 7 7
7 7 7?? 7 77 9; 7 77 77 7
7 7 ? 7? ? 77? ; 7? 7 7 99 7 7
7 ^
7 7 7 7 ?
7 , 7 7
</figure>
<figureCaption confidence="0.950704">
Figure 2: Sample Intermediate State
(following Steps 3b and 3c)
</figureCaption>
<sectionHeader confidence="0.56738" genericHeader="method">
STEP 4:
</sectionHeader>
<bodyText confidence="0.996792363636364">
Stop. When the training parameters are held con-
stant, the algorithm will converge on a stable resid-
ual set.
Note that most training examples will exhibit mul-
tiple collocations indicative of the same sense (as il-
lustrated in Figure 3). The decision list algorithm
resolves any conflicts by using only the single most
reliable piece of evidence, not a combination of all
matching collocations. This circumvents many of
the problems associated with non-independent evi-
dence sources.
</bodyText>
<figureCaption confidence="0.995571">
Figure 3: Sample Final State
</figureCaption>
<figure confidence="0.99955945">
99 77 79
77 _
79 ? 77 ?
7 97 7 7 7 77
AAAAAA A A AAA 7 7 ? 7 7 ? 9 7
AAA A SP&apos;
7 dli ? 79
?
7? 77 A AAA AA A ? ; 7 ; 7? ? ? ?? 7 7 ? ?
7 A AAA ?
7 7 7 7 7 7 7 7.9 9 7 7 ; ,7 ; 7 7
79
&amp;quot; 7 9 7 7 7&apos; 7 7 7 7 7 7 7 7 77? 77
? 77 7 7 7 7
Cell
7 7 7 7 7 7 7 7 7 7 7 77 77 77 7 ; 7 777 7 7 7 7 7 7 77 77 7?
7 7?17
79777777 9 7 77 7 7
7 7 7 7 7 77 7 7 7 7 7 7 7 7 7 Manufacturing
„ ? 7777 7777 7 ? 7
</figure>
<page confidence="0.966141">
192
</page>
<bodyText confidence="0.983755235294118">
STEP 5:
The classification procedure learned from the final
supervised training step may now be applied to new
data, and used to annotate the original untagged
corpus with sense tags and probabilities.
An abbreviated sample of the final decision list
for plant is given below. Note that the original seed
words are no longer at the top of the list. They have
been displaced by more broadly applicable colloca-
tions that better partition the newly learned classes.
In cases where there are multiple seeds, it is even
possible for an original seed for SENSE-A to become
an indicator for SENSE-B if the collocate is more com-
patible with this second class. Thus the noise intro-
duced by a few irrelevant or misleading seed words
is not fatal. It may be corrected if the majority of
the seeds forms a coherent collocation space.
</bodyText>
<table confidence="0.9928715">
Final decision list for plant (abbreviated)
LogL Collocation Sense
10.12 plant growth A
9.68 car (within ±k words) B
9.64 plant height A
9.61 union (within ±k words) = B
9.54 equipment (within ±k words) B
9.51 assembly plant B
9.50 nuclear plant B
9.31 flower (within ±k words) A
9.24 job (within ±k words) = B
9.03 fruit (within ±k words) A
9.02 plant species A
... ...
</table>
<bodyText confidence="0.9891015">
When this decision list is applied to a new test sen-
tence,
... the loss of animal and plant species through
extinction ... ,
the highest ranking collocation found in the target
context (species) is used to classify the example as
SENSE-A (a living plant). If available, information
from other occurrences of &amp;quot;plant&amp;quot; in the discourse
may override this classification, as described in Sec-
tion 7.
</bodyText>
<sectionHeader confidence="0.999259" genericHeader="method">
5 Options for Training Seeds
</sectionHeader>
<bodyText confidence="0.9787395">
The algorithm should begin with seed words that
accurately and productively distinguish the possible
senses. Such seed words can be selected by any of
the following strategies:
</bodyText>
<listItem confidence="0.91648">
• Use words in dictionary definitions
</listItem>
<bodyText confidence="0.998885125">
Extract seed words from a dictionary&apos;s entry for
the target sense. This can be done automati-
cally, using words that occur with significantly
greater frequency in the entry relative to the
entire dictionary. Words in the entry appearing
in the most reliable collocational relationships
with the target word are given the most weight,
based on the criteria given in Yarowsky (1993).
</bodyText>
<listItem confidence="0.6335525">
• Use a single defining collocate for each
class
</listItem>
<bodyText confidence="0.979584333333333">
Remarkably good performance may be achieved
by identifying a single defining collocate for each
class (e.g. bird and machine for the word crane),
and using for seeds only those contexts contain-
ing one of these words. WordNet (Miller, 1990)
is an automatic source for such defining terms.
</bodyText>
<listItem confidence="0.973267">
• Label salient corpus collocates
</listItem>
<bodyText confidence="0.999391076923077">
Words that co-occur with the target word in
unusually great frequency, especially in certain
collocational relationships, will tend to be reli-
able indicators of one of the target word&apos;s senses
(e.g. flock and bulldozer for &amp;quot;crane&amp;quot;). A human
judge must decide which one, but this can be
done very quickly (typically under 2 minutes for
a full list of 30-60 such words). Co-occurrence
analysis selects collocates that span the space
with minimal overlap, optimizing the efforts of
the human assistant. While not fully automatic,
this approach yields rich and highly reliable seed
sets with minimal work.
</bodyText>
<sectionHeader confidence="0.9808255" genericHeader="method">
6 Escaping from Initial
Misclassifications
</sectionHeader>
<bodyText confidence="0.999956545454546">
Unlike many previous bootstrapping approaches, the
present algorithm can escape from initial misclassi-
fication. Examples added to the the growing seed
sets remain there only as long as the probability of
the classification stays above the threshold. IIf their
classification begins to waver because new examples
have discredited the crucial collocate, they are re-
turned to the residual and may later be classified dif-
ferently. Thus contexts that are added to the wrong
seed set because of a misleading word in a dictionary
definition may be (and typically are) correctly re-
classified as iterative training proceeds. The redun-
dancy of language with respect to collocation makes
the process primarily self-correcting. However, cer-
tain strong collocates may become entrenched as in-
dicators for the wrong class. We discourage such be-
havior in the training algorithm by two techniques:
1) incrementally increasing the width of the context
window after intermediate convergence (which peri-
odically adds new feature values to shake up the sys-
tem) and 2) randomly perturbing the class-inclusion
threshold, similar to simulated annealing.
</bodyText>
<sectionHeader confidence="0.7052345" genericHeader="method">
7 Using the One-sense-per-discourse
Property
</sectionHeader>
<bodyText confidence="0.999638285714286">
The algorithm performs well using only local col-
locational information, treating each token of the
target word independently. However, accuracy can
be improved by also exploiting the fact that all oc-
currences of a word in the discourse are likely to
exhibit the same sense. This property may be uti-
lized in two places, either once at the end of Step
</bodyText>
<page confidence="0.996852">
193
</page>
<table confidence="0.999917529411765">
( ) (3 4) (5 I (6) 7) 8) 9 10 11
% Seed Training Options (7) + OSPD
Samp. Major Supvsd Two Dict. Top End Each Schfitze
Word Senses Size Sense Algrtm Words Defn. Coils, only Iter. Algrthm
plant living/factory 7538 53.1 97.7 97.1 97.3 97.6 98.3 98.6 92
space volume/outer 5745 50.7 93.9 89.1 92.3 93.5 93.3 93.6 90
tank vehicle/container 11420 58.2 97.1 94.2 94.6 95.8 96.1 96.5 95
motion legal/physical 11968 57.5 98.0 93.5 97.4 97.4 97.8 97.9 92
bass fish/music 1859 56.1 97.8 96.6 97.2 97.7 98.5 98.8 -
palm tree/hand 1572 74.9 96.5 93.9 94.7 95.8 95.5 95.9 -
poach steal/boil 585 84.6 97.1 96.6 97.2 97.7 98.4 98.5 -
axes grid/tools 1344 71.8 95.5 94.0 94.3 94.7 96.8 97.0 -
duty tax/obligation 1280 50.0 93.7 90.4 92.1 93.2 93.9 94.1 -
drug medicine/narcotic 1380 50.0 93.0 90.4 91.4 92.6 93.3 93.9 -
sake benefit/drink 407 82.8 96.3 59.6 95.8 96.1 96.1 97.5 -
crane bird/machine 2145 78.0 96.6 92.3 93.6 94.2 95.4 95.5 -
AVG 3936 63.9 96.1 90.6 94.8 95.5 96.1 96.5 92.2
</table>
<bodyText confidence="0.991632119047619">
4 after the algorithm has converged, or in Step 3c
after each iteration.
At the end of Step 4, this property is used for
error correction. When a polysemous word such as
plant occurs multiple times in a discourse, tokens
that were tagged by the algorithm with low con-
fidence using local collocation information may be
overridden by the dominant tag for the discourse.
The probability differentials necessary for such a re-
classification were determined empirically in an early
pilot study. The variables in this decision are the to-
tal number of occurrences of plant in the discourse
(n), the number of occurrences assigned to the ma-
jority and minor senses for the discourse, and the
cumulative scores for both (a sum of log-likelihood
ratios). If cumulative evidence for the majority sense
exceeds that of the minority by a threshold (condi-
tional on n), the minority cases are relabeled. The
case n = 2 does not admit much reclassification be-
cause it is unclear which sense is dominant. But for
n &gt; 4, all but the most confident local classifications
tend to be overridden by the dominant tag, because
of the overwhelming strength of the one-sense-per-
discourse tendency.
The use of this property after each iteration is
similar to the final post-hoc application, but helps
prevent initially mistagged collocates from gaining a
foothold. The major difference is that in discourses
where there is substantial disagreement concerning
which is the dominant sense, all instances in the
discourse are returned to the residual rather than
merely leaving their current tags unchanged. This
helps improve the purity of the training data.
The fundamental limitation of this property is
coverage. As noted in Section 2, half of the exam-
ples occur in a discourse where there are no other
instances of the same word to provide corroborating
evidence for a sense or to protect against misclas-
sification. There is additional hope for these cases,
however, as such isolated tokens tend to strongly fa-
vor a particular sense (the less &amp;quot;bursty&amp;quot; one). We
have yet to use this additional information.
</bodyText>
<sectionHeader confidence="0.988617" genericHeader="evaluation">
8 Evaluation
</sectionHeader>
<bodyText confidence="0.999973">
The words used in this evaluation were randomly
selected from those previously studied in the litera-
ture. They include words where sense differences are
realized as differences in French translation (drug
drogue/medicament, and duty .&amp;quot;-P devoir/droit),
a verb (poach) and words used in Schiitze&apos;s 1992
disambiguation experiments (tank, space, motion,
plant).1°
The data were extracted from a 460 million word
corpus containing news articles, scientific abstracts,
spoken transcripts, and novels, and almost certainly
constitute the largest training/testing sets used in
the sense-disambiguation literature.
Columns 6-8 illustrate differences in seed training
options. Using only two words as seeds does surpris-
ingly well (90.6 %). This approach is least success-
ful for senses with a complex concept space, which
cannot be adequately represented by single words.
Using the salient words of a dictionary definition as
seeds increases the coverage of the concept space, im-
proving accuracy (94.8%). However, spurious words
in example sentences can be a source of noise. Quick
hand tagging of a list of algorithmically-identified
salient collocates appears to be worth the effort, due
to the increased accuracy (95.5%) and minimal cost.
Columns 9 and 10 illustrate the effect of adding
the probabilistic one-sense-per-discourse constraint
to collocation-based models using dictionary entries
as training seeds. Column 9 shows its effectiveness
</bodyText>
<footnote confidence="0.994221">
&amp;quot;The number of words studied has been limited here
by the highly time-consuming constraint that full hand
tagging is necessary for direct comparison with super-
vised training.
</footnote>
<page confidence="0.997841">
194
</page>
<bodyText confidence="0.999975294117647">
as a post-hoc constraint. Although apparently small
in absolute terms, on average this represents a 27%
reduction in error rate.11 When applied at each iter-
ation, this process reduces the training noise, yield-
ing the optimal observed accuracy in column 10.
Comparative performance:
Column 5 shows the relative performance of su-
pervised training using the decision list algorithm,
applied to the same data and not using any discourse
information. Unsupervised training using the addi-
tional one-sense-per-discourse constraint frequently
exceeds this value. Column 11 shows the perfor-
mance of Schiitze&apos;s unsupervised algorithm applied
to some of these words, trained on a New York Times
News Service corpus. Our algorithm exceeds this ac-
curacy on each word, with an average relative per-
formance of 97% vs. 92%.12
</bodyText>
<sectionHeader confidence="0.894859" genericHeader="evaluation">
9 Comparison with Previous Work
</sectionHeader>
<bodyText confidence="0.997488">
This algorithm exhibits a fundamental advantage
over supervised learning algorithms (including Black
(1988), Hearst (1991), Gale et al. (1992), Yarowsky
(1993, 1994), Leacock et al. (1993), Bruce and
Wiebe (1994), and Lehman (1994)), as it does not re-
quire costly hand-tagged training sets. It thrives on
raw, unannotated monolingual corpora — the more
the merrier. Although there is some hope from using
aligned bilingual corpora as training data for super-
vised algorithms (Brown et al., 1991), this approach
suffers from both the limited availability of such cor-
pora, and the frequent failure of bilingual translation
differences to model monolingual sense differences.
The use of dictionary definitions as an optional
seed for the unsupervised algorithm stems from a
long history of dictionary-based approaches, includ-
ing Lesk (1986), Guthrie et al. (1991), Veronis and
Ide (1990), and Slator (1991). Although these ear-
lier approaches have used often sophisticated mea-
sures of overlap with dictionary definitions, they
have not realized the potential for combining the rel-
atively limited seed information in such definitions
with the nearly unlimited co-occurrence information
extractable from text corpora.
Other unsupervised methods have shown great
promise. Dagan and Itai (1994) have proposed a
method using co-occurrence statistics in indepen-
dent monolingual corpora of two languages to guide
lexical choice in machine translation. Translation
of a Hebrew verb-object pair such as lahtom (sign
or seal) and hoze (contract or treaty) is determined
using the most probable combination of words in
an English monolingual corpus. This work shows
&amp;quot;The maximum possible error rate reduction is 50.1%,
or the mean applicability discussed in Section 2.
&apos;This difference is even more striking given that
Schiitze&apos;s data exhibit a higher baseline probability (65%
vs. 55%) for these words, and hence constitute an easier
task.
that leveraging bilingual lexicons and monolingual
language models can overcome the need for aligned
bilingual corpora.
Hearst (1991) proposed an early application of
bootstrapping to augment training sets for a su-
pervised sense tagger. She trained her fully super-
vised algorithm on hand-labelled sentences, applied
the result to new data and added the most con-
fidently tagged examples to the training set. Re-
grettably, this algorithm was only described in two
sentences and was not developed further. Our cur-
rent work differs by eliminating the need for hand-
labelled training data entirely and by the joint use of
collocation and discourse constraints to accomplish
this.
Schiitze (1992) has pioneered work in the hier-
archical clustering of word senses. In his disam-
biguation experiments, Schiitze used post-hoc align-
ment of clusters to word senses. Because the top-
level cluster partitions based purely on distributional
information do not necessarily align with standard
sense distinctions, he generated up to 10 sense clus-
ters and manually assigned each to a fixed sense label
(based on the hand-inspection of 10-20 sentences per
cluster). In contrast, our algorithm uses automati-
cally acquired seeds to tie the sense partitions to the
desired standard at the beginning, where it can be
most useful as an anchor and guide.
In addition, Schiitze performs his classifications
by treating documents as a large unordered bag of
words. By doing so he loses many important dis-
tinctions, such as collocational distance, word se-
quence and the existence of predicate-argument rela-
tionships between words. In contrast, our algorithm
models these properties carefully, adding consider-
able discriminating power lost in other relatively im-
poverished models of language.
</bodyText>
<sectionHeader confidence="0.995742" genericHeader="conclusions">
10 Conclusion
</sectionHeader>
<bodyText confidence="0.999977473684211">
In essence, our algorithm works by harnessing sev-
eral powerful, empirically-observed properties of lan-
guage, namely the strong tendency for words to ex-
hibit only one sense per collocation and per dis-
course. It attempts to derive maximal leverage from
these properties by modeling a rich diversity of collo-
cational relationships. It thus uses more discriminat-
ing information than available to algorithms treating
documents as bags of words, ignoring relative posi-
tion and sequence. Indeed, one of the strengths of
this work is that it is sensitive to a wider range of
language detail than typically captured in statistical
sense-disambiguation algorithms.
Also, for an unsupervised algorithm it works sur-
prisingly well, directly outperforming Schiitze&apos;s un-
supervised algorithm 96.7 % to 92.2 %, on a test
of the same 4 words. More impressively, it achieves
nearly the same performance as the supervised al-
gorithm given identical training contexts (95.5 %
</bodyText>
<page confidence="0.996884">
195
</page>
<bodyText confidence="0.9999505">
vs. 96.1 %) , and in some cases actually achieves
superior performance when using the one-sense-per-
discourse constraint (96.5 % vs. 96.1%). This would
indicate that the cost of a large sense-tagged train-
ing corpus may not be necessary to achieve accurate
word-sense disambiguation.
</bodyText>
<sectionHeader confidence="0.999562" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<reference confidence="0.968202333333333">
This work was partially supported by an NDSEG Fel-
lowship, ARPA grant N00014-904-1863 and ARO grant
DAAL 03-89-00031 PRI. The author is also affiliated
with the Information Principles Research Center AT&amp;T
Bell Laboratories, and greatly appreciates the use of its
resources in support of this work. He would like to thank
Jason Eisner, Mitch Marcus, Mark Liberman, Alison
Mackey, Dan Melamed and Lyle Ungar for their valu-
able comments.
</reference>
<sectionHeader confidence="0.960007" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999929294736842">
Baum, L.E., &amp;quot;An Inequality and Associated Maximiza-
tion Technique in Statistical Estimation of Probabilis-
tic Functions of a Markov Process,&amp;quot; Inequalities, v 3,
pp 1-8, 1972.
Black, Ezra, &amp;quot;An Experiment in Computational Discrim-
ination of English Word Senses,&amp;quot; in IBM Journal of
Research and Development, v 232, pp 185-194, 1988.
Brill, Eric, &amp;quot;A Corpus-Based Approach to Language
Learning,&amp;quot; Ph.D. Thesis, University of Pennsylvania,
1993.
Brown, Peter, Stephen Della Pietra, Vincent Della
Pietra, and Robert Mercer, &amp;quot;Word Sense Disambigua-
tion using Statistical Methods,&amp;quot; Proceedings of the
29th Annual Meeting of the Association for Compu-
tational Linguistics, pp 264-270, 1991.
Bruce, Rebecca and Janyce Wiebe, &amp;quot;Word-Sense Disam-
biguation Using Decomposable Models,&amp;quot; in Proceed-
ings of the 32nd Annual Meeting of the Association
for Computational Linguistics, Las Cruces, NM, 1994.
Church, K.W., &amp;quot;A Stochastic Parts Program an Noun
Phrase Parser for Unrestricted Text,&amp;quot; in Proceeding,
IEEE International Conference on Acoustics, Speech
and Signal Processing, Glasgow, 1989.
Dagan, Ido and Alon Itai, &amp;quot;Word Sense Disambiguation
Using a Second Language Monolingual Corpus&amp;quot;, Com-
putational Linguistics, v 20, pp 563-596, 1994.
Dempster, A.P., Laird, N.M, and Rubin, D.B., &amp;quot;Maxi-
mum Likelihood From Incomplete Data via the EM
Algorithm,&amp;quot; Journal of the Royal Statistical Society,
v 39, pp 1-38, 1977.
Gale, W., K. Church, and D. Yarowsky, &amp;quot;A Method
for Disambiguating Word Senses in a Large Corpus,&amp;quot;
Computers and the Humanities, 26, pp 415-439, 1992.
Gale, W., K. Church, and D. Yarowsky. &amp;quot;Discrimina-
tion Decisions for 100,000-Dimensional Spaces.&amp;quot; In A.
Zampoli, N. Calzolari and M. Palmer (eds.), Current
Issues in Computational Linguistics: In Honour of
Don Walker, Kluwer Academic Publishers, pp. 429-
450, 1994.
Guthrie, J., L. Guthrie, Y. Wilks and H. Aidinejad,
&amp;quot;Subject Dependent Co-occurrence and Word Sense
Disambiguation,&amp;quot; in Proceedings of the 29th Annual
Meeting of the Association for Computational Linguis-
tics, pp 146-152, 1991.
Hearst, Marti, &amp;quot;Noun Homograph Disambiguation Us-
ing Local Context in Large Text Corpora,&amp;quot; in Using
Corpora, University of Waterloo, Waterloo, Ontario,
1991.
Leacock, Claudia, Geoffrey Towell and Ellen Voorhees
&amp;quot;Corpus-Based Statistical Sense Resolution,&amp;quot; in Pro-
ceedings, ARPA Human Language Technology Work-
shop, 1993.
Lehman, Jill Fain, &amp;quot;Toward the Essential Nature of Sta-
tistical Knowledge in Sense Resolution&amp;quot;, in Proceed-
ings of the Twelfth National Conference on Artificial
Intelligence, pp 734-471, 1994.
Lesk, Michael, &amp;quot;Automatic Sense Disambiguation: How
to tell a Pine Cone from an Ice Cream Cone,&amp;quot; Pro-
ceeding of the 1986 SIGDOC Conference, Association
for Computing Machinery, New York, 1986.
Miller, George, &amp;quot;WordNet: An On-Line Lexical
Database,&amp;quot; International Journal of Lexicography, 3,
4, 1990.
Mosteller, Frederick, and David Wallace, Inference and
Disputed Authorship: The Federalist, Addison-Wesley,
Reading, Massachusetts, 1964.
Rivest, R. L., &amp;quot;Learning Decision Lists,&amp;quot; in Machine
Learning, 2, pp 229-246, 1987.
Schiitze, Hinrich, &amp;quot;Dimensions of Meaning,&amp;quot; in Proceed-
ings of Supercomputing &apos;92, 1992.
Slator, Brian, &amp;quot;Using Context for Sense Preference,&amp;quot; in
Text-Based Intelligent Systems: Current Research in
Text Analysis, Information Extraction and Retrieval,
P.S. Jacobs, ed., GE Research and Development Cen-
ter, Schenectady, New York, 1990.
Veronis, Jean and Nancy Ide, &amp;quot;Word Sense Disam-
biguation with Very Large Neural Networks Extracted
from Machine Readable Dictionaries,&amp;quot; in Proceedings,
COLING-90, pp 389-394, 1990.
Yarowsky, David &amp;quot;Word-Sense Disambiguation Using
Statistical Models of Roget&apos;s Categories Trained on
Large Corpora,&amp;quot; in Proceedings, COLING-92, Nantes,
France, 1992.
Yarowsky, David, &amp;quot;One Sense Per Collocation,&amp;quot; in Pro-
ceedings, ARPA Human Language Technology Work-
shop, Princeton, 1993.
Yarowsky, David, &amp;quot;Decision Lists for Lexical Ambigu-
ity Resolution: Application to Accent Restoration in
Spanish and French,&amp;quot; in Proceedings of the 32nd An-
nual Meeting of the Association for Computational
Linguistics, Las Cruces, NM, 1994.
Yarowsky, David. &amp;quot;Homograph Disambiguation in
Speech Synthesis.&amp;quot; In J. Hirschberg, R. Sproat and
J. van Santen (eds.), Progress in Speech Synthesis,
Springer-Verlag, to appear.
</reference>
<page confidence="0.998968">
196
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.948968">
<title confidence="0.9991">UNSUPERVISED WORD SENSE DISAMBIGUATION RIVALING SUPERVISED METHODS</title>
<author confidence="0.999973">David Yarowsky</author>
<affiliation confidence="0.9999035">Department of Computer and Information Science University of Pennsylvania</affiliation>
<address confidence="0.9989">Philadelphia, PA 19104, USA</address>
<email confidence="0.999818">yarowskyOunagi.cis.upenn.edu</email>
<abstract confidence="0.995932333333333">This paper presents an unsupervised learning algorithm for sense disambiguation that, when trained on unannotated English text, rivals the performance of supervised techniques that require time-consuming hand annotations. The algorithm is based on two powerful constraints — that words tend to have one sense per discourse and one sense per collocation — exploited in an iterative bootstrapping procedure. Tested accuracy exceeds 96%.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>Jason Eisner</author>
<author>Mitch Marcus</author>
<author>Mark Liberman</author>
<author>Alison Mackey</author>
</authors>
<title>This work was partially supported by an NDSEG Fellowship, ARPA grant N00014-904-1863 and ARO grant DAAL 03-89-00031 PRI. The author is also affiliated with the Information Principles Research Center AT&amp;T Bell Laboratories, and greatly appreciates the use of its resources in support of this work. He would like to thank</title>
<marker>Eisner, Marcus, Liberman, Mackey, </marker>
<rawString>This work was partially supported by an NDSEG Fellowship, ARPA grant N00014-904-1863 and ARO grant DAAL 03-89-00031 PRI. The author is also affiliated with the Information Principles Research Center AT&amp;T Bell Laboratories, and greatly appreciates the use of its resources in support of this work. He would like to thank Jason Eisner, Mitch Marcus, Mark Liberman, Alison Mackey, Dan Melamed and Lyle Ungar for their valuable comments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L E Baum</author>
</authors>
<title>An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of a Markov Process,&amp;quot; Inequalities,</title>
<date>1972</date>
<volume>3</volume>
<pages>1--8</pages>
<contexts>
<context position="9248" citStr="Baum, 1972" startWordPosition="1453" endWordPosition="1454">facturing plant profitable without ? ... molecules found in plant and animal tissue ? ... union responses to plant closures . ... ? ... animal rather than plant tissues can be ? ... many dangers to plant and animal life ? company manufacturing plant is in Orlando ... ? ... growth of aquatic plant life in water ... ? automated manufacturing plant in Fremont , ? ... Animal and plant life are delicately ? discovered at a St. Louis plant manufacturing ? computer manufacturing plant and adjacent ... ? ... the proliferation of plant and animal life ? ... ... &apos;Including variants of the EM algorithm (Baum, 1972; Dempster et al., 1977), especially as applied in Gale, Church and Yarowsky (1994). TIndeed, any supervised classification algorithm that returns probabilities with its classifications may potentially be used here. These include Bayesian classifiers (Mosteller and Wallace, 1964) and some implementations of neural nets, but not Brill rules (Brill, 1993). 190 7 ? 7 ? 7 ? 7 7777 7 ? , ? 97 ? 7 7? 7 7 7 77 77 7 7 7 AAAA AAAA A A AAA AAA A AAAA AAA AAAAA AA A AA AA 7 ? 7 7 ? ? 7 7 7 9 7 7 7 ? 7 7 7 7 7 7 7 7 7 - 7 ? 7 ? 7 7 7 7 7 ? 7777 7 ? 7 7 77 7 7 7 7 7 7 , ? 7 7 7 7 7 7 7 7 9 7? ????7&amp;quot;9777 ? </context>
</contexts>
<marker>Baum, 1972</marker>
<rawString>Baum, L.E., &amp;quot;An Inequality and Associated Maximization Technique in Statistical Estimation of Probabilistic Functions of a Markov Process,&amp;quot; Inequalities, v 3, pp 1-8, 1972.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ezra Black</author>
</authors>
<title>An Experiment in Computational Discrimination of English Word Senses,&amp;quot;</title>
<date>1988</date>
<journal>in IBM Journal of Research and Development,</journal>
<volume>232</volume>
<pages>185--194</pages>
<contexts>
<context position="29306" citStr="Black (1988)" startWordPosition="5125" endWordPosition="5126">raining using the decision list algorithm, applied to the same data and not using any discourse information. Unsupervised training using the additional one-sense-per-discourse constraint frequently exceeds this value. Column 11 shows the performance of Schiitze&apos;s unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus. Our algorithm exceeds this accuracy on each word, with an average relative performance of 97% vs. 92%.12 9 Comparison with Previous Work This algorithm exhibits a fundamental advantage over supervised learning algorithms (including Black (1988), Hearst (1991), Gale et al. (1992), Yarowsky (1993, 1994), Leacock et al. (1993), Bruce and Wiebe (1994), and Lehman (1994)), as it does not require costly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions a</context>
</contexts>
<marker>Black, 1988</marker>
<rawString>Black, Ezra, &amp;quot;An Experiment in Computational Discrimination of English Word Senses,&amp;quot; in IBM Journal of Research and Development, v 232, pp 185-194, 1988.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>A Corpus-Based Approach to Language Learning,&amp;quot;</title>
<date>1993</date>
<tech>Ph.D. Thesis,</tech>
<institution>University of Pennsylvania,</institution>
<contexts>
<context position="9603" citStr="Brill, 1993" startWordPosition="1504" endWordPosition="1505">emont , ? ... Animal and plant life are delicately ? discovered at a St. Louis plant manufacturing ? computer manufacturing plant and adjacent ... ? ... the proliferation of plant and animal life ? ... ... &apos;Including variants of the EM algorithm (Baum, 1972; Dempster et al., 1977), especially as applied in Gale, Church and Yarowsky (1994). TIndeed, any supervised classification algorithm that returns probabilities with its classifications may potentially be used here. These include Bayesian classifiers (Mosteller and Wallace, 1964) and some implementations of neural nets, but not Brill rules (Brill, 1993). 190 7 ? 7 ? 7 ? 7 7777 7 ? , ? 97 ? 7 7? 7 7 7 77 77 7 7 7 AAAA AAAA A A AAA AAA A AAAA AAA AAAAA AA A AA AA 7 ? 7 7 ? ? 7 7 7 9 7 7 7 ? 7 7 7 7 7 7 7 7 7 - 7 ? 7 ? 7 7 7 7 7 ? 7777 7 ? 7 7 77 7 7 7 7 7 7 , ? 7 7 7 7 7 7 7 7 9 7? ????7&amp;quot;9777 ? 779, 77;7777;7 &amp;quot;777777 77 7 7 ?? 7 7 77 77 7 7 7? 77 7 7 7 7 7 1 7? : 7 17 7 77 9 ? 9 ? ? 77 79 : 7? 7 ? 7? 777 7 ? 7 7 7 , 7 &amp;quot;7 ? ; ; 77 7 7 7 7 7 7 7 7 7 ? 77 7 ? 7 7 7 7 7 7 7 ? 7 7 7 77 ? , 77 7 7 7,. _ 1 7 7 ? 7 7 &amp;quot; 77 77797777977797 7? ? 7 1 7 7 ? 7 977797999779 7 7 ? 7 7 7 o 1 7 7 7 ? 7 7 7 7 9 7 777 7 7 7 ? 7 7 7 7 7 7 ? 7 ? 777 7979 7777 79 799</context>
</contexts>
<marker>Brill, 1993</marker>
<rawString>Brill, Eric, &amp;quot;A Corpus-Based Approach to Language Learning,&amp;quot; Ph.D. Thesis, University of Pennsylvania, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Brown</author>
<author>Stephen Della Pietra</author>
<author>Vincent Della Pietra</author>
<author>Robert Mercer</author>
</authors>
<title>Word Sense Disambiguation using Statistical Methods,&amp;quot;</title>
<date>1991</date>
<booktitle>Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>264--270</pages>
<contexts>
<context position="29693" citStr="Brown et al., 1991" startWordPosition="5186" endWordPosition="5189">m exceeds this accuracy on each word, with an average relative performance of 97% vs. 92%.12 9 Comparison with Previous Work This algorithm exhibits a fundamental advantage over supervised learning algorithms (including Black (1988), Hearst (1991), Gale et al. (1992), Yarowsky (1993, 1994), Leacock et al. (1993), Bruce and Wiebe (1994), and Lehman (1994)), as it does not require costly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including Lesk (1986), Guthrie et al. (1991), Veronis and Ide (1990), and Slator (1991). Although these earlier approaches have used often sophisticated measures of overlap with dictionary definitions, they have not realized the potential for combining the relatively limited see</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1991</marker>
<rawString>Brown, Peter, Stephen Della Pietra, Vincent Della Pietra, and Robert Mercer, &amp;quot;Word Sense Disambiguation using Statistical Methods,&amp;quot; Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pp 264-270, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca Bruce</author>
<author>Janyce Wiebe</author>
</authors>
<title>Word-Sense Disambiguation Using Decomposable Models,&amp;quot;</title>
<date>1994</date>
<booktitle>in Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Las Cruces, NM,</location>
<contexts>
<context position="29411" citStr="Bruce and Wiebe (1994)" startWordPosition="5140" endWordPosition="5143"> information. Unsupervised training using the additional one-sense-per-discourse constraint frequently exceeds this value. Column 11 shows the performance of Schiitze&apos;s unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus. Our algorithm exceeds this accuracy on each word, with an average relative performance of 97% vs. 92%.12 9 Comparison with Previous Work This algorithm exhibits a fundamental advantage over supervised learning algorithms (including Black (1988), Hearst (1991), Gale et al. (1992), Yarowsky (1993, 1994), Leacock et al. (1993), Bruce and Wiebe (1994), and Lehman (1994)), as it does not require costly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approache</context>
</contexts>
<marker>Bruce, Wiebe, 1994</marker>
<rawString>Bruce, Rebecca and Janyce Wiebe, &amp;quot;Word-Sense Disambiguation Using Decomposable Models,&amp;quot; in Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Las Cruces, NM, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K W Church</author>
</authors>
<title>A Stochastic Parts Program an Noun Phrase Parser for Unrestricted Text,&amp;quot;</title>
<date>1989</date>
<booktitle>in Proceeding, IEEE International Conference on Acoustics, Speech and Signal Processing,</booktitle>
<location>Glasgow,</location>
<marker>Church, 1989</marker>
<rawString>Church, K.W., &amp;quot;A Stochastic Parts Program an Noun Phrase Parser for Unrestricted Text,&amp;quot; in Proceeding, IEEE International Conference on Acoustics, Speech and Signal Processing, Glasgow, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Alon Itai</author>
</authors>
<title>Word Sense Disambiguation Using a Second Language Monolingual Corpus&amp;quot;,</title>
<date>1994</date>
<journal>Computational Linguistics, v</journal>
<volume>20</volume>
<pages>563--596</pages>
<contexts>
<context position="30484" citStr="Dagan and Itai (1994)" startWordPosition="5301" endWordPosition="5304">rences. The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including Lesk (1986), Guthrie et al. (1991), Veronis and Ide (1990), and Slator (1991). Although these earlier approaches have used often sophisticated measures of overlap with dictionary definitions, they have not realized the potential for combining the relatively limited seed information in such definitions with the nearly unlimited co-occurrence information extractable from text corpora. Other unsupervised methods have shown great promise. Dagan and Itai (1994) have proposed a method using co-occurrence statistics in independent monolingual corpora of two languages to guide lexical choice in machine translation. Translation of a Hebrew verb-object pair such as lahtom (sign or seal) and hoze (contract or treaty) is determined using the most probable combination of words in an English monolingual corpus. This work shows &amp;quot;The maximum possible error rate reduction is 50.1%, or the mean applicability discussed in Section 2. &apos;This difference is even more striking given that Schiitze&apos;s data exhibit a higher baseline probability (65% vs. 55%) for these word</context>
</contexts>
<marker>Dagan, Itai, 1994</marker>
<rawString>Dagan, Ido and Alon Itai, &amp;quot;Word Sense Disambiguation Using a Second Language Monolingual Corpus&amp;quot;, Computational Linguistics, v 20, pp 563-596, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dempster</author>
<author>N M Laird</author>
<author>D B Rubin</author>
</authors>
<title>Maximum Likelihood From Incomplete Data via the EM Algorithm,&amp;quot;</title>
<date>1977</date>
<journal>Journal of the Royal Statistical Society, v</journal>
<volume>39</volume>
<pages>1--38</pages>
<contexts>
<context position="9272" citStr="Dempster et al., 1977" startWordPosition="1455" endWordPosition="1458">ant profitable without ? ... molecules found in plant and animal tissue ? ... union responses to plant closures . ... ? ... animal rather than plant tissues can be ? ... many dangers to plant and animal life ? company manufacturing plant is in Orlando ... ? ... growth of aquatic plant life in water ... ? automated manufacturing plant in Fremont , ? ... Animal and plant life are delicately ? discovered at a St. Louis plant manufacturing ? computer manufacturing plant and adjacent ... ? ... the proliferation of plant and animal life ? ... ... &apos;Including variants of the EM algorithm (Baum, 1972; Dempster et al., 1977), especially as applied in Gale, Church and Yarowsky (1994). TIndeed, any supervised classification algorithm that returns probabilities with its classifications may potentially be used here. These include Bayesian classifiers (Mosteller and Wallace, 1964) and some implementations of neural nets, but not Brill rules (Brill, 1993). 190 7 ? 7 ? 7 ? 7 7777 7 ? , ? 97 ? 7 7? 7 7 7 77 77 7 7 7 AAAA AAAA A A AAA AAA A AAAA AAA AAAAA AA A AA AA 7 ? 7 7 ? ? 7 7 7 9 7 7 7 ? 7 7 7 7 7 7 7 7 7 - 7 ? 7 ? 7 7 7 7 7 ? 7777 7 ? 7 7 77 7 7 7 7 7 7 , ? 7 7 7 7 7 7 7 7 9 7? ????7&amp;quot;9777 ? 779, 77;7777;7 &amp;quot;777777 7</context>
</contexts>
<marker>Dempster, Laird, Rubin, 1977</marker>
<rawString>Dempster, A.P., Laird, N.M, and Rubin, D.B., &amp;quot;Maximum Likelihood From Incomplete Data via the EM Algorithm,&amp;quot; Journal of the Royal Statistical Society, v 39, pp 1-38, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>A Method for Disambiguating Word Senses in a Large Corpus,&amp;quot;</title>
<date>1992</date>
<journal>Computers and the Humanities,</journal>
<volume>26</volume>
<pages>415--439</pages>
<contexts>
<context position="29341" citStr="Gale et al. (1992)" startWordPosition="5129" endWordPosition="5132">st algorithm, applied to the same data and not using any discourse information. Unsupervised training using the additional one-sense-per-discourse constraint frequently exceeds this value. Column 11 shows the performance of Schiitze&apos;s unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus. Our algorithm exceeds this accuracy on each word, with an average relative performance of 97% vs. 92%.12 9 Comparison with Previous Work This algorithm exhibits a fundamental advantage over supervised learning algorithms (including Black (1988), Hearst (1991), Gale et al. (1992), Yarowsky (1993, 1994), Leacock et al. (1993), Bruce and Wiebe (1994), and Lehman (1994)), as it does not require costly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions as an optional seed for the unsuperv</context>
</contexts>
<marker>Gale, Church, Yarowsky, 1992</marker>
<rawString>Gale, W., K. Church, and D. Yarowsky, &amp;quot;A Method for Disambiguating Word Senses in a Large Corpus,&amp;quot; Computers and the Humanities, 26, pp 415-439, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Gale</author>
<author>K Church</author>
<author>D Yarowsky</author>
</authors>
<title>Discrimination Decisions for 100,000-Dimensional Spaces.&amp;quot;</title>
<date>1994</date>
<booktitle>Current Issues in Computational Linguistics: In Honour of Don Walker,</booktitle>
<pages>429--450</pages>
<editor>In A. Zampoli, N. Calzolari and M. Palmer (eds.),</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<marker>Gale, Church, Yarowsky, 1994</marker>
<rawString>Gale, W., K. Church, and D. Yarowsky. &amp;quot;Discrimination Decisions for 100,000-Dimensional Spaces.&amp;quot; In A. Zampoli, N. Calzolari and M. Palmer (eds.), Current Issues in Computational Linguistics: In Honour of Don Walker, Kluwer Academic Publishers, pp. 429-450, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Guthrie</author>
<author>L Guthrie</author>
<author>Y Wilks</author>
<author>H Aidinejad</author>
</authors>
<title>Subject Dependent Co-occurrence and Word Sense Disambiguation,&amp;quot;</title>
<date>1991</date>
<booktitle>in Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>146--152</pages>
<contexts>
<context position="30058" citStr="Guthrie et al. (1991)" startWordPosition="5240" endWordPosition="5243">t does not require costly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including Lesk (1986), Guthrie et al. (1991), Veronis and Ide (1990), and Slator (1991). Although these earlier approaches have used often sophisticated measures of overlap with dictionary definitions, they have not realized the potential for combining the relatively limited seed information in such definitions with the nearly unlimited co-occurrence information extractable from text corpora. Other unsupervised methods have shown great promise. Dagan and Itai (1994) have proposed a method using co-occurrence statistics in independent monolingual corpora of two languages to guide lexical choice in machine translation. Translation of a He</context>
</contexts>
<marker>Guthrie, Guthrie, Wilks, Aidinejad, 1991</marker>
<rawString>Guthrie, J., L. Guthrie, Y. Wilks and H. Aidinejad, &amp;quot;Subject Dependent Co-occurrence and Word Sense Disambiguation,&amp;quot; in Proceedings of the 29th Annual Meeting of the Association for Computational Linguistics, pp 146-152, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti Hearst</author>
</authors>
<title>Noun Homograph Disambiguation Using Local Context in Large Text Corpora,&amp;quot; in Using Corpora,</title>
<date>1991</date>
<institution>University of Waterloo,</institution>
<location>Waterloo, Ontario,</location>
<contexts>
<context position="29321" citStr="Hearst (1991)" startWordPosition="5127" endWordPosition="5128">the decision list algorithm, applied to the same data and not using any discourse information. Unsupervised training using the additional one-sense-per-discourse constraint frequently exceeds this value. Column 11 shows the performance of Schiitze&apos;s unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus. Our algorithm exceeds this accuracy on each word, with an average relative performance of 97% vs. 92%.12 9 Comparison with Previous Work This algorithm exhibits a fundamental advantage over supervised learning algorithms (including Black (1988), Hearst (1991), Gale et al. (1992), Yarowsky (1993, 1994), Leacock et al. (1993), Bruce and Wiebe (1994), and Lehman (1994)), as it does not require costly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions as an optional s</context>
<context position="31257" citStr="Hearst (1991)" startWordPosition="5420" endWordPosition="5421">lation of a Hebrew verb-object pair such as lahtom (sign or seal) and hoze (contract or treaty) is determined using the most probable combination of words in an English monolingual corpus. This work shows &amp;quot;The maximum possible error rate reduction is 50.1%, or the mean applicability discussed in Section 2. &apos;This difference is even more striking given that Schiitze&apos;s data exhibit a higher baseline probability (65% vs. 55%) for these words, and hence constitute an easier task. that leveraging bilingual lexicons and monolingual language models can overcome the need for aligned bilingual corpora. Hearst (1991) proposed an early application of bootstrapping to augment training sets for a supervised sense tagger. She trained her fully supervised algorithm on hand-labelled sentences, applied the result to new data and added the most confidently tagged examples to the training set. Regrettably, this algorithm was only described in two sentences and was not developed further. Our current work differs by eliminating the need for handlabelled training data entirely and by the joint use of collocation and discourse constraints to accomplish this. Schiitze (1992) has pioneered work in the hierarchical clust</context>
</contexts>
<marker>Hearst, 1991</marker>
<rawString>Hearst, Marti, &amp;quot;Noun Homograph Disambiguation Using Local Context in Large Text Corpora,&amp;quot; in Using Corpora, University of Waterloo, Waterloo, Ontario, 1991.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
</authors>
<title>Geoffrey Towell and Ellen Voorhees &amp;quot;Corpus-Based Statistical Sense Resolution,&amp;quot;</title>
<date>1993</date>
<booktitle>in Proceedings, ARPA Human Language Technology Workshop,</booktitle>
<marker>Leacock, 1993</marker>
<rawString>Leacock, Claudia, Geoffrey Towell and Ellen Voorhees &amp;quot;Corpus-Based Statistical Sense Resolution,&amp;quot; in Proceedings, ARPA Human Language Technology Workshop, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jill Fain Lehman</author>
</authors>
<title>Toward the Essential Nature of Statistical Knowledge in Sense Resolution&amp;quot;,</title>
<date>1994</date>
<booktitle>in Proceedings of the Twelfth National Conference on Artificial Intelligence,</booktitle>
<pages>734--471</pages>
<contexts>
<context position="29430" citStr="Lehman (1994)" startWordPosition="5145" endWordPosition="5146">raining using the additional one-sense-per-discourse constraint frequently exceeds this value. Column 11 shows the performance of Schiitze&apos;s unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus. Our algorithm exceeds this accuracy on each word, with an average relative performance of 97% vs. 92%.12 9 Comparison with Previous Work This algorithm exhibits a fundamental advantage over supervised learning algorithms (including Black (1988), Hearst (1991), Gale et al. (1992), Yarowsky (1993, 1994), Leacock et al. (1993), Bruce and Wiebe (1994), and Lehman (1994)), as it does not require costly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including Lesk (</context>
</contexts>
<marker>Lehman, 1994</marker>
<rawString>Lehman, Jill Fain, &amp;quot;Toward the Essential Nature of Statistical Knowledge in Sense Resolution&amp;quot;, in Proceedings of the Twelfth National Conference on Artificial Intelligence, pp 734-471, 1994.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic Sense Disambiguation: How to tell a Pine Cone from an Ice Cream Cone,&amp;quot;</title>
<date>1986</date>
<booktitle>Proceeding of the 1986 SIGDOC Conference, Association for Computing Machinery,</booktitle>
<location>New York,</location>
<contexts>
<context position="30035" citStr="Lesk (1986)" startWordPosition="5238" endWordPosition="5239">(1994)), as it does not require costly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including Lesk (1986), Guthrie et al. (1991), Veronis and Ide (1990), and Slator (1991). Although these earlier approaches have used often sophisticated measures of overlap with dictionary definitions, they have not realized the potential for combining the relatively limited seed information in such definitions with the nearly unlimited co-occurrence information extractable from text corpora. Other unsupervised methods have shown great promise. Dagan and Itai (1994) have proposed a method using co-occurrence statistics in independent monolingual corpora of two languages to guide lexical choice in machine translati</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Lesk, Michael, &amp;quot;Automatic Sense Disambiguation: How to tell a Pine Cone from an Ice Cream Cone,&amp;quot; Proceeding of the 1986 SIGDOC Conference, Association for Computing Machinery, New York, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George Miller</author>
</authors>
<title>WordNet: An On-Line Lexical Database,&amp;quot;</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<contexts>
<context position="21416" citStr="Miller, 1990" startWordPosition="3863" endWordPosition="3864">get sense. This can be done automatically, using words that occur with significantly greater frequency in the entry relative to the entire dictionary. Words in the entry appearing in the most reliable collocational relationships with the target word are given the most weight, based on the criteria given in Yarowsky (1993). • Use a single defining collocate for each class Remarkably good performance may be achieved by identifying a single defining collocate for each class (e.g. bird and machine for the word crane), and using for seeds only those contexts containing one of these words. WordNet (Miller, 1990) is an automatic source for such defining terms. • Label salient corpus collocates Words that co-occur with the target word in unusually great frequency, especially in certain collocational relationships, will tend to be reliable indicators of one of the target word&apos;s senses (e.g. flock and bulldozer for &amp;quot;crane&amp;quot;). A human judge must decide which one, but this can be done very quickly (typically under 2 minutes for a full list of 30-60 such words). Co-occurrence analysis selects collocates that span the space with minimal overlap, optimizing the efforts of the human assistant. While not fully a</context>
</contexts>
<marker>Miller, 1990</marker>
<rawString>Miller, George, &amp;quot;WordNet: An On-Line Lexical Database,&amp;quot; International Journal of Lexicography, 3, 4, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David Wallace</author>
</authors>
<title>Inference and Disputed Authorship: The Federalist,</title>
<date>1964</date>
<publisher>Addison-Wesley,</publisher>
<location>Reading, Massachusetts,</location>
<contexts>
<context position="9528" citStr="Mosteller and Wallace, 1964" startWordPosition="1489" endWordPosition="1492">o ... ? ... growth of aquatic plant life in water ... ? automated manufacturing plant in Fremont , ? ... Animal and plant life are delicately ? discovered at a St. Louis plant manufacturing ? computer manufacturing plant and adjacent ... ? ... the proliferation of plant and animal life ? ... ... &apos;Including variants of the EM algorithm (Baum, 1972; Dempster et al., 1977), especially as applied in Gale, Church and Yarowsky (1994). TIndeed, any supervised classification algorithm that returns probabilities with its classifications may potentially be used here. These include Bayesian classifiers (Mosteller and Wallace, 1964) and some implementations of neural nets, but not Brill rules (Brill, 1993). 190 7 ? 7 ? 7 ? 7 7777 7 ? , ? 97 ? 7 7? 7 7 7 77 77 7 7 7 AAAA AAAA A A AAA AAA A AAAA AAA AAAAA AA A AA AA 7 ? 7 7 ? ? 7 7 7 9 7 7 7 ? 7 7 7 7 7 7 7 7 7 - 7 ? 7 ? 7 7 7 7 7 ? 7777 7 ? 7 7 77 7 7 7 7 7 7 , ? 7 7 7 7 7 7 7 7 9 7? ????7&amp;quot;9777 ? 779, 77;7777;7 &amp;quot;777777 77 7 7 ?? 7 7 77 77 7 7 7? 77 7 7 7 7 7 1 7? : 7 17 7 77 9 ? 9 ? ? 77 79 : 7? 7 ? 7? 777 7 ? 7 7 7 , 7 &amp;quot;7 ? ; ; 77 7 7 7 7 7 7 7 7 7 ? 77 7 ? 7 7 7 7 7 7 7 ? 7 7 7 77 ? , 77 7 7 7,. _ 1 7 7 ? 7 7 &amp;quot; 77 77797777977797 7? ? 7 1 7 7 ? 7 977797999779 7 7 ? 7 7 7</context>
</contexts>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>Mosteller, Frederick, and David Wallace, Inference and Disputed Authorship: The Federalist, Addison-Wesley, Reading, Massachusetts, 1964.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R L Rivest</author>
</authors>
<title>Learning Decision Lists,&amp;quot;</title>
<date>1987</date>
<booktitle>in Machine Learning,</booktitle>
<volume>2</volume>
<pages>229--246</pages>
<contexts>
<context position="4621" citStr="Rivest, 1987" startWordPosition="716" endWordPosition="717">. It is strongest for immediately adjacent collocations, and weakens with distance. It is much stronger for words in a predicate-argument relationship than for arbitrary associations at equivalent distance. It is very much stronger for collocations with content words than those with function words.&apos; In general, the high reliability of this behavior (in excess of 97% for adjacent content words, for example) makes it an extremely useful property for sense disambiguation. A supervised algorithm based on this property is given in (Yarowsky, 1994). Using a decision list control structure based on (Rivest, 1987), this algorithm integrates a wide diversity of potential evidence sources (lemmas, inflected forms, parts of speech and arbitrary word classes) in a wide diversity of positional relationships (including local and distant collocations, trigram sequences, and predicate-argument association). The training procedure computes the word-sense probability distributions for all such collocations, and orders them by the log-likelihood ratio Log( r4S enseAlCollocation,)\ 5 Sensealeollocatton,)), with optional steps for interpolation and pruning. New data are classified by using the single most predictiv</context>
</contexts>
<marker>Rivest, 1987</marker>
<rawString>Rivest, R. L., &amp;quot;Learning Decision Lists,&amp;quot; in Machine Learning, 2, pp 229-246, 1987.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hinrich Schiitze</author>
</authors>
<title>Dimensions of Meaning,&amp;quot;</title>
<date>1992</date>
<booktitle>in Proceedings of Supercomputing &apos;92,</booktitle>
<contexts>
<context position="31812" citStr="Schiitze (1992)" startWordPosition="5509" endWordPosition="5510">ercome the need for aligned bilingual corpora. Hearst (1991) proposed an early application of bootstrapping to augment training sets for a supervised sense tagger. She trained her fully supervised algorithm on hand-labelled sentences, applied the result to new data and added the most confidently tagged examples to the training set. Regrettably, this algorithm was only described in two sentences and was not developed further. Our current work differs by eliminating the need for handlabelled training data entirely and by the joint use of collocation and discourse constraints to accomplish this. Schiitze (1992) has pioneered work in the hierarchical clustering of word senses. In his disambiguation experiments, Schiitze used post-hoc alignment of clusters to word senses. Because the toplevel cluster partitions based purely on distributional information do not necessarily align with standard sense distinctions, he generated up to 10 sense clusters and manually assigned each to a fixed sense label (based on the hand-inspection of 10-20 sentences per cluster). In contrast, our algorithm uses automatically acquired seeds to tie the sense partitions to the desired standard at the beginning, where it can b</context>
</contexts>
<marker>Schiitze, 1992</marker>
<rawString>Schiitze, Hinrich, &amp;quot;Dimensions of Meaning,&amp;quot; in Proceedings of Supercomputing &apos;92, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Slator</author>
</authors>
<title>Using Context for Sense Preference,&amp;quot; in Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction</title>
<date>1990</date>
<booktitle>GE Research and Development Center,</booktitle>
<editor>and Retrieval, P.S. Jacobs, ed.,</editor>
<location>Schenectady, New York,</location>
<marker>Slator, 1990</marker>
<rawString>Slator, Brian, &amp;quot;Using Context for Sense Preference,&amp;quot; in Text-Based Intelligent Systems: Current Research in Text Analysis, Information Extraction and Retrieval, P.S. Jacobs, ed., GE Research and Development Center, Schenectady, New York, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Veronis</author>
<author>Nancy Ide</author>
</authors>
<title>Word Sense Disambiguation with Very Large Neural Networks Extracted from Machine Readable Dictionaries,&amp;quot;</title>
<date>1990</date>
<booktitle>in Proceedings, COLING-90,</booktitle>
<pages>389--394</pages>
<contexts>
<context position="30082" citStr="Veronis and Ide (1990)" startWordPosition="5244" endWordPosition="5247">ly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions as an optional seed for the unsupervised algorithm stems from a long history of dictionary-based approaches, including Lesk (1986), Guthrie et al. (1991), Veronis and Ide (1990), and Slator (1991). Although these earlier approaches have used often sophisticated measures of overlap with dictionary definitions, they have not realized the potential for combining the relatively limited seed information in such definitions with the nearly unlimited co-occurrence information extractable from text corpora. Other unsupervised methods have shown great promise. Dagan and Itai (1994) have proposed a method using co-occurrence statistics in independent monolingual corpora of two languages to guide lexical choice in machine translation. Translation of a Hebrew verb-object pair su</context>
</contexts>
<marker>Veronis, Ide, 1990</marker>
<rawString>Veronis, Jean and Nancy Ide, &amp;quot;Word Sense Disambiguation with Very Large Neural Networks Extracted from Machine Readable Dictionaries,&amp;quot; in Proceedings, COLING-90, pp 389-394, 1990.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Word-Sense Disambiguation Using Statistical Models of Roget&apos;s Categories Trained on Large Corpora,&amp;quot;</title>
<date>1992</date>
<booktitle>in Proceedings, COLING-92,</booktitle>
<location>Nantes, France,</location>
<contexts>
<context position="2355" citStr="Yarowsky (1992)" startWordPosition="348" endWordPosition="349">ation to standard sense partitions. &apos;Here I use the traditional dictionary definition of collocation — &amp;quot;appearing in the same location; a juxtaposition of words&amp;quot;. No idiomatic or non-compositional interpretation is implied. for each sense, This procedure is robust and selfcorrecting, and exhibits many strengths of supervised approaches, including sensitivity to word-order information lost in earlier unsupervised algorithms. 2 One Sense Per Discourse The observation that words strongly tend to exhibit only one sense in a given discourse or document was stated and quantified in Gale, Church and Yarowsky (1992). Yet to date, the full power of this property has not been exploited for sense disambiguation. The work reported here is the first to take advantage of this regularity in conjunction with separate models of local context for each word. Importantly, I do not use one-sense-per-discourse as a hard constraint; it affects the classification probabilistically and can be overridden when local evidence is strong. In this current work, the one-sense-per-discourse hypothesis was tested on a set of 37,232 examples (hand-tagged over a period of 3 years), the same data studied in the disambiguation experi</context>
</contexts>
<marker>Yarowsky, 1992</marker>
<rawString>Yarowsky, David &amp;quot;Word-Sense Disambiguation Using Statistical Models of Roget&apos;s Categories Trained on Large Corpora,&amp;quot; in Proceedings, COLING-92, Nantes, France, 1992.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>One Sense Per Collocation,&amp;quot;</title>
<date>1993</date>
<booktitle>in Proceedings, ARPA Human Language Technology Workshop,</booktitle>
<location>Princeton,</location>
<contexts>
<context position="3951" citStr="Yarowsky, 1993" startWordPosition="613" endWordPosition="614">le/contnr 99.6 % 50.5 % poach steal/boil 100.0 % 44.4 % palm tree/hand 99.8 % 38.5 % axes grid/tools 100.0 % 35.5 % sake benefit/drink 100.0 % 33.7 % bass fish/music 100.0 % 58.8 % space volume/outer 99.2 % 67.7 % motion legal/physical 99.9 % 49.8 % crane bird/machine 100.0 % 49.1 % Average 99.8 % 50.1 % Clearly, the claim holds with very high reliability for these words, and may be confidently exploited 189 as another source of evidence in sense tagging.&apos; 3 One Sense Per Collocation The strong tendency for words to exhibit only one sense in a given collocation was observed and quantified in (Yarowsky, 1993). This effect varies depending on the type of collocation. It is strongest for immediately adjacent collocations, and weakens with distance. It is much stronger for words in a predicate-argument relationship than for arbitrary associations at equivalent distance. It is very much stronger for collocations with content words than those with function words.&apos; In general, the high reliability of this behavior (in excess of 97% for adjacent content words, for example) makes it an extremely useful property for sense disambiguation. A supervised algorithm based on this property is given in (Yarowsky, </context>
<context position="21126" citStr="Yarowsky (1993)" startWordPosition="3815" endWordPosition="3816">s for Training Seeds The algorithm should begin with seed words that accurately and productively distinguish the possible senses. Such seed words can be selected by any of the following strategies: • Use words in dictionary definitions Extract seed words from a dictionary&apos;s entry for the target sense. This can be done automatically, using words that occur with significantly greater frequency in the entry relative to the entire dictionary. Words in the entry appearing in the most reliable collocational relationships with the target word are given the most weight, based on the criteria given in Yarowsky (1993). • Use a single defining collocate for each class Remarkably good performance may be achieved by identifying a single defining collocate for each class (e.g. bird and machine for the word crane), and using for seeds only those contexts containing one of these words. WordNet (Miller, 1990) is an automatic source for such defining terms. • Label salient corpus collocates Words that co-occur with the target word in unusually great frequency, especially in certain collocational relationships, will tend to be reliable indicators of one of the target word&apos;s senses (e.g. flock and bulldozer for &amp;quot;cra</context>
<context position="29357" citStr="Yarowsky (1993" startWordPosition="5133" endWordPosition="5134">d to the same data and not using any discourse information. Unsupervised training using the additional one-sense-per-discourse constraint frequently exceeds this value. Column 11 shows the performance of Schiitze&apos;s unsupervised algorithm applied to some of these words, trained on a New York Times News Service corpus. Our algorithm exceeds this accuracy on each word, with an average relative performance of 97% vs. 92%.12 9 Comparison with Previous Work This algorithm exhibits a fundamental advantage over supervised learning algorithms (including Black (1988), Hearst (1991), Gale et al. (1992), Yarowsky (1993, 1994), Leacock et al. (1993), Bruce and Wiebe (1994), and Lehman (1994)), as it does not require costly hand-tagged training sets. It thrives on raw, unannotated monolingual corpora — the more the merrier. Although there is some hope from using aligned bilingual corpora as training data for supervised algorithms (Brown et al., 1991), this approach suffers from both the limited availability of such corpora, and the frequent failure of bilingual translation differences to model monolingual sense differences. The use of dictionary definitions as an optional seed for the unsupervised algorithm s</context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>Yarowsky, David, &amp;quot;One Sense Per Collocation,&amp;quot; in Proceedings, ARPA Human Language Technology Workshop, Princeton, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French,&amp;quot;</title>
<date>1994</date>
<booktitle>in Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics,</booktitle>
<location>Las Cruces, NM,</location>
<contexts>
<context position="4556" citStr="Yarowsky, 1994" startWordPosition="706" endWordPosition="707">sky, 1993). This effect varies depending on the type of collocation. It is strongest for immediately adjacent collocations, and weakens with distance. It is much stronger for words in a predicate-argument relationship than for arbitrary associations at equivalent distance. It is very much stronger for collocations with content words than those with function words.&apos; In general, the high reliability of this behavior (in excess of 97% for adjacent content words, for example) makes it an extremely useful property for sense disambiguation. A supervised algorithm based on this property is given in (Yarowsky, 1994). Using a decision list control structure based on (Rivest, 1987), this algorithm integrates a wide diversity of potential evidence sources (lemmas, inflected forms, parts of speech and arbitrary word classes) in a wide diversity of positional relationships (including local and distant collocations, trigram sequences, and predicate-argument association). The training procedure computes the word-sense probability distributions for all such collocations, and orders them by the log-likelihood ratio Log( r4S enseAlCollocation,)\ 5 Sensealeollocatton,)), with optional steps for interpolation and pr</context>
<context position="9331" citStr="Yarowsky (1994)" startWordPosition="1466" endWordPosition="1467">issue ? ... union responses to plant closures . ... ? ... animal rather than plant tissues can be ? ... many dangers to plant and animal life ? company manufacturing plant is in Orlando ... ? ... growth of aquatic plant life in water ... ? automated manufacturing plant in Fremont , ? ... Animal and plant life are delicately ? discovered at a St. Louis plant manufacturing ? computer manufacturing plant and adjacent ... ? ... the proliferation of plant and animal life ? ... ... &apos;Including variants of the EM algorithm (Baum, 1972; Dempster et al., 1977), especially as applied in Gale, Church and Yarowsky (1994). TIndeed, any supervised classification algorithm that returns probabilities with its classifications may potentially be used here. These include Bayesian classifiers (Mosteller and Wallace, 1964) and some implementations of neural nets, but not Brill rules (Brill, 1993). 190 7 ? 7 ? 7 ? 7 7777 7 ? , ? 97 ? 7 7? 7 7 7 77 77 7 7 7 AAAA AAAA A A AAA AAA A AAAA AAA AAAAA AA A AA AA 7 ? 7 7 ? ? 7 7 7 9 7 7 7 ? 7 7 7 7 7 7 7 7 7 - 7 ? 7 ? 7 7 7 7 7 ? 7777 7 ? 7 7 77 7 7 7 7 7 7 , ? 7 7 7 7 7 7 7 7 9 7? ????7&amp;quot;9777 ? 779, 77;7777;7 &amp;quot;777777 77 7 7 ?? 7 7 77 77 7 7 7? 77 7 7 7 7 7 1 7? : 7 17 7 77 9 ?</context>
<context position="14049" citStr="Yarowsky, 1994" startWordPosition="2482" endWordPosition="2483">aining examples that contain either an A or B seed collocate. The bulk of the sample points &amp;quot;?&amp;quot; constitute the untagged residual. 8 For the purposes of exposition, I will assume a binary sense partition. It is straightforward to extend this to k senses using k sets of seeds. Figure 1: Sample Initial State A = SENSE-A training example B = SENSE-B training. example ? = currently unclassified training example = Set of training examples containing the collocation &amp;quot;life&amp;quot;. STEP 3a: Train the supervised classification algorithm on the SENSE-A/SENSE-B seed sets. The decision-list algorithm used here (Yarowsky, 1994) identifies other collocations that reliably partition the seed training data, ranked by the purity of the distribution. Below is an abbreviated example of the decision list trained on the plant seed data.9 Initial decision list for plant (abbreviated) LogL Collocation Sense 8.10 plant life A 7.58 manufacturing plant B 7.39 life (within ±2-10 words) A 7.20 manufacturing (in ±2-10 words) B 6.27 animal (within ±2-10 words) = A 4.70 equipment (within ±2-10 words) B 4.39 employee (within ±2-10 words) B 4.30 assembly plant B 4.10 plant closure B 3.52 plant species A 3.48 automate (within ±2-10 word</context>
</contexts>
<marker>Yarowsky, 1994</marker>
<rawString>Yarowsky, David, &amp;quot;Decision Lists for Lexical Ambiguity Resolution: Application to Accent Restoration in Spanish and French,&amp;quot; in Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, Las Cruces, NM, 1994.</rawString>
</citation>
<citation valid="false">
<authors>
<author>David Yarowsky</author>
</authors>
<title>Homograph Disambiguation in Speech Synthesis.&amp;quot;</title>
<booktitle>Progress in Speech Synthesis,</booktitle>
<editor>In J. Hirschberg, R. Sproat and J. van Santen (eds.),</editor>
<publisher>Springer-Verlag,</publisher>
<note>to appear.</note>
<marker>Yarowsky, </marker>
<rawString>Yarowsky, David. &amp;quot;Homograph Disambiguation in Speech Synthesis.&amp;quot; In J. Hirschberg, R. Sproat and J. van Santen (eds.), Progress in Speech Synthesis, Springer-Verlag, to appear.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>