<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.018217">
<title confidence="0.98568">
TwitterHawk: A Feature Bucket Approach to Sentiment Analysis
</title>
<author confidence="0.999887">
William Boag, Peter Potash, Anna Rumshisky
</author>
<affiliation confidence="0.885785333333333">
Dept. of Computer Science
University of Massachusetts Lowell
198 Riverside St, Lowell, MA 01854, USA
</affiliation>
<email confidence="0.998059">
{wboag,ppotash,arum}@cs.uml.edu
</email>
<sectionHeader confidence="0.995629" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953590909091">
This paper describes TwitterHawk, a system
for sentiment analysis of tweets which partici-
pated in the SemEval-2015 Task 10, Subtasks
A through D. The system performed com-
petitively, most notably placing 1s1 in topic-
based sentiment classification (Subtask C) and
ranking 41h out of 40 in identifying the sen-
timent of sarcastic tweets. Our submissions
in all four subtasks used a supervised learning
approach to perform three-way classification
to assign positive, negative, or neutral labels.
Our system development efforts focused on
text pre-processing and feature engineering,
with a particular focus on handling negation,
integrating sentiment lexicons, parsing hash-
tags, and handling expressive word modifica-
tions and emoticons. Two separate classifiers
were developed for phrase-level and tweet-
level sentiment classification. Our success in
aforementioned tasks came in part from lever-
aging the Subtask B data and building a single
tweet-level classifier for Subtasks B, C and D.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998953844444445">
In recent years, microblogging has developed into a
resource for quickly and easily gathering data about
how people feel about different topics. Sites such as
Twitter allow for real-time communication of senti-
ment, thus providing unprecedented insight into how
well-received products, events, and people are in the
public’s eye. But working with this new genre is
challenging. Twitter imposes a 140-character limit
on messages, which causes users to use novel abbre-
viations and often disregard standard sentence struc-
tures.
For the past three years, the International Work-
shop on Semantic Evaluation (SemEval) has been
hosting a task dedicated to sentiment analysis of
Twitter data. This year, our team participated in
four subtasks of the challenge: Contextual Polarity
Disambiguation (phrase-level), B: Message Polarity
Classification (tweet-level), C: Topic-Based Mes-
sage Polarity Classification (topic-based), and D:
Detecting Trends Towards a Topic (trending senti-
ment). For a more thorough description of the tasks,
see Rosenthal et al. (2015). Our system placed 1st
out of 7 submissions for topic-based sentiment pre-
diction (Subtask C), 3rd out of 6 submissions for de-
tecting trends toward a topic (Subtask D), 10th out
of 40 submissions for tweet-level sentiment predic-
tion (Subtask B), and 5th out of 11 for phrase-level
prediction (Subtask A). Our system also ranked 4th
out of 40 submissions in identifying the sentiment of
sarcastic tweets.
Most systems that participated in this task over the
past two years have relied on basic machine learn-
ing classifiers with a strong focus on developing ro-
bust and comprehensive feature set. The top sys-
tem for Subtask A in both 2013 and 2014 from NRC
Canada (Mohammad et al., 2013; Zhu et al., 2014)
used a simple linear SVM while putting great ef-
fort into creating and incorporating sentiment lexi-
cons as well as carefully handling negation contexts.
Other teams addressed imbalances in data distribu-
tions, but still mainly focused on feature engineer-
ing, including an improved spelling correction, POS
tagging, and word sense disambiguation (Miura et
al., 2014). The second place submission for the
2014 Task B competition also used a neural network
</bodyText>
<page confidence="0.969297">
640
</page>
<note confidence="0.9541115">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 640–646,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
</note>
<bodyText confidence="0.996297096774194">
setup to learn sentiment-specific word embedding
features along with state-of-the-art hand-crafted fea-
tures (Tang et al., 2014).
Our goal in developing TwitterHawk was to build
on the success of feature-driven approaches estab-
lished as state-of-the-art in the two previous years of
SemEval Twitter Sentiment Analysis competitions.
We therefore focused on identifying and incorporat-
ing the strongest features used by the best systems,
most notably, sentiment lexicons that showed good
performance in ablation studies. We also performed
multiple rounds of pre-processing which included
tokenization, spelling correction, hashtag segmenta-
tion, wordshape replacement of URLs, as well as
handling negated contexts. Our main insight for
Task C involved leveraging additional training data,
since the provided training data was quite small
(489 examples between training and dev). Although
not annotated with respect to a particular topic, we
found that message-level sentiment data (Subtask B)
generalized better to topic-level sentiment tracking
than span-level data (Subtask A). We therefore used
Subtask B data to train a more robust model for
topic-level sentiment detection.
The rest of this paper is organized as follows. In
Section 2, we discuss text preprocessing and nor-
malization, describe the two classifiers we created
for different subtasks, and present the features used
by each model. We report system results in Section
3, and discuss system performance and future direc-
tions in Section 4.
</bodyText>
<sectionHeader confidence="0.978285" genericHeader="method">
2 System Description
</sectionHeader>
<bodyText confidence="0.996918954545454">
We built a system to compete in four subtasks of Se-
mEval Task 10 (Rosenthal et al., 2015). Subtasks
A-C were concerned with classification of Twitter
data as either positive, negative, or neutral. Subtask
A involved phrase-level (usually 1-4 tokens) senti-
ment analysis. Subtask B dealt with classification
of the entire tweet. Subtask C involved classifying
a tweet’s sentiment towards a given topic. Subtask
D summarized the results of Subtask C by analyzing
the sentiment expressed towards a topic by a group
of tweets (as opposed to the single tweet classifica-
tion for Subtask C).
We trained two classifiers one for phrase-level
classification and one for tweet-level sentiment clas-
sification. We use the phrase-level classifier for Sub-
task A and we use the tweet-level classifier for Sub-
tasks B and C. Subtask D did not require a sepa-
rate classifier since it effectively just summarized
the output of Subtask C. We experimented to deter-
mine whether data from Subtasks A or B generalized
for C, and we found that the Subtask B model per-
formed best at predicting for Subtask C.
</bodyText>
<subsectionHeader confidence="0.99876">
2.1 Preprocessing and Normalization
</subsectionHeader>
<bodyText confidence="0.997014310344827">
Prior to feature extraction, we perform several pre-
processing steps, including tokenization, spell cor-
rection, hashtag segmentation, and normalization.
Tokenization and POS-tagging Standard word
tokenizers are trained on datasets from the Wall
Street Journal, and consequently do not perform
well on Twitter data. Some of these issues come
from shorter and ill-formed sentences, unintentional
misspellings, creative use of language, and abbrevi-
ations. We use ARK Tweet NLP toolkit for natural
language processing in social media (Owoputi et al.,
2013; Gimpel et al., 2011) for tokenization and part-
of-speech tagging. An additional tokenization pass
is used to split compound words that may have been
mis-tokenized. This includes splitting hyphenated
phrases such as ‘first-place’ or punctuation that was
not detached from its leading text such as ‘horay!!!’.
Spell Correction Twitter’s informal nature and
limited character space often cause tweets to con-
tain spelling errors and abbreviations. To address
this issue, we developed a spell correction module
that corrects errors and expands abbreviations. Spell
correction is performed in two passes. The first pass
identifies the words with alternative spellings com-
mon in social media text. The second pass uses
a general-purpose spell correction package from
PyEnchant library.1
If a word w is misspelled, we check if it is one of
four special forms we define:
</bodyText>
<listItem confidence="0.984137125">
1. non-prose - w is hashtag, URL, user mention,
number, emoticon, or proper noun.
2. abbreviation - w is in our custom hand-built
list that contains abbreviations as well as some
common misspellings.
3. elongated word - w is an elongated word, such
as ‘heyyyy’. We define ‘elongated’ as repeating
the same character 3 or more times in a row.
</listItem>
<footnote confidence="0.986627">
1http://pythonhosted.org/pyenchant/
</footnote>
<page confidence="0.995403">
641
</page>
<bodyText confidence="0.98832635">
4. colloquial - w matches a regex for identifying
common online phrases such as ‘haha’ or ‘lol’.
We use a regex rather than a closed list for elon-
gated phrases where more than one character is
repeated in order. This allows, for ‘haha’ and
‘hahaha’, for example, to be normalized to the
the same form.
Non-prose forms are handled in the tweet normal-
ization phase (see sec 2.1). For abbreviations, we
look up the expanded form in our hand-crafted list.
For elongated words, we reduce all elongated sub-
strings so that the substring’s characters only occur
twice. For example, this cuts both ‘heeeeyyyy’ and
‘heeeyyyyyyyyy’ down to ‘heeyy’. Finally, collo-
quials are normalized to the shortened form (e.g.,
‘hahaha’ becomes ‘haha’). If w is not a special form,
we feed it into PyEnchant library’s candidate gener-
ation tool. We then filter out all candidates whose
edit distance is greater than 2, and select the top can-
didate from PyEnchant.
Hashtag Segmentation Hashtags are often used
in tweets to summarize the key ideas of the message.
For instance, consider the text: We’re going bowling
#WeLoveBowling. Although the text “We’re going
bowling” does not carry any sentiment on its own,
the positive sentiment of the message is expressed
by the hashtag.
Similarly to spell correction, we define a general
algorithm for hashtag segmentation, as well as sev-
eral special cases. If hashtag h is not a special form,
we reduce all characters to lowercase and then use
a greedy segmentation algorithm which scans the
hashtag from left to right, identifying the longest
matching dictionary word. We split off the first
word and repeat the process until the entire string
is scanned. The algorithm does not backtrack at a
dead end, but rather removes the leading character
and continues. We use a trie structure to insure the
efficiency of longest-prefix queries.
We identify three special cases for a hashtag h:
</bodyText>
<listItem confidence="0.973773833333333">
1. manually segmented - h is in our custom
hand-built list of hashtags not handled correctly
by the general algorithm;
2. acronym - h is all capitals;
3. camel case - h is written in CamelCase,
checked with a regex.
</listItem>
<bodyText confidence="0.999744052631579">
For hashtags that are in the manually segmented list,
we use the segmentation that we identified as cor-
rect. If h is an acronym, we do not segment it. Fi-
nally, for CamelCase, we treat the capitalization as
indicating the segment boundaries.
Normalization and Negation During the normal-
ization phrase, all tokens are lowercased. Next,
we replace URLs, user mentions, and numbers with
generic URL, USER, and NUMBER tokens, respec-
tively. The remaining tokens are stemmed using
NLTKs Snowball stemmer (Bird et al., 2009).
We also process negation contexts following the
strategy used by Pang et al. (2002). We define a
negation context to be a text span that begins with a
negation word (such as ‘no’) and ends with a punc-
tuation mark, hashtag, user mention, or URL. The
suffix neg is appended to all words inside of a nega-
tion context. We use the list of negation words from
Potts (2011).
</bodyText>
<subsectionHeader confidence="0.994543">
2.2 Machine Learning
</subsectionHeader>
<bodyText confidence="0.999794615384615">
For the phrase-level sentiment classification, we
trained a linear Support Vector Machine (SVM)
using scikit-learn’s LinearSVC (Pedregosa et al.,
2011) on the Subtask A training data, which con-
tained 4832 positive examples, 2549 negative, and
384 neutral. The regularization parameter was set to
C=0.05, using a grid search over the development
data (648 positive, 430 negative, 57 neutral). To
account for the imbalance of label distributions, we
used sklearn’s ‘auto’ class weight adjustment which
applies a weight inversely proportional to a given
class’s frequency in the training data to the numeric
prediction of each class label.
The tweet-level model was trained using scikit-
learn’s SGDClassifier with the hinge loss function
and a learning rate of 0.001. The main difference be-
tween the learning algorithms of our classifiers was
the regularization term of the loss function. While
the phrase-level classifier uses the default SVM reg-
ularization, the tweet-level classifier uses an ‘elas-
ticnet’ penalty with l1 ratio of .85. These param-
eter values were chosen following Gunther (2014)
from last year’s SemEval and verified in cross val-
idation. We also used the ‘auto’ class weight for
this task because the training label distribution was
3640 positive, 1458 negative, and 4586 neutral. We
</bodyText>
<page confidence="0.990975">
642
</page>
<table confidence="0.87815625">
used scikit-learn’s norm mat function to normalize • whether the phrase contained a word whose
the data matrix so that each column vector is nor- length is eight or more;
malized to unit length. • whether the phrase contained an elongated
2.3 Features word (cf. Section 2.1).
</table>
<bodyText confidence="0.980334384615385">
Our system used two kinds of features: basic text
features and lexicon features. We describe the two
feature classes below. There was a substantial over-
lap between the features used for the phrase-level
classifier and those used for the tweet-level classi-
fier, with some additional features used at the phrase
level.
Basic Text Features Basic text features include
the features derived from the text representation, in-
cluding token-level unigram features, hashtag seg-
mentation, character-level analysis, and wordshape
normalization. For a given text span, basic text fea-
tures included
</bodyText>
<listItem confidence="0.993080833333333">
• presence or absence of: raw bag-of-words
(BOW) unigrams, normalized/stemmed BOW
unigrams, stemmed segmented hashtag BOW,
user mentions, URLs, hashtags;
• number of question marks and number of ex-
clamation points;
• number of positive, negative, and neutral
emoticons; emoticons were extracted from the
training data and manually tagged as positive,
negative or neutral; 2
• whether the text span contained an elongated
word (see Section 2.1, special form 3).
</listItem>
<bodyText confidence="0.99920525">
The above features were derived from the anno-
tated text span in both phrase-level and tweet-level
analysis. For the phrase-level analysis, these were
supplemented with the following:
</bodyText>
<listItem confidence="0.981336111111111">
• normalized BOW unigram features derived
from 3 tokens preceding the target phrase;
• normalized BOW unigram features derived
from 3 tokens following the target phrase;
• length 2, 3, and 4 character prefixes and suf-
fixes for each token in the target phrase;
• whether the phrase was in all caps;
• whether phrase contained only stop words;
• whether a phrase contained only punctuation;
</listItem>
<footnote confidence="0.764056">
2http://text-machine.cs.uml.edu/twitterhawk/emoticons.txt
</footnote>
<bodyText confidence="0.999813971428572">
There were a few other differences in the way each
classifier handled some of the features. The phrase-
level classifier changed the feature value from 1 to 2
for elongated unigrams. In the tweet-level classifier,
we ignored unigrams with proper noun and prepo-
sition part-of-speech tags. Negation contexts were
also handled differently. For the phrase-level clas-
sifier, a negated word was treated as a separate fea-
ture, whereas for the tweet-level classifier, negation
changed the feature value from 1 to -1.
Lexicon Features We used several Twitter-
specific and general-purpose lexicons. The lexicons
fell into one of two categories: those that provided a
numeric score (usually, -5 to 5) score and those that
sorted phrases into categories. For a given lexicon,
categories could correspond to a particular emotion,
to a strong or weak positive or negative sentiment,
or to automatically derived word clusters.
We used the features derived from the following
lexicons: AFINN (Nielsen, 2011), Opinion Lexicon
(Hu and Liu, 2004), Brown Clusters (Gimpel et al.,
2011), Hashtag Emotion (Mohammad, 2012), Sen-
timent140 (Mohammad et al., 2013), Hashtag Sen-
timent (Mohammad et al., 2013), Subjectivity (Wil-
son et al., 2005), and General Inquirer (Stone et al.,
1966). Features are derived separately for each lex-
icon. General Inquirer and Hashtag Emotion were
excluded from the tweet-level analysis since they did
not improve system performance in cross-validation.
We also experimented with features derived from
WordNet (Fellbaum, 1998), but these failed to im-
prove performance for either task in ablation studies.
See Section 3.1 for ablation results.
The features for the lexicons that provided a nu-
meric score included:
</bodyText>
<listItem confidence="0.9998985">
• the average sentiment score for the text span;
• the total number of positively scored words in
the span;
• the maximum score (or zero if no words had a
sentiment score);
• the score of the last positively scored word;
</listItem>
<page confidence="0.997374">
643
</page>
<table confidence="0.99862">
Opinion Hashtag Sentiment140 Subjectivity AFINN Hashtag Brown General
Sentiment Emotion Clusters Inquirer
Phrase-level ✓ ✓ ✓ ✓ ✓ ✓
Tweet-level ✓ ✓ ✓ ✓ ✓ ✓ ✓
</table>
<tableCaption confidence="0.992069">
Table 1: Which lexicons we used for each classifier.
</tableCaption>
<table confidence="0.999921833333334">
Withholding F-score
– (Full System) 63.76
Opinion Lexicon 63.70
Hashtag Sentiment 63.49
Sentiment140 63.22
Hashtag Emotion (HE) 63.77
Brown Clusters 63.01
Subjectivity Lexicon 63.49
AFINN Lexicon 63.43
General Inquirer (GI) 63.94
WordNet (WN) 65.49
WN, GI, HE 66.38
</table>
<tableCaption confidence="0.892494">
Table 2: Ablation results for lexicons features in tweet-
level classification.
</tableCaption>
<listItem confidence="0.993322333333333">
• three most influential (most positive or most
negative) scores for the text span; this was only
used by the phrase-level system.
</listItem>
<bodyText confidence="0.999812">
The features derived from lexicons that provided
categories for words and phrases included the num-
ber of words that belonged to each category.
For phrase-level analysis, the text span used for
these features was the target phrase itself. For the
tweet-level analysis, the text span covered the whole
tweet. Table 1 shows which lexicons we used when
building each classifier.
</bodyText>
<sectionHeader confidence="0.999952" genericHeader="evaluation">
3 Results
</sectionHeader>
<bodyText confidence="0.999955714285714">
In this section, we describe the experiments we con-
ducted during system development, as well as the
official SemEval Task 10 results.
The scores reported throughout this section are
calculated as the average of the positive and nega-
tive class F-measure (Nakov et al., 2013); the neutral
label classification does not directly affect the score.
</bodyText>
<subsectionHeader confidence="0.994877">
3.1 System Development Experiments
</subsectionHeader>
<bodyText confidence="0.999925595238095">
Both phrase-level and tweet-level systems were
tuned in 10-fold cross-validation using the 2013
training, dev, and test data (Nakov et al., 2013). We
used fixed data folds in order to compare different
runs. Feature ablation studies, parameter tuning, and
comparison of different pre-processing steps were
performed using this setup.
We conducted ablation studies for lexicon fea-
tures using tweet-level evaluation. Table 2 shows
ablation results obtained in 10-fold cross-validation.
The figures are bolded if withholding the features
derived from a given lexicon produced a higher
score. Note that these experiments were conducted
using a Linear SVM classifier with a limited subset
of basic text features.
Our best cross-validation results using the con-
figuration described in sections 2.2 and 2.3 above
were 87.12 average F-measure for phrase-level anal-
ysis (Subtask A), and 68.50 for tweet-level analysis
(Subtask B).
For topic-level sentiment detection in Subtask C,
we investigated three different approaches: (1) using
our phrase-level classifier “as is”, (2) training our
phrase level classifier only on phrases that resem-
bled topics3, and (3) using our tweet-level classifier
“as is”. We found that our phrase-level classifiers
did not perform well (F-scores in the 35-38 range),
which could be explained by the fact that the Sub-
task A data was annotated so that the target phrases
actually carried sentiment (e.g., the phrase “good
luck”), whereas the Subtask C assumption was that
the topic itself had no sentiment and that the topics
context determined the expressed sentiment. For ex-
ample, in the tweet “Gotta go see Flight tomorrow
Denzel is the greatest actor ever”, positive sentiment
is carried by the phrase “the greatest actor ever”,
rather than the token “Denzel” (corresponding to the
topic). It is therefore not surprising that our tweet-
level classifier achieved an F-score of 54.90, since
tweet-level analysis is better able to capture long-
range dependencies between sentiment-carrying ex-
pressions and the target topic. Consequently, we
</bodyText>
<footnote confidence="0.991276">
3We kept the phrases comprised by 0-or-1-determiner fol-
lowed by 0-or-more-adjectives, followed by a noun.
</footnote>
<page confidence="0.992616">
644
</page>
<table confidence="0.999783">
Phrase-level Tweet-level
Live Journal 2014 83.97 70.17
SMS 2013 86.64 62.12
Twitter 2013 82.87 68.44
Twitter 2014 84.05 70.64
Twitter 2014 Sarcasm 85.62 56.02
Twitter 2015 82.32 61.99
</table>
<tableCaption confidence="0.997134">
Table 3: Official results
</tableCaption>
<bodyText confidence="0.524957">
used the tweet-level classifier in our submission for
Subtask C.
</bodyText>
<subsectionHeader confidence="0.999517">
3.2 Official Results
</subsectionHeader>
<bodyText confidence="0.999996263157895">
Our official results for phrase-level and tweet-level
tasks on the 2014 progress tests are given in Table 3.
The models were trained on the 2013 training data.
In the official 2015 ranking, our system performed
competitively in each task. For subtask A (phrase-
level), we placed 5th with an F-score of 82.32, com-
pared to the winning teams F-score of 84.79. For
subtask B, we placed 10th out of 40 submissions,
with an F-score of 61.99, compared to the top team’s
64.84. Our classifier for Subtask C won 1st place
with an F-score of 50.51, leading the second place
entry of 45.48 by over 5 points. Finally, for Subtask
D, we came in 3rd place, with an average absolute
difference of .214 on a 0 to 1 regression, as com-
pared to the gold standard (Rosenthal et al., 2015).
Our system also ranked 4th out of 40 submissions
in identifying the message-level sentiment of sarcas-
tic tweets in 2014 data, with an F-score of 56.02, as
compared to the winning team’s F-score of 59.11.
</bodyText>
<sectionHeader confidence="0.998907" genericHeader="discussions">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999657857142857">
Consistent with previous years’ results, our system
performed better on phrase-level data than on tweet-
level data. We believe this is largely due to the
skewed class distributions, as the majority baselines
for Subtask A are much higher, and there are very
few neutral labels. This is not the case for Subtask
B, where the neutral labels outnumber positive la-
bels. Also, the phrase-level text likely carries clearer
sentiment, while the tweet-level analysis has to deal
with conflicting sentiments across a message.
Note that hashtag segmentation strategy can be
improved by using a language model to predict
which segmentations are more likely, as well as eval-
uating the hashtag’s distributional similarity to the
</bodyText>
<table confidence="0.999044">
Live SMS Twitter Twitter Twitter
Journal 2013 2013 2014 Sarcasm
2014 2014
nBow 58.64 56.55 58.38 59.18 44.67
-spell
nBOW 58.87 57.22 59.19 60.27 46.76
nBOW 58.94 57.81 60.09 61.38 53.00
+hashtag
nBOW 70.65 62.08 68.46 67.86 52.89
+lexicon
nBOW 70.59 62.23 68.78 68.22 54.27
+hashtag
+lexicon
</table>
<tableCaption confidence="0.950089666666667">
Table 4: Contribution of different features in tweet-level
classification. nBOW stands for normalized bag-of-words
features.
</tableCaption>
<bodyText confidence="0.9999772">
rest of the tweet. A language model could also be
used to improve the spell correction.
Our system’s large margin of success at detect-
ing topic-directed sentiment in Subtask C (over 5
points in F-score better than the 2nd place team)
likely comes from the fact that we leverage the large
training data of Subtask B and the tweet-level model
is able to capture long-range dependencies between
sentiment-carrying expressions and the target topic.
We found that the most influential features for de-
tecting sarcasm were normalized BOW unigrams,
lexicon-based features, and unigrams from hashtag
segmentation. Not surprisingly, lexicon features im-
proved performance for all genres, including SMS,
LiveJournal, and non-sarcastic tweets (see rows 2
and 4 in Table 4). The same was true of spelling cor-
rection (as shown in Table 4, row 1). Hashtag-based
features, on the other hand, only yielded large im-
provements for the sarcastic tweets, as shown in the
gain achieved by adding hashtag features to the nor-
malized BOW unigrams (see rows 2 and 3 in Table
4). Note that the 6.24 point gain is only observed
for sarcasm data; other genres showed the average
improvement of about 0.67. We believe that hash-
tags were so effective at predicting sentiment for sar-
casm, because sarcastic tweets facetiously emulate
literal tweets at first but then express their true sen-
timent at the end by using a hashtag, e.g. “On the
bright side we have school today... Tomorrow and
the day after! #killmenow”.
</bodyText>
<sectionHeader confidence="0.999111" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.408282">
@JonMadden @TaskOrganizers #thanks
</reference>
<page confidence="0.99889">
645
</page>
<sectionHeader confidence="0.989547" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999007571428571">
Steven Bird, Ewan Klein, and Edward Loper. 2009. Nat-
ural language processing with Python. O’Reilly Me-
dia, Inc.
Christiane Fellbaum. 1998. WordNet. Wiley Online Li-
brary.
Kevin Gimpel, Nathan Schneider, Brendan O’Connor,
Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael
Heilman, Dani Yogatama, Jeffrey Flanigan, and
Noah A Smith. 2011. Part-of-speech tagging for twit-
ter: Annotation, features, and experiments. In Pro-
ceedings of the 49th Annual Meeting of the Associa-
tion for Computational Linguistics: Human Language
Technologies: short papers-Volume 2, pages 42–47.
Association for Computational Linguistics.
Tobias G¨unther, Jean Vancoppenolle, and Richard Jo-
hansson. 2014. Rtrgo: Enhancing the gu-mlt-lt sys-
tem for sentiment analysis of short messages. In Pro-
ceedings of the 8th International Workshop on Seman-
tic Evaluation (SemEval 2014) August 23-24, 2014
Dublin, Ireland.
Minqing Hu and Bing Liu. 2004. Mining and summa-
rizing customer reviews. In Proceedings of the tenth
ACM SIGKDD international conference on Knowl-
edge discovery and data mining, pages 168–177.
ACM.
Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and
Tomoko Ohkuma. 2014. Teamx: A sentiment ana-
lyzer with enhanced lexicon mapping and weighting
scheme for unbalanced data. SemEval 2014, page 628.
Saif M Mohammad, Svetlana Kiritchenko, and Xiaodan
Zhu. 2013. Nrc-canada: Building the state-of-the-
art in sentiment analysis of tweets. arXiv preprint
arXiv:1308.6242.
Saif M Mohammad. 2012. # emotional tweets. In Pro-
ceedings of the First Joint Conference on Lexical and
Computational Semantics-Volume 1: Proceedings of
the main conference and the shared task, and Volume
2: Proceedings of the Sixth International Workshop on
Semantic Evaluation, pages 246–255. Association for
Computational Linguistics.
Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,
Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013. Semeval-2013 task 2: Sentiment analysis in
twitter. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 2: Pro-
ceedings of the Seventh International Workshop on Se-
mantic Evaluation (SemEval 2013), pages 312–320,
Atlanta, Georgia, USA, June. Association for Compu-
tational Linguistics.
Finn ˚Arup Nielsen. 2011. A new anew: Evaluation of a
word list for sentiment analysis in microblogs. arXiv
preprint arXiv:1103.2903.
Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin
Gimpel, Nathan Schneider, and Noah A Smith. 2013.
Improved part-of-speech tagging for online conversa-
tional text with word clusters. In HLT-NAACL, pages
380–390.
Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002. Thumbs up?: sentiment classification using ma-
chine learning techniques. In Proceedings of the ACL-
02 conference on Empirical methods in natural lan-
guage processing-Volume 10, pages 79–86. Associa-
tion for Computational Linguistics.
Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gram-
fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,
Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-
cent Dubourg, et al. 2011. Scikit-learn: Machine
learning in python. The Journal of Machine Learning
Research, 12:2825–2830.
Christopher Potts. 2011. Sentiment symposium tutorial.
In Sentiment Symposium Tutorial. Acknowledgments.
Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko,
Saif M Mohammad, Alan Ritter, and Veselin Stoy-
anov. 2015. Semeval-2015 task 10: Sentiment anal-
ysis in twitter. In Proceedings of the 9th Interna-
tional Workshop on Semantic Evaluation, SemEval
’2015, Denver, Colorado, June. Association for Com-
putational Linguistics.
Philip J Stone, Dexter C Dunphy, and Marshall S Smith.
1966. The general inquirer: A computer approach to
content analysis.
Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming
Zhou. 2014. Coooolll: A deep learning system for
twitter sentiment classification. SemEval 2014, page
208.
Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005. Recognizing contextual polarity in phrase-level
sentiment analysis. In Proceedings of the conference
on human language technology and empirical methods
in natural language processing, pages 347–354. Asso-
ciation for Computational Linguistics.
Xiaodan Zhu, Svetlana Kiritchenko, and Saif Moham-
mad. 2014. Nrc-canada-2014: Recent improvements
in the sentiment analysis of tweets. In Proceedings
of the 8th International Workshop on Semantic Eval-
uation (SemEval 2014), pages 443–447, Dublin, Ire-
land, August. Association for Computational Linguis-
tics and Dublin City University.
</reference>
<page confidence="0.998861">
646
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.942297">
<title confidence="0.999853">TwitterHawk: A Feature Bucket Approach to Sentiment Analysis</title>
<author confidence="0.992523">William Boag</author>
<author confidence="0.992523">Peter Potash</author>
<author confidence="0.992523">Anna</author>
<affiliation confidence="0.9999505">Dept. of Computer University of Massachusetts</affiliation>
<address confidence="0.99975">198 Riverside St, Lowell, MA 01854, USA</address>
<abstract confidence="0.997684695652174">This paper describes TwitterHawk, a system for sentiment analysis of tweets which participated in the SemEval-2015 Task 10, Subtasks A through D. The system performed commost notably placing in topicbased sentiment classification (Subtask C) and out of 40 in identifying the sentiment of sarcastic tweets. Our submissions in all four subtasks used a supervised learning approach to perform three-way classification to assign positive, negative, or neutral labels. Our system development efforts focused on text pre-processing and feature engineering, with a particular focus on handling negation, integrating sentiment lexicons, parsing hashtags, and handling expressive word modifications and emoticons. Two separate classifiers were developed for phrase-level and tweetlevel sentiment classification. Our success in aforementioned tasks came in part from leveraging the Subtask B data and building a single tweet-level classifier for Subtasks B, C and D.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<institution>JonMadden @TaskOrganizers #thanks</institution>
<marker></marker>
<rawString>@JonMadden @TaskOrganizers #thanks</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
<author>Ewan Klein</author>
<author>Edward Loper</author>
</authors>
<title>Natural language processing with Python.</title>
<date>2009</date>
<publisher>O’Reilly Media, Inc.</publisher>
<contexts>
<context position="10704" citStr="Bird et al., 2009" startWordPosition="1669" endWordPosition="1672">thm; 2. acronym - h is all capitals; 3. camel case - h is written in CamelCase, checked with a regex. For hashtags that are in the manually segmented list, we use the segmentation that we identified as correct. If h is an acronym, we do not segment it. Finally, for CamelCase, we treat the capitalization as indicating the segment boundaries. Normalization and Negation During the normalization phrase, all tokens are lowercased. Next, we replace URLs, user mentions, and numbers with generic URL, USER, and NUMBER tokens, respectively. The remaining tokens are stemmed using NLTKs Snowball stemmer (Bird et al., 2009). We also process negation contexts following the strategy used by Pang et al. (2002). We define a negation context to be a text span that begins with a negation word (such as ‘no’) and ends with a punctuation mark, hashtag, user mention, or URL. The suffix neg is appended to all words inside of a negation context. We use the list of negation words from Potts (2011). 2.2 Machine Learning For the phrase-level sentiment classification, we trained a linear Support Vector Machine (SVM) using scikit-learn’s LinearSVC (Pedregosa et al., 2011) on the Subtask A training data, which contained 4832 posi</context>
</contexts>
<marker>Bird, Klein, Loper, 2009</marker>
<rawString>Steven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python. O’Reilly Media, Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<date>1998</date>
<publisher>WordNet. Wiley Online Library.</publisher>
<contexts>
<context position="15878" citStr="Fellbaum, 1998" startWordPosition="2479" endWordPosition="2480">sed the features derived from the following lexicons: AFINN (Nielsen, 2011), Opinion Lexicon (Hu and Liu, 2004), Brown Clusters (Gimpel et al., 2011), Hashtag Emotion (Mohammad, 2012), Sentiment140 (Mohammad et al., 2013), Hashtag Sentiment (Mohammad et al., 2013), Subjectivity (Wilson et al., 2005), and General Inquirer (Stone et al., 1966). Features are derived separately for each lexicon. General Inquirer and Hashtag Emotion were excluded from the tweet-level analysis since they did not improve system performance in cross-validation. We also experimented with features derived from WordNet (Fellbaum, 1998), but these failed to improve performance for either task in ablation studies. See Section 3.1 for ablation results. The features for the lexicons that provided a numeric score included: • the average sentiment score for the text span; • the total number of positively scored words in the span; • the maximum score (or zero if no words had a sentiment score); • the score of the last positively scored word; 643 Opinion Hashtag Sentiment140 Subjectivity AFINN Hashtag Brown General Sentiment Emotion Clusters Inquirer Phrase-level ✓ ✓ ✓ ✓ ✓ ✓ Tweet-level ✓ ✓ ✓ ✓ ✓ ✓ ✓ Table 1: Which lexicons we used</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet. Wiley Online Library.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Brendan O’Connor</author>
<author>Dipanjan Das</author>
<author>Daniel Mills</author>
<author>Jacob Eisenstein</author>
<author>Michael Heilman</author>
<author>Dani Yogatama</author>
<author>Jeffrey Flanigan</author>
<author>Noah A Smith</author>
</authors>
<title>Part-of-speech tagging for twitter: Annotation, features, and experiments.</title>
<date>2011</date>
<booktitle>In Proceedings of the 49th Annual Meeting of the Association</booktitle>
<pages>42--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Gimpel, Schneider, O’Connor, Das, Mills, Eisenstein, Heilman, Yogatama, Flanigan, Smith, 2011</marker>
<rawString>Kevin Gimpel, Nathan Schneider, Brendan O’Connor, Dipanjan Das, Daniel Mills, Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A Smith. 2011. Part-of-speech tagging for twitter: Annotation, features, and experiments. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies: short papers-Volume 2, pages 42–47. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tobias G¨unther</author>
<author>Jean Vancoppenolle</author>
<author>Richard Johansson</author>
</authors>
<title>Rtrgo: Enhancing the gu-mlt-lt system for sentiment analysis of short messages.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<location>Dublin, Ireland.</location>
<marker>G¨unther, Vancoppenolle, Johansson, 2014</marker>
<rawString>Tobias G¨unther, Jean Vancoppenolle, and Richard Johansson. 2014. Rtrgo: Enhancing the gu-mlt-lt system for sentiment analysis of short messages. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014) August 23-24, 2014 Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Minqing Hu</author>
<author>Bing Liu</author>
</authors>
<title>Mining and summarizing customer reviews.</title>
<date>2004</date>
<booktitle>In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,</booktitle>
<pages>168--177</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="15374" citStr="Hu and Liu, 2004" startWordPosition="2403" endWordPosition="2406">e feature, whereas for the tweet-level classifier, negation changed the feature value from 1 to -1. Lexicon Features We used several Twitterspecific and general-purpose lexicons. The lexicons fell into one of two categories: those that provided a numeric score (usually, -5 to 5) score and those that sorted phrases into categories. For a given lexicon, categories could correspond to a particular emotion, to a strong or weak positive or negative sentiment, or to automatically derived word clusters. We used the features derived from the following lexicons: AFINN (Nielsen, 2011), Opinion Lexicon (Hu and Liu, 2004), Brown Clusters (Gimpel et al., 2011), Hashtag Emotion (Mohammad, 2012), Sentiment140 (Mohammad et al., 2013), Hashtag Sentiment (Mohammad et al., 2013), Subjectivity (Wilson et al., 2005), and General Inquirer (Stone et al., 1966). Features are derived separately for each lexicon. General Inquirer and Hashtag Emotion were excluded from the tweet-level analysis since they did not improve system performance in cross-validation. We also experimented with features derived from WordNet (Fellbaum, 1998), but these failed to improve performance for either task in ablation studies. See Section 3.1 f</context>
</contexts>
<marker>Hu, Liu, 2004</marker>
<rawString>Minqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168–177. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yasuhide Miura</author>
<author>Shigeyuki Sakaki</author>
<author>Keigo Hattori</author>
<author>Tomoko Ohkuma</author>
</authors>
<title>Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data. SemEval</title>
<date>2014</date>
<pages>628</pages>
<contexts>
<context position="3361" citStr="Miura et al., 2014" startWordPosition="506" endWordPosition="509"> over the past two years have relied on basic machine learning classifiers with a strong focus on developing robust and comprehensive feature set. The top system for Subtask A in both 2013 and 2014 from NRC Canada (Mohammad et al., 2013; Zhu et al., 2014) used a simple linear SVM while putting great effort into creating and incorporating sentiment lexicons as well as carefully handling negation contexts. Other teams addressed imbalances in data distributions, but still mainly focused on feature engineering, including an improved spelling correction, POS tagging, and word sense disambiguation (Miura et al., 2014). The second place submission for the 2014 Task B competition also used a neural network 640 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 640–646, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics setup to learn sentiment-specific word embedding features along with state-of-the-art hand-crafted features (Tang et al., 2014). Our goal in developing TwitterHawk was to build on the success of feature-driven approaches established as state-of-the-art in the two previous years of SemEval Twitter Sentiment Analysis competi</context>
</contexts>
<marker>Miura, Sakaki, Hattori, Ohkuma, 2014</marker>
<rawString>Yasuhide Miura, Shigeyuki Sakaki, Keigo Hattori, and Tomoko Ohkuma. 2014. Teamx: A sentiment analyzer with enhanced lexicon mapping and weighting scheme for unbalanced data. SemEval 2014, page 628.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
<author>Svetlana Kiritchenko</author>
<author>Xiaodan Zhu</author>
</authors>
<title>Nrc-canada: Building the state-of-theart in sentiment analysis of tweets. arXiv preprint arXiv:1308.6242.</title>
<date>2013</date>
<contexts>
<context position="2978" citStr="Mohammad et al., 2013" startWordPosition="447" endWordPosition="450">entiment prediction (Subtask C), 3rd out of 6 submissions for detecting trends toward a topic (Subtask D), 10th out of 40 submissions for tweet-level sentiment prediction (Subtask B), and 5th out of 11 for phrase-level prediction (Subtask A). Our system also ranked 4th out of 40 submissions in identifying the sentiment of sarcastic tweets. Most systems that participated in this task over the past two years have relied on basic machine learning classifiers with a strong focus on developing robust and comprehensive feature set. The top system for Subtask A in both 2013 and 2014 from NRC Canada (Mohammad et al., 2013; Zhu et al., 2014) used a simple linear SVM while putting great effort into creating and incorporating sentiment lexicons as well as carefully handling negation contexts. Other teams addressed imbalances in data distributions, but still mainly focused on feature engineering, including an improved spelling correction, POS tagging, and word sense disambiguation (Miura et al., 2014). The second place submission for the 2014 Task B competition also used a neural network 640 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 640–646, Denver, Colorado, June 4</context>
<context position="15484" citStr="Mohammad et al., 2013" startWordPosition="2419" endWordPosition="2422">on Features We used several Twitterspecific and general-purpose lexicons. The lexicons fell into one of two categories: those that provided a numeric score (usually, -5 to 5) score and those that sorted phrases into categories. For a given lexicon, categories could correspond to a particular emotion, to a strong or weak positive or negative sentiment, or to automatically derived word clusters. We used the features derived from the following lexicons: AFINN (Nielsen, 2011), Opinion Lexicon (Hu and Liu, 2004), Brown Clusters (Gimpel et al., 2011), Hashtag Emotion (Mohammad, 2012), Sentiment140 (Mohammad et al., 2013), Hashtag Sentiment (Mohammad et al., 2013), Subjectivity (Wilson et al., 2005), and General Inquirer (Stone et al., 1966). Features are derived separately for each lexicon. General Inquirer and Hashtag Emotion were excluded from the tweet-level analysis since they did not improve system performance in cross-validation. We also experimented with features derived from WordNet (Fellbaum, 1998), but these failed to improve performance for either task in ablation studies. See Section 3.1 for ablation results. The features for the lexicons that provided a numeric score included: • the average senti</context>
</contexts>
<marker>Mohammad, Kiritchenko, Zhu, 2013</marker>
<rawString>Saif M Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu. 2013. Nrc-canada: Building the state-of-theart in sentiment analysis of tweets. arXiv preprint arXiv:1308.6242.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Saif M Mohammad</author>
</authors>
<title>emotional tweets.</title>
<date>2012</date>
<booktitle>In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation,</booktitle>
<pages>246--255</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15446" citStr="Mohammad, 2012" startWordPosition="2415" endWordPosition="2416">ature value from 1 to -1. Lexicon Features We used several Twitterspecific and general-purpose lexicons. The lexicons fell into one of two categories: those that provided a numeric score (usually, -5 to 5) score and those that sorted phrases into categories. For a given lexicon, categories could correspond to a particular emotion, to a strong or weak positive or negative sentiment, or to automatically derived word clusters. We used the features derived from the following lexicons: AFINN (Nielsen, 2011), Opinion Lexicon (Hu and Liu, 2004), Brown Clusters (Gimpel et al., 2011), Hashtag Emotion (Mohammad, 2012), Sentiment140 (Mohammad et al., 2013), Hashtag Sentiment (Mohammad et al., 2013), Subjectivity (Wilson et al., 2005), and General Inquirer (Stone et al., 1966). Features are derived separately for each lexicon. General Inquirer and Hashtag Emotion were excluded from the tweet-level analysis since they did not improve system performance in cross-validation. We also experimented with features derived from WordNet (Fellbaum, 1998), but these failed to improve performance for either task in ablation studies. See Section 3.1 for ablation results. The features for the lexicons that provided a numer</context>
</contexts>
<marker>Mohammad, 2012</marker>
<rawString>Saif M Mohammad. 2012. # emotional tweets. In Proceedings of the First Joint Conference on Lexical and Computational Semantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, pages 246–255. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Preslav Nakov</author>
<author>Sara Rosenthal</author>
<author>Zornitsa Kozareva</author>
<author>Veselin Stoyanov</author>
<author>Alan Ritter</author>
<author>Theresa Wilson</author>
</authors>
<title>Semeval-2013 task 2: Sentiment analysis in twitter.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>312--320</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<contexts>
<context position="17633" citStr="Nakov et al., 2013" startWordPosition="2765" endWordPosition="2768"> provided categories for words and phrases included the number of words that belonged to each category. For phrase-level analysis, the text span used for these features was the target phrase itself. For the tweet-level analysis, the text span covered the whole tweet. Table 1 shows which lexicons we used when building each classifier. 3 Results In this section, we describe the experiments we conducted during system development, as well as the official SemEval Task 10 results. The scores reported throughout this section are calculated as the average of the positive and negative class F-measure (Nakov et al., 2013); the neutral label classification does not directly affect the score. 3.1 System Development Experiments Both phrase-level and tweet-level systems were tuned in 10-fold cross-validation using the 2013 training, dev, and test data (Nakov et al., 2013). We used fixed data folds in order to compare different runs. Feature ablation studies, parameter tuning, and comparison of different pre-processing steps were performed using this setup. We conducted ablation studies for lexicon features using tweet-level evaluation. Table 2 shows ablation results obtained in 10-fold cross-validation. The figure</context>
</contexts>
<marker>Nakov, Rosenthal, Kozareva, Stoyanov, Ritter, Wilson, 2013</marker>
<rawString>Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson. 2013. Semeval-2013 task 2: Sentiment analysis in twitter. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 312–320, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Finn ˚Arup Nielsen</author>
</authors>
<title>A new anew: Evaluation of a word list for sentiment analysis in microblogs. arXiv preprint arXiv:1103.2903.</title>
<date>2011</date>
<contexts>
<context position="15338" citStr="Nielsen, 2011" startWordPosition="2399" endWordPosition="2400">ted word was treated as a separate feature, whereas for the tweet-level classifier, negation changed the feature value from 1 to -1. Lexicon Features We used several Twitterspecific and general-purpose lexicons. The lexicons fell into one of two categories: those that provided a numeric score (usually, -5 to 5) score and those that sorted phrases into categories. For a given lexicon, categories could correspond to a particular emotion, to a strong or weak positive or negative sentiment, or to automatically derived word clusters. We used the features derived from the following lexicons: AFINN (Nielsen, 2011), Opinion Lexicon (Hu and Liu, 2004), Brown Clusters (Gimpel et al., 2011), Hashtag Emotion (Mohammad, 2012), Sentiment140 (Mohammad et al., 2013), Hashtag Sentiment (Mohammad et al., 2013), Subjectivity (Wilson et al., 2005), and General Inquirer (Stone et al., 1966). Features are derived separately for each lexicon. General Inquirer and Hashtag Emotion were excluded from the tweet-level analysis since they did not improve system performance in cross-validation. We also experimented with features derived from WordNet (Fellbaum, 1998), but these failed to improve performance for either task in</context>
</contexts>
<marker>Nielsen, 2011</marker>
<rawString>Finn ˚Arup Nielsen. 2011. A new anew: Evaluation of a word list for sentiment analysis in microblogs. arXiv preprint arXiv:1103.2903.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Olutobi Owoputi</author>
<author>Brendan O’Connor</author>
<author>Chris Dyer</author>
<author>Kevin Gimpel</author>
<author>Nathan Schneider</author>
<author>Noah A Smith</author>
</authors>
<title>Improved part-of-speech tagging for online conversational text with word clusters.</title>
<date>2013</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>380--390</pages>
<marker>Owoputi, O’Connor, Dyer, Gimpel, Schneider, Smith, 2013</marker>
<rawString>Olutobi Owoputi, Brendan O’Connor, Chris Dyer, Kevin Gimpel, Nathan Schneider, and Noah A Smith. 2013. Improved part-of-speech tagging for online conversational text with word clusters. In HLT-NAACL, pages 380–390.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bo Pang</author>
<author>Lillian Lee</author>
<author>Shivakumar Vaithyanathan</author>
</authors>
<title>Thumbs up?: sentiment classification using machine learning techniques.</title>
<date>2002</date>
<booktitle>In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10,</booktitle>
<pages>79--86</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="10789" citStr="Pang et al. (2002)" startWordPosition="1683" endWordPosition="1686">ed with a regex. For hashtags that are in the manually segmented list, we use the segmentation that we identified as correct. If h is an acronym, we do not segment it. Finally, for CamelCase, we treat the capitalization as indicating the segment boundaries. Normalization and Negation During the normalization phrase, all tokens are lowercased. Next, we replace URLs, user mentions, and numbers with generic URL, USER, and NUMBER tokens, respectively. The remaining tokens are stemmed using NLTKs Snowball stemmer (Bird et al., 2009). We also process negation contexts following the strategy used by Pang et al. (2002). We define a negation context to be a text span that begins with a negation word (such as ‘no’) and ends with a punctuation mark, hashtag, user mention, or URL. The suffix neg is appended to all words inside of a negation context. We use the list of negation words from Potts (2011). 2.2 Machine Learning For the phrase-level sentiment classification, we trained a linear Support Vector Machine (SVM) using scikit-learn’s LinearSVC (Pedregosa et al., 2011) on the Subtask A training data, which contained 4832 positive examples, 2549 negative, and 384 neutral. The regularization parameter was set t</context>
</contexts>
<marker>Pang, Lee, Vaithyanathan, 2002</marker>
<rawString>Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan. 2002. Thumbs up?: sentiment classification using machine learning techniques. In Proceedings of the ACL02 conference on Empirical methods in natural language processing-Volume 10, pages 79–86. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fabian Pedregosa</author>
<author>Alexandre Gramfort Ga¨el Varoquaux</author>
<author>Vincent Michel</author>
<author>Bertrand</author>
</authors>
<title>Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer,</title>
<date>2011</date>
<journal>The Journal of Machine Learning Research,</journal>
<pages>12--2825</pages>
<location>Ron Weiss, Vincent Dubourg, et</location>
<marker>Pedregosa, Ga¨el Varoquaux, Michel, Bertrand, 2011</marker>
<rawString>Fabian Pedregosa, Ga¨el Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python. The Journal of Machine Learning Research, 12:2825–2830.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christopher Potts</author>
</authors>
<title>Sentiment symposium tutorial.</title>
<date>2011</date>
<booktitle>In Sentiment Symposium Tutorial.</booktitle>
<publisher>Acknowledgments.</publisher>
<contexts>
<context position="11072" citStr="Potts (2011)" startWordPosition="1740" endWordPosition="1741">ng the normalization phrase, all tokens are lowercased. Next, we replace URLs, user mentions, and numbers with generic URL, USER, and NUMBER tokens, respectively. The remaining tokens are stemmed using NLTKs Snowball stemmer (Bird et al., 2009). We also process negation contexts following the strategy used by Pang et al. (2002). We define a negation context to be a text span that begins with a negation word (such as ‘no’) and ends with a punctuation mark, hashtag, user mention, or URL. The suffix neg is appended to all words inside of a negation context. We use the list of negation words from Potts (2011). 2.2 Machine Learning For the phrase-level sentiment classification, we trained a linear Support Vector Machine (SVM) using scikit-learn’s LinearSVC (Pedregosa et al., 2011) on the Subtask A training data, which contained 4832 positive examples, 2549 negative, and 384 neutral. The regularization parameter was set to C=0.05, using a grid search over the development data (648 positive, 430 negative, 57 neutral). To account for the imbalance of label distributions, we used sklearn’s ‘auto’ class weight adjustment which applies a weight inversely proportional to a given class’s frequency in the t</context>
</contexts>
<marker>Potts, 2011</marker>
<rawString>Christopher Potts. 2011. Sentiment symposium tutorial. In Sentiment Symposium Tutorial. Acknowledgments.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sara Rosenthal</author>
<author>Preslav Nakov</author>
<author>Svetlana Kiritchenko</author>
<author>Saif M Mohammad</author>
<author>Alan Ritter</author>
<author>Veselin Stoyanov</author>
</authors>
<title>Semeval-2015 task 10: Sentiment analysis in twitter.</title>
<date>2015</date>
<booktitle>In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015,</booktitle>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Denver, Colorado,</location>
<contexts>
<context position="2295" citStr="Rosenthal et al. (2015)" startWordPosition="329" endWordPosition="332">, which causes users to use novel abbreviations and often disregard standard sentence structures. For the past three years, the International Workshop on Semantic Evaluation (SemEval) has been hosting a task dedicated to sentiment analysis of Twitter data. This year, our team participated in four subtasks of the challenge: Contextual Polarity Disambiguation (phrase-level), B: Message Polarity Classification (tweet-level), C: Topic-Based Message Polarity Classification (topic-based), and D: Detecting Trends Towards a Topic (trending sentiment). For a more thorough description of the tasks, see Rosenthal et al. (2015). Our system placed 1st out of 7 submissions for topic-based sentiment prediction (Subtask C), 3rd out of 6 submissions for detecting trends toward a topic (Subtask D), 10th out of 40 submissions for tweet-level sentiment prediction (Subtask B), and 5th out of 11 for phrase-level prediction (Subtask A). Our system also ranked 4th out of 40 submissions in identifying the sentiment of sarcastic tweets. Most systems that participated in this task over the past two years have relied on basic machine learning classifiers with a strong focus on developing robust and comprehensive feature set. The to</context>
<context position="5247" citStr="Rosenthal et al., 2015" startWordPosition="787" endWordPosition="790"> generalized better to topic-level sentiment tracking than span-level data (Subtask A). We therefore used Subtask B data to train a more robust model for topic-level sentiment detection. The rest of this paper is organized as follows. In Section 2, we discuss text preprocessing and normalization, describe the two classifiers we created for different subtasks, and present the features used by each model. We report system results in Section 3, and discuss system performance and future directions in Section 4. 2 System Description We built a system to compete in four subtasks of SemEval Task 10 (Rosenthal et al., 2015). Subtasks A-C were concerned with classification of Twitter data as either positive, negative, or neutral. Subtask A involved phrase-level (usually 1-4 tokens) sentiment analysis. Subtask B dealt with classification of the entire tweet. Subtask C involved classifying a tweet’s sentiment towards a given topic. Subtask D summarized the results of Subtask C by analyzing the sentiment expressed towards a topic by a group of tweets (as opposed to the single tweet classification for Subtask C). We trained two classifiers one for phrase-level classification and one for tweet-level sentiment classifi</context>
<context position="20972" citStr="Rosenthal et al., 2015" startWordPosition="3294" endWordPosition="3297"> training data. In the official 2015 ranking, our system performed competitively in each task. For subtask A (phraselevel), we placed 5th with an F-score of 82.32, compared to the winning teams F-score of 84.79. For subtask B, we placed 10th out of 40 submissions, with an F-score of 61.99, compared to the top team’s 64.84. Our classifier for Subtask C won 1st place with an F-score of 50.51, leading the second place entry of 45.48 by over 5 points. Finally, for Subtask D, we came in 3rd place, with an average absolute difference of .214 on a 0 to 1 regression, as compared to the gold standard (Rosenthal et al., 2015). Our system also ranked 4th out of 40 submissions in identifying the message-level sentiment of sarcastic tweets in 2014 data, with an F-score of 56.02, as compared to the winning team’s F-score of 59.11. 4 Discussion Consistent with previous years’ results, our system performed better on phrase-level data than on tweetlevel data. We believe this is largely due to the skewed class distributions, as the majority baselines for Subtask A are much higher, and there are very few neutral labels. This is not the case for Subtask B, where the neutral labels outnumber positive labels. Also, the phrase</context>
</contexts>
<marker>Rosenthal, Nakov, Kiritchenko, Mohammad, Ritter, Stoyanov, 2015</marker>
<rawString>Sara Rosenthal, Preslav Nakov, Svetlana Kiritchenko, Saif M Mohammad, Alan Ritter, and Veselin Stoyanov. 2015. Semeval-2015 task 10: Sentiment analysis in twitter. In Proceedings of the 9th International Workshop on Semantic Evaluation, SemEval ’2015, Denver, Colorado, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip J Stone</author>
<author>Dexter C Dunphy</author>
<author>Marshall S Smith</author>
</authors>
<title>The general inquirer: A computer approach to content analysis.</title>
<date>1966</date>
<contexts>
<context position="15606" citStr="Stone et al., 1966" startWordPosition="2439" endWordPosition="2442"> that provided a numeric score (usually, -5 to 5) score and those that sorted phrases into categories. For a given lexicon, categories could correspond to a particular emotion, to a strong or weak positive or negative sentiment, or to automatically derived word clusters. We used the features derived from the following lexicons: AFINN (Nielsen, 2011), Opinion Lexicon (Hu and Liu, 2004), Brown Clusters (Gimpel et al., 2011), Hashtag Emotion (Mohammad, 2012), Sentiment140 (Mohammad et al., 2013), Hashtag Sentiment (Mohammad et al., 2013), Subjectivity (Wilson et al., 2005), and General Inquirer (Stone et al., 1966). Features are derived separately for each lexicon. General Inquirer and Hashtag Emotion were excluded from the tweet-level analysis since they did not improve system performance in cross-validation. We also experimented with features derived from WordNet (Fellbaum, 1998), but these failed to improve performance for either task in ablation studies. See Section 3.1 for ablation results. The features for the lexicons that provided a numeric score included: • the average sentiment score for the text span; • the total number of positively scored words in the span; • the maximum score (or zero if n</context>
</contexts>
<marker>Stone, Dunphy, Smith, 1966</marker>
<rawString>Philip J Stone, Dexter C Dunphy, and Marshall S Smith. 1966. The general inquirer: A computer approach to content analysis.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Duyu Tang</author>
<author>Furu Wei</author>
<author>Bing Qin</author>
<author>Ting Liu</author>
<author>Ming Zhou</author>
</authors>
<title>Coooolll: A deep learning system for twitter sentiment classification. SemEval</title>
<date>2014</date>
<pages>208</pages>
<contexts>
<context position="3764" citStr="Tang et al., 2014" startWordPosition="562" endWordPosition="565">ts. Other teams addressed imbalances in data distributions, but still mainly focused on feature engineering, including an improved spelling correction, POS tagging, and word sense disambiguation (Miura et al., 2014). The second place submission for the 2014 Task B competition also used a neural network 640 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 640–646, Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics setup to learn sentiment-specific word embedding features along with state-of-the-art hand-crafted features (Tang et al., 2014). Our goal in developing TwitterHawk was to build on the success of feature-driven approaches established as state-of-the-art in the two previous years of SemEval Twitter Sentiment Analysis competitions. We therefore focused on identifying and incorporating the strongest features used by the best systems, most notably, sentiment lexicons that showed good performance in ablation studies. We also performed multiple rounds of pre-processing which included tokenization, spelling correction, hashtag segmentation, wordshape replacement of URLs, as well as handling negated contexts. Our main insight </context>
</contexts>
<marker>Tang, Wei, Qin, Liu, Zhou, 2014</marker>
<rawString>Duyu Tang, Furu Wei, Bing Qin, Ting Liu, and Ming Zhou. 2014. Coooolll: A deep learning system for twitter sentiment classification. SemEval 2014, page 208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Theresa Wilson</author>
<author>Janyce Wiebe</author>
<author>Paul Hoffmann</author>
</authors>
<title>Recognizing contextual polarity in phrase-level sentiment analysis.</title>
<date>2005</date>
<booktitle>In Proceedings of the conference on human language technology and empirical methods in natural language processing,</booktitle>
<pages>347--354</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="15563" citStr="Wilson et al., 2005" startWordPosition="2431" endWordPosition="2435">icons fell into one of two categories: those that provided a numeric score (usually, -5 to 5) score and those that sorted phrases into categories. For a given lexicon, categories could correspond to a particular emotion, to a strong or weak positive or negative sentiment, or to automatically derived word clusters. We used the features derived from the following lexicons: AFINN (Nielsen, 2011), Opinion Lexicon (Hu and Liu, 2004), Brown Clusters (Gimpel et al., 2011), Hashtag Emotion (Mohammad, 2012), Sentiment140 (Mohammad et al., 2013), Hashtag Sentiment (Mohammad et al., 2013), Subjectivity (Wilson et al., 2005), and General Inquirer (Stone et al., 1966). Features are derived separately for each lexicon. General Inquirer and Hashtag Emotion were excluded from the tweet-level analysis since they did not improve system performance in cross-validation. We also experimented with features derived from WordNet (Fellbaum, 1998), but these failed to improve performance for either task in ablation studies. See Section 3.1 for ablation results. The features for the lexicons that provided a numeric score included: • the average sentiment score for the text span; • the total number of positively scored words in </context>
</contexts>
<marker>Wilson, Wiebe, Hoffmann, 2005</marker>
<rawString>Theresa Wilson, Janyce Wiebe, and Paul Hoffmann. 2005. Recognizing contextual polarity in phrase-level sentiment analysis. In Proceedings of the conference on human language technology and empirical methods in natural language processing, pages 347–354. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaodan Zhu</author>
<author>Svetlana Kiritchenko</author>
<author>Saif Mohammad</author>
</authors>
<title>Nrc-canada-2014: Recent improvements in the sentiment analysis of tweets.</title>
<date>2014</date>
<booktitle>In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval</booktitle>
<pages>443--447</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="2997" citStr="Zhu et al., 2014" startWordPosition="451" endWordPosition="454">btask C), 3rd out of 6 submissions for detecting trends toward a topic (Subtask D), 10th out of 40 submissions for tweet-level sentiment prediction (Subtask B), and 5th out of 11 for phrase-level prediction (Subtask A). Our system also ranked 4th out of 40 submissions in identifying the sentiment of sarcastic tweets. Most systems that participated in this task over the past two years have relied on basic machine learning classifiers with a strong focus on developing robust and comprehensive feature set. The top system for Subtask A in both 2013 and 2014 from NRC Canada (Mohammad et al., 2013; Zhu et al., 2014) used a simple linear SVM while putting great effort into creating and incorporating sentiment lexicons as well as carefully handling negation contexts. Other teams addressed imbalances in data distributions, but still mainly focused on feature engineering, including an improved spelling correction, POS tagging, and word sense disambiguation (Miura et al., 2014). The second place submission for the 2014 Task B competition also used a neural network 640 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 640–646, Denver, Colorado, June 4-5, 2015. c�2015 As</context>
</contexts>
<marker>Zhu, Kiritchenko, Mohammad, 2014</marker>
<rawString>Xiaodan Zhu, Svetlana Kiritchenko, and Saif Mohammad. 2014. Nrc-canada-2014: Recent improvements in the sentiment analysis of tweets. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 443–447, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>