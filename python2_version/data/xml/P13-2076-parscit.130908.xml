<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016820">
<title confidence="0.809452">
Question Classification Transfer
</title>
<author confidence="0.39446">
Anne-Laure Ligozat
</author>
<note confidence="0.5974025">
LIMSI-CNRS / BP133, 91403 Orsay cedex, France
ENSIIE / 1, square de la r´esistance, Evry, France
</note>
<email confidence="0.807393">
firstname.lastname@limsi.fr
</email>
<sectionHeader confidence="0.986721" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999525">
Question answering systems have been de-
veloped for many languages, but most re-
sources were created for English, which
can be a problem when developing a sys-
tem in another language such as French.
In particular, for question classification,
no labeled question corpus is available for
French, so this paper studies the possi-
bility to use existing English corpora and
transfer a classification by translating the
question and their labels. By translating
the training corpus, we obtain results close
to a monolingual setting.
</bodyText>
<sectionHeader confidence="0.998794" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999912361111111">
In question answering (QA), as in most Natural
Language Processing domains, English is the best
resourced language, in terms of corpora, lexicons,
or systems. Many methods are based on super-
vised machine learning which is made possible by
the great amount of resources for this language.
While developing a question answering system
for French, we were thus limited by the lack of
resources for this language. Some were created,
for example for answer validation (Grappy et al.,
2011). Yet, for question classification, although
question corpora in French exist, only a small part
of them is annotated with question classes, and
such an annotation is costly. We thus wondered
if it was possible to use existing English corpora,
in this case the data used in (Li and Roth, 2002),
to create a classification module for French.
Transfering knowledge from one language to
another is usually done by exploiting parallel cor-
pora; yet in this case, few such corpora exists
(CLEF QA datasets could be used, but question
classes are not very precise). We thus investigated
the possibility of using machine translation to cre-
ate a parallel corpus, as has been done for spoken
language understanding (Jabaian et al., 2011) for
example. The idea is that using machine transla-
tion would enable us to have a large training cor-
pus, either by using the English one and translat-
ing the test corpus, or by translating the training
corpus. One of the questions posed was whether
the quality of present machine translation systems
would enable to learn the classification properly.
This paper presents a question classification
transfer method, which results are close to those
of a monolingual system. The contributions of the
paper are the following:
</bodyText>
<listItem confidence="0.996328">
• comparison of train-on-target and test-on-
source strategies for question classification;
• creation of an effective question classification
system for French, with minimal annotation
effort.
</listItem>
<bodyText confidence="0.999736714285714">
This paper is organized as follows: The problem
of Question Classification is defined in section 2.
The proposed methods are presented in section 3,
and the experiments in section 4. Section 5 details
the related works in Question Answering. Finally,
Section 6 concludes with a summary and a few
directions for future work.
</bodyText>
<sectionHeader confidence="0.910893" genericHeader="method">
2 Problem definition
</sectionHeader>
<bodyText confidence="0.999162384615385">
A Question Answering (QA) system aims at re-
turning a precise answer to a natural language
question: if asked ”How large is the Lincoln
Memorial?”, a QA system should return the an-
swer ”164 acres” as well as a justifying snippet.
Most systems include a question classification step
which determines the expected answer type, for
example area in the previous case. This type can
then be used to extract the correct answer in docu-
ments.
Detecting the answer type is usually consid-
ered as a multiclass classification problem, with
each answer type representing a class. (Zhang and
</bodyText>
<page confidence="0.990469">
429
</page>
<note confidence="0.460528">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 429–433,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<figure confidence="0.999474269230769">
English
training corpus
learn
Question
classification
for English
predict
English
test corpus
(translation)
translation
French
test corpus
English
training corpus
translation
French
training corpus
(translation)
learn
Question
classification
for French
French
test corpus
predict
</figure>
<figureCaption confidence="0.967452">
Figure 2: Some of the question categories pro-
posed by (Li and Roth, 2002)
Figure 1: Methods for transfering question classi-
fication
</figureCaption>
<bodyText confidence="0.997325857142857">
Lee, 2003) showed that a training corpus of sev-
eral thousands of questions was required to obtain
around 90% correct classification, which makes it
a costly process to adapt a system to another lan-
guage than English. In this paper, we wish to learn
such a system for French, without having to man-
ually annotate thousands of questions.
</bodyText>
<sectionHeader confidence="0.931172" genericHeader="method">
3 Transfering question classification
</sectionHeader>
<bodyText confidence="0.999858666666667">
The two methods tested for transfering the classi-
fication, following (Jabaian et al., 2011), are pre-
sented in Figure 1:
</bodyText>
<listItem confidence="0.980473454545454">
• The first one (on the left), called test-on-
source, consists in learning a classification
model in English, and to translate the test cor-
pus from French to English, in order to apply
the English model on the translated test cor-
pus.
• The second one (on the right), called train-
on-target, consists in translating the training
corpus from English to French. We obtain an
labeled French corpus, on which it is possible
to learn a classification model.
</listItem>
<bodyText confidence="0.999930285714286">
In the first case, classification is learned on well
written questions; yet, as the test corpus is trans-
lated, translation errors may disturb the classifier.
In the second case, the classification model will
be learned on less well written questions, but the
corpus may be large enough to compensate for the
loss in quality.
</bodyText>
<sectionHeader confidence="0.999434" genericHeader="method">
4 Experiments
</sectionHeader>
<subsectionHeader confidence="0.996727">
4.1 Question classes
</subsectionHeader>
<bodyText confidence="0.999964375">
We used the question taxonomy proposed by (Li
and Roth, 2002), which enabled us to compare
our results to those obtained by (Zhang and Lee,
2003) on English. This taxonomy contains two
levels: the first one contains 50 fine grained cat-
egories, the second one contains 6 coarse grained
categories. Figure 2 presents a few of these cate-
gories.
</bodyText>
<subsectionHeader confidence="0.985775">
4.2 Corpora
</subsectionHeader>
<bodyText confidence="0.999973736842105">
For English, we used the data from (Li and Roth,
2002), which was assembled from USC, UIUC
and TREC collections, and has been manually la-
beled according to their taxonomy. The training
set contains 5,500 labeled questions, and the test-
ing set contains 500 questions.
For French, we gathered questions from several
evaluation campaigns: QA@CLEF 2005, 2006,
2007, EQueR and Quæro 2008, 2009 and 2010.
After elimination of duplicated questions, we ob-
tained a corpus of 1,421 questions, which were di-
vided into a training set of 728 questions, and a test
set of 693 questions 1. Some of these questions
were already labeled, and we manually annotated
the rest of them.
Translation was performed by Google Trans-
late online interface, which had satisfactory per-
formance on interrogative forms, which are not
well handled by all machine translation systems 2.
</bodyText>
<footnote confidence="0.9981">
1This distribution is due to further constraints on the sys-
tem.
2We tested other translation systems, but Google Trans-
late gave the best results.
</footnote>
<page confidence="0.964745">
430
</page>
<table confidence="0.999775818181818">
Train en en fr fr
(trans.)
Test en en fr fr
(trans.)
Method test- train-
on- on-
source target
50 .798 .677 .794 .769
classes
6 .90 .735 .828 .84
classes
</table>
<tableCaption confidence="0.795472666666667">
Table 1: Question classification precision for both
levels of the hierarchy (features = word n-grams,
classifier = libsvm)
</tableCaption>
<subsectionHeader confidence="0.998368">
4.3 Classification parameters
</subsectionHeader>
<bodyText confidence="0.999904285714286">
The classifier used was LibSVM (Chang and Lin,
2011) with default parameters, which offers one-
vs-one multiclass classification, and which (Zhang
and Lee, 2003) showed to be most effective for this
task.
We only considered surface features, and ex-
tracted bag-of-ngrams (with n = 1..2).
</bodyText>
<subsectionHeader confidence="0.898351">
4.4 Results and discussion
</subsectionHeader>
<bodyText confidence="0.999627095238095">
Table 1 shows the results obtained with the basic
configuration, for both transfer methods.
Results are given in precision, i.e. the propor-
tion of correctly classified questions among the
test questions 3.
Using word n-grams, monolingual English clas-
sification obtains .798 correct classification for the
fine grained classes, and .90 for the coarse grained
classes, results which are very close to those ob-
tained by (Zhang and Lee, 2003).
On French, we obtain lower results: .769 for
fine grained classes, and .84 for coarse grained
classes, probably mostly due to the smallest size
of the training corpus: (Zhang and Lee, 2003) had
a precision of .65 for the fine grained classification
with a 1,000 questions training corpus.
When translating test questions from French
to English, classification precision decreases, as
was expected from (Cumbreras et al., 2006). Yet,
when translating the training corpus from English
to French and learning the classification model
</bodyText>
<footnote confidence="0.9216695">
3We measured the significance of precision differences
(Student t test, p=.05), for each level of the hierarchy between
each test, and, unless indicated otherwise, comparable results
are significantly different in each condition.
</footnote>
<table confidence="0.9992229">
Train en fr fr
(trans.)
Test en fr fr
Method train-
on-
target
50 .822 .798 .807
classes
6 .92 .841 .872
classes
</table>
<tableCaption confidence="0.996696">
Table 2: Question classification precision for both
</tableCaption>
<bodyText confidence="0.998470578947368">
levels of the hierarchy (features = word n-grams
with abbreviations, classifier = libsvm)
on this translated corpus, precision is close to
the French monolingual one for coarse grained
classes and a little higher than monolingual for
fine grained classification (and close to the English
monolingual one): this method gives precisions of
.794 for fine grained classes and .828 for coarse
grained classes.
One possible explanation is that the condition
when test questions are translated is very sensitive
to translation errors: if one of the test questions
is not correcly translated, the classifier will have
a hard time categorizing it. If the training cor-
pus is translated, translation errors can be counter-
balanced by correct translations. In the following
results, we do not consider the ”en to en (trans)”
method since it systematically gives lower results.
As results were lower than our existing rule-
based method, we added parts-of-speech as fea-
tures in order to try to improve them, as well as
semantic classes: the classes are lists of words re-
lated to a particular category; for example ”pres-
ident” usually means that a person is expected as
an answer. Table 2 shows the classification perfor-
mance with this additional information.
Classification is slightly improved, but only for
coarse grained classes (the difference is not signif-
icant for fine grained classes).
When analyzing the results, we noted that most
confusion errors were due to the type of features
given as inputs: for example, to correctly clas-
sify the question ”What is BPH?” as a question
expecting an expression corresponding to an ab-
breviation (ABBR:exp class in the hierarchy), it
is necessary to know that ”BPH” is an abbrevia-
tion. We thus added a specific feature to detect if
a question word is an abbreviation, simply by test-
</bodyText>
<page confidence="0.996619">
431
</page>
<table confidence="0.999456285714286">
Train en fr fr
(trans.)
Test en fr fr
50 .804 .837 .828
classes
6 .904 .869 .900
classes
</table>
<tableCaption confidence="0.999201">
Table 3: Question classification precision for both
</tableCaption>
<bodyText confidence="0.9938575">
levels of the hierarchy (features = word n-grams
with abbreviations, classifier = libsvm)
ing if it contains only upper case letters, and nor-
malizing them. Table 3 gives the results with this
additional feature (we only kept the method with
translation of the training corpus since results were
much higher).
Precision is improved for both levels of the hi-
erarchy: for fine grained classes, results increase
from .794 to .837, and for coarse grained classes,
from .828 to .869. Remaining classification errors
are much more disparate.
</bodyText>
<sectionHeader confidence="0.999803" genericHeader="method">
5 Related work
</sectionHeader>
<bodyText confidence="0.999723244897959">
Most question answering systems include ques-
tion classification, which is generally based on su-
pervised learning. (Li and Roth, 2002) trained
the SNoW hierarchical classifier for question clas-
sification, with a 50 classes fine grained hierar-
chy, and a coarse grained one of 6 classes. The
features used are words, parts-of-speech, chunks,
named entities, chunk heads and words related to
a class. They obtain 98.8% correct classification
of the coarse grained classes, and 95% on the fine
grained one. This hierarchy was widely used by
other QA systems.
(Zhang and Lee, 2003) studied the classifica-
tion performance according to the classifier and
training dataser size, as well as the contribution of
question parse trees. Their results are 87% correct
classification on coarse grained classes and 80%
on fine grained classes with vectorial attributes,
and 90% correct classification on coarse grained
classes and 80% on fine grained classes with struc-
tured input and tree kerneks.
These question classifications were used for
English only. Adapting the methods to other
languages requires to annotated large corpora of
questions.
In order to classify questions in different lan-
guages, (Solorio et al., 2004) proposed an in-
ternet based approach to determine the expected
type. By combining this information with ques-
tion words, they obtain 84% correct classification
for English, 84% for Spanish and 89% for Ital-
ian, with a cross validation on a 450 question cor-
pus for 7 question classes. One of the limitations
raised by the authors is the lack of large labeled
corpora for all languages.
A possibility to overcome this lack of resources
is to use existing English resources. (Cumbreras
et al., 2006) developed a QA system for Spanish,
based on an English QA system, by translating the
questions from Spanish to English. They obtain a
65% precision for Spanish question classification,
while English classification are correctly classified
with an 80% precision. This method thus leads to
an important drop in performance.
Crosslingual QA systems, in which the question
is in a different language than the documents, also
usually rely on English systems, and translate an-
swers for example (Bos and Nissim, 2006; Bow-
den et al., 2008).
</bodyText>
<sectionHeader confidence="0.999227" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999986941176471">
This paper presents a comparison between two
transfer modes to adapt question classification
from English to French. Results show that trans-
lating the training corpus gives better results than
translating the test corpus.
Part-of-speech information only was used, but
since (Zhang and Lee, 2003) showed that best re-
sults are obtained with parse trees and tree kernels,
it could be interesting to test this additional in-
formation; yet, parsing translated questions may
prove unreliable.
Finally, as interrogative forms occur rarely is
corpora, their translation is usually of a slightly
lower quality. A possible future direction for this
work could be to use a specific model of transla-
tion for questions in order to learn question classi-
fication on higher quality translations.
</bodyText>
<sectionHeader confidence="0.999446" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994433714285714">
J. Bos and M. Nissim. 2006. Cross-lingual question
answering by answer translation. In Working Notes
of the Cross Language Evaluation Forum.
M. Bowden, M. Olteanu, P. Suriyentrakorn, T. d´Silva,
and D. Moldovan. 2008. Multilingual question
answering through intermediate translation: Lcc´s
poweranswer at qa@clef 2007. Advances in Mul-
</reference>
<page confidence="0.982152">
432
</page>
<reference confidence="0.999379717948718">
tilingual and Multimodal Information Retrieval,
5152:273–283.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technology,
2:27:1–27:27.
M. ´A.G. Cumbreras, L. L´opez, and F.M. Santiago.
2006. Bruja: Question classification for spanish. us-
ing machine translation and an english classifier. In
Proceedings of the Workshop on Multilingual Ques-
tion Answering, pages 39–44. Association for Com-
putational Linguistics.
Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco,
Anne-Laure Ligozat, Isabelle Robba, and Anne Vil-
nat. 2011. Selecting answers to questions from
web documents by a robust validation process. In
IEEE/WIC/ACM International Conference on Web
Intelligence.
Bassam Jabaian, Laurent Besacier, and Fabrice
Lef`evre. 2011. Combination of stochastic under-
standing and machine translation systems for lan-
guage portability of dialogue systems. In Acous-
tics, Speech and Signal Processing (ICASSP), 2011
IEEE International Conference on, pages 5612–
5615. IEEE.
X. Li and D. Roth. 2002. Learning question classifiers.
In Proceedings of the 19th international conference
on Computational linguistics-Volume 1, pages 1–7.
Association for Computational Linguistics.
T. Solorio, M. P´erez-Coutino, et al. 2004. A language
independent method for question classification. In
Proceedings of the 20th international conference on
Computational Linguistics, pages 1374–1380. Asso-
ciation for Computational Linguistics.
D. Zhang and W.S. Lee. 2003. Question classifica-
tion using support vector machines. In Proceedings
of the 26th annual international ACM SIGIR con-
ference on Research and development in informaion
retrieval, pages 26–32. ACM.
</reference>
<page confidence="0.999443">
433
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.209554">
<title confidence="0.9326305">Question Classification Transfer Anne-Laure</title>
<author confidence="0.665668">LIMSI-CNRS BP</author>
<author confidence="0.665668">Orsay cedex</author>
<email confidence="0.6199665">ENSIIE/1,squaredelar´esistance,Evry,firstname.lastname@limsi.fr</email>
<abstract confidence="0.999758714285714">Question answering systems have been developed for many languages, but most resources were created for English, which can be a problem when developing a system in another language such as French. In particular, for question classification, no labeled question corpus is available for French, so this paper studies the possibility to use existing English corpora and transfer a classification by translating the question and their labels. By translating the training corpus, we obtain results close to a monolingual setting.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Bos</author>
<author>M Nissim</author>
</authors>
<title>Cross-lingual question answering by answer translation. In Working Notes of the Cross Language Evaluation Forum.</title>
<date>2006</date>
<contexts>
<context position="13531" citStr="Bos and Nissim, 2006" startWordPosition="2169" endWordPosition="2172"> possibility to overcome this lack of resources is to use existing English resources. (Cumbreras et al., 2006) developed a QA system for Spanish, based on an English QA system, by translating the questions from Spanish to English. They obtain a 65% precision for Spanish question classification, while English classification are correctly classified with an 80% precision. This method thus leads to an important drop in performance. Crosslingual QA systems, in which the question is in a different language than the documents, also usually rely on English systems, and translate answers for example (Bos and Nissim, 2006; Bowden et al., 2008). 6 Conclusion This paper presents a comparison between two transfer modes to adapt question classification from English to French. Results show that translating the training corpus gives better results than translating the test corpus. Part-of-speech information only was used, but since (Zhang and Lee, 2003) showed that best results are obtained with parse trees and tree kernels, it could be interesting to test this additional information; yet, parsing translated questions may prove unreliable. Finally, as interrogative forms occur rarely is corpora, their translation is</context>
</contexts>
<marker>Bos, Nissim, 2006</marker>
<rawString>J. Bos and M. Nissim. 2006. Cross-lingual question answering by answer translation. In Working Notes of the Cross Language Evaluation Forum.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Bowden</author>
<author>M Olteanu</author>
<author>P Suriyentrakorn</author>
<author>T d´Silva</author>
<author>D Moldovan</author>
</authors>
<title>Multilingual question answering through intermediate translation: Lcc´s poweranswer at qa@clef</title>
<date>2008</date>
<booktitle>Advances in Multilingual and Multimodal Information Retrieval,</booktitle>
<pages>5152--273</pages>
<marker>Bowden, Olteanu, Suriyentrakorn, d´Silva, Moldovan, 2008</marker>
<rawString>M. Bowden, M. Olteanu, P. Suriyentrakorn, T. d´Silva, and D. Moldovan. 2008. Multilingual question answering through intermediate translation: Lcc´s poweranswer at qa@clef 2007. Advances in Multilingual and Multimodal Information Retrieval, 5152:273–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<contexts>
<context position="7190" citStr="Chang and Lin, 2011" startWordPosition="1152" endWordPosition="1155"> satisfactory performance on interrogative forms, which are not well handled by all machine translation systems 2. 1This distribution is due to further constraints on the system. 2We tested other translation systems, but Google Translate gave the best results. 430 Train en en fr fr (trans.) Test en en fr fr (trans.) Method test- trainon- onsource target 50 .798 .677 .794 .769 classes 6 .90 .735 .828 .84 classes Table 1: Question classification precision for both levels of the hierarchy (features = word n-grams, classifier = libsvm) 4.3 Classification parameters The classifier used was LibSVM (Chang and Lin, 2011) with default parameters, which offers onevs-one multiclass classification, and which (Zhang and Lee, 2003) showed to be most effective for this task. We only considered surface features, and extracted bag-of-ngrams (with n = 1..2). 4.4 Results and discussion Table 1 shows the results obtained with the basic configuration, for both transfer methods. Results are given in precision, i.e. the proportion of correctly classified questions among the test questions 3. Using word n-grams, monolingual English classification obtains .798 correct classification for the fine grained classes, and .90 for t</context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M ´A G Cumbreras</author>
<author>L L´opez</author>
<author>F M Santiago</author>
</authors>
<title>Bruja: Question classification for spanish. using machine translation and an english classifier.</title>
<date>2006</date>
<booktitle>In Proceedings of the Workshop on Multilingual Question Answering,</booktitle>
<pages>39--44</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Cumbreras, L´opez, Santiago, 2006</marker>
<rawString>M. ´A.G. Cumbreras, L. L´opez, and F.M. Santiago. 2006. Bruja: Question classification for spanish. using machine translation and an english classifier. In Proceedings of the Workshop on Multilingual Question Answering, pages 39–44. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Arnaud Grappy</author>
<author>Brigitte Grau</author>
<author>Mathieu-Henri Falco</author>
<author>Anne-Laure Ligozat</author>
<author>Isabelle Robba</author>
<author>Anne Vilnat</author>
</authors>
<title>Selecting answers to questions from web documents by a robust validation process.</title>
<date>2011</date>
<booktitle>In IEEE/WIC/ACM International Conference on Web Intelligence.</booktitle>
<contexts>
<context position="1209" citStr="Grappy et al., 2011" startWordPosition="182" endWordPosition="185">ing the question and their labels. By translating the training corpus, we obtain results close to a monolingual setting. 1 Introduction In question answering (QA), as in most Natural Language Processing domains, English is the best resourced language, in terms of corpora, lexicons, or systems. Many methods are based on supervised machine learning which is made possible by the great amount of resources for this language. While developing a question answering system for French, we were thus limited by the lack of resources for this language. Some were created, for example for answer validation (Grappy et al., 2011). Yet, for question classification, although question corpora in French exist, only a small part of them is annotated with question classes, and such an annotation is costly. We thus wondered if it was possible to use existing English corpora, in this case the data used in (Li and Roth, 2002), to create a classification module for French. Transfering knowledge from one language to another is usually done by exploiting parallel corpora; yet in this case, few such corpora exists (CLEF QA datasets could be used, but question classes are not very precise). We thus investigated the possibility of u</context>
</contexts>
<marker>Grappy, Grau, Falco, Ligozat, Robba, Vilnat, 2011</marker>
<rawString>Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco, Anne-Laure Ligozat, Isabelle Robba, and Anne Vilnat. 2011. Selecting answers to questions from web documents by a robust validation process. In IEEE/WIC/ACM International Conference on Web Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bassam Jabaian</author>
<author>Laurent Besacier</author>
<author>Fabrice Lef`evre</author>
</authors>
<title>Combination of stochastic understanding and machine translation systems for language portability of dialogue systems.</title>
<date>2011</date>
<booktitle>In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on,</booktitle>
<pages>5612--5615</pages>
<publisher>IEEE.</publisher>
<marker>Jabaian, Besacier, Lef`evre, 2011</marker>
<rawString>Bassam Jabaian, Laurent Besacier, and Fabrice Lef`evre. 2011. Combination of stochastic understanding and machine translation systems for language portability of dialogue systems. In Acoustics, Speech and Signal Processing (ICASSP), 2011 IEEE International Conference on, pages 5612– 5615. IEEE.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Li</author>
<author>D Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on Computational linguistics-Volume 1,</booktitle>
<pages>1--7</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1502" citStr="Li and Roth, 2002" startWordPosition="232" endWordPosition="235">Many methods are based on supervised machine learning which is made possible by the great amount of resources for this language. While developing a question answering system for French, we were thus limited by the lack of resources for this language. Some were created, for example for answer validation (Grappy et al., 2011). Yet, for question classification, although question corpora in French exist, only a small part of them is annotated with question classes, and such an annotation is costly. We thus wondered if it was possible to use existing English corpora, in this case the data used in (Li and Roth, 2002), to create a classification module for French. Transfering knowledge from one language to another is usually done by exploiting parallel corpora; yet in this case, few such corpora exists (CLEF QA datasets could be used, but question classes are not very precise). We thus investigated the possibility of using machine translation to create a parallel corpus, as has been done for spoken language understanding (Jabaian et al., 2011) for example. The idea is that using machine translation would enable us to have a large training corpus, either by using the English one and translating the test cor</context>
<context position="4116" citStr="Li and Roth, 2002" startWordPosition="638" endWordPosition="641">ification problem, with each answer type representing a class. (Zhang and 429 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 429–433, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics English training corpus learn Question classification for English predict English test corpus (translation) translation French test corpus English training corpus translation French training corpus (translation) learn Question classification for French French test corpus predict Figure 2: Some of the question categories proposed by (Li and Roth, 2002) Figure 1: Methods for transfering question classification Lee, 2003) showed that a training corpus of several thousands of questions was required to obtain around 90% correct classification, which makes it a costly process to adapt a system to another language than English. In this paper, we wish to learn such a system for French, without having to manually annotate thousands of questions. 3 Transfering question classification The two methods tested for transfering the classification, following (Jabaian et al., 2011), are presented in Figure 1: • The first one (on the left), called test-onsou</context>
<context position="5539" citStr="Li and Roth, 2002" startWordPosition="876" endWordPosition="879"> the right), called trainon-target, consists in translating the training corpus from English to French. We obtain an labeled French corpus, on which it is possible to learn a classification model. In the first case, classification is learned on well written questions; yet, as the test corpus is translated, translation errors may disturb the classifier. In the second case, the classification model will be learned on less well written questions, but the corpus may be large enough to compensate for the loss in quality. 4 Experiments 4.1 Question classes We used the question taxonomy proposed by (Li and Roth, 2002), which enabled us to compare our results to those obtained by (Zhang and Lee, 2003) on English. This taxonomy contains two levels: the first one contains 50 fine grained categories, the second one contains 6 coarse grained categories. Figure 2 presents a few of these categories. 4.2 Corpora For English, we used the data from (Li and Roth, 2002), which was assembled from USC, UIUC and TREC collections, and has been manually labeled according to their taxonomy. The training set contains 5,500 labeled questions, and the testing set contains 500 questions. For French, we gathered questions from s</context>
<context position="11452" citStr="Li and Roth, 2002" startWordPosition="1837" endWordPosition="1840">ns, classifier = libsvm) ing if it contains only upper case letters, and normalizing them. Table 3 gives the results with this additional feature (we only kept the method with translation of the training corpus since results were much higher). Precision is improved for both levels of the hierarchy: for fine grained classes, results increase from .794 to .837, and for coarse grained classes, from .828 to .869. Remaining classification errors are much more disparate. 5 Related work Most question answering systems include question classification, which is generally based on supervised learning. (Li and Roth, 2002) trained the SNoW hierarchical classifier for question classification, with a 50 classes fine grained hierarchy, and a coarse grained one of 6 classes. The features used are words, parts-of-speech, chunks, named entities, chunk heads and words related to a class. They obtain 98.8% correct classification of the coarse grained classes, and 95% on the fine grained one. This hierarchy was widely used by other QA systems. (Zhang and Lee, 2003) studied the classification performance according to the classifier and training dataser size, as well as the contribution of question parse trees. Their resu</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>X. Li and D. Roth. 2002. Learning question classifiers. In Proceedings of the 19th international conference on Computational linguistics-Volume 1, pages 1–7. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Solorio</author>
<author>M P´erez-Coutino</author>
</authors>
<title>A language independent method for question classification.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th international conference on Computational Linguistics,</booktitle>
<pages>1374--1380</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<marker>Solorio, P´erez-Coutino, 2004</marker>
<rawString>T. Solorio, M. P´erez-Coutino, et al. 2004. A language independent method for question classification. In Proceedings of the 20th international conference on Computational Linguistics, pages 1374–1380. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Zhang</author>
<author>W S Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval,</booktitle>
<pages>26--32</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="5623" citStr="Zhang and Lee, 2003" startWordPosition="891" endWordPosition="894">om English to French. We obtain an labeled French corpus, on which it is possible to learn a classification model. In the first case, classification is learned on well written questions; yet, as the test corpus is translated, translation errors may disturb the classifier. In the second case, the classification model will be learned on less well written questions, but the corpus may be large enough to compensate for the loss in quality. 4 Experiments 4.1 Question classes We used the question taxonomy proposed by (Li and Roth, 2002), which enabled us to compare our results to those obtained by (Zhang and Lee, 2003) on English. This taxonomy contains two levels: the first one contains 50 fine grained categories, the second one contains 6 coarse grained categories. Figure 2 presents a few of these categories. 4.2 Corpora For English, we used the data from (Li and Roth, 2002), which was assembled from USC, UIUC and TREC collections, and has been manually labeled according to their taxonomy. The training set contains 5,500 labeled questions, and the testing set contains 500 questions. For French, we gathered questions from several evaluation campaigns: QA@CLEF 2005, 2006, 2007, EQueR and Quæro 2008, 2009 an</context>
<context position="7297" citStr="Zhang and Lee, 2003" startWordPosition="1167" endWordPosition="1170">tems 2. 1This distribution is due to further constraints on the system. 2We tested other translation systems, but Google Translate gave the best results. 430 Train en en fr fr (trans.) Test en en fr fr (trans.) Method test- trainon- onsource target 50 .798 .677 .794 .769 classes 6 .90 .735 .828 .84 classes Table 1: Question classification precision for both levels of the hierarchy (features = word n-grams, classifier = libsvm) 4.3 Classification parameters The classifier used was LibSVM (Chang and Lin, 2011) with default parameters, which offers onevs-one multiclass classification, and which (Zhang and Lee, 2003) showed to be most effective for this task. We only considered surface features, and extracted bag-of-ngrams (with n = 1..2). 4.4 Results and discussion Table 1 shows the results obtained with the basic configuration, for both transfer methods. Results are given in precision, i.e. the proportion of correctly classified questions among the test questions 3. Using word n-grams, monolingual English classification obtains .798 correct classification for the fine grained classes, and .90 for the coarse grained classes, results which are very close to those obtained by (Zhang and Lee, 2003). On Fren</context>
<context position="11894" citStr="Zhang and Lee, 2003" startWordPosition="1909" endWordPosition="1912">rors are much more disparate. 5 Related work Most question answering systems include question classification, which is generally based on supervised learning. (Li and Roth, 2002) trained the SNoW hierarchical classifier for question classification, with a 50 classes fine grained hierarchy, and a coarse grained one of 6 classes. The features used are words, parts-of-speech, chunks, named entities, chunk heads and words related to a class. They obtain 98.8% correct classification of the coarse grained classes, and 95% on the fine grained one. This hierarchy was widely used by other QA systems. (Zhang and Lee, 2003) studied the classification performance according to the classifier and training dataser size, as well as the contribution of question parse trees. Their results are 87% correct classification on coarse grained classes and 80% on fine grained classes with vectorial attributes, and 90% correct classification on coarse grained classes and 80% on fine grained classes with structured input and tree kerneks. These question classifications were used for English only. Adapting the methods to other languages requires to annotated large corpora of questions. In order to classify questions in different </context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>D. Zhang and W.S. Lee. 2003. Question classification using support vector machines. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, pages 26–32. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>