<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.9994765">
A Methodology for Word Sense Disambiguation at 90%
based on large-scale CrowdSourcing
</title>
<author confidence="0.91085">
Oier Lopez de Lacalle Eneko Agirre
</author>
<affiliation confidence="0.929618">
University of the Basque Country University of the Basque Country
</affiliation>
<email confidence="0.992014">
oier.lopezdelacalle@ehu.eus e.agirre@ehu.eus
</email>
<sectionHeader confidence="0.998482" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.993267611111111">
Word Sense Disambiguation has been stuck
for many years. In this paper we explore
the use of large-scale crowdsourcing to cluster
senses that are often confused by non-expert
annotators. We show that we can increase
performance at will: our in-domain experi-
ment involving 45 highly polysemous nouns,
verbs and adjective (9.8 senses on average),
yields an average accuracy of 92.6 using a su-
pervised classifier for an average polysemy of
6.1. Our proposal has the advantage of being
cost-effective and being able to produce differ-
ent levels of granularity. Our analysis shows
that the error reduction with respect to fine-
grained senses is higher, and manual inspec-
tion show that the clusters are sensible when
compared to those of OntoNotes and WordNet
Supersenses.
</bodyText>
<sectionHeader confidence="0.99947" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999325">
Word sense ambiguity is a major hurdle for accu-
rate information extraction, summarization and ma-
chine translation. The utility of Word Sense Dis-
ambiguation (WSD) depends on the accuracy and
on how useful the sense distinctions are. The first
issue is quantitative, as it can be measured using
a WSD system on certain dataset. The second ex-
amines whether the sense distinctions are appropri-
ate, which varies from application to application.
Although usefulness can be explored in a down-
stream application (Agirre et al., 2008), it is usually
assessed subjectively, discussing the quality of the
sense distinctions (Palmer et al., 2007). Both issues
</bodyText>
<page confidence="0.516961">
61
</page>
<bodyText confidence="0.99966604">
(performance and usefulness) are linked to the gran-
ularity of the sense inventory, and conflict with each
other: finer granularity might produce more useful
distinctions but the accuracy would be worse, and
vice-versa.
WordNet (Fellbaum, 1998) is the most widely
used resource to build word sense disambiguation
tools and word sense annotated corpora, including
recent large efforts (Passonneau et al., 2012), but its
fine-grainedness has been mentioned to be a prob-
lem (Hovy et al., 2006; Palmer et al., 2007).
We think that a desiderata for a sense inventory
would be that it provides useful sense distinctions
and useful performance across a large range of appli-
cations. We would also add that it should be tightly
integrated with WordNet, given its prevalence on
NLP applications, and we thus focus on sense in-
ventories which are mapped to WordNet.
In order to asses usefulness, we need specific
measures. Downstream application is difficult, and
unfeasible for new proposals, as a full-fledged sense
inventories and associated annotations are neces-
sary. We can instead estimate usefulness of pro-
posed sense inventories using several proxy mea-
sures:
</bodyText>
<listItem confidence="0.905419">
• High polysemy. Note that polysemy alone
could be misleading, as a word with many
senses might be skewed to a single sense: 99%
of occurrences could belong to a single sense,
while the rest are only seen once. Besides the
absolute polysemy, we can use the accuracy of
the most frequent sense (MFS, estimated in
train data and applied to test data) as a simple
</listItem>
<note confidence="0.887531">
Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 61–70,
Denver, Colorado, June 4–5, 2015.
</note>
<bodyText confidence="0.822555">
and effective indication of skewness. High pol-
ysemy and low MFS are desirable properties.
</bodyText>
<listItem confidence="0.944442">
• High performance, as measured the accuracy of
a supervised system trained on hand-annotated
data. The higher the accuracy, the better.
• Flexible sense granularity, that is, the ability
to produce different degrees of polysemy and
accuracy, from fine-grained to coarse-grained.
When comparing sense inventories with differ-
ent granularities, absolute MFS and supervised
performance are not enough. We propose to use
error reduction of the supervised system with
respect to the MFS as a measure of the balance
between low MFS and high supervised perfor-
mance. The larger the error reduction the bet-
ter.
• Manual inspection of the sense distinctions, as
a complement to quantitative measures.
</listItem>
<bodyText confidence="0.999967666666667">
We propose to use crowdsourced annotations
(Passonneau and Carpenter, 2014) to cluster Word-
Net senses that are often confused by non-expert an-
notators. Our method can provide clusters at dif-
ferent levels of granularity. We show that we can
construct clusters yielding around 90% accuracy for
45 words, with higher error reduction with respect
to MFS than fine-grained senses. By construction,
we merge senses which are often confused by anno-
tators, yielding sensible sense clusters, as corrobo-
rated by manual inspection.
The paper is structured as follows. Section 2 men-
tions related work. We then present the annotations,
followed by the clustering procedure. Section 5 re-
port the main experiments. Section 6 compares our
clustering to that of OntoNotes followed by a com-
parison to WordNet Supersenses. Section 8 draws
the conclusions.
</bodyText>
<sectionHeader confidence="0.999914" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999962685185186">
Our work is close to (Passonneau and Carpenter,
2014) in that we use the same dataset and anno-
tations presented in that work. They present a
comparison of conventional expert-guided annota-
tion model with a probabilistic annotation model
that does not take agreement into account.
Previous efforts to cluster WordNet senses in
order to produce coarse-grained inventories have
shown that improved results can be obtained, but
we think our approach fits the desiderata better.
For instance, clustering together senses which have
the same Semantic File (also called Supersenses)
allowed the best supervised WSD system to date
(Zhong and Ng, 2010) to increase accuracy from
58.3% to 82.6% in the Semeval 2007 all-words
dataset (Navigli et al., 2007). Semantic Files are
useful, but don’t allow to provide flexible sense in-
ventories.
The OntoNotes project (Hovy et al., 2006) de-
vised a manual grouping method which explicitly
sought 90% accuracy. Although the method was
shown to be successful, the fixed sense groupings
had to be produced manually, included complex
mappings to WordNet (cf. Section 6), and was a
limited exercise, with annotations for around 4900
words. Our work is similar in spirit to OntoNotes,
but use a different methodology which allows for
flexible granularity, as the annotation is done at the
fine-grained level, and the clustering is done later
fully automatically.
Automatic clustering algorithms are not new.
(Tou Ng et al., 1999) propose to use annotator
agreement to cluster senses, reporting higher inter-
annotator agreement after clustering. We are in part
inspired by their approach, as we extend it from two
annotators to a sample with 25 annotators, and vali-
date the approach with WSD systems.
The rest of approaches use other sources of in-
formation. Peters et al. (1998) make use of the
WordNet hierarchy to group close senses. Mihal-
cea and Moldovan (1999) present similar approach
that is based in the structure of WordNet. To-
muro (2001) presents are more principled algorithm
based on Minimum Description Length. A work
which is closely related to our work is (Agirre and
Lopez de Lacalle, 2003), in which they examine
a variety of information sources to cluster Word-
Net word senses, including a hierarchical cluster-
ing based on distributional information. Snow et
al. (2007) present a supervised learning algorithm
that learns merging senses and make use of wide
range of WordNet-based and corpus-based features.
(Navigli et al., 2007) mapped WordNet to the top
level sense distinctions in the Oxford Dictionary of
</bodyText>
<page confidence="0.998446">
62
</page>
<bodyText confidence="0.9958165">
English. All the above rely on automatic measures,
while our method is based on human annotations.
</bodyText>
<sectionHeader confidence="0.995828" genericHeader="method">
3 MASC Crowdsourced annotations
</sectionHeader>
<bodyText confidence="0.9989313125">
The corpus used in the experiments is part of the
Manually Annotated Sub-Corpus of the Open Amer-
ican National Corpus, which contains a subsidiary
word sense sentence corpus consisting of approxi-
mately one thousand sentences per word annotated
with WordNet 3.0 sense labels (Passonneau et al.,
2012). In this work we make use of a publicly avail-
able subset of 45 words (17 nouns, 19 verbs and
9 adjective, see Table2) that have been annotated,
1000 sentences per target word, using crowdsourc-
ing (Passonneau and Carpenter, 2014). The authors
collected between 20 and 25 labels for every sen-
tence. They showed that a probabilistic annotation
model based on crowdsourced data was effective,
with favorable quality when compared to a conven-
tional expert-guided annotation model.
</bodyText>
<sectionHeader confidence="0.977595" genericHeader="method">
4 Clustering Procedure
</sectionHeader>
<bodyText confidence="0.9999774375">
Having access to multiple annotations of the same
item allows to identify correlations among senses
of a word. In particular, we can mine how many
times the annotators confused 2 particular senses of
a word. If two senses are confused very often, it
will signal that the annotators find the differences
between the two senses difficult to discriminate in
context. We also want to note that, in some cases, the
context might be underspecified, and several senses
might hold at the same time. We left this second
phenomena for a future study.
We built a confusion matrix for each target word
counting how many times two distinct senses are an-
notated in the same instance. More formally, the
confusion of two senses of a target word conf(s1, s2)
is defined as follows:
</bodyText>
<equation confidence="0.9956705">
ff((yi,n = s1 ∧ yi,m = s2)∨
(yi,n = s2 ∧ yi,m = s1))
</equation>
<bodyText confidence="0.999982305555555">
where I is number of instances of the word, Ji is the
number of turkers that annotated instance i, and yi,m
is the annotation of turker m in instance i. Finally,
ff(s) = 1 iff the condition expressed in s is true.
We cluster the senses based on the information in
the confusion matrix, i.e. two senses (s1, s2) will
tend to be in the same cluster if conf(s1, s2) is high.
We used agglomerative hierarchical clustering for
the sake of simplicity, as we obtain one hierarchy of
senses in one go, and then used different cuts in the
hierarchy to obtain clusters of different sense granu-
larities.
In order to obtain the target coarse-grained inven-
tory, the procedure was the following: (0) we start
at the leaves of the hierarchy, that is, with the fine-
grained senses; (1) we train and test a word sense
disambiguation algorithm on development data us-
ing the current sense distinctions (see the next Sec-
tion for details); (2) if the accuracy is higher than
90%, or if there are only two senses left, we stop
and output the current sense distinctions; (3) we go
up one level in the hierarchical cluster, joining to-
gether the two senses with highest confusion score,
and go to step (1). Note that the algorithm does not
guarantee obtaining 90% on the training data. Once
the coarse-grained senses are obtained, we train the
word sense disambiguation on the development data
and test over held-out data, yielding the final accu-
racy scores.
In order to contrast results, we also produced hier-
archies of senses based on random clustering, where
the clusters yield the same sense granularity as those
of the confusion-based clustering explained above.
We produced 10 random clustering for each word,
and averaged over the runs to obtain the final accu-
racy.
</bodyText>
<sectionHeader confidence="0.999279" genericHeader="method">
5 Experiments
</sectionHeader>
<bodyText confidence="0.999760916666667">
The gold standard is based on the multiple annota-
tions in the corpus, but a single sense was selected
as the correct one, following (Passonneau and Car-
penter, 2014), which use a probabilistic annotation
model (Dawid and Skene, 1979). We split the 1000
examples for each word into development and test,
sampling 85% (and 15% respectively) at random,
preserving the overall sense distribution.
The Word Sense Disambiguation algorithm of
choice is It Make Sense (IMS) (Zhong and Ng,
2010), which reports the best WSD results to date.
IMS is a freely available Java implementation1,
</bodyText>
<footnote confidence="0.66801">
1http://www.comp.nus.edu.sg/˜nlp/
</footnote>
<figure confidence="0.988912363636364">
Ji
n=m+1
1
1
I
(Ji )
2
Ji−1�
m=1
I
i=1
</figure>
<page confidence="0.994642">
63
</page>
<bodyText confidence="0.99374348">
which provides an extensible and flexible platform
for researchers interested in using a WSD compo-
nent. Following Lee and Ng (2002), IMS adopts
support vector machines as the classifier and inte-
grates the state of the features extractors including
parts-of-speech of the surrounding words, bag of
words features, and local collocations as features.
IMS provides ready-to-use models trained with
examples collected from parallel texts , SEM-
COR (Miller et al., 1993), and the DSO corpus
(Ng and Lee, 1996). In our experiments we train
IMS with the train examples of the crowdsourced
MASC. We used IMS out-of-the-box, using the
default parametrization and built-in feature extrac-
tion. We compare results obtained with IMS against
the Most Frequent Sense (MFS), which was es-
timated using the training corpus. Both systems
(IMS and MFS) could be trained on fine-grained
senses, on coarse-grained senses induced from the
confusion matrix using the 90% threshold described
above (Coarseconf) and coarse-grained senses in-
duced from random clustering using the 90% thresh-
old (Coarserandom). We also used sense clusters from
OntoNotes and WordNet Supersenses (cf. Sections
6 and 7) .
</bodyText>
<subsectionHeader confidence="0.998252">
5.1 Main results
</subsectionHeader>
<bodyText confidence="0.985541842105263">
The results of the six systems on development and
test data are shown in Table 1, showing that we suc-
cessfully attained an accuracy over 90% on average.
The results for random clustering show that not any
clustering yields meaningful results. Due to varia-
tion of the random sense-hierarchies, we calculated
the upper and lower margins with 95% of confidence
level (79.2-80.0 accuracy in test). The results show
that random clustering performs significantly lower
than the confusion based clustering. The results in
development and test are very similar, confirming
that the confusion information is stable in our in-
domain scenario.
All in all, as Table 2 shows, 30 words out of the
45 attain an accuracy higher than 90% in test (14
out of 17 nouns, 11 out of 19 verbs and 5 out of
9 adjectives). The precision for the words which
do not attain 90% is 87.4% on average, and 85.4%
for adjective, being the lowest. The polysemy is re-
</bodyText>
<table confidence="0.885343833333333">
software.html
Development Test
MFS IMS MFS IMS
Fine-grained 47.2 73.2 46.2 73.1
Coarserandom 60.4 79.9 60.2 79.6
Coarseconf 84.2 92.9 84.1 92.6
</table>
<tableCaption confidence="0.994404666666667">
Table 1: Development and test results using cross-
validation (left side) and test results (right side) for IMS
and MFS using three sense inventories.
</tableCaption>
<figure confidence="0.444933461538461">
MFS IMS
92.6
84.1
79.6
73.1
60.2
46.2
MFS IMS
72.5
70.4
26 24.2
0
0
</figure>
<figureCaption confidence="0.998525333333333">
Figure 1: Accuracy on test of sense granularities and
methods (top) and error reduction with respect to fine-
grained (bottom), for the three sense inventories.
</figureCaption>
<bodyText confidence="0.995998">
duced from 9.8 to 6.2.The appendix shows detailed
information for each target word. In all but 3 words
coarse-grained accuracy is above fine-grained. Note
that MFS and IMS produce the same results in 11
words out of 45. We will revisit MFS in Section 6.
Figure 1 plots, on top, the results (on test) grouped
on MFS and IMS for easier comparison. The figure
also plots the error reduction of each coarse-grained
inventory with respect to fine-grained. The higher
</bodyText>
<figure confidence="0.808586516129033">
90
80
70
60
50
Accuracy
40
30
80
70
60
50
40
30
20
Error Reduction
10
fine rand conf fine rand conf
fine rand conf fine rand conf
64
fine rand conf
92.6
84.1
79.6
73.1
60.2
46.2
fine rand conf
53.4
50 48 .7
0 0 0
</figure>
<figureCaption confidence="0.970873666666667">
Figure 2: Accuracy of sense granularities and methods
(top) and error reduction of IMS with respect to MFS
(bottom), for the three sense inventories.
</figureCaption>
<bodyText confidence="0.961218777777778">
error reduction of our coarse-grained inventory with
respect to the random clustering shows that the clus-
ters are meaningful, and that the performance gain
is not an artifact of reducing the sense inventory2.
Figure 2 plots, on top, the results (on test) grouped
on each sense inventory. The figure also plots the er-
ror reduction of IMS with respect to MFS in each
coarse-grained inventory. The better error reduc-
tion of IMS with respect to MFS for our coarse-
grained inventory shows that our clusters are eas-
ier to learn, in that reducing the sense inventory in-
creases the delta with respect to the MFS baseline.
Note that reducing the sense inventory is not enough
to show this effect, as exemplified by the fact that
the error reduction for the random clusters is lower
than for the fine-grained senses.
2Note that, by construction, Coarserandom and Coarseconf have
the same granularity.
</bodyText>
<subsectionHeader confidence="0.998203">
5.2 Flexible clustering
</subsectionHeader>
<bodyText confidence="0.999995807692308">
As we reached 90% of accuracy with relatively high
polysemy, we also checked whether MFS could
reach 90% of accuracy if we continued to cluster
senses. The experiments in development confirmed
that MFS gets above 90% at expenses of coarser
grained senses than IMS does: On average, the fine-
grained polysemy (9.8) would drop to 4.4, compared
to the 6.2 when clustering to reach 90% using IMS.
When we obtain MFS&gt;90% 17 words have 2 senses
and 40 words reach to 90% of accuracy, whilst when
IMS&gt;90% only 5 words are reduced to 2 senses and
42 words reach to 90%. This shows that it makes
more sense to cluster senses using the performance
of IMS as stopping criteria, as the polysemy is pre-
served better.
In case we continued clustering senses until we
have 2 senses for each word, IMS would reach
98.2% and MFS 95.7%, with an error reduction of
58% over the MFS. Note that this error reduction
compares favorably to that of our clustering when
stopping at IMS&gt;90%, showing that we could have
kept clustering senses further without losing predic-
tive power. These figures show that we could stop at
arbitrary performance figures at the cost of obtaining
highly skewed clusters (indicated by the high MFS
value). We will revisit high MFS in Section 6.
</bodyText>
<sectionHeader confidence="0.974424" genericHeader="method">
6 Comparison to OntoNotes senses
</sectionHeader>
<bodyText confidence="0.999970882352941">
In order to perform a qualitative study and check
whether our sense clusters make sense, we decided
to compare them to another coarse sense inventory
which is mapped to WordNet. We chose Ontonotes
5.0 (Hovy et al., 2006), which also had the goal of at-
taining 90% sense accuracy. Alternatively, we could
have used the Oxford Dictionary of English, which
was mapped automatically to WordNet 2.1 (Navigli
et al., 2007) but we preferred to factor out automatic
mappings and version differences from the analysis.
Ontonotes contains lexical entries for 35 of our
target words. The relation between the sense inven-
tory of WordNet and OntoNotes is complex. Given
that our work clusters WordNet senses, we focused
on the 18 words where the OntoNotes senses where
composed of one or several WordNet senses and
where all WordNet senses were covered3. Table 2
</bodyText>
<footnote confidence="0.868864">
3The rest of words include senses not mapped to Word-
</footnote>
<figure confidence="0.9765010625">
90
80
70
60
50
Accuracy
40
30
50
40
30
20
10
Error Reduction
MFS IMS MFS IMS MFS IMS
MFS IMS MFS IMS MFS IMS
</figure>
<page confidence="0.995609">
65
</page>
<table confidence="0.999918163265306">
Word Fine-grained #S Conf Mfs Random #S ON Mfs #S SS Mfs
#S IMS Mfs IMS IMS Mfs IMS IMS
common-j 10 70.5 39.0 7 87.0 83.6 74.3 46.9 - - - 1 100 100
fair-j 11 88.8 67.3 11 88.8 67.3 88.8 67.3 - - - 2 95.9 93.9
full-j 9 78.5 45.1 8 92.4 66.7 80.2 54.2 - - - 2 98.6 98.6
high-j 8 86.3 71.9 7 94.5 86.3 87.1 74.6 - - - 2 97.9 92.5
late-j 8 84.9 52.1 7 87.0 70.5 87.5 58.5 - - - 2 98.6 98.6
long-j 10 78.0 49.3 6 98.7 98.7 79.8 53.6 - - - 2 90.0 86.7
normal-j 5 81.9 66.0 4 85.4 79.9 83.8 72.2 - - - 1 100 100
particular-j 7 85.0 51.0 6 94.6 66.7 89.5 60.0 - - - 1 99.3 99.3
poor-j 6 76.2 52.4 2 94.6 62.6 88.2 73.6 - - - 2 100 100
board-n 10 88.9 79.9 9 93.8 93.1 89.8 80.9 7 88.9 79.9 5 89.6 79.9
book-n 12 56.3 64.4 11 88.9 89.6 58.3 65.3 - - - 5 65.9 71.1
color-n 9 65.5 32.4 2 99.3 99.3 85.6 76.8 - - - 4 84.8 73.8
control-n 12 79.5 46.6 6 96.6 93.8 82.8 52.0 - - - 8 78.1 46.6
date-n 9 80.1 24.1 4 91.5 65.2 84.2 43.4 - - - 5 91.5 85.1
family-n 9 64.3 26.6 2 100 100 79.7 64.9 - - - 3 99.3 99.3
image-n 10 70.6 49.0 7 90.9 85.3 74.6 59.0 - - - 7 77.6 60.8
land-n 12 57.6 20.8 6 96.5 92.4 62.9 43.2 - - - 6 62.5 28.5
level-n 9 69.9 52.1 7 94.5 94.5 74.6 59.3 7 85.6 77.4 5 77.4 53.4
life-n 15 58.0 21.7 5 89.5 88.8 67.3 43.8 - - - 10 65.0 46.2
number-n 12 87.7 71.2 11 92.5 86.3 89.6 73.9 - - - 5 89.0 71.2
paper-n 8 76.4 41.0 2 100 100 87.7 72.4 - - - 5 83.3 74.3
sense-n 6 93.8 38.6 6 93.8 38.6 93.8 38.6 6 93.8 38.6 3 94.5 66.9
time-n 11 90.1 48.4 7 94.5 93.4 89.7 51.7 - - - 5 92.9 48.4
way-n 13 72.1 55.8 7 91.2 78.9 78.6 62.4 - - - 8 78.2 59.9
window-n 9 75.9 38.6 3 91.7 60.7 84.9 59.3 - - - 4 90.3 62.8
work-n 8 69.8 20.5 2 85.9 79.5 80.9 63.1 - - - 5 75.1 38.5
add-v 7 40.3 49.3 2 91.0 91.0 74.3 77.1 3 90.3 90.3 6 40.3 49.3
appear-v 8 64.4 47.3 5 87.7 63.0 69.2 59.6 5 87.7 63.0 5 87.7 63.0
ask-v 8 78.3 36.4 6 96.5 96.5 83.9 53.8 - - - 3 100 100
find-v 17 62.4 28.4 13 86.5 85.8 65.7 30.8 6 80.9 58.9 7 75.2 41.8
fold-v 6 93.2 83.7 6 93.2 83.7 93.2 83.7 5 93.2 83.7 4 94.6 83.7
help-v 9 61.2 36.0 6 99.3 97.1 69.3 48.7 3 100 97.8 4 73.4 59.7
kill-v 16 63.9 59.7 11 89.6 86.8 67.0 62.5 9 89.6 86.8 7 82.6 81.2
know-v 12 63.1 35.4 7 89.2 77.9 64.8 40.3 7 81.5 48.7 2 100 100
live-v 8 73.5 47.6 3 97.3 94.6 82.5 66.9 - - - 3 91.2 87.8
lose-v 12 64.4 50.7 3 93.8 93.8 76.6 69.1 6 70.5 58.2 7 75.3 64.4
meet-v 14 69.7 28.9 2 86.6 59.9 83.5 61.1 7 82.4 52.1 8 78.9 59.9
read-v 12 82.8 73.9 11 85.1 80.6 84.5 76.1 8 82.1 75.4 4 91.0 88.8
say-v 12 64.9 35.7 7 96.1 96.1 67.5 44.0 6 96.1 92.2 3 100 100
serve-v 16 73.1 40.7 11 88.3 83.4 76.4 46.1 7 80.0 49.7 6 81.4 65.5
show-v 13 75.7 27.5 9 94.2 93.7 79.1 41.6 - - - 5 81.5 32.8
suggest-v 5 74.3 63.5 3 97.3 97.3 81.0 73.3 3 91.2 81.8 1 99.3 99.3
tell-v 9 57.6 38.9 6 86.1 83.3 69.2 54.6 4 94.4 92.4 3 97.9 97.2
wait-v 5 70.2 36.6 3 96.2 92.4 82.2 66.1 3 96.2 92.4 4 76.3 64.9
win-v 5 72.8l 60.5 2 100 99.3 87.9 82.3 - - - 4 76.9 70.1
AVG 45 words 9.8 73.1 46.2 6.2 92.6 84.1 79.6 60.2 - - - 4.7 84.2 73.3
AVG 18 words 10.2 69.9 48.8 6.0 91.5 83.2 76.8 59.6 5.7 88.0 73.2 4.3 86.2 74.3
</table>
<tableCaption confidence="0.959303">
Table 2: The 45 words, with PoS, polysemy, IMS and Mfs accuracy for fine-grained, our clustering (Conf.), random
clustering, OntoNotes coarse-grained senses (ON, cf. Section 6) and Supersenses (SS, cf. Section 7). The bottom
rows report averages for the 45 words and the 18 words in OntoNotes.
</tableCaption>
<page confidence="0.925755">
66
</page>
<tableCaption confidence="0.8064845">
Net, or cases where one WordNet sense was mapped to several
OntoNotes senses
</tableCaption>
<table confidence="0.999852">
Conf ON WN # Gloss
1 1 1 421 Make an addition (to); join or combine or unite with others; increase the quality,
1 1 quantity, size or scope of
1 1
1 1
2 115 State or say further
6 94 Constitute an addition
3 92 Bestow a quality on
2 2 4 47 Make an addition by combining numbers
2 2
5 44 Determine the sum of
</table>
<tableCaption confidence="0.85181">
Table 3: Senses for add-v in WN, OntoNotes (ON) and our clusters (Conf), including frequencies in train and glosses.
</tableCaption>
<table confidence="0.9319068">
Conf ON WN # Gloss
1 2 1 449 a position on a scale of intensity or amount or quality
1 2
1
2 197 a relative position or degree of value in a graded group
</table>
<figure confidence="0.985324571428571">
6 3 143 a specific identifiable position in a continuum or series or especially in a process
5 5 7 18 an abstract place usually conceived as having depth
5
6 8 13 a structure consisting of a room or set of rooms at a single position along a vertical scale
2 1 4 11 height above ground
4 3 6 3 a flat surface at right angles to a plumb line
3 4 5 2 indicator that establishes the horizontal when a bubble is centered in a tube of liquid
</figure>
<tableCaption confidence="0.980952">
Table 4: Senses for level-v in WN, OntoNotes (ON) and our clusters (Conf), including frequencies in train and glosses.
</tableCaption>
<table confidence="0.999500285714286">
Conf ON WN # Gloss
1 1 1 285 give help or assistance; be of service
1 1
1 1
1 1
1
3 217 be of use
6 194 contribute to the furtherance of
2 80 improve the condition of
2 4 21 abstain from doing; always used with a negative
5 3 8 4 improve; change for the better
3 2 5 0 help to some food; help with food or drink
2
4 7 0 take or use
</table>
<tableCaption confidence="0.995945">
Table 5: Senses for help-n in WN, OntoNotes (ON) and our clusters (Conf), including frequencies in train and glosses.
</tableCaption>
<bodyText confidence="0.999901103448276">
lists those 18 words. We leave the analysis of the
rest of words for further work, as they raise issues
about overlapping sense boundaries, and our main
goal is to check the quality of our method to group
fine-grained senses.
Table 2 shows the statistics for those 18 words.
Overall, the average polysemy of our clusters is
higher and the performance of IMS on our clusters is
also higher. We take this as an indication of the good
quality of our clusters. On the other hand, the MFS
on our clusters is considerably higher, which could
mean that our algorithm has a tendency to lump to-
gether frequent senses, casting doubts on the quality
of the clusters.
We selected three words for illustration, depend-
ing on the difference in number of clusters. Tables
3 to 5 show the senses of those four words4. In
the case of add-v (Table 3), the clusters produced
by our algorithm are the same as OntoNotes. For
level-n (Table 4), although the number of clusters is
the same, we group WordNet sense #3 together with
senses #1 and #2, while OntoNotes keeps it sepa-
rate. Note that sense #3 is very frequent, and as such
it is lumped into a coarse grained sense which covers
most of the occurrences. WordNet sense #8, on the
contrary, is grouped by Ontonotes with #7, while we
keep them separate. We think that in both cases, one
could argue that our clusters make as much senses
as those of OntoNotes, even if the distribution of our
</bodyText>
<footnote confidence="0.975767">
4Note that coarse senses not in WordNet are not included.
</footnote>
<page confidence="0.998354">
67
</page>
<bodyText confidence="0.994067369565217">
cluster is more skewed than that of Ontonotes. only coarse sense in Supersenses groups the arith-
In the case of help-v (Table 5) our clusters pro- metic operation with state or say further, begin both
duces more senses than those in Ontonotes. We communication senses, while our algorithm keeps
think that sense #4, which is always used with a neg- groups them in separate sense clusters.
ative, can be sensibly considered a separate sense. For level-n (Table 7) our algorithm produces more
Senses #5 and #7 are very similar, but being unat- senses than the number of supersenses (6 vs 4). Su-
tested in the train data, our algorithm is unable to persenses of state and attribute are distributed be-
cluster them. tween our clusters #1 and #2. Our clusters #3, #4
In summary, the analysis of those (and other) ex- and #6 are lumped together as an artifact, although
amples shows that, in general, the sense clusters pro- it would make sense to keep them separated. Finally,
duced by our algorithm make sense. In a way, this in the case of help-n (Table 8), we obtain the same
was to be expected, as the clustering decisions de- amount of senses, but grouping differs considerably.
pend on how often the volunteers confused the use For example, WordNet senses #3 and #4 are grouped
of two senses. Our analysis also shows that our clus- under the stative supersense, although the definition
tering does have an undesired tendency to cluster and use of the two senses are completely different.
together frequent senses, while senses which occur On the other hand, our cluster #1 comprises the most
rarely the train data are usually kept separate, adding frequent 4 senses.
artificially to the overall polysemy figure. Overall, the comparison of supersenses and our
In the future we would like to study whether it is confusion-based coarse grained senses show com-
possible to make our algorithm more robust to this plicated overlaps, contrary to OntoNotes, in which
tendency to join frequent senses, perhaps discount- most of the clusters in one are subsumed in the other.
ing frequency from confusion measures. Each of the sense groupings represent very different
7 Comparison to Supersenses sense inventories. This shows the difficulty of hav-
We also perform a qualitative study comparing our ing a universal sense representation that is useful for
coarse grained senses to WN Supersenses. Super- any application at hands. Actually, the choice of the
senses are based on the lexicographer file names for inventory will depend on the angle of the meanings
WordNet, where all senses of the word that belong to required by the application.
the same lexicographer file (e.g. the artifact file) are 8 Conclusions and Future Work
joined together. They include 15 sense for verbs and This work explores the use of crowdsourced an-
26 for nouns. Although WordNet also provide su- notations to cluster senses that are often confused
persenses for adjective and adverbs, these are not se- by non-expert annotators. Our method can provide
mantically motivated and do not provide any higher clusters at different levels of granularity. We show
abstraction (Johannsen et al., 2014). that, for instance, we can construct clusters yielding
Table 2 show the results for the target 45 words around 90% accuracy for 45 words, with higher er-
(adjectives included). The average polysemy of ror reduction with respect to MFS than fine-grained
the supersenses is lower for all parts of speech senses. By construction, we merge senses which
with respect to our clustered senses and OntoNotes. are often confused by annotators, yielding sensible
Note that, word-wise, polysemy varies significantly: sense clusters, as corroborated by manual inspec-
many words keep one or two senses, while others tion. The comparison to OntoNotes groupings fares
maintain high polysemy level (roughly similar to well, with similar groupings, while the comparison
fine-grained senses). IMS and MFS performances to Supersenses shows that Supersenses follow a dif-
are similar to OntoNotes. ferent grouping criterion, with overlapping clusters.
Tables 6 to 8 show the differences in clustering The main weakness of our method seems to be the
for the same set of words (add-v, level-n, and help- tendency to cluster together frequent senses.
n). In the case of add-v (Table 6), we produce two This work is a small contribution towards the
coarse grained sense against the 5 supersenses. The design of an ambitious annotation effort enabling
</bodyText>
<table confidence="0.963112153846154">
68
Conf SS WN # Gloss
1 change 1 421 make an addition (to); join or combine or unite with others; increase the qual-
1 ity, quantity, size or scope of
1
1
stative 6 94 constitute an addition
possession 3 92 bestow a quality on
communication 2 115 state or say further
communication
2 5 44 determine the sum of
2
cognition 4 47 make an addition by combining numbers
</table>
<tableCaption confidence="0.5198">
Table 6: Senses for add-v in WN, Supersenses (SS) and our clusters (Conf), including frequencies in train and glosses.
</tableCaption>
<table confidence="0.782019857142857">
Conf SS WN # Gloss
1 state 2 197 a relative position or degree of value in a graded group
1 state
1
3 143 a specific identifiable position in a continuum or series or especially in a process
attribute 1 449 a position on a scale of intensity or amount or quality
attribute
2 4 11 height above ground
5 cognition 7 18 an abstract place usually conceived as having depth
6 artifact 8 13 a structure consisting of a room or set of rooms at a single position along a vert.
artifact scale
artifact
4 6 3 a flat surface at right angles to a plumb line
3 5 2 indicator that establishes the horizontal when a bubble is centered in a tube of liq.
</table>
<tableCaption confidence="0.766884">
Table 7: Senses for level-n in WN, Supersenses (SS) and our clusters (Conf), including frequencies in train and glosses.
</tableCaption>
<table confidence="0.999544642857143">
Conf SS WN # Gloss
1 social 1 285 give help or assistance; be of service
1 social
1
1
6 194 contribute to the furtherance of
body 2 80 improve the condition of
stative 3 217 be of use
stative
2 4 21 abstain from doing; always used with a negative
5 change 8 4 improve; change for the better
3 consumption 5 0 help to some food; help with food or drink
consumption
4 7 0 take or use
</table>
<tableCaption confidence="0.995605">
Table 8: Senses for help-n in WN, Supersenses (SS) and our clusters (Conf), including freq. in train and glosses.
</tableCaption>
<bodyText confidence="0.999720111111111">
widespread use of high accuracy WSD. For the near
future we would like to improve the error reduction
with respect to the MFS trying to factor out sense
frequency from clustering decisions. We would also
like to check out-of-domain corpora, and to contrast
the results of our confusion-based clusters with re-
spect to other sense-clustering methods. Finally, we
are aware that the final validity our technique needs
to be shown in a downstream application.
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998182571428572">
This work was partially funded by MINECO
(CHIST-ERA READERS project – PCIN-2013-
002-C02-01, and SKaTeR project – TIN2012-
38584-C06-02), and the European Commission
(QTLEAP – FP7-ICT-2013.4.1-610516). The IXA
group is funded by the Basque Government (A type
Research Group).
</bodyText>
<sectionHeader confidence="0.999557" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.994757444444444">
Eneko Agirre and Oier Lopez de Lacalle. 2003. Clus-
tering wordnet word senses. In Proceedings of the
Conference on Recent Advances on Natural Language
Processing (RANLP’03), pages 11–18, Bulgary.
E. Agirre, T. Baldwin, and D. Martinez. 2008. Im-
proving Parsing and PP attachment Performance with
Sense Information. In Proceedings of the 46th Annual
Meeting of the Association for Computational Linguis-
tics (ACL/HLT’08), pages 317–325, Columbus, USA.
</reference>
<page confidence="0.98411">
69
</page>
<reference confidence="0.999685313253012">
A. P. Dawid and A. M. Skene. 1979. Maximum likeli-
hood estimation of observer error-rates using the em
algorithm. Applied Statistics, 28(1):20–28.
C. Fellbaum. 1998. WordNet: An Electronic Lexical
Database. MIT Press.
Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance
Ramshaw, and Ralph Weischedel. 2006. Ontonotes:
The 90% solution. In Proceedings of HLT-NAACL
2006, pages 57–60.
Anders Johannsen, Dirk Hovy, H´ector Martinez Alonso,
Barbara Plank, and Anders Søgaard. 2014. More
or less supervised supersense tagging of twitter. In
Proceedings of the Third Joint Conference on Lexical
and Computational Semantics (*SEM 2014), pages 1–
11, Dublin, Ireland, August. Association for Compu-
tational Linguistics and Dublin City University.
Yoong Keok Lee and Hwee Tou Ng. 2002. An empiri-
cal evaluation of knowledge sources and learning algo-
rithms for word sense disambiguation. In Proceedings
of the 2002 Conference on Empirical Methods in Nat-
ural Language Processing, pages 41–48. Association
for Computational Linguistics, July.
Rada Mihalcea and Dan I. Moldovan. 1999. A method
for word sense disambiguation of unrestricted text. In
Proceedings of the 37th Annual Meeting of the Associ-
ation for Computational Linguistics, pages 152–158,
College Park, Maryland, USA, June. Association for
Computational Linguistics.
G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993.
A Semantic Concordance. In Proceedings of the work-
shop on Human Language Technology (HLT’93).
R. Navigli, K. C. Litkowski, and O. Hargraves. 2007.
SemEval-2007 Task 07: Coarse-Grained English All-
Words Task. In Proceedings of the 4th International
Workshop on Semantic Evaluations (SemEval-2007)
in conjunction with ACL, pages 30–35, Prague, Czech
Republic.
H. T. Ng and H. B. Lee. 1996. Integrating multiple
knowledge sources to disambiguate word sense: an
exemplar-based approach. In Proceedings of the 34th
annual meeting on Association for Computational Lin-
guistics, ACL ’96, pages 40–47, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Martha Palmer, Hoa Trang Dang, and Christiane Fell-
baum. 2007. Making fine-grained and coarse-grained
sense distinctions, both manually and automatically.
Natural Language Engineering, 13(2):137–163.
Rebecca J. Passonneau and Bob Carpenter. 2014. The
benefits of a model of annotation. Transactions of
the Association for Computational Linguistics, 2(1-
9):311–326.
Rebecca J. Passonneau, Collin F. Baker, Christiane Fell-
baum, and Nancy Ide. 2012. The MASC word sense
corpus. In Nicoletta Calzolari (Conference Chair),
Khalid Choukri, Thierry Declerck, Mehmet Uur Doan,
Bente Maegaard, Joseph Mariani, Asuncion Moreno,
Jan Odijk, and Stelios Piperidis, editors, Proceed-
ings of the Eight International Conference on Lan-
guage Resources and Evaluation (LREC’12), Istanbul,
Turkey, may. European Language Resources Associa-
tion (ELRA).
W. Peters, I. Peters, , and P. Vossen. 1998. Auto-
matic sense clustering in eurowordnet. In Proceedings
of the First International Conference on Language
Resources and Evaluation, pages 409–416, Granada,
Spain.
Rion Snow, Sushant Prakash, Daniel Jurafsky, and An-
drew Y. Ng. 2007. Learning to merge word senses.
In Proceedings of the 2007 Joint Conference on Em-
pirical Methods in Natural Language Processing and
Computational Natural Language Learning (EMNLP-
CoNLL), pages 1005–1014, Prague, Czech Republic,
June. Association for Computational Linguistics.
Noriko Tomuro. 2001. Tree-cut and a lexicon based on
systematic polysemy. In Proceedings of the Second
Meeting of the North American Chapter of the Associ-
ation for Computational Linguistics, Pittsburgh, USA.
Hwee Tou Ng, Chung Yong Lim, and Shou King Foo.
1999. Siglex99: Standardizing lexical resources.
Z. Zhong and H. T. Ng. 2010. It makes sense: A wide-
coverage word sense disambiguation system for free
text. In Proceedings of the ACL 2010 System Demon-
strations, pages 78–83, Uppsala, Sweden.
</reference>
<page confidence="0.998464">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.119054">
<title confidence="0.9927955">A Methodology for Word Sense Disambiguation at based on large-scale CrowdSourcing</title>
<author confidence="0.890917">Oier Lopez de_Lacalle Eneko Agirre</author>
<affiliation confidence="0.92395">University of the Basque Country University of the Basque Country</affiliation>
<abstract confidence="0.91998145">oier.lopezdelacalle@ehu.eus e.agirre@ehu.eus Abstract Word Sense Disambiguation has been stuck for many years. In this paper we explore the use of large-scale crowdsourcing to cluster senses that are often confused by non-expert annotators. We show that we can increase performance at will: our in-domain experiment involving 45 highly polysemous nouns, verbs and adjective (9.8 senses on average), yields an average accuracy of 92.6 using a supervised classifier for an average polysemy of 6.1. Our proposal has the advantage of being cost-effective and being able to produce different levels of granularity. Our analysis shows that the error reduction with respect to finegrained senses is higher, and manual inspection show that the clusters are sensible when compared to those of OntoNotes and WordNet Supersenses.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Oier Lopez de Lacalle</author>
</authors>
<title>Clustering wordnet word senses.</title>
<date>2003</date>
<booktitle>In Proceedings of the Conference on Recent Advances on Natural Language Processing (RANLP’03),</booktitle>
<pages>11--18</pages>
<marker>Agirre, de Lacalle, 2003</marker>
<rawString>Eneko Agirre and Oier Lopez de Lacalle. 2003. Clustering wordnet word senses. In Proceedings of the Conference on Recent Advances on Natural Language Processing (RANLP’03), pages 11–18, Bulgary.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Agirre</author>
<author>T Baldwin</author>
<author>D Martinez</author>
</authors>
<title>Improving Parsing and PP attachment Performance with Sense Information.</title>
<date>2008</date>
<booktitle>In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL/HLT’08),</booktitle>
<pages>317--325</pages>
<location>Columbus, USA.</location>
<contexts>
<context position="1546" citStr="Agirre et al., 2008" startWordPosition="237" endWordPosition="240">sters are sensible when compared to those of OntoNotes and WordNet Supersenses. 1 Introduction Word sense ambiguity is a major hurdle for accurate information extraction, summarization and machine translation. The utility of Word Sense Disambiguation (WSD) depends on the accuracy and on how useful the sense distinctions are. The first issue is quantitative, as it can be measured using a WSD system on certain dataset. The second examines whether the sense distinctions are appropriate, which varies from application to application. Although usefulness can be explored in a downstream application (Agirre et al., 2008), it is usually assessed subjectively, discussing the quality of the sense distinctions (Palmer et al., 2007). Both issues 61 (performance and usefulness) are linked to the granularity of the sense inventory, and conflict with each other: finer granularity might produce more useful distinctions but the accuracy would be worse, and vice-versa. WordNet (Fellbaum, 1998) is the most widely used resource to build word sense disambiguation tools and word sense annotated corpora, including recent large efforts (Passonneau et al., 2012), but its fine-grainedness has been mentioned to be a problem (Hov</context>
</contexts>
<marker>Agirre, Baldwin, Martinez, 2008</marker>
<rawString>E. Agirre, T. Baldwin, and D. Martinez. 2008. Improving Parsing and PP attachment Performance with Sense Information. In Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics (ACL/HLT’08), pages 317–325, Columbus, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A P Dawid</author>
<author>A M Skene</author>
</authors>
<title>Maximum likelihood estimation of observer error-rates using the em algorithm.</title>
<date>1979</date>
<journal>Applied Statistics,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="11255" citStr="Dawid and Skene, 1979" startWordPosition="1827" endWordPosition="1830"> test over held-out data, yielding the final accuracy scores. In order to contrast results, we also produced hierarchies of senses based on random clustering, where the clusters yield the same sense granularity as those of the confusion-based clustering explained above. We produced 10 random clustering for each word, and averaged over the runs to obtain the final accuracy. 5 Experiments The gold standard is based on the multiple annotations in the corpus, but a single sense was selected as the correct one, following (Passonneau and Carpenter, 2014), which use a probabilistic annotation model (Dawid and Skene, 1979). We split the 1000 examples for each word into development and test, sampling 85% (and 15% respectively) at random, preserving the overall sense distribution. The Word Sense Disambiguation algorithm of choice is It Make Sense (IMS) (Zhong and Ng, 2010), which reports the best WSD results to date. IMS is a freely available Java implementation1, 1http://www.comp.nus.edu.sg/˜nlp/ Ji n=m+1 1 1 I (Ji ) 2 Ji−1� m=1 I i=1 63 which provides an extensible and flexible platform for researchers interested in using a WSD component. Following Lee and Ng (2002), IMS adopts support vector machines as the cl</context>
</contexts>
<marker>Dawid, Skene, 1979</marker>
<rawString>A. P. Dawid and A. M. Skene. 1979. Maximum likelihood estimation of observer error-rates using the em algorithm. Applied Statistics, 28(1):20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="1915" citStr="Fellbaum, 1998" startWordPosition="294" endWordPosition="295">easured using a WSD system on certain dataset. The second examines whether the sense distinctions are appropriate, which varies from application to application. Although usefulness can be explored in a downstream application (Agirre et al., 2008), it is usually assessed subjectively, discussing the quality of the sense distinctions (Palmer et al., 2007). Both issues 61 (performance and usefulness) are linked to the granularity of the sense inventory, and conflict with each other: finer granularity might produce more useful distinctions but the accuracy would be worse, and vice-versa. WordNet (Fellbaum, 1998) is the most widely used resource to build word sense disambiguation tools and word sense annotated corpora, including recent large efforts (Passonneau et al., 2012), but its fine-grainedness has been mentioned to be a problem (Hovy et al., 2006; Palmer et al., 2007). We think that a desiderata for a sense inventory would be that it provides useful sense distinctions and useful performance across a large range of applications. We would also add that it should be tightly integrated with WordNet, given its prevalence on NLP applications, and we thus focus on sense inventories which are mapped to</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>C. Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Hovy</author>
<author>Mitchell Marcus</author>
<author>Martha Palmer</author>
<author>Lance Ramshaw</author>
<author>Ralph Weischedel</author>
</authors>
<title>Ontonotes: The 90% solution.</title>
<date>2006</date>
<booktitle>In Proceedings of HLT-NAACL</booktitle>
<pages>57--60</pages>
<contexts>
<context position="2160" citStr="Hovy et al., 2006" startWordPosition="332" endWordPosition="335">08), it is usually assessed subjectively, discussing the quality of the sense distinctions (Palmer et al., 2007). Both issues 61 (performance and usefulness) are linked to the granularity of the sense inventory, and conflict with each other: finer granularity might produce more useful distinctions but the accuracy would be worse, and vice-versa. WordNet (Fellbaum, 1998) is the most widely used resource to build word sense disambiguation tools and word sense annotated corpora, including recent large efforts (Passonneau et al., 2012), but its fine-grainedness has been mentioned to be a problem (Hovy et al., 2006; Palmer et al., 2007). We think that a desiderata for a sense inventory would be that it provides useful sense distinctions and useful performance across a large range of applications. We would also add that it should be tightly integrated with WordNet, given its prevalence on NLP applications, and we thus focus on sense inventories which are mapped to WordNet. In order to asses usefulness, we need specific measures. Downstream application is difficult, and unfeasible for new proposals, as a full-fledged sense inventories and associated annotations are necessary. We can instead estimate usefu</context>
<context position="5831" citStr="Hovy et al., 2006" startWordPosition="917" endWordPosition="920">take agreement into account. Previous efforts to cluster WordNet senses in order to produce coarse-grained inventories have shown that improved results can be obtained, but we think our approach fits the desiderata better. For instance, clustering together senses which have the same Semantic File (also called Supersenses) allowed the best supervised WSD system to date (Zhong and Ng, 2010) to increase accuracy from 58.3% to 82.6% in the Semeval 2007 all-words dataset (Navigli et al., 2007). Semantic Files are useful, but don’t allow to provide flexible sense inventories. The OntoNotes project (Hovy et al., 2006) devised a manual grouping method which explicitly sought 90% accuracy. Although the method was shown to be successful, the fixed sense groupings had to be produced manually, included complex mappings to WordNet (cf. Section 6), and was a limited exercise, with annotations for around 4900 words. Our work is similar in spirit to OntoNotes, but use a different methodology which allows for flexible granularity, as the annotation is done at the fine-grained level, and the clustering is done later fully automatically. Automatic clustering algorithms are not new. (Tou Ng et al., 1999) propose to use</context>
<context position="17559" citStr="Hovy et al., 2006" startWordPosition="2898" endWordPosition="2901">reduction compares favorably to that of our clustering when stopping at IMS&gt;90%, showing that we could have kept clustering senses further without losing predictive power. These figures show that we could stop at arbitrary performance figures at the cost of obtaining highly skewed clusters (indicated by the high MFS value). We will revisit high MFS in Section 6. 6 Comparison to OntoNotes senses In order to perform a qualitative study and check whether our sense clusters make sense, we decided to compare them to another coarse sense inventory which is mapped to WordNet. We chose Ontonotes 5.0 (Hovy et al., 2006), which also had the goal of attaining 90% sense accuracy. Alternatively, we could have used the Oxford Dictionary of English, which was mapped automatically to WordNet 2.1 (Navigli et al., 2007) but we preferred to factor out automatic mappings and version differences from the analysis. Ontonotes contains lexical entries for 35 of our target words. The relation between the sense inventory of WordNet and OntoNotes is complex. Given that our work clusters WordNet senses, we focused on the 18 words where the OntoNotes senses where composed of one or several WordNet senses and where all WordNet s</context>
</contexts>
<marker>Hovy, Marcus, Palmer, Ramshaw, Weischedel, 2006</marker>
<rawString>Eduard Hovy, Mitchell Marcus, Martha Palmer, Lance Ramshaw, and Ralph Weischedel. 2006. Ontonotes: The 90% solution. In Proceedings of HLT-NAACL 2006, pages 57–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Johannsen</author>
<author>Dirk Hovy</author>
<author>H´ector Martinez Alonso</author>
<author>Barbara Plank</author>
<author>Anders Søgaard</author>
</authors>
<title>More or less supervised supersense tagging of twitter.</title>
<date>2014</date>
<booktitle>In Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014),</booktitle>
<pages>1--11</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics and Dublin City University.</institution>
<location>Dublin, Ireland,</location>
<contexts>
<context position="27991" citStr="Johannsen et al., 2014" startWordPosition="4955" endWordPosition="4958">he meanings WordNet, where all senses of the word that belong to required by the application. the same lexicographer file (e.g. the artifact file) are 8 Conclusions and Future Work joined together. They include 15 sense for verbs and This work explores the use of crowdsourced an26 for nouns. Although WordNet also provide su- notations to cluster senses that are often confused persenses for adjective and adverbs, these are not se- by non-expert annotators. Our method can provide mantically motivated and do not provide any higher clusters at different levels of granularity. We show abstraction (Johannsen et al., 2014). that, for instance, we can construct clusters yielding Table 2 show the results for the target 45 words around 90% accuracy for 45 words, with higher er(adjectives included). The average polysemy of ror reduction with respect to MFS than fine-grained the supersenses is lower for all parts of speech senses. By construction, we merge senses which with respect to our clustered senses and OntoNotes. are often confused by annotators, yielding sensible Note that, word-wise, polysemy varies significantly: sense clusters, as corroborated by manual inspecmany words keep one or two senses, while other</context>
</contexts>
<marker>Johannsen, Hovy, Alonso, Plank, Søgaard, 2014</marker>
<rawString>Anders Johannsen, Dirk Hovy, H´ector Martinez Alonso, Barbara Plank, and Anders Søgaard. 2014. More or less supervised supersense tagging of twitter. In Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 1– 11, Dublin, Ireland, August. Association for Computational Linguistics and Dublin City University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yoong Keok Lee</author>
<author>Hwee Tou Ng</author>
</authors>
<title>An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>41--48</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics,</institution>
<contexts>
<context position="11809" citStr="Lee and Ng (2002)" startWordPosition="1917" endWordPosition="1920">ch use a probabilistic annotation model (Dawid and Skene, 1979). We split the 1000 examples for each word into development and test, sampling 85% (and 15% respectively) at random, preserving the overall sense distribution. The Word Sense Disambiguation algorithm of choice is It Make Sense (IMS) (Zhong and Ng, 2010), which reports the best WSD results to date. IMS is a freely available Java implementation1, 1http://www.comp.nus.edu.sg/˜nlp/ Ji n=m+1 1 1 I (Ji ) 2 Ji−1� m=1 I i=1 63 which provides an extensible and flexible platform for researchers interested in using a WSD component. Following Lee and Ng (2002), IMS adopts support vector machines as the classifier and integrates the state of the features extractors including parts-of-speech of the surrounding words, bag of words features, and local collocations as features. IMS provides ready-to-use models trained with examples collected from parallel texts , SEMCOR (Miller et al., 1993), and the DSO corpus (Ng and Lee, 1996). In our experiments we train IMS with the train examples of the crowdsourced MASC. We used IMS out-of-the-box, using the default parametrization and built-in feature extraction. We compare results obtained with IMS against the </context>
</contexts>
<marker>Lee, Ng, 2002</marker>
<rawString>Yoong Keok Lee and Hwee Tou Ng. 2002. An empirical evaluation of knowledge sources and learning algorithms for word sense disambiguation. In Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing, pages 41–48. Association for Computational Linguistics, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Dan I Moldovan</author>
</authors>
<title>A method for word sense disambiguation of unrestricted text.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>152--158</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>College Park, Maryland, USA,</location>
<contexts>
<context position="6849" citStr="Mihalcea and Moldovan (1999)" startWordPosition="1082" endWordPosition="1086">flexible granularity, as the annotation is done at the fine-grained level, and the clustering is done later fully automatically. Automatic clustering algorithms are not new. (Tou Ng et al., 1999) propose to use annotator agreement to cluster senses, reporting higher interannotator agreement after clustering. We are in part inspired by their approach, as we extend it from two annotators to a sample with 25 annotators, and validate the approach with WSD systems. The rest of approaches use other sources of information. Peters et al. (1998) make use of the WordNet hierarchy to group close senses. Mihalcea and Moldovan (1999) present similar approach that is based in the structure of WordNet. Tomuro (2001) presents are more principled algorithm based on Minimum Description Length. A work which is closely related to our work is (Agirre and Lopez de Lacalle, 2003), in which they examine a variety of information sources to cluster WordNet word senses, including a hierarchical clustering based on distributional information. Snow et al. (2007) present a supervised learning algorithm that learns merging senses and make use of wide range of WordNet-based and corpus-based features. (Navigli et al., 2007) mapped WordNet to</context>
</contexts>
<marker>Mihalcea, Moldovan, 1999</marker>
<rawString>Rada Mihalcea and Dan I. Moldovan. 1999. A method for word sense disambiguation of unrestricted text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 152–158, College Park, Maryland, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>C Leacock</author>
<author>R Tengi</author>
<author>R Bunker</author>
</authors>
<title>A Semantic Concordance.</title>
<date>1993</date>
<booktitle>In Proceedings of the workshop on Human Language Technology (HLT’93).</booktitle>
<contexts>
<context position="12142" citStr="Miller et al., 1993" startWordPosition="1967" endWordPosition="1970">s the best WSD results to date. IMS is a freely available Java implementation1, 1http://www.comp.nus.edu.sg/˜nlp/ Ji n=m+1 1 1 I (Ji ) 2 Ji−1� m=1 I i=1 63 which provides an extensible and flexible platform for researchers interested in using a WSD component. Following Lee and Ng (2002), IMS adopts support vector machines as the classifier and integrates the state of the features extractors including parts-of-speech of the surrounding words, bag of words features, and local collocations as features. IMS provides ready-to-use models trained with examples collected from parallel texts , SEMCOR (Miller et al., 1993), and the DSO corpus (Ng and Lee, 1996). In our experiments we train IMS with the train examples of the crowdsourced MASC. We used IMS out-of-the-box, using the default parametrization and built-in feature extraction. We compare results obtained with IMS against the Most Frequent Sense (MFS), which was estimated using the training corpus. Both systems (IMS and MFS) could be trained on fine-grained senses, on coarse-grained senses induced from the confusion matrix using the 90% threshold described above (Coarseconf) and coarse-grained senses induced from random clustering using the 90% threshol</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>G.A. Miller, C. Leacock, R. Tengi, and R.Bunker. 1993. A Semantic Concordance. In Proceedings of the workshop on Human Language Technology (HLT’93).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Navigli</author>
<author>K C Litkowski</author>
<author>O Hargraves</author>
</authors>
<title>SemEval-2007 Task 07: Coarse-Grained English AllWords Task.</title>
<date>2007</date>
<booktitle>In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007) in conjunction with ACL,</booktitle>
<pages>30--35</pages>
<location>Prague, Czech Republic.</location>
<contexts>
<context position="5706" citStr="Navigli et al., 2007" startWordPosition="897" endWordPosition="900">k. They present a comparison of conventional expert-guided annotation model with a probabilistic annotation model that does not take agreement into account. Previous efforts to cluster WordNet senses in order to produce coarse-grained inventories have shown that improved results can be obtained, but we think our approach fits the desiderata better. For instance, clustering together senses which have the same Semantic File (also called Supersenses) allowed the best supervised WSD system to date (Zhong and Ng, 2010) to increase accuracy from 58.3% to 82.6% in the Semeval 2007 all-words dataset (Navigli et al., 2007). Semantic Files are useful, but don’t allow to provide flexible sense inventories. The OntoNotes project (Hovy et al., 2006) devised a manual grouping method which explicitly sought 90% accuracy. Although the method was shown to be successful, the fixed sense groupings had to be produced manually, included complex mappings to WordNet (cf. Section 6), and was a limited exercise, with annotations for around 4900 words. Our work is similar in spirit to OntoNotes, but use a different methodology which allows for flexible granularity, as the annotation is done at the fine-grained level, and the cl</context>
<context position="7431" citStr="Navigli et al., 2007" startWordPosition="1175" endWordPosition="1178">se senses. Mihalcea and Moldovan (1999) present similar approach that is based in the structure of WordNet. Tomuro (2001) presents are more principled algorithm based on Minimum Description Length. A work which is closely related to our work is (Agirre and Lopez de Lacalle, 2003), in which they examine a variety of information sources to cluster WordNet word senses, including a hierarchical clustering based on distributional information. Snow et al. (2007) present a supervised learning algorithm that learns merging senses and make use of wide range of WordNet-based and corpus-based features. (Navigli et al., 2007) mapped WordNet to the top level sense distinctions in the Oxford Dictionary of 62 English. All the above rely on automatic measures, while our method is based on human annotations. 3 MASC Crowdsourced annotations The corpus used in the experiments is part of the Manually Annotated Sub-Corpus of the Open American National Corpus, which contains a subsidiary word sense sentence corpus consisting of approximately one thousand sentences per word annotated with WordNet 3.0 sense labels (Passonneau et al., 2012). In this work we make use of a publicly available subset of 45 words (17 nouns, 19 verb</context>
<context position="17754" citStr="Navigli et al., 2007" startWordPosition="2930" endWordPosition="2933">hat we could stop at arbitrary performance figures at the cost of obtaining highly skewed clusters (indicated by the high MFS value). We will revisit high MFS in Section 6. 6 Comparison to OntoNotes senses In order to perform a qualitative study and check whether our sense clusters make sense, we decided to compare them to another coarse sense inventory which is mapped to WordNet. We chose Ontonotes 5.0 (Hovy et al., 2006), which also had the goal of attaining 90% sense accuracy. Alternatively, we could have used the Oxford Dictionary of English, which was mapped automatically to WordNet 2.1 (Navigli et al., 2007) but we preferred to factor out automatic mappings and version differences from the analysis. Ontonotes contains lexical entries for 35 of our target words. The relation between the sense inventory of WordNet and OntoNotes is complex. Given that our work clusters WordNet senses, we focused on the 18 words where the OntoNotes senses where composed of one or several WordNet senses and where all WordNet senses were covered3. Table 2 3The rest of words include senses not mapped to Word90 80 70 60 50 Accuracy 40 30 50 40 30 20 10 Error Reduction MFS IMS MFS IMS MFS IMS MFS IMS MFS IMS MFS IMS 65 Wo</context>
</contexts>
<marker>Navigli, Litkowski, Hargraves, 2007</marker>
<rawString>R. Navigli, K. C. Litkowski, and O. Hargraves. 2007. SemEval-2007 Task 07: Coarse-Grained English AllWords Task. In Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007) in conjunction with ACL, pages 30–35, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H T Ng</author>
<author>H B Lee</author>
</authors>
<title>Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach.</title>
<date>1996</date>
<booktitle>In Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL ’96,</booktitle>
<pages>40--47</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="12181" citStr="Ng and Lee, 1996" startWordPosition="1975" endWordPosition="1978">reely available Java implementation1, 1http://www.comp.nus.edu.sg/˜nlp/ Ji n=m+1 1 1 I (Ji ) 2 Ji−1� m=1 I i=1 63 which provides an extensible and flexible platform for researchers interested in using a WSD component. Following Lee and Ng (2002), IMS adopts support vector machines as the classifier and integrates the state of the features extractors including parts-of-speech of the surrounding words, bag of words features, and local collocations as features. IMS provides ready-to-use models trained with examples collected from parallel texts , SEMCOR (Miller et al., 1993), and the DSO corpus (Ng and Lee, 1996). In our experiments we train IMS with the train examples of the crowdsourced MASC. We used IMS out-of-the-box, using the default parametrization and built-in feature extraction. We compare results obtained with IMS against the Most Frequent Sense (MFS), which was estimated using the training corpus. Both systems (IMS and MFS) could be trained on fine-grained senses, on coarse-grained senses induced from the confusion matrix using the 90% threshold described above (Coarseconf) and coarse-grained senses induced from random clustering using the 90% threshold (Coarserandom). We also used sense cl</context>
</contexts>
<marker>Ng, Lee, 1996</marker>
<rawString>H. T. Ng and H. B. Lee. 1996. Integrating multiple knowledge sources to disambiguate word sense: an exemplar-based approach. In Proceedings of the 34th annual meeting on Association for Computational Linguistics, ACL ’96, pages 40–47, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Hoa Trang Dang</author>
<author>Christiane Fellbaum</author>
</authors>
<title>Making fine-grained and coarse-grained sense distinctions, both manually and automatically.</title>
<date>2007</date>
<journal>Natural Language Engineering,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="1655" citStr="Palmer et al., 2007" startWordPosition="253" endWordPosition="256">iguity is a major hurdle for accurate information extraction, summarization and machine translation. The utility of Word Sense Disambiguation (WSD) depends on the accuracy and on how useful the sense distinctions are. The first issue is quantitative, as it can be measured using a WSD system on certain dataset. The second examines whether the sense distinctions are appropriate, which varies from application to application. Although usefulness can be explored in a downstream application (Agirre et al., 2008), it is usually assessed subjectively, discussing the quality of the sense distinctions (Palmer et al., 2007). Both issues 61 (performance and usefulness) are linked to the granularity of the sense inventory, and conflict with each other: finer granularity might produce more useful distinctions but the accuracy would be worse, and vice-versa. WordNet (Fellbaum, 1998) is the most widely used resource to build word sense disambiguation tools and word sense annotated corpora, including recent large efforts (Passonneau et al., 2012), but its fine-grainedness has been mentioned to be a problem (Hovy et al., 2006; Palmer et al., 2007). We think that a desiderata for a sense inventory would be that it provi</context>
</contexts>
<marker>Palmer, Dang, Fellbaum, 2007</marker>
<rawString>Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum. 2007. Making fine-grained and coarse-grained sense distinctions, both manually and automatically. Natural Language Engineering, 13(2):137–163.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Bob Carpenter</author>
</authors>
<title>The benefits of a model of annotation.</title>
<date>2014</date>
<journal>Transactions of the Association for Computational Linguistics,</journal>
<pages>2--1</pages>
<contexts>
<context position="4185" citStr="Passonneau and Carpenter, 2014" startWordPosition="655" endWordPosition="658">. • Flexible sense granularity, that is, the ability to produce different degrees of polysemy and accuracy, from fine-grained to coarse-grained. When comparing sense inventories with different granularities, absolute MFS and supervised performance are not enough. We propose to use error reduction of the supervised system with respect to the MFS as a measure of the balance between low MFS and high supervised performance. The larger the error reduction the better. • Manual inspection of the sense distinctions, as a complement to quantitative measures. We propose to use crowdsourced annotations (Passonneau and Carpenter, 2014) to cluster WordNet senses that are often confused by non-expert annotators. Our method can provide clusters at different levels of granularity. We show that we can construct clusters yielding around 90% accuracy for 45 words, with higher error reduction with respect to MFS than fine-grained senses. By construction, we merge senses which are often confused by annotators, yielding sensible sense clusters, as corroborated by manual inspection. The paper is structured as follows. Section 2 mentions related work. We then present the annotations, followed by the clustering procedure. Section 5 repo</context>
<context position="8172" citStr="Passonneau and Carpenter, 2014" startWordPosition="1296" endWordPosition="1299"> on automatic measures, while our method is based on human annotations. 3 MASC Crowdsourced annotations The corpus used in the experiments is part of the Manually Annotated Sub-Corpus of the Open American National Corpus, which contains a subsidiary word sense sentence corpus consisting of approximately one thousand sentences per word annotated with WordNet 3.0 sense labels (Passonneau et al., 2012). In this work we make use of a publicly available subset of 45 words (17 nouns, 19 verbs and 9 adjective, see Table2) that have been annotated, 1000 sentences per target word, using crowdsourcing (Passonneau and Carpenter, 2014). The authors collected between 20 and 25 labels for every sentence. They showed that a probabilistic annotation model based on crowdsourced data was effective, with favorable quality when compared to a conventional expert-guided annotation model. 4 Clustering Procedure Having access to multiple annotations of the same item allows to identify correlations among senses of a word. In particular, we can mine how many times the annotators confused 2 particular senses of a word. If two senses are confused very often, it will signal that the annotators find the differences between the two senses dif</context>
<context position="11187" citStr="Passonneau and Carpenter, 2014" startWordPosition="1816" endWordPosition="1820"> obtained, we train the word sense disambiguation on the development data and test over held-out data, yielding the final accuracy scores. In order to contrast results, we also produced hierarchies of senses based on random clustering, where the clusters yield the same sense granularity as those of the confusion-based clustering explained above. We produced 10 random clustering for each word, and averaged over the runs to obtain the final accuracy. 5 Experiments The gold standard is based on the multiple annotations in the corpus, but a single sense was selected as the correct one, following (Passonneau and Carpenter, 2014), which use a probabilistic annotation model (Dawid and Skene, 1979). We split the 1000 examples for each word into development and test, sampling 85% (and 15% respectively) at random, preserving the overall sense distribution. The Word Sense Disambiguation algorithm of choice is It Make Sense (IMS) (Zhong and Ng, 2010), which reports the best WSD results to date. IMS is a freely available Java implementation1, 1http://www.comp.nus.edu.sg/˜nlp/ Ji n=m+1 1 1 I (Ji ) 2 Ji−1� m=1 I i=1 63 which provides an extensible and flexible platform for researchers interested in using a WSD component. Follo</context>
</contexts>
<marker>Passonneau, Carpenter, 2014</marker>
<rawString>Rebecca J. Passonneau and Bob Carpenter. 2014. The benefits of a model of annotation. Transactions of the Association for Computational Linguistics, 2(1-9):311–326.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Rebecca J Passonneau</author>
<author>Collin F Baker</author>
<author>Christiane Fellbaum</author>
<author>Nancy Ide</author>
</authors>
<title>The MASC word sense corpus.</title>
<date>2012</date>
<booktitle>Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12),</booktitle>
<editor>In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors,</editor>
<location>Istanbul, Turkey,</location>
<contexts>
<context position="2080" citStr="Passonneau et al., 2012" startWordPosition="317" endWordPosition="320">ion. Although usefulness can be explored in a downstream application (Agirre et al., 2008), it is usually assessed subjectively, discussing the quality of the sense distinctions (Palmer et al., 2007). Both issues 61 (performance and usefulness) are linked to the granularity of the sense inventory, and conflict with each other: finer granularity might produce more useful distinctions but the accuracy would be worse, and vice-versa. WordNet (Fellbaum, 1998) is the most widely used resource to build word sense disambiguation tools and word sense annotated corpora, including recent large efforts (Passonneau et al., 2012), but its fine-grainedness has been mentioned to be a problem (Hovy et al., 2006; Palmer et al., 2007). We think that a desiderata for a sense inventory would be that it provides useful sense distinctions and useful performance across a large range of applications. We would also add that it should be tightly integrated with WordNet, given its prevalence on NLP applications, and we thus focus on sense inventories which are mapped to WordNet. In order to asses usefulness, we need specific measures. Downstream application is difficult, and unfeasible for new proposals, as a full-fledged sense inv</context>
<context position="7943" citStr="Passonneau et al., 2012" startWordPosition="1256" endWordPosition="1259">rns merging senses and make use of wide range of WordNet-based and corpus-based features. (Navigli et al., 2007) mapped WordNet to the top level sense distinctions in the Oxford Dictionary of 62 English. All the above rely on automatic measures, while our method is based on human annotations. 3 MASC Crowdsourced annotations The corpus used in the experiments is part of the Manually Annotated Sub-Corpus of the Open American National Corpus, which contains a subsidiary word sense sentence corpus consisting of approximately one thousand sentences per word annotated with WordNet 3.0 sense labels (Passonneau et al., 2012). In this work we make use of a publicly available subset of 45 words (17 nouns, 19 verbs and 9 adjective, see Table2) that have been annotated, 1000 sentences per target word, using crowdsourcing (Passonneau and Carpenter, 2014). The authors collected between 20 and 25 labels for every sentence. They showed that a probabilistic annotation model based on crowdsourced data was effective, with favorable quality when compared to a conventional expert-guided annotation model. 4 Clustering Procedure Having access to multiple annotations of the same item allows to identify correlations among senses </context>
</contexts>
<marker>Passonneau, Baker, Fellbaum, Ide, 2012</marker>
<rawString>Rebecca J. Passonneau, Collin F. Baker, Christiane Fellbaum, and Nancy Ide. 2012. The MASC word sense corpus. In Nicoletta Calzolari (Conference Chair), Khalid Choukri, Thierry Declerck, Mehmet Uur Doan, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).</rawString>
</citation>
<citation valid="true">
<authors>
<author>W Peters</author>
<author>I Peters</author>
</authors>
<title>Automatic sense clustering in eurowordnet.</title>
<date>1998</date>
<booktitle>In Proceedings of the First International Conference on Language Resources and Evaluation,</booktitle>
<pages>409--416</pages>
<location>Granada,</location>
<marker>Peters, Peters, 1998</marker>
<rawString>W. Peters, I. Peters, , and P. Vossen. 1998. Automatic sense clustering in eurowordnet. In Proceedings of the First International Conference on Language Resources and Evaluation, pages 409–416, Granada, Spain.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Sushant Prakash</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Learning to merge word senses.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>1005--1014</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="7270" citStr="Snow et al. (2007)" startWordPosition="1151" endWordPosition="1154">te the approach with WSD systems. The rest of approaches use other sources of information. Peters et al. (1998) make use of the WordNet hierarchy to group close senses. Mihalcea and Moldovan (1999) present similar approach that is based in the structure of WordNet. Tomuro (2001) presents are more principled algorithm based on Minimum Description Length. A work which is closely related to our work is (Agirre and Lopez de Lacalle, 2003), in which they examine a variety of information sources to cluster WordNet word senses, including a hierarchical clustering based on distributional information. Snow et al. (2007) present a supervised learning algorithm that learns merging senses and make use of wide range of WordNet-based and corpus-based features. (Navigli et al., 2007) mapped WordNet to the top level sense distinctions in the Oxford Dictionary of 62 English. All the above rely on automatic measures, while our method is based on human annotations. 3 MASC Crowdsourced annotations The corpus used in the experiments is part of the Manually Annotated Sub-Corpus of the Open American National Corpus, which contains a subsidiary word sense sentence corpus consisting of approximately one thousand sentences p</context>
</contexts>
<marker>Snow, Prakash, Jurafsky, Ng, 2007</marker>
<rawString>Rion Snow, Sushant Prakash, Daniel Jurafsky, and Andrew Y. Ng. 2007. Learning to merge word senses. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 1005–1014, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Noriko Tomuro</author>
</authors>
<title>Tree-cut and a lexicon based on systematic polysemy.</title>
<date>2001</date>
<booktitle>In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics,</booktitle>
<location>Pittsburgh, USA.</location>
<contexts>
<context position="6931" citStr="Tomuro (2001)" startWordPosition="1098" endWordPosition="1100">done later fully automatically. Automatic clustering algorithms are not new. (Tou Ng et al., 1999) propose to use annotator agreement to cluster senses, reporting higher interannotator agreement after clustering. We are in part inspired by their approach, as we extend it from two annotators to a sample with 25 annotators, and validate the approach with WSD systems. The rest of approaches use other sources of information. Peters et al. (1998) make use of the WordNet hierarchy to group close senses. Mihalcea and Moldovan (1999) present similar approach that is based in the structure of WordNet. Tomuro (2001) presents are more principled algorithm based on Minimum Description Length. A work which is closely related to our work is (Agirre and Lopez de Lacalle, 2003), in which they examine a variety of information sources to cluster WordNet word senses, including a hierarchical clustering based on distributional information. Snow et al. (2007) present a supervised learning algorithm that learns merging senses and make use of wide range of WordNet-based and corpus-based features. (Navigli et al., 2007) mapped WordNet to the top level sense distinctions in the Oxford Dictionary of 62 English. All the </context>
</contexts>
<marker>Tomuro, 2001</marker>
<rawString>Noriko Tomuro. 2001. Tree-cut and a lexicon based on systematic polysemy. In Proceedings of the Second Meeting of the North American Chapter of the Association for Computational Linguistics, Pittsburgh, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hwee Tou Ng</author>
<author>Chung Yong Lim</author>
<author>Shou King Foo</author>
</authors>
<title>Siglex99: Standardizing lexical resources.</title>
<date>1999</date>
<contexts>
<context position="6416" citStr="Ng et al., 1999" startWordPosition="1010" endWordPosition="1013">tes project (Hovy et al., 2006) devised a manual grouping method which explicitly sought 90% accuracy. Although the method was shown to be successful, the fixed sense groupings had to be produced manually, included complex mappings to WordNet (cf. Section 6), and was a limited exercise, with annotations for around 4900 words. Our work is similar in spirit to OntoNotes, but use a different methodology which allows for flexible granularity, as the annotation is done at the fine-grained level, and the clustering is done later fully automatically. Automatic clustering algorithms are not new. (Tou Ng et al., 1999) propose to use annotator agreement to cluster senses, reporting higher interannotator agreement after clustering. We are in part inspired by their approach, as we extend it from two annotators to a sample with 25 annotators, and validate the approach with WSD systems. The rest of approaches use other sources of information. Peters et al. (1998) make use of the WordNet hierarchy to group close senses. Mihalcea and Moldovan (1999) present similar approach that is based in the structure of WordNet. Tomuro (2001) presents are more principled algorithm based on Minimum Description Length. A work w</context>
</contexts>
<marker>Ng, Lim, Foo, 1999</marker>
<rawString>Hwee Tou Ng, Chung Yong Lim, and Shou King Foo. 1999. Siglex99: Standardizing lexical resources.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z Zhong</author>
<author>H T Ng</author>
</authors>
<title>It makes sense: A widecoverage word sense disambiguation system for free text.</title>
<date>2010</date>
<booktitle>In Proceedings of the ACL 2010 System Demonstrations,</booktitle>
<pages>78--83</pages>
<location>Uppsala,</location>
<contexts>
<context position="5604" citStr="Zhong and Ng, 2010" startWordPosition="880" endWordPosition="883">assonneau and Carpenter, 2014) in that we use the same dataset and annotations presented in that work. They present a comparison of conventional expert-guided annotation model with a probabilistic annotation model that does not take agreement into account. Previous efforts to cluster WordNet senses in order to produce coarse-grained inventories have shown that improved results can be obtained, but we think our approach fits the desiderata better. For instance, clustering together senses which have the same Semantic File (also called Supersenses) allowed the best supervised WSD system to date (Zhong and Ng, 2010) to increase accuracy from 58.3% to 82.6% in the Semeval 2007 all-words dataset (Navigli et al., 2007). Semantic Files are useful, but don’t allow to provide flexible sense inventories. The OntoNotes project (Hovy et al., 2006) devised a manual grouping method which explicitly sought 90% accuracy. Although the method was shown to be successful, the fixed sense groupings had to be produced manually, included complex mappings to WordNet (cf. Section 6), and was a limited exercise, with annotations for around 4900 words. Our work is similar in spirit to OntoNotes, but use a different methodology </context>
<context position="11508" citStr="Zhong and Ng, 2010" startWordPosition="1867" endWordPosition="1870">plained above. We produced 10 random clustering for each word, and averaged over the runs to obtain the final accuracy. 5 Experiments The gold standard is based on the multiple annotations in the corpus, but a single sense was selected as the correct one, following (Passonneau and Carpenter, 2014), which use a probabilistic annotation model (Dawid and Skene, 1979). We split the 1000 examples for each word into development and test, sampling 85% (and 15% respectively) at random, preserving the overall sense distribution. The Word Sense Disambiguation algorithm of choice is It Make Sense (IMS) (Zhong and Ng, 2010), which reports the best WSD results to date. IMS is a freely available Java implementation1, 1http://www.comp.nus.edu.sg/˜nlp/ Ji n=m+1 1 1 I (Ji ) 2 Ji−1� m=1 I i=1 63 which provides an extensible and flexible platform for researchers interested in using a WSD component. Following Lee and Ng (2002), IMS adopts support vector machines as the classifier and integrates the state of the features extractors including parts-of-speech of the surrounding words, bag of words features, and local collocations as features. IMS provides ready-to-use models trained with examples collected from parallel te</context>
</contexts>
<marker>Zhong, Ng, 2010</marker>
<rawString>Z. Zhong and H. T. Ng. 2010. It makes sense: A widecoverage word sense disambiguation system for free text. In Proceedings of the ACL 2010 System Demonstrations, pages 78–83, Uppsala, Sweden.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>