<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000067">
<title confidence="0.9972515">
LIPN: Introducing a new Geographical Context Similarity Measure and a
Statistical Similarity Measure Based on the Bhattacharyya Coefficient
</title>
<author confidence="0.985713">
Davide Buscaldi, Jorge J. Garcia Flores, Joseph Le Roux, Nadi Tomeh
</author>
<affiliation confidence="0.813399">
Laboratoire d’Informatique de Paris Nord, CNRS (UMR 7030)
</affiliation>
<address confidence="0.464414">
Universit´e Paris 13, Sorbonne Paris Cit´e, Villetaneuse, France
</address>
<email confidence="0.922252">
{buscaldi,jgflores,joseph.le-roux,nadi.tomeh}@lipn.univ-paris13.fr
</email>
<author confidence="0.923179">
Bel´em Priego Sanchez
</author>
<affiliation confidence="0.879237">
Laboratoire LDI (Lexique, Dictionnaires, Informatique)
</affiliation>
<address confidence="0.7101345">
Universit´e Paris 13, Sorbonne Paris Cit´e, Villetaneuse, France
LKE, FCC, BUAP, San Manuel, Puebla, Mexico
</address>
<email confidence="0.998944">
belemps@gmail.com
</email>
<sectionHeader confidence="0.997389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999404285714286">
This paper describes the system used by
the LIPN team in the task 10, Multilin-
gual Semantic Textual Similarity, at Sem-
Eval 2014, in both the English and Span-
ish sub-tasks. The system uses a sup-
port vector regression model, combining
different text similarity measures as fea-
tures. With respect to our 2013 partici-
pation, we included a new feature to take
into account the geographical context and
a new semantic distance based on the
Bhattacharyya distance calculated on co-
occurrence distributions derived from the
Spanish Google Books n-grams dataset.
</bodyText>
<sectionHeader confidence="0.999514" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.924804888888889">
After our participation at SemEval 2013 with
LIPN-CORE (Buscaldi et al., 2013) we found that
geography has an important role in discriminating
the semantic similarity of sentences (especially in
the case of newswire). If two events happened in
a different location, their semantic relatedness is
usually low, no matter if the events are the same.
Therefore, we worked on a similarity measure able
to capture the similarity between the geographic
contexts of two sentences. We tried also to rein-
force the semantic similarity features by introduc-
ing a new measure that calculates word similari-
ties on co-occurrence distributions extracted from
Google Books bigrams. This measure was intro-
duced only for the Spanish runs, due to time con-
straints. The regression model used to integrate
the features was the v-Support Vector Regression
This work is licensed under a Creative Commons At-
tribution 4.0 International Licence. Page numbers and pro-
ceedings footer are added by the organisers. Licence de-
tails:http://creativecommons.org/licenses/by/4.0/
model (v-SVR) (Sch¨olkopf et al., 1999) imple-
mentation provided by LIBSVM (Chang and Lin,
2011), with a radial basis function kernel with the
standard parameters (v = 0.5). We describe all
the measures in Section 2; the results obtained by
the system are detailed in Section 3.
</bodyText>
<sectionHeader confidence="0.995751" genericHeader="method">
2 Similarity Measures
</sectionHeader>
<bodyText confidence="0.999748777777778">
In this section we describe the measures used as
features in our system. The description of mea-
sures already used in our 2013 participation is less
detailed than the description of the new ones. Ad-
ditional details on the measures may be found in
(Buscaldi et al., 2013). When POS tagging and
NE recognition were required, we used the Stan-
ford CoreNLP1 for English and FreeLing2 3.1 for
Spanish.
</bodyText>
<subsectionHeader confidence="0.9686">
2.1 WordNet-based Conceptual Similarity
</subsectionHeader>
<bodyText confidence="0.999888">
This measure has been introduced in order to mea-
sure similarities between concepts with respect to
an ontology. The similarity is calculated as fol-
lows: first of all, words in sentences p and q are
lemmatised and mapped to the related WordNet
synsets. All noun synsets are put into the set of
synsets associated to the sentence, Cp and CQ, re-
spectively. If the synsets are in one of the other
POS categories (verb, adjective, adverb) we look
for their derivationally related forms in order to
find a related noun synset: if there exists one, we
put this synset in Cp (or CQ). No disambigua-
tion process is carried out, so we take all possible
meanings into account.
Given Cp and CQ as the sets of concepts con-
tained in sentences p and q, respectively, with
</bodyText>
<footnote confidence="0.999976">
1http://www-nlp.stanford.edu/software/corenlp.shtml
2http://nlp.lsi.upc.edu/freeling/
</footnote>
<page confidence="0.855265">
400
</page>
<note confidence="0.747557">
Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 400–405,
Dublin, Ireland, August 23-24, 2014.
</note>
<bodyText confidence="0.918087363636364">
|Cp |&gt; |Cq|, the conceptual similarity between p
and q is calculated as:
|Cp|
where s(c1, c2) is a conceptual similarity mea-
sure. Concept similarity can be calculated in dif-
ferent ways. We used a variation of the Wu-Palmer
formula (Wu and Palmer, 1994) named “Proxi-
Genea3”, introduced by (Dudognon et al., 2010),
which is inspired by the analogy between a family
tree and the concept hierarchy in WordNet. The
ProxiGenea3 measure is defined as:
</bodyText>
<equation confidence="0.999469333333333">
1
s(c1, c2) =
1 + d(c1) + d(c2) − 2 · d(c0)
</equation>
<bodyText confidence="0.9966838">
where c0 is the most specific concept that is
present both in the synset path of c1 and c2 (that is,
the Least Common Subsumer or LCS). The func-
tion returning the depth of a concept is noted with
d.
</bodyText>
<subsectionHeader confidence="0.991671">
2.2 IC-based Similarity
</subsectionHeader>
<bodyText confidence="0.999939733333333">
This measure has been proposed by (Mihalcea et
al., 2006) as a corpus-based measure which uses
Resnik’s Information Content (IC) and the Jiang-
Conrath (Jiang and Conrath, 1997) similarity met-
ric. This measure is more precise than the one
introduced in the previous subsection because it
takes into account also the importance of concepts
and not only their relative position in the hierarchy.
We refer to (Buscaldi et al., 2013) and (Mihalcea
et al., 2006) for a detailed description of the mea-
sure. The idf weights for the words were calcu-
lated using the Google Web 1T (Brants and Franz,
2006) frequency counts, while the IC values used
are those calculated by Ted Pedersen (Pedersen et
al., 2004) on the British National Corpus3.
</bodyText>
<subsectionHeader confidence="0.998937">
2.3 Syntactic Dependencies
</subsectionHeader>
<bodyText confidence="0.99967975">
This measure tries to capture the syntactic simi-
larity between two sentences using dependencies.
Previous experiments showed that converting con-
stituents to dependencies still achieved best results
on out-of-domain texts (Le Roux et al., 2012), so
we decided to use a 2-step architecture to obtain
syntactic dependencies. First we parsed pairs of
sentences with the LORG parser4. Second we con-
</bodyText>
<footnote confidence="0.9998535">
3http://www.d.umn.edu/ tpederse/similarity.html
4https://github.com/CNGLdlab/LORG-Release
</footnote>
<bodyText confidence="0.999887588235294">
verted the resulting parse trees to Stanford depen-
dencies5.
Given the sets of parsed dependencies Dp and
Dq, for sentence p and q, a dependency d E Dx
is a triple (l, h, t) where l is the dependency label
(for instance, dobj or prep), h the governor and
t the dependant. The similarity measure between
two syntactic dependencies d1 = (l1, h1, t1) and
d2 = (l2, h2, t2) is the levenshtein distance be-
tween the labels l1 and l2 multiplied by the aver-
age of idfh * sWN(h1, h2) and idft * sWN(t1, t2),
where idfh and idft are the inverse document fre-
quencies calculated on Google Web 1T for the
governors and the dependants (we retain the max-
imum for each pair), respectively, and sWN is cal-
culated using formula ??. NOTE: This measure
was used only in the English sub-task.
</bodyText>
<subsectionHeader confidence="0.986561">
2.4 Information Retrieval-based Similarity
</subsectionHeader>
<bodyText confidence="0.983904045454545">
Let us consider two texts p and q, an IR system S
and a document collection D indexed by S. This
measure is based on the assumption that p and q
are similar if the documents retrieved by S for the
two texts, used as input queries, are ranked simi-
larly.
Let be Lp = {dp1, ... , dpK} and Lq =
{dq1, ... , dqK}, dxi E D the sets of the top K
documents retrieved by S for texts p and q, respec-
tively. Let us define sp(d) and sq(d) the scores as-
signed by S to a document d for the query p and
q, respectively. Then, the similarity score is calcu-
lated as:
simIR(p, q) = 1 |Lp n Lq|
if |Lp n Lq |=� 0, 0 otherwise.
For the participation in the English sub-task we
indexed a collection composed by the AQUAINT-
26 and the English NTCIR-87 document collec-
tions, using the Lucene8 4.2 search engine with
BM25 similarity. The Spanish index was cre-
ated using the Spanish QA@CLEF 2005 (agencia
EFE1994-95, El Mundo 1994-95) and multiUN
</bodyText>
<footnote confidence="0.993265142857143">
5We used the default built-in converter provided with the
Stanford Parser (2012-11-12 revision).
6http://www.nist.gov/tac/data/data desc.html#AQUAINT-
2
7http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8-
databases.php
8http://lucene.apache.org/core
</footnote>
<figure confidence="0.597465875">
max
c2ECq
s(c1, c2)
E
c1ECp
ss(p, q) =
E y (sp(d)−sq(d))2
dELpnLq max(sp(d),sq(d))
</figure>
<page confidence="0.98697">
401
</page>
<bodyText confidence="0.999144333333333">
(Eisele and Chen, 2010) collections. The K value
was set to 70 after a study detailed in (Buscaldi,
2013).
</bodyText>
<subsectionHeader confidence="0.994659">
2.5 N-gram Based Similarity
</subsectionHeader>
<bodyText confidence="0.9999566875">
This measure tries to capture the fact that similar
sentences have similar n-grams, even if they are
not placed in the same positions. The measure is
based on the Clustered Keywords Positional Dis-
tance (CKPD) model proposed in (Buscaldi et al.,
2009) for the passage retrieval task.
The similarity between a text fragment p and
another text fragment q is calculated as:
used as clues: we use Yago9 to check whether the
NE corresponds to a famous leader or not, and in
the affirmative case we include the related nation
to the geographical context of the sentence. For in-
stance, “Merkel” is mapped to “Germany”. Given
Gp and Gq the sets of places found in sentences p
and q, respectively, the geographical context simi-
larity is calculated as follows:
</bodyText>
<equation confidence="0.965221090909091">
�
samgeo(p, q) = 1−logK 1 +
E
xEGv
max(|Gp|,|Gq|)
1
min
yEGq
d(x, y)
�samngrams(p, q) =
bxEQ
</equation>
<bodyText confidence="0.9937192">
Where P is the set of the heaviest n-grams in p
where all terms are also contained in q; Q is the
set of all the possible n-grams in q, and n is the
total number of terms in the longest sentence. The
weights for each term wi are calculated as wi =
</bodyText>
<equation confidence="0.915905">
1 − log(ni) where ni is the frequency of term
1+log(N)
</equation>
<bodyText confidence="0.99695675">
ti in the Google Web 1T collection, and N is the
frequency of the most frequent term in the Google
Web 1T collection. The weight for each n-gram
(h(x, P)), with |P |= j is calculated as:
</bodyText>
<equation confidence="0.966871666666667">
� Ej k=1 wk
h(x, P ) =
0
</equation>
<bodyText confidence="0.999944666666667">
The function d(x, xmax) determines the minimum
distance between a n-gram x and the heaviest one
xmax as the number of words between them.
</bodyText>
<subsectionHeader confidence="0.909917">
2.6 Geographical Context Similarity
</subsectionHeader>
<bodyText confidence="0.9977353">
We observed that in many sentences, especially
those extracted from news corpora, the compati-
bility of the geographic context between the sen-
tences is an important clue to determine if the sen-
tences are related or not. This measure tries to
measure if the two sentences refer to events that
took place in the same geographical area. We built
a database of geographically-related entities, using
geo-WordNet (Buscaldi and Rosso, 2008) and ex-
panding it with all the synsets that are related to a
geographically grounded synset. This implies that
also adjectives and verbs may be used as clues for
the identification of the geographical context of a
sentence. For instance, “Afghan” is associated to
“Afghanistan”, “Sovietize” to “Soviet Union”, etc.
The Named Entities of type PER (Person) are also
Where d(x, y) is the spherical distance in Km. be-
tween x and y, and K is a normalization factor set
to 10000 Km. to obtain similarity values between
1 and 0.
</bodyText>
<subsectionHeader confidence="0.990859">
2.7 2-grams “Spectral” Distance
</subsectionHeader>
<bodyText confidence="0.977634925925926">
This measure is used to calculate the seman-
tic similarity of two words on the basis of their
context, according to the distributional hypothe-
sis. The measure exploits bi-grams in the Google
Books n-gram collection10 and is based on the dis-
tributional hypothesis, that is, “words that tend to
appear in similar contexts are supposed to have
similar meanings”. Given a word w, we calcu-
late the probability of observing a word x know-
ing that it is preceded by w as p(x|w) = p(w ∩
x)/p(w) = c(“wx”)/c(“w”), where c(“wx”) is
the number of bigrams “w x” observed in Google
Books (counting all publication years) 2-grams
and c(“w”) is the number of occurrences of w ob-
served in Google Books 1-grams. We calculate
also the probability of observing a word y know-
ing that it is followed by w as p(y|w) = p(w ∩
y)/p(w) = c(“yw”)/c(“w”). In such a way, we
may obtain for a word wi two probability distri-
butions Dwi
p and Dwi
f that can be compared to the
distributions obtained in the same way for another
word wj. Therefore, we calculate the distance of
two words comparing the distribution probabilities
built in this way, using the Bhattacharyya coeffi-
cient:
</bodyText>
<footnote confidence="0.9905935">
9http://www.mpi-inf.mpg.de/yago-naga/yago/
10https://books.google.com/ngrams/datasets
</footnote>
<equation confidence="0.893725">
h(x, P)
Eni=1 wid(x, xmax)
if x ∈ P
otherwise
402
s f (wi, wj) = − log ( 1:
x∈X
sp (wi, wj) = − log ( 1:
x∈X
</equation>
<bodyText confidence="0.9995883">
the resulting distance between wi and wj is cal-
culated as the average between sf(wi, wj) and
sp(wi, wj). All words in sentence p are compared
to the words of sentence q using this similarity
value. The words that are semantically closer are
paired; if a word cannot be paired (average dis-
tance with any of the words in the other sentence
&gt; 10), then it is left unpaired. The value used as
the final feature is the averaged sum of all distance
scores.
</bodyText>
<subsectionHeader confidence="0.961366">
2.8 Other Measures
</subsectionHeader>
<bodyText confidence="0.9970755">
In addition to the above text similarity measures,
we used also the following common measures:
</bodyText>
<subsectionHeader confidence="0.407056">
Cosine
</subsectionHeader>
<bodyText confidence="0.7681814">
Cosine distance calculated between
p = (wp1,..., wp..) and q = (wql, ... , wq..), the
vectors of tf.idf weights associated to sentences
p and q, with idf values calculated on Google Web
1T.
</bodyText>
<subsectionHeader confidence="0.914121">
Edit Distance
</subsectionHeader>
<bodyText confidence="0.999525333333333">
This similarity measure is calculated using the
Levenshtein distance on characters between the
two sentences.
</bodyText>
<subsectionHeader confidence="0.791171">
Named Entity Overlap
</subsectionHeader>
<bodyText confidence="0.9999854">
This is a per-class overlap measure (in this way,
“France” as an Organization does not match
“France” as a Location) calculated using the Dice
coefficient between the sets of NEs found, respec-
tively, in sentences p and q.
</bodyText>
<sectionHeader confidence="0.999795" genericHeader="evaluation">
3 Results
</sectionHeader>
<subsectionHeader confidence="0.998366">
3.1 Spanish
</subsectionHeader>
<bodyText confidence="0.999965571428571">
In order to train the Spanish model, we trans-
lated automatically all the sentences in the English
SemEval 2012 and 2013 using Google Translate.
We also built a corpus manually using definitions
from the RAE11 (Real Academia Espa˜nola de la
Lengua). The definitions were randomly extracted
and paired at different similarity levels (taking into
</bodyText>
<footnote confidence="0.517399">
11http://www.rae.es/
</footnote>
<bodyText confidence="0.99959555">
account the Dice coefficient calculated on the def-
initions bag-of-words). Three annotators gave in-
dependently their similarity judgments on these
paired definitions. A total of 200 definitions were
annotated for training. The official results for the
Spanish task are shown in Table 1. In Figure 1 we
show the results obtained by taking into account
each individual feature as a measure of similarity
between texts. These results show that the combi-
nation was always better than the single features
(as expected), and the feature best able to capture
semantic similarity alone was the cosine distance.
In Table 2 we show the results of the ablation
test, which shows that the features that most con-
tributed to improve the results were the IR-based
similarity for the news dataset and the cosine dis-
tance for the Wikipedia dataset. The worst feature
was the NER overlap (not taking into account it
would have allowed us to gain 2 places in the final
rankings).
</bodyText>
<table confidence="0.99702725">
Wikipedia News Overall
LIPN-run1 0.65194 0.82554 0.75558
LIPN-run2 0.71647 0.8316 0.7852
LIPN-run3 0.71618 0.80857 0.77134
</table>
<tableCaption confidence="0.999701">
Table 1: Spanish results (Official runs).
</tableCaption>
<bodyText confidence="0.999978">
The differences between the three submit-
ted runs are only in the training set used.
LTPN-run1 uses all the training data available
together, LTPN-run3 uses a training set com-
posed by the translated news for the news dataset
and the RAE training set for the Wikipedia dataset;
finally, the best run LTPN-runt uses the same
training sets of run3 together to build a single
model.
</bodyText>
<subsectionHeader confidence="0.999171">
3.2 English
</subsectionHeader>
<bodyText confidence="0.999938363636364">
Our participation in the English task was ham-
pered by some technical problems which did not
allow us to complete the parsing of the tweet data
in time. As a consequence of this and some er-
rors in the scripts launched to finalize the experi-
ments, the submitted results were incomplete and
we were able to detect the problem only after the
submission. We show in Table 3 the official re-
sults of run1 with the addition of the results on the
OnWN dataset calculated after the participation to
the task.
</bodyText>
<equation confidence="0.890039666666667">
�
�Df i (x) * Dwj
f (x)
�
�Dpwi (x) * Dwj
p (x)
</equation>
<page confidence="0.995014">
403
</page>
<figureCaption confidence="0.9931595">
Figure 1: Spanish task: results taking into account the individual features as semantic similarity mea-
sures.
</figureCaption>
<table confidence="0.999838181818182">
Ablated feature Wikipedia News Overall diff
LIPN-run2 (none) 0.7165 0.8316 0.7852 0.00%
1:CKPD 0.7216 0.8318 0.7874 0.22%
2:WN 0.7066 0.8277 0.7789 −0.63%
3:Edit Dist 0.708 0.8242 0.7774 −0.78%
4:Cosine 0.6849 0.8235 0.7677 −1.75%
5:NER overlap 0.7338 0.8341 0.7937 0.85%
6:Mihalcea-JC 0.7103 0.8301 0.7818 −0.34%
7:IRsim 0.7161 0.8026 0.7677 −1.74%
8:geosim 0.7185 0.8325 0.7865 0.14%
9:Spect. Dist 0.7243 0.8311 0.7880 0.28%
</table>
<tableCaption confidence="0.984524">
Table 2: Spanish task: ablation test.
</tableCaption>
<table confidence="0.999552666666667">
Dataset Correlation
Complete (official + OnWN) 0.6687
Complete (only official) 0.5083
deft-forum 0.4544
deft-news 0.6402
headlines 0.6527
images 0.8094
OnWN (unofficial) 0.8039
tweet-news 0.5507
</table>
<tableCaption confidence="0.9489395">
Table 3: English results (Official run + unofficial
OnWN).
</tableCaption>
<sectionHeader confidence="0.998819" genericHeader="conclusions">
4 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.999932538461539">
The introduced measures were studied on the
Spanish subtask, observing a limited contribu-
tion from geographic context similarity and spec-
tral distance. The IR-based measure introduced
in 2013 proved to be an important feature for
newswire-based datasets as in the 2013 English
task, even when trained on a training set derived
from automatic translation, which include many
errors. Our participation in the English subtask
was inconclusive due to the technical faults experi-
enced to produce our results. We will nevertheless
take into account the lessons learned in this partic-
ipation for future ones.
</bodyText>
<sectionHeader confidence="0.997758" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9981634">
Part of this work has been carried out with the sup-
port of LabEx-EFL (Empirical Foundation of Lin-
guistics) strand 5 (computational semantic analy-
sis). We are also grateful to CoNACyT (Consejo
NAcional de Ciencia y Tecnologia) for support to
</bodyText>
<page confidence="0.996892">
404
</page>
<bodyText confidence="0.846769">
this work.
</bodyText>
<sectionHeader confidence="0.989449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999318202531646">
Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram
corpus version 1.1.
Davide Buscaldi and Paolo Rosso. 2008. Geo-
WordNet: Automatic Georeferencing of WordNet.
In Proceedings of the International Conference on
Language Resources and Evaluation, LREC 2008,
Marrakech, Morocco.
Davide Buscaldi, Paolo Rosso, Jos´e Manuel G´omez,
and Emilio Sanchis. 2009. Answering ques-
tions with an n-gram based passage retrieval engine.
Journal of Intelligent Information Systems (JIIS),
34(2):113–134.
Davide Buscaldi, Joseph Le Roux, Jorge J. Garcia Flo-
res, and Adrian Popescu. 2013. Lipn-core: Seman-
tic text similarity using n-grams, wordnet, syntac-
tic analysis, esa and information retrieval based fea-
tures. In Second Joint Conference on Lexical and
Computational Semantics (*SEM), Volume 1: Pro-
ceedings of the Main Conference and the Shared
Task: Semantic Textual Similarity, pages 162–168,
Atlanta, Georgia, USA, June. Association for Com-
putational Linguistics.
Davide Buscaldi. 2013. Une mesure de similarit´e
s´emantique bas´ee sur la recherche d’information. In
5`eme Atelier Recherche d’Information SEmantique
- RISE 2013, pages 81–91, Lille, France, July.
Chih-Chung Chang and Chih-Jen Lin. 2011. LIB-
SVM: A library for support vector machines. ACM
Transactions on Intelligent Systems and Technol-
ogy, 2:27:1–27:27. Software available at http://
www.csie.ntu.edu.tw/˜cjlin/libsvm.
Damien Dudognon, Gilles Hubert, and Bachelin Jhonn
Victorino Ralalason. 2010. Proxig´en´ea : Une
mesure de similarit´e conceptuelle. In Proceedings of
the Colloque Veille Strat´egique Scientifique et Tech-
nologique (VSST 2010).
Andreas Eisele and Yu Chen. 2010. Multiun:
A multilingual corpus from united nation docu-
ments. In Daniel Tapias, Mike Rosner, Ste-
lios Piperidis, Jan Odjik, Joseph Mariani, Bente
Maegaard, Khalid Choukri, and Nicoletta Calzo-
lari (Conference Chair), editors, Proceedings of the
Seventh conference on International Language Re-
sources and Evaluation, pages 2868–2872. Euro-
pean Language Resources Association (ELRA), 5.
J.J. Jiang and D.W. Conrath. 1997. Semantic similar-
ity based on corpus statistics and lexical taxonomy.
In Proc. of the Int’l. Conf. on Research in Computa-
tional Linguistics, pages 19–33.
Joseph Le Roux, Jennifer Foster, Joachim Wagner,
Rasul Samad Zadeh Kaljahi, and Anton Bryl.
2012. DCU-Paris13 Systems for the SANCL 2012
Shared Task. In The NAACL 2012 First Workshop
on Syntactic Analysis of Non-Canonical Language
(SANCL), pages 1–4, Montr´eal, Canada, June.
Rada Mihalcea, Courtney Corley, and Carlo Strappa-
rava. 2006. Corpus-based and knowledge-based
measures of text semantic similarity. In Proceedings
of the 21st national conference on Artificial intelli-
gence - Volume 1, AAAI’06, pages 775–780. AAAI
Press.
Ted Pedersen, Siddharth Patwardhan, and Jason Miche-
lizzi. 2004. Wordnet::similarity: measuring the re-
latedness of concepts. In Demonstration Papers at
HLT-NAACL 2004, HLT-NAACL–Demonstrations
’04, pages 38–41, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Bernhard Sch¨olkopf, Peter Bartlett, Alex Smola, and
Robert Williamson. 1999. Shrinking the tube: a
new support vector regression algorithm. In Pro-
ceedings of the 1998 conference on Advances in neu-
ral information processing systems II, pages 330–
336, Cambridge, MA, USA. MIT Press.
Zhibiao Wu and Martha Palmer. 1994. Verbs seman-
tics and lexical selection. In Proceedings of the 32nd
annual meeting on Association for Computational
Linguistics, ACL ’94, pages 133–138, Stroudsburg,
PA, USA. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.999025">
405
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.369168">
<title confidence="0.995427">LIPN: Introducing a new Geographical Context Similarity Measure and a Statistical Similarity Measure Based on the Bhattacharyya Coefficient</title>
<author confidence="0.98875">Davide Buscaldi</author>
<author confidence="0.98875">Jorge J Garcia Flores</author>
<author confidence="0.98875">Joseph Le_Roux</author>
<author confidence="0.98875">Nadi Tomeh</author>
<affiliation confidence="0.871696">Laboratoire d’Informatique de Paris Nord, CNRS (UMR 7030)</affiliation>
<address confidence="0.855386">Universit´e Paris 13, Sorbonne Paris Cit´e, Villetaneuse, France</address>
<author confidence="0.779169">Bel´em Priego</author>
<affiliation confidence="0.984655">Laboratoire LDI (Lexique, Dictionnaires,</affiliation>
<address confidence="0.816161">Universit´e Paris 13, Sorbonne Paris Cit´e, Villetaneuse, LKE, FCC, BUAP, San Manuel, Puebla,</address>
<email confidence="0.99995">belemps@gmail.com</email>
<abstract confidence="0.980696866666667">This paper describes the system used by the LIPN team in the task 10, Multilingual Semantic Textual Similarity, at Sem- Eval 2014, in both the English and Spanish sub-tasks. The system uses a support vector regression model, combining different text similarity measures as features. With respect to our 2013 participation, we included a new feature to take into account the geographical context and a new semantic distance based on the Bhattacharyya distance calculated on cooccurrence distributions derived from the Spanish Google Books n-grams dataset.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Thorsten Brants</author>
<author>Alex Franz</author>
</authors>
<title>Web 1t 5-gram corpus version 1.1.</title>
<date>2006</date>
<contexts>
<context position="5244" citStr="Brants and Franz, 2006" startWordPosition="829" endWordPosition="832">th d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated by Ted Pedersen (Pedersen et al., 2004) on the British National Corpus3. 2.3 Syntactic Dependencies This measure tries to capture the syntactic similarity between two sentences using dependencies. Previous experiments showed that converting constituents to dependencies still achieved best results on out-of-domain texts (Le Roux et al., 2012), so we decided to use a 2-step architecture to obtain syntactic dependencies. First we parsed pairs of sentences with the LORG parser4. Second we con3http://www.d.umn.edu/ tpederse/similarity</context>
</contexts>
<marker>Brants, Franz, 2006</marker>
<rawString>Thorsten Brants and Alex Franz. 2006. Web 1t 5-gram corpus version 1.1.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Paolo Rosso</author>
</authors>
<title>GeoWordNet: Automatic Georeferencing of WordNet.</title>
<date>2008</date>
<booktitle>In Proceedings of the International Conference on Language Resources and Evaluation, LREC</booktitle>
<location>Marrakech, Morocco.</location>
<contexts>
<context position="10063" citStr="Buscaldi and Rosso, 2008" startWordPosition="1660" endWordPosition="1663">1 wk h(x, P ) = 0 The function d(x, xmax) determines the minimum distance between a n-gram x and the heaviest one xmax as the number of words between them. 2.6 Geographical Context Similarity We observed that in many sentences, especially those extracted from news corpora, the compatibility of the geographic context between the sentences is an important clue to determine if the sentences are related or not. This measure tries to measure if the two sentences refer to events that took place in the same geographical area. We built a database of geographically-related entities, using geo-WordNet (Buscaldi and Rosso, 2008) and expanding it with all the synsets that are related to a geographically grounded synset. This implies that also adjectives and verbs may be used as clues for the identification of the geographical context of a sentence. For instance, “Afghan” is associated to “Afghanistan”, “Sovietize” to “Soviet Union”, etc. The Named Entities of type PER (Person) are also Where d(x, y) is the spherical distance in Km. between x and y, and K is a normalization factor set to 10000 Km. to obtain similarity values between 1 and 0. 2.7 2-grams “Spectral” Distance This measure is used to calculate the semantic</context>
</contexts>
<marker>Buscaldi, Rosso, 2008</marker>
<rawString>Davide Buscaldi and Paolo Rosso. 2008. GeoWordNet: Automatic Georeferencing of WordNet. In Proceedings of the International Conference on Language Resources and Evaluation, LREC 2008, Marrakech, Morocco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
<author>Paolo Rosso</author>
<author>Jos´e Manuel G´omez</author>
<author>Emilio Sanchis</author>
</authors>
<title>Answering questions with an n-gram based passage retrieval engine.</title>
<date>2009</date>
<journal>Journal of Intelligent Information Systems (JIIS),</journal>
<volume>34</volume>
<issue>2</issue>
<marker>Buscaldi, Rosso, G´omez, Sanchis, 2009</marker>
<rawString>Davide Buscaldi, Paolo Rosso, Jos´e Manuel G´omez, and Emilio Sanchis. 2009. Answering questions with an n-gram based passage retrieval engine. Journal of Intelligent Information Systems (JIIS), 34(2):113–134.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Davide Buscaldi</author>
<author>Joseph Le Roux</author>
<author>Jorge J Garcia Flores</author>
<author>Adrian Popescu</author>
</authors>
<title>Lipn-core: Semantic text similarity using n-grams, wordnet, syntactic analysis, esa and information retrieval based features.</title>
<date>2013</date>
<booktitle>In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,</booktitle>
<pages>162--168</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Atlanta, Georgia, USA,</location>
<marker>Buscaldi, Le Roux, Flores, Popescu, 2013</marker>
<rawString>Davide Buscaldi, Joseph Le Roux, Jorge J. Garcia Flores, and Adrian Popescu. 2013. Lipn-core: Semantic text similarity using n-grams, wordnet, syntactic analysis, esa and information retrieval based features. In Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity, pages 162–168, Atlanta, Georgia, USA, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Davide Buscaldi</author>
</authors>
<title>Une mesure de similarit´e s´emantique bas´ee sur la recherche d’information.</title>
<date>2013</date>
<booktitle>In 5`eme Atelier Recherche d’Information SEmantique - RISE 2013,</booktitle>
<pages>81--91</pages>
<location>Lille, France,</location>
<contexts>
<context position="8068" citStr="Buscaldi, 2013" startWordPosition="1302" endWordPosition="1303">ns, using the Lucene8 4.2 search engine with BM25 similarity. The Spanish index was created using the Spanish QA@CLEF 2005 (agencia EFE1994-95, El Mundo 1994-95) and multiUN 5We used the default built-in converter provided with the Stanford Parser (2012-11-12 revision). 6http://www.nist.gov/tac/data/data desc.html#AQUAINT2 7http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8- databases.php 8http://lucene.apache.org/core max c2ECq s(c1, c2) E c1ECp ss(p, q) = E y (sp(d)−sq(d))2 dELpnLq max(sp(d),sq(d)) 401 (Eisele and Chen, 2010) collections. The K value was set to 70 after a study detailed in (Buscaldi, 2013). 2.5 N-gram Based Similarity This measure tries to capture the fact that similar sentences have similar n-grams, even if they are not placed in the same positions. The measure is based on the Clustered Keywords Positional Distance (CKPD) model proposed in (Buscaldi et al., 2009) for the passage retrieval task. The similarity between a text fragment p and another text fragment q is calculated as: used as clues: we use Yago9 to check whether the NE corresponds to a famous leader or not, and in the affirmative case we include the related nation to the geographical context of the sentence. For in</context>
</contexts>
<marker>Buscaldi, 2013</marker>
<rawString>Davide Buscaldi. 2013. Une mesure de similarit´e s´emantique bas´ee sur la recherche d’information. In 5`eme Atelier Recherche d’Information SEmantique - RISE 2013, pages 81–91, Lille, France, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chih-Chung Chang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBSVM: A library for support vector machines.</title>
<date>2011</date>
<journal>ACM Transactions on Intelligent Systems and Technology,</journal>
<pages>2--27</pages>
<note>Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</note>
<contexts>
<context position="2316" citStr="Chang and Lin, 2011" startWordPosition="332" endWordPosition="335">arity features by introducing a new measure that calculates word similarities on co-occurrence distributions extracted from Google Books bigrams. This measure was introduced only for the Spanish runs, due to time constraints. The regression model used to integrate the features was the v-Support Vector Regression This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details:http://creativecommons.org/licenses/by/4.0/ model (v-SVR) (Sch¨olkopf et al., 1999) implementation provided by LIBSVM (Chang and Lin, 2011), with a radial basis function kernel with the standard parameters (v = 0.5). We describe all the measures in Section 2; the results obtained by the system are detailed in Section 3. 2 Similarity Measures In this section we describe the measures used as features in our system. The description of measures already used in our 2013 participation is less detailed than the description of the new ones. Additional details on the measures may be found in (Buscaldi et al., 2013). When POS tagging and NE recognition were required, we used the Stanford CoreNLP1 for English and FreeLing2 3.1 for Spanish. </context>
</contexts>
<marker>Chang, Lin, 2011</marker>
<rawString>Chih-Chung Chang and Chih-Jen Lin. 2011. LIBSVM: A library for support vector machines. ACM Transactions on Intelligent Systems and Technology, 2:27:1–27:27. Software available at http:// www.csie.ntu.edu.tw/˜cjlin/libsvm.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Damien Dudognon</author>
<author>Gilles Hubert</author>
<author>Bachelin Jhonn Victorino Ralalason</author>
</authors>
<title>Proxig´en´ea : Une mesure de similarit´e conceptuelle.</title>
<date>2010</date>
<booktitle>In Proceedings of the Colloque Veille Strat´egique Scientifique et Technologique (VSST</booktitle>
<contexts>
<context position="4250" citStr="Dudognon et al., 2010" startWordPosition="650" endWordPosition="653">p and CQ as the sets of concepts contained in sentences p and q, respectively, with 1http://www-nlp.stanford.edu/software/corenlp.shtml 2http://nlp.lsi.upc.edu/freeling/ 400 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 400–405, Dublin, Ireland, August 23-24, 2014. |Cp |&gt; |Cq|, the conceptual similarity between p and q is calculated as: |Cp| where s(c1, c2) is a conceptual similarity measure. Concept similarity can be calculated in different ways. We used a variation of the Wu-Palmer formula (Wu and Palmer, 1994) named “ProxiGenea3”, introduced by (Dudognon et al., 2010), which is inspired by the analogy between a family tree and the concept hierarchy in WordNet. The ProxiGenea3 measure is defined as: 1 s(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) where c0 is the most specific concept that is present both in the synset path of c1 and c2 (that is, the Least Common Subsumer or LCS). The function returning the depth of a concept is noted with d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This</context>
</contexts>
<marker>Dudognon, Hubert, Ralalason, 2010</marker>
<rawString>Damien Dudognon, Gilles Hubert, and Bachelin Jhonn Victorino Ralalason. 2010. Proxig´en´ea : Une mesure de similarit´e conceptuelle. In Proceedings of the Colloque Veille Strat´egique Scientifique et Technologique (VSST 2010).</rawString>
</citation>
<citation valid="false">
<authors>
<author>Andreas Eisele</author>
<author>Yu Chen</author>
</authors>
<title>Multiun: A multilingual corpus from united nation documents.</title>
<date>2010</date>
<booktitle>Proceedings of the Seventh conference on International Language Resources and Evaluation,</booktitle>
<pages>2868--2872</pages>
<editor>In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari (Conference Chair), editors,</editor>
<contexts>
<context position="7986" citStr="Eisele and Chen, 2010" startWordPosition="1285" endWordPosition="1288">ndexed a collection composed by the AQUAINT26 and the English NTCIR-87 document collections, using the Lucene8 4.2 search engine with BM25 similarity. The Spanish index was created using the Spanish QA@CLEF 2005 (agencia EFE1994-95, El Mundo 1994-95) and multiUN 5We used the default built-in converter provided with the Stanford Parser (2012-11-12 revision). 6http://www.nist.gov/tac/data/data desc.html#AQUAINT2 7http://metadata.berkeley.edu/NTCIR-GeoTime/ntcir-8- databases.php 8http://lucene.apache.org/core max c2ECq s(c1, c2) E c1ECp ss(p, q) = E y (sp(d)−sq(d))2 dELpnLq max(sp(d),sq(d)) 401 (Eisele and Chen, 2010) collections. The K value was set to 70 after a study detailed in (Buscaldi, 2013). 2.5 N-gram Based Similarity This measure tries to capture the fact that similar sentences have similar n-grams, even if they are not placed in the same positions. The measure is based on the Clustered Keywords Positional Distance (CKPD) model proposed in (Buscaldi et al., 2009) for the passage retrieval task. The similarity between a text fragment p and another text fragment q is calculated as: used as clues: we use Yago9 to check whether the NE corresponds to a famous leader or not, and in the affirmative case</context>
</contexts>
<marker>Eisele, Chen, 2010</marker>
<rawString>Andreas Eisele and Yu Chen. 2010. Multiun: A multilingual corpus from united nation documents. In Daniel Tapias, Mike Rosner, Stelios Piperidis, Jan Odjik, Joseph Mariani, Bente Maegaard, Khalid Choukri, and Nicoletta Calzolari (Conference Chair), editors, Proceedings of the Seventh conference on International Language Resources and Evaluation, pages 2868–2872. European Language Resources Association (ELRA), 5.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J J Jiang</author>
<author>D W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proc. of the Int’l. Conf. on Research in Computational Linguistics,</booktitle>
<pages>pages</pages>
<contexts>
<context position="4826" citStr="Jiang and Conrath, 1997" startWordPosition="756" endWordPosition="759">xiGenea3”, introduced by (Dudognon et al., 2010), which is inspired by the analogy between a family tree and the concept hierarchy in WordNet. The ProxiGenea3 measure is defined as: 1 s(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) where c0 is the most specific concept that is present both in the synset path of c1 and c2 (that is, the Least Common Subsumer or LCS). The function returning the depth of a concept is noted with d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated by Ted Pedersen (Pedersen et al., 2004) on the British National Corpus3. 2.3 Syntactic Dependencies This measure trie</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>J.J. Jiang and D.W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proc. of the Int’l. Conf. on Research in Computational Linguistics, pages 19–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joseph Le Roux</author>
<author>Jennifer Foster</author>
<author>Joachim Wagner</author>
<author>Rasul Samad Zadeh Kaljahi</author>
<author>Anton Bryl</author>
</authors>
<date>2012</date>
<booktitle>DCU-Paris13 Systems for the SANCL 2012 Shared Task. In The NAACL 2012 First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL),</booktitle>
<pages>1--4</pages>
<location>Montr´eal, Canada,</location>
<marker>Le Roux, Foster, Wagner, Kaljahi, Bryl, 2012</marker>
<rawString>Joseph Le Roux, Jennifer Foster, Joachim Wagner, Rasul Samad Zadeh Kaljahi, and Anton Bryl. 2012. DCU-Paris13 Systems for the SANCL 2012 Shared Task. In The NAACL 2012 First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL), pages 1–4, Montr´eal, Canada, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st national conference on Artificial intelligence - Volume 1, AAAI’06,</booktitle>
<pages>775--780</pages>
<publisher>AAAI Press.</publisher>
<contexts>
<context position="4708" citStr="Mihalcea et al., 2006" startWordPosition="738" endWordPosition="741">y can be calculated in different ways. We used a variation of the Wu-Palmer formula (Wu and Palmer, 1994) named “ProxiGenea3”, introduced by (Dudognon et al., 2010), which is inspired by the analogy between a family tree and the concept hierarchy in WordNet. The ProxiGenea3 measure is defined as: 1 s(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) where c0 is the most specific concept that is present both in the synset path of c1 and c2 (that is, the Least Common Subsumer or LCS). The function returning the depth of a concept is noted with d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the 21st national conference on Artificial intelligence - Volume 1, AAAI’06, pages 775–780. AAAI Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ted Pedersen</author>
<author>Siddharth Patwardhan</author>
<author>Jason Michelizzi</author>
</authors>
<title>Wordnet::similarity: measuring the relatedness of concepts.</title>
<date>2004</date>
<booktitle>In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL–Demonstrations ’04,</booktitle>
<pages>38--41</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5348" citStr="Pedersen et al., 2004" startWordPosition="846" endWordPosition="849"> measure which uses Resnik’s Information Content (IC) and the JiangConrath (Jiang and Conrath, 1997) similarity metric. This measure is more precise than the one introduced in the previous subsection because it takes into account also the importance of concepts and not only their relative position in the hierarchy. We refer to (Buscaldi et al., 2013) and (Mihalcea et al., 2006) for a detailed description of the measure. The idf weights for the words were calculated using the Google Web 1T (Brants and Franz, 2006) frequency counts, while the IC values used are those calculated by Ted Pedersen (Pedersen et al., 2004) on the British National Corpus3. 2.3 Syntactic Dependencies This measure tries to capture the syntactic similarity between two sentences using dependencies. Previous experiments showed that converting constituents to dependencies still achieved best results on out-of-domain texts (Le Roux et al., 2012), so we decided to use a 2-step architecture to obtain syntactic dependencies. First we parsed pairs of sentences with the LORG parser4. Second we con3http://www.d.umn.edu/ tpederse/similarity.html 4https://github.com/CNGLdlab/LORG-Release verted the resulting parse trees to Stanford dependencie</context>
</contexts>
<marker>Pedersen, Patwardhan, Michelizzi, 2004</marker>
<rawString>Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi. 2004. Wordnet::similarity: measuring the relatedness of concepts. In Demonstration Papers at HLT-NAACL 2004, HLT-NAACL–Demonstrations ’04, pages 38–41, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernhard Sch¨olkopf</author>
<author>Peter Bartlett</author>
<author>Alex Smola</author>
<author>Robert Williamson</author>
</authors>
<title>Shrinking the tube: a new support vector regression algorithm.</title>
<date>1999</date>
<booktitle>In Proceedings of the 1998 conference on Advances in neural information processing systems II,</booktitle>
<pages>330--336</pages>
<publisher>MIT Press.</publisher>
<location>Cambridge, MA, USA.</location>
<marker>Sch¨olkopf, Bartlett, Smola, Williamson, 1999</marker>
<rawString>Bernhard Sch¨olkopf, Peter Bartlett, Alex Smola, and Robert Williamson. 1999. Shrinking the tube: a new support vector regression algorithm. In Proceedings of the 1998 conference on Advances in neural information processing systems II, pages 330– 336, Cambridge, MA, USA. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4191" citStr="Wu and Palmer, 1994" startWordPosition="641" endWordPosition="644">t, so we take all possible meanings into account. Given Cp and CQ as the sets of concepts contained in sentences p and q, respectively, with 1http://www-nlp.stanford.edu/software/corenlp.shtml 2http://nlp.lsi.upc.edu/freeling/ 400 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 400–405, Dublin, Ireland, August 23-24, 2014. |Cp |&gt; |Cq|, the conceptual similarity between p and q is calculated as: |Cp| where s(c1, c2) is a conceptual similarity measure. Concept similarity can be calculated in different ways. We used a variation of the Wu-Palmer formula (Wu and Palmer, 1994) named “ProxiGenea3”, introduced by (Dudognon et al., 2010), which is inspired by the analogy between a family tree and the concept hierarchy in WordNet. The ProxiGenea3 measure is defined as: 1 s(c1, c2) = 1 + d(c1) + d(c2) − 2 · d(c0) where c0 is the most specific concept that is present both in the synset path of c1 and c2 (that is, the Least Common Subsumer or LCS). The function returning the depth of a concept is noted with d. 2.2 IC-based Similarity This measure has been proposed by (Mihalcea et al., 2006) as a corpus-based measure which uses Resnik’s Information Content (IC) and the Jia</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, ACL ’94, pages 133–138, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>