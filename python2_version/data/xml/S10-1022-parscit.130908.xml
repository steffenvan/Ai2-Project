<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000096">
<title confidence="0.9890935">
TANL-1: Coreference Resolution by Parse Analysis and
Similarity Clustering
</title>
<author confidence="0.845668">
Giuseppe Attardi
</author>
<affiliation confidence="0.753178">
Dipartimento di Informatica
</affiliation>
<address confidence="0.6931795">
Università di Pisa
Largo B. Pontecorvo, 3
</address>
<email confidence="0.998737">
attardi@di.unipi.it
</email>
<author confidence="0.790704">
Stefano Dei Rossi
</author>
<affiliation confidence="0.702322">
Dipartimento di Informatica
</affiliation>
<address confidence="0.6862315">
Università di Pisa
Largo B. Pontecorvo, 3
</address>
<email confidence="0.998192">
deirossi@di.unipi.it
</email>
<author confidence="0.798301">
Maria Simi
</author>
<affiliation confidence="0.71015">
Dipartimento di Informatica
</affiliation>
<address confidence="0.690784">
Università di Pisa
Largo B. Pontecorvo, 3
</address>
<email confidence="0.99861">
simi@di.unipi.it
</email>
<sectionHeader confidence="0.995625" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999953">
Our submission to the Semeval 2010 task
on coreference resolution in multiple lan-
guages is based on parse analysis and si-
milarity clustering. The system uses a bi-
nary classifier, based on Maximum En-
tropy, to decide whether or not there is a
relationship between each pair of men-
tions extracted from a textual document.
Mention detection is based on the analy-
sis of the dependency parse tree.
</bodyText>
<sectionHeader confidence="0.994304" genericHeader="keywords">
1 Overview
</sectionHeader>
<bodyText confidence="0.999772828125">
Coreference resolution can be described as the
problem of clustering noun phrases (NP), also
called mentions, into sets referring to the same
discourse entity.
The “Coreference Resolution in Multiple Lan-
guages task” at SemEval-2010 is meant to assess
different machine learning techniques in a multi-
lingual context, and by means of different
evaluation metrics. Two different scenarios are
considered: a gold standard scenario (only avail-
able for Catalan and Spanish), where correct
mention boundaries are provided to the partici-
pants, and a regular scenario, where mention
boundaries are to be inferred from other linguis-
tic annotations provided in the input data. In par-
ticular the linguistic annotations provided for
each token in a sentence are: position in sentence
(ID), word (TOKEN), lemma and predicted
lemma (LEMMA and PLEMMA), morpho-
syntactic information, both gold and/or predicted
(POS and PPOS, FEAT and PFEAT), depend-
ency parsing annotations (HEAD and PHEAD,
DEPREL and PDEPREL), named entities (NE
and PNE), and semantic roles (PRED, PPRED,
and corresponding roles in the following col-
umns). In the gold scenario, mention boundaries
annotations (in column COREF) can also be used
as input.
Our approach to the task was to split corefer-
ence resolution into two sub-problems: mention
identification and creation of entities. Mention
recognition was based on the analysis of parse
trees produced from input data, which were pro-
duced by manual annotation or state-of-the-art
dependency parsers. Once the mentions are iden-
tified, coreference resolution involves partition-
ing them into subsets corresponding to the same
entity. This problem is cast into the binary classi-
fication problem of deciding whether two given
mentions are coreferent. A Maximum Entropy
classifier is trained to predict how likely two
mentions refer to the same entity. This is fol-
lowed by a greedy procedure whose purpose is to
cluster mentions into entities.
According to Ng (2005), most learning based
coreference systems can be defined by four ele-
ments: the learning algorithm used to train the
coreference classifier, the method of creating
training instances for the learner, the feature set
used to represent a training or test instance, and
the clustering algorithm used to coordinate the
coreference classification decisions. In the fol-
lowing we will detail our approach by making
explicit the strategies used in each of above men-
tioned components.
The data model used by our system is based
on the concepts of entity and mention. The col-
lection of mentions referring to the same object
in a document forms an entity. A mention is an
instance referring to an object: it is represented
by the start and end positions in a sentence, a
type and a sequence number. For convenience it
also contains a frequency count and a reference
to the containing sentence.
</bodyText>
<page confidence="0.98084">
108
</page>
<note confidence="0.48948">
Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 108–111,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.651501" genericHeader="introduction">
2 Mention detection
</sectionHeader>
<bodyText confidence="0.999968775510204">
The first stage of the coreference resolution
process tries to identify the occurrence of men-
tions in documents.
In the training phase mentions are obtained
from the NE (or PNE) column of the corpus and
are partitioned into entities using the information
provided in the COREF column.
In the regular setting, we used an algorithm for
predicting boundaries that relies on the parse tree
of the sentence produced from the gold annota-
tions in columns HEAD and DEP, if available, or
else from columns PHEAD and PDEP, the out-
put of a dependency parser provided as input da-
ta.
This analysis relied on minimal language
knowledge, in order to determine possible heads
of sub-trees counting as mentions, i.e. noun
phrases or adverbial phrases referring to quanti-
ties, times and locations. POS tags and morpho-
logical features, when available, were mostly
taken into account in determining mention heads.
The leaves of the sub-trees of each detected head
were collected as possible mentions.
The mentions identified by the NE column
were then added to this set, discarding duplicates
or partial overlaps. Partial overlaps in principle
should not occur, but were present occasionally
in the data. When this occurred, we applied a
strategy to split them into a pair of mentions.
The same mention detection strategy was used
also in the gold task, where we could have just
returned the boundaries present in the data, scor-
ing 100% in accuracy. This explains the small
loss in accuracy we achieved in mention identifi-
cation in the gold setting.
Relying on parse trees turned out to be quite
effective, especially for languages where gold
parses where available. For some other languag-
es, the strategy was less effective. This was due
to different annotation policies across different
languages, and, in part, to inconsistencies in the
data. For example in the Italian data set, named
entities may include prepositions, which are typ-
ically the head of the noun phrase, while our
strategy of looking for noun heads leaves the
preposition out of the mention boundaries.
Moreover this strategy obviously fails when
mentions span across sentences as was the case,
again, for Italian.
</bodyText>
<sectionHeader confidence="0.989962" genericHeader="method">
3 Determining coreference
</sectionHeader>
<bodyText confidence="0.9999775">
For determining which mentions belong to the
same entity, we applied a machine learning tech-
nique. We trained a Maximum Entropy classifier
written in Python (Le, 2004) to determine
whether two mentions refer to the same entity.
We did do not make any effort to optimize the
number of training instances for the pair-wise
learner: a positive instance is created for each
anaphoric NP, paired with each of its antecedents
with the same number, and a negative instance is
created by pairing each NP with each of its pre-
ceding non-coreferent noun phrases.
The classifier is trained using the following
features, extracted for each pair of mentions.
</bodyText>
<subsectionHeader confidence="0.636991">
Lexical features
</subsectionHeader>
<bodyText confidence="0.986079333333333">
Same: whether two mentions are equal;
Prefix: whether one mention is a prefix of
the other;
Suffix: whether one mention is a suffix of
the other;
Acronym: whether one mention is the
acronym of the other.
Edit distance: quantized editing distance
between two mentions.
</bodyText>
<sectionHeader confidence="0.501338" genericHeader="method">
Distance features
</sectionHeader>
<bodyText confidence="0.9489845">
Sentence distance: quantized distance be-
tween the sentences containing the two
mentions;
Token distance: quantized distance be-
tween the start tokens of the two mentions;
Mention distance: quantized number of
other mentions between two mentions.
Syntax features
Head: whether the heads of two mentions
have the same POS;
Head POS: pairs of POS of the two men-
tions heads;
</bodyText>
<subsectionHeader confidence="0.621737">
Count features
</subsectionHeader>
<bodyText confidence="0.850210714285714">
Count: pairs of quantized numbers, each
counting how many times a mention oc-
curs.
Type features
Type: whether two mentions have the
same associated NE (Named Entity) type.
Pronoun features
</bodyText>
<page confidence="0.998036">
109
</page>
<bodyText confidence="0.836274055555556">
When the most recent mention is a pronominal
anaphora, the following features are extracted:
Gender: pair of attributes {female, male or
undetermined};
Number: pair of attributes {singular, plur-
al, undetermined};
Pronoun type: this feature is language de-
pendent and represents the type of prono-
minal mention, i.e. whether the pronoun is
reflexive, possessive, relative, ...
In the submitted run we used the GIS (Genera-
lized Iterative Scaling) algorithm for parameter
estimation, with 600 iterations, which appeared
to provide better results than using L-BFGS (a
limited-memory algorithm for unconstrained op-
timization). Training times ranged from one
minute for German to 8 minutes for Italian,
hence the slower speed of GIS was not an issue.
</bodyText>
<subsectionHeader confidence="0.98911">
3.1 Entity creation
</subsectionHeader>
<bodyText confidence="0.999988428571429">
The mentions detected in the first phase were
clustered, according to the output of the classifi-
er, using a greedy clustering algorithm.
Each mention is compared to all previous
mentions, which are collected in a global men-
tions table. If the pair-wise classifier assigns a
probability greater than a given threshold to the
fact that a new mention belongs to a previously
identified entity, it is assigned to that entity. In
case more than one entity has a probability great-
er than the threshold, the mention is assigned to
the one with highest probability. This strategy
has been described as best-first clustering by Ng
(2005).
In principle the process is not optimal since,
once a mention is assigned to an entity, it cannot
be later assigned to another entity to which it
more likely refers. Luo et al. (2004) propose an
approach based on the Bell tree to address this
problem. Despite this potential limitation, our
system performed quite well.
</bodyText>
<sectionHeader confidence="0.989852" genericHeader="method">
4 Data preparation
</sectionHeader>
<bodyText confidence="0.999973655172414">
We used the data as supplied by the task organ-
izers for all languages except Italian. A modified
version of the Hunpos tagger (Halácsy, Kornai &amp;
Oravecz, 2007; Attardi et al., 2009) was used to
add to the Italian training and development cor-
pora more accurate POS tags than those supplied,
as well as missing information about morphol-
ogy. The POS tagger we used, in fact is capable
of tagging sentences with detailed POS tags,
which include morphological information; this
was added to column PFEATS in the data. Just
for this reason our submission for Italian is to be
considered an open task submission.
The Italian training corpus appears to contain
several errors related to mention boundaries. In
particular there are cases of entities starting in a
sentence and ending in the following one. This
appears to be due to sentence splitting (for in-
stance at semicolons) performed after named ent-
ities had been tagged. As explained in section 2,
our system was not prepared to deal with these
situations.
Other errors in the annotations of entities oc-
curred in the Italian test data, in particular incor-
rect balancing of openings and closings named
entities, which caused problems to our submis-
sion. We could only complete the run after the
deadline, so we could only report unofficial re-
sults for Italian.
</bodyText>
<sectionHeader confidence="0.999889" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999756888888889">
We submitted results to the gold and regular
challenges for the following languages: Catalan,
English, German and Spanish.
Table 1 summarizes the performance of our
system, according to the different accuracy
scores for the gold task, Table 2 for the regular
task. We have outlined in bold the cases where
we achieved the best scores among the partici-
pating systems.
</bodyText>
<table confidence="0.9997428">
Mention CEAF MUC B3 BLANC
Catalan 98.4 64.9 26.5 76.2 54.4
German 100 77.7 25.9 85.9 57.4
English 89.8 67.6 24.0 73.4 52.1
Spanish 98.4 65.8 25.7 76.8 54.1
</table>
<tableCaption confidence="0.999569">
Table 1. Gold task, Accuracy scores.
</tableCaption>
<table confidence="0.9999416">
Mention CEAF MUC B3 BLANC
Catalan 82.7 57.1 22.9 64.6 51.0
German 59.2 49.5 15.4 50.7 44.7
English 73.9 57.3 24.6 61.3 49.3
Spanish 83.1 59.3 21.7 66.0 51.4
</table>
<tableCaption confidence="0.999922">
Table 2. Regular task. Accuracy scores.
</tableCaption>
<sectionHeader confidence="0.988675" genericHeader="method">
6 Error analysis
</sectionHeader>
<bodyText confidence="0.999973166666667">
We performed some preliminary error analysis.
The goal was to identify systematic errors and
possible corrections for improving the perfor-
mance of our system.
We limited our analysis to the mention boun-
daries detection for the regular tasks. A similar
</bodyText>
<page confidence="0.992684">
110
</page>
<bodyText confidence="0.9173725">
analysis for coreference detection, would require
the availability of gold test data.
</bodyText>
<sectionHeader confidence="0.836465" genericHeader="method">
7 Mention detection errors
</sectionHeader>
<bodyText confidence="0.999795264705882">
As described above, the strategy used for the ex-
traction of mentions boundaries is based on de-
pendency parse trees and named entities. This
proved to be a good strategy in some languages
such as Catalan (F1 score: 82.7) and Spanish (F1
score: 83.1) in which the dependency data avail-
able in the corpora were very accurate and con-
sistent with the annotation of named entities. In-
stead, there have been unexpected problems in
other languages like English or German, where
the dependencies information were annotated
using a different approach.
For German, while we achieved the best B3
accuracy on coreference analysis in the gold set-
tings, we had a quite low accuracy in mention
detection (F1: 59.2), which was responsible of a
significant drop in coreference accuracy for the
regular task. This degradation in performance
was mainly due to punctuations, which in Ger-
man are linked to the sub-tree containing the
noun phrase rather than to the root of the sen-
tence or tokens outside the noun phrase, as it
happens in Catalan and Spanish. This misled our
mention detection algorithm to create many men-
tions with wrong boundaries, just because punc-
tuation marks were included.
In the English corpus different conventions
were apparently used for dependency parsing and
named entity annotations (Table 3), which pro-
duced discrepancies between the boundaries of
the named entities present in the data and those
predicted by our algorithm. This in turn affected
negatively the coreference detection algorithm
that uses both types of information.
</bodyText>
<table confidence="0.9876592">
ID TOKEN HEAD DEPREL NE COREF
1 Defense 2 NAME (org) (25
2 Secretary 4 NMOD _ _
3 William 4 NAME (person _
4 Cohen 5 SBJ person) 25)
</table>
<tableCaption confidence="0.981785">
Table 3. Example of different conventions for NE and
COREF in the English corpus.
</tableCaption>
<bodyText confidence="0.999934875">
Error analysis also has shown that further im-
provements could be obtained, for all languages,
by using more accurate language specific extrac-
tion rules. For example, we missed to consider a
number of specific POS tags as possible identifi-
ers for the head of noun phrases. By some simple
tuning of the algorithm we obtained some im-
provements.
</bodyText>
<sectionHeader confidence="0.998946" genericHeader="conclusions">
8 Conclusions
</sectionHeader>
<bodyText confidence="0.999953">
We reported our experiments on coreference res-
olution in multiple languages. We applied an ap-
proach based on analyzing the parse trees in or-
der to detect mention boundaries and a Maxi-
mum Entropy classifier to cluster mentions into
entities.
Despite a very simplistic approach, the results
were satisfactory and further improvements are
possible by tuning the parameters of the algo-
rithms.
</bodyText>
<sectionHeader confidence="0.99917" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999789695652174">
G. Attardi et al., 2009. Tanl (Text Analytics and Natu-
ral Language Processing). SemaWiki project:
http://medialab.di.unipi.it/wiki/SemaWiki.
P. Halácsy, A. Kornai, and C. Oravecz, 2007. Hun-
Pos: an open source trigram tagger. Proceedings of
the ACL 2007, Prague.
Z. Le, Maximum Entropy Modeling Toolkit for Pytho
and C++, Reference Manual.
X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla &amp; S.
Roukos. 2004. A Mention-Synchronous Corefer-
ence Resolution Algorithm Based on the Bell Tree.
Proceedings of the ACL 2004, Barcelona.
V. Ng, Machine Learning for Coreference Resolution:
From Local Classification to Global Ranking, Pro-
ceedings of the 43rd Annual Meeting of the Associ-
ation for Computational Linguistics (ACL), Ann
Arbor, MI, June 2005, pp. 157-164.
M. Recasens, L. Màrquez, E. Sapena, M. A. Martí,
M. Taulé, V. Hoste, M. Poesio and Y. Versley,
SemEval-2010 Task 1: Coreference resolution in
multiple languages, in Proceedings of the 5th In-
ternational Workshop on Semantic Evaluations
(SemEval-2010), Uppsala, Sweden, 2010.
</reference>
<page confidence="0.998799">
111
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.043120">
<title confidence="0.9629325">TANL-1: Coreference Resolution by Parse Analysis and Similarity Clustering</title>
<author confidence="0.996159">Giuseppe Attardi</author>
<affiliation confidence="0.998275">Dipartimento di Informatica Università di Pisa</affiliation>
<address confidence="0.982493">Largo B. Pontecorvo, 3</address>
<email confidence="0.991928">attardi@di.unipi.it</email>
<author confidence="0.998183">Stefano Dei Rossi</author>
<affiliation confidence="0.999195">Dipartimento di Informatica Università di Pisa</affiliation>
<address confidence="0.989432">Largo B. Pontecorvo, 3</address>
<email confidence="0.993448">deirossi@di.unipi.it</email>
<author confidence="0.999899">Maria Simi</author>
<affiliation confidence="0.999095">Dipartimento di Informatica Università di Pisa</affiliation>
<address confidence="0.98461">Largo B. Pontecorvo, 3</address>
<email confidence="0.996741">simi@di.unipi.it</email>
<abstract confidence="0.997515463126844">Our submission to the Semeval 2010 task on coreference resolution in multiple languages is based on parse analysis and similarity clustering. The system uses a binary classifier, based on Maximum Entropy, to decide whether or not there is a relationship between each pair of mentions extracted from a textual document. Mention detection is based on the analysis of the dependency parse tree. 1 Overview Coreference resolution can be described as the problem of clustering noun phrases (NP), also into sets referring to the same discourse entity. “Coreference Resolution in Multiple task” SemEval-2010 is meant to assess different machine learning techniques in a multilingual context, and by means of different evaluation metrics. Two different scenarios are a standard (only available for Catalan and Spanish), where correct mention boundaries are provided to the particiand a where mention boundaries are to be inferred from other linguistic annotations provided in the input data. In particular the linguistic annotations provided for each token in a sentence are: position in sentence (ID), word (TOKEN), lemma and predicted lemma (LEMMA and PLEMMA), morphosyntactic information, both gold and/or predicted (POS and PPOS, FEAT and PFEAT), dependency parsing annotations (HEAD and PHEAD, DEPREL and PDEPREL), named entities (NE and PNE), and semantic roles (PRED, PPRED, and corresponding roles in the following columns). In the gold scenario, mention boundaries annotations (in column COREF) can also be used as input. Our approach to the task was to split coreference resolution into two sub-problems: mention identification and creation of entities. Mention recognition was based on the analysis of parse trees produced from input data, which were produced by manual annotation or state-of-the-art dependency parsers. Once the mentions are identified, coreference resolution involves partitioning them into subsets corresponding to the same entity. This problem is cast into the binary classification problem of deciding whether two given mentions are coreferent. A Maximum Entropy classifier is trained to predict how likely two mentions refer to the same entity. This is followed by a greedy procedure whose purpose is to cluster mentions into entities. According to Ng (2005), most learning based coreference systems can be defined by four elethe algorithm to train the classifier, the of creating instances the learner, the set used to represent a training or test instance, and algorithm to coordinate the coreference classification decisions. In the following we will detail our approach by making explicit the strategies used in each of above mentioned components. The data model used by our system is based the concepts of The collection of mentions referring to the same object a document forms an A mention is an instance referring to an object: it is represented the in a sentence, a type and a sequence number. For convenience it also contains a frequency count and a reference to the containing sentence. 108 of the 5th International Workshop on Semantic Evaluation, ACL pages 108–111, Sweden, 15-16 July 2010. Association for Computational Linguistics 2 Mention detection The first stage of the coreference resolution process tries to identify the occurrence of mentions in documents. In the training phase mentions are obtained from the NE (or PNE) column of the corpus and are partitioned into entities using the information provided in the COREF column. In the regular setting, we used an algorithm for predicting boundaries that relies on the parse tree of the sentence produced from the gold annotations in columns HEAD and DEP, if available, or else from columns PHEAD and PDEP, the output of a dependency parser provided as input data. This analysis relied on minimal language knowledge, in order to determine possible heads of sub-trees counting as mentions, i.e. noun phrases or adverbial phrases referring to quantities, times and locations. POS tags and morphological features, when available, were mostly taken into account in determining mention heads. The leaves of the sub-trees of each detected head were collected as possible mentions. The mentions identified by the NE column were then added to this set, discarding duplicates or partial overlaps. Partial overlaps in principle should not occur, but were present occasionally in the data. When this occurred, we applied a strategy to split them into a pair of mentions. The same mention detection strategy was used also in the gold task, where we could have just returned the boundaries present in the data, scoring 100% in accuracy. This explains the small loss in accuracy we achieved in mention identification in the gold setting. Relying on parse trees turned out to be quite effective, especially for languages where gold parses where available. For some other languages, the strategy was less effective. This was due to different annotation policies across different languages, and, in part, to inconsistencies in the data. For example in the Italian data set, named entities may include prepositions, which are typically the head of the noun phrase, while our strategy of looking for noun heads leaves the preposition out of the mention boundaries. Moreover this strategy obviously fails when mentions span across sentences as was the case, again, for Italian. 3 Determining coreference For determining which mentions belong to the entity, we applied a machine learning technique. We trained a Maximum Entropy classifier written in Python (Le, 2004) to determine whether two mentions refer to the same entity. We did do not make any effort to optimize the number of training instances for the pair-wise learner: a positive instance is created for each anaphoric NP, paired with each of its antecedents with the same number, and a negative instance is created by pairing each NP with each of its preceding non-coreferent noun phrases. The classifier is trained using the following features, extracted for each pair of mentions. Lexical features two mentions are equal; one mention is a prefix of the other; one mention is a suffix of the other; one mention is the acronym of the other. distance: editing distance between two mentions. Distance features distance: distance between the sentences containing the two mentions; distance: distance between the start tokens of the two mentions; distance: number of other mentions between two mentions. Syntax features the heads of two mentions have the same POS; POS: of POS of the two mentions heads; Count features pairs of quantized numbers, each counting how many times a mention occurs. Type features whether two mentions have the same associated NE (Named Entity) type. Pronoun features 109 When the most recent mention is a pronominal anaphora, the following features are extracted: pair of attributes {female, male or undetermined}; pair of attributes {singular, plural, undetermined}; this feature is language dependent and represents the type of pronominal mention, i.e. whether the pronoun is ... In the submitted run we used the GIS (Generalized Iterative Scaling) algorithm for parameter estimation, with 600 iterations, which appeared to provide better results than using L-BFGS (a limited-memory algorithm for unconstrained optimization). Training times ranged from one minute for German to 8 minutes for Italian, hence the slower speed of GIS was not an issue. 3.1 Entity creation The mentions detected in the first phase were clustered, according to the output of the classifier, using a greedy clustering algorithm. Each mention is compared to all previous mentions, which are collected in a global mentions table. If the pair-wise classifier assigns a probability greater than a given threshold to the fact that a new mention belongs to a previously identified entity, it is assigned to that entity. In case more than one entity has a probability greater than the threshold, the mention is assigned to the one with highest probability. This strategy been described as clustering Ng (2005). In principle the process is not optimal since, once a mention is assigned to an entity, it cannot be later assigned to another entity to which it more likely refers. Luo et al. (2004) propose an approach based on the Bell tree to address this problem. Despite this potential limitation, our system performed quite well. 4 Data preparation We used the data as supplied by the task organizers for all languages except Italian. A modified version of the Hunpos tagger (Halácsy, Kornai &amp; Oravecz, 2007; Attardi et al., 2009) was used to add to the Italian training and development corpora more accurate POS tags than those supplied, as well as missing information about morphology. The POS tagger we used, in fact is capable of tagging sentences with detailed POS tags, which include morphological information; this was added to column PFEATS in the data. Just for this reason our submission for Italian is to be considered an open task submission. The Italian training corpus appears to contain several errors related to mention boundaries. In particular there are cases of entities starting in a sentence and ending in the following one. This appears to be due to sentence splitting (for instance at semicolons) performed after named entities had been tagged. As explained in section 2, our system was not prepared to deal with these situations. Other errors in the annotations of entities occurred in the Italian test data, in particular incorrect balancing of openings and closings named entities, which caused problems to our submission. We could only complete the run after the deadline, so we could only report unofficial results for Italian. 5 Results We submitted results to the gold and regular challenges for the following languages: Catalan, English, German and Spanish. Table 1 summarizes the performance of our system, according to the different accuracy scores for the gold task, Table 2 for the regular task. We have outlined in bold the cases where we achieved the best scores among the participating systems. Mention CEAF MUC BLANC Catalan 98.4 64.9 26.5 76.2 54.4 German 100 77.7 25.9 85.9 57.4 English 89.8 67.6 24.0 73.4 52.1 Spanish 98.4 65.8 25.7 76.8 54.1 Table 1. Gold task, Accuracy scores. Mention CEAF MUC BLANC Catalan 82.7 57.1 22.9 64.6 51.0 German 59.2 49.5 15.4 50.7 44.7 English 73.9 57.3 24.6 61.3 49.3 Spanish 83.1 59.3 21.7 66.0 51.4 Table 2. Regular task. Accuracy scores. 6 Error analysis We performed some preliminary error analysis. The goal was to identify systematic errors and possible corrections for improving the performance of our system. We limited our analysis to the mention boundaries detection for the regular tasks. A similar 110 analysis for coreference detection, would require the availability of gold test data. 7 Mention detection errors As described above, the strategy used for the extraction of mentions boundaries is based on dependency parse trees and named entities. This proved to be a good strategy in some languages such as Catalan (F1 score: 82.7) and Spanish (F1 score: 83.1) in which the dependency data available in the corpora were very accurate and consistent with the annotation of named entities. Instead, there have been unexpected problems in other languages like English or German, where the dependencies information were annotated using a different approach. German, while we achieved the best accuracy on coreference analysis in the gold settings, we had a quite low accuracy in mention detection (F1: 59.2), which was responsible of a significant drop in coreference accuracy for the regular task. This degradation in performance was mainly due to punctuations, which in German are linked to the sub-tree containing the noun phrase rather than to the root of the sentence or tokens outside the noun phrase, as it happens in Catalan and Spanish. This misled our mention detection algorithm to create many mentions with wrong boundaries, just because punctuation marks were included. In the English corpus different conventions were apparently used for dependency parsing and named entity annotations (Table 3), which produced discrepancies between the boundaries of the named entities present in the data and those predicted by our algorithm. This in turn affected negatively the coreference detection algorithm that uses both types of information. ID TOKEN HEAD DEPREL NE COREF 1 Defense 2 NAME (org) (25 2 Secretary 4 NMOD _ _ 3 William 4 NAME (person _ 4 Cohen 5 SBJ person) 25) Table 3. Example of different conventions for NE and COREF in the English corpus. Error analysis also has shown that further improvements could be obtained, for all languages, by using more accurate language specific extraction rules. For example, we missed to consider a number of specific POS tags as possible identifiers for the head of noun phrases. By some simple tuning of the algorithm we obtained some improvements. 8 Conclusions We reported our experiments on coreference resolution in multiple languages. We applied an approach based on analyzing the parse trees in order to detect mention boundaries and a Maximum Entropy classifier to cluster mentions into entities. Despite a very simplistic approach, the results were satisfactory and further improvements are possible by tuning the parameters of the algorithms.</abstract>
<note confidence="0.723868">References G. Attardi et al., 2009. Tanl (Text Analytics and Natural Language Processing). SemaWiki project:</note>
<web confidence="0.536029">http://medialab.di.unipi.it/wiki/SemaWiki.</web>
<note confidence="0.808935761904762">P. Halácsy, A. Kornai, and C. Oravecz, 2007. Hunan open source trigram tagger. of ACL Prague. Z. Le, Maximum Entropy Modeling Toolkit for Pytho and C++, Reference Manual. X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla &amp; S. Roukos. 2004. A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree. of the ACL Barcelona. V. Ng, Machine Learning for Coreference Resolution: Local Classification to Global Ranking, Proceedings of the 43rd Annual Meeting of the Associfor Computational Linguistics Ann Arbor, MI, June 2005, pp. 157-164. M. Recasens, L. Màrquez, E. Sapena, M. A. Martí, M. Taulé, V. Hoste, M. Poesio and Y. Versley, SemEval-2010 Task 1: Coreference resolution in multiple languages, in Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Uppsala, Sweden, 2010. 111</note>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Attardi</author>
</authors>
<title>Tanl (Text Analytics and Natural Language Processing). SemaWiki project: http://medialab.di.unipi.it/wiki/SemaWiki.</title>
<date>2009</date>
<marker>Attardi, 2009</marker>
<rawString>G. Attardi et al., 2009. Tanl (Text Analytics and Natural Language Processing). SemaWiki project: http://medialab.di.unipi.it/wiki/SemaWiki.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Halácsy</author>
<author>A Kornai</author>
<author>C Oravecz</author>
</authors>
<title>HunPos: an open source trigram tagger.</title>
<date>2007</date>
<booktitle>Proceedings of the ACL 2007,</booktitle>
<location>Prague.</location>
<marker>Halácsy, Kornai, Oravecz, 2007</marker>
<rawString>P. Halácsy, A. Kornai, and C. Oravecz, 2007. HunPos: an open source trigram tagger. Proceedings of the ACL 2007, Prague.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Z Le</author>
</authors>
<title>Maximum Entropy Modeling Toolkit for Pytho and C++, Reference Manual.</title>
<marker>Le, </marker>
<rawString>Z. Le, Maximum Entropy Modeling Toolkit for Pytho and C++, Reference Manual.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Luo</author>
<author>A Ittycheriah</author>
<author>H Jing</author>
<author>N Kambhatla</author>
<author>S Roukos</author>
</authors>
<title>A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree.</title>
<date>2004</date>
<booktitle>Proceedings of the ACL 2004,</booktitle>
<location>Barcelona.</location>
<contexts>
<context position="9143" citStr="Luo et al. (2004)" startWordPosition="1455" endWordPosition="1458">e collected in a global mentions table. If the pair-wise classifier assigns a probability greater than a given threshold to the fact that a new mention belongs to a previously identified entity, it is assigned to that entity. In case more than one entity has a probability greater than the threshold, the mention is assigned to the one with highest probability. This strategy has been described as best-first clustering by Ng (2005). In principle the process is not optimal since, once a mention is assigned to an entity, it cannot be later assigned to another entity to which it more likely refers. Luo et al. (2004) propose an approach based on the Bell tree to address this problem. Despite this potential limitation, our system performed quite well. 4 Data preparation We used the data as supplied by the task organizers for all languages except Italian. A modified version of the Hunpos tagger (Halácsy, Kornai &amp; Oravecz, 2007; Attardi et al., 2009) was used to add to the Italian training and development corpora more accurate POS tags than those supplied, as well as missing information about morphology. The POS tagger we used, in fact is capable of tagging sentences with detailed POS tags, which include mor</context>
</contexts>
<marker>Luo, Ittycheriah, Jing, Kambhatla, Roukos, 2004</marker>
<rawString>X. Luo, A. Ittycheriah, H. Jing, N. Kambhatla &amp; S. Roukos. 2004. A Mention-Synchronous Coreference Resolution Algorithm Based on the Bell Tree. Proceedings of the ACL 2004, Barcelona.</rawString>
</citation>
<citation valid="true">
<authors>
<author>V Ng</author>
</authors>
<title>Machine Learning for Coreference Resolution: From Local Classification to Global Ranking,</title>
<date>2005</date>
<booktitle>Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL),</booktitle>
<pages>157--164</pages>
<location>Ann Arbor, MI,</location>
<contexts>
<context position="2767" citStr="Ng (2005)" startWordPosition="422" endWordPosition="423"> was based on the analysis of parse trees produced from input data, which were produced by manual annotation or state-of-the-art dependency parsers. Once the mentions are identified, coreference resolution involves partitioning them into subsets corresponding to the same entity. This problem is cast into the binary classification problem of deciding whether two given mentions are coreferent. A Maximum Entropy classifier is trained to predict how likely two mentions refer to the same entity. This is followed by a greedy procedure whose purpose is to cluster mentions into entities. According to Ng (2005), most learning based coreference systems can be defined by four elements: the learning algorithm used to train the coreference classifier, the method of creating training instances for the learner, the feature set used to represent a training or test instance, and the clustering algorithm used to coordinate the coreference classification decisions. In the following we will detail our approach by making explicit the strategies used in each of above mentioned components. The data model used by our system is based on the concepts of entity and mention. The collection of mentions referring to the</context>
<context position="8958" citStr="Ng (2005)" startWordPosition="1423" endWordPosition="1424">ed in the first phase were clustered, according to the output of the classifier, using a greedy clustering algorithm. Each mention is compared to all previous mentions, which are collected in a global mentions table. If the pair-wise classifier assigns a probability greater than a given threshold to the fact that a new mention belongs to a previously identified entity, it is assigned to that entity. In case more than one entity has a probability greater than the threshold, the mention is assigned to the one with highest probability. This strategy has been described as best-first clustering by Ng (2005). In principle the process is not optimal since, once a mention is assigned to an entity, it cannot be later assigned to another entity to which it more likely refers. Luo et al. (2004) propose an approach based on the Bell tree to address this problem. Despite this potential limitation, our system performed quite well. 4 Data preparation We used the data as supplied by the task organizers for all languages except Italian. A modified version of the Hunpos tagger (Halácsy, Kornai &amp; Oravecz, 2007; Attardi et al., 2009) was used to add to the Italian training and development corpora more accurate</context>
</contexts>
<marker>Ng, 2005</marker>
<rawString>V. Ng, Machine Learning for Coreference Resolution: From Local Classification to Global Ranking, Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL), Ann Arbor, MI, June 2005, pp. 157-164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Recasens</author>
<author>L Màrquez</author>
<author>E Sapena</author>
<author>M A Martí</author>
<author>M Taulé</author>
<author>V Hoste</author>
<author>M Poesio</author>
<author>Y Versley</author>
</authors>
<date>2010</date>
<booktitle>SemEval-2010 Task 1: Coreference resolution in multiple languages, in Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010),</booktitle>
<location>Uppsala, Sweden,</location>
<marker>Recasens, Màrquez, Sapena, Martí, Taulé, Hoste, Poesio, Versley, 2010</marker>
<rawString>M. Recasens, L. Màrquez, E. Sapena, M. A. Martí, M. Taulé, V. Hoste, M. Poesio and Y. Versley, SemEval-2010 Task 1: Coreference resolution in multiple languages, in Proceedings of the 5th International Workshop on Semantic Evaluations (SemEval-2010), Uppsala, Sweden, 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>