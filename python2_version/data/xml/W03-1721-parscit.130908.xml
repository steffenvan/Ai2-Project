<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.039964">
<title confidence="0.994115">
Chinese Word Segmentation Using Minimal Linguistic Knowledge
</title>
<author confidence="0.993269">
Aitao Chen
</author>
<affiliation confidence="0.845150333333333">
School of Information Management and Systems
University of California at Berkeley
Berkeley, CA 94720, USA
</affiliation>
<email confidence="0.99879">
aitao@sims.berkeley.edu
</email>
<sectionHeader confidence="0.993899" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999865375">
This paper presents a primarily data-driven Chi-
nese word segmentation system and its perfor-
mances on the closed track using two corpora at
the first international Chinese word segmentation
bakeoff. The system consists of a new words rec-
ognizer, a base segmentation algorithm, and pro-
cedures for combining single characters, suffixes,
and checking segmentation consistencies.
</bodyText>
<sectionHeader confidence="0.9988" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999575875">
At the first Chinese word segmentation bakeoff, we partici-
pated in the closed track using the Academia Sinica corpus
( for short) and the Beijing University corpus ( for
short). We will refer to the segmented texts in the training
corpus as the training data, and to both the unsegmented
testing texts and the segmented texts (the reference texts)
as the testing data. For details on the word segmentation
bakeoff, see (Sproat and Emerson, 2003).
</bodyText>
<sectionHeader confidence="0.933544" genericHeader="method">
2 Word segmentation
</sectionHeader>
<bodyText confidence="0.9998724">
New texts are segmented in four steps which are described
in this section. New words are automatically extracted from
the unsegmented testing texts and added to the base dictio-
nary consisting of words from the training data before the
testing texts are segmented, line by line.
</bodyText>
<subsectionHeader confidence="0.993193">
2.1 Base segmentation algorithm
</subsectionHeader>
<bodyText confidence="0.995218571428571">
Given a dictionary and a sentence, our base segmenta-
tion algorithm finds all possible segmentations of the sen-
tence with respect to the dictionary, computes the prob-
ability of each segmentation, and chooses the segmenta-
tion with the highest probability. If a sentence of char-
acters, , has a segmentation of words,
, then the probability of the segmentation
is estimated as ,
where denotes a segmentation of a sentence. The prob-
ability of a word is estimated from the training corpus as
, where is the number of times that the
word occurs in the training corpus, and is the num-
ber of words in the training corpus. When a word is not
in the dictionary, a frequency of 0.5 is assigned to the new
word. The dynamic programming technique is applied to
find the segmentation of the highest probability of a sen-
tence without first enumerating all possible segmentations
of the sentence with respect to the dictionary. Consider the
text fragment with respect to a dictionary con-
taining the words and it
has three segmentations: (1) / (2) /
and (3) / / The probabilities of the three
segmentations are computed as: (1) p( )*p( ); (2)
p( )*p( ); (3) p( )*p( )*p( ). The proba-
bility of a word is estimated by its relative frequency in the
training data. Assume the first segmentation has the highest
probability, then the text fragment will be segmented into
/
</bodyText>
<subsectionHeader confidence="0.99982">
2.2 Combining single characters
</subsectionHeader>
<bodyText confidence="0.9898805">
New words are usually two or more characters long and are
often segmented into single characters. For example, the
word is segmented into / when it is not in the
dictionary. After a sentence is segmented using the base al-
gorithm, the consecutive single Hanzi characters are com-
bined into a word if the in-word probabilities of the single
characters are over a threshold which is empirically deter-
mined from the training data. The in-word probability of
a character is the probability that the character occurs in a
word of two or more characters.
Some Hanzi characters, such as and occur as
words on their own in segmented texts much more fre-
quently than in words of two or more characters. For exam-
ple, in the PK training corpus, the character occurs as a
word on its own 11,559 times, but in a word only 875 times.
On the other hand, some Hanzi characters usually do not
occur alone as words, instead they occur as part of a word.
As an example, the character occurs in a word 17,108
times, but as a word alone only 794 times in the PK training
data. For each character in the training data, we compute its
in-word probability as follow:
,
where is the number of times that character occurs
in the training data, and is the number of times
that character is in a word of two or more characters.
We do not want to combine the single characters that oc-
cur as words alone more often than not. For both the PK
training data and the AS training data, we divided the train-
ing data into two parts, two thirds for training, and one third
for system development. We found that setting the thresh-
old of the in-word probability to 0.85 or around works best
on the development data. After the initial segmentation
of a sentence, the consecutive single-characters are com-
bined into one word if their in-word probabilities are over
the threshold of 0.85. The text fragment
contains a new word which is not in the PK training
data. After the initial segmentation, the text is segmented
into / / / / /, which is subsequently
changed into / / after combining the
three consecutive characters. The in-word probabilities for
the three characters and are 0.94, 0.98, and
0.99, respectively.
</bodyText>
<subsectionHeader confidence="0.999466">
2.3 Combining suffixes
</subsectionHeader>
<bodyText confidence="0.999488166666667">
A small set of characters , such as and fre-
quently occur as the last character in words. We selected
145 such characters from the PK training corpus, and 113
from the AS corpus. After combining single characters, we
combine a suffix character with the word preceding it if the
preceding word is at least two-character long.
</bodyText>
<subsectionHeader confidence="0.995933">
2.4 Consistency check
</subsectionHeader>
<bodyText confidence="0.999973692307692">
The last step is to perform consistency checks. A seg-
mented sentence, after combining single characters and suf-
fixes, is checked against the training data to make sure
that a text fragment in a testing sentence is segmented in
the same way as in the training data if it also occurs in
the training data. From the PK training corpus, we cre-
ated a phrase segmentation table consisting of word quad-
grams, trigrams, bigrams, and unigrams, together with their
segmentations and frequencies. Our phrase table created
from the AS corpus does not include word quad-grams to
reduce the size of the phrase table. For example, from
the training text / / / we create
the following entries (only some are listed to save space):
</bodyText>
<subsectionHeader confidence="0.622631">
text fragment freq segmentation
</subsectionHeader>
<bodyText confidence="0.994758882352941">
After a new sentence is processed by the first three steps,
we look up every word quad-grams of the segmented sen-
tence in the phrase segmentation table. When a word quad-
gram is found in the phrase segmentation table with a differ-
ent segmentation, we replace the segmentation of the word
quad-gram in the segmented sentence by its segmentation
found in the phrase table. This process is continued to word
trigrams, word bigrams, and word unigrams. The idea is
that if a text fragment in a new sentence is found in the
training data, then it should be segmented in the same way
as in the training data. As an example, in the PK testing
data, the sentence is
segmented into / / / / / /
/ after the first three steps (the two characters and
are not, but should be, combined because the in-word
probability of character which is 0.71, is below the
pre-defined threshold of 0.85). The word bigram
is found in the phrase segmentation table with a differ-
ent segmentation, / / So the segmentation
/ is changed to the segmentation / /
in the final segmented sentence. In essence, when a text
fragment has two or more segmentations, its surrounding
context, which can be the preceding word, the following
word, or both, is utilized to choose the most appropriate
segmentation. When a text fragment in a testing sentence
never occurred in the same context in the training data, then
the most frequent segmentation found in the training data is
chosen. Consider the text again, in the testing data,
is segmented into / / by our base
algorithm. In this case, never occurred in the context
of or The consistency
check step changes / / into / / /
since is segmented into / 515 times, but is
treated as one word 105 times in the training data.
</bodyText>
<sectionHeader confidence="0.994631" genericHeader="method">
3 New words recognition
</sectionHeader>
<bodyText confidence="0.998317391304347">
We developed a few procedures to identify new words in
the testing data. Our first procedure is designed to recog-
nize numbers, dates, percent, time, foreign words, etc. We
defined a set of characters consisting of characters such as
the digits ‘0’ to ‘9’ (in ASCII and GB), the letters ‘a’ to
’z’, ‘A’ to ‘Z’ (in ASCII and GB), ‘
➈’, and the like. Any
consecutive sequence of the characters that are in this pre-
defined set of characters is extracted and post-processed.
A set of rules is implemented in the post-processor. One
such rule is that if an extracted text fragments ends with
the character and contains any character in
then remove the ending character and keep the
remaining fragment as a word. For example, our recognizer
will extract the text fragment and
since all the characters are in the pre-defined set of charac-
ters. The post-processor will strip off the trailing character
and return and as words. For per-
sonal names, we developed a program to extract the names
preceding texts such as and a pro-
gram to detect and extract names in a sequence of names
separated by the Chinese punctuation “ ”, such as
a program to extract
</bodyText>
<equation confidence="0.92998025">
1 / / /
1 / /
1 / /
1 /
</equation>
<table confidence="0.971217875">
1
steps dict R P F
1 1 pkd1 0.919 0.838 0.877 0.050 0.984
2 1 pkd2 0.940 0.892 0.915 0.347 0.984
3 1 pkd3 0.949 0.920 0.934 0.507 0.982
4 1-2 pkd3 0.950 0.935 0.942 0.610 0.975
5 1-3 pkd3 0.951 0.940 0.945 0.655 0.972
6 1-4 pkd3 0.955 0.938 0.946 0.647 0.977
</table>
<tableCaption confidence="0.999389">
Table 1: Results for the closed track using the PK corpus.
</tableCaption>
<bodyText confidence="0.998063682926829">
personal names (Chinese or foreign) following title or pro-
fession names, such as in the text
and a program to extract Chinese personal names
based on the preceding word and the following word. For
example, the string in is most
likely a personal name (in this case, it is) since is a Chi-
nese family name, the string is three-character long (a typ-
ical Chinese personal name is either three or two-character
long). Furthermore, the preceding word and the fol-
lowing word are highly unlikely to appear in a Chinese
personal name. For the personal names extracted from the
PK testing data, if the name is two or three-character long,
and if the first character or two is a Chinese family name,
then the family name is separated from the given name. The
family names are not separated from the given names for the
personal names extracted from the AS testing data. In some
cases, we find it difficult to decide whether or not the first
character should be removed from a personal name. Con-
sider the personal name which looks like a Chinese
personal name since the first character is a Chinese family
name, and the name is three-character long. If it is a trans-
lated foreign name (in this case, it is), then the name should
not be split into family name and given name. But if it is
the name of a Chinese personal name, then the family name
should be separated from the given name. For place
names, we developed a simple program to extract names of
cities, counties, towns, villages, streets, etc, by extracting
the strings of up to three characters appearing between two
place name designators. For example, from the text
our program will extract
The last row (in boldface) in Table 1 gives our official re-
sults for the PK closed track. Other rows in the table present
the results under different experimental conditions. The
column labeled steps refers to the executed steps of our
Chinese word segmentation algorithm. Step 1 segments a
text using the base segmentation algorithm, step 2 combines
single characters, step 3 attaches suffixes to the preceding
words, and step 4 performs consistency checks. The four
steps are described in details in section 2. The column la-
beled dict gives the dictionary used in each experiment. The
pkd1 consists of only the words from the PK training cor-
</bodyText>
<table confidence="0.999057666666667">
steps dict R P F
1 1 asd1 0.950 0.936 0.943 0.000 0.970
2 1 asd2 0.950 0.943 0.947 0.132 0.968
3 1-2 asd2 0.951 0.952 0.951 0.337 0.964
4 1-3 asd2 0.949 0.952 0.951 0.372 0.961
5 1-4 asd2 0.966 0.956 0.961 0.364 0.980
</table>
<tableCaption confidence="0.698686">
Table 2: Results for the closed track using the AS corpus.
</tableCaption>
<table confidence="0.999474333333333">
corpus dict R P F
AS asd1 0.917 0.912 0.915 0.000 0.938
PK pkd1 0.909 0.829 0.867 0.050 0.972
</table>
<tableCaption confidence="0.852556">
Table 3: Performances of the maximum matching (forward)
using words from the training data.
</tableCaption>
<bodyText confidence="0.999492694444445">
pus, pkd2 consists of the words in pkd1 and the words con-
verted from pkd1 by changing the GB encoding to ASCII
encoding for the numeric digits and the English letters, and
pkd3 consists of the words in pkd2 and the words automat-
ically extracted from the PK testing texts using the proce-
dures described in section 3. The columns labeled R, P and
F give the recall, precision, and F score, respectively. The
columns labeled and show the recall on out-of-
vocabulary words and the recall on in-vocabulary words,
respectively. All evaluation scores reported in this paper
are computed using the score program written by Richard
Sproat. We refer readers to (Sproat and Emerson, 2003) for
details on the evaluation measures. For example, row 4 in
table 1 gives the results using pkd3 dictionary when a sen-
tence is segmented by the base algorithm, and then the sin-
gle characters in the initial segmentation are combined, but
suffixes are not attached and consistency check is not per-
formed. The last row in table 2 presents our official results
for the closed track using the AS corpus. The asd1 dictio-
nary contains only the words from the AS training corpus,
while the asd2 consists of the words in asd1 and the new
words automatically extracted from the AS testing texts us-
ing the new words recognition described in section 3. The
results show that new words recognition and joining single
characters contributed the most to the increase in precision,
while the consistency check contributed the most to the in-
crease in recall. Table 3 gives the results of the maximum
matching using only the words in the training data. While
the difference between the F-scores of the maximum match-
ing and the base algorithm is small for the PK corpus, the
F-score difference for the AS corpus is much larger. Our
base algorithm performed substantially better than the max-
imum matching for the AS corpus. The performances of our
base algorithm on the testing data using the words from the
training data are presented in row 1 in table 1 for the
corpus, and row 1 in table 2 for the corpus.
</bodyText>
<sectionHeader confidence="0.996867" genericHeader="method">
5 Discussions
</sectionHeader>
<bodyText confidence="0.974502666666667">
In this section we will examine in some details the problem
of segmentation inconsistencies within the training data,
and
</bodyText>
<sectionHeader confidence="0.997433" genericHeader="evaluation">
4 Results
</sectionHeader>
<bodyText confidence="0.999750230769231">
within the testing data, and between training data and test-
ing data. Due to space limit, we will only report our find-
ings in the PK corpus though the same kinds of inconsis-
tencies also occur in the AS corpus. We understand that
it is difficult, or even impossible, to completely eliminate
segmentation inconsistencies. However, perhaps we could
learn more about the impact of segmentation inconsisten-
cies on a system’s performance by taking a close look at the
problem.
We wrote a program that takes as input a segmented cor-
pus and prints out the shortest text fragments in the corpus
that have two or more segmentations. For each text frag-
ment, the program also prints out how the text fragment is
segmented, and how many times it is segmented in a partic-
ular way. While some of the text fragments, such as
and truly have two different segmentations, depend-
ing on the contexts in which they occur or the meanings
of the text fragments, others are segmented inconsistently.
We ran this program on the PK testing data and found 21
unique shortest text fragments, which occur 87 times in to-
tal, that have two different segmentations. Some of the text
fragments, such as are inconsistently segmented.
The fragment occurs twice in the testing data and
is segmented into / in one case, but treated as one
word in the other case. We found 1,500 unique shortest text
fragments in the PK training data that have two or more seg-
mentations, and 97 unique shortest text fragments that are
segmented differently in the training data and in the test-
ing data. For example, the text is treated as one
word in the training data, but is segmented into / /
/ in the testing data. We found 11,136 unique short-
est text fragments that have two or more segmentations in
the AS training data, 21 unique shortest text fragments that
have two or more segmentations in the AS testing data, and
38 unique shortest text fragments that have different seg-
mentations in the AS training data and in the AS testing
data.
Segmentation inconsistencies not only exists within
training and testing data, but also between training and test-
ing data. For example, the text fragment occurs 35
times in the PK training data and is consistently segmented
into ” / but the same text fragment, occurring
twice in the testing data, is segmented into / / in
both cases. The text occurs 67 times in the training
data and is treated as one word in all 67 cases, but
the same text, occurring 4 times in the testing data, is seg-
mented into / in all 4 cases. The text occurs
16 times in the training data, and is treated as one word in
all cases, but in the testing data, it is treated as one word in
three cases and segmented into / in one case. The
text is segmented into / in 8 cases, but treated
as one word in one case in the training data. A couple of
text fragments seem to be incorrectly segmented. The text
in the testing data is segmented into
/ and the text segmented into /
Our segmented texts of the PK testing data differ from
the reference segmented texts for 580 text fragments (427
unique). Out of these 580 text fragments, 126 text frag-
ments are among the shortest text fragments that have one
segmentation in the training data, but another in the test-
ing data. This implies that up to 21.7% of the mistakes
committed by our system may have been impacted by the
segmentation inconsistencies between the PK training data
and the PK testing data. Since there are only 38 unique
shortest text fragments found in the AS corpus that are seg-
mented differently in the training data and the testing data ,
the inconsistency problem probably had less impact on our
AS results. Out of the same 580 text fragments, 359 text
fragments (62%) are new words in the PK testing data. For
example, the proper name which is a new word,
is incorrectly segmented into / by our system. An-
other example is the new word which is treated
as one word in the testing data, but is segmented into /
/ /by our system. Some of the longer text frag-
ments that are incorrectly segmented may also involve new
words, so at least 62%, but under 80%, of the incorrectly
segmented text fragments are either new words or involve
new words.
</bodyText>
<sectionHeader confidence="0.999577" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999972375">
We have presented our word segmentation system and the
results for the closed track using the corpus and the
corpus. The new words recognition, combining single char-
acters, and checking consistencies contributed the most to
the increase in precision and recall over the performance of
the base segmentation algorithm, which works better than
maximum matching. For the closed track experiment using
the corpus, we found that 62% of the text fragments
that are incorrectly segmented by our system are actually
new words, which clearly shows that to further improve the
performance of our system, a better new words recognition
algorithm is necessary. Our failure analysis also indicates
that up to 21.7% of the mistakes made by our system for
the PK closed track may have been impacted by the seg-
mentation inconsistencies between the training and testing
data.
</bodyText>
<sectionHeader confidence="0.998734" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.91762675">
Richard Sproat and Tom Emerson. 2003. The First Interna-
tional Chinese Word Segmentation Bakeoff. In proceed-
ings of the Second SIGHAN Workshop on Chinese Lan-
guage Processing, July 11-12, 2003, Sapporo, Japan.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.898176">
<title confidence="0.998503">Chinese Word Segmentation Using Minimal Linguistic Knowledge</title>
<author confidence="0.929548">Aitao</author>
<affiliation confidence="0.9987755">School of Information Management and University of California at</affiliation>
<address confidence="0.977571">Berkeley, CA 94720,</address>
<email confidence="0.999412">aitao@sims.berkeley.edu</email>
<abstract confidence="0.999049555555556">This paper presents a primarily data-driven Chinese word segmentation system and its performances on the closed track using two corpora at the first international Chinese word segmentation bakeoff. The system consists of a new words recognizer, a base segmentation algorithm, and procedures for combining single characters, suffixes, and checking segmentation consistencies.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Richard Sproat</author>
<author>Tom Emerson</author>
</authors>
<title>The First International Chinese Word Segmentation Bakeoff.</title>
<date>2003</date>
<booktitle>In proceedings of the Second SIGHAN Workshop on Chinese Language Processing,</booktitle>
<location>Sapporo, Japan.</location>
<contexts>
<context position="1046" citStr="Sproat and Emerson, 2003" startWordPosition="154" endWordPosition="157">onsists of a new words recognizer, a base segmentation algorithm, and procedures for combining single characters, suffixes, and checking segmentation consistencies. 1 Introduction At the first Chinese word segmentation bakeoff, we participated in the closed track using the Academia Sinica corpus ( for short) and the Beijing University corpus ( for short). We will refer to the segmented texts in the training corpus as the training data, and to both the unsegmented testing texts and the segmented texts (the reference texts) as the testing data. For details on the word segmentation bakeoff, see (Sproat and Emerson, 2003). 2 Word segmentation New texts are segmented in four steps which are described in this section. New words are automatically extracted from the unsegmented testing texts and added to the base dictionary consisting of words from the training data before the testing texts are segmented, line by line. 2.1 Base segmentation algorithm Given a dictionary and a sentence, our base segmentation algorithm finds all possible segmentations of the sentence with respect to the dictionary, computes the probability of each segmentation, and chooses the segmentation with the highest probability. If a sentence </context>
<context position="12707" citStr="Sproat and Emerson, 2003" startWordPosition="2232" endWordPosition="2235">ds converted from pkd1 by changing the GB encoding to ASCII encoding for the numeric digits and the English letters, and pkd3 consists of the words in pkd2 and the words automatically extracted from the PK testing texts using the procedures described in section 3. The columns labeled R, P and F give the recall, precision, and F score, respectively. The columns labeled and show the recall on out-ofvocabulary words and the recall on in-vocabulary words, respectively. All evaluation scores reported in this paper are computed using the score program written by Richard Sproat. We refer readers to (Sproat and Emerson, 2003) for details on the evaluation measures. For example, row 4 in table 1 gives the results using pkd3 dictionary when a sentence is segmented by the base algorithm, and then the single characters in the initial segmentation are combined, but suffixes are not attached and consistency check is not performed. The last row in table 2 presents our official results for the closed track using the AS corpus. The asd1 dictionary contains only the words from the AS training corpus, while the asd2 consists of the words in asd1 and the new words automatically extracted from the AS testing texts using the ne</context>
</contexts>
<marker>Sproat, Emerson, 2003</marker>
<rawString>Richard Sproat and Tom Emerson. 2003. The First International Chinese Word Segmentation Bakeoff. In proceedings of the Second SIGHAN Workshop on Chinese Language Processing, July 11-12, 2003, Sapporo, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>