<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005200">
<title confidence="0.996868">
The Effects of Discourse Connectives Prediction on Implicit Discourse
Relation Recognition
</title>
<author confidence="0.998324">
Zhi Min Zhou†, Man Lan†�§, Zheng Yu Niu‡, Yu Xu†, Jian Su§
</author>
<affiliation confidence="0.989454333333333">
†East China Normal University, Shanghai, PRC.
‡Baidu.com Inc., Beijing, PRC.
§Institute for Infocomm Research, Singapore.
</affiliation>
<email confidence="0.997546">
51091201052@ecnu.cn, lanman.sg@gmail.com
</email>
<sectionHeader confidence="0.993853" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999926863636364">
Implicit discourse relation recognition is
difficult due to the absence of explicit
discourse connectives between arbitrary
spans of text. In this paper, we use lan-
guage models to predict the discourse con-
nectives between the arguments pair. We
present two methods to apply the pre-
dicted connectives to implicit discourse
relation recognition. One is to use the
sense frequency of the specific connec-
tives in a supervised framework. The
other is to directly use the presence of the
predicted connectives in an unsupervised
way. Results on PDTB2 show that using
language model to predict the connectives
can achieve comparable F-scores to the
previous state-of-art method. Our method
is quite promising in that not only it has
a very small number of features but also
once a language model based on other re-
sources is trained it can be more adaptive
to other languages and domains.
</bodyText>
<sectionHeader confidence="0.999135" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999816392857143">
Discourse relation analysis involves identifying
the discourse relations (e.g., the comparison re-
lation) between arbitrary spans of text, where
the discourse connectives (e.g., “however”, “be-
cause”) may or may not explicitly exist in the text.
This analysis is one important application both as
an end in itself and as an intermediate step in var-
ious downstream NLP applications, such as text
summarization, question answering etc.
As discussed in (Pitler and Nenkova., 2009b),
although explicit discourse connectives may have
two types of ambiguity, i.e., one is discourse or
non-discourse usage (“once” can be either a tem-
poral connective or a word meaning “formerly”),
the other is discourse relation sense ambiguity
(“since” can serve as either a temporal or causal
connective), their study shows that for explicit
discourse relations in Penn Discourse Treebank
(PDTB) corpus, the most general 4 senses, i.e.,
Comparison (Comp.), Contingency (Cont.), Tem-
poral (Temp.) and Expansion (Exp.), can be eas-
ily addressed by the presence of discourse con-
nectives and a simple method only considering the
sense frequency of connectives can achieve more
than 93% accuracy. This indicates the importance
of connectives for discourse relation recognition.
However, with implicit discourse relation
recognition, there is no connective between the
textual arguments, which results in a very difficult
task. In recent years, a multitude of efforts have
been employed to solve this task. One approach
is to exploit various linguistically informed fea-
tures extracted from human-annotated corpora in
a supervised framework (Pitler et al., 2009a) and
(Lin et al., 2009). Another approach is to perform
recognition without human-annotated corpora by
creating synthetic examples of implicit relations in
an unsupervised way (Marcu and Echihabi, 2002).
Moreover, our initial study on PDTB implicit
relation data shows that the averaged F-score for
the most general 4 senses can reach 91.8% when
we obtain the sense of test examples by map-
ping each implicit connective to its most frequent
sense (i.e., sense recognition using gold-truth im-
plicit connectives). This high F-score performance
again proves that the connectives are very crucial
source for implicit relation recognition.
In this paper, we present a new method to ad-
dress the problem of recognizing implicit dis-
course relation. This method is inspired by the
above observations, especially the two gold-truth
results, which reveals that discourse connectives
are very important signals for discourse relation
recognition. Our basic idea is to recover the im-
plicit connectives (not present in real text) be-
tween two spans of text with the use of a language
</bodyText>
<subsubsectionHeader confidence="0.6776">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 139–146,
</subsubsectionHeader>
<affiliation confidence="0.891053">
The University of Tokyo, September 24-25, 2010. c�2010 Association for Computational Linguistics
</affiliation>
<page confidence="0.999691">
139
</page>
<bodyText confidence="0.99995225">
model trained on large amount of raw data without
any human-annotation. Then we use these pre-
dicted connectives to generate feature vectors in
two ways for implicit discourse relation recogni-
tion. One is to use the sense frequency of the spe-
cific connectives in a supervised framework. The
other is to directly use the presence of the pre-
dicted connectives in an unsupervised way.
We performed evaluation on explicit and im-
plicit relation data sets in the PDTB 2 corpus. Ex-
perimental results showed that the two methods
achieved comparable F-scores to the state-of-art
methods. It indicates that the method using lan-
guage model to predict connectives is very useful
in solving this task.
The rest of this paper is organized as follows.
Section 2 reviews related work. Section 3 de-
scribes our methods for implicit discourse relation
recognition. Section 4 presents experiments and
results. Section 5 offers some conclusions.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.999971264705883">
Existing works on automatic recognition of im-
plicit discourse relations fall into two categories
according to whether the method is supervised or
unsupervised.
Some works perform relation recognition with
supervised methods on human-annotated corpora,
for example, the RST Bank (Carlson et al., 2001)
used by (Soricut and Marcu, 2003), adhoc anno-
tations used by (Girju, 2003) and (Baldridge and
Lascarides, 2005), and the GraphBank (Wolf et al.,
2005) used by (Wellner et al., 2006).
Recently the release of the Penn Discourse
TreeBank (PDTB) (Prasad et al., 2006) has sig-
nificantly expanded the discourse-annotated cor-
pora available to researchers, using a comprehen-
sive scheme for both implicit and explicit rela-
tions. (Pitler et al., 2009a) performed implicit re-
lation classification on the second version of the
PDTB. They used several linguistically informed
features, such as word polarity, verb classes, and
word pairs, showing performance increases over a
random classification baseline. (Lin et al., 2009)
presented an implicit discourse relation classifier
in PDTB with the use of contextual relations, con-
stituent Parse Features, dependency parse features
and cross-argument word pairs. Although both of
two methods achieved the state of the art perfor-
mance for automatical recognition of implicit dis-
course relations, due to lack of human-annotated
corpora, their approaches are not very useful in the
real word.
Another line of research is to use the unsuper-
vised methods on unhuman-annotated corpus.
(Marcu and Echihabi, 2002) used several pat-
terns to extract instances of discourse relations
such as contrast and elaboration from unlabeled
corpora. Then they used word-pairs between argu-
ments as features for building classification mod-
els and tested their model on artificial data for im-
plicit relations.
Subsequently other studies attempt to ex-
tend the work of (Marcu and Echihabi, 2002).
(Sporleder and Lascarides, 2008) discovered that
Marcu and Echihabi’s models do not perform as
well on implicit relations as one might expect
from the test accuracy on synthetic data. (Gold-
ensohn, 2007) extended the work of (Marcu and
Echihabi, 2002) by refining the training and clas-
sification process using parameter optimization,
topic segmentation and syntactic parsing. (Saito
et al., 2006) followed the method of (Marcu and
Echihabi, 2002) and conducted experiments with
a combination of cross-argument word pairs and
phrasal patterns as features to recognize implicit
relations between adjacent sentences in a Japanese
corpus.
Previous work showed that with the use of some
patterns, structures, or the pairs of words, rela-
tion classification can be performed using unsu-
pervised methods.
In contrast to existing work, we investigated a
new knowledge source, i.e., implicit connectives
predicted using a language model, for implicit re-
lation recognition. Moreover, this method can
be applied in both supervised and unsupervised
ways by generating features on labeled and unla-
beled training data and then performing implicit
discourse connectives recognition.
</bodyText>
<sectionHeader confidence="0.999675" genericHeader="method">
3 Methodology
</sectionHeader>
<subsectionHeader confidence="0.984318">
3.1 Predicting implicit connectives via a
language model
</subsectionHeader>
<bodyText confidence="0.99984325">
Previous work (Pitler and Nenkova., 2009b)
showed that with the presence of discourse con-
nectives, explicit discourse relations in PDTB can
be easily identified with more than 90% F-score.
Our initial study on PDTB human-annotated im-
plicit relation data shows that the averaged F-score
for the most general 4 senses can reach 91.8%
when we simply map each implicit connective to
</bodyText>
<page confidence="0.981321">
140
</page>
<bodyText confidence="0.99998845">
its most frequent sense. These high F-scores indi-
cate that the connectives are very crucial source of
information for both explicit and implicit relation
recognition. However, for implicit relations, there
are no explicitly discourse connectives in real text.
This built-in absence makes the implicit relation
recognition task quite difficult. In this work we
overcome this difficulty by inserting connectives
into the two arguments with the use of a language
model.
Following the annotation scheme of PDTB, we
assume that each implicit connective takes two
arguments, denoted as Arg1 and Arg2. Typi-
cally, there are two possible positions for most
of implicit connectives, i.e., the position before
Arg1 and the position between Arg1 and Arg2.
Given a set of implicit connectives {ci}, we gen-
erate two synthetic sentences, ci+Arg1+Arg2 and
Arg1+ci+Arg2 for each ci, denoted as Sci,1 and
Sci,2. Then we calculate the perplexity (an intrin-
sic score) of these sentences with the use of a lan-
guage model, denoted as Ppl(Sci,j). According to
the value of Ppl(Sci,j) (the lower the better), we
can rank these sentences and select the connec-
tives in top N sentences as implicit connectives
for this argument pair. Here the language model
may be trained on any large amount of unanno-
tated corpora that can be cheaply acquired. Typi-
cally, a large corpora with the same domain as the
test data will be used for training language model.
Therefore, we chose news corpora, such as North
American News Corpora.
After that, we use the top N predicted connec-
tives to generate different feature vectors and per-
form the classification in two ways. One is to use
the sense frequency of predicted connectives in a
supervised framework. The other is to directly use
the presence of the predicted connectives in an un-
supervised way. The two approaches are described
as follows.
</bodyText>
<subsectionHeader confidence="0.962638">
3.2 Using sense frequency of predicted
</subsectionHeader>
<bodyText confidence="0.980037619047619">
discourse connectives as features
After the above procedure, we get a sorted set of
predicted discourse connectives. Due to the pres-
ence of an implicit connective, the implicit dis-
course relation recognition task can be addressed
with the methods for explicit relation recognition,
e.g., sense classification based only on connectives
(Pitler et al., 2009a). Inspired by their work, the
first approach is to use sense frequency of pre-
dicted discourse connectives as features. We take
the connective with the lowest perplexity value
(i.e., top 1 connective) as the real connective for
the arguments pair. Then we count the sense
frequency of this connective on the training set.
Figure 1 illustrates the procedure of generating
predicted discourse connective from a language
model and calculating its sense frequency from
training data. Here the calculation of sense fre-
quency of connective is based on the annotated
training data which has labeled discourse rela-
tions, thus this method is a supervised one.
</bodyText>
<figureCaption confidence="0.716499">
Figure 1: Procedure of generating a predicted dis-
</figureCaption>
<bodyText confidence="0.963963285714286">
course connective and its sense frequency from the
training set and a language model.
Then we can directly use the sense frequency
to generate a 4-feature vector to perform the clas-
sification. For example, the sense frequency of
the connective but in the most general 4 senses
can be counted from training set as 691, 6, 49,
2, respectively. For a given pair of arguments,
if but is predicted as the top 1 connective based
on a language model, a 4-dimension feature vec-
tor (691, 6, 49, 2) is generated for this pair and
used for training and test procedure. Figure 2
and 3 show the training and test procedure for this
method.
</bodyText>
<figureCaption confidence="0.9423095">
Figure 2: Training procedure for the first ap-
proach.
</figureCaption>
<page confidence="0.971831">
141
</page>
<figureCaption confidence="0.999283">
Figure 3: Test procedure for the first approach.
</figureCaption>
<bodyText confidence="0.910504714285714">
3.3 Using presence or absence of predicted
discourse connective as features
(Pitler et al., 2008) showed that most connectives
are unambiguous and it is possible to obtain high-
accuracy in prediction of discourse senses due to
the simple mapping relation between connectives
and senses. Given two examples:
</bodyText>
<listItem confidence="0.547193">
(E1) She paid less on her dress, but it is very nice.
(E2) We have to harry up because the raining is
getting heavier and heavier.
</listItem>
<bodyText confidence="0.999898181818182">
The two connectives, i.e., but in E1 and because
in E2, convey the Comparison and Contingency
senses respectively. In most cases, we can easily
recognize the relation sense by the appearance of
a discourse connective since it can be interpreted
in only one way. That means the ambiguity of
the mapping between sense and connective is quite
low. Therefore, the second approach is to use only
the presence of the top N predicted discourse con-
nectives to generate a feature vector for a given
pair of arguments.
</bodyText>
<sectionHeader confidence="0.99894" genericHeader="method">
4 Experiment
</sectionHeader>
<subsectionHeader confidence="0.98208">
4.1 Data sets
</subsectionHeader>
<bodyText confidence="0.9999816">
We used PDTB as our data set to perform the eval-
uation of our methods. The corpus contains anno-
tations of explicit and implicit discourse relations.
The first evaluation is performed on the annotated
implicit data set. Following the work of (Pitler et
al., 2009a), we used sections 2-20 as the training
set, sections 21-22 as the test set and sections 0-
1 as the development set for parameter optimiza-
tion (e.g., N value). The second evaluation is per-
formed on the annotated explicit data set. We fol-
low the method used in (Sporleder and Lascarides,
2008) to remove the discourse connective from the
explicit instances and consider these processed in-
stances as implicit ones.
We constructed four binary classifiers to recog-
nize each main senses (i.e., Cont., Cont., Exp.,
Temp.) from the rest. For each sense we used
equal numbers of positive and negative instances
in training set. The negative instances were cho-
sen at random from the rest of training set. For
both evaluations all instances in sections 21-22
were used as test set. Table 1 lists the numbers
of positive and negative instances for each sense
in training, development and test sets of implicit
and explicit relation data sets.
</bodyText>
<subsectionHeader confidence="0.982571">
4.2 Evaluation and classifier
</subsectionHeader>
<bodyText confidence="0.9999508">
To evaluate the performance of above systems, we
used two widely-used measures, F-score ( i.e., Fi)
and accuracy. In addition, in this work we used
the LIBSVM toolkit to construct four linear SVM
classifiers for each sense.
</bodyText>
<subsectionHeader confidence="0.998222">
4.3 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999971185185185">
We used the SRILM toolkit to build a language
model and calculated the perplexity value for each
training and test sample. The steps are described
as follows. First, since perplexity is an intrin-
sic score to measure the similarity between train-
ing and test samples, in order to fit the restric-
tion of perplexity we chose 3 widely-used cor-
pora in the Newswire domain to train the language
model, i.e., (1) the New York part of BLLIP North
American News Text (Complete), (2) the Xin and
(3) the Ltw parts of the English Gigaword Fourth
Edition. For the BLLIP corpus with 1,796,386
automatically parsed English sentences, we con-
verted the parsed sentences into original textual
data. Some punctuation marks such as commas,
periods, minuses, right/left parentheses are con-
verted into their original form. For the Xin and
Ltw parts, we only used the Sentence Detector
toolkit in OpenNLP to split each sentence. Finally
we constructed 3-, 4- and 5-grams language mod-
els from these three corpora. Table 2 lists statis-
tics of different n-grams in the different language
models and different corpora.
Next, for each instance we combined its Arg1
and Arg2 with connectives obtained from PDTB.
There are two types of connectives, single con-
nectives (e.g. “because” and “but”) and paral-
</bodyText>
<page confidence="0.998944">
142
</page>
<tableCaption confidence="0.989191">
Table 1: Statistics of positive and negative instances for each sense in training, development and test sets
of implicit and explicit relation data sets.
</tableCaption>
<table confidence="0.9989284">
Implicit Explicit
Comp. Cont. Exp. Temp. Comp. Cont. Exp. Temp.
Train(Pos/Neg) 1927/1927 3375/3375 6052/6052 730/730 4080/4080 2732/2732 4609/4609 2663/2663
Dev(Pos/Neg) 191/997 292/896 651/537 54/1134 438/1071 295/1214 514/995 262/1247
Test(Pos/Neg) 146/912 276/782 556/502 67/991 388/1025 235/1178 501/912 289/1124
</table>
<tableCaption confidence="0.9762865">
Table 2: Statistics of different n-grams in the dif-
ferent language models and different corpora.
</tableCaption>
<table confidence="0.998153714285714">
n-gram BLLIP - Gigaword- Gigaword-
New York Xin Ltw
1-gram 1638156 2068538 2276491
2-grams 26156851 23961796 33504873
3-grams 80876435 77799100 101855639
4-grams 127142452 134410879 159791916
5-grams 146454530 168166195 183794771
</table>
<bodyText confidence="0.991655407407408">
lel connectives (such as “not only ..., but also”).
Since discourse connectives may appear not only
ahead of the Arg1, but also between Arg1 and
Arg2, we considered this case. Given a set of
possible implicit connectives {cil, for a single
connective ci, we constructed two synthetic sen-
tences, ci+Arg1+Arg2 and Arg1+ci+Arg2. In case
of parallel connectives, we constructed one syn-
thetic sentence like ci,1+Arg1+ci,2+Arg2.
As a result, we obtain 198 synthetic sentences
(|ci |* 2 for single connective or |ci |for parallel
connective) for each pair of arguments. Then we
converted all words to lower cases and used the
language model trained in the above step to calcu-
late its perplexity (the lower the better) value on
sentence level. The sentences were ranked from
low to high according to their perplexity scores.
For example, given a sentence with arguments pair
as follows:
Arg1: it increased its loan-loss reserves by $93
million after reviewing its loan portfolio,
Arg2: before the loan-loss addition it had operat-
ing profit of $10 million for the quarter.
we got the perplexity (Ppl) values for this argu-
ments pair in combination with two connectives
(but and by comparison) in two positions as fol-
lows:
</bodyText>
<listItem confidence="0.9964855">
1. but + Arg1 + Arg2: Ppl= 349.622
2. Arg1 + but + Arg2: Ppl= 399.339
3. by comparison + Arg1 + Arg2: Ppl= 472.206
4. Arg1 + by comparison + Arg2: Ppl= 543.051
</listItem>
<bodyText confidence="0.999904">
In our second approach described in Section
3.3, we considered the combination of connectives
and their position as final features like mid but,
first but, where the features are binary, that is,
the presence or absence of the specific connective.
According to the value of Ppl(S,,,�), we tried var-
ious N values on development set to get the opti-
mal N value.
</bodyText>
<subsectionHeader confidence="0.799867">
4.4 Results
</subsectionHeader>
<bodyText confidence="0.999952529411765">
Table 3 summarizes the best performance
achieved using gold-truth implicit connectives,
the previous state-of-art performance achieved
by (Pitler et al., 2009a) and our approaches.
The first line shows the result by mapping the
gold-truth implicit connectives directly to the
relation’s sense. The second line presents the best
result of (Pitler et al., 2009a). One thing worth
mentioning here is that for the Expansion relation,
(Pitler et al., 2009a) expanded both training and
test sets by including EntRel relation as positive
examples, which makes it impossible to perform
direct comparison. The third and fourth lines
show the best results using our first approach,
where the sense frequency is counted on explicit
and implicit training set respectively. The last line
shows the best result of our second approach only
considering the presence of top N connectives.
Table 4 summarizes the best performance using
gold-truth explicit connectives reported in (Pitler
and Nenkova., 2009b) and our two approaches.
Figure 4 shows the curves of averaged F-scores
on implicit connective classification with differ-
ent n-gram language models. From this figure we
can see that all 4-grams language models achieved
around 0.5% better averaged F-score than 3-grams
models. And except for Ltw corpus, other 5-grams
models achieved lower averaged F-score than 4-
grams models. Specially the 5-grams result of
New York corpus is much lower than its 3-grams
result.
Figure 5 shows the averaged F-scores of dif-
ferent top N on the New York corpus with 3-,
4- and 5-grams language models. The essential
</bodyText>
<page confidence="0.999568">
143
</page>
<tableCaption confidence="0.995496">
Table 3: Best result of implicit relations compared with state-of-art methods.
</tableCaption>
<table confidence="0.99849375">
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Averaged
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07) 91.78(98.02)
gold-truth implicit connectives
Best result in (Pitler et al., 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49) 40.57(62.75)
Use sense frequency in explicit training set 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97) 35.10(49.95)
Use sense frequency in implicit training set 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51) 29.07(64.70)
Use presence of top N connectives only 21.91(52.84) 39.53(50.85) 68.84(52.93) 11.91(6.33) 35.55(40.74)
</table>
<tableCaption confidence="0.9840225">
Table 4: Best result of explicit relation conversion to implicit relation compared with results using the
same method.
</tableCaption>
<table confidence="0.974240625">
System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Average
F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc)
Sense recognition using gold-truth N/A N/A N/A N/A N/A(93.67)
explicit connectives in (Pitler et al., 2009a)
Use sense frequency in explicit training set 41.62(50.96) 27.46(59.24) 48.44(50.88) 35.14(54.28) 38.17(53.84)
Use presence of top N connectives only 42.92(55.77) 31.83(56.05) 47.26(55.77) 37.89(58.24) 39.98(56.46)
Top� value
Averaged F-Score
</table>
<figureCaption confidence="0.92127">
Figure 5: Curves of averages F-score on New York 3-, 4- and 5-grams language models with different
top N values.
</figureCaption>
<bodyText confidence="0.998907166666667">
trend of these curves cannot be summarized in
one sentence. But we can see that the best aver-
aged F-scores mostly appeared in the range from
100 − 160. For 4-grams and 5-grams models, the
system achieved the top averaged F-scores when
N = 20 as well.
</bodyText>
<subsectionHeader confidence="0.73422">
4.5 Discussion
</subsectionHeader>
<bodyText confidence="0.999964894736842">
Experimental results on PDTB showed that using
predicted connectives achieved the comparable F-
scores of the state-of-art method.
From Table 3 we can find that our results are
closely to the best performance of previous state-
of-art methods in terms of averaged F-score. On
the Comparison sense, our first approach has an
improvement of more than 4% F-score on the pre-
vious state-of-art method (Pitler et al., 2009a). As
we mentioned before, for the Expansion sense,
they included EntRel relation to expand the train-
ing set and test set, which makes it impossible to
perform a direct comparison. Since the positive in-
stances size has been increased by 50%, they may
achieve a higher F-score than our approach. For
other relations, our best performance is slightly
lower than theirs. While bearing in mind that our
approach only uses a very small amount of fea-
tures for implicit relation recognition. Compared
</bodyText>
<page confidence="0.99183">
144
</page>
<figure confidence="0.778014">
Averaged F-score
</figure>
<figureCaption confidence="0.920283666666667">
Figure 4: Curves of averaged F-score on implicit
connective classification with n-Gram language
model.
</figureCaption>
<bodyText confidence="0.99861124137931">
with other approaches involving thousands of fea-
tures, our method is quite promising.
From Table 4 we observe comparable averaged
F-score (39.98% F-score) on explicit relation data
set to that on implicit relation data set. Previ-
ously, (Sporleder and Lascarides, 2008) also used
the same conversion method to perform implicit
relation recognition on different corpora and their
best result is around 33.69% F-score. Although
the two results cannot be compared directly due to
different data sets, the magnitude of performance
quantities is comparable and reliable.
By comparing with the above different systems,
we find several useful observations. First, our
method using predicted implicit connectives via a
language model can help the task of implicit dis-
course relation recognition. The results are com-
parable to the previous state-of-art studies. Sec-
ond, our method has a lot of advantages, i.e., a
very small amount of features (several or no more
than 200 vs. ten thousand), easy computation
(only based on the trained language model vs. us-
ing a lot of NLP tools to extract a large amount of
linguistically informed features) and fast running,
which makes it more practical in real world appli-
cation. Furthermore, since the language model can
be trained on many corpora whether annotated or
unannotated, this method is more adaptive to other
languages and domains.
</bodyText>
<sectionHeader confidence="0.999633" genericHeader="conclusions">
5 Conclusions
</sectionHeader>
<bodyText confidence="0.999518692307692">
In this paper we have presented an approach to
implicit discourse relation recognition using pre-
dicted implicit connectives via a language model.
The predicted connectives have been used for im-
plicit relation recognition in two ways, i.e., super-
vised and unsupervised framework. Results on the
Penn Discourse Treebank 2.0 show that the pre-
dicted discourse connectives can help implicit re-
lation recognition and the two algorithms achieve
comparable F-scores with the state-of-art method.
In addition, this method is quite promising due to
its simple, easy to retrieve, fast run and increased
adaptivity to other languages and domains.
</bodyText>
<sectionHeader confidence="0.997386" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.996619">
We thank the reviewers for their helpful com-
ments and Jonathan Ginzburg for his mentor-
ing. This work is supported by grants from
National Natural Science Foundation of China
(No.60903093), Shanghai Pujiang Talent Program
(No.09PJ1404500) and Doctoral Fund of Ministry
of Education of China (No.20090076120029).
</bodyText>
<sectionHeader confidence="0.99858" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999681413793103">
J. Baldridge and A. Lascarides. 2005. Probabilistic
head-driven parsing for discourse structure. Pro-
ceedings of the Ninth Conference on Computational
Natural Language Learning.
L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001.
Building a discourse-tagged corpus in the frame-
work of rhetorical structure theory. Proceedings of
the Second SIG dial Workshop on Discourse and Di-
alogue.
B. Dorr. LCS Verb Database. Technical Report Online
Software Database, University of Maryland, College
Park, MD,2001.
R. Girju. 2003. Automatic detection of causal relations
for question answering. In ACL 2003 Workshops.
S. Blair-Goldensohn. 2007. Long-Answer Ques-
tion Answering and Rhetorical-Semantic Relations.
Ph.D. thesis, Columbia Unviersity.
M. Lapata and A. Lascarides. 2004. Inferring
Sentence-internal Temporal Relations. Proceedings
of the North American Chapter of the Assocation of
Computational Linguistics.
Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing
Implicit Discourse Relations in the Penn Discourse
Treebank. Proceedings of the 2009 Conference on
Empirical Methods in Natural Language Processing.
D. Marcu and A. Echihabi. 2002. An Unsupervised
Approach to Recognizing Discourse Relations. Pro-
ceedings of the 40th Annual Meeting of the Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.986353">
145
</page>
<reference confidence="0.999896214285714">
E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A.
Lee, A. Joshi. 2008. Easily Identifiable Dis-
course Relations. Coling 2008: Companion vol-
ume: Posters.
E. Pitler, A. Louis, A. Nenkova. 2009. Automatic
sense prediction for implicit discourse relations in
text. Proceedings of the 47th Annual Meeting of the
Association for Computational Linguistics.
E. Pitler and A. Nenkova. 2009. Using Syntax to Dis-
ambiguate Explicit Discourse Connectives in Text.
Proceedings of the ACL-IJCNLP 2009 Conference
Short Papers.
M. Porter. An algorithm for suffix stripping. In Pro-
gram, vol. 14, no. 3, pp.130-137, 1980.
R. Prasad, N. Dinesh, A. Lee, A. Joshi, B. Webber.
2006. Annotating attribution in the Penn Discourse
TreeBank. Proceedings of the COLING/ACL Work-
shop on Sentiment and Subjectivity in Text.
R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L.
Robaldo, A. Joshi, B. Webber. 2008. The Penn Dis-
course TreeBank 2.0. Proceedings of LREC’08.
M. Saito, K.Yamamoto, S.Sekine. 2006. Using
Phrasal Patterns to Identify Discourse Relations.
Proceeding of the HLTCNA Chapter of the ACL.
R. Soricut and D. Marcu. 2003. Sentence Level Dis-
course Parsing using Syntactic and Lexical Informa-
tion. Proceedings of the Human Language Technol-
ogy and North American Association for Computa-
tional Linguistics Conference.
C. Sporleder and A. Lascarides. 2008. Using automat-
ically labelled examples to classify rhetorical rela-
tions: an assessment. Natural Language Engineer-
ing, Volume 14, Issue 03.
B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky.
2006. Classification of discourse coherence rela-
tions: An exploratory study using multiple knowl-
edge sources. Proceedings of the 7th SIGDIAL
Workshop on Discourse and Dialogue.
F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005.
The Discourse GraphBank: A database of texts an-
notated with coherence relations. Linguistic Data
Consortium.
</reference>
<page confidence="0.998817">
146
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.331997">
<title confidence="0.999176">The Effects of Discourse Connectives Prediction on Implicit Relation Recognition</title>
<author confidence="0.999924">Min Man Zheng Yu Yu Jian</author>
<affiliation confidence="0.734672333333333">China Normal University, Shanghai, Inc., Beijing, for Infocomm Research,</affiliation>
<email confidence="0.836222">51091201052@ecnu.cn,lanman.sg@gmail.com</email>
<abstract confidence="0.998805956521739">Implicit discourse relation recognition is difficult due to the absence of explicit discourse connectives between arbitrary spans of text. In this paper, we use language models to predict the discourse connectives between the arguments pair. We present two methods to apply the predicted connectives to implicit discourse relation recognition. One is to use the sense frequency of the specific connectives in a supervised framework. The other is to directly use the presence of the predicted connectives in an unsupervised way. Results on PDTB2 show that using language model to predict the connectives can achieve comparable F-scores to the previous state-of-art method. Our method is quite promising in that not only it has a very small number of features but also once a language model based on other resources is trained it can be more adaptive to other languages and domains.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J Baldridge</author>
<author>A Lascarides</author>
</authors>
<title>Probabilistic head-driven parsing for discourse structure.</title>
<date>2005</date>
<booktitle>Proceedings of the Ninth Conference on Computational Natural Language Learning.</booktitle>
<contexts>
<context position="5494" citStr="Baldridge and Lascarides, 2005" startWordPosition="843" endWordPosition="846">zed as follows. Section 2 reviews related work. Section 3 describes our methods for implicit discourse relation recognition. Section 4 presents experiments and results. Section 5 offers some conclusions. 2 Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 200</context>
</contexts>
<marker>Baldridge, Lascarides, 2005</marker>
<rawString>J. Baldridge and A. Lascarides. 2005. Probabilistic head-driven parsing for discourse structure. Proceedings of the Ninth Conference on Computational Natural Language Learning.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Okurowski</author>
</authors>
<title>Building a discourse-tagged corpus in the framework of rhetorical structure theory.</title>
<date>2001</date>
<booktitle>Proceedings of the Second SIG dial Workshop on Discourse and Dialogue.</booktitle>
<marker>Okurowski, 2001</marker>
<rawString>L. Carlson, D. Marcu, and Ma. E. Okurowski. 2001. Building a discourse-tagged corpus in the framework of rhetorical structure theory. Proceedings of the Second SIG dial Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Dorr</author>
</authors>
<title>LCS Verb Database.</title>
<date>2001</date>
<tech>Technical Report</tech>
<institution>Online Software Database, University of Maryland, College Park,</institution>
<marker>Dorr, 2001</marker>
<rawString>B. Dorr. LCS Verb Database. Technical Report Online Software Database, University of Maryland, College Park, MD,2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Girju</author>
</authors>
<title>Automatic detection of causal relations for question answering.</title>
<date>2003</date>
<booktitle>In ACL</booktitle>
<note>Workshops.</note>
<contexts>
<context position="5457" citStr="Girju, 2003" startWordPosition="840" endWordPosition="841">is paper is organized as follows. Section 2 reviews related work. Section 3 describes our methods for implicit discourse relation recognition. Section 4 presents experiments and results. Section 5 offers some conclusions. 2 Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random clas</context>
</contexts>
<marker>Girju, 2003</marker>
<rawString>R. Girju. 2003. Automatic detection of causal relations for question answering. In ACL 2003 Workshops.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Blair-Goldensohn</author>
</authors>
<title>Long-Answer Question Answering and Rhetorical-Semantic Relations.</title>
<date>2007</date>
<tech>Ph.D. thesis,</tech>
<institution>Columbia Unviersity.</institution>
<marker>Blair-Goldensohn, 2007</marker>
<rawString>S. Blair-Goldensohn. 2007. Long-Answer Question Answering and Rhetorical-Semantic Relations. Ph.D. thesis, Columbia Unviersity.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Lapata</author>
<author>A Lascarides</author>
</authors>
<title>Inferring Sentence-internal Temporal Relations.</title>
<date>2004</date>
<booktitle>Proceedings of the North American Chapter of the Assocation of Computational Linguistics.</booktitle>
<marker>Lapata, Lascarides, 2004</marker>
<rawString>M. Lapata and A. Lascarides. 2004. Inferring Sentence-internal Temporal Relations. Proceedings of the North American Chapter of the Assocation of Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Z H Lin</author>
<author>M Y Kan</author>
<author>H T Ng</author>
</authors>
<title>Recognizing Implicit Discourse Relations in the Penn Discourse Treebank.</title>
<date>2009</date>
<booktitle>Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</booktitle>
<contexts>
<context position="2872" citStr="Lin et al., 2009" startWordPosition="434" endWordPosition="437">discourse connectives and a simple method only considering the sense frequency of connectives can achieve more than 93% accuracy. This indicates the importance of connectives for discourse relation recognition. However, with implicit discourse relation recognition, there is no connective between the textual arguments, which results in a very difficult task. In recent years, a multitude of efforts have been employed to solve this task. One approach is to exploit various linguistically informed features extracted from human-annotated corpora in a supervised framework (Pitler et al., 2009a) and (Lin et al., 2009). Another approach is to perform recognition without human-annotated corpora by creating synthetic examples of implicit relations in an unsupervised way (Marcu and Echihabi, 2002). Moreover, our initial study on PDTB implicit relation data shows that the averaged F-score for the most general 4 senses can reach 91.8% when we obtain the sense of test examples by mapping each implicit connective to its most frequent sense (i.e., sense recognition using gold-truth implicit connectives). This high F-score performance again proves that the connectives are very crucial source for implicit relation re</context>
<context position="6096" citStr="Lin et al., 2009" startWordPosition="935" endWordPosition="938">scarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 2009) presented an implicit discourse relation classifier in PDTB with the use of contextual relations, constituent Parse Features, dependency parse features and cross-argument word pairs. Although both of two methods achieved the state of the art performance for automatical recognition of implicit discourse relations, due to lack of human-annotated corpora, their approaches are not very useful in the real word. Another line of research is to use the unsupervised methods on unhuman-annotated corpus. (Marcu and Echihabi, 2002) used several patterns to extract instances of discourse relations such as</context>
</contexts>
<marker>Lin, Kan, Ng, 2009</marker>
<rawString>Z.H. Lin, M.Y. Kan and H.T. Ng. 2009. Recognizing Implicit Discourse Relations in the Penn Discourse Treebank. Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Marcu</author>
<author>A Echihabi</author>
</authors>
<title>An Unsupervised Approach to Recognizing Discourse Relations.</title>
<date>2002</date>
<booktitle>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="3051" citStr="Marcu and Echihabi, 2002" startWordPosition="458" endWordPosition="461">ves for discourse relation recognition. However, with implicit discourse relation recognition, there is no connective between the textual arguments, which results in a very difficult task. In recent years, a multitude of efforts have been employed to solve this task. One approach is to exploit various linguistically informed features extracted from human-annotated corpora in a supervised framework (Pitler et al., 2009a) and (Lin et al., 2009). Another approach is to perform recognition without human-annotated corpora by creating synthetic examples of implicit relations in an unsupervised way (Marcu and Echihabi, 2002). Moreover, our initial study on PDTB implicit relation data shows that the averaged F-score for the most general 4 senses can reach 91.8% when we obtain the sense of test examples by mapping each implicit connective to its most frequent sense (i.e., sense recognition using gold-truth implicit connectives). This high F-score performance again proves that the connectives are very crucial source for implicit relation recognition. In this paper, we present a new method to address the problem of recognizing implicit discourse relation. This method is inspired by the above observations, especially </context>
<context position="6622" citStr="Marcu and Echihabi, 2002" startWordPosition="1015" endWordPosition="1018">word pairs, showing performance increases over a random classification baseline. (Lin et al., 2009) presented an implicit discourse relation classifier in PDTB with the use of contextual relations, constituent Parse Features, dependency parse features and cross-argument word pairs. Although both of two methods achieved the state of the art performance for automatical recognition of implicit discourse relations, due to lack of human-annotated corpora, their approaches are not very useful in the real word. Another line of research is to use the unsupervised methods on unhuman-annotated corpus. (Marcu and Echihabi, 2002) used several patterns to extract instances of discourse relations such as contrast and elaboration from unlabeled corpora. Then they used word-pairs between arguments as features for building classification models and tested their model on artificial data for implicit relations. Subsequently other studies attempt to extend the work of (Marcu and Echihabi, 2002). (Sporleder and Lascarides, 2008) discovered that Marcu and Echihabi’s models do not perform as well on implicit relations as one might expect from the test accuracy on synthetic data. (Goldensohn, 2007) extended the work of (Marcu and</context>
</contexts>
<marker>Marcu, Echihabi, 2002</marker>
<rawString>D. Marcu and A. Echihabi. 2002. An Unsupervised Approach to Recognizing Discourse Relations. Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>M Raghupathy</author>
<author>H Mehta</author>
<author>A Nenkova</author>
<author>A Lee</author>
<author>A Joshi</author>
</authors>
<title>Easily Identifiable Discourse Relations. Coling 2008: Companion volume:</title>
<date>2008</date>
<publisher>Posters.</publisher>
<contexts>
<context position="12388" citStr="Pitler et al., 2008" startWordPosition="1939" endWordPosition="1942">sense frequency of the connective but in the most general 4 senses can be counted from training set as 691, 6, 49, 2, respectively. For a given pair of arguments, if but is predicted as the top 1 connective based on a language model, a 4-dimension feature vector (691, 6, 49, 2) is generated for this pair and used for training and test procedure. Figure 2 and 3 show the training and test procedure for this method. Figure 2: Training procedure for the first approach. 141 Figure 3: Test procedure for the first approach. 3.3 Using presence or absence of predicted discourse connective as features (Pitler et al., 2008) showed that most connectives are unambiguous and it is possible to obtain highaccuracy in prediction of discourse senses due to the simple mapping relation between connectives and senses. Given two examples: (E1) She paid less on her dress, but it is very nice. (E2) We have to harry up because the raining is getting heavier and heavier. The two connectives, i.e., but in E1 and because in E2, convey the Comparison and Contingency senses respectively. In most cases, we can easily recognize the relation sense by the appearance of a discourse connective since it can be interpreted in only one way</context>
</contexts>
<marker>Pitler, Raghupathy, Mehta, Nenkova, Lee, Joshi, 2008</marker>
<rawString>E. Pitler, M. Raghupathy, H. Mehta, A. Nenkova, A. Lee, A. Joshi. 2008. Easily Identifiable Discourse Relations. Coling 2008: Companion volume: Posters.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Louis</author>
<author>A Nenkova</author>
</authors>
<title>Automatic sense prediction for implicit discourse relations in text.</title>
<date>2009</date>
<booktitle>Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2847" citStr="Pitler et al., 2009" startWordPosition="429" endWordPosition="432">dressed by the presence of discourse connectives and a simple method only considering the sense frequency of connectives can achieve more than 93% accuracy. This indicates the importance of connectives for discourse relation recognition. However, with implicit discourse relation recognition, there is no connective between the textual arguments, which results in a very difficult task. In recent years, a multitude of efforts have been employed to solve this task. One approach is to exploit various linguistically informed features extracted from human-annotated corpora in a supervised framework (Pitler et al., 2009a) and (Lin et al., 2009). Another approach is to perform recognition without human-annotated corpora by creating synthetic examples of implicit relations in an unsupervised way (Marcu and Echihabi, 2002). Moreover, our initial study on PDTB implicit relation data shows that the averaged F-score for the most general 4 senses can reach 91.8% when we obtain the sense of test examples by mapping each implicit connective to its most frequent sense (i.e., sense recognition using gold-truth implicit connectives). This high F-score performance again proves that the connectives are very crucial source</context>
<context position="5823" citStr="Pitler et al., 2009" startWordPosition="896" endWordPosition="899">method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 2009) presented an implicit discourse relation classifier in PDTB with the use of contextual relations, constituent Parse Features, dependency parse features and cross-argument word pairs. Although both of two methods achieved the state of the art performance for automatical recognition of implicit discourse relations, due to lack</context>
<context position="10857" citStr="Pitler et al., 2009" startWordPosition="1681" endWordPosition="1684">ne is to use the sense frequency of predicted connectives in a supervised framework. The other is to directly use the presence of the predicted connectives in an unsupervised way. The two approaches are described as follows. 3.2 Using sense frequency of predicted discourse connectives as features After the above procedure, we get a sorted set of predicted discourse connectives. Due to the presence of an implicit connective, the implicit discourse relation recognition task can be addressed with the methods for explicit relation recognition, e.g., sense classification based only on connectives (Pitler et al., 2009a). Inspired by their work, the first approach is to use sense frequency of predicted discourse connectives as features. We take the connective with the lowest perplexity value (i.e., top 1 connective) as the real connective for the arguments pair. Then we count the sense frequency of this connective on the training set. Figure 1 illustrates the procedure of generating predicted discourse connective from a language model and calculating its sense frequency from training data. Here the calculation of sense frequency of connective is based on the annotated training data which has labeled discour</context>
<context position="13525" citStr="Pitler et al., 2009" startWordPosition="2134" endWordPosition="2137">e appearance of a discourse connective since it can be interpreted in only one way. That means the ambiguity of the mapping between sense and connective is quite low. Therefore, the second approach is to use only the presence of the top N predicted discourse connectives to generate a feature vector for a given pair of arguments. 4 Experiment 4.1 Data sets We used PDTB as our data set to perform the evaluation of our methods. The corpus contains annotations of explicit and implicit discourse relations. The first evaluation is performed on the annotated implicit data set. Following the work of (Pitler et al., 2009a), we used sections 2-20 as the training set, sections 21-22 as the test set and sections 0- 1 as the development set for parameter optimization (e.g., N value). The second evaluation is performed on the annotated explicit data set. We follow the method used in (Sporleder and Lascarides, 2008) to remove the discourse connective from the explicit instances and consider these processed instances as implicit ones. We constructed four binary classifiers to recognize each main senses (i.e., Cont., Cont., Exp., Temp.) from the rest. For each sense we used equal numbers of positive and negative inst</context>
<context position="18707" citStr="Pitler et al., 2009" startWordPosition="2970" endWordPosition="2973">3. by comparison + Arg1 + Arg2: Ppl= 472.206 4. Arg1 + by comparison + Arg2: Ppl= 543.051 In our second approach described in Section 3.3, we considered the combination of connectives and their position as final features like mid but, first but, where the features are binary, that is, the presence or absence of the specific connective. According to the value of Ppl(S,,,�), we tried various N values on development set to get the optimal N value. 4.4 Results Table 3 summarizes the best performance achieved using gold-truth implicit connectives, the previous state-of-art performance achieved by (Pitler et al., 2009a) and our approaches. The first line shows the result by mapping the gold-truth implicit connectives directly to the relation’s sense. The second line presents the best result of (Pitler et al., 2009a). One thing worth mentioning here is that for the Expansion relation, (Pitler et al., 2009a) expanded both training and test sets by including EntRel relation as positive examples, which makes it impossible to perform direct comparison. The third and fourth lines show the best results using our first approach, where the sense frequency is counted on explicit and implicit training set respectivel</context>
<context position="20499" citStr="Pitler et al., 2009" startWordPosition="3251" endWordPosition="3254">eraged F-score than 4- grams models. Specially the 5-grams result of New York corpus is much lower than its 3-grams result. Figure 5 shows the averaged F-scores of different top N on the New York corpus with 3-, 4- and 5-grams language models. The essential 143 Table 3: Best result of implicit relations compared with state-of-art methods. System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Averaged F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) F1 (Acc) Sense recognition using 94.08(98.30) 98.19(99.05) 97.79(97.64) 77.04(97.07) 91.78(98.02) gold-truth implicit connectives Best result in (Pitler et al., 2009a) 21.96(56.59) 47.13(67.30) 76.42(63.62) 16.76(63.49) 40.57(62.75) Use sense frequency in explicit training set 26.02(52.17) 35.72(51.70) 64.94(53.97) 13.76(41.97) 35.10(49.95) Use sense frequency in implicit training set 24.55(63.99) 16.26(70.79) 60.70(53.50) 14.75(70.51) 29.07(64.70) Use presence of top N connectives only 21.91(52.84) 39.53(50.85) 68.84(52.93) 11.91(6.33) 35.55(40.74) Table 4: Best result of explicit relation conversion to implicit relation compared with results using the same method. System Comp. vs. Not Cont. vs. Other Exp. vs. Other Temp. vs. Other Average F1 (Acc) F1 (A</context>
<context position="22272" citStr="Pitler et al., 2009" startWordPosition="3515" endWordPosition="3518"> But we can see that the best averaged F-scores mostly appeared in the range from 100 − 160. For 4-grams and 5-grams models, the system achieved the top averaged F-scores when N = 20 as well. 4.5 Discussion Experimental results on PDTB showed that using predicted connectives achieved the comparable Fscores of the state-of-art method. From Table 3 we can find that our results are closely to the best performance of previous stateof-art methods in terms of averaged F-score. On the Comparison sense, our first approach has an improvement of more than 4% F-score on the previous state-of-art method (Pitler et al., 2009a). As we mentioned before, for the Expansion sense, they included EntRel relation to expand the training set and test set, which makes it impossible to perform a direct comparison. Since the positive instances size has been increased by 50%, they may achieve a higher F-score than our approach. For other relations, our best performance is slightly lower than theirs. While bearing in mind that our approach only uses a very small amount of features for implicit relation recognition. Compared 144 Averaged F-score Figure 4: Curves of averaged F-score on implicit connective classification with n-Gr</context>
</contexts>
<marker>Pitler, Louis, Nenkova, 2009</marker>
<rawString>E. Pitler, A. Louis, A. Nenkova. 2009. Automatic sense prediction for implicit discourse relations in text. Proceedings of the 47th Annual Meeting of the Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Pitler</author>
<author>A Nenkova</author>
</authors>
<title>Using Syntax to Disambiguate Explicit Discourse Connectives in Text.</title>
<date>2009</date>
<booktitle>Proceedings of the ACL-IJCNLP 2009 Conference Short Papers.</booktitle>
<marker>Pitler, Nenkova, 2009</marker>
<rawString>E. Pitler and A. Nenkova. 2009. Using Syntax to Disambiguate Explicit Discourse Connectives in Text. Proceedings of the ACL-IJCNLP 2009 Conference Short Papers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Porter</author>
</authors>
<title>An algorithm for suffix stripping.</title>
<date>1980</date>
<booktitle>In Program,</booktitle>
<volume>14</volume>
<pages>130--137</pages>
<marker>Porter, 1980</marker>
<rawString>M. Porter. An algorithm for suffix stripping. In Program, vol. 14, no. 3, pp.130-137, 1980.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>Annotating attribution in the Penn Discourse TreeBank.</title>
<date>2006</date>
<booktitle>Proceedings of the COLING/ACL Workshop on Sentiment and Subjectivity in Text.</booktitle>
<contexts>
<context position="5646" citStr="Prasad et al., 2006" startWordPosition="869" endWordPosition="872">ults. Section 5 offers some conclusions. 2 Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 2009) presented an implicit discourse relation classifier in PDTB with the use of contextual relations, constituent Parse Features, dependency parse featur</context>
</contexts>
<marker>Prasad, Dinesh, Lee, Joshi, Webber, 2006</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, A. Joshi, B. Webber. 2006. Annotating attribution in the Penn Discourse TreeBank. Proceedings of the COLING/ACL Workshop on Sentiment and Subjectivity in Text.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Prasad</author>
<author>N Dinesh</author>
<author>A Lee</author>
<author>E Miltsakaki</author>
<author>L Robaldo</author>
<author>A Joshi</author>
<author>B Webber</author>
</authors>
<title>The Penn Discourse TreeBank 2.0.</title>
<date>2008</date>
<booktitle>Proceedings of LREC’08.</booktitle>
<marker>Prasad, Dinesh, Lee, Miltsakaki, Robaldo, Joshi, Webber, 2008</marker>
<rawString>R. Prasad, N. Dinesh, A. Lee, E. Miltsakaki, L. Robaldo, A. Joshi, B. Webber. 2008. The Penn Discourse TreeBank 2.0. Proceedings of LREC’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Saito</author>
<author>S Sekine K Yamamoto</author>
</authors>
<title>Using Phrasal Patterns to Identify Discourse Relations.</title>
<date>2006</date>
<journal>Proceeding of the HLTCNA Chapter of the ACL.</journal>
<marker>Saito, Yamamoto, 2006</marker>
<rawString>M. Saito, K.Yamamoto, S.Sekine. 2006. Using Phrasal Patterns to Identify Discourse Relations. Proceeding of the HLTCNA Chapter of the ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Soricut</author>
<author>D Marcu</author>
</authors>
<title>Sentence Level Discourse Parsing using Syntactic and Lexical Information.</title>
<date>2003</date>
<booktitle>Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference.</booktitle>
<contexts>
<context position="5416" citStr="Soricut and Marcu, 2003" startWordPosition="831" endWordPosition="834">s is very useful in solving this task. The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 describes our methods for implicit discourse relation recognition. Section 4 presents experiments and results. Section 5 offers some conclusions. 2 Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing</context>
</contexts>
<marker>Soricut, Marcu, 2003</marker>
<rawString>R. Soricut and D. Marcu. 2003. Sentence Level Discourse Parsing using Syntactic and Lexical Information. Proceedings of the Human Language Technology and North American Association for Computational Linguistics Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Sporleder</author>
<author>A Lascarides</author>
</authors>
<title>Using automatically labelled examples to classify rhetorical relations: an assessment.</title>
<date>2008</date>
<journal>Natural Language Engineering, Volume 14, Issue</journal>
<volume>03</volume>
<contexts>
<context position="7020" citStr="Sporleder and Lascarides, 2008" startWordPosition="1076" endWordPosition="1079">t discourse relations, due to lack of human-annotated corpora, their approaches are not very useful in the real word. Another line of research is to use the unsupervised methods on unhuman-annotated corpus. (Marcu and Echihabi, 2002) used several patterns to extract instances of discourse relations such as contrast and elaboration from unlabeled corpora. Then they used word-pairs between arguments as features for building classification models and tested their model on artificial data for implicit relations. Subsequently other studies attempt to extend the work of (Marcu and Echihabi, 2002). (Sporleder and Lascarides, 2008) discovered that Marcu and Echihabi’s models do not perform as well on implicit relations as one might expect from the test accuracy on synthetic data. (Goldensohn, 2007) extended the work of (Marcu and Echihabi, 2002) by refining the training and classification process using parameter optimization, topic segmentation and syntactic parsing. (Saito et al., 2006) followed the method of (Marcu and Echihabi, 2002) and conducted experiments with a combination of cross-argument word pairs and phrasal patterns as features to recognize implicit relations between adjacent sentences in a Japanese corpus</context>
<context position="13820" citStr="Sporleder and Lascarides, 2008" startWordPosition="2186" endWordPosition="2189">a feature vector for a given pair of arguments. 4 Experiment 4.1 Data sets We used PDTB as our data set to perform the evaluation of our methods. The corpus contains annotations of explicit and implicit discourse relations. The first evaluation is performed on the annotated implicit data set. Following the work of (Pitler et al., 2009a), we used sections 2-20 as the training set, sections 21-22 as the test set and sections 0- 1 as the development set for parameter optimization (e.g., N value). The second evaluation is performed on the annotated explicit data set. We follow the method used in (Sporleder and Lascarides, 2008) to remove the discourse connective from the explicit instances and consider these processed instances as implicit ones. We constructed four binary classifiers to recognize each main senses (i.e., Cont., Cont., Exp., Temp.) from the rest. For each sense we used equal numbers of positive and negative instances in training set. The negative instances were chosen at random from the rest of training set. For both evaluations all instances in sections 21-22 were used as test set. Table 1 lists the numbers of positive and negative instances for each sense in training, development and test sets of im</context>
<context position="23159" citStr="Sporleder and Lascarides, 2008" startWordPosition="3655" endWordPosition="3658">eve a higher F-score than our approach. For other relations, our best performance is slightly lower than theirs. While bearing in mind that our approach only uses a very small amount of features for implicit relation recognition. Compared 144 Averaged F-score Figure 4: Curves of averaged F-score on implicit connective classification with n-Gram language model. with other approaches involving thousands of features, our method is quite promising. From Table 4 we observe comparable averaged F-score (39.98% F-score) on explicit relation data set to that on implicit relation data set. Previously, (Sporleder and Lascarides, 2008) also used the same conversion method to perform implicit relation recognition on different corpora and their best result is around 33.69% F-score. Although the two results cannot be compared directly due to different data sets, the magnitude of performance quantities is comparable and reliable. By comparing with the above different systems, we find several useful observations. First, our method using predicted implicit connectives via a language model can help the task of implicit discourse relation recognition. The results are comparable to the previous state-of-art studies. Second, our meth</context>
</contexts>
<marker>Sporleder, Lascarides, 2008</marker>
<rawString>C. Sporleder and A. Lascarides. 2008. Using automatically labelled examples to classify rhetorical relations: an assessment. Natural Language Engineering, Volume 14, Issue 03.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Pustejovsky</author>
<author>C H R S</author>
<author>A Rumshisky</author>
</authors>
<title>Classification of discourse coherence relations: An exploratory study using multiple knowledge sources.</title>
<date>2006</date>
<booktitle>Proceedings of the 7th SIGDIAL Workshop on Discourse and Dialogue.</booktitle>
<marker>Pustejovsky, S, Rumshisky, 2006</marker>
<rawString>B. Wellner , J. Pustejovsky, C. H. R. S., A. Rumshisky. 2006. Classification of discourse coherence relations: An exploratory study using multiple knowledge sources. Proceedings of the 7th SIGDIAL Workshop on Discourse and Dialogue.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Wolf</author>
<author>E Gibson</author>
<author>A Fisher</author>
<author>M Knight</author>
</authors>
<title>The Discourse GraphBank: A database of texts annotated with coherence relations. Linguistic Data Consortium.</title>
<date>2005</date>
<contexts>
<context position="5533" citStr="Wolf et al., 2005" startWordPosition="850" endWordPosition="853">ion 3 describes our methods for implicit discourse relation recognition. Section 4 presents experiments and results. Section 5 offers some conclusions. 2 Related Work Existing works on automatic recognition of implicit discourse relations fall into two categories according to whether the method is supervised or unsupervised. Some works perform relation recognition with supervised methods on human-annotated corpora, for example, the RST Bank (Carlson et al., 2001) used by (Soricut and Marcu, 2003), adhoc annotations used by (Girju, 2003) and (Baldridge and Lascarides, 2005), and the GraphBank (Wolf et al., 2005) used by (Wellner et al., 2006). Recently the release of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2006) has significantly expanded the discourse-annotated corpora available to researchers, using a comprehensive scheme for both implicit and explicit relations. (Pitler et al., 2009a) performed implicit relation classification on the second version of the PDTB. They used several linguistically informed features, such as word polarity, verb classes, and word pairs, showing performance increases over a random classification baseline. (Lin et al., 2009) presented an implicit discourse rela</context>
</contexts>
<marker>Wolf, Gibson, Fisher, Knight, 2005</marker>
<rawString>F. Wolf, E. Gibson, A. Fisher, M. Knight. 2005. The Discourse GraphBank: A database of texts annotated with coherence relations. Linguistic Data Consortium.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>