<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.996184">
A Syntax-Directed Translator with Extended Domain of Locality
</title>
<author confidence="0.968984">
Liang Huang Kevin Knight Aravind Joshi
</author>
<affiliation confidence="0.9566855">
Dept. of Comp. &amp; Info. Sci. Info. Sci. Inst. Dept. of Comp. &amp; Info. Sci.
Univ. of Pennsylvania Univ. of Southern California Univ. of Pennsylvania
</affiliation>
<address confidence="0.797766">
Philadelphia, PA 19104 Marina del Rey, CA 90292 Philadelphia, PA 19104
</address>
<email confidence="0.885611">
lhuang3@cis.upenn.edu knight@isi.edu joshi@linc.cis.upenn.edu
</email>
<figure confidence="0.5655364">
Abstract
SD translation schema
(synchronous grammar)
specifies translation
(string relation)
</figure>
<bodyText confidence="0.9998542">
A syntax-directed translator first parses
the source-language input into a parse-
tree, and then recursively converts the tree
into a string in the target-language. We
model this conversion by an extended tree-
to-string transducer that have multi-level
trees on the source-side, which gives our
system more expressive power and flexi-
bility. We also define a direct probabil-
ity model and use a linear-time dynamic
programming algorithm to search for the
best derivation. The model is then ex-
tended to the general log-linear frame-
work in order to rescore with other fea-
tures like n-gram language models. We
devise a simple-yet-effective algorithm to
generate non-duplicate k-best translations
for n-gram rescoring. Initial experimen-
tal results on English-to-Chinese transla-
tion are presented.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="abstract">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999743272727273">
The concept of syntax-directed (SD) translation
was originally proposed in compiling (Irons, 1961;
Lewis and Stearns, 1968), where the source program
is parsed into a tree representation that guides the
generation of the object code. Following Aho and
Ullman (1972), a translation, as a set of string pairs,
can be specified by a syntax-directed translation
schema (SDTS), which is essentially a synchronous
context-free grammar (SCFG) that generates two
languages simultaneously. An SDTS also induces a
translator, a device that performs the transformation
</bodyText>
<page confidence="0.788876">
1
</page>
<figure confidence="0.992929666666667">
induces implements
SD translator
(source parser + recursive converter)
</figure>
<figureCaption confidence="0.9056195">
Figure 1: The relationship among SD concepts,
adapted from (Aho and Ullman, 1972).
Figure 2: An example of complex reordering repre-
sented as an STSG rule, which is beyond any SCFG.
</figureCaption>
<bodyText confidence="0.999928833333333">
from input string to output string. In this context, an
SD translator consists of two components, a source-
language parser and a recursive converter which is
usually modeled as a top-down tree-to-string trans-
ducer (G´ecseg and Steinby, 1984). The relationship
among these concepts is illustrated in Fig. 1.
This paper adapts the idea of syntax-directed
translator to statistical machine translation (MT).
We apply stochastic operations at each node of the
source-language parse-tree and search for the best
derivation (a sequence of translation steps) that con-
verts the whole tree into some target-language string
with the highest probability. However, the structural
divergence across languages often results in non-
isomorphic parse-trees that is beyond the power of
SCFGs. For example, the S(VO) structure in English
is translated into a VSO word-order in Arabic, an in-
stance of complex reordering not captured by any
</bodyText>
<equation confidence="0.998284">
S
NP(1) ↓VP
VB(2) NP(3)
↓ ↓
S
VB(2)
NP(1) NP(3)
↓ ↓ ↓
�
� � � � �
,
�
� � � � �
</equation>
<affiliation confidence="0.590254">
Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 1–8,
New York City, New York, June 2006. c�2006 Association for Computational Linguistics
</affiliation>
<page confidence="0.998151">
2
</page>
<figure confidence="0.997238025">
the
gunman
was
VBN
PP
killed
IN
NP-C
by
DT
NN
r1, r2 ⇓
VP
the
police
NP-C
IN
killed
by
DT
NN
the
r3 ⇓
police
the
police
jingfang
[police]
bei
[passive]
jibi
[killed]
o
.
qiangshou
[gunman]
VBN
PP
DT
NN
</figure>
<bodyText confidence="0.983360378378378">
SCFG (Fig. 2).
To alleviate the non-isomorphism problem, (syn-
chronous) grammars with richer expressive power
have been proposed whose rules apply to larger frag-
ments of the tree. For example, Shieber and Sch-
abes (1990) introduce synchronous tree-adjoining
grammar (STAG) and Eisner (2003) uses a syn-
chronous tree-substitution grammar (STSG), which
is a restricted version of STAG with no adjunctions.
STSGs and STAGs generate more tree relations than
SCFGs, e.g. the non-isomorphic tree pair in Fig. 2.
This extra expressive power lies in the extended do-
main of locality (EDL) (Joshi and Schabes, 1997),
i.e., elementary structures beyond the scope of one-
level context-free productions. Besides being lin-
guistically motivated, the need for EDL is also sup-
ported by empirical findings in MT that one-level
rules are often inadequate (Fox, 2002; Galley et al.,
2004). Similarly, in the tree-transducer terminology,
Graehl and Knight (2004) define extended tree trans-
ducers that have multi-level trees on the source-side.
Since an SD translator separates the source-
language analysis from the recursive transformation,
the domains of locality in these two modules are or-
thogonal to each other: in this work, we use a CFG-
based Treebank parser but focuses on the extended
domain in the recursive converter. Following Gal-
ley et al. (2004), we use a special class of extended
tree-to-string transducer (zRs for short) with multi-
level left-hand-side (LHS) trees.1 Since the right-
hand-side (RHS) string can be viewed as a flat one-
level tree with the same nonterminal root from LHS
(Fig. 2), this framework is closely related to STSGs:
they both have extended domain of locality on the
source-side, while our framework remains as a CFG
on the target-side. For instance, an equivalent zRs
rule for the complex reordering in Fig. 2 would be
</bodyText>
<equation confidence="0.801222">
S(x1:NP, VP(x2:VB, x3:NP)) → x2 x1 x3
</equation>
<bodyText confidence="0.977645333333333">
While Section 3 will define the model formally,
we first proceed with an example translation from
English to Chinese (note in particular that the in-
verted phrases between source and target):
1Throughout this paper, we will use LHS and source-side
interchangeably (so are RHS and target-side). In accordance
with our experiments, we also use English and Chinese as the
source and target languages, opposite to the Foreign-to-English
convention of Brown et al. (1993).
</bodyText>
<figure confidence="0.9983435">
(a) the gunman was [killed]1 by [the police]2 .
parser ⇓
PUNC
NP-C
S
VP
DT
NN
VBD
VP-C
.
VBD
was
(c) qiangshou
NP-C
VBN
(d) qiangshou bei
killed
r5 ⇓ r4 ⇓
(e) qiangshou bei [jingfang]2 [jibi]1 o
</figure>
<figureCaption confidence="0.853103">
Figure 3: A synatx-directed translation process for
Example (1).
(1) the gunman was killed by the police .
</figureCaption>
<bodyText confidence="0.998474142857143">
Figure 3 shows how the translator works. The En-
glish sentence (a) is first parsed into the tree in (b),
which is then recursively converted into the Chinese
string in (e) through five steps. First, at the root
node, we apply the rule r1 which preserves the top-
level word-order and translates the English period
into its Chinese counterpart:
</bodyText>
<equation confidence="0.9275055">
(r1) S (x1:NP-C x2:VP PUNC (.) ) → x1 x2 o
VP-C
o
o
</equation>
<bodyText confidence="0.991614857142857">
Then, the rule r2 grabs the whole sub-tree for “the
gunman” and translates it as a phrase:
(r2) NP-C ( DT (the) NN (gunman) ) —* qiangshou
Now we get a “partial Chinese, partial English” sen-
tence “qiangshou VP o” as shown in Fig. 3 (c). Our
recursion goes on to translate the VP sub-tree. Here
we use the rule r3 for the passive construction:
</bodyText>
<equation confidence="0.712384333333333">
__+ bei x2 x1
IN x2:NP-C
by
</equation>
<bodyText confidence="0.999901285714286">
which captures the fact that the agent (NP-C, “the
police”) and the verb (VBN, “killed”) are always
inverted between English and Chinese in a passive
voice. Finally, we apply rules r� and r5 which per-
form phrasal translations for the two remaining sub-
trees in (d), respectively, and get the completed Chi-
nese string in (e).
</bodyText>
<sectionHeader confidence="0.988013" genericHeader="related work">
2 Previous Work
</sectionHeader>
<bodyText confidence="0.999106434782609">
It is helpful to compare this approach with recent ef-
forts in statistical MT. Phrase-based models (Koehn
et al., 2003; Och and Ney, 2004) are good at learn-
ing local translations that are pairs of (consecutive)
sub-strings, but often insufficient in modeling the re-
orderings of phrases themselves, especially between
language pairs with very different word-order. This
is because the generative capacity of these models
lies within the realm of finite-state machinery (Ku-
mar and Byrne, 2003), which is unable to process
nested structures and long-distance dependencies in
natural languages.
Syntax-based models aim to alleviate this prob-
lem by exploiting the power of synchronous rewrit-
ing systems. Both Yamada and Knight (2001) and
Chiang (2005) use SCFGs as the underlying model,
so their translation schemata are syntax-directed as
in Fig. 1, but their translators are not: both systems
do parsing and transformation in a joint search, es-
sentially over a packed forest of parse-trees. To this
end, their translators are not directed by a syntac-
tic tree. Although their method potentially consid-
ers more than one single parse-tree as in our case,
the packed representation of the forest restricts the
scope of each transfer step to a one-level context-
free rule, while our approach decouples the source-
language analyzer and the recursive converter, so
that the latter can have an extended domain of local-
ity. In addition, our translator also enjoys a speed-
up by this decoupling, with each of the two stages
having a smaller search space. In fact, the recursive
transfer step can be done by a a linear-time algo-
rithm (see Section 5), and the parsing step is also
fast with the modern Treebank parsers, for instance
(Collins, 1999; Charniak, 2000). In contrast, their
decodings are reported to be computationally expen-
sive and Chiang (2005) uses aggressive pruning to
make it tractable. There also exists a compromise
between these two approaches, which uses a k-best
list of parse trees (for a relatively small k) to approx-
imate the full forest (see future work).
Besides, our model, as being linguistically mo-
tivated, is also more expressive than the formally
syntax-based models of Chiang (2005) and Wu
(1997). Consider, again, the passive example in rule
r3. In Chiang’s SCFG, there is only one nonterminal
X, so a corresponding rule would be
( was X(1) by X(2), bei X(2) X(1) )
which can also pattern-match the English sentence:
I was [asleep]1 by [sunset]2 .
and translate it into Chinese as a passive voice. This
produces very odd Chinese translation, because here
“was A by B” in the English sentence is not a pas-
sive construction. By contrast, our model applies
rule r3 only if A is a past participle (VBN) and B
is a noun phrase (NP-C). This example also shows
that, one-level SCFG rule, even if informed by the
Treebank as in (Yamada and Knight, 2001), is not
enough to capture a common construction like this
which is five levels deep (from VP to “by”).
There are also some variations of syntax-directed
translators where dependency structures are used
in place of constituent trees (Lin, 2004; Ding and
Palmer, 2005; Quirk et al., 2005). Although they
share with this work the basic motivations and simi-
lar speed-up, it is difficult to specify re-ordering in-
formation within dependency elementary structures,
so they either resort to heuristics (Lin) or a sepa-
rate ordering model for linearization (the other two
</bodyText>
<equation confidence="0.619342">
VP
VBD VP-C
(r3) was x1:VBN PP
</equation>
<page confidence="0.97052">
3
</page>
<bodyText confidence="0.939325666666667">
works).2 Our approach, in contrast, explicitly mod-
els the re-ordering of sub-trees within individual
transfer rules.
</bodyText>
<sectionHeader confidence="0.979861" genericHeader="method">
3 Extended Tree-to-String Tranducers
</sectionHeader>
<bodyText confidence="0.998916833333333">
In this section, we define the formal machinery of
our recursive transformation model as a special case
of xRs transducers (Graehl and Knight, 2004) that
has only one state, and each rule is linear (L) and
non-deleting (N) with regarding to variables in the
source and target sides (henth the name 1-xRLNs).
</bodyText>
<listItem confidence="0.849001">
Definition 1. A 1-xRLNs transducer is a tuple
(N, E, A, R) where N is the set of nonterminals, E
is the input alphabet, A is the output alphabet, and
R is a set of rules. A rule in R is a tuple (t, s, φ)
where:
1. t is the LHS tree, whose internal nodes are la-
beled by nonterminal symbols, and whose fron-
tier nodes are labeled terminals from E or vari-
ables from a set X = {x1, x2,...1;
2. s E (X U A)* is the RHS string;
3. φ is a mapping from X to nonterminals N.
</listItem>
<bodyText confidence="0.997324904761905">
We require each variable xi E X occurs exactly once
in t and exactly once in s (linear and non-deleting).
We denote ρ(t) to be the root symbol of tree t.
When writing these rules, we avoid notational over-
head by introducing a short-hand form from Galley
et al. (2004) that integrates the mapping into the tree,
which is used throughout Section 1. Following TSG
terminology (see Figure 2), we call these “variable
nodes” such as x2:NP-C substitution nodes, since
when applying a rule to a tree, these nodes will be
matched with a sub-tree with the same root symbol.
We also define |X  |to be the rank of the rule, i.e.,
the number of variables in it. For example, rules r1
and r3 in Section 1 are both of rank 2. If a rule has
no variable, i.e., it is of rank zero, then it is called a
purely lexical rule, which performs a phrasal trans-
lation as in phrase-based models. Rule r2, for in-
stance, can be thought of as a phrase pair (the gun-
man, qiangshou).
Informally speaking, a derivation in a transducer
is a sequence of steps converting a source-language
</bodyText>
<footnote confidence="0.920291666666667">
2Although hybrid approaches, such as dependency gram-
mars augmented with phrase-structure information (Alshawi et
al., 2000), can do re-ordering easily.
</footnote>
<figure confidence="0.487016">
r1
r2 r3
r4 r5
r5
(a) (b)
</figure>
<figureCaption confidence="0.941672">
Figure 4: (a) the derivation in Figure 3; (b) another
derviation producing the same output by replacing
r3 with r6 and r7, which provides another way of
translating the passive construction:
</figureCaption>
<equation confidence="0.5521">
(r6) VP (VBD (was) VP-C (x1:VBN x2:PP ) ) --+ x2 x1
(r7) PP ( IN (by) x1:NP-C) --+ bei x1
</equation>
<bodyText confidence="0.885916142857143">
tree into a target-language string, with each step ap-
plying one tranduction rule. However, it can also
be formalized as a tree, following the notion of
derivation-tree in TAG (Joshi and Schabes, 1997):
Definition 2. A derivation d, its source and target
projections, noted £(d) and C(d) respectively, are
recursively defined as follows:
</bodyText>
<listItem confidence="0.943787">
1. If r = (t, s, φ) is a purely lexical rule (φ = 0),
then d = r is a derivation, where £(d) = t and
C(d) = s;
2. If r = (t, s, φ) is a rule, and di is a (sub-)
</listItem>
<bodyText confidence="0.846490888888889">
derivation with the root symbol of its source
projection matches the corresponding substitu-
tion node in r, i.e., ρ(£(di)) = φ(xi), then
d = r(d1, ... , dm) is also a derivation, where
£(d) = [xi H £(di)]t and C(d) = [xi H
C(di)]s.
Note that we use a short-hand notation [xi H yi]t
to denote the result of substituting each xi with yi
in t, where xi ranges over all variables in t.
For example, Figure 4 shows two derivations for
the sentence pair in Example (1). In both cases, the
source projection is the English tree in Figure 3 (b),
and the target projection is the Chinese translation.
Galley et al. (2004) presents a linear-time algo-
rithm for automatic extraction of these xRs rules
from a parallel corpora with word-alignment and
parse-trees on the source-side, which will be used
in our experiments in Section 6.
</bodyText>
<equation confidence="0.915380333333333">
r1
r2 r6
r4 r7
</equation>
<page confidence="0.914353">
4
</page>
<figure confidence="0.673119">
4 Probability Models where c(r) is the count (or frequency) of rule r in
4.1 Direct Model the training data.
</figure>
<bodyText confidence="0.933173">
Departing from the conventional noisy-channel ap-
proach of Brown et al. (1993), our basic model is a
direct one:
</bodyText>
<equation confidence="0.8700765">
c* = argmax Pr(c  |e) (2)
c
</equation>
<bodyText confidence="0.99985375">
where e is the English input string and c* is the
best Chinese translation according to the translation
model Pr(c  |e). We now marginalize over all En-
glish parse trees T (e) that yield the sentence e:
</bodyText>
<equation confidence="0.999694">
Pr(c  |e) = � Pr(T, c  |e)
τET (e)
�= Pr(T  |e) Pr(c  |T) (3)
τET (e)
</equation>
<bodyText confidence="0.8491605">
Rather than taking the sum, we pick the best tree T*
and factors the search into two separate steps: pars-
ing (4) (a well-studied problem) and tree-to-string
translation (5) (Section 5):
</bodyText>
<equation confidence="0.95218825">
T* = argmax Pr(T  |e) (4)
τET (e)
c* = argmax Pr(c  |T*) (5)
c
</equation>
<bodyText confidence="0.999897">
In this sense, our approach can be considered as
a Viterbi approximation of the computationally ex-
pensive joint search using (3) directly. Similarly, we
now marginalize over all derivations
</bodyText>
<equation confidence="0.976491">
D(T*) = {d  |£(d) = T*}
</equation>
<bodyText confidence="0.997721333333333">
that translates English tree T into some Chinese
string and apply the Viterbi approximation again to
search for the best derivation d*:
</bodyText>
<equation confidence="0.9954875">
c* = C(d*) = C(argmax Pr(d)) (6)
dED(τ*)
</equation>
<bodyText confidence="0.997428">
Assuming different rules in a derivation are ap-
plied independently, we approximate Pr(d) as
</bodyText>
<equation confidence="0.8515625">
Pr(d) = H Pr(r) (7)
rEd
</equation>
<bodyText confidence="0.9984195">
where the probability Pr(r) of the rule r is estimated
by conditioning on the root symbol p(t(r)):
</bodyText>
<equation confidence="0.999604">
Pr(r) = Pr(t(r), s(r)  |p(t(r)))
Er1:ρ(t(r1))=ρ(t(r)) c(r,)
c(r) (8)
</equation>
<sectionHeader confidence="0.616392" genericHeader="method">
4.2 Log-Linear Model
</sectionHeader>
<bodyText confidence="0.999547666666667">
Following Och and Ney (2002), we extend the direct
model into a general log-linear framework in order
to incorporate other features:
</bodyText>
<equation confidence="0.877555">
c* = argmax Pr(c  |e)α · Pr(c)β · e−λ|c |(9)
c
</equation>
<bodyText confidence="0.998669">
where Pr(c) is the language model and e−λ|c |is the
length penalty term based on |c|, the length of the
translation. Parameters a, Q, and A are the weights
of relevant features. Note that positive A prefers
longer translations. We use a standard trigram model
for Pr(c).
</bodyText>
<sectionHeader confidence="0.988189" genericHeader="method">
5 Search Algorithms
</sectionHeader>
<bodyText confidence="0.99950775">
We first present a linear-time algorithm for searching
the best derivation under the direct model, and then
extend it to the log-linear case by a new variant of
k-best parsing.
</bodyText>
<subsectionHeader confidence="0.907612">
5.1 Direct Model: Memoized Recursion
</subsectionHeader>
<bodyText confidence="0.999991333333333">
Since our probability model is not based on the noisy
channel, we do not call our search module a “de-
coder” as in most statistical MT work. Instead, read-
ers who speak English but not Chinese can view it as
an “encoder” (or encryptor), which corresponds ex-
actly to our direct model.
Given a fixed parse-tree T*, we are to search
for the best derivation with the highest probability.
This can be done by a simple top-down traversal
(or depth-first search) from the root of T*: at each
node q in T*, try each possible rule r whose English-
side pattern t(r) matches the subtree T*η rooted at q,
and recursively visit each descendant node qi in T*η
that corresponds to a variable in t(r). We then col-
lect the resulting target-language strings and plug
them into the Chinese-side s(r) of rule r, getting
a translation for the subtree T*η. We finally take the
best of all translations.
With the extended LHS of our transducer, there
may be many different rules applicable at one tree
node. For example, consider the VP subtree in
Fig. 3 (c), where both r3 and r6 can apply. As a re-
sult, the number of derivations is exponential in the
size of the tree, since there are exponentially many
</bodyText>
<page confidence="0.985991">
5
</page>
<bodyText confidence="0.999946454545455">
decompositions of the tree for a given set of rules.
This problem can be solved by memoization (Cor-
men et al., 2001): we cache each subtree that has
been visited before, so that every tree node is visited
at most once. This results in a dynamic program-
ming algorithm that is guaranteed to run in O(npq)
time where n is the size of the parse tree, p is the
maximum number of rules applicable to one tree
node, and q is the maximum size of an applicable
rule. For a given rule-set, this algorithm runs in time
linear to the length of the input sentence, since p
and q are considered grammar constants, and n is
proportional to the input length. The full pseudo-
code is worked out in Algorithm 1. A restricted
version of this algorithm first appears in compiling
for optimal code generation from expression-trees
(Aho and Johnson, 1976). In computational linguis-
tics, the bottom-up version of this algorithm resem-
bles the tree parsing algorithm for TSG by Eisner
(2003). Similar algorithms have also been proposed
for dependency-based translation (Lin, 2004; Ding
and Palmer, 2005).
</bodyText>
<subsectionHeader confidence="0.995637">
5.2 Log-linear Model: k-best Search
</subsectionHeader>
<bodyText confidence="0.999945">
Under the log-linear model, one still prefers to
search for the globally best derivation d*:
</bodyText>
<equation confidence="0.985366">
d* = argmax Pr(d)α Pr(C(d)),3e−_`jC(d)j (10)
dED(r*)
</equation>
<bodyText confidence="0.999971484848485">
However, integrating the n-gram model with the
translation model in the search is computationally
very expensive. As a standard alternative, rather
than aiming at the exact best derivation, we search
for top-k derivations under the direct model using
Algorithm 1, and then rerank the k-best list with the
language model and length penalty.
Like other instances of dynamic programming,
Algorithm 1 can be viewed as a hypergraph search
problem. To this end, we use an efficient algo-
rithm by Huang and Chiang (2005, Algorithm 3)
that solves the general k-best derivations problem
in monotonic hypergraphs. It consists of a normal
forward phase for the 1-best derivation and a recur-
sive backward phase for the 2nd, 3rd, ..., kth deriva-
tions.
Unfortunately, different derivations may have the
same yield (a problem called spurious ambiguity),
due to multi-level LHS of our rules. In practice, this
results in a very small ratio of unique strings among
top-k derivations. To alleviate this problem, deter-
minization techniques have been proposed by Mohri
and Riley (2002) for finite-state automata and ex-
tended to tree automata by May and Knight (2006).
These methods eliminate spurious ambiguity by ef-
fectively transforming the grammar into an equiva-
lent deterministic form. However, this transforma-
tion often leads to a blow-up in forest size, which is
exponential to the original size in the worst-case.
So instead of determinization, here we present a
simple-yet-effective extension to the Algorithm 3 of
Huang and Chiang (2005) that guarantees to output
unique translated strings:
</bodyText>
<listItem confidence="0.701797">
• keep a hash-table of unique strings at each vertex
in the hypergraph
• when asking for the next-best derivation of a ver-
tex, keep asking until we get a new string, and
then add it into the hash-table
</listItem>
<bodyText confidence="0.994319666666667">
This method should work in general for any
equivalence relation (say, same derived tree) that can
be defined on derivations.
</bodyText>
<sectionHeader confidence="0.998769" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<bodyText confidence="0.9998418">
Our experiments are on English-to-Chinese trans-
lation, the opposite direction to most of the recent
work in SMT. We are not doing the reverse direction
at this time partly due to the lack of a sufficiently
good parser for Chinese.
</bodyText>
<subsectionHeader confidence="0.98597">
6.1 Data Preparation
</subsectionHeader>
<bodyText confidence="0.9999665625">
Our training set is a Chinese-English parallel corpus
with 1.95M aligned sentences (28.3M words on the
English side). We first word-align them by GIZA++,
then parse the English side by a variant of Collins
(1999) parser, and finally apply the rule-extraction
algorithm of Galley et al. (2004). The resulting rule
set has 24.7M xRs rules. We also use the SRI Lan-
guage Modeling Toolkit (Stolcke, 2002) to train a
Chinese trigram model with Knesser-Ney smooth-
ing on the Chinese side of the parallel corpus.
Our evaluation data consists of 140 short sen-
tences (&lt; 25 Chinese words) of the Xinhua portion
of the NIST 2003 Chinese-to-English evaluation set.
Since we are translating in the other direction, we
use the first English reference as the source input
and the Chinese as the single reference.
</bodyText>
<page confidence="0.998272">
6
</page>
<table confidence="0.992558294117647">
Algorithm 1 Top-down Memoized Recursion
function TRANSLATE(η)
if cache[η] defined then &gt; this sub-tree visited before?
return cache[η]
best +— 0
for r E R do &gt; try each rule r
matched, sublist +— PATTERNMATCH(t(r), q) &gt; tree pattern matching
if matched then &gt; if matched, sublist contains a list of matched subtrees
prob +— Pr(r) &gt; the probability of rule r
for qi E sublist do
pi, si +— TRANSLATE(ni) &gt; recursively solve each sub-problem
prob +— prob · pi
if prob &gt; best then
best +— prob
str +— [xi H si]s(r) &gt; plug in the results
cache[η] +— best, str &gt; caching the best solution for future use
return cache[η] &gt; returns the best string with its prob.
</table>
<subsectionHeader confidence="0.988266">
6.2 Initial Results
</subsectionHeader>
<bodyText confidence="0.999743541666667">
We implemented our system as follows: for each in-
put sentence, we first run Algorithm 1, which returns
the 1-best translation and also builds the derivation
forest of all translations for this sentence. Then we
extract the top 5000 non-duplicate translated strings
from this forest and rescore them with the trigram
model and the length penalty.
We compared our system with a state-of-the-art
phrase-based system Pharaoh (Koehn, 2004) on the
evaluation data. Since the target language is Chi-
nese, we report character-based BLEU score instead
of word-based to ensure our results are indepen-
dent of Chinese tokenizations (although our lan-
guage models are word-based). The BLEU scores
are based on single reference and up to 4-gram pre-
cisions (r1n4). Feature weights of both systems are
tuned on the same data set.3 For Pharaoh, we use the
standard minimum error-rate training (Och, 2003);
and for our system, since there are only two in-
dependent features (as we always fix α = 1), we
use a simple grid-based line-optimization along the
language-model weight axis. For a given language-
model weight Q, we use binary search to find the best
length penalty A that leads to a length-ratio closest
</bodyText>
<footnote confidence="0.894947">
3In this sense, we are only reporting performances on the
development set at this point. We will report results tuned and
tested on separate data sets in the final version of this paper.
</footnote>
<tableCaption confidence="0.99386">
Table 1: BLEU (r1n4) score results
</tableCaption>
<table confidence="0.99047575">
system BLEU
Pharaoh 25.5
direct model (1-best) 20.3
log-linear model (rescored 5000-best) 23.8
</table>
<bodyText confidence="0.99345225">
to 1 against the reference. The results are summa-
rized in Table 1. The rescored translations are better
than the 1-best results from the direct model, but still
slightly worse than Pharaoh.
</bodyText>
<sectionHeader confidence="0.98533" genericHeader="conclusions">
7 Conclusion and On-going Work
</sectionHeader>
<bodyText confidence="0.9998733">
This paper presents an adaptation of the clas-
sic syntax-directed translation with linguistically-
motivated formalisms for statistical MT. Currently
we are doing larger-scale experiments. We are also
investigating more principled algorithms for inte-
grating n-gram language models during the search,
rather than k-best rescoring. Besides, we will extend
this work to translating the top k parse trees, instead
of committing to the 1-best tree, as parsing errors
certainly affect translation quality.
</bodyText>
<page confidence="0.999307">
7
</page>
<sectionHeader confidence="0.987864" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999689739130434">
A. V. Aho and S. C. Johnson. 1976. Optimal code gen-
eration for expression trees. J. ACM, 23(3):488–501.
Alfred V. Aho and Jeffrey D. Ullman. 1972. The The-
ory of Parsing, Translation, and Compiling, volume I:
Parsing. Prentice Hall, Englewood Cliffs, New Jersey.
Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.
2000. Learning dependency translation models as col-
lections of finite state head transducers. Computa-
tional Linguistics, 26(1):45–60.
Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della
Pietra, and Robert L. Mercer. 1993. The mathematics
of statistical machine translation: Parameter estima-
tion. Computational Linguistics, 19:263–311.
Eugene Charniak. 2000. A maximum-entropy-inspired
parser. In Proc. ofNAACL, pages 132–139.
David Chiang. 2005. A hierarchical phrase-based model
for statistical machine translation. In Proc. of the 43rd
ACL.
Michael Collins. 1999. Head-Driven Statistical Models
for Natural Language Parsing. Ph.D. thesis, Univer-
sity of Pennsylvania.
Thomas H. Cormen, Charles E. Leiserson, Ronald L.
Rivest, and Clifford Stein. 2001. Introduction to Al-
gorithms. MIT Press, second edition.
Yuan Ding and Martha Palmer. 2005. Machine trans-
lation using probablisitic synchronous dependency in-
sertion grammars. In Proceedings of the 43rd ACL.
Jason Eisner. 2003. Learning non-isomorphic tree map-
pings for machine translation. In Proceedings ofACL
(companion volume), pages 205–208.
Heidi J. Fox. 2002. Phrasal cohesion and statistical ma-
chine translation. In In Proc. ofEMNLP.
Michel Galley, Mark Hopkins, Kevin Knight, and Daniel
Marcu. 2004. What’s in a translation rule? In HLT-
NAACL.
F. G´ecseg and M. Steinby. 1984. Tree Automata.
Akad´emiai Kiad´o, Budapest.
Jonathan Graehl and Kevin Knight. 2004. Training tree
transducers. In HLT-NAACL, pages 105–112.
Liang Huang and David Chiang. 2005. Better k-best
Parsing. In Proceedings of the Nineth International
Workshop on Parsing Technologies (IWPT-2005), 9-10
October 2005, Vancouver, Canada.
E. T. Irons. 1961. A syntax-directed compiler for AL-
GOL 60. Comm. ACM, 4(1):51–55.
Aravind Joshi and Yves Schabes. 1997. Tree-adjoining
grammars. In G. Rozenberg and A. Salomaa, editors,
Handbook of Formal Languages, volume 3, pages 69
– 124. Springer, Berlin.
Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Proc.
ofHLT-NAACL, pages 127–133.
Philipp Koehn. 2004. Pharaoh: a beam search decoder
for phrase-based statistical machine translation mod-
els. In Proc. ofAMTA, pages 115–124.
Shankar Kumar and William Byrne. 2003. A weighted
finite state transducer implementation of the alignment
template model for statistical machine translation. In
Proc. ofHLT-NAACL, pages 142–149.
P. M. Lewis and R. E. Stearns. 1968. Syntax-directed
transduction. Journal of the ACM, 15(3):465–488.
Dekang Lin. 2004. A path-based transfer model for ma-
chine translation. In Proceedings ofthe 20th COLING.
Jonathan May and Kevin Knight. 2006. A better n-best
list: Practical determinization of weighted finite tree
automata. Submitted to HLT-NAACL 2006.
Mehryar Mohri and Michael Riley. 2002. An efficient
algorithm for the n-best-strings problem. In Proceed-
ings of the International Conference on Spoken Lan-
guage Processing 2002 (ICSLP ’02), Denver, Col-
orado, September.
Franz Josef Och and Hermann Ney. 2002. Discrimina-
tive training and maximum entropy models for statis-
tical machine translation. In Proc. ofACL.
F. J. Och and H. Ney. 2004. The alignment template
approach to statistical machine translation. Computa-
tional Linguistics, 30:417–449.
Franz Och. 2003. Minimum error rate training for statis-
tical machine translation. In Proc. ofACL.
Chris Quirk, Arul Menezes, and Colin Cherry. 2005. De-
pendency treelet translation: Syntactically informed
phrasal smt. In Proceedings of the 43rd ACL.
Stuart Shieber and Yves Schabes. 1990. Synchronous
tree-adjoining grammars. In Proc. of COLING, pages
253–258.
Andrea Stolcke. 2002. Srilm: an extensible language
modeling toolkit. In Proc. ofICSLP.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Computational Linguistics, 23(3):377–404.
Kenji Yamada and Kevin Knight. 2001. A syntax-based
statistical translation model. In Proc. ofACL.
</reference>
<page confidence="0.998494">
8
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.979976">
<title confidence="0.999087">A Syntax-Directed Translator with Extended Domain of Locality</title>
<author confidence="0.999872">Liang Huang Kevin Knight Aravind Joshi</author>
<affiliation confidence="0.9999365">Dept. of Comp. &amp; Info. Sci. Info. Sci. Inst. Dept. of Comp. &amp; Info. Sci. Univ. of Pennsylvania Univ. of Southern California Univ. of Pennsylvania</affiliation>
<address confidence="0.993047">Philadelphia, PA 19104 Marina del Rey, CA 90292 Philadelphia, PA</address>
<email confidence="0.998559">lhuang3@cis.upenn.eduknight@isi.edujoshi@linc.cis.upenn.edu</email>
<abstract confidence="0.999544208333333">SD translation schema (synchronous grammar) (string relation) A syntax-directed translator first parses the source-language input into a parsetree, and then recursively converts the tree into a string in the target-language. We model this conversion by an extended treeto-string transducer that have multi-level trees on the source-side, which gives our system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear framework in order to rescore with other fealike language models. We devise a simple-yet-effective algorithm to non-duplicate translations rescoring. Initial experimental results on English-to-Chinese translation are presented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>S C Johnson</author>
</authors>
<title>Optimal code generation for expression trees.</title>
<date>1976</date>
<journal>J. ACM,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="18608" citStr="Aho and Johnson, 1976" startWordPosition="3193" endWordPosition="3196">st once. This results in a dynamic programming algorithm that is guaranteed to run in O(npq) time where n is the size of the parse tree, p is the maximum number of rules applicable to one tree node, and q is the maximum size of an applicable rule. For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length. The full pseudocode is worked out in Algorithm 1. A restricted version of this algorithm first appears in compiling for optimal code generation from expression-trees (Aho and Johnson, 1976). In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005). 5.2 Log-linear Model: k-best Search Under the log-linear model, one still prefers to search for the globally best derivation d*: d* = argmax Pr(d)α Pr(C(d)),3e−_`jC(d)j (10) dED(r*) However, integrating the n-gram model with the translation model in the search is computationally very expensive. As a standard alternative, rather than aiming at the exac</context>
</contexts>
<marker>Aho, Johnson, 1976</marker>
<rawString>A. V. Aho and S. C. Johnson. 1976. Optimal code generation for expression trees. J. ACM, 23(3):488–501.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alfred V Aho</author>
<author>Jeffrey D Ullman</author>
</authors>
<date>1972</date>
<booktitle>The Theory of Parsing, Translation, and Compiling, volume I: Parsing.</booktitle>
<publisher>Prentice Hall,</publisher>
<location>Englewood Cliffs, New Jersey.</location>
<contexts>
<context position="1541" citStr="Aho and Ullman (1972)" startWordPosition="220" endWordPosition="223">he best derivation. The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Initial experimental results on English-to-Chinese translation are presented. 1 Introduction The concept of syntax-directed (SD) translation was originally proposed in compiling (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a tree representation that guides the generation of the object code. Following Aho and Ullman (1972), a translation, as a set of string pairs, can be specified by a syntax-directed translation schema (SDTS), which is essentially a synchronous context-free grammar (SCFG) that generates two languages simultaneously. An SDTS also induces a translator, a device that performs the transformation 1 induces implements SD translator (source parser + recursive converter) Figure 1: The relationship among SD concepts, adapted from (Aho and Ullman, 1972). Figure 2: An example of complex reordering represented as an STSG rule, which is beyond any SCFG. from input string to output string. In this context, </context>
</contexts>
<marker>Aho, Ullman, 1972</marker>
<rawString>Alfred V. Aho and Jeffrey D. Ullman. 1972. The Theory of Parsing, Translation, and Compiling, volume I: Parsing. Prentice Hall, Englewood Cliffs, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Learning dependency translation models as collections of finite state head transducers.</title>
<date>2000</date>
<journal>Computational Linguistics,</journal>
<volume>26</volume>
<issue>1</issue>
<contexts>
<context position="12782" citStr="Alshawi et al., 2000" startWordPosition="2135" endWordPosition="2138"> symbol. We also define |X |to be the rank of the rule, i.e., the number of variables in it. For example, rules r1 and r3 in Section 1 are both of rank 2. If a rule has no variable, i.e., it is of rank zero, then it is called a purely lexical rule, which performs a phrasal translation as in phrase-based models. Rule r2, for instance, can be thought of as a phrase pair (the gunman, qiangshou). Informally speaking, a derivation in a transducer is a sequence of steps converting a source-language 2Although hybrid approaches, such as dependency grammars augmented with phrase-structure information (Alshawi et al., 2000), can do re-ordering easily. r1 r2 r3 r4 r5 r5 (a) (b) Figure 4: (a) the derivation in Figure 3; (b) another derviation producing the same output by replacing r3 with r6 and r7, which provides another way of translating the passive construction: (r6) VP (VBD (was) VP-C (x1:VBN x2:PP ) ) --+ x2 x1 (r7) PP ( IN (by) x1:NP-C) --+ bei x1 tree into a target-language string, with each step applying one tranduction rule. However, it can also be formalized as a tree, following the notion of derivation-tree in TAG (Joshi and Schabes, 1997): Definition 2. A derivation d, its source and target projection</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 2000</marker>
<rawString>Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 2000. Learning dependency translation models as collections of finite state head transducers. Computational Linguistics, 26(1):45–60.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="5813" citStr="Brown et al. (1993)" startWordPosition="914" endWordPosition="917">ins as a CFG on the target-side. For instance, an equivalent zRs rule for the complex reordering in Fig. 2 would be S(x1:NP, VP(x2:VB, x3:NP)) → x2 x1 x3 While Section 3 will define the model formally, we first proceed with an example translation from English to Chinese (note in particular that the inverted phrases between source and target): 1Throughout this paper, we will use LHS and source-side interchangeably (so are RHS and target-side). In accordance with our experiments, we also use English and Chinese as the source and target languages, opposite to the Foreign-to-English convention of Brown et al. (1993). (a) the gunman was [killed]1 by [the police]2 . parser ⇓ PUNC NP-C S VP DT NN VBD VP-C . VBD was (c) qiangshou NP-C VBN (d) qiangshou bei killed r5 ⇓ r4 ⇓ (e) qiangshou bei [jingfang]2 [jibi]1 o Figure 3: A synatx-directed translation process for Example (1). (1) the gunman was killed by the police . Figure 3 shows how the translator works. The English sentence (a) is first parsed into the tree in (b), which is then recursively converted into the Chinese string in (e) through five steps. First, at the root node, we apply the rule r1 which preserves the toplevel word-order and translates the </context>
<context position="14640" citStr="Brown et al. (1993)" startWordPosition="2482" endWordPosition="2485">ows two derivations for the sentence pair in Example (1). In both cases, the source projection is the English tree in Figure 3 (b), and the target projection is the Chinese translation. Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6. r1 r2 r6 r4 r7 4 4 Probability Models where c(r) is the count (or frequency) of rule r in 4.1 Direct Model the training data. Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one: c* = argmax Pr(c |e) (2) c where e is the English input string and c* is the best Chinese translation according to the translation model Pr(c |e). We now marginalize over all English parse trees T (e) that yield the sentence e: Pr(c |e) = � Pr(T, c |e) τET (e) �= Pr(T |e) Pr(c |T) (3) τET (e) Rather than taking the sum, we pick the best tree T* and factors the search into two separate steps: parsing (4) (a well-studied problem) and tree-to-string translation (5) (Section 5): T* = argmax Pr(T |e) (4) τET (e) c* = argmax Pr(c |T*) (5) c In this sense, our appro</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19:263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>A maximum-entropy-inspired parser.</title>
<date>2000</date>
<booktitle>In Proc. ofNAACL,</booktitle>
<pages>132--139</pages>
<contexts>
<context position="8966" citStr="Charniak, 2000" startWordPosition="1462" endWordPosition="1463">e as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level contextfree rule, while our approach decouples the sourcelanguage analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our translator also enjoys a speedup by this decoupling, with each of the two stages having a smaller search space. In fact, the recursive transfer step can be done by a a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3. In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be </context>
</contexts>
<marker>Charniak, 2000</marker>
<rawString>Eugene Charniak. 2000. A maximum-entropy-inspired parser. In Proc. ofNAACL, pages 132–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proc. of the 43rd ACL.</booktitle>
<contexts>
<context position="7966" citStr="Chiang (2005)" startWordPosition="1293" endWordPosition="1294">2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model, so their translation schemata are syntax-directed as in Fig. 1, but their translators are not: both systems do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their translators are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level contextfree rule, while our approach decouples the sourcelanguage analyzer and the recursive convert</context>
<context position="9417" citStr="Chiang (2005)" startWordPosition="1534" endWordPosition="1535">one by a a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3. In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be ( was X(1) by X(2), bei X(2) X(1) ) which can also pattern-match the English sentence: I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, </context>
<context position="19548" citStr="Chiang (2005" startWordPosition="3341" endWordPosition="3342"> to search for the globally best derivation d*: d* = argmax Pr(d)α Pr(C(d)),3e−_`jC(d)j (10) dED(r*) However, integrating the n-gram model with the translation model in the search is computationally very expensive. As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty. Like other instances of dynamic programming, Algorithm 1 can be viewed as a hypergraph search problem. To this end, we use an efficient algorithm by Huang and Chiang (2005, Algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, ..., kth derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations. To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proc. of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Head-Driven Statistical Models for Natural Language Parsing.</title>
<date>1999</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="8949" citStr="Collins, 1999" startWordPosition="1460" endWordPosition="1461">ingle parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level contextfree rule, while our approach decouples the sourcelanguage analyzer and the recursive converter, so that the latter can have an extended domain of locality. In addition, our translator also enjoys a speedup by this decoupling, with each of the two stages having a smaller search space. In fact, the recursive transfer step can be done by a a linear-time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3. In Chiang’s SCFG, there is only one nonterminal X, so a correspondi</context>
<context position="21421" citStr="Collins (1999)" startWordPosition="3646" endWordPosition="3647">hash-table This method should work in general for any equivalence relation (say, same derived tree) that can be defined on derivations. 6 Experiments Our experiments are on English-to-Chinese translation, the opposite direction to most of the recent work in SMT. We are not doing the reverse direction at this time partly due to the lack of a sufficiently good parser for Chinese. 6.1 Data Preparation Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side). We first word-align them by GIZA++, then parse the English side by a variant of Collins (1999) parser, and finally apply the rule-extraction algorithm of Galley et al. (2004). The resulting rule set has 24.7M xRs rules. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a Chinese trigram model with Knesser-Ney smoothing on the Chinese side of the parallel corpus. Our evaluation data consists of 140 short sentences (&lt; 25 Chinese words) of the Xinhua portion of the NIST 2003 Chinese-to-English evaluation set. Since we are translating in the other direction, we use the first English reference as the source input and the Chinese as the single reference. 6 Algorithm 1 To</context>
</contexts>
<marker>Collins, 1999</marker>
<rawString>Michael Collins. 1999. Head-Driven Statistical Models for Natural Language Parsing. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas H Cormen</author>
<author>Charles E Leiserson</author>
<author>Ronald L Rivest</author>
<author>Clifford Stein</author>
</authors>
<title>Introduction to Algorithms.</title>
<date>2001</date>
<publisher>MIT Press,</publisher>
<note>second edition.</note>
<contexts>
<context position="17892" citStr="Cormen et al., 2001" startWordPosition="3063" endWordPosition="3067">(r). We then collect the resulting target-language strings and plug them into the Chinese-side s(r) of rule r, getting a translation for the subtree T*η. We finally take the best of all translations. With the extended LHS of our transducer, there may be many different rules applicable at one tree node. For example, consider the VP subtree in Fig. 3 (c), where both r3 and r6 can apply. As a result, the number of derivations is exponential in the size of the tree, since there are exponentially many 5 decompositions of the tree for a given set of rules. This problem can be solved by memoization (Cormen et al., 2001): we cache each subtree that has been visited before, so that every tree node is visited at most once. This results in a dynamic programming algorithm that is guaranteed to run in O(npq) time where n is the size of the parse tree, p is the maximum number of rules applicable to one tree node, and q is the maximum size of an applicable rule. For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length. The full pseudocode is worked out in Algorithm 1. A restricted version of</context>
</contexts>
<marker>Cormen, Leiserson, Rivest, Stein, 2001</marker>
<rawString>Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. 2001. Introduction to Algorithms. MIT Press, second edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yuan Ding</author>
<author>Martha Palmer</author>
</authors>
<title>Machine translation using probablisitic synchronous dependency insertion grammars.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="10347" citStr="Ding and Palmer, 2005" startWordPosition="1696" endWordPosition="1699"> voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). There are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005). Although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two VP VBD VP-C (r3) was x1:VBN PP 3 works).2 Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules. 3 Extended Tree-to-String Tranducers In this section, we define the formal machinery of our recursive transformation model as a special case of xRs tran</context>
<context position="18853" citStr="Ding and Palmer, 2005" startWordPosition="3229" endWordPosition="3232">e. For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length. The full pseudocode is worked out in Algorithm 1. A restricted version of this algorithm first appears in compiling for optimal code generation from expression-trees (Aho and Johnson, 1976). In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005). 5.2 Log-linear Model: k-best Search Under the log-linear model, one still prefers to search for the globally best derivation d*: d* = argmax Pr(d)α Pr(C(d)),3e−_`jC(d)j (10) dED(r*) However, integrating the n-gram model with the translation model in the search is computationally very expensive. As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty. Like other instances of dynamic programming, Algorithm 1 can be viewed as a </context>
</contexts>
<marker>Ding, Palmer, 2005</marker>
<rawString>Yuan Ding and Martha Palmer. 2005. Machine translation using probablisitic synchronous dependency insertion grammars. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason Eisner</author>
</authors>
<title>Learning non-isomorphic tree mappings for machine translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofACL (companion volume),</booktitle>
<pages>205--208</pages>
<contexts>
<context position="3770" citStr="Eisner (2003)" startWordPosition="586" endWordPosition="587">and Language Processing, pages 1–8, New York City, New York, June 2006. c�2006 Association for Computational Linguistics 2 the gunman was VBN PP killed IN NP-C by DT NN r1, r2 ⇓ VP the police NP-C IN killed by DT NN the r3 ⇓ police the police jingfang [police] bei [passive] jibi [killed] o . qiangshou [gunman] VBN PP DT NN SCFG (Fig. 2). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig. 2. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree</context>
<context position="18741" citStr="Eisner (2003)" startWordPosition="3216" endWordPosition="3217">the maximum number of rules applicable to one tree node, and q is the maximum size of an applicable rule. For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length. The full pseudocode is worked out in Algorithm 1. A restricted version of this algorithm first appears in compiling for optimal code generation from expression-trees (Aho and Johnson, 1976). In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005). 5.2 Log-linear Model: k-best Search Under the log-linear model, one still prefers to search for the globally best derivation d*: d* = argmax Pr(d)α Pr(C(d)),3e−_`jC(d)j (10) dED(r*) However, integrating the n-gram model with the translation model in the search is computationally very expensive. As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the</context>
</contexts>
<marker>Eisner, 2003</marker>
<rawString>Jason Eisner. 2003. Learning non-isomorphic tree mappings for machine translation. In Proceedings ofACL (companion volume), pages 205–208.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Heidi J Fox</author>
</authors>
<title>Phrasal cohesion and statistical machine translation. In</title>
<date>2002</date>
<booktitle>In Proc. ofEMNLP.</booktitle>
<contexts>
<context position="4324" citStr="Fox, 2002" startWordPosition="674" endWordPosition="675">hronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig. 2. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since an SD translator separates the sourcelanguage analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFGbased Treebank parser but focuses on the extended domain in the recursive converter. Following Galley et al. (2004), we use a special class of extended tree-to-string transducer (zRs for short) with multilevel left-hand-side (</context>
</contexts>
<marker>Fox, 2002</marker>
<rawString>Heidi J. Fox. 2002. Phrasal cohesion and statistical machine translation. In In Proc. ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michel Galley</author>
<author>Mark Hopkins</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
</authors>
<title>What’s in a translation rule?</title>
<date>2004</date>
<booktitle>In HLTNAACL.</booktitle>
<contexts>
<context position="4346" citStr="Galley et al., 2004" startWordPosition="676" endWordPosition="679">e-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig. 2. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since an SD translator separates the sourcelanguage analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFGbased Treebank parser but focuses on the extended domain in the recursive converter. Following Galley et al. (2004), we use a special class of extended tree-to-string transducer (zRs for short) with multilevel left-hand-side (LHS) trees.1 Since the</context>
<context position="11872" citStr="Galley et al. (2004)" startWordPosition="1975" endWordPosition="1978">phabet, A is the output alphabet, and R is a set of rules. A rule in R is a tuple (t, s, φ) where: 1. t is the LHS tree, whose internal nodes are labeled by nonterminal symbols, and whose frontier nodes are labeled terminals from E or variables from a set X = {x1, x2,...1; 2. s E (X U A)* is the RHS string; 3. φ is a mapping from X to nonterminals N. We require each variable xi E X occurs exactly once in t and exactly once in s (linear and non-deleting). We denote ρ(t) to be the root symbol of tree t. When writing these rules, we avoid notational overhead by introducing a short-hand form from Galley et al. (2004) that integrates the mapping into the tree, which is used throughout Section 1. Following TSG terminology (see Figure 2), we call these “variable nodes” such as x2:NP-C substitution nodes, since when applying a rule to a tree, these nodes will be matched with a sub-tree with the same root symbol. We also define |X |to be the rank of the rule, i.e., the number of variables in it. For example, rules r1 and r3 in Section 1 are both of rank 2. If a rule has no variable, i.e., it is of rank zero, then it is called a purely lexical rule, which performs a phrasal translation as in phrase-based models</context>
<context position="14227" citStr="Galley et al. (2004)" startWordPosition="2411" endWordPosition="2414">a (sub-) derivation with the root symbol of its source projection matches the corresponding substitution node in r, i.e., ρ(£(di)) = φ(xi), then d = r(d1, ... , dm) is also a derivation, where £(d) = [xi H £(di)]t and C(d) = [xi H C(di)]s. Note that we use a short-hand notation [xi H yi]t to denote the result of substituting each xi with yi in t, where xi ranges over all variables in t. For example, Figure 4 shows two derivations for the sentence pair in Example (1). In both cases, the source projection is the English tree in Figure 3 (b), and the target projection is the Chinese translation. Galley et al. (2004) presents a linear-time algorithm for automatic extraction of these xRs rules from a parallel corpora with word-alignment and parse-trees on the source-side, which will be used in our experiments in Section 6. r1 r2 r6 r4 r7 4 4 Probability Models where c(r) is the count (or frequency) of rule r in 4.1 Direct Model the training data. Departing from the conventional noisy-channel approach of Brown et al. (1993), our basic model is a direct one: c* = argmax Pr(c |e) (2) c where e is the English input string and c* is the best Chinese translation according to the translation model Pr(c |e). We no</context>
<context position="21501" citStr="Galley et al. (2004)" startWordPosition="3656" endWordPosition="3659"> (say, same derived tree) that can be defined on derivations. 6 Experiments Our experiments are on English-to-Chinese translation, the opposite direction to most of the recent work in SMT. We are not doing the reverse direction at this time partly due to the lack of a sufficiently good parser for Chinese. 6.1 Data Preparation Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side). We first word-align them by GIZA++, then parse the English side by a variant of Collins (1999) parser, and finally apply the rule-extraction algorithm of Galley et al. (2004). The resulting rule set has 24.7M xRs rules. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a Chinese trigram model with Knesser-Ney smoothing on the Chinese side of the parallel corpus. Our evaluation data consists of 140 short sentences (&lt; 25 Chinese words) of the Xinhua portion of the NIST 2003 Chinese-to-English evaluation set. Since we are translating in the other direction, we use the first English reference as the source input and the Chinese as the single reference. 6 Algorithm 1 Top-down Memoized Recursion function TRANSLATE(η) if cache[η] defined then &gt; this </context>
</contexts>
<marker>Galley, Hopkins, Knight, Marcu, 2004</marker>
<rawString>Michel Galley, Mark Hopkins, Kevin Knight, and Daniel Marcu. 2004. What’s in a translation rule? In HLTNAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F G´ecseg</author>
<author>M Steinby</author>
</authors>
<title>Tree Automata. Akad´emiai Kiad´o,</title>
<date>1984</date>
<location>Budapest.</location>
<marker>G´ecseg, Steinby, 1984</marker>
<rawString>F. G´ecseg and M. Steinby. 1984. Tree Automata. Akad´emiai Kiad´o, Budapest.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
</authors>
<title>Training tree transducers.</title>
<date>2004</date>
<booktitle>In HLT-NAACL,</booktitle>
<pages>105--112</pages>
<contexts>
<context position="4419" citStr="Graehl and Knight (2004)" startWordPosition="685" endWordPosition="688">substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig. 2. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since an SD translator separates the sourcelanguage analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work, we use a CFGbased Treebank parser but focuses on the extended domain in the recursive converter. Following Galley et al. (2004), we use a special class of extended tree-to-string transducer (zRs for short) with multilevel left-hand-side (LHS) trees.1 Since the righthand-side (RHS) string can be viewed as a flat onelevel tree with t</context>
<context position="10980" citStr="Graehl and Knight, 2004" startWordPosition="1795" endWordPosition="1798">t al., 2005). Although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two VP VBD VP-C (r3) was x1:VBN PP 3 works).2 Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules. 3 Extended Tree-to-String Tranducers In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and Knight, 2004) that has only one state, and each rule is linear (L) and non-deleting (N) with regarding to variables in the source and target sides (henth the name 1-xRLNs). Definition 1. A 1-xRLNs transducer is a tuple (N, E, A, R) where N is the set of nonterminals, E is the input alphabet, A is the output alphabet, and R is a set of rules. A rule in R is a tuple (t, s, φ) where: 1. t is the LHS tree, whose internal nodes are labeled by nonterminal symbols, and whose frontier nodes are labeled terminals from E or variables from a set X = {x1, x2,...1; 2. s E (X U A)* is the RHS string; 3. φ is a mapping f</context>
</contexts>
<marker>Graehl, Knight, 2004</marker>
<rawString>Jonathan Graehl and Kevin Knight. 2004. Training tree transducers. In HLT-NAACL, pages 105–112.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liang Huang</author>
<author>David Chiang</author>
</authors>
<title>Better k-best Parsing.</title>
<date>2005</date>
<booktitle>In Proceedings of the Nineth International Workshop on Parsing Technologies (IWPT-2005),</booktitle>
<pages>9--10</pages>
<location>Vancouver, Canada.</location>
<contexts>
<context position="19548" citStr="Huang and Chiang (2005" startWordPosition="3339" endWordPosition="3342">ll prefers to search for the globally best derivation d*: d* = argmax Pr(d)α Pr(C(d)),3e−_`jC(d)j (10) dED(r*) However, integrating the n-gram model with the translation model in the search is computationally very expensive. As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty. Like other instances of dynamic programming, Algorithm 1 can be viewed as a hypergraph search problem. To this end, we use an efficient algorithm by Huang and Chiang (2005, Algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, ..., kth derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations. To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree</context>
</contexts>
<marker>Huang, Chiang, 2005</marker>
<rawString>Liang Huang and David Chiang. 2005. Better k-best Parsing. In Proceedings of the Nineth International Workshop on Parsing Technologies (IWPT-2005), 9-10 October 2005, Vancouver, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E T Irons</author>
</authors>
<title>A syntax-directed compiler for</title>
<date>1961</date>
<journal>ALGOL 60. Comm. ACM,</journal>
<volume>4</volume>
<issue>1</issue>
<contexts>
<context position="1373" citStr="Irons, 1961" startWordPosition="195" endWordPosition="196">ur system more expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Initial experimental results on English-to-Chinese translation are presented. 1 Introduction The concept of syntax-directed (SD) translation was originally proposed in compiling (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a tree representation that guides the generation of the object code. Following Aho and Ullman (1972), a translation, as a set of string pairs, can be specified by a syntax-directed translation schema (SDTS), which is essentially a synchronous context-free grammar (SCFG) that generates two languages simultaneously. An SDTS also induces a translator, a device that performs the transformation 1 induces implements SD translator (source parser + recursive converter) Figure 1: The relationship among SD concepts, adapted from (Aho an</context>
</contexts>
<marker>Irons, 1961</marker>
<rawString>E. T. Irons. 1961. A syntax-directed compiler for ALGOL 60. Comm. ACM, 4(1):51–55.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>Yves Schabes</author>
</authors>
<title>Tree-adjoining grammars.</title>
<date>1997</date>
<booktitle>Handbook of Formal Languages,</booktitle>
<volume>3</volume>
<pages>69--124</pages>
<editor>In G. Rozenberg and A. Salomaa, editors,</editor>
<publisher>Springer,</publisher>
<location>Berlin.</location>
<contexts>
<context position="4084" citStr="Joshi and Schabes, 1997" startWordPosition="635" endWordPosition="638">[gunman] VBN PP DT NN SCFG (Fig. 2). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig. 2. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are often inadequate (Fox, 2002; Galley et al., 2004). Similarly, in the tree-transducer terminology, Graehl and Knight (2004) define extended tree transducers that have multi-level trees on the source-side. Since an SD translator separates the sourcelanguage analysis from the recursive transformation, the domains of locality in these two modules are orthogonal to each other: in this work</context>
<context position="13318" citStr="Joshi and Schabes, 1997" startWordPosition="2232" endWordPosition="2235">s dependency grammars augmented with phrase-structure information (Alshawi et al., 2000), can do re-ordering easily. r1 r2 r3 r4 r5 r5 (a) (b) Figure 4: (a) the derivation in Figure 3; (b) another derviation producing the same output by replacing r3 with r6 and r7, which provides another way of translating the passive construction: (r6) VP (VBD (was) VP-C (x1:VBN x2:PP ) ) --+ x2 x1 (r7) PP ( IN (by) x1:NP-C) --+ bei x1 tree into a target-language string, with each step applying one tranduction rule. However, it can also be formalized as a tree, following the notion of derivation-tree in TAG (Joshi and Schabes, 1997): Definition 2. A derivation d, its source and target projections, noted £(d) and C(d) respectively, are recursively defined as follows: 1. If r = (t, s, φ) is a purely lexical rule (φ = 0), then d = r is a derivation, where £(d) = t and C(d) = s; 2. If r = (t, s, φ) is a rule, and di is a (sub-) derivation with the root symbol of its source projection matches the corresponding substitution node in r, i.e., ρ(£(di)) = φ(xi), then d = r(d1, ... , dm) is also a derivation, where £(d) = [xi H £(di)]t and C(d) = [xi H C(di)]s. Note that we use a short-hand notation [xi H yi]t to denote the result </context>
</contexts>
<marker>Joshi, Schabes, 1997</marker>
<rawString>Aravind Joshi and Yves Schabes. 1997. Tree-adjoining grammars. In G. Rozenberg and A. Salomaa, editors, Handbook of Formal Languages, volume 3, pages 69 – 124. Springer, Berlin.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Joseph Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proc. ofHLT-NAACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="7338" citStr="Koehn et al., 2003" startWordPosition="1196" endWordPosition="1199">o” as shown in Fig. 3 (c). Our recursion goes on to translate the VP sub-tree. Here we use the rule r3 for the passive construction: __+ bei x2 x1 IN x2:NP-C by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice. Finally, we apply rules r� and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed Chinese string in (e). 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Kni</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Joseph Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proc. ofHLT-NAACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Pharaoh: a beam search decoder for phrase-based statistical machine translation models.</title>
<date>2004</date>
<booktitle>In Proc. ofAMTA,</booktitle>
<pages>115--124</pages>
<contexts>
<context position="23116" citStr="Koehn, 2004" startWordPosition="3937" endWordPosition="3938">[xi H si]s(r) &gt; plug in the results cache[η] +— best, str &gt; caching the best solution for future use return cache[η] &gt; returns the best string with its prob. 6.2 Initial Results We implemented our system as follows: for each input sentence, we first run Algorithm 1, which returns the 1-best translation and also builds the derivation forest of all translations for this sentence. Then we extract the top 5000 non-duplicate translated strings from this forest and rescore them with the trigram model and the length penalty. We compared our system with a state-of-the-art phrase-based system Pharaoh (Koehn, 2004) on the evaluation data. Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based). The BLEU scores are based on single reference and up to 4-gram precisions (r1n4). Feature weights of both systems are tuned on the same data set.3 For Pharaoh, we use the standard minimum error-rate training (Och, 2003); and for our system, since there are only two independent features (as we always fix α = 1), we use a simple grid-based line-optimization along the </context>
</contexts>
<marker>Koehn, 2004</marker>
<rawString>Philipp Koehn. 2004. Pharaoh: a beam search decoder for phrase-based statistical machine translation models. In Proc. ofAMTA, pages 115–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shankar Kumar</author>
<author>William Byrne</author>
</authors>
<title>A weighted finite state transducer implementation of the alignment template model for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofHLT-NAACL,</booktitle>
<pages>142--149</pages>
<contexts>
<context position="7711" citStr="Kumar and Byrne, 2003" startWordPosition="1252" endWordPosition="1256">asal translations for the two remaining subtrees in (d), respectively, and get the completed Chinese string in (e). 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model, so their translation schemata are syntax-directed as in Fig. 1, but their translators are not: both systems do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their translators are not directed by a syntactic tree. Although their method potentially </context>
</contexts>
<marker>Kumar, Byrne, 2003</marker>
<rawString>Shankar Kumar and William Byrne. 2003. A weighted finite state transducer implementation of the alignment template model for statistical machine translation. In Proc. ofHLT-NAACL, pages 142–149.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P M Lewis</author>
<author>R E Stearns</author>
</authors>
<title>Syntax-directed transduction.</title>
<date>1968</date>
<journal>Journal of the ACM,</journal>
<volume>15</volume>
<issue>3</issue>
<contexts>
<context position="1399" citStr="Lewis and Stearns, 1968" startWordPosition="197" endWordPosition="200">e expressive power and flexibility. We also define a direct probability model and use a linear-time dynamic programming algorithm to search for the best derivation. The model is then extended to the general log-linear framework in order to rescore with other features like n-gram language models. We devise a simple-yet-effective algorithm to generate non-duplicate k-best translations for n-gram rescoring. Initial experimental results on English-to-Chinese translation are presented. 1 Introduction The concept of syntax-directed (SD) translation was originally proposed in compiling (Irons, 1961; Lewis and Stearns, 1968), where the source program is parsed into a tree representation that guides the generation of the object code. Following Aho and Ullman (1972), a translation, as a set of string pairs, can be specified by a syntax-directed translation schema (SDTS), which is essentially a synchronous context-free grammar (SCFG) that generates two languages simultaneously. An SDTS also induces a translator, a device that performs the transformation 1 induces implements SD translator (source parser + recursive converter) Figure 1: The relationship among SD concepts, adapted from (Aho and Ullman, 1972). Figure 2:</context>
</contexts>
<marker>Lewis, Stearns, 1968</marker>
<rawString>P. M. Lewis and R. E. Stearns. 1968. Syntax-directed transduction. Journal of the ACM, 15(3):465–488.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>A path-based transfer model for machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings ofthe 20th COLING.</booktitle>
<contexts>
<context position="10324" citStr="Lin, 2004" startWordPosition="1694" endWordPosition="1695">s a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). There are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005). Although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two VP VBD VP-C (r3) was x1:VBN PP 3 works).2 Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules. 3 Extended Tree-to-String Tranducers In this section, we define the formal machinery of our recursive transformation model as a s</context>
<context position="18829" citStr="Lin, 2004" startWordPosition="3227" endWordPosition="3228">licable rule. For a given rule-set, this algorithm runs in time linear to the length of the input sentence, since p and q are considered grammar constants, and n is proportional to the input length. The full pseudocode is worked out in Algorithm 1. A restricted version of this algorithm first appears in compiling for optimal code generation from expression-trees (Aho and Johnson, 1976). In computational linguistics, the bottom-up version of this algorithm resembles the tree parsing algorithm for TSG by Eisner (2003). Similar algorithms have also been proposed for dependency-based translation (Lin, 2004; Ding and Palmer, 2005). 5.2 Log-linear Model: k-best Search Under the log-linear model, one still prefers to search for the globally best derivation d*: d* = argmax Pr(d)α Pr(C(d)),3e−_`jC(d)j (10) dED(r*) However, integrating the n-gram model with the translation model in the search is computationally very expensive. As a standard alternative, rather than aiming at the exact best derivation, we search for top-k derivations under the direct model using Algorithm 1, and then rerank the k-best list with the language model and length penalty. Like other instances of dynamic programming, Algorit</context>
</contexts>
<marker>Lin, 2004</marker>
<rawString>Dekang Lin. 2004. A path-based transfer model for machine translation. In Proceedings ofthe 20th COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jonathan May</author>
<author>Kevin Knight</author>
</authors>
<title>A better n-best list: Practical determinization of weighted finite tree automata.</title>
<date>2006</date>
<note>Submitted to HLT-NAACL</note>
<contexts>
<context position="20182" citStr="May and Knight (2006)" startWordPosition="3440" endWordPosition="3443">) that solves the general k-best derivations problem in monotonic hypergraphs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, ..., kth derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations. To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree automata by May and Knight (2006). These methods eliminate spurious ambiguity by effectively transforming the grammar into an equivalent deterministic form. However, this transformation often leads to a blow-up in forest size, which is exponential to the original size in the worst-case. So instead of determinization, here we present a simple-yet-effective extension to the Algorithm 3 of Huang and Chiang (2005) that guarantees to output unique translated strings: • keep a hash-table of unique strings at each vertex in the hypergraph • when asking for the next-best derivation of a vertex, keep asking until we get a new string, </context>
</contexts>
<marker>May, Knight, 2006</marker>
<rawString>Jonathan May and Kevin Knight. 2006. A better n-best list: Practical determinization of weighted finite tree automata. Submitted to HLT-NAACL 2006.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehryar Mohri</author>
<author>Michael Riley</author>
</authors>
<title>An efficient algorithm for the n-best-strings problem.</title>
<date>2002</date>
<booktitle>In Proceedings of the International Conference on Spoken Language Processing 2002 (ICSLP ’02),</booktitle>
<location>Denver, Colorado,</location>
<contexts>
<context position="20101" citStr="Mohri and Riley (2002)" startWordPosition="3426" endWordPosition="3429"> To this end, we use an efficient algorithm by Huang and Chiang (2005, Algorithm 3) that solves the general k-best derivations problem in monotonic hypergraphs. It consists of a normal forward phase for the 1-best derivation and a recursive backward phase for the 2nd, 3rd, ..., kth derivations. Unfortunately, different derivations may have the same yield (a problem called spurious ambiguity), due to multi-level LHS of our rules. In practice, this results in a very small ratio of unique strings among top-k derivations. To alleviate this problem, determinization techniques have been proposed by Mohri and Riley (2002) for finite-state automata and extended to tree automata by May and Knight (2006). These methods eliminate spurious ambiguity by effectively transforming the grammar into an equivalent deterministic form. However, this transformation often leads to a blow-up in forest size, which is exponential to the original size in the worst-case. So instead of determinization, here we present a simple-yet-effective extension to the Algorithm 3 of Huang and Chiang (2005) that guarantees to output unique translated strings: • keep a hash-table of unique strings at each vertex in the hypergraph • when asking </context>
</contexts>
<marker>Mohri, Riley, 2002</marker>
<rawString>Mehryar Mohri and Michael Riley. 2002. An efficient algorithm for the n-best-strings problem. In Proceedings of the International Conference on Spoken Language Processing 2002 (ICSLP ’02), Denver, Colorado, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="15938" citStr="Och and Ney (2002)" startWordPosition="2715" endWordPosition="2718">e joint search using (3) directly. Similarly, we now marginalize over all derivations D(T*) = {d |£(d) = T*} that translates English tree T into some Chinese string and apply the Viterbi approximation again to search for the best derivation d*: c* = C(d*) = C(argmax Pr(d)) (6) dED(τ*) Assuming different rules in a derivation are applied independently, we approximate Pr(d) as Pr(d) = H Pr(r) (7) rEd where the probability Pr(r) of the rule r is estimated by conditioning on the root symbol p(t(r)): Pr(r) = Pr(t(r), s(r) |p(t(r))) Er1:ρ(t(r1))=ρ(t(r)) c(r,) c(r) (8) 4.2 Log-Linear Model Following Och and Ney (2002), we extend the direct model into a general log-linear framework in order to incorporate other features: c* = argmax Pr(c |e)α · Pr(c)β · e−λ|c |(9) c where Pr(c) is the language model and e−λ|c |is the length penalty term based on |c|, the length of the translation. Parameters a, Q, and A are the weights of relevant features. Note that positive A prefers longer translations. We use a standard trigram model for Pr(c). 5 Search Algorithms We first present a linear-time algorithm for searching the best derivation under the direct model, and then extend it to the log-linear case by a new variant </context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F J Och</author>
<author>H Ney</author>
</authors>
<title>The alignment template approach to statistical machine translation.</title>
<date>2004</date>
<journal>Computational Linguistics,</journal>
<pages>30--417</pages>
<contexts>
<context position="7358" citStr="Och and Ney, 2004" startWordPosition="1200" endWordPosition="1203">3 (c). Our recursion goes on to translate the VP sub-tree. Here we use the rule r3 for the passive construction: __+ bei x2 x1 IN x2:NP-C by which captures the fact that the agent (NP-C, “the police”) and the verb (VBN, “killed”) are always inverted between English and Chinese in a passive voice. Finally, we apply rules r� and r5 which perform phrasal translations for the two remaining subtrees in (d), respectively, and get the completed Chinese string in (e). 2 Previous Work It is helpful to compare this approach with recent efforts in statistical MT. Phrase-based models (Koehn et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chian</context>
</contexts>
<marker>Och, Ney, 2004</marker>
<rawString>F. J. Och and H. Ney. 2004. The alignment template approach to statistical machine translation. Computational Linguistics, 30:417–449.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
</authors>
<title>Minimum error rate training for statistical machine translation.</title>
<date>2003</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="23567" citStr="Och, 2003" startWordPosition="4011" endWordPosition="4012">forest and rescore them with the trigram model and the length penalty. We compared our system with a state-of-the-art phrase-based system Pharaoh (Koehn, 2004) on the evaluation data. Since the target language is Chinese, we report character-based BLEU score instead of word-based to ensure our results are independent of Chinese tokenizations (although our language models are word-based). The BLEU scores are based on single reference and up to 4-gram precisions (r1n4). Feature weights of both systems are tuned on the same data set.3 For Pharaoh, we use the standard minimum error-rate training (Och, 2003); and for our system, since there are only two independent features (as we always fix α = 1), we use a simple grid-based line-optimization along the language-model weight axis. For a given languagemodel weight Q, we use binary search to find the best length penalty A that leads to a length-ratio closest 3In this sense, we are only reporting performances on the development set at this point. We will report results tuned and tested on separate data sets in the final version of this paper. Table 1: BLEU (r1n4) score results system BLEU Pharaoh 25.5 direct model (1-best) 20.3 log-linear model (res</context>
</contexts>
<marker>Och, 2003</marker>
<rawString>Franz Och. 2003. Minimum error rate training for statistical machine translation. In Proc. ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Quirk</author>
<author>Arul Menezes</author>
<author>Colin Cherry</author>
</authors>
<title>Dependency treelet translation: Syntactically informed phrasal smt.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd ACL.</booktitle>
<contexts>
<context position="10368" citStr="Quirk et al., 2005" startWordPosition="1700" endWordPosition="1703">ery odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). There are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005). Although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two VP VBD VP-C (r3) was x1:VBN PP 3 works).2 Our approach, in contrast, explicitly models the re-ordering of sub-trees within individual transfer rules. 3 Extended Tree-to-String Tranducers In this section, we define the formal machinery of our recursive transformation model as a special case of xRs transducers (Graehl and K</context>
</contexts>
<marker>Quirk, Menezes, Cherry, 2005</marker>
<rawString>Chris Quirk, Arul Menezes, and Colin Cherry. 2005. Dependency treelet translation: Syntactically informed phrasal smt. In Proceedings of the 43rd ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stuart Shieber</author>
<author>Yves Schabes</author>
</authors>
<title>Synchronous tree-adjoining grammars.</title>
<date>1990</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>253--258</pages>
<contexts>
<context position="3700" citStr="Shieber and Schabes (1990)" startWordPosition="575" endWordPosition="579">� � � � � � Workshop on Computationally Hard Problemsand Joint Inference in Speech and Language Processing, pages 1–8, New York City, New York, June 2006. c�2006 Association for Computational Linguistics 2 the gunman was VBN PP killed IN NP-C by DT NN r1, r2 ⇓ VP the police NP-C IN killed by DT NN the r3 ⇓ police the police jingfang [police] bei [passive] jibi [killed] o . qiangshou [gunman] VBN PP DT NN SCFG (Fig. 2). To alleviate the non-isomorphism problem, (synchronous) grammars with richer expressive power have been proposed whose rules apply to larger fragments of the tree. For example, Shieber and Schabes (1990) introduce synchronous tree-adjoining grammar (STAG) and Eisner (2003) uses a synchronous tree-substitution grammar (STSG), which is a restricted version of STAG with no adjunctions. STSGs and STAGs generate more tree relations than SCFGs, e.g. the non-isomorphic tree pair in Fig. 2. This extra expressive power lies in the extended domain of locality (EDL) (Joshi and Schabes, 1997), i.e., elementary structures beyond the scope of onelevel context-free productions. Besides being linguistically motivated, the need for EDL is also supported by empirical findings in MT that one-level rules are oft</context>
</contexts>
<marker>Shieber, Schabes, 1990</marker>
<rawString>Stuart Shieber and Yves Schabes. 1990. Synchronous tree-adjoining grammars. In Proc. of COLING, pages 253–258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Stolcke</author>
</authors>
<title>Srilm: an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. ofICSLP.</booktitle>
<contexts>
<context position="21608" citStr="Stolcke, 2002" startWordPosition="3677" endWordPosition="3678">nese translation, the opposite direction to most of the recent work in SMT. We are not doing the reverse direction at this time partly due to the lack of a sufficiently good parser for Chinese. 6.1 Data Preparation Our training set is a Chinese-English parallel corpus with 1.95M aligned sentences (28.3M words on the English side). We first word-align them by GIZA++, then parse the English side by a variant of Collins (1999) parser, and finally apply the rule-extraction algorithm of Galley et al. (2004). The resulting rule set has 24.7M xRs rules. We also use the SRI Language Modeling Toolkit (Stolcke, 2002) to train a Chinese trigram model with Knesser-Ney smoothing on the Chinese side of the parallel corpus. Our evaluation data consists of 140 short sentences (&lt; 25 Chinese words) of the Xinhua portion of the NIST 2003 Chinese-to-English evaluation set. Since we are translating in the other direction, we use the first English reference as the source input and the Chinese as the single reference. 6 Algorithm 1 Top-down Memoized Recursion function TRANSLATE(η) if cache[η] defined then &gt; this sub-tree visited before? return cache[η] best +— 0 for r E R do &gt; try each rule r matched, sublist +— PATTE</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>Andrea Stolcke. 2002. Srilm: an extensible language modeling toolkit. In Proc. ofICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="9431" citStr="Wu (1997)" startWordPosition="1537" endWordPosition="1538">time algorithm (see Section 5), and the parsing step is also fast with the modern Treebank parsers, for instance (Collins, 1999; Charniak, 2000). In contrast, their decodings are reported to be computationally expensive and Chiang (2005) uses aggressive pruning to make it tractable. There also exists a compromise between these two approaches, which uses a k-best list of parse trees (for a relatively small k) to approximate the full forest (see future work). Besides, our model, as being linguistically motivated, is also more expressive than the formally syntax-based models of Chiang (2005) and Wu (1997). Consider, again, the passive example in rule r3. In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be ( was X(1) by X(2), bei X(2) X(1) ) which can also pattern-match the English sentence: I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if inform</context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proc. ofACL.</booktitle>
<contexts>
<context position="7948" citStr="Yamada and Knight (2001)" startWordPosition="1288" endWordPosition="1291">n et al., 2003; Och and Ney, 2004) are good at learning local translations that are pairs of (consecutive) sub-strings, but often insufficient in modeling the reorderings of phrases themselves, especially between language pairs with very different word-order. This is because the generative capacity of these models lies within the realm of finite-state machinery (Kumar and Byrne, 2003), which is unable to process nested structures and long-distance dependencies in natural languages. Syntax-based models aim to alleviate this problem by exploiting the power of synchronous rewriting systems. Both Yamada and Knight (2001) and Chiang (2005) use SCFGs as the underlying model, so their translation schemata are syntax-directed as in Fig. 1, but their translators are not: both systems do parsing and transformation in a joint search, essentially over a packed forest of parse-trees. To this end, their translators are not directed by a syntactic tree. Although their method potentially considers more than one single parse-tree as in our case, the packed representation of the forest restricts the scope of each transfer step to a one-level contextfree rule, while our approach decouples the sourcelanguage analyzer and the</context>
<context position="10081" citStr="Yamada and Knight, 2001" startWordPosition="1653" endWordPosition="1656">sive example in rule r3. In Chiang’s SCFG, there is only one nonterminal X, so a corresponding rule would be ( was X(1) by X(2), bei X(2) X(1) ) which can also pattern-match the English sentence: I was [asleep]1 by [sunset]2 . and translate it into Chinese as a passive voice. This produces very odd Chinese translation, because here “was A by B” in the English sentence is not a passive construction. By contrast, our model applies rule r3 only if A is a past participle (VBN) and B is a noun phrase (NP-C). This example also shows that, one-level SCFG rule, even if informed by the Treebank as in (Yamada and Knight, 2001), is not enough to capture a common construction like this which is five levels deep (from VP to “by”). There are also some variations of syntax-directed translators where dependency structures are used in place of constituent trees (Lin, 2004; Ding and Palmer, 2005; Quirk et al., 2005). Although they share with this work the basic motivations and similar speed-up, it is difficult to specify re-ordering information within dependency elementary structures, so they either resort to heuristics (Lin) or a separate ordering model for linearization (the other two VP VBD VP-C (r3) was x1:VBN PP 3 wor</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>Kenji Yamada and Kevin Knight. 2001. A syntax-based statistical translation model. In Proc. ofACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>