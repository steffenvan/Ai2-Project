<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000104">
<title confidence="0.996133">
Improving WSD with Multi-Level View of
Context Monitored by Similarity Measure
</title>
<author confidence="0.874147">
E. Crestan(1&apos;2), M. El-Beze(1) and C. de Loupy(2)
</author>
<listItem confidence="0.537856">
(1) Laboratoire d&apos;Informatique d&apos;Avignon
</listItem>
<address confidence="0.8891845">
339 ch. Des Meinajaries, BP 1228
F-84911 Avignon Cedex 9
</address>
<email confidence="0.990437">
feric.crestan, mare.elbezel@lia.univ-avignori.fr
</email>
<sectionHeader confidence="0.959362" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9991735">
The approach presented in this paper for Word
Sense Disambiguation (WSD) is based on a
combination of different views of the context.
Semantic Classification Trees (SCT) are
employed over a short and a multi-level view
of context, including rough semantic features,
while a similarity measure is used in some
particular cases to rely on a larger view of the
context. We also describe our two-step
approach based on HMM for the all-word task.
</bodyText>
<sectionHeader confidence="0.796329" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999952964285715">
In the tracks of SENSEVAL-1 (Kilgarriff and
Rosenzweig, 2000), the second edition of the
word sense disambiguation evaluation campaign
offers a new set of words to test improvements
in the domain of WSD. It also includes a new
task, aimed at disambiguating each word of a
text.
Our approach for the lexical sample task is
based on three different views of the context,
which allows us to consider more information
for sense tagging. In order to deal with short-
range view of the context, we have chosen to
use Semantic Classification Trees (SCT) (Kuhn
and De Mori, 1995), which are binary decision
trees. Moreover, based on our experience, we
will show, that using rough semantic features as
a higher-level view of the context yields
substantial increases in performance. Finally, a
similarity distance is employed in order to
capture longer-range context information.
The paper is organized as follows: in the first
part (Section 1), the work we have done on the
lexical sample task is presented. This part
includes a brief overview of the SCT approach
(Section 1.1) and we show how the coverage it
yields could be increased while using more or
less rough semantic features thanks to a
multi-level view of the context (Section 1.2). In
</bodyText>
<listItem confidence="0.853595">
(2) Sinequa
51-59 rue Ledru Rollin
F-94200 Ivry-sur-Seine
</listItem>
<email confidence="0.672734">
{crestan, loupy}@sinequa.com
</email>
<bodyText confidence="0.9995822">
Section 1.3, we propose to use a similarity
measure like those used in document retrieval in
order to select a sense among those proposed by
the SCT systems. The second part (Section 2) is
dedicated to the all-words task. A two-step
approach based on a trisem-bisem model is
presented (Section 2.1). Then, we propose to
apply a special process on the most frequent
words in the task (Section 2.2). In conclusion,
the results for both tasks are presented.
</bodyText>
<sectionHeader confidence="0.665869" genericHeader="method">
1 Lexical Sample Task
</sectionHeader>
<bodyText confidence="0.999954578947369">
The lexical sample task of SENSEVAL-2 is
composed of 29 nouns, 29 verbs and 15
adjectives in context. We decided to handle the
totality of the words, and always assign one and
only one sense to each test word
(recall = precision). For training purpose, we
used the corpus supplied for each word to be
disambiguated. However, the number of training
sentences supplied was greatly reduced
compared to that of the first SENSEVAL exercise.
By comparison, the average number of training
sentences for the nouns in SENSEVAL-1 data was
about 410 sentences/word. Here, the average
number of training sentences is only 121
sentences/word. This difference leads us to
believe that the present evaluation may be much
harder than the previous one. The senses used
for this evaluation come from the Wordnet 1.7
pre-release (Miller et al., 1990).
</bodyText>
<subsectionHeader confidence="0.999303">
1.1 Applying SCT to WSD
</subsectionHeader>
<bodyText confidence="0.999897">
Yarowsky (1993) states that most clues for the
purpose of disambiguation are present in a
micro-context of 3 or 4 words. SCT seems to be
an adequate approach to handle short contexts.
Moreover, SCT, which are binary decision trees,
permit a simple interpretation of the results, by
recovering the successive questions asked along
each path from the root to a leaf. Kuhn and
</bodyText>
<page confidence="0.998986">
67
</page>
<bodyText confidence="0.968979529411765">
De Mori (1995) have shown that these extracted
rules correspond to regular expressions.
However, this approach requires a certain
amount of data in order for the trees to be grown
with reliable questions in its nodes.
Relying on previous work in this field (Loupy
et al., 2000), the training corpus was used to
build one tree for each word to be
disambiguated. While growing the trees, the list
of possible questions is built at each node,
taking into consideration the position of an
element of the context (lemma in this case). The
Gini impurity G(X) (Breiman et al., 1984) is
then computed (formula 1) for each question in
the list, in order to extract the one which
generates the highest decrease in impurity A.Gq
(formula 2).
</bodyText>
<equation confidence="0.8101594">
G(X) =1-113(s I X)2 (1)
SEs
Where P(sIX) is the probability of sense s
given population X,
AGq =G(T)— pyes„G(Yesq)+ pN„,G(Noq) (2)
</equation>
<bodyText confidence="0.998848818181818">
Here Yesg and Nog correspond respectively to
the population answering yes or no to the
question q; p yes (respectively pivo, ) is the
proportion of population T answering yes
(respectively no) to question q.
A more detailed description of our approach to
SCT can be found in Crestan and El-Beze
(2001).
The data had to be pre-processed before they
could be used. Motivated by conclusions drawn
from recent work (see for example Loupy and
El-Beze (2000)), the context was lemmatized,
except for the word to be disambiguated. The
determiners, possessive pronouns, adverbs and
adjectives were removed, because they bring
more noise to the tree growing process than they
help capture relevant clues. However, some
adjectives were preserved, when they were part
of a compound noun, as in &amp;quot;short circuit&amp;quot;. For
the part-of-speech (POS) tagging process and
lemmatization process, the English Tree-Tagger
(Schmid, 1994) was used.
</bodyText>
<subsectionHeader confidence="0.988935">
1.2 Rough semantic features as a multi-level
</subsectionHeader>
<bodyText confidence="0.996852785714286">
view of context
Regarding previous work using SCT, the
novelty of our approach consists in the
introduction of rough semantic features into the
context in order to increase the coverage of the
trees. The process of tree growing can quickly
suffer from lack of data. The ability of our
system to view the context, not only as a
succession of lemmas, but also as a multi-level
view makes it more robust and reliable.
We used the Semantic Classes (SC) proposed in
Wordnet in order to improve the coverage of the
trees. There are 26 SC associated with nouns
(e.g. &lt;noun.body&gt; for body related nouns) and
15 SC associated with verbs (e.g. &lt;verb.motion&gt;
for motion related verbs). Because most of the
adjectives and adverbs were removed during the
pre-processing phase and because they have
only one or two possible SC, their respective SC
are not employed.
During the SCT building process, there is now
not just one question to ask at a given position in
a training sentence, but n+1 (where n is the
number of possible SC associated with a
lemma). For example, the sentence sample in
figure 1 leads to 16 possible questions if
considering SC, and only 7 questions if
considering only lemmas.
</bodyText>
<table confidence="0.638286">
04
_10
32
40 04 _04
41 _07 18
Yeltsin offer Rutskoi post of vice president
</table>
<figureCaption confidence="0.999637">
Figure 1: Example of SC usage
</figureCaption>
<bodyText confidence="0.999132904761905">
SC are added regardless of the POS. In the
example above, the term offer can only be a
verb, but we still associate with it the classes 04
(noun.act) and 10 (noun.communication), which
are associated with the noun-senses of offer.
There are two reasons for this choice: First, in
the case of erroneous POS tagging, we would
not be able to characterize a sense using the
adequate SC. Second, tests have shown that
results obtained using POS related SC or all the
SC are comparable. This last point could be
explained by the aptitude of SCT to select the
best questions. Therefore, SCT are able to
partially disambiguate the local context at a
coarse-grained sense level when enough data are
available. Consequently, it seems useless to
make assumptions about POS.
Experiments carried on the SENSEVAL-1 data,
has shown an improvement of about 2.5% on
nouns and about 3% on verbs when using the
Semantic Classes.
</bodyText>
<page confidence="0.994165">
68
</page>
<bodyText confidence="0.96190846875">
1.3 Similarity measure for a long-range view of
the context
Experience has shown us that a window size of
WS=3 is enough for disambiguation in many
cases, but there are still numerous cases for
which a larger window size is required.
However, if a larger window size can provide
more information for sense detection, it may
also add more noise. In order to cope with this
drawback, a similarity measure is employed (a
technique usually applied in the field of
document retrieval), as a ruler to decide which
sense seems the more likely, considering the
whole sentence (Figure 2). Firstly, three
different Window Sizes (WS) are considered and
run through the appropriate SCT process
(trained on the same WS). Secondly, for each
sense proposed by the SCT systems (El, E2, and
E3), a pseudo-document is built with the
corresponding sentences from the training
corpus. Then, a similarity measure as those used
in document retrieval is computed between the
test sentence (WS=ISD and each of the pseudo-
documents (i.e. senses). Finally, only the sense
having the best score is kept. The similarity
measure used here is the Cosine measure (Salton
and McGill, 1983).
The analysis of the results has shown that
monitoring several SCT based views of the
context by using the here above described
technique leads to an average precision
improvement of about 2%.
</bodyText>
<sectionHeader confidence="0.944001" genericHeader="method">
2 All-Words Task
</sectionHeader>
<bodyText confidence="0.999779947368421">
The second task proposed in SENSEVAL consists
in tagging almost all the words of a text. This is
a more difficult task because in the first one,
only some words have to be studied, whereas the
behavior of all words must be known in order to
correctly tag an entire text. Hidden Markov
Models (HMM) have shown their efficiency in
many NLP domains: part-of-speech tagging (El-
Beze and Merialdo, 1999), speech recognition
(Jelinek, 1998), etc. Moreover, they have been
used in semantic disambiguation with some
success (Loupy et al., 1998). Therefore, we
decided to use this method for the all words
task.
The test corpus supplied is composed of 2473
words to be disambiguated out of 5836 words.
All POS are represented: 1140 nouns, 544 verbs,
453 adjectives and 299 adverbs (according to the
supplied TreeBank-tagged file).
</bodyText>
<subsectionHeader confidence="0.982057">
2.1 A coarse to fine-grained sense strategy
</subsectionHeader>
<bodyText confidence="0.997380428571429">
In a previous experiment (Loupy et al., 1998),
HMM were applied directly to disambiguate
senses at fine-grained level using a
unisem-bisem model, after training on the
SemCor (Miller et al., 1993). However, even if
this method achieves correct results (72 % of
correct assignation), it does not really improve
</bodyText>
<footnote confidence="0.231769">
Mr Portillo, however, keeps his cabinet post.
</footnote>
<figureCaption confidence="0.9970405">
Figure 2: Sense selection using a similarity
measure
</figureCaption>
<bodyText confidence="0.9999655">
over the unisem model. Therefore, it is
recognized that there are not enough data to
correctly learn the transitions between senses.
On the other hand, an HMM unisem-bisem
model brings a slight improvement as compared
to unisem alone when applied to a coarser
semantic level, that is SC (Loupy et al., 1998).
We adopt the following two-step strategy:
</bodyText>
<listItem confidence="0.93955">
• Firstly, determine the SC associated with
each word in the text (formula 4)
</listItem>
<equation confidence="0.927948333333333">
= Arg MGax[P(G I .0]
(4)
= Arg MGax[P(L I G)P(G)]
</equation>
<bodyText confidence="0.9997215">
where G is the set of possible coarse-grained
semantic classes associated to the lemma L.
</bodyText>
<listItem confidence="0.974987333333333">
• Secondly, assign the most probable fine-
grained sense according to the word and the
previously retrieved SC (formula 5)
</listItem>
<bodyText confidence="0.7643565">
= Arg Alsax[P(S 1 G, L)] (5)
where S is the set of possible senses
associated with the lemma L and its possible
semantic classes G.
</bodyText>
<equation confidence="0.961274">
El 0.2
E2 0.5
E3 03
E2
</equation>
<page confidence="0.987128">
69
</page>
<bodyText confidence="0.994539">
To cope with the well-known sparse data
problem, some assumptions allow us to use a
HMM (trisem-bisem model), in order to
estimate P(G) (formula 6) and P(L/G)
(formula 7).
</bodyText>
<equation confidence="0.921354">
P(G) 112x P(g, g,,g,_,)+ (1— 2)xP(g, g1) (6)
</equation>
<bodyText confidence="0.9363036">
and P(L
In the same way, assumptions were made in
order to estimate the probability P(S/G,L)
(formula 8).
G,L)P,&apos;TIP(s,
</bodyText>
<subsectionHeader confidence="0.999522">
2.2 Using Lexical Sample Task Experience
</subsectionHeader>
<bodyText confidence="0.999769363636364">
In view of our experience with the lexical
sample task, we decided to take advantage of it.
The most frequent words among those to be
disambiguated in the all-words task and which
were also present in the SENSEVAL-2 lexical
sample task were extracted. For those words, the
technique presented in Section 1 was applied. In
this way, 4 verbs (call, develop, find and use)
and 2 nouns (child and church) were
disambiguated by the SCT-Cosine method, as
described in Section 1.3.
</bodyText>
<sectionHeader confidence="0.927529" genericHeader="evaluation">
Results and Conclusion
</sectionHeader>
<bodyText confidence="0.996509833333333">
As mentioned in section 1, the scores for the
second edition of the lexical sample task are
much lower than for the first edition (about
20%). However, our system achieved
satisfactory results comparing to other
participants (see table 1) and even accessed the
top-5 systems. The use of SC as a multi-level
view of the context has generated significant
improvements in the results. As well as, the
combination of different window sizes using
similarity measure on a larger context as a judge
has shown noticeable improvements.
</bodyText>
<table confidence="0.99834275">
Lexical Sample All-Words
Precision Recall Precision Recall
Fine 61.3% 61.3% 61.8% 61.8%
Coarse 68.2% 68.2% 62.6% 62.6%
</table>
<tableCaption confidence="0.99938">
Table 1: Results for fine and coarse-grained senses
</tableCaption>
<bodyText confidence="0.417597">
For the all-words task, our system has proven to
be one of the bests, achieving an average
precision/recall of 61.8%, and this, despite the
absence of mapping between Wordnet 1.6
senses used for training purpose (SemCor) and
Word net 1.7 senses used as test references.
</bodyText>
<sectionHeader confidence="0.79328" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998601">
L. Breiman, J. Friedman, R. Olshen, and C. Stone
(1984): &amp;quot;Classification and Regression Trees&amp;quot;,
Wadsworth.
E. Crestan and M. El-Beze (2001): &amp;quot;Improving
Supervised WSD by Including Rough Semantic
Features in a Multi-Level View of the Context&amp;quot;,
SEMPRO-2001 Workshop, Edinburgh.
hapiAvww.lia.univ-avignon.filpublicationstfich axt/LIA-SEMPRO-2001.pdf
M. El-Beze and B. Merialdo (1999): &amp;quot;HMM Based
Taggers&amp;quot;, in Syntactic Wordclass Tagging, ed.
Hans Van Halteren, Kluwer Academic Publishers,
Text and Language Technology, pp 263-284.
F. Jelinek (1998): &amp;quot;Statistical Methods for Speech
Recognition&amp;quot;, MIT Press, Cambridge.
A. Kilgarriff and J. Rosenzweig (2000): &amp;quot;English
SENSEVAL: Report and Results&amp;quot;, In Proc. LREC,
Athens, Greece, Vol 3, pp 1239-1244.
R. Kuhn and R. De Mori (1995): &amp;quot;The Application of
Semantic Classification Trees to Natural Language
Understanding&amp;quot;, IEEE Transactions on Pattern
Analysis and Machine Intelligence, 17(5),
pp 449-460.
C. de Loupy and M. El-Beze (2000): &amp;quot;Using Few
Clues can compensate the small amount of
resources available for Word Sense
Disambiguation&amp;quot;, LREC, Athens, Vol 1,
pp 219-223.
C. de Loupy, M. El-Beze and P.-F. Marteau (1998):
&amp;quot;Word Sense Disambiguation using HMM
Tagger&amp;quot;, LREC, Grenade, Vol 2, pp. 1255-1258.
C. de Loupy, M. El-Beze and P.-F. Marteau (2000):
&amp;quot;Using Semantic Classification Trees for WSD&amp;quot;,
Computer and the Humanities, N° 34, Kluwer
Academic Publishers, pp 187-192.
G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross,
and K. Miller (1990): &amp;quot;Introduction to WordNet:
An on-line lexical database,&amp;quot; International Journal
of Lexicography, vol. 3(4), pp 235-244.
G. A. Miller, C. Leacock, R. Tengi, and T. Bunker
(1993): &amp;quot;A Semantic Concordance&amp;quot;, Proceedings
of ARPA Workshop on Human Language
Technology, Plainsboro, New Jersey, pp 303-308.
G. Salton and M.J. McGill (1983): &amp;quot;Introduction to
Modern Information Retriever&apos;, McGraw-Hill,
New York.
H. Schmid (1994): &amp;quot;Probabilistic Part-of-Speech
Tagging Using Decision Trees&amp;quot;. In Proceedings of
the Conference on New Methods in Language
Processing. Manchester, UK, pp 44-49.
D. Yarowsky (1993): &amp;quot;One sense per collocation&amp;quot;, In
Proceedings of the ARPA Workshop on Human
Language Technology, pp 266-271.
</reference>
<equation confidence="0.991554333333333">
) (7)
P(S
gi,li) (8)
</equation>
<page confidence="0.984403">
70
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.542675">
<title confidence="0.994283">Improving WSD with Multi-Level View Context Monitored by Similarity Measure</title>
<author confidence="0.984182">M</author>
<author confidence="0.984182">C de</author>
<affiliation confidence="0.780758">(1) Laboratoire d&apos;Informatique d&apos;Avignon</affiliation>
<address confidence="0.9012605">339 ch. Des Meinajaries, BP F-84911 Avignon Cedex</address>
<email confidence="0.987002">feric.crestan,mare.elbezel@lia.univ-avignori.fr</email>
<abstract confidence="0.977357">The approach presented in this paper for Word Sense Disambiguation (WSD) is based on a combination of different views of the context. Semantic Classification Trees (SCT) are employed over a short and a multi-level view of context, including rough semantic features, while a similarity measure is used in some particular cases to rely on a larger view of the context. We also describe our two-step based on HMM for the</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>L Breiman</author>
<author>J Friedman</author>
<author>R Olshen</author>
<author>C Stone</author>
</authors>
<title>Classification and Regression Trees&amp;quot;,</title>
<date>1984</date>
<location>Wadsworth.</location>
<contexts>
<context position="4349" citStr="Breiman et al., 1984" startWordPosition="710" endWordPosition="713">ach path from the root to a leaf. Kuhn and 67 De Mori (1995) have shown that these extracted rules correspond to regular expressions. However, this approach requires a certain amount of data in order for the trees to be grown with reliable questions in its nodes. Relying on previous work in this field (Loupy et al., 2000), the training corpus was used to build one tree for each word to be disambiguated. While growing the trees, the list of possible questions is built at each node, taking into consideration the position of an element of the context (lemma in this case). The Gini impurity G(X) (Breiman et al., 1984) is then computed (formula 1) for each question in the list, in order to extract the one which generates the highest decrease in impurity A.Gq (formula 2). G(X) =1-113(s I X)2 (1) SEs Where P(sIX) is the probability of sense s given population X, AGq =G(T)— pyes„G(Yesq)+ pN„,G(Noq) (2) Here Yesg and Nog correspond respectively to the population answering yes or no to the question q; p yes (respectively pivo, ) is the proportion of population T answering yes (respectively no) to question q. A more detailed description of our approach to SCT can be found in Crestan and El-Beze (2001). The data h</context>
</contexts>
<marker>Breiman, Friedman, Olshen, Stone, 1984</marker>
<rawString>L. Breiman, J. Friedman, R. Olshen, and C. Stone (1984): &amp;quot;Classification and Regression Trees&amp;quot;, Wadsworth.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Crestan</author>
<author>M El-Beze</author>
</authors>
<title>Improving Supervised WSD by Including Rough Semantic Features in a Multi-Level View of the Context&amp;quot;,</title>
<date>2001</date>
<booktitle>SEMPRO-2001 Workshop,</booktitle>
<location>Edinburgh.</location>
<note>hapiAvww.lia.univ-avignon.filpublicationstfich axt/LIA-SEMPRO-2001.pdf</note>
<contexts>
<context position="4937" citStr="Crestan and El-Beze (2001)" startWordPosition="811" endWordPosition="814">i impurity G(X) (Breiman et al., 1984) is then computed (formula 1) for each question in the list, in order to extract the one which generates the highest decrease in impurity A.Gq (formula 2). G(X) =1-113(s I X)2 (1) SEs Where P(sIX) is the probability of sense s given population X, AGq =G(T)— pyes„G(Yesq)+ pN„,G(Noq) (2) Here Yesg and Nog correspond respectively to the population answering yes or no to the question q; p yes (respectively pivo, ) is the proportion of population T answering yes (respectively no) to question q. A more detailed description of our approach to SCT can be found in Crestan and El-Beze (2001). The data had to be pre-processed before they could be used. Motivated by conclusions drawn from recent work (see for example Loupy and El-Beze (2000)), the context was lemmatized, except for the word to be disambiguated. The determiners, possessive pronouns, adverbs and adjectives were removed, because they bring more noise to the tree growing process than they help capture relevant clues. However, some adjectives were preserved, when they were part of a compound noun, as in &amp;quot;short circuit&amp;quot;. For the part-of-speech (POS) tagging process and lemmatization process, the English Tree-Tagger (Schm</context>
</contexts>
<marker>Crestan, El-Beze, 2001</marker>
<rawString>E. Crestan and M. El-Beze (2001): &amp;quot;Improving Supervised WSD by Including Rough Semantic Features in a Multi-Level View of the Context&amp;quot;, SEMPRO-2001 Workshop, Edinburgh. hapiAvww.lia.univ-avignon.filpublicationstfich axt/LIA-SEMPRO-2001.pdf</rawString>
</citation>
<citation valid="true">
<authors>
<author>M El-Beze</author>
<author>B Merialdo</author>
</authors>
<title>HMM Based Taggers&amp;quot;,</title>
<date>1999</date>
<booktitle>in Syntactic Wordclass Tagging, ed. Hans Van Halteren, Kluwer Academic Publishers, Text and Language Technology,</booktitle>
<pages>263--284</pages>
<marker>El-Beze, Merialdo, 1999</marker>
<rawString>M. El-Beze and B. Merialdo (1999): &amp;quot;HMM Based Taggers&amp;quot;, in Syntactic Wordclass Tagging, ed. Hans Van Halteren, Kluwer Academic Publishers, Text and Language Technology, pp 263-284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
</authors>
<title>Statistical Methods for Speech Recognition&amp;quot;,</title>
<date>1998</date>
<publisher>MIT Press,</publisher>
<location>Cambridge.</location>
<contexts>
<context position="9607" citStr="Jelinek, 1998" startWordPosition="1598" endWordPosition="1599">has shown that monitoring several SCT based views of the context by using the here above described technique leads to an average precision improvement of about 2%. 2 All-Words Task The second task proposed in SENSEVAL consists in tagging almost all the words of a text. This is a more difficult task because in the first one, only some words have to be studied, whereas the behavior of all words must be known in order to correctly tag an entire text. Hidden Markov Models (HMM) have shown their efficiency in many NLP domains: part-of-speech tagging (ElBeze and Merialdo, 1999), speech recognition (Jelinek, 1998), etc. Moreover, they have been used in semantic disambiguation with some success (Loupy et al., 1998). Therefore, we decided to use this method for the all words task. The test corpus supplied is composed of 2473 words to be disambiguated out of 5836 words. All POS are represented: 1140 nouns, 544 verbs, 453 adjectives and 299 adverbs (according to the supplied TreeBank-tagged file). 2.1 A coarse to fine-grained sense strategy In a previous experiment (Loupy et al., 1998), HMM were applied directly to disambiguate senses at fine-grained level using a unisem-bisem model, after training on the </context>
</contexts>
<marker>Jelinek, 1998</marker>
<rawString>F. Jelinek (1998): &amp;quot;Statistical Methods for Speech Recognition&amp;quot;, MIT Press, Cambridge.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Kilgarriff</author>
<author>J Rosenzweig</author>
</authors>
<title>English SENSEVAL: Report and Results&amp;quot;,</title>
<date>2000</date>
<booktitle>In Proc. LREC,</booktitle>
<volume>3</volume>
<pages>1239--1244</pages>
<location>Athens, Greece,</location>
<contexts>
<context position="801" citStr="Kilgarriff and Rosenzweig, 2000" startWordPosition="115" endWordPosition="118"> 339 ch. Des Meinajaries, BP 1228 F-84911 Avignon Cedex 9 feric.crestan, mare.elbezel@lia.univ-avignori.fr Abstract The approach presented in this paper for Word Sense Disambiguation (WSD) is based on a combination of different views of the context. Semantic Classification Trees (SCT) are employed over a short and a multi-level view of context, including rough semantic features, while a similarity measure is used in some particular cases to rely on a larger view of the context. We also describe our two-step approach based on HMM for the all-word task. Introduction In the tracks of SENSEVAL-1 (Kilgarriff and Rosenzweig, 2000), the second edition of the word sense disambiguation evaluation campaign offers a new set of words to test improvements in the domain of WSD. It also includes a new task, aimed at disambiguating each word of a text. Our approach for the lexical sample task is based on three different views of the context, which allows us to consider more information for sense tagging. In order to deal with shortrange view of the context, we have chosen to use Semantic Classification Trees (SCT) (Kuhn and De Mori, 1995), which are binary decision trees. Moreover, based on our experience, we will show, that usi</context>
</contexts>
<marker>Kilgarriff, Rosenzweig, 2000</marker>
<rawString>A. Kilgarriff and J. Rosenzweig (2000): &amp;quot;English SENSEVAL: Report and Results&amp;quot;, In Proc. LREC, Athens, Greece, Vol 3, pp 1239-1244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Kuhn</author>
<author>R De Mori</author>
</authors>
<title>The Application of Semantic Classification Trees to Natural Language Understanding&amp;quot;,</title>
<date>1995</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence,</journal>
<volume>17</volume>
<issue>5</issue>
<pages>449--460</pages>
<marker>Kuhn, De Mori, 1995</marker>
<rawString>R. Kuhn and R. De Mori (1995): &amp;quot;The Application of Semantic Classification Trees to Natural Language Understanding&amp;quot;, IEEE Transactions on Pattern Analysis and Machine Intelligence, 17(5), pp 449-460.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de Loupy</author>
<author>M El-Beze</author>
</authors>
<title>Using Few Clues can compensate the small amount of resources available for Word Sense Disambiguation&amp;quot;,</title>
<date>2000</date>
<volume>1</volume>
<pages>219--223</pages>
<location>LREC, Athens,</location>
<marker>de Loupy, El-Beze, 2000</marker>
<rawString>C. de Loupy and M. El-Beze (2000): &amp;quot;Using Few Clues can compensate the small amount of resources available for Word Sense Disambiguation&amp;quot;, LREC, Athens, Vol 1, pp 219-223.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de Loupy</author>
<author>M El-Beze</author>
<author>P-F Marteau</author>
</authors>
<title>Word Sense Disambiguation using HMM Tagger&amp;quot;,</title>
<date>1998</date>
<journal>LREC, Grenade,</journal>
<volume>2</volume>
<pages>1255--1258</pages>
<marker>de Loupy, El-Beze, Marteau, 1998</marker>
<rawString>C. de Loupy, M. El-Beze and P.-F. Marteau (1998): &amp;quot;Word Sense Disambiguation using HMM Tagger&amp;quot;, LREC, Grenade, Vol 2, pp. 1255-1258.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C de Loupy</author>
<author>M El-Beze</author>
<author>P-F Marteau</author>
</authors>
<title>Using Semantic Classification Trees for WSD&amp;quot;,</title>
<date>2000</date>
<journal>Computer and the Humanities, N°</journal>
<volume>34</volume>
<pages>187--192</pages>
<publisher>Kluwer Academic Publishers,</publisher>
<marker>de Loupy, El-Beze, Marteau, 2000</marker>
<rawString>C. de Loupy, M. El-Beze and P.-F. Marteau (2000): &amp;quot;Using Semantic Classification Trees for WSD&amp;quot;, Computer and the Humanities, N° 34, Kluwer Academic Publishers, pp 187-192.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>R Beckwith</author>
<author>C Fellbaum</author>
<author>D Gross</author>
<author>K Miller</author>
</authors>
<title>Introduction to WordNet: An on-line lexical database,&amp;quot;</title>
<date>1990</date>
<journal>International Journal of Lexicography,</journal>
<volume>3</volume>
<issue>4</issue>
<pages>235--244</pages>
<contexts>
<context position="3371" citStr="Miller et al., 1990" startWordPosition="542" endWordPosition="545">ecall = precision). For training purpose, we used the corpus supplied for each word to be disambiguated. However, the number of training sentences supplied was greatly reduced compared to that of the first SENSEVAL exercise. By comparison, the average number of training sentences for the nouns in SENSEVAL-1 data was about 410 sentences/word. Here, the average number of training sentences is only 121 sentences/word. This difference leads us to believe that the present evaluation may be much harder than the previous one. The senses used for this evaluation come from the Wordnet 1.7 pre-release (Miller et al., 1990). 1.1 Applying SCT to WSD Yarowsky (1993) states that most clues for the purpose of disambiguation are present in a micro-context of 3 or 4 words. SCT seems to be an adequate approach to handle short contexts. Moreover, SCT, which are binary decision trees, permit a simple interpretation of the results, by recovering the successive questions asked along each path from the root to a leaf. Kuhn and 67 De Mori (1995) have shown that these extracted rules correspond to regular expressions. However, this approach requires a certain amount of data in order for the trees to be grown with reliable que</context>
</contexts>
<marker>Miller, Beckwith, Fellbaum, Gross, Miller, 1990</marker>
<rawString>G. A. Miller, R. Beckwith, C. Fellbaum, D. Gross, and K. Miller (1990): &amp;quot;Introduction to WordNet: An on-line lexical database,&amp;quot; International Journal of Lexicography, vol. 3(4), pp 235-244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G A Miller</author>
<author>C Leacock</author>
<author>R Tengi</author>
<author>T Bunker</author>
</authors>
<title>A Semantic Concordance&amp;quot;,</title>
<date>1993</date>
<booktitle>Proceedings of ARPA Workshop on Human Language Technology,</booktitle>
<pages>303--308</pages>
<location>Plainsboro, New Jersey,</location>
<contexts>
<context position="10235" citStr="Miller et al., 1993" startWordPosition="1697" endWordPosition="1700">oreover, they have been used in semantic disambiguation with some success (Loupy et al., 1998). Therefore, we decided to use this method for the all words task. The test corpus supplied is composed of 2473 words to be disambiguated out of 5836 words. All POS are represented: 1140 nouns, 544 verbs, 453 adjectives and 299 adverbs (according to the supplied TreeBank-tagged file). 2.1 A coarse to fine-grained sense strategy In a previous experiment (Loupy et al., 1998), HMM were applied directly to disambiguate senses at fine-grained level using a unisem-bisem model, after training on the SemCor (Miller et al., 1993). However, even if this method achieves correct results (72 % of correct assignation), it does not really improve Mr Portillo, however, keeps his cabinet post. Figure 2: Sense selection using a similarity measure over the unisem model. Therefore, it is recognized that there are not enough data to correctly learn the transitions between senses. On the other hand, an HMM unisem-bisem model brings a slight improvement as compared to unisem alone when applied to a coarser semantic level, that is SC (Loupy et al., 1998). We adopt the following two-step strategy: • Firstly, determine the SC associat</context>
</contexts>
<marker>Miller, Leacock, Tengi, Bunker, 1993</marker>
<rawString>G. A. Miller, C. Leacock, R. Tengi, and T. Bunker (1993): &amp;quot;A Semantic Concordance&amp;quot;, Proceedings of ARPA Workshop on Human Language Technology, Plainsboro, New Jersey, pp 303-308.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Salton</author>
<author>M J McGill</author>
</authors>
<title>Introduction to Modern Information Retriever&apos;,</title>
<date>1983</date>
<publisher>McGraw-Hill,</publisher>
<location>New York.</location>
<contexts>
<context position="8963" citStr="Salton and McGill, 1983" startWordPosition="1487" endWordPosition="1490">ely, considering the whole sentence (Figure 2). Firstly, three different Window Sizes (WS) are considered and run through the appropriate SCT process (trained on the same WS). Secondly, for each sense proposed by the SCT systems (El, E2, and E3), a pseudo-document is built with the corresponding sentences from the training corpus. Then, a similarity measure as those used in document retrieval is computed between the test sentence (WS=ISD and each of the pseudodocuments (i.e. senses). Finally, only the sense having the best score is kept. The similarity measure used here is the Cosine measure (Salton and McGill, 1983). The analysis of the results has shown that monitoring several SCT based views of the context by using the here above described technique leads to an average precision improvement of about 2%. 2 All-Words Task The second task proposed in SENSEVAL consists in tagging almost all the words of a text. This is a more difficult task because in the first one, only some words have to be studied, whereas the behavior of all words must be known in order to correctly tag an entire text. Hidden Markov Models (HMM) have shown their efficiency in many NLP domains: part-of-speech tagging (ElBeze and Meriald</context>
</contexts>
<marker>Salton, McGill, 1983</marker>
<rawString>G. Salton and M.J. McGill (1983): &amp;quot;Introduction to Modern Information Retriever&apos;, McGraw-Hill, New York.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Schmid</author>
</authors>
<title>Probabilistic Part-of-Speech Tagging Using Decision Trees&amp;quot;.</title>
<date>1994</date>
<booktitle>In Proceedings of the Conference on New Methods in Language Processing.</booktitle>
<pages>44--49</pages>
<location>Manchester, UK,</location>
<contexts>
<context position="5546" citStr="Schmid, 1994" startWordPosition="905" endWordPosition="906">001). The data had to be pre-processed before they could be used. Motivated by conclusions drawn from recent work (see for example Loupy and El-Beze (2000)), the context was lemmatized, except for the word to be disambiguated. The determiners, possessive pronouns, adverbs and adjectives were removed, because they bring more noise to the tree growing process than they help capture relevant clues. However, some adjectives were preserved, when they were part of a compound noun, as in &amp;quot;short circuit&amp;quot;. For the part-of-speech (POS) tagging process and lemmatization process, the English Tree-Tagger (Schmid, 1994) was used. 1.2 Rough semantic features as a multi-level view of context Regarding previous work using SCT, the novelty of our approach consists in the introduction of rough semantic features into the context in order to increase the coverage of the trees. The process of tree growing can quickly suffer from lack of data. The ability of our system to view the context, not only as a succession of lemmas, but also as a multi-level view makes it more robust and reliable. We used the Semantic Classes (SC) proposed in Wordnet in order to improve the coverage of the trees. There are 26 SC associated w</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>H. Schmid (1994): &amp;quot;Probabilistic Part-of-Speech Tagging Using Decision Trees&amp;quot;. In Proceedings of the Conference on New Methods in Language Processing. Manchester, UK, pp 44-49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>One sense per collocation&amp;quot;,</title>
<date>1993</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>266--271</pages>
<contexts>
<context position="3412" citStr="Yarowsky (1993)" startWordPosition="551" endWordPosition="552">sed the corpus supplied for each word to be disambiguated. However, the number of training sentences supplied was greatly reduced compared to that of the first SENSEVAL exercise. By comparison, the average number of training sentences for the nouns in SENSEVAL-1 data was about 410 sentences/word. Here, the average number of training sentences is only 121 sentences/word. This difference leads us to believe that the present evaluation may be much harder than the previous one. The senses used for this evaluation come from the Wordnet 1.7 pre-release (Miller et al., 1990). 1.1 Applying SCT to WSD Yarowsky (1993) states that most clues for the purpose of disambiguation are present in a micro-context of 3 or 4 words. SCT seems to be an adequate approach to handle short contexts. Moreover, SCT, which are binary decision trees, permit a simple interpretation of the results, by recovering the successive questions asked along each path from the root to a leaf. Kuhn and 67 De Mori (1995) have shown that these extracted rules correspond to regular expressions. However, this approach requires a certain amount of data in order for the trees to be grown with reliable questions in its nodes. Relying on previous </context>
</contexts>
<marker>Yarowsky, 1993</marker>
<rawString>D. Yarowsky (1993): &amp;quot;One sense per collocation&amp;quot;, In Proceedings of the ARPA Workshop on Human Language Technology, pp 266-271.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>