<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000017">
<title confidence="0.9922005">
Cut the noise: Mutually reinforcing reordering and alignments for
improved machine translation
</title>
<author confidence="0.811902">
Karthik Visweswariah Mitesh M. Khapra Ananthakrishnan Ramanathan
</author>
<affiliation confidence="0.786537">
IBM Research India IBM Research India IBM Research India
</affiliation>
<email confidence="0.980577">
v-karthik@in.ibm.com mikhapra@in.ibm.com anandr42@gmail.com
</email>
<sectionHeader confidence="0.993532" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999833939393939">
Preordering of a source language sentence
to match target word order has proved to
be useful for improving machine transla-
tion systems. Previous work has shown
that a reordering model can be learned
from high quality manual word alignments
to improve machine translation perfor-
mance. In this paper, we focus on further
improving the performance of the reorder-
ing model (and thereby machine transla-
tion) by using a larger corpus of sentence
aligned data for which manual word align-
ments are not available but automatic ma-
chine generated alignments are available.
The main challenge we tackle is to gen-
erate quality data for training the reorder-
ing model in spite of the machine align-
ments being noisy. To mitigate the effect
of noisy machine alignments, we propose
a novel approach that improves reorder-
ings produced given noisy alignments and
also improves word alignments using in-
formation from the reordering model. This
approach generates alignments that are 2.6
f-Measure points better than a baseline su-
pervised aligner. The data generated al-
lows us to train a reordering model that
gives an improvement of 1.8 BLEU points
on the NIST MT-08 Urdu-English eval-
uation set over a reordering model that
only uses manual word alignments, and a
gain of 5.2 BLEU points over a standard
phrase-based baseline.
</bodyText>
<sectionHeader confidence="0.999337" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999962111111111">
Dealing with word order differences between
source and target languages presents a significant
challenge for machine translation systems. Failing
to produce target words in the correct order results
in machine translation output that is not fluent and
is often very hard to understand. These problems
are particularly severe when translating between
languages which have very different structure.
Phrase based systems (Koehn et al., 2003) use
lexicalized distortion models (Al-Onaizan and Pa-
pineni, 2006; Tillman, 2004) and scores from the
target language model to produce words in the cor-
rect order in the target language. These systems
typically are only able to capture short range re-
orderings and the amount of data required to po-
tentially capture longer range reordering phenom-
ena is prohibitively large.
There has been a large body of work showing
the efficacy of preordering source sentences using
a source parser and applying hand written or auto-
matically learned rules (Collins et al., 2005; Wang
et al., 2007; Ramanathan et al., 2009; Xia and Mc-
Cord, 2004; Genzel, 2010; Visweswariah et al.,
2010). Recently, approaches that address the prob-
lem of word order differences between the source
and target language without requiring a high qual-
ity source or target parser have been proposed
(DeNero and Uszkoreit, 2011; Visweswariah et
al., 2011; Neubig et al., 2012). These methods
use a small corpus of manual word alignments
(where the words in the source sentence are man-
ually aligned to the words in the target sentence)
to learn a model to preorder the source sentence to
match target order.
In this paper, we build upon the approach in
(Visweswariah et al., 2011) which uses manual
word alignments for learning a reordering model.
Specifically, we show that we can significantly
improve reordering performance by using a large
number of sentence pairs for which manual word
alignments are not available. The motivation for
going beyond manual word alignments is clear:
the reordering model can have millions of features
and estimating weights for the features on thou-
sands of sentences of manual word alignments is
</bodyText>
<page confidence="0.909809">
1275
</page>
<note confidence="0.9131375">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1275–1284,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.983462592592593">
likely to be inadequate. One approach to deal with
this problem would be to use only part-of-speech
tags as features for all but the most frequent words.
This will cut down on the number of features and
perhaps the model would be learnable with a small
set of manual word alignments. Unfortunately, as
we will see in the experimental section, leaving
out lexical information from the models hurts per-
formance even with a relatively small set of man-
ual word alignments. Another option would be to
collect more manual word alignments but this is
undesirable because it is time consuming and ex-
pensive.
The challenge in going beyond manual word
alignments and using machine alignments is the
noise in the machine alignments which affects the
performance of the reordering model (see Section
5). We illustrate this with the help of a motivating
example. Consider the example English sentence
and its translation shown in Figure 1.
Figure 1: An example English sentence with
its Urdu translation with alignment links. Red
(dotted) links are incorrect links while the blue
(dashed) links are the corresponding correct links.
A standard word alignment algorithm that we
used (McCarley et al., 2011) made the mistake of
mis-aligning the Urdu ko and keliye (it switched
the two). Deriving reference reorderings from
these wrong alignments would give us an incor-
rect reordering. A reordering model trained on
such incorrect reorderings would obviously per-
form poorly. Our task is thus two-fold (i) im-
prove the quality of machine alignments (ii) use
these less noisy alignments to derive cleaner train-
ing data for a reordering model.
Before proceeding, we first point out that the
two tasks, viz., reordering and word alignment
are related: Having perfect reordering makes the
alignment task easier while having perfect align-
ments in turn makes the task of finding reorder-
ings trivial. Motivated by this fact, we introduce
models that allow us to connect the source/target
reordering and the word alignments and show
that these models help in mutually improving the
performance of word alignments and reordering.
Specifically, we build two models: the first scores
reorderings given the source sentence and noisy
alignments, the second scores alignments given
the noisy source and target reorderings and the
source and target sentences themselves. The sec-
ond model helps produce better alignments, while
we use the first model to help generate better ref-
erence reordering given noisy alignments. These
improved reference reorderings will then be used
to train a reordering model.
Our experiments show that reordering models
trained using these improved machine alignments
perform significantly better than models trained
only on manual word alignments. This results in
a 1.8 BLEU point gain in machine translation per-
formance on an Urdu-English machine translation
task over a preordering model trained using only
manual word alignments. In all, this increases
the gain in performance by using the preordering
model to 5.2 BLEU points over a standard phrase-
based system with no preordering.
The rest of this paper is structured as follows.
Section 2 describes the main reordering issues in
Urdu-English translation. Section 3 introduces the
reordering modeling framework that forms the ba-
sis for our work. Section 4 describes the two mod-
els we use to tie together reordering and align-
ments and how we use these models to generate
training data for training our reordering model.
Section 5 presents the experimental setup used for
evaluating the models proposed in this paper on
an Urdu-English machine translation task. Sec-
tion 6 presents the results of our experiments.
We describe related work in Section 7 and finally
present some concluding remarks and potential fu-
ture work in Section 8.
</bodyText>
<sectionHeader confidence="0.9447275" genericHeader="introduction">
2 Reordering issues in Urdu-English
translation
</sectionHeader>
<bodyText confidence="0.985560666666667">
In this section we describe the main sources of
word order differences between Urdu and English
since this is the language pair we experiment with
in this paper.
The typical word order in Urdu is Subject-
Object-Verb unlike English in which the order is
Subject-Verb-Object. Urdu has case markers that
sometimes (but not always) mark the subject and
the object of a sentence. This difference in the
placement of verbs can often lead to movements of
verbs over long distances (depending on the num-
ber of words in the object). Phrase based systems
do not capture such long distance movements well.
He went to the stadium to play
vaha khelne keliye stadium ko gaya
</bodyText>
<page confidence="0.89968">
1276
</page>
<bodyText confidence="0.999920571428571">
Another difference is that Urdu uses post-
positions unlike English which uses prepositions.
This can also lead to long range movements de-
pending on the length of the noun phrase that the
post-position follows. The order of noun phrases
and prepositional phrases is also swapped in Urdu
as compared with English.
</bodyText>
<sectionHeader confidence="0.993524" genericHeader="method">
3 Reordering model
</sectionHeader>
<bodyText confidence="0.999968533333333">
In this section we briefly describe the reordering
model (Visweswariah et al., 2011) that forms the
basis of our work. We also describe an approx-
imation we make in the training process that sig-
nificantly speeds up the training without much loss
of accuracy which enables training on much larger
data sets. Consider a source sentence w that we
would like to reorder to match the target order. Let
π represent a candidate permutation of the source
sentence w. πi denotes the index of the word in the
source sentence that maps to position i in the can-
didate reordering, thus reordering with this candi-
date permutation π we will reorder the sentence
w to wπ1, wπ2, ..., wπn. The reordering model we
use assigns costs to candidate permutations as:
</bodyText>
<equation confidence="0.9967695">
C(π|w) = � c(πi−1, πi).
i
</equation>
<bodyText confidence="0.994316611111111">
The costs c(m, n) are pairwise costs of putting
wm immediately before wn in the reordering. We
reorder the sentence w according to the permu-
tation π that minimizes the cost C(π|w). We
find the minimal cost permutation by converting
the problem into a symmetric Travelling Salesman
Problem (TSP) and then using an implementation
of the chained Lin-Kernighan heuristic (Applegate
et al., 2003). The costs in the reordering model
c(m, n) are parameterized by a linear model:
c(m, n) = θTΦ(w, m, n)
where θ is a learned vector of weights and Φ is a
vector of binary feature functions that inspect the
words and POS tags of the source sentence at and
around positions m and n. We use the features
(Φ) described in Visweswariah et al. (2011) that
were based on features used in dependency pars-
ing (McDonald et al., 2005a).
To learn the weight vector θ we require a cor-
pus of sentences w with their desired reorderings
π*. Past work Visweswariah et al. (2011) used
high quality manual word alignments to derive the
desired reorderings π* as follows. Given word
aligned source and target sentences, we drop the
source words that are not aligned1. Let mi be the
mean of the target word positions that the source
word at index i is aligned to. We then sort the
source indices in increasing order of mi (this order
defines π*). If mi = mj (for example, because wi
and wj are aligned to the same set of words) we
keep them in the same order that they occurred in
the source sentence.
We used the single best Margin Infused Relaxed
Algorithm (MIRA) (McDonald et al. (2005b),
Crammer and Singer (2003)) with online updates
to our parameters given by:
</bodyText>
<equation confidence="0.997989666666667">
θi+1 = arg min ||θ − θi||
θ
s.t. C(π*|w) &lt; C(ˆπ|w) − L(π*, ˆπ).
</equation>
<bodyText confidence="0.99986772">
In the equation above, πˆ = arg min, C(π|w) is
the best reordering based on the current parameter
value θi and L is a loss function. We take L to be
the number of words for which the hypothesized
permutation πˆ has a different preceding word as
compared with the reference permutation π*.
In this paper we focus on the case where in ad-
dition to using a relatively small number of man-
ual word aligned sentences to derive the refer-
ence permutations π* used to train our model,
we would like to use more abundant but nois-
ier machine aligned sentence pairs. To handle
the larger amount of training data we obtain from
machine alignments, we make an approximation
in training that we found empirically to not af-
fect performance but that makes training faster
by more than a factor of five. This allows us
to train the reordering model with roughly 150K
sentences in about two hours. The approximation
we make is that instead of using the chained Lin-
Kernighan heuristic to solve the TSP problem to
find πˆ = arg min, C(π|w), we select greedily
for each word the preceding word that has the low-
est cost2. Using ψi to denote arg minj c(j, i) and
letting
</bodyText>
<equation confidence="0.9812195">
C(ψ|w) = � c(ψi, i),
i
</equation>
<footnote confidence="0.746357714285714">
1Note that the unaligned source words are dropped only at
the time of training. At the time of testing all source words are
retained as the alignment information is obviously not avail-
able at test time.
2It should be noted that this approximation was done only
at the time of training. At the time of testing we still use the
chained Lin-Kernighan heuristic to solve the TSP problem.
</footnote>
<page confidence="0.984656">
1277
</page>
<figure confidence="0.647037125">
we do the update according to: Step 1: Train reordering models
using manual word alignments
θz+1 = arg min ||θ − θz ||C(7rs|ws) C(7rt|wt)
θ
Step 2: Feed predictions
of the reordering models
to the alignment model
s.t. C(π∗|w) &lt; C(ψ|w) − L(π∗,ψ).
</figure>
<bodyText confidence="0.981632">
Again the loss L(π∗, ψ) is the number of positions
i for which 7r∗z−1 is different fromψz−1.
</bodyText>
<sectionHeader confidence="0.6043505" genericHeader="method">
4 Generating reference reordering from
parallel sentences
</sectionHeader>
<bodyText confidence="0.999955628571429">
The main aim of our work is to improve the re-
ordering model by using parallel sentences for
which manual word alignments are not avail-
able. In other words, we want to generate rel-
atively clean reference reorderings from parallel
sentences and use them for training a reordering
model. A straightforward approach for this is to
use a supervised aligner to align the words in the
sentences and then derive the reference reordering
as we do for manual word alignments. However,
as we will see in the experimental results, the qual-
ity of a reordering model trained from automatic
alignments is very sensitive to the quality of align-
ments. This motivated us to explore if we can fur-
ther improve our aligner and the method for gen-
erating reference reorderings given alignments.
We improve upon the above mentioned ba-
sic approach by coupling the tasks of reorder-
ing and word alignment. We do this by build-
ing a reordering model (C(πs|ws, wt, a)) that
scores reorderings πs given the source sentence
ws, target sentence wt and machine alignments
a. Complementing this model, we build an align-
ment model (P(a|ws, wt, πs, πt)) that scores
alignments a given the source and target sen-
tences and their predicted reorderings according to
source and target reordering models. The model
(C(πs|ws, wt, a)) helps to produce better refer-
ence reorderings for training our final reordering
model given fixed machine alignments and the
alignment model (P(a|ws, wt, πs, πt)) helps im-
prove the machine alignments taking into account
information from reordering models. In the fol-
lowing sections, we describe our overall approach
followed by a description of the two models.
</bodyText>
<subsectionHeader confidence="0.771458">
4.1 Overall approach to generating training
data
</subsectionHeader>
<bodyText confidence="0.999994">
We first describe our overall approach to gen-
erating training data for the reordering model
given a small corpus of sentences with manual
</bodyText>
<equation confidence="0.9701635">
P(a|ws, wt, 7rs, 7rt)
C(7rt|wt, a)
</equation>
<figureCaption confidence="0.868779">
Figure 2: Overall approach: Building a sequence
of reordering and alignment models.
</figureCaption>
<bodyText confidence="0.9929442">
word alignments (H) and a much larger corpus of
parallel sentences (U) that are not word aligned.
The basic idea is to chain together the two models,
viz., reordering model and alignment model, as
illustrated in Figure 2. The steps involved are as
described below:
Step 1: First, we use manual word alignments
(H) to train source and target reordering models
as described in (Visweswariah et al., 2011).
Step 2: Next, we use the hand alignments to train
an alignment model P(a|ws, wt, πs, πt). In
addition to the original source and target sentence,
we also feed the predictions of the reordering
model trained in Step 1 to this alignment model
(see section 4.2 for details of the model itself).
Step 3: Finally, we use the predictions of the
alignment model trained in Step 2 to train reorder-
ing models C(πs|ws, wt, a) (see section 4.3 for
details on the reordering model itself).
After building the sequence of models shown in
Figure 2, we apply them in sequence on the un-
aligned parallel data U, starting with the reorder-
ing models C(πs|ws) and C(πt|wt). The re-
orderings obtained for the source side in U (after
applying the final model C(πs|ws, a)) are used
along with reference reorderings obtained from
the manual word alignments to train our reorder-
ing model. Note that, in theory, we could iterate
over steps 2 and 3 several times but, in practice
we did not see a benefit of going beyond one iter-
</bodyText>
<table confidence="0.46785325">
Step 3: Feed predictions
of the alignment model
to the reordering models
C(7rs|ws, a)
</table>
<page confidence="0.957515">
1278
</page>
<bodyText confidence="0.998546458333333">
ation in our experiments. Also, since we are inter-
ested only in the source side reorderings produced
by the model C(πs|ws,a), the target reordering
model C(πt|wt,a) is needed only if we iterate
over steps 2 and 3.
We now point to some practical considerations
of our approach. Consider the case when we are
training an alignment model conditioned on re-
orderings (P(a|ws, wt, πs, πt)). If the reorder-
ing model that generated these reorderings πs, πt
were trained on the same data that we are using
to train the alignment model, then the reorder-
ings would be much better than we would ex-
pect on unseen test data, and hence the align-
ment model (P(a|ws, wt, πs, πt)) may learn to
make the alignment overly consistent with the re-
orderings πs and πt. To counter this problem,
we divide the training data H into K parts and
at each stage we apply a model (reordering or
alignment) on part i that had not seen part i in
training. This ensures that the alignment model
does not see very optimistic reorderings and vice
versa. We now describe the individual models,
viz., P(a|ws, wt, πs, πt) and C(πs|ws, a).
</bodyText>
<subsectionHeader confidence="0.999071">
4.2 Modeling alignments given reordering
</subsectionHeader>
<bodyText confidence="0.999886633333333">
In this section we describe how we fuse informa-
tion from source and target reordering models to
improve word alignments.
As a base model we use the correction model
for word alignments proposed by McCarley et
al. (2011). This model was significantly better
than the MaxEnt aligner (Ittycheriah and Roukos,
2005) and is also flexible in the sense that it allows
for arbitrary features to be introduced while still
keeping training and decoding tractable by using a
greedy decoding algorithm that explores potential
alignments in a small neighborhood of the current
alignment. The model thus needs a reasonably
good initial alignment to start with for which we
use the MaxEnt aligner (Ittycheriah and Roukos,
2005) as in McCarley et al. (2011).
The correction model is a log-linear model:
the Model 1 probabilities between pairs of words
linked in the alignment a, features that inspect
source and target POS tags and parses (if avail-
able) and features that inspect the alignments of
adjacent words in the source and target sentence.
To incorporate information from the reorder-
ing model, we add features that use the predicted
source πs and target permutations πt. We intro-
duce some notation to describe these features. Let
5m and 5n be the set of indices of target words
that ws m and wsn are aligned to respectively. We de-
fine the minimum signed distance (msd) between
these two sets as:
</bodyText>
<equation confidence="0.99417925">
msd(5m, 5n) = i* − j*
where, *
(i ,j* ) = arg l |i − j|
(i,j)ES XSn
</equation>
<bodyText confidence="0.999990266666667">
We quantize and encode with binary features
the minimum signed distance between the sets of
the indices of the target words that source words
adjacent in the reordering πs (wsπs and wsπ s ) are
aligned to. We instantiate similar features with the
roles of source and target sentences reversed. With
this addition of features we use the same training
and testing procedure as in McCarley et al. (2011).
If the reorderings πs were perfect we would learn
to only allow alignments where wsπs i and wsπs
entenwere aligned to adjacent words in the target sen-
tence.
ce. Although the reordering model is not per-
fect, preferring alignments consistent with the re-
ordering models improves the aligner.
</bodyText>
<subsectionHeader confidence="0.999788">
4.3 Modeling reordering given alignments
</subsectionHeader>
<bodyText confidence="0.999975">
To model source permutations given source (ws)
and target (wt) sentences, and alignments (a) we
reuse the reordering model framework described
in Section 3 adding additional features capturing
the relation between a hypothesized permutation
π and alignments a. To allow for searching via
the same TSP formulation we once again assign
costs to candidate permutations as:
</bodyText>
<equation confidence="0.9769442">
C(πs|ws, wt, a) = � c(7ri−1, 7ri|ws, a).
i
P(a|ws, wt) = exp(ATφ(a, ws, wt))
Z(ws, wt)
.
</equation>
<bodyText confidence="0.988112818181818">
The As are trained using the LBFGS algorithm
(Liu et al., 1989) to maximize the log-likelihood
smoothed with L2 regularization. The feature
functions φ we start with are those used in Mc-
Carley et al. (2011) and include features encoding
Note that we introduce a dependence on the target
sentence wt only through the alignment a. Once
again we parameterize the costs by a linear model:
c(m, n) = θT4i(ws, a, m, n).
For the feature functions 4i, in addition to the
features that only depend on ws, m, n (that we
</bodyText>
<page confidence="0.978466">
1279
</page>
<bodyText confidence="0.999364083333333">
use in our standard reordering model) we add
binary indicator features based on msd(Sm, Sn)
and msd(Sm, Sn) conjoined with POS(wsm) and
POS(wsn).
Here, Sm and Sn are the set of indices of tar-
get words that ws m and wsn are aligned to respec-
tively. We conjoin the msd (minimum signed dis-
tance) with the POS tags to allow the model to cap-
ture the fact that the alignment error rate maybe
higher for some POS tags than others (e.g., we
have observed verbs have a higher error rate in
Urdu-English alignments).
Given these features we train the parameters θ
using the MIRA algorithm as described in Sec-
tion 3. Using this model, we can find the low-
est cost permutation C(πs|ws, a) using the Lin-
Kernighan heuristic as described in Section 3.
This model allows us to combine features from
the original reordering model along with informa-
tion coming from the alignments to find source re-
orderings given a parallel corpus and alignments.
We will see in the experimental section that this
improves upon the simple heuristic for deriving re-
orderings described in Section 3.
</bodyText>
<sectionHeader confidence="0.997522" genericHeader="method">
5 Experimental setup
</sectionHeader>
<bodyText confidence="0.999961956521739">
In this section we describe the experimental setup
that we used to evaluate the models proposed in
this paper. All experiments were done on Urdu-
English and we evaluate reordering in two ways:
Firstly, we evaluate reordering performance di-
rectly by comparing the reordered source sentence
in Urdu with a reference reordering obtained from
the manual word alignments using BLEU (Pap-
ineni et al., 2002) (we call this measure monolin-
gual BLEU or mBLEU). All mBLEU results are
reported on a small test set of about 400 sentences
set aside from our set of sentences with manual
word alignments. Additionally, we evaluate the ef-
fect of reordering on our final systems for machine
translation measured using BLEU.
We use about 10K sentences (180K words) of
manual word alignments which were created in
house using part of the NIST MT-08 training data3
to train our baseline reordering model and to train
our supervised machine aligners. We use a parallel
corpus of 3.9M words consisting of 1.7M words
from the NIST MT-08 training data set and 2.2M
words extracted from parallel news stories on the
</bodyText>
<footnote confidence="0.651091">
3http://www.ldc.upenn.edu
</footnote>
<bodyText confidence="0.999411904761905">
web4. The parallel corpus is used for building our
phrased based machine translation system and to
add training data for our reordering model. For
our English language model, we use the Gigaword
English corpus in addition to the English side of
our parallel corpus. Our Part-of-Speech tagger is
a Maximum Entropy Markov model tagger trained
on roughly fifty thousand words from the CRULP
corpus (Hussain, 2008).
For our machine translation experiments, we
used a standard phrase based system (Al-Onaizan
and Papineni, 2006) with a lexicalized distortion
model with a window size of +/-4 words5. To
extract phrases we use HMM alignments along
with higher quality alignments from a supervised
aligner (McCarley et al., 2011). We report results
on the (four reference) NIST MT-08 evaluation set
in Table 4 for the News and Web conditions. The
News and Web conditions each contain roughly
20K words in the test set, with the Web condition
containing more informal text from the web.
</bodyText>
<sectionHeader confidence="0.999565" genericHeader="evaluation">
6 Results and Discussions
</sectionHeader>
<bodyText confidence="0.981233956521739">
We now discuss the results of our experiments.
Need for additional data: We first show the need
for additional data in Urdu-English reordering.
Column 2 of Table 1 shows mBLEU as a function
of the number of sentences with manual word
alignments that are used to train the reordering
model. We see a roughly 3 mBLEU points drop
in performance per halving of data indicating a
potential for improvement by adding more data.
Using fewer features: We compare the perfor-
mance of a model trained using lexical features
for all words (Column 2 of Table 1) with a model
trained using lexical features only for the 1000
most frequent words (Column 3 of Table 1). The
motivation for this is to explore if a good model
can be learned even from a small amount of data if
we restrict the number of features in a reasonable
manner. However, we see that even with only
2.4K sentences with manual word alignments our
model benefits from lexical identities of more
than the 1000 most frequent words.
Effect of quality of machine alignments: We
next look at the use of automatically generated
</bodyText>
<footnote confidence="0.9370045">
4http://centralasiaonline.com
5Note that the same window size of +/-4 words was used
for all the systems, i.e., the baseline system as well as the
systems using different preordering techniques.
</footnote>
<page confidence="0.937811">
1280
</page>
<table confidence="0.998218">
Data size All features Frequent lex only
10K 52.5 50.8
5K 49.6 49.0
2.5K 46.6 46.2
</table>
<tableCaption confidence="0.998467">
Table 1: mBLEU scores for Urdu to English re-
</tableCaption>
<bodyText confidence="0.993316071428571">
ordering using different number of sentences of
manually word aligned training data with all fea-
tures and with lexical features instantiated only for
the 1000 most frequent words.
machine alignments to train the reordering model
and see the effect of aligner quality on the re-
ordering model generated using this data. These
experiments also form the baseline for the mod-
els we propose in this paper to clean up align-
ments. We experimented with two different super-
vised aligners : a maximum entropy aligner (Itty-
cheriah and Roukos, 2005) and an improved cor-
rection model that corrects the maximum entropy
alignments (McCarley et al., 2011).
</bodyText>
<table confidence="0.997386285714286">
Aligner Train size mBLEU
(words)
Type f-Measure
None - 35.5
Manual 180K 52.5
MaxEnt 70.0 3.9M 49.5
Correction model 78.1 3.9M 55.1
</table>
<tableCaption confidence="0.6984626">
Table 2: mBLEU scores for Urdu to English re-
ordering using models trained on different data
sources and tested on a development set of 8017
Urdu tokens.
Table 2 shows mBLEU scores when the re-
</tableCaption>
<bodyText confidence="0.968625821428572">
ordering model is trained on reordering references
created from aligners with different quality. We
see that the quality of the alignments matter a
great deal to the reordering model; using MaxEnt
alignments cause a degradation in performance
over just using a small set of manual word align-
ments. The alignments obtained using the aligner
of McCarley et al. (2011) are of much better
quality and hence give higher reordering perfor-
mance. Note that this reordering performance
is much better than that obtained using manual
word alignments because the size of machine
alignments is much larger (3.9M v/s 180K words).
Improvements in reordering performance us-
ing the proposed models: Table 3 shows im-
provements in the reordering model when using
the models proposed in this paper. We use H to re-
fer to the manually word aligned data and U to re-
fer to the additional sentence pairs for which man-
ual word alignments are not available. We report
the following numbers:
1. Base correction model: This is the baseline
where we use the correction model of McCar-
ley et al. (2011) for generating word alignments.
The f-Measure of this aligner is 78.1% (see row
1, column 2). Corresponding to this, we also re-
port the baseline for our reordering experiments
in the third column. Here, we first generate word
</bodyText>
<listItem confidence="0.899001454545455">
alignments for U using the aligner of McCarley et
al. (2011) and then extract reference reorderings
from these alignments. We then combine these
reference reorderings with the reference reorder-
ings derived from H and use this combined data to
train a reordering model which serves as the base-
line (mBLEU = 55.1).
2. Correction model, C(π|a): Here, once again
we generate alignments for U using the correc-
tion model of McCarley et al. (2011). However,
instead of using the basic approach of extracting
reference reorderings, we use our improved model
C(π|a) to generate reference reorderings from U.
These reference reorderings are again combined
with the reference reorderings derived from H and
used to train a reordering model (mBLEU = 56.4).
3. P(a|π), C(π|a): Here, we build the entire se-
quence of models shown in Figure 2. The align-
ment model P(a|π) is first improved by using pre-
dictions from the reordering model. These im-
proved alignments are then used to extract better
reference reorderings from U using C(π|a).
</listItem>
<bodyText confidence="0.999902571428572">
We see substantial improvements over simply
adding in the data from the machine alignments.
Improvements come roughly in equal parts from
the two techniques we proposed in this paper: (i)
using a model to generate reference reorderings
from noisy alignments and (ii) using reordering in-
formation to improve the aligner.
</bodyText>
<table confidence="0.99952675">
Method f-Measure mBLEU
Base Correction model 78.1 55.1
Correction model, C(π|a) 78.1 56.4
P(a|π), C(π|a) 80.7 57.6
</table>
<tableCaption confidence="0.9261825">
Table 3: mBLEU with different methods to gener-
ate reordering model training data from a machine
aligned parallel corpus in addition to manual word
alignments.
</tableCaption>
<bodyText confidence="0.995073166666667">
Improvements in MT performance using the
proposed models: We report results for a phrase
based system with different preordering tech-
niques. For results including a reordering model,
we simply reorder the source side Urdu data both
while training and at test time. In addition to
</bodyText>
<page confidence="0.97745">
1281
</page>
<bodyText confidence="0.999527777777778">
phrase based systems with different preordering
methods, we also report on a hierarchical phrase
based system for which we used Joshua 4.0 (Gan-
itkevitch et al., 2012). We see a significant gain of
1.8 BLEU points in machine translation by going
beyond manual word alignments using the best re-
ordering model reported in Table 3. We also note a
gain of 2.0 BLEU points over a hierarchical phrase
based system.
</bodyText>
<table confidence="0.999532">
System type MT-08 eval
Web News All
Baseline (no preordering) 18.4 25.6 22.2
Hierarchical phrase based 19.6 30.7 25.4
Reordering: Manual alignments 20.7 30.0 25.6
+ Machine alignments simple 21.3 30.9 26.4
+ machine alignments, model based 22.1 32.2 27.4
</table>
<tableCaption confidence="0.63689675">
Table 4: MT performance without preordering
(phrase based and hierarchical phrase based),
and with reordering models using different data
sources (phrase based).
</tableCaption>
<sectionHeader confidence="0.999374" genericHeader="related work">
7 Related work
</sectionHeader>
<bodyText confidence="0.999973737704918">
Dealing with the problem of handling word order
differences in machine translation has recently re-
ceived much attention. The approaches proposed
for solving this problem can be broadly divided
into 3 sets as discussed below.
The first set of approaches handle the reorder-
ing problem as part of the decoding process. Hier-
archical models (Chiang, 2007) and syntax based
models (Yamada and Knight, 2002; Galley et
al., 2006; Liu et al., 2006; Zollmann and Venu-
gopal, 2006) improve upon the simpler phrase
based models but with significant additional com-
putational cost (compared with phrase based sys-
tems) due to the inclusion of chart based parsing in
the decoding process. Syntax based models also
require a high quality source or target language
parser.
The second set of approaches rely on a source
language parser and treat reordering as a separate
process that is applied on the source language sen-
tence at training and test time before using a stan-
dard approach to machine translation. Preordering
the source data with hand written or automatically
learned rules is effective and efficient (Collins
et al., 2005; Wang et al., 2007; Ramanathan et
al., 2009; Xia and McCord, 2004; Genzel, 2010;
Visweswariah et al., 2010) but requires a source
language parser.
Recent approaches that avoid the need for a
source or target language parser and retain the ef-
ficiency of preordering models were proposed in
(Tromble and Eisner, 2009; DeNero and Uszko-
reit, 2011; Visweswariah et al., 2011; Neubig
et al., 2012). (DeNero and Uszkoreit, 2011;
Visweswariah et al., 2011; Neubig et al., 2012) fo-
cus on the use of manual word alignments to learn
preordering models and in both cases no benefit
was obtained by using the parallel corpus in ad-
dition to manual word alignments. Our work is
an extension of Visweswariah et al. (2011) and
we focus on being able to incorporate relatively
noisy machine alignments to improve the reorder-
ing model.
In addition to being related to work in reorder-
ing, our work is also more broadly related to sev-
eral other efforts which we now outline. Seti-
awan et al. (2010) proposed the use of function
word reordering to improve alignments. While
this work is similar to one of our models (model
of alignments given reordering) we differ in us-
ing a reordering model of all words (not just func-
tion words) and both source and target sentences
(not just the source sentence). The task of directly
learning a reordering model for language pairs that
are very different is closely related to the task of
parsing and hence work on semi-supervised pars-
ing (Koo et al., 2008; McClosky et al., 2006;
Suzuki et al., 2009) is broadly related to our work.
Our work coupling reordering and alignments is
also similar in spirit to approaches where parsing
and alignment are coupled (Wu, 1997).
</bodyText>
<sectionHeader confidence="0.997378" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999838058823529">
In the paper we showed that a reordering model
can benefit from data beyond a relatively small
corpus of manual word alignments. We proposed
a model that scores reorderings given alignments
and the source sentence that we use to gener-
ate cleaner training data from noisy alignments.
We also proposed a model that scores alignments
given source and target sentence reorderings that
improves a supervised alignment model by 2.6
points in f-Measure. While the improvement in
alignment performance is modest, the improve-
ment does result in improved reordering models.
Cumulatively, we see a gain of 1.8 BLEU points
over a baseline reordering model that only uses
manual word alignments, a gain of 2.0 BLEU
points over a hierarchical phrase based system,
and a gain of 5.2 BLEU points over a phrase based
</bodyText>
<page confidence="0.980003">
1282
</page>
<bodyText confidence="0.999811833333333">
system that uses no source preordering on a pub-
licly available Urdu-English test set.
As future work we would like to evaluate our
models on other language pairs. Another avenue
of future work we would like to explore is the use
of monolingual source and target data to further
assist the reordering model. We hope to be able to
learn lexical information such as how many argu-
ments a verb takes, what nouns are potential sub-
jects for a given verb by gathering statistics from
an English parser and projecting to the source lan-
guage via our word/phrase translation table.
</bodyText>
<sectionHeader confidence="0.998439" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999031557894737">
Yaser Al-Onaizan and Kishore Papineni. 2006. Dis-
tortion models for statistical machine translation. In
Proceedings ofACL, ACL-44, pages 529–536, Mor-
ristown, NJ, USA. Association for Computational
Linguistics.
David Applegate, William Cook, and Andre Rohe.
2003. Chained lin-kernighan for large traveling
salesman problems. In INFORMS Journal On Com-
puting.
David Chiang. 2007. Hierarchical phrase-based trans-
lation. Comput. Linguist., 33(2):201–228, June.
Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a.
2005. Clause restructuring for statistical machine
translation. In Proceedings ofACL, pages 531–540,
Morristown, NJ, USA. Association for Computa-
tional Linguistics.
Koby Crammer and Yoram Singer. 2003. Ultraconser-
vative online algorithms for multiclass problems. J.
Mach. Learn. Res., 3:951–991, March.
John DeNero and Jakob Uszkoreit. 2011. Inducing
sentence structure from parallel corpora for reorder-
ing. In Proceedings of the Conference on Empirical
Methods in Natural Language Processing, EMNLP
’11, pages 193–203, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
Michel Galley, Jonathan Graehl, Kevin Knight, Daniel
Marcu, Steve DeNeefe, Wei Wang, and Ignacio
Thayer. 2006. Scalable inference and training
of context-rich syntactic translation models. In
Proceedings of the 21st International Conference
on Computational Linguistics and the 44th annual
meeting of the Association for Computational Lin-
guistics, ACL-44, pages 961–968, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt
Post, and Chris Callison-Burch. 2012. Joshua 4.0:
Packing, pro, and paraphrases. In Proceedings of
the Seventh Workshop on Statistical Machine Trans-
lation, pages 283–291, Montr´eal, Canada, June. As-
sociation for Computational Linguistics.
Dmitriy Genzel. 2010. Automatically learning source-
side reordering rules for large scale machine transla-
tion. In Proceedings of the 23rd International Con-
ference on Computational Linguistics.
Sarmad Hussain. 2008. Resources for Urdu language
processing. In Proceedings of the 6th Workshop on
Asian Language Resources, IJCNLP’08.
Abraham Ittycheriah and Salim Roukos. 2005. A max-
imum entropy word aligner for Arabic-English ma-
chine translation. In Proceedings of HLT/EMNLP,
HLT ’05, pages 89–96, Stroudsburg, PA, USA. As-
sociation for Computational Linguistics.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrase-based translation. In Pro-
ceedings ofHLT-NAACL.
Terry Koo, Xavier Carreras, and Michael Collins.
2008. Simple semi-supervised dependency parsing.
In ACL, pages 595–603.
Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On
the limited memory bfgs method for large scale op-
timization. Mathematical Programming, 45:503–
528.
Yang Liu, Qun Liu, and Shouxun Lin. 2006. Tree-
to-string alignment template for statistical machine
translation. In Proceedings of the 21st International
Conference on Computational Linguistics and the
44th annual meeting of the Association for Com-
putational Linguistics, ACL-44, pages 609–616,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
J. Scott McCarley, Abraham Ittycheriah, Salim
Roukos, Bing Xiang, and Jian-ming Xu. 2011. A
correction model for word alignments. In Proceed-
ings of the Conference on Empirical Methods in Nat-
ural Language Processing, EMNLP ’11, pages 889–
898, Stroudsburg, PA, USA. Association for Com-
putational Linguistics.
David McClosky, Eugene Charniak, and Mark John-
son. 2006. Effective self-training for parsing. In
HLT-NAACL.
Ryan McDonald, Koby Crammer, and Fernando
Pereira. 2005a. Online large-margin training of de-
pendency parsers. In Proceedings of the 43rd An-
nual Meeting on Association for Computational Lin-
guistics, ACL ’05, pages 91–98, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Ryan McDonald, Fernando Pereira, Kiril Ribarov, and
Jan Hajiˇc. 2005b. Non-projective dependency pars-
ing using spanning tree algorithms. In Proceedings
ofHLT.
Graham Neubig, Taro Watanabe, and Shinsuke Mori.
2012. Inducing a discriminative parser to optimize
machine translation reordering. In Proceedings of
the 2012 Joint Conference on Empirical Methods
in Natural Language Processing and Computational
</reference>
<page confidence="0.532715">
1283
</page>
<reference confidence="0.999407939393939">
Natural Language Learning, pages 843–853, Jeju
Island, Korea, July. Association for Computational
Linguistics.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: a method for automatic eval-
uation of machine translation. In Proceedings of
the 40th Annual Meeting on Association for Com-
putational Linguistics, ACL ’02, pages 311–318,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Ananthakrishnan Ramanathan, Hansraj Choudhary,
Avishek Ghosh, and Pushpak Bhattacharyya. 2009.
Case markers and morphology: addressing the crux
of the fluency problem in English-Hindi smt. In Pro-
ceedings ofACL-IJCNLP.
Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010.
Discriminative word alignment with a function word
reordering model. In Proceedings of the 2010 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’10, pages 534–544, Strouds-
burg, PA, USA. Association for Computational Lin-
guistics.
Jun Suzuki, Hideki Isozaki, Xavier Carreras, and
Michael Collins. 2009. An empirical study of semi-
supervised structured conditional models for depen-
dency parsing. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing: Volume 2 - Volume 2, EMNLP ’09,
pages 551–560, Stroudsburg, PA, USA. Association
for Computational Linguistics.
Christoph Tillman. 2004. A unigram orientation
model for statistical machine translation. In Pro-
ceedings ofHLT-NAACL.
Roy Tromble and Jason Eisner. 2009. Learning linear
ordering problems for better translation. In Proceed-
ings ofEMNLP.
Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen,
Vijil Chenthamarakshan, and Nandakishore Kamb-
hatla. 2010. Syntax based reordering with automat-
ically derived rules for improved statistical machine
translation. In Proceedings of the 23rd International
Conference on Computational Linguistics.
Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur
Gandhe, Ananthakrishnan Ramanathan, and Jiri
Navratil. 2011. A word reordering model for im-
proved machine translation. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing, EMNLP ’11, pages 486–496,
Stroudsburg, PA, USA. Association for Computa-
tional Linguistics.
Chao Wang, Michael Collins, and Philipp Koehn.
2007. Chinese syntactic reordering for statistical
machine translation. In Proceedings of EMNLP-
CoNLL.
Dekai Wu. 1997. Stochastic inversion transduction
grammars and bilingual parsing of parallel corpora.
Comput. Linguist., 23(3):377–403, September.
Fei Xia and Michael McCord. 2004. Improving
a statistical MT system with automatically learned
rewrite patterns. In COLING.
Kenji Yamada and Kevin Knight. 2002. A decoder for
syntax-based statistical MT. In Proceedings ofACL.
Andreas Zollmann and Ashish Venugopal. 2006. Syn-
tax augmented machine translation via chart parsing.
In Proceedings on the Workshop on Statistical Ma-
chine Translation.
</reference>
<page confidence="0.99244">
1284
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.891863">
<title confidence="0.991134">Cut the noise: Mutually reinforcing reordering and alignments improved machine translation</title>
<author confidence="0.997851">Karthik Visweswariah Mitesh M Khapra Ananthakrishnan Ramanathan</author>
<affiliation confidence="0.999304">IBM Research India IBM Research India IBM Research</affiliation>
<email confidence="0.960292">v-karthik@in.ibm.commikhapra@in.ibm.comanandr42@gmail.com</email>
<abstract confidence="0.998375264705882">Preordering of a source language sentence to match target word order has proved to be useful for improving machine translation systems. Previous work has shown that a reordering model can be learned from high quality manual word alignments to improve machine translation performance. In this paper, we focus on further improving the performance of the reordering model (and thereby machine translation) by using a larger corpus of sentence aligned data for which manual word alignments are not available but automatic machine generated alignments are available. The main challenge we tackle is to generate quality data for training the reordering model in spite of the machine alignments being noisy. To mitigate the effect of noisy machine alignments, we propose a novel approach that improves reorderings produced given noisy alignments and also improves word alignments using information from the reordering model. This approach generates alignments that are 2.6 f-Measure points better than a baseline supervised aligner. The data generated allows us to train a reordering model that gives an improvement of 1.8 BLEU points on the NIST MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Yaser Al-Onaizan</author>
<author>Kishore Papineni</author>
</authors>
<title>Distortion models for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings ofACL, ACL-44,</booktitle>
<pages>529--536</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="2110" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="317" endWordPosition="321">el that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline. 1 Introduction Dealing with word order differences between source and target languages presents a significant challenge for machine translation systems. Failing to produce target words in the correct order results in machine translation output that is not fluent and is often very hard to understand. These problems are particularly severe when translating between languages which have very different structure. Phrase based systems (Koehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2</context>
<context position="23656" citStr="Al-Onaizan and Papineni, 2006" startWordPosition="3970" endWordPosition="3973">-08 training data set and 2.2M words extracted from parallel news stories on the 3http://www.ldc.upenn.edu web4. The parallel corpus is used for building our phrased based machine translation system and to add training data for our reordering model. For our English language model, we use the Gigaword English corpus in addition to the English side of our parallel corpus. Our Part-of-Speech tagger is a Maximum Entropy Markov model tagger trained on roughly fifty thousand words from the CRULP corpus (Hussain, 2008). For our machine translation experiments, we used a standard phrase based system (Al-Onaizan and Papineni, 2006) with a lexicalized distortion model with a window size of +/-4 words5. To extract phrases we use HMM alignments along with higher quality alignments from a supervised aligner (McCarley et al., 2011). We report results on the (four reference) NIST MT-08 evaluation set in Table 4 for the News and Web conditions. The News and Web conditions each contain roughly 20K words in the test set, with the Web condition containing more informal text from the web. 6 Results and Discussions We now discuss the results of our experiments. Need for additional data: We first show the need for additional data in</context>
</contexts>
<marker>Al-Onaizan, Papineni, 2006</marker>
<rawString>Yaser Al-Onaizan and Kishore Papineni. 2006. Distortion models for statistical machine translation. In Proceedings ofACL, ACL-44, pages 529–536, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Applegate</author>
<author>William Cook</author>
<author>Andre Rohe</author>
</authors>
<title>Chained lin-kernighan for large traveling salesman problems.</title>
<date>2003</date>
<journal>In INFORMS Journal On Computing.</journal>
<contexts>
<context position="9913" citStr="Applegate et al., 2003" startWordPosition="1591" endWordPosition="1594"> in the candidate reordering, thus reordering with this candidate permutation π we will reorder the sentence w to wπ1, wπ2, ..., wπn. The reordering model we use assigns costs to candidate permutations as: C(π|w) = � c(πi−1, πi). i The costs c(m, n) are pairwise costs of putting wm immediately before wn in the reordering. We reorder the sentence w according to the permutation π that minimizes the cost C(π|w). We find the minimal cost permutation by converting the problem into a symmetric Travelling Salesman Problem (TSP) and then using an implementation of the chained Lin-Kernighan heuristic (Applegate et al., 2003). The costs in the reordering model c(m, n) are parameterized by a linear model: c(m, n) = θTΦ(w, m, n) where θ is a learned vector of weights and Φ is a vector of binary feature functions that inspect the words and POS tags of the source sentence at and around positions m and n. We use the features (Φ) described in Visweswariah et al. (2011) that were based on features used in dependency parsing (McDonald et al., 2005a). To learn the weight vector θ we require a corpus of sentences w with their desired reorderings π*. Past work Visweswariah et al. (2011) used high quality manual word alignmen</context>
</contexts>
<marker>Applegate, Cook, Rohe, 2003</marker>
<rawString>David Applegate, William Cook, and Andre Rohe. 2003. Chained lin-kernighan for large traveling salesman problems. In INFORMS Journal On Computing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>Hierarchical phrase-based translation.</title>
<date>2007</date>
<journal>Comput. Linguist.,</journal>
<volume>33</volume>
<issue>2</issue>
<contexts>
<context position="30891" citStr="Chiang, 2007" startWordPosition="5175" endWordPosition="5176">ine alignments simple 21.3 30.9 26.4 + machine alignments, model based 22.1 32.2 27.4 Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a st</context>
</contexts>
<marker>Chiang, 2007</marker>
<rawString>David Chiang. 2007. Hierarchical phrase-based translation. Comput. Linguist., 33(2):201–228, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
<author>Ivona Kuˇcerov´a</author>
</authors>
<title>Clause restructuring for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings ofACL,</booktitle>
<pages>531--540</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<marker>Collins, Koehn, Kuˇcerov´a, 2005</marker>
<rawString>Michael Collins, Philipp Koehn, and Ivona Kuˇcerov´a. 2005. Clause restructuring for statistical machine translation. In Proceedings ofACL, pages 531–540, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Koby Crammer</author>
<author>Yoram Singer</author>
</authors>
<title>Ultraconservative online algorithms for multiclass problems.</title>
<date>2003</date>
<journal>J. Mach. Learn. Res.,</journal>
<volume>3</volume>
<contexts>
<context position="11109" citStr="Crammer and Singer (2003)" startWordPosition="1811" endWordPosition="1814"> quality manual word alignments to derive the desired reorderings π* as follows. Given word aligned source and target sentences, we drop the source words that are not aligned1. Let mi be the mean of the target word positions that the source word at index i is aligned to. We then sort the source indices in increasing order of mi (this order defines π*). If mi = mj (for example, because wi and wj are aligned to the same set of words) we keep them in the same order that they occurred in the source sentence. We used the single best Margin Infused Relaxed Algorithm (MIRA) (McDonald et al. (2005b), Crammer and Singer (2003)) with online updates to our parameters given by: θi+1 = arg min ||θ − θi|| θ s.t. C(π*|w) &lt; C(ˆπ|w) − L(π*, ˆπ). In the equation above, πˆ = arg min, C(π|w) is the best reordering based on the current parameter value θi and L is a loss function. We take L to be the number of words for which the hypothesized permutation πˆ has a different preceding word as compared with the reference permutation π*. In this paper we focus on the case where in addition to using a relatively small number of manual word aligned sentences to derive the reference permutations π* used to train our model, we would li</context>
</contexts>
<marker>Crammer, Singer, 2003</marker>
<rawString>Koby Crammer and Yoram Singer. 2003. Ultraconservative online algorithms for multiclass problems. J. Mach. Learn. Res., 3:951–991, March.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John DeNero</author>
<author>Jakob Uszkoreit</author>
</authors>
<title>Inducing sentence structure from parallel corpora for reordering.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>193--203</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2930" citStr="DeNero and Uszkoreit, 2011" startWordPosition="454" endWordPosition="457">and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) which uses manual word alignments for learning a reordering model. Specifically, we show that we can significantly improve reordering performance by using a large number of sentence pairs for which manual word alignments are not available. The mot</context>
<context position="31999" citStr="DeNero and Uszkoreit, 2011" startWordPosition="5355" endWordPosition="5359">ring as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alignments. Our work is an extension of Visweswariah et al. (2011) and we focus on being able to incorporate relatively noisy machine alignments to improve the reordering model. In addition to being related to work in reordering, our work is also more broadly related to several other efforts which we n</context>
</contexts>
<marker>DeNero, Uszkoreit, 2011</marker>
<rawString>John DeNero and Jakob Uszkoreit. 2011. Inducing sentence structure from parallel corpora for reordering. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 193–203, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Michel Galley</author>
<author>Jonathan Graehl</author>
<author>Kevin Knight</author>
<author>Daniel Marcu</author>
<author>Steve DeNeefe</author>
<author>Wei Wang</author>
<author>Ignacio Thayer</author>
</authors>
<title>Scalable inference and training of context-rich syntactic translation models.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>961--968</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="30961" citStr="Galley et al., 2006" startWordPosition="5185" endWordPosition="5188"> based 22.1 32.2 27.4 Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data wi</context>
</contexts>
<marker>Galley, Graehl, Knight, Marcu, DeNeefe, Wang, Thayer, 2006</marker>
<rawString>Michel Galley, Jonathan Graehl, Kevin Knight, Daniel Marcu, Steve DeNeefe, Wei Wang, and Ignacio Thayer. 2006. Scalable inference and training of context-rich syntactic translation models. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 961–968, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Juri Ganitkevitch</author>
<author>Yuan Cao</author>
<author>Jonathan Weese</author>
<author>Matt Post</author>
<author>Chris Callison-Burch</author>
</authors>
<title>Joshua 4.0: Packing, pro, and paraphrases.</title>
<date>2012</date>
<booktitle>In Proceedings of the Seventh Workshop on Statistical Machine Translation,</booktitle>
<pages>283--291</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="29867" citStr="Ganitkevitch et al., 2012" startWordPosition="5008" endWordPosition="5012">(π|a) 80.7 57.6 Table 3: mBLEU with different methods to generate reordering model training data from a machine aligned parallel corpus in addition to manual word alignments. Improvements in MT performance using the proposed models: We report results for a phrase based system with different preordering techniques. For results including a reordering model, we simply reorder the source side Urdu data both while training and at test time. In addition to 1281 phrase based systems with different preordering methods, we also report on a hierarchical phrase based system for which we used Joshua 4.0 (Ganitkevitch et al., 2012). We see a significant gain of 1.8 BLEU points in machine translation by going beyond manual word alignments using the best reordering model reported in Table 3. We also note a gain of 2.0 BLEU points over a hierarchical phrase based system. System type MT-08 eval Web News All Baseline (no preordering) 18.4 25.6 22.2 Hierarchical phrase based 19.6 30.7 25.4 Reordering: Manual alignments 20.7 30.0 25.6 + Machine alignments simple 21.3 30.9 26.4 + machine alignments, model based 22.1 32.2 27.4 Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reor</context>
</contexts>
<marker>Ganitkevitch, Cao, Weese, Post, Callison-Burch, 2012</marker>
<rawString>Juri Ganitkevitch, Yuan Cao, Jonathan Weese, Matt Post, and Chris Callison-Burch. 2012. Joshua 4.0: Packing, pro, and paraphrases. In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages 283–291, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dmitriy Genzel</author>
</authors>
<title>Automatically learning sourceside reordering rules for large scale machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2686" citStr="Genzel, 2010" startWordPosition="418" endWordPosition="419">odels (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) whi</context>
<context position="31736" citStr="Genzel, 2010" startWordPosition="5315" endWordPosition="5316">rase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alignments. Our work is an extension of </context>
</contexts>
<marker>Genzel, 2010</marker>
<rawString>Dmitriy Genzel. 2010. Automatically learning sourceside reordering rules for large scale machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sarmad Hussain</author>
</authors>
<title>Resources for Urdu language processing.</title>
<date>2008</date>
<booktitle>In Proceedings of the 6th Workshop on Asian Language Resources, IJCNLP’08.</booktitle>
<contexts>
<context position="23543" citStr="Hussain, 2008" startWordPosition="3956" endWordPosition="3957">achine aligners. We use a parallel corpus of 3.9M words consisting of 1.7M words from the NIST MT-08 training data set and 2.2M words extracted from parallel news stories on the 3http://www.ldc.upenn.edu web4. The parallel corpus is used for building our phrased based machine translation system and to add training data for our reordering model. For our English language model, we use the Gigaword English corpus in addition to the English side of our parallel corpus. Our Part-of-Speech tagger is a Maximum Entropy Markov model tagger trained on roughly fifty thousand words from the CRULP corpus (Hussain, 2008). For our machine translation experiments, we used a standard phrase based system (Al-Onaizan and Papineni, 2006) with a lexicalized distortion model with a window size of +/-4 words5. To extract phrases we use HMM alignments along with higher quality alignments from a supervised aligner (McCarley et al., 2011). We report results on the (four reference) NIST MT-08 evaluation set in Table 4 for the News and Web conditions. The News and Web conditions each contain roughly 20K words in the test set, with the Web condition containing more informal text from the web. 6 Results and Discussions We no</context>
</contexts>
<marker>Hussain, 2008</marker>
<rawString>Sarmad Hussain. 2008. Resources for Urdu language processing. In Proceedings of the 6th Workshop on Asian Language Resources, IJCNLP’08.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
</authors>
<title>A maximum entropy word aligner for Arabic-English machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of HLT/EMNLP, HLT ’05,</booktitle>
<pages>89--96</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="18082" citStr="Ittycheriah and Roukos, 2005" startWordPosition="3024" endWordPosition="3027">ch stage we apply a model (reordering or alignment) on part i that had not seen part i in training. This ensures that the alignment model does not see very optimistic reorderings and vice versa. We now describe the individual models, viz., P(a|ws, wt, πs, πt) and C(πs|ws, a). 4.2 Modeling alignments given reordering In this section we describe how we fuse information from source and target reordering models to improve word alignments. As a base model we use the correction model for word alignments proposed by McCarley et al. (2011). This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment. The model thus needs a reasonably good initial alignment to start with for which we use the MaxEnt aligner (Ittycheriah and Roukos, 2005) as in McCarley et al. (2011). The correction model is a log-linear model: the Model 1 probabilities between pairs of words linked in the alignment a, features that inspect source and target POS tags</context>
<context position="26076" citStr="Ittycheriah and Roukos, 2005" startWordPosition="4383" endWordPosition="4387">x only 10K 52.5 50.8 5K 49.6 49.0 2.5K 46.6 46.2 Table 1: mBLEU scores for Urdu to English reordering using different number of sentences of manually word aligned training data with all features and with lexical features instantiated only for the 1000 most frequent words. machine alignments to train the reordering model and see the effect of aligner quality on the reordering model generated using this data. These experiments also form the baseline for the models we propose in this paper to clean up alignments. We experimented with two different supervised aligners : a maximum entropy aligner (Ittycheriah and Roukos, 2005) and an improved correction model that corrects the maximum entropy alignments (McCarley et al., 2011). Aligner Train size mBLEU (words) Type f-Measure None - 35.5 Manual 180K 52.5 MaxEnt 70.0 3.9M 49.5 Correction model 78.1 3.9M 55.1 Table 2: mBLEU scores for Urdu to English reordering using models trained on different data sources and tested on a development set of 8017 Urdu tokens. Table 2 shows mBLEU scores when the reordering model is trained on reordering references created from aligners with different quality. We see that the quality of the alignments matter a great deal to the reorderi</context>
</contexts>
<marker>Ittycheriah, Roukos, 2005</marker>
<rawString>Abraham Ittycheriah and Salim Roukos. 2005. A maximum entropy word aligner for Arabic-English machine translation. In Proceedings of HLT/EMNLP, HLT ’05, pages 89–96, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrase-based translation.</title>
<date>2003</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="2045" citStr="Koehn et al., 2003" startWordPosition="309" endWordPosition="312">MT-08 Urdu-English evaluation set over a reordering model that only uses manual word alignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline. 1 Introduction Dealing with word order differences between source and target languages presents a significant challenge for machine translation systems. Failing to produce target words in the correct order results in machine translation output that is not fluent and is often very hard to understand. These problems are particularly severe when translating between languages which have very different structure. Phrase based systems (Koehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al.,</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrase-based translation. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Terry Koo</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>Simple semi-supervised dependency parsing.</title>
<date>2008</date>
<booktitle>In ACL,</booktitle>
<pages>595--603</pages>
<contexts>
<context position="33128" citStr="Koo et al., 2008" startWordPosition="5554" endWordPosition="5557">n reordering, our work is also more broadly related to several other efforts which we now outline. Setiawan et al. (2010) proposed the use of function word reordering to improve alignments. While this work is similar to one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al., 2008; McClosky et al., 2006; Suzuki et al., 2009) is broadly related to our work. Our work coupling reordering and alignments is also similar in spirit to approaches where parsing and alignment are coupled (Wu, 1997). 8 Conclusion In the paper we showed that a reordering model can benefit from data beyond a relatively small corpus of manual word alignments. We proposed a model that scores reorderings given alignments and the source sentence that we use to generate cleaner training data from noisy alignments. We also proposed a model that scores alignments given source and target sentence reorderin</context>
</contexts>
<marker>Koo, Carreras, Collins, 2008</marker>
<rawString>Terry Koo, Xavier Carreras, and Michael Collins. 2008. Simple semi-supervised dependency parsing. In ACL, pages 595–603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dong C Liu</author>
<author>Jorge Nocedal</author>
<author>C Dong</author>
</authors>
<title>On the limited memory bfgs method for large scale optimization.</title>
<date>1989</date>
<booktitle>Mathematical Programming,</booktitle>
<pages>45--503</pages>
<contexts>
<context position="20481" citStr="Liu et al., 1989" startWordPosition="3433" endWordPosition="3436">tent with the reordering models improves the aligner. 4.3 Modeling reordering given alignments To model source permutations given source (ws) and target (wt) sentences, and alignments (a) we reuse the reordering model framework described in Section 3 adding additional features capturing the relation between a hypothesized permutation π and alignments a. To allow for searching via the same TSP formulation we once again assign costs to candidate permutations as: C(πs|ws, wt, a) = � c(7ri−1, 7ri|ws, a). i P(a|ws, wt) = exp(ATφ(a, ws, wt)) Z(ws, wt) . The As are trained using the LBFGS algorithm (Liu et al., 1989) to maximize the log-likelihood smoothed with L2 regularization. The feature functions φ we start with are those used in McCarley et al. (2011) and include features encoding Note that we introduce a dependence on the target sentence wt only through the alignment a. Once again we parameterize the costs by a linear model: c(m, n) = θT4i(ws, a, m, n). For the feature functions 4i, in addition to the features that only depend on ws, m, n (that we 1279 use in our standard reordering model) we add binary indicator features based on msd(Sm, Sn) and msd(Sm, Sn) conjoined with POS(wsm) and POS(wsn). He</context>
</contexts>
<marker>Liu, Nocedal, Dong, 1989</marker>
<rawString>Dong C. Liu, Jorge Nocedal, and Dong C. 1989. On the limited memory bfgs method for large scale optimization. Mathematical Programming, 45:503– 528.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yang Liu</author>
<author>Qun Liu</author>
<author>Shouxun Lin</author>
</authors>
<title>Treeto-string alignment template for statistical machine translation.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44,</booktitle>
<pages>609--616</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="30979" citStr="Liu et al., 2006" startWordPosition="5189" endWordPosition="5192"> Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or</context>
</contexts>
<marker>Liu, Liu, Lin, 2006</marker>
<rawString>Yang Liu, Qun Liu, and Shouxun Lin. 2006. Treeto-string alignment template for statistical machine translation. In Proceedings of the 21st International Conference on Computational Linguistics and the 44th annual meeting of the Association for Computational Linguistics, ACL-44, pages 609–616, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Scott McCarley</author>
<author>Abraham Ittycheriah</author>
<author>Salim Roukos</author>
<author>Bing Xiang</author>
<author>Jian-ming Xu</author>
</authors>
<title>A correction model for word alignments.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>889--898</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="5119" citStr="McCarley et al., 2011" startWordPosition="807" endWordPosition="810"> time consuming and expensive. The challenge in going beyond manual word alignments and using machine alignments is the noise in the machine alignments which affects the performance of the reordering model (see Section 5). We illustrate this with the help of a motivating example. Consider the example English sentence and its translation shown in Figure 1. Figure 1: An example English sentence with its Urdu translation with alignment links. Red (dotted) links are incorrect links while the blue (dashed) links are the corresponding correct links. A standard word alignment algorithm that we used (McCarley et al., 2011) made the mistake of mis-aligning the Urdu ko and keliye (it switched the two). Deriving reference reorderings from these wrong alignments would give us an incorrect reordering. A reordering model trained on such incorrect reorderings would obviously perform poorly. Our task is thus two-fold (i) improve the quality of machine alignments (ii) use these less noisy alignments to derive cleaner training data for a reordering model. Before proceeding, we first point out that the two tasks, viz., reordering and word alignment are related: Having perfect reordering makes the alignment task easier whi</context>
<context position="17990" citStr="McCarley et al. (2011)" startWordPosition="3011" endWordPosition="3014">and πt. To counter this problem, we divide the training data H into K parts and at each stage we apply a model (reordering or alignment) on part i that had not seen part i in training. This ensures that the alignment model does not see very optimistic reorderings and vice versa. We now describe the individual models, viz., P(a|ws, wt, πs, πt) and C(πs|ws, a). 4.2 Modeling alignments given reordering In this section we describe how we fuse information from source and target reordering models to improve word alignments. As a base model we use the correction model for word alignments proposed by McCarley et al. (2011). This model was significantly better than the MaxEnt aligner (Ittycheriah and Roukos, 2005) and is also flexible in the sense that it allows for arbitrary features to be introduced while still keeping training and decoding tractable by using a greedy decoding algorithm that explores potential alignments in a small neighborhood of the current alignment. The model thus needs a reasonably good initial alignment to start with for which we use the MaxEnt aligner (Ittycheriah and Roukos, 2005) as in McCarley et al. (2011). The correction model is a log-linear model: the Model 1 probabilities betwee</context>
<context position="19627" citStr="McCarley et al. (2011)" startWordPosition="3294" endWordPosition="3297"> the set of indices of target words that ws m and wsn are aligned to respectively. We define the minimum signed distance (msd) between these two sets as: msd(5m, 5n) = i* − j* where, * (i ,j* ) = arg l |i − j| (i,j)ES XSn We quantize and encode with binary features the minimum signed distance between the sets of the indices of the target words that source words adjacent in the reordering πs (wsπs and wsπ s ) are aligned to. We instantiate similar features with the roles of source and target sentences reversed. With this addition of features we use the same training and testing procedure as in McCarley et al. (2011). If the reorderings πs were perfect we would learn to only allow alignments where wsπs i and wsπs entenwere aligned to adjacent words in the target sentence. ce. Although the reordering model is not perfect, preferring alignments consistent with the reordering models improves the aligner. 4.3 Modeling reordering given alignments To model source permutations given source (ws) and target (wt) sentences, and alignments (a) we reuse the reordering model framework described in Section 3 adding additional features capturing the relation between a hypothesized permutation π and alignments a. To allo</context>
<context position="23855" citStr="McCarley et al., 2011" startWordPosition="4002" endWordPosition="4005">dd training data for our reordering model. For our English language model, we use the Gigaword English corpus in addition to the English side of our parallel corpus. Our Part-of-Speech tagger is a Maximum Entropy Markov model tagger trained on roughly fifty thousand words from the CRULP corpus (Hussain, 2008). For our machine translation experiments, we used a standard phrase based system (Al-Onaizan and Papineni, 2006) with a lexicalized distortion model with a window size of +/-4 words5. To extract phrases we use HMM alignments along with higher quality alignments from a supervised aligner (McCarley et al., 2011). We report results on the (four reference) NIST MT-08 evaluation set in Table 4 for the News and Web conditions. The News and Web conditions each contain roughly 20K words in the test set, with the Web condition containing more informal text from the web. 6 Results and Discussions We now discuss the results of our experiments. Need for additional data: We first show the need for additional data in Urdu-English reordering. Column 2 of Table 1 shows mBLEU as a function of the number of sentences with manual word alignments that are used to train the reordering model. We see a roughly 3 mBLEU po</context>
<context position="26178" citStr="McCarley et al., 2011" startWordPosition="4400" endWordPosition="4403">ifferent number of sentences of manually word aligned training data with all features and with lexical features instantiated only for the 1000 most frequent words. machine alignments to train the reordering model and see the effect of aligner quality on the reordering model generated using this data. These experiments also form the baseline for the models we propose in this paper to clean up alignments. We experimented with two different supervised aligners : a maximum entropy aligner (Ittycheriah and Roukos, 2005) and an improved correction model that corrects the maximum entropy alignments (McCarley et al., 2011). Aligner Train size mBLEU (words) Type f-Measure None - 35.5 Manual 180K 52.5 MaxEnt 70.0 3.9M 49.5 Correction model 78.1 3.9M 55.1 Table 2: mBLEU scores for Urdu to English reordering using models trained on different data sources and tested on a development set of 8017 Urdu tokens. Table 2 shows mBLEU scores when the reordering model is trained on reordering references created from aligners with different quality. We see that the quality of the alignments matter a great deal to the reordering model; using MaxEnt alignments cause a degradation in performance over just using a small set of ma</context>
<context position="27573" citStr="McCarley et al. (2011)" startWordPosition="4637" endWordPosition="4641">this reordering performance is much better than that obtained using manual word alignments because the size of machine alignments is much larger (3.9M v/s 180K words). Improvements in reordering performance using the proposed models: Table 3 shows improvements in the reordering model when using the models proposed in this paper. We use H to refer to the manually word aligned data and U to refer to the additional sentence pairs for which manual word alignments are not available. We report the following numbers: 1. Base correction model: This is the baseline where we use the correction model of McCarley et al. (2011) for generating word alignments. The f-Measure of this aligner is 78.1% (see row 1, column 2). Corresponding to this, we also report the baseline for our reordering experiments in the third column. Here, we first generate word alignments for U using the aligner of McCarley et al. (2011) and then extract reference reorderings from these alignments. We then combine these reference reorderings with the reference reorderings derived from H and use this combined data to train a reordering model which serves as the baseline (mBLEU = 55.1). 2. Correction model, C(π|a): Here, once again we generate al</context>
</contexts>
<marker>McCarley, Ittycheriah, Roukos, Xiang, Xu, 2011</marker>
<rawString>J. Scott McCarley, Abraham Ittycheriah, Salim Roukos, Bing Xiang, and Jian-ming Xu. 2011. A correction model for word alignments. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 889– 898, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David McClosky</author>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Effective self-training for parsing.</title>
<date>2006</date>
<booktitle>In HLT-NAACL.</booktitle>
<contexts>
<context position="33151" citStr="McClosky et al., 2006" startWordPosition="5558" endWordPosition="5561">work is also more broadly related to several other efforts which we now outline. Setiawan et al. (2010) proposed the use of function word reordering to improve alignments. While this work is similar to one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al., 2008; McClosky et al., 2006; Suzuki et al., 2009) is broadly related to our work. Our work coupling reordering and alignments is also similar in spirit to approaches where parsing and alignment are coupled (Wu, 1997). 8 Conclusion In the paper we showed that a reordering model can benefit from data beyond a relatively small corpus of manual word alignments. We proposed a model that scores reorderings given alignments and the source sentence that we use to generate cleaner training data from noisy alignments. We also proposed a model that scores alignments given source and target sentence reorderings that improves a supe</context>
</contexts>
<marker>McClosky, Charniak, Johnson, 2006</marker>
<rawString>David McClosky, Eugene Charniak, and Mark Johnson. 2006. Effective self-training for parsing. In HLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Koby Crammer</author>
<author>Fernando Pereira</author>
</authors>
<title>Online large-margin training of dependency parsers.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05,</booktitle>
<pages>91--98</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="10335" citStr="McDonald et al., 2005" startWordPosition="1671" endWordPosition="1674"> minimal cost permutation by converting the problem into a symmetric Travelling Salesman Problem (TSP) and then using an implementation of the chained Lin-Kernighan heuristic (Applegate et al., 2003). The costs in the reordering model c(m, n) are parameterized by a linear model: c(m, n) = θTΦ(w, m, n) where θ is a learned vector of weights and Φ is a vector of binary feature functions that inspect the words and POS tags of the source sentence at and around positions m and n. We use the features (Φ) described in Visweswariah et al. (2011) that were based on features used in dependency parsing (McDonald et al., 2005a). To learn the weight vector θ we require a corpus of sentences w with their desired reorderings π*. Past work Visweswariah et al. (2011) used high quality manual word alignments to derive the desired reorderings π* as follows. Given word aligned source and target sentences, we drop the source words that are not aligned1. Let mi be the mean of the target word positions that the source word at index i is aligned to. We then sort the source indices in increasing order of mi (this order defines π*). If mi = mj (for example, because wi and wj are aligned to the same set of words) we keep them in</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>Ryan McDonald, Koby Crammer, and Fernando Pereira. 2005a. Online large-margin training of dependency parsers. In Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics, ACL ’05, pages 91–98, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ryan McDonald</author>
<author>Fernando Pereira</author>
<author>Kiril Ribarov</author>
<author>Jan Hajiˇc</author>
</authors>
<title>Non-projective dependency parsing using spanning tree algorithms.</title>
<date>2005</date>
<booktitle>In Proceedings ofHLT.</booktitle>
<marker>McDonald, Pereira, Ribarov, Hajiˇc, 2005</marker>
<rawString>Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Hajiˇc. 2005b. Non-projective dependency parsing using spanning tree algorithms. In Proceedings ofHLT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Graham Neubig</author>
<author>Taro Watanabe</author>
<author>Shinsuke Mori</author>
</authors>
<title>Inducing a discriminative parser to optimize machine translation reordering.</title>
<date>2012</date>
<booktitle>In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,</booktitle>
<pages>843--853</pages>
<institution>Jeju Island, Korea, July. Association for Computational Linguistics.</institution>
<contexts>
<context position="2979" citStr="Neubig et al., 2012" startWordPosition="462" endWordPosition="465">longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) which uses manual word alignments for learning a reordering model. Specifically, we show that we can significantly improve reordering performance by using a large number of sentence pairs for which manual word alignments are not available. The motivation for going beyond manual word alignments i</context>
<context position="32048" citStr="Neubig et al., 2012" startWordPosition="5364" endWordPosition="5367">e language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alignments. Our work is an extension of Visweswariah et al. (2011) and we focus on being able to incorporate relatively noisy machine alignments to improve the reordering model. In addition to being related to work in reordering, our work is also more broadly related to several other efforts which we now outline. Setiawan et al. (2010) proposed the u</context>
</contexts>
<marker>Neubig, Watanabe, Mori, 2012</marker>
<rawString>Graham Neubig, Taro Watanabe, and Shinsuke Mori. 2012. Inducing a discriminative parser to optimize machine translation reordering. In Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pages 843–853, Jeju Island, Korea, July. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kishore Papineni</author>
<author>Salim Roukos</author>
<author>Todd Ward</author>
<author>WeiJing Zhu</author>
</authors>
<title>Bleu: a method for automatic evaluation of machine translation.</title>
<date>2002</date>
<booktitle>In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02,</booktitle>
<pages>311--318</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="22416" citStr="Papineni et al., 2002" startWordPosition="3768" endWordPosition="3772"> to find source reorderings given a parallel corpus and alignments. We will see in the experimental section that this improves upon the simple heuristic for deriving reorderings described in Section 3. 5 Experimental setup In this section we describe the experimental setup that we used to evaluate the models proposed in this paper. All experiments were done on UrduEnglish and we evaluate reordering in two ways: Firstly, we evaluate reordering performance directly by comparing the reordered source sentence in Urdu with a reference reordering obtained from the manual word alignments using BLEU (Papineni et al., 2002) (we call this measure monolingual BLEU or mBLEU). All mBLEU results are reported on a small test set of about 400 sentences set aside from our set of sentences with manual word alignments. Additionally, we evaluate the effect of reordering on our final systems for machine translation measured using BLEU. We use about 10K sentences (180K words) of manual word alignments which were created in house using part of the NIST MT-08 training data3 to train our baseline reordering model and to train our supervised machine aligners. We use a parallel corpus of 3.9M words consisting of 1.7M words from t</context>
</contexts>
<marker>Papineni, Roukos, Ward, Zhu, 2002</marker>
<rawString>Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, ACL ’02, pages 311–318, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ananthakrishnan Ramanathan</author>
<author>Hansraj Choudhary</author>
<author>Avishek Ghosh</author>
<author>Pushpak Bhattacharyya</author>
</authors>
<title>Case markers and morphology: addressing the crux of the fluency problem in English-Hindi smt.</title>
<date>2009</date>
<booktitle>In Proceedings ofACL-IJCNLP.</booktitle>
<contexts>
<context position="2650" citStr="Ramanathan et al., 2009" startWordPosition="409" endWordPosition="412">oehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approac</context>
<context position="31700" citStr="Ramanathan et al., 2009" startWordPosition="5307" endWordPosition="5310">additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alig</context>
</contexts>
<marker>Ramanathan, Choudhary, Ghosh, Bhattacharyya, 2009</marker>
<rawString>Ananthakrishnan Ramanathan, Hansraj Choudhary, Avishek Ghosh, and Pushpak Bhattacharyya. 2009. Case markers and morphology: addressing the crux of the fluency problem in English-Hindi smt. In Proceedings ofACL-IJCNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hendra Setiawan</author>
<author>Chris Dyer</author>
<author>Philip Resnik</author>
</authors>
<title>Discriminative word alignment with a function word reordering model.</title>
<date>2010</date>
<booktitle>In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10,</booktitle>
<pages>534--544</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="32633" citStr="Setiawan et al. (2010)" startWordPosition="5467" endWordPosition="5471">ah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alignments. Our work is an extension of Visweswariah et al. (2011) and we focus on being able to incorporate relatively noisy machine alignments to improve the reordering model. In addition to being related to work in reordering, our work is also more broadly related to several other efforts which we now outline. Setiawan et al. (2010) proposed the use of function word reordering to improve alignments. While this work is similar to one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al., 2008; McClosky et al., 2006; Suzuki et al., 2009) is broadly related to our work. Our work coupling reorderin</context>
</contexts>
<marker>Setiawan, Dyer, Resnik, 2010</marker>
<rawString>Hendra Setiawan, Chris Dyer, and Philip Resnik. 2010. Discriminative word alignment with a function word reordering model. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, EMNLP ’10, pages 534–544, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Xavier Carreras</author>
<author>Michael Collins</author>
</authors>
<title>An empirical study of semisupervised structured conditional models for dependency parsing.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09,</booktitle>
<pages>551--560</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="33173" citStr="Suzuki et al., 2009" startWordPosition="5562" endWordPosition="5565">ly related to several other efforts which we now outline. Setiawan et al. (2010) proposed the use of function word reordering to improve alignments. While this work is similar to one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al., 2008; McClosky et al., 2006; Suzuki et al., 2009) is broadly related to our work. Our work coupling reordering and alignments is also similar in spirit to approaches where parsing and alignment are coupled (Wu, 1997). 8 Conclusion In the paper we showed that a reordering model can benefit from data beyond a relatively small corpus of manual word alignments. We proposed a model that scores reorderings given alignments and the source sentence that we use to generate cleaner training data from noisy alignments. We also proposed a model that scores alignments given source and target sentence reorderings that improves a supervised alignment model</context>
</contexts>
<marker>Suzuki, Isozaki, Carreras, Collins, 2009</marker>
<rawString>Jun Suzuki, Hideki Isozaki, Xavier Carreras, and Michael Collins. 2009. An empirical study of semisupervised structured conditional models for dependency parsing. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP ’09, pages 551–560, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christoph Tillman</author>
</authors>
<title>A unigram orientation model for statistical machine translation.</title>
<date>2004</date>
<booktitle>In Proceedings ofHLT-NAACL.</booktitle>
<contexts>
<context position="2126" citStr="Tillman, 2004" startWordPosition="322" endWordPosition="323">lignments, and a gain of 5.2 BLEU points over a standard phrase-based baseline. 1 Introduction Dealing with word order differences between source and target languages presents a significant challenge for machine translation systems. Failing to produce target words in the correct order results in machine translation output that is not fluent and is often very hard to understand. These problems are particularly severe when translating between languages which have very different structure. Phrase based systems (Koehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, </context>
</contexts>
<marker>Tillman, 2004</marker>
<rawString>Christoph Tillman. 2004. A unigram orientation model for statistical machine translation. In Proceedings ofHLT-NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roy Tromble</author>
<author>Jason Eisner</author>
</authors>
<title>Learning linear ordering problems for better translation.</title>
<date>2009</date>
<booktitle>In Proceedings ofEMNLP.</booktitle>
<contexts>
<context position="31971" citStr="Tromble and Eisner, 2009" startWordPosition="5351" endWordPosition="5354">ge parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alignments. Our work is an extension of Visweswariah et al. (2011) and we focus on being able to incorporate relatively noisy machine alignments to improve the reordering model. In addition to being related to work in reordering, our work is also more broadly related to seve</context>
</contexts>
<marker>Tromble, Eisner, 2009</marker>
<rawString>Roy Tromble and Jason Eisner. 2009. Learning linear ordering problems for better translation. In Proceedings ofEMNLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Jiri Navratil</author>
<author>Jeffrey Sorensen</author>
<author>Vijil Chenthamarakshan</author>
<author>Nandakishore Kambhatla</author>
</authors>
<title>Syntax based reordering with automatically derived rules for improved statistical machine translation.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics.</booktitle>
<contexts>
<context position="2714" citStr="Visweswariah et al., 2010" startWordPosition="420" endWordPosition="423">zan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) which uses manual word alignmen</context>
<context position="31764" citStr="Visweswariah et al., 2010" startWordPosition="5317" endWordPosition="5320">tems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alignments. Our work is an extension of Visweswariah et al. (2011) a</context>
</contexts>
<marker>Visweswariah, Navratil, Sorensen, Chenthamarakshan, Kambhatla, 2010</marker>
<rawString>Karthik Visweswariah, Jiri Navratil, Jeffrey Sorensen, Vijil Chenthamarakshan, and Nandakishore Kambhatla. 2010. Syntax based reordering with automatically derived rules for improved statistical machine translation. In Proceedings of the 23rd International Conference on Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Karthik Visweswariah</author>
<author>Rajakrishnan Rajkumar</author>
<author>Ankur Gandhe</author>
<author>Ananthakrishnan Ramanathan</author>
<author>Jiri Navratil</author>
</authors>
<title>A word reordering model for improved machine translation.</title>
<date>2011</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11,</booktitle>
<pages>486--496</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2957" citStr="Visweswariah et al., 2011" startWordPosition="458" endWordPosition="461">red to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et al., 2011) which uses manual word alignments for learning a reordering model. Specifically, we show that we can significantly improve reordering performance by using a large number of sentence pairs for which manual word alignments are not available. The motivation for going beyond ma</context>
<context position="8837" citStr="Visweswariah et al., 2011" startWordPosition="1405" endWordPosition="1408">over long distances (depending on the number of words in the object). Phrase based systems do not capture such long distance movements well. He went to the stadium to play vaha khelne keliye stadium ko gaya 1276 Another difference is that Urdu uses postpositions unlike English which uses prepositions. This can also lead to long range movements depending on the length of the noun phrase that the post-position follows. The order of noun phrases and prepositional phrases is also swapped in Urdu as compared with English. 3 Reordering model In this section we briefly describe the reordering model (Visweswariah et al., 2011) that forms the basis of our work. We also describe an approximation we make in the training process that significantly speeds up the training without much loss of accuracy which enables training on much larger data sets. Consider a source sentence w that we would like to reorder to match the target order. Let π represent a candidate permutation of the source sentence w. πi denotes the index of the word in the source sentence that maps to position i in the candidate reordering, thus reordering with this candidate permutation π we will reorder the sentence w to wπ1, wπ2, ..., wπn. The reorderin</context>
<context position="10257" citStr="Visweswariah et al. (2011)" startWordPosition="1657" endWordPosition="1660">ntence w according to the permutation π that minimizes the cost C(π|w). We find the minimal cost permutation by converting the problem into a symmetric Travelling Salesman Problem (TSP) and then using an implementation of the chained Lin-Kernighan heuristic (Applegate et al., 2003). The costs in the reordering model c(m, n) are parameterized by a linear model: c(m, n) = θTΦ(w, m, n) where θ is a learned vector of weights and Φ is a vector of binary feature functions that inspect the words and POS tags of the source sentence at and around positions m and n. We use the features (Φ) described in Visweswariah et al. (2011) that were based on features used in dependency parsing (McDonald et al., 2005a). To learn the weight vector θ we require a corpus of sentences w with their desired reorderings π*. Past work Visweswariah et al. (2011) used high quality manual word alignments to derive the desired reorderings π* as follows. Given word aligned source and target sentences, we drop the source words that are not aligned1. Let mi be the mean of the target word positions that the source word at index i is aligned to. We then sort the source indices in increasing order of mi (this order defines π*). If mi = mj (for ex</context>
<context position="15537" citStr="Visweswariah et al., 2011" startWordPosition="2577" endWordPosition="2580">roach to generating training data for the reordering model given a small corpus of sentences with manual P(a|ws, wt, 7rs, 7rt) C(7rt|wt, a) Figure 2: Overall approach: Building a sequence of reordering and alignment models. word alignments (H) and a much larger corpus of parallel sentences (U) that are not word aligned. The basic idea is to chain together the two models, viz., reordering model and alignment model, as illustrated in Figure 2. The steps involved are as described below: Step 1: First, we use manual word alignments (H) to train source and target reordering models as described in (Visweswariah et al., 2011). Step 2: Next, we use the hand alignments to train an alignment model P(a|ws, wt, πs, πt). In addition to the original source and target sentence, we also feed the predictions of the reordering model trained in Step 1 to this alignment model (see section 4.2 for details of the model itself). Step 3: Finally, we use the predictions of the alignment model trained in Step 2 to train reordering models C(πs|ws, wt, a) (see section 4.3 for details on the reordering model itself). After building the sequence of models shown in Figure 2, we apply them in sequence on the unaligned parallel data U, sta</context>
<context position="32026" citStr="Visweswariah et al., 2011" startWordPosition="5360" endWordPosition="5363">hat is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alignments. Our work is an extension of Visweswariah et al. (2011) and we focus on being able to incorporate relatively noisy machine alignments to improve the reordering model. In addition to being related to work in reordering, our work is also more broadly related to several other efforts which we now outline. Setiawan et al.</context>
</contexts>
<marker>Visweswariah, Rajkumar, Gandhe, Ramanathan, Navratil, 2011</marker>
<rawString>Karthik Visweswariah, Rajakrishnan Rajkumar, Ankur Gandhe, Ananthakrishnan Ramanathan, and Jiri Navratil. 2011. A word reordering model for improved machine translation. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’11, pages 486–496, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Wang</author>
<author>Michael Collins</author>
<author>Philipp Koehn</author>
</authors>
<title>Chinese syntactic reordering for statistical machine translation.</title>
<date>2007</date>
<booktitle>In Proceedings of EMNLPCoNLL.</booktitle>
<contexts>
<context position="2625" citStr="Wang et al., 2007" startWordPosition="405" endWordPosition="408">se based systems (Koehn et al., 2003) use lexicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, </context>
<context position="31675" citStr="Wang et al., 2007" startWordPosition="5303" endWordPosition="5306">t with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in add</context>
</contexts>
<marker>Wang, Collins, Koehn, 2007</marker>
<rawString>Chao Wang, Michael Collins, and Philipp Koehn. 2007. Chinese syntactic reordering for statistical machine translation. In Proceedings of EMNLPCoNLL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic inversion transduction grammars and bilingual parsing of parallel corpora.</title>
<date>1997</date>
<journal>Comput. Linguist.,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="33340" citStr="Wu, 1997" startWordPosition="5591" endWordPosition="5592"> one of our models (model of alignments given reordering) we differ in using a reordering model of all words (not just function words) and both source and target sentences (not just the source sentence). The task of directly learning a reordering model for language pairs that are very different is closely related to the task of parsing and hence work on semi-supervised parsing (Koo et al., 2008; McClosky et al., 2006; Suzuki et al., 2009) is broadly related to our work. Our work coupling reordering and alignments is also similar in spirit to approaches where parsing and alignment are coupled (Wu, 1997). 8 Conclusion In the paper we showed that a reordering model can benefit from data beyond a relatively small corpus of manual word alignments. We proposed a model that scores reorderings given alignments and the source sentence that we use to generate cleaner training data from noisy alignments. We also proposed a model that scores alignments given source and target sentence reorderings that improves a supervised alignment model by 2.6 points in f-Measure. While the improvement in alignment performance is modest, the improvement does result in improved reordering models. Cumulatively, we see </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic inversion transduction grammars and bilingual parsing of parallel corpora. Comput. Linguist., 23(3):377–403, September.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fei Xia</author>
<author>Michael McCord</author>
</authors>
<title>Improving a statistical MT system with automatically learned rewrite patterns.</title>
<date>2004</date>
<booktitle>In COLING.</booktitle>
<contexts>
<context position="2672" citStr="Xia and McCord, 2004" startWordPosition="413" endWordPosition="417">xicalized distortion models (Al-Onaizan and Papineni, 2006; Tillman, 2004) and scores from the target language model to produce words in the correct order in the target language. These systems typically are only able to capture short range reorderings and the amount of data required to potentially capture longer range reordering phenomena is prohibitively large. There has been a large body of work showing the efficacy of preordering source sentences using a source parser and applying hand written or automatically learned rules (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010). Recently, approaches that address the problem of word order differences between the source and target language without requiring a high quality source or target parser have been proposed (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). These methods use a small corpus of manual word alignments (where the words in the source sentence are manually aligned to the words in the target sentence) to learn a model to preorder the source sentence to match target order. In this paper, we build upon the approach in (Visweswariah et </context>
<context position="31722" citStr="Xia and McCord, 2004" startWordPosition="5311" endWordPosition="5314">cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is effective and efficient (Collins et al., 2005; Wang et al., 2007; Ramanathan et al., 2009; Xia and McCord, 2004; Genzel, 2010; Visweswariah et al., 2010) but requires a source language parser. Recent approaches that avoid the need for a source or target language parser and retain the efficiency of preordering models were proposed in (Tromble and Eisner, 2009; DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012). (DeNero and Uszkoreit, 2011; Visweswariah et al., 2011; Neubig et al., 2012) focus on the use of manual word alignments to learn preordering models and in both cases no benefit was obtained by using the parallel corpus in addition to manual word alignments. Our work is an</context>
</contexts>
<marker>Xia, McCord, 2004</marker>
<rawString>Fei Xia and Michael McCord. 2004. Improving a statistical MT system with automatically learned rewrite patterns. In COLING.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Yamada</author>
<author>Kevin Knight</author>
</authors>
<title>A decoder for syntax-based statistical MT.</title>
<date>2002</date>
<booktitle>In Proceedings ofACL.</booktitle>
<contexts>
<context position="30940" citStr="Yamada and Knight, 2002" startWordPosition="5181" endWordPosition="5184">machine alignments, model based 22.1 32.2 27.4 Table 4: MT performance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preorderi</context>
</contexts>
<marker>Yamada, Knight, 2002</marker>
<rawString>Kenji Yamada and Kevin Knight. 2002. A decoder for syntax-based statistical MT. In Proceedings ofACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andreas Zollmann</author>
<author>Ashish Venugopal</author>
</authors>
<title>Syntax augmented machine translation via chart parsing.</title>
<date>2006</date>
<booktitle>In Proceedings on the Workshop on Statistical Machine Translation.</booktitle>
<contexts>
<context position="31010" citStr="Zollmann and Venugopal, 2006" startWordPosition="5193" endWordPosition="5197">rmance without preordering (phrase based and hierarchical phrase based), and with reordering models using different data sources (phrase based). 7 Related work Dealing with the problem of handling word order differences in machine translation has recently received much attention. The approaches proposed for solving this problem can be broadly divided into 3 sets as discussed below. The first set of approaches handle the reordering problem as part of the decoding process. Hierarchical models (Chiang, 2007) and syntax based models (Yamada and Knight, 2002; Galley et al., 2006; Liu et al., 2006; Zollmann and Venugopal, 2006) improve upon the simpler phrase based models but with significant additional computational cost (compared with phrase based systems) due to the inclusion of chart based parsing in the decoding process. Syntax based models also require a high quality source or target language parser. The second set of approaches rely on a source language parser and treat reordering as a separate process that is applied on the source language sentence at training and test time before using a standard approach to machine translation. Preordering the source data with hand written or automatically learned rules is</context>
</contexts>
<marker>Zollmann, Venugopal, 2006</marker>
<rawString>Andreas Zollmann and Ashish Venugopal. 2006. Syntax augmented machine translation via chart parsing. In Proceedings on the Workshop on Statistical Machine Translation.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>