<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.005210">
<title confidence="0.8592245">
Software testing and the naturally occurring data
assumption in natural language processing*
</title>
<author confidence="0.542583">
K. Bretonnel Cohen William A. Baumgartner, Jr. Lawrence Hunter
</author>
<sectionHeader confidence="0.986778" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.998553444444444">
It is a widely accepted belief in natural lan-
guage processing research that naturally oc-
curring data is the best (and perhaps the only
appropriate) data for testing text mining sys-
tems. This paper compares code coverage us-
ing a suite of functional tests and using a large
corpus and finds that higher class, line, and
branch coverage is achieved with structured
tests than with even a very large corpus.
</bodyText>
<sectionHeader confidence="0.999389" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.983760714285714">
In 2006, Geoffrey Chang was a star of the protein
crystallography world. That year, a crucial compo-
nent of his code was discovered to have a simple
error with large consequences for his research. The
nature of the bug was to change the signs (positive
versus negative) of two columns of the output. The
effect of this was to reverse the predicted “handed-
ness” of the structure of the molecule—an impor-
tant feature in predicting its interactions with other
molecules. The protein for his work on which Chang
was best known is an important one in predicting
things like human response to anticancer drugs and
the likelihood of bacteria developing antibiotic re-
sistance, so his work was quite influential and heav-
ily cited. The consequences for Chang were the
withdrawal of 5 papers in some of the most presti-
gious journals in the world. The consequences for
the rest of the scientific community have not been
X Bretonnel Cohen is with The MITRE Corporation. All
three co-authors are at the Center for Computational Pharma-
cology in the University of Colorado School of Medicine.
</bodyText>
<page confidence="0.972315">
23
</page>
<bodyText confidence="0.99995247368421">
quantified, but were substantial: prior to the retrac-
tions, publishing papers with results that did not
jibe with his model’s predictions was difficult, and
obtaining grants based on preliminary results that
seemed to contradict his published results was dif-
ficult as well. The Chang story (for a succinct dis-
cussion, see (Miller, 2006), and see (Chang et al.,
2006) for the retractions) is an object illustration of
the truth of Rob Knight’s observation that “For sci-
entific work, bugs don’t just mean unhappy users
who you’ll never actually meet: they mean retracted
publications and ended careers. It is critical that
your code be fully tested before you draw conclu-
sions from results it produces” (personal communi-
cation). Nonetheless, the subject of software testing
has been largely neglected in academic natural lan-
guage processing. This paper addresses one aspect
of software testing: the monitoring of testing efforts
via code coverage.
</bodyText>
<subsectionHeader confidence="0.999373">
1.1 Code coverage
</subsectionHeader>
<bodyText confidence="0.999889615384615">
Code coverage is a numerical assessment of the
amount of code that is executed during the running
of a test suite (McConnell, 2004). Although it is
by no means a completely sufficient method for de-
termining the completeness of a testing effort, it is
nonetheless a helpful member of any suite of met-
rics for assessing testing effort completeness. Code
coverage is a metric in the range 0-1.0. A value of
0.86 indicates that 86% of the code was executed
while running a given test suite. 100% coverage is
difficult to achieve for any nontrivial application, but
in general, high degrees of “uncovered” code should
lead one to suspect that there is a large amount of
</bodyText>
<subsubsectionHeader confidence="0.760366">
Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 23–30,
</subsubsectionHeader>
<page confidence="0.446518">
Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics
</page>
<bodyText confidence="0.999949774193549">
code that might harbor undetected bugs simply due
to never having been executed. A variety of code
coverage metrics exist. Line coverage indicates the
proportion of lines of code that have been executed.
It is not the most revealing form of coverage assess-
ment (Kaner et al., 1999, p. 43), but is a basic part
of any coverage measurement assessment. Branch
coverage indicates the proportion of branches within
conditionals that have been traversed (Marick, 1997,
p. 145). For example, for the conditional if $a
&amp;&amp; $b, there are two possible branches—one is tra-
versed if the expression evaluates to true, and the
other if it evaluates to false. It is more informative
than line coverage. Logic coverage (also known as
multicondition coverage (Myers, 1979) and condi-
tion coverage (Kaner et al., 1999, p. 44) indicates the
proportion of sets of variable values that have been
tried—a superset of the possible branches traversed.
For example, for the conditional if $a  ||$b,
the possible cases (assuming no short-circuit logic)
are those of the standard (logical) truth table for that
conditional. These coverage types are progressively
more informative than line coverage. Other types of
coverage are less informative than line coverage. For
example, function coverage indicates the proportion
of functions that are called. There is no guarantee
that each line in a called function is executed, and all
the more so no guarantee that branch or logic cov-
erage is achieved within it, so this type of coverage
is weaker than line coverage. With the advent of
object-oriented programming, function coverage is
sometimes replaced by class coverage—a measure
of the number of classes that are covered.
We emphasize again that code coverage is not
a sufficient metric for evaluating testing complete-
ness in isolation—for example, it is by definition
unable to detect “errors of omission,” or bugs that
consist of a failure to implement needed functional-
ity. Nonetheless, it remains a useful part of a larger
suite of metrics, and one study found that testing in
the absence of concurrent assessment of code cov-
erage typically results in only 50-60% of the code
being executed ((McConnell, 2004, p. 526), citing
Wiegers 2002).
We set out to question whether a dominant, if of-
ten unspoken, assumption of much work in contem-
porary NLP holds true: that feeding a program a
large corpus serves to exercise it adequately. We be-
gan with an information extraction application that
had been built for us by a series of contractors, with
the contractors receiving constant remote oversight
and guidance but without ongoing monitoring of the
actual code-writing. The application had benefitted
from no testing other than that done by the develop-
ers. We used a sort of “translucent-box” or “gray-
box” paradigm, meaning in this case that we treated
the program under test essentially as a black box
whose internals were inaccessible to us, but with the
exception that we inserted hooks to a coverage tool.
We then monitored three types of coverage—line
coverage, branch coverage, and class coverage—
under a variety of contrasting conditions:
</bodyText>
<listItem confidence="0.9999686">
• A set of developer-written functional tests ver-
sus a large corpus with a set of semantic rules
optimized for that corpus.
• Varying the size of the rule set.
• Varying the size of the corpus.
</listItem>
<bodyText confidence="0.9999199">
We then looked for coverage differences between
the various combinations of input data and rule sets.
In this case, the null hypothesis is that no differences
would be observed. In contrast with the null hypoth-
esis, the unspoken assumption in much NLP work
is that the null hypothesis does not hold, that the
primary determinant of coverage will be the size of
the corpus, and that the observed pattern will be that
coverage is higher with the large corpus than when
the input is not a large corpus.
</bodyText>
<sectionHeader confidence="0.950272" genericHeader="introduction">
2 Methods and materials
</sectionHeader>
<subsectionHeader confidence="0.995879">
2.1 The application under test
</subsectionHeader>
<bodyText confidence="0.999984538461539">
The application under test was an information ex-
traction application known as OpenDMAP. It is de-
scribed in detail in (Hunter et al., 2008). It achieved
the highest performance on one measure of the
protein-protein interaction task in the BioCreative
II shared task (Krallinger et al., 2007). Its use in
that task is described specifically in (Baumgartner
Jr. et al., In press). It contains 7,024 lines of code
spread across three packages (see Table 1). One
major package deals with representing the seman-
tic grammar rules themselves, while the other deals
with applying the rules to and extracting data from
arbitrary textual input. (A minor package deals with
</bodyText>
<page confidence="0.99625">
24
</page>
<table confidence="0.9728166">
Component Lines of code
Parser 3,982
Rule-handling 2,311
Configuration 731
Total 7,024
</table>
<tableCaption confidence="0.998731">
Table 1: Distribution of lines of code in the application.
</tableCaption>
<bodyText confidence="0.999921611111111">
the configuration files and is mostly not discussed in
this paper.)
The rules and patterns that the system uses are
typical “semantic grammar” rules in that they allow
the free mixing of literals and non-terminals, with
the non-terminals typically representing domain-
specific types such as “protein interaction verb.”
Non-terminals are represented as classes. Those
classes are defined in a Prot´eg´e ontology. Rules typ-
ically contain at least one element known as a slot.
Slot-fillers can be constrained by classes in the on-
tology. Input that matches a slot is extracted as one
of the participants in a relation. A limited regular
expression language can operate over classes, liter-
als, or slots. The following is a representative rule.
Square brackets indicate slots, curly braces indicate
a class, the question-mark is a regular expression op-
erator, and any other text is a literal.
</bodyText>
<equation confidence="0.840468666666667">
{c-interact} := [interactor1]
{c-interact-verb} the?
[interactor2]
</equation>
<bodyText confidence="0.999952875">
The input NEF binds PACS-2 (PMID 18296443)
would match that rule. The result would be the
recognition of a protein interaction event, with in-
teractor1 being NEF and interactor2 being PACS-2.
Not all rules utilize all possibilities of the rule lan-
guage, and we took this into account in one of our
experiments; we discuss the rules further later in the
paper in the context of that experiment.
</bodyText>
<subsectionHeader confidence="0.993054">
2.2 Materials
</subsectionHeader>
<bodyText confidence="0.997079">
In this work, we made use of the following sets of
materials.
</bodyText>
<listItem confidence="0.951966333333333">
• A large data set distributed as training data for
part of the BioCreative II shared task. It is de-
scribed in detail in (Krallinger et al., 2007).
Briefly, its domain is molecular biology, and
in particular protein-protein interactions—an
important topic of research in computational
</listItem>
<table confidence="0.9988785">
Test type Number of tests
Basic 85
Pattern/rule 67
Patterns only 90
Slots 9
Slot nesting 7
Slot property 20
Total 278
</table>
<tableCaption confidence="0.998607">
Table 2: Distribution of functional tests.
</tableCaption>
<bodyText confidence="0.999006111111111">
bioscience, with significance to a wide range
of topics in biology, including understanding
the mechanisms of human diseases (Kann et
al., 2006). The corpus contained 3,947,200
words, making it almost an order of mag-
nitude larger than the most commonly used
biomedical corpus (GENIA, at about 433K
words). This data set is publicly available via
biocreative.sourceforge.net.
</bodyText>
<listItem confidence="0.992601692307692">
• In conjunction with that data set: a set of 98
rules written in a data-driven fashion by man-
ually examining the BioCreative II data de-
scribed just above. These rules were used in the
BioCreative II shared task, where they achieved
the highest score in one category. The set of
rules is available on our SourceForge site at
bionlp.sourceforge.net.
• A set of functional tests created by the primary
developer of the system. Table 2 describes the
breakdown of the functional tests across vari-
ous aspects of the design and functionality of
the application.
</listItem>
<subsectionHeader confidence="0.999772">
2.3 Assessing coverage
</subsectionHeader>
<bodyText confidence="0.999519625">
We used the open-source Cobertura tool
(Mark Doliner, personal communication;
cobertura.sourceforge.net) to mea-
sure coverage. Cobertura reports line coverage and
branch coverage on a per-package basis and, within
each package, on a per-class basis1.
The architecture of the application is such that
Cobertura’s per-package approach resulted in three
</bodyText>
<footnote confidence="0.969146">
1Cobertura is Java-specific. PyDEV provides code coverage
analysis for Python, as does Coverage.py.
</footnote>
<page confidence="0.997918">
25
</page>
<bodyText confidence="0.999943">
sets of coverage reports: for the configuration file
processing code, for the rule-processing code, and
for the parser code. We report results for the appli-
cation as a whole, for the parser code, and for the
rule-processing code. We did note differences in the
configuration code coverage for the various condi-
tions, but it does not change the overall conclusions
of the paper and is omitted from most of the discus-
sion due to considerations of space and of general
interest.
</bodyText>
<sectionHeader confidence="0.99997" genericHeader="method">
3 Results
</sectionHeader>
<bodyText confidence="0.997697">
We conducted three separate experiments.
</bodyText>
<subsectionHeader confidence="0.9799365">
3.1 The most basic experiment: test suite
versus corpus
</subsectionHeader>
<bodyText confidence="0.999125133333333">
In the most basic experiment, we contrasted
class, line, and branch coverage when running the
developer-constructed test suite and when running
the corpus and the corpus-based rules. Tables 3 and
4 show the resulting data. As the first two lines
of Table 3 show, for the entire application (parser,
rule-handling, and configuration), line coverage was
higher with the test suite—56% versus 41%—and
branch coverage was higher as well—41% versus
28% (see the first two lines of Table 3).
We give here a more detailed discussion of the re-
sults for the entire code base. (Detailed discussions
for the parser and rule packages, including granular
assessments of class coverage, follow.)
For the parser package:
</bodyText>
<listItem confidence="0.997992888888889">
• Class coverage was higher with the test suite
than with the corpus—88% (22/25) versus 80%
(20/25).
• For the entire parser package, line coverage
was higher with the test suite than with the
corpus—55% versus 41%.
• For the entire parser package, branch cover-
age was higher with the test suite than with the
corpus—57% versus 29%.
</listItem>
<bodyText confidence="0.783234833333333">
Table 4 gives class-level data for the two main
packages. For the parser package:
• Within the 25 individual classes of the parser
package, line coverage was equal or greater
with the test suite for 21/25 classes; it was not
just equal but greater for 14/25 classes.
• Within those 21 of the 25 individual classes
that had branching logic, branch coverage was
equal or greater with the test suite for 19/21
classes, and not just equal but greater for 18/21
classes.
For the rule-handling package:
</bodyText>
<listItem confidence="0.90089225">
• Class coverage was higher with the test suite
than with the corpus—100% (20/20) versus
90% (18/20).
• For the entire rules package, line coverage was
higher with the test suite than with the corpus—
63% versus 42%.
• For the entire rules package, branch coverage
was higher with the test suite than with the
corpus—71% versus 24%.
Table 4 gives the class-level data for the rules
package:
• Within the 20 individual classes of the rules
package, line coverage was equal or greater
with the test suite for 19/20 classes, and not just
equal but greater for 6/20 classes.
• Within those 11 of the 20 individual classes
that had branching logic, branch coverage was
equal or greater with the test suite for all
11/11 classes, and not just equal but greater for
(again) all 11/11 classes.
</listItem>
<subsectionHeader confidence="0.9834435">
3.2 The second experiment: Varying the size of
the rule set
</subsectionHeader>
<bodyText confidence="0.999922181818182">
Pilot studies suggested (as later experiments veri-
fied) that the size of the input corpus had a negligible
effect on coverage. This suggested that it would be
worthwhile to assess the effect of the rule set on cov-
erage independently. We used simple ablation (dele-
tion of portions of the rule set) to vary the size of the
rule set.
We created two versions of the original rule set.
We focussed only on the non-lexical, relational pat-
tern rules, since they are completely dependent on
the lexical rules. Each version was about half the
</bodyText>
<page confidence="0.99341">
26
</page>
<table confidence="0.998930111111111">
Metric Functional tests Corpus, all rules nominal rules verbal rules
Overall line coverage 56% 41% 41% 41%
Overall branch coverage 41% 28% 28% 28%
Parser line coverage 55% 41% 41% 41%
Parser branch coverage 57% 29% 29% 29%
Rules line coverage 63% 42% 42% 42%
Rules branch coverage 71% 24% 24% 24%
Parser class coverage 88% (22/25) 80% (20/25)
Rules class coverage 100% (20/20) 90% (18/20)
</table>
<tableCaption confidence="0.995449333333334">
Table 3: Application and package-level coverage statistics using the developer’s functional tests, the full corpus with
the full set of rules, and the full corpus with two reduced sets of rules. The highest value in a row is bolded. The final
three columns are intentionally identical (see explanation in text).
</tableCaption>
<table confidence="0.997116">
Package Line coverage &gt;= Line coverage &gt; Branch coverage &gt;= Branch coverage &gt;
Classes in parser package 21/25 14/25 19/21 18/21
Classes in rules package 19/20 6/20 11/11 11/11
</table>
<tableCaption confidence="0.997873">
Table 4: When individual classes were examined, both line and branch coverage were always higher with the functional
</tableCaption>
<bodyText confidence="0.983189555555556">
tests than with the corpus. This table shows the magnitude of the differences. &gt;= indicates the number of classes that
had equal or greater coverage with the functional tests than with the corpus, and &gt; indicates just the classes that had
greater coverage with the functional tests than with the corpus.
size of the original set. The first consisted of the
first half of the rule set, which happened to consist
primarily of verb-based patterns. The second con-
sisted of the second half of the rule set, which corre-
sponded roughly to the nominalization rules.
The last two columns of Table 3 show the
package-level results. Overall, on a per-package ba-
sis, there were no differences in line or branch cov-
erage when the data was run against the full rule set
or either half of the rule set. (The identity of the last
three columns is due to this lack of difference in re-
sults between the full rule set and the two reduced
rule sets.) On a per-class level, we did note minor
differences, but as Table 3 shows, they were within
rounding error on the package level.
</bodyText>
<subsectionHeader confidence="0.993697">
3.3 The third experiment: Coverage closure
</subsectionHeader>
<bodyText confidence="0.999959727272727">
In the third experiment, we looked at how cover-
age varies as increasingly larger amounts of the cor-
pus are processed. This methodology is compara-
ble to examining the closure properties of a corpus
in a corpus linguistics study (see e.g. Chapter 6 of
(McEnery and Wilson, 2001)) (and as such may be
sensitive to the extent to which the contents of the
corpus do or do not fit the sublanguage model). We
counted cumulative line coverage as increasingly
large amounts of the corpus were processed, rang-
ing from 0 to 100% of its contents. The results for
line coverage are shown in Figure 1. (The results for
branch coverage are quite similar, and the graph is
not shown.) Line coverage for the entire application
is indicated by the thick solid line. Line coverage
for the parser package is indicated by the thin solid
line. Line coverage for the rules package is indi-
cated by the light gray solid line. The broken line
indicates the number of pattern matches—quantities
should be read off of the right y axis.
The figure shows quite graphically the lack of ef-
fect on coverage of increasing the size of the cor-
pus. For the entire application, the line coverage is
27% when an empty document has been read in, and
39% when a single sentence has been processed; it
increases by one to 40% when 51 sentences have
been processed, and has grown as high as it ever
will—41%—by the time 1,000 sentences have been
processed. Coverage at 191,478 sentences—that is,
3,947,200 words—is no higher than at 1,000 sen-
tences, and barely higher, percentagewise, than at a
single sentence.
An especially notable pattern is that the huge rise
</bodyText>
<page confidence="0.998345">
27
</page>
<figureCaption confidence="0.8360145">
Figure 1: Increase in percentage of line coverage as in-
creasing amounts of the corpus are processed. Lefty axis
is the percent coverage. The x axis is the number of sen-
tences. Right y axis (scale 0-12,000) is the number of
rule matches. The heavy solid line is coverage for the en-
tire package, the thin solid line is coverage for the parser
package, the light gray line is coverage for the rules pack-
age, and the broken line is the number of pattern matches.
</figureCaption>
<bodyText confidence="0.997392666666667">
in the number of matches to the rules (graphed by
the broken line) between 5,000 sentences and 191K
sentences has absolutely no effect on code coverage.
</bodyText>
<sectionHeader confidence="0.999219" genericHeader="method">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999986333333333">
The null hypothesis—that a synthetic test suite
and a naturalistic corpus provide the same code
coverage—is not supported by the data shown here.
Furthermore, the widely, if implicitly, held assump-
tion that a corpus would provide the best testing data
can be rejected, as well. The results reported here
are consistent with the hypothesis that code cover-
age for this application is not affected by the size of
the corpus or by the size of the rule set, and that run-
ning it on a large corpus does not guarantee thorough
testing. Rather, coverage is optimized by traditional
software testing.
</bodyText>
<sectionHeader confidence="0.772504" genericHeader="method">
4.1 Related work
</sectionHeader>
<bodyText confidence="0.999980685714286">
Although software testing is a first-class research
object in computer science, it has received little at-
tention in the natural language processing arena. A
notable exception to this comes from the grammar
engineering community. This has produced a body
of publications that includes Oepen’s work on test
suite design (Oepen et al., 1998), Volk’s work on test
suite encoding (Volk, 1998), Oepen et al.’s work on
the Redwoods project (Oepen et al., 2002), Butt and
King’s discussion of the importance of testing (Butt
and King, 2003), Flickinger et al.’s work on “seman-
tics debugging” with Redwoods data (Flickinger et
al., 2005), and Bender et al.’s recent work on test
suite generation (Bender et al., 2007). Outside of
the realm of grammar engineering, work on test-
ing for NLP is quite limited. (Cohen et al., 2004)
describes a methodology for generating test suites
for molecular biology named entity recognition sys-
tems, and (Johnson et al., 2007) describes the de-
velopment of a fault model for linguistically-based
ontology mapping, alignment, and linking systems.
However, when most researchers in the NLP com-
munity refer in print to “testing,” they do not mean
it in the sense in which that term is used in soft-
ware engineering. Some projects have publicized as-
pects of their testing work, but have not published on
their approaches: the NLTK project posts module-
level line coverage statistics, having achieved me-
dian coverage of 55% on 116 Python modules2 and
38% coverage for the project as a whole; the MAL-
LET project indicates on its web site that it en-
courages the production of unit tests during devel-
opment, but unfortunately does not go into details
of their recommendations for unit-testing machine
learning code3.
</bodyText>
<subsectionHeader confidence="0.97662">
4.2 Conclusions
</subsectionHeader>
<bodyText confidence="0.999249">
We note a number of shortcomings of code cov-
erage. For example, poor coding conventions
can actually inflate your line coverage. Con-
sider a hypothetical application consisting only
of the following, written as a single line of code
with no line breaks: if (myVariable ==
1) doSomething elsif (myVariable
== 2) doSomethingElse elsif
(myVariable = 3) doYetAnotherThing
and a poor test suite consisting only of inputs that
will cause myVariable to ever have the value 1.
The test suite will achieve 100% line coverage for
</bodyText>
<footnote confidence="0.879573333333333">
2nltk.org/doc/guides/coverage
3mallet.cs.umass.edu/index.php/
Guidelines for writing unit tests
</footnote>
<page confidence="0.997742">
28
</page>
<bodyText confidence="0.999987140350877">
this application—and without even finding the error
that sets myVariable to 3 if it is not valued 1
or 2. If the code were written with reasonable line
breaks, code coverage would be only 20%. And,
as has been noted by others, code coverage can not
detect “sins of omission”—bugs that consist of the
failure to write needed code (e.g. for error-handling
or for input validation). We do not claim that code
coverage is wholly sufficient for evaluating a test
suite; nonetheless, it is one of a number of metrics
that are helpful in judging the adequacy of a testing
effort. Another very valuable one is the found/fixed
or open/closed graph (Black, 1999; Baumgartner Jr.
et al., 2007).
While remaining aware of the potential shortcom-
ings of code coverage, we also note that the data
reported here supports its utility. The developer-
written functional tests were produced without mon-
itoring code coverage; even though those tests rou-
tinely produced higher coverage than a large corpus
of naturalistic text, they achieved less than 60% cov-
erage overall, as predicted by Wiegers’s work cited
in the introduction. We now have the opportunity to
raise that coverage via structured testing performed
by someone other than the developer. In fact, our
first attempts to test the previously unexercised code
immediately uncovered two showstopper bugs; the
coverage analysis also led us to the discovery that
the application’s error-handling code was essentially
untested.
Although we have explored a number of dimen-
sions of the space of the coverage phenomenon, ad-
ditional work could be done. We used a relatively
naive approach to rule ablation in the second experi-
ment; a more sophisticated approach would be to ab-
late specific types of rules—for example, ones that
do or don’t contain slots, ones that do or don’t con-
tain regular expression operators, etc.—and monitor
the coverage changes. (We did run all three experi-
ments on a separate, smaller corpus as a pilot study;
we report the results for the BioCreative II data set
in this paper since that is the data for which the rules
were optimized. Results in the pilot study were en-
tirely comparable.)
In conclusion: natural language processing appli-
cations are particularly susceptible to emergent phe-
nomena, such as interactions between the contents
of a rule set and the contents of a corpus. These
are especially difficult to control when the evalua-
tion corpus is naturalistic and the rule set is data-
driven. Structured testing does not eliminate this
emergent nature of the problem space, but it does
allow for controlled evaluation of the performance
of your system. Corpora also are valuable evalua-
tion resources: the combination of a structured test
suite and a naturalistic corpus provides a powerful
set of tools for finding bugs in NLP applications.
</bodyText>
<sectionHeader confidence="0.998091" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999920833333333">
The authors thank James Firby, who wrote the func-
tional tests, and Helen L. Johnson, who wrote the
rules that were used for the BioCreative data. Steve
Bethard and Aaron Cohen recommended Python
coverage tools. We also thank the three anonymous
reviewers.
</bodyText>
<sectionHeader confidence="0.999385" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9994758125">
William A. Baumgartner Jr., K. Bretonnel Cohen, Lynne
Fox, George K. Acquaah-Mensah, and Lawrence
Hunter. 2007. Manual curation is not sufficient
for annotation of genomic databases. Bioinformatics,
23:i41–i48.
William A. Baumgartner Jr., Zhiyong Lu, Helen L. John-
son, J. Gregory Caporaso, Jesse Paquette, Anna Linde-
mann, Elizabeth K. White, Olga Medvedeva, K. Bre-
tonnel Cohen, and Lawrence Hunter. In press. Con-
cept recognition for extracting protein interaction rela-
tions from biomedical text. Genome Biology.
Emily M. Bender, Laurie Poulson, Scott Drellishak, and
Chris Evans. 2007. Validation and regression test-
ing for a cross-linguistic grammar resource. In ACL
2007 Workshop on Deep Linguistic Processing, pages
136–143, Prague, Czech Republic, June. Association
for Computational Linguistics.
Rex Black. 1999. Managing the Testing Process.
Miriam Butt and Tracy Holloway King. 2003. Grammar
writing, testing and evaluation. In Ali Farghaly, editor,
A handbook for language engineers, pages 129–179.
CSLI.
Geoffrey Chang, Christopher R. Roth, Christopher L.
Reyes, Owen Pornillos, Yen-Ju Chen, and Andy P.
Chen. 2006. Letters: Retraction. Science, 314:1875.
K. Bretonnel Cohen, Lorraine Tanabe, Shuhei Kinoshita,
and Lawrence Hunter. 2004. A resource for construct-
ing customized test suites for molecular biology entity
identification systems. In HLT-NAACL 2004 Work-
shop: BioLINK 2004, Linking Biological Literature,
Ontologies and Databases, pages 1–8. Association for
Computational Linguistics.
</reference>
<page confidence="0.973973">
29
</page>
<reference confidence="0.999703113207547">
Dan Flickinger, Alexander Koller, and Stefan Thater.
2005. A new well-formedness criterion for semantics
debugging. In Proceedings of the HPSG05 Confer-
ence.
Lawrence Hunter, Zhiyong Lu, James Firby, William
A. Baumgartner Jr., Helen L. Johnson, Philip V. Ogren,
and K. Bretonnel Cohen. 2008. OpenDMAP: An
open-source, ontology-driven concept analysis engine,
with applications to capturing knowledge regarding
protein transport, protein interactions and cell-specific
gene expression. BMC Bioinformatics, 9(78).
Helen L. Johnson, K. Bretonnel Cohen, and Lawrence
Hunter. 2007. A fault model for ontology mapping,
alignment, and linking systems. In Pacific Sympo-
sium on Biocomputing, pages 233–244. World Scien-
tific Publishing Company.
Cem Kaner, Hung Quoc Nguyen, and Jack Falk. 1999.
Testing computer software, 2nd edition. John Wiley
and Sons.
Maricel Kann, Yanay Ofran, Marco Punta, and Predrag
Radivojac. 2006. Protein interactions and disease. In
Pacific Symposium on Biocomputing, pages 351–353.
World Scientific Publishing Company.
Martin Krallinger, Florian Leitner, and Alfonso Valen-
cia. 2007. Assessment of the second BioCreative PPI
task: automatic extraction of protein-protein interac-
tions. In Proceedings of the Second BioCreative Chal-
lenge Evaluation Workshop.
Brian Marick. 1997. The craft of software testing:
subsystem testing including object-based and object-
oriented testing. Prentice Hall.
Steve McConnell. 2004. Code complete. Microsoft
Press, 2nd edition.
Tony McEnery and Andrew Wilson. 2001. Corpus Lin-
guistics. Edinburgh University Press, 2nd edition.
Greg Miller. 2006. A scientist’s nightmare: software
problem leads to five retractions. Science, 314:1856–
1857.
Glenford Myers. 1979. The art of software testing. John
Wiley and Sons.
S. Oepen, K. Netter, and J. Klein. 1998. TSNLP - test
suites for natural language processing. In John Ner-
bonne, editor, Linguistic Databases, chapter 2, pages
13–36. CSLI Publications.
Stephan Oepen, Kristina Toutanova, Stuart Shieber,
Christopher Manning, Dan Flickinger, and Thorsten
Brants. 2002. The LinGO Redwoods treebank: mo-
tivation and preliminary applications. In Proceedings
of the 19th international conference on computational
linguistics, volume 2.
Martin Volk. 1998. Markup of a test suite with SGML.
In John Nerbonne, editor, Linguistic databases, pages
59–76. CSLI Publications.
</reference>
<page confidence="0.998813">
30
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.980853">
<title confidence="0.9982525">Software testing and the naturally occurring in natural language</title>
<author confidence="0.997062">Lawrence Hunter</author>
<abstract confidence="0.9985517">It is a widely accepted belief in natural language processing research that naturally occurring data is the best (and perhaps the only appropriate) data for testing text mining systems. This paper compares code coverage using a suite of functional tests and using a large corpus and finds that higher class, line, and branch coverage is achieved with structured tests than with even a very large corpus.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>William A Baumgartner Jr</author>
<author>K Bretonnel Cohen</author>
<author>Lynne Fox</author>
<author>George K Acquaah-Mensah</author>
<author>Lawrence Hunter</author>
</authors>
<title>Manual curation is not sufficient for annotation of genomic databases.</title>
<date>2007</date>
<journal>Bioinformatics,</journal>
<pages>23--41</pages>
<marker>Jr, Cohen, Fox, Acquaah-Mensah, Hunter, 2007</marker>
<rawString>William A. Baumgartner Jr., K. Bretonnel Cohen, Lynne Fox, George K. Acquaah-Mensah, and Lawrence Hunter. 2007. Manual curation is not sufficient for annotation of genomic databases. Bioinformatics, 23:i41–i48.</rawString>
</citation>
<citation valid="false">
<authors>
<author>William A Baumgartner Jr</author>
<author>Zhiyong Lu</author>
<author>Helen L Johnson</author>
<author>J Gregory Caporaso</author>
<author>Jesse Paquette</author>
<author>Anna Lindemann</author>
<author>Elizabeth K White</author>
<author>Olga Medvedeva</author>
<author>K Bretonnel Cohen</author>
<author>Lawrence Hunter</author>
</authors>
<title>In press. Concept recognition for extracting protein interaction relations from biomedical text. Genome Biology.</title>
<marker>Jr, Lu, Johnson, Caporaso, Paquette, Lindemann, White, Medvedeva, Cohen, Hunter, </marker>
<rawString>William A. Baumgartner Jr., Zhiyong Lu, Helen L. Johnson, J. Gregory Caporaso, Jesse Paquette, Anna Lindemann, Elizabeth K. White, Olga Medvedeva, K. Bretonnel Cohen, and Lawrence Hunter. In press. Concept recognition for extracting protein interaction relations from biomedical text. Genome Biology.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Emily M Bender</author>
<author>Laurie Poulson</author>
<author>Scott Drellishak</author>
<author>Chris Evans</author>
</authors>
<title>Validation and regression testing for a cross-linguistic grammar resource.</title>
<date>2007</date>
<booktitle>In ACL 2007 Workshop on Deep Linguistic Processing,</booktitle>
<pages>136--143</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="20589" citStr="Bender et al., 2007" startWordPosition="3427" endWordPosition="3430">eceived little attention in the natural language processing arena. A notable exception to this comes from the grammar engineering community. This has produced a body of publications that includes Oepen’s work on test suite design (Oepen et al., 1998), Volk’s work on test suite encoding (Volk, 1998), Oepen et al.’s work on the Redwoods project (Oepen et al., 2002), Butt and King’s discussion of the importance of testing (Butt and King, 2003), Flickinger et al.’s work on “semantics debugging” with Redwoods data (Flickinger et al., 2005), and Bender et al.’s recent work on test suite generation (Bender et al., 2007). Outside of the realm of grammar engineering, work on testing for NLP is quite limited. (Cohen et al., 2004) describes a methodology for generating test suites for molecular biology named entity recognition systems, and (Johnson et al., 2007) describes the development of a fault model for linguistically-based ontology mapping, alignment, and linking systems. However, when most researchers in the NLP community refer in print to “testing,” they do not mean it in the sense in which that term is used in software engineering. Some projects have publicized aspects of their testing work, but have no</context>
</contexts>
<marker>Bender, Poulson, Drellishak, Evans, 2007</marker>
<rawString>Emily M. Bender, Laurie Poulson, Scott Drellishak, and Chris Evans. 2007. Validation and regression testing for a cross-linguistic grammar resource. In ACL 2007 Workshop on Deep Linguistic Processing, pages 136–143, Prague, Czech Republic, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rex Black</author>
</authors>
<title>Managing the Testing Process.</title>
<date>1999</date>
<contexts>
<context position="22897" citStr="Black, 1999" startWordPosition="3807" endWordPosition="3808">g the error that sets myVariable to 3 if it is not valued 1 or 2. If the code were written with reasonable line breaks, code coverage would be only 20%. And, as has been noted by others, code coverage can not detect “sins of omission”—bugs that consist of the failure to write needed code (e.g. for error-handling or for input validation). We do not claim that code coverage is wholly sufficient for evaluating a test suite; nonetheless, it is one of a number of metrics that are helpful in judging the adequacy of a testing effort. Another very valuable one is the found/fixed or open/closed graph (Black, 1999; Baumgartner Jr. et al., 2007). While remaining aware of the potential shortcomings of code coverage, we also note that the data reported here supports its utility. The developerwritten functional tests were produced without monitoring code coverage; even though those tests routinely produced higher coverage than a large corpus of naturalistic text, they achieved less than 60% coverage overall, as predicted by Wiegers’s work cited in the introduction. We now have the opportunity to raise that coverage via structured testing performed by someone other than the developer. In fact, our first att</context>
</contexts>
<marker>Black, 1999</marker>
<rawString>Rex Black. 1999. Managing the Testing Process.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Miriam Butt</author>
<author>Tracy Holloway King</author>
</authors>
<title>Grammar writing, testing and evaluation.</title>
<date>2003</date>
<pages>129--179</pages>
<editor>In Ali Farghaly, editor,</editor>
<publisher>CSLI.</publisher>
<contexts>
<context position="20413" citStr="Butt and King, 2003" startWordPosition="3398" endWordPosition="3401">esting. Rather, coverage is optimized by traditional software testing. 4.1 Related work Although software testing is a first-class research object in computer science, it has received little attention in the natural language processing arena. A notable exception to this comes from the grammar engineering community. This has produced a body of publications that includes Oepen’s work on test suite design (Oepen et al., 1998), Volk’s work on test suite encoding (Volk, 1998), Oepen et al.’s work on the Redwoods project (Oepen et al., 2002), Butt and King’s discussion of the importance of testing (Butt and King, 2003), Flickinger et al.’s work on “semantics debugging” with Redwoods data (Flickinger et al., 2005), and Bender et al.’s recent work on test suite generation (Bender et al., 2007). Outside of the realm of grammar engineering, work on testing for NLP is quite limited. (Cohen et al., 2004) describes a methodology for generating test suites for molecular biology named entity recognition systems, and (Johnson et al., 2007) describes the development of a fault model for linguistically-based ontology mapping, alignment, and linking systems. However, when most researchers in the NLP community refer in p</context>
</contexts>
<marker>Butt, King, 2003</marker>
<rawString>Miriam Butt and Tracy Holloway King. 2003. Grammar writing, testing and evaluation. In Ali Farghaly, editor, A handbook for language engineers, pages 129–179. CSLI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geoffrey Chang</author>
<author>Christopher R Roth</author>
<author>Christopher L Reyes</author>
<author>Owen Pornillos</author>
<author>Yen-Ju Chen</author>
<author>Andy P Chen</author>
</authors>
<date>2006</date>
<journal>Letters: Retraction. Science,</journal>
<pages>314--1875</pages>
<contexts>
<context position="2029" citStr="Chang et al., 2006" startWordPosition="333" endWordPosition="336">e world. The consequences for the rest of the scientific community have not been X Bretonnel Cohen is with The MITRE Corporation. All three co-authors are at the Center for Computational Pharmacology in the University of Colorado School of Medicine. 23 quantified, but were substantial: prior to the retractions, publishing papers with results that did not jibe with his model’s predictions was difficult, and obtaining grants based on preliminary results that seemed to contradict his published results was difficult as well. The Chang story (for a succinct discussion, see (Miller, 2006), and see (Chang et al., 2006) for the retractions) is an object illustration of the truth of Rob Knight’s observation that “For scientific work, bugs don’t just mean unhappy users who you’ll never actually meet: they mean retracted publications and ended careers. It is critical that your code be fully tested before you draw conclusions from results it produces” (personal communication). Nonetheless, the subject of software testing has been largely neglected in academic natural language processing. This paper addresses one aspect of software testing: the monitoring of testing efforts via code coverage. 1.1 Code coverage Co</context>
</contexts>
<marker>Chang, Roth, Reyes, Pornillos, Chen, Chen, 2006</marker>
<rawString>Geoffrey Chang, Christopher R. Roth, Christopher L. Reyes, Owen Pornillos, Yen-Ju Chen, and Andy P. Chen. 2006. Letters: Retraction. Science, 314:1875.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Bretonnel Cohen</author>
<author>Lorraine Tanabe</author>
<author>Shuhei Kinoshita</author>
<author>Lawrence Hunter</author>
</authors>
<title>A resource for constructing customized test suites for molecular biology entity identification systems.</title>
<date>2004</date>
<booktitle>In HLT-NAACL 2004 Workshop: BioLINK 2004, Linking Biological Literature, Ontologies and Databases,</booktitle>
<pages>1--8</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="20698" citStr="Cohen et al., 2004" startWordPosition="3447" endWordPosition="3450">grammar engineering community. This has produced a body of publications that includes Oepen’s work on test suite design (Oepen et al., 1998), Volk’s work on test suite encoding (Volk, 1998), Oepen et al.’s work on the Redwoods project (Oepen et al., 2002), Butt and King’s discussion of the importance of testing (Butt and King, 2003), Flickinger et al.’s work on “semantics debugging” with Redwoods data (Flickinger et al., 2005), and Bender et al.’s recent work on test suite generation (Bender et al., 2007). Outside of the realm of grammar engineering, work on testing for NLP is quite limited. (Cohen et al., 2004) describes a methodology for generating test suites for molecular biology named entity recognition systems, and (Johnson et al., 2007) describes the development of a fault model for linguistically-based ontology mapping, alignment, and linking systems. However, when most researchers in the NLP community refer in print to “testing,” they do not mean it in the sense in which that term is used in software engineering. Some projects have publicized aspects of their testing work, but have not published on their approaches: the NLTK project posts modulelevel line coverage statistics, having achieved</context>
</contexts>
<marker>Cohen, Tanabe, Kinoshita, Hunter, 2004</marker>
<rawString>K. Bretonnel Cohen, Lorraine Tanabe, Shuhei Kinoshita, and Lawrence Hunter. 2004. A resource for constructing customized test suites for molecular biology entity identification systems. In HLT-NAACL 2004 Workshop: BioLINK 2004, Linking Biological Literature, Ontologies and Databases, pages 1–8. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Flickinger</author>
<author>Alexander Koller</author>
<author>Stefan Thater</author>
</authors>
<title>A new well-formedness criterion for semantics debugging.</title>
<date>2005</date>
<booktitle>In Proceedings of the HPSG05 Conference.</booktitle>
<contexts>
<context position="20509" citStr="Flickinger et al., 2005" startWordPosition="3413" endWordPosition="3416">ough software testing is a first-class research object in computer science, it has received little attention in the natural language processing arena. A notable exception to this comes from the grammar engineering community. This has produced a body of publications that includes Oepen’s work on test suite design (Oepen et al., 1998), Volk’s work on test suite encoding (Volk, 1998), Oepen et al.’s work on the Redwoods project (Oepen et al., 2002), Butt and King’s discussion of the importance of testing (Butt and King, 2003), Flickinger et al.’s work on “semantics debugging” with Redwoods data (Flickinger et al., 2005), and Bender et al.’s recent work on test suite generation (Bender et al., 2007). Outside of the realm of grammar engineering, work on testing for NLP is quite limited. (Cohen et al., 2004) describes a methodology for generating test suites for molecular biology named entity recognition systems, and (Johnson et al., 2007) describes the development of a fault model for linguistically-based ontology mapping, alignment, and linking systems. However, when most researchers in the NLP community refer in print to “testing,” they do not mean it in the sense in which that term is used in software engin</context>
</contexts>
<marker>Flickinger, Koller, Thater, 2005</marker>
<rawString>Dan Flickinger, Alexander Koller, and Stefan Thater. 2005. A new well-formedness criterion for semantics debugging. In Proceedings of the HPSG05 Conference.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lawrence Hunter</author>
<author>Zhiyong Lu</author>
<author>James Firby</author>
<author>William A Baumgartner Jr</author>
<author>Helen L Johnson</author>
<author>Philip V Ogren</author>
<author>K Bretonnel Cohen</author>
</authors>
<title>OpenDMAP: An open-source, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-specific gene expression.</title>
<date>2008</date>
<journal>BMC Bioinformatics,</journal>
<volume>9</volume>
<issue>78</issue>
<contexts>
<context position="7477" citStr="Hunter et al., 2008" startWordPosition="1229" endWordPosition="1232">of input data and rule sets. In this case, the null hypothesis is that no differences would be observed. In contrast with the null hypothesis, the unspoken assumption in much NLP work is that the null hypothesis does not hold, that the primary determinant of coverage will be the size of the corpus, and that the observed pattern will be that coverage is higher with the large corpus than when the input is not a large corpus. 2 Methods and materials 2.1 The application under test The application under test was an information extraction application known as OpenDMAP. It is described in detail in (Hunter et al., 2008). It achieved the highest performance on one measure of the protein-protein interaction task in the BioCreative II shared task (Krallinger et al., 2007). Its use in that task is described specifically in (Baumgartner Jr. et al., In press). It contains 7,024 lines of code spread across three packages (see Table 1). One major package deals with representing the semantic grammar rules themselves, while the other deals with applying the rules to and extracting data from arbitrary textual input. (A minor package deals with 24 Component Lines of code Parser 3,982 Rule-handling 2,311 Configuration 73</context>
</contexts>
<marker>Hunter, Lu, Firby, Jr, Johnson, Ogren, Cohen, 2008</marker>
<rawString>Lawrence Hunter, Zhiyong Lu, James Firby, William A. Baumgartner Jr., Helen L. Johnson, Philip V. Ogren, and K. Bretonnel Cohen. 2008. OpenDMAP: An open-source, ontology-driven concept analysis engine, with applications to capturing knowledge regarding protein transport, protein interactions and cell-specific gene expression. BMC Bioinformatics, 9(78).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helen L Johnson</author>
<author>K Bretonnel Cohen</author>
<author>Lawrence Hunter</author>
</authors>
<title>A fault model for ontology mapping, alignment, and linking systems.</title>
<date>2007</date>
<booktitle>In Pacific Symposium on Biocomputing,</booktitle>
<pages>233--244</pages>
<publisher>World Scientific Publishing Company.</publisher>
<contexts>
<context position="20832" citStr="Johnson et al., 2007" startWordPosition="3467" endWordPosition="3470">., 1998), Volk’s work on test suite encoding (Volk, 1998), Oepen et al.’s work on the Redwoods project (Oepen et al., 2002), Butt and King’s discussion of the importance of testing (Butt and King, 2003), Flickinger et al.’s work on “semantics debugging” with Redwoods data (Flickinger et al., 2005), and Bender et al.’s recent work on test suite generation (Bender et al., 2007). Outside of the realm of grammar engineering, work on testing for NLP is quite limited. (Cohen et al., 2004) describes a methodology for generating test suites for molecular biology named entity recognition systems, and (Johnson et al., 2007) describes the development of a fault model for linguistically-based ontology mapping, alignment, and linking systems. However, when most researchers in the NLP community refer in print to “testing,” they do not mean it in the sense in which that term is used in software engineering. Some projects have publicized aspects of their testing work, but have not published on their approaches: the NLTK project posts modulelevel line coverage statistics, having achieved median coverage of 55% on 116 Python modules2 and 38% coverage for the project as a whole; the MALLET project indicates on its web si</context>
</contexts>
<marker>Johnson, Cohen, Hunter, 2007</marker>
<rawString>Helen L. Johnson, K. Bretonnel Cohen, and Lawrence Hunter. 2007. A fault model for ontology mapping, alignment, and linking systems. In Pacific Symposium on Biocomputing, pages 233–244. World Scientific Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cem Kaner</author>
<author>Hung Quoc Nguyen</author>
<author>Jack Falk</author>
</authors>
<title>Testing computer software, 2nd edition.</title>
<date>1999</date>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="3752" citStr="Kaner et al., 1999" startWordPosition="612" endWordPosition="615">ficult to achieve for any nontrivial application, but in general, high degrees of “uncovered” code should lead one to suspect that there is a large amount of Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 23–30, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics code that might harbor undetected bugs simply due to never having been executed. A variety of code coverage metrics exist. Line coverage indicates the proportion of lines of code that have been executed. It is not the most revealing form of coverage assessment (Kaner et al., 1999, p. 43), but is a basic part of any coverage measurement assessment. Branch coverage indicates the proportion of branches within conditionals that have been traversed (Marick, 1997, p. 145). For example, for the conditional if $a &amp;&amp; $b, there are two possible branches—one is traversed if the expression evaluates to true, and the other if it evaluates to false. It is more informative than line coverage. Logic coverage (also known as multicondition coverage (Myers, 1979) and condition coverage (Kaner et al., 1999, p. 44) indicates the proportion of sets of variable values that have been tried—a</context>
</contexts>
<marker>Kaner, Nguyen, Falk, 1999</marker>
<rawString>Cem Kaner, Hung Quoc Nguyen, and Jack Falk. 1999. Testing computer software, 2nd edition. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maricel Kann</author>
<author>Yanay Ofran</author>
<author>Marco Punta</author>
<author>Predrag Radivojac</author>
</authors>
<title>Protein interactions and disease.</title>
<date>2006</date>
<booktitle>In Pacific Symposium on Biocomputing,</booktitle>
<pages>351--353</pages>
<publisher>World Scientific Publishing Company.</publisher>
<contexts>
<context position="10162" citStr="Kann et al., 2006" startWordPosition="1660" endWordPosition="1663">ollowing sets of materials. • A large data set distributed as training data for part of the BioCreative II shared task. It is described in detail in (Krallinger et al., 2007). Briefly, its domain is molecular biology, and in particular protein-protein interactions—an important topic of research in computational Test type Number of tests Basic 85 Pattern/rule 67 Patterns only 90 Slots 9 Slot nesting 7 Slot property 20 Total 278 Table 2: Distribution of functional tests. bioscience, with significance to a wide range of topics in biology, including understanding the mechanisms of human diseases (Kann et al., 2006). The corpus contained 3,947,200 words, making it almost an order of magnitude larger than the most commonly used biomedical corpus (GENIA, at about 433K words). This data set is publicly available via biocreative.sourceforge.net. • In conjunction with that data set: a set of 98 rules written in a data-driven fashion by manually examining the BioCreative II data described just above. These rules were used in the BioCreative II shared task, where they achieved the highest score in one category. The set of rules is available on our SourceForge site at bionlp.sourceforge.net. • A set of functiona</context>
</contexts>
<marker>Kann, Ofran, Punta, Radivojac, 2006</marker>
<rawString>Maricel Kann, Yanay Ofran, Marco Punta, and Predrag Radivojac. 2006. Protein interactions and disease. In Pacific Symposium on Biocomputing, pages 351–353. World Scientific Publishing Company.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Krallinger</author>
<author>Florian Leitner</author>
<author>Alfonso Valencia</author>
</authors>
<title>Assessment of the second BioCreative PPI task: automatic extraction of protein-protein interactions.</title>
<date>2007</date>
<booktitle>In Proceedings of the Second BioCreative Challenge Evaluation Workshop.</booktitle>
<contexts>
<context position="7629" citStr="Krallinger et al., 2007" startWordPosition="1252" endWordPosition="1255">nspoken assumption in much NLP work is that the null hypothesis does not hold, that the primary determinant of coverage will be the size of the corpus, and that the observed pattern will be that coverage is higher with the large corpus than when the input is not a large corpus. 2 Methods and materials 2.1 The application under test The application under test was an information extraction application known as OpenDMAP. It is described in detail in (Hunter et al., 2008). It achieved the highest performance on one measure of the protein-protein interaction task in the BioCreative II shared task (Krallinger et al., 2007). Its use in that task is described specifically in (Baumgartner Jr. et al., In press). It contains 7,024 lines of code spread across three packages (see Table 1). One major package deals with representing the semantic grammar rules themselves, while the other deals with applying the rules to and extracting data from arbitrary textual input. (A minor package deals with 24 Component Lines of code Parser 3,982 Rule-handling 2,311 Configuration 731 Total 7,024 Table 1: Distribution of lines of code in the application. the configuration files and is mostly not discussed in this paper.) The rules a</context>
<context position="9718" citStr="Krallinger et al., 2007" startWordPosition="1593" endWordPosition="1596">ractor2] The input NEF binds PACS-2 (PMID 18296443) would match that rule. The result would be the recognition of a protein interaction event, with interactor1 being NEF and interactor2 being PACS-2. Not all rules utilize all possibilities of the rule language, and we took this into account in one of our experiments; we discuss the rules further later in the paper in the context of that experiment. 2.2 Materials In this work, we made use of the following sets of materials. • A large data set distributed as training data for part of the BioCreative II shared task. It is described in detail in (Krallinger et al., 2007). Briefly, its domain is molecular biology, and in particular protein-protein interactions—an important topic of research in computational Test type Number of tests Basic 85 Pattern/rule 67 Patterns only 90 Slots 9 Slot nesting 7 Slot property 20 Total 278 Table 2: Distribution of functional tests. bioscience, with significance to a wide range of topics in biology, including understanding the mechanisms of human diseases (Kann et al., 2006). The corpus contained 3,947,200 words, making it almost an order of magnitude larger than the most commonly used biomedical corpus (GENIA, at about 433K wo</context>
</contexts>
<marker>Krallinger, Leitner, Valencia, 2007</marker>
<rawString>Martin Krallinger, Florian Leitner, and Alfonso Valencia. 2007. Assessment of the second BioCreative PPI task: automatic extraction of protein-protein interactions. In Proceedings of the Second BioCreative Challenge Evaluation Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Brian Marick</author>
</authors>
<title>The craft of software testing: subsystem testing including object-based and objectoriented testing.</title>
<date>1997</date>
<publisher>Prentice Hall.</publisher>
<contexts>
<context position="3933" citStr="Marick, 1997" startWordPosition="641" endWordPosition="642">, and Quality Assurance for Natural Language Processing, pages 23–30, Columbus, Ohio, USA, June 2008. c�2008 Association for Computational Linguistics code that might harbor undetected bugs simply due to never having been executed. A variety of code coverage metrics exist. Line coverage indicates the proportion of lines of code that have been executed. It is not the most revealing form of coverage assessment (Kaner et al., 1999, p. 43), but is a basic part of any coverage measurement assessment. Branch coverage indicates the proportion of branches within conditionals that have been traversed (Marick, 1997, p. 145). For example, for the conditional if $a &amp;&amp; $b, there are two possible branches—one is traversed if the expression evaluates to true, and the other if it evaluates to false. It is more informative than line coverage. Logic coverage (also known as multicondition coverage (Myers, 1979) and condition coverage (Kaner et al., 1999, p. 44) indicates the proportion of sets of variable values that have been tried—a superset of the possible branches traversed. For example, for the conditional if $a ||$b, the possible cases (assuming no short-circuit logic) are those of the standard (logical) t</context>
</contexts>
<marker>Marick, 1997</marker>
<rawString>Brian Marick. 1997. The craft of software testing: subsystem testing including object-based and objectoriented testing. Prentice Hall.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steve McConnell</author>
</authors>
<title>Code complete.</title>
<date>2004</date>
<publisher>Microsoft Press,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="2758" citStr="McConnell, 2004" startWordPosition="451" endWordPosition="452">k, bugs don’t just mean unhappy users who you’ll never actually meet: they mean retracted publications and ended careers. It is critical that your code be fully tested before you draw conclusions from results it produces” (personal communication). Nonetheless, the subject of software testing has been largely neglected in academic natural language processing. This paper addresses one aspect of software testing: the monitoring of testing efforts via code coverage. 1.1 Code coverage Code coverage is a numerical assessment of the amount of code that is executed during the running of a test suite (McConnell, 2004). Although it is by no means a completely sufficient method for determining the completeness of a testing effort, it is nonetheless a helpful member of any suite of metrics for assessing testing effort completeness. Code coverage is a metric in the range 0-1.0. A value of 0.86 indicates that 86% of the code was executed while running a given test suite. 100% coverage is difficult to achieve for any nontrivial application, but in general, high degrees of “uncovered” code should lead one to suspect that there is a large amount of Software Engineering, Testing, and Quality Assurance for Natural L</context>
<context position="5654" citStr="McConnell, 2004" startWordPosition="920" endWordPosition="921"> programming, function coverage is sometimes replaced by class coverage—a measure of the number of classes that are covered. We emphasize again that code coverage is not a sufficient metric for evaluating testing completeness in isolation—for example, it is by definition unable to detect “errors of omission,” or bugs that consist of a failure to implement needed functionality. Nonetheless, it remains a useful part of a larger suite of metrics, and one study found that testing in the absence of concurrent assessment of code coverage typically results in only 50-60% of the code being executed ((McConnell, 2004, p. 526), citing Wiegers 2002). We set out to question whether a dominant, if often unspoken, assumption of much work in contemporary NLP holds true: that feeding a program a large corpus serves to exercise it adequately. We began with an information extraction application that had been built for us by a series of contractors, with the contractors receiving constant remote oversight and guidance but without ongoing monitoring of the actual code-writing. The application had benefitted from no testing other than that done by the developers. We used a sort of “translucent-box” or “graybox” parad</context>
</contexts>
<marker>McConnell, 2004</marker>
<rawString>Steve McConnell. 2004. Code complete. Microsoft Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tony McEnery</author>
<author>Andrew Wilson</author>
</authors>
<title>Corpus Linguistics.</title>
<date>2001</date>
<publisher>Edinburgh University Press,</publisher>
<note>2nd edition.</note>
<contexts>
<context position="17300" citStr="McEnery and Wilson, 2001" startWordPosition="2859" endWordPosition="2862">t or either half of the rule set. (The identity of the last three columns is due to this lack of difference in results between the full rule set and the two reduced rule sets.) On a per-class level, we did note minor differences, but as Table 3 shows, they were within rounding error on the package level. 3.3 The third experiment: Coverage closure In the third experiment, we looked at how coverage varies as increasingly larger amounts of the corpus are processed. This methodology is comparable to examining the closure properties of a corpus in a corpus linguistics study (see e.g. Chapter 6 of (McEnery and Wilson, 2001)) (and as such may be sensitive to the extent to which the contents of the corpus do or do not fit the sublanguage model). We counted cumulative line coverage as increasingly large amounts of the corpus were processed, ranging from 0 to 100% of its contents. The results for line coverage are shown in Figure 1. (The results for branch coverage are quite similar, and the graph is not shown.) Line coverage for the entire application is indicated by the thick solid line. Line coverage for the parser package is indicated by the thin solid line. Line coverage for the rules package is indicated by th</context>
</contexts>
<marker>McEnery, Wilson, 2001</marker>
<rawString>Tony McEnery and Andrew Wilson. 2001. Corpus Linguistics. Edinburgh University Press, 2nd edition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Greg Miller</author>
</authors>
<title>A scientist’s nightmare: software problem leads to five retractions.</title>
<date>2006</date>
<journal>Science,</journal>
<volume>314</volume>
<pages>1857</pages>
<contexts>
<context position="1999" citStr="Miller, 2006" startWordPosition="329" endWordPosition="330">estigious journals in the world. The consequences for the rest of the scientific community have not been X Bretonnel Cohen is with The MITRE Corporation. All three co-authors are at the Center for Computational Pharmacology in the University of Colorado School of Medicine. 23 quantified, but were substantial: prior to the retractions, publishing papers with results that did not jibe with his model’s predictions was difficult, and obtaining grants based on preliminary results that seemed to contradict his published results was difficult as well. The Chang story (for a succinct discussion, see (Miller, 2006), and see (Chang et al., 2006) for the retractions) is an object illustration of the truth of Rob Knight’s observation that “For scientific work, bugs don’t just mean unhappy users who you’ll never actually meet: they mean retracted publications and ended careers. It is critical that your code be fully tested before you draw conclusions from results it produces” (personal communication). Nonetheless, the subject of software testing has been largely neglected in academic natural language processing. This paper addresses one aspect of software testing: the monitoring of testing efforts via code </context>
</contexts>
<marker>Miller, 2006</marker>
<rawString>Greg Miller. 2006. A scientist’s nightmare: software problem leads to five retractions. Science, 314:1856– 1857.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Glenford Myers</author>
</authors>
<title>The art of software testing.</title>
<date>1979</date>
<publisher>John Wiley and Sons.</publisher>
<contexts>
<context position="4226" citStr="Myers, 1979" startWordPosition="690" endWordPosition="691">ates the proportion of lines of code that have been executed. It is not the most revealing form of coverage assessment (Kaner et al., 1999, p. 43), but is a basic part of any coverage measurement assessment. Branch coverage indicates the proportion of branches within conditionals that have been traversed (Marick, 1997, p. 145). For example, for the conditional if $a &amp;&amp; $b, there are two possible branches—one is traversed if the expression evaluates to true, and the other if it evaluates to false. It is more informative than line coverage. Logic coverage (also known as multicondition coverage (Myers, 1979) and condition coverage (Kaner et al., 1999, p. 44) indicates the proportion of sets of variable values that have been tried—a superset of the possible branches traversed. For example, for the conditional if $a ||$b, the possible cases (assuming no short-circuit logic) are those of the standard (logical) truth table for that conditional. These coverage types are progressively more informative than line coverage. Other types of coverage are less informative than line coverage. For example, function coverage indicates the proportion of functions that are called. There is no guarantee that each l</context>
</contexts>
<marker>Myers, 1979</marker>
<rawString>Glenford Myers. 1979. The art of software testing. John Wiley and Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Oepen</author>
<author>K Netter</author>
<author>J Klein</author>
</authors>
<title>TSNLP - test suites for natural language processing. In</title>
<date>1998</date>
<booktitle>Linguistic Databases, chapter 2,</booktitle>
<pages>13--36</pages>
<editor>John Nerbonne, editor,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="20219" citStr="Oepen et al., 1998" startWordPosition="3365" endWordPosition="3368"> hypothesis that code coverage for this application is not affected by the size of the corpus or by the size of the rule set, and that running it on a large corpus does not guarantee thorough testing. Rather, coverage is optimized by traditional software testing. 4.1 Related work Although software testing is a first-class research object in computer science, it has received little attention in the natural language processing arena. A notable exception to this comes from the grammar engineering community. This has produced a body of publications that includes Oepen’s work on test suite design (Oepen et al., 1998), Volk’s work on test suite encoding (Volk, 1998), Oepen et al.’s work on the Redwoods project (Oepen et al., 2002), Butt and King’s discussion of the importance of testing (Butt and King, 2003), Flickinger et al.’s work on “semantics debugging” with Redwoods data (Flickinger et al., 2005), and Bender et al.’s recent work on test suite generation (Bender et al., 2007). Outside of the realm of grammar engineering, work on testing for NLP is quite limited. (Cohen et al., 2004) describes a methodology for generating test suites for molecular biology named entity recognition systems, and (Johnson </context>
</contexts>
<marker>Oepen, Netter, Klein, 1998</marker>
<rawString>S. Oepen, K. Netter, and J. Klein. 1998. TSNLP - test suites for natural language processing. In John Nerbonne, editor, Linguistic Databases, chapter 2, pages 13–36. CSLI Publications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stephan Oepen</author>
<author>Kristina Toutanova</author>
<author>Stuart Shieber</author>
<author>Christopher Manning</author>
<author>Dan Flickinger</author>
<author>Thorsten Brants</author>
</authors>
<title>The LinGO Redwoods treebank: motivation and preliminary applications.</title>
<date>2002</date>
<booktitle>In Proceedings of the 19th international conference on computational linguistics,</booktitle>
<volume>2</volume>
<contexts>
<context position="20334" citStr="Oepen et al., 2002" startWordPosition="3385" endWordPosition="3388"> rule set, and that running it on a large corpus does not guarantee thorough testing. Rather, coverage is optimized by traditional software testing. 4.1 Related work Although software testing is a first-class research object in computer science, it has received little attention in the natural language processing arena. A notable exception to this comes from the grammar engineering community. This has produced a body of publications that includes Oepen’s work on test suite design (Oepen et al., 1998), Volk’s work on test suite encoding (Volk, 1998), Oepen et al.’s work on the Redwoods project (Oepen et al., 2002), Butt and King’s discussion of the importance of testing (Butt and King, 2003), Flickinger et al.’s work on “semantics debugging” with Redwoods data (Flickinger et al., 2005), and Bender et al.’s recent work on test suite generation (Bender et al., 2007). Outside of the realm of grammar engineering, work on testing for NLP is quite limited. (Cohen et al., 2004) describes a methodology for generating test suites for molecular biology named entity recognition systems, and (Johnson et al., 2007) describes the development of a fault model for linguistically-based ontology mapping, alignment, and </context>
</contexts>
<marker>Oepen, Toutanova, Shieber, Manning, Flickinger, Brants, 2002</marker>
<rawString>Stephan Oepen, Kristina Toutanova, Stuart Shieber, Christopher Manning, Dan Flickinger, and Thorsten Brants. 2002. The LinGO Redwoods treebank: motivation and preliminary applications. In Proceedings of the 19th international conference on computational linguistics, volume 2.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martin Volk</author>
</authors>
<title>Markup of a test suite with SGML. In</title>
<date>1998</date>
<booktitle>Linguistic databases,</booktitle>
<pages>59--76</pages>
<editor>John Nerbonne, editor,</editor>
<publisher>CSLI Publications.</publisher>
<contexts>
<context position="20268" citStr="Volk, 1998" startWordPosition="3375" endWordPosition="3376">t affected by the size of the corpus or by the size of the rule set, and that running it on a large corpus does not guarantee thorough testing. Rather, coverage is optimized by traditional software testing. 4.1 Related work Although software testing is a first-class research object in computer science, it has received little attention in the natural language processing arena. A notable exception to this comes from the grammar engineering community. This has produced a body of publications that includes Oepen’s work on test suite design (Oepen et al., 1998), Volk’s work on test suite encoding (Volk, 1998), Oepen et al.’s work on the Redwoods project (Oepen et al., 2002), Butt and King’s discussion of the importance of testing (Butt and King, 2003), Flickinger et al.’s work on “semantics debugging” with Redwoods data (Flickinger et al., 2005), and Bender et al.’s recent work on test suite generation (Bender et al., 2007). Outside of the realm of grammar engineering, work on testing for NLP is quite limited. (Cohen et al., 2004) describes a methodology for generating test suites for molecular biology named entity recognition systems, and (Johnson et al., 2007) describes the development of a faul</context>
</contexts>
<marker>Volk, 1998</marker>
<rawString>Martin Volk. 1998. Markup of a test suite with SGML. In John Nerbonne, editor, Linguistic databases, pages 59–76. CSLI Publications.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>