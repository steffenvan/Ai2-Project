<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000566">
<title confidence="0.992867">
Unsupervised Methods of Topical Text Segmentation for Polish
</title>
<author confidence="0.994092">
Dominik Flejter
</author>
<affiliation confidence="0.66608075">
University of Economics
al. Niepodległo´sci 10
Pozna´n, Poland
D.Flejter@
</affiliation>
<email confidence="0.625126">
kie.ae.poznan.pl
</email>
<author confidence="0.983464">
Karol Wieloch
</author>
<affiliation confidence="0.6358595">
University of Economics
al. Niepodległo´sci 10
Pozna´n, Poland
K.Wieloch@
</affiliation>
<email confidence="0.575812">
kie.ae.poznan.pl
</email>
<author confidence="0.896729">
Witold Abramowicz
</author>
<affiliation confidence="0.91561">
University of Economics
</affiliation>
<bodyText confidence="0.34267125">
al. Niepodległo´sci 10
Pozna´n, Poland
W.Abramowicz@
kie.ae.poznan.pl
</bodyText>
<sectionHeader confidence="0.981014" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999776647058824">
This paper describes a study on performance
of existing unsupervised algorithms of text
documents topical segmentation when ap-
plied to Polish plain text documents. For
performance measurement five existing top-
ical segmentation algorithms were selected,
three different Polish test collections were
created and seven approaches to text pre-
processing were implemented. Based on
quantitative results (Pk and WindowDiff
metrics) use of specific algorithm was rec-
ommended and impact of pre-processing
strategies was assessed. Thanks to use of
standardized metrics and application of pre-
viously described methodology for test col-
lection development, comparative results for
Polish and English were also obtained.
</bodyText>
<sectionHeader confidence="0.999119" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998192071428572">
Rapid development of Internet-based information
services is marked by a proliferation of information
available on-line. Even if the Web shifts towards
multimedia content and structured documents, con-
tents of the Web sources remains predominantly tex-
tual and poorly structured; an important fraction of
this flood of plain text documents consists in multi-
topical documents. This abundance of complex but
plain text or just visually structured documents (as
is in case of most HTML files) creates a strong need
for intelligent text processing including robust and
efficient information extraction and retrieval.
One way of increasing efficiency of typical text
processing tasks consists in processing separately
</bodyText>
<page confidence="0.977377">
51
</page>
<bodyText confidence="0.999960852941177">
text segments instead of whole documents. While
different text segmentation strategies can be ap-
plied including splitting text into segments of equal
length, using moving windows of constant size or
discourse units (Reynar, 1998), division into topical
segment is intuitively more justified. This approach
is applicable in several IR and NLP areas including
document indexing, automatic summarization and
question answering (Choi, 2002).
For Information Extraction domain, two main ap-
plication of topical document segmentation are re-
lated to documents pre-processing and supporting
some basic tasks widely used by IE tools. Topi-
cal segmentation applied to individual documents as
well as documents streams (e.g. dialogues of radio
broadcast transcripts) is an initial step for further IE
processing (Manning, 1998); when combined with
segments labelling, classification or clustering (Al-
lan et al., 1998) it allows to pre-select ranges of text
to be mined for mentions of events, entities and re-
lations relevant to users needs and available IE re-
sources. This limits significantly size of text to be
processed by IE methods (Hearst and Plaunt, 1993)
and thus influences significantly overall IE perfor-
mance.
On the other hand, many basic tasks required by
IE domain (both related to IE resources creation
and document (pre-)processing) make use of sec-
tions rather than whole documents. In these tasks
including language modelling (esp. gathering of
co-occurrences statistics for trigger-based language
models), anaphora resolution, word sense disam-
biguation and coreference detection, definition of
proper context is crucial. To this extend entities
</bodyText>
<subsubsectionHeader confidence="0.755645">
Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 51–58,
</subsubsectionHeader>
<bodyText confidence="0.994707833333333">
Prague, June 2007. c�2007 Association for Computational Linguistics
discovered by topical segmentation are more reli-
able than document, paragraph or sentence contexts
as they help avoid usage of unrelated parts of doc-
uments as well as minimize sparse data problems
(Choi, 2002).
</bodyText>
<subsectionHeader confidence="0.999717">
1.1 Problem Statement
</subsectionHeader>
<bodyText confidence="0.999697393939394">
In this study we adopted definition proposed in (Fle-
jter, 2006) stating that ”linear topical segmentation
of text consists in extracting coherent blocks of text
such that: 1) any block focuses on exactly one topic
2) any pair of consecutive blocks have different main
topics.” We also accepted that two segments should
be defined as having different or same topics when-
ever they are judged so by people.
Depending on the used document collection and
user needs, text segmentation objective is to find top-
ical segments in individual multi-topical text docu-
ments or to discover boundaries between consecu-
tive stories or news items (e.g. in radio broadcast
transcriptions or text news streams).
Approaches to the problem of text segmentation
can be divided according to a number of dimensions
(Flejter, 2006) including cognitive approach (in op-
timistic approach text structure intended by author is
cognitively accessible, in pessimistic approach it is
not), hierarchical or linear segmentation (do we seg-
ment text into some levels of embedded segments?),
completeness of segmentation (does the segmenta-
tion need to cover the whole text?), disjointness of
segments (can two segments have any common text
ranges?), fuzziness of segments boundaries (how
fuzzy is the actual boundary location?), global or lo-
cal view of topics (is there any global, document-
independent list of topics?).
In this study we investigate linear, complete, dis-
joint segments with binary boundaries and local
view of topics. Selected algorithms focus on text
segmentation without considering labelling or clus-
tering of discovered segments.
</bodyText>
<subsectionHeader confidence="0.970272">
1.2 Our Contributions
</subsectionHeader>
<bodyText confidence="0.9995617">
The contributions of our research described in this
paper are twofold. Firstly, we developed three Pol-
ish test collections for text segmentation task (see:
Section 3.1). Secondly, we performed an extensive
study of performance of most popular segmentation
algorithms (see: Section 3.2) and pre-processing
strategies (see: Section 3.3) when applied to Pol-
ish documents; a total of 42 scenarios including dif-
ferent algorithms, pre-processing strategies and test
collections were evaluated (see: Section 4).
</bodyText>
<sectionHeader confidence="0.873981" genericHeader="method">
2 Approaches to Topical Segmentation
</sectionHeader>
<bodyText confidence="0.999986571428572">
As in case of most NLP tasks, a number of differ-
ent linguistic theories influenced topic segmentation
resulting in a variety of approaches applied. This
section gives a short presentation of major theories
underlying topical segmentation and an overview of
most popular segmentation algorithms with empha-
sis on those evaluated in our experiment.
</bodyText>
<subsectionHeader confidence="0.987261">
2.1 Theoretical Foundations
</subsectionHeader>
<bodyText confidence="0.999971045454545">
Out of linguistic approaches cohesion theory of Hal-
liday and Hassan had the strongest impact on top-
ical text segmentation. It analyzes several mecha-
nisms of documents internal cohesion including ref-
erences, substitution, ellipsis, conjunction (logical
relations) and lexical cohesion (reuse of the same
words to address the same object or objects of the
same class as well as use of terms which are more
general or semantically related in systematic or non-
systematic way). Other relevant linguistic theories
include Grosz and Sinder’s discourse segmentation
theory, Rhetorical Structure Theory and taxonomy
of text structures proposed Skorochod’ko (Reynar,
1998).
Out of empirical statistical rules some authors
make use of heuristics resulting form Heaps’ law for
new-words-based topical segment boundary detec-
tion. However, the most important theoretical foun-
dations in quantitative methods are related to strong
probabilistic frameworks including Hidden Markov
Models (Mulbregt et al., 1998) and Maximal En-
tropy Theory (Beeferman et al., 1997).
</bodyText>
<subsectionHeader confidence="0.974581">
2.2 Basic Methods
</subsectionHeader>
<bodyText confidence="0.999982125">
The most simple but also the most frequently used
methods of topical text segmentation do not re-
quire training (thus they are domain-independent)
nor make use of any complex linguistic resources or
utilities. Apart from methods based on new vocab-
ulary analysis, this category of algorithms applies
widely the simplest form of lexical cohesion i.e. re-
iterations of the same word.
</bodyText>
<page confidence="0.990788">
52
</page>
<bodyText confidence="0.999946255319149">
The first classical text segmentation algorithm
of this type is TextTiling described in (Hearst and
Plaunt, 1993). Its analysis unit consists of pseudo-
sentences corresponding to series of consecutive
words (typically 20 words). After the whole text
is divided into pseudo-sentences, a window of 12
pseudo-sentences is slid over the text (with one
pseudo-sentence step). At any position the window
is decomposed into two six-pseudo-sentences blocks
and their similarity is calculated by means of cosine
measure. Measurements for all consecutive window
positions (understood as positions of centre of the
window) form lexical cohesion curve local minima
of which correspond to segments boundaries. The
original algorithm was further enhanced in several
ways including use of words similarity measurement
based on co-occurrences (Kaufmann, 1999).
Another group of basic algorithms makes use of
technique of DotPlotting, originally proposed by
Raynar in (Reynar, 1994). In this approach 2D chart
is used for lexical cohesion analysis with both axes
corresponding to positions (in words) in the text; on
the chart points are drawn at coordinates (x, y) and
(y, x) iff words at positions x and y are equal. In this
settings coherent text segments correspond visually
to squares with high density of points. DotPlotting
image is than segmented using one of two strate-
gies: minimization of points density at the bound-
aries (minimization of external incoherence) or max-
imization of density of segments (maximization of
internal coherence) (Reynar, 1998). The original
DotPlotting algorithm requires to explicitly provide
expected number of segments as input.
Improved version of DotPlotting algorithm called
C99 (Choi, 2000) uses DotPlotting chart for vi-
sualization of similarity measurements at consecu-
tive point of the text (thus resulting in point with
different levels of intensity) instead of words co-
occurences. Afterwards, mask-based ranking tech-
nique is used for image enhancement. For ac-
tual segmentation, dynamic programming technique
similar to DotPlotting maximization algorithm is
used; an optional automatic termination strategy is
also implemented thus allowing the algorithm to as-
sess number of boundaries. In further work of the
same and other authors several enhancements of C99
algorithm were proposed.
</bodyText>
<subsectionHeader confidence="0.998863">
2.3 Methods Requiring External Resources
</subsectionHeader>
<bodyText confidence="0.999977692307693">
Still not requiring training and domain independent,
some methods make use of linguistic resources more
sophisticated than stop-list. Two classes of such so-
lution described in existing work are solutions us-
ing lexical chains (Morris and Hirst, 1991; Min-
Yen Kan, 1998) (which require to use some the-
saurus) and based on spreading activation (Kozima,
1993) (which depend on weights-based semantic
network constructed from thesaurus). In both cases
the effort put in algorithm enactment is quite high;
however in principle no additional resources need to
be developed for new texts (even from different do-
mains).
</bodyText>
<subsectionHeader confidence="0.992502">
2.4 Methods Requiring Training
</subsectionHeader>
<bodyText confidence="0.9999343">
Last group of methods includes supervised meth-
ods with generally strong mathematical foundations.
They perform very well; however they require train-
ing that possibly needs to be repeated when new
domain needs to be addressed. The methods in
this group use probabilistic frameworks including
maximal entropy (Beeferman et al., 1997), Hidden
Markov Models (Mulbregt et al., 1998) and Proba-
bilistic Latent Semantic Analysis (Blei and Moreno,
2001).
</bodyText>
<sectionHeader confidence="0.997693" genericHeader="method">
3 Experimental Setup
</sectionHeader>
<bodyText confidence="0.999926857142857">
Segmentation algorithms performance was evalu-
ated for 42 scenarios corresponding to different al-
gorithms, pre-processing strategies and test collec-
tions. For quantitative analysis and comparability
with previous and future research results two stan-
dard segmentation metrics were applied in all sce-
narios.
</bodyText>
<subsectionHeader confidence="0.999696">
3.1 Test Collections
</subsectionHeader>
<bodyText confidence="0.9998942">
For performance measurement three test collections
corresponding to different types of segmentation
tasks were developed: artificial documents collec-
tion (AC), stream collection (5C) and individual
documents collection (DC). AC and 5C were con-
structed based on 936 issues of EuroPAP (Euro-
pean information service of Polish Press Agency)
plain-text e-mail newsletter (EuroPAP, 2005) col-
lected from November 2001 to May 2005. Typ-
ical issue of EuroPAP newsletter contained about
</bodyText>
<page confidence="0.994172">
53
</page>
<bodyText confidence="0.998907">
25 complete news articles and a number of short
(containing at most several sentences) news items.
DC was constructed based on articles retrieved form
Wikipedia corresponding to ten most populous Pol-
ish cities (Wikipedia, 2007) covering typically sev-
eral topics (e.g. geography, culture, transportation).
For AC creation we followed precisely the
method applied for English in (Choi, 2000). Each
artificial document was created as a concatenation of
random number (n) of first sentences from ten news
articles randomly selected from a total of 24927
news items in EuroPAP corpus. Four subcollections
were created depending on allowed range of n as
listed in Table 1. Any two selected articles were as-
sumed to cover two different topics; thus reference
segmentation boundaries corresponded to points of
concatenations.
</bodyText>
<table confidence="0.995451">
AC AC1 AC2 AC3 AC4
n 3-11 3-5 6-8 9-11
documents count 400 100 100 100
</table>
<tableCaption confidence="0.999185">
Table 1: Artificial collection subcollections
</tableCaption>
<bodyText confidence="0.999941789473684">
For 5C creation newsletter messages from Eu-
roPAP were used as text streams (936 messages).
The reference segmentation was created using origi-
nal article boundaries present in EuroPAP mail mes-
sages (almost 30000 segments were marked).
For individual documents collection development
text content was extracted from Wikipedia docu-
ments, all headings were removed and all list items
(LI tags) with no terminal punctuation sign were
added a dot. Manual tagging by two authors of
this paper was performed. The instructions were
to put segment boundaries in the places of poten-
tial section titles. Obtained percent agreement of
0.988 and n coefficient (Carletta, 1996) of 0.975
suggest high convergence of both annotations. Fur-
ther, in places where the two annotators opinions dif-
fered (one marked boundary and the other did not),
negotiation-based approach (Flejter, 2006) was ap-
plied in order to develop reference segmentation.
</bodyText>
<subsectionHeader confidence="0.999114">
3.2 Selected algorithms
</subsectionHeader>
<bodyText confidence="0.998372">
In our experiment we used Choi’s publicly available
implementation of several text segmentation algo-
rithms not requiring training (with several adapta-
</bodyText>
<table confidence="0.9985998">
Sentences Tokens
avg std avg std
AC 6.8 1.7 122.6 33.4
SC 15.0 3.8 267.6 64.0
DC 28.5 9.9 300.0 110.2
</table>
<tableCaption confidence="0.999368">
Table 2: The average length of reference segments
</tableCaption>
<bodyText confidence="0.9964765">
tion concerning pre-processing stages). Specifically
we used Choi’s implementation of TextTiling algo-
rithm (TT), C99 algorithm for both known (C99l)
and unknown (C99) number of boundaries as well
as DotPlotting maximization (DP) and minimiza-
tion (DPmin) algorithms.
Algorithms not requiring to provide number of
segments as input (TT, C99) were evaluated on all
test collections; performance of the remaining algo-
rithms (C99l, DP, DPmin) was measured only for
</bodyText>
<sectionHeader confidence="0.859263" genericHeader="method">
AC.
</sectionHeader>
<subsectionHeader confidence="0.999427">
3.3 Pre-processing variants
</subsectionHeader>
<bodyText confidence="0.999890538461539">
We decided to prepare seven variants of the test col-
lections (see Table 3). The motivation for the first
group (variants: P1, P2, P3, P4) was to be as close
to Choi’s methodology as possible. That’s why we
used simple pre-processing techniques like lemma-
tization and stop-lists. The remaining variants (P5,
P6, P7) were chosen arbitrarily to check how addi-
tional morphological information will influence the
performance of the main segmentation algorithms in
case of Polish language.
The pre-processing stage included two steps. Ini-
tially, documents were split into sentences and word
tokens (punctuation signs were removed) by means
of tokenizer and sentence boundary recognizer of
SProUT — a shallow text processing system tai-
lored to processing Polish language (Piskorski et
al., 2004). Afterwards the generated token stream
was normalized; for this task SProUT’s interface for
a dictionary based Polish morphological analyzer
— Morfeusz (Woli´nski, 2007) was used. This al-
lowed us to use variety of morphological informa-
tion (including STEM, POS, NUMBER, TENSE).
The drawback of such an approach was that to-
kens not present in Morfeusz’s dictionary were not
stemmed (accounting for 12.8% of all tokens or
31.7% of unique tokens; note that Morfeusz input
</bodyText>
<page confidence="0.99645">
54
</page>
<table confidence="0.9994525">
Id Variant Description
P1 I no changes (tokens remain inflected)
P2 L lemmatized tokens
P3 LSL L − words in the lemmatized stop-list
P4 ISI I − without words in the inflected stop-list
P5 L-VT L + verbs tagged with POS and TENSE
P6 L-VT- L-VT + nouns and adjectives tagged with
N-A POS
P7 L-VT- L-VT-N-A + nouns and adjectives tagged
NN-AN with NUMBER
</table>
<tableCaption confidence="0.999894">
Table 3: pre-processing variants
</tableCaption>
<bodyText confidence="0.9997858">
contained all the tokens including numbers, hyper-
links, dates, etc.). Another problematic issue was
ambiguity of morphological analysis (1.4 interpre-
tation on average); we addressed this issue by us-
ing the following order of preference: 1) verbs, 2)
nouns, 3) adjectives, 4) other word classes.
The stop-lists (inflected and lemmatized versions
contained 616 and 350 tokens respectively) were
prepared manually by analysing frequency lists of
previously used text corpus.
</bodyText>
<subsectionHeader confidence="0.951299">
3.4 Evaluation metrics
</subsectionHeader>
<bodyText confidence="0.9990895">
A number of different measurement methods were
applied to topical texts segmentation including
recall-precision pair (Hearst and Plaunt, 1993; Pas-
sonneau and Litman, 1997), edit distance (Ponte
and Croft, 1997), Pµ (Beeferman et al., 1997), Pk
(Beeferman et al., 1999) and WindowDiff (Pevzner
and Hearst, 2002).
Pk is simplified version of probabilistic measure
Pµ based on assumption that any two consecutive
boundaries are at distance of k sentences (k being
parameter normally set to half of length of aver-
age segment in reference segmentation). After some
simplifications Pk is defined by the following for-
mula (Flejter, 2006):
</bodyText>
<equation confidence="0.85879">
(|Sr(i, k) − Sh(i, k)|)
</equation>
<bodyText confidence="0.999976823529412">
where SX(i, k) equals to one if ith and (i + k)th
sentences are in the same segment of segmentation
X, otherwise it is equal to zero; X = r corresponds
to reference segmentation and X = h corresponds
to hypothetical (algorithm-generated) segmentation.
In most publication instead of performance mea-
surement using Pk, probabilistic error metric (P =
1 − Pk(r, h)) is applied. For easier comparison
with previous results we calculated this measure for
tested evaluation scenarios.
Based on a profound analysis of Pk and proba-
bilistic metric drawbacks more recently WindowD-
iff error measure was proposed based on counting
number of boundaries within window of size of k
sentences sliding parallelly over both hypothetical
and reference segmentations. WindowDiff can be
calculated by the following formula:
</bodyText>
<equation confidence="0.93491">
(|br(i, k) − bh(i, k) |&gt; 0)
</equation>
<bodyText confidence="0.997861166666667">
with bX(i, k) corresponding to the number of
boundaries between positions i and i + k in segmen-
tation X.
Both probabilistic error and WindowDiff measure
the segmentation error; therefore, the lower is their
value, the better is segmentation result.
</bodyText>
<sectionHeader confidence="0.997594" genericHeader="evaluation">
4 Experimental Results
</sectionHeader>
<bodyText confidence="0.990991333333333">
Calculated values of P and WindowDiff (WD)
measures were used to compare performance of
different algorithms, collections and pre-processing
strategies. If not stated otherwise, results displayed
in this Section correspond to P1 variant (no pre-
processing) of test collections.
</bodyText>
<table confidence="0.996072222222222">
C99 TT
P WD P WD
AC 0.365 0.355 0.527 0.638
AC1 0.360 0.350 0.539 0.639
AC2 0.387 0.360 0.435 0.436
AC3 0.370 0.360 0.549 0.650
AC4 0.359 0.364 0.551 0.821
SC 0.381 0.390 0.562 0.932
DC 0.429 0.477 0.554 0.877
</table>
<tableCaption confidence="0.99766">
Table 4: Comparison of methods not requiring num-
</tableCaption>
<bodyText confidence="0.8028335">
ber of segments as input
Comparative results of both algorithms not requir-
ing to provide expected number of segments as in-
put (i.e. C99 and TT) are listed in Table 4. C99
</bodyText>
<equation confidence="0.996816">
1
Pk(r, h) = 1 −
n−k�
Z=1
n − k
N−k
1
Wk(r, h) =
N
− k Z=1
</equation>
<page confidence="0.992359">
55
</page>
<bodyText confidence="0.999784666666667">
performs much better than TextTiling on all test col-
lections with extremely high WindowDiff value for
TextTiling (especially for longer texts). Both al-
gorithms perform better on artificial than on actual
documents; for C99 drop in performance between
stream and cities documents is also visible.
</bodyText>
<table confidence="0.999358428571429">
C99l DP DPmin
P WD P WD P WD
AC 0.339 0.332 0.353 0.338 0.462 0.485
AC1 0.342 0.336 0.368 0.353 0.460 0.483
AC2 0.324 0.304 0.343 0.318 0.455 0.483
AC3 0.342 0.337 0.347 0.332 0.464 0.487
AC4 0.338 0.341 0.310 0.300 0.475 0.495
</table>
<tableCaption confidence="0.946424">
Table 5: Comparison of methods requiring number
of segments as input
</tableCaption>
<bodyText confidence="0.997614368421053">
Probabilistic error and WindowDiff results for al-
gorithms requiring expected number of segments
as input (C99l, DP, DPmin) are listed in Ta-
ble 5. C99l performs slightly better than DotPlotting
with maximization strategy (DP) and the perfor-
mance of DotPlotting applying minimization strat-
egy (DPmin) is visibly lower. Results do not differ
significantly between AC subcollections suggesting
that length of document has minor impact on perfor-
mance.
As C99 was developed both in version requiring
and not requiring to specify the expected number of
segments, impact of this information on algorithm
performance was analyzed. As expected C99l out-
performs C99; in our experiments additional infor-
mation on segments count lowered the error rates by
4–16% (see: Table 6).
As previous research on English text segmenta-
tion (Choi, 2000) was led for the same artificial col-
</bodyText>
<table confidence="0.996397571428571">
C99 C99l A%
P WD P WD P WD
AC 0.365 0.355 0.339 0.332 7% 6%
AC1 0.360 0.350 0.342 0.336 5% 4%
AC2 0.387 0.360 0.324 0.304 16% 16%
AC3 0.370 0.360 0.342 0.337 8% 6%
AC4 0.359 0.364 0.338 0.341 6% 6%
</table>
<tableCaption confidence="0.980994">
Table 6: Impact of segments count provided as input
</tableCaption>
<table confidence="0.999954636363636">
3-11 3-5 6-8 9-11
C99(P3) PL 0.32 0.34 0.31 0.29
EN 0.13 0.18 0.10 0.10
C99l(P3) PL 0.30 0.27 0.29 0.28
EN 0.12 0.12 0.09 0.09
DP(P3) PL 0.32 0.28 0.26 0.24
EN 0.22 0.21 0.18 0.16
DPmin(P3) PL 0.46 0.46 0.46 0.47
EN n/a 0.34 0.37 0.37
TT(P3) PL 0.50 0.39 0.49 0.54
EN 0.54 0.45 0.52 0.53
</table>
<tableCaption confidence="0.999661">
Table 7: Polish versus English results
</tableCaption>
<bodyText confidence="0.999806266666667">
lection creation methodology (see section 3.1), algo-
rithm implementations and probabilistic error met-
ric, comparison of algorithms performance for Pol-
ish and English was possible. The results of such
comparison are gathered in Table 7; for English re-
sults were taken from Choi’s paper and for Polish
P3 variant of our artificial collection corresponding
to Choi’s pre-processing approach was used. Com-
parative analysis shows that all algorithms (except
for TT which is highly inefficient for both Polish
and English) perform significantly worse for Polish.
Our hypothesis is that it can be attributed both to
lower performance of pre-processing tools for Pol-
ish and usage of domain specific corpus as opposed
to balanced Brown corpus used by Choi.
</bodyText>
<table confidence="0.999653">
AC + C99l SC + C99 DC + C99
P WD P WD P WD
P1 0.365 0.355 0.381 0.390 0.429 0.477
P2 0.322 0.315 0.356 0.367 0.433 0.477
P3 0.319 0.311 0.354 0.368 0.452 0.497
P4 0.342 0.334 0.381 0.391 0.425 0.473
P5 0.362 0.318 0.356 0.368 0.435 0.480
P6 0.416 0.403 0.413 0.419 0.406 0.430
P7 0.416 0.403 0.413 0.419 0.406 0.430
A% 13% 12% 7% 6% 5% 10%
</table>
<tableCaption confidence="0.999594">
Table 8: Impact of different pre-processing variants
</tableCaption>
<bodyText confidence="0.9992025">
Final part of our analysis of quantitative results
focused on impact of different pre-processing strate-
gies. For each of three test collections we analyzed
the impact of pre-processing strategies in case of
</bodyText>
<page confidence="0.988612">
56
</page>
<bodyText confidence="0.999593181818182">
best performing algorithm.
As the results from Table 8 show, for artificial and
stream collections the most promising strategy is to
use P3 (LSL) variant of pre-processing which de-
creased the error metric by up to 13% as compared
with P1 (no pre-processing) variant. Interestingly,
the same approach applied to individual documents
collection actually increased values of error met-
rics; in this case significant decrease of error rates
was possible by use of the most complex P6 and P7
strategies. Reasons for this difference may include:
a) disproportion in number of unrecognized tokens
(22.8% for DC vs. 12.75% for AC/SC), b) different
structure of DC reference segments (higher number
of shorter sentences, see: Table 2), c) standard de-
viation of segment length in DC much higher than
in AC/DC (35% of average length in case of DC vs.
25% in case of both AC and SC). We leave analysis
of this factors’ impact for future work.
Our analysis also shows that adding NUMBER
tag for nouns and adjectives (P7) has no impact on
algorithms performance as compared with P6.
</bodyText>
<sectionHeader confidence="0.998055" genericHeader="conclusions">
5 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.99960576">
In this paper we analyzed performance of several
topical text segmentation algorithms for Polish with
several pre-processing strategies on three different
test collections. Our research demonstrate that sim-
ilarly to English C99 (and its variant with expected
segments count input C991) is the best performing
segmentation algorithm and we recommend that it
be applied to text segmentation for Polish. Based on
our research we also suggest that lemmatization and
stop-list words removal (P3 variant) be used for fur-
ther improvement of performance. However, our re-
search revealed that the performance of almost all al-
gorithms (including C99) is significantly worse for
Polish than for English and remains unsatisfactory.
Therefore our further research direction will be to
focus on improvements at the pre-processing stages
of text segmentation (including enhancements in
text division into sentences, lemmatization of proper
names, and filtering of unrecognized tokens with
low document-based frequency) as well as on anal-
ysis of performance of more recent algorithms both
requiring and not requiring linguistic resources. We
would like also to evaluate text segmentation impact
on performance of coreference resolution algorithm
we are currently developing.
</bodyText>
<sectionHeader confidence="0.989259" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998831511111111">
James Allan, Jaime Carbonell, George Doddington,
Jonathan Yamron, and Yiming Yang. 1998. Topic de-
tection and tracking pilot study. final report.
Doug Beeferman, Adam Berger, and John Lafferty.
1997. Text segmentation using exponential models. In
Claire Cardie and Ralph Weischedel, editors, Proceed-
ings of the Second Conference on Empirical Methods
in Natural Language Processing, pages 35–46. Asso-
ciation for Computational Linguistics, Somerset, New
Jersey.
Doug Beeferman, Adam Berger, and John Lafferty.
1999. Statistical models for text segmentation. Ma-
chine Learning, 34(1-3):177–210.
David M. Blei and Pedro J. Moreno. 2001. Topic seg-
mentation with an aspect hidden Markov model. In
SIGIR ’01: Proceedings of the 24th annual interna-
tional ACM SIGIR conference on Research and devel-
opment in information retrieval, pages 343–348, New
York, NY, USA. ACM Press.
Jean Carletta. 1996. Assessing agreement on classifica-
tion task: the kappa statistic. Computational Linguis-
tic, 22:250–254.
Freddy Y. Y. Choi. 2000. Advances in domain indepen-
dent linear text segmentation. In Proceedings of the
first conference on North American chapter of the As-
sociation for Computational Linguistics, pages 26–33,
San Francisco, CA, USA. Morgan Kaufmann Publish-
ers Inc.
Freddy Y. Y. Choi. 2002. Content-based Text Naviga-
tion. Ph.D. thesis, Department of Computer Science,
University of Manchester.
EuroPAP. 2005. Serwis o Unii Europejskiej,
http://euro.pap.com.pl/ (2001-2005).
Dominik Flejter. 2006. Automatic topical segmentation
of documents in text collections (in polish). Master’s
thesis, Poznan University of Economics, June.
Marti A. Hearst and Christian Plaunt. 1993. Subtopic
structuring for full-length document access. In SI-
GIR ’93: Proceedings of the 16th annual international
ACM SIGIR conference on Research and development
in information retrieval, pages 59–68, New York, NY,
USA. ACM Press.
Stefan Kaufmann. 1999. Cohesion and collocation: Us-
ing context vectors in text segmentation. In Proceed-
ings of the 37th annual meeting of the Association for
</reference>
<page confidence="0.980791">
57
</page>
<reference confidence="0.999871839285714">
Computational Linguistics on Computational Linguis-
tics, pages 591–595, Morristown, NJ, USA. Associa-
tion for Computational Linguistics.
Hideki Kozima. 1993. Text segmentation based on sim-
ilarity between words. In Proceedings of the 31st an-
nual meeting on Association for Computational Lin-
guistics, pages 286–288, Morristown, NJ, USA. Asso-
ciation for Computational Linguistics.
C. Manning. 1998. Rethinking text segmentation mod-
els: An information extraction case study. Technical
report, University of Sydney.
Kathleen R. McKeown Min-Yen Kan, Judith L. Klavans.
1998. Linear segmentation and segment significance.
In Proceedings of 6th International Workshop of Very
Large Corpora (WVLC-6), pages 197–205.
Jane Morris and Graeme Hirst. 1991. Lexical cohe-
sion computed by thesaural relations as an indicator
of the structure of text. Computational Linguistics,
17(1):21–48.
P. Mulbregt, I. van, L. Gillick, S. Lowe, and J. Yamron.
1998. Text segmentation and topic tracking on broad-
cast news via a hidden markov model approach. In
Proceedings of the ICSLP’98, volume 6, pages 2519–
2522.
Rebecca J. Passonneau and Diane J. Litman. 1997. Dis-
course segmentation by human and automated means.
Comput. Linguist., 23(1):103–139.
Lev Pevzner and Marti A. Hearst. 2002. A critique and
improvement of an evaluation metric for text segmen-
tation. Comput. Linguist., 28(1):19–36.
Jakub Piskorski, Peter Homola, Malgorzata Marciniak,
Agnieszka Mykowiecka, Adam Przepi´orkowski, and
Marcin Wolinski. 2004. Information extraction for
Polish using the SProUT platform. In Intelligent In-
formation Processing and Web Mining, Proceedings
of the International Intelligent Information Systems:
IIPWM’04 Conference, Advances in Soft Computing,
pages 227–236. Springer.
Jay M. Ponte and W. Bruce Croft. 1997. Text segmen-
tation by topic. In ECDL ’97: Proceedings of the
First European Conference on Research and Advanced
Technology for Digital Libraries, pages 113–125, Lon-
don, UK. Springer-Verlag.
Jeffrey C. Reynar. 1994. An automatic method of finding
topic boundaries. In Proceedings of the 32nd annual
meeting on Association for Computational Linguistics,
pages 331–333, Morristown, NJ, USA. Association for
Computational Linguistics.
Jeffrey C. Reynar. 1998. Topic segmentation: algorithms
and applications. A dissertation in Computer and In-
formation Science. Ph.D. thesis, University of Penn-
sylvania.
Wikipedia. 2007. Miasta w Polsce wedlug liczby
ludnosci, Accessed on 20.03.2007.
Marcin Woli´nski. 2007. Analizator morfologiczny Mor-
feusz SIAT. On-line. Accessed on: 30.01.2007.
</reference>
<page confidence="0.999262">
58
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.068598">
<title confidence="0.999811">Unsupervised Methods of Topical Text Segmentation for Polish</title>
<author confidence="0.998233">Dominik</author>
<affiliation confidence="0.996473">University of</affiliation>
<author confidence="0.703843">Niepodległo´sci</author>
<affiliation confidence="0.683322">Pozna´n,</affiliation>
<email confidence="0.978991">kie.ae.poznan.pl</email>
<author confidence="0.884684">Karol</author>
<affiliation confidence="0.991018">University of</affiliation>
<author confidence="0.703799">Niepodległo´sci</author>
<affiliation confidence="0.682029">Pozna´n,</affiliation>
<email confidence="0.988951">kie.ae.poznan.pl</email>
<affiliation confidence="0.7145015">Witold University of</affiliation>
<author confidence="0.712707">Niepodległo´sci</author>
<affiliation confidence="0.682854">Pozna´n,</affiliation>
<email confidence="0.995174">kie.ae.poznan.pl</email>
<abstract confidence="0.993713833333333">This paper describes a study on performance of existing unsupervised algorithms of text documents topical segmentation when applied to Polish plain text documents. For performance measurement five existing topical segmentation algorithms were selected, three different Polish test collections were created and seven approaches to text preprocessing were implemented. Based on results WindowDiff metrics) use of specific algorithm was recommended and impact of pre-processing strategies was assessed. Thanks to use of standardized metrics and application of previously described methodology for test collection development, comparative results for Polish and English were also obtained.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>James Allan</author>
<author>Jaime Carbonell</author>
<author>George Doddington</author>
<author>Jonathan Yamron</author>
<author>Yiming Yang</author>
</authors>
<title>Topic detection and tracking pilot study. final report.</title>
<date>1998</date>
<contexts>
<context position="2733" citStr="Allan et al., 1998" startWordPosition="378" endWordPosition="382">ified. This approach is applicable in several IR and NLP areas including document indexing, automatic summarization and question answering (Choi, 2002). For Information Extraction domain, two main application of topical document segmentation are related to documents pre-processing and supporting some basic tasks widely used by IE tools. Topical segmentation applied to individual documents as well as documents streams (e.g. dialogues of radio broadcast transcripts) is an initial step for further IE processing (Manning, 1998); when combined with segments labelling, classification or clustering (Allan et al., 1998) it allows to pre-select ranges of text to be mined for mentions of events, entities and relations relevant to users needs and available IE resources. This limits significantly size of text to be processed by IE methods (Hearst and Plaunt, 1993) and thus influences significantly overall IE performance. On the other hand, many basic tasks required by IE domain (both related to IE resources creation and document (pre-)processing) make use of sections rather than whole documents. In these tasks including language modelling (esp. gathering of co-occurrences statistics for trigger-based language mo</context>
</contexts>
<marker>Allan, Carbonell, Doddington, Yamron, Yang, 1998</marker>
<rawString>James Allan, Jaime Carbonell, George Doddington, Jonathan Yamron, and Yiming Yang. 1998. Topic detection and tracking pilot study. final report.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Text segmentation using exponential models.</title>
<date>1997</date>
<booktitle>In Claire Cardie and Ralph Weischedel, editors, Proceedings of the Second Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>35--46</pages>
<location>Somerset, New Jersey.</location>
<contexts>
<context position="7451" citStr="Beeferman et al., 1997" startWordPosition="1088" endWordPosition="1091">ally related in systematic or nonsystematic way). Other relevant linguistic theories include Grosz and Sinder’s discourse segmentation theory, Rhetorical Structure Theory and taxonomy of text structures proposed Skorochod’ko (Reynar, 1998). Out of empirical statistical rules some authors make use of heuristics resulting form Heaps’ law for new-words-based topical segment boundary detection. However, the most important theoretical foundations in quantitative methods are related to strong probabilistic frameworks including Hidden Markov Models (Mulbregt et al., 1998) and Maximal Entropy Theory (Beeferman et al., 1997). 2.2 Basic Methods The most simple but also the most frequently used methods of topical text segmentation do not require training (thus they are domain-independent) nor make use of any complex linguistic resources or utilities. Apart from methods based on new vocabulary analysis, this category of algorithms applies widely the simplest form of lexical cohesion i.e. reiterations of the same word. 52 The first classical text segmentation algorithm of this type is TextTiling described in (Hearst and Plaunt, 1993). Its analysis unit consists of pseudosentences corresponding to series of consecutiv</context>
<context position="11179" citStr="Beeferman et al., 1997" startWordPosition="1653" endWordPosition="1656">1993) (which depend on weights-based semantic network constructed from thesaurus). In both cases the effort put in algorithm enactment is quite high; however in principle no additional resources need to be developed for new texts (even from different domains). 2.4 Methods Requiring Training Last group of methods includes supervised methods with generally strong mathematical foundations. They perform very well; however they require training that possibly needs to be repeated when new domain needs to be addressed. The methods in this group use probabilistic frameworks including maximal entropy (Beeferman et al., 1997), Hidden Markov Models (Mulbregt et al., 1998) and Probabilistic Latent Semantic Analysis (Blei and Moreno, 2001). 3 Experimental Setup Segmentation algorithms performance was evaluated for 42 scenarios corresponding to different algorithms, pre-processing strategies and test collections. For quantitative analysis and comparability with previous and future research results two standard segmentation metrics were applied in all scenarios. 3.1 Test Collections For performance measurement three test collections corresponding to different types of segmentation tasks were developed: artificial docum</context>
<context position="17177" citStr="Beeferman et al., 1997" startWordPosition="2576" endWordPosition="2579">f morphological analysis (1.4 interpretation on average); we addressed this issue by using the following order of preference: 1) verbs, 2) nouns, 3) adjectives, 4) other word classes. The stop-lists (inflected and lemmatized versions contained 616 and 350 tokens respectively) were prepared manually by analysing frequency lists of previously used text corpus. 3.4 Evaluation metrics A number of different measurement methods were applied to topical texts segmentation including recall-precision pair (Hearst and Plaunt, 1993; Passonneau and Litman, 1997), edit distance (Ponte and Croft, 1997), Pµ (Beeferman et al., 1997), Pk (Beeferman et al., 1999) and WindowDiff (Pevzner and Hearst, 2002). Pk is simplified version of probabilistic measure Pµ based on assumption that any two consecutive boundaries are at distance of k sentences (k being parameter normally set to half of length of average segment in reference segmentation). After some simplifications Pk is defined by the following formula (Flejter, 2006): (|Sr(i, k) − Sh(i, k)|) where SX(i, k) equals to one if ith and (i + k)th sentences are in the same segment of segmentation X, otherwise it is equal to zero; X = r corresponds to reference segmentation and X</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1997</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1997. Text segmentation using exponential models. In Claire Cardie and Ralph Weischedel, editors, Proceedings of the Second Conference on Empirical Methods in Natural Language Processing, pages 35–46. Association for Computational Linguistics, Somerset, New Jersey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Doug Beeferman</author>
<author>Adam Berger</author>
<author>John Lafferty</author>
</authors>
<title>Statistical models for text segmentation.</title>
<date>1999</date>
<booktitle>Machine Learning,</booktitle>
<pages>34--1</pages>
<contexts>
<context position="17206" citStr="Beeferman et al., 1999" startWordPosition="2581" endWordPosition="2584"> interpretation on average); we addressed this issue by using the following order of preference: 1) verbs, 2) nouns, 3) adjectives, 4) other word classes. The stop-lists (inflected and lemmatized versions contained 616 and 350 tokens respectively) were prepared manually by analysing frequency lists of previously used text corpus. 3.4 Evaluation metrics A number of different measurement methods were applied to topical texts segmentation including recall-precision pair (Hearst and Plaunt, 1993; Passonneau and Litman, 1997), edit distance (Ponte and Croft, 1997), Pµ (Beeferman et al., 1997), Pk (Beeferman et al., 1999) and WindowDiff (Pevzner and Hearst, 2002). Pk is simplified version of probabilistic measure Pµ based on assumption that any two consecutive boundaries are at distance of k sentences (k being parameter normally set to half of length of average segment in reference segmentation). After some simplifications Pk is defined by the following formula (Flejter, 2006): (|Sr(i, k) − Sh(i, k)|) where SX(i, k) equals to one if ith and (i + k)th sentences are in the same segment of segmentation X, otherwise it is equal to zero; X = r corresponds to reference segmentation and X = h corresponds to hypotheti</context>
</contexts>
<marker>Beeferman, Berger, Lafferty, 1999</marker>
<rawString>Doug Beeferman, Adam Berger, and John Lafferty. 1999. Statistical models for text segmentation. Machine Learning, 34(1-3):177–210.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Blei</author>
<author>Pedro J Moreno</author>
</authors>
<title>Topic segmentation with an aspect hidden Markov model.</title>
<date>2001</date>
<booktitle>In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>343--348</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="11292" citStr="Blei and Moreno, 2001" startWordPosition="1670" endWordPosition="1673"> algorithm enactment is quite high; however in principle no additional resources need to be developed for new texts (even from different domains). 2.4 Methods Requiring Training Last group of methods includes supervised methods with generally strong mathematical foundations. They perform very well; however they require training that possibly needs to be repeated when new domain needs to be addressed. The methods in this group use probabilistic frameworks including maximal entropy (Beeferman et al., 1997), Hidden Markov Models (Mulbregt et al., 1998) and Probabilistic Latent Semantic Analysis (Blei and Moreno, 2001). 3 Experimental Setup Segmentation algorithms performance was evaluated for 42 scenarios corresponding to different algorithms, pre-processing strategies and test collections. For quantitative analysis and comparability with previous and future research results two standard segmentation metrics were applied in all scenarios. 3.1 Test Collections For performance measurement three test collections corresponding to different types of segmentation tasks were developed: artificial documents collection (AC), stream collection (5C) and individual documents collection (DC). AC and 5C were constructed</context>
</contexts>
<marker>Blei, Moreno, 2001</marker>
<rawString>David M. Blei and Pedro J. Moreno. 2001. Topic segmentation with an aspect hidden Markov model. In SIGIR ’01: Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval, pages 343–348, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jean Carletta</author>
</authors>
<title>Assessing agreement on classification task: the kappa statistic.</title>
<date>1996</date>
<journal>Computational Linguistic,</journal>
<pages>22--250</pages>
<contexts>
<context position="13717" citStr="Carletta, 1996" startWordPosition="2039" endWordPosition="2040">P were used as text streams (936 messages). The reference segmentation was created using original article boundaries present in EuroPAP mail messages (almost 30000 segments were marked). For individual documents collection development text content was extracted from Wikipedia documents, all headings were removed and all list items (LI tags) with no terminal punctuation sign were added a dot. Manual tagging by two authors of this paper was performed. The instructions were to put segment boundaries in the places of potential section titles. Obtained percent agreement of 0.988 and n coefficient (Carletta, 1996) of 0.975 suggest high convergence of both annotations. Further, in places where the two annotators opinions differed (one marked boundary and the other did not), negotiation-based approach (Flejter, 2006) was applied in order to develop reference segmentation. 3.2 Selected algorithms In our experiment we used Choi’s publicly available implementation of several text segmentation algorithms not requiring training (with several adaptaSentences Tokens avg std avg std AC 6.8 1.7 122.6 33.4 SC 15.0 3.8 267.6 64.0 DC 28.5 9.9 300.0 110.2 Table 2: The average length of reference segments tion concern</context>
</contexts>
<marker>Carletta, 1996</marker>
<rawString>Jean Carletta. 1996. Assessing agreement on classification task: the kappa statistic. Computational Linguistic, 22:250–254.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Advances in domain independent linear text segmentation.</title>
<date>2000</date>
<booktitle>In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics,</booktitle>
<pages>26--33</pages>
<publisher>Morgan Kaufmann Publishers Inc.</publisher>
<location>San Francisco, CA, USA.</location>
<contexts>
<context position="9568" citStr="Choi, 2000" startWordPosition="1413" endWordPosition="1414">hart points are drawn at coordinates (x, y) and (y, x) iff words at positions x and y are equal. In this settings coherent text segments correspond visually to squares with high density of points. DotPlotting image is than segmented using one of two strategies: minimization of points density at the boundaries (minimization of external incoherence) or maximization of density of segments (maximization of internal coherence) (Reynar, 1998). The original DotPlotting algorithm requires to explicitly provide expected number of segments as input. Improved version of DotPlotting algorithm called C99 (Choi, 2000) uses DotPlotting chart for visualization of similarity measurements at consecutive point of the text (thus resulting in point with different levels of intensity) instead of words cooccurences. Afterwards, mask-based ranking technique is used for image enhancement. For actual segmentation, dynamic programming technique similar to DotPlotting maximization algorithm is used; an optional automatic termination strategy is also implemented thus allowing the algorithm to assess number of boundaries. In further work of the same and other authors several enhancements of C99 algorithm were proposed. 2.</context>
<context position="12509" citStr="Choi, 2000" startWordPosition="1848" endWordPosition="1849"> on 936 issues of EuroPAP (European information service of Polish Press Agency) plain-text e-mail newsletter (EuroPAP, 2005) collected from November 2001 to May 2005. Typical issue of EuroPAP newsletter contained about 53 25 complete news articles and a number of short (containing at most several sentences) news items. DC was constructed based on articles retrieved form Wikipedia corresponding to ten most populous Polish cities (Wikipedia, 2007) covering typically several topics (e.g. geography, culture, transportation). For AC creation we followed precisely the method applied for English in (Choi, 2000). Each artificial document was created as a concatenation of random number (n) of first sentences from ten news articles randomly selected from a total of 24927 news items in EuroPAP corpus. Four subcollections were created depending on allowed range of n as listed in Table 1. Any two selected articles were assumed to cover two different topics; thus reference segmentation boundaries corresponded to points of concatenations. AC AC1 AC2 AC3 AC4 n 3-11 3-5 6-8 9-11 documents count 400 100 100 100 Table 1: Artificial collection subcollections For 5C creation newsletter messages from EuroPAP were </context>
<context position="20885" citStr="Choi, 2000" startWordPosition="3191" endWordPosition="3192">egy (DP) and the performance of DotPlotting applying minimization strategy (DPmin) is visibly lower. Results do not differ significantly between AC subcollections suggesting that length of document has minor impact on performance. As C99 was developed both in version requiring and not requiring to specify the expected number of segments, impact of this information on algorithm performance was analyzed. As expected C99l outperforms C99; in our experiments additional information on segments count lowered the error rates by 4–16% (see: Table 6). As previous research on English text segmentation (Choi, 2000) was led for the same artificial colC99 C99l A% P WD P WD P WD AC 0.365 0.355 0.339 0.332 7% 6% AC1 0.360 0.350 0.342 0.336 5% 4% AC2 0.387 0.360 0.324 0.304 16% 16% AC3 0.370 0.360 0.342 0.337 8% 6% AC4 0.359 0.364 0.338 0.341 6% 6% Table 6: Impact of segments count provided as input 3-11 3-5 6-8 9-11 C99(P3) PL 0.32 0.34 0.31 0.29 EN 0.13 0.18 0.10 0.10 C99l(P3) PL 0.30 0.27 0.29 0.28 EN 0.12 0.12 0.09 0.09 DP(P3) PL 0.32 0.28 0.26 0.24 EN 0.22 0.21 0.18 0.16 DPmin(P3) PL 0.46 0.46 0.46 0.47 EN n/a 0.34 0.37 0.37 TT(P3) PL 0.50 0.39 0.49 0.54 EN 0.54 0.45 0.52 0.53 Table 7: Polish versus Eng</context>
</contexts>
<marker>Choi, 2000</marker>
<rawString>Freddy Y. Y. Choi. 2000. Advances in domain independent linear text segmentation. In Proceedings of the first conference on North American chapter of the Association for Computational Linguistics, pages 26–33, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Freddy Y Y Choi</author>
</authors>
<title>Content-based Text Navigation.</title>
<date>2002</date>
<tech>Ph.D. thesis,</tech>
<institution>Department of Computer Science, University of Manchester.</institution>
<contexts>
<context position="2265" citStr="Choi, 2002" startWordPosition="312" endWordPosition="313">ng including robust and efficient information extraction and retrieval. One way of increasing efficiency of typical text processing tasks consists in processing separately 51 text segments instead of whole documents. While different text segmentation strategies can be applied including splitting text into segments of equal length, using moving windows of constant size or discourse units (Reynar, 1998), division into topical segment is intuitively more justified. This approach is applicable in several IR and NLP areas including document indexing, automatic summarization and question answering (Choi, 2002). For Information Extraction domain, two main application of topical document segmentation are related to documents pre-processing and supporting some basic tasks widely used by IE tools. Topical segmentation applied to individual documents as well as documents streams (e.g. dialogues of radio broadcast transcripts) is an initial step for further IE processing (Manning, 1998); when combined with segments labelling, classification or clustering (Allan et al., 1998) it allows to pre-select ranges of text to be mined for mentions of events, entities and relations relevant to users needs and avail</context>
<context position="3833" citStr="Choi, 2002" startWordPosition="547" endWordPosition="548">e tasks including language modelling (esp. gathering of co-occurrences statistics for trigger-based language models), anaphora resolution, word sense disambiguation and coreference detection, definition of proper context is crucial. To this extend entities Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 51–58, Prague, June 2007. c�2007 Association for Computational Linguistics discovered by topical segmentation are more reliable than document, paragraph or sentence contexts as they help avoid usage of unrelated parts of documents as well as minimize sparse data problems (Choi, 2002). 1.1 Problem Statement In this study we adopted definition proposed in (Flejter, 2006) stating that ”linear topical segmentation of text consists in extracting coherent blocks of text such that: 1) any block focuses on exactly one topic 2) any pair of consecutive blocks have different main topics.” We also accepted that two segments should be defined as having different or same topics whenever they are judged so by people. Depending on the used document collection and user needs, text segmentation objective is to find topical segments in individual multi-topical text documents or to discover </context>
</contexts>
<marker>Choi, 2002</marker>
<rawString>Freddy Y. Y. Choi. 2002. Content-based Text Navigation. Ph.D. thesis, Department of Computer Science, University of Manchester.</rawString>
</citation>
<citation valid="true">
<authors>
<author>EuroPAP</author>
</authors>
<title>Serwis o Unii Europejskiej,</title>
<date>2005</date>
<pages>2001--2005</pages>
<location>http://euro.pap.com.pl/</location>
<contexts>
<context position="12022" citStr="EuroPAP, 2005" startWordPosition="1773" endWordPosition="1774"> algorithms, pre-processing strategies and test collections. For quantitative analysis and comparability with previous and future research results two standard segmentation metrics were applied in all scenarios. 3.1 Test Collections For performance measurement three test collections corresponding to different types of segmentation tasks were developed: artificial documents collection (AC), stream collection (5C) and individual documents collection (DC). AC and 5C were constructed based on 936 issues of EuroPAP (European information service of Polish Press Agency) plain-text e-mail newsletter (EuroPAP, 2005) collected from November 2001 to May 2005. Typical issue of EuroPAP newsletter contained about 53 25 complete news articles and a number of short (containing at most several sentences) news items. DC was constructed based on articles retrieved form Wikipedia corresponding to ten most populous Polish cities (Wikipedia, 2007) covering typically several topics (e.g. geography, culture, transportation). For AC creation we followed precisely the method applied for English in (Choi, 2000). Each artificial document was created as a concatenation of random number (n) of first sentences from ten news a</context>
</contexts>
<marker>EuroPAP, 2005</marker>
<rawString>EuroPAP. 2005. Serwis o Unii Europejskiej, http://euro.pap.com.pl/ (2001-2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dominik Flejter</author>
</authors>
<title>Automatic topical segmentation of documents in text collections (in polish). Master’s thesis,</title>
<date>2006</date>
<institution>Poznan University of Economics,</institution>
<contexts>
<context position="3920" citStr="Flejter, 2006" startWordPosition="560" endWordPosition="562">r trigger-based language models), anaphora resolution, word sense disambiguation and coreference detection, definition of proper context is crucial. To this extend entities Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 51–58, Prague, June 2007. c�2007 Association for Computational Linguistics discovered by topical segmentation are more reliable than document, paragraph or sentence contexts as they help avoid usage of unrelated parts of documents as well as minimize sparse data problems (Choi, 2002). 1.1 Problem Statement In this study we adopted definition proposed in (Flejter, 2006) stating that ”linear topical segmentation of text consists in extracting coherent blocks of text such that: 1) any block focuses on exactly one topic 2) any pair of consecutive blocks have different main topics.” We also accepted that two segments should be defined as having different or same topics whenever they are judged so by people. Depending on the used document collection and user needs, text segmentation objective is to find topical segments in individual multi-topical text documents or to discover boundaries between consecutive stories or news items (e.g. in radio broadcast transcrip</context>
<context position="13922" citStr="Flejter, 2006" startWordPosition="2070" endWordPosition="2071">ments collection development text content was extracted from Wikipedia documents, all headings were removed and all list items (LI tags) with no terminal punctuation sign were added a dot. Manual tagging by two authors of this paper was performed. The instructions were to put segment boundaries in the places of potential section titles. Obtained percent agreement of 0.988 and n coefficient (Carletta, 1996) of 0.975 suggest high convergence of both annotations. Further, in places where the two annotators opinions differed (one marked boundary and the other did not), negotiation-based approach (Flejter, 2006) was applied in order to develop reference segmentation. 3.2 Selected algorithms In our experiment we used Choi’s publicly available implementation of several text segmentation algorithms not requiring training (with several adaptaSentences Tokens avg std avg std AC 6.8 1.7 122.6 33.4 SC 15.0 3.8 267.6 64.0 DC 28.5 9.9 300.0 110.2 Table 2: The average length of reference segments tion concerning pre-processing stages). Specifically we used Choi’s implementation of TextTiling algorithm (TT), C99 algorithm for both known (C99l) and unknown (C99) number of boundaries as well as DotPlotting maximi</context>
<context position="17568" citStr="Flejter, 2006" startWordPosition="2640" endWordPosition="2641">erent measurement methods were applied to topical texts segmentation including recall-precision pair (Hearst and Plaunt, 1993; Passonneau and Litman, 1997), edit distance (Ponte and Croft, 1997), Pµ (Beeferman et al., 1997), Pk (Beeferman et al., 1999) and WindowDiff (Pevzner and Hearst, 2002). Pk is simplified version of probabilistic measure Pµ based on assumption that any two consecutive boundaries are at distance of k sentences (k being parameter normally set to half of length of average segment in reference segmentation). After some simplifications Pk is defined by the following formula (Flejter, 2006): (|Sr(i, k) − Sh(i, k)|) where SX(i, k) equals to one if ith and (i + k)th sentences are in the same segment of segmentation X, otherwise it is equal to zero; X = r corresponds to reference segmentation and X = h corresponds to hypothetical (algorithm-generated) segmentation. In most publication instead of performance measurement using Pk, probabilistic error metric (P = 1 − Pk(r, h)) is applied. For easier comparison with previous results we calculated this measure for tested evaluation scenarios. Based on a profound analysis of Pk and probabilistic metric drawbacks more recently WindowDiff </context>
</contexts>
<marker>Flejter, 2006</marker>
<rawString>Dominik Flejter. 2006. Automatic topical segmentation of documents in text collections (in polish). Master’s thesis, Poznan University of Economics, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
<author>Christian Plaunt</author>
</authors>
<title>Subtopic structuring for full-length document access.</title>
<date>1993</date>
<booktitle>In SIGIR ’93: Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval,</booktitle>
<pages>59--68</pages>
<publisher>ACM Press.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="2978" citStr="Hearst and Plaunt, 1993" startWordPosition="422" endWordPosition="425">n are related to documents pre-processing and supporting some basic tasks widely used by IE tools. Topical segmentation applied to individual documents as well as documents streams (e.g. dialogues of radio broadcast transcripts) is an initial step for further IE processing (Manning, 1998); when combined with segments labelling, classification or clustering (Allan et al., 1998) it allows to pre-select ranges of text to be mined for mentions of events, entities and relations relevant to users needs and available IE resources. This limits significantly size of text to be processed by IE methods (Hearst and Plaunt, 1993) and thus influences significantly overall IE performance. On the other hand, many basic tasks required by IE domain (both related to IE resources creation and document (pre-)processing) make use of sections rather than whole documents. In these tasks including language modelling (esp. gathering of co-occurrences statistics for trigger-based language models), anaphora resolution, word sense disambiguation and coreference detection, definition of proper context is crucial. To this extend entities Balto-Slavonic Natural Language Processing 2007, June 29, 2007, pages 51–58, Prague, June 2007. c�2</context>
<context position="7966" citStr="Hearst and Plaunt, 1993" startWordPosition="1170" endWordPosition="1173">ks including Hidden Markov Models (Mulbregt et al., 1998) and Maximal Entropy Theory (Beeferman et al., 1997). 2.2 Basic Methods The most simple but also the most frequently used methods of topical text segmentation do not require training (thus they are domain-independent) nor make use of any complex linguistic resources or utilities. Apart from methods based on new vocabulary analysis, this category of algorithms applies widely the simplest form of lexical cohesion i.e. reiterations of the same word. 52 The first classical text segmentation algorithm of this type is TextTiling described in (Hearst and Plaunt, 1993). Its analysis unit consists of pseudosentences corresponding to series of consecutive words (typically 20 words). After the whole text is divided into pseudo-sentences, a window of 12 pseudo-sentences is slid over the text (with one pseudo-sentence step). At any position the window is decomposed into two six-pseudo-sentences blocks and their similarity is calculated by means of cosine measure. Measurements for all consecutive window positions (understood as positions of centre of the window) form lexical cohesion curve local minima of which correspond to segments boundaries. The original algo</context>
<context position="17079" citStr="Hearst and Plaunt, 1993" startWordPosition="2560" endWordPosition="2563"> the tokens including numbers, hyperlinks, dates, etc.). Another problematic issue was ambiguity of morphological analysis (1.4 interpretation on average); we addressed this issue by using the following order of preference: 1) verbs, 2) nouns, 3) adjectives, 4) other word classes. The stop-lists (inflected and lemmatized versions contained 616 and 350 tokens respectively) were prepared manually by analysing frequency lists of previously used text corpus. 3.4 Evaluation metrics A number of different measurement methods were applied to topical texts segmentation including recall-precision pair (Hearst and Plaunt, 1993; Passonneau and Litman, 1997), edit distance (Ponte and Croft, 1997), Pµ (Beeferman et al., 1997), Pk (Beeferman et al., 1999) and WindowDiff (Pevzner and Hearst, 2002). Pk is simplified version of probabilistic measure Pµ based on assumption that any two consecutive boundaries are at distance of k sentences (k being parameter normally set to half of length of average segment in reference segmentation). After some simplifications Pk is defined by the following formula (Flejter, 2006): (|Sr(i, k) − Sh(i, k)|) where SX(i, k) equals to one if ith and (i + k)th sentences are in the same segment o</context>
</contexts>
<marker>Hearst, Plaunt, 1993</marker>
<rawString>Marti A. Hearst and Christian Plaunt. 1993. Subtopic structuring for full-length document access. In SIGIR ’93: Proceedings of the 16th annual international ACM SIGIR conference on Research and development in information retrieval, pages 59–68, New York, NY, USA. ACM Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Stefan Kaufmann</author>
</authors>
<title>Cohesion and collocation: Using context vectors in text segmentation.</title>
<date>1999</date>
<booktitle>In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics,</booktitle>
<pages>591--595</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8695" citStr="Kaufmann, 1999" startWordPosition="1277" endWordPosition="1278"> After the whole text is divided into pseudo-sentences, a window of 12 pseudo-sentences is slid over the text (with one pseudo-sentence step). At any position the window is decomposed into two six-pseudo-sentences blocks and their similarity is calculated by means of cosine measure. Measurements for all consecutive window positions (understood as positions of centre of the window) form lexical cohesion curve local minima of which correspond to segments boundaries. The original algorithm was further enhanced in several ways including use of words similarity measurement based on co-occurrences (Kaufmann, 1999). Another group of basic algorithms makes use of technique of DotPlotting, originally proposed by Raynar in (Reynar, 1994). In this approach 2D chart is used for lexical cohesion analysis with both axes corresponding to positions (in words) in the text; on the chart points are drawn at coordinates (x, y) and (y, x) iff words at positions x and y are equal. In this settings coherent text segments correspond visually to squares with high density of points. DotPlotting image is than segmented using one of two strategies: minimization of points density at the boundaries (minimization of external i</context>
</contexts>
<marker>Kaufmann, 1999</marker>
<rawString>Stefan Kaufmann. 1999. Cohesion and collocation: Using context vectors in text segmentation. In Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics, pages 591–595, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hideki Kozima</author>
</authors>
<title>Text segmentation based on similarity between words.</title>
<date>1993</date>
<booktitle>In Proceedings of the 31st annual meeting on Association for Computational Linguistics,</booktitle>
<pages>286--288</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="10561" citStr="Kozima, 1993" startWordPosition="1562" endWordPosition="1563">ptional automatic termination strategy is also implemented thus allowing the algorithm to assess number of boundaries. In further work of the same and other authors several enhancements of C99 algorithm were proposed. 2.3 Methods Requiring External Resources Still not requiring training and domain independent, some methods make use of linguistic resources more sophisticated than stop-list. Two classes of such solution described in existing work are solutions using lexical chains (Morris and Hirst, 1991; MinYen Kan, 1998) (which require to use some thesaurus) and based on spreading activation (Kozima, 1993) (which depend on weights-based semantic network constructed from thesaurus). In both cases the effort put in algorithm enactment is quite high; however in principle no additional resources need to be developed for new texts (even from different domains). 2.4 Methods Requiring Training Last group of methods includes supervised methods with generally strong mathematical foundations. They perform very well; however they require training that possibly needs to be repeated when new domain needs to be addressed. The methods in this group use probabilistic frameworks including maximal entropy (Beefe</context>
</contexts>
<marker>Kozima, 1993</marker>
<rawString>Hideki Kozima. 1993. Text segmentation based on similarity between words. In Proceedings of the 31st annual meeting on Association for Computational Linguistics, pages 286–288, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Manning</author>
</authors>
<title>Rethinking text segmentation models: An information extraction case study.</title>
<date>1998</date>
<tech>Technical report,</tech>
<institution>University of Sydney.</institution>
<contexts>
<context position="2643" citStr="Manning, 1998" startWordPosition="368" endWordPosition="369">iscourse units (Reynar, 1998), division into topical segment is intuitively more justified. This approach is applicable in several IR and NLP areas including document indexing, automatic summarization and question answering (Choi, 2002). For Information Extraction domain, two main application of topical document segmentation are related to documents pre-processing and supporting some basic tasks widely used by IE tools. Topical segmentation applied to individual documents as well as documents streams (e.g. dialogues of radio broadcast transcripts) is an initial step for further IE processing (Manning, 1998); when combined with segments labelling, classification or clustering (Allan et al., 1998) it allows to pre-select ranges of text to be mined for mentions of events, entities and relations relevant to users needs and available IE resources. This limits significantly size of text to be processed by IE methods (Hearst and Plaunt, 1993) and thus influences significantly overall IE performance. On the other hand, many basic tasks required by IE domain (both related to IE resources creation and document (pre-)processing) make use of sections rather than whole documents. In these tasks including lan</context>
</contexts>
<marker>Manning, 1998</marker>
<rawString>C. Manning. 1998. Rethinking text segmentation models: An information extraction case study. Technical report, University of Sydney.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kathleen R McKeown Min-Yen Kan</author>
<author>Judith L Klavans</author>
</authors>
<title>Linear segmentation and segment significance.</title>
<date>1998</date>
<booktitle>In Proceedings of 6th International Workshop of Very Large Corpora (WVLC-6),</booktitle>
<pages>197--205</pages>
<marker>Kan, Klavans, 1998</marker>
<rawString>Kathleen R. McKeown Min-Yen Kan, Judith L. Klavans. 1998. Linear segmentation and segment significance. In Proceedings of 6th International Workshop of Very Large Corpora (WVLC-6), pages 197–205.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jane Morris</author>
<author>Graeme Hirst</author>
</authors>
<title>Lexical cohesion computed by thesaural relations as an indicator of the structure of text.</title>
<date>1991</date>
<journal>Computational Linguistics,</journal>
<volume>17</volume>
<issue>1</issue>
<contexts>
<context position="10455" citStr="Morris and Hirst, 1991" startWordPosition="1542" endWordPosition="1545"> For actual segmentation, dynamic programming technique similar to DotPlotting maximization algorithm is used; an optional automatic termination strategy is also implemented thus allowing the algorithm to assess number of boundaries. In further work of the same and other authors several enhancements of C99 algorithm were proposed. 2.3 Methods Requiring External Resources Still not requiring training and domain independent, some methods make use of linguistic resources more sophisticated than stop-list. Two classes of such solution described in existing work are solutions using lexical chains (Morris and Hirst, 1991; MinYen Kan, 1998) (which require to use some thesaurus) and based on spreading activation (Kozima, 1993) (which depend on weights-based semantic network constructed from thesaurus). In both cases the effort put in algorithm enactment is quite high; however in principle no additional resources need to be developed for new texts (even from different domains). 2.4 Methods Requiring Training Last group of methods includes supervised methods with generally strong mathematical foundations. They perform very well; however they require training that possibly needs to be repeated when new domain need</context>
</contexts>
<marker>Morris, Hirst, 1991</marker>
<rawString>Jane Morris and Graeme Hirst. 1991. Lexical cohesion computed by thesaural relations as an indicator of the structure of text. Computational Linguistics, 17(1):21–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Mulbregt</author>
<author>I van</author>
<author>L Gillick</author>
<author>S Lowe</author>
<author>J Yamron</author>
</authors>
<title>Text segmentation and topic tracking on broadcast news via a hidden markov model approach.</title>
<date>1998</date>
<booktitle>In Proceedings of the ICSLP’98,</booktitle>
<volume>6</volume>
<pages>2519--2522</pages>
<contexts>
<context position="7399" citStr="Mulbregt et al., 1998" startWordPosition="1079" endWordPosition="1082"> as use of terms which are more general or semantically related in systematic or nonsystematic way). Other relevant linguistic theories include Grosz and Sinder’s discourse segmentation theory, Rhetorical Structure Theory and taxonomy of text structures proposed Skorochod’ko (Reynar, 1998). Out of empirical statistical rules some authors make use of heuristics resulting form Heaps’ law for new-words-based topical segment boundary detection. However, the most important theoretical foundations in quantitative methods are related to strong probabilistic frameworks including Hidden Markov Models (Mulbregt et al., 1998) and Maximal Entropy Theory (Beeferman et al., 1997). 2.2 Basic Methods The most simple but also the most frequently used methods of topical text segmentation do not require training (thus they are domain-independent) nor make use of any complex linguistic resources or utilities. Apart from methods based on new vocabulary analysis, this category of algorithms applies widely the simplest form of lexical cohesion i.e. reiterations of the same word. 52 The first classical text segmentation algorithm of this type is TextTiling described in (Hearst and Plaunt, 1993). Its analysis unit consists of p</context>
<context position="11225" citStr="Mulbregt et al., 1998" startWordPosition="1660" endWordPosition="1663">etwork constructed from thesaurus). In both cases the effort put in algorithm enactment is quite high; however in principle no additional resources need to be developed for new texts (even from different domains). 2.4 Methods Requiring Training Last group of methods includes supervised methods with generally strong mathematical foundations. They perform very well; however they require training that possibly needs to be repeated when new domain needs to be addressed. The methods in this group use probabilistic frameworks including maximal entropy (Beeferman et al., 1997), Hidden Markov Models (Mulbregt et al., 1998) and Probabilistic Latent Semantic Analysis (Blei and Moreno, 2001). 3 Experimental Setup Segmentation algorithms performance was evaluated for 42 scenarios corresponding to different algorithms, pre-processing strategies and test collections. For quantitative analysis and comparability with previous and future research results two standard segmentation metrics were applied in all scenarios. 3.1 Test Collections For performance measurement three test collections corresponding to different types of segmentation tasks were developed: artificial documents collection (AC), stream collection (5C) a</context>
</contexts>
<marker>Mulbregt, van, Gillick, Lowe, Yamron, 1998</marker>
<rawString>P. Mulbregt, I. van, L. Gillick, S. Lowe, and J. Yamron. 1998. Text segmentation and topic tracking on broadcast news via a hidden markov model approach. In Proceedings of the ICSLP’98, volume 6, pages 2519– 2522.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rebecca J Passonneau</author>
<author>Diane J Litman</author>
</authors>
<title>Discourse segmentation by human and automated means.</title>
<date>1997</date>
<journal>Comput. Linguist.,</journal>
<volume>23</volume>
<issue>1</issue>
<contexts>
<context position="17109" citStr="Passonneau and Litman, 1997" startWordPosition="2564" endWordPosition="2568">bers, hyperlinks, dates, etc.). Another problematic issue was ambiguity of morphological analysis (1.4 interpretation on average); we addressed this issue by using the following order of preference: 1) verbs, 2) nouns, 3) adjectives, 4) other word classes. The stop-lists (inflected and lemmatized versions contained 616 and 350 tokens respectively) were prepared manually by analysing frequency lists of previously used text corpus. 3.4 Evaluation metrics A number of different measurement methods were applied to topical texts segmentation including recall-precision pair (Hearst and Plaunt, 1993; Passonneau and Litman, 1997), edit distance (Ponte and Croft, 1997), Pµ (Beeferman et al., 1997), Pk (Beeferman et al., 1999) and WindowDiff (Pevzner and Hearst, 2002). Pk is simplified version of probabilistic measure Pµ based on assumption that any two consecutive boundaries are at distance of k sentences (k being parameter normally set to half of length of average segment in reference segmentation). After some simplifications Pk is defined by the following formula (Flejter, 2006): (|Sr(i, k) − Sh(i, k)|) where SX(i, k) equals to one if ith and (i + k)th sentences are in the same segment of segmentation X, otherwise it</context>
</contexts>
<marker>Passonneau, Litman, 1997</marker>
<rawString>Rebecca J. Passonneau and Diane J. Litman. 1997. Discourse segmentation by human and automated means. Comput. Linguist., 23(1):103–139.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lev Pevzner</author>
<author>Marti A Hearst</author>
</authors>
<title>A critique and improvement of an evaluation metric for text segmentation.</title>
<date>2002</date>
<journal>Comput. Linguist.,</journal>
<volume>28</volume>
<issue>1</issue>
<contexts>
<context position="17248" citStr="Pevzner and Hearst, 2002" startWordPosition="2587" endWordPosition="2590">d this issue by using the following order of preference: 1) verbs, 2) nouns, 3) adjectives, 4) other word classes. The stop-lists (inflected and lemmatized versions contained 616 and 350 tokens respectively) were prepared manually by analysing frequency lists of previously used text corpus. 3.4 Evaluation metrics A number of different measurement methods were applied to topical texts segmentation including recall-precision pair (Hearst and Plaunt, 1993; Passonneau and Litman, 1997), edit distance (Ponte and Croft, 1997), Pµ (Beeferman et al., 1997), Pk (Beeferman et al., 1999) and WindowDiff (Pevzner and Hearst, 2002). Pk is simplified version of probabilistic measure Pµ based on assumption that any two consecutive boundaries are at distance of k sentences (k being parameter normally set to half of length of average segment in reference segmentation). After some simplifications Pk is defined by the following formula (Flejter, 2006): (|Sr(i, k) − Sh(i, k)|) where SX(i, k) equals to one if ith and (i + k)th sentences are in the same segment of segmentation X, otherwise it is equal to zero; X = r corresponds to reference segmentation and X = h corresponds to hypothetical (algorithm-generated) segmentation. In</context>
</contexts>
<marker>Pevzner, Hearst, 2002</marker>
<rawString>Lev Pevzner and Marti A. Hearst. 2002. A critique and improvement of an evaluation metric for text segmentation. Comput. Linguist., 28(1):19–36.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jakub Piskorski</author>
<author>Peter Homola</author>
<author>Malgorzata Marciniak</author>
<author>Agnieszka Mykowiecka</author>
<author>Adam Przepi´orkowski</author>
<author>Marcin Wolinski</author>
</authors>
<title>Information extraction for Polish using the SProUT platform.</title>
<date>2004</date>
<booktitle>In Intelligent Information Processing and Web Mining, Proceedings of the International Intelligent Information Systems: IIPWM’04 Conference, Advances in Soft Computing,</booktitle>
<pages>227--236</pages>
<publisher>Springer.</publisher>
<marker>Piskorski, Homola, Marciniak, Mykowiecka, Przepi´orkowski, Wolinski, 2004</marker>
<rawString>Jakub Piskorski, Peter Homola, Malgorzata Marciniak, Agnieszka Mykowiecka, Adam Przepi´orkowski, and Marcin Wolinski. 2004. Information extraction for Polish using the SProUT platform. In Intelligent Information Processing and Web Mining, Proceedings of the International Intelligent Information Systems: IIPWM’04 Conference, Advances in Soft Computing, pages 227–236. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay M Ponte</author>
<author>W Bruce Croft</author>
</authors>
<title>Text segmentation by topic.</title>
<date>1997</date>
<booktitle>In ECDL ’97: Proceedings of the First European Conference on Research and Advanced Technology for Digital Libraries,</booktitle>
<pages>113--125</pages>
<publisher>Springer-Verlag.</publisher>
<location>London, UK.</location>
<contexts>
<context position="17148" citStr="Ponte and Croft, 1997" startWordPosition="2571" endWordPosition="2574">ematic issue was ambiguity of morphological analysis (1.4 interpretation on average); we addressed this issue by using the following order of preference: 1) verbs, 2) nouns, 3) adjectives, 4) other word classes. The stop-lists (inflected and lemmatized versions contained 616 and 350 tokens respectively) were prepared manually by analysing frequency lists of previously used text corpus. 3.4 Evaluation metrics A number of different measurement methods were applied to topical texts segmentation including recall-precision pair (Hearst and Plaunt, 1993; Passonneau and Litman, 1997), edit distance (Ponte and Croft, 1997), Pµ (Beeferman et al., 1997), Pk (Beeferman et al., 1999) and WindowDiff (Pevzner and Hearst, 2002). Pk is simplified version of probabilistic measure Pµ based on assumption that any two consecutive boundaries are at distance of k sentences (k being parameter normally set to half of length of average segment in reference segmentation). After some simplifications Pk is defined by the following formula (Flejter, 2006): (|Sr(i, k) − Sh(i, k)|) where SX(i, k) equals to one if ith and (i + k)th sentences are in the same segment of segmentation X, otherwise it is equal to zero; X = r corresponds to</context>
</contexts>
<marker>Ponte, Croft, 1997</marker>
<rawString>Jay M. Ponte and W. Bruce Croft. 1997. Text segmentation by topic. In ECDL ’97: Proceedings of the First European Conference on Research and Advanced Technology for Digital Libraries, pages 113–125, London, UK. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>An automatic method of finding topic boundaries.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>331--333</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Morristown, NJ, USA.</location>
<contexts>
<context position="8817" citStr="Reynar, 1994" startWordPosition="1295" endWordPosition="1296">do-sentence step). At any position the window is decomposed into two six-pseudo-sentences blocks and their similarity is calculated by means of cosine measure. Measurements for all consecutive window positions (understood as positions of centre of the window) form lexical cohesion curve local minima of which correspond to segments boundaries. The original algorithm was further enhanced in several ways including use of words similarity measurement based on co-occurrences (Kaufmann, 1999). Another group of basic algorithms makes use of technique of DotPlotting, originally proposed by Raynar in (Reynar, 1994). In this approach 2D chart is used for lexical cohesion analysis with both axes corresponding to positions (in words) in the text; on the chart points are drawn at coordinates (x, y) and (y, x) iff words at positions x and y are equal. In this settings coherent text segments correspond visually to squares with high density of points. DotPlotting image is than segmented using one of two strategies: minimization of points density at the boundaries (minimization of external incoherence) or maximization of density of segments (maximization of internal coherence) (Reynar, 1998). The original DotPl</context>
</contexts>
<marker>Reynar, 1994</marker>
<rawString>Jeffrey C. Reynar. 1994. An automatic method of finding topic boundaries. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 331–333, Morristown, NJ, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jeffrey C Reynar</author>
</authors>
<title>Topic segmentation: algorithms and applications. A dissertation in Computer and Information Science.</title>
<date>1998</date>
<tech>Ph.D. thesis,</tech>
<institution>University of Pennsylvania.</institution>
<contexts>
<context position="2058" citStr="Reynar, 1998" startWordPosition="284" endWordPosition="285">cuments consists in multitopical documents. This abundance of complex but plain text or just visually structured documents (as is in case of most HTML files) creates a strong need for intelligent text processing including robust and efficient information extraction and retrieval. One way of increasing efficiency of typical text processing tasks consists in processing separately 51 text segments instead of whole documents. While different text segmentation strategies can be applied including splitting text into segments of equal length, using moving windows of constant size or discourse units (Reynar, 1998), division into topical segment is intuitively more justified. This approach is applicable in several IR and NLP areas including document indexing, automatic summarization and question answering (Choi, 2002). For Information Extraction domain, two main application of topical document segmentation are related to documents pre-processing and supporting some basic tasks widely used by IE tools. Topical segmentation applied to individual documents as well as documents streams (e.g. dialogues of radio broadcast transcripts) is an initial step for further IE processing (Manning, 1998); when combined</context>
<context position="7067" citStr="Reynar, 1998" startWordPosition="1035" endWordPosition="1036">alliday and Hassan had the strongest impact on topical text segmentation. It analyzes several mechanisms of documents internal cohesion including references, substitution, ellipsis, conjunction (logical relations) and lexical cohesion (reuse of the same words to address the same object or objects of the same class as well as use of terms which are more general or semantically related in systematic or nonsystematic way). Other relevant linguistic theories include Grosz and Sinder’s discourse segmentation theory, Rhetorical Structure Theory and taxonomy of text structures proposed Skorochod’ko (Reynar, 1998). Out of empirical statistical rules some authors make use of heuristics resulting form Heaps’ law for new-words-based topical segment boundary detection. However, the most important theoretical foundations in quantitative methods are related to strong probabilistic frameworks including Hidden Markov Models (Mulbregt et al., 1998) and Maximal Entropy Theory (Beeferman et al., 1997). 2.2 Basic Methods The most simple but also the most frequently used methods of topical text segmentation do not require training (thus they are domain-independent) nor make use of any complex linguistic resources o</context>
<context position="9397" citStr="Reynar, 1998" startWordPosition="1390" endWordPosition="1391">posed by Raynar in (Reynar, 1994). In this approach 2D chart is used for lexical cohesion analysis with both axes corresponding to positions (in words) in the text; on the chart points are drawn at coordinates (x, y) and (y, x) iff words at positions x and y are equal. In this settings coherent text segments correspond visually to squares with high density of points. DotPlotting image is than segmented using one of two strategies: minimization of points density at the boundaries (minimization of external incoherence) or maximization of density of segments (maximization of internal coherence) (Reynar, 1998). The original DotPlotting algorithm requires to explicitly provide expected number of segments as input. Improved version of DotPlotting algorithm called C99 (Choi, 2000) uses DotPlotting chart for visualization of similarity measurements at consecutive point of the text (thus resulting in point with different levels of intensity) instead of words cooccurences. Afterwards, mask-based ranking technique is used for image enhancement. For actual segmentation, dynamic programming technique similar to DotPlotting maximization algorithm is used; an optional automatic termination strategy is also im</context>
</contexts>
<marker>Reynar, 1998</marker>
<rawString>Jeffrey C. Reynar. 1998. Topic segmentation: algorithms and applications. A dissertation in Computer and Information Science. Ph.D. thesis, University of Pennsylvania.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wikipedia</author>
</authors>
<title>Miasta w Polsce wedlug liczby ludnosci,</title>
<date>2007</date>
<journal>Accessed on</journal>
<pages>20--03</pages>
<contexts>
<context position="12347" citStr="Wikipedia, 2007" startWordPosition="1825" endWordPosition="1826">mentation tasks were developed: artificial documents collection (AC), stream collection (5C) and individual documents collection (DC). AC and 5C were constructed based on 936 issues of EuroPAP (European information service of Polish Press Agency) plain-text e-mail newsletter (EuroPAP, 2005) collected from November 2001 to May 2005. Typical issue of EuroPAP newsletter contained about 53 25 complete news articles and a number of short (containing at most several sentences) news items. DC was constructed based on articles retrieved form Wikipedia corresponding to ten most populous Polish cities (Wikipedia, 2007) covering typically several topics (e.g. geography, culture, transportation). For AC creation we followed precisely the method applied for English in (Choi, 2000). Each artificial document was created as a concatenation of random number (n) of first sentences from ten news articles randomly selected from a total of 24927 news items in EuroPAP corpus. Four subcollections were created depending on allowed range of n as listed in Table 1. Any two selected articles were assumed to cover two different topics; thus reference segmentation boundaries corresponded to points of concatenations. AC AC1 AC</context>
</contexts>
<marker>Wikipedia, 2007</marker>
<rawString>Wikipedia. 2007. Miasta w Polsce wedlug liczby ludnosci, Accessed on 20.03.2007.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marcin Woli´nski</author>
</authors>
<date>2007</date>
<booktitle>Analizator morfologiczny Morfeusz SIAT. On-line. Accessed on:</booktitle>
<pages>30--01</pages>
<marker>Woli´nski, 2007</marker>
<rawString>Marcin Woli´nski. 2007. Analizator morfologiczny Morfeusz SIAT. On-line. Accessed on: 30.01.2007.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>