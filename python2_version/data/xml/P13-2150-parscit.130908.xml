<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.941328">
Explicit and Implicit Syntactic Features for Text Classification
</title>
<author confidence="0.998223">
Matt Post&apos; and Shane Bergsma&apos;,2
</author>
<affiliation confidence="0.930192666666667">
&apos;Human Language Technology Center of Excellence
2Center for Language and Speech Processing
Johns Hopkins University
</affiliation>
<address confidence="0.588517">
Baltimore, MD
</address>
<sectionHeader confidence="0.843552" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99995415">
Syntactic features are useful for many
text classification tasks. Among these,
tree kernels (Collins and Duffy, 2001)
have been perhaps the most robust and
effective syntactic tool, appealing for
their empirical success, but also be-
cause they do not require an answer
to the difficult question of which tree
features to use for a given task. We
compare tree kernels to different ex-
plicit sets of tree features on five diverse
tasks, and find that explicit features of-
ten perform as well as tree kernels on
accuracy and always in orders of mag-
nitude less time, and with smaller mod-
els. Since explicit features are easy to
generate and use (with publicly avail-
able tools), we suggest they should al-
ways be included as baseline compar-
isons in tree kernel method evaluations.
</bodyText>
<sectionHeader confidence="0.996303" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999845038461539">
Features computed over parse trees are use-
ful for a range of discriminative tasks, in-
cluding authorship attribution (Baayen et al.,
1996), parse reranking (Collins and Duffy,
2002), language modeling (Cherry and Quirk,
2008), and native-language detection (Wong
and Dras, 2011). A major distinction among
these uses of syntax is how the features are rep-
resented. The implicit approach uses tree
kernels (Collins and Duffy, 2001), which make
predictions with inner products between tree
pairs. These products can be computed effi-
ciently with a dynamic program that produces
weighted counts of all the shared tree frag-
ments between a pair of trees, essentially in-
corporating all fragments without representing
any of them explicitly. Tree kernel approaches
have been applied successfully in many areas
of NLP (Collins and Duffy, 2002; Moschitti,
2004; Pighin and Moschitti, 2009).
Tree kernels were inspired in part by ideas
from Data-Oriented Parsing (Scha, 1990; Bod,
1993), which was in turn motivated by uncer-
tainty about which fragments to include in a
grammar. However, manual and automatic
approaches to inducing tree fragments have
recently been found to be useful in an ex-
plicit approach to text classification, which
employs specific tree fragments as features in
standard classifiers (Post, 2011; Wong and
Dras, 2011; Swanson and Charniak, 2012).
These feature sets necessarily represent only a
small subset of all possible tree patterns, leav-
ing open the question of what further gains
might be had from the unusued fragments.
Somewhat surprisingly, explicit and implicit
syntactic features have been explored largely
independently. Here, we compare them on a
range of classification tasks: (1,2) grammati-
cal classification (is a sentence written by a hu-
man?), (3) question classification (what type
of answer is sought by this question?), and
(4,5) native language prediction (what is the
native language of a text’s author?).
Our main contribution is to show that an ex-
plicit syntactic feature set performs as well or
better than tree kernels on each tested task,
and in orders of magnitude less time. Since
explicit features are simple to generate (with
publicly available tools) and flexible to use, we
recommend they be included as baseline com-
parisons in tree kernel method evaluations.
</bodyText>
<sectionHeader confidence="0.990575" genericHeader="method">
2 Experimental setup
</sectionHeader>
<bodyText confidence="0.847163">
We used the following feature sets:
N-grams All unigrams and bigrams.&apos;
&apos;Experiments with trigrams did not show any im-
</bodyText>
<page confidence="0.981176">
866
</page>
<bodyText confidence="0.959809971428571">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866–872,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
CFG rules Counts of depth-one context-
free grammar (CFG) productions obtained
from the Berkeley parser (Petrov et al., 2006).
C&amp;J features The parse-tree reranking
feature set of Charniak and Johnson (2005),
extracted from the Berkeley parse trees.
TSG features We also parsed with a
Bayesian tree substitution grammar (Post and
Gildea, 2009, TSG)2 and extracted fragment
counts from Viterbi derivations.
We build classifiers with LIBLINEAR3 (Fan
et al., 2008). We divided each dataset into
training, dev, and test sets. We then trained
an L2-regularized L1-loss support vector ma-
chine (-s 3) with a bias parameter of 1 (-B 1),
optimizing the regularization parameter (-c)
on the dev set over the range 10.0001 ... 1001
by multiples of 10. The best model was then
used to classify the test set. A sentence length
feature was included for every sentence.
For tree kernels, we used SVM-light-TK4
(Moschitti, 2004; Moschitti, 2006) with the
default settings (-t 5 -D 1 -L 0.4),5 which
also solves an L2-regularized L1-loss SVM op-
timization problem. We tuned the regulariza-
tion parameter (-c) on the dev set in the same
manner as described above, providing 4 GB of
memory to the kernel cache (-m 4000).6 We
used subset tree kernels, which compute the
similarity between two trees by implicitly enu-
merating all possible fragments of the trees (in
contrast with subtree kernels, where all frag-
ments fully extend to the leaves).
</bodyText>
<sectionHeader confidence="0.986604" genericHeader="method">
3 Tasks
</sectionHeader>
<bodyText confidence="0.652203">
Table 1 summarizes our datasets.
</bodyText>
<subsectionHeader confidence="0.999688">
3.1 Coarse grammatical classification
</subsectionHeader>
<bodyText confidence="0.9999098">
Our first comparison is coarse grammatical
classification, where the goal is to distin-
guish between human-written sentences and
“pseudo-negative” sentences sampled from a
trigram language model constructed from in-
</bodyText>
<footnote confidence="0.8905702">
provement.
2github.com/mjpost/dptsg
3www.csie.ntu.edu.tw/~cjlin/liblinear/
4disi.unitn.it/moschitti/Tree-Kernel.htm
5Optimizing SVM-TK’s decay parameter (-L) did
not improve test-set accuracy, but did increase training
time (squaring the number of hyperparameter combi-
nations to evaluate), so we stuck with the default.
6Increased from the default of 40 MB, which halves
the running time.
</footnote>
<figure confidence="0.889563615384615">
train dev test
Coarse grammaticality (BLLIP)
sentences 100,000 6,000 6,000
Fine grammaticality (PTB)
sentences 79,664 3,978 3,840
Question classification (TREC-10)
sentences 4,907 545 500
Native language (ICLE; 7 languages)
documents 490 105 175
sentences 17,715 3,968 6,777
Native language (ACL; 5 languages)
documents 987 195 185
sentences 146,257 28,139 28,403
</figure>
<tableCaption confidence="0.93445">
Table 1: Datasets.
</tableCaption>
<figure confidence="0.408002142857143">
system accuracy CPU time
Chance 50.0 -
N-gram 68.4 minutes
86.3 minutes
89.8 minutes
92.9 an hour
SVM-TK 91.0 a week
</figure>
<tableCaption confidence="0.844553">
Table 2: Coarse grammaticality. CPU time is
for classifier setup, training, and testing.
</tableCaption>
<bodyText confidence="0.998263">
domain data (Okanohara and Tsujii, 2007).
Cherry and Quirk (2008) first applied syn-
tax to this task, learning weighted parameters
for a CFG with a latent SVM. Post (2011)
found further improvements with fragment-
based representations (TSGs and C&amp;J) with a
regular SVM. Here, we compare their results
to kernel methods. We repeat Post’s experi-
ments on the BLLIP dataset,7 using his exact
data splits (Table 2). To our knowledge, tree
kernels have not been applied to this task.
</bodyText>
<subsectionHeader confidence="0.999282">
3.2 Fine grammatical classification
</subsectionHeader>
<bodyText confidence="0.999943777777778">
Real-world grammaticality judgments require
much finer-grained distinctions than the
coarse ones of the previous section (for exam-
ple, marking dropped determiners or wrong
verb inflections). For this task, we too pos-
itive examples from all sentences of sections
2–21 of the WSJ portion of the Penn Tree-
bank (Marcus et al., 1993). Negative exam-
ples were created by inserting one or two errors
</bodyText>
<figure confidence="0.9785071">
7LDC Catalog No. LDC2000T43
CFG
TSG
C&amp;J
867
Wong &amp; Dras
60.6 -
Chance
N-gram
50.0 -
61.4 minutes
CFG
TSG
C&amp;J
67.8 weeks
SVM-TK
system accuracy CPU time
64.5 minutes
67.0 minutes
71.9 an hour
</figure>
<figureCaption confidence="0.886331">
Table 3: Fine-grained classification accuracy
(the Wong and Dras (2010) score is the highest
score from the last column of their Table 3).
</figureCaption>
<table confidence="0.992502833333333">
system sent. voting whole
Wong &amp; Dras - - 80.0
Style 42.0 75.3 86.8
CFG 39.5 73.2 83.7
TSG 38.7 72.1 83.2
C&amp;J 42.9 76.3 86.3
SVM-TK 40.7 69.5 -
Style 42.5 65.3 83.7
CFG 39.2 52.6 86.3
TSG 40.4 56.8 84.7
C&amp;J 49.2 66.3 81.1
SVM-TK 42.1 52.6 -
</table>
<figure confidence="0.916182">
accuracy CPU time
86.6 -
73.2 seconds
90.0 seconds
85.6 seconds
89.6 minutes
</figure>
<tableCaption confidence="0.950361">
Table 5: Accuracy on ICLE (7 languages, top)
</tableCaption>
<bodyText confidence="0.543637">
and ACL (five, bottom) datasets at the sen-
tence and document levels. All documents
were signature-stylized (§3.4).
</bodyText>
<figure confidence="0.995764333333333">
system
Pighin &amp; Moschitti
Bigram
CFG
TSG
C&amp;J
</figure>
<tableCaption confidence="0.727731">
SVM-TK 87.7 twenty min.
Table 4: Question classification (6 classes).
</tableCaption>
<bodyText confidence="0.99997725">
into the parse trees from the positive data us-
ing GenERRate (Foster and Andersen, 2009).
An example sentence pair is But the ballplay-
ers disagree[ing], where the negative exam-
ple incorrectly inflects the verb. Wong and
Dras (2010) reported good results with parsers
trained separately on the positive and negative
sides of the training data and classifiers built
from comparisons between the CFG produc-
tions of those parsers. We obtained their data
splits (described as NoisyWSJ in their paper)
and repeat their experiments here (Table 3).
</bodyText>
<subsectionHeader confidence="0.99636">
3.3 Question Classification
</subsectionHeader>
<bodyText confidence="0.976775708333333">
We look next at question classification (QC).
Li and Roth (2002) introduced the TREC-10
dataset,8 a set of questions paired with labels
that categorize the question by the type of an-
swer it seeks. The labels are organized hi-
erarchically into six (coarse) top-level labels
and fifty (fine) refinements. An example ques-
tion from the ENTY/animal category is What
was the first domesticated bird?. Table 4 con-
tains results predicting just the coarse labels.
We compare to Pighin and Moschitti (2009),
and also repeat their experiments, finding a
slightly better result for them.
8cogcomp.cs.illinois.edu/Data/QA/QC/
We also experimented with the refined ver-
sion of the task, where we directly predict one
of the fifty refined categories, and found nearly
identical relative results, with the best explicit
feature set (CFG) returning an accuracy of
83.6% (in seconds), and the tree kernel system
69.8% (in an hour). For reference, Zhang and
Lee (2003) report 80.2% accuracy when train-
ing on the full training set (5,500 examples)
with an SVM and bag-of-words features.9
</bodyText>
<subsectionHeader confidence="0.898014">
3.4 Native language identification
</subsectionHeader>
<bodyText confidence="0.99995975">
Native language identification (NLI) is the
task of determining a text’s author’s native
language. This is usually cast as a document-
level task, since there are often not enough
cues to identify native languages at smaller
granularities. As such, this task presents a
challenge to tree kernels, which are defined at
the level of a single parse tree and have no ob-
vious document-level extension. Table 5 there-
fore presents three evaluations: (a) sentence-
level accuracy, and document-level accuracy
from (b) sentence-level voting and (c) direct,
whole-document classification.
We perform these experiments on two
datasets. In order to mitigate topic bias10 and
other problems that have been reported with
</bodyText>
<footnote confidence="0.9913622">
9Pighin and Moschitti (2009) did not report results
on this version of the task.
10E.g., when we train with all words, the keyword
’Japanese’ is a strong indicator for Japanese authors,
while ’Arabic’ is a strong indicator for English ones.
</footnote>
<page confidence="0.993277">
868
</page>
<bodyText confidence="0.999906571428571">
the ICLE dataset (Tetreault et al., 2012),11 we
preprocessed each dataset into two signature-
stylized versions by replacing all words not in a
stopword list.12 The first version replaces non-
stopwords with word classes computed from
surface-form signatures,13 and the second with
POS tags.14 N-gram features are then taken
from both stylized versions of the corpus.
Restricting the feature representation to be
topic-independent is standard-practice in sty-
lometric tasks like authorship attribution, gen-
der identification, and native-language identi-
fication (Mosteller and Wallace, 1984; Koppel
et al., 2003; Tomokiyo and Jones, 2001).
</bodyText>
<sectionHeader confidence="0.743206" genericHeader="method">
3.4.1 ICLE v.2
</sectionHeader>
<bodyText confidence="0.999932444444445">
The first dataset is a seven-language subset
of the International Corpus of Learner En-
glish, Version 2 (ICLE) (Granger et al., 2009),
which contains 3.7 million words of English
documents written by people with sixteen dif-
ferent native languages. Table 1 contains
scores, including one reported by Wong and
Dras (2011), who used the CFG and C&amp;J fea-
tures, and whose data splits we mirror.15
</bodyText>
<subsectionHeader confidence="0.593793">
3.4.2 ACL Anthology Network
</subsectionHeader>
<bodyText confidence="0.884484">
We also experimented with native language
classification on scientific documents using
a version of the ACL Anthology Network
(Radev et al., 2009, AAN) annotated for ex-
periments in stylemetric tasks, including a
native/non-native author judgment (Bergsma
et al., 2012). For NLI, we further anno-
tated this dataset in a semi-automatic fash-
ion for the five most-common native languages
of ACL authors in our training era: English,
Japanese, German, Chinese, and French. The
annotation heuristics, designed to favor pre-
cision over recall, provided annotations for
1,959 of 8,483 papers (23%) in the 2001–2009
AAN.16
11Including prompts, characters, and special tokens
that correlate strongly with particular outcomes.
12The stopword list contains the set of 524 SMART-
system stopwords used by Tomokiyo and Jones (2001),
plus punctuation and Latin abbreviations.
</bodyText>
<footnote confidence="0.940667">
13For example, suffix and capitalization.
14Via CRFTagger (Phan, 2006).
15Tetreault et al. reported accuracies up to 90.1 in a
cross-validation setting that isn’t directly comparable.
16Details and data at old-site.clsp.jhu.edu/
~sbergsma/Stylo/.
</footnote>
<bodyText confidence="0.7815625">
0 0.01 0.1 1 10 100 1,000
Figure1: Trainng time (1000 onds) vs.
training time (thousands of seconds
test accuracy for coarse grammaticality, plot-
ting test scores from models trained on 100,
300, 1k, 3k, 10k, 30k, and 100k instances.
</bodyText>
<sectionHeader confidence="0.99935" genericHeader="evaluation">
4 Discussion
</sectionHeader>
<bodyText confidence="0.999983307692308">
Syntactic features improve upon the n-gram
baseline for all tasks except whole-document
classification for ICLE. Tree kernels are often
among the best, but always trail (by orders
of magnitude) when runtime is considered.
Constructing the multi-class SVM-TK models
for the NLI tasks in particular was computa-
tionally burdensome, requiring cpu-months of
time. The C&amp;J features are similarly often the
best, but incur a runtime cost due to the large
models. CFG and TSG features balance per-
formance, model size, and runtime. We now
compare these approaches in more depth.
</bodyText>
<subsectionHeader confidence="0.997272">
4.1 Training time versus accuracy
</subsectionHeader>
<bodyText confidence="0.9969059">
Tree kernel training is quadratic in the size of
the training data, and its empirical slowness
is known. It is informative to examine learn-
ing curves to see how the time-accuracy trade-
offs extrapolate. We compared models trained
on the first 100, 300, 1k, 3k, 10k, 30k, and
100k data points of the coarse grammaticality
dataset, split evenly between positive and neg-
ative examples (Figure 1). SVM-TK improves
over the TSG and CFG models in the limit,
but at an extraordinary cost in training time:
100k training examples is already pushing the
bounds of practicality for tree kernel learning,
and generating curve’s next point would re-
quire several months of time. Kernel methods
also produce large models that result in slow
test-time performance, a problem dubbed the
“curse of kernelization” (Wang et al., 2010).
Approximate kernel methods designed to
scale to large datasets address this (Severyn
</bodyText>
<figure confidence="0.9885959">
CFG
TSG
C&amp;J
SVM-TK
uSVM-TK
100
90
80
70
60
</figure>
<page confidence="0.996064">
869
</page>
<bodyText confidence="0.998860166666667">
and Moschitti, 2010). We investigated the
uSVM-TK toolkit,17 which enables tuning the
tradeoff between training time and accuracy.
While faster than SVM-TK, its performance
was never better than explicit methods along
both dimensions (time and accuracy).
</bodyText>
<subsectionHeader confidence="0.982849">
4.2 Overfitting
</subsectionHeader>
<bodyText confidence="0.999952363636364">
Overfitting is also a problem for kernel meth-
ods. The best models often had a huge number
of support vectors, achieving near-perfect ac-
curacy on the training set but making many
errors on the dev. and test sets. On the ICLE
task, close to 75% of all the training exam-
ples were used as support vectors. We found
only half as many support vectors used for the
explicit representations, implying less error
(Vapnik, 1998), and saw much lower variance
between training and testing performance.
</bodyText>
<subsectionHeader confidence="0.995131">
4.3 Which fragments?
</subsectionHeader>
<bodyText confidence="0.999980586206896">
Our findings support the observations of
Cumby and Roth (2003), who point out that
kernels introduce a large number of irrelevant
features that may be especially harmful in
small-data settings, and that, when possible, it
is often better to have a set of explicit, relevant
features. In other words, it is better to have
the right features than all of them. Tree ker-
nels provide a robust, efficiently-computable
measure of comparison, but they also skirt the
difficult question, Which fragments?
So what are the “right” features? Table 6)
presents an intuitive list from the coarse gram-
maticality task: phenomena such as balanced
parenthetical phrases and quotations are asso-
ciated with grammaticality, while small, flat,
abstract rules indicate samples from the n-
gram model. Similar intuitive results hold for
the other tasks. The immediate interpretabil-
ity of the explicit formalisms is another ad-
vantage, although recent work has shown that
weights on the implicit features can also be ob-
tained after a kind of linearization of the tree
kernel (Pighin and Moschitti, 2009).
Ultimately, which features matter is task-
dependent, and skirting the question is ad-
vantageous in many settings. But it is also
encouraging that methods for selecting frag-
ments and other tree features work so well,
</bodyText>
<page confidence="0.34882">
17disi.unitn.it/~severyn/code.html
</page>
<table confidence="0.9716986">
(TOP (S “ S , ” NP (VP (VBZ says) ADVP) .))
(FRAG (X SYM) VP .)
(PRN (-LRB- -LRB-) S (-RRB- -RRB-))
(PRN (-LRB- -LRB-) NP (-RRB- -RRB-))
(S NPVP.)
(NP (NP DT CD (NN %)) PP)
(NP DT)
(PP (IN of))
(TOP (NP NP PP PP .))
(NP DT JJ NNS)
</table>
<tableCaption confidence="0.992799">
Table 6: The highest- and lowest-weighted
TSG features (coarse grammaticality).
</tableCaption>
<bodyText confidence="0.91082">
yielding quick, light-weight models that con-
trast with the heavy machinery of tree kernels.
</bodyText>
<sectionHeader confidence="0.995907" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999941694444444">
Tree kernels provide a robust measure of com-
parison between trees, effectively making use
of all fragments. We have shown that for
some tasks, it is sufficient (and advantageous)
to instead use an explicitly-represented subset
of them. In addition to their flexibility and
interpetability, explicit syntactic features of-
ten outperformed tree kernels in accuracy, and
even where they did not, the cost was multiple
orders of magnitude increase in both training
and testing time. These results were consistent
across a range of task types, dataset sizes, and
classification arities (binary and multiclass).
There are a number of important caveats.
We explored a range of data settings, but
there are many others where tree kernels have
been proven useful, such as parse tree rerank-
ing (Collins and Duffy, 2002; Shen and Joshi,
2003), sentence subjectivity (Suzuki et al.,
2004), pronoun resolution (Yang et al., 2006),
relation extraction (Culotta and Sorensen,
2004), machine translation evaluation (Liu
and Gildea, 2005), predicate-argument recog-
nition, and semantic role labeling (Pighin and
Moschitti, 2009). There are also tree ker-
nel variations such as dependency tree kernels
(Culotta and Sorensen, 2004) and shallow se-
mantic tree kernels (Moschitti et al., 2007).
These variables provide a rich environment for
future work; in the meantime, we take these re-
sults as compelling motivation for the contin-
ued development of explicit syntactic features
(both manual and automatically induced), and
suggest that such features should be part of
the baseline systems on applicable discrimina-
tive NLP tasks.
</bodyText>
<page confidence="0.994277">
870
</page>
<sectionHeader confidence="0.976178" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998888700934579">
Harald Baayen, Hans Van Halteren, and Fiona
Tweedie. 1996. Outside the cave of shadows:
Using syntactic annotation to enhance author-
ship attribution. Literary and Linguistic Com-
puting, 11(3):121.
Shane Bergsma, Matt Post, and David Yarowsky.
2012. Stylometric analysis of scientific arti-
cles. In Proc. of NAACL-HLT, pages 327–337,
Montr´eal, Canada, June. Association for Com-
putational Linguistics.
Rens Bod. 1993. Using an annotated corpus as a
stochastic grammar. In Proc. of ACL, Colum-
bus, Ohio, USA, June.
Eugene Charniak and Mark Johnson. 2005.
Coarse-to-fine n-best parsing and MaxEnt dis-
criminative reranking. In Proc. of ACL, pages
173–180, Ann Arbor, Michigan, USA, June.
Colin Cherry and Chris Quirk. 2008. Discrimi-
native, syntactic language modeling through la-
tent SVMs. In Proc. of AMTA, Waikiki, Hawaii,
USA, October.
Michael Collins and Nigel Duffy. 2001. Convolu-
tion kernels for natural language. In Proc. of
NIPS.
Michael Collins and Nigel Duffy. 2002. New rank-
ing algorithms for parsing and tagging: kernels
over discrete structures, and the voted percep-
tron. In Proc. of ACL, pages 173–180, Philadel-
phia, Pennsylvania, USA, July.
Aron Culotta and Jeffrey Sorensen. 2004. Depen-
dency tree kernels for relation extraction. In
Proc. of ACL, pages 423–429.
Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,
Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIB-
LINEAR: A library for large linear classification.
Journal of Machine Learning Research, 9:1871–
1874.
Jennifer Foster and Øistein E. Andersen. 2009.
GenERRate: Generating errors for use in gram-
matical error detection. In Proceedings of the
fourth workshop on innovative use of NLP for
building educational applications, pages 82–90.
Sylviane Granger, Estelle Dagneaux, Fanny Me-
unier, and Magali Paquot. 2009. The Inter-
national Corpus of Learner English. Version 2.
Handbook and CD-Rom.
Moshe Koppel, Shlomo Argamon, and Anat Rachel
Shimoni. 2003. Automatically categorizing
written texts by author gender. Literary and
Linguistic Computing, 17(4):401–412.
Xin Li and Dan Roth. 2002. Learning question
classifiers. In Proc. of COLING, pages 1–7.
Ding Liu and Daniel Gildea. 2005. Syntactic fea-
tures for evaluation of machine translation. In
Proceedings of the ACL Workshop on Intrinsic
and Extrinsic Evaluation Measures for Machine
Translation and/or Summarization, pages 25–
32.
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large an-
notated corpus of English: The Penn Treebank.
Computational Linguistics, 19(2):330.
Alessandro Moschitti, Silvia Quarteroni, Roberto
Basili, and Suresh Manandhar. 2007. Exploit-
ing syntactic and shallow semantic kernels for
question answer classification. In Proc. of ACL,
pages 776–783, Prague, Czech Republic, June.
Alessandro Moschitti. 2004. A study on convo-
lution kernels for shallow semantic parsing. In
Proc. of ACL.
Alessandro Moschitti. 2006. Making tree kernels
practical for natural language learning. In Proc.
of EACL, volume 6, pages 113–120.
Frederick Mosteller and David L. Wallace. 1984.
Applied Bayesian and Classical Inference: The
Case of the Federalist Papers. Springer-Verlag.
Daisuke Okanohara and Jun’ichi Tsujii. 2007.
A discriminative language model with pseudo-
negative samples. In Proc. of ACL, Prague,
Czech Republic, June.
Slav Petrov, Leon Barrett, Romain Thibaux, and
Dan Klein. 2006. Learning accurate, compact,
and interpretable tree annotation. In Proc. of
ACL, Sydney, Australia, July.
Xuan-Hieu Phan. 2006. CRFTagger: CRF En-
glish POS Tagger. crftagger.sourceforge.net.
Daniele Pighin and Alessandro Moschitti. 2009.
Reverse engineering of tree kernel feature spaces.
In Proc. of EMNLP, pages 111–120, Singapore,
August.
Matt Post and Daniel Gildea. 2009. Bayesian
learning of a tree substitution grammar. In
Proc. of ACL (short paper track), Suntec, Sin-
gapore, August.
Matt Post. 2011. Judging grammaticality with
tree substitution grammar derivations. In Proc.
of ACL, Portland, Oregon, USA, June.
Dragomir R. Radev, Pradeep Muthukrishnan, and
Vahed Qazvinian. 2009. The ACL anthology
network corpus. In Proc. of ACL Workshop on
Natural Language Processing and Information
Retrieval for Digital Libraries, pages 54–61.
Remko Scha. 1990. Taaltheorie en taaltechnologie;
competence en performance. In R. de Kort and
G.L.J. Leerdam, editors, Computertoepassingen
in de neerlandistiek, pages 7–22, Almere, the
Netherlands. De Vereniging.
</reference>
<page confidence="0.979723">
871
</page>
<reference confidence="0.99940493877551">
Aliaksei Severyn and Alessandro Moschitti. 2010.
Large-scale support vector learning with struc-
tural kernels. In Proc. of ECML/PKDD, pages
229–244.
Libin Shen and Aravind K. Joshi. 2003. An SVM-
based voting algorithm with application to parse
reranking. In Proc. of CoNLL, pages 9–16.
Jun Suzuki, Hideki Isozaki, and Eisaku Maeda.
2004. Convolution kernels with feature selection
for natural language processing tasks. In Proc.
of ACL, pages 119–126.
Benjamin Swanson and Eugene Charniak. 2012.
Native language detection with tree substitu-
tion grammars. In Proc. of ACL (short papers),
pages 193–197, Jeju Island, Korea, July.
Joel Tetreault, Daniel Blanchard, Aoife Cahill, and
Martin Chodorow. 2012. Native tongues, lost
and found: Resources and empirical evaluations
in native language identification. In Proc. of
COLING, pages 2585–2602, Mumbai, India, De-
cember.
Laura Mayfield Tomokiyo and Rosie Jones. 2001.
You’re not from ’round here, are you? Naive
Bayes detection of non-native utterances. In
Proc. of NAACL.
Vladimir N. Vapnik. 1998. Statistical Learning
Theory. John Wiley &amp; Sons.
Zhuang Wang, Koby Crammer, and Slobodan
Vucetic. 2010. Multi-class pegasos on a bud-
get. In ICML, pages 1143–1150.
Sze-Meng Jojo Wong and Mark Dras. 2010. Parser
features for sentence grammaticality classifica-
tion. In Proceedings of the Australasian Lan-
guage Technology Association Workshop, Mel-
bourne, Australia, December.
Sze-Meng Jojo Wong and Mark Dras. 2011. Ex-
ploiting parse structures for native language
identification. In Proc. of EMNLP, pages 1600–
1610, Edinburgh, Scotland, UK., July.
Xiaofeng Yang, Jian Su, and Chew Lim Tan.
2006. Kernel-based pronoun resolution with
structured syntactic knowledge. In Proc. of
Coling-ACL, pages 41–48.
Dell Zhang and Wee Sun Lee. 2003. Question
classification using support vector machines. In
Proceedings of the 26th annual international
ACM SIGIR conference on Research and de-
velopment in informaion retrieval, SIGIR ’03,
pages 26–32, New York, NY, USA. ACM.
</reference>
<page confidence="0.99808">
872
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.595588">
<title confidence="0.995932">Explicit and Implicit Syntactic Features for Text Classification Language Technology Center of for Language and Speech</title>
<author confidence="0.815684">Johns Hopkins</author>
<address confidence="0.780313">Baltimore, MD</address>
<abstract confidence="0.998315952380952">Syntactic features are useful for many text classification tasks. Among these, tree kernels (Collins and Duffy, 2001) have been perhaps the most robust and effective syntactic tool, appealing for their empirical success, but also because they do not require an answer the difficult question of features to use for a given task. We compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly available tools), we suggest they should always be included as baseline comparisons in tree kernel method evaluations.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Harald Baayen</author>
<author>Hans Van Halteren</author>
<author>Fiona Tweedie</author>
</authors>
<title>Outside the cave of shadows: Using syntactic annotation to enhance authorship attribution.</title>
<date>1996</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>11--3</pages>
<marker>Baayen, Van Halteren, Tweedie, 1996</marker>
<rawString>Harald Baayen, Hans Van Halteren, and Fiona Tweedie. 1996. Outside the cave of shadows: Using syntactic annotation to enhance authorship attribution. Literary and Linguistic Computing, 11(3):121.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Matt Post</author>
<author>David Yarowsky</author>
</authors>
<title>Stylometric analysis of scientific articles.</title>
<date>2012</date>
<booktitle>In Proc. of NAACL-HLT,</booktitle>
<pages>327--337</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Montr´eal, Canada,</location>
<contexts>
<context position="12125" citStr="Bergsma et al., 2012" startWordPosition="1900" endWordPosition="1903"> Corpus of Learner English, Version 2 (ICLE) (Granger et al., 2009), which contains 3.7 million words of English documents written by people with sixteen different native languages. Table 1 contains scores, including one reported by Wong and Dras (2011), who used the CFG and C&amp;J features, and whose data splits we mirror.15 3.4.2 ACL Anthology Network We also experimented with native language classification on scientific documents using a version of the ACL Anthology Network (Radev et al., 2009, AAN) annotated for experiments in stylemetric tasks, including a native/non-native author judgment (Bergsma et al., 2012). For NLI, we further annotated this dataset in a semi-automatic fashion for the five most-common native languages of ACL authors in our training era: English, Japanese, German, Chinese, and French. The annotation heuristics, designed to favor precision over recall, provided annotations for 1,959 of 8,483 papers (23%) in the 2001–2009 AAN.16 11Including prompts, characters, and special tokens that correlate strongly with particular outcomes. 12The stopword list contains the set of 524 SMARTsystem stopwords used by Tomokiyo and Jones (2001), plus punctuation and Latin abbreviations. 13For examp</context>
</contexts>
<marker>Bergsma, Post, Yarowsky, 2012</marker>
<rawString>Shane Bergsma, Matt Post, and David Yarowsky. 2012. Stylometric analysis of scientific articles. In Proc. of NAACL-HLT, pages 327–337, Montr´eal, Canada, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rens Bod</author>
</authors>
<title>Using an annotated corpus as a stochastic grammar.</title>
<date>1993</date>
<booktitle>In Proc. of ACL,</booktitle>
<location>Columbus, Ohio, USA,</location>
<contexts>
<context position="1996" citStr="Bod, 1993" startWordPosition="313" endWordPosition="314">The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about which fragments to include in a grammar. However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers (Post, 2011; Wong and Dras, 2011; Swanson and Charniak, 2012). These feature sets necessarily represent only a small subset of all possible tree patterns, leaving open the question of what further gains might be had from the unusued fragments. Somewhat surprisingly, explicit and </context>
</contexts>
<marker>Bod, 1993</marker>
<rawString>Rens Bod. 1993. Using an annotated corpus as a stochastic grammar. In Proc. of ACL, Columbus, Ohio, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
<author>Mark Johnson</author>
</authors>
<title>Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.</title>
<date>2005</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>173--180</pages>
<location>Ann Arbor, Michigan, USA,</location>
<contexts>
<context position="3860" citStr="Charniak and Johnson (2005)" startWordPosition="600" endWordPosition="603">se, we recommend they be included as baseline comparisons in tree kernel method evaluations. 2 Experimental setup We used the following feature sets: N-grams All unigrams and bigrams.&apos; &apos;Experiments with trigrams did not show any im866 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866–872, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics CFG rules Counts of depth-one contextfree grammar (CFG) productions obtained from the Berkeley parser (Petrov et al., 2006). C&amp;J features The parse-tree reranking feature set of Charniak and Johnson (2005), extracted from the Berkeley parse trees. TSG features We also parsed with a Bayesian tree substitution grammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with LIBLINEAR3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range 10.0001 ... 1001 by multiples of 10. The best model was then used to classify the test set. A sentence </context>
</contexts>
<marker>Charniak, Johnson, 2005</marker>
<rawString>Eugene Charniak and Mark Johnson. 2005. Coarse-to-fine n-best parsing and MaxEnt discriminative reranking. In Proc. of ACL, pages 173–180, Ann Arbor, Michigan, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Colin Cherry</author>
<author>Chris Quirk</author>
</authors>
<title>Discriminative, syntactic language modeling through latent SVMs.</title>
<date>2008</date>
<booktitle>In Proc. of AMTA,</booktitle>
<location>Waikiki, Hawaii, USA,</location>
<contexts>
<context position="1247" citStr="Cherry and Quirk, 2008" startWordPosition="195" endWordPosition="198">sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly available tools), we suggest they should always be included as baseline comparisons in tree kernel method evaluations. 1 Introduction Features computed over parse trees are useful for a range of discriminative tasks, including authorship attribution (Baayen et al., 1996), parse reranking (Collins and Duffy, 2002), language modeling (Cherry and Quirk, 2008), and native-language detection (Wong and Dras, 2011). A major distinction among these uses of syntax is how the features are represented. The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches have been applied successfully in many areas of NLP (Collins and Duff</context>
<context position="6411" citStr="Cherry and Quirk (2008)" startWordPosition="985" endWordPosition="988">ality (BLLIP) sentences 100,000 6,000 6,000 Fine grammaticality (PTB) sentences 79,664 3,978 3,840 Question classification (TREC-10) sentences 4,907 545 500 Native language (ICLE; 7 languages) documents 490 105 175 sentences 17,715 3,968 6,777 Native language (ACL; 5 languages) documents 987 195 185 sentences 146,257 28,139 28,403 Table 1: Datasets. system accuracy CPU time Chance 50.0 - N-gram 68.4 minutes 86.3 minutes 89.8 minutes 92.9 an hour SVM-TK 91.0 a week Table 2: Coarse grammaticality. CPU time is for classifier setup, training, and testing. domain data (Okanohara and Tsujii, 2007). Cherry and Quirk (2008) first applied syntax to this task, learning weighted parameters for a CFG with a latent SVM. Post (2011) found further improvements with fragmentbased representations (TSGs and C&amp;J) with a regular SVM. Here, we compare their results to kernel methods. We repeat Post’s experiments on the BLLIP dataset,7 using his exact data splits (Table 2). To our knowledge, tree kernels have not been applied to this task. 3.2 Fine grammatical classification Real-world grammaticality judgments require much finer-grained distinctions than the coarse ones of the previous section (for example, marking dropped de</context>
</contexts>
<marker>Cherry, Quirk, 2008</marker>
<rawString>Colin Cherry and Chris Quirk. 2008. Discriminative, syntactic language modeling through latent SVMs. In Proc. of AMTA, Waikiki, Hawaii, USA, October.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>Convolution kernels for natural language.</title>
<date>2001</date>
<booktitle>In Proc. of NIPS.</booktitle>
<contexts>
<context position="1451" citStr="Collins and Duffy, 2001" startWordPosition="227" endWordPosition="230">explicit features are easy to generate and use (with publicly available tools), we suggest they should always be included as baseline comparisons in tree kernel method evaluations. 1 Introduction Features computed over parse trees are useful for a range of discriminative tasks, including authorship attribution (Baayen et al., 1996), parse reranking (Collins and Duffy, 2002), language modeling (Cherry and Quirk, 2008), and native-language detection (Wong and Dras, 2011). A major distinction among these uses of syntax is how the features are represented. The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about whic</context>
</contexts>
<marker>Collins, Duffy, 2001</marker>
<rawString>Michael Collins and Nigel Duffy. 2001. Convolution kernels for natural language. In Proc. of NIPS.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
<author>Nigel Duffy</author>
</authors>
<title>New ranking algorithms for parsing and tagging: kernels over discrete structures, and the voted perceptron.</title>
<date>2002</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>173--180</pages>
<location>Philadelphia, Pennsylvania, USA,</location>
<contexts>
<context position="1203" citStr="Collins and Duffy, 2002" startWordPosition="189" endWordPosition="192">e compare tree kernels to different explicit sets of tree features on five diverse tasks, and find that explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly available tools), we suggest they should always be included as baseline comparisons in tree kernel method evaluations. 1 Introduction Features computed over parse trees are useful for a range of discriminative tasks, including authorship attribution (Baayen et al., 1996), parse reranking (Collins and Duffy, 2002), language modeling (Cherry and Quirk, 2008), and native-language detection (Wong and Dras, 2011). A major distinction among these uses of syntax is how the features are represented. The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches have been applied success</context>
<context position="18090" citStr="Collins and Duffy, 2002" startWordPosition="2852" endWordPosition="2855">stead use an explicitly-represented subset of them. In addition to their flexibility and interpetability, explicit syntactic features often outperformed tree kernels in accuracy, and even where they did not, the cost was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the conti</context>
</contexts>
<marker>Collins, Duffy, 2002</marker>
<rawString>Michael Collins and Nigel Duffy. 2002. New ranking algorithms for parsing and tagging: kernels over discrete structures, and the voted perceptron. In Proc. of ACL, pages 173–180, Philadelphia, Pennsylvania, USA, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aron Culotta</author>
<author>Jeffrey Sorensen</author>
</authors>
<title>Dependency tree kernels for relation extraction.</title>
<date>2004</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>423--429</pages>
<contexts>
<context position="18248" citStr="Culotta and Sorensen, 2004" startWordPosition="2874" endWordPosition="2877">ree kernels in accuracy, and even where they did not, the cost was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of explicit syntactic features (both manual and automatically induced), and suggest that such features should be part of the baseline systems</context>
</contexts>
<marker>Culotta, Sorensen, 2004</marker>
<rawString>Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction. In Proc. of ACL, pages 423–429.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rong-En Fan</author>
<author>Kai-Wei Chang</author>
<author>Cho-Jui Hsieh</author>
<author>Xiang-Rui Wang</author>
<author>Chih-Jen Lin</author>
</authors>
<title>LIBLINEAR: A library for large linear classification.</title>
<date>2008</date>
<journal>Journal of Machine Learning Research,</journal>
<volume>9</volume>
<pages>1874</pages>
<contexts>
<context position="4114" citStr="Fan et al., 2008" startWordPosition="638" endWordPosition="641">l Meeting of the Association for Computational Linguistics, pages 866–872, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics CFG rules Counts of depth-one contextfree grammar (CFG) productions obtained from the Berkeley parser (Petrov et al., 2006). C&amp;J features The parse-tree reranking feature set of Charniak and Johnson (2005), extracted from the Berkeley parse trees. TSG features We also parsed with a Bayesian tree substitution grammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with LIBLINEAR3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range 10.0001 ... 1001 by multiples of 10. The best model was then used to classify the test set. A sentence length feature was included for every sentence. For tree kernels, we used SVM-light-TK4 (Moschitti, 2004; Moschitti, 2006) with the default settings (-t 5 -D 1 -L 0.4),5 which also solves an L2-regularized L1-loss SVM optimization problem. We tuned the r</context>
</contexts>
<marker>Fan, Chang, Hsieh, Wang, Lin, 2008</marker>
<rawString>Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin. 2008. LIBLINEAR: A library for large linear classification. Journal of Machine Learning Research, 9:1871– 1874.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jennifer Foster</author>
<author>Øistein E Andersen</author>
</authors>
<title>GenERRate: Generating errors for use in grammatical error detection.</title>
<date>2009</date>
<booktitle>In Proceedings of the</booktitle>
<pages>82--90</pages>
<contexts>
<context position="8260" citStr="Foster and Andersen, 2009" startWordPosition="1301" endWordPosition="1304">2.0 75.3 86.8 CFG 39.5 73.2 83.7 TSG 38.7 72.1 83.2 C&amp;J 42.9 76.3 86.3 SVM-TK 40.7 69.5 - Style 42.5 65.3 83.7 CFG 39.2 52.6 86.3 TSG 40.4 56.8 84.7 C&amp;J 49.2 66.3 81.1 SVM-TK 42.1 52.6 - accuracy CPU time 86.6 - 73.2 seconds 90.0 seconds 85.6 seconds 89.6 minutes Table 5: Accuracy on ICLE (7 languages, top) and ACL (five, bottom) datasets at the sentence and document levels. All documents were signature-stylized (§3.4). system Pighin &amp; Moschitti Bigram CFG TSG C&amp;J SVM-TK 87.7 twenty min. Table 4: Question classification (6 classes). into the parse trees from the positive data using GenERRate (Foster and Andersen, 2009). An example sentence pair is But the ballplayers disagree[ing], where the negative example incorrectly inflects the verb. Wong and Dras (2010) reported good results with parsers trained separately on the positive and negative sides of the training data and classifiers built from comparisons between the CFG productions of those parsers. We obtained their data splits (described as NoisyWSJ in their paper) and repeat their experiments here (Table 3). 3.3 Question Classification We look next at question classification (QC). Li and Roth (2002) introduced the TREC-10 dataset,8 a set of questions pa</context>
</contexts>
<marker>Foster, Andersen, 2009</marker>
<rawString>Jennifer Foster and Øistein E. Andersen. 2009. GenERRate: Generating errors for use in grammatical error detection. In Proceedings of the fourth workshop on innovative use of NLP for building educational applications, pages 82–90.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sylviane Granger</author>
<author>Estelle Dagneaux</author>
<author>Fanny Meunier</author>
<author>Magali Paquot</author>
</authors>
<date>2009</date>
<booktitle>The International Corpus of Learner English. Version 2. Handbook and CD-Rom.</booktitle>
<contexts>
<context position="11571" citStr="Granger et al., 2009" startWordPosition="1814" endWordPosition="1817">he first version replaces nonstopwords with word classes computed from surface-form signatures,13 and the second with POS tags.14 N-gram features are then taken from both stylized versions of the corpus. Restricting the feature representation to be topic-independent is standard-practice in stylometric tasks like authorship attribution, gender identification, and native-language identification (Mosteller and Wallace, 1984; Koppel et al., 2003; Tomokiyo and Jones, 2001). 3.4.1 ICLE v.2 The first dataset is a seven-language subset of the International Corpus of Learner English, Version 2 (ICLE) (Granger et al., 2009), which contains 3.7 million words of English documents written by people with sixteen different native languages. Table 1 contains scores, including one reported by Wong and Dras (2011), who used the CFG and C&amp;J features, and whose data splits we mirror.15 3.4.2 ACL Anthology Network We also experimented with native language classification on scientific documents using a version of the ACL Anthology Network (Radev et al., 2009, AAN) annotated for experiments in stylemetric tasks, including a native/non-native author judgment (Bergsma et al., 2012). For NLI, we further annotated this dataset i</context>
</contexts>
<marker>Granger, Dagneaux, Meunier, Paquot, 2009</marker>
<rawString>Sylviane Granger, Estelle Dagneaux, Fanny Meunier, and Magali Paquot. 2009. The International Corpus of Learner English. Version 2. Handbook and CD-Rom.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Moshe Koppel</author>
</authors>
<title>Shlomo Argamon, and Anat Rachel Shimoni.</title>
<date>2003</date>
<booktitle>Literary and Linguistic Computing,</booktitle>
<pages>17--4</pages>
<marker>Koppel, 2003</marker>
<rawString>Moshe Koppel, Shlomo Argamon, and Anat Rachel Shimoni. 2003. Automatically categorizing written texts by author gender. Literary and Linguistic Computing, 17(4):401–412.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xin Li</author>
<author>Dan Roth</author>
</authors>
<title>Learning question classifiers.</title>
<date>2002</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>1--7</pages>
<contexts>
<context position="8805" citStr="Li and Roth (2002)" startWordPosition="1386" endWordPosition="1389"> trees from the positive data using GenERRate (Foster and Andersen, 2009). An example sentence pair is But the ballplayers disagree[ing], where the negative example incorrectly inflects the verb. Wong and Dras (2010) reported good results with parsers trained separately on the positive and negative sides of the training data and classifiers built from comparisons between the CFG productions of those parsers. We obtained their data splits (described as NoisyWSJ in their paper) and repeat their experiments here (Table 3). 3.3 Question Classification We look next at question classification (QC). Li and Roth (2002) introduced the TREC-10 dataset,8 a set of questions paired with labels that categorize the question by the type of answer it seeks. The labels are organized hierarchically into six (coarse) top-level labels and fifty (fine) refinements. An example question from the ENTY/animal category is What was the first domesticated bird?. Table 4 contains results predicting just the coarse labels. We compare to Pighin and Moschitti (2009), and also repeat their experiments, finding a slightly better result for them. 8cogcomp.cs.illinois.edu/Data/QA/QC/ We also experimented with the refined version of the</context>
</contexts>
<marker>Li, Roth, 2002</marker>
<rawString>Xin Li and Dan Roth. 2002. Learning question classifiers. In Proc. of COLING, pages 1–7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ding Liu</author>
<author>Daniel Gildea</author>
</authors>
<title>Syntactic features for evaluation of machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,</booktitle>
<pages>25--32</pages>
<contexts>
<context position="18303" citStr="Liu and Gildea, 2005" startWordPosition="2881" endWordPosition="2884">t was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of explicit syntactic features (both manual and automatically induced), and suggest that such features should be part of the baseline systems on applicable discriminative NLP tasks. 870 References</context>
</contexts>
<marker>Liu, Gildea, 2005</marker>
<rawString>Ding Liu and Daniel Gildea. 2005. Syntactic features for evaluation of machine translation. In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization, pages 25– 32.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics,</title>
<date>1993</date>
<contexts>
<context position="7186" citStr="Marcus et al., 1993" startWordPosition="1110" endWordPosition="1113">resentations (TSGs and C&amp;J) with a regular SVM. Here, we compare their results to kernel methods. We repeat Post’s experiments on the BLLIP dataset,7 using his exact data splits (Table 2). To our knowledge, tree kernels have not been applied to this task. 3.2 Fine grammatical classification Real-world grammaticality judgments require much finer-grained distinctions than the coarse ones of the previous section (for example, marking dropped determiners or wrong verb inflections). For this task, we too positive examples from all sentences of sections 2–21 of the WSJ portion of the Penn Treebank (Marcus et al., 1993). Negative examples were created by inserting one or two errors 7LDC Catalog No. LDC2000T43 CFG TSG C&amp;J 867 Wong &amp; Dras 60.6 - Chance N-gram 50.0 - 61.4 minutes CFG TSG C&amp;J 67.8 weeks SVM-TK system accuracy CPU time 64.5 minutes 67.0 minutes 71.9 an hour Table 3: Fine-grained classification accuracy (the Wong and Dras (2010) score is the highest score from the last column of their Table 3). system sent. voting whole Wong &amp; Dras - - 80.0 Style 42.0 75.3 86.8 CFG 39.5 73.2 83.7 TSG 38.7 72.1 83.2 C&amp;J 42.9 76.3 86.3 SVM-TK 40.7 69.5 - Style 42.5 65.3 83.7 CFG 39.2 52.6 86.3 TSG 40.4 56.8 84.7 C&amp;J</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
<author>Silvia Quarteroni</author>
<author>Roberto Basili</author>
<author>Suresh Manandhar</author>
</authors>
<title>Exploiting syntactic and shallow semantic kernels for question answer classification.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>776--783</pages>
<location>Prague, Czech Republic,</location>
<marker>Moschitti, Quarteroni, Basili, Manandhar, 2007</marker>
<rawString>Alessandro Moschitti, Silvia Quarteroni, Roberto Basili, and Suresh Manandhar. 2007. Exploiting syntactic and shallow semantic kernels for question answer classification. In Proc. of ACL, pages 776–783, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>A study on convolution kernels for shallow semantic parsing.</title>
<date>2004</date>
<booktitle>In Proc. of ACL.</booktitle>
<contexts>
<context position="1871" citStr="Moschitti, 2004" startWordPosition="294" endWordPosition="295">ive-language detection (Wong and Dras, 2011). A major distinction among these uses of syntax is how the features are represented. The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about which fragments to include in a grammar. However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers (Post, 2011; Wong and Dras, 2011; Swanson and Charniak, 2012). These feature sets necessarily represent only a small subset of all possible tree patterns, </context>
<context position="4564" citStr="Moschitti, 2004" startWordPosition="717" endWordPosition="718">ee substitution grammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with LIBLINEAR3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range 10.0001 ... 1001 by multiples of 10. The best model was then used to classify the test set. A sentence length feature was included for every sentence. For tree kernels, we used SVM-light-TK4 (Moschitti, 2004; Moschitti, 2006) with the default settings (-t 5 -D 1 -L 0.4),5 which also solves an L2-regularized L1-loss SVM optimization problem. We tuned the regularization parameter (-c) on the dev set in the same manner as described above, providing 4 GB of memory to the kernel cache (-m 4000).6 We used subset tree kernels, which compute the similarity between two trees by implicitly enumerating all possible fragments of the trees (in contrast with subtree kernels, where all fragments fully extend to the leaves). 3 Tasks Table 1 summarizes our datasets. 3.1 Coarse grammatical classification Our first</context>
</contexts>
<marker>Moschitti, 2004</marker>
<rawString>Alessandro Moschitti. 2004. A study on convolution kernels for shallow semantic parsing. In Proc. of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alessandro Moschitti</author>
</authors>
<title>Making tree kernels practical for natural language learning.</title>
<date>2006</date>
<booktitle>In Proc. of EACL,</booktitle>
<volume>6</volume>
<pages>113--120</pages>
<contexts>
<context position="4582" citStr="Moschitti, 2006" startWordPosition="719" endWordPosition="720">rammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with LIBLINEAR3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range 10.0001 ... 1001 by multiples of 10. The best model was then used to classify the test set. A sentence length feature was included for every sentence. For tree kernels, we used SVM-light-TK4 (Moschitti, 2004; Moschitti, 2006) with the default settings (-t 5 -D 1 -L 0.4),5 which also solves an L2-regularized L1-loss SVM optimization problem. We tuned the regularization parameter (-c) on the dev set in the same manner as described above, providing 4 GB of memory to the kernel cache (-m 4000).6 We used subset tree kernels, which compute the similarity between two trees by implicitly enumerating all possible fragments of the trees (in contrast with subtree kernels, where all fragments fully extend to the leaves). 3 Tasks Table 1 summarizes our datasets. 3.1 Coarse grammatical classification Our first comparison is coa</context>
</contexts>
<marker>Moschitti, 2006</marker>
<rawString>Alessandro Moschitti. 2006. Making tree kernels practical for natural language learning. In Proc. of EACL, volume 6, pages 113–120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David L Wallace</author>
</authors>
<title>Applied Bayesian and Classical Inference: The Case of the Federalist Papers.</title>
<date>1984</date>
<publisher>Springer-Verlag.</publisher>
<contexts>
<context position="11374" citStr="Mosteller and Wallace, 1984" startWordPosition="1781" endWordPosition="1784">s a strong indicator for English ones. 868 the ICLE dataset (Tetreault et al., 2012),11 we preprocessed each dataset into two signaturestylized versions by replacing all words not in a stopword list.12 The first version replaces nonstopwords with word classes computed from surface-form signatures,13 and the second with POS tags.14 N-gram features are then taken from both stylized versions of the corpus. Restricting the feature representation to be topic-independent is standard-practice in stylometric tasks like authorship attribution, gender identification, and native-language identification (Mosteller and Wallace, 1984; Koppel et al., 2003; Tomokiyo and Jones, 2001). 3.4.1 ICLE v.2 The first dataset is a seven-language subset of the International Corpus of Learner English, Version 2 (ICLE) (Granger et al., 2009), which contains 3.7 million words of English documents written by people with sixteen different native languages. Table 1 contains scores, including one reported by Wong and Dras (2011), who used the CFG and C&amp;J features, and whose data splits we mirror.15 3.4.2 ACL Anthology Network We also experimented with native language classification on scientific documents using a version of the ACL Anthology</context>
</contexts>
<marker>Mosteller, Wallace, 1984</marker>
<rawString>Frederick Mosteller and David L. Wallace. 1984. Applied Bayesian and Classical Inference: The Case of the Federalist Papers. Springer-Verlag.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daisuke Okanohara</author>
<author>Jun’ichi Tsujii</author>
</authors>
<title>A discriminative language model with pseudonegative samples.</title>
<date>2007</date>
<booktitle>In Proc. of ACL,</booktitle>
<location>Prague, Czech Republic,</location>
<contexts>
<context position="6386" citStr="Okanohara and Tsujii, 2007" startWordPosition="981" endWordPosition="984">ain dev test Coarse grammaticality (BLLIP) sentences 100,000 6,000 6,000 Fine grammaticality (PTB) sentences 79,664 3,978 3,840 Question classification (TREC-10) sentences 4,907 545 500 Native language (ICLE; 7 languages) documents 490 105 175 sentences 17,715 3,968 6,777 Native language (ACL; 5 languages) documents 987 195 185 sentences 146,257 28,139 28,403 Table 1: Datasets. system accuracy CPU time Chance 50.0 - N-gram 68.4 minutes 86.3 minutes 89.8 minutes 92.9 an hour SVM-TK 91.0 a week Table 2: Coarse grammaticality. CPU time is for classifier setup, training, and testing. domain data (Okanohara and Tsujii, 2007). Cherry and Quirk (2008) first applied syntax to this task, learning weighted parameters for a CFG with a latent SVM. Post (2011) found further improvements with fragmentbased representations (TSGs and C&amp;J) with a regular SVM. Here, we compare their results to kernel methods. We repeat Post’s experiments on the BLLIP dataset,7 using his exact data splits (Table 2). To our knowledge, tree kernels have not been applied to this task. 3.2 Fine grammatical classification Real-world grammaticality judgments require much finer-grained distinctions than the coarse ones of the previous section (for ex</context>
</contexts>
<marker>Okanohara, Tsujii, 2007</marker>
<rawString>Daisuke Okanohara and Jun’ichi Tsujii. 2007. A discriminative language model with pseudonegative samples. In Proc. of ACL, Prague, Czech Republic, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Slav Petrov</author>
<author>Leon Barrett</author>
<author>Romain Thibaux</author>
<author>Dan Klein</author>
</authors>
<title>Learning accurate, compact, and interpretable tree annotation.</title>
<date>2006</date>
<booktitle>In Proc. of ACL,</booktitle>
<location>Sydney, Australia,</location>
<contexts>
<context position="3778" citStr="Petrov et al., 2006" startWordPosition="588" endWordPosition="591">es are simple to generate (with publicly available tools) and flexible to use, we recommend they be included as baseline comparisons in tree kernel method evaluations. 2 Experimental setup We used the following feature sets: N-grams All unigrams and bigrams.&apos; &apos;Experiments with trigrams did not show any im866 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866–872, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics CFG rules Counts of depth-one contextfree grammar (CFG) productions obtained from the Berkeley parser (Petrov et al., 2006). C&amp;J features The parse-tree reranking feature set of Charniak and Johnson (2005), extracted from the Berkeley parse trees. TSG features We also parsed with a Bayesian tree substitution grammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with LIBLINEAR3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range 10.0001 ... 1001 by m</context>
</contexts>
<marker>Petrov, Barrett, Thibaux, Klein, 2006</marker>
<rawString>Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. 2006. Learning accurate, compact, and interpretable tree annotation. In Proc. of ACL, Sydney, Australia, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xuan-Hieu Phan</author>
</authors>
<date>2006</date>
<journal>CRFTagger: CRF English POS Tagger. crftagger.sourceforge.net.</journal>
<contexts>
<context position="12784" citStr="Phan, 2006" startWordPosition="1999" endWordPosition="2000">a semi-automatic fashion for the five most-common native languages of ACL authors in our training era: English, Japanese, German, Chinese, and French. The annotation heuristics, designed to favor precision over recall, provided annotations for 1,959 of 8,483 papers (23%) in the 2001–2009 AAN.16 11Including prompts, characters, and special tokens that correlate strongly with particular outcomes. 12The stopword list contains the set of 524 SMARTsystem stopwords used by Tomokiyo and Jones (2001), plus punctuation and Latin abbreviations. 13For example, suffix and capitalization. 14Via CRFTagger (Phan, 2006). 15Tetreault et al. reported accuracies up to 90.1 in a cross-validation setting that isn’t directly comparable. 16Details and data at old-site.clsp.jhu.edu/ ~sbergsma/Stylo/. 0 0.01 0.1 1 10 100 1,000 Figure1: Trainng time (1000 onds) vs. training time (thousands of seconds test accuracy for coarse grammaticality, plotting test scores from models trained on 100, 300, 1k, 3k, 10k, 30k, and 100k instances. 4 Discussion Syntactic features improve upon the n-gram baseline for all tasks except whole-document classification for ICLE. Tree kernels are often among the best, but always trail (by orde</context>
</contexts>
<marker>Phan, 2006</marker>
<rawString>Xuan-Hieu Phan. 2006. CRFTagger: CRF English POS Tagger. crftagger.sourceforge.net.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniele Pighin</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Reverse engineering of tree kernel feature spaces.</title>
<date>2009</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>111--120</pages>
<location>Singapore,</location>
<contexts>
<context position="1900" citStr="Pighin and Moschitti, 2009" startWordPosition="296" endWordPosition="299">ction (Wong and Dras, 2011). A major distinction among these uses of syntax is how the features are represented. The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about which fragments to include in a grammar. However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers (Post, 2011; Wong and Dras, 2011; Swanson and Charniak, 2012). These feature sets necessarily represent only a small subset of all possible tree patterns, leaving open the question of </context>
<context position="9236" citStr="Pighin and Moschitti (2009)" startWordPosition="1456" endWordPosition="1459">d their data splits (described as NoisyWSJ in their paper) and repeat their experiments here (Table 3). 3.3 Question Classification We look next at question classification (QC). Li and Roth (2002) introduced the TREC-10 dataset,8 a set of questions paired with labels that categorize the question by the type of answer it seeks. The labels are organized hierarchically into six (coarse) top-level labels and fifty (fine) refinements. An example question from the ENTY/animal category is What was the first domesticated bird?. Table 4 contains results predicting just the coarse labels. We compare to Pighin and Moschitti (2009), and also repeat their experiments, finding a slightly better result for them. 8cogcomp.cs.illinois.edu/Data/QA/QC/ We also experimented with the refined version of the task, where we directly predict one of the fifty refined categories, and found nearly identical relative results, with the best explicit feature set (CFG) returning an accuracy of 83.6% (in seconds), and the tree kernel system 69.8% (in an hour). For reference, Zhang and Lee (2003) report 80.2% accuracy when training on the full training set (5,500 examples) with an SVM and bag-of-words features.9 3.4 Native language identific</context>
<context position="10573" citStr="Pighin and Moschitti (2009)" startWordPosition="1661" endWordPosition="1664">is usually cast as a documentlevel task, since there are often not enough cues to identify native languages at smaller granularities. As such, this task presents a challenge to tree kernels, which are defined at the level of a single parse tree and have no obvious document-level extension. Table 5 therefore presents three evaluations: (a) sentencelevel accuracy, and document-level accuracy from (b) sentence-level voting and (c) direct, whole-document classification. We perform these experiments on two datasets. In order to mitigate topic bias10 and other problems that have been reported with 9Pighin and Moschitti (2009) did not report results on this version of the task. 10E.g., when we train with all words, the keyword ’Japanese’ is a strong indicator for Japanese authors, while ’Arabic’ is a strong indicator for English ones. 868 the ICLE dataset (Tetreault et al., 2012),11 we preprocessed each dataset into two signaturestylized versions by replacing all words not in a stopword list.12 The first version replaces nonstopwords with word classes computed from surface-form signatures,13 and the second with POS tags.14 N-gram features are then taken from both stylized versions of the corpus. Restricting the fea</context>
<context position="16616" citStr="Pighin and Moschitti, 2009" startWordPosition="2609" endWordPosition="2612"> skirt the difficult question, Which fragments? So what are the “right” features? Table 6) presents an intuitive list from the coarse grammaticality task: phenomena such as balanced parenthetical phrases and quotations are associated with grammaticality, while small, flat, abstract rules indicate samples from the ngram model. Similar intuitive results hold for the other tasks. The immediate interpretability of the explicit formalisms is another advantage, although recent work has shown that weights on the implicit features can also be obtained after a kind of linearization of the tree kernel (Pighin and Moschitti, 2009). Ultimately, which features matter is taskdependent, and skirting the question is advantageous in many settings. But it is also encouraging that methods for selecting fragments and other tree features work so well, 17disi.unitn.it/~severyn/code.html (TOP (S “ S , ” NP (VP (VBZ says) ADVP) .)) (FRAG (X SYM) VP .) (PRN (-LRB- -LRB-) S (-RRB- -RRB-)) (PRN (-LRB- -LRB-) NP (-RRB- -RRB-)) (S NPVP.) (NP (NP DT CD (NN %)) PP) (NP DT) (PP (IN of)) (TOP (NP NP PP PP .)) (NP DT JJ NNS) Table 6: The highest- and lowest-weighted TSG features (coarse grammaticality). yielding quick, light-weight models th</context>
</contexts>
<marker>Pighin, Moschitti, 2009</marker>
<rawString>Daniele Pighin and Alessandro Moschitti. 2009. Reverse engineering of tree kernel feature spaces. In Proc. of EMNLP, pages 111–120, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
<author>Daniel Gildea</author>
</authors>
<title>Bayesian learning of a tree substitution grammar.</title>
<date>2009</date>
<booktitle>In Proc. of ACL (short paper track),</booktitle>
<location>Suntec, Singapore,</location>
<contexts>
<context position="3995" citStr="Post and Gildea, 2009" startWordPosition="621" endWordPosition="624">re sets: N-grams All unigrams and bigrams.&apos; &apos;Experiments with trigrams did not show any im866 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866–872, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics CFG rules Counts of depth-one contextfree grammar (CFG) productions obtained from the Berkeley parser (Petrov et al., 2006). C&amp;J features The parse-tree reranking feature set of Charniak and Johnson (2005), extracted from the Berkeley parse trees. TSG features We also parsed with a Bayesian tree substitution grammar (Post and Gildea, 2009, TSG)2 and extracted fragment counts from Viterbi derivations. We build classifiers with LIBLINEAR3 (Fan et al., 2008). We divided each dataset into training, dev, and test sets. We then trained an L2-regularized L1-loss support vector machine (-s 3) with a bias parameter of 1 (-B 1), optimizing the regularization parameter (-c) on the dev set over the range 10.0001 ... 1001 by multiples of 10. The best model was then used to classify the test set. A sentence length feature was included for every sentence. For tree kernels, we used SVM-light-TK4 (Moschitti, 2004; Moschitti, 2006) with the def</context>
</contexts>
<marker>Post, Gildea, 2009</marker>
<rawString>Matt Post and Daniel Gildea. 2009. Bayesian learning of a tree substitution grammar. In Proc. of ACL (short paper track), Suntec, Singapore, August.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matt Post</author>
</authors>
<title>Judging grammaticality with tree substitution grammar derivations.</title>
<date>2011</date>
<booktitle>In Proc. of ACL,</booktitle>
<location>Portland, Oregon, USA,</location>
<contexts>
<context position="2327" citStr="Post, 2011" startWordPosition="365" endWordPosition="366"> representing any of them explicitly. Tree kernel approaches have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about which fragments to include in a grammar. However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers (Post, 2011; Wong and Dras, 2011; Swanson and Charniak, 2012). These feature sets necessarily represent only a small subset of all possible tree patterns, leaving open the question of what further gains might be had from the unusued fragments. Somewhat surprisingly, explicit and implicit syntactic features have been explored largely independently. Here, we compare them on a range of classification tasks: (1,2) grammatical classification (is a sentence written by a human?), (3) question classification (what type of answer is sought by this question?), and (4,5) native language prediction (what is the nati</context>
<context position="6516" citStr="Post (2011)" startWordPosition="1006" endWordPosition="1007">ation (TREC-10) sentences 4,907 545 500 Native language (ICLE; 7 languages) documents 490 105 175 sentences 17,715 3,968 6,777 Native language (ACL; 5 languages) documents 987 195 185 sentences 146,257 28,139 28,403 Table 1: Datasets. system accuracy CPU time Chance 50.0 - N-gram 68.4 minutes 86.3 minutes 89.8 minutes 92.9 an hour SVM-TK 91.0 a week Table 2: Coarse grammaticality. CPU time is for classifier setup, training, and testing. domain data (Okanohara and Tsujii, 2007). Cherry and Quirk (2008) first applied syntax to this task, learning weighted parameters for a CFG with a latent SVM. Post (2011) found further improvements with fragmentbased representations (TSGs and C&amp;J) with a regular SVM. Here, we compare their results to kernel methods. We repeat Post’s experiments on the BLLIP dataset,7 using his exact data splits (Table 2). To our knowledge, tree kernels have not been applied to this task. 3.2 Fine grammatical classification Real-world grammaticality judgments require much finer-grained distinctions than the coarse ones of the previous section (for example, marking dropped determiners or wrong verb inflections). For this task, we too positive examples from all sentences of secti</context>
</contexts>
<marker>Post, 2011</marker>
<rawString>Matt Post. 2011. Judging grammaticality with tree substitution grammar derivations. In Proc. of ACL, Portland, Oregon, USA, June.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dragomir R Radev</author>
<author>Pradeep Muthukrishnan</author>
<author>Vahed Qazvinian</author>
</authors>
<title>The ACL anthology network corpus.</title>
<date>2009</date>
<booktitle>In Proc. of ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries,</booktitle>
<pages>54--61</pages>
<contexts>
<context position="12002" citStr="Radev et al., 2009" startWordPosition="1883" endWordPosition="1886">t al., 2003; Tomokiyo and Jones, 2001). 3.4.1 ICLE v.2 The first dataset is a seven-language subset of the International Corpus of Learner English, Version 2 (ICLE) (Granger et al., 2009), which contains 3.7 million words of English documents written by people with sixteen different native languages. Table 1 contains scores, including one reported by Wong and Dras (2011), who used the CFG and C&amp;J features, and whose data splits we mirror.15 3.4.2 ACL Anthology Network We also experimented with native language classification on scientific documents using a version of the ACL Anthology Network (Radev et al., 2009, AAN) annotated for experiments in stylemetric tasks, including a native/non-native author judgment (Bergsma et al., 2012). For NLI, we further annotated this dataset in a semi-automatic fashion for the five most-common native languages of ACL authors in our training era: English, Japanese, German, Chinese, and French. The annotation heuristics, designed to favor precision over recall, provided annotations for 1,959 of 8,483 papers (23%) in the 2001–2009 AAN.16 11Including prompts, characters, and special tokens that correlate strongly with particular outcomes. 12The stopword list contains th</context>
</contexts>
<marker>Radev, Muthukrishnan, Qazvinian, 2009</marker>
<rawString>Dragomir R. Radev, Pradeep Muthukrishnan, and Vahed Qazvinian. 2009. The ACL anthology network corpus. In Proc. of ACL Workshop on Natural Language Processing and Information Retrieval for Digital Libraries, pages 54–61.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Remko Scha</author>
</authors>
<title>Taaltheorie en taaltechnologie; competence en performance.</title>
<date>1990</date>
<booktitle>Computertoepassingen in de neerlandistiek,</booktitle>
<pages>pages</pages>
<editor>In R. de Kort and G.L.J. Leerdam, editors,</editor>
<contexts>
<context position="1984" citStr="Scha, 1990" startWordPosition="311" endWordPosition="312">epresented. The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about which fragments to include in a grammar. However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers (Post, 2011; Wong and Dras, 2011; Swanson and Charniak, 2012). These feature sets necessarily represent only a small subset of all possible tree patterns, leaving open the question of what further gains might be had from the unusued fragments. Somewhat surprisingly, e</context>
</contexts>
<marker>Scha, 1990</marker>
<rawString>Remko Scha. 1990. Taaltheorie en taaltechnologie; competence en performance. In R. de Kort and G.L.J. Leerdam, editors, Computertoepassingen in de neerlandistiek, pages 7–22, Almere, the Netherlands. De Vereniging.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aliaksei Severyn</author>
<author>Alessandro Moschitti</author>
</authors>
<title>Large-scale support vector learning with structural kernels.</title>
<date>2010</date>
<booktitle>In Proc. of ECML/PKDD,</booktitle>
<pages>229--244</pages>
<marker>Severyn, Moschitti, 2010</marker>
<rawString>Aliaksei Severyn and Alessandro Moschitti. 2010. Large-scale support vector learning with structural kernels. In Proc. of ECML/PKDD, pages 229–244.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Libin Shen</author>
<author>Aravind K Joshi</author>
</authors>
<title>An SVMbased voting algorithm with application to parse reranking.</title>
<date>2003</date>
<booktitle>In Proc. of CoNLL,</booktitle>
<pages>9--16</pages>
<contexts>
<context position="18113" citStr="Shen and Joshi, 2003" startWordPosition="2856" endWordPosition="2859">epresented subset of them. In addition to their flexibility and interpetability, explicit syntactic features often outperformed tree kernels in accuracy, and even where they did not, the cost was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of exp</context>
</contexts>
<marker>Shen, Joshi, 2003</marker>
<rawString>Libin Shen and Aravind K. Joshi. 2003. An SVMbased voting algorithm with application to parse reranking. In Proc. of CoNLL, pages 9–16.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jun Suzuki</author>
<author>Hideki Isozaki</author>
<author>Eisaku Maeda</author>
</authors>
<title>Convolution kernels with feature selection for natural language processing tasks.</title>
<date>2004</date>
<booktitle>In Proc. of ACL,</booktitle>
<pages>119--126</pages>
<contexts>
<context position="18158" citStr="Suzuki et al., 2004" startWordPosition="2862" endWordPosition="2865">r flexibility and interpetability, explicit syntactic features often outperformed tree kernels in accuracy, and even where they did not, the cost was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of explicit syntactic features (both manual and aut</context>
</contexts>
<marker>Suzuki, Isozaki, Maeda, 2004</marker>
<rawString>Jun Suzuki, Hideki Isozaki, and Eisaku Maeda. 2004. Convolution kernels with feature selection for natural language processing tasks. In Proc. of ACL, pages 119–126.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benjamin Swanson</author>
<author>Eugene Charniak</author>
</authors>
<title>Native language detection with tree substitution grammars.</title>
<date>2012</date>
<booktitle>In Proc. of ACL (short papers),</booktitle>
<pages>193--197</pages>
<location>Jeju Island, Korea,</location>
<contexts>
<context position="2377" citStr="Swanson and Charniak, 2012" startWordPosition="371" endWordPosition="374">tly. Tree kernel approaches have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009). Tree kernels were inspired in part by ideas from Data-Oriented Parsing (Scha, 1990; Bod, 1993), which was in turn motivated by uncertainty about which fragments to include in a grammar. However, manual and automatic approaches to inducing tree fragments have recently been found to be useful in an explicit approach to text classification, which employs specific tree fragments as features in standard classifiers (Post, 2011; Wong and Dras, 2011; Swanson and Charniak, 2012). These feature sets necessarily represent only a small subset of all possible tree patterns, leaving open the question of what further gains might be had from the unusued fragments. Somewhat surprisingly, explicit and implicit syntactic features have been explored largely independently. Here, we compare them on a range of classification tasks: (1,2) grammatical classification (is a sentence written by a human?), (3) question classification (what type of answer is sought by this question?), and (4,5) native language prediction (what is the native language of a text’s author?). Our main contrib</context>
</contexts>
<marker>Swanson, Charniak, 2012</marker>
<rawString>Benjamin Swanson and Eugene Charniak. 2012. Native language detection with tree substitution grammars. In Proc. of ACL (short papers), pages 193–197, Jeju Island, Korea, July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joel Tetreault</author>
<author>Daniel Blanchard</author>
<author>Aoife Cahill</author>
<author>Martin Chodorow</author>
</authors>
<title>Native tongues, lost and found: Resources and empirical evaluations in native language identification.</title>
<date>2012</date>
<booktitle>In Proc. of COLING,</booktitle>
<pages>2585--2602</pages>
<location>Mumbai, India,</location>
<contexts>
<context position="10831" citStr="Tetreault et al., 2012" startWordPosition="1705" endWordPosition="1708">us document-level extension. Table 5 therefore presents three evaluations: (a) sentencelevel accuracy, and document-level accuracy from (b) sentence-level voting and (c) direct, whole-document classification. We perform these experiments on two datasets. In order to mitigate topic bias10 and other problems that have been reported with 9Pighin and Moschitti (2009) did not report results on this version of the task. 10E.g., when we train with all words, the keyword ’Japanese’ is a strong indicator for Japanese authors, while ’Arabic’ is a strong indicator for English ones. 868 the ICLE dataset (Tetreault et al., 2012),11 we preprocessed each dataset into two signaturestylized versions by replacing all words not in a stopword list.12 The first version replaces nonstopwords with word classes computed from surface-form signatures,13 and the second with POS tags.14 N-gram features are then taken from both stylized versions of the corpus. Restricting the feature representation to be topic-independent is standard-practice in stylometric tasks like authorship attribution, gender identification, and native-language identification (Mosteller and Wallace, 1984; Koppel et al., 2003; Tomokiyo and Jones, 2001). 3.4.1 I</context>
</contexts>
<marker>Tetreault, Blanchard, Cahill, Chodorow, 2012</marker>
<rawString>Joel Tetreault, Daniel Blanchard, Aoife Cahill, and Martin Chodorow. 2012. Native tongues, lost and found: Resources and empirical evaluations in native language identification. In Proc. of COLING, pages 2585–2602, Mumbai, India, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Laura Mayfield Tomokiyo</author>
<author>Rosie Jones</author>
</authors>
<title>You’re not from ’round here, are you? Naive Bayes detection of non-native utterances.</title>
<date>2001</date>
<booktitle>In Proc. of NAACL.</booktitle>
<contexts>
<context position="11422" citStr="Tomokiyo and Jones, 2001" startWordPosition="1789" endWordPosition="1792">E dataset (Tetreault et al., 2012),11 we preprocessed each dataset into two signaturestylized versions by replacing all words not in a stopword list.12 The first version replaces nonstopwords with word classes computed from surface-form signatures,13 and the second with POS tags.14 N-gram features are then taken from both stylized versions of the corpus. Restricting the feature representation to be topic-independent is standard-practice in stylometric tasks like authorship attribution, gender identification, and native-language identification (Mosteller and Wallace, 1984; Koppel et al., 2003; Tomokiyo and Jones, 2001). 3.4.1 ICLE v.2 The first dataset is a seven-language subset of the International Corpus of Learner English, Version 2 (ICLE) (Granger et al., 2009), which contains 3.7 million words of English documents written by people with sixteen different native languages. Table 1 contains scores, including one reported by Wong and Dras (2011), who used the CFG and C&amp;J features, and whose data splits we mirror.15 3.4.2 ACL Anthology Network We also experimented with native language classification on scientific documents using a version of the ACL Anthology Network (Radev et al., 2009, AAN) annotated for</context>
<context position="12670" citStr="Tomokiyo and Jones (2001)" startWordPosition="1983" endWordPosition="1986">tric tasks, including a native/non-native author judgment (Bergsma et al., 2012). For NLI, we further annotated this dataset in a semi-automatic fashion for the five most-common native languages of ACL authors in our training era: English, Japanese, German, Chinese, and French. The annotation heuristics, designed to favor precision over recall, provided annotations for 1,959 of 8,483 papers (23%) in the 2001–2009 AAN.16 11Including prompts, characters, and special tokens that correlate strongly with particular outcomes. 12The stopword list contains the set of 524 SMARTsystem stopwords used by Tomokiyo and Jones (2001), plus punctuation and Latin abbreviations. 13For example, suffix and capitalization. 14Via CRFTagger (Phan, 2006). 15Tetreault et al. reported accuracies up to 90.1 in a cross-validation setting that isn’t directly comparable. 16Details and data at old-site.clsp.jhu.edu/ ~sbergsma/Stylo/. 0 0.01 0.1 1 10 100 1,000 Figure1: Trainng time (1000 onds) vs. training time (thousands of seconds test accuracy for coarse grammaticality, plotting test scores from models trained on 100, 300, 1k, 3k, 10k, 30k, and 100k instances. 4 Discussion Syntactic features improve upon the n-gram baseline for all tas</context>
</contexts>
<marker>Tomokiyo, Jones, 2001</marker>
<rawString>Laura Mayfield Tomokiyo and Rosie Jones. 2001. You’re not from ’round here, are you? Naive Bayes detection of non-native utterances. In Proc. of NAACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir N Vapnik</author>
</authors>
<title>Statistical Learning Theory.</title>
<date>1998</date>
<publisher>John Wiley &amp; Sons.</publisher>
<contexts>
<context position="15448" citStr="Vapnik, 1998" startWordPosition="2426" endWordPosition="2427">uning the tradeoff between training time and accuracy. While faster than SVM-TK, its performance was never better than explicit methods along both dimensions (time and accuracy). 4.2 Overfitting Overfitting is also a problem for kernel methods. The best models often had a huge number of support vectors, achieving near-perfect accuracy on the training set but making many errors on the dev. and test sets. On the ICLE task, close to 75% of all the training examples were used as support vectors. We found only half as many support vectors used for the explicit representations, implying less error (Vapnik, 1998), and saw much lower variance between training and testing performance. 4.3 Which fragments? Our findings support the observations of Cumby and Roth (2003), who point out that kernels introduce a large number of irrelevant features that may be especially harmful in small-data settings, and that, when possible, it is often better to have a set of explicit, relevant features. In other words, it is better to have the right features than all of them. Tree kernels provide a robust, efficiently-computable measure of comparison, but they also skirt the difficult question, Which fragments? So what are</context>
</contexts>
<marker>Vapnik, 1998</marker>
<rawString>Vladimir N. Vapnik. 1998. Statistical Learning Theory. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhuang Wang</author>
<author>Koby Crammer</author>
<author>Slobodan Vucetic</author>
</authors>
<title>Multi-class pegasos on a budget.</title>
<date>2010</date>
<booktitle>In ICML,</booktitle>
<pages>1143--1150</pages>
<contexts>
<context position="14625" citStr="Wang et al., 2010" startWordPosition="2289" endWordPosition="2292">late. We compared models trained on the first 100, 300, 1k, 3k, 10k, 30k, and 100k data points of the coarse grammaticality dataset, split evenly between positive and negative examples (Figure 1). SVM-TK improves over the TSG and CFG models in the limit, but at an extraordinary cost in training time: 100k training examples is already pushing the bounds of practicality for tree kernel learning, and generating curve’s next point would require several months of time. Kernel methods also produce large models that result in slow test-time performance, a problem dubbed the “curse of kernelization” (Wang et al., 2010). Approximate kernel methods designed to scale to large datasets address this (Severyn CFG TSG C&amp;J SVM-TK uSVM-TK 100 90 80 70 60 869 and Moschitti, 2010). We investigated the uSVM-TK toolkit,17 which enables tuning the tradeoff between training time and accuracy. While faster than SVM-TK, its performance was never better than explicit methods along both dimensions (time and accuracy). 4.2 Overfitting Overfitting is also a problem for kernel methods. The best models often had a huge number of support vectors, achieving near-perfect accuracy on the training set but making many errors on the dev</context>
</contexts>
<marker>Wang, Crammer, Vucetic, 2010</marker>
<rawString>Zhuang Wang, Koby Crammer, and Slobodan Vucetic. 2010. Multi-class pegasos on a budget. In ICML, pages 1143–1150.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Parser features for sentence grammaticality classification.</title>
<date>2010</date>
<booktitle>In Proceedings of the Australasian Language Technology Association Workshop,</booktitle>
<location>Melbourne, Australia,</location>
<contexts>
<context position="7512" citStr="Wong and Dras (2010)" startWordPosition="1167" endWordPosition="1170">s require much finer-grained distinctions than the coarse ones of the previous section (for example, marking dropped determiners or wrong verb inflections). For this task, we too positive examples from all sentences of sections 2–21 of the WSJ portion of the Penn Treebank (Marcus et al., 1993). Negative examples were created by inserting one or two errors 7LDC Catalog No. LDC2000T43 CFG TSG C&amp;J 867 Wong &amp; Dras 60.6 - Chance N-gram 50.0 - 61.4 minutes CFG TSG C&amp;J 67.8 weeks SVM-TK system accuracy CPU time 64.5 minutes 67.0 minutes 71.9 an hour Table 3: Fine-grained classification accuracy (the Wong and Dras (2010) score is the highest score from the last column of their Table 3). system sent. voting whole Wong &amp; Dras - - 80.0 Style 42.0 75.3 86.8 CFG 39.5 73.2 83.7 TSG 38.7 72.1 83.2 C&amp;J 42.9 76.3 86.3 SVM-TK 40.7 69.5 - Style 42.5 65.3 83.7 CFG 39.2 52.6 86.3 TSG 40.4 56.8 84.7 C&amp;J 49.2 66.3 81.1 SVM-TK 42.1 52.6 - accuracy CPU time 86.6 - 73.2 seconds 90.0 seconds 85.6 seconds 89.6 minutes Table 5: Accuracy on ICLE (7 languages, top) and ACL (five, bottom) datasets at the sentence and document levels. All documents were signature-stylized (§3.4). system Pighin &amp; Moschitti Bigram CFG TSG C&amp;J SVM-TK 87</context>
</contexts>
<marker>Wong, Dras, 2010</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2010. Parser features for sentence grammaticality classification. In Proceedings of the Australasian Language Technology Association Workshop, Melbourne, Australia, December.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sze-Meng Jojo Wong</author>
<author>Mark Dras</author>
</authors>
<title>Exploiting parse structures for native language identification.</title>
<date>2011</date>
<booktitle>In Proc. of EMNLP,</booktitle>
<pages>1600--1610</pages>
<location>Edinburgh, Scotland, UK.,</location>
<contexts>
<context position="1300" citStr="Wong and Dras, 2011" startWordPosition="202" endWordPosition="205">at explicit features often perform as well as tree kernels on accuracy and always in orders of magnitude less time, and with smaller models. Since explicit features are easy to generate and use (with publicly available tools), we suggest they should always be included as baseline comparisons in tree kernel method evaluations. 1 Introduction Features computed over parse trees are useful for a range of discriminative tasks, including authorship attribution (Baayen et al., 1996), parse reranking (Collins and Duffy, 2002), language modeling (Cherry and Quirk, 2008), and native-language detection (Wong and Dras, 2011). A major distinction among these uses of syntax is how the features are represented. The implicit approach uses tree kernels (Collins and Duffy, 2001), which make predictions with inner products between tree pairs. These products can be computed efficiently with a dynamic program that produces weighted counts of all the shared tree fragments between a pair of trees, essentially incorporating all fragments without representing any of them explicitly. Tree kernel approaches have been applied successfully in many areas of NLP (Collins and Duffy, 2002; Moschitti, 2004; Pighin and Moschitti, 2009)</context>
<context position="11757" citStr="Wong and Dras (2011)" startWordPosition="1843" endWordPosition="1846">s of the corpus. Restricting the feature representation to be topic-independent is standard-practice in stylometric tasks like authorship attribution, gender identification, and native-language identification (Mosteller and Wallace, 1984; Koppel et al., 2003; Tomokiyo and Jones, 2001). 3.4.1 ICLE v.2 The first dataset is a seven-language subset of the International Corpus of Learner English, Version 2 (ICLE) (Granger et al., 2009), which contains 3.7 million words of English documents written by people with sixteen different native languages. Table 1 contains scores, including one reported by Wong and Dras (2011), who used the CFG and C&amp;J features, and whose data splits we mirror.15 3.4.2 ACL Anthology Network We also experimented with native language classification on scientific documents using a version of the ACL Anthology Network (Radev et al., 2009, AAN) annotated for experiments in stylemetric tasks, including a native/non-native author judgment (Bergsma et al., 2012). For NLI, we further annotated this dataset in a semi-automatic fashion for the five most-common native languages of ACL authors in our training era: English, Japanese, German, Chinese, and French. The annotation heuristics, design</context>
</contexts>
<marker>Wong, Dras, 2011</marker>
<rawString>Sze-Meng Jojo Wong and Mark Dras. 2011. Exploiting parse structures for native language identification. In Proc. of EMNLP, pages 1600– 1610, Edinburgh, Scotland, UK., July.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaofeng Yang</author>
<author>Jian Su</author>
<author>Chew Lim Tan</author>
</authors>
<title>Kernel-based pronoun resolution with structured syntactic knowledge.</title>
<date>2006</date>
<booktitle>In Proc. of Coling-ACL,</booktitle>
<pages>41--48</pages>
<contexts>
<context position="18198" citStr="Yang et al., 2006" startWordPosition="2868" endWordPosition="2871">t syntactic features often outperformed tree kernels in accuracy, and even where they did not, the cost was multiple orders of magnitude increase in both training and testing time. These results were consistent across a range of task types, dataset sizes, and classification arities (binary and multiclass). There are a number of important caveats. We explored a range of data settings, but there are many others where tree kernels have been proven useful, such as parse tree reranking (Collins and Duffy, 2002; Shen and Joshi, 2003), sentence subjectivity (Suzuki et al., 2004), pronoun resolution (Yang et al., 2006), relation extraction (Culotta and Sorensen, 2004), machine translation evaluation (Liu and Gildea, 2005), predicate-argument recognition, and semantic role labeling (Pighin and Moschitti, 2009). There are also tree kernel variations such as dependency tree kernels (Culotta and Sorensen, 2004) and shallow semantic tree kernels (Moschitti et al., 2007). These variables provide a rich environment for future work; in the meantime, we take these results as compelling motivation for the continued development of explicit syntactic features (both manual and automatically induced), and suggest that su</context>
</contexts>
<marker>Yang, Su, Tan, 2006</marker>
<rawString>Xiaofeng Yang, Jian Su, and Chew Lim Tan. 2006. Kernel-based pronoun resolution with structured syntactic knowledge. In Proc. of Coling-ACL, pages 41–48.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dell Zhang</author>
<author>Wee Sun Lee</author>
</authors>
<title>Question classification using support vector machines.</title>
<date>2003</date>
<booktitle>In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03,</booktitle>
<pages>26--32</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="9688" citStr="Zhang and Lee (2003)" startWordPosition="1525" endWordPosition="1528">the ENTY/animal category is What was the first domesticated bird?. Table 4 contains results predicting just the coarse labels. We compare to Pighin and Moschitti (2009), and also repeat their experiments, finding a slightly better result for them. 8cogcomp.cs.illinois.edu/Data/QA/QC/ We also experimented with the refined version of the task, where we directly predict one of the fifty refined categories, and found nearly identical relative results, with the best explicit feature set (CFG) returning an accuracy of 83.6% (in seconds), and the tree kernel system 69.8% (in an hour). For reference, Zhang and Lee (2003) report 80.2% accuracy when training on the full training set (5,500 examples) with an SVM and bag-of-words features.9 3.4 Native language identification Native language identification (NLI) is the task of determining a text’s author’s native language. This is usually cast as a documentlevel task, since there are often not enough cues to identify native languages at smaller granularities. As such, this task presents a challenge to tree kernels, which are defined at the level of a single parse tree and have no obvious document-level extension. Table 5 therefore presents three evaluations: (a) s</context>
</contexts>
<marker>Zhang, Lee, 2003</marker>
<rawString>Dell Zhang and Wee Sun Lee. 2003. Question classification using support vector machines. In Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval, SIGIR ’03, pages 26–32, New York, NY, USA. ACM.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>