<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000061">
<title confidence="0.7873135">
A Stochastic Language Model using Dependency
and Its Improvement by Word Clustering
</title>
<author confidence="0.507309">
Shinsuke Mori *
</author>
<affiliation confidence="0.348408">
Tokyo Research Labolatory,
</affiliation>
<note confidence="0.684051">
IBM Japan, Ltd.
</note>
<sectionHeader confidence="0.829028714285714" genericHeader="abstract">
1623-14 Shimotsuruma
Yamatoshi, Japan
Makoto Nagao
Kyoto University
Yoshida-honmachi Sakyo
Kyoto, Japan
Abstract
</sectionHeader>
<bodyText confidence="0.996683846153846">
In this paper, we present a stochastic language
model for Japanese using dependency. The predic-
tion unit in this model is an attribute of &amp;quot;bunsetsu&amp;quot;.
This is represented by the product of the head of con-
tent words and that of function words. The relation
between the attributes of &amp;quot;bunsetsu&amp;quot; is ruled by a
context-free grammar. The word sequences are pre-
dicted from the attribute using word n-gram model.
The spell of Unknow word is predicted using charac-
ter n-gram model. This model is robust in that it can
compute the probability of an arbitrary string and
is complete in that it models from unknown word to
dependency at the same time.
</bodyText>
<sectionHeader confidence="0.998458" genericHeader="method">
1 Introduction
</sectionHeader>
<bodyText confidence="0.996757444444444">
An effectiveness of stochastic language modeling as
a methodology of natural language processing has
been attested by various applications to the recog-
nition system such as speech recognition and to the
analysis system such as part-of-speech (POS) tagger.
In this methodology a stochastic language model
with some parameters is built and they are estimated
in order to maximize its prediction power (minimize
the cross entropy) on an unknown input. Consid-
ering a single application, it might be better to es-
timate the parameters taking account of expected
accuracy of recognition or analysis. This method is,
however, heavily dependent on the problem and of-
fers no systematic solution, as far as we know. The
methodology of stochastic language modeling, how-
ever, allows us to separate, from various frameworks
of natural language processing, the language descrip-
tion model common to them and enables us a sys-
tematic improvement of each application.
In this framework a description on a language is
represented as a map from a sequence of alphabetic
characters to a probability value. The first model
is C. E. Shannon&apos;s n-gram model (Shannon, 1951).
The parameters of the model are estimated from the
frequency of n character sequences of the alphabet
(n-gram) on a corpus containing a large number of
sentences of a language. This is the same model as
This work is done when the anther was at Kyoto Univ.
used in almost all of the recent practical applications
in that it describes only relations between sequential
elements. Some linguistic phenomena, however, are
better described by assuming relations between sep-
arated elements. And modeling this kind of phenom-
ena, the accuracies of various application are gener-
ally augmented.
As for English, there have been researches in
which a stochastic context-free grammar (SCFG)
(Fujisaki et al., 1989) is used for model descrip-
tion. Recently some researchers have pointed out the
importance of the lexicon and proposed lexicalized
models (Jelinek et al., 1994; Collins, 1997). In these
models, every headword is propagated up through
the derivation tree such that every parent receives a
headword from the head-child. This kind of special-
ization may, however, be excessive if the criterion is
predictive power of the model. Research aimed at
estimating the best specialization level for 2-gram
model (Mori et al., 1997) shows a class-based model
is more predictive than a word-based 2-gram model,
a completely lexicalized model, comparing cross en-
tropy of a POS-based 2-gram model, a word-based
2-gram model and a class-based 2-gram model, es-
timated from information theoretical point of view.
As fora parser based on a class-based SCFG, Char-
nia.k (1997) reports better accuracy than the above
lexicalized models, but the clustering method is not
clear enough and, in addition, there is no report
on predictive power (cross entropy or perplexity).
Hogenhout and Matsumato (1997) propose a word-
clustering method based on syntactic behavior, but
no language model is discussed. As the experiments
in the present paper attest, word-class relation is
dependent on language model.
In this paper, taking Japanese as the object lan-
guage, we propose two complete stochastic language
models using dependency between bunsetsu, a se-
quence of one or more content words followed by
zero, one or more function words, and evaluate their
predictive power by cross entropy. Since the number
of sorts of bunsetsu is enormous, considering it as a
symbol to be predicted would surely invoke the data-
sparseness problem. To cope with this problem we
</bodyText>
<page confidence="0.997311">
898
</page>
<bodyText confidence="0.999983108108108">
use the concept of class proposed for a word n-gram
model (Brown et al., 1992). Each bunsetsu is repre-
sented by the class calculated from the POS of its
last content word and that of its last function word.
The relation between bunsetsu, called dependency, is
described by a stochastic context-free grammar (Fu,
1974) on the classes. From the class of a bunsetsu,
the content word sequence and the function word se-
quence are independently predicted by word n-gram
models equipped with unknown word models (Mori
and Yamaji, 1997).
The above model assumes that the syntactic be-
havior of each bunsetsu depends only on POS. The
POS system invented by grammarians may not al-
ways be the best in terms of stochastic language
modeling. This is experimentally attested by the
paper (Mori et al., 1997) reporting comparisons be-
tween a P OS-based n-gram model and a class-based
n-gram model induced automatically. We now pro-
pose, based on this report, a word-clustering method
on the model we have mentioned above to success-
fully improve the predictive power. In addition, we
discuss a parsing method as an application of the
model.
We also report the result of experiments con-
ducted on EDR corpus (Jap, 1993) The corpus is di-
vided into ten parts and the models estimated from
nine of them are tested on the rest in terms of cross
entropy. As the result, the cross entropy of the POS-
based dependency model is 5.3536 bits and that of
the class-based dependency model estimated by our
method is 4.9944 bits. This shows that the clus-
tering method we propose improves the predictive
power of the POS-based model notably. Addition-
ally, a parsing experiment proved that the parser
based on the improved model has a higher accuracy
than the POS-based one.
</bodyText>
<sectionHeader confidence="0.9061135" genericHeader="method">
2 Stochastic Language Model based
on Dependency
</sectionHeader>
<bodyText confidence="0.9996473">
In this section, we propose a stochastic language
model based on dependency. Formally this model is
based on a stochastic context-free grammar (SCFG).
The terminal symbol is the attribute of a bunsetsu,
represented by the product of the head of the con-
tent part and that of the function past. From the
attribute, a word sequence that matches the bun-
setsu is predicted by a word-based 2-gram model,
and unknown words are predicted from POS by a
character-based 2-gram model.
</bodyText>
<subsectionHeader confidence="0.929">
2.1 Sentence Model
</subsectionHeader>
<bodyText confidence="0.97116135">
A Japanese sentence is considered as a sequence of
units called bunsetsu composed of one or more con-
tent words and function words. Let Cont be a set
of content words, Func a set of function words and
Sign a set of punctuation symbols. Then bunsetsu
is defined as follows:
Bnst = Cont+ Fune U Cont+ Func* Sign,
where the signs &amp;quot;+&amp;quot; and &amp;quot;*&amp;quot; mean positive closure
and Kleene closure respectively. Since the relations
between bunsetsu known as dependency are not al-
ways between sequential ones, we use SCFG to de-
scribe them (Fu, 1974). The first problem is how
to choose terminal symbols. The simplest way is to
select each bunsetsu as a terminal symbol. In this
case, however, the data-sparseness problem would
surely be invoked, since the number of possible bun-
setsu is enormous. To avoid this problem we use the
concept of class proposed for a word n-gram model
(Brown et al., 1992). All bunsetsu are grouped by
the attribute defined as follows:
</bodyText>
<equation confidence="0.9894085">
attrib(b) (1)
= (last(cont(b)), last(func(b)), last(sign(b))),
</equation>
<bodyText confidence="0.9999652">
where the functions cont, func and sign take a
bunsetsu as their argument and return its content
word sequence, its function word sequence and its
punctuation respectively. In addition, the function
last(m) returns the POS of the last element of word
sequence m or NULL if the sequence has no word.
Given the attribute, the content word sequence and
the function word sequence of the bunsetsu are inde-
pendently generated by word-based 2-gram models
(Mori and Yamaji, 1997).
</bodyText>
<subsectionHeader confidence="0.962094">
2.2 Dependency Model
</subsectionHeader>
<bodyText confidence="0.99999572">
In order to describe the relation between bunsetsu
called dependency, we make the generally accepted
assumption that no two dependency relations cross
each other, and we introduce a SCFG with the at-
tribute of bunsetsu as terminals. It is known, as a
characteristic of the Japanese language, that each
bunsetsu depends on the single bunsetsu appearing
just before it. We say of two sequential bunsetsu
that the first to appear is the anterior and the sec-
ond is the posterior. We assume, in addition, that
the dependency relation is a binary relation — that
each relation is independent of the others. Then
this relation is representing by the following form of
rewriting rule of CFG: B = AB, where A is the at-
tribute of the anterior bunsetsu and B is that of the
posterior.
Similarly to terminal symbols, non-terminal sym-
bols can be defined as the attribute of bunsetsu. Also
they can be defined as the product of the attribute
and some additional information to reflect the char-
acteristics of the dependency. It is reported that the
dependency is more frequent between closer bunsetsu
in terms of the position in the sentence (Maruyama
and Ogino, 1992). In order to model these char-
acteristics, we add to the attribute of bunsetsu an
</bodyText>
<page confidence="0.990866">
899
</page>
<bodyText confidence="0.858801">
(verb, ending. period. 2, 0)
</bodyText>
<equation confidence="0.894970307692308">
(noun. NULL. comma, 0, 0)
(noun. postp., NULL. 0, 0)
(verb, ending, period, 0. 0)
kyou / noun . / sign
(today)
Kyoto / noun daigaku / noun he / postp.
(Kyoto) (university) (to)
i / verb ku / ending . / sign
(go)
t
(verb, ending, period. I, 0)
SCFG
n-gram
</equation>
<figureCaption confidence="0.995852">
Figure 1: Dependency model based on bunsetsu
</figureCaption>
<bodyText confidence="0.999971363636364">
additional information field holding the number of
bunsetsu depending on it. Also the fact that a bun-
setsu has a tendency to depend on a bunsetsu with
comma. For this reason the number of bunsetsu with
comma depending on it is also added. To avoid
data-sparseness problem we set an upper bound for
these numbers. Let d be the number of bunsetsu de-
pending on it and v be the number of bunsetsu with
comma depending on it, the set of terminal symbols
T and that of non-terminal symbols V is represented
as follows (see Figure 1):
</bodyText>
<equation confidence="0.999942">
T = attrib(b) x {0} x {0}
V = attrib(b) x {1, 2, dmoz} x {0, 1, • • /47.}.
</equation>
<bodyText confidence="0.999723">
It should be noted that terminal symbols have no
bunsetsu depending on them. It follows that all
rewriting rules are in the following forms:
</bodyText>
<equation confidence="0.968657571428571">
S (a, d, v) (2)
(al, dl, vi).(a2, d2, v2)(a3, d3, v3) (3)
al = a3
d1 = min(d3 1, dmaz)
min(v3 1, v,,„0.)
t11 = if sign(a2) = comma
v3 otherwise
</equation>
<bodyText confidence="0.998882619047619">
where a is the attribute of bunsetsu.
The attribute sequence of a sentence is generated
through applications of these rewriting rules to the
start symbol S. Each rewriting rule has a probability
and the probability of the attribute sequence is the
product of those of the rewriting rules used for its
generation. Taking the example of Figure 1, this
value is calculated as follows:
P((noun, NULL, comma, 0,0&gt;
(noun, postp., NULL, 0,0)
(verb, ending, period, 0,0&gt;)
= P(S (verb, ending, period, 2, 0))
x P((verb, ending, period, 2,0&gt;
(noun, NULL, comma, 0,0)
(verb, ending, period, 1,0&gt;)
x P((verb, ending, period, 1,0&gt;
(noun, postp., NULL, 0, 0)
(verb, ending, period, 0,0)).
The probability value of each rewriting rule is esti-
mated from its frequency N in a syntactically anno-
tated corpus as follows:
</bodyText>
<equation confidence="0.947253">
P(S d1, vi))
N(S (a1, d1, vi))
N(S)
P((ai, di, vi) = (a2, d2, n2)(a3, d3, v3))
N((al, dl, v1)=0. (a2, d2, v2)(a3, d3, v3))
N((ai, d1, vi))
</equation>
<bodyText confidence="0.99767725">
In a word n-gram model, in order to cope with
data-sparseness problem, the interpolation tech-
nique is applicable to SCFG. The probability of the
interpolated model of grammars G1 and G2, whose
</bodyText>
<page confidence="0.977411">
900
</page>
<bodyText confidence="0.9954765">
probabilities are Pi. and P2 respectively, is repre-
sented as follows:
</bodyText>
<equation confidence="0.9752975">
P(A a) = Pi (A a) + A2P2 (A a)
0 &lt; Ai &lt; 1 (j = 1,2) and A1 +A2 = 1 (4)
</equation>
<bodyText confidence="0.999903">
where A E V and a E (V UT)*. The coefficients are
estimated by held-out method or deleted interpola-
tion method (Jelinek et al., 1991).
</bodyText>
<sectionHeader confidence="0.99059" genericHeader="method">
3 Word Clustering
</sectionHeader>
<bodyText confidence="0.999817090909091">
The model we have mentioned above uses the POS
given manually for the attribute of bunsetsu. Chang-
ing it into some class may improve the predictive
power of the model. This change needs only a slight
replacement in the model representing formula (1):
the function last returns the class of the last word of
a word sequence m instead of the POS. The problem
we have to solve here is how to obtain such classes
i.e. word clustering. In this section, we propose
an objective function and a search algorithm of the
word clustering.
</bodyText>
<subsectionHeader confidence="0.99679">
3.1 Objective Function
</subsectionHeader>
<bodyText confidence="0.994925">
The aim of word clustering is to build a language
model with less cross entropy without referring to
the test corpus. Similar research has been success-
ful, aiming at an improvement of a word n-gram
model both in English and Japanese (Mori et al.,
1997). So we have decided to extend this research
to obtain an optimal word-class relation. The only
difference from the previous research is the language
model. In this case, it is a SCFG in stead of a n-
gram model. Therefore the objective function, called
average cross entropy, is defined as follows:
Let m1, m2, ... , mn be .A4 sorted
in the descending order of frequency.
</bodyText>
<equation confidence="0.838724875">
Tn2) • • • inn}
C =
foreach i (1, 2, • , n)
f (mi) :=
foreach i (1, 2, • • • , n)
c:= 71-(move(f, mi, c))
argmincEcuc„
if (H(move(f, mi, c)) &lt; H(f)) then
f :=move(f, mi,
update interpolation coefficients.
if (c = cne.) then
C := CU {cnetv}
...... .......
i = 3 LO
i = 4 ICD1 (1)1
update interpolation coefficients
[joi [lc) [&apos;so]
move( in
update interpolation coefficients
move( 1, )1
i= 2
update interpolation coefficients
no move
move( in
</equation>
<bodyText confidence="0.999606666666667">
where Li is the i-th learning corpus and Mi is the
language model estimated from the learning corpus
excluding the i-th learning corpus.
</bodyText>
<subsectionHeader confidence="0.998719">
3.2 Algorithm
</subsectionHeader>
<bodyText confidence="0.99996">
The solution space of the word clustering is the set of
all possible word-class relations. The cardinality of
the set, however, is too enormous for the dependency
model to calculate the average cross entropy for all
word-class relations and select the best one. So we
abandoned the best solution and adopted a greedy
algorithm as shown in Figure 2.
</bodyText>
<sectionHeader confidence="0.992734" genericHeader="method">
4 Syntactic Analysis
</sectionHeader>
<bodyText confidence="0.993478">
Syntactic Analysis is defined as a function which
receives a character sequence as an input, divides
it into a bunsetsu sequence and determines depen-
dency relations among them, where the concatena-
tion of character sequences of all the bunsetsu must
</bodyText>
<figureCaption confidence="0.99959">
Figure 2: The clustering algorithm.
</figureCaption>
<bodyText confidence="0.999621090909091">
be equal to the input. Generally there are one Or
more solutions for any input. A syntactic analyzer
chooses the structure which seems the most similar
to the human decision. There are two kinds of an-
alyzer: one is called a rule-based analyzer, which is
based on rules described according to the intuition
of grammarians; the other is called a corpus-based
analyzer, because it is based on a large number of
analyzed examples. In this section, we describe a
stochastic syntactic analyzer, which belongs to the
second category.
</bodyText>
<subsectionHeader confidence="0.99838">
4.1 Stochastic Syntactic Analyzer
</subsectionHeader>
<bodyText confidence="0.999995">
A stochastic syntactic analyzer, based on a stochas-
tic language model including the concept of depen-
dency, calculates the syntactic tree (see Figure 1)
with the highest probability for a given input x ac-
cording to the following formula:
</bodyText>
<equation confidence="0.869833">
th = argmaxP(Tlx)
tv(T)--z
— 1 m
H H(Li,Mi),
m
1=1
(5)
</equation>
<page confidence="0.997404">
901
</page>
<tableCaption confidence="0.999397">
Table 1: Corpus.
</tableCaption>
<figure confidence="0.497923666666667">
#sentences #bunsetsu #word
learning 174,524 1,610,832 4,251,085
test 19,397 178,415 471,189
= argmax P(Tx)P(x)
W(T)=X
=argmax P(x1T)P(T) (..• Bayes&apos; formula)
W(T)=X
= argmax P(T) (..• P(xIT) = 1),
W(T)=X
</figure>
<bodyText confidence="0.9997725">
where w(T) represents the character sequence of the
syntactic tree T. P(T) in the last line is a stochas-
tic language model including the concept of depen-
dency. We use, as such a model, the POS-based de-
pendency model described in section 2 or the class-
based dependency model described in section 3.
</bodyText>
<subsectionHeader confidence="0.997301">
4.2 Solution Search Algorithm
</subsectionHeader>
<bodyText confidence="0.999996454545454">
The stochastic context-free grammar used for syn-
tactic analysis consists of rewriting rules (see for-
mula (3)) in Chomsky normal form (Hoperoft and
Ullman, 1979) except for the derivation from the
start symbol (formula (2)). It follows that a CKY
method extended to SCFG, a dynamic-programming
method, is applicable to calculate the best solution
in 0(n3) time, where n is the number of input char-
acters. It should be noted that it is necessary to
multiply the probability of the derivation from the
start symbol at the end of the process.
</bodyText>
<sectionHeader confidence="0.997275" genericHeader="evaluation">
5 Evaluation
</sectionHeader>
<bodyText confidence="0.999992714285714">
We constructed the POS-based dependency model
and the class-based dependency model to evaluate
their predictive power. In addition, we implemented
parsers based on them which calculate the best syn-
tactic tree from a given sequence of bunsetsu to ob-
serve their accuracy. In this section, we present the
experimental results and discuss them.
</bodyText>
<subsectionHeader confidence="0.998967">
5.1 Conditions on the Experiments
</subsectionHeader>
<bodyText confidence="0.999712181818182">
As a syntactically annotated corpus we used EDR
corpus (Jap, 1993). The corpus was divided into
ten parts and the models estimated from nine of
them were tested on the rest in terms of cross en-
tropy (see Table 1). The number of characters in
the Japanese writing system is set to 6,879. Two
parameters which have not been determined yet in
the explanation of the models (d,„„„z and vniaz) are
both set to 1. Although the best value for each of
them can also be estimated using the average cross
entropy, they are fixed through the experiments.
</bodyText>
<tableCaption confidence="0.9971">
Table 2: Predictive power.
</tableCaption>
<table confidence="0.997890333333333">
language model #non-terminal cross
+#terminal entropy
5.3536
4.9944
POS-based model 576
class-based model 10,752
</table>
<subsectionHeader confidence="0.999882">
5.2 Evaluation of Predictive Power
</subsectionHeader>
<bodyText confidence="0.999995538461539">
For the purpose of evaluating the predictive power
of the models, we calculated their cross entropy on
the test corpus. In this process the annotated tree
is used as the structure of the sentences in the test
corpus. Therefore the probability of each sentence
in the test corpus is not the summation over all its
possible derivations. In order to compare the POS-
based dependency model and the class-based depen-
dency model, we constructed these models from the
same learning corpus and calculated their cross en-
tropy on the same test corpus. They are both inter-
polated with the SCFG with uniform distribution.
The processes for their construction are as follows:
</bodyText>
<listItem confidence="0.987851785714286">
• POS-based dependency model
1. estimate the interpolation coefficients in
Formula (4) by the deleted interpolation
method
2. count the frequency of each rewriting rule
on the whole learning corpus
• class-based dependency model
1. estimate the interpolation coefficients in
Formula (4) by the deleted interpolation
method
2. calculate an optimal word-class relation by
the method proposed in Section 3.
3. count the frequency of each rewriting rule
on the whole learning corpus
</listItem>
<bodyText confidence="0.9999845">
The word-based 2-gram model for bunsetsu gener-
ation and the character-based 2-gram model as an
unknown word model (Mori and Yarnaji, 1997) are
common to the POS-based model and class-based
model. Their contribution to the cross entropy is
constant on the condition that the dependency mod-
els contain the prediction of the last word of the con-
tent word sequence and that of the function word
sequence.
Table 2 shows the cross entropy of each model
on the test corpus. The cross entropy of the class-
based dependency model is lower than that of the
POS-based dependency model. This result attests
experimentally that the class-based model estimated
by our clustering method is more predictive than
the POS-based model and that our word clustering
</bodyText>
<page confidence="0.995929">
902
</page>
<tableCaption confidence="0.999516">
Table 3: Accuracy of each model.
</tableCaption>
<bodyText confidence="0.9998755625">
method is efficient at improvement of a dependency
model.
We also calculated the cross entropy of the class-
based model which we estimated with a word 2-gram
model as the model M in the Formula (5). The num-
ber of terminals and non-terminals is 1,148,916 and
the cross entropy is 6.3358, which is much higher
than that of the POS-base model. This result indi-
cates that the best word-class relation for the depen-
dency model is quite different from the best word-
class relation for the n-gram model. Comparing the
number of the terminals and non-terminals, the best
word-class relation for n-gram model is exceedingly
specialized for a dependency model. We can con-
clude that word-class relation depends on the lan-
guage model.
</bodyText>
<subsectionHeader confidence="0.999358">
5.3 Evaluation of Syntactic Analysis
</subsectionHeader>
<bodyText confidence="0.999970275862069">
We implemented a parser based on the dependency
models. Since our models, equipped with a word-
based 2-gram model for bunsetsu generation and the
character-based 2-gram as an unknown word model,
can return the probability for any input, we can
build a parser, based on our model, receiving a char-
acter sequence as input. Its evaluation is not easy,
however, because errors may occur in bunsetsu gen-
eration or in POS estimation of unknown words. For
this reason, in the following description, we assume
a bunsetsu sequence as the input.
The criterion we adopted is the accuracy of depen-
dency relation, but the last bunsetsu, which has no
bunsetsu to depend on, and the second-to-last bun-
setsu, which depends always on the last bunsetsu,
are excluded from consideration.
Table 3 shows cross entropy and parsing accuracy
of the POS-based dependency model and the class-
based dependency model. This result tells us our
word clustering method increases parsing accuracy
considerably. This is quite natural in the light of the
decrease of cross entropy.
The relation between the learning corpus size and
cross entropy or parsing accuracy is shown in Fig-
ure 3. The lower bound of cross entropy is the en-
tropy of Japanese, which is estimated to be 4.3033
bit (Mori and Yamaji, 1997). Taking this fact into
consideration, the cross entropy of both of the mod-
els has stronger tendency to decrease. As for ac-
</bodyText>
<figure confidence="0.680169">
100 101 102 10&apos; 104 10&apos; 106
Ocharacters in learning corpus
</figure>
<figureCaption confidence="0.9834865">
Figure 3: Relation between cross entropy and pars-
ing accuracy.
</figureCaption>
<bodyText confidence="0.999587833333333">
curacy, there also is a tendency to get more accu-
rate as the learning corpus size increases, but it is a
strong tendency for the class-based model than for
the POS-based model. It follows that the class-based
model profits more greatly from an increase of the
learning corpus size.
</bodyText>
<sectionHeader confidence="0.999433" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999361833333333">
In this paper we have presented dependency mod-
els for Japanese based on the attribute of bunsetsu.
They are the first fully stochastic dependency mod-
els for Japanese which describes from character se-
quence to syntactic tree. Next we have proposed
a word clustering method, an extension of deleted
interpolation technique, which has been proven to
be efficient in terms of improvement of the pre-
dictive power. Finally we have discussed parsers
based on our model which demonstrated a remark-
able improvement in parsing accuracy by our word-
clustering method.
</bodyText>
<sectionHeader confidence="0.997427" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.961988066666667">
Peter F. Brown, Vincent J. Della Pietra, Peter V.
deSouza, Jennifer C. Lai, and Robert L. Mercer.
1992. Class-based n-gram models of natural lan-
guage. Computational Linguistics, 18(4):467-479.
Eugene Charniak. 1997. Statistical parsing with a
context-free grammar and word statistics. In Pro-
ceedings of the 14th National Conference on Arti-
ficial Intelligence, pages 598-603.
Michael Collins. 1997. Three generative, lexicalised
models for statistical parsing. In Proceedings of
the 35th Annual Meeting of the Association for
Computational Linguistics, pages 16-23.
King Sun Fu. 1974. Syntactic Methods in Pattern
Recognition, volume 12 of Mathematics in Science
and Engineering. Accademic Press.
</reference>
<figure confidence="0.987029625">
language model
POS-based model
class-based model
select always
the next bunsetsu
cross
entropy
5.3536
4.9944
accuracy
68.77%
81.96%
53.10%
11:10.130d SS0J3
12
10
8
6
4
2
accuracy
POS-lusui deraulescy model
dass-baced depowleacy model
100 lc
80
•
2
60 O-
a
40 7,
20
2
</figure>
<page confidence="0.994584">
903
</page>
<reference confidence="0.990296875">
T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and
T. Nishino. 1989. A probabilistic parsing method
for sentence disambiguation. In Proceedings of the
International Parsing Workshop.
Wide R. Hogenhout and Yuji Matsumoto. 1997. A
preliminary study of word clustering based on syn-
tactic behavior. In Proceedings of the Computa-
tional Natural Language Learning, pages 16-24.
John E. Hoperoft and Jeffrey D. Ullman. 1979. In-
troduction to Automata Theory, Languages and
Computation. Addison-Wesley Publishing.
Japan Electronic Dictionary Research Institute,
Ltd., 1993. EDR Electronic Dictionary Technical
Guide.
Fredelick Jelinek, Robert L. Mercer, and Salim
Roukos. 1991. Principles of lexical language
modeling for speech recognition. In Advances in
Speech Signal Processing, chapter 21, pages 651-
699. Dekker.
F. Jelinek, J. Lafferty, D. Magerman, R. Mercer,
A. Rantnaparkhi, and S. Roukos. 1994. Decision
tree parsing using a hidden derivation model. In
Proceedings of the ARPA Workshop on Human
Language Technology, pages 256-261.
Hiroshi Maruyama and Shiho Ogino. 1992. A statis-
tical property of japanese phrase-to-phrase modifi-
cations. Mathematical Linguistics, 18(7):348-352.
Shinsuke Mori and Osamu Yamaji. 1997. An
estimate of an upper bound for the entropy
of japanese. Transactions of Information Pro-
cessing Society of Japan, 38(11):2191-2199. (In
Japanese).
Shinsuke Mori, Masafumi Nishimura, and Nobuyuki
Ito. 1997. Word clustering for class-based lan-
guage models. Transactions of Information Pro-
cessing Society of Japan, 38(11):2200-2208. (In
Japanese).
C. E. Shannon. 1951. Prediction and entropy of
printed english. Bell System Technical Journal,
30:50-64.
</reference>
<page confidence="0.988442">
904
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.173918">
<title confidence="0.9630615">A Stochastic Language Model using Dependency and Its Improvement by Word Clustering</title>
<author confidence="0.839291">Shinsuke Mori</author>
<affiliation confidence="0.9844805">Tokyo Research Labolatory, IBM Japan, Ltd.</affiliation>
<address confidence="0.895336">1623-14 Shimotsuruma Yamatoshi, Japan</address>
<author confidence="0.990868">Makoto Nagao</author>
<affiliation confidence="0.999729">Kyoto University</affiliation>
<author confidence="0.484124">Yoshida-honmachi Sakyo Kyoto</author>
<author confidence="0.484124">Japan</author>
<abstract confidence="0.997294928571428">In this paper, we present a stochastic language model for Japanese using dependency. The prediction unit in this model is an attribute of &amp;quot;bunsetsu&amp;quot;. This is represented by the product of the head of content words and that of function words. The relation between the attributes of &amp;quot;bunsetsu&amp;quot; is ruled by a context-free grammar. The word sequences are predicted from the attribute using word n-gram model. The spell of Unknow word is predicted using character n-gram model. This model is robust in that it can compute the probability of an arbitrary string and is complete in that it models from unknown word to dependency at the same time.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Vincent J Della Pietra</author>
<author>Peter V deSouza</author>
<author>Jennifer C Lai</author>
<author>Robert L Mercer</author>
</authors>
<title>Class-based n-gram models of natural language.</title>
<date>1992</date>
<journal>Computational Linguistics,</journal>
<pages>18--4</pages>
<contexts>
<context position="4569" citStr="Brown et al., 1992" startWordPosition="731" endWordPosition="734">nts in the present paper attest, word-class relation is dependent on language model. In this paper, taking Japanese as the object language, we propose two complete stochastic language models using dependency between bunsetsu, a sequence of one or more content words followed by zero, one or more function words, and evaluate their predictive power by cross entropy. Since the number of sorts of bunsetsu is enormous, considering it as a symbol to be predicted would surely invoke the datasparseness problem. To cope with this problem we 898 use the concept of class proposed for a word n-gram model (Brown et al., 1992). Each bunsetsu is represented by the class calculated from the POS of its last content word and that of its last function word. The relation between bunsetsu, called dependency, is described by a stochastic context-free grammar (Fu, 1974) on the classes. From the class of a bunsetsu, the content word sequence and the function word sequence are independently predicted by word n-gram models equipped with unknown word models (Mori and Yamaji, 1997). The above model assumes that the syntactic behavior of each bunsetsu depends only on POS. The POS system invented by grammarians may not always be t</context>
<context position="7650" citStr="Brown et al., 1992" startWordPosition="1260" endWordPosition="1263">s follows: Bnst = Cont+ Fune U Cont+ Func* Sign, where the signs &amp;quot;+&amp;quot; and &amp;quot;*&amp;quot; mean positive closure and Kleene closure respectively. Since the relations between bunsetsu known as dependency are not always between sequential ones, we use SCFG to describe them (Fu, 1974). The first problem is how to choose terminal symbols. The simplest way is to select each bunsetsu as a terminal symbol. In this case, however, the data-sparseness problem would surely be invoked, since the number of possible bunsetsu is enormous. To avoid this problem we use the concept of class proposed for a word n-gram model (Brown et al., 1992). All bunsetsu are grouped by the attribute defined as follows: attrib(b) (1) = (last(cont(b)), last(func(b)), last(sign(b))), where the functions cont, func and sign take a bunsetsu as their argument and return its content word sequence, its function word sequence and its punctuation respectively. In addition, the function last(m) returns the POS of the last element of word sequence m or NULL if the sequence has no word. Given the attribute, the content word sequence and the function word sequence of the bunsetsu are independently generated by word-based 2-gram models (Mori and Yamaji, 1997).</context>
</contexts>
<marker>Brown, Pietra, deSouza, Lai, Mercer, 1992</marker>
<rawString>Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza, Jennifer C. Lai, and Robert L. Mercer. 1992. Class-based n-gram models of natural language. Computational Linguistics, 18(4):467-479.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eugene Charniak</author>
</authors>
<title>Statistical parsing with a context-free grammar and word statistics.</title>
<date>1997</date>
<booktitle>In Proceedings of the 14th National Conference on Artificial Intelligence,</booktitle>
<pages>598--603</pages>
<marker>Charniak, 1997</marker>
<rawString>Eugene Charniak. 1997. Statistical parsing with a context-free grammar and word statistics. In Proceedings of the 14th National Conference on Artificial Intelligence, pages 598-603.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Collins</author>
</authors>
<title>Three generative, lexicalised models for statistical parsing.</title>
<date>1997</date>
<booktitle>In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>16--23</pages>
<contexts>
<context position="2934" citStr="Collins, 1997" startWordPosition="469" endWordPosition="470"> in almost all of the recent practical applications in that it describes only relations between sequential elements. Some linguistic phenomena, however, are better described by assuming relations between separated elements. And modeling this kind of phenomena, the accuracies of various application are generally augmented. As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et al., 1989) is used for model description. Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from the head-child. This kind of specialization may, however, be excessive if the criterion is predictive power of the model. Research aimed at estimating the best specialization level for 2-gram model (Mori et al., 1997) shows a class-based model is more predictive than a word-based 2-gram model, a completely lexicalized model, comparing cross entropy of a POS-based 2-gram model, a word-based 2-gram model and a class-based 2-gram model, estimated from information theoreti</context>
</contexts>
<marker>Collins, 1997</marker>
<rawString>Michael Collins. 1997. Three generative, lexicalised models for statistical parsing. In Proceedings of the 35th Annual Meeting of the Association for Computational Linguistics, pages 16-23.</rawString>
</citation>
<citation valid="true">
<authors>
<author>King Sun Fu</author>
</authors>
<title>Syntactic Methods in Pattern Recognition,</title>
<date>1974</date>
<booktitle>Mathematics in Science and Engineering.</booktitle>
<volume>12</volume>
<publisher>Accademic Press.</publisher>
<contexts>
<context position="4808" citStr="Fu, 1974" startWordPosition="772" endWordPosition="773">re content words followed by zero, one or more function words, and evaluate their predictive power by cross entropy. Since the number of sorts of bunsetsu is enormous, considering it as a symbol to be predicted would surely invoke the datasparseness problem. To cope with this problem we 898 use the concept of class proposed for a word n-gram model (Brown et al., 1992). Each bunsetsu is represented by the class calculated from the POS of its last content word and that of its last function word. The relation between bunsetsu, called dependency, is described by a stochastic context-free grammar (Fu, 1974) on the classes. From the class of a bunsetsu, the content word sequence and the function word sequence are independently predicted by word n-gram models equipped with unknown word models (Mori and Yamaji, 1997). The above model assumes that the syntactic behavior of each bunsetsu depends only on POS. The POS system invented by grammarians may not always be the best in terms of stochastic language modeling. This is experimentally attested by the paper (Mori et al., 1997) reporting comparisons between a P OS-based n-gram model and a class-based n-gram model induced automatically. We now propose</context>
<context position="7299" citStr="Fu, 1974" startWordPosition="1201" endWordPosition="1202">ds are predicted from POS by a character-based 2-gram model. 2.1 Sentence Model A Japanese sentence is considered as a sequence of units called bunsetsu composed of one or more content words and function words. Let Cont be a set of content words, Func a set of function words and Sign a set of punctuation symbols. Then bunsetsu is defined as follows: Bnst = Cont+ Fune U Cont+ Func* Sign, where the signs &amp;quot;+&amp;quot; and &amp;quot;*&amp;quot; mean positive closure and Kleene closure respectively. Since the relations between bunsetsu known as dependency are not always between sequential ones, we use SCFG to describe them (Fu, 1974). The first problem is how to choose terminal symbols. The simplest way is to select each bunsetsu as a terminal symbol. In this case, however, the data-sparseness problem would surely be invoked, since the number of possible bunsetsu is enormous. To avoid this problem we use the concept of class proposed for a word n-gram model (Brown et al., 1992). All bunsetsu are grouped by the attribute defined as follows: attrib(b) (1) = (last(cont(b)), last(func(b)), last(sign(b))), where the functions cont, func and sign take a bunsetsu as their argument and return its content word sequence, its functi</context>
</contexts>
<marker>Fu, 1974</marker>
<rawString>King Sun Fu. 1974. Syntactic Methods in Pattern Recognition, volume 12 of Mathematics in Science and Engineering. Accademic Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Fujisaki</author>
<author>F Jelinek</author>
<author>J Cocke</author>
<author>E Black</author>
<author>T Nishino</author>
</authors>
<title>A probabilistic parsing method for sentence disambiguation.</title>
<date>1989</date>
<booktitle>In Proceedings of the International Parsing Workshop.</booktitle>
<contexts>
<context position="2760" citStr="Fujisaki et al., 1989" startWordPosition="440" endWordPosition="443">quences of the alphabet (n-gram) on a corpus containing a large number of sentences of a language. This is the same model as This work is done when the anther was at Kyoto Univ. used in almost all of the recent practical applications in that it describes only relations between sequential elements. Some linguistic phenomena, however, are better described by assuming relations between separated elements. And modeling this kind of phenomena, the accuracies of various application are generally augmented. As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et al., 1989) is used for model description. Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from the head-child. This kind of specialization may, however, be excessive if the criterion is predictive power of the model. Research aimed at estimating the best specialization level for 2-gram model (Mori et al., 1997) shows a class-based model is more predictive than a word-based 2-gram model, a co</context>
</contexts>
<marker>Fujisaki, Jelinek, Cocke, Black, Nishino, 1989</marker>
<rawString>T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and T. Nishino. 1989. A probabilistic parsing method for sentence disambiguation. In Proceedings of the International Parsing Workshop.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Wide R Hogenhout</author>
<author>Yuji Matsumoto</author>
</authors>
<title>A preliminary study of word clustering based on syntactic behavior.</title>
<date>1997</date>
<booktitle>In Proceedings of the Computational Natural Language Learning,</booktitle>
<pages>16--24</pages>
<marker>Hogenhout, Matsumoto, 1997</marker>
<rawString>Wide R. Hogenhout and Yuji Matsumoto. 1997. A preliminary study of word clustering based on syntactic behavior. In Proceedings of the Computational Natural Language Learning, pages 16-24.</rawString>
</citation>
<citation valid="true">
<authors>
<author>John E Hoperoft</author>
<author>Jeffrey D Ullman</author>
</authors>
<title>Introduction to Automata Theory, Languages and Computation.</title>
<date>1979</date>
<publisher>Addison-Wesley Publishing.</publisher>
<contexts>
<context position="16146" citStr="Hoperoft and Ullman, 1979" startWordPosition="2748" endWordPosition="2751">st 19,397 178,415 471,189 = argmax P(Tx)P(x) W(T)=X =argmax P(x1T)P(T) (..• Bayes&apos; formula) W(T)=X = argmax P(T) (..• P(xIT) = 1), W(T)=X where w(T) represents the character sequence of the syntactic tree T. P(T) in the last line is a stochastic language model including the concept of dependency. We use, as such a model, the POS-based dependency model described in section 2 or the classbased dependency model described in section 3. 4.2 Solution Search Algorithm The stochastic context-free grammar used for syntactic analysis consists of rewriting rules (see formula (3)) in Chomsky normal form (Hoperoft and Ullman, 1979) except for the derivation from the start symbol (formula (2)). It follows that a CKY method extended to SCFG, a dynamic-programming method, is applicable to calculate the best solution in 0(n3) time, where n is the number of input characters. It should be noted that it is necessary to multiply the probability of the derivation from the start symbol at the end of the process. 5 Evaluation We constructed the POS-based dependency model and the class-based dependency model to evaluate their predictive power. In addition, we implemented parsers based on them which calculate the best syntactic tree</context>
</contexts>
<marker>Hoperoft, Ullman, 1979</marker>
<rawString>John E. Hoperoft and Jeffrey D. Ullman. 1979. Introduction to Automata Theory, Languages and Computation. Addison-Wesley Publishing.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<booktitle>EDR Electronic Dictionary Technical Guide.</booktitle>
<institution>Japan Electronic Dictionary Research Institute, Ltd.,</institution>
<marker>1993</marker>
<rawString>Japan Electronic Dictionary Research Institute, Ltd., 1993. EDR Electronic Dictionary Technical Guide.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fredelick Jelinek</author>
<author>Robert L Mercer</author>
<author>Salim Roukos</author>
</authors>
<title>Principles of lexical language modeling for speech recognition.</title>
<date>1991</date>
<booktitle>In Advances in Speech Signal Processing, chapter 21,</booktitle>
<pages>651--699</pages>
<publisher>Dekker.</publisher>
<contexts>
<context position="12139" citStr="Jelinek et al., 1991" startWordPosition="2060" endWordPosition="2063">s follows: P(S d1, vi)) N(S (a1, d1, vi)) N(S) P((ai, di, vi) = (a2, d2, n2)(a3, d3, v3)) N((al, dl, v1)=0. (a2, d2, v2)(a3, d3, v3)) N((ai, d1, vi)) In a word n-gram model, in order to cope with data-sparseness problem, the interpolation technique is applicable to SCFG. The probability of the interpolated model of grammars G1 and G2, whose 900 probabilities are Pi. and P2 respectively, is represented as follows: P(A a) = Pi (A a) + A2P2 (A a) 0 &lt; Ai &lt; 1 (j = 1,2) and A1 +A2 = 1 (4) where A E V and a E (V UT)*. The coefficients are estimated by held-out method or deleted interpolation method (Jelinek et al., 1991). 3 Word Clustering The model we have mentioned above uses the POS given manually for the attribute of bunsetsu. Changing it into some class may improve the predictive power of the model. This change needs only a slight replacement in the model representing formula (1): the function last returns the class of the last word of a word sequence m instead of the POS. The problem we have to solve here is how to obtain such classes i.e. word clustering. In this section, we propose an objective function and a search algorithm of the word clustering. 3.1 Objective Function The aim of word clustering is</context>
</contexts>
<marker>Jelinek, Mercer, Roukos, 1991</marker>
<rawString>Fredelick Jelinek, Robert L. Mercer, and Salim Roukos. 1991. Principles of lexical language modeling for speech recognition. In Advances in Speech Signal Processing, chapter 21, pages 651-699. Dekker.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Jelinek</author>
<author>J Lafferty</author>
<author>D Magerman</author>
<author>R Mercer</author>
<author>A Rantnaparkhi</author>
<author>S Roukos</author>
</authors>
<title>Decision tree parsing using a hidden derivation model.</title>
<date>1994</date>
<booktitle>In Proceedings of the ARPA Workshop on Human Language Technology,</booktitle>
<pages>256--261</pages>
<contexts>
<context position="2918" citStr="Jelinek et al., 1994" startWordPosition="465" endWordPosition="468">as at Kyoto Univ. used in almost all of the recent practical applications in that it describes only relations between sequential elements. Some linguistic phenomena, however, are better described by assuming relations between separated elements. And modeling this kind of phenomena, the accuracies of various application are generally augmented. As for English, there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et al., 1989) is used for model description. Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from the head-child. This kind of specialization may, however, be excessive if the criterion is predictive power of the model. Research aimed at estimating the best specialization level for 2-gram model (Mori et al., 1997) shows a class-based model is more predictive than a word-based 2-gram model, a completely lexicalized model, comparing cross entropy of a POS-based 2-gram model, a word-based 2-gram model and a class-based 2-gram model, estimated from info</context>
</contexts>
<marker>Jelinek, Lafferty, Magerman, Mercer, Rantnaparkhi, Roukos, 1994</marker>
<rawString>F. Jelinek, J. Lafferty, D. Magerman, R. Mercer, A. Rantnaparkhi, and S. Roukos. 1994. Decision tree parsing using a hidden derivation model. In Proceedings of the ARPA Workshop on Human Language Technology, pages 256-261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Hiroshi Maruyama</author>
<author>Shiho Ogino</author>
</authors>
<title>A statistical property of japanese phrase-to-phrase modifications.</title>
<date>1992</date>
<booktitle>Mathematical Linguistics,</booktitle>
<pages>18--7</pages>
<contexts>
<context position="9424" citStr="Maruyama and Ogino, 1992" startWordPosition="1554" endWordPosition="1557"> is a binary relation — that each relation is independent of the others. Then this relation is representing by the following form of rewriting rule of CFG: B = AB, where A is the attribute of the anterior bunsetsu and B is that of the posterior. Similarly to terminal symbols, non-terminal symbols can be defined as the attribute of bunsetsu. Also they can be defined as the product of the attribute and some additional information to reflect the characteristics of the dependency. It is reported that the dependency is more frequent between closer bunsetsu in terms of the position in the sentence (Maruyama and Ogino, 1992). In order to model these characteristics, we add to the attribute of bunsetsu an 899 (verb, ending. period. 2, 0) (noun. NULL. comma, 0, 0) (noun. postp., NULL. 0, 0) (verb, ending, period, 0. 0) kyou / noun . / sign (today) Kyoto / noun daigaku / noun he / postp. (Kyoto) (university) (to) i / verb ku / ending . / sign (go) t (verb, ending, period. I, 0) SCFG n-gram Figure 1: Dependency model based on bunsetsu additional information field holding the number of bunsetsu depending on it. Also the fact that a bunsetsu has a tendency to depend on a bunsetsu with comma. For this reason the number </context>
</contexts>
<marker>Maruyama, Ogino, 1992</marker>
<rawString>Hiroshi Maruyama and Shiho Ogino. 1992. A statistical property of japanese phrase-to-phrase modifications. Mathematical Linguistics, 18(7):348-352.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Osamu Yamaji</author>
</authors>
<title>An estimate of an upper bound for the entropy of japanese.</title>
<date>1997</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<pages>38--11</pages>
<note>(In Japanese).</note>
<contexts>
<context position="5019" citStr="Mori and Yamaji, 1997" startWordPosition="805" endWordPosition="808">be predicted would surely invoke the datasparseness problem. To cope with this problem we 898 use the concept of class proposed for a word n-gram model (Brown et al., 1992). Each bunsetsu is represented by the class calculated from the POS of its last content word and that of its last function word. The relation between bunsetsu, called dependency, is described by a stochastic context-free grammar (Fu, 1974) on the classes. From the class of a bunsetsu, the content word sequence and the function word sequence are independently predicted by word n-gram models equipped with unknown word models (Mori and Yamaji, 1997). The above model assumes that the syntactic behavior of each bunsetsu depends only on POS. The POS system invented by grammarians may not always be the best in terms of stochastic language modeling. This is experimentally attested by the paper (Mori et al., 1997) reporting comparisons between a P OS-based n-gram model and a class-based n-gram model induced automatically. We now propose, based on this report, a word-clustering method on the model we have mentioned above to successfully improve the predictive power. In addition, we discuss a parsing method as an application of the model. We als</context>
<context position="8249" citStr="Mori and Yamaji, 1997" startWordPosition="1354" endWordPosition="1357">el (Brown et al., 1992). All bunsetsu are grouped by the attribute defined as follows: attrib(b) (1) = (last(cont(b)), last(func(b)), last(sign(b))), where the functions cont, func and sign take a bunsetsu as their argument and return its content word sequence, its function word sequence and its punctuation respectively. In addition, the function last(m) returns the POS of the last element of word sequence m or NULL if the sequence has no word. Given the attribute, the content word sequence and the function word sequence of the bunsetsu are independently generated by word-based 2-gram models (Mori and Yamaji, 1997). 2.2 Dependency Model In order to describe the relation between bunsetsu called dependency, we make the generally accepted assumption that no two dependency relations cross each other, and we introduce a SCFG with the attribute of bunsetsu as terminals. It is known, as a characteristic of the Japanese language, that each bunsetsu depends on the single bunsetsu appearing just before it. We say of two sequential bunsetsu that the first to appear is the anterior and the second is the posterior. We assume, in addition, that the dependency relation is a binary relation — that each relation is inde</context>
<context position="21584" citStr="Mori and Yamaji, 1997" startWordPosition="3651" endWordPosition="3654">pend on, and the second-to-last bunsetsu, which depends always on the last bunsetsu, are excluded from consideration. Table 3 shows cross entropy and parsing accuracy of the POS-based dependency model and the classbased dependency model. This result tells us our word clustering method increases parsing accuracy considerably. This is quite natural in the light of the decrease of cross entropy. The relation between the learning corpus size and cross entropy or parsing accuracy is shown in Figure 3. The lower bound of cross entropy is the entropy of Japanese, which is estimated to be 4.3033 bit (Mori and Yamaji, 1997). Taking this fact into consideration, the cross entropy of both of the models has stronger tendency to decrease. As for ac100 101 102 10&apos; 104 10&apos; 106 Ocharacters in learning corpus Figure 3: Relation between cross entropy and parsing accuracy. curacy, there also is a tendency to get more accurate as the learning corpus size increases, but it is a strong tendency for the class-based model than for the POS-based model. It follows that the class-based model profits more greatly from an increase of the learning corpus size. 6 Conclusion In this paper we have presented dependency models for Japane</context>
</contexts>
<marker>Mori, Yamaji, 1997</marker>
<rawString>Shinsuke Mori and Osamu Yamaji. 1997. An estimate of an upper bound for the entropy of japanese. Transactions of Information Processing Society of Japan, 38(11):2191-2199. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shinsuke Mori</author>
<author>Masafumi Nishimura</author>
<author>Nobuyuki Ito</author>
</authors>
<title>Word clustering for class-based language models.</title>
<date>1997</date>
<journal>Transactions of Information Processing Society of Japan,</journal>
<pages>38--11</pages>
<note>(In Japanese).</note>
<contexts>
<context position="3278" citStr="Mori et al., 1997" startWordPosition="522" endWordPosition="525">there have been researches in which a stochastic context-free grammar (SCFG) (Fujisaki et al., 1989) is used for model description. Recently some researchers have pointed out the importance of the lexicon and proposed lexicalized models (Jelinek et al., 1994; Collins, 1997). In these models, every headword is propagated up through the derivation tree such that every parent receives a headword from the head-child. This kind of specialization may, however, be excessive if the criterion is predictive power of the model. Research aimed at estimating the best specialization level for 2-gram model (Mori et al., 1997) shows a class-based model is more predictive than a word-based 2-gram model, a completely lexicalized model, comparing cross entropy of a POS-based 2-gram model, a word-based 2-gram model and a class-based 2-gram model, estimated from information theoretical point of view. As fora parser based on a class-based SCFG, Charnia.k (1997) reports better accuracy than the above lexicalized models, but the clustering method is not clear enough and, in addition, there is no report on predictive power (cross entropy or perplexity). Hogenhout and Matsumato (1997) propose a wordclustering method based on</context>
<context position="5283" citStr="Mori et al., 1997" startWordPosition="851" endWordPosition="854"> that of its last function word. The relation between bunsetsu, called dependency, is described by a stochastic context-free grammar (Fu, 1974) on the classes. From the class of a bunsetsu, the content word sequence and the function word sequence are independently predicted by word n-gram models equipped with unknown word models (Mori and Yamaji, 1997). The above model assumes that the syntactic behavior of each bunsetsu depends only on POS. The POS system invented by grammarians may not always be the best in terms of stochastic language modeling. This is experimentally attested by the paper (Mori et al., 1997) reporting comparisons between a P OS-based n-gram model and a class-based n-gram model induced automatically. We now propose, based on this report, a word-clustering method on the model we have mentioned above to successfully improve the predictive power. In addition, we discuss a parsing method as an application of the model. We also report the result of experiments conducted on EDR corpus (Jap, 1993) The corpus is divided into ten parts and the models estimated from nine of them are tested on the rest in terms of cross entropy. As the result, the cross entropy of the POSbased dependency mod</context>
<context position="12962" citStr="Mori et al., 1997" startWordPosition="2205" endWordPosition="2208"> only a slight replacement in the model representing formula (1): the function last returns the class of the last word of a word sequence m instead of the POS. The problem we have to solve here is how to obtain such classes i.e. word clustering. In this section, we propose an objective function and a search algorithm of the word clustering. 3.1 Objective Function The aim of word clustering is to build a language model with less cross entropy without referring to the test corpus. Similar research has been successful, aiming at an improvement of a word n-gram model both in English and Japanese (Mori et al., 1997). So we have decided to extend this research to obtain an optimal word-class relation. The only difference from the previous research is the language model. In this case, it is a SCFG in stead of a ngram model. Therefore the objective function, called average cross entropy, is defined as follows: Let m1, m2, ... , mn be .A4 sorted in the descending order of frequency. Tn2) • • • inn} C = foreach i (1, 2, • , n) f (mi) := foreach i (1, 2, • • • , n) c:= 71-(move(f, mi, c)) argmincEcuc„ if (H(move(f, mi, c)) &lt; H(f)) then f :=move(f, mi, update interpolation coefficients. if (c = cne.) then C := </context>
</contexts>
<marker>Mori, Nishimura, Ito, 1997</marker>
<rawString>Shinsuke Mori, Masafumi Nishimura, and Nobuyuki Ito. 1997. Word clustering for class-based language models. Transactions of Information Processing Society of Japan, 38(11):2200-2208. (In Japanese).</rawString>
</citation>
<citation valid="true">
<authors>
<author>C E Shannon</author>
</authors>
<title>Prediction and entropy of printed english.</title>
<date>1951</date>
<journal>Bell System Technical Journal,</journal>
<pages>30--50</pages>
<contexts>
<context position="2058" citStr="Shannon, 1951" startWordPosition="329" endWordPosition="330">ng account of expected accuracy of recognition or analysis. This method is, however, heavily dependent on the problem and offers no systematic solution, as far as we know. The methodology of stochastic language modeling, however, allows us to separate, from various frameworks of natural language processing, the language description model common to them and enables us a systematic improvement of each application. In this framework a description on a language is represented as a map from a sequence of alphabetic characters to a probability value. The first model is C. E. Shannon&apos;s n-gram model (Shannon, 1951). The parameters of the model are estimated from the frequency of n character sequences of the alphabet (n-gram) on a corpus containing a large number of sentences of a language. This is the same model as This work is done when the anther was at Kyoto Univ. used in almost all of the recent practical applications in that it describes only relations between sequential elements. Some linguistic phenomena, however, are better described by assuming relations between separated elements. And modeling this kind of phenomena, the accuracies of various application are generally augmented. As for English</context>
</contexts>
<marker>Shannon, 1951</marker>
<rawString>C. E. Shannon. 1951. Prediction and entropy of printed english. Bell System Technical Journal, 30:50-64.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>