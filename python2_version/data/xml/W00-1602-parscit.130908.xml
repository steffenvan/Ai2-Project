<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000002">
<title confidence="0.972542">
Precompilation of HPSG in ALE into a CFG For Fast Parsing
</title>
<author confidence="0.906118">
John C. Brown
</author>
<affiliation confidence="0.714224">
INVU Services Ltd.,
</affiliation>
<address confidence="0.728717">
Blisworth Hill Farm,
Stoke Road,
Blisworth, UK, NN7 3DB
</address>
<email confidence="0.965133">
johnbrown@research.softnet.co.uk
</email>
<author confidence="0.737077">
Suresh Manandhar
</author>
<affiliation confidence="0.996362">
Department of Computer Science,
University of York,
</affiliation>
<address confidence="0.80549">
York, UK, YO1 5DD
</address>
<email confidence="0.964895">
Suresh@cs.york.ac.uk
</email>
<sectionHeader confidence="0.996815" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999905">
Context free grammars parse faster than TFS
grammars, but have disadvantages. On our test
TFS grammar, precompilation into CFG results
in a speedup of 16 times for parsing without
taking into account additional mechanisms for
increasing parsing efficiency. A formal
overview is given of precompilation and
parsing. Modifications to ALE rules permit a
closure over the rules from the lexicon, and
analysis leading to a fast treatment of semantic
structure. The closure algorithm, and retrieval
of full semantic structure are described.
</bodyText>
<sectionHeader confidence="0.975642" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999447">
Head Driven Phrase Structure Grammar
(HPSG), Pollard and Sag (1994) is expressed
in Typed Feature Structures (TFSs). Context
Free Grammar (CFG) without features
supports much faster parsing, but a TFS
grammar has many advantages. Fast parsing
can be obtained by precompiling a CFG
approximation, with TFSs converted into CF
near-equivalents. CFG parsing eliminates
impossible trees, and TFS unification over the
remainder eliminates more, and instantiates
path values. Our method treats slashes
separately in a precompiled table, and careful
allocation of categories to TFSs makes TFS
unification unnecessary: instead skeleton
semantic structures are formed in parsing, and
full structures retrieved afterwards.
A prototype precompiler and fast parser1
were built in Prolog, and tested with an HPSG
grammar of English by Matheson (1996),
</bodyText>
<note confidence="0.512565">
1 Downloadable code on
</note>
<footnote confidence="0.9531835">
http://www.cs.york.ac.uk/~johnb or
http://www.soft.net.uk/research/hsppar.htm .
</footnote>
<bodyText confidence="0.998149636363636">
written in the ALE formalism, by Carpenter
and Penn (1996). This has the 6 schemas and 5
principles of HPSG, with 184 lexemes.
A complex sentence,“kim can believe sandy
can expect sandy to persuade kim to promise
sandy to try to expect sandy to persuade kim to
promise kim to give sandy a happy happy
book”, parsed in 3.3s. with retrieval, 5.6 times
faster than with TFSs, Brown and Manandhar
(2000). An 11 word sentence was 18 times
faster at 87ms., or 16 times counting retrieval.
</bodyText>
<sectionHeader confidence="0.988421" genericHeader="method">
1 Relationship to Work Elsewhere
</sectionHeader>
<bodyText confidence="0.999790407407407">
In precompilation, Torisawa et. al. (2000)
repeatedly applied rules leading to maximal
projections from lexical heads, allocating
categories to mothers and non-head daughters.
Our approach allocates categories in a closure
over the rules starting with the lexicon, as in
Kiefer and Krieger (2000). We differ from
both these in that the CFG grammar equates to
the TFS grammar, accepting exactly the same
strings: a CFG parse tree translates into a TFS
tree with no loss of nodes.
We precompiled 550 CF categories and
18,000 rules in 1 hour on a 280MHz. Pentium
II. This compares to 5,500 categories and
2,200,000 rules from 11,000 lexemes in 45
hours by Kiefer and Krieger on a 300MHz.
Sun Ultrasparc 2, where sets of TFSs in the
closure are replaced by common most-general
unifiers. The common unifier technique was
used to add a CFG back-bone to a unification
grammar by Carroll (1993). An np has the
same number, person and gender features
irrespective of an optional specifier and
multiple adjectives. A lexical vp, partially
saturated vp and sentence contain decreasing
subcategorisation data. Therefore numbers of
phrasal and lexical categories are comparable,
</bodyText>
<page confidence="0.998299">
13
</page>
<bodyText confidence="0.994683714285714">
The value following a feature in a TFS of a
particular type is a sub-type of that given by an
appropriateness function Approp:T × F → T
where F and T denote finite sets of feature
symbols and types. T is organised into a
subsumption hierarchy with a single root bot.
The grammar may either be lexicalist, or
have a large number of specialised rules. Each
TFS is well-formed, containing no feature not
appropriate to its type: it is not totally well-
formed with all appropriate features present.
TFS unification involves type-coercion which
also occurs in constraint application to a TFS,
for example of feature: value in a rule. Where
a TFS of type t1 does not contain feature, it is
coerced into the most general sub-type t2 of t1
for which Approp t2, feature is defined.
An integer Tl , corresponding to a CFG
category, is allocated in our precompilation to
each unique lexical TFSl , ignoring nested
TFSs encoding slashes and semantics, so that
</bodyText>
<equation confidence="0.988821125">
−
T retrieve (omit( :
1
= slash v 1
l
re introduce( : ,
− index v2
omit(semantics : v3 ,TFSl ))))
</equation>
<bodyText confidence="0.999844875">
where slash, semantics and index are paths and
v1, v2 and v3 are values. Implementation is
by matching against a discrimination tree
which is traversed in correspondance with
TFSl , from which types are ignored in a
subtree reached by path where
omit path:v, TFSl applies and considered
where re − introduce path:v,TFSl applies.
Where corresponding types from TFSl and the
tree are unequal, a new branch is grown,
terminating with a new Tl : this mechanism
ensures each terminal is marked. Because co-
indexing by syntactic paths in a rule (Section
5) unifies index, this is re-introduced in an np
and in a category that unifies with an np to
form an np. The function in (3) is an example
</bodyText>
<figure confidence="0.977672470588235">
,
(3)
value → TFS
TFS → FS
path feature
→
FS path value
→ :
type FS
&amp;
var iable TFS
&amp;
feature path
: (2)
FS FS
&amp;
type
</figure>
<bodyText confidence="0.991386333333333">
even with exact CF representation. Since this
eliminates Kiefer and Krieger’s annotation
with values of relevant paths, large rule
numbers are more tolerable.
Our parsing speed-ups are comparable with
Kentaro Torisawa’s speed-ups of between 47
and 4 in C++ with arrays to store rules and
edges, on a 300MHz. Sun Ultrasparc. We used
Prolog without constant-time access arrays,
with clauses hash-indexed on just the first
argument. Global data structures for edges in a
chart necessitate dynamic clauses with heavy
assertion costs: when referenced, all arguments
are treated before matching any against
precompiled tables. Tree-structured tables are
inefficient, using either one clause per arc with
heavy invocation costs, or arguments of nested
structures, copied on clause invocation. From
full instrumentation revealing bottlenecks, we
predict a further speed-up of 10 times, using
ideal global data structures in imperative code.
</bodyText>
<sectionHeader confidence="0.804708" genericHeader="method">
2 Formal View of Precompilation and
Parsing
</sectionHeader>
<bodyText confidence="0.999918695652174">
Our CF and TFS grammars are equivalent
for two reasons. First, in CF category
allocation, TFSs in the list reached by
NONLOCAL: INHERITED: SLASH are
allocated separate categories and are otherwise
ignored. The filler-head schema unifies a list
member with a TFS from the first daughter, so
a precompiled table can predict the outcome
during CF parsing. This avoids over-generation
of categories formed by alternative slash TFSs
in multiple phrasal TFSs arising in long-range
dependencies. Second, although semantic sub-
structure (also containing word morphology) is
omitted in category allocation, its index
structure may be considered depending on the
HEAD value. This makes CFG categories
maximally selective. The now unnecessary
TFS unification over the CFG parse tree is
replaced by semantic TFS retrieval from
skeleton structures in constituents.
The method treats grammar rules where
mother and daughters each comprise a TFS,
expressed as conjoined constraints :
</bodyText>
<equation confidence="0.941906">
TFS0→ TFS1 TFSn (1)
</equation>
<bodyText confidence="0.992795">
where a TFS is given by:
</bodyText>
<page confidence="0.998247">
14
</page>
<bodyText confidence="0.998592333333333">
of a restrictor, Shieber (1985), although here
we shall show that it does not lead to any
approximation in parsing.
Precompilation generates multiple
instantiations of each rule (1), paired with
equivalent CFG rules, stored in a tuple:
</bodyText>
<equation confidence="0.994243">
T0 — T1 Tn , rule − name (4)
</equation>
<bodyText confidence="0.9973159">
Each represents the pair-wise unification of
some TFS1 TFSn with a sequence of TFSs
where each is either lexical, like TFSl , or
derives from some rule instantiation, like
TFS0 . For each sequence T1 Tn of
categories derivable from such instantiations,
using the discrimination tree, only the first
corresponding instantiation encountered is
treated, since all generate the same T0.
By these means the closure becomes a
bounded operation. This approach is valid
since slashes are treated separately during
parsing, and as in GPSG, Gazdar et. al. (1985),
no phrase acceptable on syntactic grounds is
then rejected on semantic grounds.
The CFG rules (4) are treated by a parser
with a chart containing constituents :
The latter requires a CFG rule T0— T1 Tn
and consecutive constituents:
start end T sem slashes
</bodyText>
<equation confidence="0.994432666666667">
1 , 1 1
, , ,
1 1
</equation>
<bodyText confidence="0.852587">
startn ,endn ,Tn , semn , slashesn
where for &gt; ,
j start end
</bodyText>
<equation confidence="0.9724985">
1 j = −
j 1
</equation>
<bodyText confidence="0.9317224">
Slashes are treated by precompiling a table
with entries Ts , T where Ts represents Tl or
T0, and T is the category assigned to a slash
taken from a lexical TFS, and where
retrieve
T
prefixed by SYNSEM : LOC . Where in (4)
rule − name = filler − head , (5) is formed only
This mimics the operation of the filler − head
schema. In this case, in (5),
</bodyText>
<equation confidence="0.848366">
slashes0 = slashes1 U slashes2 − Ts2
</equation>
<bodyText confidence="0.994586175">
whereas with other schemas Ts2 = 0 : this
mimics the operation of the non-local-feature
principle. The CFG grammar generated by this
method accepts the same strings as the HPSG
grammar and does not just approximate it.
The semantic component sem0 represents a
TFS, identical to semj in that sub-constituent
known as the semantic head, according to the
semantics principle of HPSG, except that some
paths are co-indexed with paths in the semantic
components of the other sub-constituents:
sem0 = f sem1, ,semn (7)
To reduce copying overheads and eliminate
TFS unification, sem0 is encoded in a skeleton
form, which is a Prolog structure:
s_ n sem_ type0 , p1 , , pm (8)
The type sem_ type0 is not equal to T0, and is
the category of the lexical source of sem0 ,
which is a transitive semantic head of the
corresponding TFS0 . For example, the
sentence TFS contains a semantic component
that derives from the head verb. In parsing,
pk ,1s k s m is bound to the unique identifier
of the sub-constituent containing a path that
must be co-indexed with a path in semj . In the
prototype parser this path is the kth. one during
a traversal of semj in the lexical source, that is
found to be co-indexed with a syntactic path,
that is in turn found to be prefixed by a path
co-indexed between the corresponding
daughters of the original rule (1) forming
TFS0 (see Section 5).
At retrieval time,
given
by sem ( sem _ type pk , [p1 , ,pl ]), (10)
from the pk th. sub-constituent. A retrieval
graph is compiled at category allocation time
to support retrieve. Currently the semantic
details of the slash are not recovered: sentences
with slashes are accepted identically to ALE.
</bodyText>
<figure confidence="0.760360333333333">
start end T sem slashes
, , l , l, l
start, end, T0 ,sem0 ,slashes0
and
if 3 Ts,T • Ts = T1 ,, T = Ts2 Eslashes2
(6)
(5)
s unifies with retrieve T
sem sem(sem type [p
</figure>
<equation confidence="0.648888">
0 = _ 0 , 1 , ,pm]) (9)
</equation>
<bodyText confidence="0.974434833333333">
where this indicates the semantic sub-structure
in retrieve(sem _ type0 ), with the addition
that each path for co-indexing associated with
1s k s m is co-indexed with the appropriate
path
in the TFS
</bodyText>
<page confidence="0.989669">
15
</page>
<sectionHeader confidence="0.802987" genericHeader="method">
3 Modification of ALE rules
</sectionHeader>
<bodyText confidence="0.995687230769231">
Figure 1 is the head-subject-complement
schema in the ALE formalism. Figure 2 shows
sections of the single Prolog clause containing
57 goals in 61 lines including the head,
produced when ALE compiles the schema.
This is invoked by the ALE chart parser after
choosing the first of a speculative edge
sequence. Its TFS is the structure SVs, which
resembles (2): the functor represents the type,
and each argument is a value from a
(feature: value) pair. The feature is retrieved
by successive instantiations of an ALE clause
compiled from the grammar definition:
</bodyText>
<equation confidence="0.943039545454546">
approps( +type, (-approp_feature:
-approp_type)) (10)
(phrase,Mother,synsem:loc:(cat)
:(spr:[],subj:[],comps:[])) M
===&gt;
cat&gt; word,HeadDtr,synsem:loc:(cat):
(head:inv:plus,
spr:Spr,
subj:[SubjSynsem],
comps:CompSynsems)), D1
goal&gt; (list_sign_to_synsem(CompDtrs,
CompSynsems)), P1
cat&gt; (SubjDtr), D2
goal&gt; (sign_to_synsem(SubjDtr,
SubjSynsem)), P2
cats&gt; (CompDtrs,ne_list), D3
goal&gt; head_feature_principle(Mother,
HeadDtr), P3
semantics_principle(Mother,HeadDtr), P4
marking_principle(Mother,HeadDtr), P5
nonlocal_feature_principle(Mother,
HeadDtr,[SubjDtr|CompDtrs])). P6
</equation>
<figureCaption confidence="0.952289">
Figure 1: The Head_subject_complement
Schema from HPSG
</figureCaption>
<bodyText confidence="0.999945444444444">
Goals treat a TFS in a structure Tag-SVs, to
allow type-coercion during unification into a
sub-type, possibly supporting additional (never
fewer) features. Tag, originally unbound, is
then bound to a new copy: argument values are
appropriate types (10) for new features, and
unchanged for existing ones. Goal 21
references the second edge in the sequence,
corresponding to D2 of the original schema.
</bodyText>
<equation confidence="0.964516711111111">
rule(Tag, SVs, Iqs, Start, End,Edge_no) :- 1
add_to_type_word(Tag-SVs, Iqs, Iqs_out0), 2
ud(A-bot, Tag-SVs, Iqs_out0, Iqs_out1), 3
featval_synsem(Tag-SVs, FS2, Iqs_out1,
Iqs_out2), 4
. . . . .
featval_inv(FS5, FS6, Iqs_out5, Iqs_out6), 8
add_to_type_plus(FS6, Iqs_out6, Iqs_out7), 9
featval_subj(FS4, FS7,Iqs_out7, Iqs_out8),10
featval_hd(FS7, Tag2-SVs2, Iqs_out8,
Iqs_out9), 11
ud(B-bot, Tag2-SVs2, Iqs_out9,
Iqs_out10), 12
featval_tl(FS7,FS8,Iqs_out10, Iqs_out11), 13
add_to_type_e_list(FS8, Iqs_out11,
Iqs_out12), 14
featval_comps(FS4, Tag3-SVs3, Iqs_out12,
Iqs_out13), 15
ud(C-bot, Tag3-SVs3, Iqs_out13,
Iqs_out14), 16
. . . . .
ud(D-bot, E-bot, Iqs_out15, Iqs_out16), 18
ud(C-bot, F-bot, Iqs_out16, Iqs_out17), 19
solve(list_sign_to_synsem(E-bot,F-bot),[],
Iqs_out17, Iqs_out18), 20
edge(Edge_noB,End,EndB,TagB,SVsB,
IqsB,DaughtsB,RuleB), 21
. . . . .
deref(I, bot, Tag4, SVsList), 31
SVsList=..[Type |MemList], 32
match_list_rest(Type,MemList,EndB,
EndEdges,Edge_nos,[],Iqs_out27,
Iqs_out28), 33
. . . . .
solve(head_feature_principle(K-bot,L-bot),46
[semantics_principle(M-bot,N-bot), 47
marking_principle(O-bot,P-bot), 48
nonlocal_feature_principle(Q-bot,R-bot,
S-bot)], Iqs_out40,Iqs_out41), 49
add_to_type_phrase(Z-bot, Iqs_out41,
Iqs_out42), 50
. . . . .
add_edge_deref(Start,EndEdges,Z,bot,
Iqs_out52,[Edge_no,Edge_noB|Edge_nos],
head_subject_complement). 61
</equation>
<figureCaption confidence="0.933329">
Figure 2: The Head-subject-complement
Schema after Compilation by ALE
</figureCaption>
<bodyText confidence="0.6023815">
Goal 33 corresponding to D3 references the
remaining edges: their number equals the
</bodyText>
<page confidence="0.990824">
16
</page>
<bodyText confidence="0.991443012048193">
number in the comps list of the sub-constituent
unifying with the head daughter D1. Goal 61
creates the edge of the new phrase and
corresponds to the mother M. Goal 20, and
lines 46 to 49 forming one goal, are
invocations of ALE procedures, corresponding
to P1, and P3 to P6. Apart from the first, these
lines invoke HPSG principles.
ALE supports inequalities which HPSG
does not use, here in lists with names of the
form Iqs_*: the output from one goal is input
to the next, and edges contribute new lists for
concatenation. The following three goals
enforce constraints in the schema:
ud(Tag1-SVs1, Tag2-SVs2, Iqs_in, Iqs_out),
featval_FEAT(FS_in, FS, Iqs_in, Iqs_out),
add_to_type_TYPE(FS_in, Iqs_in, Iqs_out)
The first invokes a general purpose procedure
for full-scale unification of the TFSs in its first
two arguments. Often this just generates a co-
indexing variable for reference elsewhere: A in
goal 3 corresponds to HeadDtr in D1. A is an
unbound Tag and bot is the most general type
in the hierarchy: ud makes A reference Tag-
SVs. If SVs becomes subject to type coercion,
A references the new structure through Tag.
The second returns in FS the value of FEAT
from FS_in, type-coercing this when FEAT is
not appropriate. The add_to_TYPE goal
obtains the common sub-type of TYPE and the
type of FS_in, which is coerced to adopt this
sub-type: the procedure is precompiled from
the type hierarchy. Goals 4 to 9 use
featval_FEAT and add_to_type_TYPE to
enforce a (path: value) constraint in the first
line of D1. Goals 10 to 16 treat three other
constraints in D1: two of the values are co-
indexing variables. For conciseness, the figure
omits such goals after the first edge reference.
Goals 31 and 32 extract the list of synsem
structures CompDtrs, returned by the ALE
procedure list_sign_to_synsem: the list derives
from the value of the list CompSynsems.
Goal 50 coerces the initial type of Z-bot, the
new constituent, to phrase as required in M.
Then unshown goals constrain paths in this
TFS according to the (path: value) constraints
in M. Goal 61 invokes a procedure that asserts
a new edge containing this coerced TFS
referenced by Z, between positions Start, and
EndEdges from the last edge of goal 33. The
new edge contains a list of edge identifiers in
the sequence, and the schema name, from the
last two arguments.
The schema of Figure 2 is extended by our
precompiler, to generate the tuples in a closure
and details of co-indexing in order to guide
semantics treatment. The modified goals are
shown in Figure 3: clause modification is
easier than modifying the complex compiler
code in ALE, and the compiled schema already
invokes ALE procedures appropriately.
During the closure, procedures invoked by
goals 21 and 33 must constrain rule application
so each sequence of edges is treated just once
by each rule. Prolog backtracking cannot be
altered to achieve this, and the edge and
match_list_rest goals are modified. A list of
identifiers of edges already invoked is passed
between instances of these goals.
Detection of co-indexing requires access to
the TFS in each sub-constituent after constraint
application, and to the new TFS inside
add_edge_deref before edge assertion, when
co-indexing information is lost in copying.
Since each schema is applied without
backtracking to a single sequence of edges
each containing Tag-Bot, the list of sub-
constituent TFSs can be returned through the
head of the rule: the new TFS, Z-bot appears as
two arguments of the last goal.
rulejcb(Tag,SVs,Iqs,Start,End,Edge_no,
- [Tag-SVs,TagB-SVsB,MemList],
</bodyText>
<equation confidence="0.988397923076923">
- Z-bot, + Edge_countA,
- head_subject_complement, + Edges_in,
-Edges_outC):- 1
edgejcb(Edge_noB,StartB,EndB,TagB,SVsB,
IqsB,DaughtsB,RuleB,Edge_countA,
Edge_countB, Edges_in, Edges_outB), 21
match_list_restjcb(Type,MemList,EndB,
EndEdges,Edge_nos,[],Iqs_out27,
Iqs_out28,Edge_countB, Edge_countC,
Edges_outB, Edges_outC), 33
replace_add_edge_deref(Start, EndEdges,
Z, bot, Iqs_out52,
[Edge_no, Edge_noB  |Edge_nos],
</equation>
<bodyText confidence="0.512181">
head_subject_complement ) 61
</bodyText>
<figureCaption confidence="0.8834645">
Figure 3: The Head_subject_complement
Schema after Further Compilation
</figureCaption>
<page confidence="0.996539">
17
</page>
<bodyText confidence="0.99957424">
The extra argument 7 of rulejcb is a list of
sub-constituent TFSs after constraint
application. Z-bot is the new TFS.
Edge_countA is an initial count of 1 of the
edges encountered so far. Remaining
arguments are the rule name, and lists of edge
identifiers before and after rule application.
Goal 21 invokes a new clause that invokes
edge, and adds 1 to Edge_countA to form
Edge_countB. The edge number, Edge_noB, is
added to the head of Edges_in, to form
Edges_out. Goal 33 invokes a recursive clause
which similarly treats elements of MemList.
Goal 61 invokes a new procedure without
additional arguments, to assign T0 to the new
TFS, Z-bot, using the discrimination tree and
to assert a tuple (4). If T0 is new, a fully
dereferenced Z-bot is added to the retrieval
graph, and a new edge asserted with the
retrieved TFS. Another asserted clause
associates T0 with the edge number: similar
clauses were asserted in lexical edge creation.
They are referenced in tuple formation, from
edge numbers in argument 6.
For debugging, an asserted clause contains
the string deriving a tuple, structured into sub-
phrases using brackets. Only the first sub-
phrase deriving each category is used, to
restrict numbers. Even so, this permitted the
detection of over-generation arising from
ungrammatical strings. This arose from the
verbs is, can, be, seem, and the infinitival to
not specifying their subject beyond co-
indexing it with the subject of their
complement, which is variously another
(sometimes infinitival) verb or a predicate.
This allowed generation of infinite sequences
like “X is isÉ” where X is any phrase. It was
cured by hand-specifying each subject as np.
Complements were similarly treated for
believe and expect. An automatic approach
would propagate possible subjects, including
alternatives to np in a larger grammar, from
the complement into the outer verb.
To allow for large numbers of phrasal
edges, edges optionally have unbound SUs
arguments which are bound using the retrieval
graph when referenced With over-generation
cured, only 18,000 tuples were generated, and
the facility was unused.
</bodyText>
<sectionHeader confidence="0.981211" genericHeader="method">
4 The Closure Algorithm
</sectionHeader>
<bodyText confidence="0.997755857142857">
Using the TFS of each lexeme, an edge is
asserted with an identifier between Bottom and
Last_free_edge_number-1: Start and End are
unbound so every sequence of edges is
considered by rulejcb. Then the algorithm of
Figure 4 is executed. On each recursive
invocation, numbered in argument three,
repeat_apply makes a pass over these edges
and new edges generated in previous
invocations. The first unused identifier after a
pass is asserted in last_free_edge_number, and
when this is unchanged after a pass in which
no edges were asserted, termination occurs.
The first edge in a sequence is selected by
apply_schemas_to_a_first_edge: N is the
identifier which is incremented on each
recursion between Bottom and End. If
SUs in an edge is unbound, it is re-formed from
the retrieval graph by add_SVs_to_edge.
Selection of the remaining edges in a
sequence occurs non-deterministically within
the compiled grammar rules. These are
invoked by try_all_rules which references
rulenames, previously asserted to identify all
rules in a list. It invokes try_all_rules/4 to
recurse through the list, each time invoking
try_all_edges inside a negation, since a
failure-driven loop treats multiple edge
sequences by invoking the non-deterministic
rulejcb. The TFS of the first edge appears in
the first two arguments: its identifier N appears
alone and as the first in a list of edge identifiers
for the sequence, completed inside rulejcb.
To minimise compilation time, each
sequence must be treated once by each rule.
Sequences unpredictably contain 2 or 3 edges,
and a first edge E1 can combine with edges
created by sequences of edges treated after E1.
No edge can ever be discarded as a candidate
for any daughter in any rule. Sequences are too
numerous to record by asserting clauses to be
referenced before rule application.
The chosen solution is straightforward and
fairly efficient. On each pass of repeat_apply, a
range of acceptable edge identifiers is
established. Four global variables holding
identifiers, Bottom, Top, End and
Last_free_edge_number are asserted in clauses
bottom, top, end and last_free_edge_number
</bodyText>
<page confidence="0.997976">
18
</page>
<bodyText confidence="0.999989111111111">
respectively. The first three are adjusted in
repeat_apply, whilst the last is incremented in
replace_add_edge_deref on edge assertion.
On any pass, edges with N&gt;End are
asserted, and apply_schemas_to_a_first_edge
treats first edges between Bottom and End. At
the end of a pass, Top is set equal to End and
End is then set to Last_free_edge_number-1:
before the first pass Top is set to Bottom Ð 1.
</bodyText>
<equation confidence="0.90783752">
repeat_apply(Bottom,Last_free_edge_number,
Pass_no):-
end(End),
apply_schemas_to_a_first_edge(Bottom,End),
last_free_edge_number(Last_free_edge_no2),
End2 is Last_free_edge_number2 Ð 1,
retractall(end(j),assert(end(End2)),
Pass_no_out is Pass_no + 1,
(! , ( (not(Last_free_edge_number = =
Last_free_edge_number2))
-&gt;
(repeat_apply(Bottom,Last_free_edge_no2,
Pass_no_out),!)
;true
).
apply_schemas_to_a_first_edge(N,L):-
edge(N,Start,End,Tag, SVs, Iqs, Dtrs,
Rule_name),!,
(var(SVs) -&gt; add_SVs_to_edge(N,Tag,SVs)
; true),
try_all_rules(Tag,SVs,N),!,
New_N is N + 1,
(not(New_N &gt; L) -&gt;
(apply_schemas_to_a_first_edge(New_N,L))
; true).
</equation>
<bodyText confidence="0.963084090909091">
try_all_rules(Tag,SVs,N):-
rulenames(Rulenames),
try_all_rules(Tag, SVs,N,Rulenames).
try_all_rules(Tag,SVs,N,
[Rulename|Rulenames]):-
not(try_all_edges(Tag, SVs,N,Rulename)),
try_all_rules(Tag, SVs,N,Rulenames),!.
try_all_edges(Tag, SVs,N,Rulename):-
rulejcb(Tag, SVs, [], _ , _ , N, Daughters,
Mother,1,Rulename,[N],D2),
fail.
</bodyText>
<figureCaption confidence="0.983377">
Figure 4: Algorithm for Tuple Generation
</figureCaption>
<bodyText confidence="0.996676413043478">
Consequently, edges between Top+1 and End
were always created during the last pass:
initially this is the set of lexical edges.
In edgejcb and match_list_restjcb, an edge
identified by d2 or d3 depending on position in
a sequence, has its SVs unified with the rule
daughter if a following test succeeds. Test 1
succeeds if d1 was created on the previous pass
(or as a lexical edge, for pass 1), and d2 (and
d3 for a 3-daughter sequence), were created on
any pass (including lexemes) up to and
including the last. The d1 restriction prevents
treatment on multiple passes. If Test 1 fails,
Test 2 succeeds if d2 is newly created on the
previous pass, and d1, (and d3), were created
on any pass (including lexemes) up to and
including the last. If this fails, Test 3 is passed
if d1 and d2 were created on any pass up to but
not including the last pass, and if the same rule
successfully treated them earlier as the start of
a 3-edge sequence.
By delaying combination of a new edge into
a sequence, until the pass after its creation, we
avoid a repeat pass to catch the case where the
other edges do not yet exist at creation time.
Overall this necessitates &lt; 1 extra, low cost
pass. HPSG eliminates sequences mainly by
constraints between daughters, avoided on this
extra pass by trivial arithmetic comparisons in
O End − Top x End − Bottom whilst the
unavoidable O End −Top costs of first-
daughter unification are small
Test 3 treats a new d3, with old d1 and d2,
one of which must have been new on an earlier
pass, when it was treated under Test 1 or 2.
Since d3 is a potential complement of either d1
or d2, some sequence [d1, d2, d3&apos;] was treated
earlier, even if d3&apos; failed unification: at that
time treated_edges_before(Rulename, [d1,d2])
was asserted. Unification of d2 in [d1, d2, d3]
takes place (on repeated passes) only if this
asserted edge is found. Successful unifications
will be repeated, but the closure on our test
grammar has only 7 passes, and in
backtracking only one [d1, d2] unification
takes place for all [d1, d2, d3].
</bodyText>
<page confidence="0.998837">
19
</page>
<figureCaption confidence="0.999508">
Figure 5: Application of Head-Complement Schema to an Edge Sequence
</figureCaption>
<sectionHeader confidence="0.56928" genericHeader="method">
5 Treatment of Semantic Structures
</sectionHeader>
<bodyText confidence="0.998125229166667">
In a constituent, the co-indexing of paths in a
copy of the semantic sub-structure of a
semantic head was explained in (7) to (10).
Figure 5 illustrates this for the head-
complement schema.. In the verb (the semantic
head), the semantic and syntactic paths Pm
and Ps co-index at the variable Ind2, where :
Pm = SYNSEM: LOCAL: CONTENT:
NUCLEUS: LIKEE and
Ps = SYNSEM: LOCAL: CATEGORY:
COMPS: HD: LOCAL: CONTENT: INDEX
The rule co-indexes a pair of syntactic paths in
head and non-head daughters:
Ph = SYNSEM: LOCAL: CATEGORY:
COMPS: HD and
Pnh = SYNSEM
Since the concatenation (Ph : Psuffix) = Ps
where Psuffix = LOCAL: CONTENT: INDEX,
then the TFS reached by Pm in the head unifies
with that reached by (Pnh: Psuffix) in the other
sub-constituent. The index structure reached by
INDEX has a dual role in an np: as well as
appearing in the semantic structure of the
phrase, it also tests syntactic agreement of its
number, person and gender features via co-
indexing in the rule. It was therefore
considered in category allocation.
Analysis involves applying each grammar
rule to a sequence of Tag-bot structures. In
these, the Tag of each co-indexed node is
bound to a 3-digit integer xyy, where yy and x
are ordinals identifying the co-indexed node in
the rule, and the first daughter with a path to
that node. The head daughter is distinguished
since its path SYNSEM: LOCAL: CONTENT
co-indexes with that path in the mother. This
leads to &lt;Ph, Pnh&gt; pairs, each identified by an
integer Path_no, for each rule. For each pair in
each rule a clause is asserted:
rule_paths(Rulename, Path_no, _, Daught_no)
where Daught_no locates Pnh.
Each lexeme that a tuple (4) shows to be
unifiable with the head daughter of some rule,
is treated to detect 3-tuples of the form:
&lt; Indn, Path_no, Suffix_no &gt; according to the
mechanism illustrated in the example above:
Suffix_no identifies a single Psuffix path. A
skeleton semantic structure (8) is derived,
</bodyText>
<page confidence="0.97994">
20
</page>
<bodyText confidence="0.999769303030304">
where each pk argument corresponds to a 3-
tuple, ordered as Indn nodes are encountered in
a TFS traversal. Path_no fields appear in the
same order in an asserted clause
r_n(cat, Paths) where cat is Tl allocated to the
lexeme. During constituent construction in
CFG parsing, Daught_no of the sub-
constituent is known, and Rulename is deduced
from the tuple (4), so r_n and rule_paths
identify the pk argument in (8) to bind to the
edge identifier of the sub-constituent.
Retrieval graph arcs leading to nodes like
Ind2 are marked by asserted clauses:
arc_to_retrieval_tag2(+Current_arc_no,
+Category, _ , -Path_no, -Suffix_no).
For each possible &lt;Path_no, Suffix_no&gt; pair
generating (Pnh : Psuffix), this path is
speculatively followed in the retrieval graph
for each lexeme to identify a node and assert:
find_type_node_compiled(+Path_no,
+Category, +Suffix_no,
-Type_node_no, -Arc_no_in).
When retrieving the TFS0 of a CFG
constituent from the retrieval graph, once the
semantic path semantics from (3) is
encountered, the sem_ type0 of (8) is treated,
starting at the node reachable by semantics.
Where an arc number matches that in an
arc_to_retrieval_tag2 clause, Path_no and
Suffix_no are used to address
find_type_node_compiled. Category here
derives from sem_typepk from (10), being the
category of the semantic skeleton in a sub-
constituent identified by the appropriate pk in
(8): the ordering ensures that successive arcs
matching arc_to_retrieval_tag2 clauses
correspond correctly with consecutive pk
arguments. The arc in the TFS being
constructed is redirected to a copy of the
indicated node, and traversal of the retrieval
graph continues from that node. Arc_no_in is
used recursively with arc_to_retrieval_ttag2 to
detect if the target type-node should iself be
replaced by a node derived from a further sub-
constituent.
This technique also properly treats the
head-subject-complement, subject-head and
adjunct-head rules. In this last case the RESTR
set of identifiers for the noun and adjectives is
properly constructed. This set is not accessible
from the semantic TFS of the sentence since
the grammar (probably incorrectly) co-indexes
the Indn of the verb with the node reached by
INDEX in the np, rather than with that from
which the INDEX and RESTR arcs emerge.
The specifier-head rule uses a different form of
co-indexing which we have not yet treated:
appropriate semantic structures are still
returned for the sample grammar, where no
specifier has a more specific index structure
than any head np. The counter-example“a
sheep” which derives its number from the
specifier is not in the lexicon. Similarly, the
approximation that an arc in the head is
redirected to a node in another sub-constituent
avoids unification of index structures to
properly treat “É sheep eat(s)”: such low-cost
unification can easily be added to retrieval.
Our precompiled CFG is an exact
equivalent of the TFS grammar rather than an
approximation, since our restrictor does not
eliminate paths affecting agreement except for
slash: slashes are treated separately in our
parsing algorithm, as in CFG. Since the
schemas treated enforce agreement through a
syntactic path in the head, the semantics in the
head can be omitted by the restrictor. This also
eliminates the major source of TFS expansion
in a closure. The test grammar does not make
the daughter TFSs of a phrase accessible
except through its semantics, so these are also
eliminated from consideration.
The path to the index structure in an np is
co-indexed by the mechanism in Figure 5, and
by other schemas, some of which co-index it
with index in another sign combining with np
to produce np. Therefore it is re-introduced for
category allocation after semantics is excluded
(3). The RESTR component is still excluded: a
linguistic reason is that its value depends on
word morphology, whilst syntactic agreement
depends on more general features.
However, no automatic mechanism could
restrict the index treatment as we do. Verbs
like believe, seem, persuade, expect and
promise take a vp or an s as complement.
They could potentially specify agreement with
the semantic part of a vp of varying saturation.
In practice only the syntactic HEAD of the
</bodyText>
<page confidence="0.996118">
21
</page>
<bodyText confidence="0.999338">
complement is constrained, so these verbs
differ from verbs that take np as a subject. A
linguistic reason is that the semantic structure
of a verb depends on its word morphology as
in Figure 5, whilst in an np this dependency
applies only to RESTR and not to INDEX.
An automatic mechanism to generate our
restrictor would necessitate a closure from a
sample of the lexicon, since only when
syntactic agreement occurs can the need for
semantic agreement be assessed. The linguist
can predict such agreement by inspection, so a
better approach might be to automatically
generate diagrams like Figure 5 to guide in the
choice of restrictor. An over-drastic restrictor
becomes apparent only when a retrieved TFS
from CF parsing does not match the original
from TFS parsing. Automatic mechanisms for
comparison might be worth investigating.
Automatic mechanisms to derive our
treatment of slashes may be possible, since
they are associated with lists that grow during
parsing, not shrink as do subcat, subj or comps
lists. Our test grammar does not maintain
quantifier lists, but their behaviour is in many
ways similar to that of slashes.
We do not currently retrieve the semantic
structure of slashes. If each slash is paired with
the edge number of its lexical origin, and
details of slashes satisfying the filler-head
schema during parsing are indexed by edge
number, then the semantics can be retrieved
from the TFS corresponding to T1 in (6), when
the slash is encountered during retrieval.
</bodyText>
<sectionHeader confidence="0.999742" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999993111111111">
It has proved practical to precompile in an
acceptable time a realistic HPSG grammar into
exactly equivalent (neglecting semantics) CFG
categories and rules, of reasonable number and
compact size, together with a table to control
slash agreement. It was also possible to
generate data structures for building skeleton
semantic structure and retrieving its full
structure after parsing, obviating the need for
TFS unification. A Prolog prototype parses 18
times faster, and is estimated to be 180 times
faster in an optimum imperative code solution.
This predicted speed-up would exceed that
obtained with a CFG approximation, where
TFS unification must follow CFG parsing. The
kind of co-indexing used in the specifier-head
schema is not treated in semantic retrieval, but
the method seems extensible to embrace this.
</bodyText>
<sectionHeader confidence="0.997275" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.999702333333333">
This research was funded by a legacy from
Miss Nora Brown, and Workshop attendance
funded by INVU. Our thanks to the
anonymous referees and to Mr. Stephan Oepen
for their suggestions, and to Bernd Kiefer and
Kentaro Torisawa for copies of their papers.
</bodyText>
<sectionHeader confidence="0.999407" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999756162162162">
Brown, J.C. and Manandhar, S. (2000) Compilation
versus Abstract Machines for Fast Parsing of
Typed Feature Structure Grammars, Future
Generation Computer Systems 16, pp. 771-791.
Carpenter, B. and Penn, G. (1996) Compiling Typed
Attribute-value Logic Grammars, in “Recent
Advances in Parsing Technology”, H.Bunt, M.
Tomita, ed., Kluwer Academic Press, Dordrecht,
pp. 145-168.
Carroll, J.A. (1993) Practical Unification-based
Parsing of Natural Language, Technical Report
No. 314, University of Cambridge Computer
Laboratory.
Gazdar, G., Klein, E., Pullum, G., Sag, I. (1985)
Generalized Phrase Structure Grammar,
Blackwell, Oxford.
Kiefer, B. and Krieger, H.-U. (2000) A Context-
Free Approximation of Head-driven Phrase
Structure Grammar, in Proceedings of the 6th.
Int. Workshop on Parsing Technologies (IWPT),
Trento, Italy, pp. 135-146.
Matheson, C. (1996) Developing HPSG Grammars
in ALE, Course Notes, Human Communications
Research Centre, University of Edinburgh.
http://www.ltg.hcrc.ed.ac.uk/projects/ledtools/al
e-hpsg/index.html.
Pollard, C. and Sag, I.A. (1994) Head-Driven
Phrase Structure Grammar, University of
Chicago Press, Chicago.
Shieber, S.C. (1985) Using Restriction to Extend
Parsing Algorithms for Complex Feature Based
Formalisms, in Proceedings of the 23rd. Annual
Meeting of the Association for Computational
Linguistics, pp. 145-152.
Torisawa, K., Nishida, K., Miyao,Y., and Tsujii, J.-
I. (2000) An HPSG Parser with CFG Filtering,
Natural Language Engineering 6 (2), pp. 1-18.
</reference>
<page confidence="0.999027">
22
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.235334">
<title confidence="0.999845">Precompilation of HPSG in ALE into a CFG For Fast Parsing</title>
<author confidence="0.993588">C John</author>
<affiliation confidence="0.684593666666667">INVU Services Blisworth Hill Stoke</affiliation>
<address confidence="0.957672">Blisworth, UK, NN7</address>
<email confidence="0.994567">johnbrown@research.softnet.co.uk</email>
<author confidence="0.603767">Suresh</author>
<affiliation confidence="0.9997085">Department of Computer University of</affiliation>
<address confidence="0.997875">York, UK, YO1</address>
<email confidence="0.974419">Suresh@cs.york.ac.uk</email>
<abstract confidence="0.999345076923077">Context free grammars parse faster than TFS grammars, but have disadvantages. On our test TFS grammar, precompilation into CFG results in a speedup of 16 times for parsing without taking into account additional mechanisms for increasing parsing efficiency. A formal overview is given of precompilation and parsing. Modifications to ALE rules permit a closure over the rules from the lexicon, and analysis leading to a fast treatment of semantic structure. The closure algorithm, and retrieval of full semantic structure are described.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J C Brown</author>
<author>S Manandhar</author>
</authors>
<title>Compilation versus Abstract Machines for Fast Parsing of Typed Feature Structure Grammars,</title>
<date>2000</date>
<journal>Future Generation Computer Systems</journal>
<volume>16</volume>
<pages>771--791</pages>
<contexts>
<context position="2186" citStr="Brown and Manandhar (2000)" startWordPosition="320" endWordPosition="323">prototype precompiler and fast parser1 were built in Prolog, and tested with an HPSG grammar of English by Matheson (1996), 1 Downloadable code on http://www.cs.york.ac.uk/~johnb or http://www.soft.net.uk/research/hsppar.htm . written in the ALE formalism, by Carpenter and Penn (1996). This has the 6 schemas and 5 principles of HPSG, with 184 lexemes. A complex sentence,“kim can believe sandy can expect sandy to persuade kim to promise sandy to try to expect sandy to persuade kim to promise kim to give sandy a happy happy book”, parsed in 3.3s. with retrieval, 5.6 times faster than with TFSs, Brown and Manandhar (2000). An 11 word sentence was 18 times faster at 87ms., or 16 times counting retrieval. 1 Relationship to Work Elsewhere In precompilation, Torisawa et. al. (2000) repeatedly applied rules leading to maximal projections from lexical heads, allocating categories to mothers and non-head daughters. Our approach allocates categories in a closure over the rules starting with the lexicon, as in Kiefer and Krieger (2000). We differ from both these in that the CFG grammar equates to the TFS grammar, accepting exactly the same strings: a CFG parse tree translates into a TFS tree with no loss of nodes. We p</context>
</contexts>
<marker>Brown, Manandhar, 2000</marker>
<rawString>Brown, J.C. and Manandhar, S. (2000) Compilation versus Abstract Machines for Fast Parsing of Typed Feature Structure Grammars, Future Generation Computer Systems 16, pp. 771-791.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Carpenter</author>
<author>G Penn</author>
</authors>
<title>Compiling Typed Attribute-value Logic Grammars,</title>
<date>1996</date>
<booktitle>in “Recent Advances in Parsing Technology”,</booktitle>
<pages>145--168</pages>
<editor>H.Bunt, M. Tomita, ed.,</editor>
<publisher>Kluwer Academic Press,</publisher>
<location>Dordrecht,</location>
<contexts>
<context position="1845" citStr="Carpenter and Penn (1996)" startWordPosition="259" endWordPosition="262">e trees, and TFS unification over the remainder eliminates more, and instantiates path values. Our method treats slashes separately in a precompiled table, and careful allocation of categories to TFSs makes TFS unification unnecessary: instead skeleton semantic structures are formed in parsing, and full structures retrieved afterwards. A prototype precompiler and fast parser1 were built in Prolog, and tested with an HPSG grammar of English by Matheson (1996), 1 Downloadable code on http://www.cs.york.ac.uk/~johnb or http://www.soft.net.uk/research/hsppar.htm . written in the ALE formalism, by Carpenter and Penn (1996). This has the 6 schemas and 5 principles of HPSG, with 184 lexemes. A complex sentence,“kim can believe sandy can expect sandy to persuade kim to promise sandy to try to expect sandy to persuade kim to promise kim to give sandy a happy happy book”, parsed in 3.3s. with retrieval, 5.6 times faster than with TFSs, Brown and Manandhar (2000). An 11 word sentence was 18 times faster at 87ms., or 16 times counting retrieval. 1 Relationship to Work Elsewhere In precompilation, Torisawa et. al. (2000) repeatedly applied rules leading to maximal projections from lexical heads, allocating categories t</context>
</contexts>
<marker>Carpenter, Penn, 1996</marker>
<rawString>Carpenter, B. and Penn, G. (1996) Compiling Typed Attribute-value Logic Grammars, in “Recent Advances in Parsing Technology”, H.Bunt, M. Tomita, ed., Kluwer Academic Press, Dordrecht, pp. 145-168.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J A Carroll</author>
</authors>
<title>Practical Unification-based Parsing of Natural Language,</title>
<date>1993</date>
<tech>Technical Report No. 314,</tech>
<institution>University of Cambridge Computer Laboratory.</institution>
<contexts>
<context position="3189" citStr="Carroll (1993)" startWordPosition="490" endWordPosition="491">iefer and Krieger (2000). We differ from both these in that the CFG grammar equates to the TFS grammar, accepting exactly the same strings: a CFG parse tree translates into a TFS tree with no loss of nodes. We precompiled 550 CF categories and 18,000 rules in 1 hour on a 280MHz. Pentium II. This compares to 5,500 categories and 2,200,000 rules from 11,000 lexemes in 45 hours by Kiefer and Krieger on a 300MHz. Sun Ultrasparc 2, where sets of TFSs in the closure are replaced by common most-general unifiers. The common unifier technique was used to add a CFG back-bone to a unification grammar by Carroll (1993). An np has the same number, person and gender features irrespective of an optional specifier and multiple adjectives. A lexical vp, partially saturated vp and sentence contain decreasing subcategorisation data. Therefore numbers of phrasal and lexical categories are comparable, 13 The value following a feature in a TFS of a particular type is a sub-type of that given by an appropriateness function Approp:T × F → T where F and T denote finite sets of feature symbols and types. T is organised into a subsumption hierarchy with a single root bot. The grammar may either be lexicalist, or have a la</context>
</contexts>
<marker>Carroll, 1993</marker>
<rawString>Carroll, J.A. (1993) Practical Unification-based Parsing of Natural Language, Technical Report No. 314, University of Cambridge Computer Laboratory.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar,</title>
<date>1985</date>
<location>Blackwell, Oxford.</location>
<marker>Gazdar, Klein, Pullum, Sag, 1985</marker>
<rawString>Gazdar, G., Klein, E., Pullum, G., Sag, I. (1985) Generalized Phrase Structure Grammar, Blackwell, Oxford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Kiefer</author>
<author>H-U Krieger</author>
</authors>
<title>A ContextFree Approximation of Head-driven Phrase Structure Grammar,</title>
<date>2000</date>
<booktitle>in Proceedings of the 6th. Int. Workshop on Parsing Technologies (IWPT),</booktitle>
<pages>135--146</pages>
<location>Trento, Italy,</location>
<contexts>
<context position="2599" citStr="Kiefer and Krieger (2000)" startWordPosition="383" endWordPosition="386">o persuade kim to promise sandy to try to expect sandy to persuade kim to promise kim to give sandy a happy happy book”, parsed in 3.3s. with retrieval, 5.6 times faster than with TFSs, Brown and Manandhar (2000). An 11 word sentence was 18 times faster at 87ms., or 16 times counting retrieval. 1 Relationship to Work Elsewhere In precompilation, Torisawa et. al. (2000) repeatedly applied rules leading to maximal projections from lexical heads, allocating categories to mothers and non-head daughters. Our approach allocates categories in a closure over the rules starting with the lexicon, as in Kiefer and Krieger (2000). We differ from both these in that the CFG grammar equates to the TFS grammar, accepting exactly the same strings: a CFG parse tree translates into a TFS tree with no loss of nodes. We precompiled 550 CF categories and 18,000 rules in 1 hour on a 280MHz. Pentium II. This compares to 5,500 categories and 2,200,000 rules from 11,000 lexemes in 45 hours by Kiefer and Krieger on a 300MHz. Sun Ultrasparc 2, where sets of TFSs in the closure are replaced by common most-general unifiers. The common unifier technique was used to add a CFG back-bone to a unification grammar by Carroll (1993). An np ha</context>
</contexts>
<marker>Kiefer, Krieger, 2000</marker>
<rawString>Kiefer, B. and Krieger, H.-U. (2000) A ContextFree Approximation of Head-driven Phrase Structure Grammar, in Proceedings of the 6th. Int. Workshop on Parsing Technologies (IWPT), Trento, Italy, pp. 135-146.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Matheson</author>
</authors>
<title>Developing HPSG Grammars</title>
<date>1996</date>
<editor>in ALE, Course Notes,</editor>
<institution>Human Communications Research Centre, University of Edinburgh.</institution>
<note>http://www.ltg.hcrc.ed.ac.uk/projects/ledtools/al e-hpsg/index.html.</note>
<contexts>
<context position="1682" citStr="Matheson (1996)" startWordPosition="243" endWordPosition="244">antages. Fast parsing can be obtained by precompiling a CFG approximation, with TFSs converted into CF near-equivalents. CFG parsing eliminates impossible trees, and TFS unification over the remainder eliminates more, and instantiates path values. Our method treats slashes separately in a precompiled table, and careful allocation of categories to TFSs makes TFS unification unnecessary: instead skeleton semantic structures are formed in parsing, and full structures retrieved afterwards. A prototype precompiler and fast parser1 were built in Prolog, and tested with an HPSG grammar of English by Matheson (1996), 1 Downloadable code on http://www.cs.york.ac.uk/~johnb or http://www.soft.net.uk/research/hsppar.htm . written in the ALE formalism, by Carpenter and Penn (1996). This has the 6 schemas and 5 principles of HPSG, with 184 lexemes. A complex sentence,“kim can believe sandy can expect sandy to persuade kim to promise sandy to try to expect sandy to persuade kim to promise kim to give sandy a happy happy book”, parsed in 3.3s. with retrieval, 5.6 times faster than with TFSs, Brown and Manandhar (2000). An 11 word sentence was 18 times faster at 87ms., or 16 times counting retrieval. 1 Relationsh</context>
</contexts>
<marker>Matheson, 1996</marker>
<rawString>Matheson, C. (1996) Developing HPSG Grammars in ALE, Course Notes, Human Communications Research Centre, University of Edinburgh. http://www.ltg.hcrc.ed.ac.uk/projects/ledtools/al e-hpsg/index.html.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I A Sag</author>
</authors>
<date>1994</date>
<institution>Head-Driven Phrase Structure Grammar, University of Chicago Press,</institution>
<location>Chicago.</location>
<contexts>
<context position="913" citStr="Pollard and Sag (1994)" startWordPosition="130" endWordPosition="133">stract Context free grammars parse faster than TFS grammars, but have disadvantages. On our test TFS grammar, precompilation into CFG results in a speedup of 16 times for parsing without taking into account additional mechanisms for increasing parsing efficiency. A formal overview is given of precompilation and parsing. Modifications to ALE rules permit a closure over the rules from the lexicon, and analysis leading to a fast treatment of semantic structure. The closure algorithm, and retrieval of full semantic structure are described. Introduction Head Driven Phrase Structure Grammar (HPSG), Pollard and Sag (1994) is expressed in Typed Feature Structures (TFSs). Context Free Grammar (CFG) without features supports much faster parsing, but a TFS grammar has many advantages. Fast parsing can be obtained by precompiling a CFG approximation, with TFSs converted into CF near-equivalents. CFG parsing eliminates impossible trees, and TFS unification over the remainder eliminates more, and instantiates path values. Our method treats slashes separately in a precompiled table, and careful allocation of categories to TFSs makes TFS unification unnecessary: instead skeleton semantic structures are formed in parsin</context>
</contexts>
<marker>Pollard, Sag, 1994</marker>
<rawString>Pollard, C. and Sag, I.A. (1994) Head-Driven Phrase Structure Grammar, University of Chicago Press, Chicago.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S C Shieber</author>
</authors>
<title>Using Restriction to Extend Parsing Algorithms for Complex Feature Based Formalisms,</title>
<date>1985</date>
<booktitle>in Proceedings of the 23rd. Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>145--152</pages>
<contexts>
<context position="7346" citStr="Shieber (1985)" startWordPosition="1176" endWordPosition="1177">ltiple phrasal TFSs arising in long-range dependencies. Second, although semantic substructure (also containing word morphology) is omitted in category allocation, its index structure may be considered depending on the HEAD value. This makes CFG categories maximally selective. The now unnecessary TFS unification over the CFG parse tree is replaced by semantic TFS retrieval from skeleton structures in constituents. The method treats grammar rules where mother and daughters each comprise a TFS, expressed as conjoined constraints : TFS0→ TFS1 TFSn (1) where a TFS is given by: 14 of a restrictor, Shieber (1985), although here we shall show that it does not lead to any approximation in parsing. Precompilation generates multiple instantiations of each rule (1), paired with equivalent CFG rules, stored in a tuple: T0 — T1 Tn , rule − name (4) Each represents the pair-wise unification of some TFS1 TFSn with a sequence of TFSs where each is either lexical, like TFSl , or derives from some rule instantiation, like TFS0 . For each sequence T1 Tn of categories derivable from such instantiations, using the discrimination tree, only the first corresponding instantiation encountered is treated, since all gener</context>
</contexts>
<marker>Shieber, 1985</marker>
<rawString>Shieber, S.C. (1985) Using Restriction to Extend Parsing Algorithms for Complex Feature Based Formalisms, in Proceedings of the 23rd. Annual Meeting of the Association for Computational Linguistics, pp. 145-152.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Torisawa</author>
<author>K Nishida</author>
<author>Y Miyao</author>
<author>J-I Tsujii</author>
</authors>
<date>2000</date>
<journal>An HPSG Parser with CFG Filtering, Natural Language Engineering</journal>
<volume>6</volume>
<issue>2</issue>
<pages>1--18</pages>
<marker>Torisawa, Nishida, Miyao, Tsujii, 2000</marker>
<rawString>Torisawa, K., Nishida, K., Miyao,Y., and Tsujii, J.-I. (2000) An HPSG Parser with CFG Filtering, Natural Language Engineering 6 (2), pp. 1-18.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>