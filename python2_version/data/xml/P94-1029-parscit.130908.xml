<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.921906">
AN EXTENDED THEORY
OF HEAD-DRIVEN PARSING
</title>
<author confidence="0.992481">
Mark-Jan Nederhof *
</author>
<affiliation confidence="0.999709">
University of Nijmegen
Department of Computer Science
</affiliation>
<address confidence="0.6667235">
Toernooiveld, 6525 ED Nijmegen
The Netherlands
</address>
<email confidence="0.638072">
markj aacs kun .n1
</email>
<sectionHeader confidence="0.937628" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999954">
We show that more head-driven parsing algorithms can
be formulated than those occurring in the existing lit-
erature. These algorithms are inspired by a family of
left-to-right parsing algorithms from a recent publica-
tion. We further introduce a more advanced notion of
&amp;quot;head-driven parsing&amp;quot; which allows more detailed spec-
ification of the processing order of non-head elements
in the right-hand side. We develop a parsing algorithm
for this strategy, based on LR parsing techniques.
</bodyText>
<sectionHeader confidence="0.959587" genericHeader="introduction">
Introduction
</sectionHeader>
<bodyText confidence="0.999187214285714">
According to the head-driven paradigm, parsing of a
formal language is started from the elements within the
input string that are most contentful either from a syn-
tactic or, more generally, from an information theoretic
point of view. This results in the weakening of the
left-to-right feature of most traditional parsing meth-
ods. Following a pervasive trend in modern theories of
Grammar (consider for instance [5, 3, 11]) the compu-
tational linguistics community has paid large attention
to the head-driven paradigm by investigating its appli-
cations to context-free language parsing.
Several methods have been proposed so far exploit-
ing some nondeterministic head-driven strategy for
context-free language parsing (see among others [6, 13,
2, 14]). All these proposals can be seen as general-
izations to the head-driven case of parsing prescrip-
tions originally conceived for the left-to-right case. The
methods above suffer from deficiencies that are also no-
ticeable in the left-to-right case. In fact, when more
rules in the grammar share the same head element, or
share some infix of their right-hand side including the
head, the recognizer nondeterministically guesses a rule
just after having seen the head. In this way analyses
that could have been shared are duplicated in the pars-
ing process.
Interesting techniques have been proposed in the left-
to-right deterministic parsing literature to overcome re-
dundancy problems of the above kind, thus reducing
</bodyText>
<note confidence="0.5527665">
*Supported by the Dutch Organisation for Scientific Re-
search (NWO), under grant 00-62-518
</note>
<author confidence="0.405332">
Giorgio Satta
</author>
<bodyText confidence="0.92839715">
Universita di Padova
Dipartimento di Elettronica e Informatica
via Gradenigo 6/A, 35131 Padova
Italy
sattadei.unipd.it
the degree of nondeterminism of the resulting methods.
These solutions range from predictive LR parsing to LR
parsing [15, 1]. On the basis of work in [8] for nonde-
terministic left-to-right parsing, we trace here a theory
of head-driven parsing going from crude top-down and
head-corner to more sophisticated solutions, in the at-
tempt to successively make more deterministic the be-
haviour of head-driven methods.
Finally, we propose an original generalization of head-
driven parsing, allowing a more detailed specification of
the order in which elements of a right-hand side are to
be processed. We study in detail a solution to such
a head-driven strategy based on LR parsing. Other
methods presented in this paper could be extended as
well.
</bodyText>
<subsectionHeader confidence="0.711146">
Preliminaries
</subsectionHeader>
<bodyText confidence="0.987905346153846">
The notation used in the sequel is for the most part
standard and is summarised below.
Let D be an alphabet (a finite set of symbols); D+
denotes the set of all (finite) non-empty strings over D
and D* denotes D+ U {e}, where e denotes the empty
string. Let R be a binary relation; R+ denotes the
transitive closure of R and R* denotes the reflexive and
transitive closure of R.
A context-free grammar G = (N, T, P, S) consists of
two finite disjoint sets N and T of nonterminal and
terminal symbols, respectively, a start symbol S E N,
and a finite set of rules P. Every rule has the form
A a, where the left-hand side (lhs) A is an element
from N and the right-hand side (rhs) a is an element
from V+, where V denotes (N U T). (Note that we
do not allow rules with empty right-hand sides. This
is for the sake of presentational simplicity.) We use
symbols A, B,C, . .. to range over N, symbols X, Y, Z
to range over V, symbols a, fl, 7, . . . to range over V*,
and v, w, x,... to range over T*.
In the context-free grammars that we will consider,
called head grammars, exactly one member from each
rhs is distinguished as the head. We indicate the head
by underlining it, e.g., we write A —■ a2C3. An expres-
sion A ay [3 denotes a rule in which the head is some
member wain -y. We define a binary relation 0 such
</bodyText>
<page confidence="0.996648">
210
</page>
<bodyText confidence="0.99673175">
that B 0 A if and only if A --0 a 13fi for some a and 0.
Relation 0* is called the head-corner relation.
For technical reasons we sometimes need the aug-
mented set of rules Pt, consisting of all rules in P plus
the extra rule S&apos; 1S, where S&apos; is a fresh nontermi-
nal, and 1 is a fresh terminal acting as an imaginary
zeroth input symbol. The relation Pt is extended to a
relation on V* x V* as usual. We write 7 b when-
</bodyText>
<equation confidence="0.711156333333333">
ever 7 b holds as an extension of p E Pt. We write
p1p2-19. .r pl c P2 c
7 0 11 7 —+02 • • • 458-1P-4
</equation>
<bodyText confidence="0.985049454545455">
For a fixed grammar, a head-driven recognition algo-
rithm can be specified by means of a stack automa-
ton A = (T, Alph, Fin(n)), parameterised
with the length n of the input. In A, symbols T
and Alph are the input and stack alphabets respec-
tively, Init(n), Fin(n) E Alph are two distinguished
stack symbols and is the transition relation, defined
on Alph+ x Alph+ and implicitly parameterised with the
input.
Such an automaton manipulates stacks r E Alph+,
(constructed from left to right) while consulting the
symbols in the given input string. The initial stack
is Init(n). Whenever F F&apos; holds, one step of the
automaton may, under some conditions on the input,
transform a stack of the form r&apos;r into the stack ry.
In words, r r&apos; denotes that if the top-most few sym-
bols on the stack are F then these may be replaced by
the symbols r. Finally, the input is accepted whenever
the automaton reaches stack Fin(n). Stack automata
presented in what follows act as recognizers. Parsing
algorithms can directly be obtained by pairing these
automata with an output effect.
</bodyText>
<subsectionHeader confidence="0.841976">
A family of head-driven algorithms
</subsectionHeader>
<bodyText confidence="0.99989075">
This section investigates the adaptation of a family of
left-to-right parsing algorithms from [8], viz, top-down,
left-corner, PLR, ELR, and LR parsing, to head gram-
mars.
</bodyText>
<subsectionHeader confidence="0.95168">
Top-down parsing
</subsectionHeader>
<bodyText confidence="0.8422992">
The following is a straightforward adaptation of top-
down (TD) parsing [1] to head grammars.
There are two kinds of stack symbol (items), one of
the form [i, A, j], which indicates that some subderiva-
tion from A is needed deriving a substring of ai+i
the other of the form [i, k, A —0 a • 7 • (3,m, j], which
also indicates that some subderivation from A is needed
deriving a substring of ai+i a,, but specifically using
the rule A --p a7fl, where 7 ak+1 ...am has already
been established. Formally, we have
</bodyText>
<equation confidence="0.963154133333333">
{[i, A, j] &lt;i}
JrD = {[i,k, A —+ a • 7 • )3,m, j] I A --0 a7# E A
i &lt; k &lt; m &lt;j}
Algorithm 1 (Head-driven top-down)
ATV = (T, J1TD U J2TD, Init(n), 1-0, Fin(n)), where
Init(n) = [-1, —1, S&apos; --0 • I. • S, 0 , n],
Fin(n) = [-1, —1, S&apos; --0 • IS n, n], and the transition
relation is given by the following clauses.
0 [i, A, j] 1-, [i, A, j][i, B, j]
where there is A E Pt
Oa [i, k, A a • -y • B 3, m, j]
[i, k, A —0 a • -y • B , m, j][m, B, j]
Ob [i, k , A B • -y • g,m,
[i, k, A —0 aB • 7 • fl,m, A[i, B, k]
1 [i, A, j]1-0 [i, k — 1, A a • a•fi,k, j]
</equation>
<bodyText confidence="0.970569">
where there are A -- aa/3 E Pt and k such that
i &lt; k &lt;j and ak = a
</bodyText>
<equation confidence="0.876377">
2a [i, k, A --, a • -y • afl, m, j]
[i, k , A —0 a • -ya • m 1,j]
provided m &lt;j and am+j = a
2b Symmetric to 2a (cf. Oa and Ob)
3 [i, A, j][ii, k, B • b • , 1---0
[i,k,A--■ a • B • /3,m, j]
where there is A —■ a/13 E Pt
(i = i&apos; and j = j&apos; are automatically satisfied)
4a [i,k, A —0 a • 7 • BO, m, j][i&apos; ,k&apos; , B *So, m&apos;, ji]
k, A a 7B • g, m&apos;, j]
provided m = k&apos;
(m = i&apos; and j = j&apos; are automatically satisfied)
4b Symmetric to 4a
</equation>
<bodyText confidence="0.999964846153846">
We call a grammar head-recursive if A 0+ A for some
A. Head-driven TD parsing may loop exactly for the
grammars which are head-recursive. Head recursion is a
generalization of left recursion for traditional TD pars-
ing.
In the case of grammars with some parameter mech-
anism, top-down parsing has the advantage over other
kinds of parsing that top-down propagation of parame-
ter values is possible in collaboration with context-free
parsing (cf. the standard evaluation of definite clause
grammars), which may lead to more efficient process-
ing. This holds for left-to-right parsing as well as for
head-driven parsing [10].
</bodyText>
<subsectionHeader confidence="0.980002">
Head-corner parsing
</subsectionHeader>
<bodyText confidence="0.9991868">
The predictive steps from Algorithm 1, represented by
Clause 0 and supported by Clauses Oa and Ob, can be
compiled into the head-corner relation 0*. This gives
the head-corner (HC) algorithm below. The items from
ip are no longer needed now. We define //lc =
</bodyText>
<equation confidence="0.94893375">
Algorithm 2 (head-corner)
= (T, Pic , Fin(n)), where
Init(n) = [-1, —1, S&apos; —+ • 1. • S, 0 , n],
Fin(n) = [-1, —1, S&apos; —0 • IS *, n, n], and 1—, is given
</equation>
<bodyText confidence="0.880971142857143">
by the following clauses. (Clauses lb, 2b, 3b, 4b are
omitted, since these are symmetric to la, 2a, 3a, 4a,
respectively.)
la [i,k,A--0 a •-y • Bfl,m,j]1-0
[i, k, A --. a • 7 • 1313,m, j][m,p-1,C q•a•O, p, j]
where there are C na0 E Pt and p such that m &lt;
p &lt; j and ap =a and C 0* B
</bodyText>
<page confidence="0.929437">
211
</page>
<equation confidence="0.889175583333333">
2a [i, k, A a • -y • afl,m, )-+
[i, k , A a • 7a • 13,m+ 1,j]
provided m &lt; j and am+1 = a
3a [i, k , D cro7.A)3,m, j][i&apos; , k&apos; , B .6. , m&apos; , j&apos;]
[i, k, D av7.A)3,rn, j][i&apos; ,k&apos; ,C , f]
provided m = i&apos;, where there is C --■ r1B9 E Pt such
that C 0* A
(j = j&apos; is automatically satisfied)
4a [i, k, A a • 7 • B m, j][i&apos; , k&apos; , B , 1-4
[i,k, A a .7B • )3, ne,
provided m =
(m = i&apos; and j = f are automatically satisfied)
</equation>
<bodyText confidence="0.9999265">
Head-corner parsing as well as all algorithms in the
remainder of this paper may loop exactly for the gram-
mars which are cyclic (where A -,+ A for some A).
The head-corner algorithm above is the only one in
this paper which has already appeared in the literature,
in different guises [6, 13, 2, 14].
</bodyText>
<subsectionHeader confidence="0.935072">
Predictive HI parsing
</subsectionHeader>
<bodyText confidence="0.999867363636364">
We say two rules A al and B a2 have a common
infix a if al = /Awn and a2 = )32a72, for some /32,
-yi and 72. The notion of common infix is an adaptation
of the notion of common prefix [8] to head grammars.
If a grammar contains many common infixes, then
HC parsing may be very nondeterministic; in particular,
Clauses 1 or 3 may be applied with different rules C
?la E Pt or C n138 E PI for fixed a or B.
In [15] an idea is described that allows reduction of
nondeterminism in case of common prefixes and left-
corner parsing. The resulting algorithm is called pre-
dictive LR (PLR) parsing. The following is an adapta-
tion of this idea to HC parsing. The resulting algorithm
is called predictive HI (PHI) parsing. (HI parsing, to
be discussed later, is a generalization of LR parsing to
head grammars.)
First, we need a different kind of item, viz. of the
form [i, k , A -&gt; 7,m, where there is some rule A -4
a7)3. With such an item, we simulate computation of
different items [i, k, A -- a • 7 • f3,m,j] E pic for
different a and )3, which would be treated individually
by an HC parser. Formally, we have
</bodyText>
<equation confidence="0.773369888888889">
/PHI {[i, k , A 7,m, j] I A a7 E Pt A
&lt; k &lt; m &lt; j}
Algorithm 3 _(Predictive HI)
APH/ = (T, , Fin(n)), where
Init(n) = [-1, -1, S&apos; J_, 0, n],
Fin(n) = [-1, -1, S&apos; IS, n, n], and 1-4 is given by
the following (symmetric &amp;quot;b-clauses&amp;quot; omitted).
la [i, k, A --, 7 , rn,
[i, k, A 7,m, j][m,p - 1,C a, p,
</equation>
<bodyText confidence="0.999374">
where there are C na0, A -&gt; a7 .13 E Pt and p
such that in &lt;p &lt;j and ap = a an-a C 0* B
</bodyText>
<equation confidence="0.587910777777778">
2a [i, k, A -&gt; 7,m, j] [i, k, A ---&gt; -ya, in + 1,j]
provided m &lt;j and am+1 = a, where there is A -■
a7ai3 E Pt
3a [i, k , D 7 , m, j][i&apos; , , B --&gt; , m&apos; ,
[i, k, D 7,m, j][i&apos; ,k&apos; , C B , m&apos; , j1]
provided m = i&apos; and B -&gt; E Pt, where there are
D a&apos; A/3, C -&gt;ijO E Pt such that C 0* A
4a [i,k, A 7,m, j][i&apos; , , B 6,m&apos; ,j&apos;]
[i, k, A -k 7B, in&apos;,
</equation>
<bodyText confidence="0.912183">
provided m = k&apos; and B --+ 6 E Pt, where there is
A -■ a7B,3 E Pt
</bodyText>
<subsectionHeader confidence="0.954117">
Extended HI parsing
</subsectionHeader>
<bodyText confidence="0.9999395">
The PHI algorithm can process simultaneously a com-
mon infix a in two different rules A -&gt; /31a71 and
A --, 132a1y2, which reduces nondeterminism.
We may however also specify an algorithm which suc-
ceeds in simultaneously processing all common infixes,
irrespective of whether the left-hand sides of the cor-
responding rules are the same. This algorithm is in-
spired by extended LR ( EL R) parsing [12, 7] for ex-
tended context-free grammars (where right-hand sides
consist of regular expressions over V). By analogy, it
will be called extended HI (EHI) parsing.
This algorithm uses yet another kind of item, viz.
of the form [i, k, {Ai, A2, .,A,,} -&gt; 7,m, j], where
there exists at least one rule A a7)3 for each
A E {Ai, A2, .,A}. With such an item, we simu-
late computation of different items [i, k, A -■ a • 7 •
)3,m, E Plc which would be treated individually by
an HC parser. Formally, we have
</bodyText>
<equation confidence="0.925983111111111">
{[i, --&apos; mci]
0CAC{AIA--a-y0EPt} A
i &lt; k &lt; m &lt; j}
Algorithm 4 _(Extended HI)
A = (T, 1E1&apos;1 , Fin(n)), where
Init(n) = [-1,-i, {S&apos;} -&gt; 1,0,n],
Fin(n) = [-1 , -1 , {S&apos;} IS, n, n], and I, is given by:
la [i, k , A 7,m, j] 1-,
[i, k , A 7,m, j][m, p - 1,A&apos; a, p, j]
</equation>
<bodyText confidence="0.974473666666667">
where there is p such that in &lt; p &lt; j and ap = a
and A&apos; = {C I 3C ria0, A -,- a-yB E Pt (A E
A A C 0* B)} is not empty
</bodyText>
<equation confidence="0.7791675">
2a [i, k, A -y, m, j] [i, k, -4 7a, m + 1, j]
provided m &lt; j and am+1 = a and A&apos; = E
A I A a7a3 E Pt} is not empty
3a [i, k, A -4 -y, rn, j][i&apos;, , A&apos; -4 6, m&apos; , j&apos;i
[i, k, A -y, m, j][i&apos; , , A&amp;quot; B, rni, ji]
provided m = i&apos; and B E Pt for some B E A&apos;
such that A&amp;quot; = {C I 3C n BO , D ay A3 E
Pt (D E A AC 0* A)} is not empty
4a [i, k, A -y, m, j] [i&apos; , k&apos;, A&apos; 6, in&apos;, j&apos;] I-+
-yB,m&apos; , j]
</equation>
<bodyText confidence="0.932792666666667">
provided m = k&apos; and B 6 E Pt for some B E A&apos;
such that A&amp;quot; = {A EA IA a7B/3 E Pt} is not
empty
</bodyText>
<page confidence="0.991444">
212
</page>
<bodyText confidence="0.992889">
This algorithm can be simplified by omitting the
sets from the items. This results in common infix
(CI) parsing, which is a generalization of common pre-
fix parsing [8]. Cl parsing does not satisfy the correct
subsequence property, to be discussed later. For space
reasons, we omit further discussion of CI parsing.
</bodyText>
<subsectionHeader confidence="0.494741">
HI parsing
</subsectionHeader>
<bodyText confidence="0.999937833333333">
If we translate the difference between ELR and LR pars-
ing [8] to head-driven parsing, we are led to HI parsing,
starting from EHI parsing, as described below. The al-
gorithm is called HI because it computes head-inward
derivations in reverse, in the same way as LR parsing
computes rightmost derivations in reverse [1]. Head-
inward derivations will be discussed later in this paper.
HI parsing uses items of the form [i, k, Q, in, jj, where
Q is a non-empty set of &amp;quot;double-dotted&amp;quot; rules A —0 a •
-y • 3. The fundamental difference with the items in
/EH/ is that the infix -y in the right-hand sides does not
have to be fixed. Formally, we have
</bodyText>
<equation confidence="0.916139333333333">
= k C , in,
0CQC{A—■a•7•/31A—■ a-y,3 E PI} A
i &lt; k &lt; rn &lt;j}
</equation>
<bodyText confidence="0.999279916666667">
We explain the difference in behaviour of HI parsing
with regard to EHI parsing by investigating Clauses la
and 2a of Algorithm 4. (Clauses 3a and 4a would give
rise to a similar discussion.) Clauses la and 2a both ad-
dress some terminal ap, with in &lt;p &lt; j. In Clause la,
the case is treated that ap is the head (which is not
necessarily the leftmost member) of a rhs which the al-
gorithm sets out to recognize; in Clause 2a, the case is
treated that ap is the next member of a rhs of which
some members have already been recognized, in which
case we must of course have p = m + 1.
By using the items from PH we may do both kinds
of action simultaneously, provided p = m + 1 and ap is
the leftmost member of some rhs of some rule, where
it occurs as head.&apos; The Ihs of such a rule should sat-
isfy a requirement which is more specific than the usual
requirement with regard to the head-corner relation,2
We define the left head-corner relation (and the right
head-corner relation, by symmetry) as a subrelation of
the head-corner relation as follows.
We define: B L A if and only if A --4 Ba for some
a. The relation L* now is called the left head-corner
relation.
We define
</bodyText>
<equation confidence="0.99858175">
gotorighti(Q, X) =
{C--471•X•OIC-4t/X0EPI A
3.A —4 a • -y • B E Q(C (&gt;* B)}
gotoright2(Q , X) =
</equation>
<footnote confidence="0.7560445">
11f ap is not the leftmost member, then no successful
parse will be found, due to the absence of rules with empty
right-hand sides (epsilon rules).
2Again, the absence of epsilon rules is of importance here.
</footnote>
<equation confidence="0.695941666666667">
{C-0•X•0IC-0 XOEPt A
3A —0 a • -y • 13,3 E Q(C L* B)} U
{A a • -yX • # LA---0a•-y• XflEQ}
</equation>
<bodyText confidence="0.9453009">
and assume symmetric definitions for gotolef ti and
gotoleft2.
The above discussion gives rise to the new Clauses la
and 2a of the algorithm below. The other clauses are
derived analogously from the corresponding clauses of
Algorithm 4. Note that in Clauses 2a and 4a the new
item does not replace the existing item, but is pushed
on top of it; this requires extra items to be popped off
the stack in Clauses 3a and 4a.3
Algorithm 5 (HI)
</bodyText>
<equation confidence="0.95457975">
Am = (T, , Init(n), , Fin(n)), where
Init(n) = [-1, —1, {S&apos; —0 • 1 • S} , 0 , n],
Fin(n) = [-1,—i, {S&apos; —4 • IS •}, n, n], and defined:
la [i, k, C21 M1 [i)k)Q) rn,i][rn,P --- Qi 7 1:1)
</equation>
<bodyText confidence="0.9996465">
where there is p such that m + 1 &lt;p &lt; j and ap = a
and Q&apos; = gotorighti(Q, a) is not empty
</bodyText>
<equation confidence="0.972143777777778">
2a [i, k , Q , m, j] 0-0 [i, k,Q ,m, j][i, k , , m j]
provided in &lt; j and ani+i = a and Q&apos; =
gotoright2(Q , a) is not empty
3a • • . , k` , Qi, mi ,
k, Q&amp;quot; rai
provided m &lt; k&apos;, where there is B —0 • X1 • • • Xr •
E Q&apos; such that Q&amp;quot; = gotorighti(Q, B) is not empty
4a [i, k, Q, 7n, h • . . , k&apos; ,Q&apos;, .21]
[2, k, Q, m , j][i , k , Q&amp;quot;, in&apos;, jj
</equation>
<bodyText confidence="0.9725395625">
provided in = k&apos; or k = k&apos;, where there is B —0 •
Xi . Xr • E Q&apos; such that Q&amp;quot; = gotoright2(Q , B) is
not empty
We feel that this algorithm has only limited advan-
tages over the EHI algorithm for other than degenerate
head grammars, in which the heads occur either mostly
leftmost or mostly rightmost in right-hand sides. In
particular, if there are few sequences of rules of the form
A —4 Alai, A1 A2 a2, Amam, or of
the form A aiAi , Ai a2A2, • • • , Am— arnArn,
then the left and right head-corner relations are very
sparse and HI parsing virtually simplifies to EHI pars-
ing.
In the following we discuss a variant of head gram-
mars which may provide more opportunities to use the
advantages of the LR technique.
</bodyText>
<subsectionHeader confidence="0.772617">
A generalization of head grammars
</subsectionHeader>
<bodyText confidence="0.9997815">
The essence of head-driven parsing is that there is a
distinguished member in each rhs which is recognized
first. Subsequently, the other members to the right and
to the left of the head may be recognized.
An artifact of most head-driven parsing algorithms is
that the members to the left of the head are recognized
</bodyText>
<footnote confidence="0.428328">
3/1 represent a number of items, as many as there
are members in the rule recognized, minus one.
</footnote>
<page confidence="0.998775">
213
</page>
<bodyText confidence="0.995659309523809">
strictly from right to left, and vice versa for the mem-
bers to the right of the head (although recognition of
the members in the left part and in the right part may
be interleaved). This restriction does not seem to be
justified, except by some practical considerations, and
it prevents truly non-directional parsing.
We propose a generalization of head grammars in
such a way that each of the two parts of a rhs on both
sides of the head again have a head. The same holds
recursively for the smaller parts of the rhs. The con-
sequence is that a rhs can be seen as a binary tree, in
which each node is labelled by a grammar symbol. The
root of the tree represents the main head. The left son
of the root represents the head of the part of the rhs to
the left of the main head, etc.
We denote binary trees using a linear notation. For
example, if a and /3 are binary trees, then (a)X (f3)
denotes the binary tree consisting of a root labelled X,
a left subtree a and a right subtree f3. The notation of
empty (sub)trees (e) may be omitted. The relation —■*
ignores the head information as usual.
Regarding the procedural aspects of grammars, gen-
eralized head grammars have no more power than tra-
ditional head grammars. This fact is demonstrated by
a transformation &apos;ilead from the former to the latter
class of grammars. A transformed grammar rhead(G)
contains special nonterminals of the form [a], where a
is a proper subtree of some rhs in the original gram-
mar G = (T, N, P, S). The rules of the transformed
grammar are given by:
A —. [a] X [0] for each A —&gt; (a)X ( f3) E P
[(a)X (i3)] [a] .7j [g] for each proper subtree
(a)X(fi) of a rhs in G
where we assume that each member of the form [c] in
the transformed grammar is omitted.
It is interesting to note that Thead is a generalization
of a transformation nwo which can be used to transform
a context-free grammar into two normal form (each rhs
contains one or two symbols). A transformed grammar
Ttwo(G) contains special nonterminals of the form [a],
where a is a proper suffix of a rhs in G. The rules of
r0(G) are given by
</bodyText>
<equation confidence="0.9939015">
A —4 X [a] for each A Xa E P
[X a] X [a] for each proper suffix X a of a rhs in G
</equation>
<bodyText confidence="0.9999955">
where we assume that each member of the form [c] in
the transformed grammar is omitted.
</bodyText>
<subsectionHeader confidence="0.591258">
HI parsing revisited
</subsectionHeader>
<bodyText confidence="0.998851833333333">
Our next step is to show that generalized head gram-
mars can be effectively handled with a generalization
of HI parsing (generalized HI (GHI) parsing). This
new algorithm exhibits a superficial similarity to the
2-dimensional LR parsing algorithm from [16]. For a
set Q of trees and rules,&apos; closure(Q) is defined to be
</bodyText>
<footnote confidence="0.958424666666667">
4It is interesting to compare the relation between trees
and rules with the one between kernel and nonkernel items
of LR parsing [1].
</footnote>
<equation confidence="0.82371425">
the smallest set which satisfies
closure(Q) 2 Q U
{A —4 (a)X(f3) E P (7)A(5) E closure(Q) V
B (7)A(b) E closure(Q)}
</equation>
<bodyText confidence="0.845596">
The trees or rules of which the main head is some
specified symbol X can be selected from a set Q by
</bodyText>
<equation confidence="0.778321">
goto(Q, X) = It EQIt= (a)X (#) V t = A (a)X (/3)}
</equation>
<bodyText confidence="0.9668235">
In a similar way, we can select trees and rules according
to a left or right subtree.
</bodyText>
<equation confidence="0.998239">
gotoleft(Q , a) = It EQIt= (a)X(#) V
t = A (a)X(13)}
</equation>
<bodyText confidence="0.999478333333333">
We assume a symmetric definition for gotoright.
When we set out to recognize the left subtrees from
a set of trees and rules, we use the following function.
</bodyText>
<equation confidence="0.746749">
left(Q) = closurega (a),(03) E V
A (a)X (13) E Q})
</equation>
<bodyText confidence="0.99995">
We assume a symmetric definition for right.
The set /Gm contains different kinds of item:
</bodyText>
<listItem confidence="0.820038">
• Items of the form [i., k,Q,m, j], with i &lt; k &lt; In &lt;j,
</listItem>
<bodyText confidence="0.9911805">
indicate that trees (a)X (0) and rules A (a)X(j3)
in Q are needed deriving a substring of ai+i aj,
where X —.* ak+i ... am has already been estab-
lished.
</bodyText>
<listItem confidence="0.864626">
• Items of the form [k,Q,m, j], with k &lt; m &lt;j, indi-
</listItem>
<bodyText confidence="0.849234833333333">
cate that trees (a)X(/3) and rules A (a)X (0) in Q
a X -- ** m • a has already been established.
where
are needed deriving a substring of ak+i ai ,
Items of the form [i, k, Q, in] have a symmetric mean-
ing.
</bodyText>
<listItem confidence="0.999321666666667">
• Items of the form [k, i, in], with k &lt; in, indicate that
--4* ak+i ... am has been established for tree t
or rule t = A —.
</listItem>
<equation confidence="0.935662076923077">
Al orithm 6 (Generalized HI parsing)
A&amp;quot; = (T, IGHI , Init(n),,—*, Fin(n)), where
Init(n) = [-1, {S&apos; 1.(S)} , 0, n],
Fin(n) = [-1,S&apos; 1(S), n], and defined:
la [i, k, Q, m, [i, k,Q&apos; , in]
provided Q&apos; = gotoright(Q , c) is not empty
lb [i, k,Q, m, j] [k , Q&apos; , m, j]
provided Q&apos; = gotoleft(Q, c) is not empty
lc [k,Q,m, j] [k,t,m]
provided t E gotoright(Q , c)
id [i, k,Q,m] [k, t, m]
provided t E gotoleft(Q, c)
2a [i, k,Q, m, j] [i, k,Q, m, j][m, p — 1, Q&apos; , p, j]
</equation>
<bodyText confidence="0.9720565">
where there is p such that in &lt; p &lt; j and Q&apos; =
goto(right(Q), ap) is not empty
</bodyText>
<footnote confidence="0.642122">
2b [i, k,Q, m, j] [i, k , Q , m, j][i, p — 1, Q&apos; p, k]
</footnote>
<bodyText confidence="0.7440845">
where there is p such that i &lt; p &lt; k and Q&apos; =
goio(left(Q), ap) is not empty
</bodyText>
<page confidence="0.994657">
214
</page>
<table confidence="0.9987355">
Stack Clause
[-I, {S&apos; -■ _L(S)}, 0, 41 3a
-1, {S&apos; -. 1(5)}, 0,4] [0,3, {S ---+ ((c)A(b))s, S --4 (A(d))s, S --+ (B)s} , 4,4] la
-1, {S&apos; ---■ _L(S)} , 0,4] [0,3, IS -. ((c)A(b))s, S --+ (A(d))s, S --, (B)s} , 4] 3b
[-1, {S&apos; --4 _..(S)}, 0,4] [0,3, IS ---, ((c)A(b))s, S --4 (A(d))s, S --+ (B)s} , 4] [0, 1, {A -. a), 2,3] la
-1, {S&apos; -, l(S)}, 0,4] [0,3, {S -. ((c)A(b))s, S --+ (A(d))s, S -4 (B)s} , 4] [0, 1, {A --0 a), 2] ld
-1, {S&apos; --■ ..L(S)}, 0,4] [0,3, IS ---+ ((c)A(b))s, S --+ (A(d))s, S --+ (B)s} , 4] [1, A --. a, 2] 7b
[-1, {S&apos; -4 ..L(S)}, 0,4] [0,3, IS -. ((c)A(b))s, S --+ (A(d))s, S --+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] 2a
[0,3, {S --■ ((c)A(b))s, S --+ (A(d))s, S -+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] [2,2, Oh 3,3] la, ld
[ ... ] [0,3, {S ---0 ((c)A(b))s, S --4 (A(d))s, S ---+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] [2, b, 3] 4a
[ ... ] [0,3, {S --■ ((c)A(b))s, S -* (A(d))s, S --+ (B)8}, 4] [0, 1, {(c)A(b), A(b)} , 3] 36
[ ... ] [0,3, IS --■ ((c)A(b))s, S -+ (A(d))s, S --+ (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0,0, {c}, 1, 1] la, ld
[-1, {S&apos; -.1(S)}, 0,4] [0,3, IS --0 ((c)A(b))s, S --4 (A(d))s, S -. (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0, c, 1] 56
(-1, {S&apos; ---■ 1(S)}, 0,4] [0,3, {S -4 ((c)A(b))s, S --+ (A(d))s, S --4 (B)s} , 4] [0, (c)A(b), 3] 56
[- 1, {S&apos; -. 1(S)}, 0, 4] [0, S -+ ((c)A(b))s, 4] 7a
[-1, {S&apos; -■ _L(S)}, 0, 4] [0, 0, {S} , 4, 4] la, ld
[-1, {S&apos; --0 .1_(S)}, 0, 4] [0, S, 4] 5a
[-1, S&apos; -, _L(S), 4]
</table>
<figureCaption confidence="0.951776">
Figure 1:
</figureCaption>
<equation confidence="0.420255">
3a [k ni, [k,Q,m, - 1, Q&apos;,
</equation>
<bodyText confidence="0.815197">
where there is p such that in &lt; p &lt; j and Q&apos;
goto(right(Q), ap) is not empty
</bodyText>
<equation confidence="0.686287">
3b [i, k, Q , m] [i, k,Q ,m][i,p- 1, Q&apos; ,p,k]
</equation>
<bodyText confidence="0.78423">
where there is p such that i &lt; p &lt; k and Q,=
goto(left(Q), ap) is not empty
</bodyText>
<equation confidence="0.627959">
4a [i, k, Q, m, j][k&apos; , y, mi] 1-4 [i, k, Q&apos;,
provided in = k&apos;, where Q&apos; = gotoright(Q, 7)
4b Symmetric to 4a (cf. 2a and 2b)
5a [k Q , m, j][ki , 7, ml] [k ,t, m&apos;l
provided m = k&apos;, where t E gotoright(Q,7)
5b Symmetric to 5a (cf. 3a and 3b)
6a [i, k, Q, in, j][k&apos;, A -+y, rre] 14+
[i, k, Q, in, j][m, k&apos;, Q&apos;, in&apos;, j]
provided m &lt; k&apos;, where Q&apos; = goto(right(Q), A)
6b Symmetric to 6a
7a
[k Q, m, Q&apos; m&apos;
</equation>
<bodyText confidence="0.9852123">
provided in &lt;k&apos;, where Q&apos; = goto(right(Q), A)
7b Symmetric to 7a
The algorithm above is based on the transformation
Thead. It is therefore not surprising that this algorithm
is reminiscent of LR parsing [1] for a transformed gram-
mar rtwo(G). For most clauses, a rough correspondence
with actions of LR parsing can be found: Clauses 2
and 3 correspond with shifts. Clause 5 corresponds
with reductions with rules of the form [Xa] X [a]
in rt„(G). Clauses 6 and 7 correspond with reduc-
tions with rules of the form A --+ X [a] in rt.,(G). For
Clauses 1 and 4, corresponding actions are hard to find,
since these clauses seem to be specific to generalized
head-driven parsing.
The reason that we based Algorithm 6 on Thead is
twofold. Firstly, the algorithm above is more appro-
priate for presentational purposes than an alternative
Apart from HI parsing, also TD, HC, PHI, and EHI
parsing can be adapted to generalized head-driven pars-
ing.
</bodyText>
<subsectionHeader confidence="0.792928">
Correctness
</subsectionHeader>
<bodyText confidence="0.98793">
The head-driven stack automata studied so far differ
from one another in their degree of nondeterminism.
In this section we take a different perspective. For all
these devices, we show that quite similar relations ex-
ist between stack contents and the way input strings
are visited. Correctness results easily follow from such
characterisations. (Proofs of statements in this section
are omitted for reasons of space.)
Let G = (N ,T , P, S) be a head grammar. To be used
below, we introduce a special kind of derivation.
&apos;It is interesting to compare LR parsing for a context-free
grammar G with LR parsing for the transformed grammar
rtwo(G). The transformation has the effect that a reduc-
tion with a rule is replaced by a cascade of reductions with
smaller rules; apart from this, the transformation does not
affect the global run-time behaviour of LR parsing. More
serious are the consequences for the size of the parser: the
required number of LR states for the transformed grammar
is smaller [9].
</bodyText>
<subsectionHeader confidence="0.440785">
Generalized HI parsing
</subsectionHeader>
<bodyText confidence="0.8598484">
algorithm we have in mind which is not based on Thead,
and secondly, the resulting parsers need less sets Q.
This is similar in the case of LR parsing.&apos;
Example 1 Consider the generalized head grammar
with the following rules:
</bodyText>
<figure confidence="0.9959785">
S ((c)A(b))s (A(d))s j (B)s
A -+ a
B A(b)
Assume the input is given by aia2a3a4 =cabs. The
steps performed by the algorithm are given in Figure 1.
0
</figure>
<page confidence="0.646377">
215
</page>
<figure confidence="0.916219">
Y3,0 x3,1 Y3,1 .x3,2 73,2 x3,3 Y3,3
</figure>
<figureCaption confidence="0.9856698">
Figure 2: A head-outward sentential form derived by
the composition of a-derivations pi, 1 &lt; i &lt; 3. The
starting place of each a-derivation is indicated, each
triangle representing the application of a single produc-
tion.
</figureCaption>
<figure confidence="0.4378475">
Definition 1 A a-derivation has the form
A P1P2&amp;quot;.P•-1 70B71
P. l&apos;oalb371
-roax#71, (1)
</figure>
<bodyText confidence="0.993405666666667">
where p1,p2,—,p, are productions in pt, s &gt; 1, pi
rewrites the unique nonterminal occurrence introduced
as the head element of pi_i for 2 &lt; i &lt; s, p, = (B
</bodyText>
<equation confidence="0.5819785">
etni3) and p E rewrites /7 into x E T.
.
</equation>
<bodyText confidence="0.9976577">
The indicated occurrence of string &apos;in (1) is called the
handle of the a-derivation. When defined, the right-
most (leftmost) nonterminal occurrence in a (#, re-
spectively) is said to be adjacent to the handle. The
notions of handle and adjacent nonterminal occurrence
extend in an obvious way to derivations of the form
OAO-P-+ 070x709, where A -P-■ -yox-yi is a a-derivation.
By composing a-derivations, we can now define the
class of sentential forms we are interested in. (Figure 2
shows a case example.)
</bodyText>
<figure confidence="0.973834444444444">
Definition 2 A head-outward sentential form is ob-
tained through a derivation
P1
71,021,171,1
P2
72,0X2,172,1X2,272,2
. . .
P9
-4 7q,OXq,17q,1Xq,27q,2 • • • 7q,,7-1X,2,q7q,q (2)
</figure>
<bodyText confidence="0.998853333333333">
where q&gt; 1, each pi is a a-derivation and, for 2 &lt; i &lt;
q, only one string 7iij is rewritten by applying pi at a
nonterminal occurrence adjacent to the handle of pi-i.
Sequence pi , p2,...,pg is said to derive the sentential
form in (2).
The definition of head-outward sentential form sug-
gests a corresponding notion of head-outward deriva-
tion. Informally, a head-outward derivation proceeds by
recursively expanding to a terminal string first the head
of a rule, and then the remaining members of the rhs,
in an outward order. Conversely, we have head-inward
(HI) derivations, where first the remaining members
in the rhs are expanded, in an inward order (toward
the head), after which the head itself is recursively ex-
panded. Note that HI parsing recognizes a string by
computing an HI derivation in reverse (cf. LR parsing).
Let w = a1a2 • • , n &gt; 1, be a string over T and let
ao = 1. For -1 &lt; i &lt;j &lt;n, we write (i, j],„ to denote
</bodyText>
<figure confidence="0.358372384615385">
substring ai+i • • .
Theorem 1 For A one of AHC, AEHI or AEHI, the
following facts are equivalent:
(i) A reaches a configuration whose stack contents are
1112 • • • Ig, q&gt; 1, with
= [it , kt, At at • Tit • /3t, rnt, it] Or
It = [it, k, At int, .it] or
= [it, ki,
for the respective automata, 1 &lt; t &lt; q;
(ii) a sequence of a-derivations pi, p2,..., pg, q &gt; 1, de-
rives a head-outward sentential form
70(k 2. in1r(i)].71(k 742), in7q2)1w &amp;quot; •
• • • 7q-1(kr(q), m7(q)1w7q
</figure>
<construct confidence="0.739650666666667">
where r is a permutation of {1, ...,q}, pt has han-
dle lit which derives (k,r(t),m7,(t)b,,, 1 &lt; t &lt; q, and
m,r(t_o&lt; Ic„(t), 2 &lt; t &lt; q.
</construct>
<bodyText confidence="0.992717714285714">
As an example, an accepting stack configuration
[-I, -1, 5&apos; • IS •, n, n] corresponds to a cr-
derivation (S&apos; _LS)p, p E P+ , with handle
IS which derives the head-outward sentential form
70(.1, n1„7i = 1w, from which the correctness of the
head-corner algorithm follows directly.
If we assume that G does not contain any useless sym-
bols, then Theorem 1 has the following consequence. If
the automaton at some point has consulted the sym-
bols a11, ,a,, from the input string,.....i,,
increasing indexes, then there is a string in the language
generated by G of the form vo ai, vi • • vm- iai„,vm.
Such a statement may be called correct subsequence
property (a generalization of correct prefix property [8]).
Note that the order in which the input symbols are con-
sulted is only implicit in Theorem 1 (the permutation
ir) but is severely restricted by the definition of head-
outward sentential form. A more careful characterisa-
tion can be obtained, but will take us outside of the
scope of this paper.
The correct subsequence property is enforced by the
(top-down) predictive feature of the automata, and
holds also for ATD and Am. Characterisations simi-
lar to Theorem 1 can be provided for these devices. We
investigate below the GIII automaton.
For an item I E IGHI of the form [i,k,Q,m,j],
{k, Q, in, j], [i, k, Q, in] or [k, 1, in], we say that k (in
respectively) is its left (right) component. Let N&apos; be
</bodyText>
<page confidence="0.996169">
216
</page>
<bodyText confidence="0.996182333333333">
the set of nonterminals of the head grammar Thead(GA.
We need a function yld from reachable items in /G
into (N&apos; U Tr , specified as follows. If we assume
</bodyText>
<equation confidence="0.943717">
that (a)X(13) E QV A (a)X (3) E Q and t
(a)X(0) V t = A (a)X(13), then
X if 1= [i, k, Q, m, j]
yld(I) = [;1&apos;,8C-] if 1= [k,Q,m,jJ
if / =
if I =[k,t,m]
x [/3]
</equation>
<bodyText confidence="0.982917666666667">
It is not difficult to show that the definition of yld is
consistent (i.e. the particular choice of a tree or rule
from Q is irrelevant).
</bodyText>
<subsubsectionHeader confidence="0.314231">
Theorem 2 The following facts are equivalent:
</subsubsectionHeader>
<reference confidence="0.79681">
(i) A CHI reaches a configuration whose stack contents
are 1112- • I, q &gt; 1, with kt and int the left and right
components, respectively, of It, and yld(h) = no for
1 &lt; t &lt; q;
(ii) a sequence of a-derivations P1, p2, , pq, q &gt; 1, de-
rives in Thead(G) a head-outward sentential form
7o (kr(1)) Mr(1)]tv71(k7(2)17nr(2)1w72 &amp;quot;
• &apos; • 7q-1(kr(q), Mr(q)]w•Yq
</reference>
<bodyText confidence="0.632895666666667">
where ir is a permutation of {1,. , A has han-
dle zit which derives (k„.(t),m„(0],„, 1 &lt; t &lt; q, and
m,(t_o&lt; k,r(t), 2 &lt; t &lt; q.
</bodyText>
<sectionHeader confidence="0.971813" genericHeader="discussions">
Discussion
</sectionHeader>
<bodyText confidence="0.99997662962963">
We have presented a family of head-driven algorithms:
TD, HC, PHI, EHI, and HI parsing. The existence of
this family demonstrates that head-driven parsing cov-
ers a range of parsing algorithms wider than commonly
thought.
The algorithms in this family are increasingly deter-
ministic, which means that the search trees have a de-
creasing size, and therefore simple realizations, such as
backtracking, are increasingly efficient.
However, similar to the left-to-right case, this does
not necessarily hold for tabular realizations of these al-
gorithms. The reason is that the more refined an al-
gorithm is, the more items represent computation of
a single subderivation, and therefore some subderiva-
tions may be computed more than once. This is called
redundancy. Redundancy has been investigated for the
left-to-right case in [8], which solves this problem for
ELR parsing. Head-driven algorithms have an addi-
tional source of redundancy, which has been solved for
tabular HC parsing in [14]. The idea from [14] can also
be applied to the other head-driven algorithms from
this paper.
We have further proposed a generalization of head-
driven parsing, and we have shown an example of
such an algorithm based on LR parsing. Prospects to
even further generalize the ideas from this paper seem
promising.
</bodyText>
<sectionHeader confidence="0.992293" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999944875">
[1] A.V. Aho, R. Sethi, and J.D. Ullman. Compil-
ers: Principles, Techniques, and Tools. Addison-
Wesley, 1986.
[2] G. Bouma and G. van Noord. Head-driven parsing
for lexicalist grammars: Experimental results. In
Sixth Conference of the European Chapter of the
ACL, pages 71-80, April 1993.
[3] G. Gazdar, E. Klein, G. Pullum, and I. Sag. Gen-
eralized Phrase Structure Grammar. Harvard Uni-
versity Press, Cambridge, MA, 1985.
[4] Third International Workshop on Parsing Tech-
nologies (IWPT 3), Tilburg (The Netherlands) and
Durbuy (Belgium), August 1993.
[5] R. Jackendoff. X-bar Syntax: A Study of Phrase
Structure. The MIT Press, Cambridge, MA, 1977.
[6] M. Kay. Head-driven parsing. In International
Parsing Workshop &apos;89, pages 52-62, Pittsburgh,
1989.
[7] R. Leermakers. How to cover a grammar. In 27th
Annual Meeting of the ACL, pages 135-142, June
1989.
[8] M.J. Nederhof. An optimal tabular parsing algo-
rithm. In this proceedings.
[9] M.J. Nederhof and J.J. Sarbo. Increasing the ap-
plicability of LR parsing. In [4], pages 187-201.
[10] G. van Noord. Reversibility in Natural Language
Processing. PhD thesis, University of Utrecht,
1993.
[11] C. Pollard and I. Sag. Information-Based Syntax
and Semantics, volume 1: Fundamentals. CSLI
Lecture Notes Series No. 13, Center for the Study
of Language and Information, Stanford University,
Stanford, California, 1987.
[12] P.W. Purdom, Jr. and C.A. Brown. Parsing
extended LR(k) grammars. Acta Informatica,
15:115-127,1981.
[13] G. Satta and 0. Stock. Head-driven bidirectional
parsing: A tabular method. In International Pars-
ing Workshop &apos;89, pages 43-51, Pittsburgh, 1989.
[14] K. Sikkel and R. op den Akker. Predictive head-
corner chart parsing. In [4], pages 267-276.
[15] E. Soisalon-Soininen and E. Ukkonen. A method
for transforming grammars into LL(k) form. Acta
Informatica, 12:339-369,1979.
[16] M. Tomita. Parsing 2-dimensional language. In
M. Tomita, editor, Current Issues in Parsing Tech-
nology, chapter 18, pages 277-289. Kluwer Aca-
demic Publishers, 1991.
</reference>
<page confidence="0.998404">
217
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.416593">
<title confidence="0.962914">AN EXTENDED THEORY OF HEAD-DRIVEN PARSING</title>
<author confidence="0.891672">Mark-Jan Nederhof</author>
<affiliation confidence="0.9995925">University of Nijmegen Department of Computer Science</affiliation>
<address confidence="0.9266315">Toernooiveld, 6525 ED Nijmegen The Netherlands</address>
<note confidence="0.55409">markj aacs kun .n1</note>
<abstract confidence="0.9993333">We show that more head-driven parsing algorithms can be formulated than those occurring in the existing literature. These algorithms are inspired by a family of left-to-right parsing algorithms from a recent publication. We further introduce a more advanced notion of &amp;quot;head-driven parsing&amp;quot; which allows more detailed specification of the processing order of non-head elements in the right-hand side. We develop a parsing algorithm for this strategy, based on LR parsing techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<title>(i) A CHI reaches a configuration whose stack contents are 1112- • I, q &gt; 1, with kt and int the left and right components, respectively, of It, and yld(h) = no for 1 &lt; t &lt; q; (ii) a sequence of a-derivations P1, p2, , pq, q &gt; 1, derives in Thead(G) a head-outward sentential form 7o (kr(1)) Mr(1)]tv71(k7(2)17nr(2)1w72 &amp;quot; • &apos; • 7q-1(kr(q),</title>
<location>Mr(q)]w•Yq</location>
<marker></marker>
<rawString> (i) A CHI reaches a configuration whose stack contents are 1112- • I, q &gt; 1, with kt and int the left and right components, respectively, of It, and yld(h) = no for 1 &lt; t &lt; q; (ii) a sequence of a-derivations P1, p2, , pq, q &gt; 1, derives in Thead(G) a head-outward sentential form 7o (kr(1)) Mr(1)]tv71(k7(2)17nr(2)1w72 &amp;quot; • &apos; • 7q-1(kr(q), Mr(q)]w•Yq</rawString>
</citation>
<citation valid="true">
<authors>
<author>A V Aho</author>
<author>R Sethi</author>
<author>J D Ullman</author>
</authors>
<date>1986</date>
<booktitle>Compilers: Principles, Techniques, and Tools.</booktitle>
<publisher>AddisonWesley,</publisher>
<contexts>
<context position="2494" citStr="[15, 1]" startWordPosition="377" endWordPosition="378"> the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in the leftto-right deterministic parsing literature to overcome redundancy problems of the above kind, thus reducing *Supported by the Dutch Organisation for Scientific Research (NWO), under grant 00-62-518 Giorgio Satta Universita di Padova Dipartimento di Elettronica e Informatica via Gradenigo 6/A, 35131 Padova Italy sattadei.unipd.it the degree of nondeterminism of the resulting methods. These solutions range from predictive LR parsing to LR parsing [15, 1]. On the basis of work in [8] for nondeterministic left-to-right parsing, we trace here a theory of head-driven parsing going from crude top-down and head-corner to more sophisticated solutions, in the attempt to successively make more deterministic the behaviour of head-driven methods. Finally, we propose an original generalization of headdriven parsing, allowing a more detailed specification of the order in which elements of a right-hand side are to be processed. We study in detail a solution to such a head-driven strategy based on LR parsing. Other methods presented in this paper could be e</context>
<context position="6321" citStr="[1]" startWordPosition="1087" endWordPosition="1087"> few symbols on the stack are F then these may be replaced by the symbols r. Finally, the input is accepted whenever the automaton reaches stack Fin(n). Stack automata presented in what follows act as recognizers. Parsing algorithms can directly be obtained by pairing these automata with an output effect. A family of head-driven algorithms This section investigates the adaptation of a family of left-to-right parsing algorithms from [8], viz, top-down, left-corner, PLR, ELR, and LR parsing, to head grammars. Top-down parsing The following is a straightforward adaptation of topdown (TD) parsing [1] to head grammars. There are two kinds of stack symbol (items), one of the form [i, A, j], which indicates that some subderivation from A is needed deriving a substring of ai+i the other of the form [i, k, A —0 a • 7 • (3,m, j], which also indicates that some subderivation from A is needed deriving a substring of ai+i a,, but specifically using the rule A --p a7fl, where 7 ak+1 ...am has already been established. Formally, we have {[i, A, j] &lt;i} JrD = {[i,k, A —+ a • 7 • )3,m, j] I A --0 a7# E A i &lt; k &lt; m &lt;j} Algorithm 1 (Head-driven top-down) ATV = (T, J1TD U J2TD, Init(n), 1-0, Fin(n)), wher</context>
<context position="14188" citStr="[1]" startWordPosition="2828" endWordPosition="2828">ting the sets from the items. This results in common infix (CI) parsing, which is a generalization of common prefix parsing [8]. Cl parsing does not satisfy the correct subsequence property, to be discussed later. For space reasons, we omit further discussion of CI parsing. HI parsing If we translate the difference between ELR and LR parsing [8] to head-driven parsing, we are led to HI parsing, starting from EHI parsing, as described below. The algorithm is called HI because it computes head-inward derivations in reverse, in the same way as LR parsing computes rightmost derivations in reverse [1]. Headinward derivations will be discussed later in this paper. HI parsing uses items of the form [i, k, Q, in, jj, where Q is a non-empty set of &amp;quot;double-dotted&amp;quot; rules A —0 a • -y • 3. The fundamental difference with the items in /EH/ is that the infix -y in the right-hand sides does not have to be fixed. Formally, we have = k C , in, 0CQC{A—■a•7•/31A—■ a-y,3 E PI} A i &lt; k &lt; rn &lt;j} We explain the difference in behaviour of HI parsing with regard to EHI parsing by investigating Clauses la and 2a of Algorithm 4. (Clauses 3a and 4a would give rise to a similar discussion.) Clauses la and 2a both </context>
<context position="21073" citStr="[1]" startWordPosition="4194" endWordPosition="4194">a] for each proper suffix X a of a rhs in G where we assume that each member of the form [c] in the transformed grammar is omitted. HI parsing revisited Our next step is to show that generalized head grammars can be effectively handled with a generalization of HI parsing (generalized HI (GHI) parsing). This new algorithm exhibits a superficial similarity to the 2-dimensional LR parsing algorithm from [16]. For a set Q of trees and rules,&apos; closure(Q) is defined to be 4It is interesting to compare the relation between trees and rules with the one between kernel and nonkernel items of LR parsing [1]. the smallest set which satisfies closure(Q) 2 Q U {A —4 (a)X(f3) E P (7)A(5) E closure(Q) V B (7)A(b) E closure(Q)} The trees or rules of which the main head is some specified symbol X can be selected from a set Q by goto(Q, X) = It EQIt= (a)X (#) V t = A (a)X (/3)} In a similar way, we can select trees and rules according to a left or right subtree. gotoleft(Q , a) = It EQIt= (a)X(#) V t = A (a)X(13)} We assume a symmetric definition for gotoright. When we set out to recognize the left subtrees from a set of trees and rules, we use the following function. left(Q) = closurega (a),(03) E V A </context>
<context position="24254" citStr="[0, c, 1]" startWordPosition="4890" endWordPosition="4892">--+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] 2a [0,3, {S --■ ((c)A(b))s, S --+ (A(d))s, S -+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] [2,2, Oh 3,3] la, ld [ ... ] [0,3, {S ---0 ((c)A(b))s, S --4 (A(d))s, S ---+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] [2, b, 3] 4a [ ... ] [0,3, {S --■ ((c)A(b))s, S -* (A(d))s, S --+ (B)8}, 4] [0, 1, {(c)A(b), A(b)} , 3] 36 [ ... ] [0,3, IS --■ ((c)A(b))s, S -+ (A(d))s, S --+ (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0,0, {c}, 1, 1] la, ld [-1, {S&apos; -.1(S)}, 0,4] [0,3, IS --0 ((c)A(b))s, S --4 (A(d))s, S -. (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0, c, 1] 56 (-1, {S&apos; ---■ 1(S)}, 0,4] [0,3, {S -4 ((c)A(b))s, S --+ (A(d))s, S --4 (B)s} , 4] [0, (c)A(b), 3] 56 [- 1, {S&apos; -. 1(S)}, 0, 4] [0, S -+ ((c)A(b))s, 4] 7a [-1, {S&apos; -■ _L(S)}, 0, 4] [0, 0, {S} , 4, 4] la, ld [-1, {S&apos; --0 .1_(S)}, 0, 4] [0, S, 4] 5a [-1, S&apos; -, _L(S), 4] Figure 1: 3a [k ni, [k,Q,m, - 1, Q&apos;, where there is p such that in &lt; p &lt; j and Q&apos; goto(right(Q), ap) is not empty 3b [i, k, Q , m] [i, k,Q ,m][i,p- 1, Q&apos; ,p,k] where there is p such that i &lt; p &lt; k and Q,= goto(left(Q), ap) is not empty 4a [i, k, Q, m, j][k&apos; , y, mi] 1-4 [i, k, Q&apos;, provided in = k&apos;, where Q&apos; = gotoright(Q, 7) 4</context>
</contexts>
<marker>[1]</marker>
<rawString>A.V. Aho, R. Sethi, and J.D. Ullman. Compilers: Principles, Techniques, and Tools. AddisonWesley, 1986.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bouma</author>
<author>G van Noord</author>
</authors>
<title>Head-driven parsing for lexicalist grammars: Experimental results.</title>
<date>1993</date>
<booktitle>In Sixth Conference of the European Chapter of the ACL,</booktitle>
<pages>71--80</pages>
<contexts>
<context position="1433" citStr="[6, 13, 2, 14]" startWordPosition="212" endWordPosition="215">t contentful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of Grammar (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th</context>
<context position="9851" citStr="[6, 13, 2, 14]" startWordPosition="1870" endWordPosition="1873">, D cro7.A)3,m, j][i&apos; , k&apos; , B .6. , m&apos; , j&apos;] [i, k, D av7.A)3,rn, j][i&apos; ,k&apos; ,C , f] provided m = i&apos;, where there is C --■ r1B9 E Pt such that C 0* A (j = j&apos; is automatically satisfied) 4a [i, k, A a • 7 • B m, j][i&apos; , k&apos; , B , 1-4 [i,k, A a .7B • )3, ne, provided m = (m = i&apos; and j = f are automatically satisfied) Head-corner parsing as well as all algorithms in the remainder of this paper may loop exactly for the grammars which are cyclic (where A -,+ A for some A). The head-corner algorithm above is the only one in this paper which has already appeared in the literature, in different guises [6, 13, 2, 14]. Predictive HI parsing We say two rules A al and B a2 have a common infix a if al = /Awn and a2 = )32a72, for some /32, -yi and 72. The notion of common infix is an adaptation of the notion of common prefix [8] to head grammars. If a grammar contains many common infixes, then HC parsing may be very nondeterministic; in particular, Clauses 1 or 3 may be applied with different rules C ?la E Pt or C n138 E PI for fixed a or B. In [15] an idea is described that allows reduction of nondeterminism in case of common prefixes and leftcorner parsing. The resulting algorithm is called predictive LR (PL</context>
<context position="23924" citStr="[2, b, 3]" startWordPosition="4816" endWordPosition="4818"> S --4 (A(d))s, S --+ (B)s} , 4] [0, 1, {A -. a), 2,3] la -1, {S&apos; -, l(S)}, 0,4] [0,3, {S -. ((c)A(b))s, S --+ (A(d))s, S -4 (B)s} , 4] [0, 1, {A --0 a), 2] ld -1, {S&apos; --■ ..L(S)}, 0,4] [0,3, IS ---+ ((c)A(b))s, S --+ (A(d))s, S --+ (B)s} , 4] [1, A --. a, 2] 7b [-1, {S&apos; -4 ..L(S)}, 0,4] [0,3, IS -. ((c)A(b))s, S --+ (A(d))s, S --+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] 2a [0,3, {S --■ ((c)A(b))s, S --+ (A(d))s, S -+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] [2,2, Oh 3,3] la, ld [ ... ] [0,3, {S ---0 ((c)A(b))s, S --4 (A(d))s, S ---+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] [2, b, 3] 4a [ ... ] [0,3, {S --■ ((c)A(b))s, S -* (A(d))s, S --+ (B)8}, 4] [0, 1, {(c)A(b), A(b)} , 3] 36 [ ... ] [0,3, IS --■ ((c)A(b))s, S -+ (A(d))s, S --+ (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0,0, {c}, 1, 1] la, ld [-1, {S&apos; -.1(S)}, 0,4] [0,3, IS --0 ((c)A(b))s, S --4 (A(d))s, S -. (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0, c, 1] 56 (-1, {S&apos; ---■ 1(S)}, 0,4] [0,3, {S -4 ((c)A(b))s, S --+ (A(d))s, S --4 (B)s} , 4] [0, (c)A(b), 3] 56 [- 1, {S&apos; -. 1(S)}, 0, 4] [0, S -+ ((c)A(b))s, 4] 7a [-1, {S&apos; -■ _L(S)}, 0, 4] [0, 0, {S} , 4, 4] la, ld [-1, {S&apos; --0 .1_(S)}, 0, 4] [0, S, 4] 5a [-1, S&apos; -, _L(S), 4</context>
</contexts>
<marker>[2]</marker>
<rawString>G. Bouma and G. van Noord. Head-driven parsing for lexicalist grammars: Experimental results. In Sixth Conference of the European Chapter of the ACL, pages 71-80, April 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Gazdar</author>
<author>E Klein</author>
<author>G Pullum</author>
<author>I Sag</author>
</authors>
<title>Generalized Phrase Structure Grammar.</title>
<date>1985</date>
<publisher>Harvard University Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="1108" citStr="[5, 3, 11]" startWordPosition="167" endWordPosition="169"> detailed specification of the processing order of non-head elements in the right-hand side. We develop a parsing algorithm for this strategy, based on LR parsing techniques. Introduction According to the head-driven paradigm, parsing of a formal language is started from the elements within the input string that are most contentful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of Grammar (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in </context>
<context position="23924" citStr="[2, b, 3]" startWordPosition="4816" endWordPosition="4818"> S --4 (A(d))s, S --+ (B)s} , 4] [0, 1, {A -. a), 2,3] la -1, {S&apos; -, l(S)}, 0,4] [0,3, {S -. ((c)A(b))s, S --+ (A(d))s, S -4 (B)s} , 4] [0, 1, {A --0 a), 2] ld -1, {S&apos; --■ ..L(S)}, 0,4] [0,3, IS ---+ ((c)A(b))s, S --+ (A(d))s, S --+ (B)s} , 4] [1, A --. a, 2] 7b [-1, {S&apos; -4 ..L(S)}, 0,4] [0,3, IS -. ((c)A(b))s, S --+ (A(d))s, S --+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] 2a [0,3, {S --■ ((c)A(b))s, S --+ (A(d))s, S -+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] [2,2, Oh 3,3] la, ld [ ... ] [0,3, {S ---0 ((c)A(b))s, S --4 (A(d))s, S ---+ (B)s} , 4] [0, 1, {(c)A(b), A(d), A(b)} , 2,3] [2, b, 3] 4a [ ... ] [0,3, {S --■ ((c)A(b))s, S -* (A(d))s, S --+ (B)8}, 4] [0, 1, {(c)A(b), A(b)} , 3] 36 [ ... ] [0,3, IS --■ ((c)A(b))s, S -+ (A(d))s, S --+ (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0,0, {c}, 1, 1] la, ld [-1, {S&apos; -.1(S)}, 0,4] [0,3, IS --0 ((c)A(b))s, S --4 (A(d))s, S -. (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0, c, 1] 56 (-1, {S&apos; ---■ 1(S)}, 0,4] [0,3, {S -4 ((c)A(b))s, S --+ (A(d))s, S --4 (B)s} , 4] [0, (c)A(b), 3] 56 [- 1, {S&apos; -. 1(S)}, 0, 4] [0, S -+ ((c)A(b))s, 4] 7a [-1, {S&apos; -■ _L(S)}, 0, 4] [0, 0, {S} , 4, 4] la, ld [-1, {S&apos; --0 .1_(S)}, 0, 4] [0, S, 4] 5a [-1, S&apos; -, _L(S), 4</context>
</contexts>
<marker>[3]</marker>
<rawString>G. Gazdar, E. Klein, G. Pullum, and I. Sag. Generalized Phrase Structure Grammar. Harvard University Press, Cambridge, MA, 1985.</rawString>
</citation>
<citation valid="true">
<date>1993</date>
<booktitle>Third International Workshop on Parsing Technologies (IWPT 3),</booktitle>
<institution>Tilburg (The Netherlands) and Durbuy (Belgium),</institution>
<contexts>
<context position="24501" citStr="[0, S, 4]" startWordPosition="4948" endWordPosition="4950">b), A(d), A(b)} , 2,3] [2, b, 3] 4a [ ... ] [0,3, {S --■ ((c)A(b))s, S -* (A(d))s, S --+ (B)8}, 4] [0, 1, {(c)A(b), A(b)} , 3] 36 [ ... ] [0,3, IS --■ ((c)A(b))s, S -+ (A(d))s, S --+ (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0,0, {c}, 1, 1] la, ld [-1, {S&apos; -.1(S)}, 0,4] [0,3, IS --0 ((c)A(b))s, S --4 (A(d))s, S -. (B)s} , 4] [0, 1, {(c)A(b), A(b)} , 3] [0, c, 1] 56 (-1, {S&apos; ---■ 1(S)}, 0,4] [0,3, {S -4 ((c)A(b))s, S --+ (A(d))s, S --4 (B)s} , 4] [0, (c)A(b), 3] 56 [- 1, {S&apos; -. 1(S)}, 0, 4] [0, S -+ ((c)A(b))s, 4] 7a [-1, {S&apos; -■ _L(S)}, 0, 4] [0, 0, {S} , 4, 4] la, ld [-1, {S&apos; --0 .1_(S)}, 0, 4] [0, S, 4] 5a [-1, S&apos; -, _L(S), 4] Figure 1: 3a [k ni, [k,Q,m, - 1, Q&apos;, where there is p such that in &lt; p &lt; j and Q&apos; goto(right(Q), ap) is not empty 3b [i, k, Q , m] [i, k,Q ,m][i,p- 1, Q&apos; ,p,k] where there is p such that i &lt; p &lt; k and Q,= goto(left(Q), ap) is not empty 4a [i, k, Q, m, j][k&apos; , y, mi] 1-4 [i, k, Q&apos;, provided in = k&apos;, where Q&apos; = gotoright(Q, 7) 4b Symmetric to 4a (cf. 2a and 2b) 5a [k Q , m, j][ki , 7, ml] [k ,t, m&apos;l provided m = k&apos;, where t E gotoright(Q,7) 5b Symmetric to 5a (cf. 3a and 3b) 6a [i, k, Q, in, j][k&apos;, A -+y, rre] 14+ [i, k, Q, in, j][m, k&apos;, Q&apos;, in&apos;, j] provided m &lt; k&apos;, wher</context>
</contexts>
<marker>[4]</marker>
<rawString>Third International Workshop on Parsing Technologies (IWPT 3), Tilburg (The Netherlands) and Durbuy (Belgium), August 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Jackendoff</author>
</authors>
<title>X-bar Syntax: A Study of Phrase Structure.</title>
<date>1977</date>
<publisher>The MIT Press,</publisher>
<location>Cambridge, MA,</location>
<contexts>
<context position="1108" citStr="[5, 3, 11]" startWordPosition="167" endWordPosition="169"> detailed specification of the processing order of non-head elements in the right-hand side. We develop a parsing algorithm for this strategy, based on LR parsing techniques. Introduction According to the head-driven paradigm, parsing of a formal language is started from the elements within the input string that are most contentful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of Grammar (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in </context>
</contexts>
<marker>[5]</marker>
<rawString>R. Jackendoff. X-bar Syntax: A Study of Phrase Structure. The MIT Press, Cambridge, MA, 1977.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Kay</author>
</authors>
<title>Head-driven parsing.</title>
<date>1989</date>
<booktitle>In International Parsing Workshop &apos;89,</booktitle>
<pages>52--62</pages>
<location>Pittsburgh,</location>
<contexts>
<context position="1433" citStr="[6, 13, 2, 14]" startWordPosition="212" endWordPosition="215">t contentful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of Grammar (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th</context>
<context position="9851" citStr="[6, 13, 2, 14]" startWordPosition="1870" endWordPosition="1873">, D cro7.A)3,m, j][i&apos; , k&apos; , B .6. , m&apos; , j&apos;] [i, k, D av7.A)3,rn, j][i&apos; ,k&apos; ,C , f] provided m = i&apos;, where there is C --■ r1B9 E Pt such that C 0* A (j = j&apos; is automatically satisfied) 4a [i, k, A a • 7 • B m, j][i&apos; , k&apos; , B , 1-4 [i,k, A a .7B • )3, ne, provided m = (m = i&apos; and j = f are automatically satisfied) Head-corner parsing as well as all algorithms in the remainder of this paper may loop exactly for the grammars which are cyclic (where A -,+ A for some A). The head-corner algorithm above is the only one in this paper which has already appeared in the literature, in different guises [6, 13, 2, 14]. Predictive HI parsing We say two rules A al and B a2 have a common infix a if al = /Awn and a2 = )32a72, for some /32, -yi and 72. The notion of common infix is an adaptation of the notion of common prefix [8] to head grammars. If a grammar contains many common infixes, then HC parsing may be very nondeterministic; in particular, Clauses 1 or 3 may be applied with different rules C ?la E Pt or C n138 E PI for fixed a or B. In [15] an idea is described that allows reduction of nondeterminism in case of common prefixes and leftcorner parsing. The resulting algorithm is called predictive LR (PL</context>
</contexts>
<marker>[6]</marker>
<rawString>M. Kay. Head-driven parsing. In International Parsing Workshop &apos;89, pages 52-62, Pittsburgh, 1989.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Leermakers</author>
</authors>
<title>How to cover a grammar.</title>
<date>1989</date>
<booktitle>In 27th Annual Meeting of the ACL,</booktitle>
<pages>135--142</pages>
<contexts>
<context position="12189" citStr="[12, 7]" startWordPosition="2378" endWordPosition="2379">E Pt, where there are D a&apos; A/3, C -&gt;ijO E Pt such that C 0* A 4a [i,k, A 7,m, j][i&apos; , , B 6,m&apos; ,j&apos;] [i, k, A -k 7B, in&apos;, provided m = k&apos; and B --+ 6 E Pt, where there is A -■ a7B,3 E Pt Extended HI parsing The PHI algorithm can process simultaneously a common infix a in two different rules A -&gt; /31a71 and A --, 132a1y2, which reduces nondeterminism. We may however also specify an algorithm which succeeds in simultaneously processing all common infixes, irrespective of whether the left-hand sides of the corresponding rules are the same. This algorithm is inspired by extended LR ( EL R) parsing [12, 7] for extended context-free grammars (where right-hand sides consist of regular expressions over V). By analogy, it will be called extended HI (EHI) parsing. This algorithm uses yet another kind of item, viz. of the form [i, k, {Ai, A2, .,A,,} -&gt; 7,m, j], where there exists at least one rule A a7)3 for each A E {Ai, A2, .,A}. With such an item, we simulate computation of different items [i, k, A -■ a • 7 • )3,m, E Plc which would be treated individually by an HC parser. Formally, we have {[i, --&apos; mci] 0CAC{AIA--a-y0EPt} A i &lt; k &lt; m &lt; j} Algorithm 4 _(Extended HI) A = (T, 1E1&apos;1 , Fin(n)), where </context>
</contexts>
<marker>[7]</marker>
<rawString>R. Leermakers. How to cover a grammar. In 27th Annual Meeting of the ACL, pages 135-142, June 1989.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M J Nederhof</author>
</authors>
<title>An optimal tabular parsing algorithm. In this proceedings.</title>
<contexts>
<context position="2523" citStr="[8]" startWordPosition="385" endWordPosition="385">hat could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in the leftto-right deterministic parsing literature to overcome redundancy problems of the above kind, thus reducing *Supported by the Dutch Organisation for Scientific Research (NWO), under grant 00-62-518 Giorgio Satta Universita di Padova Dipartimento di Elettronica e Informatica via Gradenigo 6/A, 35131 Padova Italy sattadei.unipd.it the degree of nondeterminism of the resulting methods. These solutions range from predictive LR parsing to LR parsing [15, 1]. On the basis of work in [8] for nondeterministic left-to-right parsing, we trace here a theory of head-driven parsing going from crude top-down and head-corner to more sophisticated solutions, in the attempt to successively make more deterministic the behaviour of head-driven methods. Finally, we propose an original generalization of headdriven parsing, allowing a more detailed specification of the order in which elements of a right-hand side are to be processed. We study in detail a solution to such a head-driven strategy based on LR parsing. Other methods presented in this paper could be extended as well. Preliminarie</context>
<context position="6157" citStr="[8]" startWordPosition="1061" endWordPosition="1061"> one step of the automaton may, under some conditions on the input, transform a stack of the form r&apos;r into the stack ry. In words, r r&apos; denotes that if the top-most few symbols on the stack are F then these may be replaced by the symbols r. Finally, the input is accepted whenever the automaton reaches stack Fin(n). Stack automata presented in what follows act as recognizers. Parsing algorithms can directly be obtained by pairing these automata with an output effect. A family of head-driven algorithms This section investigates the adaptation of a family of left-to-right parsing algorithms from [8], viz, top-down, left-corner, PLR, ELR, and LR parsing, to head grammars. Top-down parsing The following is a straightforward adaptation of topdown (TD) parsing [1] to head grammars. There are two kinds of stack symbol (items), one of the form [i, A, j], which indicates that some subderivation from A is needed deriving a substring of ai+i the other of the form [i, k, A —0 a • 7 • (3,m, j], which also indicates that some subderivation from A is needed deriving a substring of ai+i a,, but specifically using the rule A --p a7fl, where 7 ak+1 ...am has already been established. Formally, we have {</context>
<context position="10062" citStr="[8]" startWordPosition="1919" endWordPosition="1919">, B , 1-4 [i,k, A a .7B • )3, ne, provided m = (m = i&apos; and j = f are automatically satisfied) Head-corner parsing as well as all algorithms in the remainder of this paper may loop exactly for the grammars which are cyclic (where A -,+ A for some A). The head-corner algorithm above is the only one in this paper which has already appeared in the literature, in different guises [6, 13, 2, 14]. Predictive HI parsing We say two rules A al and B a2 have a common infix a if al = /Awn and a2 = )32a72, for some /32, -yi and 72. The notion of common infix is an adaptation of the notion of common prefix [8] to head grammars. If a grammar contains many common infixes, then HC parsing may be very nondeterministic; in particular, Clauses 1 or 3 may be applied with different rules C ?la E Pt or C n138 E PI for fixed a or B. In [15] an idea is described that allows reduction of nondeterminism in case of common prefixes and leftcorner parsing. The resulting algorithm is called predictive LR (PLR) parsing. The following is an adaptation of this idea to HC parsing. The resulting algorithm is called predictive HI (PHI) parsing. (HI parsing, to be discussed later, is a generalization of LR parsing to head</context>
<context position="13712" citStr="[8]" startWordPosition="2749" endWordPosition="2749">rovided m &lt; j and am+1 = a and A&apos; = E A I A a7a3 E Pt} is not empty 3a [i, k, A -4 -y, rn, j][i&apos;, , A&apos; -4 6, m&apos; , j&apos;i [i, k, A -y, m, j][i&apos; , , A&amp;quot; B, rni, ji] provided m = i&apos; and B E Pt for some B E A&apos; such that A&amp;quot; = {C I 3C n BO , D ay A3 E Pt (D E A AC 0* A)} is not empty 4a [i, k, A -y, m, j] [i&apos; , k&apos;, A&apos; 6, in&apos;, j&apos;] I-+ -yB,m&apos; , j] provided m = k&apos; and B 6 E Pt for some B E A&apos; such that A&amp;quot; = {A EA IA a7B/3 E Pt} is not empty 212 This algorithm can be simplified by omitting the sets from the items. This results in common infix (CI) parsing, which is a generalization of common prefix parsing [8]. Cl parsing does not satisfy the correct subsequence property, to be discussed later. For space reasons, we omit further discussion of CI parsing. HI parsing If we translate the difference between ELR and LR parsing [8] to head-driven parsing, we are led to HI parsing, starting from EHI parsing, as described below. The algorithm is called HI because it computes head-inward derivations in reverse, in the same way as LR parsing computes rightmost derivations in reverse [1]. Headinward derivations will be discussed later in this paper. HI parsing uses items of the form [i, k, Q, in, jj, where Q </context>
<context position="30975" citStr="[8]" startWordPosition="6150" endWordPosition="6150">tion (S&apos; _LS)p, p E P+ , with handle IS which derives the head-outward sentential form 70(.1, n1„7i = 1w, from which the correctness of the head-corner algorithm follows directly. If we assume that G does not contain any useless symbols, then Theorem 1 has the following consequence. If the automaton at some point has consulted the symbols a11, ,a,, from the input string,.....i,, increasing indexes, then there is a string in the language generated by G of the form vo ai, vi • • vm- iai„,vm. Such a statement may be called correct subsequence property (a generalization of correct prefix property [8]). Note that the order in which the input symbols are consulted is only implicit in Theorem 1 (the permutation ir) but is severely restricted by the definition of headoutward sentential form. A more careful characterisation can be obtained, but will take us outside of the scope of this paper. The correct subsequence property is enforced by the (top-down) predictive feature of the automata, and holds also for ATD and Am. Characterisations similar to Theorem 1 can be provided for these devices. We investigate below the GIII automaton. For an item I E IGHI of the form [i,k,Q,m,j], {k, Q, in, j], </context>
</contexts>
<marker>[8]</marker>
<rawString>M.J. Nederhof. An optimal tabular parsing algorithm. In this proceedings.</rawString>
</citation>
<citation valid="false">
<authors>
<author>M J Nederhof</author>
<author>J J Sarbo</author>
</authors>
<title>Increasing the applicability of LR parsing.</title>
<journal>In</journal>
<volume>4</volume>
<pages>187--201</pages>
<contexts>
<context position="27110" citStr="[9]" startWordPosition="5441" endWordPosition="5441">or reasons of space.) Let G = (N ,T , P, S) be a head grammar. To be used below, we introduce a special kind of derivation. &apos;It is interesting to compare LR parsing for a context-free grammar G with LR parsing for the transformed grammar rtwo(G). The transformation has the effect that a reduction with a rule is replaced by a cascade of reductions with smaller rules; apart from this, the transformation does not affect the global run-time behaviour of LR parsing. More serious are the consequences for the size of the parser: the required number of LR states for the transformed grammar is smaller [9]. Generalized HI parsing algorithm we have in mind which is not based on Thead, and secondly, the resulting parsers need less sets Q. This is similar in the case of LR parsing.&apos; Example 1 Consider the generalized head grammar with the following rules: S ((c)A(b))s (A(d))s j (B)s A -+ a B A(b) Assume the input is given by aia2a3a4 =cabs. The steps performed by the algorithm are given in Figure 1. 0 215 Y3,0 x3,1 Y3,1 .x3,2 73,2 x3,3 Y3,3 Figure 2: A head-outward sentential form derived by the composition of a-derivations pi, 1 &lt; i &lt; 3. The starting place of each a-derivation is indicated, each </context>
</contexts>
<marker>[9]</marker>
<rawString>M.J. Nederhof and J.J. Sarbo. Increasing the applicability of LR parsing. In [4], pages 187-201.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Noord</author>
</authors>
<title>Reversibility in Natural Language Processing.</title>
<date>1993</date>
<tech>PhD thesis,</tech>
<institution>University of Utrecht,</institution>
<contexts>
<context position="8418" citStr="[10]" startWordPosition="1557" endWordPosition="1557">mar head-recursive if A 0+ A for some A. Head-driven TD parsing may loop exactly for the grammars which are head-recursive. Head recursion is a generalization of left recursion for traditional TD parsing. In the case of grammars with some parameter mechanism, top-down parsing has the advantage over other kinds of parsing that top-down propagation of parameter values is possible in collaboration with context-free parsing (cf. the standard evaluation of definite clause grammars), which may lead to more efficient processing. This holds for left-to-right parsing as well as for head-driven parsing [10]. Head-corner parsing The predictive steps from Algorithm 1, represented by Clause 0 and supported by Clauses Oa and Ob, can be compiled into the head-corner relation 0*. This gives the head-corner (HC) algorithm below. The items from ip are no longer needed now. We define //lc = Algorithm 2 (head-corner) = (T, Pic , Fin(n)), where Init(n) = [-1, —1, S&apos; —+ • 1. • S, 0 , n], Fin(n) = [-1, —1, S&apos; —0 • IS *, n, n], and 1—, is given by the following clauses. (Clauses lb, 2b, 3b, 4b are omitted, since these are symmetric to la, 2a, 3a, 4a, respectively.) la [i,k,A--0 a •-y • Bfl,m,j]1-0 [i, k, A --</context>
</contexts>
<marker>[10]</marker>
<rawString>G. van Noord. Reversibility in Natural Language Processing. PhD thesis, University of Utrecht, 1993.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Pollard</author>
<author>I Sag</author>
</authors>
<title>Information-Based Syntax and Semantics, volume 1: Fundamentals.</title>
<date>1987</date>
<booktitle>CSLI Lecture Notes Series No. 13, Center for the Study of Language and Information,</booktitle>
<location>Stanford University, Stanford, California,</location>
<contexts>
<context position="1108" citStr="[5, 3, 11]" startWordPosition="167" endWordPosition="169"> detailed specification of the processing order of non-head elements in the right-hand side. We develop a parsing algorithm for this strategy, based on LR parsing techniques. Introduction According to the head-driven paradigm, parsing of a formal language is started from the elements within the input string that are most contentful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of Grammar (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in </context>
</contexts>
<marker>[11]</marker>
<rawString>C. Pollard and I. Sag. Information-Based Syntax and Semantics, volume 1: Fundamentals. CSLI Lecture Notes Series No. 13, Center for the Study of Language and Information, Stanford University, Stanford, California, 1987.</rawString>
</citation>
<citation valid="false">
<authors>
<author>C A Brown</author>
</authors>
<title>Parsing extended LR(k) grammars.</title>
<journal>Acta Informatica,</journal>
<pages>15--115</pages>
<contexts>
<context position="12189" citStr="[12, 7]" startWordPosition="2378" endWordPosition="2379">E Pt, where there are D a&apos; A/3, C -&gt;ijO E Pt such that C 0* A 4a [i,k, A 7,m, j][i&apos; , , B 6,m&apos; ,j&apos;] [i, k, A -k 7B, in&apos;, provided m = k&apos; and B --+ 6 E Pt, where there is A -■ a7B,3 E Pt Extended HI parsing The PHI algorithm can process simultaneously a common infix a in two different rules A -&gt; /31a71 and A --, 132a1y2, which reduces nondeterminism. We may however also specify an algorithm which succeeds in simultaneously processing all common infixes, irrespective of whether the left-hand sides of the corresponding rules are the same. This algorithm is inspired by extended LR ( EL R) parsing [12, 7] for extended context-free grammars (where right-hand sides consist of regular expressions over V). By analogy, it will be called extended HI (EHI) parsing. This algorithm uses yet another kind of item, viz. of the form [i, k, {Ai, A2, .,A,,} -&gt; 7,m, j], where there exists at least one rule A a7)3 for each A E {Ai, A2, .,A}. With such an item, we simulate computation of different items [i, k, A -■ a • 7 • )3,m, E Plc which would be treated individually by an HC parser. Formally, we have {[i, --&apos; mci] 0CAC{AIA--a-y0EPt} A i &lt; k &lt; m &lt; j} Algorithm 4 _(Extended HI) A = (T, 1E1&apos;1 , Fin(n)), where </context>
</contexts>
<marker>[12]</marker>
<rawString>P.W. Purdom, Jr. and C.A. Brown. Parsing extended LR(k) grammars. Acta Informatica, 15:115-127,1981.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Satta</author>
</authors>
<title>Head-driven bidirectional parsing: A tabular method.</title>
<date>1989</date>
<booktitle>In International Parsing Workshop &apos;89,</booktitle>
<pages>43--51</pages>
<location>Pittsburgh,</location>
<contexts>
<context position="1433" citStr="[6, 13, 2, 14]" startWordPosition="212" endWordPosition="215">t contentful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of Grammar (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th</context>
<context position="9851" citStr="[6, 13, 2, 14]" startWordPosition="1870" endWordPosition="1873">, D cro7.A)3,m, j][i&apos; , k&apos; , B .6. , m&apos; , j&apos;] [i, k, D av7.A)3,rn, j][i&apos; ,k&apos; ,C , f] provided m = i&apos;, where there is C --■ r1B9 E Pt such that C 0* A (j = j&apos; is automatically satisfied) 4a [i, k, A a • 7 • B m, j][i&apos; , k&apos; , B , 1-4 [i,k, A a .7B • )3, ne, provided m = (m = i&apos; and j = f are automatically satisfied) Head-corner parsing as well as all algorithms in the remainder of this paper may loop exactly for the grammars which are cyclic (where A -,+ A for some A). The head-corner algorithm above is the only one in this paper which has already appeared in the literature, in different guises [6, 13, 2, 14]. Predictive HI parsing We say two rules A al and B a2 have a common infix a if al = /Awn and a2 = )32a72, for some /32, -yi and 72. The notion of common infix is an adaptation of the notion of common prefix [8] to head grammars. If a grammar contains many common infixes, then HC parsing may be very nondeterministic; in particular, Clauses 1 or 3 may be applied with different rules C ?la E Pt or C n138 E PI for fixed a or B. In [15] an idea is described that allows reduction of nondeterminism in case of common prefixes and leftcorner parsing. The resulting algorithm is called predictive LR (PL</context>
</contexts>
<marker>[13]</marker>
<rawString>G. Satta and 0. Stock. Head-driven bidirectional parsing: A tabular method. In International Parsing Workshop &apos;89, pages 43-51, Pittsburgh, 1989.</rawString>
</citation>
<citation valid="false">
<authors>
<author>K Sikkel</author>
<author>R op den Akker</author>
</authors>
<title>Predictive headcorner chart parsing.</title>
<journal>In</journal>
<volume>4</volume>
<pages>267--276</pages>
<contexts>
<context position="1433" citStr="[6, 13, 2, 14]" startWordPosition="212" endWordPosition="215">t contentful either from a syntactic or, more generally, from an information theoretic point of view. This results in the weakening of the left-to-right feature of most traditional parsing methods. Following a pervasive trend in modern theories of Grammar (consider for instance [5, 3, 11]) the computational linguistics community has paid large attention to the head-driven paradigm by investigating its applications to context-free language parsing. Several methods have been proposed so far exploiting some nondeterministic head-driven strategy for context-free language parsing (see among others [6, 13, 2, 14]). All these proposals can be seen as generalizations to the head-driven case of parsing prescriptions originally conceived for the left-to-right case. The methods above suffer from deficiencies that are also noticeable in the left-to-right case. In fact, when more rules in the grammar share the same head element, or share some infix of their right-hand side including the head, the recognizer nondeterministically guesses a rule just after having seen the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in th</context>
<context position="9851" citStr="[6, 13, 2, 14]" startWordPosition="1870" endWordPosition="1873">, D cro7.A)3,m, j][i&apos; , k&apos; , B .6. , m&apos; , j&apos;] [i, k, D av7.A)3,rn, j][i&apos; ,k&apos; ,C , f] provided m = i&apos;, where there is C --■ r1B9 E Pt such that C 0* A (j = j&apos; is automatically satisfied) 4a [i, k, A a • 7 • B m, j][i&apos; , k&apos; , B , 1-4 [i,k, A a .7B • )3, ne, provided m = (m = i&apos; and j = f are automatically satisfied) Head-corner parsing as well as all algorithms in the remainder of this paper may loop exactly for the grammars which are cyclic (where A -,+ A for some A). The head-corner algorithm above is the only one in this paper which has already appeared in the literature, in different guises [6, 13, 2, 14]. Predictive HI parsing We say two rules A al and B a2 have a common infix a if al = /Awn and a2 = )32a72, for some /32, -yi and 72. The notion of common infix is an adaptation of the notion of common prefix [8] to head grammars. If a grammar contains many common infixes, then HC parsing may be very nondeterministic; in particular, Clauses 1 or 3 may be applied with different rules C ?la E Pt or C n138 E PI for fixed a or B. In [15] an idea is described that allows reduction of nondeterminism in case of common prefixes and leftcorner parsing. The resulting algorithm is called predictive LR (PL</context>
</contexts>
<marker>[14]</marker>
<rawString>K. Sikkel and R. op den Akker. Predictive headcorner chart parsing. In [4], pages 267-276.</rawString>
</citation>
<citation valid="false">
<authors>
<author>E Soisalon-Soininen</author>
<author>E Ukkonen</author>
</authors>
<title>A method for transforming grammars into LL(k) form.</title>
<journal>Acta Informatica,</journal>
<pages>12--339</pages>
<contexts>
<context position="2494" citStr="[15, 1]" startWordPosition="377" endWordPosition="378"> the head. In this way analyses that could have been shared are duplicated in the parsing process. Interesting techniques have been proposed in the leftto-right deterministic parsing literature to overcome redundancy problems of the above kind, thus reducing *Supported by the Dutch Organisation for Scientific Research (NWO), under grant 00-62-518 Giorgio Satta Universita di Padova Dipartimento di Elettronica e Informatica via Gradenigo 6/A, 35131 Padova Italy sattadei.unipd.it the degree of nondeterminism of the resulting methods. These solutions range from predictive LR parsing to LR parsing [15, 1]. On the basis of work in [8] for nondeterministic left-to-right parsing, we trace here a theory of head-driven parsing going from crude top-down and head-corner to more sophisticated solutions, in the attempt to successively make more deterministic the behaviour of head-driven methods. Finally, we propose an original generalization of headdriven parsing, allowing a more detailed specification of the order in which elements of a right-hand side are to be processed. We study in detail a solution to such a head-driven strategy based on LR parsing. Other methods presented in this paper could be e</context>
<context position="10287" citStr="[15]" startWordPosition="1964" endWordPosition="1964">here A -,+ A for some A). The head-corner algorithm above is the only one in this paper which has already appeared in the literature, in different guises [6, 13, 2, 14]. Predictive HI parsing We say two rules A al and B a2 have a common infix a if al = /Awn and a2 = )32a72, for some /32, -yi and 72. The notion of common infix is an adaptation of the notion of common prefix [8] to head grammars. If a grammar contains many common infixes, then HC parsing may be very nondeterministic; in particular, Clauses 1 or 3 may be applied with different rules C ?la E Pt or C n138 E PI for fixed a or B. In [15] an idea is described that allows reduction of nondeterminism in case of common prefixes and leftcorner parsing. The resulting algorithm is called predictive LR (PLR) parsing. The following is an adaptation of this idea to HC parsing. The resulting algorithm is called predictive HI (PHI) parsing. (HI parsing, to be discussed later, is a generalization of LR parsing to head grammars.) First, we need a different kind of item, viz. of the form [i, k , A -&gt; 7,m, where there is some rule A -4 a7)3. With such an item, we simulate computation of different items [i, k, A -- a • 7 • f3,m,j] E pic for d</context>
</contexts>
<marker>[15]</marker>
<rawString>E. Soisalon-Soininen and E. Ukkonen. A method for transforming grammars into LL(k) form. Acta Informatica, 12:339-369,1979.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Tomita</author>
</authors>
<title>Parsing 2-dimensional language.</title>
<date>1991</date>
<booktitle>Current Issues in Parsing Technology, chapter 18,</booktitle>
<pages>277--289</pages>
<editor>In M. Tomita, editor,</editor>
<publisher>Kluwer Academic Publishers,</publisher>
<contexts>
<context position="20878" citStr="[16]" startWordPosition="4158" endWordPosition="4158">bols). A transformed grammar Ttwo(G) contains special nonterminals of the form [a], where a is a proper suffix of a rhs in G. The rules of r0(G) are given by A —4 X [a] for each A Xa E P [X a] X [a] for each proper suffix X a of a rhs in G where we assume that each member of the form [c] in the transformed grammar is omitted. HI parsing revisited Our next step is to show that generalized head grammars can be effectively handled with a generalization of HI parsing (generalized HI (GHI) parsing). This new algorithm exhibits a superficial similarity to the 2-dimensional LR parsing algorithm from [16]. For a set Q of trees and rules,&apos; closure(Q) is defined to be 4It is interesting to compare the relation between trees and rules with the one between kernel and nonkernel items of LR parsing [1]. the smallest set which satisfies closure(Q) 2 Q U {A —4 (a)X(f3) E P (7)A(5) E closure(Q) V B (7)A(b) E closure(Q)} The trees or rules of which the main head is some specified symbol X can be selected from a set Q by goto(Q, X) = It EQIt= (a)X (#) V t = A (a)X (/3)} In a similar way, we can select trees and rules according to a left or right subtree. gotoleft(Q , a) = It EQIt= (a)X(#) V t = A (a)X(13</context>
</contexts>
<marker>[16]</marker>
<rawString>M. Tomita. Parsing 2-dimensional language. In M. Tomita, editor, Current Issues in Parsing Technology, chapter 18, pages 277-289. Kluwer Academic Publishers, 1991.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>