<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000040">
<title confidence="0.9984565">
Crowd Prefers the Middle Path: A New IAA Metric for Crowdsourcing
Reveals Turker Biases in Query Segmentation
</title>
<author confidence="0.715617">
Rohan Ramanath∗
</author>
<affiliation confidence="0.3179495">
R. V. College of Engineering
Bangalore, India
</affiliation>
<email confidence="0.959547">
ronramanath@gmail.com
</email>
<author confidence="0.981837">
Kalika Bali
</author>
<affiliation confidence="0.8202875">
Microsoft Research Lab India
Bangalore, India
</affiliation>
<email confidence="0.996912">
kalikab@microsoft.com
</email>
<sectionHeader confidence="0.99389" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999379733333333">
Query segmentation, like text chunking,
is the first step towards query understand-
ing. In this study, we explore the effec-
tiveness of crowdsourcing for this task.
Through carefully designed control ex-
periments and Inter Annotator Agreement
metrics for analysis of experimental data,
we show that crowdsourcing may not be a
suitable approach for query segmentation
because the crowd seems to have a very
strong bias towards dividing the query into
roughly equal (often only two) parts. Sim-
ilarly, in the case of hierarchical or nested
segmentation, turkers have a strong prefer-
ence towards balanced binary trees.
</bodyText>
<sectionHeader confidence="0.998992" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999662866666667">
Text chunking of Natural Language (NL) sentences
is a well studied problem that is an essential pre-
processing step for many NLP applications (Ab-
ney, 1991; Abney, 1995). In the context of Web
search queries, query segmentation is similarly the
first step towards analysis and understanding of
queries (Hagen et al., 2011). The task in both the
cases is to divide the sentence or the query into
contiguous segments or chunks of words such that
the words from a segment are related to each other
more strongly than words from different segments
(Bendersky et al., 2009). It is typically assumed
that the segments are structurally and semantically
coherent and, therefore, the information contained
in them can be processed holistically.
</bodyText>
<footnote confidence="0.974791">
∗The work was done during author’s internship at Mi-
crosoft Research Lab India.
† This author was supported by Microsoft Corporation
and Microsoft Research India under the Microsoft Research
India PhD Fellowship Award.
</footnote>
<note confidence="0.720323666666667">
Monojit Choudhury
Microsoft Research Lab India
Bangalore, India
</note>
<email confidence="0.80626">
monojitc@microsoft.com
</email>
<author confidence="0.980413">
Rishiraj Saha Roy†
</author>
<affiliation confidence="0.9458655">
Indian Institute of Technology Kharagpur
Kharagpur, India
</affiliation>
<email confidence="0.936915">
rishiraj@cse.iitkgp.ernet.in
</email>
<table confidence="0.9182926">
f Pipe representation Boundary var.
4 apply  |first aid course  |on line 1 0 0 1 0
3 apply first aid course  |on line 0 0 0 1 0
2 apply first aid  |course on line 0 0 1 0 0
1 apply  |first aid  |course  |on line 1 0 1 1 0
</table>
<tableCaption confidence="0.947353">
Table 1: Example of flat segmentation by Turkers.
f is the frequency of annotations; segment bound-
aries are represented by |.
</tableCaption>
<table confidence="0.9867155">
f Bracket representation Boundary var.
4 ((apply first) ((aid course) (on line))) 0 2 0 1 0
2 (((apply (first aid)) course) (on line)) 1 0 2 3 0
2 ((apply ((first aid) course)) (on line)) 2 0 1 3 0
1 (apply (((first aid) course) (on line))) 3 0 1 2 0
1 ((apply (first aid)) (course (on line))) 1 0 2 1 0
</table>
<tableCaption confidence="0.979593">
Table 2: Example of nested segmentation by Turk-
</tableCaption>
<bodyText confidence="0.9737125">
ers. f is the frequency of annotations.
A majority of work on query segmentation re-
lies on manually segmented queries by human ex-
perts for training and evaluation of segmentation
algorithms. These are typically small datasets and
even with detailed annotation guidelines and/or
close supervision, low Inter Annotator Agreement
(IAA) remains an issue. For instance, Table 1 il-
lustrates the variation in flat segmentation by 10
annotators. This confusion is mainly because the
definition of a segment in a query is ambiguous
and of an unspecified granularity. This is fur-
ther compounded by the fact that other than eas-
ily recognizable and agreed upon segments such as
Named Entities or Multi-Word Expressions, there
is no established notion of linguistic grouping such
as phrases and clauses in a query.
Although there is little work on the use of
crowdsourcing for query segmentation (Hagen et
al., 2011; Hagen et al., 2012), the idea that the
</bodyText>
<page confidence="0.854824">
1713
</page>
<note confidence="0.91356">
Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1713–1722,
Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics
</note>
<bodyText confidence="0.99993236">
crowd could be a potential (and cheaper) source
for reliable segmentation seems a reasonable as-
sumption. The need for larger datasets makes this
an attractive proposition. Also, a larger number
of annotations could be appropriately distilled to
obtain better quality segmentations.
In this paper we explore crowdsourcing as an
option for query segmentation through experi-
ments designed using Amazon Mechanical Turk
(AMT)1. We compare the results against gold
datasets created by trained annotators. We ad-
dress the issues pertaining to disagreements due to
both ambiguity and granularity and attempt to ob-
jectively quantify their role in IAA. To this end,
we also conduct similar annotation experiments
for NL sentences and randomly generated queries.
While queries are not as structured as NL sen-
tences they are not simply a set of random words.
Thus, it is necessary to compare query segmenta-
tion to the ¨uber-structure of NL sentences as well
as the unter-structure of random n-grams. This has
important implications for understanding any in-
herent biases annotators may have as a result of
the apparent lack of structure of the queries.
To quantify the effect of granularity on segmen-
tation, we also ask annotators to provide hierar-
chical or nested segmentations for real and ran-
dom queries, as well as sentences. Following
Abney’s (1992) proposal for hierarchical chunk-
ing of NL, we ask the annotators to group ex-
actly two words or segments at a time to recur-
sively form bigger segments. The concept is illus-
trated in Fig. 1. Table 2 shows annotations from
10 Turkers. It is important to constrain the join-
ing of exactly two segments or words at a time
to avoid the issue of fuzziness in granularity. We
shall refer to this style of annotation as Nested
segmentation, whereas the non-hierarchical non-
constrained chunking will be referred to as Flat
segmentation.
Through statistical analysis of the experimen-
tal data we show that crowdsourcing may not be
the best practice for query segmentation, not only
because of ambiguity and granularity issues, but
because there exist very strong biases amongst an-
notators to divide a query into two roughly equal
parts that result in misleadingly high agreements.
As a part of our analysis framework, we introduce
a new IAA metric for comparison across flat and
nested segmentations. This versatile metric can be
</bodyText>
<footnote confidence="0.987731">
1https://www.mturk.com/mturk/welcome
</footnote>
<figureCaption confidence="0.999924">
Figure 1: Nested Segmentation: Illustration.
</figureCaption>
<bodyText confidence="0.9996418125">
readily adapted for measuring IAA for other lin-
guistic annotation tasks, especially when done us-
ing crowdsourcing.
The rest of the paper is organized as follows.
Sec 2 provides a brief overview of related work.
Sec 3 describes the experiment design and proce-
dure. In Sec 4, we introduce a new metric for IAA,
that could be uniformly applied across flat and
nested segmentations. Results of the annotation
experiments are reported in Sec 5. In Sec 6, we an-
alyze the possible statistical and linguistic biases
in annotation. Sec 7 concludes the paper by sum-
marizing the work and discussing future research
directions. All the annotated datasets used in this
research are freely available for non-commercial
research purposes2.
</bodyText>
<sectionHeader confidence="0.999692" genericHeader="related work">
2 Related Work
</sectionHeader>
<bodyText confidence="0.99940465">
Query segmentation was introduced by Risvik et.
al. (2003) as apossible means to improve Informa-
tion Retrieval. Since then there has been a signif-
icant amount of research exploring various algo-
rithms for this task and its use in IR (see Hagen et.
al. (2011) for a survey). Most of the research and
evaluation considers query segmentation as a pro-
cess analogous to identification of phrases within
a query which when put within double-quotes (im-
plying exact matching of the quoted phrase in the
document) leads to better IR performance. How-
ever, this is a very restricted view of the process
and does not take into account the full potential of
query segmentation.
A more generic notion of segments leads to di-
verse and ambiguous definitions, making its eval-
uation a hard problem (see Saha Roy et. al. (2012)
for a discussion on issues with evaluation). Most
automatic segmentation techniques (Bergsma and
Wang, 2007; Tan and Peng, 2008; Zhang et al.,
</bodyText>
<footnote confidence="0.999595666666667">
2Related datasets and supplementary material can be ac-
cessed from http://bit.ly/161Gkk9 or can be ob-
tained by directly emailing the authors.
</footnote>
<page confidence="0.834141">
2
</page>
<figure confidence="0.999189714285714">
0
3
on line
1
apply 0
first aid
course
</figure>
<page confidence="0.995131">
1714
</page>
<bodyText confidence="0.99991144">
2009; Brenes et al., 2010; Hagen et al., 2011; Li et
al., 2011) have so far been evaluated only against
a small set of human-annotated queries (Bergsma
and Wang, 2007). The reported low IAA for such
datasets casts serious doubts on the reliability of
annotation and the performance of the algorithms
evaluated on them (Hagen et al., 2011; Saha Roy
et al., 2012).
To address the problem of data scarcity, Ha-
gen et. al. (2011) have created larger annotated
datasets through crowdsourcing3. However, in
their approach the crowd is provided with a few
(four) possible segmentations of a query to choose
from (known through a personal communication
with a authors). Thus, it presupposes an automatic
process that can generate the correct segmentation
of a query within top few options. It is far from
obvious how to generate these initial segmenta-
tions in a reliable manner. This may also result
in an over-optimistic IAA. An ideal segmentation
should be based on the annotators’ own interpreta-
tion of the query. Nevertheless, if large scale data
has to be procured, crowdsourcing seems to be the
only efficient and effective model for this task, and
has been proven to be so for other IR and linguistic
annotations; see Carvalho et al. (2011) for exam-
ples of crowdsourcing for IR resources and (Snow
et al., 2008; Callison-Burch, 2009) for language
resources.
In the context of NL text, segmentation has
been traditionally referred to as chunking and is
a well-studied problem. Abney (1991; 1992;
1995) defines a chunk as a sub-tree within a
syntactic phrase structure tree corresponding to
Noun, Prepositional, Adjectival, Adverbial and
Verb Phrases. Similarly, Bharati et al (1995) de-
fines it as Noun Group and Verb Group based only
on local surface information. However, cognitive
and annotation experiments for chunking of En-
glish (Abney, 1992) and other language text (Bali
et al., 2009) have shown that native speakers agree
on major clause and phrase boundaries, but may
not do so on more fine-grained chunks. One im-
portant implication of this is that annotators are
expected to agree more on the higher level bound-
aries for nested segmentation than the lower ones.
We note that hierarchical query segmentation was
proposed for the first time by Huang et al. (2010),
where the authors recursively split a query (or its
fragment) into exactly two parts and evaluate the
</bodyText>
<footnote confidence="0.796526">
3http://www.webis.de/research/corpora
</footnote>
<bodyText confidence="0.505402">
final output against human annotations.
</bodyText>
<sectionHeader confidence="0.996208" genericHeader="method">
3 Experiments
</sectionHeader>
<bodyText confidence="0.999993057142857">
The annotation experiments have been designed to
systematically study the various aspects of query
segmentation. In order to verify the effective-
ness and reliability of crowdsourcing, we designed
an AMT experiment for flat segmentation of Web
search queries. As a baseline, we would like to
compare these annotations with those from hu-
man experts trained for the task. We shall refer
to this baseline as the Gold annotation set. Since
we believe that the issue of granularity could be
the prime reason for previously reported low IAA
for segmentation, we also designed AMT-based
nested segmentation experiments for the same set
of queries, and obtained the corresponding gold
annotations.
Finally, to estimate the role of ambiguity inher-
ent in the structure of Web search queries on IAA,
we conducted two more control experiments, both
through crowdsourcing. First, flat and nested seg-
mentation of well-formed English, i.e., NL sen-
tences of similar length distribution; and second,
flat and nested segmentation of randomly gener-
ated queries. Higher IAA for NL sentences would
lead us to conclude that ambiguity and lack of
structure in queries is the main reason for low
agreements. On the other hand high or comparable
IAA for random queries would mean that annota-
tions have strong biases.
Thus, we have the following four pairs of anno-
tation experiments: flat and nested segmentation
of queries from crowdsourcing, corresponding flat
and nested gold annotations, flat and nested seg-
mentation of English sentences from crowdsourc-
ing, and flat and nested segmentations for ran-
domly generated queries through crowdsourcing.
</bodyText>
<subsectionHeader confidence="0.992928">
3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999855181818182">
For our experiments, we need a set of Web search
queries and well-formed English sentences. Fur-
thermore, for generating the random queries, we
will use search query logs to learn n-gram mod-
els. In particular, we use the following datasets:
Q500, QG500: Saha Roy et al. (2012) re-
leased a dataset of 500 queries, 5 to 8 words long,
for evaluation of various segmentation algorithms.
This dataset has flat segmentations from three an-
notators obtained under controlled experimental
settings, and can be considered as Gold annota-
</bodyText>
<page confidence="0.989483">
1715
</page>
<figureCaption confidence="0.999192">
Figure 2: Length distribution of datasets.
</figureCaption>
<bodyText confidence="0.999413277777778">
tions. Hence, we select this set for our experiments
as well. We procured the corresponding nested
segmentation for these queries from two human
experts, who are regular search engine users, be-
tween 20 and 30 years old, and familiar with var-
ious linguistic annotation tasks. They annotated
the data under supervision. They were trained and
paid for the task. We shall refer to the set of flat
and nested gold annotations as QG500, whereas
Q500 will be reserved for AMT experiments.
Q700: Since 500 queries may not be enough
for reliable conclusion and since the queries may
not have been chosen specifically for the purpose
of annotation experiments, we expanded the set
with another 700 queries sampled from a slice of
the query logs of Bing Australia4 containing 16.7
million queries issued over a period of one month
(May 2010). We picked, uniformly at random,
queries that are 4 to 8 words long, have only En-
glish letters and numerals, and a high click entropy
because “a query with a larger click entropy value
is more likely to be an informational or ambiguous
query” (Dou et al., 2008). Q500 consists of tail-
ish queries with frequency between 5 and 15 that
have at least one multiword named entity; but un-
like the case of Q700, click-entropy was not con-
sidered during sampling. As we shall see, this dif-
ference is clearly reflected in the results.
S300: We randomly selected 300 English sen-
tences from a collection of full texts of public do-
main books5 that were 5 to 15 words long, and
checked them for well-formedness. This set will
be referred to as S300.
QRand: Instead of generating search queries
by throwing in words randomly, we thought it
will be more interesting to explore annotation of
</bodyText>
<footnote confidence="0.9999465">
4http://www.bing.com/?cc=au
5http://www.gutenberg.org
</footnote>
<table confidence="0.963974428571429">
Parameter Flat Details Nested Details
Time needed: actual (allotted) 49 sec (10 min) 1 min 52 sec (15 min)
Reward per HIT $0.02 $0.06
Instruction video duration 26 sec 1 min 40 sec
Turker qualification Completion rate &gt;100 tasks
Turker approval rate Acceptance rate &gt;60 %
Turker location United States of America
</table>
<tableCaption confidence="0.999777">
Table 3: Specifics of the HITs for AMT.
</tableCaption>
<bodyText confidence="0.998143111111111">
queries generated using n-gram models for n =
1, 2, 3. We estimated the models from the Bing
Australia log of 16.7 million queries. We gener-
ated 250 queries each of desired length distribu-
tion using the 1, 2 and 3-gram models. We shall
refer to these as U250, B250, T250 (for Uni, Bi
and Trigram) respectively, and the whole dataset
as QRand. Fig. 2 shows the query and sentence
length distribution for the various sets.
</bodyText>
<subsectionHeader confidence="0.999803">
3.2 Crowdsourcing Experiments
</subsectionHeader>
<bodyText confidence="0.9999615">
We used AMT to get our annotations through
crowdsourcing. Pilot experiments were carried out
to test the instruction set and examples presented.
Based on the feedback, the precise instructions for
the final experiments were designed.
Two separate AMT Human Intelligence Tasks
(HITs) were designed for flat and nested query
segmentation. Also, the experiments for queries
(Q500+Q700) were conducted separately from
S300 and QRand. Thus, we had six HITs in
all. The concept of flat and nested segmentation
was introduced to the Turkers with the help of ex-
amples presented in two short videos6. When in
doubt regarding the meaning of a query, the Turk-
ers were advised to issue the query on a search
engine of their choice and find out its possible
interpretation(s). Note that we intentionally kept
definitions of flat and nested segmentation fuzzy
because (a) it would require very long instruction
manuals to cover all possible cases and (b) Turkers
do not tend to read verbose and complex instruc-
tions. Table 3 summarizes other specifics of HITs.
Honey pots or trap questions whose answers are
known a priori are often included in a HIT to iden-
tify turkers who are unable to solve the task ap-
propriately leading to incorrect annotations. How-
ever, this trick cannot be employed in our case be-
cause there is no notion of an absolutely correct
segmentation. We observe that even with unam-
biguous queries, even expert annotators may dis-
</bodyText>
<footnote confidence="0.998242">
6Flat: http://youtu.be/eMeLjJIvIh0, Nested:
http://youtu.be/xE3rwANbFvU
</footnote>
<page confidence="0.987859">
1716
</page>
<bodyText confidence="0.99976775">
agree on some of the segment boundaries. Hence,
we decided to include annotations from all the
turkers, except for those that were syntactically ill-
formed (e.g., non-binary nested segmentation).
</bodyText>
<sectionHeader confidence="0.997749" genericHeader="method">
4 Inter Annotator Agreement
</sectionHeader>
<bodyText confidence="0.991406239130435">
Inter Annotator Agreement is the only way to
judge the reliability of annotated data in absence
of an end application. Therefore, before we can
venture into analysis of the experimental data, we
need to formalize the notion of IAA for flat and
nested queries. The task is non-trivial for two
reasons. First, traditional IAA measures are de-
fined for a fixed set of annotators. However, for
crowdsourcing based annotations, different anno-
tators might have annotated different parts of the
dataset. For instance, we observed that a total
of 128 turkers have provided the flat annotations
for Q700, when we had only asked for 10 anno-
tations per query. Thus, on average, a turker has
annotated only 7.81% of the 700 queries. In fact,
we found that 31 turkers had annotated less than
5 queries. Hence, measures such as Cohen’s κ
(1960) cannot be directly applied in this context
because for crowdsourced annotations, we cannot
meaningfully compute annotator-specific distribu-
tion of the labels and biases.
Second, most of the standard annotation metrics
do not generalize for flat segmentation and trees.
Artstein and Poesio (2008) provides a comprehen-
sive survey of the IAA metrics and their usage in
NLP. They note that all the metrics assume that
a fixed set of labels are used for items. There-
fore, it is far from obvious how to compare chunk-
ing or segmentation that covers the whole text or
that might have overlapping units as in the case of
nested segmentation. Furthermore, we would like
to compare the reliability of flat and nested seg-
mentation, and therefore, ideally we would like to
have an IAA metric that can be meaningfully ap-
plied to both of these cases.
After considering various measures, we decided
to appropriately generalize one of the most versa-
tile and effective IAA metrics proposed till date,
the Kripendorff’s α (2004). To be consistent with
prior work, we will stick to the notation used
in Artstein and Poesio (2008) and redefine the
α in the context of flat and nested segmentation.
Note that though the notations introduced here will
be from the perspective of queries, it is equally
applicable to sentences and the generalization is
straightforward.
</bodyText>
<subsectionHeader confidence="0.964668">
4.1 Notations and Definitions
</subsectionHeader>
<bodyText confidence="0.999784461538462">
Let Q be the set of all queries with cardinality q.
A query q E Q can be represented as a sequence of
JqJ words: w1w2 ... w|q|. We introduce Jq−1J ran-
dom variables, b1, b2,... b|q|−1, such that bz rep-
resents the boundary between the words wz and
wz+1. A flat or nested segmentation of q, repre-
sented by qj, j varying from 1 to total number of
annotations c, is a particular instantiation of these
boundary variables as described below.
Definition. A flat segmentation, qj can be
uniquely defined by a binary assignment of the
boundary variables bj,z, where bj,z = 1 iff wz and
wz+1 belong to two different flat segments. Oth-
erwise, bj,z = 0. Thus, q has 2|q|−1 possible flat
segmentations.
Definition. A nested segmentation qj can also
be uniquely defined by assigning non-negative in-
tegers to the boundary variables such that bj,z = 0
iff words wz and wz+1 form an atomic segment
(i.e., they are grouped together), else bj,z = 1 +
max(leftz, rightz), where leftz and rightz are
the heights of the largest subtrees ending at wz and
beginning at wz+1 respectively.
This numbering scheme for nested segmenta-
tion can be understood through Fig. 1. Every in-
ternal node of the binary tree corresponding to the
nested segmentation is numbered according to its
height. The lowest internal nodes, both of whose
children are query words, are assigned a value of
0. Other internal nodes get a value of one greater
than the height of its higher child. Since every in-
ternal node corresponds to a boundary, we assign
the height of the node to the corresponding bound-
aries. The number of unique nested segmentations
of a query of length JqJ is its corresponding Cata-
lan number7.
Boundary variables for flat and nested segmen-
tation are illustrated with an example of each kind
in Tables 1 and 2 (last column).
</bodyText>
<subsectionHeader confidence="0.972258">
4.2 Krippendorff ’s α for Segmentation
</subsectionHeader>
<bodyText confidence="0.999661">
Krippendorff ’s α (Krippendorff, 2004) is an ex-
tremely versatile agreement coefficient, which is
based on the assumption that the expected agree-
ment is calculated by looking at the overall distri-
bution ofjudgments without regard to which anno-
tator produced them (Artstein and Poesio, 2008).
</bodyText>
<footnote confidence="0.943358">
7http://goo.gl/vKQvK
</footnote>
<page confidence="0.987996">
1717
</page>
<bodyText confidence="0.9837925">
Hence, it is appropriate for crowdsourced annota-
tion, where the judgments come from a large num-
ber of unrelated annotators. Moreover, it allows
for different magnitudes of disagreement, which
is a useful feature as we might want to differen-
tially penalize disagreements at various levels of
the tree for nested segmentation.
α is defined as
</bodyText>
<equation confidence="0.868071666666667">
(1)
2
stotal
</equation>
<bodyText confidence="0.9997335">
where Do and De are, respectively, the observed
and expected disagreements that are measured by
</bodyText>
<equation confidence="0.473838">
2
swithin
</equation>
<bodyText confidence="0.99827625">
item and s2total – variance across annotations of
all items. We adapt the equations presented in
pp.565-566 of Artstein and Poesio (2008) for mea-
suring these quantities for queries:
</bodyText>
<equation confidence="0.998727">
1 X
2qc(c − 1) q∈Q
2 1
stotal = 2qc(qc − 1) q∈Q
</equation>
<bodyText confidence="0.999945833333333">
where, d(qm, q′n) is a distance metric for the agree-
ment between annotations qm and q′n.
We define two different distance metrics d1 and
d2 that are applicable to flat and nested segmenta-
tion. We shall first define these metrics for com-
paring queries with equal length (i.e., |q |= |q′|):
</bodyText>
<equation confidence="0.99651425">
|q|−1
1 X
d1(qm, q′ n) = |bm,i − b′ n,i |(4)
|q |− 1
X
|b2m,i − (b′ n,i)2 |(5)
|q |− 1
i=1
</equation>
<bodyText confidence="0.997243857142857">
While d1 penalizes all disagreements equally, d2
penalizes disagreements higher up the tree more.
d2 might be a desirable metric for nested seg-
mentation, because research on sentence chunk-
ing shows that annotators agree more on clause or
major phrase boundaries, even though they may
not always agree on intra-clausal or intra-phrasal
boundaries (Bali et al., 2009). Note that for flat
segmentation, d1 and d2 are identical, and hence
we will denote them as d.
We propose the following extension to these
metrics for queries of unequal lengths. Without
loss of generality, let us assume that |q |&lt; |q′|. k
is 1 or 2; r = |q′ |− |q |+ 1.
</bodyText>
<equation confidence="0.983312">
1
dk(qm, q′ n) = r(|q |− 1)
</equation>
<subsectionHeader confidence="0.812424">
4.3 IAA under Random Bias Assumption
</subsectionHeader>
<bodyText confidence="0.994542125">
Krippendorff’s α uses the cross-item variance as
an estimate of chance agreement, which is reli-
able in general. However, this might result in mis-
leadingly low values of IAA, especially when the
items in the set are indeed expected to have sim-
ilar annotations. To resolve this, we also com-
pute the chance agreement under a random bias
model. The random model assumes that all the
structural annotations of q are equiprobable. For
flat segmentation, it boils down to the fact that
all the 2|q|−1 annotations are equally likely, which
is equivalent to the assumption that any boundary
variable bi has 0.5 probability of being 0 and 0.5
for 1.
Analytical computation of the expected proba-
bility distributions of d1(qm, qn) and d2(qm, qn)
is harder for nested segmentation. Therefore, we
programmatically generate all possible trees for q,
which is again dependent only on |q |and com-
pute d1 and d2 between all pairs of trees, from
which the expected distributions can be readily
estimated. Let us denote this expected cumula-
tive probability distribution for flat segmentation
as Pd(x; |q|) = the probability that for a pair
of randomly chosen flat segmentations of q, qm
and qn, d(qm, qn) ≥ x. Likewise, let Pd1(x; |q|)
and Pd2(x; |q|) be the respective probabilities that
for any two nested segmentations qm and qn of
q, the following holds: d1(qm,qn) ≥ x and
d2(qm, qn) ≥ x.
We define the IAA under random bias model as
(k is 1, 2 or null):
</bodyText>
<equation confidence="0.997944">
1 X S = qc2
</equation>
<bodyText confidence="0.986887444444445">
Thus, S is the expected probability of observing a
similar or worse agreement by random chance, av-
eraged over all pairs of annotations for all queries,
and not a chance corrected IAA metric such as
α. Thus, S = 1 implies that the observed agree-
ment is almost always better than that by random
chance and S = 0.5 and 0 respectively imply that
the observed agreement is as good as and almost
always worse than that by random chance. We
</bodyText>
<equation confidence="0.753696421052631">
Do
= 1
De
α = 1 −
s2
within
– the variance within the annotation of an
2
swithin =
d(qm,qn)
(2)
XC
n=1
XC
m=1
d(qm, q′n)
(3)
XC
n=1
XC
m=1
X
q′∈Q
i =1
|q|−1
1
d2(qm,q′n) =
|bkm,i − (b′n,i+a)k |(6)
r−1�
a=0
|q|−1
i=1
q∈Q
Pdk(dk(qm, qn); |q|) (7)
XC
n=1
XC
m=1
</equation>
<page confidence="0.967908">
1718
</page>
<table confidence="0.999851555555555">
Dataset Flat Nested
d1 d1 d2
Q700 0.21(0.59) 0.21(0.89) 0.16(0.68)
Q500 0.22(0.62) 0.15(0.70) 0.15(0.44)
QG500 0.61(0.88) 0.66(0.88) 0.67(0.80)
S300 0.27(0.74) 0.18(0.94) 0.14(0.75)
U250 0.23(0.89) 0.42(0.90) 0.30(0.78)
B250 0.22(0.86) 0.34(0.88) 0.22(0.71)
T250 0.20(0.86) 0.44(0.89) 0.34(0.76)
</table>
<tableCaption confidence="0.999394">
Table 4: Agreement Statistics: α(5).
</tableCaption>
<bodyText confidence="0.999902181818182">
also note that a high value of 5 and low value
of α indicate that though the annotators agree on
the judgment of individual items, they also tend to
agree on judgments of two different items, which
in turn, could be due to strong annotator biases or
due to lack of variability of the dataset.
In the supplementary material, computations of
α and 5 have been explained in further details
through worked out examples. Tables for the ex-
pected distributions of d, d1 and d2 under the ran-
dom annotation assumption are also available.
</bodyText>
<sectionHeader confidence="0.999812" genericHeader="method">
5 Results
</sectionHeader>
<bodyText confidence="0.999971268292683">
Table 4 reports the values of α and 5 for flat
and nested segmentation on the various datasets.
For nested segmentation, the values were com-
puted for two different distance metrics d1 and
d2. As expected, the highest value of α for both
flat and nested segmentation is observed for gold
annotations. An α &gt; 0.6 indicates quite good
IAA, and thus, reliable annotations. Higher α for
nested segmentation QG500 than flat further vali-
dates our initial postulate that nested segmentation
may reduce disagreement from granularity issues
inherent in the definition of flat segmentation.
Opposite trends are observed for Q700, Q500
and S300, where α for flat is the highest, followed
by that for nested using d1, and then d2. More-
over, except for flat segmentation of sentences, α
lies between 0.14 and 0.22, which is quite low.
This clearly shows that segmentation, either flat
or nested, cannot be reliably procured through
crowdsourcing. Lower α for d2 than d1 further
indicates that annotators disagree more for higher
levels of the trees, contrary to what we had ex-
pected. However, nearly equal IAA for sentences
and queries implies that low agreement may not be
an outcome of inherent ambiguity in the structure
of queries. Slightly higher α for flat segmentation
and a much higher α for nested segmentation of
QRand reinforce the fact that low IAA is not due
to a lack of structure in queries.
It is interesting to note that α for nested segmen-
tation of S300 and all segmentations of QRand
are low or medium despite the fact that 5 is very
high in all these cases. Thus, it is clear that an-
notators have a strong bias towards certain struc-
tures across queries. In the next section, we will
analyze some of these biases. We also computed
the IAA between QG500 and Q500, and found
α = 0.27. This is much lower than α for QG500,
though slightly higher than that for Q500. We did
not observe any significant variation in agreement
with respect to the length of the queries.
</bodyText>
<sectionHeader confidence="0.982707" genericHeader="method">
6 Biases in Annotation
</sectionHeader>
<bodyText confidence="0.998941454545455">
The IAA statistics clearly show that there are cer-
tain strong biases in both flat and nested query
segmentation, especially those obtained through
crowdsourcing. To identify these biases, we went
through the annotations and came up with possi-
ble hypotheses, which we tried to verify through
statistical analysis of the data. Here, we report the
most prominent biases that were thus discovered.
Bias 1: During flat segmentation, annotators pre-
fer dividing the query into two segments ofroughly
equal length.
As discussed earlier, one of the major problems
of flat segmentation is the fuzziness in granularity.
In our experiments, we intentionally left the de-
cision of whether to go for fine or coarse-grained
segmentation to the annotator. However, it is sur-
prising to observe that annotators typically divide
the query into two segments (see Fig. 3, plots A1
and A2), and at times three, but hardly ever more
than three. This bias is observed across queries,
sentences and random queries, where the percent-
age of annotations with 2 or 3 segments are greater
than 83%, 91% and 96% respectively. This bias
is most strongly visible for QRand because the
lack of syntactic or semantic cohesion between the
words provides no clue for segmentation.
Furthermore, we observe that typically seg-
ments tend to be of equal length. For this, we com-
puted standard deviations (sd) of segment lengths
for all annotations having 2 or 3 segments; the dis-
tribution of sd is shown in Fig. 3, plots B1 and B2.
We observe that for all datasets, sd lies mainly be-
tween 0.5 and 1 (for perspective, consider a query
</bodyText>
<page confidence="0.997094">
1719
</page>
<figureCaption confidence="0.987054">
Figure 3: Analysis of annotation biases: A1, A2 – number of segments per flat segmentation vs. length;
B1, B2 – standard deviation of segment length for flat segmentation; C1, C2 – distribution of the tree
heights in nested segmentation.
</figureCaption>
<table confidence="0.9824918">
Length Expected Q500 QG500 Q700 S300 QRand
5 2.57 2.00 2.02 2.08 2.02 2.01
6 3.24 2.26 2.23 2.23 2.24 2.02
7 3.88 2.70 2.71 2.67 2.55 2.62
8 4.47 2.89 2.68 2.72 2.72 2.35
</table>
<tableCaption confidence="0.999792">
Table 5: Average height for nested segmentation.
</tableCaption>
<bodyText confidence="0.995153490909091">
with 7 words; with two segments of length 3 and
4 the sd is 0.5, and for 2 and 5, the sd is 1.5), im-
plying that segments are roughly of equal length.
It is likely that due to this bias, the 5 or observed
agreement is moderately high for queries and very
high for sentences, but then it also leads to high
agreement across different queries and sentences
(i.e., high s2total) especially when they are of equal
length, which in turn brings down the value of α –
the true agreement after bias correction.
Bias 2: During nested segmentation, annotators
prefer balanced binary trees.
Quite analogous to bias 1, for nested segmen-
tation we observe that annotators tend to prefer
more balanced binary trees. Fig. 3 plots C1 and C2
show the distribution of the tree heights for various
cases and Table 5 reports the corresponding aver-
age height of the trees for queries and sentences
of various lengths and the the expected value of
the height if all trees were equally likely. The ob-
served heights are much lower than the expected
values clearly implying the preference of the an-
notators for more balanced trees.
Thus, the crowd seems to choose the middle
path, avoiding extremes and hence may not be a
reliable source of annotation for query segmen-
tation. It can be argued that similar biases are
also observed for gold annotations, and therefore,
probably it is the inherent structure of the queries
and sentences that lead to such biased distribution
of segmentation patterns. However, note that α for
QG500 is much higher than all other cases, which
shows that the true agreement between gold anno-
tators is immune to such biases or skewed distri-
butions in the datasets. Furthermore, high values
of α for QRand despite the very strong biases in
annotation shows that there perhaps is very little
choice that the annotators have while segmenting
randomly generated queries. On the other hand,
the textual coherence of the real queries and sen-
tences provide many different choices for segmen-
tation and the Turker typically gets carried away
by these biases, leading to low α.
Bias 3: Phrase structure drives segmentation only
when reconcilable with Bias 1. Whenever the sen-
tence or query has a verb phrase (VP) spanning
roughly half of it, annotators seem to chunk be-
fore the VP as one would expect, quite as of-
ten as just after the verb, which is quite unex-
pected. For instance, the sentence A gentle
sarcasm ruffled her anger. gathers as
many as eight flat annotations with a boundary be-
tween sarcasm and ruffled, and four with
a boundary between ruffled and her. How-
ever, if the VP is very short consisting of a single
</bodyText>
<page confidence="0.947091">
1720
</page>
<table confidence="0.9996032">
Position Q500 QG500 Q700 S300 QRand
Both 2.24 0.37 2.78 2.08 0.63
None 50.34 56.85 35.74 35.84 39.81
Right 23.86 21.50 19.02 12.52 15.23
Left 18.08 15.97 40.59 45.96 21.21
</table>
<tableCaption confidence="0.876851">
Table 6: Percentages of positions of segment
</tableCaption>
<bodyText confidence="0.996234232558139">
boundaries with respect to prepositions. Prepo-
sitions occurring in the beginning or end of a
query/sentence have been excluded from the anal-
ysis; hence, numbers in a column do not total 100.
verb, as in A fleeting and furtive air
of triumph erupted., annotators seem to
attempt for a balanced annotation due to Bias 1.
As a clear middle boundary is not present in such
sentences, the annotations show a lot more varia-
tion and disagreement. For instance, only 1 out of
10 annotations had a boundary before erupted
in the above example. In fact, at least one anno-
tation had a boundary after each word in the sen-
tence, with no clear majority.
Bias 4: Prepositions influence segment bound-
aries differently for queries and sentences. We
automatically labeled all the prepositions in the
flat annotations and classified them according to
the criterion of whether a boundary was placed
immediately before or after it, or on both sides
or neither side. The statistics, reported in Ta-
ble 6, show that for NL sentences a majority
of the boundaries are present before the prepo-
sition, marking the beginning of a prepositional
phrase. However, for queries, a much richer pat-
tern emerges depending on the specific preposi-
tion. For instance, to, of and for are often
chunked with the previous word (e.g., how to |
choose a bike size, birthday party
ideas for  |one year old). We believe
that this difference is because in sentences due
to the presence of a verb, the PP has a well-
defined head, lack of which leads to preposition
in queries getting chunked with words that form
more commonly seen patterns (e.g., flights
to and tickets for).
Bias 3 and 4 present the complex interpretation
of the structure of queries by the annotators which
could be due to some emerging cognitive model of
queries among the search engine users. This is a
fascinating and unexplored aspect of query struc-
tures that demands deeper investigation through
cognitive and psycholinguistic experiments.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999993682926829">
We have studied various aspects of query segmen-
tation through crowdsourcing by designing and
conducting suitable experiments. Analysis of ex-
perimental data leads us to conclude the follow-
ing: (a) crowdsoucing may not be a very effective
way to collect judgments for query segmentation;
(b) addressing fuzziness of granularity for flat seg-
mentation by introducing strict binary nested seg-
ments does not lead to better agreement in crowd-
sourced annotations, though it definitely improves
the IAA for gold standard segmentations, imply-
ing that low IAA in flat segmentation among ex-
perts is primarily an effect of unspecified granular-
ity of segments; (c) low IAA is not due to the in-
herent structural ambiguity in queries as this holds
true for sentences as well; (d) there are strong bi-
ases in crowdsourced annotations, mostly because
turkers prefer more balanced segment structures;
and (e) while annotators are by and large guided
by linguistic principles, application of these prin-
ciples differ between query and NL sentences and
also closely interact with other biases.
One of the important contributions of this work
is the formulation of a new IAA metric for com-
paring across flat and nested segmentations, espe-
cially for crowdsourcing based annotations. Since
trees are commonly used across various linguistic
annotations, this metric can have wide applicabil-
ity. The metric, moreover, can be easily adapted
to other annotation schemes as well by defining an
appropriate distance metric between annotations.
Since large scale data for query segmentation is
very useful, it would be interesting to see if the
problem can be rephrased to the Turkers in a way
so as to obtain more reliable judgments. Yet a
deeper question is regarding the theoretical status
of query structure, which though in an emergent
state is definitely an operating model for the anno-
tators. Our future work in this area would specifi-
cally target understanding and formalization of the
theoretical model underpinning a query.
</bodyText>
<sectionHeader confidence="0.999441" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<reference confidence="0.5570382">
We thank Ed Cutrell and Andrew Cross, Microsoft
Research Lab India, for their help in setting up the
AMT experiments. We would also like to thank
Anusha Suresh, IIT Kharagpur, India, for helping
us with data preparation.
</reference>
<page confidence="0.992952">
1721
</page>
<sectionHeader confidence="0.995747" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999936906542056">
Steven P. Abney. 1991. Parsing By Chunks. Kluwer
Academic Publishers.
Steven P. Abney. 1992. Prosodic Structure, Perfor-
mance Structure And Phrase Structure. In Proceed-
ings 5th DARPA Workshop on Speech and Natural
Language, pages 425–428. Morgan Kaufmann.
Steven P. Abney. 1995. Chunks and dependencies:
Bringing processing evidence to bear on syntax.
Computational Linguistics and the Foundations of
Linguistic Theory, pages 145–164.
Ron Artstein and Massimo Poesio. 2008. Inter-coder
agreement for computational linguistics. Computa-
tional Linguistics, 34(4):555–596.
Kalika Bali, Monojit Choudhury, Diptesh Chatterjee,
Sankalan Prasad, and Arpit Maheswari. 2009. Cor-
relates between Performance, Prosodic and Phrase
Structures in Bangla and Hindi: Insights from a Psy-
cholinguistic Experiment. In Proceedings of Inter-
national Conference on Natural Language Process-
ing, pages 101 – 110.
Michael Bendersky, W. B. Croft, and David A. Smith.
2009. Two-stage query segmentation for informa-
tion retrieval. In Proceedings of the 32nd interna-
tional ACM Special Interest Group on Information
Retrieval (SIGIR) Conference on Research and De-
velopment in Information Retrieval, pages 810–811.
ACM.
Shane Bergsma and Qin Iris Wang. 2007. Learning
Noun Phrase Query Segmentation. In Proceedings
ofEmpirical Methods in Natural Language Process-
ing and Computational Natural Language Learning
(EMNLP-CoNLL), pages 819–826.
Akshar Bharati, Vineet Chaitanya, Rajeev Sangal, and
KV Ramakrishnamacharyulu. 1995. Natural lan-
guage processing: a Paninian perspective. Prentice-
Hall of India New Delhi.
David J. Brenes, Daniel Gayo-Avello, and Rodrigo
Garcia. 2010. On the fly query segmentation using
snippets. In CERI ’10, pages 259–266.
Chris Callison-Burch. 2009. Fast, cheap, and cre-
ative: evaluating translation quality using amazon’s
mechanical turk. In Proceedings of the 2009 Con-
ference on Empirical Methods in Natural Language
Processing, EMNLP ’09, pages 286–295. Associa-
tion for Computational Linguistics.
Vitor R Carvalho, Matthew Lease, and Emine Yilmaz.
2011. Crowdsourcing for search evaluation. ACM
Sigir forum, 44(2):17–22.
Jacob Cohen. 1960. A Coefficient of Agreement for
Nominal Scales. Educational and Psychological
Measurement, 20(1):37–46.
Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and Ji-
Rong Wen. 2008. Are Click-through Data Adequate
for Learning Web Search Rankings? In Proceed-
ings of the 17th ACM Conference on Information
and Knowledge Management, pages 73–82. ACM.
Matthias Hagen, Martin Potthast, Benno Stein, and
Christof Br¨autigam. 2011. Query Segmentation
Revisited. In Proceedings of the 20th Interna-
tional Conference on World Wide Web, pages 97–
106. ACM.
Matthias Hagen, Martin Potthast, Anna Beyer, and
Benno Stein. 2012. Towards Optimum Query Seg-
mentation: In Doubt Without. In Proceedings of the
Conference on Information and Knowledge Man-
agement, pages 1015–1024.
Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong
Li, Kuansan Wang, Fritz Behr, and C. Lee Giles.
2010. Exploring web scale language models for
search query processing. In Proceedings of the 19th
international conference on World wide web, WWW
’10, pages 451–460, New York, NY, USA. ACM.
Klaus Krippendorff. 2004. Content Analysis: An
Introduction to its Methodology. Sage,Thousand
Oaks, CA.
Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, and
Kuansan Wang. 2011. Unsupervised query segmen-
tation using clickthrough for information retrieval.
In SIGIR ’11, pages 285–294. ACM.
Knut Magne Risvik, Tomasz Mikolajewski, and Peter
Boros. 2003. Query segmentation for web search.
In WWW (Posters).
Rishiraj Saha Roy, Niloy Ganguly, Monojit Choud-
hury, and Srivatsan Laxman. 2012. An IR-based
Evaluation Framework for Web Search Query Seg-
mentation. In Proceedings of the International ACM
Special Interest Group on Information Retrieval (SI-
GIR) Conference on Research and Development in
Information Retrieval, pages 881–890. ACM.
Rion Snow, Brendan O’Connor, Daniel Jurafsky, and
Andrew Y. Ng. 2008. Cheap and fast—but is it
good?: evaluating non-expert annotations for natural
language tasks. In Proceedings of the Conference on
Empirical Methods in Natural Language Process-
ing, EMNLP ’08, pages 254–263, Stroudsburg, PA,
USA. Association for Computational Linguistics.
Bin Tan and Fuchun Peng. 2008. Unsupervised Query
Segmentation Using Generative Language Models
and Wikipedia. In Proceedings of the 17th Inter-
national Conference on World Wide Web (WWW),
pages 347–356. ACM.
Chao Zhang, Nan Sun, Xia Hu, Tingzhu Huang, and
Tat-Seng Chua. 2009. Query segmentation based on
eigenspace similarity. In Proceedings of the ACL-
IJCNLP 2009 Conference Short Papers, ACLShort
’09, pages 185–188, Stroudsburg, PA, USA. Associ-
ation for Computational Linguistics.
</reference>
<page confidence="0.993315">
1722
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.282967">
<title confidence="0.999637">Crowd Prefers the Middle Path: A New IAA Metric for Reveals Turker Biases in Query Segmentation</title>
<author confidence="0.927397">R V College of</author>
<affiliation confidence="0.854073">Bangalore,</affiliation>
<email confidence="0.999565">ronramanath@gmail.com</email>
<author confidence="0.598955">Kalika</author>
<affiliation confidence="0.790351">Microsoft Research Lab Bangalore,</affiliation>
<email confidence="0.999873">kalikab@microsoft.com</email>
<abstract confidence="0.998602125">Query segmentation, like text chunking, is the first step towards query understanding. In this study, we explore the effectiveness of crowdsourcing for this task. Through carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Similarly, in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="false">
<authors>
<author>We thank Ed Cutrell</author>
<author>Andrew Cross</author>
</authors>
<title>Microsoft Research Lab India, for their help in setting up the AMT experiments. We would also like to thank Anusha Suresh, IIT Kharagpur, India, for helping us with data preparation.</title>
<marker>Cutrell, Cross, </marker>
<rawString>We thank Ed Cutrell and Andrew Cross, Microsoft Research Lab India, for their help in setting up the AMT experiments. We would also like to thank Anusha Suresh, IIT Kharagpur, India, for helping us with data preparation.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Parsing By Chunks.</title>
<date>1991</date>
<publisher>Kluwer Academic Publishers.</publisher>
<contexts>
<context position="1063" citStr="Abney, 1991" startWordPosition="158" endWordPosition="160">h carefully designed control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Similarly, in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees. 1 Introduction Text chunking of Natural Language (NL) sentences is a well studied problem that is an essential preprocessing step for many NLP applications (Abney, 1991; Abney, 1995). In the context of Web search queries, query segmentation is similarly the first step towards analysis and understanding of queries (Hagen et al., 2011). The task in both the cases is to divide the sentence or the query into contiguous segments or chunks of words such that the words from a segment are related to each other more strongly than words from different segments (Bendersky et al., 2009). It is typically assumed that the segments are structurally and semantically coherent and, therefore, the information contained in them can be processed holistically. ∗The work was done </context>
<context position="9686" citStr="Abney (1991" startWordPosition="1582" endWordPosition="1583"> may also result in an over-optimistic IAA. An ideal segmentation should be based on the annotators’ own interpretation of the query. Nevertheless, if large scale data has to be procured, crowdsourcing seems to be the only efficient and effective model for this task, and has been proven to be so for other IR and linguistic annotations; see Carvalho et al. (2011) for examples of crowdsourcing for IR resources and (Snow et al., 2008; Callison-Burch, 2009) for language resources. In the context of NL text, segmentation has been traditionally referred to as chunking and is a well-studied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney, 1992) and other language text (Bali et al., 2009) have shown that native speakers agree on major clause and phrase boundaries, but may not do so on more fine-grained chunks. One important implication of this is that annotators are expected to agr</context>
</contexts>
<marker>Abney, 1991</marker>
<rawString>Steven P. Abney. 1991. Parsing By Chunks. Kluwer Academic Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Prosodic Structure, Performance Structure And Phrase Structure.</title>
<date>1992</date>
<booktitle>In Proceedings 5th DARPA Workshop on Speech and Natural Language,</booktitle>
<pages>425--428</pages>
<publisher>Morgan Kaufmann.</publisher>
<contexts>
<context position="10045" citStr="Abney, 1992" startWordPosition="1637" endWordPosition="1638">(2011) for examples of crowdsourcing for IR resources and (Snow et al., 2008; Callison-Burch, 2009) for language resources. In the context of NL text, segmentation has been traditionally referred to as chunking and is a well-studied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney, 1992) and other language text (Bali et al., 2009) have shown that native speakers agree on major clause and phrase boundaries, but may not do so on more fine-grained chunks. One important implication of this is that annotators are expected to agree more on the higher level boundaries for nested segmentation than the lower ones. We note that hierarchical query segmentation was proposed for the first time by Huang et al. (2010), where the authors recursively split a query (or its fragment) into exactly two parts and evaluate the 3http://www.webis.de/research/corpora final output against human annotat</context>
</contexts>
<marker>Abney, 1992</marker>
<rawString>Steven P. Abney. 1992. Prosodic Structure, Performance Structure And Phrase Structure. In Proceedings 5th DARPA Workshop on Speech and Natural Language, pages 425–428. Morgan Kaufmann.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven P Abney</author>
</authors>
<title>Chunks and dependencies: Bringing processing evidence to bear on syntax.</title>
<date>1995</date>
<booktitle>Computational Linguistics and the Foundations of Linguistic Theory,</booktitle>
<pages>145--164</pages>
<contexts>
<context position="1077" citStr="Abney, 1995" startWordPosition="161" endWordPosition="162">esigned control experiments and Inter Annotator Agreement metrics for analysis of experimental data, we show that crowdsourcing may not be a suitable approach for query segmentation because the crowd seems to have a very strong bias towards dividing the query into roughly equal (often only two) parts. Similarly, in the case of hierarchical or nested segmentation, turkers have a strong preference towards balanced binary trees. 1 Introduction Text chunking of Natural Language (NL) sentences is a well studied problem that is an essential preprocessing step for many NLP applications (Abney, 1991; Abney, 1995). In the context of Web search queries, query segmentation is similarly the first step towards analysis and understanding of queries (Hagen et al., 2011). The task in both the cases is to divide the sentence or the query into contiguous segments or chunks of words such that the words from a segment are related to each other more strongly than words from different segments (Bendersky et al., 2009). It is typically assumed that the segments are structurally and semantically coherent and, therefore, the information contained in them can be processed holistically. ∗The work was done during author’</context>
</contexts>
<marker>Abney, 1995</marker>
<rawString>Steven P. Abney. 1995. Chunks and dependencies: Bringing processing evidence to bear on syntax. Computational Linguistics and the Foundations of Linguistic Theory, pages 145–164.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Artstein</author>
<author>Massimo Poesio</author>
</authors>
<title>Inter-coder agreement for computational linguistics.</title>
<date>2008</date>
<journal>Computational Linguistics,</journal>
<volume>34</volume>
<issue>4</issue>
<contexts>
<context position="18292" citStr="Artstein and Poesio (2008)" startWordPosition="2979" endWordPosition="2982"> instance, we observed that a total of 128 turkers have provided the flat annotations for Q700, when we had only asked for 10 annotations per query. Thus, on average, a turker has annotated only 7.81% of the 700 queries. In fact, we found that 31 turkers had annotated less than 5 queries. Hence, measures such as Cohen’s κ (1960) cannot be directly applied in this context because for crowdsourced annotations, we cannot meaningfully compute annotator-specific distribution of the labels and biases. Second, most of the standard annotation metrics do not generalize for flat segmentation and trees. Artstein and Poesio (2008) provides a comprehensive survey of the IAA metrics and their usage in NLP. They note that all the metrics assume that a fixed set of labels are used for items. Therefore, it is far from obvious how to compare chunking or segmentation that covers the whole text or that might have overlapping units as in the case of nested segmentation. Furthermore, we would like to compare the reliability of flat and nested segmentation, and therefore, ideally we would like to have an IAA metric that can be meaningfully applied to both of these cases. After considering various measures, we decided to appropria</context>
<context position="21495" citStr="Artstein and Poesio, 2008" startWordPosition="3531" endWordPosition="3534">assign the height of the node to the corresponding boundaries. The number of unique nested segmentations of a query of length JqJ is its corresponding Catalan number7. Boundary variables for flat and nested segmentation are illustrated with an example of each kind in Tables 1 and 2 (last column). 4.2 Krippendorff ’s α for Segmentation Krippendorff ’s α (Krippendorff, 2004) is an extremely versatile agreement coefficient, which is based on the assumption that the expected agreement is calculated by looking at the overall distribution ofjudgments without regard to which annotator produced them (Artstein and Poesio, 2008). 7http://goo.gl/vKQvK 1717 Hence, it is appropriate for crowdsourced annotation, where the judgments come from a large number of unrelated annotators. Moreover, it allows for different magnitudes of disagreement, which is a useful feature as we might want to differentially penalize disagreements at various levels of the tree for nested segmentation. α is defined as (1) 2 stotal where Do and De are, respectively, the observed and expected disagreements that are measured by 2 swithin item and s2total – variance across annotations of all items. We adapt the equations presented in pp.565-566 of A</context>
</contexts>
<marker>Artstein, Poesio, 2008</marker>
<rawString>Ron Artstein and Massimo Poesio. 2008. Inter-coder agreement for computational linguistics. Computational Linguistics, 34(4):555–596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kalika Bali</author>
<author>Monojit Choudhury</author>
<author>Diptesh Chatterjee</author>
<author>Sankalan Prasad</author>
<author>Arpit Maheswari</author>
</authors>
<title>Correlates between Performance, Prosodic and Phrase Structures in Bangla and Hindi: Insights from a Psycholinguistic Experiment. In</title>
<date>2009</date>
<booktitle>Proceedings of International Conference on Natural Language Processing,</booktitle>
<pages>101--110</pages>
<contexts>
<context position="10089" citStr="Bali et al., 2009" startWordPosition="1643" endWordPosition="1646">or IR resources and (Snow et al., 2008; Callison-Burch, 2009) for language resources. In the context of NL text, segmentation has been traditionally referred to as chunking and is a well-studied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney, 1992) and other language text (Bali et al., 2009) have shown that native speakers agree on major clause and phrase boundaries, but may not do so on more fine-grained chunks. One important implication of this is that annotators are expected to agree more on the higher level boundaries for nested segmentation than the lower ones. We note that hierarchical query segmentation was proposed for the first time by Huang et al. (2010), where the authors recursively split a query (or its fragment) into exactly two parts and evaluate the 3http://www.webis.de/research/corpora final output against human annotations. 3 Experiments The annotation experimen</context>
<context position="22958" citStr="Bali et al., 2009" startWordPosition="3784" endWordPosition="3787">cs d1 and d2 that are applicable to flat and nested segmentation. We shall first define these metrics for comparing queries with equal length (i.e., |q |= |q′|): |q|−1 1 X d1(qm, q′ n) = |bm,i − b′ n,i |(4) |q |− 1 X |b2m,i − (b′ n,i)2 |(5) |q |− 1 i=1 While d1 penalizes all disagreements equally, d2 penalizes disagreements higher up the tree more. d2 might be a desirable metric for nested segmentation, because research on sentence chunking shows that annotators agree more on clause or major phrase boundaries, even though they may not always agree on intra-clausal or intra-phrasal boundaries (Bali et al., 2009). Note that for flat segmentation, d1 and d2 are identical, and hence we will denote them as d. We propose the following extension to these metrics for queries of unequal lengths. Without loss of generality, let us assume that |q |&lt; |q′|. k is 1 or 2; r = |q′ |− |q |+ 1. 1 dk(qm, q′ n) = r(|q |− 1) 4.3 IAA under Random Bias Assumption Krippendorff’s α uses the cross-item variance as an estimate of chance agreement, which is reliable in general. However, this might result in misleadingly low values of IAA, especially when the items in the set are indeed expected to have similar annotations. To </context>
</contexts>
<marker>Bali, Choudhury, Chatterjee, Prasad, Maheswari, 2009</marker>
<rawString>Kalika Bali, Monojit Choudhury, Diptesh Chatterjee, Sankalan Prasad, and Arpit Maheswari. 2009. Correlates between Performance, Prosodic and Phrase Structures in Bangla and Hindi: Insights from a Psycholinguistic Experiment. In Proceedings of International Conference on Natural Language Processing, pages 101 – 110.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Bendersky</author>
<author>W B Croft</author>
<author>David A Smith</author>
</authors>
<title>Two-stage query segmentation for information retrieval.</title>
<date>2009</date>
<booktitle>In Proceedings of the 32nd international ACM Special Interest Group on Information Retrieval (SIGIR) Conference on Research and Development in Information Retrieval,</booktitle>
<pages>810--811</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1476" citStr="Bendersky et al., 2009" startWordPosition="227" endWordPosition="230">preference towards balanced binary trees. 1 Introduction Text chunking of Natural Language (NL) sentences is a well studied problem that is an essential preprocessing step for many NLP applications (Abney, 1991; Abney, 1995). In the context of Web search queries, query segmentation is similarly the first step towards analysis and understanding of queries (Hagen et al., 2011). The task in both the cases is to divide the sentence or the query into contiguous segments or chunks of words such that the words from a segment are related to each other more strongly than words from different segments (Bendersky et al., 2009). It is typically assumed that the segments are structurally and semantically coherent and, therefore, the information contained in them can be processed holistically. ∗The work was done during author’s internship at Microsoft Research Lab India. † This author was supported by Microsoft Corporation and Microsoft Research India under the Microsoft Research India PhD Fellowship Award. Monojit Choudhury Microsoft Research Lab India Bangalore, India monojitc@microsoft.com Rishiraj Saha Roy† Indian Institute of Technology Kharagpur Kharagpur, India rishiraj@cse.iitkgp.ernet.in f Pipe representation</context>
</contexts>
<marker>Bendersky, Croft, Smith, 2009</marker>
<rawString>Michael Bendersky, W. B. Croft, and David A. Smith. 2009. Two-stage query segmentation for information retrieval. In Proceedings of the 32nd international ACM Special Interest Group on Information Retrieval (SIGIR) Conference on Research and Development in Information Retrieval, pages 810–811. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Shane Bergsma</author>
<author>Qin Iris Wang</author>
</authors>
<title>Learning Noun Phrase Query Segmentation.</title>
<date>2007</date>
<booktitle>In Proceedings ofEmpirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL),</booktitle>
<pages>819--826</pages>
<contexts>
<context position="7978" citStr="Bergsma and Wang, 2007" startWordPosition="1293" endWordPosition="1296"> and evaluation considers query segmentation as a process analogous to identification of phrases within a query which when put within double-quotes (implying exact matching of the quoted phrase in the document) leads to better IR performance. However, this is a very restricted view of the process and does not take into account the full potential of query segmentation. A more generic notion of segments leads to diverse and ambiguous definitions, making its evaluation a hard problem (see Saha Roy et. al. (2012) for a discussion on issues with evaluation). Most automatic segmentation techniques (Bergsma and Wang, 2007; Tan and Peng, 2008; Zhang et al., 2Related datasets and supplementary material can be accessed from http://bit.ly/161Gkk9 or can be obtained by directly emailing the authors. 2 0 3 on line 1 apply 0 first aid course 1714 2009; Brenes et al., 2010; Hagen et al., 2011; Li et al., 2011) have so far been evaluated only against a small set of human-annotated queries (Bergsma and Wang, 2007). The reported low IAA for such datasets casts serious doubts on the reliability of annotation and the performance of the algorithms evaluated on them (Hagen et al., 2011; Saha Roy et al., 2012). To address the</context>
</contexts>
<marker>Bergsma, Wang, 2007</marker>
<rawString>Shane Bergsma and Qin Iris Wang. 2007. Learning Noun Phrase Query Segmentation. In Proceedings ofEmpirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 819–826.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Akshar Bharati</author>
<author>Vineet Chaitanya</author>
<author>Rajeev Sangal</author>
<author>KV Ramakrishnamacharyulu</author>
</authors>
<title>Natural language processing: a Paninian perspective. PrenticeHall of India</title>
<date>1995</date>
<location>New Delhi.</location>
<contexts>
<context position="9880" citStr="Bharati et al (1995)" startWordPosition="1608" endWordPosition="1611">d, crowdsourcing seems to be the only efficient and effective model for this task, and has been proven to be so for other IR and linguistic annotations; see Carvalho et al. (2011) for examples of crowdsourcing for IR resources and (Snow et al., 2008; Callison-Burch, 2009) for language resources. In the context of NL text, segmentation has been traditionally referred to as chunking and is a well-studied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney, 1992) and other language text (Bali et al., 2009) have shown that native speakers agree on major clause and phrase boundaries, but may not do so on more fine-grained chunks. One important implication of this is that annotators are expected to agree more on the higher level boundaries for nested segmentation than the lower ones. We note that hierarchical query segmentation was proposed for the first time by Huang et al. (2010), where the</context>
</contexts>
<marker>Bharati, Chaitanya, Sangal, Ramakrishnamacharyulu, 1995</marker>
<rawString>Akshar Bharati, Vineet Chaitanya, Rajeev Sangal, and KV Ramakrishnamacharyulu. 1995. Natural language processing: a Paninian perspective. PrenticeHall of India New Delhi.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David J Brenes</author>
<author>Daniel Gayo-Avello</author>
<author>Rodrigo Garcia</author>
</authors>
<title>On the fly query segmentation using snippets.</title>
<date>2010</date>
<booktitle>In CERI ’10,</booktitle>
<pages>259--266</pages>
<contexts>
<context position="8226" citStr="Brenes et al., 2010" startWordPosition="1338" endWordPosition="1341"> this is a very restricted view of the process and does not take into account the full potential of query segmentation. A more generic notion of segments leads to diverse and ambiguous definitions, making its evaluation a hard problem (see Saha Roy et. al. (2012) for a discussion on issues with evaluation). Most automatic segmentation techniques (Bergsma and Wang, 2007; Tan and Peng, 2008; Zhang et al., 2Related datasets and supplementary material can be accessed from http://bit.ly/161Gkk9 or can be obtained by directly emailing the authors. 2 0 3 on line 1 apply 0 first aid course 1714 2009; Brenes et al., 2010; Hagen et al., 2011; Li et al., 2011) have so far been evaluated only against a small set of human-annotated queries (Bergsma and Wang, 2007). The reported low IAA for such datasets casts serious doubts on the reliability of annotation and the performance of the algorithms evaluated on them (Hagen et al., 2011; Saha Roy et al., 2012). To address the problem of data scarcity, Hagen et. al. (2011) have created larger annotated datasets through crowdsourcing3. However, in their approach the crowd is provided with a few (four) possible segmentations of a query to choose from (known through a pers</context>
</contexts>
<marker>Brenes, Gayo-Avello, Garcia, 2010</marker>
<rawString>David J. Brenes, Daniel Gayo-Avello, and Rodrigo Garcia. 2010. On the fly query segmentation using snippets. In CERI ’10, pages 259–266.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
</authors>
<title>Fast, cheap, and creative: evaluating translation quality using amazon’s mechanical turk.</title>
<date>2009</date>
<booktitle>In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09,</booktitle>
<pages>286--295</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="9532" citStr="Callison-Burch, 2009" startWordPosition="1558" endWordPosition="1559">n generate the correct segmentation of a query within top few options. It is far from obvious how to generate these initial segmentations in a reliable manner. This may also result in an over-optimistic IAA. An ideal segmentation should be based on the annotators’ own interpretation of the query. Nevertheless, if large scale data has to be procured, crowdsourcing seems to be the only efficient and effective model for this task, and has been proven to be so for other IR and linguistic annotations; see Carvalho et al. (2011) for examples of crowdsourcing for IR resources and (Snow et al., 2008; Callison-Burch, 2009) for language resources. In the context of NL text, segmentation has been traditionally referred to as chunking and is a well-studied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney, 1992) and other language text (Bali et al., 2009) have shown that native speakers agree on m</context>
</contexts>
<marker>Callison-Burch, 2009</marker>
<rawString>Chris Callison-Burch. 2009. Fast, cheap, and creative: evaluating translation quality using amazon’s mechanical turk. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, EMNLP ’09, pages 286–295. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vitor R Carvalho</author>
<author>Matthew Lease</author>
<author>Emine Yilmaz</author>
</authors>
<title>Crowdsourcing for search evaluation.</title>
<date>2011</date>
<journal>ACM Sigir forum,</journal>
<pages>44--2</pages>
<contexts>
<context position="9439" citStr="Carvalho et al. (2011)" startWordPosition="1541" endWordPosition="1544">gh a personal communication with a authors). Thus, it presupposes an automatic process that can generate the correct segmentation of a query within top few options. It is far from obvious how to generate these initial segmentations in a reliable manner. This may also result in an over-optimistic IAA. An ideal segmentation should be based on the annotators’ own interpretation of the query. Nevertheless, if large scale data has to be procured, crowdsourcing seems to be the only efficient and effective model for this task, and has been proven to be so for other IR and linguistic annotations; see Carvalho et al. (2011) for examples of crowdsourcing for IR resources and (Snow et al., 2008; Callison-Burch, 2009) for language resources. In the context of NL text, segmentation has been traditionally referred to as chunking and is a well-studied problem. Abney (1991; 1992; 1995) defines a chunk as a sub-tree within a syntactic phrase structure tree corresponding to Noun, Prepositional, Adjectival, Adverbial and Verb Phrases. Similarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney,</context>
</contexts>
<marker>Carvalho, Lease, Yilmaz, 2011</marker>
<rawString>Vitor R Carvalho, Matthew Lease, and Emine Yilmaz. 2011. Crowdsourcing for search evaluation. ACM Sigir forum, 44(2):17–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jacob Cohen</author>
</authors>
<title>A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement,</title>
<date>1960</date>
<marker>Cohen, 1960</marker>
<rawString>Jacob Cohen. 1960. A Coefficient of Agreement for Nominal Scales. Educational and Psychological Measurement, 20(1):37–46.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhicheng Dou</author>
<author>Ruihua Song</author>
<author>Xiaojie Yuan</author>
<author>JiRong Wen</author>
</authors>
<title>Are Click-through Data Adequate for Learning Web Search Rankings?</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th ACM Conference on Information and Knowledge Management,</booktitle>
<pages>73--82</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="13964" citStr="Dou et al., 2008" startWordPosition="2271" endWordPosition="2274">ments. Q700: Since 500 queries may not be enough for reliable conclusion and since the queries may not have been chosen specifically for the purpose of annotation experiments, we expanded the set with another 700 queries sampled from a slice of the query logs of Bing Australia4 containing 16.7 million queries issued over a period of one month (May 2010). We picked, uniformly at random, queries that are 4 to 8 words long, have only English letters and numerals, and a high click entropy because “a query with a larger click entropy value is more likely to be an informational or ambiguous query” (Dou et al., 2008). Q500 consists of tailish queries with frequency between 5 and 15 that have at least one multiword named entity; but unlike the case of Q700, click-entropy was not considered during sampling. As we shall see, this difference is clearly reflected in the results. S300: We randomly selected 300 English sentences from a collection of full texts of public domain books5 that were 5 to 15 words long, and checked them for well-formedness. This set will be referred to as S300. QRand: Instead of generating search queries by throwing in words randomly, we thought it will be more interesting to explore a</context>
</contexts>
<marker>Dou, Song, Yuan, Wen, 2008</marker>
<rawString>Zhicheng Dou, Ruihua Song, Xiaojie Yuan, and JiRong Wen. 2008. Are Click-through Data Adequate for Learning Web Search Rankings? In Proceedings of the 17th ACM Conference on Information and Knowledge Management, pages 73–82. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Hagen</author>
<author>Martin Potthast</author>
<author>Benno Stein</author>
<author>Christof Br¨autigam</author>
</authors>
<title>Query Segmentation Revisited.</title>
<date>2011</date>
<booktitle>In Proceedings of the 20th International Conference on World Wide Web,</booktitle>
<pages>97--106</pages>
<publisher>ACM.</publisher>
<marker>Hagen, Potthast, Stein, Br¨autigam, 2011</marker>
<rawString>Matthias Hagen, Martin Potthast, Benno Stein, and Christof Br¨autigam. 2011. Query Segmentation Revisited. In Proceedings of the 20th International Conference on World Wide Web, pages 97– 106. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthias Hagen</author>
<author>Martin Potthast</author>
<author>Anna Beyer</author>
<author>Benno Stein</author>
</authors>
<title>Towards Optimum Query Segmentation: In Doubt Without.</title>
<date>2012</date>
<booktitle>In Proceedings of the Conference on Information and Knowledge Management,</booktitle>
<pages>1015--1024</pages>
<contexts>
<context position="3670" citStr="Hagen et al., 2012" startWordPosition="599" endWordPosition="602">er Annotator Agreement (IAA) remains an issue. For instance, Table 1 illustrates the variation in flat segmentation by 10 annotators. This confusion is mainly because the definition of a segment in a query is ambiguous and of an unspecified granularity. This is further compounded by the fact that other than easily recognizable and agreed upon segments such as Named Entities or Multi-Word Expressions, there is no established notion of linguistic grouping such as phrases and clauses in a query. Although there is little work on the use of crowdsourcing for query segmentation (Hagen et al., 2011; Hagen et al., 2012), the idea that the 1713 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1713–1722, Sofia, Bulgaria, August 4-9 2013. c�2013 Association for Computational Linguistics crowd could be a potential (and cheaper) source for reliable segmentation seems a reasonable assumption. The need for larger datasets makes this an attractive proposition. Also, a larger number of annotations could be appropriately distilled to obtain better quality segmentations. In this paper we explore crowdsourcing as an option for query segmentation through experiments designed </context>
</contexts>
<marker>Hagen, Potthast, Beyer, Stein, 2012</marker>
<rawString>Matthias Hagen, Martin Potthast, Anna Beyer, and Benno Stein. 2012. Towards Optimum Query Segmentation: In Doubt Without. In Proceedings of the Conference on Information and Knowledge Management, pages 1015–1024.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jian Huang</author>
<author>Jianfeng Gao</author>
<author>Jiangbo Miao</author>
<author>Xiaolong Li</author>
<author>Kuansan Wang</author>
<author>Fritz Behr</author>
<author>C Lee Giles</author>
</authors>
<title>Exploring web scale language models for search query processing.</title>
<date>2010</date>
<booktitle>In Proceedings of the 19th international conference on World wide web, WWW ’10,</booktitle>
<pages>451--460</pages>
<publisher>ACM.</publisher>
<location>New York, NY, USA.</location>
<contexts>
<context position="10469" citStr="Huang et al. (2010)" startWordPosition="1708" endWordPosition="1711">imilarly, Bharati et al (1995) defines it as Noun Group and Verb Group based only on local surface information. However, cognitive and annotation experiments for chunking of English (Abney, 1992) and other language text (Bali et al., 2009) have shown that native speakers agree on major clause and phrase boundaries, but may not do so on more fine-grained chunks. One important implication of this is that annotators are expected to agree more on the higher level boundaries for nested segmentation than the lower ones. We note that hierarchical query segmentation was proposed for the first time by Huang et al. (2010), where the authors recursively split a query (or its fragment) into exactly two parts and evaluate the 3http://www.webis.de/research/corpora final output against human annotations. 3 Experiments The annotation experiments have been designed to systematically study the various aspects of query segmentation. In order to verify the effectiveness and reliability of crowdsourcing, we designed an AMT experiment for flat segmentation of Web search queries. As a baseline, we would like to compare these annotations with those from human experts trained for the task. We shall refer to this baseline as </context>
</contexts>
<marker>Huang, Gao, Miao, Li, Wang, Behr, Giles, 2010</marker>
<rawString>Jian Huang, Jianfeng Gao, Jiangbo Miao, Xiaolong Li, Kuansan Wang, Fritz Behr, and C. Lee Giles. 2010. Exploring web scale language models for search query processing. In Proceedings of the 19th international conference on World wide web, WWW ’10, pages 451–460, New York, NY, USA. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klaus Krippendorff</author>
</authors>
<title>Content Analysis: An Introduction to its Methodology. Sage,Thousand Oaks,</title>
<date>2004</date>
<location>CA.</location>
<contexts>
<context position="21244" citStr="Krippendorff, 2004" startWordPosition="3493" endWordPosition="3494"> height. The lowest internal nodes, both of whose children are query words, are assigned a value of 0. Other internal nodes get a value of one greater than the height of its higher child. Since every internal node corresponds to a boundary, we assign the height of the node to the corresponding boundaries. The number of unique nested segmentations of a query of length JqJ is its corresponding Catalan number7. Boundary variables for flat and nested segmentation are illustrated with an example of each kind in Tables 1 and 2 (last column). 4.2 Krippendorff ’s α for Segmentation Krippendorff ’s α (Krippendorff, 2004) is an extremely versatile agreement coefficient, which is based on the assumption that the expected agreement is calculated by looking at the overall distribution ofjudgments without regard to which annotator produced them (Artstein and Poesio, 2008). 7http://goo.gl/vKQvK 1717 Hence, it is appropriate for crowdsourced annotation, where the judgments come from a large number of unrelated annotators. Moreover, it allows for different magnitudes of disagreement, which is a useful feature as we might want to differentially penalize disagreements at various levels of the tree for nested segmentati</context>
</contexts>
<marker>Krippendorff, 2004</marker>
<rawString>Klaus Krippendorff. 2004. Content Analysis: An Introduction to its Methodology. Sage,Thousand Oaks, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yanen Li</author>
<author>Bo-Jun Paul Hsu</author>
<author>ChengXiang Zhai</author>
<author>Kuansan Wang</author>
</authors>
<title>Unsupervised query segmentation using clickthrough for information retrieval.</title>
<date>2011</date>
<booktitle>In SIGIR ’11,</booktitle>
<pages>285--294</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8264" citStr="Li et al., 2011" startWordPosition="1346" endWordPosition="1349">ocess and does not take into account the full potential of query segmentation. A more generic notion of segments leads to diverse and ambiguous definitions, making its evaluation a hard problem (see Saha Roy et. al. (2012) for a discussion on issues with evaluation). Most automatic segmentation techniques (Bergsma and Wang, 2007; Tan and Peng, 2008; Zhang et al., 2Related datasets and supplementary material can be accessed from http://bit.ly/161Gkk9 or can be obtained by directly emailing the authors. 2 0 3 on line 1 apply 0 first aid course 1714 2009; Brenes et al., 2010; Hagen et al., 2011; Li et al., 2011) have so far been evaluated only against a small set of human-annotated queries (Bergsma and Wang, 2007). The reported low IAA for such datasets casts serious doubts on the reliability of annotation and the performance of the algorithms evaluated on them (Hagen et al., 2011; Saha Roy et al., 2012). To address the problem of data scarcity, Hagen et. al. (2011) have created larger annotated datasets through crowdsourcing3. However, in their approach the crowd is provided with a few (four) possible segmentations of a query to choose from (known through a personal communication with a authors). Th</context>
</contexts>
<marker>Li, Hsu, Zhai, Wang, 2011</marker>
<rawString>Yanen Li, Bo-Jun Paul Hsu, ChengXiang Zhai, and Kuansan Wang. 2011. Unsupervised query segmentation using clickthrough for information retrieval. In SIGIR ’11, pages 285–294. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Knut Magne Risvik</author>
<author>Tomasz Mikolajewski</author>
<author>Peter Boros</author>
</authors>
<title>Query segmentation for web search. In</title>
<date>2003</date>
<tech>WWW (Posters).</tech>
<marker>Risvik, Mikolajewski, Boros, 2003</marker>
<rawString>Knut Magne Risvik, Tomasz Mikolajewski, and Peter Boros. 2003. Query segmentation for web search. In WWW (Posters).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rishiraj Saha Roy</author>
<author>Niloy Ganguly</author>
<author>Monojit Choudhury</author>
<author>Srivatsan Laxman</author>
</authors>
<title>An IR-based Evaluation Framework for Web Search Query Segmentation.</title>
<date>2012</date>
<booktitle>In Proceedings of the International ACM Special Interest Group on Information Retrieval (SIGIR) Conference on Research and Development in Information Retrieval,</booktitle>
<pages>881--890</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="8562" citStr="Roy et al., 2012" startWordPosition="1396" endWordPosition="1399">echniques (Bergsma and Wang, 2007; Tan and Peng, 2008; Zhang et al., 2Related datasets and supplementary material can be accessed from http://bit.ly/161Gkk9 or can be obtained by directly emailing the authors. 2 0 3 on line 1 apply 0 first aid course 1714 2009; Brenes et al., 2010; Hagen et al., 2011; Li et al., 2011) have so far been evaluated only against a small set of human-annotated queries (Bergsma and Wang, 2007). The reported low IAA for such datasets casts serious doubts on the reliability of annotation and the performance of the algorithms evaluated on them (Hagen et al., 2011; Saha Roy et al., 2012). To address the problem of data scarcity, Hagen et. al. (2011) have created larger annotated datasets through crowdsourcing3. However, in their approach the crowd is provided with a few (four) possible segmentations of a query to choose from (known through a personal communication with a authors). Thus, it presupposes an automatic process that can generate the correct segmentation of a query within top few options. It is far from obvious how to generate these initial segmentations in a reliable manner. This may also result in an over-optimistic IAA. An ideal segmentation should be based on th</context>
<context position="12575" citStr="Roy et al. (2012)" startWordPosition="2037" endWordPosition="2040">us, we have the following four pairs of annotation experiments: flat and nested segmentation of queries from crowdsourcing, corresponding flat and nested gold annotations, flat and nested segmentation of English sentences from crowdsourcing, and flat and nested segmentations for randomly generated queries through crowdsourcing. 3.1 Dataset For our experiments, we need a set of Web search queries and well-formed English sentences. Furthermore, for generating the random queries, we will use search query logs to learn n-gram models. In particular, we use the following datasets: Q500, QG500: Saha Roy et al. (2012) released a dataset of 500 queries, 5 to 8 words long, for evaluation of various segmentation algorithms. This dataset has flat segmentations from three annotators obtained under controlled experimental settings, and can be considered as Gold annota1715 Figure 2: Length distribution of datasets. tions. Hence, we select this set for our experiments as well. We procured the corresponding nested segmentation for these queries from two human experts, who are regular search engine users, between 20 and 30 years old, and familiar with various linguistic annotation tasks. They annotated the data unde</context>
</contexts>
<marker>Roy, Ganguly, Choudhury, Laxman, 2012</marker>
<rawString>Rishiraj Saha Roy, Niloy Ganguly, Monojit Choudhury, and Srivatsan Laxman. 2012. An IR-based Evaluation Framework for Web Search Query Segmentation. In Proceedings of the International ACM Special Interest Group on Information Retrieval (SIGIR) Conference on Research and Development in Information Retrieval, pages 881–890. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rion Snow</author>
<author>Brendan O’Connor</author>
<author>Daniel Jurafsky</author>
<author>Andrew Y Ng</author>
</authors>
<title>Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks.</title>
<date>2008</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08,</booktitle>
<pages>254--263</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Snow, O’Connor, Jurafsky, Ng, 2008</marker>
<rawString>Rion Snow, Brendan O’Connor, Daniel Jurafsky, and Andrew Y. Ng. 2008. Cheap and fast—but is it good?: evaluating non-expert annotations for natural language tasks. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, EMNLP ’08, pages 254–263, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bin Tan</author>
<author>Fuchun Peng</author>
</authors>
<title>Unsupervised Query Segmentation Using Generative Language Models and Wikipedia.</title>
<date>2008</date>
<booktitle>In Proceedings of the 17th International Conference on World Wide Web (WWW),</booktitle>
<pages>347--356</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="7998" citStr="Tan and Peng, 2008" startWordPosition="1297" endWordPosition="1300">s query segmentation as a process analogous to identification of phrases within a query which when put within double-quotes (implying exact matching of the quoted phrase in the document) leads to better IR performance. However, this is a very restricted view of the process and does not take into account the full potential of query segmentation. A more generic notion of segments leads to diverse and ambiguous definitions, making its evaluation a hard problem (see Saha Roy et. al. (2012) for a discussion on issues with evaluation). Most automatic segmentation techniques (Bergsma and Wang, 2007; Tan and Peng, 2008; Zhang et al., 2Related datasets and supplementary material can be accessed from http://bit.ly/161Gkk9 or can be obtained by directly emailing the authors. 2 0 3 on line 1 apply 0 first aid course 1714 2009; Brenes et al., 2010; Hagen et al., 2011; Li et al., 2011) have so far been evaluated only against a small set of human-annotated queries (Bergsma and Wang, 2007). The reported low IAA for such datasets casts serious doubts on the reliability of annotation and the performance of the algorithms evaluated on them (Hagen et al., 2011; Saha Roy et al., 2012). To address the problem of data sca</context>
</contexts>
<marker>Tan, Peng, 2008</marker>
<rawString>Bin Tan and Fuchun Peng. 2008. Unsupervised Query Segmentation Using Generative Language Models and Wikipedia. In Proceedings of the 17th International Conference on World Wide Web (WWW), pages 347–356. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chao Zhang</author>
<author>Nan Sun</author>
<author>Xia Hu</author>
<author>Tingzhu Huang</author>
<author>Tat-Seng Chua</author>
</authors>
<title>Query segmentation based on eigenspace similarity.</title>
<date>2009</date>
<booktitle>In Proceedings of the ACLIJCNLP 2009 Conference Short Papers, ACLShort ’09,</booktitle>
<pages>185--188</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Stroudsburg, PA, USA.</location>
<marker>Zhang, Sun, Hu, Huang, Chua, 2009</marker>
<rawString>Chao Zhang, Nan Sun, Xia Hu, Tingzhu Huang, and Tat-Seng Chua. 2009. Query segmentation based on eigenspace similarity. In Proceedings of the ACLIJCNLP 2009 Conference Short Papers, ACLShort ’09, pages 185–188, Stroudsburg, PA, USA. Association for Computational Linguistics.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>