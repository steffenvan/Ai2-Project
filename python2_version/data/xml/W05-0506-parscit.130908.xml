<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.010176">
<title confidence="0.9995055">
A Second Language Acquisition Model Using
Example Generalization and Concept Categories
</title>
<author confidence="0.996583">
Ari Rappoport Vera Sheinman
</author>
<affiliation confidence="0.864761333333333">
Institute of Computer Science Institute of Computer Science
The Hebrew University The Hebrew University
Jerusalem, Israel Jerusalem, Israel
</affiliation>
<email confidence="0.997257">
arir@cs.huji.ac.il vera46@cl.cs.titech.ac.jp
</email>
<sectionHeader confidence="0.995611" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99996">
We present a computational model of ac-
quiring a second language from example
sentences. Our learning algorithms build a
construction grammar language model,
and generalize using form-based patterns
and the learner’s conceptual system. We
use a unique professional language learn-
ing corpus, and show that substantial reli-
able learning can be achieved even though
the corpus is very small. The model is ap-
plied to assisting the authoring of Japa-
nese language learning corpora.
</bodyText>
<sectionHeader confidence="0.998978" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999282941176471">
Second Language Acquisition (SLA) is a central
topic in many of the fields of activity related to
human languages. SLA is studied in cognitive sci-
ence and theoretical linguistics in order to gain a
better understanding of our general cognitive abili-
ties and of first language acquisition (FLA)1. Gov-
ernments, enterprises and individuals invest
heavily in foreign language learning due to busi-
ness, cultural, and leisure time considerations. SLA
is thus vital for both theory and practice and should
be seriously examined in computational linguistics
(CL), especially when considering the close rela-
tionship to FLA and the growing attention devoted
to the latter by the CL community.
In this paper we present a computational model
of SLA. As far as we could determine, this is the
first model that simulates the learning process
</bodyText>
<footnote confidence="0.535675">
1 Note that the F stands here for ‘First’, not ‘Foreign’.
</footnote>
<bodyText confidence="0.999824222222222">
computationally. Learning is done from examples,
with no reliance on explicit rules. The model is
unique in the usage of a conceptual system by the
learning algorithms. We use a unique professional
language learning corpus, showing effective learn-
ing from a very small number of examples. We
evaluate the model by applying it to assisting the
authoring of Japanese language learning corpora.
We focus here on basic linguistic aspects of
SLA, leaving other aspects to future papers. In par-
ticular, we assume that the learner possesses per-
fect memory and is capable of invoking the
provided learning algorithms without errors.
In sections 2 and 3 we provide relevant back-
ground and discuss previous work. Our input,
learner and language models are presented in sec-
tion 4, and the learning algorithms in section 5.
Section 6 discusses the authoring application.
</bodyText>
<sectionHeader confidence="0.97985" genericHeader="introduction">
2 Background
</sectionHeader>
<bodyText confidence="0.999935769230769">
We use the term ‘second language acquisition’ to
refer to any situation in which adults learn a new
language2. A major concept in SLA theory
[Gass01, Mitchell03] is that of interlanguage:
when learning a new language (L2), at any given
point in time the learner has a valid partial L2 lan-
guage system that differs from his/her native lan-
guage(s) (L1) and from the L2. The SLA process is
that of progressive enhancement and refinement of
interlanguage. The main trigger for interlanguage
modification is when the learner notices a gap be-
tween interlanguage and L2 forms. In order for this
to happen, the learner must be provided with com-
</bodyText>
<footnote confidence="0.998682">
2 Some SLA texts distinguish between ‘second’ and ‘foreign’
and between ‘acquisition’ and ‘learning’. We will not make
those distinctions here.
</footnote>
<page confidence="0.983414">
45
</page>
<note confidence="0.9944655">
Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 45–52,
Ann Arbor, June 2005. c�2005 Association for Computational Linguistics
</note>
<bodyText confidence="0.99995528125">
prehensible input. Our model directly supports all
of these notions.
A central, debated issue in language acquisition
is whether FLA mechanisms [Clark03] are avail-
able in SLA. What is clear is that SL learners al-
ready possess a mature conceptual system and are
capable of explicit symbolic reasoning and abstrac-
tion. In addition, the amount of input and time
available for FLA are usually orders of magnitude
larger than those for SLA.
The general linguistic framework that we utilize
in this paper is that of Construction Grammar
(CG) [Goldberg95, Croft01], in which the building
blocks of language are words, phrases and phrase
templates that carry meanings. [Tomasello03] pre-
sents a CG theory of FLA in which children learn
whole constructions as ‘islands’ that are gradually
generalized and merged. Our SLA model is quite
similar to this process.
In language education, current classroom meth-
ods use a combination of formal rules and commu-
nicative situations. Radically different is the
Pimsleur method [Pimsleur05], an audio-based
self-study method in which rules and explanations
are kept to a minimum and most learning occurs by
letting the learner infer L2 constructs from transla-
tions of contextual L1 sentences. Substantial anec-
dotal evidence (as manifested by learner comments
and our own experience) suggests that the method
is highly effective. We have used a Pimsleur cor-
pus in our experiments. One of the goals of our
model is to assist the authoring of such corpora.
</bodyText>
<sectionHeader confidence="0.998536" genericHeader="method">
3 Previous Work
</sectionHeader>
<bodyText confidence="0.999790028571429">
There is almost no previous CL work explicitly
addressing SLA. The only one of which we are
aware is [Maritxalar97], which represents interlan-
guage levels using manually defined symbolic
rules. No language model (in the CL sense) or
automatic learning are provided.
Many aspects of SLA are similar to first lan-
guage acquisition. Unsupervised grammar induc-
tion from corpora is a growing CL research area
([Clark01, Klein05] and references there), mostly
using statistical learning of model parameters or
pattern identification by distributional criteria. The
resulting models are not easily presentable to hu-
mans, and do not utilize semantics.
[Edelman04] presents an elegant FLA system in
which constructions and word categories are iden-
tified iteratively using a graph. [Chang04] presents
an FLA system that truly supports construction
grammar and is unique in its incorporation of gen-
eral cognitive concepts and embodied semantics.
SLA is related to machine translation (MT),
since learning how to translate is a kind of acquisi-
tion of the L2. Most relevant to us here is modern
example-based machine translation (EBMT) [So-
mers01, Carl03], due to its explicit computation of
translation templates and to the naturalness of
learning from a small number of examples
[Brown00, Cicekli01].
The Computer Assisted Language Learning
(CALL) literature [Levy97, Chapelle01] is rich in
project descriptions, and there are several commer-
cial CALL software applications. In general,
CALL applications focus on teacher, environment,
memory and automatization aspects, and are thus
complementary to the goals that we address here.
</bodyText>
<sectionHeader confidence="0.688293" genericHeader="method">
4 Input, Learner and Language Knowl-
edge Models
</sectionHeader>
<bodyText confidence="0.99926075">
Our ultimate goal is a comprehensive computa-
tional model of SLA that covers all aspects of the
phenomenon. The present paper is a first step in
that direction. Our goals here are to:
</bodyText>
<listItem confidence="0.902093857142857">
• Explore what can be learned from exam-
ple-based, small, beginner-level input
corpora tailored for SLA;
• Model a learner having a mature concep-
tual system;
• Use an L2 language knowledge model
that supports sentence enumeration;
• Identify cognitively plausible and effective
SL learning algorithms;
• Apply the model in assisting the author-
ing of corpora tailored for SLA.
In this section we present the first three compo-
nents; the learning algorithms and the application
are presented in the next two sections.
</listItem>
<subsectionHeader confidence="0.84063">
4.1 Input Model
</subsectionHeader>
<bodyText confidence="0.999883">
The input potentially available for SL learners is of
high variability, consisting of meta-linguistic rules,
usage examples isolated for learning purposes, us-
age examples partially or fully understood in con-
text, dictionary-like word definitions, free-form
explanations, and more.
</bodyText>
<page confidence="0.998807">
46
</page>
<bodyText confidence="0.999988237288136">
One of our major goals is to explore the rela-
tionship between first and second language acqui-
sition. Methodologically, it therefore makes sense
to first study input that is the most similar linguis-
tically to that available during FLA, usage exam-
ples. As noted in section 2, a fundamental property
of SLA is that learners are capable of mature un-
derstanding. Input in our model will thus consist of
an ordered set of comprehensible usage exam-
ples, where an example is a pair of L1, L2 sen-
tences such that the former is a translation of the
latter in a certain understood context.
We focus here on modeling beginner-level pro-
ficiency, which is qualitatively different from na-
tive-like fluency [Gass01] and should be studied
before the latter.
We are interested in relatively small input cor-
pora (thousands of examples at most), because this
is an essential part of SLA modeling. In addition, it
is of great importance, in both theoretical and
computational linguistics, to explore the limits of
what can be learned from meager input.
One of the main goals of SLA modeling is to
discover which input is most effective for SLA,
because a substantial part of learners’ input can be
controlled, while their time capacity is small. We
thus allow our input to be optimized for SLA, by
containing examples that are sub-parts of other
examples and whose sole purpose is to facilitate
learning those (our corpus is also optimized in the
sense of covering simpler constructs and words
first, but this issue is orthogonal to our model). We
utilize two types of such sub-examples. First, we
require that new words are always presented first
on their own. This is easy to achieve in controlled
teaching, and is actually very frequent in FLA as
well [Clark03]. In the present paper we will as-
sume that this completely solves the task of seg-
menting a sentence into words, which is reasonable
for a beginner level corpus where the total number
of words is relatively small. Word boundaries are
thus explicitly and consistently marked.
Second, the sub-example mechanism is also use-
ful when learning a construction. For example, if
the L2 sentence is ‘the boy went to school’ (where
the L2 here is English), it could help learning algo-
rithms if it were preceded by ‘to school’ or ‘the
boy’. Hence we do not require examples to be
complete sentences.
In this paper we do not deal with phonetics or
writing systems, assuming L2 speech has been
consistently transcribed using a quasi-phonetic
writing system. Learning L2 phonemes is certainly
an important task in SLA, but most linguistic and
cognitive theories view it as separable from the rest
of language acquisition [Fromkin02, Medin05].
The input corpus we have used is a transcribed
Pimsleur Japanese course, which fits the input
specification above.
</bodyText>
<subsectionHeader confidence="0.944243">
4.2 Learner Model
</subsectionHeader>
<bodyText confidence="0.9998938">
A major aspect of SLA is that learners already pos-
sess a mature conceptual system (CS), influenced
by their life experience (including languages they
know). Our learning algorithms utilize a CS model.
We opted for being conservative: the model is only
allowed to contain concepts that are clearly pos-
sessed by the learner before learning starts. Con-
cepts that are particular to the L2 (e.g., ‘noun
gender’ for English speakers learning Spanish) are
not allowed. Examples for concept classes include
fruits, colors, human-made objects, physical activi-
ties and emotions, as well as meta-linguistic con-
cepts such as pronouns and prepositions. A single
concept is simply represented by a prototypical
English word denoting it (e.g., ‘child’, ‘school’). A
concept class is represented by the concepts it con-
tains and is conveniently named using an English
word or phrase (e.g., ‘types of people’, ‘buildings’,
‘language names’).
Our learners can explicitly reason about concept
inter-relationships. Is-a relationships between
classes are represented when they are beyond any
doubt (e.g., ‘buildings’ and ‘people’ are both
‘physical things’).
A basic conceptual system is assumed to exist
before the SLA process starts. When the input is
controlled and small, as in our case, it is both
methodologically valid and practical to prepare the
CS manually. CS design is discussed in detail in
section 6.
In the model described in the present paper we
do not automatically modify the CS during the
learning process; CS evolution will be addressed in
future models.
As stated in section 1, in this paper we focus on
linguistic SLA aspects and do not address issues
such as human errors, motivation and attention.
We thus assume that our learner possesses perfect
memory and can invoke our learning algorithms
without any mistakes.
</bodyText>
<page confidence="0.996291">
47
</page>
<subsectionHeader confidence="0.989026">
4.3 Language Knowledge Model
</subsectionHeader>
<bodyText confidence="0.999994447368421">
We require our model to support a basic capability
of a grammar: enumeration of language sentences
(parsing will be reported in other papers). In addi-
tion, we provide a degree of certainty for each. The
model’s quality is evaluated by its applicability for
learning corpora authoring assistance (section 6).
The representation is based on construction
grammar (CG), explicitly storing a set of construc-
tions and their inter-relationships. CG is ideally
suited for SLA interlanguage because it enables the
representation of partial knowledge: every lan-
guage form, from concrete words and sentences to
the most abstract constructs, counts as a construc-
tion. The generative capacity of language is ob-
tained by allowing constructions to replace
arguments. For example, (child), (the child goes to
school), (&lt;x&gt; goes to school), (&lt;x&gt; &lt;v&gt; to school)
and (X goes Z) are all constructions, where &lt;x&gt;,
&lt;v&gt; denote word classes and X, Z denote other
constructions.
SL learners can make explicit judgments as to
their level of confidence in the grammaticality of
utterances. To model this, our learning algorithms
assign a degree of certainty (DOC) to each con-
struction and to the possibility of it being an argu-
ment of another construction. The certainty of a
sentence is a function (e.g., sum or maximum) of
the DOCs present in its derivation path.
Our representation is equivalent to a graph
whose nodes are constructions and whose directed,
labeled arcs denote the possibility of a node filling
a particular argument of another node. When the
graph is a-cyclic the resulting language contains a
finite number of concrete sentences, easily com-
puted by graph traversal. This is similar to [Edel-
man04]; we differ in our partial support for
semantics through a conceptual system (section 5)
and in the notion of a degree of certainty.
</bodyText>
<sectionHeader confidence="0.984139" genericHeader="method">
5 Learning Algorithms
</sectionHeader>
<bodyText confidence="0.999785322033899">
Our general SLA scheme is that of incremental
learning – examples are given one by one, each
causing an update to the model. A major goal of
our model is to identify effective, cognitively plau-
sible learning algorithms. In this section we present
a concrete set of such algorithms.
Structured categorization is a major driving
force in perception and other cognitive processes
[Medin05]. Our learners are thus driven by the de-
sire to form useful generalizations over the input.
A generalization of two or more examples is possi-
ble when there is sufficient similarity of form and
meaning between them. Hence, the basic ingredi-
ent of our learning algorithms is identifying such
similarities.
To identify concrete effective learning algo-
rithms, we have followed our own inference proc-
esses when learning a foreign language from an
example-based corpus (section 6). The set of algo-
rithms described below are the result of this study.
The basic form similarity algorithm is Single
Word Difference (SWD). When two examples
share all but a single word, a construction is
formed in which that word is replaced by an argu-
ment class containing those words. For example,
given ‘eigo ga wakari mas’ and ‘nihongo ga wakari
mas’, the construction (&lt;eigo, nihongo&gt; ga wakari
mas) (‘I understand English/Japanese’), containing
one argument class, is created. In itself, SWD only
compresses the input, so its degree of certainty is
maximal. It does not create new sentences, but it
organizes knowledge in a form suitable for gener-
alization.
The basic meaning-based similarity algorithm is
Extension by Conceptual Categories (ECC). For
an argument class W in a construction C, ECC at-
tempts to find the smallest concept category U’
that contains W’, the set of concepts corresponding
to the words in W. If no such U’ exists, C is re-
moved from the model. If U’ was found, W is re-
placed by U, which contains the L2 words
corresponding to the concepts in U’. When the re-
placement occurs, it is possible that not all such
words have already been taught; when a new word
is taught, we add it to all such classes U (easily
implemented using the new word’s translation,
which is given when it is introduced.)
In the above example, the words in W are ‘eigo‘
and ‘nihongo’, with corresponding concepts ‘Eng-
lish’ and ‘Japanese’. Both are contained in W’, the
‘language names’ category, so in this case U’
equals W’. The language names category contains
concepts for many other language names, includ-
ing Korean, so it suffices to teach our learner the
Japanese word for Korean (‘kankokugo’) at some
point in the future in order to update the construc-
tion to be (&lt;eigo, nihongo, kankokugo&gt; ga wakari
mas). This creates a new sentence ‘kankokugo ga
wakari mas’ meaning ‘I understand Korean’. An
</bodyText>
<page confidence="0.998691">
48
</page>
<bodyText confidence="0.989795950819672">
example in which U’ does not equal W’ is given in
Table 1 by ‘child’ and ‘car’.
L2 words might be ambiguous – several con-
cepts might correspond to a single word. Because
example semantics are not explicitly represented,
our system has no way of knowing which concept
is the correct one for a given construction, so it
considers all possibilities. For example, the Japa-
nese ‘ni’ means both ‘two’ and ‘at/in’, so when
attempting to generalize a construction in which
‘ni’ appears in an argument class, ECC would con-
sider both the ‘numbers’ and ‘prepositions’ con-
cepts.
The degree of certainty assigned to the new con-
struction by ECC is a function of the quality of the
match between W and U’. The more abstract is U,
the lower the certainty.
The main form-based induction algorithm is
Shared Prefix, Generated Suffix (SPGS). Given
an example ‘x y’ (x, y are word sequences), if there
exist (1) an example of the form ‘x z’, (2) an ex-
ample ‘x’, and (3) a construction K that derives ‘z’
or ‘y’, we create the construction (x K) having a
degree of certainty lower than that of K. A Shared
Suffix version can be defined similarly. Require-
ment (2) ensures that the cut after the prefix will
not be arbitrary, and assumes that the lesson author
presents constituents as partial examples before-
hand (as indeed is the case in our corpus).
SPGS utilizes the learner’s current generative
capacity. Assume input ‘watashi wa biru o nomi
mas’ (‘I drink beer’), previous inputs ‘watashi wa
america jin des’ (‘I am American’), ‘watashi wa’
(‘as to me...’) and an existing construction K =
(&lt;biru, wain&gt; o nomi mas). SPGS would create the
construction (watashi wa K), yielding the new sen-
tence ‘watashi wa wain o nomi mas’ (‘I drink
wine’).
To enable faster learning of more abstract con-
structions, we use generalized versions of SWD
and SPGS, which allow the differing or shared
elements to be a construction rather than a word or
a word sequence.
The combined learning algorithm is: given a
new example, iteratively invoke each of the above
algorithms at the given order until nothing new can
be learned. Our system is thus a kind of inductive
programming system (see [Thompson99] for a sys-
tem using inductive logic programming for seman-
tic parsing).
Note that the above algorithms treat words as
atomic units, so they can only learn morphological
rules if boundaries between morphemes are
marked in the corpus. They are thus more useful
for languages such as Japanese than, say, for Ro-
mance or Semitic languages.
Our algorithms have been motivated by general
cognitive considerations. It is possible to refine
them even further, e.g. by assigning a higher cer-
tainty when the focus element is a prefix or a suf-
fix, which are more conspicuous cognitively.
</bodyText>
<sectionHeader confidence="0.997569" genericHeader="evaluation">
6 Results and Application to Authoring of
</sectionHeader>
<subsectionHeader confidence="0.900353">
Learning Corpora
</subsectionHeader>
<bodyText confidence="0.999991">
We have experimented with our model using the
Pimsleur Japanese I (for English speakers) course,
which comprises 30 half-hour lessons, 1823 differ-
ent examples, and about 350 words. We developed
a simple set of tools to assist transcription, using an
arbitrary, consistent Latin script transliteration
based on how the Japanese phonemes are pre-
sented in the course, which differs at places from
common transliterations (e.g., we use ‘mas’, not
‘masu’). Word boundaries were marked during
transliteration, as justified in section 4.
Example sentences from the corpus are ‘nani o
shi mas kaa ? / what are you going to do?’, ‘wa-
tashi ta chi wa koko ni i mas / we are here’, ‘kyo
wa kaeri masen / today I am not going back’,
‘demo hitori de kaeri mas / but I am going to return
alone’, etc. Sentences are relatively short and ap-
propriate for a beginner level learner.
Evaluating the quality of induced language
models is notoriously difficult. Current FLA prac-
tice favors comparison of predicted parses with
ones in human annotated corpora. We have fo-
cused on another basic task of a grammar, sentence
enumeration, with the goal of showing that our
model is useful for a real application, assistance for
authoring of learning corpora.
The algorithm has learned 113 constructions
from the 1823 examples, generating 525 new sen-
tences. These numbers do not include construc-
tions that are subsumed by more abstract ones
(generating a superset of their sentences) or those
involving number words, which would distort the
count upwards. The number of potential new sen-
tences is much higher: these numbers are based
only on the 350 words present, organized in a
rather flat CS. The constructions contain many
</bodyText>
<page confidence="0.9987">
49
</page>
<bodyText confidence="0.999982129032259">
placeholders for concepts whose words would be
taught in the future, which could increase the num-
ber exponentially.
In terms of precision, 514 of the 525 sentences
were judged (by humans) to be syntactically cor-
rect (53 of those were problematic semantically).
Regarding recall, it is very difficult to assess for-
mally. Our subjective impression is that the learned
constructions do cover most of what a reasonable
person would learn from the examples, but this is
not highly informative – as indicated, the algo-
rithms were discovered by following our own in-
herence processes. In any case, our algorithms
have been deliberately designed to be conservative
to ensure precision, which we consider more im-
portant than recall for our model and application.
There is no available standard benchmark to
serve as a baseline, so we used a simpler version of
our own system as a baseline. We modified ECC to
not remove C in case of failure of concept match
(see ECC’s definition in section 5). The number of
constructions generated after seeing 1300 exam-
ples is 3,954 (yielding 35,429 sentences), almost
all of which are incorrect.
The applicative scenario we have in mind is the
following. The corpus author initially specifies the
desired target vocabulary and the desired syntacti-
cal constructs, by writing examples (the easiest
interface for humans). Vocabulary is selected ac-
cording to linguistic or subject (e.g., tourism,
sports) considerations. The examples are fed one
by one into the model (see Table 1). For a single
word example, its corresponding concepts are first
manually added to the CS.
The system now lists the constructions learned.
For a beginner level and the highest degree of cer-
tainty, the sentences licensed by the model can be
easily grasped just by looking at the constructions.
The fact that our model’s representations can be
easily communicated to people is also an advan-
tage from an SLA theory point of view, where ‘fo-
cus on form’ is a major topic [Gass01]. For
advanced levels or lower certainties, viewing the
sentences themselves (or a sample, when their
number gets too large) might be necessary.
The author can now check the learned items for
errors. There are two basic error types, errors
stemming from model deficiencies and errors that
human learners would make too. As an example of
the former, wrong generalizations may result from
discrepancies between the modeled conceptual sys-
tem and that of a real person. In this case the au-
thor fixes the modeled CS. Discovering errors of
the second kind is exactly the point where the
model is useful. To address those, the author usu-
ally introduces new full or partial examples that
would enable the learner to induce correct syntax.
In extreme cases there is no other practical choice
but to provide explicit linguistic explanations in
order to clarify examples that are very far from the
learner’s current knowledge. For example, English
speakers might be confused by the variability of
the Japanese counting system, so it might be useful
to insert an explanation of the sort ‘X is usually
used when counting long and thin objects, but be
aware that there are exceptions’. In the scenario of
Table 1, the author might eventually notice that the
learner is not aware that when speaking of some-
body else’s child a more polite reference is in or-
der, which can be fixed by giving examples
followed by an explanation. The DOC can be used
to draw the author’s attention to potential prob-
lems.
Preparation of the CS is a sensitive issue in our
model, because it is done manually while it is not
clear at all what kind of CS people have (WordNet
is sometimes criticized for being arbitrary, too fine,
and omitting concepts). We were highly conserva-
tive in that only concepts that are clearly part of the
conceptual system of English speakers before any
exposure to Japanese were included. Our task is
made easier by the fact that it is guided by words
actually appearing in the corpus, whose number is
not large, so that it took only about one hour to
produce a reasonable CS. Example categories are
names (for languages, places and people), places
(park, station, toilet, hotel, restaurant, shop, etc),
people (person, friend, wife, husband, girl, boy),
food, drink, feelings towards something (like,
need, want), self motion activities (arrive, come,
return), judgments of size, numbers, etc. We also
included language-related categories such as pro-
nouns and prepositions.
</bodyText>
<sectionHeader confidence="0.999659" genericHeader="conclusions">
7 Discussion
</sectionHeader>
<bodyText confidence="0.9999175">
We have presented a computational model of sec-
ond language acquisition. SLA is a central subject
in linguistics theory and practice, and our main
contribution is in addressing it in computational
linguistics. The model’s learning algorithms are
unique in their usage of a conceptual system, and
</bodyText>
<page confidence="0.985026">
50
</page>
<bodyText confidence="0.999801863636363">
its generative capacity is unique in its support for
degrees of certainty. The model was tested on a
unique corpus.
The dominant trend in CL in the last years has
been the usage of ever growing corpora. We have
shown that meaningful learning can be achieved
from a small corpus when the corpus has been pre-
pared by a ‘good teacher’. Automatic identification
(and ordering) of corpora subsets from which
learning is effective should be a fruitful research
direction for CL.
We have shown that using a simple conceptual
system can greatly assist language learning algo-
rithms. Previous FLA algorithms have in effect
computed a CS simultaneously with the syntax;
decoupling the two stages could be a promising
direction for FLA.
The model presented here is the first computa-
tional SLA model and obviously needs to be ex-
tended to address more SLA phenomena. It is clear
that the powerful notion of certainty is only used in
a rudimentary manner. Future research should also
address constraints (e.g. for morphology and agree-
ment), recursion, explicit semantics (e.g. parsing
into a semantic representation), word segmenta-
tion, statistics (e.g. collocations), and induction of
new concept categories that result from the learned
language itself (e.g. the Japanese counting system).
An especially important SLA issue is L1 trans-
fer, which refers to the effect that the L1 has on the
learning process. In this paper the only usage of the
L1 part of the examples was for accessing a con-
ceptual system. Using the L1 sentences (and the
existing conceptual system) to address transfer is
an interesting direction for research, in addition to
using the L1 sentences for modeling sentence se-
mantics.
Many additional important SLA issues will be
addressed in future research, including memory,
errors, attention, noticing, explicit learning, and
motivation. We also plan additional applications,
such as automatic lesson generation.
Acknowledgement. We would like to thank Dan
Melamed for his comments on a related document.
</bodyText>
<sectionHeader confidence="0.998991" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999613648148148">
Brown Ralf, 2000, Automated Generalization of Trans-
lation Examples, COLING ’00.
Carl Michael, Way Andy, (eds), 2003, Recent Advances
in Example Based Machine Translation, Kluwer.
Chang Nancy, Gurevich Olya, 2004. Context-Driven
Construction Learning. Proceedings, Cognitive Sci-
ence ‘04.
Chapelle Carol, 2001. Computer Applications in SLA.
Cambridge University Press. .
Cicekli Ilyas, Gu”venir Altay, 2001, Learning Transla-
tion Templates from Bilingual Translational Exam-
ples. Applied Intelligence 15:57-76, 2001.
Clark Alexander, 2001. Unsupervised Language Acqui-
sition: Theory and Practice. PhD thesis, University of
Sussex.
Clark Eve Vivienne, 2003. First Language Acquisition.
Cambridge University Press.
Croft, William, 2001. Radical Construction Grammar.
Oxford University Press.
Edelman Shimon, Solan Zach, Horn David, Ruppin
Eytan, 2004. Bridging Computational, Formal and
Psycholinguistic Approaches to Language. Proceed-
ings, Cognitive Science ‘04.
Fromkin Victoria, Rodman Robert, Hyams Nina, 2002.
An Introduction to Language, 7th ed. Harcourt.
Gass Susan M, Selinker Larry, 2001. Second Language
Acquisition: an Introductory Course. 2nd ed. LEA
Publishing.
Goldberg Adele, 1995. Constructions: a Construction
Grammar Approach to Argument Structure. Chicago
University Press.
Klein Dan, 2005. The Unsupervised Learning of Natural
Language Structure. PhD Thesis, Stanford.
Levy Michael, 1997. Computer-Assisted Language
Learning. Cambridge University Press.
Maritxalar Montse, Diaz de Ilarraza Arantza, Oronoz
Maite, 1997. From Psycholinguistic Modelling of In-
terlanguage in SLA to a Computational Model.
CoNLL ’97.
Medin Douglas, Ross Brian, Markman Arthur, 2005.
Cognitive Psychology, 4th ed. John Wiley &amp; Sons.
Mitchell Rosamond, Myles Florence, 2003. Second
Language Learning Theories. 2nd ed. Arnold Publica-
tion.
Pimsleur 2005. www.simonsays.com, under ‘foreign
language instruction’.
Somers Harold, 2001. Example-based Machine Transla-
tion. Machine Translation 14:113-158.
Thompson Cynthia, Califf Mary Elaine, Mooney Ray-
mond, 1999. Active Learning for Natural Language
Parsing and Information Extraction. ICML ’99.
Tomasello Michael, 2003. Constructing a Language: a
Usage Based Theory of Language Acquisition. Har-
vard University Press.
</reference>
<page confidence="0.993648">
51
</page>
<figure confidence="0.728458375">
Construction DOC Source Comment
1 anata / you 0 example
2 watashi / I 0 example
3 anata no / your 0 example
4 watashi no / my 0 example
5 (&lt;anata,watashi&gt; no ) 0 SWI)(3,4) The first words of 3 and 4 are different, the
rest is identical.
6 (W no), where W is &lt;anata, -1 ECC(5) The concept category W’={I, you, we} was
</figure>
<bodyText confidence="0.914475">
watashi, Japanese word for found in the CS. We know how to say ‘I’ and
‘we’&gt; ‘you’, but not ‘we’.
7 watashi ta chi / we 0 example
8 (W no), where W is -2 ECC(6,7) We were taught how to say ‘we’, and an
&lt;anata, watashi, watashi ta empty slot for it was found in 6.
chi&gt;
Now we can generate a new sentence: ‘wa-
tashi ta chi no’, whose meaning (‘our’) is
inferred from the meaning of construction 6.
9 chiisai / small 0 example
10 kuruma / car 0 example
11 chiisai kuruma / a small car 0 example
12 watashi ta chi no kuruma / our 0 example
car
13 ((W no) kuruma) -3 SSGP (12, Shared Suffix Generated Prefix:
11, 10, 8) (0) new example 12 = ‘y x’ (x: kuruma)
(1) existing example 11 = ‘z x’
(2) existing example 10 = ‘x’
(3) construction K (#8) deriving ‘y’
learns the new construction (K x)
Now we can generate a new sentence: ‘wa-
tashi no kuruma’, meaning ‘my car’.
14 kodomo / child 0 example
... ... 0 examples Skipping a few examples...
20 ((W no) kodomo) -3 ... This construction was learned using the
skipped examples.
21 ((W no) &lt;kuruma, kodomo&gt;) -3 SWI) (13, Note that the shared element is a construction
20) this time, not a sub-sentence.
22 ((W no) P), where P is the set -4 ECC (21) The smallest category that contains the con-
of Japanese words for physi- cepts ‘car’ and ‘child’ is P’=PhysicalThings.
cal things (animate or inani-
mate)
Now we can generate many new sen-
tences, meaning ‘my X’ where X is any
Japanese word we will learn in the future
denoting a physical thing.
</bodyText>
<tableCaption confidence="0.9465102">
Table 1: A learning scenario. For simplicity, the degree of certainty here is computed by adding that of the algorithm
type to that of the most uncertain construction used. Note that the notation used was designed for succinct presen-
tation and is not the optimal one for authors of learning corpora (for example, it is probably easier to visualize the
sentences generated by construction #22 if it were shown as ((&lt;watashi, anata, watashi ta chi&gt; no) &lt;kuruma,
kodomo&gt;).)
</tableCaption>
<page confidence="0.997784">
52
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.808809">
<title confidence="0.999319">A Second Language Acquisition Model Example Generalization and Concept Categories</title>
<author confidence="0.999835">Ari Rappoport Vera Sheinman</author>
<affiliation confidence="0.999883">Institute of Computer Science Institute of Computer Science The Hebrew University The Hebrew University</affiliation>
<address confidence="0.999929">Jerusalem, Israel Jerusalem, Israel</address>
<email confidence="0.88933">arir@cs.huji.ac.ilvera46@cl.cs.titech.ac.jp</email>
<abstract confidence="0.990268076923077">We present a computational model of acquiring a second language from example sentences. Our learning algorithms build a construction grammar language model, and generalize using form-based patterns and the learner’s conceptual system. We use a unique professional language learning corpus, and show that substantial reliable learning can be achieved even though the corpus is very small. The model is applied to assisting the authoring of Japanese language learning corpora.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Brown Ralf</author>
</authors>
<title>Automated Generalization of Translation Examples,</title>
<date>2000</date>
<journal>COLING</journal>
<volume>00</volume>
<marker>Ralf, 2000</marker>
<rawString>Brown Ralf, 2000, Automated Generalization of Translation Examples, COLING ’00.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carl Michael</author>
<author>Way Andy</author>
</authors>
<date>2003</date>
<booktitle>Recent Advances in Example Based Machine Translation,</booktitle>
<publisher>Kluwer.</publisher>
<marker>Michael, Andy, 2003</marker>
<rawString>Carl Michael, Way Andy, (eds), 2003, Recent Advances in Example Based Machine Translation, Kluwer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chang Nancy</author>
<author>Gurevich Olya</author>
</authors>
<date>2004</date>
<journal>Context-Driven Construction Learning. Proceedings, Cognitive Science</journal>
<volume>04</volume>
<marker>Nancy, Olya, 2004</marker>
<rawString>Chang Nancy, Gurevich Olya, 2004. Context-Driven Construction Learning. Proceedings, Cognitive Science ‘04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chapelle Carol</author>
</authors>
<title>Computer Applications in SLA.</title>
<date>2001</date>
<publisher>Cambridge University Press. .</publisher>
<marker>Carol, 2001</marker>
<rawString>Chapelle Carol, 2001. Computer Applications in SLA. Cambridge University Press. .</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cicekli Ilyas</author>
</authors>
<title>Gu”venir Altay,</title>
<date>2001</date>
<journal>Applied Intelligence</journal>
<pages>15--57</pages>
<marker>Ilyas, 2001</marker>
<rawString>Cicekli Ilyas, Gu”venir Altay, 2001, Learning Translation Templates from Bilingual Translational Examples. Applied Intelligence 15:57-76, 2001.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clark Alexander</author>
</authors>
<title>Unsupervised Language Acquisition: Theory and Practice.</title>
<date>2001</date>
<tech>PhD thesis,</tech>
<institution>University of Sussex.</institution>
<marker>Alexander, 2001</marker>
<rawString>Clark Alexander, 2001. Unsupervised Language Acquisition: Theory and Practice. PhD thesis, University of Sussex.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Clark Eve Vivienne</author>
</authors>
<title>First Language Acquisition.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<marker>Vivienne, 2003</marker>
<rawString>Clark Eve Vivienne, 2003. First Language Acquisition. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William Croft</author>
</authors>
<title>Radical Construction Grammar.</title>
<date>2001</date>
<publisher>Oxford University Press.</publisher>
<marker>Croft, 2001</marker>
<rawString>Croft, William, 2001. Radical Construction Grammar. Oxford University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Edelman Shimon</author>
<author>Solan Zach</author>
<author>Horn David</author>
<author>Ruppin Eytan</author>
</authors>
<title>Bridging Computational, Formal and Psycholinguistic Approaches to Language. Proceedings,</title>
<date>2004</date>
<journal>Cognitive Science</journal>
<volume>04</volume>
<marker>Shimon, Zach, David, Eytan, 2004</marker>
<rawString>Edelman Shimon, Solan Zach, Horn David, Ruppin Eytan, 2004. Bridging Computational, Formal and Psycholinguistic Approaches to Language. Proceedings, Cognitive Science ‘04.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Fromkin Victoria</author>
<author>Rodman Robert</author>
<author>Hyams Nina</author>
</authors>
<title>An Introduction to Language, 7th ed.</title>
<date>2002</date>
<publisher>Harcourt.</publisher>
<marker>Victoria, Robert, Nina, 2002</marker>
<rawString>Fromkin Victoria, Rodman Robert, Hyams Nina, 2002. An Introduction to Language, 7th ed. Harcourt.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gass Susan M</author>
<author>Selinker Larry</author>
</authors>
<date>2001</date>
<booktitle>Second Language Acquisition: an Introductory Course. 2nd</booktitle>
<editor>ed.</editor>
<publisher>LEA Publishing.</publisher>
<marker>M, Larry, 2001</marker>
<rawString>Gass Susan M, Selinker Larry, 2001. Second Language Acquisition: an Introductory Course. 2nd ed. LEA Publishing.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Goldberg Adele</author>
</authors>
<title>Constructions: a Construction Grammar Approach to Argument Structure.</title>
<date>1995</date>
<publisher>Chicago University Press.</publisher>
<marker>Adele, 1995</marker>
<rawString>Goldberg Adele, 1995. Constructions: a Construction Grammar Approach to Argument Structure. Chicago University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Klein Dan</author>
</authors>
<title>The Unsupervised Learning of Natural Language Structure. PhD Thesis,</title>
<date>2005</date>
<location>Stanford.</location>
<marker>Dan, 2005</marker>
<rawString>Klein Dan, 2005. The Unsupervised Learning of Natural Language Structure. PhD Thesis, Stanford.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Levy Michael</author>
</authors>
<title>Computer-Assisted Language Learning.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<marker>Michael, 1997</marker>
<rawString>Levy Michael, 1997. Computer-Assisted Language Learning. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Maritxalar Montse</author>
</authors>
<title>Diaz de Ilarraza Arantza, Oronoz Maite,</title>
<date>1997</date>
<journal>CoNLL</journal>
<volume>97</volume>
<marker>Montse, 1997</marker>
<rawString>Maritxalar Montse, Diaz de Ilarraza Arantza, Oronoz Maite, 1997. From Psycholinguistic Modelling of Interlanguage in SLA to a Computational Model. CoNLL ’97.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Medin Douglas</author>
<author>Ross Brian</author>
<author>Markman Arthur</author>
</authors>
<date>2005</date>
<booktitle>Cognitive Psychology, 4th ed.</booktitle>
<publisher>John Wiley &amp; Sons.</publisher>
<marker>Douglas, Brian, Arthur, 2005</marker>
<rawString>Medin Douglas, Ross Brian, Markman Arthur, 2005. Cognitive Psychology, 4th ed. John Wiley &amp; Sons.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell Rosamond</author>
<author>Myles Florence</author>
</authors>
<title>Second Language Learning Theories. 2nd ed. Arnold Publication.</title>
<date>2003</date>
<marker>Rosamond, Florence, 2003</marker>
<rawString>Mitchell Rosamond, Myles Florence, 2003. Second Language Learning Theories. 2nd ed. Arnold Publication.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pimsleur</author>
</authors>
<title>www.simonsays.com, under ‘foreign language instruction’.</title>
<date>2005</date>
<marker>Pimsleur, 2005</marker>
<rawString>Pimsleur 2005. www.simonsays.com, under ‘foreign language instruction’.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Somers Harold</author>
</authors>
<title>Example-based Machine Translation.</title>
<date>2001</date>
<journal>Machine Translation</journal>
<pages>14--113</pages>
<marker>Harold, 2001</marker>
<rawString>Somers Harold, 2001. Example-based Machine Translation. Machine Translation 14:113-158.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thompson Cynthia</author>
<author>Califf Mary Elaine</author>
<author>Mooney Raymond</author>
</authors>
<title>Active Learning for Natural Language Parsing and Information Extraction.</title>
<date>1999</date>
<journal>ICML</journal>
<volume>99</volume>
<marker>Cynthia, Elaine, Raymond, 1999</marker>
<rawString>Thompson Cynthia, Califf Mary Elaine, Mooney Raymond, 1999. Active Learning for Natural Language Parsing and Information Extraction. ICML ’99.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomasello Michael</author>
</authors>
<title>Constructing a Language: a Usage Based Theory of Language Acquisition.</title>
<date>2003</date>
<publisher>Harvard University Press.</publisher>
<marker>Michael, 2003</marker>
<rawString>Tomasello Michael, 2003. Constructing a Language: a Usage Based Theory of Language Acquisition. Harvard University Press.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>