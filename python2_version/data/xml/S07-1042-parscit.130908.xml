<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.001651">
<title confidence="0.954287">
JHU1 : An Unsupervised Approach to Person Name Disambiguation
using Web Snippets
</title>
<author confidence="0.993022">
Delip Rao Nikesh Garera David Yarowsky
</author>
<affiliation confidence="0.91606">
Dept. of Computer Science
Johns Hopkins University
</affiliation>
<address confidence="0.921871">
Baltimore, MD 21218
</address>
<email confidence="0.990372">
{delip, ngarera, yarowsky}@cs.jhu.edu
</email>
<sectionHeader confidence="0.989894" genericHeader="abstract">
2 Approaches
</sectionHeader>
<bodyText confidence="0.961767866666667">
Our framework focuses on the K-means clustering
model using both bag of words as features and vari-
ous augumented feature sets. We experimented with
several similarity functions and chose Pearson’s cor-
relation coefficient1 as the distance measure for clus-
tering. The weights for the features were set to the
term frequency of their respective words in the doc-
ument.2
2.1 Submitted system: Clustering using Web
Snippets
We queried the Google search engine with the
target person names and extracted up to the top
one thousand results. For each result we also
extracted the snippet associated with it. An example
is shown below in Figure 2.1. As can be seen the
</bodyText>
<sectionHeader confidence="0.667116" genericHeader="keywords">
Abstract
</sectionHeader>
<bodyText confidence="0.999890428571429">
This paper presents an approach to person
name disambiguation using K-means clus-
tering on rich-feature-enhanced document
vectors, augmented with additional web-
extracted snippets surrounding the polyse-
mous names to facilitate term bridging. This
yields a significant F-measure improvement
on the shared task training data set. The pa-
per also illustrates the significant divergence
between the properties of the training and
test data in this shared task, substantially
skewing results. Our system optimized on
F0.2 rather than F0.5 would have achieved
top performance in the shared task.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.997180928571428">
Being able to automatically distinguish between
John Doe, the musician, and John Doe, the actor, on
the Web is a task of significant importance with ap-
plications in IR and other information management
tasks. Mann and Yarowsky (2004) used bigograph-
ical data annotated with named entitities and per-
form fusion of extracted information across multiple
documents. Bekkerman and McCallum (2005) stud-
ied the problem in a social network setting exploit-
ing link topology to disambiguate namesakes. Al-
Kamha and Embley (2004) used a combination of
attributes (like zipcodes, state, etc.), links, and page
similarity to derive the name clusters while Wan et.
al. (2005) used lexical features and named entities.
</bodyText>
<figureCaption confidence="0.997882">
Figure 1: Google snippet for “Dekang Lin”
</figureCaption>
<bodyText confidence="0.997615666666667">
snippets contain high quality, low noise features that
could be used to improve the performance of the
system. Each snippet was treated as a document and
</bodyText>
<footnote confidence="0.9973285">
1This performs better than the standard measures like Eu-
clidean and Cosine with K-means clustering on this data.
2We found that using TF weights instead of TF-IDF weights
gives a better performance on this task.
</footnote>
<page confidence="0.985505">
199
</page>
<bodyText confidence="0.946763695652174">
Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 199–202,
Prague, June 2007. c�2007 Association for Computational Linguistics
clustered along with the supplied documents. This
process is illustrated in Figure 2. The following
example illustrates how these web snippets can
improve performance by lexical transitivity. In
this hypothetical example, a short test document
contains a Canadian postal code (T6G 2H1) not
found in any of the training documents. However,
there may exist an additional web page not in the
training or test data which contains both this term
and also overlap with other terms in the training data
(e.g. 492-9920), serving as an effective transitive
bridge between the two.
Training Document 1 492-9920, not(T6G 2H1)
Web Snippet 2 both 492-9920, T6G 2H1
Test Document 3 T6G 2H1, not(492-9920)
Thus K-means clustering is likely to cluster the
three documents above together while without this
transitive bridge the association between training
and test documents is much less strong. The final
clustering of the test data is simply a projection with
the training documents and web snippets removed.
</bodyText>
<figureCaption confidence="0.979714">
Figure 2: Clustering using Web Snippets
</figureCaption>
<subsectionHeader confidence="0.998686">
2.2 Baselines
</subsectionHeader>
<bodyText confidence="0.997245">
In this section we describe several trivial baselines:
</bodyText>
<listItem confidence="0.973482571428572">
1. Singletons: A clustering where each cluster
has only one document hence number of clus-
ters is same as the number of documents.
2. One Cluster: A clustering with only one clus-
ter containing all documents.
3. Random: A clustering scheme which parti-
tions the documents uniformly at random into
</listItem>
<bodyText confidence="0.955663571428571">
K clusters, where the value of K were the op-
timal K on the training and test data.
These results are summarized in Table 1. Note that
all average F-scores mentioned in this table and the
rest of the paper are microaverages obtained by av-
eraging the purity and invese purity over all names
and then calculating the F-score.
</bodyText>
<table confidence="0.9987334">
Train Test
Baseline F0.2 F0.5 F0.2 F0.5
Singletons .676 .511 .843 .730
One Cluster .688 .638 .378 .327
Random .556 .493 .801 .668
</table>
<tableCaption confidence="0.99979">
Table 1: Baseline performance
</tableCaption>
<subsectionHeader confidence="0.990936">
2.3 K-means on Bag of Words model
</subsectionHeader>
<bodyText confidence="0.999988">
The standard unaugumented Bag of Words model
achieves F0.5 of 0.666 on training data, as shown
in Table 2.
</bodyText>
<subsectionHeader confidence="0.999146">
2.4 Part of speech tag features
</subsectionHeader>
<bodyText confidence="0.999976333333333">
We then consider only terms that are nouns (NN,
NNP) and adjectives (JJ) with the intuition that
most of the content bearing words and descriptive
words that disambiguate a person would fall in these
classes. The result then improves to 0.67 on the
training data.
</bodyText>
<subsectionHeader confidence="0.995724">
2.5 Rich features
</subsectionHeader>
<bodyText confidence="0.9998915">
Another variant of this system, that we call Rich-
Feats, gives preferential weighting to terms that are
immediately around all variants of the person name
in question, place names, occupation names, and
titles. For marking up place names, occupation
names, and titles we used gazetteer3 lookup with-
out explicit named entity disambiguation. The key-
words that appeared in the HTML tag &lt;META ..&gt;
were also given higher weights. This resulted in an
F0.5 of 0.664.
</bodyText>
<subsectionHeader confidence="0.967864">
2.6 Snippets from the Web
</subsectionHeader>
<bodyText confidence="0.998148">
The addition of web snippets as described in Sec-
tion 2.1 yeilds a significant F0.5 improvement to
0.72.
</bodyText>
<footnote confidence="0.9248475">
3Totalling 19646 terms, gathered from publicly available re-
sources on the web. Further details are available on request.
</footnote>
<table confidence="0.77464">
Initial clusters of web snippets + test documents
Projection of test documents
Test document
Web snippet document
</table>
<page confidence="0.906885">
200
</page>
<subsectionHeader confidence="0.918846">
2.7 Snippets and Rich features
</subsectionHeader>
<bodyText confidence="0.9971346">
This is a combination of the models mentioned in
Sections 2.5 and 2.6. This model combination re-
sulted in a slight degradation of performance over
snippets by themselves on the training data but a
slight improvement on test data.
</bodyText>
<table confidence="0.999797333333333">
Model K F0.2 F0.5
Vanilla BOW 10% 0.702 0.666
BOW + PoS 10% 0.706 0.670
BOW + RichFeats 10% 0.700 0.664
Snippets 10 0.721 0.718
Snippets + RichFeats 10 0.714 0.712
</table>
<tableCaption confidence="0.99929">
Table 2: Performance on Training Data
</tableCaption>
<sectionHeader confidence="0.740116" genericHeader="method">
3 Selection of Parameters
</sectionHeader>
<bodyText confidence="0.92314428">
The main parameter for K-means clustering is
choosing the number of clusters, K. We optimized
K over the training data varying K from 10%,
20%,· · ·,100% of the number of documents as well
as varying absolute K values from 10, 20, · · · to 100
documents.4 The evaluation score of F-measure can
be highly sensitive to this parameter K, as shown
in Table 3. The value of K that gives the best F-
measure on training set using vanilla bag of words
(BOW) model is K = 10%, however we see in Ta-
ble 3 that this value of K actually performs much
worse on the test data as compared to other K val-
ues.
4 Training/Test discrepancy and
re-evaluation using cross validation on
test data
Table 4 compares cluster statistics between the train-
ing and test data. This data was derived from Artiles
et. al (2007). The large difference between aver-
age number of clusters in training and test sets in-
dicates that the parameter K, optimized on training
set cannot be transferred to test set as these two sets
belong to a very different distribution. This can be
emprically seen in Table 3 where applying the best
K on training results in a significant performance
</bodyText>
<footnote confidence="0.68537825">
4We discard the training and test documents that have no text
content, thus the absolute value K = 10 and percentage value K
= 10% can result in different K’s, even if name had originally
100 documents to begin with.
</footnote>
<bodyText confidence="0.995835">
drop on test set given this divergence when param-
eters are optimized for F0.5 (although performance
does transfer well when parameters are optimized on
F0.2). This was observed in our primary evaluation
system which was optimized for F0.5 and resulted in
a low official score of F0.5 = .53 and F0.2 = .65.
</bodyText>
<table confidence="0.9996945">
Train Test
K F0.2 F0.5 F0.2 F0.5
10% .702 .666 .527 .600
20% .716 .644 .617 .630
30% .724 .631 .683 .676
40% .724 .618 .728 .705
50% .732 .614 .762 .724
60% .731 .601 .798 .747
70% .730 .593 .832 .766
80% .732 .586 .855 .773
90% .714 .558 .861 .764
100% .670 .502 .843 .730
</table>
<tableCaption confidence="0.994706">
Table 3: Selecting the optimal parameter on training
</tableCaption>
<bodyText confidence="0.972308571428572">
data and application to test data
Thus an interesting question is to measure per-
formance when parameters are chosen on data shar-
ing the distributional character of the test data rather
than the highly divergent training set. To do this, we
used a standard 2-fold cross validation to estimate
clustering parameters from a held-out, alternate-half
portion of the test data5, which more fairly repre-
sents the character of the other half of the test data
than does the very different training data. We di-
vide the test set into two equal halves (taking first
fifteen names alphabetically in one set and the rest
in another). We optimize K on the first half, test
on the other half and vice versa. We report the two
K-values and their corresponding F-measures in Ta-
ble 5 and we also report the average in order to com-
pare it with the results on the test set obtained using
K optimized on training. Further, we also report
what would be oracle best K, that is, if we optimize
K on the entire test data 6. We can see in Table 5
that how optimizing K on a devlopment set with
</bodyText>
<footnote confidence="0.999023714285714">
5This also prevents overfitting as the two halves for training
and testing are disjoint.
6By oracle best K we mean the K obtained by optimizing
over the entire test data. Note that, the oracle best K is just
for comparison because it would be unfair to claim results by
optimizing K on the entire test set, all our claimed results for
different models are based on 2-fold cross validation.
</footnote>
<page confidence="0.997666">
201
</page>
<bodyText confidence="0.9997836875">
same distribution as test set can give us F-measure
in the range of 77%, a significant increase as com-
pared to the F-measure obtained by optimizing K on
given training data. Further, Table 5, also indicates
results by a custom clustering method, that takes the
best K-means clustering using vanilla bag of words
model, retains the largest cluster and splits all the
other clusters into singleton clusters. This method
gives an improved 2-fold F-measure score over the
simple bag of words model, implying that most of
the namesakes in test data have one (or few) domi-
nant cluster and a lot of singleton clusters. Table 6
shows a full enumeration of model variance under
this cross validated test evaluation. POS and Rich-
Feats yield small gains, and a best F0.5 performance
of .776.
</bodyText>
<table confidence="0.9993295">
Data set cluster size # of clusters
Mean Variance Mean Variance
Train 5.4 144.0 10.8 146.3
Test 3.1 26.5 45.9 574.1
</table>
<tableCaption confidence="0.8305735">
Table 4: Cluster statistics from the test and training
data
</tableCaption>
<table confidence="0.999949666666667">
Data set K F0.2 F0.5
F0.5 Best K on train 10% .702 .666
F0.2 Best K on train 10 .707 .663
Best K on train 10% .527 .560
applied to test 10 .540 .571
2Fold on Test 80 .847 .748
80% .862 .793
.854* .771*
2Fold on Single 80 .847 .749
Largest Cluster 80 .866 .795
.856* .772*
Oracle on Test 80 .858 .774
</table>
<tableCaption confidence="0.964979666666667">
Table 5: Comparision of training and test results us-
ing Vanilla Bag-of-words model. The values indi-
cated with * represent the average value.
</tableCaption>
<sectionHeader confidence="0.998356" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999891571428571">
We presented a K-means clustering approach for the
task of person name disambiguation using several
augmented feature sets including HTML meta fea-
tures, part-of-speech-filtered features, and inclusion
of additional web snippets extracted from Google
to facilitate term bridging. The latter showed sig-
nificant empirical gains on the training data. Best
</bodyText>
<table confidence="0.999657909090909">
Model K F0.2 F0.5
Vanilla BOW 80/ .847/.862 .749/.793
80% Avg = .854 Avg = .771
BOW + PoS 80%/ .844/.865 .749/.795
80% Avg = .854 Avg = .772
BOW 80%/ .847/.868 .754/.798
RichFeats 80% Avg = .858 Avg = .776
Snippets 50%/ .842/.875 .746/.800
50% Avg = .859 Avg = .773
Snippets + 40%/ .836/.874 .750/.798
RichFeats 50% Avg = .855 Avg = .774
</table>
<tableCaption confidence="0.999686">
Table 6: Performance on 2Fold Test Data
</tableCaption>
<bodyText confidence="0.999343533333333">
performance on test data, when parameters are op-
timized for F0.2 on training (Table 3), yielded a top
performing F0.2 of .855 on test data (and F0.5=.773
on test data). We also explored the striking discrep-
ancy between training and test data characteristics
and showed how optimizing the clustering param-
eters on given training data does not transfer well
to the divergent test data. To control for similar
training and test distributional characteristics, we re-
evaluated our test results estimating clustering pa-
rameters from alternate held-out portions of the test
set. Our models achieved cross validated F0.5 of .77-
.78 on test data for all feature combinations, further
showing the broad strong performance of these tech-
niques.
</bodyText>
<sectionHeader confidence="0.99925" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9977828">
Reema Al-Kamha and David W. Embley. 2004. Grouping
search-engine returned citations for person-name queries. In
Proceedings of the 6th annual ACM international workshop
on Web information and data management, pages 96–103.
Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2007. Eval-
uation: Establishing a benchmark for the web people search
task. In Proceedings of Semeval 2007, Association for Com-
putational Linguistics.
Ron Bekkerman and Andrew McCallum. 2005. Disambiguat-
ing web appearances of people in a social network. In Pro-
ceedings of the 14th international conference on World Wide
Web, pages 463–470.
Gideon S. Mann and David Yarowsky. 2004. Unsupervised
personal name disambiguation. In Proceedings of the sev-
enth conference on Natural language learning (CONLL),
pages 33–40.
Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding. 2005.
Person resolution in person search results: Webhawk. In
Proceedings of the 14th ACM international conference on
Information and knowledge management, pages 163–170.
</reference>
<page confidence="0.998286">
202
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.479294">
<title confidence="0.983042">JHU1 : An Unsupervised Approach to Person Name Disambiguation using Web Snippets</title>
<author confidence="0.99924">Delip Rao Nikesh Garera David Yarowsky</author>
<affiliation confidence="0.999621">Dept. of Computer Science Johns Hopkins University</affiliation>
<address confidence="0.999931">Baltimore, MD 21218</address>
<email confidence="0.63289">ngarera,</email>
<abstract confidence="0.994652266666666">2 Approaches Our framework focuses on the K-means clustering model using both bag of words as features and various augumented feature sets. We experimented with several similarity functions and chose Pearson’s coras the distance measure for clustering. The weights for the features were set to the term frequency of their respective words in the doc- 2.1 Submitted system: Clustering using Web Snippets We queried the Google search engine with the target person names and extracted up to the top one thousand results. For each result we also extracted the snippet associated with it. An example is shown below in Figure 2.1. As can be seen the Abstract This paper presents an approach to person name disambiguation using K-means clustering on rich-feature-enhanced document vectors, augmented with additional webextracted snippets surrounding the polysemous names to facilitate term bridging. This yields a significant F-measure improvement on the shared task training data set. The paper also illustrates the significant divergence between the properties of the training and test data in this shared task, substantially skewing results. Our system optimized on than would have achieved top performance in the shared task.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Reema Al-Kamha</author>
<author>David W Embley</author>
</authors>
<title>Grouping search-engine returned citations for person-name queries.</title>
<date>2004</date>
<booktitle>In Proceedings of the 6th annual ACM international workshop on Web information and data management,</booktitle>
<pages>96--103</pages>
<marker>Al-Kamha, Embley, 2004</marker>
<rawString>Reema Al-Kamha and David W. Embley. 2004. Grouping search-engine returned citations for person-name queries. In Proceedings of the 6th annual ACM international workshop on Web information and data management, pages 96–103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Javier Artiles</author>
<author>Julio Gonzalo</author>
<author>Felisa Verdejo</author>
</authors>
<title>Evaluation: Establishing a benchmark for the web people search task.</title>
<date>2007</date>
<booktitle>In Proceedings of Semeval 2007, Association for Computational Linguistics.</booktitle>
<marker>Artiles, Gonzalo, Verdejo, 2007</marker>
<rawString>Javier Artiles, Julio Gonzalo, and Felisa Verdejo. 2007. Evaluation: Establishing a benchmark for the web people search task. In Proceedings of Semeval 2007, Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ron Bekkerman</author>
<author>Andrew McCallum</author>
</authors>
<title>Disambiguating web appearances of people in a social network.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th international conference on World Wide Web,</booktitle>
<pages>463--470</pages>
<contexts>
<context position="1901" citStr="Bekkerman and McCallum (2005)" startWordPosition="288" endWordPosition="291">ficant divergence between the properties of the training and test data in this shared task, substantially skewing results. Our system optimized on F0.2 rather than F0.5 would have achieved top performance in the shared task. 1 Introduction Being able to automatically distinguish between John Doe, the musician, and John Doe, the actor, on the Web is a task of significant importance with applications in IR and other information management tasks. Mann and Yarowsky (2004) used bigographical data annotated with named entitities and perform fusion of extracted information across multiple documents. Bekkerman and McCallum (2005) studied the problem in a social network setting exploiting link topology to disambiguate namesakes. AlKamha and Embley (2004) used a combination of attributes (like zipcodes, state, etc.), links, and page similarity to derive the name clusters while Wan et. al. (2005) used lexical features and named entities. Figure 1: Google snippet for “Dekang Lin” snippets contain high quality, low noise features that could be used to improve the performance of the system. Each snippet was treated as a document and 1This performs better than the standard measures like Euclidean and Cosine with K-means clus</context>
</contexts>
<marker>Bekkerman, McCallum, 2005</marker>
<rawString>Ron Bekkerman and Andrew McCallum. 2005. Disambiguating web appearances of people in a social network. In Proceedings of the 14th international conference on World Wide Web, pages 463–470.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Gideon S Mann</author>
<author>David Yarowsky</author>
</authors>
<title>Unsupervised personal name disambiguation.</title>
<date>2004</date>
<booktitle>In Proceedings of the seventh conference on Natural language learning (CONLL),</booktitle>
<pages>33--40</pages>
<contexts>
<context position="1744" citStr="Mann and Yarowsky (2004)" startWordPosition="266" endWordPosition="269"> to facilitate term bridging. This yields a significant F-measure improvement on the shared task training data set. The paper also illustrates the significant divergence between the properties of the training and test data in this shared task, substantially skewing results. Our system optimized on F0.2 rather than F0.5 would have achieved top performance in the shared task. 1 Introduction Being able to automatically distinguish between John Doe, the musician, and John Doe, the actor, on the Web is a task of significant importance with applications in IR and other information management tasks. Mann and Yarowsky (2004) used bigographical data annotated with named entitities and perform fusion of extracted information across multiple documents. Bekkerman and McCallum (2005) studied the problem in a social network setting exploiting link topology to disambiguate namesakes. AlKamha and Embley (2004) used a combination of attributes (like zipcodes, state, etc.), links, and page similarity to derive the name clusters while Wan et. al. (2005) used lexical features and named entities. Figure 1: Google snippet for “Dekang Lin” snippets contain high quality, low noise features that could be used to improve the perfo</context>
</contexts>
<marker>Mann, Yarowsky, 2004</marker>
<rawString>Gideon S. Mann and David Yarowsky. 2004. Unsupervised personal name disambiguation. In Proceedings of the seventh conference on Natural language learning (CONLL), pages 33–40.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Xiaojun Wan</author>
<author>Jianfeng Gao</author>
<author>Mu Li</author>
<author>Binggong Ding</author>
</authors>
<title>Person resolution in person search results: Webhawk.</title>
<date>2005</date>
<booktitle>In Proceedings of the 14th ACM international conference on Information and knowledge management,</booktitle>
<pages>163--170</pages>
<marker>Wan, Gao, Li, Ding, 2005</marker>
<rawString>Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding. 2005. Person resolution in person search results: Webhawk. In Proceedings of the 14th ACM international conference on Information and knowledge management, pages 163–170.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>