<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000152">
<title confidence="0.999517666666667">
On Closed Task of Chinese Word Segmentation: An Improved CRF
Model Coupled with Character Clustering and
Automatically Generated Template Matching
</title>
<author confidence="0.9627885">
Richard Tzong-Han Tsai, Hsieh-Chuan Hung, Cheng-Lung Sung,
Hong-Jie Dai, and Wen-Lian Hsu
</author>
<affiliation confidence="0.9730205">
Intelligent Agent Systems Lab
Institute of Information Science, Academia Sinica
</affiliation>
<address confidence="0.982969">
No. 128, Sec. 2, Academia Rd., 115 Nankang, Taipei, Taiwan, R.O.C.
</address>
<email confidence="0.999198">
{thtsai,yabt,clsung,hongjie,hsu}@iis.sinica.edu.tw
</email>
<sectionHeader confidence="0.998601" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99993116">
This paper addresses two major prob-
lems in closed task of Chinese word
segmentation (CWS): tagging sentences
interspersed with non-Chinese words,
and long named entity (NE) identifica-
tion. To resolve the former, we apply K-
means clustering to identify non-Chinese
characters, and then adopt a two-tagger
architecture: one for Chinese text and the
other for non-Chinese text. For the latter
problem, we apply postprocessing to our
CWS output using automatically gener-
ated templates. The experiment results
show that, when non-Chinese characters
are sparse in the training corpus, our
two-tagger method significantly im-
proves the segmentation of sentences
containing non-Chinese words. Identifi-
cation of long NEs and long words is
also enhanced by template-based post-
processing. In the closed task of
SIGHAN 2006 CWS, our system
achieved F-scores of 0.957, 0.972, and
0.955 on the CKIP, CTU, and MSR cor-
pora respectively.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999918">
Unlike Western languages, Chinese does not
have explicit word delimiters. Therefore, word
segmentation (CWS) is essential for Chinese
text processing or indexing. There are two main
problems in the closed CWS task. The first is to
identify and segment non-Chinese word se-
quences in Chinese documents, especially in a
closed task (described later). A good CWS sys-
tem should be able to handle Chinese texts pep-
pered with non-Chinese words or phrases. Since
non-Chinese language morphologies are quite
different from that of Chinese, our approach
must depend on how many non-Chinese words
appear, whether they are connected with each
other, and whether they are interleaved with
Chinese words. If we can distinguish non-
Chinese characters automatically and apply dif-
ferent strategies, the segmentation performance
can be improved. The second problem in closed
CWS is to correctly identify longer NEs. Most
ML-based CWS systems use a five-character
context window to determine the current charac-
ter’s tag. In the majority of cases, given the con-
straints of computational resources, this com-
promise is acceptable. However, limited by the
window size, these systems often handle long
words poorly.
In this paper, our goal is to construct a general
CWS system that can deal with the above prob-
lems. We adopt CRF as our ML model.
</bodyText>
<sectionHeader confidence="0.910518" genericHeader="method">
2 Chinese Word Segmentation System
</sectionHeader>
<subsectionHeader confidence="0.98933">
2.1 Conditional Random Fields
</subsectionHeader>
<bodyText confidence="0.883891285714286">
Conditional random fields (CRFs) are undirected
graphical models trained to maximize a condi-
tional probability (Lafferty et al., 2001). A lin-
ear-chain CRF with parameters Λ={λ1, λ2, ...}
defines a conditional probability for a state se-
quence y = y1 ...yT , given that an input sequence
x = x1 ...xT is
</bodyText>
<equation confidence="0.853024">
( T
t=1 k a kfk (yt− 1 ,yt,x,t))
</equation>
<bodyText confidence="0.99971025">
where Zx is the normalization factor that makes
the probability of all state sequences sum to one;
fk(yt-1, yt, x, t) is often a binary-valued feature
function and λk is its weight. The feature
</bodyText>
<equation confidence="0.996998333333333">
1
(  |)
y x = exp
Zx
PA
,(1)
</equation>
<page confidence="0.928027">
134
</page>
<bodyText confidence="0.956421375">
Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 134–137,
Sydney, July 2006. c�2006 Association for Computational Linguistics
functions can measure any aspect of a state
transition, yt-1eyt, and the entire observation
sequence, x, centered at the current time step, t.
For example, one feature function might have
the value 1 when yt-1 is the state B, yt is the state
I, and xt is the character “国”.
</bodyText>
<subsectionHeader confidence="0.998767">
2.2 Character Clustering
</subsectionHeader>
<bodyText confidence="0.999989916666667">
In many cases, Chinese sentences may be inter-
spersed with non-Chinese words. In a closed
task, there is no way of knowing how many lan-
guages there are in a given text. Our solution is
to apply a clustering algorithm to find homoge-
neous characters belonging to the same character
clusters. One general rule we adopted is that a
language’s characters tend to appear together in
tokens. In addition, character clusters exhibit
certain distinct properties. The first property is
that the order of characters in some pairs can be
interchanged. This is referred to as exchange-
ability. The second property is that some charac-
ters, such as lowercase characters, can appear in
any position of a word; while others, such as
uppercase characters, cannot. This is referred to
as location independence. According to the gen-
eral rule, we can calculate the pairing frequency
of characters in tokens by checking all tokens in
the corpus. Assuming the alphabet is I, we first
need to represent each character as a |I|-
dimensional vector. For each character ci, we use
vj to represent its j-dimension value, which is
calculated as follows:
</bodyText>
<equation confidence="0.892579666666667">
= α + (1− α)[min( , )] ’ (2),
r
vj f ij fji
</equation>
<bodyText confidence="0.999610774193548">
where fij denotes the frequency with which ci and
cj appear in the same word when ci’s position
precedes that of cj. We take the minimum value
of fij and fji because even when ci and cj have a
high co-occurrence frequency, if either fij or fji is
low, then one order does not occur often, so vj’s
value will be low. We use two parameters to
normalize vj within the range 0 to 1; α is used to
enlarge the gap between non-zero and zero fre-
quencies, and y is used to weaken the influence
of very high frequencies.
Next, we apply the K-means algorithm to
generate candidate cluster sets composed of K
clusters (Hartigan et al., 1979). Different K’s,
α’s, and y’s are used to generate possible charac-
ter cluster sets. Our K-means algorithm uses the
cosine distance.
After obtaining the K clusters, we need to se-
lect the N1 best character clusters among them.
Assuming the angle between the cluster centroid
vector and (1, 1, ..., 1) is 0, the cluster with the
largest cosine 0 will be removed. This is because
characters whose co-occurrence frequencies are
nearly all zero will be transformed into vectors
very close to (α, α, ... , α); thus, their centroids
will also be very close to (α, α, ... , α), leading to
unreasonable clustering results.
After removing these two types of clusters,
for each character c in a cluster M, we calculate
the inverse relative distance (IRDist) of c using
(3):
</bodyText>
<equation confidence="0.977739833333333">
⎛ cos(c, m⎜∑⎞
i ) ⎟
i
log ⎜ ⎟ ⎜
cos(c, m) ⎟
⎝ ⎠
</equation>
<bodyText confidence="0.999984076923077">
where mi stands for the centroid of cluster Mi,
and m stands for the centroid of M.
We then calculate the average inverse dis-
tance for each cluster M. The N1 best clusters are
selected from the original K clusters.
The above K-means clustering and character
cluster selection steps are executed iteratively
for each cluster set generated from K-means
clustering with different K’s, α’s, and y’s.
After selecting the N1 best clusters for each
cluster set, we pool and rank them according to
their inner ratios. Each cluster’s inner ratio is
calculated by the following formula:
</bodyText>
<equation confidence="0.862313">
, (4)
</equation>
<bodyText confidence="0.9993835">
where co-occurrence(ci, cj) denotes the fre-
quency with which characters ci and cj co-occur
in the same word.
To ensure that we select a balanced mix of
clusters, for each character in an incoming clus-
ter M, we use Algorithm 1 to check if the fre-
quency of each character in C∪M is greater
than a threshold r.
</bodyText>
<subsectionHeader confidence="0.379557">
Algorithm 1 Balanced Cluster Selection
</subsectionHeader>
<bodyText confidence="0.879156333333333">
Input: A set of character clusters P={M1 , . . . , MK}
Number of selections N2,
Output: A set of clusters Q={ &apos;
</bodyText>
<equation confidence="0.84121775">
M1 , . . . , &apos;
MN2 }.
c
c
</equation>
<listItem confidence="0.9103399">
1: C={}
2: sort the clusters in P by their inner ratios;
3: while |C|&lt;=N2 do
4: pick the cluster M that has highest inner ratio;
5: for each character c in M do
6: if the frequency of c in C∪M is over thresh-
old z
7: PSP-M;
8: continue;
9 : else
</listItem>
<figure confidence="0.906474666666667">
IRDist(c)
, (3)
∑ co− occurence(ci, c j )
inner(M ) = ij M
, c ∈
∑ co− occurence(ci , cj )
cj
i
,
</figure>
<page confidence="0.724484">
135
</page>
<listItem confidence="0.6202656">
10: C←C∪M;
11: P←P-M;
12: end;
13: end;
14: end
</listItem>
<bodyText confidence="0.996282555555556">
The above algorithm yields the best N1 clus-
ters in terms of exchangeability. Next, we exe-
cute the above procedures again to select the
best N2 clusters based on their location inde-
pendence and exchangeability. However, for
each character ci, we use vj to denote the value of
its j-th dimension. We calculate vj as follows:
vj =α+(1−α)[min(f.,f&apos;ij ,fji,f&apos;ji )]r , (5)
where fij stands for the frequency with which ci
and cj appear in the same word when ci is the
first character; and f’ij stands for the frequency
with which ci and cj co-occur in the same word
when ci precedes cj but not in the first position.
We choose the minimum value from fij , f’ij, fji ,
and f’ji because if ci and cj both appear in the
first position of a word and their order is ex-
changeable, the four frequency values, including
the minimum value, will all be large enough.
</bodyText>
<table confidence="0.9988257">
Type Cluster Inner (K,
α, γ)
EX ,.0123456789 0.94 (10, 0.60, 0.16)
-/ABCDEFGHIKLMNOPR 0.93 (10, 0.70, 0.16)
STUVWabcdefghiklmnoprst
uvwxy
EL -/ABCDEFGHIKLMNO 0.84 (10, 0.50, 0.25)
PRSTUVWabcdefghiklmno
prstvwxy
0123456789 0.76 (10, 0.50, 0.26)
</table>
<tableCaption confidence="0.999909">
Table 1. Clustering Results of the CTU corpus
</tableCaption>
<bodyText confidence="0.999946666666667">
Our next goal is to create the best hybrid of
the above two cluster sets. The set selected for
exchangeability is referred to as the EX set,
while the set selected for both exchangeability
and location independence is referred to as the
EL set. We create a development set and use the
best first strategy to build the optimal cluster set
from EX∪EL. The EX and EL for the CTU
corpus are shown in Table 1.
</bodyText>
<subsectionHeader confidence="0.999505">
2.3 Handling Non-Chinese Words
</subsectionHeader>
<bodyText confidence="0.999992169811321">
Non-Chinese characters suffer from a serious
data sparseness problem, since their frequencies
are much lower than those of Chinese characters.
In bigrams containing at least one non-Chinese
character (referred as non-Chinese bigrams), the
problem is more serious. Take the phrase “約莫
20 歲” (about 20 years old) for example. “2” is
usually predicted as I, (i.e., “約莫” is connected
with “2”) resulting in incorrect segmentation,
because the frequency of “2” in the I class is
much higher than that of “2” in the B class, even
though the feature C-2C-1=”約莫” has a high
weight for assigning “2” to the B class.
Traditional approaches to CWS only use one
general tagger (referred as the G tagger) for
segmentation. In our system, we use two CWS
taggers. One is a general tagger, similar to the
traditional approaches; the other is a specialized
tagger designed to deal with non-Chinese words.
We refer to the composite tagger (the general
tagger plus the specialized tagger) as the GS
tagger.
Here, we refer to all characters in the selected
clusters as non-Chinese characters. In the devel-
opment stage, the best-first feature selector de-
termines which clusters will be used. Then, we
convert each sentence in the training data and
test data into a normalized sentence. Each non-
Chinese character c is replaced by a cluster rep-
resentative symbol σM, where c is in the cluster
M. We refer to the string composed of all σM as
F. If the length of F is more than that of W, it
will be shortened to W. The normalized sentence
is then placed in one file, and the non-Chinese
character sequence is placed in another. Next,
we use the normalized training and test file for
the general tagger, and the non-Chinese se-
quence training and test file for the specialized
tagger. Finally, the results of these two taggers
are combined.
The advantage of this approach is that it re-
solves the data sparseness problem in non-
Chinese bigrams. Consider the previous example
in which σ stands for the numeral cluster. Since
there is a phrase “約莫 8 年” in the training data,
C-1C0= “莫 8” is still an unknown bigram using
the G tagger. By using the GS tagger, however,
“約莫20 歲” and “約莫 8 年” will be converted
as “約莫 σσ 歲” and “約莫 σ 年”, respectively.
Therefore, the bigram feature C-1C0=“莫 σ” is no
longer unknown. Also, since σ in “ 莫 σ” is
tagged as B, (i.e., “莫” and “σ” are separated),
“莫” and “σ” will be separated in “約莫 σσ 歲”.
</bodyText>
<subsectionHeader confidence="0.997956">
2.4 Generating and Applying Templates
Template Generation
</subsectionHeader>
<bodyText confidence="0.9998458">
We first extract all possible word candidates
from the training set. Given a minimum word
length L, we extract all words whose length is
greater than or equal to L, after which we align
all word pairs. For each pair, if more than fifty
</bodyText>
<page confidence="0.991501">
136
</page>
<figure confidence="0.9954565">
Microsoft Research (MSR) 88 K 2.37 M 13 K 107 K
Training Size Test Size
Types Words Types Words
Corpus
CKIP
City University (CTU)
69 K 1.46 M 9 K 41 K
141 K 5.45 M 19 K 122 K
</figure>
<bodyText confidence="0.9902455">
percent of the characters are identical, a template
will be generated to match both words in the pair.
</bodyText>
<subsectionHeader confidence="0.798466">
Template Filtering
</subsectionHeader>
<bodyText confidence="0.99995775">
We have two criteria for filtering the extracted
templates. First, we test the matching accuracy
of each template t on the development set. This
is calculated by the following formula:
</bodyText>
<table confidence="0.875112666666667">
# of matched strings with no separators
.
# of all matched strings
</table>
<tableCaption confidence="0.998228">
Table 2. Corpora Information
</tableCaption>
<subsectionHeader confidence="0.447744">
3.2 Results
</subsectionHeader>
<bodyText confidence="0.5586535">
Table 3 lists the best combination of n-gram fea-
tures used in the G tagger.
</bodyText>
<figure confidence="0.973566">
A
(t) =
Bigram
Uni-gram
</figure>
<bodyText confidence="0.999675428571429">
In our system, templates whose accuracy is
lower than the threshold r1 are discarded. For the
remaining templates, we apply two different
strategies. According to our observations of the
development set, most templates whose accu-
racy is less than r2 are ineffective. To refine such
templates, we employ the character class infor-
mation generated by character clustering to im-
pose a class limitation on certain template slots.
This regulates the potential input and improves
the precision. Consider a template with one or
more wildcard slots. If any string matched with
these wildcard slots contains characters in dif-
ferent clusters, this template is also discarded.
</bodyText>
<subsectionHeader confidence="0.737744">
Template-Based Post-Processing (TBPP)
</subsectionHeader>
<bodyText confidence="0.999984230769231">
After the generated templates have been filtered,
they are used to match our CWS output and
check if the matched tokens can be combined
into complete words. If a template’s accuracy is
greater than r2, then all separators within the
matched strings will be eliminated; otherwise,
for a template t with accuracy between r1 and r2,
we eliminate all separators in its matched string
if no substring matched with t’s wildcard slots
contains characters in different clusters. Resul-
tant words of less than three characters in length
are discarded because CRF performs well with
such words.
</bodyText>
<sectionHeader confidence="0.99963" genericHeader="method">
3 Experiment
</sectionHeader>
<subsectionHeader confidence="0.881149">
3.1 Dataset
</subsectionHeader>
<bodyText confidence="0.999362571428571">
We use the three larger corpora in SIGHAN
Bakeoff 2006: a Simplified Chinese corpus pro-
vided by Microsoft Research Beijing, and two
Traditional Chinese corpora provided by Aca-
demia Sinica in Taiwan and the City University
of Hong Kong respectively. Details of each cor-
pus are listed in Table 2.
</bodyText>
<tableCaption confidence="0.649347">
C-2, C-1, C0, C1 C-2C-1, C-1C0, C0C1, C-3C-1, C-2C0, C-1C1
Table 3. Best Combination of N-gram Features
</tableCaption>
<bodyText confidence="0.990351692307692">
Table 4 compares the baseline G tagger and the
enhanced GST tagger. We observe that the GST
tagger outperforms the G tagger on all three cor-
pora.
Conf R P F ROOV RIV
CKIP-g 0.958 0.949 0.954 0.690 0.969
CKIP-gst 0.961 0.953 0.957 0.658 0.974
CTU-g 0.966 0.967 0.966 0.786 0.973
CTU-gst 0.973 0.972 0.972 0.787 0.981
MSR-g 0.949 0.957 0.953 0.673 0.959
MSR-gst 0.953 0.956 0.955 0.574 0.966
Table 4 Performance Comparison of the G Tag-
ger and the GST Tagger
</bodyText>
<sectionHeader confidence="0.99972" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999875333333333">
The contribution of this paper is two fold. First,
we successfully apply the K-means algorithm to
character clustering and develop several cluster
set selection algorithms for our GS tagger. This
significantly improves the handling of sentences
containing non-Chinese words as well as the
overall performance. Second, we develop a post-
processing method that compensates for the
weakness of ML-based CWS on longer words.
</bodyText>
<sectionHeader confidence="0.999473" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.967597571428571">
Hartigan, J. A., &amp; Wong, M. A. (1979). A K-means
Clustering Algorithm. Applied Statistics, 28, 100-
108.
Lafferty, J., McCallum, A., &amp; Pereira, F. (2001).
Conditional Random Fields: Probabilistic Models
for Segmenting and Labeling Sequence Data. Pa-
per presented at the ICML-01.
</reference>
<page confidence="0.997207">
137
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.305675">
<title confidence="0.999155">On Closed Task of Chinese Word Segmentation: An Improved Model Coupled with Character Clustering Automatically Generated Template Matching</title>
<author confidence="0.622039">Richard Tzong-Han Tsai</author>
<author confidence="0.622039">Hsieh-Chuan Hung</author>
<author confidence="0.622039">Cheng-Lung Hong-Jie Dai</author>
<author confidence="0.622039">Wen-Lian</author>
<affiliation confidence="0.701622">Intelligent Agent Systems Institute of Information Science, Academia</affiliation>
<address confidence="0.913333">No. 128, Sec. 2, Academia Rd., 115 Nankang, Taipei, Taiwan, R.O.C.</address>
<email confidence="0.944048">thtsai@iis.sinica.edu.tw</email>
<email confidence="0.944048">yabt@iis.sinica.edu.tw</email>
<email confidence="0.944048">clsung@iis.sinica.edu.tw</email>
<email confidence="0.944048">hongjie@iis.sinica.edu.tw</email>
<email confidence="0.944048">hsu@iis.sinica.edu.tw</email>
<abstract confidence="0.996075461538462">This paper addresses two major problems in closed task of Chinese word segmentation (CWS): tagging sentences interspersed with non-Chinese words, and long named entity (NE) identification. To resolve the former, we apply Kmeans clustering to identify non-Chinese characters, and then adopt a two-tagger architecture: one for Chinese text and the other for non-Chinese text. For the latter problem, we apply postprocessing to our CWS output using automatically generated templates. The experiment results show that, when non-Chinese characters are sparse in the training corpus, our two-tagger method significantly improves the segmentation of sentences containing non-Chinese words. Identification of long NEs and long words is also enhanced by template-based postprocessing. In the closed task of SIGHAN 2006 CWS, our system achieved F-scores of 0.957, 0.972, and 0.955 on the CKIP, CTU, and MSR corpora respectively.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>J A Hartigan</author>
<author>M A Wong</author>
</authors>
<title>A K-means Clustering Algorithm.</title>
<date>1979</date>
<journal>Applied Statistics,</journal>
<volume>28</volume>
<pages>100--108</pages>
<marker>Hartigan, Wong, 1979</marker>
<rawString>Hartigan, J. A., &amp; Wong, M. A. (1979). A K-means Clustering Algorithm. Applied Statistics, 28, 100-108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Paper presented at the ICML-01.</title>
<date>2001</date>
<contexts>
<context position="2898" citStr="Lafferty et al., 2001" startWordPosition="434" endWordPosition="437">ost ML-based CWS systems use a five-character context window to determine the current character’s tag. In the majority of cases, given the constraints of computational resources, this compromise is acceptable. However, limited by the window size, these systems often handle long words poorly. In this paper, our goal is to construct a general CWS system that can deal with the above problems. We adopt CRF as our ML model. 2 Chinese Word Segmentation System 2.1 Conditional Random Fields Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional probability (Lafferty et al., 2001). A linear-chain CRF with parameters Λ={λ1, λ2, ...} defines a conditional probability for a state sequence y = y1 ...yT , given that an input sequence x = x1 ...xT is ( T t=1 k a kfk (yt− 1 ,yt,x,t)) where Zx is the normalization factor that makes the probability of all state sequences sum to one; fk(yt-1, yt, x, t) is often a binary-valued feature function and λk is its weight. The feature 1 ( |) y x = exp Zx PA ,(1) 134 Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 134–137, Sydney, July 2006. c�2006 Association for Computational Linguistics functions can mea</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>Lafferty, J., McCallum, A., &amp; Pereira, F. (2001). Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. Paper presented at the ICML-01.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>