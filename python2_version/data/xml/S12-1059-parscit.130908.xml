<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.022348">
<title confidence="0.9989635">
UKP: Computing Semantic Textual Similarity by
Combining Multiple Content Similarity Measures
</title>
<author confidence="0.973678">
Daniel B¨ar†, Chris Biemann†, Iryna Gurevych†‡, and Torsten Zesch†‡
</author>
<affiliation confidence="0.8739085">
†Ubiquitous Knowledge Processing Lab (UKP-TUDA)
Department of Computer Science, Technische Universit¨at Darmstadt
‡Ubiquitous Knowledge Processing Lab (UKP-DIPF)
German Institute for Educational Research and Educational Information
</affiliation>
<email confidence="0.932611">
www.ukp.tu-darmstadt.de
</email>
<sectionHeader confidence="0.99468" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999116666666667">
We present the UKP system which performed
best in the Semantic Textual Similarity (STS)
task at SemEval-2012 in two out of three met-
rics. It uses a simple log-linear regression
model, trained on the training data, to combine
multiple text similarity measures of varying
complexity. These range from simple char-
acter and word n-grams and common sub-
sequences to complex features such as Ex-
plicit Semantic Analysis vector comparisons
and aggregation of word similarity based on
lexical-semantic resources. Further, we em-
ploy a lexical substitution system and statisti-
cal machine translation to add additional lex-
emes, which alleviates lexical gaps. Our final
models, one per dataset, consist of a log-linear
combination of about 20 features, out of the
possible 300+ features implemented.
</bodyText>
<sectionHeader confidence="0.999131" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999982175">
The goal of the pilot Semantic Textual Similarity
(STS) task at SemEval-2012 is to measure the de-
gree of semantic equivalence between pairs of sen-
tences. STS is fundamental to a variety of tasks
and applications such as question answering (Lin
and Pantel, 2001), text reuse detection (Clough et
al., 2002) or automatic essay grading (Attali and
Burstein, 2006). STS is also closely related to tex-
tual entailment (TE) (Dagan et al., 2006) and para-
phrase recognition (Dolan et al., 2004). It differs
from both tasks, though, insofar as those operate on
binary similarity decisions while STS is defined as
a graded notion of similarity. STS further requires a
bidirectional similarity relationship to hold between
a pair of sentences rather than a unidirectional en-
tailment relation as for the TE task.
A multitude of measures for computing similar-
ity between texts have been proposed in the past
based on surface-level and/or semantic content fea-
tures (Mihalcea et al., 2006; Landauer et al., 1998;
Gabrilovich and Markovitch, 2007). The exist-
ing measures exhibit two major limitations, though:
Firstly, measures are typically used in separation.
Thereby, the assumption is made that a single
measure inherently captures all text characteristics
which are necessary for computing similarity. Sec-
ondly, existing measures typically exclude similar-
ity features beyond content per se, thereby implying
that similarity can be computed by comparing text
content exclusively, leaving out any other text char-
acteristics. While we can only briefly tackle the sec-
ond issue here, we explicitly address the first one by
combining several measures using a supervised ma-
chine learning approach. With this, we hope to take
advantage of the different facets and intuitions that
are captured in the single measures.
In the following section, we describe the feature
space in detail. Section 3 describes the machine
learning setup. After describing our submitted runs,
we discuss the results and conclude.
</bodyText>
<sectionHeader confidence="0.918178" genericHeader="method">
2 Text Similarity Measures
</sectionHeader>
<bodyText confidence="0.999646">
We now describe the various features we have tried,
also listing features that did not prove useful.
</bodyText>
<subsectionHeader confidence="0.999537">
2.1 Simple String-based Measures
</subsectionHeader>
<bodyText confidence="0.9344505">
String Similarity Measures These measures op-
erate on string sequences. The longest common
</bodyText>
<page confidence="0.987362">
435
</page>
<note confidence="0.528392">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 435–440,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<bodyText confidence="0.999919">
substring measure (Gusfield, 1997) compares the
length of the longest contiguous sequence of char-
acters. The longest common subsequence measure
(Allison and Dix, 1986) drops the contiguity re-
quirement and allows to detect similarity in case
of word insertions/deletions. Greedy String Tiling
(Wise, 1996) allows to deal with reordered text parts
as it determines a set of shared contiguous sub-
strings, whereby each substring is a match of maxi-
mal length. We further used the following measures,
which, however, did not make it into the final mod-
els, since they were subsumed by the other mea-
sures: Jaro (1989), Jaro-Winkler (Winkler, 1990),
Monge and Elkan (1997), and Levenshtein (1966).
Character/word n-grams We compare character
n-grams following the implementation by Barr´on-
Cede˜no et al. (2010), thereby generalizing the orig-
inal trigram variant to n = 2, 3, ... ,15. We also
compare word n-grams using the Jaccard coefficient
as previously done by Lyon et al. (2001), and the
containment measure (Broder, 1997). As high n led
to instabilities of the classifier due to their high in-
tercorrelation, only n = 1, 2, 3, 4 was used.
</bodyText>
<subsectionHeader confidence="0.999774">
2.2 Semantic Similarity Measures
</subsectionHeader>
<bodyText confidence="0.999144852941176">
Pairwise Word Similarity The measures for
computing word similarity on a semantic level op-
erate on a graph-based representation of words and
the semantic relations among them within a lexical-
semantic resource. For this system, we used the al-
gorithms by Jiang and Conrath (1997), Lin (1998a),
and Resnik (1995) on WordNet (Fellbaum, 1998).
In order to scale the resulting pairwise word sim-
ilarities to the text level, we applied the aggregation
strategy by Mihalcea et al. (2006): The sum of the
idf -weighted similarity scores of each word with the
best-matching counterpart in the other text is com-
puted in both directions, then averaged. In our ex-
periments, the measure by Resnik (1995) proved to
be superior to the other measures and was used in all
word similarity settings throughout this paper.
Explicit Semantic Analysis We also used the vec-
tor space model Explicit Semantic Analysis (ESA)
(Gabrilovich and Markovitch, 2007). Besides Word-
Net, we used two additional lexical-semantic re-
sources for the construction of the ESA vector space:
Wikipedia and Wiktionary1.
Textual Entailment We experimented with using
the BIUTEE textual entailment system (Stern and
Dagan, 2011) for generating entailment scores to
serve as features for the classifier. However, these
features were not selected by the classifier.
Distributional Thesaurus We used similarities
from a Distributional Thesaurus (similar to Lin
(1998b)) computed on 10M dependency-parsed sen-
tences of English newswire as a source for pairwise
word similarity, one additional feature per POS tag.
However, only the feature based on cardinal num-
bers (CD) was selected in the final models.
</bodyText>
<subsectionHeader confidence="0.996576">
2.3 Text Expansion Mechanisms
</subsectionHeader>
<bodyText confidence="0.999448869565217">
Lexical Substitution System We used the lexical
substitution system based on supervised word sense
disambiguation (Biemann, 2012). This system au-
tomatically provides substitutions for a set of about
1,000 frequent English nouns with high precision.
For each covered noun, we added the substitutions
to the text and computed the pairwise word similar-
ity for the texts as described above. This feature al-
leviates the lexical gap for a subset of words.
Statistical Machine Translation We used the
Moses SMT system (Koehn et al., 2007) to trans-
late the original English texts via three bridge lan-
guages (Dutch, German, Spanish) back to English.
Thereby, the idea was that in the translation pro-
cess additional lexemes are introduced which allevi-
ate potential lexical gaps. The system was trained on
Europarl made available by Koehn (2005), using the
following configuration which was not optimized for
this task: WMT112 baseline without tuning, with
MGIZA alignment. The largest improvement was
reached for computing pairwise word similarity (as
described above) on the concatenation of the origi-
nal text and the three back-translations.
</bodyText>
<subsectionHeader confidence="0.993666">
2.4 Measures Related to Structure and Style
</subsectionHeader>
<bodyText confidence="0.999996">
In our system, we also used measures which go
beyond content and capture similarity along the
structure and style dimensions inherent to texts.
However, as we report later on, for this content-
</bodyText>
<footnote confidence="0.965229666666667">
1www.wiktionary.org
20-5-grams, grow-diag-final-and alignment, msd-bidirec-
tional-fe reodering, interpolation and kndiscount
</footnote>
<page confidence="0.998588">
436
</page>
<bodyText confidence="0.999914285714286">
oriented task they were not selected by the classifier.
Nonetheless, we briefly list them for completeness.
Structural similarity between texts can be de-
tected by computing stopword n-grams (Sta-
matatos, 2011). Thereby, all content-bearing words
are removed while stopwords are preserved. Stop-
word n-grams of both texts are compared using the
containment measure (Broder, 1997). In our experi-
ments, we tested n-gram sizes for n = 2, 3, ... ,10.
We also compute part-of-speech n-grams for
various POS tags which we then compare using the
containment measure and the Jaccard coefficient.
We also used two similarity measures between
pairs of words (Hatzivassiloglou et al., 1999): Word
pair order tells whether two words occur in the
same order in both texts (with any number of words
in between), word pair distance counts the number
of words which lie between those of a given pair.
To compare texts along the stylistic dimension,
we further use a function word frequencies mea-
sure (Dinu and Popescu, 2009) which operates on a
set of 70 function words identified by Mosteller and
Wallace (1964). Function word frequency vectors
are computed and compared by Pearson correlation.
We also include a number of measures which
capture statistical properties of texts, such as type-
token ratio (TTR) (Templin, 1957) and sequential
TTR (McCarthy and Jarvis, 2010).
</bodyText>
<sectionHeader confidence="0.995895" genericHeader="method">
3 System Description
</sectionHeader>
<bodyText confidence="0.999966">
We first run each of the similarity measures intro-
duced above separately. We then use the resulting
scores as features for a machine learning classifier.
Pre-processing Our system is based on DKPro3,
a collection of software components for natural
language processing built upon the Apache UIMA
framework. During the pre-processing phase, we to-
kenize the input texts and lemmatize using the Tree-
Tagger implementation (Schmid, 1994). For some
measures, we additionally apply a stopword filter.
Feature Generation We now compute similarity
scores for the input texts with all measures and for
all configurations introduced in Section 2. This re-
sulted in 300+ individual score vectors which served
as features for the following step.
</bodyText>
<footnote confidence="0.979304">
3http://dkpro-core-asl.googlecode.com
</footnote>
<table confidence="0.993240444444445">
Run Features
1 Greedy String Tiling
Longest common subsequence (2 normalizations)
Longest common substring
Character 2-, 3-, and 4-grams
Word 1- and 2-grams (Containment, w/o stopwords)
Word 1-, 3-, and 4-grams (Jaccard)
Word 2- and 4-grams (Jaccard, w/o stopwords)
Word Similarity (Resnik (1995) on WordNet
aggregated according to Mihalcea et al. (2006);
2 variants: complete texts + difference only)
Explicit Semantic Analysis (Wikipedia, Wiktionary)
Distributional Thesaurus (POS: Cardinal numbers)
2 All Features of Run 1
Lexical Substitution for Word Sim. (complete texts)
SMT for Word Sim. (complete texts as above)
3 All Features of Run 2
Random numbers from [4.5, 5] for surprise datasets
</table>
<tableCaption confidence="0.999831">
Table 1: Feature sets of our three system configurations
</tableCaption>
<bodyText confidence="0.999111083333333">
Feature Combination The feature combination
step uses the pre-computed similarity scores, and
combines their log-transformed values using a linear
regression classifier from the WEKA toolkit (Hall et
al., 2009). We trained the classifier on the training
datasets of the STS task. During the development
cycle, we evaluated using 10-fold cross-validation.
Post-processing For Runs 2 and 3, we applied a
post-processing filter which stripped all characters
off the texts which are not in the character range
[a-zA-Z0-9]. If the texts match, we set their similar-
ity score to 5.0 regardless of the classifier’s output.
</bodyText>
<sectionHeader confidence="0.99707" genericHeader="method">
4 Submitted Runs
</sectionHeader>
<bodyText confidence="0.999402266666667">
Run 1 During the development cycle, we identi-
fied 19 features (see Table 1) which achieved the
best performance on the training data. For each
of the known datasets, we trained a separate clas-
sifier and applied it to the test data. For the surprise
datasets, we trained the classifier on a joint dataset
of all known training datasets.
Run 2 For the Run 2, we were interested in the
effects of two additional features: lexical substitu-
tion and statistical machine translation. We added
the corresponding measures to the feature set of Run
1 and followed the same evaluation procedure.
Run 3 For the third run, we used the same feature
set as for Run 2, but returned random numbers from
[4.5, 5] for the sentence pairs in the surprise datasets.
</bodyText>
<page confidence="0.997051">
437
</page>
<table confidence="0.9997516875">
Dim. Text Similarity Features PAR VID SE
Best Feature Set, Run 1 .711 .868 .735
Best Feature Set, Run 2 .724 .868 .742
Content Pairwise Word Similarity .564 .835 .527
Character n-grams .658 .771 .554
Explicit Semantic Analysis .427 .781 .619
Word n-grams .474 .782 .619
String Similarity .593 .677 .744
Distributional Thesaurus .494 .481 .365
Lexical Substitution .228 .554 .483
Statistical Machine Translation .287 .652 .516
Structure Part-of-speech n-grams .193 .265 .557
Stopword n-grams .211 .118 .379
Word Pair Order .104 .077 .295
Style Statistical Properties .168 .225 .325
Function Word Frequencies .179 .142 .189
</table>
<tableCaption confidence="0.992578">
Table 2: Best results for single measures, grouped by di-
mension, on the training datasets MSRpar, MSRvid, and
SMTeuroparl, using 10-fold cross-validation
</tableCaption>
<sectionHeader confidence="0.997695" genericHeader="method">
5 Results on Training Data
</sectionHeader>
<bodyText confidence="0.9999878">
Evaluation was carried out using the official scorer
which computes Pearson correlation of the human
rated similarity scores with the the system’s output.
In Table 2, we report the results achieved on
each of the training datasets using 10-fold cross-
validation. The best results were achieved for the
feature set of Run 2, with Pearson’s r = .724,
r = .868, and r = .742 for the datasets MSR-
par, MSRvid, and SMTeuroparl, respectively. While
individual classes of content similarity measures
achieved good results, a different class performed
best for each dataset. However, text similarity mea-
sures related to structure and style achieved only
poor results on the training data. This was to be ex-
pected due to the nature of the data, though.
</bodyText>
<sectionHeader confidence="0.999391" genericHeader="evaluation">
6 Results on Test Data
</sectionHeader>
<bodyText confidence="0.996370818181818">
Besides the Pearson correlation for the union of all
datasets (ALL), the organizers introduced two addi-
tional evaluation metrics after system submission:
ALLnrm computes Pearson correlation after the sys-
tem outputs for each dataset are fitted to the gold
standard using least squares, and Mean refers to the
weighted mean across all datasets, where the weight
depends on the number of pairs in each dataset.
In Table 3, we report the offical results achieved
on the test data. The best configuration of our system
was Run 2 which was ranked #1 for the evaluation
</bodyText>
<table confidence="0.99657225">
#1 #2 #3 Sys. r1 r2 r3 PAR VID SE WN SN
1 2 1 UKP2 .823 .857 .677 .683 .873 .528 .664 .493
2 3 5 TL .813 .856 .660 .698 .862 .361 .704 .468
3 1 2 TL .813 .863 .675 .734 .880 .477 .679 .398
4 4 4 UKP1 .811 .855 .670 .682 .870 .511 .664 .467
5 6 13 UNT .784 .844 .616 .535 .875 .420 .671 .403
... ... ... ... ... ... ... ... ... ... ... ...
87 85 70 B/L .311 .673 .435 .433 .299 .454 .586 .390
</table>
<tableCaption confidence="0.998605">
Table 3: Official results on the test data for the top 5
</tableCaption>
<bodyText confidence="0.817139181818182">
participating runs out of 89 which were achieved on the
known datasets MSRpar, MSRvid, and SMTeuroparl, as
well as on the surprise datasets OnWN and SMTnews. We
report the ranks (#1: ALL, #2: ALLnrm, #3: Mean) and
the corresponding Pearson correlation r according to the
three offical evaluation metrics (see Sec. 6). The provided
baseline is shown at the bottom of this table.
metrics ALL (r = .823)4 and Mean (r = .677), and
#2 for ALLnrm (r = .857). An exhaustive overview
of all participating systems can be found in the STS
task description (Agirre et al., 2012).
</bodyText>
<sectionHeader confidence="0.974929" genericHeader="conclusions">
7 Conclusions and Future Work
</sectionHeader>
<bodyText confidence="0.998488913043478">
In this paper, we presented the UKP system, which
performed best across the three official evaluation
metrics in the pilot Semantic Textual Similarity
(STS) task at SemEval-2012. While we did not
reach the highest scores on any of the single datasets,
our system was most robust across different data. In
future work, it would be interesting to inspect the
performance of a system that combines the output
of all participating systems in a single linear model.
We also propose that two major issues with the
datasets are tackled in future work: (a) It is unclear
how to judge similarity between pairs of texts which
contain contextual references such as on Monday
vs. after the Thanksgiving weekend. (b) For several
pairs, it is unclear what point of view to take, e.g. for
the pair An animal is eating / The animal is hopping.
Is the pair to be considered similar (an animal is do-
ing something) or rather not (eating vs. hopping)?
Acknowledgements This work has been sup-
ported by the Volkswagen Foundation as part of the
Lichtenberg-Professorship Program under grant No.
I/82806, and by the Klaus Tschira Foundation under
project No. 00.133.2008.
</bodyText>
<footnote confidence="0.551428">
499% confidence interval: .807 &lt; r &lt; .837
</footnote>
<page confidence="0.996429">
438
</page>
<sectionHeader confidence="0.905517" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995561537735849">
Eneko Agirre, Daniel Cer, Mona Diab, and Aitor
Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pi-
lot on Semantic Textual Similarity. In Proceedings of
the 6th International Workshop on Semantic Evalua-
tion, in conjunction with the 1st Joint Conference on
Lexical and Computational Semantics.
Lloyd Allison and Trevor I. Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23:305–310.
Yigal Attali and Jill Burstein. 2006. Automated es-
say scoring with e-rater v.2.0. Journal of Technology,
Learning, and Assessment, 4(3).
Alberto Barr´on-Cede˜no, Paolo Rosso, Eneko Agirre, and
Gorka Labaka. 2010. Plagiarism Detection across
Distant Language Pairs. In Proceedings of the 23rd
International Conference on Computational Linguis-
tics, pages 37–45.
Chris Biemann. 2012. Creating a System for Lexi-
cal Substitutions from Scratch using Crowdsourcing.
Language Resources and Evaluation: Special Issue
on Collaboratively Constructed Language Resources,
46(2).
Andrei Z. Broder. 1997. On the resemblance and con-
tainment of documents. Proceedings of the Compres-
sion and Complexity of Sequences, pages 21–29.
Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and
Yorick Wilks. 2002. METER: MEasuring TExt
Reuse. In Proceedings of 40th Annual Meeting of
the Association for Computational Linguistics, pages
152–159.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006. The PASCAL Recognising Textual Entailment
Challenge. In Machine Learning Challenges, Lecture
Notes in Computer Science, pages 177–190. Springer.
Liviu P. Dinu and Marius Popescu. 2009. Ordinal mea-
sures in authorship identification. In Proceedings of
the 3rd PAN Workshop. Uncovering Plagiarism, Au-
thorship and Social Software Misuse, pages 62–66.
William B. Dolan, Chris Quirk, and Chris Brockett.
2004. Unsupervised Construction of Large Paraphrase
Corpora: Exploiting Massively Parallel News Sources.
In Proceedings of the 20th International Conference
on Computational Linguistics, pages 350–356.
Christiane Fellbaum. 1998. WordNet: An Electronic
Lexical Database. MIT Press.
Evgeniy Gabrilovich and Shaul Markovitch. 2007. Com-
puting Semantic Relatedness using Wikipedia-based
Explicit Semantic Analysis. In Proceedings of the
20th International Joint Conference on Artificial In-
telligence, pages 1606–1611.
Dan Gusfield. 1997. Algorithms on Strings, Trees and
Sequences: Computer Science and Computational Bi-
ology. Cambridge University Press.
Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard
Pfahringer, Peter Reutemann, and Ian H. Witten.
2009. The WEKA Data Mining Software: An Update.
SIGKDD Explorations, 11(1):10–18.
Vasileios Hatzivassiloglou, Judith L. Klavans, and
Eleazar Eskin. 1999. Detecting text similarity over
short passages: Exploring linguistic feature combina-
tions via machine learning. In Proceedings of the Joint
SIGDAT Conference on Empirical Methods in Natural
Language Processing and Very Large Corpora, pages
203–212.
Matthew A. Jaro. 1989. Advances in record linkage
methodology as applied to the 1985 census of Tampa
Florida. Journal of the American Statistical Associa-
tion, 84(406):414–420.
Jay J. Jiang and David W. Conrath. 1997. Semantic sim-
ilarity based on corpus statistics and lexical taxonomy.
In Proceedings of the 10th International Conference
on Research in Computational Linguistics.
Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris
Callison-Burch, Marcello Federico, Nicola Bertoldi,
Brooke Cowan, Wade Shen, Christine Moran, Richard
Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Con-
stantin, and Evan Herbst. 2007. Moses: Open Source
Toolkit for Statistical Machine Translation. In Pro-
ceedings of the ACL 2007 Demo and Poster Sessions,
pages 177–180.
Philipp Koehn. 2005. Europarl: A Parallel Corpus for
Statistical Machine Translation. In Proceedings of the
10th Machine Translation Summit, pages 79–86.
Thomas K. Landauer, Peter W. Foltz, and Darrell La-
ham. 1998. An introduction to latent semantic analy-
sis. Discourse Processes, 25(2):259–284.
Vladimir I. Levenshtein. 1966. Binary codes capable of
correcting deletions, insertions, and reversals. Soviet
Physics Doklady, 10(8):707–710.
Dekang Lin and Patrick Pantel. 2001. Discovery of In-
ference Rules for Question Answering. Natural Lan-
guage Engineering, 7(4):343–360.
Dekang Lin. 1998a. An information-theoretic definition
of similarity. In Proceedings of International Confer-
ence on Machine Learning, pages 296–304.
Dekang Lin. 1998b. Automatic Retrieval and Clustering
of Similar Words. In Proceedings of the 36th Annual
Meeting of the Association for Computational Linguis-
tics, pages 768–774.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in large
document collections. In Proceedings of Conference
on Empirical Methods in Natural Language Process-
ing, pages 118–125.
Philip M. McCarthy and Scott Jarvis. 2010. MTLD,
vocd-D, and HD-D: A validation study of sophisti-
</reference>
<page confidence="0.990899">
439
</page>
<reference confidence="0.996473404761905">
cated approaches to lexical diversity assessment. Be-
havior research methods, 42(2):381–92.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and Knowledge-based Measures
of Text Semantic Similarity. In Proceedings of the 21st
National Conference on Artificial Intelligence, pages
775–780.
Alvaro Monge and Charles Elkan. 1997. An efficient
domain-independent algorithm for detecting approxi-
mately duplicate database records. In Proceedings of
the SIGMOD Workshop on Data Mining and Knowl-
edge Discovery, pages 23–29.
Frederick Mosteller and David L. Wallace. 1964. In-
ference and disputed authorship: The Federalist.
Addison-Wesley.
Philip Resnik. 1995. Using Information Content to Eval-
uate Semantic Similarity in a Taxonomy. In Proceed-
ings of the 14th International Joint Conference on Ar-
tificial Intelligence, pages 448–453.
Helmut Schmid. 1994. Probabilistic part-of-speech tag-
ging using decision trees. In Proceedings of the In-
ternational Conference on New Methods in Language
Processing, pages 44–49.
Efstathios Stamatatos. 2011. Plagiarism detection
using stopword n-grams. Journal of the Ameri-
can Society for Information Science and Technology,
62(12):2512–2527.
Asher Stern and Ido Dagan. 2011. A Confidence
Model for Syntactically-Motivated Entailment Proofs.
In Proceedings of the International Conference on Re-
cent Advances in Natural Language Processing, pages
455–462.
Mildred C. Templin. 1957. Certain language skills in
children. University of Minnesota Press.
William E. Winkler. 1990. String Comparator Metrics
and Enhanced Decision Rules in the Fellegi-Sunter
Model of Record Linkage. In Proceedings of the Sec-
tion on Survey Research Methods, pages 354–359.
Michael J. Wise. 1996. YAP3: Improved detection of
similarities in computer program and other texts. In
Proceedings of the 27th SIGCSE technical symposium
on Computer science education, pages 130–134.
</reference>
<page confidence="0.998078">
440
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.684715">
<title confidence="0.9994255">UKP: Computing Semantic Textual Similarity Combining Multiple Content Similarity Measures</title>
<author confidence="0.975189">Chris Iryna</author>
<author confidence="0.975189">Torsten</author>
<affiliation confidence="0.9287785">Knowledge Processing Lab Department of Computer Science, Technische Universit¨at Darmstadt Knowledge Processing Lab (UKP-DIPF) German Institute for Educational Research and Educational Information</affiliation>
<email confidence="0.981979">www.ukp.tu-darmstadt.de</email>
<abstract confidence="0.997517368421053">We present the UKP system which performed best in the Semantic Textual Similarity (STS) task at SemEval-2012 in two out of three met- It uses a simple regression model, trained on the training data, to combine multiple text similarity measures of varying complexity. These range from simple charand word and common subsequences to complex features such as Explicit Semantic Analysis vector comparisons and aggregation of word similarity based on lexical-semantic resources. Further, we employ a lexical substitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. Our final one per dataset, consist of a combination of about 20 features, out of the possible 300+ features implemented.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor Gonzalez-Agirre</author>
</authors>
<title>SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity.</title>
<date>2012</date>
<booktitle>In Proceedings of the 6th International Workshop on Semantic Evaluation, in conjunction with the 1st Joint Conference on Lexical and Computational Semantics.</booktitle>
<contexts>
<context position="15410" citStr="Agirre et al., 2012" startWordPosition="2445" endWordPosition="2448">e 3: Official results on the test data for the top 5 participating runs out of 89 which were achieved on the known datasets MSRpar, MSRvid, and SMTeuroparl, as well as on the surprise datasets OnWN and SMTnews. We report the ranks (#1: ALL, #2: ALLnrm, #3: Mean) and the corresponding Pearson correlation r according to the three offical evaluation metrics (see Sec. 6). The provided baseline is shown at the bottom of this table. metrics ALL (r = .823)4 and Mean (r = .677), and #2 for ALLnrm (r = .857). An exhaustive overview of all participating systems can be found in the STS task description (Agirre et al., 2012). 7 Conclusions and Future Work In this paper, we presented the UKP system, which performed best across the three official evaluation metrics in the pilot Semantic Textual Similarity (STS) task at SemEval-2012. While we did not reach the highest scores on any of the single datasets, our system was most robust across different data. In future work, it would be interesting to inspect the performance of a system that combines the output of all participating systems in a single linear model. We also propose that two major issues with the datasets are tackled in future work: (a) It is unclear how t</context>
</contexts>
<marker>Agirre, Cer, Diab, Gonzalez-Agirre, 2012</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. 2012. SemEval-2012 Task 6: A Pilot on Semantic Textual Similarity. In Proceedings of the 6th International Workshop on Semantic Evaluation, in conjunction with the 1st Joint Conference on Lexical and Computational Semantics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor I Dix</author>
</authors>
<title>A bit-string longest-common-subsequence algorithm. Information Processing Letters,</title>
<date>1986</date>
<pages>23--305</pages>
<contexts>
<context position="3804" citStr="Allison and Dix, 1986" startWordPosition="563" endWordPosition="566">cuss the results and conclude. 2 Text Similarity Measures We now describe the various features we have tried, also listing features that did not prove useful. 2.1 Simple String-based Measures String Similarity Measures These measures operate on string sequences. The longest common 435 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 435–440, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics substring measure (Gusfield, 1997) compares the length of the longest contiguous sequence of characters. The longest common subsequence measure (Allison and Dix, 1986) drops the contiguity requirement and allows to detect similarity in case of word insertions/deletions. Greedy String Tiling (Wise, 1996) allows to deal with reordered text parts as it determines a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementati</context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor I. Dix. 1986. A bit-string longest-common-subsequence algorithm. Information Processing Letters, 23:305–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Yigal Attali</author>
<author>Jill Burstein</author>
</authors>
<title>Automated essay scoring with e-rater v.2.0.</title>
<date>2006</date>
<journal>Journal of Technology, Learning, and Assessment,</journal>
<volume>4</volume>
<issue>3</issue>
<contexts>
<context position="1587" citStr="Attali and Burstein, 2006" startWordPosition="226" endWordPosition="229">stitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (Mihalcea </context>
</contexts>
<marker>Attali, Burstein, 2006</marker>
<rawString>Yigal Attali and Jill Burstein. 2006. Automated essay scoring with e-rater v.2.0. Journal of Technology, Learning, and Assessment, 4(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alberto Barr´on-Cede˜no</author>
<author>Paolo Rosso</author>
<author>Eneko Agirre</author>
<author>Gorka Labaka</author>
</authors>
<title>Plagiarism Detection across Distant Language Pairs.</title>
<date>2010</date>
<booktitle>In Proceedings of the 23rd International Conference on Computational Linguistics,</booktitle>
<pages>37--45</pages>
<marker>Barr´on-Cede˜no, Rosso, Agirre, Labaka, 2010</marker>
<rawString>Alberto Barr´on-Cede˜no, Paolo Rosso, Eneko Agirre, and Gorka Labaka. 2010. Plagiarism Detection across Distant Language Pairs. In Proceedings of the 23rd International Conference on Computational Linguistics, pages 37–45.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Biemann</author>
</authors>
<title>Creating a System for Lexical Substitutions from Scratch using Crowdsourcing. Language Resources and Evaluation: Special Issue on Collaboratively Constructed Language Resources,</title>
<date>2012</date>
<volume>46</volume>
<issue>2</issue>
<contexts>
<context position="6615" citStr="Biemann, 2012" startWordPosition="1010" endWordPosition="1011">ntailment scores to serve as features for the classifier. However, these features were not selected by the classifier. Distributional Thesaurus We used similarities from a Distributional Thesaurus (similar to Lin (1998b)) computed on 10M dependency-parsed sentences of English newswire as a source for pairwise word similarity, one additional feature per POS tag. However, only the feature based on cardinal numbers (CD) was selected in the final models. 2.3 Text Expansion Mechanisms Lexical Substitution System We used the lexical substitution system based on supervised word sense disambiguation (Biemann, 2012). This system automatically provides substitutions for a set of about 1,000 frequent English nouns with high precision. For each covered noun, we added the substitutions to the text and computed the pairwise word similarity for the texts as described above. This feature alleviates the lexical gap for a subset of words. Statistical Machine Translation We used the Moses SMT system (Koehn et al., 2007) to translate the original English texts via three bridge languages (Dutch, German, Spanish) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced </context>
</contexts>
<marker>Biemann, 2012</marker>
<rawString>Chris Biemann. 2012. Creating a System for Lexical Substitutions from Scratch using Crowdsourcing. Language Resources and Evaluation: Special Issue on Collaboratively Constructed Language Resources, 46(2).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrei Z Broder</author>
</authors>
<title>On the resemblance and containment of documents.</title>
<date>1997</date>
<booktitle>Proceedings of the Compression and Complexity of Sequences,</booktitle>
<pages>21--29</pages>
<contexts>
<context position="4655" citStr="Broder, 1997" startWordPosition="702" endWordPosition="703">reby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strate</context>
<context position="8360" citStr="Broder, 1997" startWordPosition="1274" endWordPosition="1275"> along the structure and style dimensions inherent to texts. However, as we report later on, for this content1www.wiktionary.org 20-5-grams, grow-diag-final-and alignment, msd-bidirectional-fe reodering, interpolation and kndiscount 436 oriented task they were not selected by the classifier. Nonetheless, we briefly list them for completeness. Structural similarity between texts can be detected by computing stopword n-grams (Stamatatos, 2011). Thereby, all content-bearing words are removed while stopwords are preserved. Stopword n-grams of both texts are compared using the containment measure (Broder, 1997). In our experiments, we tested n-gram sizes for n = 2, 3, ... ,10. We also compute part-of-speech n-grams for various POS tags which we then compare using the containment measure and the Jaccard coefficient. We also used two similarity measures between pairs of words (Hatzivassiloglou et al., 1999): Word pair order tells whether two words occur in the same order in both texts (with any number of words in between), word pair distance counts the number of words which lie between those of a given pair. To compare texts along the stylistic dimension, we further use a function word frequencies mea</context>
</contexts>
<marker>Broder, 1997</marker>
<rawString>Andrei Z. Broder. 1997. On the resemblance and containment of documents. Proceedings of the Compression and Complexity of Sequences, pages 21–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Clough</author>
<author>Robert Gaizauskas</author>
<author>Scott S L Piao</author>
<author>Yorick Wilks</author>
</authors>
<title>METER: MEasuring TExt Reuse.</title>
<date>2002</date>
<booktitle>In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>152--159</pages>
<contexts>
<context position="1532" citStr="Clough et al., 2002" startWordPosition="218" endWordPosition="221">antic resources. Further, we employ a lexical substitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on su</context>
</contexts>
<marker>Clough, Gaizauskas, Piao, Wilks, 2002</marker>
<rawString>Paul Clough, Robert Gaizauskas, Scott S.L. Piao, and Yorick Wilks. 2002. METER: MEasuring TExt Reuse. In Proceedings of 40th Annual Meeting of the Association for Computational Linguistics, pages 152–159.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ido Dagan</author>
<author>Oren Glickman</author>
<author>Bernardo Magnini</author>
</authors>
<title>The PASCAL Recognising Textual Entailment Challenge.</title>
<date>2006</date>
<booktitle>In Machine Learning Challenges, Lecture Notes in Computer Science,</booktitle>
<pages>177--190</pages>
<publisher>Springer.</publisher>
<contexts>
<context position="1664" citStr="Dagan et al., 2006" startWordPosition="240" endWordPosition="243">h alleviates lexical gaps. Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (Mihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). The e</context>
</contexts>
<marker>Dagan, Glickman, Magnini, 2006</marker>
<rawString>Ido Dagan, Oren Glickman, and Bernardo Magnini. 2006. The PASCAL Recognising Textual Entailment Challenge. In Machine Learning Challenges, Lecture Notes in Computer Science, pages 177–190. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Liviu P Dinu</author>
<author>Marius Popescu</author>
</authors>
<title>Ordinal measures in authorship identification.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd PAN Workshop. Uncovering Plagiarism, Authorship and Social Software Misuse,</booktitle>
<pages>62--66</pages>
<contexts>
<context position="8989" citStr="Dinu and Popescu, 2009" startWordPosition="1380" endWordPosition="1383">ur experiments, we tested n-gram sizes for n = 2, 3, ... ,10. We also compute part-of-speech n-grams for various POS tags which we then compare using the containment measure and the Jaccard coefficient. We also used two similarity measures between pairs of words (Hatzivassiloglou et al., 1999): Word pair order tells whether two words occur in the same order in both texts (with any number of words in between), word pair distance counts the number of words which lie between those of a given pair. To compare texts along the stylistic dimension, we further use a function word frequencies measure (Dinu and Popescu, 2009) which operates on a set of 70 function words identified by Mosteller and Wallace (1964). Function word frequency vectors are computed and compared by Pearson correlation. We also include a number of measures which capture statistical properties of texts, such as typetoken ratio (TTR) (Templin, 1957) and sequential TTR (McCarthy and Jarvis, 2010). 3 System Description We first run each of the similarity measures introduced above separately. We then use the resulting scores as features for a machine learning classifier. Pre-processing Our system is based on DKPro3, a collection of software comp</context>
</contexts>
<marker>Dinu, Popescu, 2009</marker>
<rawString>Liviu P. Dinu and Marius Popescu. 2009. Ordinal measures in authorship identification. In Proceedings of the 3rd PAN Workshop. Uncovering Plagiarism, Authorship and Social Software Misuse, pages 62–66.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William B Dolan</author>
<author>Chris Quirk</author>
<author>Chris Brockett</author>
</authors>
<title>Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources.</title>
<date>2004</date>
<booktitle>In Proceedings of the 20th International Conference on Computational Linguistics,</booktitle>
<pages>350--356</pages>
<contexts>
<context position="1712" citStr="Dolan et al., 2004" startWordPosition="248" endWordPosition="251"> per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (Mihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). The existing measures exhibit two major limitations, </context>
</contexts>
<marker>Dolan, Quirk, Brockett, 2004</marker>
<rawString>William B. Dolan, Chris Quirk, and Chris Brockett. 2004. Unsupervised Construction of Large Paraphrase Corpora: Exploiting Massively Parallel News Sources. In Proceedings of the 20th International Conference on Computational Linguistics, pages 350–356.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Christiane Fellbaum</author>
</authors>
<title>WordNet: An Electronic Lexical Database.</title>
<date>1998</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="5142" citStr="Fellbaum, 1998" startWordPosition="784" endWordPosition="785"> word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). Besi</context>
</contexts>
<marker>Fellbaum, 1998</marker>
<rawString>Christiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Evgeniy Gabrilovich</author>
<author>Shaul Markovitch</author>
</authors>
<title>Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.</title>
<date>2007</date>
<booktitle>In Proceedings of the 20th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>1606--1611</pages>
<contexts>
<context position="2257" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="334" endWordPosition="337">ual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (Mihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). The existing measures exhibit two major limitations, though: Firstly, measures are typically used in separation. Thereby, the assumption is made that a single measure inherently captures all text characteristics which are necessary for computing similarity. Secondly, existing measures typically exclude similarity features beyond content per se, thereby implying that similarity can be computed by comparing text content exclusively, leaving out any other text characteristics. While we can only briefly tackle the second issue here, we explicitly address the first one by combining several measur</context>
<context position="5736" citStr="Gabrilovich and Markovitch, 2007" startWordPosition="880" endWordPosition="883">snik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). Besides WordNet, we used two additional lexical-semantic resources for the construction of the ESA vector space: Wikipedia and Wiktionary1. Textual Entailment We experimented with using the BIUTEE textual entailment system (Stern and Dagan, 2011) for generating entailment scores to serve as features for the classifier. However, these features were not selected by the classifier. Distributional Thesaurus We used similarities from a Distributional Thesaurus (similar to Lin (1998b)) computed on 10M dependency-parsed sentences of English newswire as a source for pairwise word similarity, one add</context>
</contexts>
<marker>Gabrilovich, Markovitch, 2007</marker>
<rawString>Evgeniy Gabrilovich and Shaul Markovitch. 2007. Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis. In Proceedings of the 20th International Joint Conference on Artificial Intelligence, pages 1606–1611.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Gusfield</author>
</authors>
<title>Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology.</title>
<date>1997</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="3671" citStr="Gusfield, 1997" startWordPosition="545" endWordPosition="546">cribe the feature space in detail. Section 3 describes the machine learning setup. After describing our submitted runs, we discuss the results and conclude. 2 Text Similarity Measures We now describe the various features we have tried, also listing features that did not prove useful. 2.1 Simple String-based Measures String Similarity Measures These measures operate on string sequences. The longest common 435 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 435–440, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics substring measure (Gusfield, 1997) compares the length of the longest contiguous sequence of characters. The longest common subsequence measure (Allison and Dix, 1986) drops the contiguity requirement and allows to detect similarity in case of word insertions/deletions. Greedy String Tiling (Wise, 1996) allows to deal with reordered text parts as it determines a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, </context>
</contexts>
<marker>Gusfield, 1997</marker>
<rawString>Dan Gusfield. 1997. Algorithms on Strings, Trees and Sequences: Computer Science and Computational Biology. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mark Hall</author>
<author>Eibe Frank</author>
<author>Geoffrey Holmes</author>
<author>Bernhard Pfahringer</author>
<author>Peter Reutemann</author>
<author>Ian H Witten</author>
</authors>
<title>The WEKA Data Mining Software: An Update.</title>
<date>2009</date>
<journal>SIGKDD Explorations,</journal>
<volume>11</volume>
<issue>1</issue>
<contexts>
<context position="11093" citStr="Hall et al., 2009" startWordPosition="1693" endWordPosition="1696">. (2006); 2 variants: complete texts + difference only) Explicit Semantic Analysis (Wikipedia, Wiktionary) Distributional Thesaurus (POS: Cardinal numbers) 2 All Features of Run 1 Lexical Substitution for Word Sim. (complete texts) SMT for Word Sim. (complete texts as above) 3 All Features of Run 2 Random numbers from [4.5, 5] for surprise datasets Table 1: Feature sets of our three system configurations Feature Combination The feature combination step uses the pre-computed similarity scores, and combines their log-transformed values using a linear regression classifier from the WEKA toolkit (Hall et al., 2009). We trained the classifier on the training datasets of the STS task. During the development cycle, we evaluated using 10-fold cross-validation. Post-processing For Runs 2 and 3, we applied a post-processing filter which stripped all characters off the texts which are not in the character range [a-zA-Z0-9]. If the texts match, we set their similarity score to 5.0 regardless of the classifier’s output. 4 Submitted Runs Run 1 During the development cycle, we identified 19 features (see Table 1) which achieved the best performance on the training data. For each of the known datasets, we trained a</context>
</contexts>
<marker>Hall, Frank, Holmes, Pfahringer, Reutemann, Witten, 2009</marker>
<rawString>Mark Hall, Eibe Frank, Geoffrey Holmes, Bernhard Pfahringer, Peter Reutemann, and Ian H. Witten. 2009. The WEKA Data Mining Software: An Update. SIGKDD Explorations, 11(1):10–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vasileios Hatzivassiloglou</author>
<author>Judith L Klavans</author>
<author>Eleazar Eskin</author>
</authors>
<title>Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning.</title>
<date>1999</date>
<booktitle>In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>203--212</pages>
<contexts>
<context position="8660" citStr="Hatzivassiloglou et al., 1999" startWordPosition="1322" endWordPosition="1325">assifier. Nonetheless, we briefly list them for completeness. Structural similarity between texts can be detected by computing stopword n-grams (Stamatatos, 2011). Thereby, all content-bearing words are removed while stopwords are preserved. Stopword n-grams of both texts are compared using the containment measure (Broder, 1997). In our experiments, we tested n-gram sizes for n = 2, 3, ... ,10. We also compute part-of-speech n-grams for various POS tags which we then compare using the containment measure and the Jaccard coefficient. We also used two similarity measures between pairs of words (Hatzivassiloglou et al., 1999): Word pair order tells whether two words occur in the same order in both texts (with any number of words in between), word pair distance counts the number of words which lie between those of a given pair. To compare texts along the stylistic dimension, we further use a function word frequencies measure (Dinu and Popescu, 2009) which operates on a set of 70 function words identified by Mosteller and Wallace (1964). Function word frequency vectors are computed and compared by Pearson correlation. We also include a number of measures which capture statistical properties of texts, such as typetok</context>
</contexts>
<marker>Hatzivassiloglou, Klavans, Eskin, 1999</marker>
<rawString>Vasileios Hatzivassiloglou, Judith L. Klavans, and Eleazar Eskin. 1999. Detecting text similarity over short passages: Exploring linguistic feature combinations via machine learning. In Proceedings of the Joint SIGDAT Conference on Empirical Methods in Natural Language Processing and Very Large Corpora, pages 203–212.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthew A Jaro</author>
</authors>
<title>Advances in record linkage methodology as applied to the 1985 census of Tampa Florida.</title>
<date>1989</date>
<journal>Journal of the American Statistical Association,</journal>
<volume>84</volume>
<issue>406</issue>
<contexts>
<context position="4246" citStr="Jaro (1989)" startWordPosition="640" endWordPosition="641">s substring measure (Gusfield, 1997) compares the length of the longest contiguous sequence of characters. The longest common subsequence measure (Allison and Dix, 1986) drops the contiguity requirement and allows to detect similarity in case of word insertions/deletions. Greedy String Tiling (Wise, 1996) allows to deal with reordered text parts as it determines a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for</context>
</contexts>
<marker>Jaro, 1989</marker>
<rawString>Matthew A. Jaro. 1989. Advances in record linkage methodology as applied to the 1985 census of Tampa Florida. Journal of the American Statistical Association, 84(406):414–420.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jay J Jiang</author>
<author>David W Conrath</author>
</authors>
<title>Semantic similarity based on corpus statistics and lexical taxonomy.</title>
<date>1997</date>
<booktitle>In Proceedings of the 10th International Conference on Research in Computational Linguistics.</booktitle>
<contexts>
<context position="5082" citStr="Jiang and Conrath (1997)" startWordPosition="773" endWordPosition="776">ng the original trigram variant to n = 2, 3, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Sema</context>
</contexts>
<marker>Jiang, Conrath, 1997</marker>
<rawString>Jay J. Jiang and David W. Conrath. 1997. Semantic similarity based on corpus statistics and lexical taxonomy. In Proceedings of the 10th International Conference on Research in Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Hieu Hoang</author>
<author>Alexandra Birch</author>
<author>Chris Callison-Burch</author>
<author>Marcello Federico</author>
<author>Nicola Bertoldi</author>
<author>Brooke Cowan</author>
<author>Wade Shen</author>
</authors>
<title>Moses: Open Source Toolkit for Statistical Machine Translation.</title>
<date>2007</date>
<booktitle>In Proceedings of the ACL</booktitle>
<pages>177--180</pages>
<location>Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra</location>
<contexts>
<context position="7017" citStr="Koehn et al., 2007" startWordPosition="1075" endWordPosition="1078">n cardinal numbers (CD) was selected in the final models. 2.3 Text Expansion Mechanisms Lexical Substitution System We used the lexical substitution system based on supervised word sense disambiguation (Biemann, 2012). This system automatically provides substitutions for a set of about 1,000 frequent English nouns with high precision. For each covered noun, we added the substitutions to the text and computed the pairwise word similarity for the texts as described above. This feature alleviates the lexical gap for a subset of words. Statistical Machine Translation We used the Moses SMT system (Koehn et al., 2007) to translate the original English texts via three bridge languages (Dutch, German, Spanish) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. The system was trained on Europarl made available by Koehn (2005), using the following configuration which was not optimized for this task: WMT112 baseline without tuning, with MGIZA alignment. The largest improvement was reached for computing pairwise word similarity (as described above) on the concatenation of the original text and the three back-translation</context>
</contexts>
<marker>Koehn, Hoang, Birch, Callison-Burch, Federico, Bertoldi, Cowan, Shen, 2007</marker>
<rawString>Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondˇrej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open Source Toolkit for Statistical Machine Translation. In Proceedings of the ACL 2007 Demo and Poster Sessions, pages 177–180.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Europarl: A Parallel Corpus for Statistical Machine Translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 10th Machine Translation Summit,</booktitle>
<pages>79--86</pages>
<contexts>
<context position="7320" citStr="Koehn (2005)" startWordPosition="1126" endWordPosition="1127">nouns with high precision. For each covered noun, we added the substitutions to the text and computed the pairwise word similarity for the texts as described above. This feature alleviates the lexical gap for a subset of words. Statistical Machine Translation We used the Moses SMT system (Koehn et al., 2007) to translate the original English texts via three bridge languages (Dutch, German, Spanish) back to English. Thereby, the idea was that in the translation process additional lexemes are introduced which alleviate potential lexical gaps. The system was trained on Europarl made available by Koehn (2005), using the following configuration which was not optimized for this task: WMT112 baseline without tuning, with MGIZA alignment. The largest improvement was reached for computing pairwise word similarity (as described above) on the concatenation of the original text and the three back-translations. 2.4 Measures Related to Structure and Style In our system, we also used measures which go beyond content and capture similarity along the structure and style dimensions inherent to texts. However, as we report later on, for this content1www.wiktionary.org 20-5-grams, grow-diag-final-and alignment, m</context>
</contexts>
<marker>Koehn, 2005</marker>
<rawString>Philipp Koehn. 2005. Europarl: A Parallel Corpus for Statistical Machine Translation. In Proceedings of the 10th Machine Translation Summit, pages 79–86.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Peter W Foltz</author>
<author>Darrell Laham</author>
</authors>
<title>An introduction to latent semantic analysis.</title>
<date>1998</date>
<booktitle>Discourse Processes,</booktitle>
<pages>25--2</pages>
<contexts>
<context position="2222" citStr="Landauer et al., 1998" startWordPosition="330" endWordPosition="333">closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (Mihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). The existing measures exhibit two major limitations, though: Firstly, measures are typically used in separation. Thereby, the assumption is made that a single measure inherently captures all text characteristics which are necessary for computing similarity. Secondly, existing measures typically exclude similarity features beyond content per se, thereby implying that similarity can be computed by comparing text content exclusively, leaving out any other text characteristics. While we can only briefly tackle the second issue here, we explicitly address the fi</context>
</contexts>
<marker>Landauer, Foltz, Laham, 1998</marker>
<rawString>Thomas K. Landauer, Peter W. Foltz, and Darrell Laham. 1998. An introduction to latent semantic analysis. Discourse Processes, 25(2):259–284.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Vladimir I Levenshtein</author>
</authors>
<title>Binary codes capable of correcting deletions, insertions, and reversals.</title>
<date>1966</date>
<journal>Soviet Physics Doklady,</journal>
<volume>10</volume>
<issue>8</issue>
<contexts>
<context position="4324" citStr="Levenshtein (1966)" startWordPosition="650" endWordPosition="651"> contiguous sequence of characters. The longest common subsequence measure (Allison and Dix, 1986) drops the contiguity requirement and allows to detect similarity in case of word insertions/deletions. Greedy String Tiling (Wise, 1996) allows to deal with reordered text parts as it determines a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based repres</context>
</contexts>
<marker>Levenshtein, 1966</marker>
<rawString>Vladimir I. Levenshtein. 1966. Binary codes capable of correcting deletions, insertions, and reversals. Soviet Physics Doklady, 10(8):707–710.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
<author>Patrick Pantel</author>
</authors>
<title>Discovery of Inference Rules for Question Answering.</title>
<date>2001</date>
<journal>Natural Language Engineering,</journal>
<volume>7</volume>
<issue>4</issue>
<contexts>
<context position="1488" citStr="Lin and Pantel, 2001" startWordPosition="211" endWordPosition="214">ation of word similarity based on lexical-semantic resources. Further, we employ a lexical substitution system and statistical machine translation to add additional lexemes, which alleviates lexical gaps. Our final models, one per dataset, consist of a log-linear combination of about 20 features, out of the possible 300+ features implemented. 1 Introduction The goal of the pilot Semantic Textual Similarity (STS) task at SemEval-2012 is to measure the degree of semantic equivalence between pairs of sentences. STS is fundamental to a variety of tasks and applications such as question answering (Lin and Pantel, 2001), text reuse detection (Clough et al., 2002) or automatic essay grading (Attali and Burstein, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between text</context>
</contexts>
<marker>Lin, Pantel, 2001</marker>
<rawString>Dekang Lin and Patrick Pantel. 2001. Discovery of Inference Rules for Question Answering. Natural Language Engineering, 7(4):343–360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of International Conference on Machine Learning,</booktitle>
<pages>296--304</pages>
<contexts>
<context position="5093" citStr="Lin (1998" startWordPosition="777" endWordPosition="778">riant to n = 2, 3, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analys</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998a. An information-theoretic definition of similarity. In Proceedings of International Conference on Machine Learning, pages 296–304.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>Automatic Retrieval and Clustering of Similar Words.</title>
<date>1998</date>
<booktitle>In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>768--774</pages>
<contexts>
<context position="5093" citStr="Lin (1998" startWordPosition="777" endWordPosition="778">riant to n = 2, 3, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analys</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998b. Automatic Retrieval and Clustering of Similar Words. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics, pages 768–774.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>James Malcolm</author>
<author>Bob Dickerson</author>
</authors>
<title>Detecting short passages of similar text in large document collections.</title>
<date>2001</date>
<booktitle>In Proceedings of Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>118--125</pages>
<contexts>
<context position="4611" citStr="Lyon et al. (2001)" startWordPosition="694" endWordPosition="697">rmines a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the t</context>
</contexts>
<marker>Lyon, Malcolm, Dickerson, 2001</marker>
<rawString>Caroline Lyon, James Malcolm, and Bob Dickerson. 2001. Detecting short passages of similar text in large document collections. In Proceedings of Conference on Empirical Methods in Natural Language Processing, pages 118–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip M McCarthy</author>
<author>Scott Jarvis</author>
</authors>
<title>MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Behavior research methods,</title>
<date>2010</date>
<pages>42--2</pages>
<contexts>
<context position="9337" citStr="McCarthy and Jarvis, 2010" startWordPosition="1434" endWordPosition="1437">the same order in both texts (with any number of words in between), word pair distance counts the number of words which lie between those of a given pair. To compare texts along the stylistic dimension, we further use a function word frequencies measure (Dinu and Popescu, 2009) which operates on a set of 70 function words identified by Mosteller and Wallace (1964). Function word frequency vectors are computed and compared by Pearson correlation. We also include a number of measures which capture statistical properties of texts, such as typetoken ratio (TTR) (Templin, 1957) and sequential TTR (McCarthy and Jarvis, 2010). 3 System Description We first run each of the similarity measures introduced above separately. We then use the resulting scores as features for a machine learning classifier. Pre-processing Our system is based on DKPro3, a collection of software components for natural language processing built upon the Apache UIMA framework. During the pre-processing phase, we tokenize the input texts and lemmatize using the TreeTagger implementation (Schmid, 1994). For some measures, we additionally apply a stopword filter. Feature Generation We now compute similarity scores for the input texts with all mea</context>
</contexts>
<marker>McCarthy, Jarvis, 2010</marker>
<rawString>Philip M. McCarthy and Scott Jarvis. 2010. MTLD, vocd-D, and HD-D: A validation study of sophisticated approaches to lexical diversity assessment. Behavior research methods, 42(2):381–92.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and Knowledge-based Measures of Text Semantic Similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the 21st National Conference on Artificial Intelligence,</booktitle>
<pages>775--780</pages>
<contexts>
<context position="2199" citStr="Mihalcea et al., 2006" startWordPosition="326" endWordPosition="329">in, 2006). STS is also closely related to textual entailment (TE) (Dagan et al., 2006) and paraphrase recognition (Dolan et al., 2004). It differs from both tasks, though, insofar as those operate on binary similarity decisions while STS is defined as a graded notion of similarity. STS further requires a bidirectional similarity relationship to hold between a pair of sentences rather than a unidirectional entailment relation as for the TE task. A multitude of measures for computing similarity between texts have been proposed in the past based on surface-level and/or semantic content features (Mihalcea et al., 2006; Landauer et al., 1998; Gabrilovich and Markovitch, 2007). The existing measures exhibit two major limitations, though: Firstly, measures are typically used in separation. Thereby, the assumption is made that a single measure inherently captures all text characteristics which are necessary for computing similarity. Secondly, existing measures typically exclude similarity features beyond content per se, thereby implying that similarity can be computed by comparing text content exclusively, leaving out any other text characteristics. While we can only briefly tackle the second issue here, we ex</context>
<context position="5283" citStr="Mihalcea et al. (2006)" startWordPosition="806" endWordPosition="809">igh n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). Besides WordNet, we used two additional lexical-semantic resources for the construction of the ESA vector space: Wikipedia and Wiktionary1. Textu</context>
<context position="10483" citStr="Mihalcea et al. (2006)" startWordPosition="1603" endWordPosition="1606">Generation We now compute similarity scores for the input texts with all measures and for all configurations introduced in Section 2. This resulted in 300+ individual score vectors which served as features for the following step. 3http://dkpro-core-asl.googlecode.com Run Features 1 Greedy String Tiling Longest common subsequence (2 normalizations) Longest common substring Character 2-, 3-, and 4-grams Word 1- and 2-grams (Containment, w/o stopwords) Word 1-, 3-, and 4-grams (Jaccard) Word 2- and 4-grams (Jaccard, w/o stopwords) Word Similarity (Resnik (1995) on WordNet aggregated according to Mihalcea et al. (2006); 2 variants: complete texts + difference only) Explicit Semantic Analysis (Wikipedia, Wiktionary) Distributional Thesaurus (POS: Cardinal numbers) 2 All Features of Run 1 Lexical Substitution for Word Sim. (complete texts) SMT for Word Sim. (complete texts as above) 3 All Features of Run 2 Random numbers from [4.5, 5] for surprise datasets Table 1: Feature sets of our three system configurations Feature Combination The feature combination step uses the pre-computed similarity scores, and combines their log-transformed values using a linear regression classifier from the WEKA toolkit (Hall et </context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and Knowledge-based Measures of Text Semantic Similarity. In Proceedings of the 21st National Conference on Artificial Intelligence, pages 775–780.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alvaro Monge</author>
<author>Charles Elkan</author>
</authors>
<title>An efficient domain-independent algorithm for detecting approximately duplicate database records.</title>
<date>1997</date>
<booktitle>In Proceedings of the SIGMOD Workshop on Data Mining and Knowledge Discovery,</booktitle>
<pages>23--29</pages>
<contexts>
<context position="4300" citStr="Monge and Elkan (1997)" startWordPosition="645" endWordPosition="648">es the length of the longest contiguous sequence of characters. The longest common subsequence measure (Allison and Dix, 1986) drops the contiguity requirement and allows to detect similarity in case of word insertions/deletions. Greedy String Tiling (Wise, 1996) allows to deal with reordered text parts as it determines a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate</context>
</contexts>
<marker>Monge, Elkan, 1997</marker>
<rawString>Alvaro Monge and Charles Elkan. 1997. An efficient domain-independent algorithm for detecting approximately duplicate database records. In Proceedings of the SIGMOD Workshop on Data Mining and Knowledge Discovery, pages 23–29.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frederick Mosteller</author>
<author>David L Wallace</author>
</authors>
<title>Inference and disputed authorship: The Federalist.</title>
<date>1964</date>
<publisher>Addison-Wesley.</publisher>
<contexts>
<context position="9077" citStr="Mosteller and Wallace (1964)" startWordPosition="1395" endWordPosition="1398">of-speech n-grams for various POS tags which we then compare using the containment measure and the Jaccard coefficient. We also used two similarity measures between pairs of words (Hatzivassiloglou et al., 1999): Word pair order tells whether two words occur in the same order in both texts (with any number of words in between), word pair distance counts the number of words which lie between those of a given pair. To compare texts along the stylistic dimension, we further use a function word frequencies measure (Dinu and Popescu, 2009) which operates on a set of 70 function words identified by Mosteller and Wallace (1964). Function word frequency vectors are computed and compared by Pearson correlation. We also include a number of measures which capture statistical properties of texts, such as typetoken ratio (TTR) (Templin, 1957) and sequential TTR (McCarthy and Jarvis, 2010). 3 System Description We first run each of the similarity measures introduced above separately. We then use the resulting scores as features for a machine learning classifier. Pre-processing Our system is based on DKPro3, a collection of software components for natural language processing built upon the Apache UIMA framework. During the </context>
</contexts>
<marker>Mosteller, Wallace, 1964</marker>
<rawString>Frederick Mosteller and David L. Wallace. 1964. Inference and disputed authorship: The Federalist. Addison-Wesley.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philip Resnik</author>
</authors>
<title>Using Information Content to Evaluate Semantic Similarity in a Taxonomy.</title>
<date>1995</date>
<booktitle>In Proceedings of the 14th International Joint Conference on Artificial Intelligence,</booktitle>
<pages>448--453</pages>
<contexts>
<context position="5114" citStr="Resnik (1995)" startWordPosition="780" endWordPosition="781">, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on a semantic level operate on a graph-based representation of words and the semantic relations among them within a lexicalsemantic resource. For this system, we used the algorithms by Jiang and Conrath (1997), Lin (1998a), and Resnik (1995) on WordNet (Fellbaum, 1998). In order to scale the resulting pairwise word similarities to the text level, we applied the aggregation strategy by Mihalcea et al. (2006): The sum of the idf -weighted similarity scores of each word with the best-matching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich</context>
<context position="10425" citStr="Resnik (1995)" startWordPosition="1596" endWordPosition="1597">we additionally apply a stopword filter. Feature Generation We now compute similarity scores for the input texts with all measures and for all configurations introduced in Section 2. This resulted in 300+ individual score vectors which served as features for the following step. 3http://dkpro-core-asl.googlecode.com Run Features 1 Greedy String Tiling Longest common subsequence (2 normalizations) Longest common substring Character 2-, 3-, and 4-grams Word 1- and 2-grams (Containment, w/o stopwords) Word 1-, 3-, and 4-grams (Jaccard) Word 2- and 4-grams (Jaccard, w/o stopwords) Word Similarity (Resnik (1995) on WordNet aggregated according to Mihalcea et al. (2006); 2 variants: complete texts + difference only) Explicit Semantic Analysis (Wikipedia, Wiktionary) Distributional Thesaurus (POS: Cardinal numbers) 2 All Features of Run 1 Lexical Substitution for Word Sim. (complete texts) SMT for Word Sim. (complete texts as above) 3 All Features of Run 2 Random numbers from [4.5, 5] for surprise datasets Table 1: Feature sets of our three system configurations Feature Combination The feature combination step uses the pre-computed similarity scores, and combines their log-transformed values using a li</context>
</contexts>
<marker>Resnik, 1995</marker>
<rawString>Philip Resnik. 1995. Using Information Content to Evaluate Semantic Similarity in a Taxonomy. In Proceedings of the 14th International Joint Conference on Artificial Intelligence, pages 448–453.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Helmut Schmid</author>
</authors>
<title>Probabilistic part-of-speech tagging using decision trees.</title>
<date>1994</date>
<booktitle>In Proceedings of the International Conference on New Methods in Language Processing,</booktitle>
<pages>44--49</pages>
<contexts>
<context position="9791" citStr="Schmid, 1994" startWordPosition="1505" endWordPosition="1506"> number of measures which capture statistical properties of texts, such as typetoken ratio (TTR) (Templin, 1957) and sequential TTR (McCarthy and Jarvis, 2010). 3 System Description We first run each of the similarity measures introduced above separately. We then use the resulting scores as features for a machine learning classifier. Pre-processing Our system is based on DKPro3, a collection of software components for natural language processing built upon the Apache UIMA framework. During the pre-processing phase, we tokenize the input texts and lemmatize using the TreeTagger implementation (Schmid, 1994). For some measures, we additionally apply a stopword filter. Feature Generation We now compute similarity scores for the input texts with all measures and for all configurations introduced in Section 2. This resulted in 300+ individual score vectors which served as features for the following step. 3http://dkpro-core-asl.googlecode.com Run Features 1 Greedy String Tiling Longest common subsequence (2 normalizations) Longest common substring Character 2-, 3-, and 4-grams Word 1- and 2-grams (Containment, w/o stopwords) Word 1-, 3-, and 4-grams (Jaccard) Word 2- and 4-grams (Jaccard, w/o stopwor</context>
</contexts>
<marker>Schmid, 1994</marker>
<rawString>Helmut Schmid. 1994. Probabilistic part-of-speech tagging using decision trees. In Proceedings of the International Conference on New Methods in Language Processing, pages 44–49.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Efstathios Stamatatos</author>
</authors>
<title>Plagiarism detection using stopword n-grams.</title>
<date>2011</date>
<journal>Journal of the American Society for Information Science and Technology,</journal>
<volume>62</volume>
<issue>12</issue>
<contexts>
<context position="8192" citStr="Stamatatos, 2011" startWordPosition="1249" endWordPosition="1251">iginal text and the three back-translations. 2.4 Measures Related to Structure and Style In our system, we also used measures which go beyond content and capture similarity along the structure and style dimensions inherent to texts. However, as we report later on, for this content1www.wiktionary.org 20-5-grams, grow-diag-final-and alignment, msd-bidirectional-fe reodering, interpolation and kndiscount 436 oriented task they were not selected by the classifier. Nonetheless, we briefly list them for completeness. Structural similarity between texts can be detected by computing stopword n-grams (Stamatatos, 2011). Thereby, all content-bearing words are removed while stopwords are preserved. Stopword n-grams of both texts are compared using the containment measure (Broder, 1997). In our experiments, we tested n-gram sizes for n = 2, 3, ... ,10. We also compute part-of-speech n-grams for various POS tags which we then compare using the containment measure and the Jaccard coefficient. We also used two similarity measures between pairs of words (Hatzivassiloglou et al., 1999): Word pair order tells whether two words occur in the same order in both texts (with any number of words in between), word pair dis</context>
</contexts>
<marker>Stamatatos, 2011</marker>
<rawString>Efstathios Stamatatos. 2011. Plagiarism detection using stopword n-grams. Journal of the American Society for Information Science and Technology, 62(12):2512–2527.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Asher Stern</author>
<author>Ido Dagan</author>
</authors>
<title>A Confidence Model for Syntactically-Motivated Entailment Proofs.</title>
<date>2011</date>
<booktitle>In Proceedings of the International Conference on Recent Advances in Natural Language Processing,</booktitle>
<pages>455--462</pages>
<contexts>
<context position="5984" citStr="Stern and Dagan, 2011" startWordPosition="916" endWordPosition="919">ching counterpart in the other text is computed in both directions, then averaged. In our experiments, the measure by Resnik (1995) proved to be superior to the other measures and was used in all word similarity settings throughout this paper. Explicit Semantic Analysis We also used the vector space model Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007). Besides WordNet, we used two additional lexical-semantic resources for the construction of the ESA vector space: Wikipedia and Wiktionary1. Textual Entailment We experimented with using the BIUTEE textual entailment system (Stern and Dagan, 2011) for generating entailment scores to serve as features for the classifier. However, these features were not selected by the classifier. Distributional Thesaurus We used similarities from a Distributional Thesaurus (similar to Lin (1998b)) computed on 10M dependency-parsed sentences of English newswire as a source for pairwise word similarity, one additional feature per POS tag. However, only the feature based on cardinal numbers (CD) was selected in the final models. 2.3 Text Expansion Mechanisms Lexical Substitution System We used the lexical substitution system based on supervised word sense</context>
</contexts>
<marker>Stern, Dagan, 2011</marker>
<rawString>Asher Stern and Ido Dagan. 2011. A Confidence Model for Syntactically-Motivated Entailment Proofs. In Proceedings of the International Conference on Recent Advances in Natural Language Processing, pages 455–462.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mildred C Templin</author>
</authors>
<title>Certain language skills in children.</title>
<date>1957</date>
<publisher>University of Minnesota Press.</publisher>
<contexts>
<context position="9290" citStr="Templin, 1957" startWordPosition="1429" endWordPosition="1430">r tells whether two words occur in the same order in both texts (with any number of words in between), word pair distance counts the number of words which lie between those of a given pair. To compare texts along the stylistic dimension, we further use a function word frequencies measure (Dinu and Popescu, 2009) which operates on a set of 70 function words identified by Mosteller and Wallace (1964). Function word frequency vectors are computed and compared by Pearson correlation. We also include a number of measures which capture statistical properties of texts, such as typetoken ratio (TTR) (Templin, 1957) and sequential TTR (McCarthy and Jarvis, 2010). 3 System Description We first run each of the similarity measures introduced above separately. We then use the resulting scores as features for a machine learning classifier. Pre-processing Our system is based on DKPro3, a collection of software components for natural language processing built upon the Apache UIMA framework. During the pre-processing phase, we tokenize the input texts and lemmatize using the TreeTagger implementation (Schmid, 1994). For some measures, we additionally apply a stopword filter. Feature Generation We now compute sim</context>
</contexts>
<marker>Templin, 1957</marker>
<rawString>Mildred C. Templin. 1957. Certain language skills in children. University of Minnesota Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William E Winkler</author>
</authors>
<title>String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage.</title>
<date>1990</date>
<booktitle>In Proceedings of the Section on Survey Research Methods,</booktitle>
<pages>354--359</pages>
<contexts>
<context position="4276" citStr="Winkler, 1990" startWordPosition="643" endWordPosition="644">ld, 1997) compares the length of the longest contiguous sequence of characters. The longest common subsequence measure (Allison and Dix, 1986) drops the contiguity requirement and allows to detect similarity in case of word insertions/deletions. Greedy String Tiling (Wise, 1996) allows to deal with reordered text parts as it determines a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, ... ,15. We also compare word n-grams using the Jaccard coefficient as previously done by Lyon et al. (2001), and the containment measure (Broder, 1997). As high n led to instabilities of the classifier due to their high intercorrelation, only n = 1, 2, 3, 4 was used. 2.2 Semantic Similarity Measures Pairwise Word Similarity The measures for computing word similarity on </context>
</contexts>
<marker>Winkler, 1990</marker>
<rawString>William E. Winkler. 1990. String Comparator Metrics and Enhanced Decision Rules in the Fellegi-Sunter Model of Record Linkage. In Proceedings of the Section on Survey Research Methods, pages 354–359.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael J Wise</author>
</authors>
<title>YAP3: Improved detection of similarities in computer program and other texts.</title>
<date>1996</date>
<booktitle>In Proceedings of the 27th SIGCSE technical symposium on Computer science education,</booktitle>
<pages>130--134</pages>
<contexts>
<context position="3941" citStr="Wise, 1996" startWordPosition="585" endWordPosition="586">ve useful. 2.1 Simple String-based Measures String Similarity Measures These measures operate on string sequences. The longest common 435 First Joint Conference on Lexical and Computational Semantics (*SEM), pages 435–440, Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics substring measure (Gusfield, 1997) compares the length of the longest contiguous sequence of characters. The longest common subsequence measure (Allison and Dix, 1986) drops the contiguity requirement and allows to detect similarity in case of word insertions/deletions. Greedy String Tiling (Wise, 1996) allows to deal with reordered text parts as it determines a set of shared contiguous substrings, whereby each substring is a match of maximal length. We further used the following measures, which, however, did not make it into the final models, since they were subsumed by the other measures: Jaro (1989), Jaro-Winkler (Winkler, 1990), Monge and Elkan (1997), and Levenshtein (1966). Character/word n-grams We compare character n-grams following the implementation by Barr´onCede˜no et al. (2010), thereby generalizing the original trigram variant to n = 2, 3, ... ,15. We also compare word n-grams </context>
</contexts>
<marker>Wise, 1996</marker>
<rawString>Michael J. Wise. 1996. YAP3: Improved detection of similarities in computer program and other texts. In Proceedings of the 27th SIGCSE technical symposium on Computer science education, pages 130–134.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>