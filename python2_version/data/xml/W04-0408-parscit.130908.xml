<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000008">
<note confidence="0.988695">
Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 56-63
</note>
<title confidence="0.976923">
Multiword expressions as dependency subgraphs
</title>
<author confidence="0.921483">
Ralph Debusmann
</author>
<affiliation confidence="0.817933">
Programming Systems Lab
Saarland University
</affiliation>
<address confidence="0.9716345">
Postfach 15 11 50
66041 Saarbr¨ucken, Germany
</address>
<email confidence="0.99869">
rade@ps.uni-sb.de
</email>
<sectionHeader confidence="0.995633" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9996595">
We propose to model multiword expres-
sions as dependency subgraphs, and re-
alize this idea in the grammar formal-
ism of Extensible Dependency Gram-
mar (XDG). We extend XDG to lexi-
calize dependency subgraphs, and show
how to compile them into simple lexical
entries, amenable to parsing and gener-
ation with the existing XDG constraint
solver.
</bodyText>
<sectionHeader confidence="0.998994" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999255988505748">
In recent years, dependency grammar (DG)
(Tesni`ere, 1959; Sgall et al., 1986; Mel’ˇcuk, 1988)
has received resurgent interest. Core concepts
such as grammatical functions, valency and the
head-dependent asymmetry have now found their
way into most grammar formalisms, including
phrase structure-based ones such as HPSG, LFG
and TAG. This renewed interest in DG has also
given rise to new grammar formalisms based di-
rectly on DG (Nasr, 1995; Heinecke et al., 1998;
Br¨oker, 1999; Gerdes and Kahane, 2001; Kruijff,
2001; Joshi and Rambow, 2003).
A controversy among DG grammarians cir-
cles around the question of assuming a 1:1-
correspondence between words and nodes in the
dependency graph. This assumption simplifies
the formalization of DGs substantially, and is of-
ten required for parsing. But as soon as se-
mantics comes in, the picture changes. Clearly,
the 1:1-correspondence between words and nodes
does not hold anymore for multiword expressions
(MWEs), where one semantic unit, represented
by a node in a semantically oriented dependency
graph, corresponds not to one, but to more than
one word.
Most DGs interested in semantics have thus
weakened the 1:1-assumption, starting with
Tesni`ere’s work. Tesni`ere proposed the con-
cept of nuclei to group together sets of nodes.
FGD, on the other hand, allows for the dele-
tion of solely syntactically motivated nodes in the
tectogrammatical structure. Similarly, in MTT,
nodes present on the syntactic structures can be
deleted on the semantic structure. This can happen
e.g. through paraphrasing rules implemented by
lexical functions (Mel’ˇcuk, 1996). Unfortunately,
these attempts to break the 1:1-correspondence
have not yet been formalized in a declarative way.
Extensible Dependency Grammar (XDG) is a
new grammar formalism based on Topological
Dependency Grammar (TDG) (Duchier and De-
busmann, 2001). From TDG, it inherits declara-
tive word order constraints, the ability to distin-
guish multiple dimensions of linguistic descrip-
tion, and an axiomatization as a constraint sat-
isfaction problem (Duchier, 2003) solvable using
constraint programming (Apt, 2003). One of the
benefits of this axiomatization is that the linear or-
der of the words can be left underspecified, with
the effect that the constraint solver can be applied
for both parsing and generation.
XDG solving is efficient at least for the smaller-
scale example grammars tested so far, but these
good results hinge substantially on the assump-
tion of a 1:1-correspondence between words and
nodes. As XDG has been created to cover not only
syntax but also semantics, we have no choice but
to weaken the 1:1-correspondence.
In this paper, we outline a way to break out of
the 1:1-straightjacket, without sacrificing the po-
tential for efficient parsing and generation. We in-
troduce a new layer of lexical organization called
groups above the basic XDG lexicon, allowing us
to describe MWEs as tuples of dependency sub-
graphs. Groups can be compiled into simple lexi-
cal entries, which can then be used by the already
existing XDG solver for parsing and generation.
With groups, we can omit nodes present in the syn-
tactic dimensions in the semantic dimensions, and
thus get away from the 1:1-correspondence.
Groups are motivated by the continuity hypothe-
sis of (Kay and Fillmore, 1999), assuming the con-
tinuity of the lexicon and the construction. They
can also be regarded as a declarative formalization
of Mel’ˇcuk’s paraphrasing rules, or as a realization
of the extended domain of locality of TAG (Joshi,
1987) in terms of dependency grammar, as also
proposed in (Nasr, 1996) and (Joshi and Rambow,
2003).
The structure of this paper is as follows. We in-
troduce XDG in section 2. In section 3, we intro-
duce groups, the new layer of lexical organization
required for the modeling of MWEs, and in sec-
tion 4, we show how to compile groups into sim-
ple lexical entries amenable for parsing and gener-
ation. Section 5 concludes the paper and outlines
future work.
</bodyText>
<sectionHeader confidence="0.991427" genericHeader="introduction">
2 XDG
</sectionHeader>
<bodyText confidence="0.99980625">
In this section, we explain the intuitions behind
XDG, before proceeding with its formalization,
and a description of the XDG solver for parsing
and generation.
</bodyText>
<subsectionHeader confidence="0.883889">
2.1 XDG intuitions
</subsectionHeader>
<bodyText confidence="0.999506085714286">
Extensible Dependency Grammar (XDG) is a new
grammar formalism generalizing Topological De-
pendency Grammar (TDG) (Duchier and Debus-
mann, 2001). XDG characterizes linguistic struc-
ture along arbitrary many dimensions of descrip-
tion. All dimensions correspond to labeled graphs,
sharing the same set of nodes but having different
edges.
The well-formedness conditions for XDG anal-
yses are determined by principles. Principles can
either be one-dimensional, applying to a single
dimension only, or multi-dimensional, constrain-
ing the relation of several dimensions. Basic one-
dimensional principles are treeness and valency.
Multi-dimensional principles include climbing (as
in (Duchier and Debusmann, 2001); one dimen-
sion must be a flattening of another) and linking
(e.g. to specify how semantic arguments must be
realized syntactically).
The lexicon plays a central role in XDG. For
each node, it provides a set of possible lexical en-
tries (feature structures) serving as the parameters
for the principles. Because a lexical entry con-
strains all dimensions simultaneously, it can also
help to synchronize the various dimensions, e.g.
with respect to valency. For instance, a lexical en-
try could synchronize syntactic and semantic di-
mensions by requiring a subject in the syntax, and
an agent in the semantics.
As an example, we show in (1) an analysis
for the sentence He dates her, along two dimen-
sions of description, immediate dominance (ID)
and predicate-argument structure (PA). We display
the ID part of the analysis on the left, and the PA
part on the right:1
</bodyText>
<equation confidence="0.966641">
(1)
</equation>
<bodyText confidence="0.985912333333333">
He dates her He dates her
The set of edge labels on the ID dimension in-
cludes subj for subject and obj for object. On
the PA dimension, we have arg1 and arg2 stand-
ing for the argument slots of semantic predicates.2
The ID part of the analysis states that He is the sub-
ject, and her the object of dates. The PA part states
that He is the first argument and her the second ar-
gument of dates.
</bodyText>
<footnote confidence="0.978436714285714">
1For lack of space, we omit the dimension of linear prece-
dence (LP) from the presentation in this paper. For this di-
mension, we use the same theory as for TDG (Duchier and
Debusmann, 2001).
2We could also use some set of thematic roles for the PA
edge labels, but since the assumption of thematic roles is very
controversial, we decided to choose more neutral labels.
</footnote>
<bodyText confidence="0.999953206896552">
The principles of the underlying grammar re-
quire that the ID part of each analysis is a tree,
and the PA part a directed acyclic graph (dag).34
In addition, we employ the valency principle on
both dimensions, specifying the licensed incom-
ing and outgoing edges of each node. The only
employed multi-dimensional principle is the link-
ing principle, specifying how semantic arguments
are realized syntactically.
Figure 1 shows the lexicon of the underlying
grammar. Each lexical entry corresponds to both
a word and a semantic literal. inID and outID
parametrize the valency principle on the ID di-
mension. inID specifies the licensed incoming,
and outID the licensed outgoing edges. E.g.
He licenses zero or one incoming edges labeled
subj, and no outgoing edges. inPA and outPA
parametrize the valency principle on the PA dimen-
sion. E.g. dates licenses no incoming edges, and
requires precisely one outgoing edge labeled arg1
and one labeled arg2. link parametrizes the multi-
dimensional linking principle. E.g. dates syntac-
tically realizes its first argument by a subject, the
second argument by an object.
Observe that all the principles are satisfied in
(1), and hence the analysis is well-formed. Also
notice that we can use the same grammar and lex-
icon for both parsing (from words) and generation
(from semantic literals).
</bodyText>
<subsectionHeader confidence="0.992371">
2.2 XDG formalization
</subsectionHeader>
<bodyText confidence="0.999746">
Formally, an XDG grammar is built up of dimen-
sions, a lexicon and principle, and characterizes a
set of well-formed analyses.
A dimension is a tuple D = (Lab, Fea, Val, Pri)
of a set Lab of edge labels, a set Fea of fea-
tures, a set Val of feature values, and a set of one-
dimensional principles Pri. A lexicon for the di-
mension D is a set Lex C_ Fea —* Val of total fea-
ture assignments called lexical entries. An analy-
sis on dimension D is a triple (V, E, F) of a set V
of nodes, a set E C_ V x V x Lab of directed la-
beled edges, and an assignment F : V —* (Fea —*
Val) of lexical entries to nodes. V and E form a
</bodyText>
<footnote confidence="0.9625874">
3In the following, we will call the ID part of an analysis
ID tree, and the PA part PA dag.
4The PA structure is a dag and not a tree because we
want it to reflect the re-entrancy e.g. in control constructions,
where the same subject is shared by more than one node.
</footnote>
<bodyText confidence="0.999749392857143">
graph. We write AnaD for the set of all possible
analyses on dimension D. The principles charac-
terize subsets of AnaD. We assume that the ele-
ments of Pri are finite representations of such sub-
sets.
An XDG grammar ((Labi,Feai, Vali,Prii)ni=1,
Pri, Lex) consists of n dimensions, multi-
dimensional principles Pri, and a lexicon Lex.
An XDG analysis (V, Ei, Fi)ni=1 is an element of
Ana = Ana1 x · · · x Anan where all dimensions
share the same set of nodes V . We call a dimen-
sion of a grammar grammar dimension.
Multi-dimensional principles specify subsets of
Ana, i.e. of tuples of analyses for the individual di-
mensions. The lexicon Lex C_ Lex1 x · · · x Lexn
constrains all dimensions at once, thereby syn-
chronizing them. An XDG analysis is licensed by
Lex iff (F1(v), ... , Fn(v)) E Lex for every node
vEV.
In order to compute analyses for a given input,
we employ a set of input constraints (Inp), which
again specify a subset of Ana. XDG solving then
amounts to finding elements of Ana that are li-
censed by Lex, and consistent with Inp and Pri.
The input constraints e.g. determine whether XDG
solving is to be used for parsing or generation. For
parsing, they specify a sequence of words, and for
generation, a multiset of semantic literals.
</bodyText>
<subsectionHeader confidence="0.997892">
2.3 XDG solver
</subsectionHeader>
<bodyText confidence="0.999848388888889">
XDG solving has a natural reading as a constraint
satisfaction problem (CSP) on finite sets of inte-
gers, where well-formed analyses correspond to
the solutions of the CSP (Duchier, 2003). We
have implemented an XDG solver (Debusmann,
2003) using the Mozart-Oz programming system
(Mozart Consortium, 2004).
XDG solving operates on all dimensions con-
currently. This means that the solver can infer in-
formation about one dimension from information
on another, if there is either a multi-dimensional
principle linking the two dimensions, or by the
synchronization induced by the lexical entries. For
instance, not only can syntactic information trig-
ger inferences in syntax, but also vice versa.
Because XDG allows us to write grammars
with completely free word order, XDG solving is
an NP-complete problem (Koller and Striegnitz,
</bodyText>
<table confidence="0.76947075">
word literal inID outID inPA outPA link
He he’ {subj?} {} {arg1?,arg2?} {} {}
dates date’ {} {subj!, obj!} {} {arg1!, arg2!} {arg1  {subj}, arg2  {obj}}
her she’ {obj?} {} {arg1?,arg2?} {} {}
</table>
<figureCaption confidence="0.989828">
Figure 1: Lexicon for the sentence He dates her.
</figureCaption>
<bodyText confidence="0.997776666666667">
2002). This means that the worst-case complex-
ity of the solver is exponential. The average-case
complexity of many smaller-scale grammars that
we have experimented with seems polynomial, but
it remains to be seen whether we can scale this up
to large-scale grammars.
</bodyText>
<sectionHeader confidence="0.999244" genericHeader="method">
3 Groups
</sectionHeader>
<bodyText confidence="0.996794666666667">
In this section, we consider MWE paraphrases and
propose to model them as tuples of dependency
subgraphs called groups. We start with an exam-
ple: Consider (3), a paraphrase of (2):
He dates her. (2)
He takes her out. (3)
We display the XDG analysis of (3) in (4).
Again, the ID tree is on the left, and the PA dag
on the right:5
</bodyText>
<equation confidence="0.600534">
(4)
</equation>
<bodyText confidence="0.999798642857143">
This example demonstrates that we cannot sim-
ply treat MWEs as contiguous word strings and
include those in the lexicon, since the MWE takes
out is interrupted by the object her in (3). Instead,
we choose to implement the continuity hypothe-
sis of (Kay and Fillmore, 1999) in terms of DG,
and model MWEs as dependency subgraphs. We
realize this idea by a new layer of lexical orga-
nization called groups. A group is tuple of de-
pendency subgraphs covering one or more nodes,
where each component of the tuple corresponds to
a grammar dimension. We call a group compo-
nent group dimension. We display the group cor-
responding to dates in (5), and the one correspond-
</bodyText>
<footnote confidence="0.847199">
5In the ID tree, part stands for a particle.
</footnote>
<bodyText confidence="0.762491">
ing to takes out in (6).
</bodyText>
<subsubsectionHeader confidence="0.406441">
dates dates
</subsubsectionHeader>
<bodyText confidence="0.96761725">
Groups can leave out nodes present in the ID
dimension on the PA dimension. E.g. in (6), the
node corresponding to the particle out is present
on the ID dimension, but left out on the PA dimen-
sion. In this way, groups help us to weaken the
1:1-correspondence between words and nodes.
We can make use of groups also to handle more
complicated MWE paraphrases. Consider (8) be-
low, a support verb construction paraphrasing (7):
He argues with her. (7)
He has an argument with her. (8)
In (8), has is only a support verb; the semantic
head of the construction is the noun argument. We
display the ID tree and PA dag of (7) in (9). The ID
tree of (8) is in (10), and the PA dag in (11):6
He argues with her He argues with her
</bodyText>
<footnote confidence="0.83868575">
(9)
6In the ID trees, pobj stands for a prepositional object,
pcomp for the complement of a preposition, pmod for a
prepositional modifier, and det for a determiner.
</footnote>
<bodyText confidence="0.976191">
takes out takes out
He takes her out He takes her out
tuples of dependency subgraphs spanning over a
shared set of nodes. This sharing enables groups
to express interdependencies between the differ-
ent dimensions. For instance in (13) and (14), the
noun argument, the object of the support verb has
in the ID dimension, is the semantic head in the PA
dimension.
He has an argument
. with her (10)
He has an argument with her (11)
In (9), the node corresponding to with is deleted
in the PA dag. In (11), the support verb construc-
tion leads to the deletion of three nodes (corre-
sponding to resp. has, an and with). These dele-
tions are reflected in the corresponding groups.
The group corresponding to argues with is dis-
played in (12), and the group corresponding to has
an argument with in (13) (ID) and (14) (PA):
</bodyText>
<sectionHeader confidence="0.997065" genericHeader="method">
4 Compilation
</sectionHeader>
<bodyText confidence="0.99985325">
In this section, we show how to compile groups
into simple lexical entries. The benefit of this is
that we can retain XDG in its entirety, i.e. we can
retain its formalization and its axiomatization as
a constraint satisfaction problem. This means we
can also retain the implementation of the XDG
solver, and continue to use it for parsing and gen-
eration.
</bodyText>
<subsectionHeader confidence="0.985617">
4.1 Node deletion
</subsectionHeader>
<bodyText confidence="0.992443709677419">
The 1:1-correspondence between words and nodes
is a key assumption of XDG. It requires that on
each dimension, each word must correspond to
precisely one node in the dependency graph. The
groups shown in the previous section clearly vio-
late this assumption as they allow nodes present on
the ID dimension to be omitted on the PA dimen-
sion.
The first step of the compilation aims to accom-
modate this deletion of nodes. To this end, we
assume for each analysis an additional root node,
e.g. corresponding to the full stop at the end of the
sentence. Each root in an XDG dependency graph
becomes a daughter of this additional root node,
the edge labeled root. The trick for deleting nodes
is now the following: Each deleted node also be-
comes a daughter of the additional root node, but
with a special node label indicating that this node
is regarded to be deleted (del).
As an example, here is the PA dag for example
(3) including the additional root node:
has an argument with (14)
argues with argues with
has an argument
. with
Groups can capture difficult constructions such
as the support verb construction above in an el-
egant and transparent way, without having to re-
sort to complicated machinery. The key aspect
of groups is their multi-dimensionality, describing
He takes her out .
</bodyText>
<page confidence="0.434181">
(15)
</page>
<subsectionHeader confidence="0.977934">
4.2 Dependency subgraphs
</subsectionHeader>
<bodyText confidence="0.999573555555556">
The second step of the compilation is to compile
the dependency subgraphs into lexical entries for
individual words. To this end, we make use of the
valency principle. For each edge from v to v la-
beled l within a group, we require that v has an
outgoing edge labeled l, and that v licenses an in-
coming edge labeled l. I.e. we include l! in the out
specification of the lexical entry for v, and l? in
the in specification of the lexical entry for v.
</bodyText>
<subsectionHeader confidence="0.998049">
4.3 Group coherence
</subsectionHeader>
<bodyText confidence="0.993251357142857">
The final step of the compilation is about ensur-
ing group coherence, i.e. to ensure that the inner
nodes of a group dimension (neither the root nor
the leaves) stay within this group in the depen-
dency analysis. In other words, group coherence
make sure that the inner nodes of a group dimen-
sion cannot become daughters of nodes of a dif-
ferent group. In our support verb example, group
coherence ensures that e.g. that the determiner an
cannot become the determiner of a noun of an-
other group. We realize this idea through a new
XDG principle called group coherence principle.
This principle must hold on all dimensions of the
grammar.
Given a set of group identifiers Group, the prin-
ciple assumes two new features: group : Group,
and outgroups : Lab —* 2GTD71p. For each node v,
group(v) denotes the group identifier of the group
of v. For each edge within a group from v to
v labeled l, i.e. for each edge to an inner node,
outgroups(v)(l) denotes the singleton set contain-
ing the group of both v and v. For each edge
from v labeled l which goes outside of a group,
outgroups(v)(l) = Group, i.e. the edge can end
in any other group. We formalize the group coher-
ence principle as follows, where v —* v denotes
an edge from v to v labeled l:
v —*v =&gt;. group(v) E outgroups(v)(l)
</bodyText>
<equation confidence="0.3051725">
-
(16)
</equation>
<subsectionHeader confidence="0.973818">
4.4 Examples
</subsectionHeader>
<bodyText confidence="0.998491333333333">
For illustration, we display the lexical entries of
two compiled groups: The group argues with
(with identifier g1), and the group has an argu-
ment with (with identifier g2), resp. in Figure 2 and
Figure 3. We omit the specification of outgroups
for the PA dimension for lack of space, and since it
is not relevant for the example: In all groups, there
are no edges which stay within a group in the PA
dimension.
</bodyText>
<subsectionHeader confidence="0.999543">
4.5 Parsing and generation
</subsectionHeader>
<bodyText confidence="0.986281081081081">
We can use the same group lexicon for parsing and
generation, but we have to slightly adapt the com-
pilation for the generation case.
For parsing, we can use the XDG parser as be-
fore, without any changes.
For generation, we assume a set Sem of seman-
tic literals, multisets of which must be verbalized.
To do this, we assume a function groups : Sem —*
2GTD71p, mapping each semantic literal to a set of
groups which verbalize it.
Now before using the XDG solver for genera-
tion, we have to make sure for each semantic lit-
eral s to be verbalized that the groups which can
potentially verbalize it have the same number of
nodes. For this, we calculate the maximum num-
ber n of syntactic nodes for the groups assigned
to s, and fill up the groups having less syntactic
nodes with deleted nodes. Then for XDG solving,
we introduce precisely n nodes for literals. Using
groups for generation thus comes at the expense of
having to introduce additional nodes.7
As an example, consider we want to verbal-
ize the semantic literal argue. groups(argue) =
{g1, g2}, i.e. either groups g1 or g2 can verbalize
it. The maximum number n of syntactic nodes for
the two groups is 4 for g2 (has an argument with).
Hence, we have to fill up the groups having less
syntactic nodes with deleted nodes. In this case,
we have to add two deleted nodes to the group g1
(argue with) to get to four nodes. The resulting
lexical entries encoding g1 are displayed in Fig-
ure 4. The lexical entries for g2 stay the same as
in Figure 3.
After this step is done, we can use the existing
XDG solver to generate from the semantic literals
precisely the two possible verbalizations (7) and
(8), as intended.
</bodyText>
<footnote confidence="0.99845175">
7We should emphasize that n is not at all unrestricted.
For each semantic literal s to be verbalized, we have to intro-
duce only as many nodes as are contained in the largest group
which can verbalize s.
</footnote>
<note confidence="0.5597515">
word literal group outgroupsID inID outID inPA outPA link
argues argue’ g1 {pobj  {g1}} {root?} {subj!, pobj!} {root?} {arg1, arg2} { arg1  {subj} }
arg2  {pcomp}
with argue’ g1 {} {pobj?} {pcomp!} {del?} {} {}
</note>
<figureCaption confidence="0.931189">
Figure 2: Lexical entries encoding the group for argues with
</figureCaption>
<figure confidence="0.895664625">
word literal group outgroupsID inID outID inPA outPA link
has argue’ g2 {obj  {g2}} {root?} {subj!, obj!} {del?} {} {}
{det?} {} {del?} {}et
{}
sub
argument argue’ g2 { pmod {g{g2}} {obj?} {det!, pmod!} {root?} {arg1! arg2!} { arg2  arg1  {pcomp}}
with argue’ g2 {} {pmod?} {pcomp!} {del?} {} {}
an argue’ g2 {}
</figure>
<figureCaption confidence="0.998877">
Figure 3: Lexical entries encoding the group for has an argument with
</figureCaption>
<sectionHeader confidence="0.997995" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999973833333333">
We extended the XDG grammar formalism with
a means to break out of the straightjacket of the
1:1-correspondence between words and nodes. To
this end, we proposed the new notion of groups,
allowing to enrich the XDG lexicon with tuples
of dependency subgraphs. We illustrated how to
tackle complicated MWEs such as support verb
constructions with this new idea, and how to com-
pile groups into simple lexical entries.
We see two main benefits of our approach. The
first is that we can retain the XDG formalization,
and also its axiomatization as a constraint satisfac-
tion problem, in its entirety. Thus, we can simply
continue to use the existing XDG solver for pars-
ing and generation. The second benefit is that we
can use the same group lexicon for both parsing
and generation, the only difference being that for
generation, we have to slightly adapt the compila-
tion into lexical entries.
There are many issues which have remained un-
touched in this paper. For one, we did not talk
about our treatment of word order in this paper for
lack of space. Word order is among the best re-
searched issues in the context of XDG. For a thor-
ough discussion about word order in XDG, we re-
fer to (Duchier and Debusmann, 2001) and (De-
busmann, 2001).
Another issue is that of the relation between
groups and the meta-grammatical functionality
of the XDG lexicon, offering lexical inheritance,
templates and also disjunction in the sense of
crossings (Candito, 1996) to lexically state lin-
guistic generalizations. How well groups can be
integrated with this meta-grammatical functional-
ity is an open question.
XDG research has so far mainly been focused
on parsing, only to a very small extent on genera-
tion, and to no extent at all on Machine Translation
(MT). There is still a lot to do on both of the lat-
ter topics, and even for parsing, we are only at the
beginning. Although with our smaller-scale exam-
ple grammars, parsing and generation takes poly-
nomial time, we have yet to find out how we can
scale this up to large-scale grammars. We have
started importing and inducing large-scale gram-
mars from existing resources, but can so far only
speculate about if and how we can parse them ef-
ficiently. In a related line of research, we are also
working on the incorporation of statistical infor-
mation (Dienes et al., 2003) to help us to guide the
search for solutions. This could improve perfor-
mance because we would only have to search for
a few good analyses instead of enumerating hun-
dreds of them.
</bodyText>
<sectionHeader confidence="0.994948" genericHeader="acknowledgments">
Acknowledgements
</sectionHeader>
<bodyText confidence="0.9420765">
The idea for groups and this paper was triggered
by three events in fall 2003. The first was a work-
shop where Peter Dienes, Stefan Thater and me
worked out how to do generation with XDG. The
second was a presentation on the same workshop
by Aravind Joshi and Owen Rambow of their en-
coding of DG in TAG, and the third was a talk
by Charles Fillmore titled Multiword Expressions:
An Extremist Approach. I’d like to thank all of
them. And I’d like to thank all the others involved
with XDG, in alphabetical order: Ondrej Bojar,
Denys Duchier, Alexander Koller, Geert-Jan Krui-
</bodyText>
<table confidence="0.4607655">
word literal group outgroupsID inID outID inPA outPA link
argues argue’ g1 {pobj  {g1}} {root?} {subj!, pobj!} {root?} {arg1, arg2} { arg1  {subj} }
arg2  {pcomp}
with argue’ g1 {} {pobj?} {pcomp!} {del?} {} {}
E argue’ g1 {} {del?} {} {del?} {} {}
E argue’ g1 {} {del?} {} {del?} {} {}
</table>
<figureCaption confidence="0.99778">
Figure 4: Lexical entries for generation, encoding the group for argues with
</figureCaption>
<bodyText confidence="0.973202666666667">
jff, Vladislav Kubon, Marco Kuhlmann, Joachim
Niehren, Martin Platek and Gert Smolka for their
support and many helpful discussions.
</bodyText>
<sectionHeader confidence="0.998264" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999950213333333">
Krzysztof R. Apt. 2003. Principles of Constraint Pro-
gramming. Cambridge University Press.
Norbert Br¨oker. 1999. Eine Dependenzgrammatik
zur Kopplung heterogener Wissensquellen. Lin-
guistische Arbeiten 405. Max Niemeyer Verlag,
T¨ubingen/GER.
Marie-H`el´ene Candito. 1996. A principle-based hier-
archical representation of LTAG. In Proceedings of
COLING 1996, Kopenhagen/DEN.
Ralph Debusmann. 2001. A declarative grammar for-
malism for dependency grammar. Master’s thesis,
University of Saarland.
Ralph Debusmann. 2003. A parser system for extensi-
ble dependency grammar. In Denys Duchier, editor,
Prospects and Advances in the Syntax/Semantics In-
terface, pages 103–106. LORIA, Nancy/FRA.
Peter Dienes, Alexander Koller, and Marco Kuhlmann.
2003. Statistical A* Dependency Parsing. In
Prospects and Advances in the Syntax/Semantics In-
terface, Nancy/FRA.
Denys Duchier and Ralph Debusmann. 2001. Topo-
logical dependency trees: A constraint-based ac-
count of linear precedence. In Proceedings of ACL
2001, Toulouse/FRA.
Denys Duchier. 2003. Configuration of labeled trees
under lexicalized constraints and principles. Re-
search on Language and Computation, 1(3–4):307–
336.
Kim Gerdes and Sylvain Kahane. 2001. Word order
in german: A formal dependency grammar using a
topological hierarchy. In Proceedings ofACL 2001,
Toulouse/FRA.
Johannes Heinecke, J¨urgen Kunze, Wolfgang Menzel,
and Ingo Schr¨oder. 1998. Eliminative parsing with
graded constraints. In Proceedings of COLING/ACL
1998, pages 526–530, Montreal/CAN.
Aravind Joshi and Owen Rambow. 2003. A formal-
ism for dependency grammar based on tree adjoin-
ing grammar. In Proceedings of MTT 2003, pages
207–216, Paris/FRA.
Aravind K. Joshi. 1987. An introduction to tree-
adjoining grammars. In Alexis Manaster-Ramer, ed-
itor, Mathematics ofLanguage, pages 87–115. John
Benjamins, Amsterdam/NL.
Paul Kay and Charles J. Fillmore. 1999. Grammati-
cal constructions and linguistic generalizations: the
what’s x doing y? construction. Language, 75:1–33.
Alexander Koller and Kristina Striegnitz. 2002. Gen-
eration as dependency parsing. In Proceedings of
ACL 2002, Philadelphia/USA.
Geert-Jan M. Kruijff. 2001. A Categorial-Modal Ar-
chitecture of Informativity. Ph.D. thesis, Charles
University, Prague/CZ.
Igor Mel’ˇcuk. 1988. Dependency Syntax: Theory
and Practice. State Univ. Press of New York, Al-
bany/USA.
Igor Mel’ˇcuk. 1996. Lexical functions: a tool for the
description of lexical relations in a lexicon. In Leo
Wanner, editor, Lexical Functions in Lexicography
and Natural Language Processing. John Benjamins.
Mozart Consortium. 2004. The Mozart-Oz website.
http://www.mozart-oz.org/.
Alexis Nasr. 1995. A formalism and a parser for lexi-
calised dependency grammars. In 4th International
Workshop on Parsing Technologies, pages 186–195,
Prague/CZ.
Alexis Nasr. 1996. Un syst`eme de reformulation au-
tomatique de phrases fond´e sur la Th´eorie Sens-
Texte: application aux langues contrˆol´ees. Ph.D.
thesis, Universit´e Paris 7.
Petr Sgall, Eva Hajicova, and Jarmila Panevova. 1986.
The Meaning of the Sentence in its Semantic and
Pragmatic Aspects. D. Reidel, Dordrecht/NL.
Lucien Tesni`ere. 1959. El´ements de Syntaxe Struc-
turale. Klincksiek, Paris/FRA.
</reference>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.495778">
<note confidence="0.923034">Second ACL Workshop on Multiword Expressions: Integrating Processing, July 2004, pp. 56-63</note>
<title confidence="0.990743">Multiword expressions as dependency subgraphs</title>
<author confidence="0.942679">Ralph</author>
<affiliation confidence="0.9019035">Programming Systems Saarland</affiliation>
<address confidence="0.804678">Postfach 15 11 66041 Saarbr¨ucken,</address>
<email confidence="0.99682">rade@ps.uni-sb.de</email>
<abstract confidence="0.992816">We propose to model multiword expressions as dependency subgraphs, and realize this idea in the grammar formalism of Extensible Dependency Grammar (XDG). We extend XDG to lexicalize dependency subgraphs, and show how to compile them into simple lexical entries, amenable to parsing and generation with the existing XDG constraint solver.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Krzysztof R Apt</author>
</authors>
<title>Principles of Constraint Programming.</title>
<date>2003</date>
<publisher>Cambridge University Press.</publisher>
<contexts>
<context position="2739" citStr="Apt, 2003" startWordPosition="409" endWordPosition="410">. This can happen e.g. through paraphrasing rules implemented by lexical functions (Mel’ˇcuk, 1996). Unfortunately, these attempts to break the 1:1-correspondence have not yet been formalized in a declarative way. Extensible Dependency Grammar (XDG) is a new grammar formalism based on Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001). From TDG, it inherits declarative word order constraints, the ability to distinguish multiple dimensions of linguistic description, and an axiomatization as a constraint satisfaction problem (Duchier, 2003) solvable using constraint programming (Apt, 2003). One of the benefits of this axiomatization is that the linear order of the words can be left underspecified, with the effect that the constraint solver can be applied for both parsing and generation. XDG solving is efficient at least for the smallerscale example grammars tested so far, but these good results hinge substantially on the assumption of a 1:1-correspondence between words and nodes. As XDG has been created to cover not only syntax but also semantics, we have no choice but to weaken the 1:1-correspondence. In this paper, we outline a way to break out of the 1:1-straightjacket, with</context>
</contexts>
<marker>Apt, 2003</marker>
<rawString>Krzysztof R. Apt. 2003. Principles of Constraint Programming. Cambridge University Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Norbert Br¨oker</author>
</authors>
<title>Eine Dependenzgrammatik zur Kopplung heterogener Wissensquellen. Linguistische Arbeiten 405.</title>
<date>1999</date>
<publisher>Max Niemeyer Verlag, T¨ubingen/GER.</publisher>
<marker>Br¨oker, 1999</marker>
<rawString>Norbert Br¨oker. 1999. Eine Dependenzgrammatik zur Kopplung heterogener Wissensquellen. Linguistische Arbeiten 405. Max Niemeyer Verlag, T¨ubingen/GER.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie-H`el´ene Candito</author>
</authors>
<title>A principle-based hierarchical representation of LTAG.</title>
<date>1996</date>
<booktitle>In Proceedings of COLING</booktitle>
<location>Kopenhagen/DEN.</location>
<contexts>
<context position="22637" citStr="Candito, 1996" startWordPosition="3923" endWordPosition="3924">o slightly adapt the compilation into lexical entries. There are many issues which have remained untouched in this paper. For one, we did not talk about our treatment of word order in this paper for lack of space. Word order is among the best researched issues in the context of XDG. For a thorough discussion about word order in XDG, we refer to (Duchier and Debusmann, 2001) and (Debusmann, 2001). Another issue is that of the relation between groups and the meta-grammatical functionality of the XDG lexicon, offering lexical inheritance, templates and also disjunction in the sense of crossings (Candito, 1996) to lexically state linguistic generalizations. How well groups can be integrated with this meta-grammatical functionality is an open question. XDG research has so far mainly been focused on parsing, only to a very small extent on generation, and to no extent at all on Machine Translation (MT). There is still a lot to do on both of the latter topics, and even for parsing, we are only at the beginning. Although with our smaller-scale example grammars, parsing and generation takes polynomial time, we have yet to find out how we can scale this up to large-scale grammars. We have started importing</context>
</contexts>
<marker>Candito, 1996</marker>
<rawString>Marie-H`el´ene Candito. 1996. A principle-based hierarchical representation of LTAG. In Proceedings of COLING 1996, Kopenhagen/DEN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Debusmann</author>
</authors>
<title>A declarative grammar formalism for dependency grammar. Master’s thesis,</title>
<date>2001</date>
<institution>University of Saarland.</institution>
<contexts>
<context position="2481" citStr="Debusmann, 2001" startWordPosition="371" endWordPosition="373">nuclei to group together sets of nodes. FGD, on the other hand, allows for the deletion of solely syntactically motivated nodes in the tectogrammatical structure. Similarly, in MTT, nodes present on the syntactic structures can be deleted on the semantic structure. This can happen e.g. through paraphrasing rules implemented by lexical functions (Mel’ˇcuk, 1996). Unfortunately, these attempts to break the 1:1-correspondence have not yet been formalized in a declarative way. Extensible Dependency Grammar (XDG) is a new grammar formalism based on Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001). From TDG, it inherits declarative word order constraints, the ability to distinguish multiple dimensions of linguistic description, and an axiomatization as a constraint satisfaction problem (Duchier, 2003) solvable using constraint programming (Apt, 2003). One of the benefits of this axiomatization is that the linear order of the words can be left underspecified, with the effect that the constraint solver can be applied for both parsing and generation. XDG solving is efficient at least for the smallerscale example grammars tested so far, but these good results hinge substantially on the ass</context>
<context position="4924" citStr="Debusmann, 2001" startWordPosition="775" endWordPosition="777">section 2. In section 3, we introduce groups, the new layer of lexical organization required for the modeling of MWEs, and in section 4, we show how to compile groups into simple lexical entries amenable for parsing and generation. Section 5 concludes the paper and outlines future work. 2 XDG In this section, we explain the intuitions behind XDG, before proceeding with its formalization, and a description of the XDG solver for parsing and generation. 2.1 XDG intuitions Extensible Dependency Grammar (XDG) is a new grammar formalism generalizing Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001). XDG characterizes linguistic structure along arbitrary many dimensions of description. All dimensions correspond to labeled graphs, sharing the same set of nodes but having different edges. The well-formedness conditions for XDG analyses are determined by principles. Principles can either be one-dimensional, applying to a single dimension only, or multi-dimensional, constraining the relation of several dimensions. Basic onedimensional principles are treeness and valency. Multi-dimensional principles include climbing (as in (Duchier and Debusmann, 2001); one dimension must be a flattening of </context>
<context position="6929" citStr="Debusmann, 2001" startWordPosition="1113" endWordPosition="1114">t, and the PA part on the right:1 (1) He dates her He dates her The set of edge labels on the ID dimension includes subj for subject and obj for object. On the PA dimension, we have arg1 and arg2 standing for the argument slots of semantic predicates.2 The ID part of the analysis states that He is the subject, and her the object of dates. The PA part states that He is the first argument and her the second argument of dates. 1For lack of space, we omit the dimension of linear precedence (LP) from the presentation in this paper. For this dimension, we use the same theory as for TDG (Duchier and Debusmann, 2001). 2We could also use some set of thematic roles for the PA edge labels, but since the assumption of thematic roles is very controversial, we decided to choose more neutral labels. The principles of the underlying grammar require that the ID part of each analysis is a tree, and the PA part a directed acyclic graph (dag).34 In addition, we employ the valency principle on both dimensions, specifying the licensed incoming and outgoing edges of each node. The only employed multi-dimensional principle is the linking principle, specifying how semantic arguments are realized syntactically. Figure 1 sh</context>
<context position="22399" citStr="Debusmann, 2001" startWordPosition="3888" endWordPosition="3889">. Thus, we can simply continue to use the existing XDG solver for parsing and generation. The second benefit is that we can use the same group lexicon for both parsing and generation, the only difference being that for generation, we have to slightly adapt the compilation into lexical entries. There are many issues which have remained untouched in this paper. For one, we did not talk about our treatment of word order in this paper for lack of space. Word order is among the best researched issues in the context of XDG. For a thorough discussion about word order in XDG, we refer to (Duchier and Debusmann, 2001) and (Debusmann, 2001). Another issue is that of the relation between groups and the meta-grammatical functionality of the XDG lexicon, offering lexical inheritance, templates and also disjunction in the sense of crossings (Candito, 1996) to lexically state linguistic generalizations. How well groups can be integrated with this meta-grammatical functionality is an open question. XDG research has so far mainly been focused on parsing, only to a very small extent on generation, and to no extent at all on Machine Translation (MT). There is still a lot to do on both of the latter topics, and even </context>
</contexts>
<marker>Debusmann, 2001</marker>
<rawString>Ralph Debusmann. 2001. A declarative grammar formalism for dependency grammar. Master’s thesis, University of Saarland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ralph Debusmann</author>
</authors>
<title>A parser system for extensible dependency grammar.</title>
<date>2003</date>
<booktitle>Prospects and Advances in the Syntax/Semantics Interface,</booktitle>
<pages>103--106</pages>
<editor>In Denys Duchier, editor,</editor>
<publisher>LORIA, Nancy/FRA.</publisher>
<contexts>
<context position="10838" citStr="Debusmann, 2003" startWordPosition="1815" endWordPosition="1816">straints (Inp), which again specify a subset of Ana. XDG solving then amounts to finding elements of Ana that are licensed by Lex, and consistent with Inp and Pri. The input constraints e.g. determine whether XDG solving is to be used for parsing or generation. For parsing, they specify a sequence of words, and for generation, a multiset of semantic literals. 2.3 XDG solver XDG solving has a natural reading as a constraint satisfaction problem (CSP) on finite sets of integers, where well-formed analyses correspond to the solutions of the CSP (Duchier, 2003). We have implemented an XDG solver (Debusmann, 2003) using the Mozart-Oz programming system (Mozart Consortium, 2004). XDG solving operates on all dimensions concurrently. This means that the solver can infer information about one dimension from information on another, if there is either a multi-dimensional principle linking the two dimensions, or by the synchronization induced by the lexical entries. For instance, not only can syntactic information trigger inferences in syntax, but also vice versa. Because XDG allows us to write grammars with completely free word order, XDG solving is an NP-complete problem (Koller and Striegnitz, word literal</context>
</contexts>
<marker>Debusmann, 2003</marker>
<rawString>Ralph Debusmann. 2003. A parser system for extensible dependency grammar. In Denys Duchier, editor, Prospects and Advances in the Syntax/Semantics Interface, pages 103–106. LORIA, Nancy/FRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter Dienes</author>
<author>Alexander Koller</author>
<author>Marco Kuhlmann</author>
</authors>
<title>Statistical A* Dependency Parsing.</title>
<date>2003</date>
<booktitle>In Prospects and Advances in the Syntax/Semantics Interface,</booktitle>
<location>Nancy/FRA.</location>
<contexts>
<context position="23495" citStr="Dienes et al., 2003" startWordPosition="4073" endWordPosition="4076">ion, and to no extent at all on Machine Translation (MT). There is still a lot to do on both of the latter topics, and even for parsing, we are only at the beginning. Although with our smaller-scale example grammars, parsing and generation takes polynomial time, we have yet to find out how we can scale this up to large-scale grammars. We have started importing and inducing large-scale grammars from existing resources, but can so far only speculate about if and how we can parse them efficiently. In a related line of research, we are also working on the incorporation of statistical information (Dienes et al., 2003) to help us to guide the search for solutions. This could improve performance because we would only have to search for a few good analyses instead of enumerating hundreds of them. Acknowledgements The idea for groups and this paper was triggered by three events in fall 2003. The first was a workshop where Peter Dienes, Stefan Thater and me worked out how to do generation with XDG. The second was a presentation on the same workshop by Aravind Joshi and Owen Rambow of their encoding of DG in TAG, and the third was a talk by Charles Fillmore titled Multiword Expressions: An Extremist Approach. I’</context>
</contexts>
<marker>Dienes, Koller, Kuhlmann, 2003</marker>
<rawString>Peter Dienes, Alexander Koller, and Marco Kuhlmann. 2003. Statistical A* Dependency Parsing. In Prospects and Advances in the Syntax/Semantics Interface, Nancy/FRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denys Duchier</author>
<author>Ralph Debusmann</author>
</authors>
<title>Topological dependency trees: A constraint-based account of linear precedence.</title>
<date>2001</date>
<booktitle>In Proceedings of ACL 2001,</booktitle>
<location>Toulouse/FRA.</location>
<contexts>
<context position="2481" citStr="Duchier and Debusmann, 2001" startWordPosition="369" endWordPosition="373"> concept of nuclei to group together sets of nodes. FGD, on the other hand, allows for the deletion of solely syntactically motivated nodes in the tectogrammatical structure. Similarly, in MTT, nodes present on the syntactic structures can be deleted on the semantic structure. This can happen e.g. through paraphrasing rules implemented by lexical functions (Mel’ˇcuk, 1996). Unfortunately, these attempts to break the 1:1-correspondence have not yet been formalized in a declarative way. Extensible Dependency Grammar (XDG) is a new grammar formalism based on Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001). From TDG, it inherits declarative word order constraints, the ability to distinguish multiple dimensions of linguistic description, and an axiomatization as a constraint satisfaction problem (Duchier, 2003) solvable using constraint programming (Apt, 2003). One of the benefits of this axiomatization is that the linear order of the words can be left underspecified, with the effect that the constraint solver can be applied for both parsing and generation. XDG solving is efficient at least for the smallerscale example grammars tested so far, but these good results hinge substantially on the ass</context>
<context position="4924" citStr="Duchier and Debusmann, 2001" startWordPosition="773" endWordPosition="777">duce XDG in section 2. In section 3, we introduce groups, the new layer of lexical organization required for the modeling of MWEs, and in section 4, we show how to compile groups into simple lexical entries amenable for parsing and generation. Section 5 concludes the paper and outlines future work. 2 XDG In this section, we explain the intuitions behind XDG, before proceeding with its formalization, and a description of the XDG solver for parsing and generation. 2.1 XDG intuitions Extensible Dependency Grammar (XDG) is a new grammar formalism generalizing Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001). XDG characterizes linguistic structure along arbitrary many dimensions of description. All dimensions correspond to labeled graphs, sharing the same set of nodes but having different edges. The well-formedness conditions for XDG analyses are determined by principles. Principles can either be one-dimensional, applying to a single dimension only, or multi-dimensional, constraining the relation of several dimensions. Basic onedimensional principles are treeness and valency. Multi-dimensional principles include climbing (as in (Duchier and Debusmann, 2001); one dimension must be a flattening of </context>
<context position="6929" citStr="Duchier and Debusmann, 2001" startWordPosition="1111" endWordPosition="1114">s on the left, and the PA part on the right:1 (1) He dates her He dates her The set of edge labels on the ID dimension includes subj for subject and obj for object. On the PA dimension, we have arg1 and arg2 standing for the argument slots of semantic predicates.2 The ID part of the analysis states that He is the subject, and her the object of dates. The PA part states that He is the first argument and her the second argument of dates. 1For lack of space, we omit the dimension of linear precedence (LP) from the presentation in this paper. For this dimension, we use the same theory as for TDG (Duchier and Debusmann, 2001). 2We could also use some set of thematic roles for the PA edge labels, but since the assumption of thematic roles is very controversial, we decided to choose more neutral labels. The principles of the underlying grammar require that the ID part of each analysis is a tree, and the PA part a directed acyclic graph (dag).34 In addition, we employ the valency principle on both dimensions, specifying the licensed incoming and outgoing edges of each node. The only employed multi-dimensional principle is the linking principle, specifying how semantic arguments are realized syntactically. Figure 1 sh</context>
<context position="22399" citStr="Duchier and Debusmann, 2001" startWordPosition="3886" endWordPosition="3889">its entirety. Thus, we can simply continue to use the existing XDG solver for parsing and generation. The second benefit is that we can use the same group lexicon for both parsing and generation, the only difference being that for generation, we have to slightly adapt the compilation into lexical entries. There are many issues which have remained untouched in this paper. For one, we did not talk about our treatment of word order in this paper for lack of space. Word order is among the best researched issues in the context of XDG. For a thorough discussion about word order in XDG, we refer to (Duchier and Debusmann, 2001) and (Debusmann, 2001). Another issue is that of the relation between groups and the meta-grammatical functionality of the XDG lexicon, offering lexical inheritance, templates and also disjunction in the sense of crossings (Candito, 1996) to lexically state linguistic generalizations. How well groups can be integrated with this meta-grammatical functionality is an open question. XDG research has so far mainly been focused on parsing, only to a very small extent on generation, and to no extent at all on Machine Translation (MT). There is still a lot to do on both of the latter topics, and even </context>
</contexts>
<marker>Duchier, Debusmann, 2001</marker>
<rawString>Denys Duchier and Ralph Debusmann. 2001. Topological dependency trees: A constraint-based account of linear precedence. In Proceedings of ACL 2001, Toulouse/FRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Denys Duchier</author>
</authors>
<title>Configuration of labeled trees under lexicalized constraints and principles.</title>
<date>2003</date>
<journal>Research on Language and Computation,</journal>
<volume>1</volume>
<issue>3</issue>
<pages>336</pages>
<contexts>
<context position="2689" citStr="Duchier, 2003" startWordPosition="403" endWordPosition="404">ic structures can be deleted on the semantic structure. This can happen e.g. through paraphrasing rules implemented by lexical functions (Mel’ˇcuk, 1996). Unfortunately, these attempts to break the 1:1-correspondence have not yet been formalized in a declarative way. Extensible Dependency Grammar (XDG) is a new grammar formalism based on Topological Dependency Grammar (TDG) (Duchier and Debusmann, 2001). From TDG, it inherits declarative word order constraints, the ability to distinguish multiple dimensions of linguistic description, and an axiomatization as a constraint satisfaction problem (Duchier, 2003) solvable using constraint programming (Apt, 2003). One of the benefits of this axiomatization is that the linear order of the words can be left underspecified, with the effect that the constraint solver can be applied for both parsing and generation. XDG solving is efficient at least for the smallerscale example grammars tested so far, but these good results hinge substantially on the assumption of a 1:1-correspondence between words and nodes. As XDG has been created to cover not only syntax but also semantics, we have no choice but to weaken the 1:1-correspondence. In this paper, we outline </context>
<context position="10785" citStr="Duchier, 2003" startWordPosition="1807" endWordPosition="1808">ses for a given input, we employ a set of input constraints (Inp), which again specify a subset of Ana. XDG solving then amounts to finding elements of Ana that are licensed by Lex, and consistent with Inp and Pri. The input constraints e.g. determine whether XDG solving is to be used for parsing or generation. For parsing, they specify a sequence of words, and for generation, a multiset of semantic literals. 2.3 XDG solver XDG solving has a natural reading as a constraint satisfaction problem (CSP) on finite sets of integers, where well-formed analyses correspond to the solutions of the CSP (Duchier, 2003). We have implemented an XDG solver (Debusmann, 2003) using the Mozart-Oz programming system (Mozart Consortium, 2004). XDG solving operates on all dimensions concurrently. This means that the solver can infer information about one dimension from information on another, if there is either a multi-dimensional principle linking the two dimensions, or by the synchronization induced by the lexical entries. For instance, not only can syntactic information trigger inferences in syntax, but also vice versa. Because XDG allows us to write grammars with completely free word order, XDG solving is an NP-</context>
</contexts>
<marker>Duchier, 2003</marker>
<rawString>Denys Duchier. 2003. Configuration of labeled trees under lexicalized constraints and principles. Research on Language and Computation, 1(3–4):307– 336.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kim Gerdes</author>
<author>Sylvain Kahane</author>
</authors>
<title>Word order in german: A formal dependency grammar using a topological hierarchy.</title>
<date>2001</date>
<booktitle>In Proceedings ofACL 2001,</booktitle>
<location>Toulouse/FRA.</location>
<contexts>
<context position="1126" citStr="Gerdes and Kahane, 2001" startWordPosition="165" endWordPosition="168">into simple lexical entries, amenable to parsing and generation with the existing XDG constraint solver. 1 Introduction In recent years, dependency grammar (DG) (Tesni`ere, 1959; Sgall et al., 1986; Mel’ˇcuk, 1988) has received resurgent interest. Core concepts such as grammatical functions, valency and the head-dependent asymmetry have now found their way into most grammar formalisms, including phrase structure-based ones such as HPSG, LFG and TAG. This renewed interest in DG has also given rise to new grammar formalisms based directly on DG (Nasr, 1995; Heinecke et al., 1998; Br¨oker, 1999; Gerdes and Kahane, 2001; Kruijff, 2001; Joshi and Rambow, 2003). A controversy among DG grammarians circles around the question of assuming a 1:1- correspondence between words and nodes in the dependency graph. This assumption simplifies the formalization of DGs substantially, and is often required for parsing. But as soon as semantics comes in, the picture changes. Clearly, the 1:1-correspondence between words and nodes does not hold anymore for multiword expressions (MWEs), where one semantic unit, represented by a node in a semantically oriented dependency graph, corresponds not to one, but to more than one word.</context>
</contexts>
<marker>Gerdes, Kahane, 2001</marker>
<rawString>Kim Gerdes and Sylvain Kahane. 2001. Word order in german: A formal dependency grammar using a topological hierarchy. In Proceedings ofACL 2001, Toulouse/FRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Johannes Heinecke</author>
<author>J¨urgen Kunze</author>
<author>Wolfgang Menzel</author>
<author>Ingo Schr¨oder</author>
</authors>
<title>Eliminative parsing with graded constraints.</title>
<date>1998</date>
<booktitle>In Proceedings of COLING/ACL</booktitle>
<pages>526--530</pages>
<marker>Heinecke, Kunze, Menzel, Schr¨oder, 1998</marker>
<rawString>Johannes Heinecke, J¨urgen Kunze, Wolfgang Menzel, and Ingo Schr¨oder. 1998. Eliminative parsing with graded constraints. In Proceedings of COLING/ACL 1998, pages 526–530, Montreal/CAN.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind Joshi</author>
<author>Owen Rambow</author>
</authors>
<title>A formalism for dependency grammar based on tree adjoining grammar.</title>
<date>2003</date>
<booktitle>In Proceedings of MTT 2003,</booktitle>
<pages>207--216</pages>
<contexts>
<context position="1166" citStr="Joshi and Rambow, 2003" startWordPosition="171" endWordPosition="174"> parsing and generation with the existing XDG constraint solver. 1 Introduction In recent years, dependency grammar (DG) (Tesni`ere, 1959; Sgall et al., 1986; Mel’ˇcuk, 1988) has received resurgent interest. Core concepts such as grammatical functions, valency and the head-dependent asymmetry have now found their way into most grammar formalisms, including phrase structure-based ones such as HPSG, LFG and TAG. This renewed interest in DG has also given rise to new grammar formalisms based directly on DG (Nasr, 1995; Heinecke et al., 1998; Br¨oker, 1999; Gerdes and Kahane, 2001; Kruijff, 2001; Joshi and Rambow, 2003). A controversy among DG grammarians circles around the question of assuming a 1:1- correspondence between words and nodes in the dependency graph. This assumption simplifies the formalization of DGs substantially, and is often required for parsing. But as soon as semantics comes in, the picture changes. Clearly, the 1:1-correspondence between words and nodes does not hold anymore for multiword expressions (MWEs), where one semantic unit, represented by a node in a semantically oriented dependency graph, corresponds not to one, but to more than one word. Most DGs interested in semantics have t</context>
<context position="4243" citStr="Joshi and Rambow, 2003" startWordPosition="659" endWordPosition="662"> can then be used by the already existing XDG solver for parsing and generation. With groups, we can omit nodes present in the syntactic dimensions in the semantic dimensions, and thus get away from the 1:1-correspondence. Groups are motivated by the continuity hypothesis of (Kay and Fillmore, 1999), assuming the continuity of the lexicon and the construction. They can also be regarded as a declarative formalization of Mel’ˇcuk’s paraphrasing rules, or as a realization of the extended domain of locality of TAG (Joshi, 1987) in terms of dependency grammar, as also proposed in (Nasr, 1996) and (Joshi and Rambow, 2003). The structure of this paper is as follows. We introduce XDG in section 2. In section 3, we introduce groups, the new layer of lexical organization required for the modeling of MWEs, and in section 4, we show how to compile groups into simple lexical entries amenable for parsing and generation. Section 5 concludes the paper and outlines future work. 2 XDG In this section, we explain the intuitions behind XDG, before proceeding with its formalization, and a description of the XDG solver for parsing and generation. 2.1 XDG intuitions Extensible Dependency Grammar (XDG) is a new grammar formalis</context>
</contexts>
<marker>Joshi, Rambow, 2003</marker>
<rawString>Aravind Joshi and Owen Rambow. 2003. A formalism for dependency grammar based on tree adjoining grammar. In Proceedings of MTT 2003, pages 207–216, Paris/FRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Aravind K Joshi</author>
</authors>
<title>An introduction to treeadjoining grammars.</title>
<date>1987</date>
<booktitle>Mathematics ofLanguage,</booktitle>
<pages>87--115</pages>
<editor>In Alexis Manaster-Ramer, editor,</editor>
<publisher>John Benjamins, Amsterdam/NL.</publisher>
<contexts>
<context position="4149" citStr="Joshi, 1987" startWordPosition="645" endWordPosition="646"> of dependency subgraphs. Groups can be compiled into simple lexical entries, which can then be used by the already existing XDG solver for parsing and generation. With groups, we can omit nodes present in the syntactic dimensions in the semantic dimensions, and thus get away from the 1:1-correspondence. Groups are motivated by the continuity hypothesis of (Kay and Fillmore, 1999), assuming the continuity of the lexicon and the construction. They can also be regarded as a declarative formalization of Mel’ˇcuk’s paraphrasing rules, or as a realization of the extended domain of locality of TAG (Joshi, 1987) in terms of dependency grammar, as also proposed in (Nasr, 1996) and (Joshi and Rambow, 2003). The structure of this paper is as follows. We introduce XDG in section 2. In section 3, we introduce groups, the new layer of lexical organization required for the modeling of MWEs, and in section 4, we show how to compile groups into simple lexical entries amenable for parsing and generation. Section 5 concludes the paper and outlines future work. 2 XDG In this section, we explain the intuitions behind XDG, before proceeding with its formalization, and a description of the XDG solver for parsing an</context>
</contexts>
<marker>Joshi, 1987</marker>
<rawString>Aravind K. Joshi. 1987. An introduction to treeadjoining grammars. In Alexis Manaster-Ramer, editor, Mathematics ofLanguage, pages 87–115. John Benjamins, Amsterdam/NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Kay</author>
<author>Charles J Fillmore</author>
</authors>
<title>Grammatical constructions and linguistic generalizations: the what’s x doing y? construction.</title>
<date>1999</date>
<journal>Language,</journal>
<pages>75--1</pages>
<contexts>
<context position="3920" citStr="Kay and Fillmore, 1999" startWordPosition="606" endWordPosition="609"> break out of the 1:1-straightjacket, without sacrificing the potential for efficient parsing and generation. We introduce a new layer of lexical organization called groups above the basic XDG lexicon, allowing us to describe MWEs as tuples of dependency subgraphs. Groups can be compiled into simple lexical entries, which can then be used by the already existing XDG solver for parsing and generation. With groups, we can omit nodes present in the syntactic dimensions in the semantic dimensions, and thus get away from the 1:1-correspondence. Groups are motivated by the continuity hypothesis of (Kay and Fillmore, 1999), assuming the continuity of the lexicon and the construction. They can also be regarded as a declarative formalization of Mel’ˇcuk’s paraphrasing rules, or as a realization of the extended domain of locality of TAG (Joshi, 1987) in terms of dependency grammar, as also proposed in (Nasr, 1996) and (Joshi and Rambow, 2003). The structure of this paper is as follows. We introduce XDG in section 2. In section 3, we introduce groups, the new layer of lexical organization required for the modeling of MWEs, and in section 4, we show how to compile groups into simple lexical entries amenable for pars</context>
<context position="12550" citStr="Kay and Fillmore, 1999" startWordPosition="2102" endWordPosition="2105"> grammars. 3 Groups In this section, we consider MWE paraphrases and propose to model them as tuples of dependency subgraphs called groups. We start with an example: Consider (3), a paraphrase of (2): He dates her. (2) He takes her out. (3) We display the XDG analysis of (3) in (4). Again, the ID tree is on the left, and the PA dag on the right:5 (4) This example demonstrates that we cannot simply treat MWEs as contiguous word strings and include those in the lexicon, since the MWE takes out is interrupted by the object her in (3). Instead, we choose to implement the continuity hypothesis of (Kay and Fillmore, 1999) in terms of DG, and model MWEs as dependency subgraphs. We realize this idea by a new layer of lexical organization called groups. A group is tuple of dependency subgraphs covering one or more nodes, where each component of the tuple corresponds to a grammar dimension. We call a group component group dimension. We display the group corresponding to dates in (5), and the one correspond5In the ID tree, part stands for a particle. ing to takes out in (6). dates dates Groups can leave out nodes present in the ID dimension on the PA dimension. E.g. in (6), the node corresponding to the particle ou</context>
</contexts>
<marker>Kay, Fillmore, 1999</marker>
<rawString>Paul Kay and Charles J. Fillmore. 1999. Grammatical constructions and linguistic generalizations: the what’s x doing y? construction. Language, 75:1–33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexander Koller</author>
<author>Kristina Striegnitz</author>
</authors>
<title>Generation as dependency parsing.</title>
<date>2002</date>
<booktitle>In Proceedings of ACL 2002,</booktitle>
<location>Philadelphia/USA.</location>
<marker>Koller, Striegnitz, 2002</marker>
<rawString>Alexander Koller and Kristina Striegnitz. 2002. Generation as dependency parsing. In Proceedings of ACL 2002, Philadelphia/USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Geert-Jan M Kruijff</author>
</authors>
<title>A Categorial-Modal Architecture of Informativity.</title>
<date>2001</date>
<tech>Ph.D. thesis,</tech>
<institution>Charles University, Prague/CZ.</institution>
<contexts>
<context position="1141" citStr="Kruijff, 2001" startWordPosition="169" endWordPosition="170">es, amenable to parsing and generation with the existing XDG constraint solver. 1 Introduction In recent years, dependency grammar (DG) (Tesni`ere, 1959; Sgall et al., 1986; Mel’ˇcuk, 1988) has received resurgent interest. Core concepts such as grammatical functions, valency and the head-dependent asymmetry have now found their way into most grammar formalisms, including phrase structure-based ones such as HPSG, LFG and TAG. This renewed interest in DG has also given rise to new grammar formalisms based directly on DG (Nasr, 1995; Heinecke et al., 1998; Br¨oker, 1999; Gerdes and Kahane, 2001; Kruijff, 2001; Joshi and Rambow, 2003). A controversy among DG grammarians circles around the question of assuming a 1:1- correspondence between words and nodes in the dependency graph. This assumption simplifies the formalization of DGs substantially, and is often required for parsing. But as soon as semantics comes in, the picture changes. Clearly, the 1:1-correspondence between words and nodes does not hold anymore for multiword expressions (MWEs), where one semantic unit, represented by a node in a semantically oriented dependency graph, corresponds not to one, but to more than one word. Most DGs inter</context>
</contexts>
<marker>Kruijff, 2001</marker>
<rawString>Geert-Jan M. Kruijff. 2001. A Categorial-Modal Architecture of Informativity. Ph.D. thesis, Charles University, Prague/CZ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Dependency Syntax: Theory and Practice. State Univ.</title>
<date>1988</date>
<publisher>Press of</publisher>
<location>New York, Albany/USA.</location>
<marker>Mel’ˇcuk, 1988</marker>
<rawString>Igor Mel’ˇcuk. 1988. Dependency Syntax: Theory and Practice. State Univ. Press of New York, Albany/USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Igor Mel’ˇcuk</author>
</authors>
<title>Lexical functions: a tool for the description of lexical relations in a lexicon.</title>
<date>1996</date>
<booktitle>Lexical Functions in Lexicography and Natural Language Processing.</booktitle>
<editor>In Leo Wanner, editor,</editor>
<publisher>John Benjamins.</publisher>
<marker>Mel’ˇcuk, 1996</marker>
<rawString>Igor Mel’ˇcuk. 1996. Lexical functions: a tool for the description of lexical relations in a lexicon. In Leo Wanner, editor, Lexical Functions in Lexicography and Natural Language Processing. John Benjamins.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mozart Consortium</author>
</authors>
<date>2004</date>
<note>The Mozart-Oz website. http://www.mozart-oz.org/.</note>
<contexts>
<context position="10903" citStr="Consortium, 2004" startWordPosition="1823" endWordPosition="1824"> then amounts to finding elements of Ana that are licensed by Lex, and consistent with Inp and Pri. The input constraints e.g. determine whether XDG solving is to be used for parsing or generation. For parsing, they specify a sequence of words, and for generation, a multiset of semantic literals. 2.3 XDG solver XDG solving has a natural reading as a constraint satisfaction problem (CSP) on finite sets of integers, where well-formed analyses correspond to the solutions of the CSP (Duchier, 2003). We have implemented an XDG solver (Debusmann, 2003) using the Mozart-Oz programming system (Mozart Consortium, 2004). XDG solving operates on all dimensions concurrently. This means that the solver can infer information about one dimension from information on another, if there is either a multi-dimensional principle linking the two dimensions, or by the synchronization induced by the lexical entries. For instance, not only can syntactic information trigger inferences in syntax, but also vice versa. Because XDG allows us to write grammars with completely free word order, XDG solving is an NP-complete problem (Koller and Striegnitz, word literal inID outID inPA outPA link He he’ {subj?} {} {arg1?,arg2?} {} {}</context>
</contexts>
<marker>Consortium, 2004</marker>
<rawString>Mozart Consortium. 2004. The Mozart-Oz website. http://www.mozart-oz.org/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
</authors>
<title>A formalism and a parser for lexicalised dependency grammars.</title>
<date>1995</date>
<booktitle>In 4th International Workshop on Parsing Technologies,</booktitle>
<pages>186--195</pages>
<contexts>
<context position="1063" citStr="Nasr, 1995" startWordPosition="157" endWordPosition="158">ependency subgraphs, and show how to compile them into simple lexical entries, amenable to parsing and generation with the existing XDG constraint solver. 1 Introduction In recent years, dependency grammar (DG) (Tesni`ere, 1959; Sgall et al., 1986; Mel’ˇcuk, 1988) has received resurgent interest. Core concepts such as grammatical functions, valency and the head-dependent asymmetry have now found their way into most grammar formalisms, including phrase structure-based ones such as HPSG, LFG and TAG. This renewed interest in DG has also given rise to new grammar formalisms based directly on DG (Nasr, 1995; Heinecke et al., 1998; Br¨oker, 1999; Gerdes and Kahane, 2001; Kruijff, 2001; Joshi and Rambow, 2003). A controversy among DG grammarians circles around the question of assuming a 1:1- correspondence between words and nodes in the dependency graph. This assumption simplifies the formalization of DGs substantially, and is often required for parsing. But as soon as semantics comes in, the picture changes. Clearly, the 1:1-correspondence between words and nodes does not hold anymore for multiword expressions (MWEs), where one semantic unit, represented by a node in a semantically oriented depen</context>
</contexts>
<marker>Nasr, 1995</marker>
<rawString>Alexis Nasr. 1995. A formalism and a parser for lexicalised dependency grammars. In 4th International Workshop on Parsing Technologies, pages 186–195, Prague/CZ.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Alexis Nasr</author>
</authors>
<title>Un syst`eme de reformulation automatique de phrases fond´e sur la Th´eorie SensTexte: application aux langues contrˆol´ees.</title>
<date>1996</date>
<tech>Ph.D. thesis,</tech>
<institution>Universit´e Paris</institution>
<contexts>
<context position="4214" citStr="Nasr, 1996" startWordPosition="656" endWordPosition="657">al entries, which can then be used by the already existing XDG solver for parsing and generation. With groups, we can omit nodes present in the syntactic dimensions in the semantic dimensions, and thus get away from the 1:1-correspondence. Groups are motivated by the continuity hypothesis of (Kay and Fillmore, 1999), assuming the continuity of the lexicon and the construction. They can also be regarded as a declarative formalization of Mel’ˇcuk’s paraphrasing rules, or as a realization of the extended domain of locality of TAG (Joshi, 1987) in terms of dependency grammar, as also proposed in (Nasr, 1996) and (Joshi and Rambow, 2003). The structure of this paper is as follows. We introduce XDG in section 2. In section 3, we introduce groups, the new layer of lexical organization required for the modeling of MWEs, and in section 4, we show how to compile groups into simple lexical entries amenable for parsing and generation. Section 5 concludes the paper and outlines future work. 2 XDG In this section, we explain the intuitions behind XDG, before proceeding with its formalization, and a description of the XDG solver for parsing and generation. 2.1 XDG intuitions Extensible Dependency Grammar (X</context>
</contexts>
<marker>Nasr, 1996</marker>
<rawString>Alexis Nasr. 1996. Un syst`eme de reformulation automatique de phrases fond´e sur la Th´eorie SensTexte: application aux langues contrˆol´ees. Ph.D. thesis, Universit´e Paris 7.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Petr Sgall</author>
<author>Eva Hajicova</author>
<author>Jarmila Panevova</author>
</authors>
<title>The Meaning of the Sentence in its Semantic and Pragmatic Aspects.</title>
<date>1986</date>
<location>D. Reidel, Dordrecht/NL.</location>
<contexts>
<context position="700" citStr="Sgall et al., 1986" startWordPosition="99" endWordPosition="102">p. 56-63 Multiword expressions as dependency subgraphs Ralph Debusmann Programming Systems Lab Saarland University Postfach 15 11 50 66041 Saarbr¨ucken, Germany rade@ps.uni-sb.de Abstract We propose to model multiword expressions as dependency subgraphs, and realize this idea in the grammar formalism of Extensible Dependency Grammar (XDG). We extend XDG to lexicalize dependency subgraphs, and show how to compile them into simple lexical entries, amenable to parsing and generation with the existing XDG constraint solver. 1 Introduction In recent years, dependency grammar (DG) (Tesni`ere, 1959; Sgall et al., 1986; Mel’ˇcuk, 1988) has received resurgent interest. Core concepts such as grammatical functions, valency and the head-dependent asymmetry have now found their way into most grammar formalisms, including phrase structure-based ones such as HPSG, LFG and TAG. This renewed interest in DG has also given rise to new grammar formalisms based directly on DG (Nasr, 1995; Heinecke et al., 1998; Br¨oker, 1999; Gerdes and Kahane, 2001; Kruijff, 2001; Joshi and Rambow, 2003). A controversy among DG grammarians circles around the question of assuming a 1:1- correspondence between words and nodes in the depe</context>
</contexts>
<marker>Sgall, Hajicova, Panevova, 1986</marker>
<rawString>Petr Sgall, Eva Hajicova, and Jarmila Panevova. 1986. The Meaning of the Sentence in its Semantic and Pragmatic Aspects. D. Reidel, Dordrecht/NL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lucien Tesni`ere</author>
</authors>
<title>El´ements de Syntaxe Structurale.</title>
<date>1959</date>
<location>Klincksiek, Paris/FRA.</location>
<marker>Tesni`ere, 1959</marker>
<rawString>Lucien Tesni`ere. 1959. El´ements de Syntaxe Structurale. Klincksiek, Paris/FRA.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>