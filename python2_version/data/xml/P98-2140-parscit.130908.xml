<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000120">
<title confidence="0.999511">
Feature Lattices for Maximum Entropy Modelling
</title>
<author confidence="0.99727">
Andrei Mikheev*
</author>
<affiliation confidence="0.9327565">
HCRC, Language Technology Group, University of Edinburgh,
2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, UK.
</affiliation>
<email confidence="0.910017">
e-mail: Andrei.MikheevOed.ac.uk
</email>
<sectionHeader confidence="0.996388" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999635785714286">
Maximum entropy framework proved to be ex-
pressive and powerful for the statistical lan-
guage modelling, but it suffers from the com-
putational expensiveness of the model build-
ing. The iterative scaling algorithm that is used
for the parameter estimation is computation-
ally expensive while the feature selection pro-
cess might require to estimate parameters for
many candidate features many times. In this
paper we present a novel approach for building
maximum entropy models. Our approach uses
the feature collocation lattice and builds com-
plex candidate features without resorting to it-
erative scaling.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999916571428571">
Maximum entropy modelling has been recently
introduced to the NLP community and proved
to be an expressive and powerful framework.
The maximum entropy model is a model which
fits a set of pre-defined constraints and assumes
maximum ignorance about everything which is
not subject to its constraints thus assigning such
cases with the most uniform distribution. The
most uniform distribution will have the entropy
on its maximum
Because of its ability to handle overlapping
features the maximum entropy framework pro-
vides a principle way to incorporate informa-
tion from multiple knowledge sources. It is
superior to traditionally used for this purpose
linear interpolation and Katz back-off method.
(Rosenfeld, 1996) evaluates in detail a maxi-
mum entropy language model which combines
unigrams, bigrams, trigrams and long-distance
trigger words, and provides a thorough analysis
of all the merits of the approach.
</bodyText>
<listItem confidence="0.971881">
• Now at Harlequin Ltd.
</listItem>
<bodyText confidence="0.990165925">
The iterative scaling algorithm
(Darroch&amp;Ratcliff, 1972) applied for the pa-
rameter estimation of maximum entropy mod-
els computes a set of feature weights (As) which
ensure that the model fits the reference distri-
bution and does not make spurious assumptions
(as required by the maximum entropy principle)
about events beyond the reference distribution.
It, however, does not guarantee that the fea-
tures employed by the model are good features
and the model is useful. Thus the most im-
portant past of the model building is the fea-
ture selection procedure. The key idea of the
feature selection is that if we notice an interac-
tion between certain features we should build a
more complex feature which will account for this
interaction. The newly added feature should
improve the model: its Kullback-Leibler diver-
gence from the reference distribution should de-
crease and the conditional maximum entropy
model will also have the greatest log-likelihood
(L) value:
The basic feature induction algorithm pre-
sented in (Della Pietra et al., 1995) starts with
an empty feature space and iteratively tries all
possible feature candidates. These candidates
are either atomic features or complex features
produced as a combination of an atomic feature
with the features already selected to the model&apos;s
feature space. For every feature from the can-
didate feature set the algorithm prescribes to
compute the maximum entropy model using the
iterative scaling algorithm described above, and
select the feature which in the largest way min-
imizes the Kullback-Leibler divergence or max-
imizes the log-likelihood of the model. This ap-
proach, however, is not computationally feasi-
ble since the iterative scaling is computation-
ally expensive and to compute models for many
candidate features many times is unreal. To
</bodyText>
<page confidence="0.995402">
848
</page>
<bodyText confidence="0.99994045">
make feature ranking computationally tractable
in (Della Pietra et al., 1995) and (Berger et al.,
1996) a simplified process proposed: at the fea-
ture ranking stage when adding a new feature
to the model, all previously computed parame-
ters are kept fixed and, thus, we have to fit only
one new constraint imposed by the candidate
feature. Then after the best ranked feature has
been established, it is added to the feature space
and the weights for all the features are recom-
puted. This approach estimates good features
relatively fast but it does not guarantee that at
every single point we add the best feature be-
cause when we add a new feature to the model
all its parameters can change.
In this paper we present a novel approach to
feature selection for the maximum entropy mod-
els. Our approach uses a feature collocation lat-
tice and selects candidate features without re-
sorting to the iterative scaling.
</bodyText>
<sectionHeader confidence="0.985622" genericHeader="method">
2 Feature Collocation Lattice
</sectionHeader>
<bodyText confidence="0.999172444444444">
We start the modelling process by building a
sample space w to train our model on. The sam-
ple space consists of observed events of interest
mapped to a set of atomic features T which we
should define beforehand. Thus every observa-
tion from the sample space is a binary vector
of atomic features: if an observation includes
a certain feature, its corresponding bit in the
vector is turned on (set to 1) otherwise it is 0.
When we have a set of atomic features T and
a training sample of configurations w, we can
build the feature collocation lattice. Such collo-
cation lattice will represent, in fact, the factorial
constraint space (x) for the maximum entropy
model and at the same time will contain all seen
and logically implied configurations (w+). For-
mally, the feature collocation lattice is a 3-ple:
(0,c, v.) where
</bodyText>
<listItem confidence="0.956749642857143">
• 9 is a set of nodes of the lattice which corre-
sponds to the union of the feature space of
the maximum entropy model and the con-
figuration space: 9 = XU (w). In fact, the
nodes in the lattice (0) can have dual in-
terpretation - on one hand they can act as
mapped configurations from the extended
configuration space (w+) and on the other
hand they can act as features from the con-
straint space (x);
• C is a transitive, antisymmetric relation
over 0 x 9 - a partial ordering. We also will
need the indicator function to flag whether
the relation C holds from node i to node k:
</listItem>
<equation confidence="0.974825">
1 if Oi C Ok
f01(Ok) = 0 otherwise
</equation>
<listItem confidence="0.9991964">
• VI is a set of configuration frequency counts
of the nodes (0) of the lattice. This repre-
sents how many times we saw a particu-
lar configuration in our training samples.
Because of the dual interpretation of the
nodes, a node can also be associated with
its feature frequency count i.e. the num-
ber of times we see this feature combina-
tion anywhere in the lattice. The feature
frequency of a node will then be x(0k) =
</listItem>
<bodyText confidence="0.7339642">
Ee,Ee fok (Os) * which is the sum of all
the configuration frequency counts (el of
the descendant nodes.
Suppose we have a lattice of nodes
A, B, [AB] with obvious relations: A C
</bodyText>
<figure confidence="0.387827666666667">
[AB]; B C [AB]:
7, [AB] ,c
A B
</figure>
<bodyText confidence="0.991118958333334">
The configuration frequency a will be the
number of times we saw A but not [AB]
and then the feature frequency of A will
be: e)i = ety, + &amp;4B i.e. the number of times
we saw A in all the nodes.
When we construct the feature collocation
lattice from a set of samples, each sample repre-
sents a feature configuration which we must add
to the lattice as its node (Ok). To support gener-
alizations over the domain we also want to add
to the lattice the nodes which are shared parts
with other nodes in the lattice. Thus we add
to the lattice all sub-configurations of a newly
added configuration which are the intersections
with the other nodes. We increment the con-
figuration frequency (ak ) of a node each time
we see in the training samples this particular
configuration in full. For example, if a config-
uration [ABC D] comes from a training sam-
ple and it is still not in the lattice, we create
a node [AR CD] and set its configuration fre-
quency ew,
[ABCD]t0 1. If by that time there is a
node [ABDE] in the lattice, we then also create
</bodyText>
<page confidence="0.991428">
849
</page>
<bodyText confidence="0.999936">
the node [ABDJ, relate it to the nodes [ABCD]
and [ABDE] and set its configuration frequency
to 0. If [ABCD] had already existed in the lat-
tice, we would simply incremented its configu-
</bodyText>
<equation confidence="0.59001">
ration frequency: f
[ABCD] + 1.
</equation>
<bodyText confidence="0.999980105263158">
Thus in the feature lattice we have nodes with
non-zero configuration frequencies, which we
call reference nodes and nodes with zero config-
uration frequencies which we call latent or hid-
den nodes. Reference nodes actually represent
the observed configuration space (w). Hidden
nodes are never observed on their own but only
as parts of the reference nodes and represent
possible generalizations about domain: low-
complexity constraints (x) and logically possi-
ble configurations (w+).
This method of building the feature colloca-
tion lattice ensures that along with true obser-
vations it contains hidden nodes which can pro-
vide generalizations about the domain. At the
same time there is no over-generation of the hid-
den nodes: no logically impossible feature com-
binations and no hidden nodes without general-
ization power are included.
</bodyText>
<sectionHeader confidence="0.987696" genericHeader="method">
3 Feature Selection
</sectionHeader>
<bodyText confidence="0.9999965">
After we constructed from a set of samples the
feature collocation lattice (0,C, ti)), which we
will call the empirical lattice, we try to esti-
mate which features contribute and which do
not to the frequency distribution on the refer-
ence nodes. Thus only the predictive features
will be retained in the lattice. The optimized
feature space can be seen as a feature lattice de-
fined over the empirical feature lattice: 0&apos; C
and initially it is empty: 0&apos; =,0. We build the
optimized lattice by incrementally adding a fea-
ture (atomic or complex) from the empirical lat-
tice, together with the nodes which are the min-
imal collocations of this feature with the nodes
already included into the optimized lattice. The
necessity to add the collocations comes from the
fact that the features (or nodes) can overlap
with each other and we want to have a unique
node for such overlap. So if in the optimized
feature lattice there is just one feature A, then
when we add the feature B we also have to add
the collocation [AB] if it exists in the empirical
lattice. The configuration frequency of a node
in the optimized lattice (e1w) then can be corn-
</bodyText>
<equation confidence="0.933141">
puted as:
tio:=E[o,€e &amp; 9,06, &amp; •&amp; ekce, &amp; e,cei) fek(611)*eit:
(1)
</equation>
<bodyText confidence="0.994032181818182">
Thus a node in the optimized lattice takes all
configuration frequencies (V&apos;) of itself and the
above related nodes if these nodes do not belong
to the optimized lattice themselves and there is
no higher node in the optimized lattice related
to them.
Figure 1 shows how the configuration frequen-
cies in the optimized lattice are redistributed
when adding a new feature. First the lat-
tice is empty. When we add the feature A
to the optimized lattice (Figure 1.a), because
no other features are present in the optimized
lattice, it takes all the configuration frequen-
cies of the nodes where we see the feature A:
eir = -I- cf,B + + Ulric. Case 13) of Fig-
ure 1 represents the situation when we add the
feature B to the optimized lattice which already
includes the feature A. Apart from the node B
we also add the collocation of the nodes A and
B to the optimized lattice. Now we have to re-
distribute the configuration frequencies in the
optimized lattice. The configuration frequency
of the node A now will become the number of
times of seeing the featureA but not the fea-
ture combination AB: a ± Cfic. The
configuration frequency of the node B will be
the number of times of seeing the node B but
not the node AB: el = + ey3&apos;c. The con-
figuration frequency of the node AB will be:
eAwg = aB+6,,,4,Bc. When we add the feature C
to the optimized lattice (Figure 1.c) we produce
a fully saturated lattice identical to the empiri-
cal lattice, since the node C will collocate with
the node A producing AC and will collocate
with the node B producing BC. These nodes
in their turn will collocate with each other and
with the node AB producing the node ABC.
During the optimized lattice construction all
the features (atomic and complex) from the em-
pirical lattice compete, and we include the one
which results in a optimized lattice with the
smallest divergence D(p II p&apos;) and equation ??)
and therefore with the greatest log-likelihood
L(p1) , where:
</bodyText>
<listItem confidence="0.843185">
• p(0i) is the probability for the i-th node in
</listItem>
<page confidence="0.926632">
850
</page>
<table confidence="0.648739">
a b) c)
eit = ± Civ.B ABC CW A &amp;quot;A=CW eCT =
ei = ± CW
CIW =+ == c
Cw e11,3 = eifc =
B BBC — _CWsc
ViB = + a BC
</table>
<figureCaption confidence="0.804841333333333">
Figure 1: This figure shows the redistribution of the configuration frequencies in the optimized feature lattice when
adding new nodes. Case a) stands for adding the feature A to the empty lattice, case b) stands for adding the feature
B to the lattice with the feature A and case c) stands for adding the feature C to the lattice with the atomic features
A and B and their collocations. The unfilled nodes stand for the nodes in the empirical lattice which don&apos;t have
reference in the optimized lattice. The nodes in bold stand for the nodes decided by the optimized lattice (i.e. they
can be assigned with non-default probabilities).
</figureCaption>
<bodyText confidence="0.879226">
the empirical lattice:
= where N =E (2)
</bodyText>
<subsubsectionHeader confidence="0.513967">
OjEO
</subsubsectionHeader>
<listItem confidence="0.990233666666667">
• p&apos;(02) is the probability assigned to the i-th
node using only the nodes included into the
optimized lattice.
</listItem>
<equation confidence="0.938456555555556">
e87
7.1,- if 0i E 0&apos;
ee&apos;t&apos;
pt(9) . 7,- if ei V O&apos; &amp;
{ 2
[29j: 0j E 0&apos; Sz 03 C 0i] Sz
Ok E 0&apos; Sz Ok C Oi Sz Oi C
1/ I Y I otherwise
(3)
</equation>
<bodyText confidence="0.9997955">
The optimized lattice assigns the probabil-
ity to a node in the empirical lattice equal
to that of its most specific sub-node from
the optimized lattice. For reference nodes
which do not have sub-nodes in the opti-
mized lattice at all (undecided nodes) ac-
cording to the maximum entropy principle
we assign the uniform probability of mak-
ing an arbitrary prediction.
For instance, for the example on Figure 1.b
the optimized lattice includes only three nodes
but there is just one undecided node (C) which
is not shown in bold. So the probabilities for
the nodes will be:
</bodyText>
<equation confidence="0.997222">
11(A) = (B) CluN pi (AB) = 93-
p&apos; (AC) = p&apos; (A) pi(BC) = il(B)
p&apos; (ABC) = (AB) 1)(C) = 5-11
</equation>
<bodyText confidence="0.9786175">
N is the total count on the empirical lattice and
is calculated as shown in equation 2:
</bodyText>
<equation confidence="0.947719">
N=a+G+q+ eldk) + ± 1‘14)13C&amp;quot;
</equation>
<bodyText confidence="0.9999788">
Oki The presented above method provides us with
an efficient way of selecting only important fea-
tures from the initial set of candidate features
without resorting to iterative scaling. When
this way we add the features to the optimized
lattice some candidate features might not suf-
ficiently contribute to the probability distribu-
tion on the lattice. For instance, in the example
presented on Figure 1, after we added the fea-
ture [B] (case b) the only remaining undecided
node was [C]. If the node [C] is truly hidden (i.e.
it does not have its own observation frequency)
and all other nodes are optimally decided, there
is no point to add the node [C] into the lattice
and instead of having 9 nodes we will have only
</bodyText>
<page confidence="0.995196">
851
</page>
<listItem confidence="0.673503">
3. Another consideration which we apply during
</listItem>
<bodyText confidence="0.949018666666667">
the lattice building is to penalize the develop-
ment of low frequency (but not zero frequency)
nodes i.e. the nodes with no reliable statistics
on them. Thus we smooth the estimates on such
nodes with the uniform distribution (which has
the entropy on its maximum):
</bodyText>
<equation confidence="0.963465">
p&amp;quot;(92) =L* +[- + (1- L)*11(9i) where L
THRESHOLD
THRESHOLD+q
</equation>
<bodyText confidence="0.999986636363636">
So for high frequency nodes this smoothing is
very minor but for nodes with frequencies less
than two thresholds the penalty will be consid-
erable. This will favor nodes which do not cre-
ate sparce collocations with other nodes.
The described method is similar in spirit to
the method of word trigger incorporation to a
trigram model suggested in (Rosenfeld, 1996):
if a trigram predicts well enough there is no
need for an additional trigger. The main differ-
ence is that we do not recompute the maximum
entropy model every time but use our own fre-
quency redistribution method over the colloca-
tion lattice. This is the crucial difference which
makes a tremendous saving in time. We also
do not require a newly added feature to be ei-
ther atomic or a collocation of an atomic feature
with a feature already included into the model
as it was proposed in (Della Pietra et al., 1995)
(Berger et al., 1996). All the features are cre-
ated equal and the model should decide on the
level of granularity by itself.
</bodyText>
<sectionHeader confidence="0.979842" genericHeader="method">
4 Model Generalization
</sectionHeader>
<bodyText confidence="0.999981166666667">
After we have chosen a subset of features for our
model, we restrict our feature lattice to the op-
timized lattice. Now we can compute the max-
imum entropy model taking the reference prob-
abilities (which are configuration probabilities)
as in equation 3.
The nodes from the optimized lattice serve
both as possible domain configurations and as
potential constraint features to our model. We,
however, want to constrain only the nodes with
the reliable statistics on them in order not to
overfit the model. This in its turn will take off
certain computational load, since we expect a
considerable number of fragmented (simply in-
frequent) nodes in the optimized lattice. This
comes from the requirement to build all the col-
locations when we add a new node. Although
many top-level nodes will not be constrained,
the information from such infrequent nodes will
not be lost completely - it will contribute to
more general nodes since for every constrained
node we marginalize over all its unconstrained
descendants (more specific nodes). Thus as
possible constraints for the model we will con-
sider only those nodes from the optimized lat-
tice, whose marginalized over responses feature
frequency countsl are greater than a certain
threshold, e.g.: .&apos;ox (x,y) &gt; 5. This considera-
tion is slightly different from the one suggested
in (Ristad, 1996) where it was proposed to un-
constrain nodes with infrequent joint feature
frequency counts. Thus if we saw a certain fea-
ture configuration say 5,000 times and it always
gave a single response we suggest to constrain
as well the observation that we never saw this
configuration with the other responses. If we
applied the suggestion of (Ristad, 1996) and
cut out on the basis of the joint frequency we
would lose the negative evidence, which is quite
reliable judging by the total frequency of the
observation.
Initially we constrain all the nodes which sat-
isfy the above requirement. In order to gener-
alize and simplify our maximum entropy model,
we uncgnstrain the most specific features, com-
pute a new simplified maximum entropy model,
and if it still predicts well, we repeat the pro-
cess. So our aim is to remove from the con-
straints as many top level nodes as possible
without losing the model fitness to the refer-
ence distribution (13) of the optimized feature
lattice. The necessary condition for a node to
be taken as a candidate to unconstrain, is that
this node shouldn&apos;t have any constrained nodes
above it. There is also a natural ranking for
the candidate nodes: the closer to 1 the weight
(A) of a such a node is, the less it is important
for the model. We can set a certain thresh-
old on the weights, so all the candidate nodes
whose As differ from 1 less than this threshold
will be unconstrained in one go. Therefore we
don&apos;t have to use the iterative scaling for feature
ranking and apply it only for model recompu-
tation, possibly un-constraining several feature
configurations (nodes) at once. This method, in
fact, resembles the Backward Sequential Search
</bodyText>
<page confidence="0.970197">
6
</page>
<subsubsectionHeader confidence="0.33995">
Tx(ek)= Ea; Ea, fek(19i)* 7
</subsubsectionHeader>
<page confidence="0.993606">
852
</page>
<bodyText confidence="0.998863291666667">
(BSS) proposed in (Pedersen&amp;Bruce, 1997) for
decomposable models. There is also a sig-
nificant reduction in computational load since
the generalized smaller model deviates from the
previous larger model only in a small number of
constraints. So we use the parameters of that
larger model2 as the initial values for the itera-
tive scaling algorithm. This proved to decrease
the number of required iterations by about ten-
fold, which makes a tremendous saving in time.
There can be many possible criteria when to
stop the generalization algorithm. The sim-
plest one is just to set a predefined threshold
on the deviation D(13 II p) of the generalized
model from the reference distribution. (Peder-
sen&amp;Bruce, 1997) suggest to use Akaike&apos;s Infor-
mation Criteria (AIC) to judge the acceptabil-
ity of a new model. AIC rewards good model fit
and penalizes models with high complexity mea-
sured in the number of features. We adopted
the stop condition suggested in (Berger et al.,
1996) - the maximization of the likelihood on a
cross-validation set of samples which is unseen
at the parameter estimation.
</bodyText>
<sectionHeader confidence="0.931445" genericHeader="method">
5 Application: Fullstop Problem
</sectionHeader>
<bodyText confidence="0.999609452054795">
Sentence boundary disambiguation has recently
gained certain attention of the language engi-
neering community. It is required for most text
processing tasks such as, tagging, parsing, par-
allel corpora alignment etc., and, as it turned
out to be, this is a non-trivial task itself. A
period can act as the end of a sentence or be
a part of an abbreviation, but when an abbre-
viation is the last word in a sentence, the pe-
riod denotes the end of a sentence as well. The
simplest &amp;quot;period-space-capital_letter&amp;quot; approach
works well for simple texts but is rather unre-
liable for texts with many proper names and
abbreviations at the end of sentence as, for in-
stance, the Wall Street Journal (WSJ) corpus (
(Marcus et al., 1993) ).
One well-known trainable systems - SATZ
- is described in (Palmer&amp;Hearst, 1997). It
uses a neural network with two layers of hid-
den units. It was trained on the most prob-
able parts-of-speech of three words before and
three words after the period using 573 samples
from the WSJ corpus. It was then tested on
2instead of the uniform distribution as prescribed in
the step 1 of the Improved Iterative Scaling algorithm.
unseen 27,294 sentences from the same corpus
and achieved 1.5% error rate. Another auto-
matically trainable system described in (Rey-
nar&amp;Ratnaparkhi, 1997). This system is sim-
ilar to ours in the model choice - it uses the
maximum entropy framework. It was trained
on two different feature sets and scored 1.2%
error rate on the corpus tuned feature set and
2% error rate on a more portable feature set.
The features themselves were words and their
classes in the immediate context of the period
mark. (Reynar&amp;Ratnaparkhi, 1997) don&apos;t re-
port on the number of features utilized by their
model and don&apos;t describe their approach to fea-
ture selection but judging by the time their sys-
tem was trained (18 minutes3) it did not aim
to produce the best performing feature-set but
estimated a given one.
To tackle this problem we applied our method
to a maximum entropy model which used a
lexicon of words associated with one or more
categories from the set: abbreviation, proper
noun, content word, closed-class word. This
model employed atomic features such as the lex-
icon information for the words before and after
the period, their capitalization and spellings.
For training we collected from the WSJ cor-
pus 51,000 samples of the form (Y,F..F) and
(N, F..F), where Y stands for the end of sen-
tence, N stands for otherwise and Fs stand for
the atomic features of the model. We started to
built the model with 238 most frequent atomic
features which gave us the collocation lattice of
8,245 nodes in 8 minutes of processor time on
five SUN Ultra-1 workstations working in par-
allel by means of multi-threading and Remote
Process Communication. When we applied the
feature selection algorithm (section 3), we in 53
minutes boiled the lattice down to 769 nodes.
Then constraining all the nodes, we compiled
a maximum entropy model in about 15 minutes
and then using the constraint removal process in
two hours we boiled the constraint space down
to 283. In this set only 31 atomic features re-
mained. This model was detected to achieve the
best performance on a specified cross-validation
set. For the evaluation we used the same 27,294
sentences as in (Palmer&amp;Hearst, 1997)4 which
</bodyText>
<footnote confidence="0.960506666666667">
3Personal communication
&apos;We would like to thank David Palmer for making his
test data available to us.
</footnote>
<page confidence="0.998591">
853
</page>
<bodyText confidence="0.999906545454546">
were also used by (ReynarScRatnaparkhi, 1997)
in the evaluation of their system. These sen-
tences, of course, were not seen at the train-
ing phase of our model. Our model achieved
99,2477% accuracy which is the highest quoted
score on this test-set known to the authors.
We attribute this to the fact that although we
started with roughly the same atomic features
as (ReynarSzRatnaparkhi, 1997) our system
created complex features with higher prediction
power.
</bodyText>
<sectionHeader confidence="0.998919" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999853463414634">
In this paper we presented a novel approach for
building maximum entropy models. Our ap-
proach uses a feature collocation lattice and se-
lects the candidate features without resorting
to iterative scaling. Instead we use our own
frequency redistribution algorithm. After the
candidate features have been selected we, us-
ing the iterative scaling, compute a fully satu-
rated model for the maximal constraint space
and then apply relaxation to the most specific
constraints.
We applied the described method to sev-
eral language modelling tasks such as sentence
boundary disambiguation, part-of-speech tag-
ging, stress prediction in continues speech gen-
eration, etc., and proved its feasibility for select-
ing and building the models with the complex-
ity of tens of thousands constraints. We see the
major achievement of our method in building
compact models with only a fraction of possi-
ble features (usually there is a few hundred fea-
tures) and at the same time performing at least
as good as state-of-the-art: in fact, our sen-
tence boundary disambiguater scored the high-
est known to the author accuracy (99.2477%)
and our part-of-speech tagging model general-
ized for a new domain with only a tiny degra-
dation in performance.
A potential drawback of our approach is that
we require to build the feature collocation lat-
tice for the whole observed feature-space which
might not be feasible for applications with hun-
dreds of thousands of features. So one of the
directions in our future work is to find effi-
cient ways for a decomposition of the feature
lattice into non-overlapping sub-lattices which
then can be handled by our method. Another
avenue for further improvement is to introduce
the &amp;quot;or&amp;quot; operation on the nodes of the lattice.
This can provide a further generalization over
the features employed by the model.
</bodyText>
<sectionHeader confidence="0.997963" genericHeader="acknowledgments">
7 Acknowledgements
</sectionHeader>
<bodyText confidence="0.999724285714286">
The work reported in this paper was supported
in part by grant GR/L21952 (Text Tokenisa-
tion Tool) from the Engineering and Physical
Sciences Research Council, UK. We would also
like to acknowledge that this work was based on
a long-standing collaborative relationship with
Steve Finch.
</bodyText>
<sectionHeader confidence="0.999189" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.995214108108108">
A. Berger, S. Della Pietra, V. Della Pietra,
1996. A Maximum Entropy Approach to Nat-
ural Language Processing In Computational
Linguistics vol.22(1)
J.N. Darroch and D. Ratcliff 1972. Generalized
Iterative Scaling for Log-Linear Models. The
Annals of Mathematical Statistics, 43(5).
S. Della Pietra, V.. Della Pietra, and J. Lafferty
1995. Inducing Features of Random Fields
Technical report CMU-CS-95-144
M. Marcus, M.A. Marcinkiewicz, and B. San-
torini 1993. Building a Large Annotated Cor-
pus of English: The Penn Treebank. In Com-
putational Linguistics, vol 19(2), ACL.
D. D. Palmer and M. A. Hearst 1997. Adaptive
Multilingual Sentence Boundary Disambigua-
tion. In Computational Linguistics, vol 23(2),
ACL. pp. 241-269
T. Pedersen and R. Bruce 1997. A New Su-
pervised Learning Algorithm for Word Sense
Disambiguation. In Proceedings of the Four-
teenth National Conference on Artificial In-
telligence, Providence, RI.
J. C. Reynar and A. Ratnaparkhi 1997. A
Maximum Entropy Approach to Identifying
Sentence Boundaries. In Proceedings of the
Fifth ACL Conference on Applied Natural
Language Processing (ANLP&apos;97), Washing-
ton D.C., ACL.
E. S. Ristad 1996. Maximum Entropy Mod-
elling Toolkit. Documentation for Version
1.3 Beta, Draft,
R. Rosenfeld 1996. A Maximum Entropy
Approach to Adaptive Statistical Language
Learning. In Computer Speech and Language,
vol.10(3), Academic Press Limited, pp. 197-
228
</reference>
<page confidence="0.999101">
854
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.928097">
<title confidence="0.99993">Feature Lattices for Maximum Entropy Modelling</title>
<author confidence="0.998284">Andrei Mikheev</author>
<affiliation confidence="0.999728">HCRC, Language Technology Group, University of Edinburgh,</affiliation>
<address confidence="0.988348">2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, UK.</address>
<email confidence="0.999262">e-mail:Andrei.MikheevOed.ac.uk</email>
<abstract confidence="0.9960732">Maximum entropy framework proved to be expressive and powerful for the statistical language modelling, but it suffers from the computational expensiveness of the model building. The iterative scaling algorithm that is used for the parameter estimation is computationally expensive while the feature selection process might require to estimate parameters for many candidate features many times. In this paper we present a novel approach for building maximum entropy models. Our approach uses the feature collocation lattice and builds complex candidate features without resorting to iterative scaling.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>A Berger</author>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing In</title>
<date>1996</date>
<journal>Computational Linguistics</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="3669" citStr="Berger et al., 1996" startWordPosition="562" endWordPosition="565">model&apos;s feature space. For every feature from the candidate feature set the algorithm prescribes to compute the maximum entropy model using the iterative scaling algorithm described above, and select the feature which in the largest way minimizes the Kullback-Leibler divergence or maximizes the log-likelihood of the model. This approach, however, is not computationally feasible since the iterative scaling is computationally expensive and to compute models for many candidate features many times is unreal. To 848 make feature ranking computationally tractable in (Della Pietra et al., 1995) and (Berger et al., 1996) a simplified process proposed: at the feature ranking stage when adding a new feature to the model, all previously computed parameters are kept fixed and, thus, we have to fit only one new constraint imposed by the candidate feature. Then after the best ranked feature has been established, it is added to the feature space and the weights for all the features are recomputed. This approach estimates good features relatively fast but it does not guarantee that at every single point we add the best feature because when we add a new feature to the model all its parameters can change. In this paper</context>
<context position="15676" citStr="Berger et al., 1996" startWordPosition="2745" endWordPosition="2748">thod of word trigger incorporation to a trigram model suggested in (Rosenfeld, 1996): if a trigram predicts well enough there is no need for an additional trigger. The main difference is that we do not recompute the maximum entropy model every time but use our own frequency redistribution method over the collocation lattice. This is the crucial difference which makes a tremendous saving in time. We also do not require a newly added feature to be either atomic or a collocation of an atomic feature with a feature already included into the model as it was proposed in (Della Pietra et al., 1995) (Berger et al., 1996). All the features are created equal and the model should decide on the level of granularity by itself. 4 Model Generalization After we have chosen a subset of features for our model, we restrict our feature lattice to the optimized lattice. Now we can compute the maximum entropy model taking the reference probabilities (which are configuration probabilities) as in equation 3. The nodes from the optimized lattice serve both as possible domain configurations and as potential constraint features to our model. We, however, want to constrain only the nodes with the reliable statistics on them in o</context>
<context position="19819" citStr="Berger et al., 1996" startWordPosition="3445" endWordPosition="3448">s proved to decrease the number of required iterations by about tenfold, which makes a tremendous saving in time. There can be many possible criteria when to stop the generalization algorithm. The simplest one is just to set a predefined threshold on the deviation D(13 II p) of the generalized model from the reference distribution. (Pedersen&amp;Bruce, 1997) suggest to use Akaike&apos;s Information Criteria (AIC) to judge the acceptability of a new model. AIC rewards good model fit and penalizes models with high complexity measured in the number of features. We adopted the stop condition suggested in (Berger et al., 1996) - the maximization of the likelihood on a cross-validation set of samples which is unseen at the parameter estimation. 5 Application: Fullstop Problem Sentence boundary disambiguation has recently gained certain attention of the language engineering community. It is required for most text processing tasks such as, tagging, parsing, parallel corpora alignment etc., and, as it turned out to be, this is a non-trivial task itself. A period can act as the end of a sentence or be a part of an abbreviation, but when an abbreviation is the last word in a sentence, the period denotes the end of a sent</context>
</contexts>
<marker>Berger, Pietra, Pietra, 1996</marker>
<rawString>A. Berger, S. Della Pietra, V. Della Pietra, 1996. A Maximum Entropy Approach to Natural Language Processing In Computational Linguistics vol.22(1)</rawString>
</citation>
<citation valid="true">
<authors>
<author>J N Darroch</author>
<author>D Ratcliff</author>
</authors>
<title>Generalized Iterative Scaling for Log-Linear Models.</title>
<date>1972</date>
<journal>The Annals of Mathematical Statistics,</journal>
<volume>43</volume>
<issue>5</issue>
<marker>Darroch, Ratcliff, 1972</marker>
<rawString>J.N. Darroch and D. Ratcliff 1972. Generalized Iterative Scaling for Log-Linear Models. The Annals of Mathematical Statistics, 43(5).</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Della Pietra</author>
<author>V Della Pietra</author>
<author>J Lafferty</author>
</authors>
<title>Inducing Features of Random Fields</title>
<date>1995</date>
<tech>Technical report CMU-CS-95-144</tech>
<contexts>
<context position="2805" citStr="Pietra et al., 1995" startWordPosition="428" endWordPosition="431"> model are good features and the model is useful. Thus the most important past of the model building is the feature selection procedure. The key idea of the feature selection is that if we notice an interaction between certain features we should build a more complex feature which will account for this interaction. The newly added feature should improve the model: its Kullback-Leibler divergence from the reference distribution should decrease and the conditional maximum entropy model will also have the greatest log-likelihood (L) value: The basic feature induction algorithm presented in (Della Pietra et al., 1995) starts with an empty feature space and iteratively tries all possible feature candidates. These candidates are either atomic features or complex features produced as a combination of an atomic feature with the features already selected to the model&apos;s feature space. For every feature from the candidate feature set the algorithm prescribes to compute the maximum entropy model using the iterative scaling algorithm described above, and select the feature which in the largest way minimizes the Kullback-Leibler divergence or maximizes the log-likelihood of the model. This approach, however, is not </context>
<context position="15654" citStr="Pietra et al., 1995" startWordPosition="2741" endWordPosition="2744">ar in spirit to the method of word trigger incorporation to a trigram model suggested in (Rosenfeld, 1996): if a trigram predicts well enough there is no need for an additional trigger. The main difference is that we do not recompute the maximum entropy model every time but use our own frequency redistribution method over the collocation lattice. This is the crucial difference which makes a tremendous saving in time. We also do not require a newly added feature to be either atomic or a collocation of an atomic feature with a feature already included into the model as it was proposed in (Della Pietra et al., 1995) (Berger et al., 1996). All the features are created equal and the model should decide on the level of granularity by itself. 4 Model Generalization After we have chosen a subset of features for our model, we restrict our feature lattice to the optimized lattice. Now we can compute the maximum entropy model taking the reference probabilities (which are configuration probabilities) as in equation 3. The nodes from the optimized lattice serve both as possible domain configurations and as potential constraint features to our model. We, however, want to constrain only the nodes with the reliable s</context>
</contexts>
<marker>Pietra, Pietra, Lafferty, 1995</marker>
<rawString>S. Della Pietra, V.. Della Pietra, and J. Lafferty 1995. Inducing Features of Random Fields Technical report CMU-CS-95-144</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Marcus</author>
<author>M A Marcinkiewicz</author>
<author>B Santorini</author>
</authors>
<title>Building a Large Annotated Corpus of English: The Penn Treebank.</title>
<date>1993</date>
<booktitle>In Computational Linguistics, vol 19(2), ACL.</booktitle>
<contexts>
<context position="20690" citStr="Marcus et al., 1993" startWordPosition="3594" endWordPosition="3597">community. It is required for most text processing tasks such as, tagging, parsing, parallel corpora alignment etc., and, as it turned out to be, this is a non-trivial task itself. A period can act as the end of a sentence or be a part of an abbreviation, but when an abbreviation is the last word in a sentence, the period denotes the end of a sentence as well. The simplest &amp;quot;period-space-capital_letter&amp;quot; approach works well for simple texts but is rather unreliable for texts with many proper names and abbreviations at the end of sentence as, for instance, the Wall Street Journal (WSJ) corpus ( (Marcus et al., 1993) ). One well-known trainable systems - SATZ - is described in (Palmer&amp;Hearst, 1997). It uses a neural network with two layers of hidden units. It was trained on the most probable parts-of-speech of three words before and three words after the period using 573 samples from the WSJ corpus. It was then tested on 2instead of the uniform distribution as prescribed in the step 1 of the Improved Iterative Scaling algorithm. unseen 27,294 sentences from the same corpus and achieved 1.5% error rate. Another automatically trainable system described in (Reynar&amp;Ratnaparkhi, 1997). This system is similar t</context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>M. Marcus, M.A. Marcinkiewicz, and B. Santorini 1993. Building a Large Annotated Corpus of English: The Penn Treebank. In Computational Linguistics, vol 19(2), ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D D Palmer</author>
<author>M A Hearst</author>
</authors>
<title>Adaptive Multilingual Sentence Boundary Disambiguation.</title>
<date>1997</date>
<booktitle>In Computational Linguistics, vol 23(2), ACL.</booktitle>
<pages>241--269</pages>
<marker>Palmer, Hearst, 1997</marker>
<rawString>D. D. Palmer and M. A. Hearst 1997. Adaptive Multilingual Sentence Boundary Disambiguation. In Computational Linguistics, vol 23(2), ACL. pp. 241-269</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Pedersen</author>
<author>R Bruce</author>
</authors>
<title>A New Supervised Learning Algorithm for Word Sense Disambiguation.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fourteenth National Conference on Artificial Intelligence,</booktitle>
<location>Providence, RI.</location>
<marker>Pedersen, Bruce, 1997</marker>
<rawString>T. Pedersen and R. Bruce 1997. A New Supervised Learning Algorithm for Word Sense Disambiguation. In Proceedings of the Fourteenth National Conference on Artificial Intelligence, Providence, RI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J C Reynar</author>
<author>A Ratnaparkhi</author>
</authors>
<title>A Maximum Entropy Approach to Identifying Sentence Boundaries.</title>
<date>1997</date>
<booktitle>In Proceedings of the Fifth ACL Conference on Applied Natural Language Processing (ANLP&apos;97),</booktitle>
<location>Washington D.C., ACL.</location>
<marker>Reynar, Ratnaparkhi, 1997</marker>
<rawString>J. C. Reynar and A. Ratnaparkhi 1997. A Maximum Entropy Approach to Identifying Sentence Boundaries. In Proceedings of the Fifth ACL Conference on Applied Natural Language Processing (ANLP&apos;97), Washington D.C., ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E S Ristad</author>
</authors>
<title>Maximum Entropy Modelling Toolkit. Documentation for Version 1.3</title>
<date>1996</date>
<location>Beta, Draft,</location>
<contexts>
<context position="17141" citStr="Ristad, 1996" startWordPosition="2988" endWordPosition="2989">ons when we add a new node. Although many top-level nodes will not be constrained, the information from such infrequent nodes will not be lost completely - it will contribute to more general nodes since for every constrained node we marginalize over all its unconstrained descendants (more specific nodes). Thus as possible constraints for the model we will consider only those nodes from the optimized lattice, whose marginalized over responses feature frequency countsl are greater than a certain threshold, e.g.: .&apos;ox (x,y) &gt; 5. This consideration is slightly different from the one suggested in (Ristad, 1996) where it was proposed to unconstrain nodes with infrequent joint feature frequency counts. Thus if we saw a certain feature configuration say 5,000 times and it always gave a single response we suggest to constrain as well the observation that we never saw this configuration with the other responses. If we applied the suggestion of (Ristad, 1996) and cut out on the basis of the joint frequency we would lose the negative evidence, which is quite reliable judging by the total frequency of the observation. Initially we constrain all the nodes which satisfy the above requirement. In order to gene</context>
</contexts>
<marker>Ristad, 1996</marker>
<rawString>E. S. Ristad 1996. Maximum Entropy Modelling Toolkit. Documentation for Version 1.3 Beta, Draft,</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Rosenfeld</author>
</authors>
<title>A Maximum Entropy Approach to Adaptive Statistical Language Learning.</title>
<date>1996</date>
<journal>In Computer Speech and Language,</journal>
<volume>10</volume>
<issue>3</issue>
<pages>197--228</pages>
<publisher>Academic Press Limited,</publisher>
<contexts>
<context position="1545" citStr="Rosenfeld, 1996" startWordPosition="228" endWordPosition="229">ressive and powerful framework. The maximum entropy model is a model which fits a set of pre-defined constraints and assumes maximum ignorance about everything which is not subject to its constraints thus assigning such cases with the most uniform distribution. The most uniform distribution will have the entropy on its maximum Because of its ability to handle overlapping features the maximum entropy framework provides a principle way to incorporate information from multiple knowledge sources. It is superior to traditionally used for this purpose linear interpolation and Katz back-off method. (Rosenfeld, 1996) evaluates in detail a maximum entropy language model which combines unigrams, bigrams, trigrams and long-distance trigger words, and provides a thorough analysis of all the merits of the approach. • Now at Harlequin Ltd. The iterative scaling algorithm (Darroch&amp;Ratcliff, 1972) applied for the parameter estimation of maximum entropy models computes a set of feature weights (As) which ensure that the model fits the reference distribution and does not make spurious assumptions (as required by the maximum entropy principle) about events beyond the reference distribution. It, however, does not gua</context>
<context position="15140" citStr="Rosenfeld, 1996" startWordPosition="2649" endWordPosition="2650">t not zero frequency) nodes i.e. the nodes with no reliable statistics on them. Thus we smooth the estimates on such nodes with the uniform distribution (which has the entropy on its maximum): p&amp;quot;(92) =L* +[- + (1- L)*11(9i) where L THRESHOLD THRESHOLD+q So for high frequency nodes this smoothing is very minor but for nodes with frequencies less than two thresholds the penalty will be considerable. This will favor nodes which do not create sparce collocations with other nodes. The described method is similar in spirit to the method of word trigger incorporation to a trigram model suggested in (Rosenfeld, 1996): if a trigram predicts well enough there is no need for an additional trigger. The main difference is that we do not recompute the maximum entropy model every time but use our own frequency redistribution method over the collocation lattice. This is the crucial difference which makes a tremendous saving in time. We also do not require a newly added feature to be either atomic or a collocation of an atomic feature with a feature already included into the model as it was proposed in (Della Pietra et al., 1995) (Berger et al., 1996). All the features are created equal and the model should decide</context>
</contexts>
<marker>Rosenfeld, 1996</marker>
<rawString>R. Rosenfeld 1996. A Maximum Entropy Approach to Adaptive Statistical Language Learning. In Computer Speech and Language, vol.10(3), Academic Press Limited, pp. 197-228</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>