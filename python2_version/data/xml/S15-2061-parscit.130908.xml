<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.095352">
<title confidence="0.996325">
UNIBA: Combining Distributional Semantic Models and Sense Distribution
for Multilingual All-Words Sense Disambiguation and Entity Linking
</title>
<author confidence="0.963739">
Pierpaolo Basile and Annalina Caputo and Giovanni Semeraro
</author>
<affiliation confidence="0.9966955">
Department of Computer Science
University of Bari Aldo Moro
</affiliation>
<address confidence="0.957995">
Via, E. Orabona, 4 - 70125 Bari (Italy)
</address>
<email confidence="0.999434">
{pierpaolo.basile,annalina.caputo,giovanni.semeraro}@uniba.it
</email>
<sectionHeader confidence="0.996667" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999129545454545">
This paper describes the participation of the
UNIBA team in the Task 13 of SemEval-2015
about Multilingual All-Words Sense Disam-
biguation and Entity Linking. We propose
an algorithm able to disambiguate both word
senses and named entities by combining the
simple Lesk approach with information com-
ing from both a distributional semantic model
and usage frequency of meanings. The results
for both English and Italian show satisfactory
performance.
</bodyText>
<sectionHeader confidence="0.998768" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999620055555555">
SemEval-2015 Task 13 (Moro and Navigli, 2015)
aims to evaluate systems that provide a compre-
hensive representation of text through linking of
both words and entities with concepts in a knowl-
edge base. Besides the traditional difficulties of
word sense disambiguation, this task requires spe-
cific methods able to tackle the challenges posed by
the named entity recognition, disambiguation and
linking steps.
This paper proposes a unified strategy for word
sense and named entity disambiguation which lever-
ages BabelNet, a multilingual resource that encom-
passes both encyclopedic and lexicographic knowl-
edge (Navigli and Ponzetto, 2012). Our approach
relies on the Distributional Lesk (DL-WSD) algo-
rithm (Basile et al., 2014), which is able to disam-
biguate a word occurrence by computing the similar-
ity between word context and the glosses associated
with all possible word meanings. Such a similarity is
computed through a Distributional Semantic Model
(DSM) (Sahlgren, 2006).
In this work we describe an extension of the
DL-WSD algorithm that exploits a specific module
for entity discovery given a list of possible surface
forms. In particular, we build an index in which each
surface form (i.e. candidate entity) is paired to the
list of all its possible meanings in a semantic net-
work. This index of surface forms is exploited to
look up all candidate entities in a text.
The rest of this paper is structured as follows:
Section 2 provides details about the adopted strat-
egy, and describes the two main steps: 1) Entity
Recognition and 2) Disambiguation. An experimen-
tal evaluation, along with details about results, is
presented in Section 3, while conclusions close the
paper.
</bodyText>
<sectionHeader confidence="0.993854" genericHeader="introduction">
2 Methodology
</sectionHeader>
<bodyText confidence="0.999960375">
Our methodology is a two-step algorithm consist-
ing in an initial identification of all possible entities
mentioned in a text followed by the disambiguation
of both words and named entities through the DL-
WSD algorithm. The semantic network is exploited
twice in order to 1) extract all the possible surface
forms related to entities, and 2) retrieve glosses used
in the disambiguation process.
</bodyText>
<subsectionHeader confidence="0.973615">
2.1 Entity Recognition
</subsectionHeader>
<bodyText confidence="0.995373">
In order to speed up the entity recognition step we
build an index in which for each surface form (en-
tity) the set of all its possible meanings in the se-
mantic network is reported. Lucene1 is exploited to
</bodyText>
<footnote confidence="0.993118">
1http://lucene.apache.org/
</footnote>
<page confidence="0.952042">
360
</page>
<bodyText confidence="0.895066">
Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 360–364,
Denver, Colorado, June 4-5, 2015. c�2015 Association for Computational Linguistics
build the index, specifically for each surface form
(lexeme) occurring in BabelNet, a document com-
posed of two fields is created. The first field stores
the surface form, while the second one contains the
list of all possible BabelSynsets that refer to the sur-
face form in the first field. The index is built sep-
arately for each language, Italian and English. The
entity recognition module exploits this index in or-
der to find entities in a text. Given a text fragment,
the module performs the following steps:
</bodyText>
<listItem confidence="0.996226333333333">
• Building all n-grams up to five words;
• Querying the index and retrieving the list of the
top t matching surface forms for each n-gram.
It is possible to enable a multi-match strat-
egy; for example the 3-gram “European Union
Commission” can match two entities: “Euro-
pean Union” and “European Union Commis-
sion”. The multi-match strategy provides dis-
ambiguation for all the possible entities, other-
wise the longest surface form is selected;
• Scoring each surface form by exploiting two
different approaches:
</listItem>
<bodyText confidence="0.978403166666667">
EXACT MATCH computes the linear combi-
nation between the score provided by the
search engine and a string similarity func-
tion based on the Levenshtein Distance
between the n-gram and the candidate sur-
face form in the index;
PARTIAL MATCH computes the linear
combination between the two scores
provided by the EXACT MATCH and the
Jaccard Index in terms of common words
between the n-gram and the candidate
surface form;
</bodyText>
<listItem confidence="0.803012142857143">
• Filtering the candidate entities recognized in
the previous steps; entities are removed if the
score computed in the previous step is below
a given threshold and/or the sequence of PoS-
tags related to the n-gram does not match a set
of defined patterns;
• Assigning to each candidate entity two addi-
</listItem>
<bodyText confidence="0.994687684210526">
tional scores according to the percentage of: 1)
stop words, and 2) words that do not contain
at least one upper-case character. A threshold
can be fixed for each score to filter out some
entities.
Moreover, for each entity we build a set of alterna-
tives. For example, given the candidate entity “Euro-
pean Union” we create the set of alternative surface
forms {European, Union, EU, E.U.}. Then, we add
all the BabelSynsets of “European Union” to the list
of possible meanings of those words that follow the
candidate entity and belong to the set of alternative
forms.
The output of the entity recognition module is a
list of candidate entities in which a set of possible
meanings (BabelSynset) is assigned to each surface
form in the list. The set of named entities extracted
by this module and the list of all the words in the text
are the input to the DL-WSD algorithm.
</bodyText>
<subsectionHeader confidence="0.99402">
2.2 DL-WSD
</subsectionHeader>
<bodyText confidence="0.999871678571429">
We exploit the distributional Lesk algorithm pro-
posed by Basile et al. (2014) for disambiguating
words and named entities. The algorithm replaces
the concept of word overlap initially introduced
by (Lesk, 1986) with the broader concept of se-
mantic similarity computed in a distributional se-
mantic space. Let w1, w2, ...wr,, be a sequence of
words/entities, the algorithm disambiguates each
target word/entity wz by computing the semantic
similarity between the glosses of senses associated
with the target word/entity and its context. This sim-
ilarity is computed by representing in a DSM both
the gloss and the context as the sum of words they
are composed of; then this similarity takes into ac-
count the co-occurrence evidences previously col-
lected through a corpus of documents. The corpus
plays a key role since the richer it is the higher is
the probability that each word is fully represented
in all its contexts of use. We exploit the word2vec
tool2(Mikolov et al., 2013) in order to build a DSM,
by analyzing all the pages in the last English/Italian
Wikipedia Dump. The correct sense for a word is
the one whose gloss maximizes the semantic simi-
larity with the word/entity context. The sense de-
scription can still be too short for a meaningful com-
parison with the word/entity context. Following this
observation, we adopted an approach inspired by the
adapted Lesk (Banerjee and Pedersen, 2002), and
</bodyText>
<footnote confidence="0.978126">
2https://code.google.com/p/word2vec/
</footnote>
<page confidence="0.996354">
361
</page>
<bodyText confidence="0.988514413793104">
we decided to enrich the gloss of the sense with
those of related meanings, duly weighted to reflect
their distances with respect to the original sense.
The algorithm consists of the following steps.
Building the glosses. We retrieve the set
Si = {si1, si2, ..., sik} of senses associated to the
word/entity wi. For named entities such a set is
provided by the entity recognition module, while
for words the set is obtained by firstly looking up to
the WordNet portion of BabelNet, then if no sense is
found we seek for senses from Wikipedia. For each
sense sij, the algorithm builds the extended gloss
representation g�ij by adding to the original gloss gij
the glosses of related meanings retrieved through
the BabelNet function getRelatedMap, with the
exception of antonym senses. Each word in g�ij is
weighted by a function inversely proportional to the
distance d between sij and the related glosses where
the word occurs. Moreover, in order to emphasize
discriminative words among the different senses, in
the weight we introduce a variation of the inverse
document frequency (idf) for retrieval that we
named inverse gloss frequency (igf). The igf for a
word wk occurring gfk times in the set of extended
glosses for all the senses in Si (the sense inventory
of wi) is computed as IGFk = 1 + log2xSi*1. The
fk
final weight for the word wk appearing h times in
the extended gloss g�ij is given by:
</bodyText>
<equation confidence="0.9956655">
1
weight(wk, g� ij) = h × IGFk × 1 + d (1)
</equation>
<bodyText confidence="0.99993959375">
Building the context. The context C for the word
wi is represented by all the words that occur in the
text.
Building the vector representations. The con-
text C and each extended gloss g�ij are represented
as vectors in the SemanticSpace built through the
DSM.
Sense ranking. The algorithm computes the co-
sine similarity between the vector representation of
each extended gloss g�ij and that of the context C.
Then, the cosine similarity is linearly combined with
a function which takes into account the usage of
the meaning in the language. We analyse a func-
tion that computes the probability assigned to each
synset given a word/named entity as follows:
Word. We exploit a synset-tagged corpus and we
attempt to map each word occurrence to Word-
Net (Miller, 1995). Then, we select the Word-
Net sysnet with the maximum probability.
Named Entity. We retrieve from BabelNet the
Wikipedia title pages related to the Babel-
Synset and count the number of times a
Wikipedia page is linked from another page. In
this way we use Wikipedia as a synset-tagged
corpus.
We define the probability p(sij|wi) that takes
into account the sense distribution of sij given the
word/entity wi. The sense distribution is computed
as the number of times the word/entity wi is tagged
with the sense. Zero probabilities are avoided by
introducing an additive (Laplace) smoothing. The
probability is computed as follows:
</bodyText>
<equation confidence="0.999498333333333">
t(wi, sij) + 1
p(sij|wi) = (2)
#wi + |Si|
</equation>
<bodyText confidence="0.9988245">
where t(wi, sij) is the number of times the
word/entity wi is tagged with the sense sij.
</bodyText>
<sectionHeader confidence="0.995591" genericHeader="method">
3 Evaluation
</sectionHeader>
<bodyText confidence="0.9978157">
The evaluation aims at comparing the system result
against a gold standard manually annotated using
synsets from BabelNet 2.5.1. Test data consists of
four documents that belong to three different do-
mains: biomedical, maths and computer science,
and general. The idea is to evaluate the algorithm
performance both in general and specific domains.
We submitted three runs with different parameter
settings that mainly affected the entity recognition
module. System settings are reported in Table 1.
</bodyText>
<table confidence="0.99932875">
Run Match PoS-Tag Threshold
Run1 EXACT YES 1.0
Run2 PARTIAL YES 0.75
Run3 PARTIAL NO 0.75
</table>
<tableCaption confidence="0.999914">
Table 1: System settings.
</tableCaption>
<bodyText confidence="0.999832">
The Match column indicates the type of match-
ing used during the entity recognition step, PoS-Tag
reports the usage of the filter based on PoS-Tag pat-
terns, and finally the table reports the Threshold used
by the matching filter. Moreover, we set the number
</bodyText>
<page confidence="0.99354">
362
</page>
<table confidence="0.999407833333333">
EN IT
Run all NE WSD n v r a all NE WSD n v r a
best 65.8 88.9 64.6 70.3 57.7 79.0 79.5 59.9 54.9 61.3 56.6 62.7 62.5 69.6
Run1 58.4 84.4 56.5 63.3 57.1 79.0 - 50.8 48.5 51.0 53.7 61.1 60.0 -
Run2 58.3 82.9 56.5 63.2 57.1 79.0 - 50.9 48.5 51.0 53.8 61.1 60.0 -
Run3 58.3 82.9 56.5 63.2 57.1 79.0 - 50.9 50.0 51.0 53.7 61.1 60.0 -
</table>
<tableCaption confidence="0.840687">
Table 2: Official task results.
</tableCaption>
<table confidence="0.9998466">
EN IT
Run all NE WSD a all NE WSD a
Run1 61.3 88.1 59.5 48.2 59.5 51.0 59.9 77.7
Run2 61.0 85.2 59.3 47.6 59.6 51.0 60.0 77.7
Run3 60.8 84.4 59.2 47.6 59.5 51.0 59.9 77.7
</table>
<tableCaption confidence="0.999781">
Table 3: Task results after the adjective fix.
</tableCaption>
<bodyText confidence="0.999861774193548">
of entities retrieved by the search engine to 25, and
the thresholds for stop-word and lower-case filters to
0.3.
Table 2 reports the official results released by the
task organizers. Our best system ranks 4th among
17 submissions for English, and 4th among 8 for
Italian. As reported in Table 2, our system is not
scored for adjective. This issue is due to a problem
with PoS-tag: in trial data adjectives are tagged with
‘A’, while in the test data with ‘J’. Inadvertently, we
did not report this modification in our system during
the testing. After the release of the gold standard,
we fixed that issue in our system and performed a
new experiment whose results are reported in Table
3. Since results for noun, verbs and adverbs are not
affected by the fix, they are not reported again in the
table. Considering the new results reported in Table
3, our system is able to rank 3rd for English, and 2nd
for Italian.
Another goal of the task is to evaluate system per-
formance on different domains. In particular three
domains were provided: biomedical (bio), maths
and computer science (math), and general domain
(gnr). Results for each domain and language are re-
ported in Table 4. Our performance on each domain
shows a trend very similar to the best system for
each language: the math/computer science domain
is the hardest to disambiguate, while the biomedi-
cal one seems to be the easiest. A deep analysis of
domain results shows that our system is the best to
disambiguate named entities for Italian biomedical
</bodyText>
<table confidence="0.995155166666667">
EN IT
Run bio math gnr bio math gnr
best 71.2 54.1 67.2 65.5 52.1 61.0
Run1 66.6 50.8 62.0 64.4 51.2 58.4
Run2 66.4 50.8 60.7 64.4 51.2 58.7
Run3 66.4 50.8 60.2 64.4 51.2 58.4
</table>
<tableCaption confidence="0.999836">
Table 4: System performance for each domain.
</tableCaption>
<bodyText confidence="0.9996682">
and math/computer science domains, while it pro-
vides the lowest performance in the general domain
for both Italian and English. It is important to note
that the system settings seem not to affect the over-
all performance, while a deep analysis focused on
the only named entities reveals slight differences be-
tween settings. This behaviour is due to the different
methods used to recognize named entities. The task
description paper reports more details about results
(Moro and Navigli, 2015).
</bodyText>
<sectionHeader confidence="0.999429" genericHeader="conclusions">
4 Conclusions
</sectionHeader>
<bodyText confidence="0.9999938">
We presented a unified approach to entity linking
and word sense disambiguation which relies on a
distributional extension of the simple Lesk disam-
biguation algorithm. This algorithm has been ex-
tended with an entity recognition module able to rec-
ognize candidate named entities. We evaluated three
different configurations of such recognition module
within the Task 13 of SemEval-2015. Experimental
evaluation showed competitive results, with our best
run ranked among the top systems.
</bodyText>
<page confidence="0.99893">
363
</page>
<sectionHeader confidence="0.993746" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.998734611111111">
Satanjeev Banerjee and Ted Pedersen. 2002. An Adapted
Lesk Algorithm for Word Sense Disambiguation Us-
ing WordNet. In Alexander Gelbukh, editor, Com-
putational Linguistics and Intelligent Text Processing,
volume 2276 of Lecture Notes in Computer Science,
pages 136–145.
Pierpaolo Basile, Annalina Caputo, and Giovanni Semer-
aro. 2014. An Enhanced Lesk Word Sense Disam-
biguation Algorithm through a Distributional Seman-
tic Model. In Proceedings of COLING 2014, the 25th
International Conference on Computational Linguis-
tics: Technical Papers, pages 1591–1600, Dublin, Ire-
land.
Michael Lesk. 1986. Automatic Sense Disambiguation
Using Machine Readable Dictionaries: How to Tell a
Pine Cone from an Ice Cream Cone. In Proceedings
of the 5th Annual International Conference on Sys-
tems Documentation, SIGDOC ’86, pages 24–26, New
York, NY, USA.
Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey
Dean. 2013. Efficient estimation of word represen-
tations in vector space. CoRR, abs/1301.3781.
George A Miller. 1995. WordNet: a lexical database for
English. Communications of the ACM, 38(11):39–41.
Andrea Moro and Roberto Navigli. 2015. SemEval-2015
Task 13: Multilingual All-Words Sense Disambigua-
tion and Entity Linking. In Proceedings of SemEval-
2015.
Roberto Navigli and Simone Paolo Ponzetto. 2012. Ba-
belNet: The automatic construction, evaluation and
application of a wide-coverage multilingual semantic
network. Artificial Intelligence, 193:217–250.
Magnus Sahlgren. 2006. The Word-Space Model: Us-
ing distributional analysis to represent syntagmatic
and paradigmatic relations between words in high-
dimensional vector spaces. Ph.D. thesis.
</reference>
<page confidence="0.998935">
364
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.866318">
<title confidence="0.9996415">UNIBA: Combining Distributional Semantic Models and Sense Distribution for Multilingual All-Words Sense Disambiguation and Entity Linking</title>
<author confidence="0.999733">Basile Caputo</author>
<affiliation confidence="0.9996505">Department of Computer University of Bari Aldo</affiliation>
<address confidence="0.947718">Via, E. Orabona, 4 - 70125 Bari</address>
<abstract confidence="0.99243125">This paper describes the participation of the UNIBA team in the Task 13 of SemEval-2015 about Multilingual All-Words Sense Disambiguation and Entity Linking. We propose an algorithm able to disambiguate both word senses and named entities by combining the simple Lesk approach with information coming from both a distributional semantic model and usage frequency of meanings. The results for both English and Italian show satisfactory performance.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Satanjeev Banerjee</author>
<author>Ted Pedersen</author>
</authors>
<title>An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet.</title>
<date>2002</date>
<booktitle>Computational Linguistics and Intelligent Text Processing,</booktitle>
<volume>2276</volume>
<pages>136--145</pages>
<editor>In Alexander Gelbukh, editor,</editor>
<contexts>
<context position="7385" citStr="Banerjee and Pedersen, 2002" startWordPosition="1189" endWordPosition="1192">ments. The corpus plays a key role since the richer it is the higher is the probability that each word is fully represented in all its contexts of use. We exploit the word2vec tool2(Mikolov et al., 2013) in order to build a DSM, by analyzing all the pages in the last English/Italian Wikipedia Dump. The correct sense for a word is the one whose gloss maximizes the semantic similarity with the word/entity context. The sense description can still be too short for a meaningful comparison with the word/entity context. Following this observation, we adopted an approach inspired by the adapted Lesk (Banerjee and Pedersen, 2002), and 2https://code.google.com/p/word2vec/ 361 we decided to enrich the gloss of the sense with those of related meanings, duly weighted to reflect their distances with respect to the original sense. The algorithm consists of the following steps. Building the glosses. We retrieve the set Si = {si1, si2, ..., sik} of senses associated to the word/entity wi. For named entities such a set is provided by the entity recognition module, while for words the set is obtained by firstly looking up to the WordNet portion of BabelNet, then if no sense is found we seek for senses from Wikipedia. For each s</context>
</contexts>
<marker>Banerjee, Pedersen, 2002</marker>
<rawString>Satanjeev Banerjee and Ted Pedersen. 2002. An Adapted Lesk Algorithm for Word Sense Disambiguation Using WordNet. In Alexander Gelbukh, editor, Computational Linguistics and Intelligent Text Processing, volume 2276 of Lecture Notes in Computer Science, pages 136–145.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pierpaolo Basile</author>
<author>Annalina Caputo</author>
<author>Giovanni Semeraro</author>
</authors>
<title>An Enhanced Lesk Word Sense Disambiguation Algorithm through a Distributional Semantic Model.</title>
<date>2014</date>
<booktitle>In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,</booktitle>
<pages>1591--1600</pages>
<location>Dublin, Ireland.</location>
<contexts>
<context position="1554" citStr="Basile et al., 2014" startWordPosition="218" endWordPosition="221">resentation of text through linking of both words and entities with concepts in a knowledge base. Besides the traditional difficulties of word sense disambiguation, this task requires specific methods able to tackle the challenges posed by the named entity recognition, disambiguation and linking steps. This paper proposes a unified strategy for word sense and named entity disambiguation which leverages BabelNet, a multilingual resource that encompasses both encyclopedic and lexicographic knowledge (Navigli and Ponzetto, 2012). Our approach relies on the Distributional Lesk (DL-WSD) algorithm (Basile et al., 2014), which is able to disambiguate a word occurrence by computing the similarity between word context and the glosses associated with all possible word meanings. Such a similarity is computed through a Distributional Semantic Model (DSM) (Sahlgren, 2006). In this work we describe an extension of the DL-WSD algorithm that exploits a specific module for entity discovery given a list of possible surface forms. In particular, we build an index in which each surface form (i.e. candidate entity) is paired to the list of all its possible meanings in a semantic network. This index of surface forms is exp</context>
<context position="6060" citStr="Basile et al. (2014)" startWordPosition="971" endWordPosition="974">ative surface forms {European, Union, EU, E.U.}. Then, we add all the BabelSynsets of “European Union” to the list of possible meanings of those words that follow the candidate entity and belong to the set of alternative forms. The output of the entity recognition module is a list of candidate entities in which a set of possible meanings (BabelSynset) is assigned to each surface form in the list. The set of named entities extracted by this module and the list of all the words in the text are the input to the DL-WSD algorithm. 2.2 DL-WSD We exploit the distributional Lesk algorithm proposed by Basile et al. (2014) for disambiguating words and named entities. The algorithm replaces the concept of word overlap initially introduced by (Lesk, 1986) with the broader concept of semantic similarity computed in a distributional semantic space. Let w1, w2, ...wr,, be a sequence of words/entities, the algorithm disambiguates each target word/entity wz by computing the semantic similarity between the glosses of senses associated with the target word/entity and its context. This similarity is computed by representing in a DSM both the gloss and the context as the sum of words they are composed of; then this simila</context>
</contexts>
<marker>Basile, Caputo, Semeraro, 2014</marker>
<rawString>Pierpaolo Basile, Annalina Caputo, and Giovanni Semeraro. 2014. An Enhanced Lesk Word Sense Disambiguation Algorithm through a Distributional Semantic Model. In Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1591–1600, Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Lesk</author>
</authors>
<title>Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone.</title>
<date>1986</date>
<booktitle>In Proceedings of the 5th Annual International Conference on Systems Documentation, SIGDOC ’86,</booktitle>
<pages>24--26</pages>
<location>New York, NY, USA.</location>
<contexts>
<context position="6193" citStr="Lesk, 1986" startWordPosition="992" endWordPosition="993">se words that follow the candidate entity and belong to the set of alternative forms. The output of the entity recognition module is a list of candidate entities in which a set of possible meanings (BabelSynset) is assigned to each surface form in the list. The set of named entities extracted by this module and the list of all the words in the text are the input to the DL-WSD algorithm. 2.2 DL-WSD We exploit the distributional Lesk algorithm proposed by Basile et al. (2014) for disambiguating words and named entities. The algorithm replaces the concept of word overlap initially introduced by (Lesk, 1986) with the broader concept of semantic similarity computed in a distributional semantic space. Let w1, w2, ...wr,, be a sequence of words/entities, the algorithm disambiguates each target word/entity wz by computing the semantic similarity between the glosses of senses associated with the target word/entity and its context. This similarity is computed by representing in a DSM both the gloss and the context as the sum of words they are composed of; then this similarity takes into account the co-occurrence evidences previously collected through a corpus of documents. The corpus plays a key role s</context>
</contexts>
<marker>Lesk, 1986</marker>
<rawString>Michael Lesk. 1986. Automatic Sense Disambiguation Using Machine Readable Dictionaries: How to Tell a Pine Cone from an Ice Cream Cone. In Proceedings of the 5th Annual International Conference on Systems Documentation, SIGDOC ’86, pages 24–26, New York, NY, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Tomas Mikolov</author>
<author>Kai Chen</author>
<author>Greg Corrado</author>
<author>Jeffrey Dean</author>
</authors>
<title>Efficient estimation of word representations in vector space.</title>
<date>2013</date>
<location>CoRR, abs/1301.3781.</location>
<contexts>
<context position="6960" citStr="Mikolov et al., 2013" startWordPosition="1118" endWordPosition="1121"> the algorithm disambiguates each target word/entity wz by computing the semantic similarity between the glosses of senses associated with the target word/entity and its context. This similarity is computed by representing in a DSM both the gloss and the context as the sum of words they are composed of; then this similarity takes into account the co-occurrence evidences previously collected through a corpus of documents. The corpus plays a key role since the richer it is the higher is the probability that each word is fully represented in all its contexts of use. We exploit the word2vec tool2(Mikolov et al., 2013) in order to build a DSM, by analyzing all the pages in the last English/Italian Wikipedia Dump. The correct sense for a word is the one whose gloss maximizes the semantic similarity with the word/entity context. The sense description can still be too short for a meaningful comparison with the word/entity context. Following this observation, we adopted an approach inspired by the adapted Lesk (Banerjee and Pedersen, 2002), and 2https://code.google.com/p/word2vec/ 361 we decided to enrich the gloss of the sense with those of related meanings, duly weighted to reflect their distances with respec</context>
</contexts>
<marker>Mikolov, Chen, Corrado, Dean, 2013</marker>
<rawString>Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. CoRR, abs/1301.3781.</rawString>
</citation>
<citation valid="true">
<authors>
<author>George A Miller</author>
</authors>
<title>WordNet: a lexical database for English.</title>
<date>1995</date>
<journal>Communications of the ACM,</journal>
<volume>38</volume>
<issue>11</issue>
<contexts>
<context position="9641" citStr="Miller, 1995" startWordPosition="1578" endWordPosition="1579">context C and each extended gloss g�ij are represented as vectors in the SemanticSpace built through the DSM. Sense ranking. The algorithm computes the cosine similarity between the vector representation of each extended gloss g�ij and that of the context C. Then, the cosine similarity is linearly combined with a function which takes into account the usage of the meaning in the language. We analyse a function that computes the probability assigned to each synset given a word/named entity as follows: Word. We exploit a synset-tagged corpus and we attempt to map each word occurrence to WordNet (Miller, 1995). Then, we select the WordNet sysnet with the maximum probability. Named Entity. We retrieve from BabelNet the Wikipedia title pages related to the BabelSynset and count the number of times a Wikipedia page is linked from another page. In this way we use Wikipedia as a synset-tagged corpus. We define the probability p(sij|wi) that takes into account the sense distribution of sij given the word/entity wi. The sense distribution is computed as the number of times the word/entity wi is tagged with the sense. Zero probabilities are avoided by introducing an additive (Laplace) smoothing. The probab</context>
</contexts>
<marker>Miller, 1995</marker>
<rawString>George A Miller. 1995. WordNet: a lexical database for English. Communications of the ACM, 38(11):39–41.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Andrea Moro</author>
<author>Roberto Navigli</author>
</authors>
<date>2015</date>
<booktitle>SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking. In Proceedings of SemEval2015.</booktitle>
<contexts>
<context position="876" citStr="Moro and Navigli, 2015" startWordPosition="116" endWordPosition="119">oro Via, E. Orabona, 4 - 70125 Bari (Italy) {pierpaolo.basile,annalina.caputo,giovanni.semeraro}@uniba.it Abstract This paper describes the participation of the UNIBA team in the Task 13 of SemEval-2015 about Multilingual All-Words Sense Disambiguation and Entity Linking. We propose an algorithm able to disambiguate both word senses and named entities by combining the simple Lesk approach with information coming from both a distributional semantic model and usage frequency of meanings. The results for both English and Italian show satisfactory performance. 1 Introduction SemEval-2015 Task 13 (Moro and Navigli, 2015) aims to evaluate systems that provide a comprehensive representation of text through linking of both words and entities with concepts in a knowledge base. Besides the traditional difficulties of word sense disambiguation, this task requires specific methods able to tackle the challenges posed by the named entity recognition, disambiguation and linking steps. This paper proposes a unified strategy for word sense and named entity disambiguation which leverages BabelNet, a multilingual resource that encompasses both encyclopedic and lexicographic knowledge (Navigli and Ponzetto, 2012). Our appro</context>
</contexts>
<marker>Moro, Navigli, 2015</marker>
<rawString>Andrea Moro and Roberto Navigli. 2015. SemEval-2015 Task 13: Multilingual All-Words Sense Disambiguation and Entity Linking. In Proceedings of SemEval2015.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Roberto Navigli</author>
<author>Simone Paolo Ponzetto</author>
</authors>
<title>BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network.</title>
<date>2012</date>
<journal>Artificial Intelligence,</journal>
<pages>193--217</pages>
<contexts>
<context position="1465" citStr="Navigli and Ponzetto, 2012" startWordPosition="204" endWordPosition="207">-2015 Task 13 (Moro and Navigli, 2015) aims to evaluate systems that provide a comprehensive representation of text through linking of both words and entities with concepts in a knowledge base. Besides the traditional difficulties of word sense disambiguation, this task requires specific methods able to tackle the challenges posed by the named entity recognition, disambiguation and linking steps. This paper proposes a unified strategy for word sense and named entity disambiguation which leverages BabelNet, a multilingual resource that encompasses both encyclopedic and lexicographic knowledge (Navigli and Ponzetto, 2012). Our approach relies on the Distributional Lesk (DL-WSD) algorithm (Basile et al., 2014), which is able to disambiguate a word occurrence by computing the similarity between word context and the glosses associated with all possible word meanings. Such a similarity is computed through a Distributional Semantic Model (DSM) (Sahlgren, 2006). In this work we describe an extension of the DL-WSD algorithm that exploits a specific module for entity discovery given a list of possible surface forms. In particular, we build an index in which each surface form (i.e. candidate entity) is paired to the li</context>
</contexts>
<marker>Navigli, Ponzetto, 2012</marker>
<rawString>Roberto Navigli and Simone Paolo Ponzetto. 2012. BabelNet: The automatic construction, evaluation and application of a wide-coverage multilingual semantic network. Artificial Intelligence, 193:217–250.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Magnus Sahlgren</author>
</authors>
<title>The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces.</title>
<date>2006</date>
<tech>Ph.D. thesis.</tech>
<contexts>
<context position="1805" citStr="Sahlgren, 2006" startWordPosition="259" endWordPosition="260"> recognition, disambiguation and linking steps. This paper proposes a unified strategy for word sense and named entity disambiguation which leverages BabelNet, a multilingual resource that encompasses both encyclopedic and lexicographic knowledge (Navigli and Ponzetto, 2012). Our approach relies on the Distributional Lesk (DL-WSD) algorithm (Basile et al., 2014), which is able to disambiguate a word occurrence by computing the similarity between word context and the glosses associated with all possible word meanings. Such a similarity is computed through a Distributional Semantic Model (DSM) (Sahlgren, 2006). In this work we describe an extension of the DL-WSD algorithm that exploits a specific module for entity discovery given a list of possible surface forms. In particular, we build an index in which each surface form (i.e. candidate entity) is paired to the list of all its possible meanings in a semantic network. This index of surface forms is exploited to look up all candidate entities in a text. The rest of this paper is structured as follows: Section 2 provides details about the adopted strategy, and describes the two main steps: 1) Entity Recognition and 2) Disambiguation. An experimental </context>
</contexts>
<marker>Sahlgren, 2006</marker>
<rawString>Magnus Sahlgren. 2006. The Word-Space Model: Using distributional analysis to represent syntagmatic and paradigmatic relations between words in highdimensional vector spaces. Ph.D. thesis.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>