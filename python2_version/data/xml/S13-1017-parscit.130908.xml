<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.008651">
<title confidence="0.7734115">
ECNUCS: Measuring Short Text Semantic Equivalence Using Multiple
Similarity Measurements
</title>
<author confidence="0.99712">
Tian Tian ZHU Man LAN*
</author>
<affiliation confidence="0.958909666666667">
Department of Computer Science and Department of Computer Science and
Technology Technology
East China Normal University East China Normal University
</affiliation>
<email confidence="0.992651">
51111201046@student.ecnu.edu.cn mlan@cs.ecnu.edu.cn
</email>
<sectionHeader confidence="0.998523" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.997529533333333">
This paper reports our submissions to the
Semantic Textual Similarity (STS) task in
*SEM Shared Task 2013. We submitted three
Support Vector Regression (SVR) systems in
core task, using 6 types of similarity mea-
sures, i.e., string similarity, number similar-
ity, knowledge-based similarity, corpus-based
similarity, syntactic dependency similarity and
machine translation similarity. Our third sys-
tem with different training data and different
feature sets for each test data set performs the
best and ranks 35 out of 90 runs. We also sub-
mitted two systems in typed task using string
based measure and Named Entity based mea-
sure. Our best system ranks 5 out of 15 runs.
</bodyText>
<sectionHeader confidence="0.999518" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9995774375">
The task of semantic textual similarity (STS) is to
measure the degree of semantic equivalence between
two sentences, which plays an increasingly impor-
tant role in natural language processing (NLP) ap-
plications. For example, in text categorization (Yang
and Wen, 2007), two documents which are more
similar are more likely to be grouped in the same
class. In information retrieval (Sahami and Heil-
man, 2006), text similarity improves the effective-
ness of a semantic search engine by providing in-
formation which holds high similarity with the input
query. In machine translation (Kauchak and Barzi-
lay, 2006), sentence similarity can be applied for
automatic evaluation of the output translation and
the reference translations. In question answering
(Mohler and Mihalcea, 2009), once the question and
</bodyText>
<page confidence="0.986482">
124
</page>
<bodyText confidence="0.999945212121212">
the candidate answers are treated as two texts, the
answer text which has a higher relevance with the
question text may have higher probability to be the
right one.
The STS task in *SEM Shared Task 2013 consists
of two subtasks, i.e., core task and typed task, and
we participate in both of them. The core task aims
to measure the semantic similarity of two sentences,
resulting in a similarity score which ranges from 5
(semantic equivalence) to 0 (no relation). The typed
task is a pilot task on typed-similarity between semi-
structured records. The types of similarity to be
measured include location, author, people involved,
time, events or actions, subject and description as
well as the general similarity of two texts (Agirre et
al., 2013).
In this work we present a Support Vector Re-
gression (SVR) system to measure sentence seman-
tic similarity by integrating multiple measurements,
i.e., string similarity, knowledge based similarity,
corpus based similarity, number similarity and ma-
chine translation metrics. Most of these similari-
ties are borrowed from previous work, e.g., (B¨ar et
al., 2012), (ˇSaric et al., 2012) and (de Souza et al.,
2012). We also propose a novel syntactic depen-
dency similarity. Our best system ranks 35 out of
90 runs in core task and ranks 5 out of 15 runs in
typed task.
The rest of this paper is organized as follows. Sec-
tion 2 describes the similarity measurements used in
this work in detail. Section 3 presents experiments
and the results of two tasks. Conclusions and future
work are given in Section 4.
</bodyText>
<note confidence="0.9321165">
Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference
and the Shared Task, pages 124–131, Atlanta, Georgia, June 13-14, 2013. c�2013 Association for Computational Linguistics
</note>
<sectionHeader confidence="0.918519" genericHeader="method">
2 Text Similarity Measurements
</sectionHeader>
<bodyText confidence="0.999965538461539">
To compute semantic textual similarity, previous
work has adopted multiple semantic similarity mea-
surements. In this work, we adopt 6 types of
measures, i.e., string similarity, number similarity,
knowledge-based similarity, corpus-based similar-
ity, syntactic dependency similarity and machine
translation similarity. Most of them are borrowed
from previous work due to their superior perfor-
mance reported. Besides, we also propose two syn-
tactic dependency similarity measures. Totally we
get 33 similarity measures. Generally, these simi-
larity measures are represented as numerical values
and combined using regression model.
</bodyText>
<subsectionHeader confidence="0.991889">
2.1 Preprocessing
</subsectionHeader>
<bodyText confidence="0.999699285714286">
Generally, we perform text preprocessing before we
compute each text similarity measurement. Firstly,
Stanford parser1 is used for sentence tokenization
and parsing. Specifically, the tokens n’t and ’m are
replaced with not and am. Secondly, Stanford POS
Tagger2 is used for POS tagging. Thirdly, Natu-
ral Language Toolkit3 is used for WordNet based
Lemmatization, which lemmatizes the word to its
nearest base form that appears in WordNet, for ex-
ample, was is lemmatized as is, not be.
Given two short texts or sentences s1 and s2, we
denote the word set of s1 and s2 as S1 and S2, the
length (i.e., number of words) of s1 and s2 as |S1|
and |S2|.
</bodyText>
<subsectionHeader confidence="0.999965">
2.2 String Similarity
</subsectionHeader>
<bodyText confidence="0.996384">
Intuitively, if two sentences share more strings, they
are considered to have higher semantic similarity.
Therefore, we create 12 string based features in con-
sideration of the common sequence shared by two
texts.
Longest Common sequence (LCS). The widely
used LCS is proposed by (Allison and Dix, 1986),
which is to find the maximum length of a com-
mon subsequence of two strings and here the sub-
sequence need to be contiguous. In consideration of
the different length of two texts, we compute LCS
</bodyText>
<footnote confidence="0.999717666666667">
1http://nlp.stanford.edu/software/lex-parser.shtml
2http://nlp.stanford.edu/software/tagger.shtml
3http://nltk.org/
</footnote>
<equation confidence="0.9408712">
similarity using Formula (1) as follows:
SimLCS =
Length of LCS
(1)
min(|S1|,|S2|)
</equation>
<bodyText confidence="0.999803444444444">
In order to eliminate the impacts of various forms
of word, we also compute a Lemma LCS similarity
score after sentences being lemmatized.
word n-grams. Following (Lyon et al., 2001), we
calculate the word n-grams similarity using the Jac-
card coefficient as shown in Formula (2), where p is
the number of n-grams shared by s1 and s2, q and r
are the number of n-grams not shared by s1 and s2,
respectively.
</bodyText>
<equation confidence="0.991110333333333">
()
Jacc = p 2
p + q + r
</equation>
<bodyText confidence="0.999524">
Since we focus on short texts, here only n=1,2,3,4
is used in this work. Similar with LCS, we also com-
pute a Lemma n-grams similarity score.
Weighted Word Overlap (WWO). (ˇSaric et al.,
2012) pointed out that when measuring sentence
similarity, different words may convey different con-
tent information. Therefore, we consider to assign
more importance to those words bearing more con-
tent information. To measure the importance of each
word, we use Formula (3) to calculate the informa-
tion content for each word w:
</bodyText>
<equation confidence="0.997872666666667">
ic(w) = ln ∑w′EC f (w′
) (3)
freq(w)
</equation>
<bodyText confidence="0.9998425">
where C is the set of words in the corpus and
freq(w) is the frequency of the word w in the cor-
pus. To compute ic(w), we use the Web 1T 5-gram
Corpus4, which is generated from approximately
one trillion word tokens of text from Web pages.
Obviously, the WWO scores between two sen-
tences is non-symmetric. The WWO of s2 by s1 is
given by Formula (4):
</bodyText>
<equation confidence="0.97141575">
∑
wES,nS2 ic(w)
Simwwo(s1, s2) = ∑w′ES2 ic(w′)
(4)
</equation>
<bodyText confidence="0.99392175">
Likewise, we can get Simwwo(s2, s1) score.
Then the final WWO score is the harmonic mean of
Simwwo(s1, s2) and Simwwo(s2, s1). Similarly, we
get a Lemma WWO score as well.
</bodyText>
<footnote confidence="0.928442">
4http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13
</footnote>
<page confidence="0.994276">
125
</page>
<subsectionHeader confidence="0.997325">
2.3 Knowledge Based Similarity
</subsectionHeader>
<bodyText confidence="0.999371032258064">
Knowledge based similarity approaches rely on
a semantic network of words. In this work
all knowledge-based word similarity measures are
computed based on WordNet. For word similarity,
we employ four WordNet-based similarity metrics:
the Path similarity (Banea et al., 2012); the WUP
similarity (Wu and Palmer, 1994); the LCH similar-
ity (Leacock and Chodorow, 1998); the Lin similar-
ity (Lin, 1998). We adopt the NLTK library (Bird,
2006) to compute all these word similarities.
In order to determine the similarity of sentences,
we employ two strategies to convert the word simi-
larity into sentence similarity, i.e., (1) the best align-
ment strategy (align) (Banea et al., 2012) and (2) the
aggregation strategy (agg) (Mihalcea et al., 2006).
The best alignment strategy is computed as below:
(5)
where ω is the number of shared terms between s1
and s2, list cp contains the similarities of non-shared
words in shorter text, cpi is the highest similarity
score of the ith word among all words of the longer
text. The aggregation strategy is calculated as be-
low:
(6)
where maxSim(w, S2) is the highest WordNet-
based score between word w and all words of sen-
tence S2. To compute ic(w), we use the same cor-
pus as WWO, i.e., the Web 1T 5-gram Corpus. The
final score of the aggregation strategy is the mean of
Simagg(s1, s2) and Simagg(s2, s1). Finally we get
8 knowledge based features.
</bodyText>
<subsectionHeader confidence="0.996915">
2.4 Corpus Based Similarity
</subsectionHeader>
<bodyText confidence="0.936124857142857">
Latent Semantic Analysis (LSA) (Landauer et al.,
1997). In LSA, term-context associations are cap-
tured by means of a dimensionality reduction op-
eration performing singular value decomposition
(SVD) on the term-by-context matrix T, where T
is induced from a large corpus. We use the TASA
corpus5 to obtain the matrix and compute the word
</bodyText>
<footnote confidence="0.784374">
5http://lsa.colorado.edu/
</footnote>
<bodyText confidence="0.998824">
similarity using cosine similarity of the two vectors
of the words. After that we transform word similar-
ity to sentence similarity based on Formula (5).
</bodyText>
<sectionHeader confidence="0.41761" genericHeader="method">
Co-occurrence Retrieval Model (CRM) (Weeds,
</sectionHeader>
<bodyText confidence="0.998158909090909">
2003). CRM is based on a notion of substitutabil-
ity. That is, the more appropriate it is to substitute
word w1 in place of word w2 in a suitable natural
language task, the more semantically similar they
are. The degree of substitutability of w2 with w1
is dependent on the proportion of co-occurrences of
w1 that are also the co-occurrences of w2, and the
proportion of co-occurrences of w2 that are also the
co-occurrences of w1. Following (Weeds, 2003), the
CRM word similarity is computed using Formula
(7):
</bodyText>
<equation confidence="0.9987735">
SimCRM(w1, w2) = 2 */ |c(w1) n c(w2)  |(7)
c(w1)  |+ |c(w2)|
</equation>
<bodyText confidence="0.999979533333333">
where c(w) is the set of words that co-occur with
w. We use the 5-gram part of the Web 1T 5-gram
Corpus to obtain c(w). If two words appear in one
5-gram, we will treat one word as the co-occurring
word of each other. To obtain c(w), we propose two
methods. In the first CRM similarity, we only con-
sider the word w with |c(w) |&gt; 200, and then take
the top 200 co-occurring words ranked by the co-
occurrence frequency as its c(w). To relax restric-
tions, we also present an extended CRM (denoted
by ExCRM), which extends the CRM list that all w
with |c(w) |&gt; 50 are taken into consideration, but
the maximum of |c(w) |is still set to 200. Finally,
these two CRM word similarity measures are trans-
formed to sentence similarity using Formula (5).
</bodyText>
<subsectionHeader confidence="0.994652">
2.5 Syntactic Dependency Similarity
</subsectionHeader>
<bodyText confidence="0.950570416666667">
As (ˇSaric et al., 2012) pointed out that dependency
relations of sentences often contain semantic infor-
mation, in this work we propose two novel syntactic
dependency similarity features to capture their pos-
sible semantic similarity.
Simple Dependency Overlap. First we measure the
simple dependency overlap between two sentences
based on matching dependency relations. Stanford
Parser provides 53 dependency relations, for exam-
ple:
nsubj(remain − 16, leader − 4)
dobj(return − 10, home − 11)
</bodyText>
<equation confidence="0.9996976">
Simalign(s1, s2) = |S1 |+ |S2|
(ω + ∑|&apos;|
i=1 cpi) * (2|S1||S2|)
Simagg(s1, s2) = ∑wE{S1} ic(w)
∑wES1(maxSim(w, S2) * ic(w))
</equation>
<page confidence="0.984476">
126
</page>
<bodyText confidence="0.99739264">
where nsubj (nominal subject) and dobj (direct ob-
ject) are two dependency types, remain is the gov-
erning lemma and leader is the dependent lemma.
Two syntactic dependencies are considered equal
when they have the same dependency type, govern-
ing lemma, and dependent lemma.
Let R1 and R2 be the set of all dependency rela-
tions in s1 and s2, we compute Simple Dependency
Overlap using Formula (8):
(8)
Special Dependency Overlap. Several types of de-
pendency relations are believed to contain the pri-
mary content of a sentence. So we extract three roles
from those special dependency relations, i.e., pred-
icate, subject and object. For example, from above
dependency relation dobj, we can extract the object
of the sentence, i.e., home. For each of these three
roles, we get a similarity score. For example, to cal-
culate Simpredicate, we denote the sets of predicates
of two sentences as Sp1 and Sp2. We first use LCH to
compute word similarity and then compute sentence
similarity using Formula (5). Similarly, the Simsubj
and Simobj are obtained in the same way. In the end
we average the similarity scores of the three roles as
the final Special Dependency Overlap score.
</bodyText>
<subsectionHeader confidence="0.984677">
2.6 Number Similarity
</subsectionHeader>
<bodyText confidence="0.999917">
Numbers in the sentence occasionally carry similar-
ity information. If two sentences contain different
sets of numbers even though their sentence structure
is quite similar, they may be given a low similarity
score. Here we adopt two features following (ˇSaric
et al., 2012), which are computed as follow:
</bodyText>
<equation confidence="0.999525">
log(1 + |N1 |+ |N2|) (9)
2 * |N1 n N2|/(|N1 |+ |N2|) (10)
</equation>
<bodyText confidence="0.9998185">
where N1 and N2 are the sets of all numbers in s1
and s2. We extract the number information from
sentences by checking if the POS tag is CD (cardinal
number).
</bodyText>
<subsectionHeader confidence="0.960644">
2.7 Machine Translation Similarity
</subsectionHeader>
<bodyText confidence="0.998506636363636">
Machine translation (MT) evaluation metrics are de-
signed to assess whether the output of a MT sys-
tem is semantically equivalent to a set of reference
translations. The two given sentences can be viewed
as one input and one output of a MT system, then
the MT measures can be used to measure their se-
mantic similarity. We use the following 6 lexical
level metrics (de Souza et al., 2012): WER, TER,
PER, NIST, ROUGE-L, GTM-1. All these measures
are obtained using the Asiya Open Toolkit for Auto-
matic Machine Translation (Meta-) Evaluation6.
</bodyText>
<sectionHeader confidence="0.995031" genericHeader="evaluation">
3 Experiment and Results
</sectionHeader>
<subsectionHeader confidence="0.930002">
3.1 Regression Model
</subsectionHeader>
<bodyText confidence="0.999986285714286">
We adopt LIBSVM7 to build Support Vector Regres-
sion (SVR) model for regression. To obtain the op-
timal SVR parameters C, g, and p, we employ grid
search with 10-fold cross validation on training data.
Specifically, if the score returned by the regression
model is bigger than 5 or less than 0, we normalize
it as 5 or 0, respectively.
</bodyText>
<subsectionHeader confidence="0.999766">
3.2 Core Task
</subsectionHeader>
<bodyText confidence="0.99992564">
The organizers provided four different test sets to
evaluate the performance of the submitted systems.
We have submitted three systems for core task, i.e.,
Run 1, Run 2 and Run 3. Run 1 is trained on all
training data sets with all features except the num-
ber based features, because most of the test data do
not contain number. Run 2 uses the same feature sets
as Run 1 but different training data sets for different
test data as listed in Table 1, where different training
data sets are combined together as they have simi-
lar structures with the test data. Run 3 uses different
feature sets as well as different training data sets for
each test data. Table 2 shows the best feature sets
used for each test data set, where “+” means the fea-
ture is selected and “-” means not selected. We did
not use the whole feature set because in our prelimi-
nary experiments, some features performed not well
on some training data sets, and they even reduced
the performance of our system. To select features,
we trained two SVR models for each feature, one
with all features and another with all features except
this feature. If the first model outperforms the sec-
ond model, this feature is chosen.
Table 3 lists the performance of these three sys-
tems as well as the baseline and the best results on
</bodyText>
<footnote confidence="0.9884495">
6http://nlp.lsi.upc.edu/asiya/
7http://www.csie.ntu.edu.tw/ cjlin/libsvm/
</footnote>
<equation confidence="0.991586">
SimSimDep(s1, s2) = |R1 |+ |R2|
2 * |R1 n R2 |* |R1||R2|
</equation>
<page confidence="0.992561">
127
</page>
<table confidence="0.9735985">
Test Training
Headline MSRpar
OnWN+FNWN MSRpar+OnWN
SMT SMTnews+SMTeuroparl
</table>
<tableCaption confidence="0.99819">
Table 1: Different training data sets used for each test data set
</tableCaption>
<figure confidence="0.969618208333333">
type Features Headline OnWN and FNWN SMT
LCS + + -
Lemma LCS + + -
String N-gram + 1+2gram 1gram
Based Lemma N-gram + 1+2gram 1gram
WWO + + +
Lemma WWO + + +
Path,WUP,LCH,Lin + + +
Knowledge +aligh
Based Path,WUP,LCH,Lin + + +
+ic-weighted
Corpus LSA + + +
Based CRM,ExCRM + + +
Simple Dependency + + +
Syntactic Overlap
Dependency Special Dependency + - +
Overlap
Number Number + - -
WER - + +
TER - + +
PER + + +
MT NIST + + -
ROUGE-L + + +
GTM-1 + + +
</figure>
<tableCaption confidence="0.987511">
Table 2: Best feature combination for each data set
</tableCaption>
<table confidence="0.999030333333333">
System Mean Headline OnWN FNWN SMT
Best 0.6181 0.7642 0.7529 0.5818 0.3804
Baseline 0.3639 0.5399 0.2828 0.2146 0.2861
Run 1 0.3533 0.5656 0.2083 0.1725 0.2949
Run 2 0.4720 0.7120 0.5388 0.2013 0.2504
Run 3 (rank 35) 0.4967 0.6799 0.5284 0.2203 0.3595
</table>
<tableCaption confidence="0.999571">
Table 3: Final results on STS core task
</tableCaption>
<bodyText confidence="0.98487575">
STS core task in ∗SEM Shared Task 2013. For the
three runs we submitted to the task organizers, Run
3 performs the best results and ranks 35 out of 90
runs. Run 2 performs much better than Run 1. It in-
dicates that using different training data sets for dif-
ferent test sets indeed improves results. Run 3 out-
performs Run 2 and Run 1. It shows that our feature
selection process for each test data set does help im-
</bodyText>
<page confidence="0.996414">
128
</page>
<bodyText confidence="0.999968357142857">
prove the performance too. From this table, we find
that different features perform different on different
kinds of data sets and thus using proper feature sub-
sets for each test data set would make improvement.
Besides, results on the four test data sets are quite
different. Headline always gets the best result on
each run and OnWN follows second. And results
of FNWN and SMT are much lower than Headline
and OnWN. One reason of the poor performance of
FNWN may be the big length difference of sentence
pairs. That is, sentence from WordNet is short while
sentence from FrameNet is quite longer, and some
samples even have more than one sentence (e.g. “do-
ing as one pleases or chooses” VS “there exist a
number of different possible events that may happen
in the future in most cases, there is an agent involved
who has to consider which of the possible events will
or should occur a salient entity which is deeply in-
volved in the event may also be mentioned”). As
a result, even though the two sentences are similar
in meaning, most of our measures would give low
scores due to quite different sentence length.
In order to understand the contributions of each
similarity measurement, we trained 6 SVR regres-
sion models based on 6 types on MSRpar data set.
Table 4 presents the Pearson’s correlation scores
of the 6 types of measurements on MSRpar. We
can see that the corpus-based measure achieves the
best, then the knowledge-based measure and the MT
measure follow. Number similarity performs sur-
prisingly well, which benefits from the property of
data set that MSRpar contains many numbers in sen-
tences and the sentence similarity depends a lot on
those numbers as well. The string similarity is not
as good as the knowledge-based, the corpus-based
and the MT similarity because of its disability of ex-
tracting semantic characteristics of sentence. Sur-
prisingly, the Syntactic dependency similarity per-
forms the worst. Since we only extract two features
based on sentence dependency, they may not enough
to capture the key semantic similarity information
from the sentences.
</bodyText>
<subsectionHeader confidence="0.999456">
3.3 Typed Task
</subsectionHeader>
<bodyText confidence="0.99791625">
For typed task, we also adopt a SVR model for
each type. Since several previous similarity mea-
sures used for core task are not suitable for evalu-
ation of the similarity of people involved, time pe-
</bodyText>
<table confidence="0.996239857142857">
Features results
string 0.4757
knowledge-based 0.5640
corpus-based 0.5842
syntactic dependency 0.3528
number 0.5278
MT metrics 0.5595
</table>
<tableCaption confidence="0.99571">
Table 4: Pearson correlation of features of the six aspects
on MSRpar
</tableCaption>
<bodyText confidence="0.999623714285714">
riod, location and event or action involved, we add
two Named Entity Recognition (NER) based fea-
tures. Firstly we use Stanford NERB to obtain per-
son, location and date information from the whole
text with NER tags of “PERSON”, “LOCATION”
and “DATE”. Then for each list of entity, we get two
feature values using the following two formulas:
</bodyText>
<equation confidence="0.917382166666667">
SimNER Num(L1NER, L2NER) =
min(|L1NER|, |L2NER|) (11)
max(|L1NER|, |L2NER|)
SimNER(L1NER,L2NER) = |L1NER |* |L2NER|
Num(equalpairs)
(12)
</equation>
<bodyText confidence="0.999959789473684">
where LNER is the list of one entity type from
the text, and for two lists of NERs L1NER and
L2NER, there are |L1NER |* |L2NER |NER pairs.
Num(equalpairs) is the number of equal pairs.
Here we expand the condition of equivalence: two
NERs are considered equal if one is part of another
(e.g. “John Warson” VS “Warson”). Features and
content we used for each similarity are presented in
Table 5. For the three similarities: people involved,
time period, location, we compute the two NER
based features for each similarity with NER type of
“PERSON”, “LOCATION” and “DATE”. And for
event or action involved, we add the above 6 NER
feature scores as its feature set. The NER based sim-
ilarity used in description is the same as event or ac-
tion involved but only based on “dcDescription” part
of text. Besides, we add a length feature in descrip-
tion, which is the ratio of shorter length and longer
length of descriptions.
</bodyText>
<footnote confidence="0.957801">
8http://nlp.stanford.edu/software/CRF-NER.shtml
</footnote>
<page confidence="0.986209">
129
</page>
<table confidence="0.999770444444445">
Type Features Content used
author string based (+ knowledge based for Run2) dcCreator
people involved NER based whole text
time period NER based whole text
location NER based whole text
event or action involved NER based whole text
subject string based (+ knowledge based for Run2) dcSubject
description string based, NER based,length dcDescription
General the 7 similarities above
</table>
<tableCaption confidence="0.999493">
Table 5: Feature sets and content used of 8 type similarities of Typed data
</tableCaption>
<bodyText confidence="0.999991923076923">
We have submitted two runs. Run 1 uses only
string based and NER based features. Besides fea-
tures used in Run 1, Run 2 also adds knowledge
based features. Table 6 shows the performance of
our two runs as well as the baseline and the best re-
sults on STS typed task in *SEM Shared Task 2013.
Our Run 1 ranks 5 and Run 2 ranks 7 out of 15 runs.
Run 2 performed worse than Run 1 and the possible
reason may be the knowledge based method is not
suitable for this kind of data. Furthermore, since we
only use NER based features which involves three
entities for these similarities, they are not enough to
capture the relevant information for other types.
</bodyText>
<sectionHeader confidence="0.999615" genericHeader="conclusions">
4 Conclusion
</sectionHeader>
<bodyText confidence="0.999579210526316">
In this paper we described our submissions to the
Semantic Textual Similarity Task in *SEM Shared
Task 2013. For core task, we collect 6 types of simi-
larity measures, i.e., string similarity, number sim-
ilarity, knowledge-based similarity, corpus-based
similarity, syntactic dependency similarity and ma-
chine translation similarity. And our Run 3 with dif-
ferent training data and different feature sets for each
test data set ranks 35 out of 90 runs. For typed task,
we adopt string based measure, NER based mea-
sure and knowledge based measure, our best system
ranks 5 out of 15 runs. Clearly, these similarity mea-
sures are not quite enough. For the core task, in our
future work we will consider the measures to eval-
uate the sentence difference as well. For the typed
task, with the help of more advanced IE tools to ex-
tract more information regarding different types, we
need to propose more methods to evaluate the simi-
larity.
</bodyText>
<sectionHeader confidence="0.998089" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9998981">
The authors would like to thank the organizers and
reviewers for this interesting task and their helpful
suggestions and comments, which improved the fi-
nal version of this paper. This research is supported
by grants from National Natural Science Foundation
of China (No.60903093), Shanghai Pujiang Talent
Program (No.09PJ1404500), Doctoral Fund of Min-
istry of Education of China (No.20090076120029)
and Shanghai Knowledge Service Platform Project
(No.ZF1213).
</bodyText>
<sectionHeader confidence="0.999142" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999455217391304">
Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-
Agirre, and Weiwei Guo. 2013. *sem 2013 shared
task: Semantic textual similarity, including a pilot on
typed-similarity. In *SEM 2013: The Second Joint
Conference on Lexical and Computational Semantics.
Association for Computational Linguistics.
Lloyd Allison and Trevor I Dix. 1986. A bit-string
longest-common-subsequence algorithm. Information
Processing Letters, 23(5):305–310.
Carmen Banea, Samer Hassan, Michael Mohler, and
Rada Mihalcea. 2012. Unt: A supervised synergistic
approach to semantic text similarity. pages 635–642.
First Joint Conference on Lexical and Computational
Semantics (*SEM).
Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten
Zesch. 2012. Ukp: Computing semantic textual sim-
ilarity by combining multiple content similarity mea-
sures. pages 435–440. First Joint Conference on Lex-
ical and Computational Semantics (*SEM).
Steven Bird. 2006. Nltk: the natural language toolkit. In
Proceedings of the COLING/ACL on Interactive pre-
sentation sessions, pages 69–72. Association for Com-
putational Linguistics.
</reference>
<page confidence="0.994591">
130
</page>
<table confidence="0.977136">
System general author people time location event subject description mean
Best 0.7981 0.8158 0.6922 0.7471 0.7723 0.6835 0.7875 0.7996 0.7620
Baseline 0.6691 0.4278 0.4460 0.5002 0.4835 0.3062 0.5015 0.5810 0.4894
Run 1 0.6040 0.7362 0.3663 0.4685 0.3844 0.4057 0.5229 0.6027 0.5113
Run 2 0.6064 0.5684 0.3663 0.4685 0.3844 0.4057 0.5563 0.6027 0.4948
</table>
<tableCaption confidence="0.979748">
Table 6: Final results on STS typed task
</tableCaption>
<reference confidence="0.9996204">
Jos´e Guilherme C de Souza, Matteo Negri, Trento Povo,
and Yashar Mehdad. 2012. Fbk: Machine trans-
lation evaluation and word similarity metrics for se-
mantic textual similarity. pages 624–630. First Joint
Conference on Lexical and Computational Semantics
(*SEM).
David Kauchak and Regina Barzilay. 2006. Para-
phrasing for automatic evaluation. In Proceedings of
the main conference on Human Language Technol-
ogy Conference of the North American Chapter of the
Association of Computational Linguistics, pages 455–
462. Association for Computational Linguistics.
Thomas K Landauer, Darrell Laham, Bob Rehder, and
Missy E Schreiner. 1997. How well can passage
meaning be derived without using word order? a com-
parison of latent semantic analysis and humans. In
Proceedings of the 19th annual meeting of the Cog-
nitive Science Society, pages 412–417.
Claudia Leacock and Martin Chodorow. 1998. Com-
bining local context and wordnet similarity for word
sense identification. WordNet: An electronic lexical
database, 49(2):265–283.
Dekang Lin. 1998. An information-theoretic defini-
tion of similarity. In Proceedings of the 15th inter-
national conference on Machine Learning, volume 1,
pages 296–304. San Francisco.
Caroline Lyon, James Malcolm, and Bob Dickerson.
2001. Detecting short passages of similar text in large
document collections. In Proceedings of the 2001
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 118–125.
Rada Mihalcea, Courtney Corley, and Carlo Strapparava.
2006. Corpus-based and knowledge-based measures
of text semantic similarity. In Proceedings of the na-
tional conference on artificial intelligence, volume 21,
page 775. Menlo Park, CA; Cambridge, MA; London;
AAAI Press; MIT Press; 1999.
Michael Mohler and Rada Mihalcea. 2009. Text-to-text
semantic similarity for automatic short answer grad-
ing. In Proceedings of the 12th Conference of the Eu-
ropean Chapter of the Association for Computational
Linguistics, pages 567–575. Association for Computa-
tional Linguistics.
Mehran Sahami and Timothy D Heilman. 2006. A web-
based kernel function for measuring the similarity of
short text snippets. In Proceedings of the 15th interna-
tional conference on World Wide Web, pages 377–386.
ACM.
Frane ˇSaric, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder,
and Bojana Dalbelo Baˇsic. 2012. Takelab: Systems
for measuring semantic text similarity. pages 441–
448. First Joint Conference on Lexical and Compu-
tational Semantics (*SEM).
Julie Elizabeth Weeds. 2003. Measures and applications
of lexical distributional similarity. Ph.D. thesis, Cite-
seer.
Zhibiao Wu and Martha Palmer. 1994. Verbs semantics
and lexical selection. In Proceedings of the 32nd an-
nual meeting on Association for Computational Lin-
guistics, pages 133–138. Association for Computa-
tional Linguistics.
Cha Yang and Jun Wen. 2007. Text categorization based
on similarity approach. In Proceedings of Interna-
tional Conference on Intelligence Systems and Knowl-
edge Engineering (ISKE).
</reference>
<page confidence="0.998305">
131
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.577969">
<title confidence="0.9984875">ECNUCS: Measuring Short Text Semantic Equivalence Using Multiple Similarity Measurements</title>
<author confidence="0.999825">Tian ZHU Man</author>
<affiliation confidence="0.998238">Department of Computer Science and Department of Computer Science and Technology Technology East China Normal University East China Normal University</affiliation>
<address confidence="0.696993">51111201046@student.ecnu.edu.cn mlan@cs.ecnu.edu.cn</address>
<abstract confidence="0.9895996875">This paper reports our submissions to the Semantic Textual Similarity (STS) task in Shared Task 2013. We submitted three Support Vector Regression (SVR) systems in core task, using 6 types of similarity measures, i.e., string similarity, number similarity, knowledge-based similarity, corpus-based similarity, syntactic dependency similarity and machine translation similarity. Our third system with different training data and different feature sets for each test data set performs the best and ranks 35 out of 90 runs. We also submitted two systems in typed task using string based measure and Named Entity based measure. Our best system ranks 5 out of 15 runs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eneko Agirre</author>
<author>Daniel Cer</author>
<author>Mona Diab</author>
<author>Aitor GonzalezAgirre</author>
<author>Weiwei Guo</author>
</authors>
<title>sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity.</title>
<date>2013</date>
<booktitle>In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</booktitle>
<contexts>
<context position="2556" citStr="Agirre et al., 2013" startWordPosition="393" endWordPosition="396">igher probability to be the right one. The STS task in *SEM Shared Task 2013 consists of two subtasks, i.e., core task and typed task, and we participate in both of them. The core task aims to measure the semantic similarity of two sentences, resulting in a similarity score which ranges from 5 (semantic equivalence) to 0 (no relation). The typed task is a pilot task on typed-similarity between semistructured records. The types of similarity to be measured include location, author, people involved, time, events or actions, subject and description as well as the general similarity of two texts (Agirre et al., 2013). In this work we present a Support Vector Regression (SVR) system to measure sentence semantic similarity by integrating multiple measurements, i.e., string similarity, knowledge based similarity, corpus based similarity, number similarity and machine translation metrics. Most of these similarities are borrowed from previous work, e.g., (B¨ar et al., 2012), (ˇSaric et al., 2012) and (de Souza et al., 2012). We also propose a novel syntactic dependency similarity. Our best system ranks 35 out of 90 runs in core task and ranks 5 out of 15 runs in typed task. The rest of this paper is organized </context>
</contexts>
<marker>Agirre, Cer, Diab, GonzalezAgirre, Guo, 2013</marker>
<rawString>Eneko Agirre, Daniel Cer, Mona Diab, Aitor GonzalezAgirre, and Weiwei Guo. 2013. *sem 2013 shared task: Semantic textual similarity, including a pilot on typed-similarity. In *SEM 2013: The Second Joint Conference on Lexical and Computational Semantics. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lloyd Allison</author>
<author>Trevor I Dix</author>
</authors>
<title>A bit-string longest-common-subsequence algorithm.</title>
<date>1986</date>
<journal>Information Processing Letters,</journal>
<volume>23</volume>
<issue>5</issue>
<contexts>
<context position="5243" citStr="Allison and Dix, 1986" startWordPosition="816" endWordPosition="819">emmatization, which lemmatizes the word to its nearest base form that appears in WordNet, for example, was is lemmatized as is, not be. Given two short texts or sentences s1 and s2, we denote the word set of s1 and s2 as S1 and S2, the length (i.e., number of words) of s1 and s2 as |S1| and |S2|. 2.2 String Similarity Intuitively, if two sentences share more strings, they are considered to have higher semantic similarity. Therefore, we create 12 string based features in consideration of the common sequence shared by two texts. Longest Common sequence (LCS). The widely used LCS is proposed by (Allison and Dix, 1986), which is to find the maximum length of a common subsequence of two strings and here the subsequence need to be contiguous. In consideration of the different length of two texts, we compute LCS 1http://nlp.stanford.edu/software/lex-parser.shtml 2http://nlp.stanford.edu/software/tagger.shtml 3http://nltk.org/ similarity using Formula (1) as follows: SimLCS = Length of LCS (1) min(|S1|,|S2|) In order to eliminate the impacts of various forms of word, we also compute a Lemma LCS similarity score after sentences being lemmatized. word n-grams. Following (Lyon et al., 2001), we calculate the word </context>
</contexts>
<marker>Allison, Dix, 1986</marker>
<rawString>Lloyd Allison and Trevor I Dix. 1986. A bit-string longest-common-subsequence algorithm. Information Processing Letters, 23(5):305–310.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Carmen Banea</author>
<author>Samer Hassan</author>
<author>Michael Mohler</author>
<author>Rada Mihalcea</author>
</authors>
<title>Unt: A supervised synergistic approach to semantic text similarity.</title>
<date>2012</date>
<booktitle>First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<pages>635--642</pages>
<contexts>
<context position="7551" citStr="Banea et al., 2012" startWordPosition="1199" endWordPosition="1202">of s2 by s1 is given by Formula (4): ∑ wES,nS2 ic(w) Simwwo(s1, s2) = ∑w′ES2 ic(w′) (4) Likewise, we can get Simwwo(s2, s1) score. Then the final WWO score is the harmonic mean of Simwwo(s1, s2) and Simwwo(s2, s1). Similarly, we get a Lemma WWO score as well. 4http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13 125 2.3 Knowledge Based Similarity Knowledge based similarity approaches rely on a semantic network of words. In this work all knowledge-based word similarity measures are computed based on WordNet. For word similarity, we employ four WordNet-based similarity metrics: the Path similarity (Banea et al., 2012); the WUP similarity (Wu and Palmer, 1994); the LCH similarity (Leacock and Chodorow, 1998); the Lin similarity (Lin, 1998). We adopt the NLTK library (Bird, 2006) to compute all these word similarities. In order to determine the similarity of sentences, we employ two strategies to convert the word similarity into sentence similarity, i.e., (1) the best alignment strategy (align) (Banea et al., 2012) and (2) the aggregation strategy (agg) (Mihalcea et al., 2006). The best alignment strategy is computed as below: (5) where ω is the number of shared terms between s1 and s2, list cp contains the </context>
</contexts>
<marker>Banea, Hassan, Mohler, Mihalcea, 2012</marker>
<rawString>Carmen Banea, Samer Hassan, Michael Mohler, and Rada Mihalcea. 2012. Unt: A supervised synergistic approach to semantic text similarity. pages 635–642. First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel B¨ar</author>
<author>Chris Biemann</author>
<author>Iryna Gurevych</author>
<author>Torsten Zesch</author>
</authors>
<title>Ukp: Computing semantic textual similarity by combining multiple content similarity measures.</title>
<date>2012</date>
<booktitle>First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<pages>435--440</pages>
<marker>B¨ar, Biemann, Gurevych, Zesch, 2012</marker>
<rawString>Daniel B¨ar, Chris Biemann, Iryna Gurevych, and Torsten Zesch. 2012. Ukp: Computing semantic textual similarity by combining multiple content similarity measures. pages 435–440. First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Steven Bird</author>
</authors>
<title>Nltk: the natural language toolkit.</title>
<date>2006</date>
<booktitle>In Proceedings of the COLING/ACL on Interactive presentation sessions,</booktitle>
<pages>69--72</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7714" citStr="Bird, 2006" startWordPosition="1229" endWordPosition="1230"> mean of Simwwo(s1, s2) and Simwwo(s2, s1). Similarly, we get a Lemma WWO score as well. 4http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13 125 2.3 Knowledge Based Similarity Knowledge based similarity approaches rely on a semantic network of words. In this work all knowledge-based word similarity measures are computed based on WordNet. For word similarity, we employ four WordNet-based similarity metrics: the Path similarity (Banea et al., 2012); the WUP similarity (Wu and Palmer, 1994); the LCH similarity (Leacock and Chodorow, 1998); the Lin similarity (Lin, 1998). We adopt the NLTK library (Bird, 2006) to compute all these word similarities. In order to determine the similarity of sentences, we employ two strategies to convert the word similarity into sentence similarity, i.e., (1) the best alignment strategy (align) (Banea et al., 2012) and (2) the aggregation strategy (agg) (Mihalcea et al., 2006). The best alignment strategy is computed as below: (5) where ω is the number of shared terms between s1 and s2, list cp contains the similarities of non-shared words in shorter text, cpi is the highest similarity score of the ith word among all words of the longer text. The aggregation strategy </context>
</contexts>
<marker>Bird, 2006</marker>
<rawString>Steven Bird. 2006. Nltk: the natural language toolkit. In Proceedings of the COLING/ACL on Interactive presentation sessions, pages 69–72. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jos´e Guilherme C de Souza</author>
<author>Matteo Negri</author>
<author>Trento Povo</author>
<author>Yashar Mehdad</author>
</authors>
<title>Fbk: Machine translation evaluation and word similarity metrics for semantic textual similarity.</title>
<date>2012</date>
<booktitle>First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<pages>624--630</pages>
<marker>de Souza, Negri, Povo, Mehdad, 2012</marker>
<rawString>Jos´e Guilherme C de Souza, Matteo Negri, Trento Povo, and Yashar Mehdad. 2012. Fbk: Machine translation evaluation and word similarity metrics for semantic textual similarity. pages 624–630. First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Kauchak</author>
<author>Regina Barzilay</author>
</authors>
<title>Paraphrasing for automatic evaluation.</title>
<date>2006</date>
<booktitle>In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics,</booktitle>
<pages>455--462</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1613" citStr="Kauchak and Barzilay, 2006" startWordPosition="238" endWordPosition="242">runs. 1 Introduction The task of semantic textual similarity (STS) is to measure the degree of semantic equivalence between two sentences, which plays an increasingly important role in natural language processing (NLP) applications. For example, in text categorization (Yang and Wen, 2007), two documents which are more similar are more likely to be grouped in the same class. In information retrieval (Sahami and Heilman, 2006), text similarity improves the effectiveness of a semantic search engine by providing information which holds high similarity with the input query. In machine translation (Kauchak and Barzilay, 2006), sentence similarity can be applied for automatic evaluation of the output translation and the reference translations. In question answering (Mohler and Mihalcea, 2009), once the question and 124 the candidate answers are treated as two texts, the answer text which has a higher relevance with the question text may have higher probability to be the right one. The STS task in *SEM Shared Task 2013 consists of two subtasks, i.e., core task and typed task, and we participate in both of them. The core task aims to measure the semantic similarity of two sentences, resulting in a similarity score wh</context>
</contexts>
<marker>Kauchak, Barzilay, 2006</marker>
<rawString>David Kauchak and Regina Barzilay. 2006. Paraphrasing for automatic evaluation. In Proceedings of the main conference on Human Language Technology Conference of the North American Chapter of the Association of Computational Linguistics, pages 455– 462. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Thomas K Landauer</author>
<author>Darrell Laham</author>
<author>Bob Rehder</author>
<author>Missy E Schreiner</author>
</authors>
<title>How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans.</title>
<date>1997</date>
<booktitle>In Proceedings of the 19th annual meeting of the Cognitive Science Society,</booktitle>
<pages>412--417</pages>
<contexts>
<context position="8741" citStr="Landauer et al., 1997" startWordPosition="1405" endWordPosition="1408">s1 and s2, list cp contains the similarities of non-shared words in shorter text, cpi is the highest similarity score of the ith word among all words of the longer text. The aggregation strategy is calculated as below: (6) where maxSim(w, S2) is the highest WordNetbased score between word w and all words of sentence S2. To compute ic(w), we use the same corpus as WWO, i.e., the Web 1T 5-gram Corpus. The final score of the aggregation strategy is the mean of Simagg(s1, s2) and Simagg(s2, s1). Finally we get 8 knowledge based features. 2.4 Corpus Based Similarity Latent Semantic Analysis (LSA) (Landauer et al., 1997). In LSA, term-context associations are captured by means of a dimensionality reduction operation performing singular value decomposition (SVD) on the term-by-context matrix T, where T is induced from a large corpus. We use the TASA corpus5 to obtain the matrix and compute the word 5http://lsa.colorado.edu/ similarity using cosine similarity of the two vectors of the words. After that we transform word similarity to sentence similarity based on Formula (5). Co-occurrence Retrieval Model (CRM) (Weeds, 2003). CRM is based on a notion of substitutability. That is, the more appropriate it is to su</context>
</contexts>
<marker>Landauer, Laham, Rehder, Schreiner, 1997</marker>
<rawString>Thomas K Landauer, Darrell Laham, Bob Rehder, and Missy E Schreiner. 1997. How well can passage meaning be derived without using word order? a comparison of latent semantic analysis and humans. In Proceedings of the 19th annual meeting of the Cognitive Science Society, pages 412–417.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Claudia Leacock</author>
<author>Martin Chodorow</author>
</authors>
<title>Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database,</title>
<date>1998</date>
<pages>49--2</pages>
<contexts>
<context position="7642" citStr="Leacock and Chodorow, 1998" startWordPosition="1214" endWordPosition="1217"> (4) Likewise, we can get Simwwo(s2, s1) score. Then the final WWO score is the harmonic mean of Simwwo(s1, s2) and Simwwo(s2, s1). Similarly, we get a Lemma WWO score as well. 4http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13 125 2.3 Knowledge Based Similarity Knowledge based similarity approaches rely on a semantic network of words. In this work all knowledge-based word similarity measures are computed based on WordNet. For word similarity, we employ four WordNet-based similarity metrics: the Path similarity (Banea et al., 2012); the WUP similarity (Wu and Palmer, 1994); the LCH similarity (Leacock and Chodorow, 1998); the Lin similarity (Lin, 1998). We adopt the NLTK library (Bird, 2006) to compute all these word similarities. In order to determine the similarity of sentences, we employ two strategies to convert the word similarity into sentence similarity, i.e., (1) the best alignment strategy (align) (Banea et al., 2012) and (2) the aggregation strategy (agg) (Mihalcea et al., 2006). The best alignment strategy is computed as below: (5) where ω is the number of shared terms between s1 and s2, list cp contains the similarities of non-shared words in shorter text, cpi is the highest similarity score of th</context>
</contexts>
<marker>Leacock, Chodorow, 1998</marker>
<rawString>Claudia Leacock and Martin Chodorow. 1998. Combining local context and wordnet similarity for word sense identification. WordNet: An electronic lexical database, 49(2):265–283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekang Lin</author>
</authors>
<title>An information-theoretic definition of similarity.</title>
<date>1998</date>
<booktitle>In Proceedings of the 15th international conference on Machine Learning,</booktitle>
<volume>1</volume>
<pages>296--304</pages>
<location>San Francisco.</location>
<contexts>
<context position="7674" citStr="Lin, 1998" startWordPosition="1222" endWordPosition="1223">hen the final WWO score is the harmonic mean of Simwwo(s1, s2) and Simwwo(s2, s1). Similarly, we get a Lemma WWO score as well. 4http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13 125 2.3 Knowledge Based Similarity Knowledge based similarity approaches rely on a semantic network of words. In this work all knowledge-based word similarity measures are computed based on WordNet. For word similarity, we employ four WordNet-based similarity metrics: the Path similarity (Banea et al., 2012); the WUP similarity (Wu and Palmer, 1994); the LCH similarity (Leacock and Chodorow, 1998); the Lin similarity (Lin, 1998). We adopt the NLTK library (Bird, 2006) to compute all these word similarities. In order to determine the similarity of sentences, we employ two strategies to convert the word similarity into sentence similarity, i.e., (1) the best alignment strategy (align) (Banea et al., 2012) and (2) the aggregation strategy (agg) (Mihalcea et al., 2006). The best alignment strategy is computed as below: (5) where ω is the number of shared terms between s1 and s2, list cp contains the similarities of non-shared words in shorter text, cpi is the highest similarity score of the ith word among all words of th</context>
</contexts>
<marker>Lin, 1998</marker>
<rawString>Dekang Lin. 1998. An information-theoretic definition of similarity. In Proceedings of the 15th international conference on Machine Learning, volume 1, pages 296–304. San Francisco.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Caroline Lyon</author>
<author>James Malcolm</author>
<author>Bob Dickerson</author>
</authors>
<title>Detecting short passages of similar text in large document collections.</title>
<date>2001</date>
<booktitle>In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing,</booktitle>
<pages>118--125</pages>
<contexts>
<context position="5819" citStr="Lyon et al., 2001" startWordPosition="898" endWordPosition="901">LCS is proposed by (Allison and Dix, 1986), which is to find the maximum length of a common subsequence of two strings and here the subsequence need to be contiguous. In consideration of the different length of two texts, we compute LCS 1http://nlp.stanford.edu/software/lex-parser.shtml 2http://nlp.stanford.edu/software/tagger.shtml 3http://nltk.org/ similarity using Formula (1) as follows: SimLCS = Length of LCS (1) min(|S1|,|S2|) In order to eliminate the impacts of various forms of word, we also compute a Lemma LCS similarity score after sentences being lemmatized. word n-grams. Following (Lyon et al., 2001), we calculate the word n-grams similarity using the Jaccard coefficient as shown in Formula (2), where p is the number of n-grams shared by s1 and s2, q and r are the number of n-grams not shared by s1 and s2, respectively. () Jacc = p 2 p + q + r Since we focus on short texts, here only n=1,2,3,4 is used in this work. Similar with LCS, we also compute a Lemma n-grams similarity score. Weighted Word Overlap (WWO). (ˇSaric et al., 2012) pointed out that when measuring sentence similarity, different words may convey different content information. Therefore, we consider to assign more importance</context>
</contexts>
<marker>Lyon, Malcolm, Dickerson, 2001</marker>
<rawString>Caroline Lyon, James Malcolm, and Bob Dickerson. 2001. Detecting short passages of similar text in large document collections. In Proceedings of the 2001 Conference on Empirical Methods in Natural Language Processing, pages 118–125.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Rada Mihalcea</author>
<author>Courtney Corley</author>
<author>Carlo Strapparava</author>
</authors>
<title>Corpus-based and knowledge-based measures of text semantic similarity.</title>
<date>2006</date>
<booktitle>In Proceedings of the national conference on artificial intelligence,</booktitle>
<volume>21</volume>
<pages>775</pages>
<publisher>AAAI Press; MIT Press;</publisher>
<location>Menlo Park, CA; Cambridge, MA; London;</location>
<contexts>
<context position="8017" citStr="Mihalcea et al., 2006" startWordPosition="1276" endWordPosition="1279">arity measures are computed based on WordNet. For word similarity, we employ four WordNet-based similarity metrics: the Path similarity (Banea et al., 2012); the WUP similarity (Wu and Palmer, 1994); the LCH similarity (Leacock and Chodorow, 1998); the Lin similarity (Lin, 1998). We adopt the NLTK library (Bird, 2006) to compute all these word similarities. In order to determine the similarity of sentences, we employ two strategies to convert the word similarity into sentence similarity, i.e., (1) the best alignment strategy (align) (Banea et al., 2012) and (2) the aggregation strategy (agg) (Mihalcea et al., 2006). The best alignment strategy is computed as below: (5) where ω is the number of shared terms between s1 and s2, list cp contains the similarities of non-shared words in shorter text, cpi is the highest similarity score of the ith word among all words of the longer text. The aggregation strategy is calculated as below: (6) where maxSim(w, S2) is the highest WordNetbased score between word w and all words of sentence S2. To compute ic(w), we use the same corpus as WWO, i.e., the Web 1T 5-gram Corpus. The final score of the aggregation strategy is the mean of Simagg(s1, s2) and Simagg(s2, s1). F</context>
</contexts>
<marker>Mihalcea, Corley, Strapparava, 2006</marker>
<rawString>Rada Mihalcea, Courtney Corley, and Carlo Strapparava. 2006. Corpus-based and knowledge-based measures of text semantic similarity. In Proceedings of the national conference on artificial intelligence, volume 21, page 775. Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Michael Mohler</author>
<author>Rada Mihalcea</author>
</authors>
<title>Text-to-text semantic similarity for automatic short answer grading.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics,</booktitle>
<pages>567--575</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="1782" citStr="Mohler and Mihalcea, 2009" startWordPosition="262" endWordPosition="265">ortant role in natural language processing (NLP) applications. For example, in text categorization (Yang and Wen, 2007), two documents which are more similar are more likely to be grouped in the same class. In information retrieval (Sahami and Heilman, 2006), text similarity improves the effectiveness of a semantic search engine by providing information which holds high similarity with the input query. In machine translation (Kauchak and Barzilay, 2006), sentence similarity can be applied for automatic evaluation of the output translation and the reference translations. In question answering (Mohler and Mihalcea, 2009), once the question and 124 the candidate answers are treated as two texts, the answer text which has a higher relevance with the question text may have higher probability to be the right one. The STS task in *SEM Shared Task 2013 consists of two subtasks, i.e., core task and typed task, and we participate in both of them. The core task aims to measure the semantic similarity of two sentences, resulting in a similarity score which ranges from 5 (semantic equivalence) to 0 (no relation). The typed task is a pilot task on typed-similarity between semistructured records. The types of similarity t</context>
</contexts>
<marker>Mohler, Mihalcea, 2009</marker>
<rawString>Michael Mohler and Rada Mihalcea. 2009. Text-to-text semantic similarity for automatic short answer grading. In Proceedings of the 12th Conference of the European Chapter of the Association for Computational Linguistics, pages 567–575. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mehran Sahami</author>
<author>Timothy D Heilman</author>
</authors>
<title>A webbased kernel function for measuring the similarity of short text snippets.</title>
<date>2006</date>
<booktitle>In Proceedings of the 15th international conference on World Wide Web,</booktitle>
<pages>377--386</pages>
<publisher>ACM.</publisher>
<contexts>
<context position="1414" citStr="Sahami and Heilman, 2006" startWordPosition="207" endWordPosition="211">est data set performs the best and ranks 35 out of 90 runs. We also submitted two systems in typed task using string based measure and Named Entity based measure. Our best system ranks 5 out of 15 runs. 1 Introduction The task of semantic textual similarity (STS) is to measure the degree of semantic equivalence between two sentences, which plays an increasingly important role in natural language processing (NLP) applications. For example, in text categorization (Yang and Wen, 2007), two documents which are more similar are more likely to be grouped in the same class. In information retrieval (Sahami and Heilman, 2006), text similarity improves the effectiveness of a semantic search engine by providing information which holds high similarity with the input query. In machine translation (Kauchak and Barzilay, 2006), sentence similarity can be applied for automatic evaluation of the output translation and the reference translations. In question answering (Mohler and Mihalcea, 2009), once the question and 124 the candidate answers are treated as two texts, the answer text which has a higher relevance with the question text may have higher probability to be the right one. The STS task in *SEM Shared Task 2013 c</context>
</contexts>
<marker>Sahami, Heilman, 2006</marker>
<rawString>Mehran Sahami and Timothy D Heilman. 2006. A webbased kernel function for measuring the similarity of short text snippets. In Proceedings of the 15th international conference on World Wide Web, pages 377–386. ACM.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Frane ˇSaric</author>
<author>Goran Glavaˇs</author>
<author>Mladen Karan</author>
</authors>
<title>Snajder, and Bojana Dalbelo Baˇsic.</title>
<date></date>
<booktitle>First Joint Conference on Lexical and Computational Semantics (*SEM).</booktitle>
<pages>441--448</pages>
<marker>ˇSaric, Glavaˇs, Karan, </marker>
<rawString>Frane ˇSaric, Goran Glavaˇs, Mladen Karan, Jan ˇSnajder, and Bojana Dalbelo Baˇsic. 2012. Takelab: Systems for measuring semantic text similarity. pages 441– 448. First Joint Conference on Lexical and Computational Semantics (*SEM).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Julie Elizabeth Weeds</author>
</authors>
<title>Measures and applications of lexical distributional similarity.</title>
<date>2003</date>
<tech>Ph.D. thesis, Citeseer.</tech>
<contexts>
<context position="9252" citStr="Weeds, 2003" startWordPosition="1485" endWordPosition="1486">ge based features. 2.4 Corpus Based Similarity Latent Semantic Analysis (LSA) (Landauer et al., 1997). In LSA, term-context associations are captured by means of a dimensionality reduction operation performing singular value decomposition (SVD) on the term-by-context matrix T, where T is induced from a large corpus. We use the TASA corpus5 to obtain the matrix and compute the word 5http://lsa.colorado.edu/ similarity using cosine similarity of the two vectors of the words. After that we transform word similarity to sentence similarity based on Formula (5). Co-occurrence Retrieval Model (CRM) (Weeds, 2003). CRM is based on a notion of substitutability. That is, the more appropriate it is to substitute word w1 in place of word w2 in a suitable natural language task, the more semantically similar they are. The degree of substitutability of w2 with w1 is dependent on the proportion of co-occurrences of w1 that are also the co-occurrences of w2, and the proportion of co-occurrences of w2 that are also the co-occurrences of w1. Following (Weeds, 2003), the CRM word similarity is computed using Formula (7): SimCRM(w1, w2) = 2 */ |c(w1) n c(w2) |(7) c(w1) |+ |c(w2)| where c(w) is the set of words that</context>
</contexts>
<marker>Weeds, 2003</marker>
<rawString>Julie Elizabeth Weeds. 2003. Measures and applications of lexical distributional similarity. Ph.D. thesis, Citeseer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhibiao Wu</author>
<author>Martha Palmer</author>
</authors>
<title>Verbs semantics and lexical selection.</title>
<date>1994</date>
<booktitle>In Proceedings of the 32nd annual meeting on Association for Computational Linguistics,</booktitle>
<pages>133--138</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<contexts>
<context position="7593" citStr="Wu and Palmer, 1994" startWordPosition="1206" endWordPosition="1209">S,nS2 ic(w) Simwwo(s1, s2) = ∑w′ES2 ic(w′) (4) Likewise, we can get Simwwo(s2, s1) score. Then the final WWO score is the harmonic mean of Simwwo(s1, s2) and Simwwo(s2, s1). Similarly, we get a Lemma WWO score as well. 4http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13 125 2.3 Knowledge Based Similarity Knowledge based similarity approaches rely on a semantic network of words. In this work all knowledge-based word similarity measures are computed based on WordNet. For word similarity, we employ four WordNet-based similarity metrics: the Path similarity (Banea et al., 2012); the WUP similarity (Wu and Palmer, 1994); the LCH similarity (Leacock and Chodorow, 1998); the Lin similarity (Lin, 1998). We adopt the NLTK library (Bird, 2006) to compute all these word similarities. In order to determine the similarity of sentences, we employ two strategies to convert the word similarity into sentence similarity, i.e., (1) the best alignment strategy (align) (Banea et al., 2012) and (2) the aggregation strategy (agg) (Mihalcea et al., 2006). The best alignment strategy is computed as below: (5) where ω is the number of shared terms between s1 and s2, list cp contains the similarities of non-shared words in shorte</context>
</contexts>
<marker>Wu, Palmer, 1994</marker>
<rawString>Zhibiao Wu and Martha Palmer. 1994. Verbs semantics and lexical selection. In Proceedings of the 32nd annual meeting on Association for Computational Linguistics, pages 133–138. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Cha Yang</author>
<author>Jun Wen</author>
</authors>
<title>Text categorization based on similarity approach.</title>
<date>2007</date>
<booktitle>In Proceedings of International Conference on Intelligence Systems and Knowledge Engineering (ISKE).</booktitle>
<contexts>
<context position="1275" citStr="Yang and Wen, 2007" startWordPosition="184" endWordPosition="187">cy similarity and machine translation similarity. Our third system with different training data and different feature sets for each test data set performs the best and ranks 35 out of 90 runs. We also submitted two systems in typed task using string based measure and Named Entity based measure. Our best system ranks 5 out of 15 runs. 1 Introduction The task of semantic textual similarity (STS) is to measure the degree of semantic equivalence between two sentences, which plays an increasingly important role in natural language processing (NLP) applications. For example, in text categorization (Yang and Wen, 2007), two documents which are more similar are more likely to be grouped in the same class. In information retrieval (Sahami and Heilman, 2006), text similarity improves the effectiveness of a semantic search engine by providing information which holds high similarity with the input query. In machine translation (Kauchak and Barzilay, 2006), sentence similarity can be applied for automatic evaluation of the output translation and the reference translations. In question answering (Mohler and Mihalcea, 2009), once the question and 124 the candidate answers are treated as two texts, the answer text w</context>
</contexts>
<marker>Yang, Wen, 2007</marker>
<rawString>Cha Yang and Jun Wen. 2007. Text categorization based on similarity approach. In Proceedings of International Conference on Intelligence Systems and Knowledge Engineering (ISKE).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>