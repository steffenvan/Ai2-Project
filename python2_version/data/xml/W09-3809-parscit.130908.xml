<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.006617">
<title confidence="0.969942">
Deductive Parsing with Interaction Grammars
</title>
<author confidence="0.997183">
Joseph Le Roux
</author>
<affiliation confidence="0.9909815">
NCLT, School of Computing,
Dublin City University
</affiliation>
<email confidence="0.967753">
jleroux@computing.dcu.ie
</email>
<sectionHeader confidence="0.997081" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999777833333333">
We present a parsing algorithm for In-
teraction Grammars using the deductive
parsing framework. This approach brings
new perspectives to this problem, depart-
ing from previous methods which rely on
constraint-solving techniques.
</bodyText>
<sectionHeader confidence="0.999647" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999913222222222">
An Interaction Grammar (IG) (Guillaume and Per-
rier, 2008) is a lexicalized grammatical formal-
ism that primarily focuses on valency, explicitly
expressed using polarities decorating syntagms.
These polarities and the use of underspecified
structures naturally lead parsing to be viewed as
a constraint-solving problem – for example (Bon-
fante et al.) reduce the parsing problem to a graph-
rewriting problem in (2003) .
However, in this article we depart from this ap-
proach and present an algorithm close to (Earley,
1970) for context-free grammars. We introduce
this algorithm using the standard framework of de-
ductive parsing (Shieber et al., 1995).
This article is organised as follows: we first
present IGs (section 2), then we describe the algo-
rithm (section 3). Finally we discuss some techni-
cal points and conclude (sections 4 and 5).
</bodyText>
<sectionHeader confidence="0.995725" genericHeader="method">
2 Interaction Grammars
</sectionHeader>
<bodyText confidence="0.9999">
We briefly introduce IGs as in (Guillaume and Per-
rier, 2008)1. However, we omit polarized feature
structures, for the sake of exposition.
</bodyText>
<subsectionHeader confidence="0.898648">
2.1 Polarized Tree Descriptions
</subsectionHeader>
<bodyText confidence="0.99989375">
The structures associated with words by the lexi-
con are Polarized Tree Descriptions (PTDs). They
represent fragments of parse trees. The nodes of
these structures are labelled with a category and a
</bodyText>
<footnote confidence="0.9814765">
1This paper also discusses the linguistic motivations be-
hind IGs.
</footnote>
<bodyText confidence="0.9983922">
polarity. IGs use 4 polarities, P = {→,←, =, ∼},
namely positive, negative, neutral and virtual.
A multiset of polarities is superposable2 if it
contains at most one → and at most one ←.
A multiset of polarities is saturated if it contains
either (1) one →, one ← and any number of ∼ and
=, or (2) zero →, zero ←, any number of ∼ and at
least one =.
The two previous definitions can be extended to
nodes: a multiset of nodes is saturated (resp. su-
perposable) if all the elements have the same cat-
egory and if the induced multiset of polarities is
saturated (resp. superposable).
A PTD is a DAG with four binary relations: the
immediate dominance &gt;, the general dominance
&gt;*, the immediate precedence ≺ and the general
precedence ≺+. A valid PTD is a PTD where (1)
&gt; and &gt;* define a tree structure3 , (2) ≺ and ≺+
are restricted to couples of nodes having the same
ancestor by &gt;, and (3) one leaf is the anchor. In
the rest of this paper, all PTDs will be valid.
We now introduce some notations : if n &gt;*
m, we say that m is constrained by n and for a
set of nodes N, we define N&apos; = {N|∃M ∈
N, M►Z4N} where ►Z4 is a binary relation.
</bodyText>
<subsectionHeader confidence="0.996786">
2.2 Grammars
</subsectionHeader>
<bodyText confidence="0.9999791">
An IG is a tuple G = {E, C, 5, P, phon}, where E
is the terminal alphabet, C the non-terminal alpha-
bet, 5 ∈ C the initial symbol, P is a set of PTDs
with node labels in C × P, and phon is a function
from anchors in P to E.
The structure obtained from parsing is a syntac-
tic tree, a totally ordered tree in which all nodes
are labelled with a non-terminal. We call lab(A)
the label of node A. If a leaf L is labelled with a
terminal, this terminal is denoted word(L).
</bodyText>
<footnote confidence="0.9964125">
2This name comes from the superposition introduced in
previous presentations of IGs.
3For readers familiar with D-Tree Grammars (Rambow et
al., 1995), &gt; adds an i-edge while &gt;∗ adds a d-edge.
</footnote>
<page confidence="0.996437">
65
</page>
<bodyText confidence="0.96338647368421">
Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 65–68,
Paris, October 2009. c�2009 Association for Computational Linguistics
We will write M » N if the node M is
the mother of N and N » [N1,..., Nk] if the
N is the mother of the ordered list of nodes
[N1,... , Nk]. The order between siblings can also
be expressed using the relation --&lt;--&lt;: M --&lt;--&lt; N
means that N is the immediate successor of M.
--&lt;--&lt;+ is the transitive closure of --&lt;--&lt; and »* the
reflexive transitive closure of ».
We define the phonological projection PP
of a node as : PP(M) = [t] if M »
[] and word(M) = t, or PP(M) =
[PP(N1) ... PP(Nk)] if M » [N1, ... , Nk]
A syntactic tree T is a model for a multiset D of
PTDs if there exists a total function I from nodes
in D (ND) to nodes in T (NT). I must respect
the following conditions, where variables M, N
range over ND and A, B over NT :
</bodyText>
<listItem confidence="0.943366">
1. I−1(A) is saturated and non-empty.
2. if M &gt; N then I(M) » I(N)
3. if M &gt;* N then I(M) »* I(N)
4. if M --&lt; N then I(M) --&lt;--&lt; I(N)
5. if M --&lt;+ N then I(M) --&lt;--&lt;+ I(N)
6. if A » B then there exists M E I−1(A) and
N E I−1(B) such that M &gt; N
7. lab(A) = lab(M) for all M E I−1(A)
8. if phon(M) = w then PP(I(M)) = [w]
</listItem>
<bodyText confidence="0.999299">
Given an IG !9 = {E, C, 5, P, phon} and a sen-
tence s = w1, ... , wn in E*, a syntactic tree T is
a parse tree for s if there exists a multiset of PTDs
D from P such that the root node R of T is la-
belled with 5 and PP(R) = [w1, ... , wn]. The
language generated by !9 is the set of strings in E*
for which there is a parse tree.
</bodyText>
<sectionHeader confidence="0.97331" genericHeader="method">
3 Parsing Algorithm
</sectionHeader>
<bodyText confidence="0.999935">
We use the deductive parsing framework (Shieber
et al., 1995). A state of the parser is encoded as an
item, created by applying deductive rules. Our al-
gorithm resembles the Earley algorithm for CFGs
and uses the same rules : prediction, scanning and
completion.
</bodyText>
<subsectionHeader confidence="0.989239">
3.1 Items
</subsectionHeader>
<bodyText confidence="0.9955485">
Items [A(H, N, F) — α • Q, i, j, (O, U, D)] con-
sist of a dotted rule, 2 position indexes and a 3-
tuple of sets of constrained nodes.
The dotted rule A(H, N, F) — α • Q means
that there exists a node A in the parse tree with
antecedents H UN UF. Elements of the sequence
α are also nodes of the parse tree. For the sequence
Q, the elements have the form Bk(Hk) where Bk
is a node of the parse tree and Hk is a subset of its
antecedents, the predicted antecedents.
</bodyText>
<listItem confidence="0.7908235">
This item asserts that a syntactic tree can be par-
tially built from the input grammar and sentence,
that contains A » [A1 ... AkB1 ... Bl] and that
PP(A1) o ··· o PP(Ak) = [mi+1 ... mj].
</listItem>
<bodyText confidence="0.7788995">
The proper use of constrained nodes is managed
by O, U and D:
</bodyText>
<listItem confidence="0.918978142857143">
• Nodes in D are available in prediction to find
antecedents for new parse tree nodes.
• Nodes in O must be used in a sub-parse. To
use an item as a completer, O must be empty.
• U contains constrained nodes that have been
used in a prediction, and for which the con-
straining nodes have not been completed yet.
</listItem>
<bodyText confidence="0.999924857142857">
Moreover, we will use 3 additional symbols: T
as the left-hand side of the axiom item which can
be seen as a dummy root, and ■ or + that mark
items for which prediction is not terminated.
We will need sequences of antecedents that re-
spect the order relations of an IG. Given a set of
nodes N, we define the set of all these orderings:
</bodyText>
<equation confidence="0.922291142857143">
ord(N) = {[N1 ... Nk]
(Ni)1&lt;i&lt;k is a partition of Nn
1 &lt; i &lt; k, Ni is superposable n
if n1, n2 E N and n1 --&lt; n2 then
11 &lt; j &lt; k s.t. n1 E Nj and n2 E Nj+1n
if n1, n2 E N and n1 --&lt;+ n2 then
11 &lt; i &lt; j &lt; k s.t. n1 E Ni and n2 E Nj}
</equation>
<subsectionHeader confidence="0.99231">
3.2 Deductive Rules
</subsectionHeader>
<bodyText confidence="0.994052">
In this section, we assume an input sentence s =
w1, ... , wn and a IG !9 = {E, C, 5, P, phon}.
Axiom This rule creates the first item. It pre-
pares the prediction of a node of category 5 start-
ing at position 0 without constrained nodes.
</bodyText>
<equation confidence="0.965636">
[T — •5(0), ti, ti, (0, 0, 0)]ax
</equation>
<page confidence="0.777639">
66
</page>
<bodyText confidence="0.980255666666667">
Finally, the nodes that must be used in a sub-
parse are the ones that are constrained by an-
tecedents of C and not antecedents themselves.
Prediction This rule initializes a sub-parse. We
divide it in three in order to introduce the different
constraints one at a time.
</bodyText>
<equation confidence="0.9556242">
�
i
[A(H, N, F) → α • C(HC)β, i, j, (O, U, D)]
[C(HC, ∅, ∅) → 0, j, j, (∅, U, D ∪ O)] 1
p
</equation>
<bodyText confidence="0.999793285714286">
In this first step, we initialize a new sub-parse
at the current position j where C will be the pre-
dicted node that we want to find antecedents for. If
some antecedents HC have already been predicted
we use them. The nodes in O, that must be used
in one of the sub-parse of A, become available as
possible antecedents for C.
</bodyText>
<equation confidence="0.999886142857143">
[C(HC, ∅, ∅) → 0,j,j, (∅, U1, D1)]
[C(HC, NC, ∅) → ♦, j, j, (∅, U2, D2)]p2
{ HC ∪ NC =6∅
HC ∪ NC is superposable
NC ⊂ D1 ∪ roots(P)
D2 = D1 − NC
U2 = U1 ∪ (D1 ∩ NC)
</equation>
<bodyText confidence="0.999573428571429">
In this second step, new antecedents for C are
added from the set NC, chosen among available
nodes in D1 and root nodes from the PTDs of
the grammar. The 3 node sets are then updated.
Constrained nodes that have been chosen as an-
tecedents for C are not available anymore and are
added to the set of used constrained nodes.
</bodyText>
<equation confidence="0.948692714285714">
[C(HC, NC, ∅) → ♦, j, j, (∅, U, D)]
[C(HC, NC, FC) → •&apos;Y, j, j, (O, U, D)]p3
{ HC ∪ NC ∪ FC is saturated
&apos;Y ∈ ord((HC ∪ NC ∪ FC)&gt;)
FC = Ui Qi, Q0 ⊆ (HC ∪ NC)&gt;*, Qi+1 ⊆ Q&gt;*
i
O = (HC ∪ NC ∪ FC)&gt;* − FC
</equation>
<bodyText confidence="0.98408225">
no anchor node in HC ∪ NC ∪ FC
In this last step of prediction, we can choose
new antecedents for C among nodes constrained
by antecedents already chosen in the previous
steps in order to saturate them. This choice is re-
cursive : each added antecedent triggers the pos-
sibility of choosing the nodes it constrains. The
second part of this step consists of predicting the
shape of the tree. We need to order and superpose
the daughter nodes of the antecedents in such a
way that ordering relations in PTDs are respected:
an element of ord((HC ∪ NC ∪ FC)&gt;) is chosen.
Scan This is the rule that checks predictions
against the input string. It is similar to the previ-
ous rule, but one (and only one) of the antecedents
must be an anchor.
</bodyText>
<equation confidence="0.993887888888889">
[C(HC, NC, ∅) → ♦, j, j, (∅, U, D)] s
[C(HC, NC, FC) → •, j, j + 1, (∅, U, D)]
HC ∪ NC ∪ FC is saturated
(HC ∪ NC ∪ FC)&gt; = ∅
FC = Ui Qi, Q0 ⊆ (HC ∪ NC)&gt;*, Qi+1 ⊆ Q&gt;*
i
(HC ∪ NC ∪ FC)&gt;* − FC = ∅
one anchor a in HC ∪ NC ∪ FC
phon(a) = wj+1
</equation>
<bodyText confidence="0.997436833333333">
If the expected terminal is read on the input
string, parsing can proceed. Note that antecedents
for C should not constrain nodes that are not an-
tecedents of C themselves.
Completion This rule extends a parse by com-
bining it with a complete sub-parse.
</bodyText>
<equation confidence="0.9649753">
[A(H, N, F) → α • C(Hc)β, i, j, (O1, U1, D1)]
[C(HC, NC, FC) → &apos;Y•, j, k, (∅, U2, D2)] c
[A(H, N, F) → αC • β, i, k, (O3, U3, D3)]
i
� NC ⊆ D1 ∪ O1 ∪ P
D2 ⊆ (D1 ∪ O1) − NC
D3 = D1 − U2
O3 = O1 − U2
U1 ⊆ U2
U3 = U2 − O1
</equation>
<bodyText confidence="0.991589444444445">
We have to make sure that the second hypothe-
sis is a sub-parse for the first : (1) the set of avail-
able nodes in the sub-parse must be a subset of
the available nodes for current parse, (2) the set of
used nodes in the main parse must be a subset of
the used nodes in the sub-parse and (3) used nodes
constrained by the first hypothesis disappear.
Goal Parsing is successful if the following item
is created: [&gt; → S•, 0, n, (∅, ∅, ∅)].
</bodyText>
<sectionHeader confidence="0.999843" genericHeader="method">
4 Discussion
</sectionHeader>
<subsectionHeader confidence="0.991777">
4.1 Consistency and completeness
</subsectionHeader>
<bodyText confidence="0.80452">
An item [A(H, N, F) → α • β, i, j, (O, U, D)] as-
serts the following invariants :
</bodyText>
<page confidence="0.997305">
67
</page>
<listItem confidence="0.906073090909091">
• A and the elements αl of α are models for
saturated sets of nodes. Conditions 1, 7 and 3
(reflexive case) of a model are respected.
• Elements Qk of Q are superposable. Then we
have Qk C (A−1)&gt; (conditions 2 and 6).
• the sequence αQ is compatible with the order
relations from the PTDs (conditions 4 and 5).
• PP(α1) o ... o PP(αl) = [wz+1 ··· wj]
• a node N in U is a constrained node in re-
lation &gt;* with a node such that condition 3
holds.
</listItem>
<bodyText confidence="0.99998525">
These invariants can be checked by induction
on rules. Hence, such an item asserts there exists a
function J from the nodes of a subset of the PTDs
of an IG to a syntactic tree with its root labelled
by S and phonological projection w1 ... wj. This
function has the same properties as the function I
for models but conditions 2 to 5 only apply if both
nodes are in the domain of J. The parsing process
extends the domain until (1) all the nodes of each
PTD selected are used and (2) the input string has
been read completely. Then J defines a syntactic
tree which is a parse tree.
</bodyText>
<subsectionHeader confidence="0.999343">
4.2 Sources of non-determinism
</subsectionHeader>
<bodyText confidence="0.9999738">
The parsing problem in IGs is a NP-hard prob-
lem (Bonfante et al., 2003). Our presentation lets
us see several sources of non-determinism.
In p2, new antecedents are chosen among avail-
able nodes and root nodes of PTDs from the in-
put grammar. There is an exponential number of
such choices. However, IGs are lexicalized : only
PTDs associated with a word in the sentence will
be used and efficient lexical filters have been de-
veloped for IGs (Bonfante et al., 2006) that drasti-
cally decrease the number of PTDs to consider.
In p3 and s, constrained nodes can be chosen as
antecedents (nodes in FC). There is again an ex-
ponential number of such choices. But in existing
IGs, nodes have at most one successor by &gt;* and
there is no chain of nodes in relation by &gt;*. Con-
sequently, |FC |can be bounded by |HC U NC|.
In p3, daughters must be partitioned. Instead of
building all these partitions in p3 and generating
many useless items, one can think of a lazy ap-
proach like the one proposed by (Nederhof et al.,
2003) for pomset-CFGS.
It can be noticed that the completion rule, while
having the most positional indexes, is not a partic-
ular source of non-determinism.
</bodyText>
<sectionHeader confidence="0.99936" genericHeader="conclusions">
5 Conclusion
</sectionHeader>
<bodyText confidence="0.999873133333333">
We presented a parsing algorithm for IGs. Al-
though we used a simplified version without polar-
ized feature structures, adding a unification mech-
anism shouldn’t be an issue. The novelty of
this presentation is the use of deductive parsing
for a formalism developed in the model-theoretic
framework (Pullum and Scholz, 2001).
This change of perspective provides new in-
sights on the causes of non-determinism. It is
a first step to a precise complexity study of the
problem. In the future, it will be interesting to
search for algorithmical approximations to im-
prove efficiency. Another way to overcome NP-
hardness is to restrict superpositions, as in (k-)TT-
MCTAGs (Kallmeyer and Parmentier, 2008).
</bodyText>
<sectionHeader confidence="0.999449" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999878875">
G. Bonfante, B. Guillaume, and G. Perrier. 2003.
Analyse syntaxique ´electrostatique. Traitement Au-
tomatique des Langues, 44(3).
G. Bonfante, J. Le Roux, and G. Perrier. 2006. Lexi-
cal disambiguation with polarities and automata. In
Proceedings of CIAA.
J. Earley. 1970. An efficient context-free parsing algo-
rithm. Communications of the ACM, 13(2):94–102.
B. Guillaume and G. Perrier. 2008. Interaction Gram-
mars. Research Report RR-6621, INRIA.
L. Kallmeyer and Y. Parmentier. 2008. On the relation
between TT-MCTAG and RCG. In Proceedings of
LATA.
M.J. Nederhof, G. Satta, and S. Shieber. 2003. Par-
tially ordered multiset context-free grammars and
ID/LP parsing. In Proceedings of IWPT.
G. Pullum and B. Scholz. 2001. On the distinction be-
tween model-theoretic and generative-enumerative
syntactic frameworks. In Proccedings of LACL.
O. Rambow, K. Vijay-Shanker, and D. Weir. 1995. D-
tree grammars. In Proceedings of ACL.
S. Shieber, Y. Schabes, and F. Pereira. 1995. Principles
and implementation of deductive parsing. Journal of
Logic Programming, 24(1–2):3–36.
</reference>
<page confidence="0.999446">
68
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.801016">
<title confidence="0.999891">Deductive Parsing with Interaction Grammars</title>
<author confidence="0.999402">Joseph Le</author>
<affiliation confidence="0.908342">NCLT, School of Dublin City</affiliation>
<email confidence="0.950474">jleroux@computing.dcu.ie</email>
<abstract confidence="0.999157428571429">We present a parsing algorithm for Interaction Grammars using the deductive parsing framework. This approach brings new perspectives to this problem, departing from previous methods which rely on constraint-solving techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>G Bonfante</author>
<author>B Guillaume</author>
<author>G Perrier</author>
</authors>
<date>2003</date>
<booktitle>Analyse syntaxique ´electrostatique. Traitement Automatique des Langues,</booktitle>
<volume>44</volume>
<issue>3</issue>
<contexts>
<context position="11721" citStr="Bonfante et al., 2003" startWordPosition="2360" endWordPosition="2363"> such an item asserts there exists a function J from the nodes of a subset of the PTDs of an IG to a syntactic tree with its root labelled by S and phonological projection w1 ... wj. This function has the same properties as the function I for models but conditions 2 to 5 only apply if both nodes are in the domain of J. The parsing process extends the domain until (1) all the nodes of each PTD selected are used and (2) the input string has been read completely. Then J defines a syntactic tree which is a parse tree. 4.2 Sources of non-determinism The parsing problem in IGs is a NP-hard problem (Bonfante et al., 2003). Our presentation lets us see several sources of non-determinism. In p2, new antecedents are chosen among available nodes and root nodes of PTDs from the input grammar. There is an exponential number of such choices. However, IGs are lexicalized : only PTDs associated with a word in the sentence will be used and efficient lexical filters have been developed for IGs (Bonfante et al., 2006) that drastically decrease the number of PTDs to consider. In p3 and s, constrained nodes can be chosen as antecedents (nodes in FC). There is again an exponential number of such choices. But in existing IGs,</context>
</contexts>
<marker>Bonfante, Guillaume, Perrier, 2003</marker>
<rawString>G. Bonfante, B. Guillaume, and G. Perrier. 2003. Analyse syntaxique ´electrostatique. Traitement Automatique des Langues, 44(3).</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Bonfante</author>
<author>J Le Roux</author>
<author>G Perrier</author>
</authors>
<title>Lexical disambiguation with polarities and automata.</title>
<date>2006</date>
<booktitle>In Proceedings of CIAA.</booktitle>
<marker>Bonfante, Le Roux, Perrier, 2006</marker>
<rawString>G. Bonfante, J. Le Roux, and G. Perrier. 2006. Lexical disambiguation with polarities and automata. In Proceedings of CIAA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Earley</author>
</authors>
<title>An efficient context-free parsing algorithm.</title>
<date>1970</date>
<journal>Communications of the ACM,</journal>
<volume>13</volume>
<issue>2</issue>
<contexts>
<context position="903" citStr="Earley, 1970" startWordPosition="130" endWordPosition="131">m, departing from previous methods which rely on constraint-solving techniques. 1 Introduction An Interaction Grammar (IG) (Guillaume and Perrier, 2008) is a lexicalized grammatical formalism that primarily focuses on valency, explicitly expressed using polarities decorating syntagms. These polarities and the use of underspecified structures naturally lead parsing to be viewed as a constraint-solving problem – for example (Bonfante et al.) reduce the parsing problem to a graphrewriting problem in (2003) . However, in this article we depart from this approach and present an algorithm close to (Earley, 1970) for context-free grammars. We introduce this algorithm using the standard framework of deductive parsing (Shieber et al., 1995). This article is organised as follows: we first present IGs (section 2), then we describe the algorithm (section 3). Finally we discuss some technical points and conclude (sections 4 and 5). 2 Interaction Grammars We briefly introduce IGs as in (Guillaume and Perrier, 2008)1. However, we omit polarized feature structures, for the sake of exposition. 2.1 Polarized Tree Descriptions The structures associated with words by the lexicon are Polarized Tree Descriptions (PT</context>
</contexts>
<marker>Earley, 1970</marker>
<rawString>J. Earley. 1970. An efficient context-free parsing algorithm. Communications of the ACM, 13(2):94–102.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Guillaume</author>
<author>G Perrier</author>
</authors>
<title>Interaction Grammars.</title>
<date>2008</date>
<tech>Research Report RR-6621, INRIA.</tech>
<contexts>
<context position="1306" citStr="Guillaume and Perrier, 2008" startWordPosition="193" endWordPosition="197">straint-solving problem – for example (Bonfante et al.) reduce the parsing problem to a graphrewriting problem in (2003) . However, in this article we depart from this approach and present an algorithm close to (Earley, 1970) for context-free grammars. We introduce this algorithm using the standard framework of deductive parsing (Shieber et al., 1995). This article is organised as follows: we first present IGs (section 2), then we describe the algorithm (section 3). Finally we discuss some technical points and conclude (sections 4 and 5). 2 Interaction Grammars We briefly introduce IGs as in (Guillaume and Perrier, 2008)1. However, we omit polarized feature structures, for the sake of exposition. 2.1 Polarized Tree Descriptions The structures associated with words by the lexicon are Polarized Tree Descriptions (PTDs). They represent fragments of parse trees. The nodes of these structures are labelled with a category and a 1This paper also discusses the linguistic motivations behind IGs. polarity. IGs use 4 polarities, P = {→,←, =, ∼}, namely positive, negative, neutral and virtual. A multiset of polarities is superposable2 if it contains at most one → and at most one ←. A multiset of polarities is saturated i</context>
</contexts>
<marker>Guillaume, Perrier, 2008</marker>
<rawString>B. Guillaume and G. Perrier. 2008. Interaction Grammars. Research Report RR-6621, INRIA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>L Kallmeyer</author>
<author>Y Parmentier</author>
</authors>
<title>On the relation between TT-MCTAG and RCG.</title>
<date>2008</date>
<booktitle>In Proceedings of LATA.</booktitle>
<marker>Kallmeyer, Parmentier, 2008</marker>
<rawString>L. Kallmeyer and Y. Parmentier. 2008. On the relation between TT-MCTAG and RCG. In Proceedings of LATA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M J Nederhof</author>
<author>G Satta</author>
<author>S Shieber</author>
</authors>
<title>Partially ordered multiset context-free grammars and ID/LP parsing.</title>
<date>2003</date>
<booktitle>In Proceedings of IWPT.</booktitle>
<contexts>
<context position="12660" citStr="Nederhof et al., 2003" startWordPosition="2531" endWordPosition="2534">ficient lexical filters have been developed for IGs (Bonfante et al., 2006) that drastically decrease the number of PTDs to consider. In p3 and s, constrained nodes can be chosen as antecedents (nodes in FC). There is again an exponential number of such choices. But in existing IGs, nodes have at most one successor by &gt;* and there is no chain of nodes in relation by &gt;*. Consequently, |FC |can be bounded by |HC U NC|. In p3, daughters must be partitioned. Instead of building all these partitions in p3 and generating many useless items, one can think of a lazy approach like the one proposed by (Nederhof et al., 2003) for pomset-CFGS. It can be noticed that the completion rule, while having the most positional indexes, is not a particular source of non-determinism. 5 Conclusion We presented a parsing algorithm for IGs. Although we used a simplified version without polarized feature structures, adding a unification mechanism shouldn’t be an issue. The novelty of this presentation is the use of deductive parsing for a formalism developed in the model-theoretic framework (Pullum and Scholz, 2001). This change of perspective provides new insights on the causes of non-determinism. It is a first step to a precis</context>
</contexts>
<marker>Nederhof, Satta, Shieber, 2003</marker>
<rawString>M.J. Nederhof, G. Satta, and S. Shieber. 2003. Partially ordered multiset context-free grammars and ID/LP parsing. In Proceedings of IWPT.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G Pullum</author>
<author>B Scholz</author>
</authors>
<title>On the distinction between model-theoretic and generative-enumerative syntactic frameworks.</title>
<date>2001</date>
<booktitle>In Proccedings of LACL.</booktitle>
<marker>Pullum, Scholz, 2001</marker>
<rawString>G. Pullum and B. Scholz. 2001. On the distinction between model-theoretic and generative-enumerative syntactic frameworks. In Proccedings of LACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Rambow</author>
<author>K Vijay-Shanker</author>
<author>D Weir</author>
</authors>
<title>Dtree grammars.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL.</booktitle>
<contexts>
<context position="3441" citStr="Rambow et al., 1995" startWordPosition="597" endWordPosition="600">ars An IG is a tuple G = {E, C, 5, P, phon}, where E is the terminal alphabet, C the non-terminal alphabet, 5 ∈ C the initial symbol, P is a set of PTDs with node labels in C × P, and phon is a function from anchors in P to E. The structure obtained from parsing is a syntactic tree, a totally ordered tree in which all nodes are labelled with a non-terminal. We call lab(A) the label of node A. If a leaf L is labelled with a terminal, this terminal is denoted word(L). 2This name comes from the superposition introduced in previous presentations of IGs. 3For readers familiar with D-Tree Grammars (Rambow et al., 1995), &gt; adds an i-edge while &gt;∗ adds a d-edge. 65 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 65–68, Paris, October 2009. c�2009 Association for Computational Linguistics We will write M » N if the node M is the mother of N and N » [N1,..., Nk] if the N is the mother of the ordered list of nodes [N1,... , Nk]. The order between siblings can also be expressed using the relation --&lt;--&lt;: M --&lt;--&lt; N means that N is the immediate successor of M. --&lt;--&lt;+ is the transitive closure of --&lt;--&lt; and »* the reflexive transitive closure of ». We define the phonological</context>
</contexts>
<marker>Rambow, Vijay-Shanker, Weir, 1995</marker>
<rawString>O. Rambow, K. Vijay-Shanker, and D. Weir. 1995. Dtree grammars. In Proceedings of ACL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Shieber</author>
<author>Y Schabes</author>
<author>F Pereira</author>
</authors>
<title>Principles and implementation of deductive parsing.</title>
<date>1995</date>
<journal>Journal of Logic Programming,</journal>
<pages>24--1</pages>
<contexts>
<context position="1031" citStr="Shieber et al., 1995" startWordPosition="147" endWordPosition="150">G) (Guillaume and Perrier, 2008) is a lexicalized grammatical formalism that primarily focuses on valency, explicitly expressed using polarities decorating syntagms. These polarities and the use of underspecified structures naturally lead parsing to be viewed as a constraint-solving problem – for example (Bonfante et al.) reduce the parsing problem to a graphrewriting problem in (2003) . However, in this article we depart from this approach and present an algorithm close to (Earley, 1970) for context-free grammars. We introduce this algorithm using the standard framework of deductive parsing (Shieber et al., 1995). This article is organised as follows: we first present IGs (section 2), then we describe the algorithm (section 3). Finally we discuss some technical points and conclude (sections 4 and 5). 2 Interaction Grammars We briefly introduce IGs as in (Guillaume and Perrier, 2008)1. However, we omit polarized feature structures, for the sake of exposition. 2.1 Polarized Tree Descriptions The structures associated with words by the lexicon are Polarized Tree Descriptions (PTDs). They represent fragments of parse trees. The nodes of these structures are labelled with a category and a 1This paper also </context>
<context position="5118" citStr="Shieber et al., 1995" startWordPosition="959" endWordPosition="962">&lt; N then I(M) --&lt;--&lt; I(N) 5. if M --&lt;+ N then I(M) --&lt;--&lt;+ I(N) 6. if A » B then there exists M E I−1(A) and N E I−1(B) such that M &gt; N 7. lab(A) = lab(M) for all M E I−1(A) 8. if phon(M) = w then PP(I(M)) = [w] Given an IG !9 = {E, C, 5, P, phon} and a sentence s = w1, ... , wn in E*, a syntactic tree T is a parse tree for s if there exists a multiset of PTDs D from P such that the root node R of T is labelled with 5 and PP(R) = [w1, ... , wn]. The language generated by !9 is the set of strings in E* for which there is a parse tree. 3 Parsing Algorithm We use the deductive parsing framework (Shieber et al., 1995). A state of the parser is encoded as an item, created by applying deductive rules. Our algorithm resembles the Earley algorithm for CFGs and uses the same rules : prediction, scanning and completion. 3.1 Items Items [A(H, N, F) — α • Q, i, j, (O, U, D)] consist of a dotted rule, 2 position indexes and a 3- tuple of sets of constrained nodes. The dotted rule A(H, N, F) — α • Q means that there exists a node A in the parse tree with antecedents H UN UF. Elements of the sequence α are also nodes of the parse tree. For the sequence Q, the elements have the form Bk(Hk) where Bk is a node of the pa</context>
</contexts>
<marker>Shieber, Schabes, Pereira, 1995</marker>
<rawString>S. Shieber, Y. Schabes, and F. Pereira. 1995. Principles and implementation of deductive parsing. Journal of Logic Programming, 24(1–2):3–36.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>