<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.986876">
Retrieving Correct Semantic Boundaries in Dependency Structure
</title>
<author confidence="0.995542">
Jinho D. Choi Martha Palmer
</author>
<affiliation confidence="0.998307">
Department of Computer Science Department of Linguistics
University of Colorado at Boulder University of Colorado at Boulder
</affiliation>
<email confidence="0.997868">
choijd@colorado.edu martha.palmer@colorado.edu
</email>
<sectionHeader confidence="0.993861" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999993583333333">
This paper describes the retrieval of cor-
rect semantic boundaries for predicate-
argument structures annotated by depen-
dency structure. Unlike phrase structure,
in which arguments are annotated at the
phrase level, dependency structure does
not have phrases so the argument labels are
associated with head words instead: the
subtree of each head word is assumed to
include the same set of words as the an-
notated phrase does in phrase structure.
However, at least in English, retrieving
such subtrees does not always guarantee
retrieval of the correct phrase boundaries.
In this paper, we present heuristics that
retrieve correct phrase boundaries for se-
mantic arguments, called semantic bound-
aries, from dependency trees. By apply-
ing heuristics, we achieved an F1-score
of 99.54% for correct representation of
semantic boundaries. Furthermore, error
analysis showed that some of the errors
could also be considered correct, depend-
ing on the interpretation of the annotation.
</bodyText>
<sectionHeader confidence="0.999132" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.991834694444445">
Dependency structure has recently gained wide in-
terest because it is simple yet provides useful in-
formation for many NLP tasks such as sentiment
analysis (Kessler and Nicolov, 2009) or machine
translation (Gildea, 2004). Although dependency
structure is a kind of syntactic structure, it is quite
different from phrase structure: phrase structure
gives phrase information by grouping constituents
whereas dependency structure gives dependency
relations between pairs of words. Many depen-
dency relations (e.g., subject, object) have high
correlations with semantic roles (e.g., agent, pa-
tient), which makes dependency structure suit-
able for representing semantic information such as
predicate-argument structure.
In 2009, the Conference on Computational Nat-
ural Language Learning (CoNLL) opened a shared
task: the participants were supposed to take de-
pendency trees as input and produce semantic role
labels as output (Hajiˇc et al., 2009). The depen-
dency trees were automatically converted from the
Penn Treebank (Marcus et al., 1993), which con-
sists of phrase structure trees, using some heuris-
tics (cf. Section 3). The semantic roles were ex-
tracted from the Propbank (Palmer et al., 2005).
Since Propbank arguments were originally anno-
tated at the phrase level using the Penn Treebank
and the phrase information got lost during the con-
version to the dependency trees, arguments are an-
notated on head words instead of phrases in depen-
dency trees; the subtree of each head word is as-
sumed to include the same set of words as the an-
notated phrase does in phrase structure. Figure 1
shows a dependency tree that has been converted
from the corresponding phrase structure tree.
root The results appear in today &apos;s news
</bodyText>
<figureCaption confidence="0.99976">
Figure 1: Phrase vs. dependency structure
</figureCaption>
<figure confidence="0.998847173913043">
VP
NP1
IN
NP
S
DT NNS
The results
VBP
PP1
appear
NN
today
POS
’s
news
ROOT PMOD
NMOD
NMOD
NMOD SBJ
LOC
in
NP
NN
</figure>
<page confidence="0.989782">
91
</page>
<note confidence="0.9556295">
Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 91–99,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.999678038461538">
In the phrase structure tree, arguments of the verb
predicate appear are annotated on the phrases:
NP1 as ARG0 and PP1 as ARGM-LOC. In the de-
pendency tree, the arguments are annotated on the
head words instead: results as the ARG0 and in as
the ARGM-LOC. In this example, both PP1 and the
subtree of in consist of the same set of words {in,
today, ’s, news} (as is the case for NP1 and the
subtree of results); therefore, the phrase bound-
aries for the semantic arguments, called semantic
boundaries, are retrieved correctly from the depen-
dency tree.
Retrieving the subtrees of head words usually
gives correct semantic boundaries; however, there
are cases where the strategy does not work. For
example, if the verb predicate is a gerund or a past-
participle, it is possible that the predicate becomes
a syntactic child of the head word annotated as a
semantic argument of the predicate. In Figure 2,
the head word plant is annotated as ARG1 of the
verb predicate owned, where owned is a child of
plant in the dependency tree. Thus, retrieving the
subtree of plant would include the predicate it-
self, which is not the correct semantic boundary
for the argument (the correct boundary would be
only {The, plant}).
</bodyText>
<figureCaption confidence="0.996002">
Figure 2: Past-participle example
</figureCaption>
<bodyText confidence="0.9997666">
For such cases, we need some alternative for re-
trieving the correct semantic boundaries. This is
an important issue that has not yet been thoroughly
addressed. In this paper, we first show how to con-
vert the Penn Treebank style phrase structure to
dependency structure. We then describe how to
annotate the Propbank arguments, already anno-
tated in the phrase structure, on head words in the
dependency structure. Finally, we present heuris-
tics that correctly retrieve semantic boundaries in
most cases. For our experiments, we used the en-
tire Penn Treebank (Wall Street Journal). Our ex-
periments show that it is possible to achieve an F1-
score of 99.54% for correct representation of the
semantic boundaries.
</bodyText>
<sectionHeader confidence="0.999647" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999943">
Ekeklint and Nivre (2007) tried to retrieve seman-
tic boundaries by adding extra arcs to dependency
trees, so the structure is no longer a tree but a
graph. They experimented with the same cor-
pus, the Penn Treebank, but used a different de-
pendency conversion tool, Penn2Malt.1 Our work
is distinguished from theirs because we keep the
tree structure but use heuristics to find the bound-
aries. Johansson (2008) also tried to find seman-
tic boundaries for evaluation of his semantic role
labeling system using dependency structure. He
used heuristics that apply to general cases whereas
we add more detailed heuristics for specific cases.
</bodyText>
<sectionHeader confidence="0.78275" genericHeader="method">
3 Converting phrase structure to
dependency structure
</sectionHeader>
<bodyText confidence="0.999991">
We used the same tool as the one used for the
CoNLL’09 shared task to automatically convert
the phrase structure trees in the Penn Treebank
to the dependency trees (Johansson and Nugues,
2007). The script gives several options for the con-
version; we mostly used the default values except
for the following options:2
</bodyText>
<listItem confidence="0.998724176470588">
• splitSlash=false: do not split slashes. This
option is taken so the dependency trees pre-
serve the same number of word-tokens as the
original phrase structure trees.
• noSecEdges=true: ignore secondary edges
if present. This option is taken so all sib-
lings of verb predicates in phrase structure
become children of the verbs in dependency
structure regardless of empty categories. Fig-
ure 3 shows the converted dependency tree,
which is produced when the secondary edge
(*ICH*) is not ignored, and Figure 4 shows
the one produced by ignoring the secondary
edge. This option is useful because NP* and
PP-2* are annotated as separate arguments of
the verb predicate paid in Propbank (NP* as
ARG1 and PP-2* as ARGM-MNR).
</listItem>
<footnote confidence="0.999966">
1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html
2http://nlp.cs.lth.se/software/treebank converter/
</footnote>
<figure confidence="0.976542227272727">
was
PP-2*
NP*
NP-1
VP
He
VBD
VP
S
NP
VBN
paid
*-1
PP
with..
NP
.. salary
*ICH*-2
NMOD NMOD LGS PMOD
The plant owned by Mark
92
root He was paid a $342K salary with a $280K bonus
</figure>
<figureCaption confidence="0.777385">
Figure 3: When the secondary edge is not ignored
</figureCaption>
<figure confidence="0.949099470588235">
ROOT
SBJ
VC
NMOD
NMOD
NMOD
OBJ
NMOD
NMOD
NMOD
Input: Sp = a set of words for each argument
in the Propbank
Output: Sd = a set of head words whose
subtrees cover all words in Sp
1 Algorithm:getHeadWords(Sp)
2 Sd = {}
root He was paid a $342K salary with a $280K bonus
</figure>
<figureCaption confidence="0.999251">
Figure 4: When the secondary edge is ignored
</figureCaption>
<bodyText confidence="0.9266355">
Total 49,208 dependency trees were converted
from the Penn Treebank. Although it was pos-
sible to apply different values for other options,
we found them not helpful in finding correct se-
mantic boundaries of Propbank arguments. Note
that some of non-projective dependencies are re-
moved by ignoring the secondary edges. However,
it did not make all dependency trees projective;
our methods can be applied for either projective
or non-projective dependency trees.
4 Adding semantic roles to dependency
structure
</bodyText>
<subsectionHeader confidence="0.99871">
4.1 Finding the head words
</subsectionHeader>
<bodyText confidence="0.999888913043478">
For each argument in the Propbank annotated on
a phrase, we extracted the set of words belonging
to the phrase. Let this set be Sp. In Figure 1, PP1
is the ARGM-LOC of appear so Sp is {in, today,
’s, news}. Next, we found a set of head words,
say Sd, whose subtrees cover all words in Sp (e.g.,
Sd = {in} in Figure 1). It would be ideal if there
existed one head word whose subtree covers all
words in Sp, but this is not always the case. It is
possible that Sd needs more than one head word to
cover all the words in Sp.
Figure 5 shows an algorithm that finds a set of
head words Sd whose subtrees cover all words in
Sp. For each word w in Sp, the algorithm checks
if w’s subtree gives the maximum coverage (if w’s
subtree contains more words than any other sub-
tree); if it does, the algorithm adds w to Sd, re-
moves all words in w’s subtree from Sp, then re-
peats the search. The search ends when all words
in Sp are covered by some subtree of a head word
in Sd. Notice that the algorithm searches for the
minimum number of head words by matching the
maximum coverages.
</bodyText>
<figure confidence="0.998896363636364">
3 while Sp =74 0 do
4 max = None
5 foreach w E Sp do
6 if |subtree(w) |&gt; |subtree(max)|
then
7 max = w
8 end
9 Sd.add(max)
10 Sp.removeAll(subtree(max))
11 end
12 return Sd
</figure>
<figureCaption confidence="0.99994">
Figure 5: Finding the min-set of head words
</figureCaption>
<bodyText confidence="0.999996428571429">
The algorithm guarantees to find the min-set Sd
whose subtrees cover all words in Sp. This gives
100% recall for Sd compared to Sp; however, the
precision is not guaranteed to be as perfect. Sec-
tion 5 illustrates heuristics that remove the over-
generated words so we could improve the preci-
sion as well.
</bodyText>
<subsectionHeader confidence="0.999041">
4.2 Ignoring empty categories
</subsectionHeader>
<bodyText confidence="0.999985153846154">
As described in Figures 3 and 4, dependency trees
do not include any empty categories (e.g., null
elements, traces, PRO’s): the empty categories
are dropped during the conversion to the depen-
dency trees. In the Penn Treebank, 11.5% of the
Propbank arguments are annotated on empty cat-
egories. Although this is a fair amount, we de-
cided to ignore them for now since dependency
structure is not naturally designed to handle empty
categories. Nonetheless, we are in the process of
finding ways of automatically adding empty cate-
gories to dependency trees so we can deal with the
remaining of 11.5% Propbank arguments.
</bodyText>
<subsectionHeader confidence="0.998899">
4.3 Handling disjoint arguments
</subsectionHeader>
<bodyText confidence="0.999689714285714">
Some Propbank arguments are disjoint in the
phrase structure so that they cannot be represented
as single head words in dependency trees. For ex-
ample in Figure 6, both NP-1* and S* are ARG1 of
the verb predicate continued but there is no head
word for the dependency tree that can represent
both phrases. The algorithm in Figure 5 naturally
</bodyText>
<figure confidence="0.9989413">
ROOT
SBJ
VC
ADV
OBJ
NMOD
NMOD
NMOD
NMOD
NMOD
</figure>
<page confidence="0.993191">
93
</page>
<bodyText confidence="0.99985875">
handles this kind of disjoint arguments. Although
words in Sp are not entirely consecutive ({Yields,
on, mutual, funds, to, slide}), it iteratively finds
both head words correctly: Yields and to.
</bodyText>
<equation confidence="0.302025">
S
</equation>
<sectionHeader confidence="0.908091" genericHeader="method">
5 Retrieving fine-grained semantic
boundaries
</sectionHeader>
<bodyText confidence="0.999997846153846">
There are a total of 292,073 Propbank arguments
in the Penn Treebank, and only 88% of them map
to correct semantic boundaries from the depen-
dency trees by taking the subtrees of head words.
The errors are typically caused by including more
words than required: the recall is still 100% for the
error cases whereas the precision is not. Among
several error cases, the most critical one is caused
by verb predicates whose semantic arguments are
the parents of themselves in the dependency trees
(cf. Figure 2). In this section, we present heuris-
tics to handle such cases so we can achieve preci-
sion nearly as good as the recall.
</bodyText>
<subsectionHeader confidence="0.991767">
5.1 Modals
</subsectionHeader>
<bodyText confidence="0.9994385">
In the current dependency structure, modals (e.g.,
will, can, do) become the heads of the main verbs.
In Figure 7, will is the head of the verb predicate
remain in the dependency tree; however, it is also
an argument (ARGM-MOD) of the verb in Prop-
bank. This can be resolved by retrieving only the
head word, but not the subtree. Thus, only will is
retrieved as the ARGM-MOD of remain.
Modals can be followed by conjuncts that are
also modals. In this case, the entire coordination
is retrieved as ARGM-MOD (e.g., {may, or, may,
not} in Figure 8).
</bodyText>
<figureCaption confidence="0.873971333333333">
Figure 7: Modal example 1
root He may or may not read the book
Figure 8: Modal example 2
</figureCaption>
<subsectionHeader confidence="0.993288">
5.2 Negations
</subsectionHeader>
<bodyText confidence="0.9999354">
Negations (e.g., not, no longer) are annotated as
ARGM-NEG in Propbank. In most cases, nega-
tions do not have any child in dependency trees,
so retrieving only the negations themselves gives
the correct semantic boundaries for ARGM-NEG,
but there are exceptions. One is where a negation
comes after a conjunction; in which case, the nega-
tion becomes the parent of the main verb. In Fig-
ure 9, not is the parent of the verb predicate copy
although it is the ARGM-NEG of the verb.
</bodyText>
<subsectionHeader confidence="0.99919">
5.3 Overlapping arguments
</subsectionHeader>
<bodyText confidence="0.99989725">
Propbank does not allow overlapping arguments.
For each predicate, if a word is included in one
argument, it cannot be included in any other argu-
ment of the predicate. In Figure 11, burdens and
in the region are annotated as ARG1 and ARGM-
LOC of the verb predicate share, respectively. The
arguments were originally annotated as two sepa-
rate phrases in the phrase structure tree; however,
</bodyText>
<figure confidence="0.994346083333334">
S*
VP
NP-1*
to
slide
Yields
IN
NP
continued
NP
VP
root Yields on mutual funds continued to slide
</figure>
<figureCaption confidence="0.868715">
Figure 6: Disjoint argument example
</figureCaption>
<figure confidence="0.999696384615385">
IM
OPRD
NMOD
ROOT
SBJ
PMOD
NMOD
on mutual funds
*-1
VP
TO
VBD
NP
PP
ROOT
VC PRD
They will remain on the list
PRD
NMOD
SBJ
root
ROOT
COORD
SBJ COORD CONJ ADV
OBJ
NMOD
</figure>
<figureCaption confidence="0.999919">
Figure 9: Negation example 1
</figureCaption>
<bodyText confidence="0.5805518">
The other case is where a negation is modified by
some adverb; in which case, the adverb should
also be retrieved as well as the negation. In Fig-
ure 10, both no and longer should be retrieved as
the ARGM-NEG of the verb predicate oppose.
</bodyText>
<figure confidence="0.972150333333333">
root
You may come to read but not
ROOT
SBJ VC PRP IM COORD CONJ COORD
copy
root They no longer oppose the legislation
</figure>
<figureCaption confidence="0.94489">
Figure 10: Negation example 2
</figureCaption>
<figure confidence="0.781197166666667">
OBJ
NMOD
ROOT
SBJ
TMP
AMOD
</figure>
<page confidence="0.994178">
94
</page>
<bodyText confidence="0.997973333333333">
in became the child of burdens during the conver-
sion, so the subtree of burdens includes the subtree
of in, which causes overlapping arguments.
</bodyText>
<figure confidence="0.659065">
S
root U.S. encourages Japan to share burdens in the region
</figure>
<figureCaption confidence="0.997782">
Figure 11: Overlapping argument example 1
</figureCaption>
<bodyText confidence="0.9996154">
When this happens, we reconstruct the depen-
dency tree so in becomes the child of share instead
of burdens (Figure 12). By doing so, taking the
subtrees of burdens and in no longer causes over-
lapping arguments.3
</bodyText>
<subsubsectionHeader confidence="0.558023">
5.4.1 Verb chains
</subsubsectionHeader>
<bodyText confidence="0.999607260869565">
Three kinds of verb chains exist in the current
dependency structure: auxiliary verbs (including
modals and be-verbs), infinitive markers, and con-
junctions. As discussed in Section 5.1, verb chains
become the parents of their main verbs in depen-
dency trees. This indicates that when the subtree
of the main verb is to be excluded from semantic
arguments, the verb chain needs to be excluded as
well. This usually happens when the main verbs
are used within relative clauses. In addition, more
heuristics are needed for retrieving correct seman-
tic boundaries for relative clauses, which are fur-
ther discussed in Section 5.4.2.
The following figures show examples of each
kind of verb chain. It is possible that multiple verb
chains are joined with one main verb. In this case,
we find the top-most verb chain and exclude its
entire subtree from the semantic argument. In Fig-
ure 13, part is annotated as ARG1 of the verb pred-
icate gone, chained with the auxiliary verb be, and
again chained with the modal may. Since may is
the top-most verb chain, we exclude its subtree so
only a part is retrieved as the ARG1 of gone.
</bodyText>
<figure confidence="0.99826">
NP VP
encourages
NP
VP
VBZ
S
Japan
TO
VP
to
NP
VB
U.S.
share
NP
burdens
PP
in ..
OPRD
OBJ IM OBJ
ROOT
SBJ
PMOD
LOC
NMOD
a part that may be gone
NMOD
DEP VC PRD
NMOD
ROOT OPRD LOC
</figure>
<figureCaption confidence="0.78856">
Figure 13: Auxiliary verb example
</figureCaption>
<figure confidence="0.99177525">
PMOD
SBJ
OBJ IM OBJ
NMOD
</figure>
<figureCaption confidence="0.801551">
root U.S. encourages Japan to share burdens in the region
Figure 12: Overlapping argument example 2
</figureCaption>
<subsectionHeader confidence="0.496521">
5.4 Verb predicates whose semantic
arguments are their syntactic heads
</subsectionHeader>
<bodyText confidence="0.99974475">
There are several cases where semantic arguments
of verb predicates become the syntactic heads of
the verbs. The modals and negations in the previ-
ous sections are special cases where the seman-
tic boundaries can be retrieved correctly with-
out compromising recall. The following sec-
tions describe other cases, such as relative clauses
(Section 5.4.2), gerunds and past-participles (Sec-
tion 5.4.3), that may cause a slight decrease in re-
call by finding more fine-grained semantic bound-
aries. In these cases, the subtree of the verb predi-
cates are excluded from the semantic arguments.
</bodyText>
<footnote confidence="0.9356405">
3This can be considered as a Treebank/Propbank dis-
agreement, which is further discussed in Sectino 6.2.
</footnote>
<figureCaption confidence="0.585091">
Figure 14 shows the case of infinitive markers.
those is annotated as ARG0 of the verb predicate
</figureCaption>
<bodyText confidence="0.99171725">
leave, which is first chained with the infinitive
marker to then chained with the verb required. By
excluding the subtree of required, only those is re-
trieved as the ARG0 of leave.
</bodyText>
<figure confidence="0.462877">
ROOT
PRD AMOD PMOD APPO OPRD IM
root rules are tough on those
</figure>
<figureCaption confidence="0.977468">
Figure 14: Infinitive marker example
</figureCaption>
<bodyText confidence="0.870849727272727">
Figure 15 shows the case of conjunctions. people
is annotated as ARG0 of the verb predicate exceed,
which is first chained with or then chained with
meet. By excluding the subtree of meet, only peo-
ple is retrieved as the ARG0 of exceed.
When a verb predicate is followed by an ob-
ject complement (OPRD), the subtree of the object
complement is not excluded from the semantic ar-
gument. In Figure 16, distribution is annotated as
SBJ
required to leave
</bodyText>
<page confidence="0.804346">
95
</page>
<figure confidence="0.996293166666667">
people who meet or exceed the expectation
NMOD
DEP
COORD CONJ
OBJ
NMOD
</figure>
<figureCaption confidence="0.999937">
Figure 15: Conjunction example
</figureCaption>
<bodyText confidence="0.997014714285714">
ARG1 of the verb predicate expected. By excluding
the subtree of expected, the object complement to
occur would be excluded as well; however, Prop-
bank annotation requires keeping the object com-
plement as the part of the argument. Thus, a dis-
tribution to occur is retrieved as the ARG1 of ex-
pected.
</bodyText>
<figure confidence="0.728009">
NMOD APPO OPRD IM
a distribution expected to occur
</figure>
<figureCaption confidence="0.994111">
Figure 16: Object complement example
</figureCaption>
<subsectionHeader confidence="0.523535">
5.4.2 Relative clauses
</subsectionHeader>
<bodyText confidence="0.999644157894737">
When a verb predicate is within a relative clause,
Propbank annotates both the relativizer (if present)
and its antecedent as part of the argument. For ex-
ample in Figure 15, people is annotated as ARG0
of both meet and exceed. By excluding the subtree
of meet, the relativizer who is also excluded from
the semantic argument, which is different from the
original Propbank annotation. In this case, we
keep the relativizer as part of the ARG0; thus, peo-
ple who is retrieved as the ARG0 (similarly, a part
that is retrieved as the ARG0 of gone in Figure 13).
It is possible that a relativizer is headed by a
preposition. In Figure 17, climate is annotated as
ARGM-LOC of the verb predicate made and the
relativizer which is headed by the preposition in.
In this case, both the relativizer and the preposi-
tion are included in the semantic argument. Thus,
the climate in which becomes the ARGM-LOC of
made.
</bodyText>
<figureCaption confidence="0.946031">
Figure 17: Relativizer example
</figureCaption>
<subsectionHeader confidence="0.661207">
5.4.3 Gerunds and past-participles
</subsectionHeader>
<bodyText confidence="0.992490944444445">
In English, when gerunds and past-participles are
used without the presence of be-verbs, they often
function as noun modifiers. Propbank still treats
them as verb predicates; however, these verbs be-
come children of the nouns they modify in the de-
pendency structure, so the heuristics discussed in
Section 5.4 and 5.4.1 need to be applied to find the
correct semantic boundaries. Furthermore, since
these are special kinds of verbs, they require even
more rigorous pruning.
When a head word, annotated to be a seman-
tic argument of a verb predicate, comes after the
verb, every word prior to the verb predicate needs
to be excluded from the semantic argument. In
Figure 18, group is annotated as ARG0 of the
verb predicate publishing, so all words prior to the
predicate (the Dutch) need to be excluded. Thus,
only group is retrieved as the ARG0 of publishing.
</bodyText>
<figureCaption confidence="0.964835">
Figure 18: Gerund example
</figureCaption>
<bodyText confidence="0.998560222222222">
When the head word comes before the verb pred-
icate, the subtree of the head word, excluding the
subtree of the verb predicate, is retrieved as the se-
mantic argument. In Figure 19, correspondence is
annotated as ARG1 of the verb predicate mailed,
so the subtree of correspondence, excluding the
subtree of mailed, is retrieved to be the argument.
Thus, correspondence about incomplete 8300s be-
comes the ARG1 of mailed.
</bodyText>
<figureCaption confidence="0.948837">
Figure 19: Past-participle example 1
</figureCaption>
<bodyText confidence="0.999984727272727">
When the subtree of the verb predicate is imme-
diately followed by comma-like punctuation (e.g.,
comma, colon, semi-colon, etc.) and the head
word comes before the predicate, every word after
the punctuation is excluded from the semantic ar-
gument. In Figure 20, fellow is annotated as ARG1
of the verb predicate named, so both the subtree
of the verb (named John) and every word after the
comma (, who stayed for years) are excluded from
the semantic argument. Thus, only a fellow is re-
trieved as the ARG1 of named.
</bodyText>
<subsectionHeader confidence="0.987467">
5.5 Punctuation
</subsectionHeader>
<bodyText confidence="0.999627">
For evaluation, we built a model that excludes
punctuation from semantic boundaries for two rea-
sons. First, it is often not clear how punctuation
</bodyText>
<figure confidence="0.983911689655172">
the climate in which the decisions was made
LOC
DEP
PMOD
NMOD
VC
NMOD
NMOD
NMOD
NMOD
the Dutch publishing group
NMOD
NMOD
correspondence mailed about incomplete 8300s
PMOD
NMOD
96
a fellow named John , who stayed for years
NMOD
P
APPO OPRD
DEP
TMP PMOD
NMOD
Accuracy =
1
T ·
Barg
c(gold(arg),sys(arg))
</figure>
<figureCaption confidence="0.998925">
Figure 20: Past-participle example 2
</figureCaption>
<bodyText confidence="0.999534272727273">
needs to be annotated in either Treebank or Prop-
bank; because of that, annotation for punctuation
is not entirely consistent, which makes it hard to
evaluate. Second, although punctuation gives use-
ful information for obtaining semantic boundaries,
it is not crucial for semantic roles. In fact, some
of the state-of-art semantic role labeling systems,
such as ASSERT (Pradhan et al., 2004), give an
option for omitting punctuation from the output.
For these reasons, our final model ignores punctu-
ation for semantic boundaries.
</bodyText>
<sectionHeader confidence="0.99951" genericHeader="method">
6 Evaluations
</sectionHeader>
<subsectionHeader confidence="0.999942">
6.1 Model comparisons
</subsectionHeader>
<bodyText confidence="0.999980888888889">
The following list describes six models used for
the experiments. Model I is the baseline approach
that retrieves all words in the subtrees of head
words as semantic boundaries. Model II to VI use
the heuristics discussed in the previous sections.
Each model inherits all the heuristics from the pre-
vious model and adds new heuristics; therefore,
each model is expected to perform better than the
previous model.
</bodyText>
<listItem confidence="0.999570571428571">
• I - all words in the subtrees (baseline)
• II - modals + negations (Sections 5.1, 5.2)
• III - overlapping arguments (Section 5.3)
• IV - verb chains + relative clauses (Sec-
tions 5.4.1, 5.4.2)
• V - gerunds + past-participles (Section 5.4.3)
• VI - excluding punctuations (Section 5.5)
</listItem>
<bodyText confidence="0.999954">
The following list shows measurements used for
the evaluations. gold(arg) is the gold-standard
set of words for the argument arg. sys(arg) is
the set of words for arg produced by our system.
c(arg1, arg2) returns 1 if arg1 is equal to arg2;
otherwise, returns 0. T is the total number of ar-
guments in the Propbank.
</bodyText>
<figure confidence="0.856149666666667">
|gold(arg) n sys(arg)|
|sys(arg)|
|gold(arg) n sys(arg)|
|gold(arg)|
2 · Precision · Recall
Precision + Recall
</figure>
<bodyText confidence="0.999457">
Table 1 shows the results from the models us-
ing the measurements. As expected, each model
shows improvement over the previous one in
terms of accuracy and F1-score. The F1-score
of Model VI shows improvement that is statisti-
cally significant compared to Model I using t-test
(t = 149.00, p &lt; 0.0001). The result from the
final model is encouraging because it enables us
to take full advantage of dependency structure for
semantic role labeling. Without finding the correct
semantic boundaries, even if a semantic role label-
ing system did an excellent job finding the right
head words, we would not be able to find the ac-
tual chunks for the arguments. By using our ap-
proach, finding the correct semantic boundaries is
no longer an issue for using dependency structure
for automatic semantic role labeling.
</bodyText>
<table confidence="0.998823571428571">
Model Accuracy Precision Recall F1
I 88.00 92.51 100 96.11
II 91.84 95.77 100 97.84
III 92.17 97.08 100 98.52
IV 95.89 98.51 99.95 99.23
V 97.00 98.94 99.95 99.44
VI 98.20 99.14 99.95 99.54
</table>
<tableCaption confidence="0.999711">
Table 1: Model comparisons (in percentage)
</tableCaption>
<subsectionHeader confidence="0.995424">
6.2 Error analysis
</subsectionHeader>
<bodyText confidence="0.999892222222222">
Although each model consistently shows improve-
ment on the precision, the recall is reduced a bit for
some models. Specifically, the recalls for Mod-
els II and III are not 100% but rather 99.9994%
and 99.996%, respectively. We manually checked
all errors for Models II and III and found that they
are caused by inconsistent annotations in the gold-
standard. For Model II, Propbank annotation for
ARGM-MOD was not done consistently with con-
</bodyText>
<figure confidence="0.989961222222222">
Precision = 1
T ·
Barg
1
Recall =
�
T ·
Barg
F1 =
</figure>
<page confidence="0.997188">
97
</page>
<bodyText confidence="0.999977928571429">
junctions. For example in Figure 8, instead of an-
notating may or may not as the ARGM-MOD, some
annotations include only may and may not but not
the conjunction or. Since our system consistently
included the conjunctions, they appeared to be dif-
ferent from the gold-standard, but are not errors.
For Model III, Treebank annotation was not
done consistently for adverbs modifying nega-
tions. For example in Figure 10, longer is some-
times (but rarely) annotated as an adjective where
it is supposed to be an adverb. Furthermore,
longer sometimes becomes a child of the verb
predicate oppose (instead of being the child of no).
Such annotations made our system exclude longer
as a part of ARGM-NEG, but it would have found
them correctly if the trees were annotated consis-
tently.
There are a few cases that caused errors in Mod-
els IV and V. The most critical one is caused by PP
(prepositional phrase) attachment. In Figure 21,
enthusiasm is annotated as ARG1 of the verb pred-
icate showed, so our system retrieved the subtree
of enthusiasm, excluding the subtree of showed,
as the semantic boundary for the ARG1 (e.g., the
enthusiasm). However, Propbank originally an-
notated both the enthusiasm and for stocks as the
ARG1 in the phrase structure tree (so the preposi-
tional phrase got lost in our system).
</bodyText>
<figureCaption confidence="0.947238">
Figure 21: PP-attachment example 1
</figureCaption>
<bodyText confidence="0.999991363636364">
This happens when there is a disagreement be-
tween Treebank and Propbank annotations: the
Treebank annotation attached the PP (for stocks)
to the verb (showed) whereas the Propbank anno-
tation attached the PP to the noun (enthusiasm).
This is a potential error in the Treebank. In this
case, we can trust the Propbank annotation and re-
construct the tree so the Treebank and Propbank
annotations agree with each other. After the re-
construction, the dependency tree would look like
one in Figure 22.
</bodyText>
<sectionHeader confidence="0.90543" genericHeader="conclusions">
7 Conclusion and future work
</sectionHeader>
<bodyText confidence="0.999985565217391">
We have discussed how to convert phrase struc-
ture trees to dependency trees, how to find the
minimum-set of head words for Propbank argu-
ments in dependency structure, and heuristics for
retrieving fine-grained semantic boundaries. By
using our approach, we correctly retrieved the se-
mantic boundaries of 98.2% of the Propbank ar-
guments (F1-score of 99.54%). Furthermore, the
heuristics can be used to fix some of the incon-
sistencies in both Treebank and Propbank annota-
tions. Moreover, they suggest ways of reconstruct-
ing dependency structure so that it can fit better
with semantic roles.
Retrieving correct semantic boundaries is im-
portant for tasks like machine translation where
not only the head words but also all other words
matter to complete the task (Choi et al., 2009).
In the future, we are going to apply our approach
to other corpora and see how well the heuristics
work. In addition, we will try to find ways of auto-
matically adding empty categories to dependency
structure so we can deal with the full set of Prop-
bank arguments.
</bodyText>
<sectionHeader confidence="0.9982" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.998595117647059">
Special thanks are due to Professor Joakim Nivre
of Uppsala University and Claire Bonial of the
University of Colorado at Boulder for very helpful
insights. We gratefully acknowledge the support
of the National Science Foundation Grants CISE-
CRI-0551615, Towards a Comprehensive Lin-
guistic Annotation and CISE-CRI 0709167, Col-
laborative: A Multi-Representational and Multi-
Layered Treebank for Hindi/Urdu, and a grant
from the Defense Advanced Research Projects
Agency (DARPA/IPTO) under the GALE pro-
gram, DARPA/CMO Contract No. HR0011-06-C-
0022, subcontract from BBN, Inc. Any opinions,
findings, and conclusions or recommendations ex-
pressed in this material are those of the authors and
do not necessarily reflect the views of the National
Science Foundation.
</bodyText>
<figure confidence="0.9490452">
the enthusiasm investors showed for stocks
NMOD
SBJ ADV PMOD
NMOD
the enthusiasm investors showed for stocks
NMOD
ADV
NMOD
SBJ
PMOD
</figure>
<figureCaption confidence="0.998623">
Figure 22: PP-attachment example 2
</figureCaption>
<sectionHeader confidence="0.998335" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9980592">
Jinho D. Choi, Martha Palmer, and Nianwen Xue.
2009. Using parallel propbanks to enhance word-
alignments. In Proceedings of ACL-IJCNLP work-
shop on Linguistic Annotation (LAW‘09), pages
121–124.
</reference>
<page confidence="0.989811">
98
</page>
<reference confidence="0.999606488888889">
Susanne Ekeklint and Joakim Nivre. 2007. A
dependency-based conversion of propbank. In
Proceedings of NODALIDA workshop on Building
Frame Semantics Resources for Scandinavian and
Baltic Languages (FRAME’07), pages 19–25.
Daniel Gildea. 2004. Dependencies vs. constituents
for tree-based alignment. In Proceedings of the
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP’04), pages 214–221.
Jan Hajiˇc, Massimiliano Ciaramita, Richard Johans-
son, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs
M`arquez, Adam Meyers, Joakim Nivre, Sebastian
Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu,
Nianwen Xue, and Yi Zhang. 2009. The conll-2009
shared task: Syntactic and semantic dependencies in
multiple languages. In Proceedings of the 13th Con-
ference on Computational Natural Language Learn-
ing (CoNLL’09), pages 1–18.
Richard Johansson and Pierre Nugues. 2007. Ex-
tended constituent-to-dependency conversion for en-
glish. In Proceedings of the 16th Nordic Conference
of Computational Linguistics (NODALIDA’07).
Richard Johansson. 2008. Dependency-based Seman-
tic Analysis of Natural-language Text. Ph.D. thesis,
Lund University.
Jason S. Kessler and Nicolas Nicolov. 2009. Targeting
sentiment expressions through supervised ranking of
linguistic configurations. In Proceedings of the 3rd
International AAAI Conference on Weblogs and So-
cial Media (ICWSM’09).
Mitchell P. Marcus, Mary Ann Marcinkiewicz, and
Beatrice Santorini. 1993. Building a large anno-
tated corpus of english: The penn treebank. Compu-
tational Linguistics, 19(2):313–330.
Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005. The proposition bank: An annotated cor-
pus of semantic roles. Computational Linguistics,
31(1):71–106.
Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu,
James H. Martin, and Daniel Jurafsky. 2004. Shal-
low semantic parsing using support vector machines.
In Proceedings of the Human Language Technology
Conference/North American chapter of the Associ-
ation for Computational Linguistics annual meeting
(HLT/NAACL’04).
</reference>
<page confidence="0.998926">
99
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.941128">
<title confidence="0.999717">Retrieving Correct Semantic Boundaries in Dependency Structure</title>
<author confidence="0.999925">Jinho D Choi Martha Palmer</author>
<affiliation confidence="0.9997355">Department of Computer Science Department of Linguistics University of Colorado at Boulder University of Colorado at Boulder</affiliation>
<email confidence="0.999589">choijd@colorado.edumartha.palmer@colorado.edu</email>
<abstract confidence="0.9975836">This paper describes the retrieval of correct semantic boundaries for predicateargument structures annotated by dependency structure. Unlike phrase structure, in which arguments are annotated at the phrase level, dependency structure does not have phrases so the argument labels are associated with head words instead: the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. However, at least in English, retrieving such subtrees does not always guarantee retrieval of the correct phrase boundaries. In this paper, we present heuristics that retrieve correct phrase boundaries for semantic arguments, called semantic boundaries, from dependency trees. By applying heuristics, we achieved an F1-score of 99.54% for correct representation of semantic boundaries. Furthermore, error analysis showed that some of the errors could also be considered correct, depending on the interpretation of the annotation.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Jinho D Choi</author>
<author>Martha Palmer</author>
<author>Nianwen Xue</author>
</authors>
<title>Using parallel propbanks to enhance wordalignments.</title>
<date>2009</date>
<booktitle>In Proceedings of ACL-IJCNLP workshop on Linguistic Annotation (LAW‘09),</booktitle>
<pages>121--124</pages>
<contexts>
<context position="27225" citStr="Choi et al., 2009" startWordPosition="4632" endWordPosition="4635">ture, and heuristics for retrieving fine-grained semantic boundaries. By using our approach, we correctly retrieved the semantic boundaries of 98.2% of the Propbank arguments (F1-score of 99.54%). Furthermore, the heuristics can be used to fix some of the inconsistencies in both Treebank and Propbank annotations. Moreover, they suggest ways of reconstructing dependency structure so that it can fit better with semantic roles. Retrieving correct semantic boundaries is important for tasks like machine translation where not only the head words but also all other words matter to complete the task (Choi et al., 2009). In the future, we are going to apply our approach to other corpora and see how well the heuristics work. In addition, we will try to find ways of automatically adding empty categories to dependency structure so we can deal with the full set of Propbank arguments. Acknowledgments Special thanks are due to Professor Joakim Nivre of Uppsala University and Claire Bonial of the University of Colorado at Boulder for very helpful insights. We gratefully acknowledge the support of the National Science Foundation Grants CISECRI-0551615, Towards a Comprehensive Linguistic Annotation and CISE-CRI 07091</context>
</contexts>
<marker>Choi, Palmer, Xue, 2009</marker>
<rawString>Jinho D. Choi, Martha Palmer, and Nianwen Xue. 2009. Using parallel propbanks to enhance wordalignments. In Proceedings of ACL-IJCNLP workshop on Linguistic Annotation (LAW‘09), pages 121–124.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Susanne Ekeklint</author>
<author>Joakim Nivre</author>
</authors>
<title>A dependency-based conversion of propbank.</title>
<date>2007</date>
<booktitle>In Proceedings of NODALIDA workshop on Building Frame Semantics Resources for Scandinavian and Baltic Languages (FRAME’07),</booktitle>
<pages>pages</pages>
<contexts>
<context position="5291" citStr="Ekeklint and Nivre (2007)" startWordPosition="842" endWordPosition="845">not yet been thoroughly addressed. In this paper, we first show how to convert the Penn Treebank style phrase structure to dependency structure. We then describe how to annotate the Propbank arguments, already annotated in the phrase structure, on head words in the dependency structure. Finally, we present heuristics that correctly retrieve semantic boundaries in most cases. For our experiments, we used the entire Penn Treebank (Wall Street Journal). Our experiments show that it is possible to achieve an F1- score of 99.54% for correct representation of the semantic boundaries. 2 Related work Ekeklint and Nivre (2007) tried to retrieve semantic boundaries by adding extra arcs to dependency trees, so the structure is no longer a tree but a graph. They experimented with the same corpus, the Penn Treebank, but used a different dependency conversion tool, Penn2Malt.1 Our work is distinguished from theirs because we keep the tree structure but use heuristics to find the boundaries. Johansson (2008) also tried to find semantic boundaries for evaluation of his semantic role labeling system using dependency structure. He used heuristics that apply to general cases whereas we add more detailed heuristics for specif</context>
</contexts>
<marker>Ekeklint, Nivre, 2007</marker>
<rawString>Susanne Ekeklint and Joakim Nivre. 2007. A dependency-based conversion of propbank. In Proceedings of NODALIDA workshop on Building Frame Semantics Resources for Scandinavian and Baltic Languages (FRAME’07), pages 19–25.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Gildea</author>
</authors>
<title>Dependencies vs. constituents for tree-based alignment.</title>
<date>2004</date>
<booktitle>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’04),</booktitle>
<pages>214--221</pages>
<contexts>
<context position="1479" citStr="Gildea, 2004" startWordPosition="215" endWordPosition="216">heuristics that retrieve correct phrase boundaries for semantic arguments, called semantic boundaries, from dependency trees. By applying heuristics, we achieved an F1-score of 99.54% for correct representation of semantic boundaries. Furthermore, error analysis showed that some of the errors could also be considered correct, depending on the interpretation of the annotation. 1 Introduction Dependency structure has recently gained wide interest because it is simple yet provides useful information for many NLP tasks such as sentiment analysis (Kessler and Nicolov, 2009) or machine translation (Gildea, 2004). Although dependency structure is a kind of syntactic structure, it is quite different from phrase structure: phrase structure gives phrase information by grouping constituents whereas dependency structure gives dependency relations between pairs of words. Many dependency relations (e.g., subject, object) have high correlations with semantic roles (e.g., agent, patient), which makes dependency structure suitable for representing semantic information such as predicate-argument structure. In 2009, the Conference on Computational Natural Language Learning (CoNLL) opened a shared task: the partic</context>
</contexts>
<marker>Gildea, 2004</marker>
<rawString>Daniel Gildea. 2004. Dependencies vs. constituents for tree-based alignment. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’04), pages 214–221.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Jan Hajiˇc</author>
<author>Massimiliano Ciaramita</author>
<author>Richard Johansson</author>
<author>Daisuke Kawahara</author>
<author>Maria Ant`onia Mart´ı</author>
<author>Llu´ıs M`arquez</author>
<author>Adam Meyers</author>
<author>Joakim Nivre</author>
<author>Sebastian Pad´o</author>
<author>Jan ˇStˇep´anek</author>
<author>Pavel Straˇn´ak</author>
<author>Mihai Surdeanu</author>
<author>Nianwen Xue</author>
<author>Yi Zhang</author>
</authors>
<title>The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages.</title>
<date>2009</date>
<booktitle>In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL’09),</booktitle>
<pages>1--18</pages>
<marker>Hajiˇc, Ciaramita, Johansson, Kawahara, Mart´ı, M`arquez, Meyers, Nivre, Pad´o, ˇStˇep´anek, Straˇn´ak, Surdeanu, Xue, Zhang, 2009</marker>
<rawString>Jan Hajiˇc, Massimiliano Ciaramita, Richard Johansson, Daisuke Kawahara, Maria Ant`onia Mart´ı, Llu´ıs M`arquez, Adam Meyers, Joakim Nivre, Sebastian Pad´o, Jan ˇStˇep´anek, Pavel Straˇn´ak, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The conll-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the 13th Conference on Computational Natural Language Learning (CoNLL’09), pages 1–18.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
<author>Pierre Nugues</author>
</authors>
<title>Extended constituent-to-dependency conversion for english.</title>
<date>2007</date>
<booktitle>In Proceedings of the 16th Nordic Conference of Computational Linguistics (NODALIDA’07).</booktitle>
<contexts>
<context position="6147" citStr="Johansson and Nugues, 2007" startWordPosition="982" endWordPosition="985">ion tool, Penn2Malt.1 Our work is distinguished from theirs because we keep the tree structure but use heuristics to find the boundaries. Johansson (2008) also tried to find semantic boundaries for evaluation of his semantic role labeling system using dependency structure. He used heuristics that apply to general cases whereas we add more detailed heuristics for specific cases. 3 Converting phrase structure to dependency structure We used the same tool as the one used for the CoNLL’09 shared task to automatically convert the phrase structure trees in the Penn Treebank to the dependency trees (Johansson and Nugues, 2007). The script gives several options for the conversion; we mostly used the default values except for the following options:2 • splitSlash=false: do not split slashes. This option is taken so the dependency trees preserve the same number of word-tokens as the original phrase structure trees. • noSecEdges=true: ignore secondary edges if present. This option is taken so all siblings of verb predicates in phrase structure become children of the verbs in dependency structure regardless of empty categories. Figure 3 shows the converted dependency tree, which is produced when the secondary edge (*ICH*</context>
</contexts>
<marker>Johansson, Nugues, 2007</marker>
<rawString>Richard Johansson and Pierre Nugues. 2007. Extended constituent-to-dependency conversion for english. In Proceedings of the 16th Nordic Conference of Computational Linguistics (NODALIDA’07).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Richard Johansson</author>
</authors>
<title>Dependency-based Semantic Analysis of Natural-language Text.</title>
<date>2008</date>
<tech>Ph.D. thesis,</tech>
<institution>Lund University.</institution>
<contexts>
<context position="5674" citStr="Johansson (2008)" startWordPosition="909" endWordPosition="910">ments, we used the entire Penn Treebank (Wall Street Journal). Our experiments show that it is possible to achieve an F1- score of 99.54% for correct representation of the semantic boundaries. 2 Related work Ekeklint and Nivre (2007) tried to retrieve semantic boundaries by adding extra arcs to dependency trees, so the structure is no longer a tree but a graph. They experimented with the same corpus, the Penn Treebank, but used a different dependency conversion tool, Penn2Malt.1 Our work is distinguished from theirs because we keep the tree structure but use heuristics to find the boundaries. Johansson (2008) also tried to find semantic boundaries for evaluation of his semantic role labeling system using dependency structure. He used heuristics that apply to general cases whereas we add more detailed heuristics for specific cases. 3 Converting phrase structure to dependency structure We used the same tool as the one used for the CoNLL’09 shared task to automatically convert the phrase structure trees in the Penn Treebank to the dependency trees (Johansson and Nugues, 2007). The script gives several options for the conversion; we mostly used the default values except for the following options:2 • s</context>
</contexts>
<marker>Johansson, 2008</marker>
<rawString>Richard Johansson. 2008. Dependency-based Semantic Analysis of Natural-language Text. Ph.D. thesis, Lund University.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Jason S Kessler</author>
<author>Nicolas Nicolov</author>
</authors>
<title>Targeting sentiment expressions through supervised ranking of linguistic configurations.</title>
<date>2009</date>
<booktitle>In Proceedings of the 3rd International AAAI Conference on Weblogs and Social Media (ICWSM’09).</booktitle>
<contexts>
<context position="1441" citStr="Kessler and Nicolov, 2009" startWordPosition="208" endWordPosition="211">rrect phrase boundaries. In this paper, we present heuristics that retrieve correct phrase boundaries for semantic arguments, called semantic boundaries, from dependency trees. By applying heuristics, we achieved an F1-score of 99.54% for correct representation of semantic boundaries. Furthermore, error analysis showed that some of the errors could also be considered correct, depending on the interpretation of the annotation. 1 Introduction Dependency structure has recently gained wide interest because it is simple yet provides useful information for many NLP tasks such as sentiment analysis (Kessler and Nicolov, 2009) or machine translation (Gildea, 2004). Although dependency structure is a kind of syntactic structure, it is quite different from phrase structure: phrase structure gives phrase information by grouping constituents whereas dependency structure gives dependency relations between pairs of words. Many dependency relations (e.g., subject, object) have high correlations with semantic roles (e.g., agent, patient), which makes dependency structure suitable for representing semantic information such as predicate-argument structure. In 2009, the Conference on Computational Natural Language Learning (C</context>
</contexts>
<marker>Kessler, Nicolov, 2009</marker>
<rawString>Jason S. Kessler and Nicolas Nicolov. 2009. Targeting sentiment expressions through supervised ranking of linguistic configurations. In Proceedings of the 3rd International AAAI Conference on Weblogs and Social Media (ICWSM’09).</rawString>
</citation>
<citation valid="true">
<authors>
<author>Mitchell P Marcus</author>
<author>Mary Ann Marcinkiewicz</author>
<author>Beatrice Santorini</author>
</authors>
<title>Building a large annotated corpus of english: The penn treebank.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="2294" citStr="Marcus et al., 1993" startWordPosition="330" endWordPosition="333">y structure gives dependency relations between pairs of words. Many dependency relations (e.g., subject, object) have high correlations with semantic roles (e.g., agent, patient), which makes dependency structure suitable for representing semantic information such as predicate-argument structure. In 2009, the Conference on Computational Natural Language Learning (CoNLL) opened a shared task: the participants were supposed to take dependency trees as input and produce semantic role labels as output (Hajiˇc et al., 2009). The dependency trees were automatically converted from the Penn Treebank (Marcus et al., 1993), which consists of phrase structure trees, using some heuristics (cf. Section 3). The semantic roles were extracted from the Propbank (Palmer et al., 2005). Since Propbank arguments were originally annotated at the phrase level using the Penn Treebank and the phrase information got lost during the conversion to the dependency trees, arguments are annotated on head words instead of phrases in dependency trees; the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. Figure 1 shows a dependency tree that has been converted from </context>
</contexts>
<marker>Marcus, Marcinkiewicz, Santorini, 1993</marker>
<rawString>Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. 1993. Building a large annotated corpus of english: The penn treebank. Computational Linguistics, 19(2):313–330.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Martha Palmer</author>
<author>Daniel Gildea</author>
<author>Paul Kingsbury</author>
</authors>
<title>The proposition bank: An annotated corpus of semantic roles.</title>
<date>2005</date>
<journal>Computational Linguistics,</journal>
<volume>31</volume>
<issue>1</issue>
<contexts>
<context position="2450" citStr="Palmer et al., 2005" startWordPosition="357" endWordPosition="360">(e.g., agent, patient), which makes dependency structure suitable for representing semantic information such as predicate-argument structure. In 2009, the Conference on Computational Natural Language Learning (CoNLL) opened a shared task: the participants were supposed to take dependency trees as input and produce semantic role labels as output (Hajiˇc et al., 2009). The dependency trees were automatically converted from the Penn Treebank (Marcus et al., 1993), which consists of phrase structure trees, using some heuristics (cf. Section 3). The semantic roles were extracted from the Propbank (Palmer et al., 2005). Since Propbank arguments were originally annotated at the phrase level using the Penn Treebank and the phrase information got lost during the conversion to the dependency trees, arguments are annotated on head words instead of phrases in dependency trees; the subtree of each head word is assumed to include the same set of words as the annotated phrase does in phrase structure. Figure 1 shows a dependency tree that has been converted from the corresponding phrase structure tree. root The results appear in today &apos;s news Figure 1: Phrase vs. dependency structure VP NP1 IN NP S DT NNS The result</context>
</contexts>
<marker>Palmer, Gildea, Kingsbury, 2005</marker>
<rawString>Martha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71–106.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sameer S Pradhan</author>
<author>Wayne Ward</author>
<author>Kadri Hacioglu</author>
<author>James H Martin</author>
<author>Daniel Jurafsky</author>
</authors>
<title>Shallow semantic parsing using support vector machines.</title>
<date>2004</date>
<booktitle>In Proceedings of the Human Language Technology Conference/North American chapter of the Association for Computational Linguistics annual meeting (HLT/NAACL’04).</booktitle>
<contexts>
<context position="21738" citStr="Pradhan et al., 2004" startWordPosition="3704" endWordPosition="3707">roup NMOD NMOD correspondence mailed about incomplete 8300s PMOD NMOD 96 a fellow named John , who stayed for years NMOD P APPO OPRD DEP TMP PMOD NMOD Accuracy = 1 T · Barg c(gold(arg),sys(arg)) Figure 20: Past-participle example 2 needs to be annotated in either Treebank or Propbank; because of that, annotation for punctuation is not entirely consistent, which makes it hard to evaluate. Second, although punctuation gives useful information for obtaining semantic boundaries, it is not crucial for semantic roles. In fact, some of the state-of-art semantic role labeling systems, such as ASSERT (Pradhan et al., 2004), give an option for omitting punctuation from the output. For these reasons, our final model ignores punctuation for semantic boundaries. 6 Evaluations 6.1 Model comparisons The following list describes six models used for the experiments. Model I is the baseline approach that retrieves all words in the subtrees of head words as semantic boundaries. Model II to VI use the heuristics discussed in the previous sections. Each model inherits all the heuristics from the previous model and adds new heuristics; therefore, each model is expected to perform better than the previous model. • I - all wo</context>
</contexts>
<marker>Pradhan, Ward, Hacioglu, Martin, Jurafsky, 2004</marker>
<rawString>Sameer S. Pradhan, Wayne Ward, Kadri Hacioglu, James H. Martin, and Daniel Jurafsky. 2004. Shallow semantic parsing using support vector machines. In Proceedings of the Human Language Technology Conference/North American chapter of the Association for Computational Linguistics annual meeting (HLT/NAACL’04).</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>