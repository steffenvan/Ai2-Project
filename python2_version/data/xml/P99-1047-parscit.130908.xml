<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.003430">
<title confidence="0.999253">
A Decision-Based Approach to Rhetorical Parsing
</title>
<author confidence="0.998726">
Daniel Marcu
</author>
<affiliation confidence="0.903507">
Information Sciences Institute and Department of Computer Science
University of Southern California
4676 Admiralty Way, Suite 1001
</affiliation>
<address confidence="0.741659">
Marina del Rey, CA 90292-6601
</address>
<email confidence="0.999286">
marcu@isi.edu
</email>
<sectionHeader confidence="0.993909" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999526">
We present a shift-reduce rhetorical parsing algo-
rithm that learns to construct rhetorical structures
of texts from a corpus of discourse-parse action se-
quences. The algorithm exploits robust lexical, syn-
tactic, and semantic knowledge sources.
</bodyText>
<sectionHeader confidence="0.999266" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999929388888889">
The application of decision-based learning tech-
niques over rich sets of linguistic features has
improved significantly the coverage and perfor-
mance of syntactic (and to various degrees seman-
tic) parsers (Simmons and Yu, 1992; Magerman,
1995; Hermjakob and Mooney, 1997). In this pa-
per, we apply a similar paradigm to developing a
rhetorical parser that derives the discourse structure
of unrestricted texts.
Crucial to our approach is the reliance on a cor-
pus of 90 texts which were manually annotated with
discourse trees and the adoption of a shift-reduce
parsing model that is well-suited for learning. Both
the corpus and the parsing model are used to gener-
ate learning cases of how texts should be partitioned
into elementary discourse units and how discourse
units and segments should be assembled into dis-
course trees.
</bodyText>
<sectionHeader confidence="0.992412" genericHeader="method">
2 The Corpus
</sectionHeader>
<bodyText confidence="0.99958012244898">
We used a corpus of 90 rhetorical structure trees,
which were built manually using rhetorical rela-
tions that were defined informally in the style of
Mann and Thompson (1988): 30 trees were built
for short personal news stories from the MUC7 co-
reference corpus (Hirschman and Chinchor, 1997);
30 trees for scientific texts from the Brown corpus;
and 30 trees for editorials from the Wall Street Jour-
nal (WSJ). The average number of words for each
text was 405 in the MUC corpus, 2029 in the Brown
corpus, and 878 in the WSJ corpus. Each MUC text
was tagged by three annotators; each Brown and
WSJ text was tagged by two annotators.
The rhetorical structure assigned to each text is a
(possibly non-binary) tree whose leaves correspond
to elementary discourse units (edu)s, and whose in-
ternal nodes correspond to contiguous text spans.
Each internal node is characterized by a rhetori-
cal relation, such as ELABORATION and CONTRAST.
Each relation holds between two non-overlapping
text spans called NUCLEUS and SATELLITE. (There
are a few exceptions to this rule: some relations,
such as SEQUENCE and CONTRAST, are multinu-
clear.) The distinction between nuclei and satellites
comes from the empirical observation that the nu-
cleus expresses what is more essential to the writer&apos;s
purpose than the satellite. Each node in the tree is
also characterized by a promotion set that denotes
the units that are important in the corresponding
subtree. The promotion sets of leaf nodes are the
leaves themselves. The promotion sets of internal
nodes are given by the union of the promotion sets
of the immediate nuclei nodes.
Edus are defined functionally as clauses or
clause-like units that are unequivocally the NU-
CLEUS or SATELLITE of a rhetorical relation that
holds between two adjacent spans of text. For ex-
ample, &amp;quot;because of the low atmospheric pressure&amp;quot;
in text (1) is not a fully fleshed clause. However,
since it is the SATELLITE of an EXPLANATION rela-
tion, we treat it as elementary.
[Only the midday sun at tropical latitudes is warm (1)
enough] [to thaw ice on occasion,] [but any liquid wa-
ter formed in this way would evaporate almost instantly]
[because of the low atmospheric pressure.]
Some edus may contain parenthetical units, i.e.,
embedded units whose deletion does not affect the
understanding of the edu to which they belong. For
example, the unit shown in italics in (2) is paren-
</bodyText>
<page confidence="0.987115">
365
</page>
<bodyText confidence="0.999621117647059">
thetic. input list contains the edts that correspond to units
This book, which I have received from John, is the best (2) whose numbers are higher than or equal to 19.
book that I have read in a while. ... [Close parallels between tests and practice tests (3)
The annotation process was carried out using a are common,12] [some educators and researchers
rhetorical tagging tool. The process consisted in as- say.13] [Test-preparation booklets, software and work-
signing edu and parenthetical unit boundaries, in as- sheets are a booming publishing subindustry.&amp;quot; ] [But
sembling edus and spans into discourse trees, and in some practice products are so similar to the tests them-
labeling the relations between edus and spans with selves that critics say they represent a form of school-
rhetorical relation names from a taxonomy of 71 re- sponsored cheating.15 ]
lations. No explicit distinction was made between [Ulf I took these preparation booklets into my
intentional, informational, and textual relations. In ciassroom,16] [I&apos;d have a hard time justifying to my stu-
addition, we also marked two constituency relations dents and parents that it wasn&apos;t cheating,&amp;quot;17 ] [says John
that were ubiquitous in our corpora and that often Kaminsky,18] [a Traverse City, Mich., teacher who has
subsumed complex rhetorical constituents. These studied test coaching.19 ] ...
relations were ATTRIBUTION, which was used to la- At step i the parser decides to perform a SHIFT op-
bel the relation between a reporting and a reported eration. As a result, the edt corresponding to unit
clause, and APPOSITION. Marcu et al. (1999) discuss 19 becomes the top of the stack. At step i + 1, the
in detail the annotation tool and protocol and assess parser performs a REDUCE-APPOSITION-NS opera-
the inter-judge agreement and the reliability of the tion, that combines edts 18 and 19 into a discourse
annotation. tree whose nucleus is unit 18 and whose satellite
3 The parsing model is unit 19. The rhetorical relation that holds be-
We model the discourse parsing process as a se- tween units 18 and 19 is APPOSITION. At step i+2,
quence of shift-reduce operations. As front-end, the the trees that span over units [16,17] and [18,19]
parser uses a discourse segmenter, i.e., an algorithm are combined into a larger tree, using a REDUCE-
that partitions the input text into edus. The dis- ATTRIBUTION-NS operation. As a result, the status
course segmenter, which is also decision-based, is of the tree [16,17] becomes NUCLEUS and the status
presented and evaluated in section 4. of the tree [18,19] becomes SATELLITE. The rhetor-
The input to the parser is an empty stack and an ical relation between the two trees is ATTRIBUTION.
input list that contains a sequence of elementary dis- At step i + 3, the trees at the top of the stack are
course trees, edts, one edt for each edu produced by combined using a REDUCE-ELABORATION-NS oper-
the discourse segmenter. The status and rhetorical ation. The effect of the operation is shown at the
relation associated with each edt is UNDEFINED, and bottom of figure 1.
the promotion set is given by the corresponding edu. In order to enable a shift-reduce discourse parser
At each step, the parser applies a SHIFT or a REDUCE derive any discourse tree, it is sufficient to imple-
operation. Shift operations transfer the first edt of ment one SHIFT operation and six types of REDUCE
the input list to the top of the stack. Reduce opera- operations, whose operational semantics is shown
tions pop the two discourse trees located on the top in figure 2. For each possible pair of nuclearity
of the stack; combine them into a new tree updating assignments NUCLEUS-SATELLITE (NS), SATELLITE-
the statuses, rhetorical relation names, and promo- NUCLEUS (SN), and NUCLEUS-NUCLEUS (NN) there
tion sets associated with the trees involved in the are two possible ways to attach the tree located at
operation; and push the new tree on the top of the position top in the stack to the tree located at po-
stack. sition top — 1. If one wants to create a binary tree
Assume, for example, that the discourse seg- whose immediate children are the trees at top and
menter partitions a text given as input as shown top — 1, an operation of type REDUCE-NS, REDUCE-
in (3). (Only the edus numbered from 12 to 19 are SN, or REDUCE-NN needs to be employed. If one
shown.) Figure 1 shows the actions taken by a shift- wants to attach the tree at top as an extra-child
reduce discourse parser starting with step i. At step of the tree at top — 1, thus creating or modifying
i, the stack contains 4 partial discourse trees, which a non-binary tree, an operation of type REDUCE-
span units 11,11], [12,15], [16,17], and [18], and the BELOW-NS, REDUCE-BELOW-SN, or REDUCE-BELOW-
366 NN needs to be employed. Figure 2 illustrates how
the statuses and promotion sets associated with the
</bodyText>
<figure confidence="0.953020270588235">
Top 4111.1 and Ltd ef danenta0
&amp;seam. tenet
Undefised Undefined Undefined Undefined Undefined
Unfit:sof Undefined Undefined Undefned Undefined
5 (17 (1.) (19)
Nucleus Sdellite Naclan
Span Condition SP. SUITT OPERATION
(14) 1171
Nwlm
List
((4)
Undefined Undefined Undefined Undefined Undefined
Undefined Undefined Undefined Undefined Undefised
1 (110 (IS) 1201
REDUCE -APPORTION-NS OtEllATION
Nneleus Satellite Nucleus
Span Ceeditioe Spa
1151 (171
Undefined Undefined Undefsed
Undefined Undefined Usdefini,jd Undeford
Undefined 1201
NOCIMIS Satellite Nucleus Nucleus Satellite
Condition Spa. Spot Mende= REDUCE.AITRIBUTION-NS 111/01AT1014
(15) (16) 1171 11.1 101
Undefined Undefined Undefined
Undefined Undefined Undefined
15 071 (20)
Nucleus Necked Satellite
Sp.. Spa Attribution INDUCE-121ABORA11ON•NS OPERATION
1(5) 101
Sadist Naked Naas. Satellite
Condition Span Spa Apposition
(16) (17) HO 091
Undefined Undefined
Undefined Undefiled
1(5) (20)
lens Satellite
SP. Bamako
15 1(7)
?laded Nucleus Satellite
Spas Ann
(151 1(.)
5111CIlib Nucleus Neclan Satellite
Condition Span 4411 Aspen..
1161 (17) (11) 1(9)
Satellite
Condom
112341
/43
Undefined
Undefined
(4 /
Narks,
U.
1121
Step I•2
Si--
Coes.
112.14)
Srellite
Contrast
(1234)
Step 1.4
SI.,&apos;
Nucleus
Spa
112)
Satellite
Attrifinien
(I7)
Mirka
Sem
1121
Satellite
Attribution
113/
Satellite
Conn.
112,141
N-
spa
1121
544(44
Attribution
(0)
</figure>
<figureCaption confidence="0.999996">
Figure 1: Example of a sequence of shift-reduce operations that concern the discourse parsing of text (3).
</figureCaption>
<bodyText confidence="0.995142714285714">
trees involved in the reduce operations are affected
in each case.
Since the labeled data that we relied upon
was sparse, we grouped the relations that shared
some rhetorical meaning into clusters of rhetor-
ical similarity. For example, the cluster named
CONTRAST contained the contrast-like rhetorical
relations of ANTITHESIS, CONTRAST, and CON-
CESSION. The cluster named EVALUATION-
INTERPRETATION contained the rhetorical relations
of EVALUATION and INTERPRETATION. And the
cluster named OTHER contained rhetorical rela-
tions such as QUESTION-ANSWER, PROPORTION, RE-
STATEMENT, and COMPARISON, which were used
</bodyText>
<page confidence="0.993605">
367
</page>
<table confidence="0.997385657142857">
Relation-NS Undeftned \
Undefined
Undefined Undefuted
Undefined Undefuted
PI P2
Relation-Below-NS
Nucleus Satellite
Span Relation
PI
Undefined Undefined Undefined
Undefined Undefined Undefined
PI P2
Satellite
Relation
P2
Undefined
Undefined
Undefined Undefined Relation-SN P2
Undefined Uodefuted
PI P2
Relatlow-NN
Undefined
Undefined
P1
U P2
Undefined Undefined
Undefined Undefined
P2
Relation-BelowNN
Nucleus Nude,.
Relation Relation
PI e2
Undefined Undefined Undefined
Undefined Undefined Undefined
PI P2 PI U P2
</table>
<figureCaption confidence="0.989327">
Figure 2: The reduce operations supported by our
parsing model.
</figureCaption>
<bodyText confidence="0.8851775">
very seldom in the corpus. The grouping pro-
cess yielded 17 clusters, each characterized by
a generalized rhetorical relation name. These
names were: APPOSITION-PARENTHETICAL, ATTRI-
</bodyText>
<construct confidence="0.7221052">
BUTION, CONTRAST, BACKGROUND-CIRCUMSTANCE,
CAUSE-REASON-EXPLANATION, CONDITION, ELABO-
RATION, EVALUATION-INTERPRETATION, EVIDENCE,
EXAMPLE, MANNER-MEANS, ALTERNATIVE, PUR-
POSE, TEMPORAL, LIST, TEXTUAL, and OTHER.
</construct>
<bodyText confidence="0.999927272727273">
In the work described in this paper, we attempted
to automatically derive rhetorical structures trees
that were labeled with relations names that corre-
sponded to the 17 clusters of rhetorical similarity.
Since there are 6 types of reduce operations and
since each discourse tree in our study uses relation
names that correspond to the 17 clusters of rhetor-
ical similarity, it follows that our discourse parser
needs to learn what operation to choose from a set
of 6 x 17 + 1 = 103 operations (the 1 corresponds
to the SHIFT operation).
</bodyText>
<sectionHeader confidence="0.985431" genericHeader="method">
4 The discourse segmenter
</sectionHeader>
<subsectionHeader confidence="0.999914">
4.1 Generation of learning examples
</subsectionHeader>
<bodyText confidence="0.999743363636364">
The discourse segmenter we implemented processes
an input text one lexeme (word or punctuation
mark) at a time and recognizes sentence and edu
boundaries and beginnings and ends of parentheti-
cal units. We used the leaves of the discourse trees
that were built manually in order to derive the learn-
ing cases. To each lexeme in a text, we associated
one learning case, using the features described in
section 4.2. The classes to be learned, which are as-
sociated with each lexeme, are sentence-break, edu-
break, start-paren, end-paren, and none.
</bodyText>
<subsectionHeader confidence="0.977628">
4.2 Features used for learning
</subsectionHeader>
<bodyText confidence="0.998477333333334">
To partition a text into edus and to detect parentheti-
cal unit boundaries, we relied on features that model
both the local and global contexts.
The local context consists of a window of size
5 that enumerates the Part-Of-Speech (POS) tags
of the lexeme under scrutiny and the two lexemes
found immediately before and after it. The POS
tags are determined automatically, using the Brill
tagger (1995). Since discourse markers, such as
because and and, have been shown to play a ma-
jor role in rhetorical parsing (Marcu, 1997), we also
consider a list of features that specify whether a lex-
eme found within the local contextual window is a
potential discourse marker. The local context also
contains features that estimate whether the lexemes
within the window are potential abbreviations.
The global context reflects features that pertain to
the boundary identification process. These features
specify whether a discourse marker that introduces
expectations (Cristea and Webber, 1997) (such as
although) was used in the sentence under consider-
ation, whether there are any commas or dashes be-
fore the estimated end of the sentence, and whether
there are any verbs in the unit under consideration.
A binary representation of the features that char-
acterize both the local and global contexts yields
learning examples with 2417 features/example.
</bodyText>
<subsectionHeader confidence="0.977893">
4.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.9995615">
We used the C4.5 program (Quinlan, 1993) in order
to learn decision trees and rules that classify lex-
</bodyText>
<figure confidence="0.938833">
Undefined
Undefined
P2
Undefined
Undefined
P1
Satellite Nucleus
Relation Span
PI
Relation-Belowt5N Undefined
Undefined
P2
Satellite
Relation
P1
</figure>
<page confidence="0.891308">
368
</page>
<table confidence="0.9993655">
Corpus # cases B1(%) B2(%) Acc(%)
MUC 14362 91.28 93.1 96.24±0.06
WSJ 31309 92.39 94.6 97.14±0.10
Brown 72092 93.84 96.8 97.87±0.04
</table>
<tableCaption confidence="0.837426">
Table 1: Performance of a discourse segmenter that
uses a decision-tree, non-binary classifier.
</tableCaption>
<table confidence="0.9999575">
Action (a) (b) (c) (d) (e)
sentence-break (a) 272 4
edu-break (b) 133 3 84
start-paren (c) 4 26
end-paren (d) 20 6
none (e) 2 38 1 4 7555
</table>
<tableCaption confidence="0.9335535">
Table 2: Confusion matrix for the decision-tree,
non-binary classifier (the Brown corpus).
</tableCaption>
<figure confidence="0.994732307692308">
Acc
96.20
96.00
95.80
95.60
95.40
95.20
95.00
94.80
94.60
94.40
cases x 103
2.00 4.00 6.00 8.00 1000 12 00
</figure>
<figureCaption confidence="0.994084333333333">
edu boundaries. The performance is high with re-
spect to recognizing sentence boundaries and ends
of parenthetical units. The performance with re-
spect to identifying sentence boundaries appears
to be close to that of systems aimed at identify-
ing only sentence boundaries (Palmer and Hearst,
1997), whose accuracy is in the range of 99%.
Figure 3: Learning curve for discourse segmenter
(the MUC corpus).
</figureCaption>
<bodyText confidence="0.998696161290323">
emes as boundaries of sentences, edus, or parenthet-
ical units, or as non-boundaries. We learned both
from binary (when we could) and non-binary repre-
sentations of the cases.1 In general the binary rep-
resentations yielded slightly better results than the
non-binary representations and the tree classifiers
were slightly better than the rule-based ones. Due
to space constraints, we show here (in table 1) only
accuracy results that concern non-binary, decision-
tree classifiers. The accuracy figures were com-
puted using a ten-fold cross-validation procedure.
In table 1, B1 corresponds to a majority-based base-
line classifier that assigns none to all lexemes, and
B2 to a baseline classifier that assigns a sentence
boundary to every DOT lexeme and a non-boundary
to all other lexemes.
Figure 3 shows the learning curve that corre-
sponds to the MUC corpus. It suggests that more
data can increase the accuracy of the classifier.
The confusion matrix shown in table 2 corre-
sponds to a non-binary-based tree classifier that
was trained on cases derived from 27 Brown texts
and that was tested on cases derived from 3 dif-
ferent Brown texts, which were selected randomly.
The matrix shows that the segmenter has problems
mostly with identifying the beginning of parentheti-
cal units and the intra-sentential edu boundaries; for
example, it correctly identifies only 133 of the 220
&apos;Learning from binary representations of features in the
Brown corpus was too computationally expensive to terminate
— the Brown data file had about 0.5GBytes.
</bodyText>
<sectionHeader confidence="0.971954" genericHeader="method">
5 The shift-reduce action identifier
</sectionHeader>
<subsectionHeader confidence="0.997051">
5.1 Generation of learning examples
</subsectionHeader>
<bodyText confidence="0.989484789473684">
The learning cases were generated automatically,
in the style of Magerman (1995), by traversing in-
order the final rhetorical structures built by anno-
tators and by generating a sequence of discourse
parse actions that used only SHIFT and REDUCE op-
erations of the kinds discussed in section 3. When
a derived sequence is applied as described in the
parsing model, it produces a rhetorical tree that is
a one-to-one copy of the original tree that was used
to generate the sequence. For example, the tree at
the bottom of figure 1 — the tree found at the top
of the stack at step i + 4 — can be built if the fol-
lowing sequence of operations is performed: {SHIFT
12; SHIFT 13; REDUCE-ATTRIBUTION-NS; SHIFT 14;
REDUCE-JOINT-NN; SHIFT 15; REDUCE-CONTRAST-
SN; SHIFT 16; SHIFT 17; REDUCE-CONDITION-
SN; SHIFT 18; SHIFT 19; REDUCE-APPOSITION-NS;
REDUCE-ATTRIBUTION-NS; REDUCE-ELABORATION-
NS.}
</bodyText>
<subsectionHeader confidence="0.997467">
5.2 Features used for learning
</subsectionHeader>
<bodyText confidence="0.920325285714286">
To make decisions with respect to parsing actions,
the shift-reduce action identifier focuses on the three
top most trees in the stack and the first edt in the in-
put list. We refer to these trees as the trees in focus.
The identifier relies on the following classes of fea-
tures.
Structural features.
</bodyText>
<listItem confidence="0.9968012">
• Features that reflect the number of trees in the
stack and the number of edts in the input list.
• Features that describe the structure of the trees in
focus in terms of the type of textual units that they
subsume (sentences, paragraphs, titles); the number
</listItem>
<page confidence="0.998594">
369
</page>
<bodyText confidence="0.89898825">
of immediate children of the root nodes; the rhetor-
ical relations that link the immediate children of the
root nodes, etc.2
Lexical (cue-phrase-like) and syntactic features.
</bodyText>
<listItem confidence="0.986000166666667">
• Features that denote the actual words and POS
tags of the first and last two lexemes of the text
spans subsumed by the trees in focus.
• Features that denote whether the first and last
units of the trees in focus contain potential discourse
markers and the position of these markers in the
corresponding textual units (beginning, middle, or
end).
Operational features.
• Features that specify what the last five parsing op-
erations performed by the parser were.3
Semantic-similarity-based features.
• Features that denote the semantic similarity be-
tween the textual segments subsumed by the trees
in focus. This similarity is computed by applying in
the style of Hearst (1997) a cosine-based metric on
the morphed segments.
• Features that denote Wordnet-based measures of
</listItem>
<bodyText confidence="0.985834409090909">
similarity between the bags of words in the promo-
tion sets of the trees in focus. We use 14 Wordnet-
based measures of similarity, one for each Word-
net relation (Fellbaum, 1998). Each of these sim-
ilarities is computed using a metric similar to the
cosine-based metric. Wordnet-based similarities re-
flect the degree of synonymy, antonymy, meronymy,
hyponymy, etc. between the textual segments sub-
sumed by the trees in focus. We also use 14 x 13/2
relative Wordnet-based measures of similarity, one
for each possible pair of Wordnet-based relations.
For each pair of Wordnet-based measures of simi-
larity wr2 and w„ , each relative measure (feature)
takes the value &lt; , =, or &gt;, depending on whether
the Wordnet-based similarity w„ between the bags
of words in the promotion sets of the trees in focus is
lower, equal, or higher that the Wordnet-based sim-
ilarity wr2 between the same bags of words. For ex-
ample, if both the synonymy- and meronymy-based
measures of similarity are 0, the relative similarity
between the synonymy and meronymy of the trees
in focus will have the value =.
</bodyText>
<footnote confidence="0.99591675">
2The identifier assumes that each sentence break that ends
in a period and is followed by two &apos; \ n&apos; characters, for example,
is a paragraph break; and that a sentence break that does not end
in a punctuation mark and is followed by two &apos; \ n&apos; characters is
a title.
3We could generate these features because, for learning, we
used sequences of shift-reduce operations and not discourse
trees.
</footnote>
<table confidence="0.994911">
Corpus # cases B3(%) B4(%) Acc(%)
MUC 1996 50.75 26.9 61.12±1.61
WSJ 4360 50.34 27.3 61.65±0.41
Brown 8242 50.18 28.1 61.81±0.48
</table>
<tableCaption confidence="0.9850615">
Table 3: Performance of the tree-based, shift-reduce
action classifiers.
</tableCaption>
<figure confidence="0.996188">
Acc
60.00
58.00
56.00
54.00
52.00
50.00
48.00
46.00
S cases 103
0.50 1.00 1.50
</figure>
<figureCaption confidence="0.9916405">
Figure 4: Learning curve for the shift-reduce action
identifier (the MUC corpus).
</figureCaption>
<bodyText confidence="0.9418715">
A binary representation of these features yields
learning examples with 2789 features/example.
</bodyText>
<subsectionHeader confidence="0.992572">
5.3 Evaluation
</subsectionHeader>
<bodyText confidence="0.999953434782609">
The shift-reduce action identifier uses the C4.5 pro-
gram in order to learn decision trees and rules that
specify how discourse segments should be assem-
bled into trees. In general, the tree-based classifiers
performed slightly better than the rule-based classi-
fiers. Due to space constraints, we present here only
performance results that concern the tree classifiers.
Table 3 displays the accuracy of the shift-reduce ac-
tion identifiers, determined for each of the three cor-
pora by means of a ten-fold cross-validation proce-
dure. In table 3, the B3 column gives the accuracy
of a majority-based classifier, which chooses action
SHIFT in all cases. Since choosing only the action
SHIFT never produces a discourse tree, in column
B4, we present the accuracy of a baseline classifier
that chooses shift-reduce operations randomly, with
probabilities that reflect the probability distribution
of the operations in each corpus.
Figure 4 shows the learning curve that corre-
sponds to the MUC corpus. As in the case of the
discourse segmenter, this learning curve also sug-
gests that more data can increase the accuracy of
the shift-reduce action identifier.
</bodyText>
<page confidence="0.494324">
6 Evaluation of the rhetorical parser
</page>
<bodyText confidence="0.9807455">
Obviously, by applying the two classifiers sequen-
tially, one can derive the rhetorical structure of any
</bodyText>
<page confidence="0.991191">
370
</page>
<table confidence="0.999053235294118">
Corpus Seg- Train- Elementary units Hierarchical spans Span nuclearity Rhetorical relations
ment- ing
er corpus
Judges Parser Judges Parser Judges Parser Judges Parser
R P R PR PR P R P R P R PR P
MUC DT MUC 88.0 88.0 37.1 100.0 84.4 84.4 38.2 61.0 79.1 83.5 25.5 51.5 78.6 78.6 14.9 28.7
DT All 75.4 96.9 70.9 72.8 58.3 68.9 38.4 45.3
M MUC 100.0 100.0 87.5 82.3 68.8 78.2 72.4 62.8
M All 100.0 100.0 84.8 73.5 71.0 69.3 66.5 53.9
WSJ DT WSJ 85.1 86.8 18.1 95.8 79.9 80.1 34.0 65.8 67.6 77.1 21.6 54.0 73.1 73.3 13.0 34.3
DT All 25.1 79.6 40.1 66.3 30.3 58.5 17.3 36.0
M WSJ 100.0 100.0 83.4 84.2 63.7 79.9 56.3 57.9
M All 100.0 100.0 83.0 85.0 69.0 82.4 59.8 63.2
Brown DT Brown 89.5 88.5 60.5 79.4 80.6 79.5 57.3 63.3 67.6 75.8 44.6 57.3 69.7 68.3 26.7 35.3
DT All 44.2 80.3 44.7 59.1 33.2 51.8 15.7 25.7
M Brown 100.0 100.0 81.1 73.4 60.1 67.0 59.5 45.5
M All 100.0 100.0 80.8 77.5 60.0 72.0 51.8 44.7
</table>
<tableCaption confidence="0.9694495">
Table 4: Performance of the rhetorical parser: labeled (R)ecall and (P)recision. The segmenter is either
Decision-Tree-Based (DT) or Manual (M).
</tableCaption>
<bodyText confidence="0.999977391304348">
text. Unfortunately, the performance results pre-
sented in sections 4 and 5 only suggest how well
the discourse segmenter and the shift-reduce action
identifier perform with respect to individual cases.
They say nothing about the performance of a rhetor-
ical parser that relies on these classifiers.
In order to evaluate the rhetorical parser as a
whole, we partitioned randomly each corpus into
two sets of texts: 27 texts were used for training and
the last 3 texts were used for testing. The evalua-
tion employs labeled recall and precision measures,
which are extensively used to study the performance
of syntactic parsers. Labeled recall reflects the num-
ber of correctly labeled constituents identified by
the rhetorical parser with respect to the number of
labeled constituents in the corresponding manually
built tree. Labeled precision reflects the number
of correctly labeled constituents identified by the
rhetorical parser with respect to the total number of
labeled constituents identified by the parser.
We computed labeled recall and precision figures
with respect to the ability of our discourse parser
to identify elementary units, hierarchical text spans,
text span nuclei and satellites, and rhetorical rela-
tions. Table 4 displays results obtained using seg-
menters and shift-reduce action identifiers that were
trained either on 27 texts from each corpus and
tested on 3 unseen texts from the same corpus; or
that were trained on 27 x 3 texts from all corpora
and tested on 3 unseen texts from each corpus. The
training and test texts were chosen randomly. Ta-
ble 4 also displays results obtained using a man-
ual discourse segmenter, which identified correctly
all edus. Since all texts in our corpora were man-
ually annotated by multiple judges, we could also
compute an upper-bound of the performance of the
rhetorical parser by calculating for each text in the
test corpus and each judge the average labeled recall
and precision figures with respect to the discourse
trees built by the other judges. Table 4 displays
these upper-bound figures as well.
The results in table 4 primarily show that errors in
the discourse segmentation stage affect significantly
the quality of the trees our parser builds. When
a segmenter is trained only on 27 texts (especially
for the MUC and WSJ corpora, which have shorter
texts than the Brown corpus), it has very low per-
formance. Many of the intra-sentential edu bound-
aries are not identified, and as a consequence, the
overall performance of the parser is low. When
the segmenter is trained on 27 x 3 texts, its perfor-
mance increases significantly with respect to the
MUC and WSJ corpora, but decreases with respect
to the Brown corpus. This can be explained by the
significant differences in style and discourse marker
usage between the three corpora. When a perfect
segmenter is used, the rhetorical parser determines
hierarchical constituents and assigns them a nude-
arity status at levels of performance that are not far
from those of humans. However, the rhetorical la-
beling of discourse spans is even in this case about
15-20% below human performance.
These results suggest that the features that we use
are sufficient for determining the hierarchical struc-
ture of texts and the nuclearity statuses of discourse
segments. However, they are insufficient for deter-
mining correctly the elementary units of discourse
and the rhetorical relations that hold between dis-
course segments.
</bodyText>
<page confidence="0.997949">
371
</page>
<sectionHeader confidence="0.999829" genericHeader="method">
7 Related work
</sectionHeader>
<bodyText confidence="0.99998">
The rhetorical parser presented here is the first that
employs learning methods and a thorough evalua-
tion methodology. All previous parsers aimed at
determining the rhetorical structure of unrestricted
texts (Sumita et at., 1992; Kurohashi and Nagao,
1994; Marcu, 1997; Corston-Oliver, 1998) em-
ployed manually written rules. Because of the lack
of discourse corpora, these parsers did not evaluate
the correctness of the discourse trees they built per
se, but rather their adequacy for specific purposes:
experiments carried out by Miike et al. (1994) and
Marcu (1999) showed only that the discourse struc-
tures built by rhetorical parsers (Sumita et al., 1992;
Marcu, 1997) can be used successfully in order to
improve retrieval performance and summarize text.
</bodyText>
<sectionHeader confidence="0.998643" genericHeader="conclusions">
8 Conclusion
</sectionHeader>
<bodyText confidence="0.999891586206896">
In this paper, we presented a shift-reduce rhetori-
cal parsing algorithm that learns to construct rhetor-
ical structures of texts from tagged data. The parser
has two components: a discourse segmenter, which
identifies the elementary discourse units in a text;
and a shift-reduce action identifier, which deter-
mines how these units should be assembled into
rhetorical structure trees.
Our results suggest that a high-performance dis-
course segmenter would need to rely on more train-
ing data and more elaborate features than the ones
described in this paper - the learning curves did
not converge to performance limits. If one&apos;s goal is,
however, to construct discourse trees whose leaves
are sentences (or units that can be identified at
high levels of performance), then the segmenter de-
scribed here appears to be adequate. Our results
also suggest that the rich set of features that consti-
tute the foundation of the action identifier are suffi-
cient for constructing discourse hierarchies and for
assigning to discourse segments a rhetorical status
of nucleus or satellite at levels of performance that
are close to those of humans. However, more re-
search is needed in order to approach human perfor-
mance in the task of assigning to segments correct
rhetorical relation labels.
Acknowledgements. I am grateful to Ulf Herm-
jalcob, Kevin Knight, and Eric Breck for comments
on previous drafts of this paper.
</bodyText>
<sectionHeader confidence="0.999081" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.9972128">
Eric Brill. 1995. Transformation-based error-driven
learning and natural language processing: A case
study in part-of-speech tagging. Computational Lin-
guistics, 21(4):543-565.
Simon H. Corston-Oliver. 1998. Beyond string match-
ing and cue phrases: Improving efficiency and cover-
age in discourse analysis. The AAAI Spring Sympo-
sium on Intelligent Text Summarization, pages 9-15.
Dan Cristea and Bonnie L. Webber. 1997. Expectations
in incremental discourse processing. In Proceedings
of ACL/EACE97, pages 88-95.
Christiane Fellbaum, editor. 1998. Wordnet: An Elec-
tronic Lexical Database. The MIT Press.
Marti A. Hearst. 1997. TextTiling: Segmenting text
into multi-paragraph subtopic passages. Computa-
tional Linguistics, 23(1):33-64.
Ulf Hermjakob and Raymond J. Mooney. 1997. Learn-
ing parse and translation decisions from examples
with rich context. In Proceedings of ACLEACL&apos;97,
pages 482-489.
Lynette Hirschman and Nancy Chinchor, 1997. MUC-7
Coreference Task Definition.
Sadao Kurohashi and Makoto Nagao. 1994. Automatic
detection of discourse structure by checking surface
information in sentences. In Proceedings of COL-
ING&apos;94, volume 2, pages 1123-1127.
David M. Magerman. 1995. Statistical decision-tree
models for parsing. In Proceedings of ACL&apos;95, pages
276-283.
William C. Mann and Sandra A. Thompson. 1988.
Rhetorical structure theory: Toward a functional the-
ory of text organization. Text, 8(3):243-281.
Daniel Marcu. 1997. The rhetorical parsing of natu-
ral language texts. In Proceedings of ACLEACL&apos;97,
pages 96-103.
Daniel Marcu. 1999. Discourse trees are good indica-
tors of importance in text. In Inderjeet Mani and Mark
Maybury, editors, Advances in Automatic Text Sum-
marization. The MIT Press. To appear.
Daniel Marcu, Estibaliz Amorrortu, and Magdalena
Romera. 1999. Experiments in constructing a corpus
of discourse trees. The ACL&apos;99 Workshop on Stan-
dards and Tools for Discourse Tagging.
Seiji Miike, Etsuo Itoh, Kenji Ono, and Kazuo Sumita.
1994. A full-text retrieval system with a dynamic
abstract generation function. In Proceedings of SI-
GIR&apos;94, pages 152-161.
David D. Palmer and Marti A. Hearst. 1997. Adap-
tive multilingual sentence boundary disambiguation.
Computational Linguistics, 23(2):241-269.
J. Ross Quinlan. 1993. C4.5: Programs for Machine
Learning. Morgan Kaufmann Publishers.
R.F. Simmons and Yeong-Ho Yu. 1992. The acquisition
and use of context-dependent grammars for English.
Computational Linguistics, 18(4):391-418.
K. Sumita, K. Ono, T. Chino, T. Ukita, and S. Amano.
1992. A discourse structure analyzer for Japanese
text. In Proceedings of the International Conference
on Fifth Generation Computer Systems, volume 2,
pages 1133-1140.
</reference>
<page confidence="0.998357">
372
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.926132">
<title confidence="0.999971">A Decision-Based Approach to Rhetorical Parsing</title>
<author confidence="0.999696">Daniel Marcu</author>
<affiliation confidence="0.9986155">Information Sciences Institute and Department of Computer Science University of Southern California</affiliation>
<address confidence="0.9896595">4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292-6601</address>
<email confidence="0.999344">marcu@isi.edu</email>
<abstract confidence="0.991396333333333">present shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences. The algorithm exploits robust lexical, syntactic, and semantic knowledge sources.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Eric Brill</author>
</authors>
<title>Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging.</title>
<date>1995</date>
<journal>Computational Linguistics,</journal>
<pages>21--4</pages>
<marker>Brill, 1995</marker>
<rawString>Eric Brill. 1995. Transformation-based error-driven learning and natural language processing: A case study in part-of-speech tagging. Computational Linguistics, 21(4):543-565.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Simon H Corston-Oliver</author>
</authors>
<title>Beyond string matching and cue phrases: Improving efficiency and coverage in discourse analysis. The AAAI Spring Symposium on Intelligent Text Summarization,</title>
<date>1998</date>
<pages>9--15</pages>
<contexts>
<context position="27477" citStr="Corston-Oliver, 1998" startWordPosition="4443" endWordPosition="4444">gest that the features that we use are sufficient for determining the hierarchical structure of texts and the nuclearity statuses of discourse segments. However, they are insufficient for determining correctly the elementary units of discourse and the rhetorical relations that hold between discourse segments. 371 7 Related work The rhetorical parser presented here is the first that employs learning methods and a thorough evaluation methodology. All previous parsers aimed at determining the rhetorical structure of unrestricted texts (Sumita et at., 1992; Kurohashi and Nagao, 1994; Marcu, 1997; Corston-Oliver, 1998) employed manually written rules. Because of the lack of discourse corpora, these parsers did not evaluate the correctness of the discourse trees they built per se, but rather their adequacy for specific purposes: experiments carried out by Miike et al. (1994) and Marcu (1999) showed only that the discourse structures built by rhetorical parsers (Sumita et al., 1992; Marcu, 1997) can be used successfully in order to improve retrieval performance and summarize text. 8 Conclusion In this paper, we presented a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structure</context>
</contexts>
<marker>Corston-Oliver, 1998</marker>
<rawString>Simon H. Corston-Oliver. 1998. Beyond string matching and cue phrases: Improving efficiency and coverage in discourse analysis. The AAAI Spring Symposium on Intelligent Text Summarization, pages 9-15.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dan Cristea</author>
<author>Bonnie L Webber</author>
</authors>
<title>Expectations in incremental discourse processing.</title>
<date>1997</date>
<booktitle>In Proceedings of ACL/EACE97,</booktitle>
<pages>88--95</pages>
<contexts>
<context position="13816" citStr="Cristea and Webber, 1997" startWordPosition="2193" endWordPosition="2196">tomatically, using the Brill tagger (1995). Since discourse markers, such as because and and, have been shown to play a major role in rhetorical parsing (Marcu, 1997), we also consider a list of features that specify whether a lexeme found within the local contextual window is a potential discourse marker. The local context also contains features that estimate whether the lexemes within the window are potential abbreviations. The global context reflects features that pertain to the boundary identification process. These features specify whether a discourse marker that introduces expectations (Cristea and Webber, 1997) (such as although) was used in the sentence under consideration, whether there are any commas or dashes before the estimated end of the sentence, and whether there are any verbs in the unit under consideration. A binary representation of the features that characterize both the local and global contexts yields learning examples with 2417 features/example. 4.3 Evaluation We used the C4.5 program (Quinlan, 1993) in order to learn decision trees and rules that classify lexUndefined Undefined P2 Undefined Undefined P1 Satellite Nucleus Relation Span PI Relation-Belowt5N Undefined Undefined P2 Sate</context>
</contexts>
<marker>Cristea, Webber, 1997</marker>
<rawString>Dan Cristea and Bonnie L. Webber. 1997. Expectations in incremental discourse processing. In Proceedings of ACL/EACE97, pages 88-95.</rawString>
</citation>
<citation valid="true">
<title>Wordnet: An Electronic Lexical Database.</title>
<date>1998</date>
<editor>Christiane Fellbaum, editor.</editor>
<publisher>The MIT Press.</publisher>
<marker>1998</marker>
<rawString>Christiane Fellbaum, editor. 1998. Wordnet: An Electronic Lexical Database. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marti A Hearst</author>
</authors>
<title>TextTiling: Segmenting text into multi-paragraph subtopic passages.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--1</pages>
<contexts>
<context position="15297" citStr="Hearst, 1997" startWordPosition="2435" endWordPosition="2436"> (d) (e) sentence-break (a) 272 4 edu-break (b) 133 3 84 start-paren (c) 4 26 end-paren (d) 20 6 none (e) 2 38 1 4 7555 Table 2: Confusion matrix for the decision-tree, non-binary classifier (the Brown corpus). Acc 96.20 96.00 95.80 95.60 95.40 95.20 95.00 94.80 94.60 94.40 cases x 103 2.00 4.00 6.00 8.00 1000 12 00 edu boundaries. The performance is high with respect to recognizing sentence boundaries and ends of parenthetical units. The performance with respect to identifying sentence boundaries appears to be close to that of systems aimed at identifying only sentence boundaries (Palmer and Hearst, 1997), whose accuracy is in the range of 99%. Figure 3: Learning curve for discourse segmenter (the MUC corpus). emes as boundaries of sentences, edus, or parenthetical units, or as non-boundaries. We learned both from binary (when we could) and non-binary representations of the cases.1 In general the binary representations yielded slightly better results than the non-binary representations and the tree classifiers were slightly better than the rule-based ones. Due to space constraints, we show here (in table 1) only accuracy results that concern non-binary, decisiontree classifiers. The accuracy f</context>
<context position="19337" citStr="Hearst (1997)" startWordPosition="3095" endWordPosition="3096">t and last two lexemes of the text spans subsumed by the trees in focus. • Features that denote whether the first and last units of the trees in focus contain potential discourse markers and the position of these markers in the corresponding textual units (beginning, middle, or end). Operational features. • Features that specify what the last five parsing operations performed by the parser were.3 Semantic-similarity-based features. • Features that denote the semantic similarity between the textual segments subsumed by the trees in focus. This similarity is computed by applying in the style of Hearst (1997) a cosine-based metric on the morphed segments. • Features that denote Wordnet-based measures of similarity between the bags of words in the promotion sets of the trees in focus. We use 14 Wordnetbased measures of similarity, one for each Wordnet relation (Fellbaum, 1998). Each of these similarities is computed using a metric similar to the cosine-based metric. Wordnet-based similarities reflect the degree of synonymy, antonymy, meronymy, hyponymy, etc. between the textual segments subsumed by the trees in focus. We also use 14 x 13/2 relative Wordnet-based measures of similarity, one for each</context>
</contexts>
<marker>Hearst, 1997</marker>
<rawString>Marti A. Hearst. 1997. TextTiling: Segmenting text into multi-paragraph subtopic passages. Computational Linguistics, 23(1):33-64.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Ulf Hermjakob</author>
<author>Raymond J Mooney</author>
</authors>
<title>Learning parse and translation decisions from examples with rich context.</title>
<date>1997</date>
<booktitle>In Proceedings of ACLEACL&apos;97,</booktitle>
<pages>482--489</pages>
<contexts>
<context position="773" citStr="Hermjakob and Mooney, 1997" startWordPosition="105" endWordPosition="108">ern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292-6601 marcu@isi.edu Abstract We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences. The algorithm exploits robust lexical, syntactic, and semantic knowledge sources. 1 Introduction The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic (and to various degrees semantic) parsers (Simmons and Yu, 1992; Magerman, 1995; Hermjakob and Mooney, 1997). In this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts. Crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning. Both the corpus and the parsing model are used to generate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments should be assembled into discourse trees. 2 The Corpus We used a corpus of 90 rhetori</context>
</contexts>
<marker>Hermjakob, Mooney, 1997</marker>
<rawString>Ulf Hermjakob and Raymond J. Mooney. 1997. Learning parse and translation decisions from examples with rich context. In Proceedings of ACLEACL&apos;97, pages 482-489.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Lynette Hirschman</author>
<author>Nancy Chinchor</author>
</authors>
<date>1997</date>
<booktitle>MUC-7 Coreference Task Definition.</booktitle>
<contexts>
<context position="1633" citStr="Hirschman and Chinchor, 1997" startWordPosition="249" endWordPosition="252">d with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning. Both the corpus and the parsing model are used to generate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments should be assembled into discourse trees. 2 The Corpus We used a corpus of 90 rhetorical structure trees, which were built manually using rhetorical relations that were defined informally in the style of Mann and Thompson (1988): 30 trees were built for short personal news stories from the MUC7 coreference corpus (Hirschman and Chinchor, 1997); 30 trees for scientific texts from the Brown corpus; and 30 trees for editorials from the Wall Street Journal (WSJ). The average number of words for each text was 405 in the MUC corpus, 2029 in the Brown corpus, and 878 in the WSJ corpus. Each MUC text was tagged by three annotators; each Brown and WSJ text was tagged by two annotators. The rhetorical structure assigned to each text is a (possibly non-binary) tree whose leaves correspond to elementary discourse units (edu)s, and whose internal nodes correspond to contiguous text spans. Each internal node is characterized by a rhetorical rela</context>
</contexts>
<marker>Hirschman, Chinchor, 1997</marker>
<rawString>Lynette Hirschman and Nancy Chinchor, 1997. MUC-7 Coreference Task Definition.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Sadao Kurohashi</author>
<author>Makoto Nagao</author>
</authors>
<title>Automatic detection of discourse structure by checking surface information in sentences.</title>
<date>1994</date>
<booktitle>In Proceedings of COLING&apos;94,</booktitle>
<volume>2</volume>
<pages>1123--1127</pages>
<contexts>
<context position="27441" citStr="Kurohashi and Nagao, 1994" startWordPosition="4437" endWordPosition="4440">low human performance. These results suggest that the features that we use are sufficient for determining the hierarchical structure of texts and the nuclearity statuses of discourse segments. However, they are insufficient for determining correctly the elementary units of discourse and the rhetorical relations that hold between discourse segments. 371 7 Related work The rhetorical parser presented here is the first that employs learning methods and a thorough evaluation methodology. All previous parsers aimed at determining the rhetorical structure of unrestricted texts (Sumita et at., 1992; Kurohashi and Nagao, 1994; Marcu, 1997; Corston-Oliver, 1998) employed manually written rules. Because of the lack of discourse corpora, these parsers did not evaluate the correctness of the discourse trees they built per se, but rather their adequacy for specific purposes: experiments carried out by Miike et al. (1994) and Marcu (1999) showed only that the discourse structures built by rhetorical parsers (Sumita et al., 1992; Marcu, 1997) can be used successfully in order to improve retrieval performance and summarize text. 8 Conclusion In this paper, we presented a shift-reduce rhetorical parsing algorithm that lear</context>
</contexts>
<marker>Kurohashi, Nagao, 1994</marker>
<rawString>Sadao Kurohashi and Makoto Nagao. 1994. Automatic detection of discourse structure by checking surface information in sentences. In Proceedings of COLING&apos;94, volume 2, pages 1123-1127.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David M Magerman</author>
</authors>
<title>Statistical decision-tree models for parsing.</title>
<date>1995</date>
<booktitle>In Proceedings of ACL&apos;95,</booktitle>
<pages>276--283</pages>
<contexts>
<context position="744" citStr="Magerman, 1995" startWordPosition="103" endWordPosition="104">versity of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292-6601 marcu@isi.edu Abstract We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences. The algorithm exploits robust lexical, syntactic, and semantic knowledge sources. 1 Introduction The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic (and to various degrees semantic) parsers (Simmons and Yu, 1992; Magerman, 1995; Hermjakob and Mooney, 1997). In this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts. Crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning. Both the corpus and the parsing model are used to generate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments should be assembled into discourse trees. 2 The Corpus W</context>
<context position="17092" citStr="Magerman (1995)" startWordPosition="2719" endWordPosition="2720"> that was tested on cases derived from 3 different Brown texts, which were selected randomly. The matrix shows that the segmenter has problems mostly with identifying the beginning of parenthetical units and the intra-sentential edu boundaries; for example, it correctly identifies only 133 of the 220 &apos;Learning from binary representations of features in the Brown corpus was too computationally expensive to terminate — the Brown data file had about 0.5GBytes. 5 The shift-reduce action identifier 5.1 Generation of learning examples The learning cases were generated automatically, in the style of Magerman (1995), by traversing inorder the final rhetorical structures built by annotators and by generating a sequence of discourse parse actions that used only SHIFT and REDUCE operations of the kinds discussed in section 3. When a derived sequence is applied as described in the parsing model, it produces a rhetorical tree that is a one-to-one copy of the original tree that was used to generate the sequence. For example, the tree at the bottom of figure 1 — the tree found at the top of the stack at step i + 4 — can be built if the following sequence of operations is performed: {SHIFT 12; SHIFT 13; REDUCE-A</context>
</contexts>
<marker>Magerman, 1995</marker>
<rawString>David M. Magerman. 1995. Statistical decision-tree models for parsing. In Proceedings of ACL&apos;95, pages 276-283.</rawString>
</citation>
<citation valid="true">
<authors>
<author>William C Mann</author>
<author>Sandra A Thompson</author>
</authors>
<title>Rhetorical structure theory: Toward a functional theory of text organization.</title>
<date>1988</date>
<tech>Text,</tech>
<pages>8--3</pages>
<contexts>
<context position="1516" citStr="Mann and Thompson (1988)" startWordPosition="230" endWordPosition="233">unrestricted texts. Crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning. Both the corpus and the parsing model are used to generate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments should be assembled into discourse trees. 2 The Corpus We used a corpus of 90 rhetorical structure trees, which were built manually using rhetorical relations that were defined informally in the style of Mann and Thompson (1988): 30 trees were built for short personal news stories from the MUC7 coreference corpus (Hirschman and Chinchor, 1997); 30 trees for scientific texts from the Brown corpus; and 30 trees for editorials from the Wall Street Journal (WSJ). The average number of words for each text was 405 in the MUC corpus, 2029 in the Brown corpus, and 878 in the WSJ corpus. Each MUC text was tagged by three annotators; each Brown and WSJ text was tagged by two annotators. The rhetorical structure assigned to each text is a (possibly non-binary) tree whose leaves correspond to elementary discourse units (edu)s, a</context>
</contexts>
<marker>Mann, Thompson, 1988</marker>
<rawString>William C. Mann and Sandra A. Thompson. 1988. Rhetorical structure theory: Toward a functional theory of text organization. Text, 8(3):243-281.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>The rhetorical parsing of natural language texts.</title>
<date>1997</date>
<booktitle>In Proceedings of ACLEACL&apos;97,</booktitle>
<pages>96--103</pages>
<contexts>
<context position="13357" citStr="Marcu, 1997" startWordPosition="2128" endWordPosition="2129">ntence-break, edubreak, start-paren, end-paren, and none. 4.2 Features used for learning To partition a text into edus and to detect parenthetical unit boundaries, we relied on features that model both the local and global contexts. The local context consists of a window of size 5 that enumerates the Part-Of-Speech (POS) tags of the lexeme under scrutiny and the two lexemes found immediately before and after it. The POS tags are determined automatically, using the Brill tagger (1995). Since discourse markers, such as because and and, have been shown to play a major role in rhetorical parsing (Marcu, 1997), we also consider a list of features that specify whether a lexeme found within the local contextual window is a potential discourse marker. The local context also contains features that estimate whether the lexemes within the window are potential abbreviations. The global context reflects features that pertain to the boundary identification process. These features specify whether a discourse marker that introduces expectations (Cristea and Webber, 1997) (such as although) was used in the sentence under consideration, whether there are any commas or dashes before the estimated end of the sent</context>
<context position="27454" citStr="Marcu, 1997" startWordPosition="4441" endWordPosition="4442">e results suggest that the features that we use are sufficient for determining the hierarchical structure of texts and the nuclearity statuses of discourse segments. However, they are insufficient for determining correctly the elementary units of discourse and the rhetorical relations that hold between discourse segments. 371 7 Related work The rhetorical parser presented here is the first that employs learning methods and a thorough evaluation methodology. All previous parsers aimed at determining the rhetorical structure of unrestricted texts (Sumita et at., 1992; Kurohashi and Nagao, 1994; Marcu, 1997; Corston-Oliver, 1998) employed manually written rules. Because of the lack of discourse corpora, these parsers did not evaluate the correctness of the discourse trees they built per se, but rather their adequacy for specific purposes: experiments carried out by Miike et al. (1994) and Marcu (1999) showed only that the discourse structures built by rhetorical parsers (Sumita et al., 1992; Marcu, 1997) can be used successfully in order to improve retrieval performance and summarize text. 8 Conclusion In this paper, we presented a shift-reduce rhetorical parsing algorithm that learns to constru</context>
</contexts>
<marker>Marcu, 1997</marker>
<rawString>Daniel Marcu. 1997. The rhetorical parsing of natural language texts. In Proceedings of ACLEACL&apos;97, pages 96-103.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
</authors>
<title>Discourse trees are good indicators of importance in text.</title>
<date>1999</date>
<booktitle>In Inderjeet Mani and Mark Maybury, editors, Advances in Automatic Text Summarization. The</booktitle>
<publisher>MIT Press.</publisher>
<note>To appear.</note>
<contexts>
<context position="27754" citStr="Marcu (1999)" startWordPosition="4488" endWordPosition="4489">ween discourse segments. 371 7 Related work The rhetorical parser presented here is the first that employs learning methods and a thorough evaluation methodology. All previous parsers aimed at determining the rhetorical structure of unrestricted texts (Sumita et at., 1992; Kurohashi and Nagao, 1994; Marcu, 1997; Corston-Oliver, 1998) employed manually written rules. Because of the lack of discourse corpora, these parsers did not evaluate the correctness of the discourse trees they built per se, but rather their adequacy for specific purposes: experiments carried out by Miike et al. (1994) and Marcu (1999) showed only that the discourse structures built by rhetorical parsers (Sumita et al., 1992; Marcu, 1997) can be used successfully in order to improve retrieval performance and summarize text. 8 Conclusion In this paper, we presented a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from tagged data. The parser has two components: a discourse segmenter, which identifies the elementary discourse units in a text; and a shift-reduce action identifier, which determines how these units should be assembled into rhetorical structure trees. Our results</context>
</contexts>
<marker>Marcu, 1999</marker>
<rawString>Daniel Marcu. 1999. Discourse trees are good indicators of importance in text. In Inderjeet Mani and Mark Maybury, editors, Advances in Automatic Text Summarization. The MIT Press. To appear.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Daniel Marcu</author>
<author>Estibaliz Amorrortu</author>
<author>Magdalena Romera</author>
</authors>
<title>Experiments in constructing a corpus of discourse trees.</title>
<date>1999</date>
<booktitle>The ACL&apos;99 Workshop on Standards and Tools for Discourse Tagging.</booktitle>
<contexts>
<context position="5344" citStr="Marcu et al. (1999)" startWordPosition="861" endWordPosition="864">extual relations. In ciassroom,16] [I&apos;d have a hard time justifying to my stuaddition, we also marked two constituency relations dents and parents that it wasn&apos;t cheating,&amp;quot;17 ] [says John that were ubiquitous in our corpora and that often Kaminsky,18] [a Traverse City, Mich., teacher who has subsumed complex rhetorical constituents. These studied test coaching.19 ] ... relations were ATTRIBUTION, which was used to la- At step i the parser decides to perform a SHIFT opbel the relation between a reporting and a reported eration. As a result, the edt corresponding to unit clause, and APPOSITION. Marcu et al. (1999) discuss 19 becomes the top of the stack. At step i + 1, the in detail the annotation tool and protocol and assess parser performs a REDUCE-APPOSITION-NS operathe inter-judge agreement and the reliability of the tion, that combines edts 18 and 19 into a discourse annotation. tree whose nucleus is unit 18 and whose satellite 3 The parsing model is unit 19. The rhetorical relation that holds beWe model the discourse parsing process as a se- tween units 18 and 19 is APPOSITION. At step i+2, quence of shift-reduce operations. As front-end, the the trees that span over units [16,17] and [18,19] par</context>
</contexts>
<marker>Marcu, Amorrortu, Romera, 1999</marker>
<rawString>Daniel Marcu, Estibaliz Amorrortu, and Magdalena Romera. 1999. Experiments in constructing a corpus of discourse trees. The ACL&apos;99 Workshop on Standards and Tools for Discourse Tagging.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Seiji Miike</author>
<author>Etsuo Itoh</author>
<author>Kenji Ono</author>
<author>Kazuo Sumita</author>
</authors>
<title>A full-text retrieval system with a dynamic abstract generation function.</title>
<date>1994</date>
<booktitle>In Proceedings of SIGIR&apos;94,</booktitle>
<pages>152--161</pages>
<contexts>
<context position="27737" citStr="Miike et al. (1994)" startWordPosition="4483" endWordPosition="4486"> relations that hold between discourse segments. 371 7 Related work The rhetorical parser presented here is the first that employs learning methods and a thorough evaluation methodology. All previous parsers aimed at determining the rhetorical structure of unrestricted texts (Sumita et at., 1992; Kurohashi and Nagao, 1994; Marcu, 1997; Corston-Oliver, 1998) employed manually written rules. Because of the lack of discourse corpora, these parsers did not evaluate the correctness of the discourse trees they built per se, but rather their adequacy for specific purposes: experiments carried out by Miike et al. (1994) and Marcu (1999) showed only that the discourse structures built by rhetorical parsers (Sumita et al., 1992; Marcu, 1997) can be used successfully in order to improve retrieval performance and summarize text. 8 Conclusion In this paper, we presented a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from tagged data. The parser has two components: a discourse segmenter, which identifies the elementary discourse units in a text; and a shift-reduce action identifier, which determines how these units should be assembled into rhetorical structure t</context>
</contexts>
<marker>Miike, Itoh, Ono, Sumita, 1994</marker>
<rawString>Seiji Miike, Etsuo Itoh, Kenji Ono, and Kazuo Sumita. 1994. A full-text retrieval system with a dynamic abstract generation function. In Proceedings of SIGIR&apos;94, pages 152-161.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David D Palmer</author>
<author>Marti A Hearst</author>
</authors>
<title>Adaptive multilingual sentence boundary disambiguation.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<pages>23--2</pages>
<contexts>
<context position="15297" citStr="Palmer and Hearst, 1997" startWordPosition="2433" endWordPosition="2436">(a) (b) (c) (d) (e) sentence-break (a) 272 4 edu-break (b) 133 3 84 start-paren (c) 4 26 end-paren (d) 20 6 none (e) 2 38 1 4 7555 Table 2: Confusion matrix for the decision-tree, non-binary classifier (the Brown corpus). Acc 96.20 96.00 95.80 95.60 95.40 95.20 95.00 94.80 94.60 94.40 cases x 103 2.00 4.00 6.00 8.00 1000 12 00 edu boundaries. The performance is high with respect to recognizing sentence boundaries and ends of parenthetical units. The performance with respect to identifying sentence boundaries appears to be close to that of systems aimed at identifying only sentence boundaries (Palmer and Hearst, 1997), whose accuracy is in the range of 99%. Figure 3: Learning curve for discourse segmenter (the MUC corpus). emes as boundaries of sentences, edus, or parenthetical units, or as non-boundaries. We learned both from binary (when we could) and non-binary representations of the cases.1 In general the binary representations yielded slightly better results than the non-binary representations and the tree classifiers were slightly better than the rule-based ones. Due to space constraints, we show here (in table 1) only accuracy results that concern non-binary, decisiontree classifiers. The accuracy f</context>
</contexts>
<marker>Palmer, Hearst, 1997</marker>
<rawString>David D. Palmer and Marti A. Hearst. 1997. Adaptive multilingual sentence boundary disambiguation. Computational Linguistics, 23(2):241-269.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Ross Quinlan</author>
</authors>
<title>C4.5: Programs for Machine Learning.</title>
<date>1993</date>
<publisher>Morgan Kaufmann Publishers.</publisher>
<contexts>
<context position="14229" citStr="Quinlan, 1993" startWordPosition="2262" endWordPosition="2263">ions. The global context reflects features that pertain to the boundary identification process. These features specify whether a discourse marker that introduces expectations (Cristea and Webber, 1997) (such as although) was used in the sentence under consideration, whether there are any commas or dashes before the estimated end of the sentence, and whether there are any verbs in the unit under consideration. A binary representation of the features that characterize both the local and global contexts yields learning examples with 2417 features/example. 4.3 Evaluation We used the C4.5 program (Quinlan, 1993) in order to learn decision trees and rules that classify lexUndefined Undefined P2 Undefined Undefined P1 Satellite Nucleus Relation Span PI Relation-Belowt5N Undefined Undefined P2 Satellite Relation P1 368 Corpus # cases B1(%) B2(%) Acc(%) MUC 14362 91.28 93.1 96.24±0.06 WSJ 31309 92.39 94.6 97.14±0.10 Brown 72092 93.84 96.8 97.87±0.04 Table 1: Performance of a discourse segmenter that uses a decision-tree, non-binary classifier. Action (a) (b) (c) (d) (e) sentence-break (a) 272 4 edu-break (b) 133 3 84 start-paren (c) 4 26 end-paren (d) 20 6 none (e) 2 38 1 4 7555 Table 2: Confusion matrix</context>
</contexts>
<marker>Quinlan, 1993</marker>
<rawString>J. Ross Quinlan. 1993. C4.5: Programs for Machine Learning. Morgan Kaufmann Publishers.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R F Simmons</author>
<author>Yeong-Ho Yu</author>
</authors>
<title>The acquisition and use of context-dependent grammars for English. Computational Linguistics,</title>
<date>1992</date>
<pages>18--4</pages>
<contexts>
<context position="728" citStr="Simmons and Yu, 1992" startWordPosition="99" endWordPosition="102">f Computer Science University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292-6601 marcu@isi.edu Abstract We present a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from a corpus of discourse-parse action sequences. The algorithm exploits robust lexical, syntactic, and semantic knowledge sources. 1 Introduction The application of decision-based learning techniques over rich sets of linguistic features has improved significantly the coverage and performance of syntactic (and to various degrees semantic) parsers (Simmons and Yu, 1992; Magerman, 1995; Hermjakob and Mooney, 1997). In this paper, we apply a similar paradigm to developing a rhetorical parser that derives the discourse structure of unrestricted texts. Crucial to our approach is the reliance on a corpus of 90 texts which were manually annotated with discourse trees and the adoption of a shift-reduce parsing model that is well-suited for learning. Both the corpus and the parsing model are used to generate learning cases of how texts should be partitioned into elementary discourse units and how discourse units and segments should be assembled into discourse trees</context>
</contexts>
<marker>Simmons, Yu, 1992</marker>
<rawString>R.F. Simmons and Yeong-Ho Yu. 1992. The acquisition and use of context-dependent grammars for English. Computational Linguistics, 18(4):391-418.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Sumita</author>
<author>K Ono</author>
<author>T Chino</author>
<author>T Ukita</author>
<author>S Amano</author>
</authors>
<title>A discourse structure analyzer for Japanese text.</title>
<date>1992</date>
<booktitle>In Proceedings of the International Conference on Fifth Generation Computer Systems,</booktitle>
<volume>2</volume>
<pages>1133--1140</pages>
<contexts>
<context position="27845" citStr="Sumita et al., 1992" startWordPosition="4501" endWordPosition="4504">the first that employs learning methods and a thorough evaluation methodology. All previous parsers aimed at determining the rhetorical structure of unrestricted texts (Sumita et at., 1992; Kurohashi and Nagao, 1994; Marcu, 1997; Corston-Oliver, 1998) employed manually written rules. Because of the lack of discourse corpora, these parsers did not evaluate the correctness of the discourse trees they built per se, but rather their adequacy for specific purposes: experiments carried out by Miike et al. (1994) and Marcu (1999) showed only that the discourse structures built by rhetorical parsers (Sumita et al., 1992; Marcu, 1997) can be used successfully in order to improve retrieval performance and summarize text. 8 Conclusion In this paper, we presented a shift-reduce rhetorical parsing algorithm that learns to construct rhetorical structures of texts from tagged data. The parser has two components: a discourse segmenter, which identifies the elementary discourse units in a text; and a shift-reduce action identifier, which determines how these units should be assembled into rhetorical structure trees. Our results suggest that a high-performance discourse segmenter would need to rely on more training da</context>
</contexts>
<marker>Sumita, Ono, Chino, Ukita, Amano, 1992</marker>
<rawString>K. Sumita, K. Ono, T. Chino, T. Ukita, and S. Amano. 1992. A discourse structure analyzer for Japanese text. In Proceedings of the International Conference on Fifth Generation Computer Systems, volume 2, pages 1133-1140.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>