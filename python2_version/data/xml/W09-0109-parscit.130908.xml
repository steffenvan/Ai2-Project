<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.531289">
<title confidence="0.963012">
What Do Computational Linguists Need to Know about Linguistics?
</title>
<author confidence="0.989223">
Robert C. Moore
</author>
<affiliation confidence="0.958284">
Microsoft Research
</affiliation>
<address confidence="0.909605">
Redmond, Washington, USA
</address>
<email confidence="0.996198">
bobmoore@microsoft.com
</email>
<sectionHeader confidence="0.993831" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999753875">
In this position paper, we argue that al-
though the data-driven, empirical para-
digm for computational linguistics seems
to be the best way forward at the moment,
a thorough grounding in descriptive lin-
guistics is still needed to do competent
work in the field. Examples are given of
how knowledge of linguistic phenomena
leads to understanding the limitations of
particular statistical models and to better
feature selection for such models.
Over the last twenty years, the field of com-
putational linguistics has undergone a dramatic
shift in focus from hand encoding linguistic facts
in computer-oriented formalisms to applying sta-
tistical analysis and machine learning techniques
to large linguistic corpora. Speaking as someone
who has worked with both approaches, I believe
that this change has been largely for the good, but
I do not intend to argue that point here. Instead, I
wish to consider what computational linguists (if it
is still appropriate to call them that) need to know
about linguistics, in order to work most produc-
tively within the current data-driven paradigm.
My view is that, while computational linguists
may not need to know the details of particular lin-
guistic theories (e.g., minimalism, LFG, HPSG),
they do need to have an extensive understanding of
the phenomena of language at a descriptive level.
I can think of at least two somewhat distinct ap-
plications of this sort of knowledge in empirical
computational linguistics.
One application is to understand the structural
limitations of particular types of statistical models.
For example, a descriptive generalization about
language is that coordinated structures tend to be
interpreted in such a way as to maximize structural
parallelism. Thus, in the phrase “young men and
women”, “young” would normally be interpreted
as applying to both “men” and “women”, but in
the phrase “young men and intelligent women”,
“young” would normally be interpreted as apply-
ing only to “men”. Although both interpreta-
tions are structurally possible for both phrases, the
preferred interpretations are the ones that maxi-
mize structural parallelism. This is a phenom-
enon that is not describable in a general way in
a simple statistical model in the form of a proba-
bilistic context-free grammar (PCFG). We could
enumerate many specific cases by making fine-
grained distinctions in the nonterminals of the
grammar, but the tendency to favor parallel coordi-
nated structures in general would not be expressed.
This is not necessarily fatal to successful engi-
neering applications of PCFGs, but a competent
computational linguist should understand what the
limitations of the formalism are.
Let me give another example from the notori-
ously empirical field of statistical machine transla-
tion (SMT). At least some linguistic structure has
been creeping back into SMT recently in the form
of hierarchical translation models, many of which
can be viewed as instances of synchronous proba-
bilistic (or more generally, weighted) context-free
grammars (SPCFGs). This approach seems quite
promising, but since it is based on a bilingual ver-
sion of PCFGs, not only does it share the limi-
tations of monolingual PCFGs alluded to above,
but it also has additional structural limitations in
the kind of generalizations over types of bilingual
mappings it can model.
My favorite example of such a limitation is
the translation of constituent (i.e., “WH”) ques-
tions between languages that move questioned
constituents to the front of the question (“WH-
movement”) and those that leave the questioned
constituents in situ. English is an example of the
former type of language, and Chinese (so I am
told) is an example of the latter. If we wanted to
make a model of question translation from Chi-
</bodyText>
<note confidence="0.687403">
Proceedings of the EACL 2009 Workshop on the Interaction between Linguistics and Computational Linguistics, pages 41–42,
Athens, Greece, 30 March, 2009. c�2009 Association for Computational Linguistics
</note>
<page confidence="0.999499">
41
</page>
<bodyText confidence="0.999972296296297">
nese to English, we would like it to represent in
a unitary (or at least finitary) way the generaliza-
tion, “Translate the questioned constituent from
Chinese to English and move it to the front of the
English sentence being constructed.” This gener-
alization cannot be expressed in an SPCFG, be-
cause this type of model allows reordering to take
place only among siblings of the same parent in
the constituent structure. Fronting a questioned
constituent, however, typically requires moving
an embedded constituent up several levels in the
constituent structure. While we can express spe-
cific instances of this type of movement using an
SPCFG by flattening the intervening structure, we
cannot hope to capture the generalization in full
because WH-movement in English is famously
unbounded, as in “What translation formalism did
Moore claim to show that WH-movement could
not be modeled in?”
In addition to providing a basis for understand-
ing the limitations of what phenomena various
statistical models can capture, a good knowledge
of descriptive linguistics is also very useful as
a source of features in statistical models. A
good example of this comes from acoustic mod-
eling in speech recognition. Acoustic models in
speech recognition are typically composed of se-
quences of “phone” models, where a phone cor-
responds approximately to the linguistic unit of
a phoneme. For good recognition performance,
however, phone models need to be contextualized
according to the other phones around them. Com-
monly, “triphone” models are used, in which a
separate model is used for each combination of
the phone preceding and following the phone be-
ing modeled. This can require over 100000 dis-
tinct models, depending on how many triphones
are possible in a given language, which creates
a sparse data problem for statistical estimation,
since many of the possible combinations are only
rarely observed.
One response to this sparse data problem is to
cluster the states of the triphone models to reduce
the number of separate models that need to be es-
timated, and an effective way to do this is to use
decision trees. Using a decision tree clustering
procedure, the set of all possible triphones is re-
cursively split on relevant features of the triphone.
At each decision point, the feature chosen for split-
ting is the one that produces the greatest improve-
ment in the resulting model. But what features
should be used in such a decision tree? I once
heard a leading speech recognition engineer say
that he chose his feature set by including all the
features he could find in the linguistic phonetics
literature. Given that feature set, the decision tree
learning procedure decided which ones to actually
use, and in what order.
The examples presented above illustrate some
of the kinds of linguistic knowledge that a compe-
tent computational linguist needs to know in order
perform research at the highest level. I am con-
cerned that many of the students currently grad-
uating in the field do not seem to have received
sufficient exposure to the structure of language at
this level of detail. For instance, a few years ago
I pointed out the problem of modeling question
translation between Chinese and English to one
of the brightest young researchers working with
SPCFGs, and the problem had never occurred to
him, even though he was a fluent speaker of both
languages. I am sure this would be one of the
first things that would occur to anyone brought
up on the debates of the 1980s about the limi-
tations of context-free grammar, upon first expo-
sure to the SPCFG formalism. So, although I am
a firm believer that the data-driven empirical ap-
proach computational linguistics will remain the
most fruitful research paradigm for the foresee-
able future, I also think that researchers need a
firm grounding in descriptive linguistics.
</bodyText>
<page confidence="0.998867">
42
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.621523">
<title confidence="0.998913">What Do Computational Linguists Need to Know about Linguistics?</title>
<author confidence="0.999209">C Robert</author>
<affiliation confidence="0.941728">Microsoft</affiliation>
<address confidence="0.96039">Redmond, Washington,</address>
<email confidence="0.99995">bobmoore@microsoft.com</email>
<abstract confidence="0.996595357575758">In this position paper, we argue that although the data-driven, empirical paradigm for computational linguistics seems to be the best way forward at the moment, a thorough grounding in descriptive linguistics is still needed to do competent work in the field. Examples are given of how knowledge of linguistic phenomena leads to understanding the limitations of particular statistical models and to better feature selection for such models. Over the last twenty years, the field of computational linguistics has undergone a dramatic shift in focus from hand encoding linguistic facts in computer-oriented formalisms to applying statistical analysis and machine learning techniques to large linguistic corpora. Speaking as someone who has worked with both approaches, I believe that this change has been largely for the good, but I do not intend to argue that point here. Instead, I wish to consider what computational linguists (if it is still appropriate to call them that) need to know about linguistics, in order to work most productively within the current data-driven paradigm. My view is that, while computational linguists may not need to know the details of particular linguistic theories (e.g., minimalism, LFG, HPSG), they do need to have an extensive understanding of the phenomena of language at a descriptive level. I can think of at least two somewhat distinct applications of this sort of knowledge in empirical computational linguistics. One application is to understand the structural limitations of particular types of statistical models. For example, a descriptive generalization about language is that coordinated structures tend to be interpreted in such a way as to maximize structural parallelism. Thus, in the phrase “young men and women”, “young” would normally be interpreted as applying to both “men” and “women”, but in the phrase “young men and intelligent women”, “young” would normally be interpreted as applying only to “men”. Although both interpretations are structurally possible for both phrases, the preferred interpretations are the ones that maximize structural parallelism. This is a phenomenon that is not describable in a general way in a simple statistical model in the form of a probabilistic context-free grammar (PCFG). We could enumerate many specific cases by making finegrained distinctions in the nonterminals of the grammar, but the tendency to favor parallel coordinated structures in general would not be expressed. This is not necessarily fatal to successful engineering applications of PCFGs, but a competent computational linguist should understand what the limitations of the formalism are. Let me give another example from the notoriously empirical field of statistical machine translation (SMT). At least some linguistic structure has been creeping back into SMT recently in the form of hierarchical translation models, many of which can be viewed as instances of synchronous probabilistic (or more generally, weighted) context-free grammars (SPCFGs). This approach seems quite promising, but since it is based on a bilingual version of PCFGs, not only does it share the limitations of monolingual PCFGs alluded to above, but it also has additional structural limitations in the kind of generalizations over types of bilingual mappings it can model. My favorite example of such a limitation is the translation of constituent (i.e., “WH”) questions between languages that move questioned constituents to the front of the question (“WHmovement”) and those that leave the questioned English is an example of the former type of language, and Chinese (so I am told) is an example of the latter. If we wanted to a model of question translation from Chiof the EACL 2009 Workshop on the Interaction between Linguistics and Computational pages 41–42, Greece, 30 March, 2009. Association for Computational Linguistics 41 nese to English, we would like it to represent in a unitary (or at least finitary) way the generalization, “Translate the questioned constituent from Chinese to English and move it to the front of the English sentence being constructed.” This generalization cannot be expressed in an SPCFG, because this type of model allows reordering to take place only among siblings of the same parent in the constituent structure. Fronting a questioned constituent, however, typically requires moving an embedded constituent up several levels in the constituent structure. While we can express specific instances of this type of movement using an SPCFG by flattening the intervening structure, we cannot hope to capture the generalization in full because WH-movement in English is famously unbounded, as in “What translation formalism did Moore claim to show that WH-movement could not be modeled in?” In addition to providing a basis for understanding the limitations of what phenomena various statistical models can capture, a good knowledge of descriptive linguistics is also very useful as a source of features in statistical models. A good example of this comes from acoustic modeling in speech recognition. Acoustic models in speech recognition are typically composed of sequences of “phone” models, where a phone corresponds approximately to the linguistic unit of a phoneme. For good recognition performance, however, phone models need to be contextualized according to the other phones around them. Commonly, “triphone” models are used, in which a separate model is used for each combination of the phone preceding and following the phone being modeled. This can require over 100000 distinct models, depending on how many triphones are possible in a given language, which creates a sparse data problem for statistical estimation, since many of the possible combinations are only rarely observed. One response to this sparse data problem is to cluster the states of the triphone models to reduce the number of separate models that need to be estimated, and an effective way to do this is to use decision trees. Using a decision tree clustering procedure, the set of all possible triphones is recursively split on relevant features of the triphone. At each decision point, the feature chosen for splitting is the one that produces the greatest improvement in the resulting model. But what features should be used in such a decision tree? I once heard a leading speech recognition engineer say that he chose his feature set by including all the features he could find in the linguistic phonetics literature. Given that feature set, the decision tree learning procedure decided which ones to actually use, and in what order. The examples presented above illustrate some of the kinds of linguistic knowledge that a competent computational linguist needs to know in order perform research at the highest level. I am concerned that many of the students currently graduating in the field do not seem to have received sufficient exposure to the structure of language at this level of detail. For instance, a few years ago I pointed out the problem of modeling question translation between Chinese and English to one of the brightest young researchers working with SPCFGs, and the problem had never occurred to him, even though he was a fluent speaker of both languages. I am sure this would be one of the first things that would occur to anyone brought up on the debates of the 1980s about the limitations of context-free grammar, upon first exposure to the SPCFG formalism. So, although I am a firm believer that the data-driven empirical approach computational linguistics will remain the most fruitful research paradigm for the foreseeable future, I also think that researchers need a firm grounding in descriptive linguistics.</abstract>
<intro confidence="0.892239">42</intro>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>