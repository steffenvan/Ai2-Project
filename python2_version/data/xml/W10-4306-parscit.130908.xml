<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.997597">
Probabilistic Ontology Trees for Belief Tracking in Dialog Systems
</title>
<author confidence="0.997608">
Neville Mehta Rakesh Gupta Antoine Raux
</author>
<affiliation confidence="0.994381">
Oregon State University Honda Research Institute Honda Research Institute
</affiliation>
<email confidence="0.9858">
mehtane@eecs.oregonstate.edu rgupta@hra.com araux@hra.com
</email>
<author confidence="0.978992">
Deepak Ramachandran
</author>
<affiliation confidence="0.984062">
Honda Research Institute
</affiliation>
<email confidence="0.978027">
dramachandran@hra.com
</email>
<sectionHeader confidence="0.982186" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9999715">
We introduce a novel approach for robust
belief tracking of user intention within
a spoken dialog system. The space of
user intentions is modeled by a proba-
bilistic extension of the underlying do-
main ontology called a probabilistic on-
tology tree (POT). POTs embody a prin-
cipled approach to leverage the dependen-
cies among domain concepts and incorpo-
rate corroborating or conflicting dialog ob-
servations in the form of interpreted user
utterances across dialog turns. We tailor
standard inference algorithms to the POT
framework to efficiently compute the user
intentions in terms of m-best most proba-
ble explanations. We empirically validate
the efficacy of our POT and compare it to
a hierarchical frame-based approach in ex-
periments with users of a tourism informa-
tion system.
</bodyText>
<sectionHeader confidence="0.995168" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9945665">
A central function of a spoken dialog system
(SDS) is to estimate the user’s intention based on
the utterances. The information gathered across
multiple turns needs to be combined and under-
stood in context after automatic speech recogni-
tion (ASR). Traditionally, this has been addressed
by dialog models and data structures such as forms
(Goddeau et al., 1996) and hierarchical task de-
composition (Rich and Sidner, 1998). To formal-
ize knowledge representation within the SDS and
enable the development of reusable software and
resources, researchers have investigated the or-
ganization of domain concepts using IS-A/HAS-A
ontologies (van Zanten, 1998; Noh et al., 2003).
Because the SDS only has access to noisy ob-
servations of what the user really uttered due to
speech recognition and understanding errors, be-
lief tracking in speech understanding has received
</bodyText>
<author confidence="0.705341">
Stefan Krawczyk
</author>
<affiliation confidence="0.662849">
Stanford University
</affiliation>
<email confidence="0.816795">
stefank@cs.stanford.edu
</email>
<bodyText confidence="0.999924829268293">
particular attention from proponents of probabilis-
tic approaches to dialog management (Bohus and
Rudnicky, 2006; Williams, 2006). The mecha-
nism for belief tracking often employs a Bayesian
network (BN) that represents the joint probabil-
ity space of concepts while leveraging conditional
independences among them (Paek and Horvitz,
2000). Designing a domain-specific BN requires
significant effort and expert knowledge that is not
always readily available. Additionally, real-world
systems typically yield large networks on which
inference is intractable without major assumptions
and approximations. A common workaround to
mitigate the intensive computation of the joint dis-
tribution over user intentions is to assume full con-
ditional independence between concepts which vi-
olates the ground truth in most domains (Bohus
and Rudnicky, 2006; Williams, 2006).
We propose a novel approach to belief track-
ing for an SDS that solves both the design and
tractability issues while making more realistic
conditional independence assumptions. We repre-
sent the space of user intentions via a probabilistic
ontology tree (POT) which is a tree-structured BN
whose structure is directly derived from the hier-
archical concept structure of the domain specified
via an IS-A/HAS-A ontology. The specialization
(IS-A) and composition (HAS-A) relationships be-
tween the domain concepts are intuitive and pro-
vide a systematic way of representing ontological
knowledge for a wide range of domains.
The remainder of the paper is structured as fol-
lows. We begin by describing the construction of
the POT given a domain ontology. We show how
a POT employs null semantics to represent con-
sistent user intentions based on the specialization
and composition constraints of the domain. We
then show how standard inference algorithms can
be tailored to exploit the characteristics of the POT
to efficiently infer the m-best list of probable ex-
planations of user intentions given the observa-
</bodyText>
<note confidence="0.9680815">
Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 37–46,
The University of Tokyo, September 24-25, 2010. p@c 2010 Association for Computational Linguistics
</note>
<page confidence="0.937695">
37
</page>
<bodyText confidence="0.9999593">
tions. The POT and the associated inference al-
gorithm empower a dialog manager (DM) to ac-
count for uncertainty while avoiding the design
complexity, intractability issues, and other restric-
tive assumptions that characterize state-of-the-art
systems. The section on empirical evaluation de-
scribes experiments in a tourist information do-
main that compare the performance of the POT
system to a frame-based baseline system. The pa-
per concludes with a discussion of related work.
</bodyText>
<sectionHeader confidence="0.947492" genericHeader="method">
2 Problem Formulation
</sectionHeader>
<bodyText confidence="0.984730368421053">
Let {X1, X2,. . . , XN} be a set of N concepts.
Every concept XZ takes its value from its finite dis-
crete domain D(XZ) which includes a special null
element for the cases where XZ is irrelevant. The
user intention space is defined as U = D(X1) x
D(X2) x · · · x D(XN). At each dialog turn t,
the system makes a noisy observation ot about
the true user intention u E U. ot consists of
a set of slots. A slot is a tuple (v, d, c) where
v E {X1, ... , XN}, d E D(v) is a value of v,
and c E R is the confidence score assigned to that
concept-value combination by the speech under-
standing (SU) system.
The goal of belief tracking is to maintain
Pr(X1, ... , XNJo1, ... , ot), a distribution over
the N-dimensional space U conditioned on all the
observations made up to turn t. At each turn, the
belief is updated based on the new observations to
estimate the true, unobserved, user intention.
</bodyText>
<sectionHeader confidence="0.960002" genericHeader="method">
3 Probabilistic Ontology Trees
</sectionHeader>
<bodyText confidence="0.9997192">
We model the space of the user intentions via a
POT. A POT is a tree-structured BN that extends
a domain ontology by specifying probability dis-
tributions over its possible instantiations based on
specializations and compositions.
</bodyText>
<subsectionHeader confidence="0.999321">
3.1 Domain Ontology
</subsectionHeader>
<bodyText confidence="0.90498">
To ensure that the corresponding POTs are tree-
structured, we consider a restricted class of do-
main ontologies over concepts.
Definition 1. A domain ontology is a labeled di-
rected acyclic graph. The set of vertices (corre-
sponding to the domain concepts) is partitioned
into {V0}, VS, and VC, where V0 is the only root
node, VS is the set of specialization nodes (re-
lated via IS-A to their parents), and VC is the set
of composition nodes (related via HAS-A to their
parents). The set of edges satisfy the constraints
</bodyText>
<figureCaption confidence="0.990098">
Figure 1: The ontology for a sample domain where
</figureCaption>
<bodyText confidence="0.93485036">
B IS-A A, C IS-A A, D IS-A A, E IS-A B, F IS-A B,
C HAS-A G (essential), D HAS-A G (nonessential),
H IS-A D, E HAS-A I (essential), J IS-A G, and
K IS-A G. Specialization nodes are drawn single-
lined, composition nodes are drawn double-lined,
and the root node is drawn triple-lined. Special-
ization subtrees are marked by dashed ovals.
that a specialization node has exactly one parent
and a composition node may only have more than
one parent if they are all specialization nodes with
a common parent.
Specialization nodes represent refinements of
their parent concepts. Specializations of a con-
cept are disjoint, that is, for any particular instance
of the parent exactly one specialization is applica-
ble and the rest are inapplicable. For example, if
Dog IS-A Animal and Cat IS-A Animal, then Cat
is inapplicable when Dog is applicable, and vice
versa. Composition nodes represent attributes of
their parents and may be essential or nonessential,
e.g., Dog HAS-A Color (essential), Dog HAS-A
Tail (nonessential). These definitions correspond
with the standard semantics in the knowledge rep-
resentation community (Noh et al., 2003). An ex-
ample ontology is shown in Figure 1.
</bodyText>
<construct confidence="0.797891666666667">
Definition 2. A specialization subtree (spec-tree)
in the ontology is a subtree consisting of a node
with its specialization children (if any).
</construct>
<subsectionHeader confidence="0.997866">
3.2 POT Construction
</subsectionHeader>
<bodyText confidence="0.9999618">
We now describe how a POT may be constructed
from a domain ontology. The purpose of the POT
is to maintain a distribution of possible instanti-
ations of the ontology such that the ontological
structure is respected.
</bodyText>
<equation confidence="0.805370666666667">
E
I
38
</equation>
<bodyText confidence="0.9985475">
Given an ontology G, the corresponding POT is
a tree-structured BN defined as follows:
Variables. Let T be a spec-tree in G with root
R. Unless R is a (non-root) specialization node
with no specialization children, T is represented
in the POT by a variable X with the domain
</bodyText>
<equation confidence="0.999447666666667">
D(X) = { {exists, null}, if ChildrenT(R) = 0
ChildrenT(R), if R = Vo
ChildrenT (R) U {null}, otherwise.
</equation>
<bodyText confidence="0.870693666666667">
Edges. Let POT variables X and Y correspond
to distinct spec-trees TX and TY in G. There is a
directed edge from X to Y if and only if either
</bodyText>
<listItem confidence="0.987521181818182">
• A leaf of TX is the root of TY .
• There is an edge from a leaf in TX to the non-
specialization root of TY .
• There is an edge from the non-specialization
root of TX to that of TY .
Conditional Probability Tables (CPTs). If X
(corresponding to spec-tree TX) is the parent of Y
(corresponding to spec-tree TY ) in the POT, then
Y ’s CPT is conditioned as follows:
• If TY is rooted at one of the leaves of TX,
then
</listItem>
<equation confidence="0.999985">
Pr(Y = null|X = Y) = 0
Pr(Y = null|X =� Y) = 1
</equation>
<bodyText confidence="0.98128">
where Y is the domain value of X corre-
sponding to child Y .
</bodyText>
<listItem confidence="0.961034666666667">
• If R is the root of TX, and TY has a compo-
sition root node that is attached only to nodes
in S C ChildrenTX (R), then
</listItem>
<equation confidence="0.997466">
Pr(Y = null|X = V) = 1
</equation>
<bodyText confidence="0.998685">
for any domain value V of X corresponding
to a node V E ChildrenTX (R) − S.
</bodyText>
<listItem confidence="0.734443">
• If the root of TY is an essential composition
node attached to a leaf V of TX, then
</listItem>
<equation confidence="0.997618">
Pr(Y = null|X = V) = 0
</equation>
<bodyText confidence="0.995388235294118">
where V is the domain value of X corre-
sponding to the leaf V .
We label a POT variable with that of the root of
the corresponding spec-tree for convenience. The
domain of a POT variable representing a spec-tree
comprises the specialization children (node names
in sanserif font) and the special value null; the null
Figure 2: The POT for the example domain. If a
node represents a spec-tree in the ontology, then it
is labeled by the root of the spec-tree; otherwise,
it is labeled with the name of the corresponding
ontology node. D(A) = {B, C, D}, D(B) = {E, F,
null}, D(D) = {H, null}, and Pr(A), Pr(B|A) and
Pr(D|A) represent some distributions over the re-
spective specializations. D(I) = {exists, null} and
D(G) = {J, K, null}. Note that a composition node
(G) can be shared between multiple specializa-
tions (C and D) in the ontology while the resulting
POT remains tree-structured.
value allows us to render any node (except the
root) inapplicable. Spec-trees comprising single
nodes have the domain value exists to switch be-
tween being applicable and inapplicable. The CPT
entries determine the joint probabilities over pos-
sible valid instantiations of the ontology and could
be based on expert knowledge or learned from
data. The conditions we impose on them (null se-
mantics) ensure that inconsistent instantiations of
the ontology have probability 0 in the POT. While
the ontology might have undirected cycles involv-
ing the children of spec-trees, the corresponding
POT is a tree because spec-trees in the ontology
collapse into single POT nodes. The POT for the
example domain is shown in Figure 2.
</bodyText>
<subsectionHeader confidence="0.994789">
3.3 Tourist Information POT
</subsectionHeader>
<bodyText confidence="0.961059">
For the empirical analysis, we designed a POT for
a tourist information system that informs the user
about places to shop, eat, get service, and displays
relevant information such as the distance to an in-
tended location. The user can also provide con-
versational commands such as stop, reset, undo,
etc. The full ontology for the tourist information
domain is shown in Figure 3 and the POT is in
Figure 4. In the POT, Action is the root node, with
D(Action) = {Venue, Command}, and D(Venue)
</bodyText>
<figure confidence="0.885121333333333">
39
Ambi
Cui
</figure>
<figureCaption confidence="0.722665">
Figure 3: The ontology for the tourist information domain. All the composition nodes have specializa-
tions of their own (such as Japanese and Greek for Cuisine), but have not been shown for the sake of
compactness.
</figureCaption>
<bodyText confidence="0.934522153846154">
= {Restaurant, Store, Service, null}. All the com-
position (or attribute) nodes such as Hours and
Rating are made children of Venue by construc-
tion. Since a Command is inapplicable when the
Action is a Venue, we have Pr(Command = null
 |Action = Venue) = 1. The composition nodes
(Cuisine, Street, etc.) have specializations of their
own ({Japanese, Greek, ... }, {Castro, Elm, ... },
etc.), but are not shown for the sake of clarity.
Since Cuisine is an essential attribute of Restau-
rant, Pr(Cuisine = null  |Venue = Restaurant) = 0;
moreover, Pr(Cuisine = null  |Venue = Service) =
1 because Cuisine is not relevant for Service.
</bodyText>
<sectionHeader confidence="0.968916" genericHeader="method">
4 Inferring User Intention
</sectionHeader>
<bodyText confidence="0.9999706">
We have seen how the POT provides the proba-
bilistic machinery to represent domain knowledge.
We now discuss how the POT structure can be
leveraged to infer user intention based on the slots
provided by the SU.
</bodyText>
<subsectionHeader confidence="0.999352">
4.1 Soft Evidence
</subsectionHeader>
<bodyText confidence="0.999980333333333">
Every slot retrieved from the SU needs to be incor-
porated as observed evidence in the POT. We can
set the associated node within the POT directly to
its domain value as hard evidence when we know
these values with certainty. Instead, we employ
probabilistic observations to soften the evidence
entered into the POT. We assume that the confi-
dence score c E [0,100] of a slot corresponds to
the degree of certainty in the observation. For an
observed slot variable X, we create an observation
node X� on the fly with the same domain as X and
make it a child of X. If x is the observed value for
slot X, then the CPT of X� is constructed from the
slot’s confidence score as follows:
The probability values are generated by lin-
early interpolating between the uniform probabil-
ity value and 1 based on the confidence score. For
the remaining values,
</bodyText>
<equation confidence="0.98051925">
�
1 − ε(|D(X) |− 1), X = X
Pr( X|X =� x) =
ε, X� =� X
</equation>
<bodyText confidence="0.999993357142857">
where ε &gt; 0.1 Since the confidence score gives an
indication of the probability for the observed value
of a slot but says nothing about the remaining val-
ues, the diagonal elements for the remaining val-
ues are near 1. We cannot make them exactly 1
because the observation node needs to coexist with
possibly conflicting observations in the POT.
If the user confirms the current POT hypothesis,
then observations corresponding to the current hy-
pothesis (with CPTs proportional to the score of
the confirmation) are added to the POT to enforce
the belief. If the user denies the current hypothe-
sis, then all observations corresponding to the cur-
rent hypothesis are removed from the POT.
</bodyText>
<equation confidence="0.933110625">
1In our experiments, we use a = 10−10.
�
D
X|X = x) |1)(X
)l 00)I
Pr(
c(|D(X)|−1)/100+1 ,X� = x
|D(X) |, X� =� x
</equation>
<figure confidence="0.912031666666666">
40
Ambie
Cuisi
Cuisi
Japanese Greek
null
Japanese 0.6 0.2 0.2
Greek E 1-2E E
null E E 1-2E
</figure>
<figureCaption confidence="0.995826">
Figure 4: The POT for the tourist information domain. Assuming that D(Cuisine) = {Japanese, Greek,
</figureCaption>
<bodyText confidence="0.994629714285714">
null} and D(Street) = {Castro, Elm, null}, the shaded observation nodes represent the soft evidence for
input slots (Cuisine, Japanese, 40) and (Street, Castro, 70).
The POT for the tourist information domain af-
ter getting two slots as input is shown in Figure 4.
The attached nodes are set to the observed slot val-
ues and the evidence propagates through the POT
as explained in the next section.
</bodyText>
<subsectionHeader confidence="0.936929">
4.2 POT Inference
</subsectionHeader>
<bodyText confidence="0.99975772">
A probable explanation (PE) or hypothesis is an
assignment of values to the variables in the POT,
and the most probable explanation (MPE) within
the POT is the explanation that maximizes the
joint probability conditioned on the observed vari-
ables. The top m estimates of the user’s intentions
correspond to the m-best MPEs. The design of the
POT ensures that the m-best MPEs are all con-
sistent across specializations, that is, exactly one
specialization is applicable per node in any PE; all
inconsistent explanations have a probability of 0.
The m-best MPEs could be found naively us-
ing the Join-Tree algorithm to compute the joint
distribution over all variables and then use that to
find the top m explanations. The space required to
store the joint distribution alone is O(nN), where
N is the number of nodes and n the number of
values per node. Because the run time complex-
ity is at least as much as this, it is impractical for
any reasonably sized tree. However, we can get
a significant speedup for a fixed m by using the
properties of the POT.
Algorithm 1 uses a message-passing protocol,
similar to many in the graphical models litera-
ture (Koller and Friedman, 2009), to simulate a
</bodyText>
<figure confidence="0.4212533125">
Algorithm 1 COMPUTE-PE
Input: POT T with root X0, number of MPEs m, evidence E
Output: m MPEs for T
1: for X E T in reverse topological order do
2: Collect messages oYi from all children Y of X
3: oX = COMPUTE-MPE-MESSAGE(X, m, {oYi1)
4: end for
5: return top m elements of Pr(X0|E)oX0(-) without E
Algorithm 2 COMPUTE-MPE-MESSAGE
Input: POT node X, number of MPEs m, messages from
children oYi
Output: Message oX(-)
1: if X is a leaf node then
2: oX(x) +— 1, Vx E D(X)
3: return oX
4: end if
</figure>
<listItem confidence="0.914911125">
5: for x E D(X) do
6: for 9� = ((y1, 91), ... ,(yk, zk)) E {D(oY�)x...x
D(oYk) : Pr (Y = null|X = x, E) &lt; 11 do
7: 0&apos;X (x, -1) &apos;— lli [ Pr(Yi = yi|X = x, E),OYi (yi, 4)]
8: end for
9: oX(x) +— top m elements of oz(x).
10: end for
11: return oX
</listItem>
<bodyText confidence="0.998939">
dynamic programming procedure across the lev-
els of the tree (see Figure 5). In Algorithm 2, an
MPE message is computed at each node X using
messages from the children, and sent to the par-
ent. The message from X is the function (or ta-
ble) OX(x, z) that represents the probabilities of
the top m explanations, z, of the subtree rooted at
X for a particular value of X = x. At the root
node X0 we try all values of x0 to find the top m
MPEs for the entire tree. Note that in step 7, we
</bodyText>
<page confidence="0.853097">
41
</page>
<figureCaption confidence="0.764084">
Figure 5: COMPUTE-MPE applied to the exam-
</figureCaption>
<bodyText confidence="0.98370816">
ple POT. (a) Inference starts with the messages be-
ing passed up from the leaves to the root A. Every
message OX is an m x n table that contains the
probabilities for the m-best MPEs of the subtree
rooted at X for all the n domain values of X. (b)
At the root, A is set to its first element B, and its
marginal Pr(A = B) is combined with the mes-
sage OB. The semantics of the POT ensures that
the other messages can be safely ignored because
those subtrees are known to be null with probabil-
ity 1. (c) A is set to C and only the essential at-
tribute G is non-null. (d) A is set to its final el-
ement D, and consequently both the node D and
the nonessential attribute G are non-null and their
messages are mutually independent.
need the marginal P(Y |X, E) which can be ef-
ficiently computed by a parallel message-passing
method. Evidence nodes can only appear as leaves
because of our soft evidence representation, and
are encompassed by the base case. The algorithm
leverages the fact that the joint of any entire sub-
tree rooted at a node that is null with probability 1
can be safely assumed to be null with probability
1. The validity of Algorithm 1 is proven in Ap-
pendix A.
</bodyText>
<subsectionHeader confidence="0.99946">
4.3 Complexity Analysis
</subsectionHeader>
<bodyText confidence="0.999975238095238">
At a POT node with at most n values and branch-
ing factor k, we do n maximizations over the prod-
uct space of k nm-sized lists. Thus, the time
complexity of Algorithm 1 on a POT with N
nodes is O(N(nm)k) and the space complexity is
O(Nnmk). (Insertion sort maintains a sorted list
truncated at m elements to keep track of the top
m elements at any time.) However, the algorithm
is significantly faster on specialization nodes be-
cause only one child is applicable and needs to be
considered in the maximization (step 7). In the ex-
treme case of a specialization-only POT, the time
and space complexities both drop to O(Nmn).
A similar algorithm for incrementally finding
m-best MPEs in a general BN is given in Srinivas
and Nayak (1996). However, our approach has the
ability to leverage the null semantics in POTs re-
sulting in significant speedup as described above.
This is crucial because the run-time complexity of
enumerating MPEs is known to be PPP-Complete
for a general BN (Kwisthout, 2008).
</bodyText>
<sectionHeader confidence="0.992381" genericHeader="method">
5 Empirical Evaluation
</sectionHeader>
<bodyText confidence="0.9927205625">
To test the effectiveness of our POT approach, we
compare it to a frame-based baseline system for
inferring user intentions.
The baseline system uses a hierarchical frame-
based approach. Each frame maps to a par-
ticular user intention, and the frames are filled
concurrently from the dialog observations. The
slots from a turn overwrite matching slots re-
ceived in previous turns. The baseline system uses
the same ontology as the POT to insure that it
only produces consistent hypotheses, e.g., it never
produces ”Venue=Service, Cuisine=Japanese” be-
cause Service does not have a Cuisine attribute.
When several hypotheses compete, the system se-
lects the one with the maximum allocated slots.
We implemented the POT engine based on the
Probabilistic Network Library (Intel, 2005). It
takes a POT specification as input, receives the
ASR slots, and returns its m-best MPEs.
Using a tourism information spoken dialog sys-
tem, we collected a corpus of 375 dialogs from
15 users with a total of 720 turns (details in
Appendix B). Evaluation is performed by run-
ning these collected dialogs in batch and pro-
viding the ASR slots of each turn to both the
baseline and POT belief-tracking systems.2 Af-
ter each turn, both systems return their best hy-
pothesis of the overall user intention in the form
of a set of concept-value pairs. These hypothe-
2Speech recognition and understanding was performed
using the Nuance Speech Recognition System v8.5 running
manual and statistical grammars with robust interpretation.
</bodyText>
<figure confidence="0.9991405">
OB
OB
OD
OD
(c)
(d)
B
B
ψI
ψI
I
I
(a)
(b)
A = C
A = D
B
OG
D
B
OG
D
OI
OI
I G
I G
</figure>
<page confidence="0.480572">
42
</page>
<table confidence="0.997260428571429">
System Precision Recall F1
Top hypothesis 0.84 0.81 0.83
Top 2 hypotheses 0.87 0.84 0.85
POT Top 3 hypotheses 0.89 0.85 0.87
Top 4 hypotheses 0.91 0.86 0.89
Top 5 hypotheses 0.92 0.86 0.89
Baseline 0.84 0.79 0.81
</table>
<tableCaption confidence="0.986181">
Table 1: Precision/recall results comparing the
</tableCaption>
<bodyText confidence="0.976702027027027">
baseline system against the POT-based system on
the 25-scenario experiment. Results are averaged
over all 15 users.
Figure 6: F1 score as a function of the log-
likelihood of the top hypothesis for the user’s goal.
ses are compared to the true user intention ex-
pressed so far in the dialog (e.g., if the user wants
a cheap restaurant but has not mentioned it yet,
PriceRange=Cheap is not considered part of the
ground truth). This offline approach allows us to
compare both versions on the same input.
Table 1 shows the precision/recall results for the
experiment based on comparing the set of true user
intention concepts to the inferred hypotheses of
the POT and baseline systems. The average word
error rate for all users is 29.6%. The POT sys-
tem shows a 2% improvement in recall and F1
over the baseline. Additionally, leveraging the m-
best hypotheses beyond just the top one could help
enhance performance or guide useful clarification
questions as shown by the improved performance
when using the top 2–5 hypotheses; we assume
an oracle for selecting the hypothesis with highest
F1 among the top m hypotheses. All of the CPTs
in the POT (besides the structural constraints) are
uniformly distributed. Thus, the performance of
the POT could be further improved by training the
CPTs on real data.
To assess the quality of likelihood returned by
the POT as a belief confidence measure, we binned
dialog turns according to the log-likelihood of the
top hypothesis and then computed the F1 score of
each bin. Figure 6 shows that belief log-likelihood
is indeed a good predictor of the F1 score. This
information could be very useful to a dialog man-
ager to trigger confirmation or clarification ques-
tions for example.
</bodyText>
<sectionHeader confidence="0.997496" genericHeader="method">
6 Discussion
</sectionHeader>
<bodyText confidence="0.999947425">
The definition and construction of POTs provide a
principled and systematic way to construct proba-
bilistic models for an SDS. While any BN can be
used to model the space of user intentions, design-
ing an effective network is not an easy task for sys-
tem designers not well versed in graphical mod-
els. In previous belief tracking work, researchers
describe their networks with little indication on
how they arrived at the specific structure (Paek and
Horvitz, 2000; Thomson and Young, 2009). Prior
work on ontologies for SDSs (van Zanten, 1998;
Noh et al., 2003) as well as the prominence of
concept hierarchies in other areas such as object-
oriented programming and knowledge engineer-
ing make them a natural and intuitive way of repre-
senting SDS domains. The development of POTs
builds on past research on constructing BNs based
on ontological knowledge (Helsper and van der
Gaag, 2002; Pfeffer et al., 1999).
While most approaches to belief tracking in the
dialog systems community make a strict indepen-
dence assumption between concepts (Bohus and
Rudnicky, 2006; Williams, 2006), POTs model
the dependencies between concepts connected by
specialization and composition relationships while
remaining significantly more tractable than gen-
eral BNs and being very straightforward to de-
sign. The null semantics allow a POT to capture
disjoint values and the applicability of attributes
which are common aspects of concept ontologies.
Obviously, a POT cannot capture all types of con-
cept relationships since each concept can have
only one parent. However, this restriction allows
us to perform efficient exact computation of the
m-best MPEs which is a significant advantage.
Statistical Relational Learning approaches such as
Markov Logic Networks (Richardson and Domin-
gos, 2006) have been developed for more general
relational models than strict ontologies, but they
lack the parsimony and efficiency of POTs.
</bodyText>
<figure confidence="0.994635272727273">
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2−1 −0.8 −0.6 −0.4 −0.2 0
Log−likelihood of top POT hypothesis
F1
</figure>
<page confidence="0.971965">
43
</page>
<bodyText confidence="0.999986676470588">
Thomson and Young (2009) describe an ap-
proach to dialog management based on a partially
observable Markov decision process (POMDP)
whose policy depends only on individual con-
cepts’ marginal distributions rather than on the
overall user intention. Because their system per-
forms belief tracking with a dynamic Bayesian
network (DBN) rather than a static BN, the ex-
act marginal computation is intractable and the au-
thors use loopy belief propagation to compute the
marginals. Even then, they indicate that the depen-
dencies of the subgoals must be limited to enable
tractability. In practice, all concepts are made in-
dependent except for the binary validity nodes that
deterministically govern the dependence between
nodes (similar to the null semantics of a POT).
Williams (2007) also represents the user goal as
a DBN for a POMDP-based DM. They perform
belief updating using particle filtering and approx-
imate the joint probability over the user intention
with the product of the concept marginals. This
could lead to inaccurate estimation for condition-
ally dependent concepts.
Among authors who have used m-best lists of
dialog states for dialog management, Higashinaka
et al. (2003) have shown empirically that main-
taining multiple state hypotheses facilitates shorter
dialogs. Their system scores each dialog state
using a linear combination of linguistic and dis-
course features, and this score is used by a hand-
crafted dialog policy. While illustrating the advan-
tages of m-best lists, this scoring approach lacks
theoretical justification and ability to include prior
knowledge that POTs inherit from BNs.
</bodyText>
<sectionHeader confidence="0.996742" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999984">
We have presented the POT framework for belief
tracking in an SDS. We have shown how a POT
can be constructed from the domain ontology and
provided an exact algorithm to infer the user’s in-
tention in real-time. POTs strike a balance be-
tween representing rich concept dependencies and
facilitating efficient tracking of the m-best user in-
tentions based on exact joint probabilities rather
than approximations such as concept marginals.
</bodyText>
<sectionHeader confidence="0.990052" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999112654545455">
D. Bohus and A. Rudnicky. 2006. A K Hypotheses
+ Other Belief Updating Model. In AAAI Workshop
on Statistical and Empirical Approaches to Spoken
Dialogue Systems.
D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and
S. Busayapongchai. 1996. A Form-Based Dialogue
Manager for Spoken Language Applications. In IC-
SLP.
E. Helsper and L. van der Gaag. 2002. Building
Bayesian Networks through Ontologies. In Euro-
pean Conference on Artificial Intelligence.
R. Higashinaka, M. Nakano, and K. Aikawa. 2003.
Corpus based Discourse Understanding on Spoken
Dialog Systems. In Annual Meeting on Association
for Computational Linguistics.
Intel. 2005. Probabilistic Network Library. http://
sourceforge.net/projects/openpnl/.
D. Koller and N. Friedman. 2009. Probabilistic
Graphical Models: Principles and Techniques. MIT
Press.
J. Kwisthout. 2008. Complexity Results for Enumerat-
ing MPE and Partial MAP. In European Workshop
on Probabilistic Graphical Models.
H. Noh, C. Lee, and G. Lee. 2003. Ontology-based
Inference for Information-seeking in Natural Lan-
guage Dialog Systems. In IEEE International Con-
ference on Industrial Informatics.
T. Paek and E. Horvitz. 2000. Conversation as Ac-
tion under Uncertainty. In Uncertainty in Artificial
Intelligence.
A. Pfeffer, D. Koller, B. Milch, and K. T. Takusagawa.
1999. Spook: A system for probabilistic object-
oriented knowledge representation. In Uncertainty
in Artifical Intelligence.
C. Rich and C. Sidner. 1998. COLLAGEN: a Col-
laboration Manager for Software Interface Agents.
An International Journal: User Modeling and User
Adapted Interaction, 8.
M. Richardson and P. Domingos. 2006. Markov Logic
Networks. Machine Learning, 62:107–136.
S. Srinivas and P. Nayak. 1996. Efficient Enumeration
of Instantiations in Bayesian Networks. In UAI.
B. Thomson and S. Young. 2009. Bayesian Update
of Dialogue State: A POMDP Framework for Spo-
ken Dialogue Systems. Computer Speech and Lan-
guage.
G. van Zanten. 1998. Adaptive Mixed-Initiative Dia-
logue Management. In IEEE Workshop on Interac-
tive Voice Technology for Telecommunications Ap-
plications.
J. Williams. 2006. Partially Observable Markov Deci-
sion Processes for Dialog Management.
J. Williams. 2007. Using Particle Filters to Track
Dialogue State. In IEEE Workshop on Automatic
Speech Recognition &amp; Understanding.
</reference>
<page confidence="0.94195">
44
</page>
<bodyText confidence="0.954533941176471">
A Analysis of the Inference Algorithm
Theorem 1. Algorithm 1 returns the top m MPEs
of the POT along with their joint probabilities.
Proof. We first prove this for the special case of
m = 1 to simplify notation. For the base case
of a node with no children, Algorithm 2 sim-
ply returns a message with all probabilities at
1 for all values of that node. Now, consider a
node X with children Y1, ... , Yk. Let Desc(Y)
be the descendants of node Y . Since Algo-
rithm 2 given node X returns exactly one expla-
nation, z for each x E D(X), we will define
OX(x) = OX(x, z). Now, to show that OX(x) =
maxDesc(X) Pr(Desc(X)|X = x, E), that is, Al-
gorithm 2 returns the top explanation of the entire
subtree rooted at X for every value in D(X), we
use structural induction on the tree.
</bodyText>
<equation confidence="0.972716">
Pr(Desc(X)jX = x, E)
</equation>
<bodyText confidence="0.971377692307692">
6. Find an ATM on Castro Street in Mountain
View.
Figure 7 shows a typical interaction with the
system for the first scenario along with a possi-
ble hypothesis inferred by the system at every turn
of the dialog. Figure 8 shows an example where
the POT system is able to discard an incorrect ob-
servation about a restaurant based on the accumu-
lated belief about bookstores over multiple turns.
Figure 9 shows how the POT is able to leverage the
ontological structure to pick out higher-level con-
cepts with lower confidence scores over spurious
low-level concepts with higher confidence scores.
</bodyText>
<equation confidence="0.9219774375">
max
Desc(X)
= max Pr(Y1:k, Desc(Y1:k)jX = x, E)
Y1:k,Desc(Y1:k)
Y
=max Pr(YijX = x, E) Pr(Desc(Yi)jYi, E)
Y1:k,Desc(Y1:k)
i
h i
max Pr(YijX = x, E) Pr(Desc(Yi)jYi, E)
Yi,Desc(Yi)
hPr(Yi jX = x, E) max Pr(Desc(Yi)jYi, E)i
Desc(Yi )
h i
Pr(YijX = x, E)�Yi(yi) {Inductive step}
= ?PX(x),
</equation>
<bodyText confidence="0.89526575">
The proof for m &gt; 1, where every maximization
returns a list of the top m elements, is similar.
B Dialogs in the Tourist Information
Domain
Each user conducted 25 dialogs according to pre-
scribed scenarios for the tourist information do-
main. The order of scenarios was randomized for
each user. Sample scenarios:
</bodyText>
<listItem confidence="0.6730714">
1. Find a good and cheap Mexican restaurant in
Mountain View.
2. There is a medical emergency and you need
to get to the hospital. Find a route.
3. You need to find your favorite coffee fran-
chise. You have 10 minutes to get coffee.
4. Find a place to buy some fruits and vegeta-
bles.
5. Find a Chinese restaurant in Santa Clara with
good ambiance, and display travel distance.
</listItem>
<figure confidence="0.996193772727273">
Y=
i
Y=
i
Y=
i
max
Yi
max
Yi
45
User Find a Mexican restaurant in Mountain View.
Hypothesis [venue restaurant] [area mountain view] [cuisine italian]
{Note: Mexican is misrecognized as Italian.}
User No, Mexican.
Hypothesis [venue restaurant] [area mountain view] [cuisine mexican]
User Show me ones with at least four star rating.
Hypothesis [venue restaurant] [area mountain view] [cuisine mexican] [rating four star]
User I want a cheap place.
Hypothesis [venue restaurant] [area mountain view] [cuisine mexican] [rating four star] [price cheap]
User Is there anything on Castro?
Hypothesis [venue restaurant] [area mountain view] [street castro] [cuisine mexican] [rating four star] [price cheap]
</figure>
<figureCaption confidence="0.989336666666667">
Figure 7: A sample dialog in the tourism information domain showing the inferred hypothesis of the
user’s intention at every turn. The information response from the system’s back-end is based on its
current hypothesis.
</figureCaption>
<figure confidence="0.9723056">
User utterance Where is the bookstore?
ASR where is the bookstore
True hypothesis [action venue] [venue store] [sell book]
Baseline hypothesis [action venue] [venue store] [sell book]
POT hypothesis [action venue] [venue store] [sell book]
User utterance Store on Market Street.
ASR store on market street
True hypothesis [action venue] [venue store] [sell book] [street market]
Baseline hypothesis [action venue] [venue store] [sell book] [street market]
POT hypothesis [action venue] [venue store] [sell book] [street market]
User utterance In downtown.
ASR dennys
True hypothesis [action venue] [venue store] [sell book] [street market] [area downtown]
Baseline hypothesis [action venue] [venue restaurant] [brand dennys]
POT hypothesis [action venue] [venue store] [sell book] [street market]
</figure>
<figureCaption confidence="0.988233666666667">
Figure 8: A dialog showing the ASR input for the user’s utterance, and the corresponding true, baseline,
and POT hypotheses. The POT is able to correctly discard the inconsistent observation in the third turn
with the observations in previous turns.
</figureCaption>
<figure confidence="0.9320688">
User utterance Where should I go to buy Lego for my kid?
SU slots (Venue Store 38) (ServiceType GolfCourse 60)
True hypothesis [action venue] [venue store] [storetype toy]
Baseline hypothesis [action venue] [venue service] [servicetype golf course]
POT hypothesis [action venue] [venue store]
</figure>
<figureCaption confidence="0.987743">
Figure 9: A single dialog turn showing the SU slots for the user’s utterance, and the corresponding
</figureCaption>
<bodyText confidence="0.993405">
baseline, POT, and true hypotheses. Any system that looks at the individual confidence scores will base
its hypothesis on the (ServiceType GolfCourse 60) slot. Instead, the POT hypothesis is influenced by
(Venue Store 38) because its score in combination with the concept’s location in the POT makes it more
likely than the other slot.
</bodyText>
<page confidence="0.951878">
46
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.892786">
<title confidence="0.999964">Probabilistic Ontology Trees for Belief Tracking in Dialog Systems</title>
<author confidence="0.998361">Neville Mehta Rakesh Gupta Antoine Raux</author>
<affiliation confidence="0.999908">Oregon State University Honda Research Institute Honda Research Institute</affiliation>
<email confidence="0.984635">mehtane@eecs.oregonstate.edurgupta@hra.comaraux@hra.com</email>
<author confidence="0.936481">Deepak Ramachandran</author>
<affiliation confidence="0.998589">Honda Research Institute</affiliation>
<email confidence="0.999277">dramachandran@hra.com</email>
<abstract confidence="0.998581761904762">We introduce a novel approach for robust belief tracking of user intention within a spoken dialog system. The space of user intentions is modeled by a probabilistic extension of the underlying domain ontology called a probabilistic ontology tree (POT). POTs embody a principled approach to leverage the dependencies among domain concepts and incorporate corroborating or conflicting dialog observations in the form of interpreted user utterances across dialog turns. We tailor standard inference algorithms to the POT framework to efficiently compute the user in terms of most probable explanations. We empirically validate the efficacy of our POT and compare it to a hierarchical frame-based approach in experiments with users of a tourism information system.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>D Bohus</author>
<author>A Rudnicky</author>
</authors>
<title>A K Hypotheses + Other Belief Updating Model.</title>
<date>2006</date>
<booktitle>In AAAI Workshop on Statistical and Empirical Approaches to Spoken Dialogue Systems.</booktitle>
<contexts>
<context position="2141" citStr="Bohus and Rudnicky, 2006" startWordPosition="311" endWordPosition="314">idner, 1998). To formalize knowledge representation within the SDS and enable the development of reusable software and resources, researchers have investigated the organization of domain concepts using IS-A/HAS-A ontologies (van Zanten, 1998; Noh et al., 2003). Because the SDS only has access to noisy observations of what the user really uttered due to speech recognition and understanding errors, belief tracking in speech understanding has received Stefan Krawczyk Stanford University stefank@cs.stanford.edu particular attention from proponents of probabilistic approaches to dialog management (Bohus and Rudnicky, 2006; Williams, 2006). The mechanism for belief tracking often employs a Bayesian network (BN) that represents the joint probability space of concepts while leveraging conditional independences among them (Paek and Horvitz, 2000). Designing a domain-specific BN requires significant effort and expert knowledge that is not always readily available. Additionally, real-world systems typically yield large networks on which inference is intractable without major assumptions and approximations. A common workaround to mitigate the intensive computation of the joint distribution over user intentions is to </context>
<context position="24223" citStr="Bohus and Rudnicky, 2006" startWordPosition="4211" endWordPosition="4214">e (Paek and Horvitz, 2000; Thomson and Young, 2009). Prior work on ontologies for SDSs (van Zanten, 1998; Noh et al., 2003) as well as the prominence of concept hierarchies in other areas such as objectoriented programming and knowledge engineering make them a natural and intuitive way of representing SDS domains. The development of POTs builds on past research on constructing BNs based on ontological knowledge (Helsper and van der Gaag, 2002; Pfeffer et al., 1999). While most approaches to belief tracking in the dialog systems community make a strict independence assumption between concepts (Bohus and Rudnicky, 2006; Williams, 2006), POTs model the dependencies between concepts connected by specialization and composition relationships while remaining significantly more tractable than general BNs and being very straightforward to design. The null semantics allow a POT to capture disjoint values and the applicability of attributes which are common aspects of concept ontologies. Obviously, a POT cannot capture all types of concept relationships since each concept can have only one parent. However, this restriction allows us to perform efficient exact computation of the m-best MPEs which is a significant adv</context>
</contexts>
<marker>Bohus, Rudnicky, 2006</marker>
<rawString>D. Bohus and A. Rudnicky. 2006. A K Hypotheses + Other Belief Updating Model. In AAAI Workshop on Statistical and Empirical Approaches to Spoken Dialogue Systems.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Goddeau</author>
<author>H Meng</author>
<author>J Polifroni</author>
<author>S Seneff</author>
<author>S Busayapongchai</author>
</authors>
<title>A Form-Based Dialogue Manager for Spoken Language Applications.</title>
<date>1996</date>
<booktitle>In ICSLP.</booktitle>
<contexts>
<context position="1469" citStr="Goddeau et al., 1996" startWordPosition="215" endWordPosition="218">ficiently compute the user intentions in terms of m-best most probable explanations. We empirically validate the efficacy of our POT and compare it to a hierarchical frame-based approach in experiments with users of a tourism information system. 1 Introduction A central function of a spoken dialog system (SDS) is to estimate the user’s intention based on the utterances. The information gathered across multiple turns needs to be combined and understood in context after automatic speech recognition (ASR). Traditionally, this has been addressed by dialog models and data structures such as forms (Goddeau et al., 1996) and hierarchical task decomposition (Rich and Sidner, 1998). To formalize knowledge representation within the SDS and enable the development of reusable software and resources, researchers have investigated the organization of domain concepts using IS-A/HAS-A ontologies (van Zanten, 1998; Noh et al., 2003). Because the SDS only has access to noisy observations of what the user really uttered due to speech recognition and understanding errors, belief tracking in speech understanding has received Stefan Krawczyk Stanford University stefank@cs.stanford.edu particular attention from proponents of</context>
</contexts>
<marker>Goddeau, Meng, Polifroni, Seneff, Busayapongchai, 1996</marker>
<rawString>D. Goddeau, H. Meng, J. Polifroni, S. Seneff, and S. Busayapongchai. 1996. A Form-Based Dialogue Manager for Spoken Language Applications. In ICSLP.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E Helsper</author>
<author>L van der Gaag</author>
</authors>
<title>Building Bayesian Networks through Ontologies.</title>
<date>2002</date>
<booktitle>In European Conference on Artificial Intelligence.</booktitle>
<marker>Helsper, van der Gaag, 2002</marker>
<rawString>E. Helsper and L. van der Gaag. 2002. Building Bayesian Networks through Ontologies. In European Conference on Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Higashinaka</author>
<author>M Nakano</author>
<author>K Aikawa</author>
</authors>
<title>Corpus based Discourse Understanding on Spoken Dialog Systems.</title>
<date>2003</date>
<booktitle>In Annual Meeting on Association for Computational Linguistics.</booktitle>
<contexts>
<context position="26350" citStr="Higashinaka et al. (2003)" startWordPosition="4542" endWordPosition="4545">able tractability. In practice, all concepts are made independent except for the binary validity nodes that deterministically govern the dependence between nodes (similar to the null semantics of a POT). Williams (2007) also represents the user goal as a DBN for a POMDP-based DM. They perform belief updating using particle filtering and approximate the joint probability over the user intention with the product of the concept marginals. This could lead to inaccurate estimation for conditionally dependent concepts. Among authors who have used m-best lists of dialog states for dialog management, Higashinaka et al. (2003) have shown empirically that maintaining multiple state hypotheses facilitates shorter dialogs. Their system scores each dialog state using a linear combination of linguistic and discourse features, and this score is used by a handcrafted dialog policy. While illustrating the advantages of m-best lists, this scoring approach lacks theoretical justification and ability to include prior knowledge that POTs inherit from BNs. 7 Conclusion We have presented the POT framework for belief tracking in an SDS. We have shown how a POT can be constructed from the domain ontology and provided an exact algo</context>
</contexts>
<marker>Higashinaka, Nakano, Aikawa, 2003</marker>
<rawString>R. Higashinaka, M. Nakano, and K. Aikawa. 2003. Corpus based Discourse Understanding on Spoken Dialog Systems. In Annual Meeting on Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Intel</author>
</authors>
<title>Probabilistic Network Library.</title>
<date>2005</date>
<note>http:// sourceforge.net/projects/openpnl/.</note>
<contexts>
<context position="20364" citStr="Intel, 2005" startWordPosition="3548" endWordPosition="3549"> framebased approach. Each frame maps to a particular user intention, and the frames are filled concurrently from the dialog observations. The slots from a turn overwrite matching slots received in previous turns. The baseline system uses the same ontology as the POT to insure that it only produces consistent hypotheses, e.g., it never produces ”Venue=Service, Cuisine=Japanese” because Service does not have a Cuisine attribute. When several hypotheses compete, the system selects the one with the maximum allocated slots. We implemented the POT engine based on the Probabilistic Network Library (Intel, 2005). It takes a POT specification as input, receives the ASR slots, and returns its m-best MPEs. Using a tourism information spoken dialog system, we collected a corpus of 375 dialogs from 15 users with a total of 720 turns (details in Appendix B). Evaluation is performed by running these collected dialogs in batch and providing the ASR slots of each turn to both the baseline and POT belief-tracking systems.2 After each turn, both systems return their best hypothesis of the overall user intention in the form of a set of concept-value pairs. These hypothe2Speech recognition and understanding was p</context>
</contexts>
<marker>Intel, 2005</marker>
<rawString>Intel. 2005. Probabilistic Network Library. http:// sourceforge.net/projects/openpnl/.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Koller</author>
<author>N Friedman</author>
</authors>
<title>Probabilistic Graphical Models: Principles and Techniques.</title>
<date>2009</date>
<publisher>MIT Press.</publisher>
<contexts>
<context position="16092" citStr="Koller and Friedman, 2009" startWordPosition="2750" endWordPosition="2753">best MPEs could be found naively using the Join-Tree algorithm to compute the joint distribution over all variables and then use that to find the top m explanations. The space required to store the joint distribution alone is O(nN), where N is the number of nodes and n the number of values per node. Because the run time complexity is at least as much as this, it is impractical for any reasonably sized tree. However, we can get a significant speedup for a fixed m by using the properties of the POT. Algorithm 1 uses a message-passing protocol, similar to many in the graphical models literature (Koller and Friedman, 2009), to simulate a Algorithm 1 COMPUTE-PE Input: POT T with root X0, number of MPEs m, evidence E Output: m MPEs for T 1: for X E T in reverse topological order do 2: Collect messages oYi from all children Y of X 3: oX = COMPUTE-MPE-MESSAGE(X, m, {oYi1) 4: end for 5: return top m elements of Pr(X0|E)oX0(-) without E Algorithm 2 COMPUTE-MPE-MESSAGE Input: POT node X, number of MPEs m, messages from children oYi Output: Message oX(-) 1: if X is a leaf node then 2: oX(x) +— 1, Vx E D(X) 3: return oX 4: end if 5: for x E D(X) do 6: for 9� = ((y1, 91), ... ,(yk, zk)) E {D(oY�)x...x D(oYk) : Pr (Y = nu</context>
</contexts>
<marker>Koller, Friedman, 2009</marker>
<rawString>D. Koller and N. Friedman. 2009. Probabilistic Graphical Models: Principles and Techniques. MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Kwisthout</author>
</authors>
<title>Complexity Results for Enumerating MPE and Partial MAP.</title>
<date>2008</date>
<booktitle>In European Workshop on Probabilistic Graphical Models.</booktitle>
<contexts>
<context position="19563" citStr="Kwisthout, 2008" startWordPosition="3423" endWordPosition="3424"> significantly faster on specialization nodes because only one child is applicable and needs to be considered in the maximization (step 7). In the extreme case of a specialization-only POT, the time and space complexities both drop to O(Nmn). A similar algorithm for incrementally finding m-best MPEs in a general BN is given in Srinivas and Nayak (1996). However, our approach has the ability to leverage the null semantics in POTs resulting in significant speedup as described above. This is crucial because the run-time complexity of enumerating MPEs is known to be PPP-Complete for a general BN (Kwisthout, 2008). 5 Empirical Evaluation To test the effectiveness of our POT approach, we compare it to a frame-based baseline system for inferring user intentions. The baseline system uses a hierarchical framebased approach. Each frame maps to a particular user intention, and the frames are filled concurrently from the dialog observations. The slots from a turn overwrite matching slots received in previous turns. The baseline system uses the same ontology as the POT to insure that it only produces consistent hypotheses, e.g., it never produces ”Venue=Service, Cuisine=Japanese” because Service does not have </context>
</contexts>
<marker>Kwisthout, 2008</marker>
<rawString>J. Kwisthout. 2008. Complexity Results for Enumerating MPE and Partial MAP. In European Workshop on Probabilistic Graphical Models.</rawString>
</citation>
<citation valid="true">
<authors>
<author>H Noh</author>
<author>C Lee</author>
<author>G Lee</author>
</authors>
<title>Ontology-based Inference for Information-seeking in Natural Language Dialog Systems.</title>
<date>2003</date>
<booktitle>In IEEE International Conference on Industrial Informatics.</booktitle>
<contexts>
<context position="1777" citStr="Noh et al., 2003" startWordPosition="260" endWordPosition="263">is to estimate the user’s intention based on the utterances. The information gathered across multiple turns needs to be combined and understood in context after automatic speech recognition (ASR). Traditionally, this has been addressed by dialog models and data structures such as forms (Goddeau et al., 1996) and hierarchical task decomposition (Rich and Sidner, 1998). To formalize knowledge representation within the SDS and enable the development of reusable software and resources, researchers have investigated the organization of domain concepts using IS-A/HAS-A ontologies (van Zanten, 1998; Noh et al., 2003). Because the SDS only has access to noisy observations of what the user really uttered due to speech recognition and understanding errors, belief tracking in speech understanding has received Stefan Krawczyk Stanford University stefank@cs.stanford.edu particular attention from proponents of probabilistic approaches to dialog management (Bohus and Rudnicky, 2006; Williams, 2006). The mechanism for belief tracking often employs a Bayesian network (BN) that represents the joint probability space of concepts while leveraging conditional independences among them (Paek and Horvitz, 2000). Designing</context>
<context position="7579" citStr="Noh et al., 2003" startWordPosition="1208" endWordPosition="1211">n nodes represent refinements of their parent concepts. Specializations of a concept are disjoint, that is, for any particular instance of the parent exactly one specialization is applicable and the rest are inapplicable. For example, if Dog IS-A Animal and Cat IS-A Animal, then Cat is inapplicable when Dog is applicable, and vice versa. Composition nodes represent attributes of their parents and may be essential or nonessential, e.g., Dog HAS-A Color (essential), Dog HAS-A Tail (nonessential). These definitions correspond with the standard semantics in the knowledge representation community (Noh et al., 2003). An example ontology is shown in Figure 1. Definition 2. A specialization subtree (spec-tree) in the ontology is a subtree consisting of a node with its specialization children (if any). 3.2 POT Construction We now describe how a POT may be constructed from a domain ontology. The purpose of the POT is to maintain a distribution of possible instantiations of the ontology such that the ontological structure is respected. E I 38 Given an ontology G, the corresponding POT is a tree-structured BN defined as follows: Variables. Let T be a spec-tree in G with root R. Unless R is a (non-root) special</context>
<context position="23722" citStr="Noh et al., 2003" startWordPosition="4131" endWordPosition="4134">tion or clarification questions for example. 6 Discussion The definition and construction of POTs provide a principled and systematic way to construct probabilistic models for an SDS. While any BN can be used to model the space of user intentions, designing an effective network is not an easy task for system designers not well versed in graphical models. In previous belief tracking work, researchers describe their networks with little indication on how they arrived at the specific structure (Paek and Horvitz, 2000; Thomson and Young, 2009). Prior work on ontologies for SDSs (van Zanten, 1998; Noh et al., 2003) as well as the prominence of concept hierarchies in other areas such as objectoriented programming and knowledge engineering make them a natural and intuitive way of representing SDS domains. The development of POTs builds on past research on constructing BNs based on ontological knowledge (Helsper and van der Gaag, 2002; Pfeffer et al., 1999). While most approaches to belief tracking in the dialog systems community make a strict independence assumption between concepts (Bohus and Rudnicky, 2006; Williams, 2006), POTs model the dependencies between concepts connected by specialization and com</context>
</contexts>
<marker>Noh, Lee, Lee, 2003</marker>
<rawString>H. Noh, C. Lee, and G. Lee. 2003. Ontology-based Inference for Information-seeking in Natural Language Dialog Systems. In IEEE International Conference on Industrial Informatics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>T Paek</author>
<author>E Horvitz</author>
</authors>
<title>Conversation as Action under Uncertainty.</title>
<date>2000</date>
<booktitle>In Uncertainty in Artificial Intelligence.</booktitle>
<contexts>
<context position="2366" citStr="Paek and Horvitz, 2000" startWordPosition="344" endWordPosition="347">an Zanten, 1998; Noh et al., 2003). Because the SDS only has access to noisy observations of what the user really uttered due to speech recognition and understanding errors, belief tracking in speech understanding has received Stefan Krawczyk Stanford University stefank@cs.stanford.edu particular attention from proponents of probabilistic approaches to dialog management (Bohus and Rudnicky, 2006; Williams, 2006). The mechanism for belief tracking often employs a Bayesian network (BN) that represents the joint probability space of concepts while leveraging conditional independences among them (Paek and Horvitz, 2000). Designing a domain-specific BN requires significant effort and expert knowledge that is not always readily available. Additionally, real-world systems typically yield large networks on which inference is intractable without major assumptions and approximations. A common workaround to mitigate the intensive computation of the joint distribution over user intentions is to assume full conditional independence between concepts which violates the ground truth in most domains (Bohus and Rudnicky, 2006; Williams, 2006). We propose a novel approach to belief tracking for an SDS that solves both the </context>
<context position="23624" citStr="Paek and Horvitz, 2000" startWordPosition="4114" endWordPosition="4117">redictor of the F1 score. This information could be very useful to a dialog manager to trigger confirmation or clarification questions for example. 6 Discussion The definition and construction of POTs provide a principled and systematic way to construct probabilistic models for an SDS. While any BN can be used to model the space of user intentions, designing an effective network is not an easy task for system designers not well versed in graphical models. In previous belief tracking work, researchers describe their networks with little indication on how they arrived at the specific structure (Paek and Horvitz, 2000; Thomson and Young, 2009). Prior work on ontologies for SDSs (van Zanten, 1998; Noh et al., 2003) as well as the prominence of concept hierarchies in other areas such as objectoriented programming and knowledge engineering make them a natural and intuitive way of representing SDS domains. The development of POTs builds on past research on constructing BNs based on ontological knowledge (Helsper and van der Gaag, 2002; Pfeffer et al., 1999). While most approaches to belief tracking in the dialog systems community make a strict independence assumption between concepts (Bohus and Rudnicky, 2006;</context>
</contexts>
<marker>Paek, Horvitz, 2000</marker>
<rawString>T. Paek and E. Horvitz. 2000. Conversation as Action under Uncertainty. In Uncertainty in Artificial Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Pfeffer</author>
<author>D Koller</author>
<author>B Milch</author>
<author>K T Takusagawa</author>
</authors>
<title>Spook: A system for probabilistic objectoriented knowledge representation.</title>
<date>1999</date>
<booktitle>In Uncertainty in Artifical Intelligence.</booktitle>
<contexts>
<context position="24068" citStr="Pfeffer et al., 1999" startWordPosition="4188" endWordPosition="4191">phical models. In previous belief tracking work, researchers describe their networks with little indication on how they arrived at the specific structure (Paek and Horvitz, 2000; Thomson and Young, 2009). Prior work on ontologies for SDSs (van Zanten, 1998; Noh et al., 2003) as well as the prominence of concept hierarchies in other areas such as objectoriented programming and knowledge engineering make them a natural and intuitive way of representing SDS domains. The development of POTs builds on past research on constructing BNs based on ontological knowledge (Helsper and van der Gaag, 2002; Pfeffer et al., 1999). While most approaches to belief tracking in the dialog systems community make a strict independence assumption between concepts (Bohus and Rudnicky, 2006; Williams, 2006), POTs model the dependencies between concepts connected by specialization and composition relationships while remaining significantly more tractable than general BNs and being very straightforward to design. The null semantics allow a POT to capture disjoint values and the applicability of attributes which are common aspects of concept ontologies. Obviously, a POT cannot capture all types of concept relationships since each</context>
</contexts>
<marker>Pfeffer, Koller, Milch, Takusagawa, 1999</marker>
<rawString>A. Pfeffer, D. Koller, B. Milch, and K. T. Takusagawa. 1999. Spook: A system for probabilistic objectoriented knowledge representation. In Uncertainty in Artifical Intelligence.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C Rich</author>
<author>C Sidner</author>
</authors>
<title>COLLAGEN: a Collaboration Manager for Software Interface Agents. An International Journal: User Modeling and User Adapted</title>
<date>1998</date>
<journal>Interaction,</journal>
<volume>8</volume>
<contexts>
<context position="1529" citStr="Rich and Sidner, 1998" startWordPosition="224" endWordPosition="227">st probable explanations. We empirically validate the efficacy of our POT and compare it to a hierarchical frame-based approach in experiments with users of a tourism information system. 1 Introduction A central function of a spoken dialog system (SDS) is to estimate the user’s intention based on the utterances. The information gathered across multiple turns needs to be combined and understood in context after automatic speech recognition (ASR). Traditionally, this has been addressed by dialog models and data structures such as forms (Goddeau et al., 1996) and hierarchical task decomposition (Rich and Sidner, 1998). To formalize knowledge representation within the SDS and enable the development of reusable software and resources, researchers have investigated the organization of domain concepts using IS-A/HAS-A ontologies (van Zanten, 1998; Noh et al., 2003). Because the SDS only has access to noisy observations of what the user really uttered due to speech recognition and understanding errors, belief tracking in speech understanding has received Stefan Krawczyk Stanford University stefank@cs.stanford.edu particular attention from proponents of probabilistic approaches to dialog management (Bohus and Ru</context>
</contexts>
<marker>Rich, Sidner, 1998</marker>
<rawString>C. Rich and C. Sidner. 1998. COLLAGEN: a Collaboration Manager for Software Interface Agents. An International Journal: User Modeling and User Adapted Interaction, 8.</rawString>
</citation>
<citation valid="true">
<authors>
<author>M Richardson</author>
<author>P Domingos</author>
</authors>
<title>Markov Logic Networks.</title>
<date>2006</date>
<booktitle>Machine Learning,</booktitle>
<pages>62--107</pages>
<contexts>
<context position="24935" citStr="Richardson and Domingos, 2006" startWordPosition="4314" endWordPosition="4318">alization and composition relationships while remaining significantly more tractable than general BNs and being very straightforward to design. The null semantics allow a POT to capture disjoint values and the applicability of attributes which are common aspects of concept ontologies. Obviously, a POT cannot capture all types of concept relationships since each concept can have only one parent. However, this restriction allows us to perform efficient exact computation of the m-best MPEs which is a significant advantage. Statistical Relational Learning approaches such as Markov Logic Networks (Richardson and Domingos, 2006) have been developed for more general relational models than strict ontologies, but they lack the parsimony and efficiency of POTs. 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2−1 −0.8 −0.6 −0.4 −0.2 0 Log−likelihood of top POT hypothesis F1 43 Thomson and Young (2009) describe an approach to dialog management based on a partially observable Markov decision process (POMDP) whose policy depends only on individual concepts’ marginal distributions rather than on the overall user intention. Because their system performs belief tracking with a dynamic Bayesian network (DBN) rather than a static BN, the exact m</context>
</contexts>
<marker>Richardson, Domingos, 2006</marker>
<rawString>M. Richardson and P. Domingos. 2006. Markov Logic Networks. Machine Learning, 62:107–136.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Srinivas</author>
<author>P Nayak</author>
</authors>
<title>Efficient Enumeration of Instantiations in Bayesian Networks.</title>
<date>1996</date>
<booktitle>In UAI.</booktitle>
<contexts>
<context position="19301" citStr="Srinivas and Nayak (1996)" startWordPosition="3379" endWordPosition="3382"> k nm-sized lists. Thus, the time complexity of Algorithm 1 on a POT with N nodes is O(N(nm)k) and the space complexity is O(Nnmk). (Insertion sort maintains a sorted list truncated at m elements to keep track of the top m elements at any time.) However, the algorithm is significantly faster on specialization nodes because only one child is applicable and needs to be considered in the maximization (step 7). In the extreme case of a specialization-only POT, the time and space complexities both drop to O(Nmn). A similar algorithm for incrementally finding m-best MPEs in a general BN is given in Srinivas and Nayak (1996). However, our approach has the ability to leverage the null semantics in POTs resulting in significant speedup as described above. This is crucial because the run-time complexity of enumerating MPEs is known to be PPP-Complete for a general BN (Kwisthout, 2008). 5 Empirical Evaluation To test the effectiveness of our POT approach, we compare it to a frame-based baseline system for inferring user intentions. The baseline system uses a hierarchical framebased approach. Each frame maps to a particular user intention, and the frames are filled concurrently from the dialog observations. The slots </context>
</contexts>
<marker>Srinivas, Nayak, 1996</marker>
<rawString>S. Srinivas and P. Nayak. 1996. Efficient Enumeration of Instantiations in Bayesian Networks. In UAI.</rawString>
</citation>
<citation valid="true">
<authors>
<author>B Thomson</author>
<author>S Young</author>
</authors>
<title>Bayesian Update of Dialogue State: A POMDP Framework for Spoken Dialogue Systems. Computer Speech and Language.</title>
<date>2009</date>
<contexts>
<context position="23650" citStr="Thomson and Young, 2009" startWordPosition="4118" endWordPosition="4121">. This information could be very useful to a dialog manager to trigger confirmation or clarification questions for example. 6 Discussion The definition and construction of POTs provide a principled and systematic way to construct probabilistic models for an SDS. While any BN can be used to model the space of user intentions, designing an effective network is not an easy task for system designers not well versed in graphical models. In previous belief tracking work, researchers describe their networks with little indication on how they arrived at the specific structure (Paek and Horvitz, 2000; Thomson and Young, 2009). Prior work on ontologies for SDSs (van Zanten, 1998; Noh et al., 2003) as well as the prominence of concept hierarchies in other areas such as objectoriented programming and knowledge engineering make them a natural and intuitive way of representing SDS domains. The development of POTs builds on past research on constructing BNs based on ontological knowledge (Helsper and van der Gaag, 2002; Pfeffer et al., 1999). While most approaches to belief tracking in the dialog systems community make a strict independence assumption between concepts (Bohus and Rudnicky, 2006; Williams, 2006), POTs mod</context>
<context position="25192" citStr="Thomson and Young (2009)" startWordPosition="4360" endWordPosition="4363">of concept ontologies. Obviously, a POT cannot capture all types of concept relationships since each concept can have only one parent. However, this restriction allows us to perform efficient exact computation of the m-best MPEs which is a significant advantage. Statistical Relational Learning approaches such as Markov Logic Networks (Richardson and Domingos, 2006) have been developed for more general relational models than strict ontologies, but they lack the parsimony and efficiency of POTs. 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2−1 −0.8 −0.6 −0.4 −0.2 0 Log−likelihood of top POT hypothesis F1 43 Thomson and Young (2009) describe an approach to dialog management based on a partially observable Markov decision process (POMDP) whose policy depends only on individual concepts’ marginal distributions rather than on the overall user intention. Because their system performs belief tracking with a dynamic Bayesian network (DBN) rather than a static BN, the exact marginal computation is intractable and the authors use loopy belief propagation to compute the marginals. Even then, they indicate that the dependencies of the subgoals must be limited to enable tractability. In practice, all concepts are made independent e</context>
</contexts>
<marker>Thomson, Young, 2009</marker>
<rawString>B. Thomson and S. Young. 2009. Bayesian Update of Dialogue State: A POMDP Framework for Spoken Dialogue Systems. Computer Speech and Language.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G van Zanten</author>
</authors>
<title>Adaptive Mixed-Initiative Dialogue Management.</title>
<date>1998</date>
<booktitle>In IEEE Workshop on Interactive Voice Technology for Telecommunications Applications.</booktitle>
<marker>van Zanten, 1998</marker>
<rawString>G. van Zanten. 1998. Adaptive Mixed-Initiative Dialogue Management. In IEEE Workshop on Interactive Voice Technology for Telecommunications Applications.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
</authors>
<title>Partially Observable Markov Decision Processes for Dialog Management.</title>
<date>2006</date>
<contexts>
<context position="2158" citStr="Williams, 2006" startWordPosition="315" endWordPosition="316"> knowledge representation within the SDS and enable the development of reusable software and resources, researchers have investigated the organization of domain concepts using IS-A/HAS-A ontologies (van Zanten, 1998; Noh et al., 2003). Because the SDS only has access to noisy observations of what the user really uttered due to speech recognition and understanding errors, belief tracking in speech understanding has received Stefan Krawczyk Stanford University stefank@cs.stanford.edu particular attention from proponents of probabilistic approaches to dialog management (Bohus and Rudnicky, 2006; Williams, 2006). The mechanism for belief tracking often employs a Bayesian network (BN) that represents the joint probability space of concepts while leveraging conditional independences among them (Paek and Horvitz, 2000). Designing a domain-specific BN requires significant effort and expert knowledge that is not always readily available. Additionally, real-world systems typically yield large networks on which inference is intractable without major assumptions and approximations. A common workaround to mitigate the intensive computation of the joint distribution over user intentions is to assume full condi</context>
<context position="24240" citStr="Williams, 2006" startWordPosition="4215" endWordPosition="4216"> Thomson and Young, 2009). Prior work on ontologies for SDSs (van Zanten, 1998; Noh et al., 2003) as well as the prominence of concept hierarchies in other areas such as objectoriented programming and knowledge engineering make them a natural and intuitive way of representing SDS domains. The development of POTs builds on past research on constructing BNs based on ontological knowledge (Helsper and van der Gaag, 2002; Pfeffer et al., 1999). While most approaches to belief tracking in the dialog systems community make a strict independence assumption between concepts (Bohus and Rudnicky, 2006; Williams, 2006), POTs model the dependencies between concepts connected by specialization and composition relationships while remaining significantly more tractable than general BNs and being very straightforward to design. The null semantics allow a POT to capture disjoint values and the applicability of attributes which are common aspects of concept ontologies. Obviously, a POT cannot capture all types of concept relationships since each concept can have only one parent. However, this restriction allows us to perform efficient exact computation of the m-best MPEs which is a significant advantage. Statistic</context>
</contexts>
<marker>Williams, 2006</marker>
<rawString>J. Williams. 2006. Partially Observable Markov Decision Processes for Dialog Management.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Williams</author>
</authors>
<title>Using Particle Filters to Track Dialogue State.</title>
<date>2007</date>
<booktitle>In IEEE Workshop on Automatic Speech Recognition &amp; Understanding.</booktitle>
<contexts>
<context position="25944" citStr="Williams (2007)" startWordPosition="4480" endWordPosition="4481">ndividual concepts’ marginal distributions rather than on the overall user intention. Because their system performs belief tracking with a dynamic Bayesian network (DBN) rather than a static BN, the exact marginal computation is intractable and the authors use loopy belief propagation to compute the marginals. Even then, they indicate that the dependencies of the subgoals must be limited to enable tractability. In practice, all concepts are made independent except for the binary validity nodes that deterministically govern the dependence between nodes (similar to the null semantics of a POT). Williams (2007) also represents the user goal as a DBN for a POMDP-based DM. They perform belief updating using particle filtering and approximate the joint probability over the user intention with the product of the concept marginals. This could lead to inaccurate estimation for conditionally dependent concepts. Among authors who have used m-best lists of dialog states for dialog management, Higashinaka et al. (2003) have shown empirically that maintaining multiple state hypotheses facilitates shorter dialogs. Their system scores each dialog state using a linear combination of linguistic and discourse featu</context>
</contexts>
<marker>Williams, 2007</marker>
<rawString>J. Williams. 2007. Using Particle Filters to Track Dialogue State. In IEEE Workshop on Automatic Speech Recognition &amp; Understanding.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>