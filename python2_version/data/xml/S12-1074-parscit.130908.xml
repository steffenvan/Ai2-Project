<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.014599">
<title confidence="0.887144">
NJU-Parser: Achievements on Semantic Dependency Parsing
</title>
<author confidence="0.773836">
Guangchao Tang1 Bin Li1,2 Shuaishuai Xu1 Xinyu Dai1 Jiajun Chen1
</author>
<affiliation confidence="0.532672">
1 State Key Lab for Novel Software Technology, Nanjing University
2 Research Center of Language and Informatics, Nanjing Normal University
Nanjing, Jiangsu, China
</affiliation>
<email confidence="0.930264">
{tanggc, lib, xuss, dxy, chenjj}@nlp.nju.edu.cn
</email>
<sectionHeader confidence="0.997152" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.9995955">
In this paper, we introduce our work on
SemEval-2012 task 5: Chinese Semantic De-
pendency Parsing. Our system is based on
MSTParser and two effective methods are
proposed: splitting sentence by punctuations
and extracting last character of word as lemma.
The experiments show that, with a combina-
tion of the two proposed methods, our system
can improve LAS about one percent and final-
ly get the second prize out of nine participat-
ing systems. We also try to handle the multi-
level labels, but with no improvement.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999959619047619">
Task 5 of SemEval-2012 tries to find approaches to
improve Chinese sematic dependency parsing
(SDP). SDP is a kind of dependency parsing. Cur-
rently, there are many dependency parsers availa-
ble, such as Eisner’s probabilistic dependency
parser (Eisner, 1996), McDonald’s MSTParser
(McDonald et al. 2005a; McDonald et al. 2005b)
and Nivre’s MaltParser (Nivre, 2006).
Despite of elaborate models, lots of problems
still exist in dependency parsing. For example, sen-
tence length has been proved to show great impact
on the parsing performance. (Li et al., 2010) used a
two-stage approach based on sentence fragment for
high-order graph-based dependency parsing. Lack-
ing of linguistic knowledge is also blamed.
Three methods are promoted in this paper try-
ing to improve the performance: splitting sentence
by commas and semicolons, extracting last charac-
ter of word as lemma and handling multi-level la-
bels. Improvements could be achieved through the
first two methods while not for the third.
</bodyText>
<sectionHeader confidence="0.939013" genericHeader="method">
2 Overview of Our System
</sectionHeader>
<bodyText confidence="0.9954059">
Our system is based on MSTParser which is one of
the state-of-the-art parsers. MSTParser tries to ob-
tain the maximum spanning tree of a sentence. For
projective parsing task, it takes Eisner’s algorithm
(Eisner, 1996) to get the dependency tree in O(n3)
time. Meanwhile, Chu-Liu-Edmond’s algorithm
(Chu and Liu, 1965) is applied for non-projective
task, which takes O(n2) time.
Three methods are adopted to MSTParser in our
system:
</bodyText>
<listItem confidence="0.92555188">
1) Sentences are split into sub-sentences by
commas and semicolons, for which there
are two ways. Splitting sentences by all
commas and semicolons is used in our
primary system. In our contrast system, we
use a classifier to determine whether a
comma or semicolon can be used to split
the sentence. In the primary and contrast
system, the proto sentences and the sub-
sentences are trained and tested separately
and the outputs are merged in the end.
2) In a Chinese word, the last character usual-
ly contains main sense or semantic class.
We treat the last character of the word as
word lemma and find it gets a slightly im-
provement in the experiment.
3) An experiment trying to solve the problem
of multi-level labels was conducted by
parsing different levels separately and con-
sequently merging the outputs together.
The experiment results have shown that the first
two methods could enhance the system perfor-
mance while further improvements could be ob-
tained through a combination of them in our sub-
submitted systems.
</listItem>
<page confidence="0.973216">
519
</page>
<note confidence="0.6638475">
First Joint Conference on Lexical and Computational Semantics (*SEM), pages 519–523,
Montr´eal, Canada, June 7-8, 2012. c�2012 Association for Computational Linguistics
</note>
<figure confidence="0.984436">
a) The proto sentence from train data
b) The first sub sentence of a) c) The second sub sentence of a)
</figure>
<figureCaption confidence="0.999335">
Figure 1. An example of the split procedure.
</figureCaption>
<sectionHeader confidence="0.999728" genericHeader="method">
3 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999476">
3.1 Split sentences by commas and semicolons
</subsectionHeader>
<bodyText confidence="0.999874285714286">
It is observed that the performance decreases as
the length of the sentences increases. Table 1
shows the statistical analysis on the data including
SemEval-2012, Conll-07’s Chinese corpus and a
subset extracted from CTB using Penn2Malt. Long
sentence can be split into sub-sentences to get bet-
ter parsing result.
</bodyText>
<table confidence="0.999397090909091">
Items SemEval Conll- CTB
-2012 07 CN
Postages count 35 13 33
Dependency 122 69 12
labels count
Average sentence 30.15 5.92 25.89
length
Average 4.80 1.71 4.36
dependency length
LAS 61.37 82.89 67.35
UAS 80.18 87.64 79.90
</table>
<tableCaption confidence="0.99924">
Table 1. Statistical analysis on the data. The CTB data is
a subset extracted from CTB using Penn2Malt.
</tableCaption>
<bodyText confidence="0.999721">
Our work can be described as following steps:
Step 1: Use MSTParser to parse the data. We
name the result as “normal output”.
Step 2: Split train and test data by all commas
and semicolons. The delimiters are removed in the
sub sentences. For train data, a word’s dependency
relation is kept if the word’s head is under the cov-
er of the sub sentence. Otherwise, its head will be
set to root and its label will be set to ROOT (ROOT
is the default label of dependency arcs whose head
is root). We define the word as “sentence head” if
its head is root. “Sub-sentence head” indicates the
sentence head of a sub-sentence. After splitting,
there may be more than one sub-sentence heads in
a sub-sentence. Figure 1 shows an example of the
split procedure.
Step 3: Use MSTParser to parse the data gener-
ated in step 2. We name the parsing result “split
output”. In split output, there may be more than
one sub-sentences corresponding to a single sen-
tence in normal output.
Step 4: Merge the split output and the normal
output. The outputs of sub-sentences are merged
with delimiters restored. Dependency relations are
recovered for all punctuations and sub-sentence
heads in split output with relations in normal out-
put. The sentence head of normal output is kept in
final output. The result is called “merged split out-
put”. This step need to be consummated because it
may result in a dependency tree not well formed
with several sentence heads or even circles.
The results of experiments on develop data and
test data are showed in table 2. For develop data,
an improvement of 0.85 could be obtained while
0.93 for test data, both on LAS.
In step 2, there is an alternative to split the sen-
tences, i.e., using a classifier to determine which
comma and semicolon can be split. This method is
taken in the contrast system. When applying the
classifier, all commas and semicolons in train data
</bodyText>
<page confidence="0.974302">
520
</page>
<bodyText confidence="0.951274818181818">
are labeled with S-IN or S-STOP while other
words with NULL. If the sub sentence before the
comma or semicolon has only one sub-sentence
head, it is labeled with S-STOP, otherwise with S-
IN. A model is built from train data with CRF++
and test data is evaluated with it. Features used are
listed in table 3. Only commas and semicolons
with label S-STOP can be used to split the sen-
tence in step 2. Other steps are the same as above.
The result is also shown in table 2 as “merged split
output with CRF++”.
</bodyText>
<table confidence="0.999888705882353">
Data Methods LAS UAS
Develop normal output 61.37 80.18
data
merged split output 62.22 80.56
merged split output 61.97 80.73
with CRF++
lemma output 61.64 80.47
primary system output 62.41 80.96
contrast system output 62.05 80.90
Test normal output 60.63 79.37
data
merged split output 61.56 80.17
merged split output 61.42 80.20
with CRF++
lemma output 60.88 79.42
primary system output 61.63 80.35
contrast system output 61.64 80.29
</table>
<tableCaption confidence="0.998249">
Table 2. Results of the experiments.
</tableCaption>
<table confidence="0.990190375">
w-4,w-3,w-2,w-1,w,w+1,w+2,w+3,w+4
p-4,p-3,p-2,p-1,p,p+1,p+2,p+3,p+4
wp-4,wp-3,wp-2,wp-1,wp wp+1,wp+2,wp+3,wp+4
w-4|w-3,w-3|w-2,w-2|w-1,w-1|w,
w|w+1,w+1|w+2,w+2|w+3,w+3|w+4
p-4|p-3,p-3|p-2,p-2|p-1,p-1|p,
p|p+1,p+1|p+2,p+2|p+3,p+3|p+4
first word of sub-sentence before the delimiter
</table>
<tableCaption confidence="0.995342666666667">
Table 3. Features used in CRF++. w represents for word
and p for PosTag. +1 means the index after current
while -1 means before.
</tableCaption>
<subsectionHeader confidence="0.998269">
3.2 Extract last character of word as lemma
</subsectionHeader>
<bodyText confidence="0.9725458">
In Chinese, the last character of a word usually
contains main sense or semantic class, which indi-
cates that it may represent the whole word. For
example, “ 国 ”(country) can represent “ 中
国 ”(China) and “ 恋 ”(love) can represent “ 热
恋”(crazy love).
The last character is used as lemma in the ex-
periment, with an improvement of 0.27 for LAS on
develop data and 0.24 on test data. Details of the
scores are listed in table 2 as “lemma output”.
</bodyText>
<subsectionHeader confidence="0.997691">
3.3 Multi-level labels experiment
</subsectionHeader>
<bodyText confidence="0.996394853658537">
A notable characteristic of SemEval-2012’s da-
ta is multi-level labels. It introduces four kinds of
multi-level labels which are s-X, d-X, j-X and r-X.
The first level represents the basic semantic rela-
tion of the dependency while the second level
shows the second import, except that s-X repre-
sents sub-sentence relation.
The r-X label means that a verb modifies a
noun and the relation between them is reverse. For
example, in phrase “贫户(poor) 出身(born) 的
星(star)”, “出身” is headed to “明星” with label r-
agent. It means that “明星” is the agent of “出身” .
When a verbal noun is the head word and its
child has indirect relation to it, the dependency is
labeled with j-X. In phrase “学校(school) 建设
(construction)”, “建设” is the head of “学校” with
label j-content. “学校” is the content of “建设” .
The d-X label means that the child modifies the
head with an additional relation. For example, in
phrase “科技(technology) 企业(enterprise)”, “科
技” modifies “企业” and the domain of “企业” is
“科技” .
A heuristic method is tried in the experiment.
The multi-level labels of d-X, j-X and r-X are sep-
arated into two parts for each level. For example,
“d-content” will be separated to “d” and “content”.
For each part, MSTParser is used to train and test.
We call the outputs “first-level output” and “se-
cond-level output”. The outputs of each level and
normal output are merged then.
In our experiments, only the word satisfies the
following conditions need to be merged:
a) The dependency label in normal output is
started with d-, j- or r-.
b) The dependency label in first-level output is
d, j or r.
c) The heads in first-level output and second-
level output are of the same.
Otherwise, the dependency relation in normal
output will be kept. There are also three ways in
merging outputs:
</bodyText>
<listItem confidence="0.988419833333333">
a) Label in first-level output and label in se-
cond-level output are merged.
b) First level label in normal output and label
in second-level output are merged.
c) Label in first-level output and second level
label in normal output are merged.
</listItem>
<page confidence="0.99636">
521
</page>
<bodyText confidence="0.997911666666667">
Experiment has been done on develop data. In
the experiment, 24% of the labels are merged and
92% of the new merged labels are the same as
original. The results of three ways are listed in ta-
ble 4. All of them get decline compared to normal
output.
</bodyText>
<table confidence="0.9987856">
outputs LAS UAS
normal output 61.37 80.18
way a) 61.18 80.18
way b) 61.25 80.18
way c) 61.25 80.18
</table>
<tableCaption confidence="0.9970375">
Table 4. Results of multi-level labels experiment on
develop data.
</tableCaption>
<subsectionHeader confidence="0.938218">
3.4 Combined experiment on split and lemma
</subsectionHeader>
<bodyText confidence="0.999921833333333">
Improvements are achieved by first two meth-
ods in the experiment while a further enhancement
is made with a combination of them in the submit-
ted systems. The split method and lemma method
are combined as primary system. The split method
with CRF++ and lemma method are combined as
contrast system. When combining the two methods,
last character of the word is firstly extracted as
lemma for train data and test data. Then the split or
split with CRF++ method is used.
The outputs of the primary system and contrast
system are listed in table 2.
</bodyText>
<sectionHeader confidence="0.990534" genericHeader="method">
4 Analysis and Discussion
</sectionHeader>
<bodyText confidence="0.9864859375">
The contrast system presented in this paper finally
got the second prize among nine systems. The pri-
mary system gets the third. There is an improve-
ment of about one percent for both primary and
contrast system. The following conclusions can be
made from the experiments:
1) Parsing is more effective and accurate on
short sentences. A word prefers to depend
on another near to it. A sentence can be
split to several sub sentences by commas
and semicolons to get better parsing output.
Result may be improved with a classifier to
determine whether a comma or semicolon
can be used to split the sentence.
2) Last character of word is a useful feature.
In the experiment, the last character is
coarsely used as lemma and a minor im-
provement is achieved. Much more lan-
guage knowledge can be used in parsing.
3) The label set of the data is worthy to be re-
viewed. The meanings of the labels are not
given in the task. Some of them are confus-
ing especially the multi-level labels. The
trying of training and testing multi-level la-
bels separately by levels fails with a slight-
ly decline of the score. Multi-level also
causes too many labels: any single-level la-
bel can be prefixed to form a new multi-
level label. It’s a great problem for current
parsers. Whether the label set is suitable to
Chinese semantic dependency parsing
should be discussed.
</bodyText>
<sectionHeader confidence="0.998297" genericHeader="conclusions">
5 Conclusion and Future Work
</sectionHeader>
<bodyText confidence="0.999947842105263">
Three methods applied in NJU-Parser are de-
scribed in this paper: splitting sentences by com-
mas and semicolons, taking last character of word
as lemma and handling multi-level labels. The first
two get improvements in the experiments. Our
primary system is a combination of the first two
methods. The contrast system is the same as prima-
ry system except that it has a classifier implement-
ed in CRF++ to determine whether a comma or a
semicolon should be used to split the sentence.
Both of the systems get improvements for about
one percent on LAS.
In the future, a better classifier should be devel-
oped to split the sentence. New method should be
applied in merging split outputs to get a well
formed dependency tree. And we hope there will
be a better label set which are more capable of de-
scribing semantic dependency relations for Chi-
nese.
</bodyText>
<sectionHeader confidence="0.998493" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.9134734">
This paper is supported in part by National Natural
Science Fund of China under contract 61170181,
Natural Science Fund of Jiangsu under contract
BK2011192, and National Social Science Fund of
China under contract 10CYY021.
</bodyText>
<sectionHeader confidence="0.994975" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.988457833333333">
Y.J. Chu and T.H. Liu. 1965. On the shortest arbores-
cence of a directed graph. Science Sinica, 14:1396–
1400.
MSTParser:
http://www.seas.upenn.edu/~strctlrn/MSTParser/MS
TParser.html
</reference>
<page confidence="0.971538">
522
</page>
<reference confidence="0.999417857142857">
J. Eisner. 1996. Three new probabilistic models for de-
pendency parsing: An exploration. In Proc. COLING.
J. Nivre. 2006. Inductive Dependency Parsing. Springer.
R. McDonald, K. Crammer, and F. Pereira. 2005.
Online Large-Margin Training of Dependency
Parsers. 43rd Annual Meeting of the Association for
Computational Linguistics (ACL 2005).
R. McDonald, F. Pereira, K. Ribarov, and J. Hajič. 2005.
Non-projective Dependency Parsing using Spanning
Tree Algorithms. Proceedings of HLT/EMNLP 2005.
Zhenghua Li, Wanxiang Che, Ting Liu. 2010. Improv-
ing Dependency Parsing Using Punctuation. Interna-
tional Conference on Asian Language
Processing(IALP) 2010.
</reference>
<page confidence="0.998918">
523
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.874441">
<title confidence="0.999692">NJU-Parser: Achievements on Semantic Dependency Parsing</title>
<author confidence="0.957915">Bin Shuaishuai Xinyu Jiajun</author>
<affiliation confidence="0.998291">Key Lab for Novel Software Technology, Nanjing University Center of Language and Informatics, Nanjing Normal University</affiliation>
<address confidence="0.999537">Nanjing, Jiangsu, China</address>
<email confidence="0.990575">tanggc@nlp.nju.edu.cn</email>
<email confidence="0.990575">lib@nlp.nju.edu.cn</email>
<email confidence="0.990575">xuss@nlp.nju.edu.cn</email>
<email confidence="0.990575">dxy@nlp.nju.edu.cn</email>
<email confidence="0.990575">chenjj@nlp.nju.edu.cn</email>
<abstract confidence="0.994092615384615">In this paper, we introduce our work on SemEval-2012 task 5: Chinese Semantic Dependency Parsing. Our system is based on MSTParser and two effective methods are proposed: splitting sentence by punctuations and extracting last character of word as lemma. The experiments show that, with a combination of the two proposed methods, our system can improve LAS about one percent and finally get the second prize out of nine participating systems. We also try to handle the multilevel labels, but with no improvement.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Y J Chu</author>
<author>T H Liu</author>
</authors>
<title>On the shortest arborescence of a directed graph.</title>
<date>1965</date>
<journal>Science Sinica,</journal>
<volume>14</volume>
<pages>1400</pages>
<contexts>
<context position="2199" citStr="Chu and Liu, 1965" startWordPosition="338" endWordPosition="341">ethods are promoted in this paper trying to improve the performance: splitting sentence by commas and semicolons, extracting last character of word as lemma and handling multi-level labels. Improvements could be achieved through the first two methods while not for the third. 2 Overview of Our System Our system is based on MSTParser which is one of the state-of-the-art parsers. MSTParser tries to obtain the maximum spanning tree of a sentence. For projective parsing task, it takes Eisner’s algorithm (Eisner, 1996) to get the dependency tree in O(n3) time. Meanwhile, Chu-Liu-Edmond’s algorithm (Chu and Liu, 1965) is applied for non-projective task, which takes O(n2) time. Three methods are adopted to MSTParser in our system: 1) Sentences are split into sub-sentences by commas and semicolons, for which there are two ways. Splitting sentences by all commas and semicolons is used in our primary system. In our contrast system, we use a classifier to determine whether a comma or semicolon can be used to split the sentence. In the primary and contrast system, the proto sentences and the subsentences are trained and tested separately and the outputs are merged in the end. 2) In a Chinese word, the last chara</context>
</contexts>
<marker>Chu, Liu, 1965</marker>
<rawString>Y.J. Chu and T.H. Liu. 1965. On the shortest arborescence of a directed graph. Science Sinica, 14:1396– 1400. TParser.html</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Eisner</author>
</authors>
<title>Three new probabilistic models for dependency parsing: An exploration.</title>
<date>1996</date>
<booktitle>In Proc. COLING.</booktitle>
<publisher>Springer.</publisher>
<contexts>
<context position="1125" citStr="Eisner, 1996" startWordPosition="171" endWordPosition="172">splitting sentence by punctuations and extracting last character of word as lemma. The experiments show that, with a combination of the two proposed methods, our system can improve LAS about one percent and finally get the second prize out of nine participating systems. We also try to handle the multilevel labels, but with no improvement. 1 Introduction Task 5 of SemEval-2012 tries to find approaches to improve Chinese sematic dependency parsing (SDP). SDP is a kind of dependency parsing. Currently, there are many dependency parsers available, such as Eisner’s probabilistic dependency parser (Eisner, 1996), McDonald’s MSTParser (McDonald et al. 2005a; McDonald et al. 2005b) and Nivre’s MaltParser (Nivre, 2006). Despite of elaborate models, lots of problems still exist in dependency parsing. For example, sentence length has been proved to show great impact on the parsing performance. (Li et al., 2010) used a two-stage approach based on sentence fragment for high-order graph-based dependency parsing. Lacking of linguistic knowledge is also blamed. Three methods are promoted in this paper trying to improve the performance: splitting sentence by commas and semicolons, extracting last character of w</context>
</contexts>
<marker>Eisner, 1996</marker>
<rawString>J. Eisner. 1996. Three new probabilistic models for dependency parsing: An exploration. In Proc. COLING. J. Nivre. 2006. Inductive Dependency Parsing. Springer.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>K Crammer</author>
<author>F Pereira</author>
</authors>
<title>Online Large-Margin Training of Dependency Parsers.</title>
<date>2005</date>
<booktitle>43rd Annual Meeting of the Association for Computational Linguistics (ACL</booktitle>
<contexts>
<context position="1169" citStr="McDonald et al. 2005" startWordPosition="175" endWordPosition="178">d extracting last character of word as lemma. The experiments show that, with a combination of the two proposed methods, our system can improve LAS about one percent and finally get the second prize out of nine participating systems. We also try to handle the multilevel labels, but with no improvement. 1 Introduction Task 5 of SemEval-2012 tries to find approaches to improve Chinese sematic dependency parsing (SDP). SDP is a kind of dependency parsing. Currently, there are many dependency parsers available, such as Eisner’s probabilistic dependency parser (Eisner, 1996), McDonald’s MSTParser (McDonald et al. 2005a; McDonald et al. 2005b) and Nivre’s MaltParser (Nivre, 2006). Despite of elaborate models, lots of problems still exist in dependency parsing. For example, sentence length has been proved to show great impact on the parsing performance. (Li et al., 2010) used a two-stage approach based on sentence fragment for high-order graph-based dependency parsing. Lacking of linguistic knowledge is also blamed. Three methods are promoted in this paper trying to improve the performance: splitting sentence by commas and semicolons, extracting last character of word as lemma and handling multi-level labels</context>
</contexts>
<marker>McDonald, Crammer, Pereira, 2005</marker>
<rawString>R. McDonald, K. Crammer, and F. Pereira. 2005. Online Large-Margin Training of Dependency Parsers. 43rd Annual Meeting of the Association for Computational Linguistics (ACL 2005).</rawString>
</citation>
<citation valid="true">
<authors>
<author>R McDonald</author>
<author>F Pereira</author>
<author>K Ribarov</author>
<author>J Hajič</author>
</authors>
<title>Non-projective Dependency Parsing using Spanning Tree Algorithms.</title>
<date>2005</date>
<booktitle>Proceedings of HLT/EMNLP</booktitle>
<contexts>
<context position="1169" citStr="McDonald et al. 2005" startWordPosition="175" endWordPosition="178">d extracting last character of word as lemma. The experiments show that, with a combination of the two proposed methods, our system can improve LAS about one percent and finally get the second prize out of nine participating systems. We also try to handle the multilevel labels, but with no improvement. 1 Introduction Task 5 of SemEval-2012 tries to find approaches to improve Chinese sematic dependency parsing (SDP). SDP is a kind of dependency parsing. Currently, there are many dependency parsers available, such as Eisner’s probabilistic dependency parser (Eisner, 1996), McDonald’s MSTParser (McDonald et al. 2005a; McDonald et al. 2005b) and Nivre’s MaltParser (Nivre, 2006). Despite of elaborate models, lots of problems still exist in dependency parsing. For example, sentence length has been proved to show great impact on the parsing performance. (Li et al., 2010) used a two-stage approach based on sentence fragment for high-order graph-based dependency parsing. Lacking of linguistic knowledge is also blamed. Three methods are promoted in this paper trying to improve the performance: splitting sentence by commas and semicolons, extracting last character of word as lemma and handling multi-level labels</context>
</contexts>
<marker>McDonald, Pereira, Ribarov, Hajič, 2005</marker>
<rawString>R. McDonald, F. Pereira, K. Ribarov, and J. Hajič. 2005. Non-projective Dependency Parsing using Spanning Tree Algorithms. Proceedings of HLT/EMNLP 2005.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Zhenghua Li</author>
<author>Wanxiang Che</author>
<author>Ting Liu</author>
</authors>
<title>Improving Dependency Parsing Using Punctuation.</title>
<date>2010</date>
<booktitle>International Conference on Asian Language Processing(IALP)</booktitle>
<contexts>
<context position="1425" citStr="Li et al., 2010" startWordPosition="216" endWordPosition="219">multilevel labels, but with no improvement. 1 Introduction Task 5 of SemEval-2012 tries to find approaches to improve Chinese sematic dependency parsing (SDP). SDP is a kind of dependency parsing. Currently, there are many dependency parsers available, such as Eisner’s probabilistic dependency parser (Eisner, 1996), McDonald’s MSTParser (McDonald et al. 2005a; McDonald et al. 2005b) and Nivre’s MaltParser (Nivre, 2006). Despite of elaborate models, lots of problems still exist in dependency parsing. For example, sentence length has been proved to show great impact on the parsing performance. (Li et al., 2010) used a two-stage approach based on sentence fragment for high-order graph-based dependency parsing. Lacking of linguistic knowledge is also blamed. Three methods are promoted in this paper trying to improve the performance: splitting sentence by commas and semicolons, extracting last character of word as lemma and handling multi-level labels. Improvements could be achieved through the first two methods while not for the third. 2 Overview of Our System Our system is based on MSTParser which is one of the state-of-the-art parsers. MSTParser tries to obtain the maximum spanning tree of a sentenc</context>
</contexts>
<marker>Li, Che, Liu, 2010</marker>
<rawString>Zhenghua Li, Wanxiang Che, Ting Liu. 2010. Improving Dependency Parsing Using Punctuation. International Conference on Asian Language Processing(IALP) 2010.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>