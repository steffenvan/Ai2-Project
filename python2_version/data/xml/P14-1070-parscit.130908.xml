<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.99407">
Strategies for Contiguous Multiword Expression Analysis and
Dependency Parsing
</title>
<author confidence="0.748698">
Marie Candito
</author>
<affiliation confidence="0.454981">
Alpage
</affiliation>
<author confidence="0.333764">
Paris Diderot Univ
</author>
<bodyText confidence="0.362265333333333">
INRIA
marie.candito@
linguist.univ-paris-diderot.fr
</bodyText>
<sectionHeader confidence="0.970539" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999877388888889">
In this paper, we investigate various strate-
gies to predict both syntactic dependency
parsing and contiguous multiword expres-
sion (MWE) recognition, testing them on
the dependency version of French Tree-
bank (Abeill´e and Barrier, 2004), as in-
stantiated in the SPMRL Shared Task
(Seddah et al., 2013). Our work focuses
on using an alternative representation of
syntactically regular MWEs, which cap-
tures their syntactic internal structure. We
obtain a system with comparable perfor-
mance to that of previous works on this
dataset, but which predicts both syntactic
dependencies and the internal structure of
MWEs. This can be useful for capturing
the various degrees of semantic composi-
tionality of MWEs.
</bodyText>
<sectionHeader confidence="0.998989" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.9999196">
A real-life parsing system should comprise the
recognition of multi-word expressions (MWEs1),
first because downstream semantic-oriented ap-
plications need some marking in order to dis-
tinguish between regular semantic composition
and the typical semantic non-compositionality of
MWEs. Second, MWE information, is intuitively
supposed to help parsing.
That intuition is confirmed in a classical but
non-realistic setting in which gold MWEs are pre-
grouped (Arun and Keller, 2005; Nivre and Nils-
son, 2004; Eryiˇgit et al., 2011). But the situation
is much less clear when switching to automatic
MWE prediction. While Cafferkey et al. (2007)
report a small improvement on the pure parsing
</bodyText>
<footnote confidence="0.999663">
1Multiword expressions can be roughly defined as con-
tinuous or discontinuous sets of tokens, which either do not
exhibit full freedom in lexical selection or whose meaning is
not fully compositional. We focus in this paper on contiguous
multiword expressions, also known as “words with spaces”.
</footnote>
<note confidence="0.7224798">
Matthieu Constant
Universit´e Paris-Est
LIGM
CNRS
Matthieu.Constant@
</note>
<bodyText confidence="0.984816702702703">
u-pem.fr
task when using external MWE lexicons to help
English parsing, Constant et al. (2012) report re-
sults on the joint MWE recognition and parsing
task, in which errors in MWE recognition allevi-
ate their positive effect on parsing performance.
While the realistic scenario of syntactic pars-
ing with automatic MWE recognition (either done
jointly or in a pipeline) has already been investi-
gated in constituency parsing (Green et al., 2011;
Constant et al., 2012; Green et al., 2013), the
French dataset of the SPMRL 2013 Shared Task
(Seddah et al., 2013) only recently provided the
opportunity to evaluate this scenario within the
framework of dependency syntax.2 In such a sce-
nario, a system predicts dependency trees with
marked groupings of tokens into MWEs. The
trees show syntactic dependencies between se-
mantically sound units (made of one or several
tokens), and are thus particularly appealing for
downstream semantic-oriented applications, as de-
pendency trees are considered to be closer to
predicate-argument structures.
In this paper, we investigate various strate-
gies for predicting from a tokenized sentence
both MWEs and syntactic dependencies, using the
French dataset of the SPMRL 13 Shared Task. We
focus on the use of an alternative representation
for those MWEs that exhibit regular internal syn-
tax. The idea is to represent these using regular
syntactic internal structure, while keeping the se-
mantic information that they are MWEs.
We devote section 2 to related work. In sec-
tion 3, we describe the French dataset, how MWEs
are originally represented in it, and we present
and motivate an alternative representation. Sec-
tion 4 describes the different architectures we test
</bodyText>
<footnote confidence="0.6574955">
2The main focus of the Shared Task was on pre-
dicting both morphological and syntactic analysis for
morphologically-rich languages. The French dataset is the
only one containing MWEs: the French treebank has the
particularity to contain a high ratio of tokens belonging to
a MWE (12.7% of non numerical tokens).
</footnote>
<page confidence="0.960464">
743
</page>
<note confidence="0.8318245">
Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 743–753,
Baltimore, Maryland, USA, June 23-25 2014. c�2014 Association for Computational Linguistics
</note>
<bodyText confidence="0.9996024">
for predicting both syntax and MWEs. Section 5
presents the external resources targeted to improve
MWE recognition. We describe experiments and
discuss their results in section 6 and conclude in
section 7.
</bodyText>
<sectionHeader confidence="0.998773" genericHeader="introduction">
2 Related work
</sectionHeader>
<bodyText confidence="0.999836424242424">
We gave in introduction references to previous
work on predicting MWEs and constituency pars-
ing. To our knowledge, the first works3 on predict-
ing both MWEs and dependency trees are those
presented to the SPMRL 2013 Shared Task that
provided scores for French (which is the only
dataset containing MWEs). Constant et al. (2013)
proposed to combine pipeline and joint systems in
a reparser (Sagae and Lavie, 2006), and ranked
first at the Shared Task. Our contribution with
respect to that work is the representation of the
internal syntactic structure of MWEs, and use of
MWE-specific features for the joint system. The
system of Bj¨orkelund et al. (2013) ranked second
on French, though with close UAS/LAS scores. It
is a less language-specific system that reranks n-
best dependency parses from 3 parsers, informed
with features from predicted constituency trees. It
uses no feature nor treatment specific to MWEs as
it focuses on the general aim of the Shared Task,
namely coping with prediction of morphological
and syntactic analysis.
Concerning related work on the representa-
tion of MWE internal structure, we can cite the
Prague Dependency Bank, which captures both
regular syntax of non-compositional MWEs and
their MWE status, in two distinct annotation lay-
ers (Bejˇcek and Stranak, 2010). Our represen-
tation also resembles that of light-verb construc-
tions (LVC) in the hungarian dependency treebank
(Vincze et al., 2010): the construction has regular
syntax, and a suffix is used on labels to express it
is a LVC (Vincze et al., 2013).
</bodyText>
<sectionHeader confidence="0.946361" genericHeader="method">
3 Data: MWEs in Dependency Trees
</sectionHeader>
<bodyText confidence="0.992161727272727">
The data we use is the SPMRL 13 dataset for
French, in dependency format. It contains pro-
jective dependency trees that were automatically
derived from the latest status of the French Tree-
bank (Abeill´e and Barrier, 2004), which con-
sists of constituency trees for sentences from the
3Concerning non contiguous MWEs, we can cite the work
of Vincze et al. (2013), who experimented joint dependency
parsing and light verb construction identification.
newspaper Le Monde, manually annotated with
phrase structures, morphological information, and
grammatical functional tags for dependents of
verbs. The Shared Task used an enhanced version
of the constituency-to-dependency conversion of
Candito et al. (2010), with different handling of
MWEs. The dataset consists of 18535 sentences,
split into 14759, 1235 and 2541 sentences for
training, development, and final evaluation respec-
tively.
We describe below the flat representation of
MWEs in this dataset, and the modified represen-
tation for regular MWEs that we propose.
</bodyText>
<figure confidence="0.8132555">
a. Flat representation:
L’ abus de biens sociaux fut denonce en vain
b. Structured representation:
L’ abus de biens sociaux fut denonce en vain
</figure>
<figureCaption confidence="0.995615">
Figure 1: French dependency tree for L’abus de
</figureCaption>
<bodyText confidence="0.6955165">
biens sociaux fut denonce en vain (literally the
misuse of assets social was denounced in vain,
meaning The misuse of corporate assets was de-
nounced in vain), containing two MWEs (in red).
Top: original flat representation. Bottom: Tree af-
ter regular MWEs structuring.
</bodyText>
<subsectionHeader confidence="0.999286">
3.1 MWEs in Gold Data: Flat representation
</subsectionHeader>
<bodyText confidence="0.999988166666667">
In gold data, the MWEs appear in an expanded
flat format: each MWE bears a part-of-speech
and consists of a sequence of tokens (hereafter
the “components” of the MWE), each having their
proper POS, lemma and morphological features.
In the dependency trees, there is no “node” for a
MWE as a whole, but one node per MWE com-
ponent (more generally one node per token). The
first component of a MWE is taken as the head
of the MWE. All subsequent components of the
MWE depend on the first one, with the special
label dep_cpd (hence the name flat represen-
</bodyText>
<page confidence="0.996712">
744
</page>
<bodyText confidence="0.999846888888889">
tation). Furthermore, the first MWE component
bears a feature mwehead equal to the POS of the
MWE. An example is shown in Figure 1. The
MWE en vain (pointlessly) is an adverb, contain-
ing a preposition and an adjective. The latter de-
pends on former, which bears mwehead=ADV+.
The algorithm to recover MWEs is: any node
having dependents with the dep_cpd label forms
a MWE with such dependents.
</bodyText>
<subsectionHeader confidence="0.9930705">
3.2 Alternative representation for regular
MWEs
</subsectionHeader>
<bodyText confidence="0.999982636363636">
In the alternative representation we propose, ir-
regular MWEs are unchanged and appear as flat
MWEs (e.g. en vain in Figure 1 has pattern prepo-
sition+adjective, which is not considered regular
for an adverb, and is thus unchanged). Regular
MWEs appear with ’structured’ syntax: we mod-
ify the tree structure to recover the regular syn-
tactic dependencies. For instance, in the bottom
tree of the figure, biens is attached to the prepo-
sition, and the adjective sociaux is attached to bi-
ens, with regular labels. Structured MWEs can-
not be spotted using the tree topology and la-
bels only. Features are added for that purpose:
the syntactic head of the structured MWE bears
a regmwehead for the POS of the MWE (abus
in Figure 1), and the other components of the
MWE bear a regcomponent feature (the orange
tokens in Figure 1).4 With this representation,
the algorithm to recover regular MWEs is: any
node bearing regmwehead forms a MWE with
the set of direct or indirect dependents bearing a
regcomponent feature.
</bodyText>
<subsectionHeader confidence="0.941545">
3.2.1 Motivations
</subsectionHeader>
<bodyText confidence="0.999928538461538">
Our first motivation is to increase the quantity of
information conveyed by the dependency trees,
by distinguishing syntactic regularity and seman-
tic regularity. Syntactically regular MWEs (here-
after regular MWEs) show various degrees of se-
mantic non-compositionality. For instance, in the
French Treebank, population active (lit. active
population, meaning ’working population’) is a
partially compositional MWE. Furthermore, some
sequences are both syntactically and semantically
regular, but encoded as MWE due to frozen lexi-
cal selection. This is the case for d´eicit budg´etaire
(lit. budgetary deicit, meaning ’budget deicit’),
</bodyText>
<footnote confidence="0.986931">
4The syntactic head of a structured MWE may not be the
first token, whereas the head token of a flat MWE is always
the first one.
</footnote>
<bodyText confidence="0.998306545454546">
because it is not possible to use d´eicit du bud-
get (budget deicit). Our alternative representa-
tion distinguishes between syntactic internal reg-
ularity and semantic regularity. This renders the
syntactic description more uniform and it provides
an internal structure for regular MWEs, which is
meaningful if the MWE is fully or partially com-
positional. For instance, it is meaningful to have
the adjective sociaux attach to biens instead of on
the first component abus. Moreover, such a dis-
tinction opens the way to a non-binary classifica-
tion of MWE status: the various criteria leading to
classify a sequence as MWE could be annotated
separately and using nominal or scaled categories
for each criteria. For instance, d´eicit budg´etaire
could be marked as fully compositional, but with
frozen lexical selection. Further, annotation is of-
ten incoherent for the MWEs with both regular
syntax and a certain amount of semantic compo-
sitionality, the same token sequence (with same
meaning) being sometimes annotated as MWE
and sometimes not.
More generally, keeping a regular representa-
tion would allow to better deal with the interac-
tion between idiomatic status and regular syntax,
such as the insertion of modifiers on MWE sub-
parts (e.g. make a quick decision).
Finally, using regular syntax for MWEs pro-
vides a more uniform training set. For instance for
a sequence N1 preposition N2, though some exter-
nal attachments might vary depending on whether
the sequence forms a MWE or not, some may
not, and the internal dependency structure (N1 →
(preposition → N2)) is quite regular. One objec-
tive of the current work is to investigate whether
this increased uniformity eases parsing or whether
it is mitigated by the additional difficulty of find-
ing the internal structure of a MWE.
Total Nb of regular MWEs
nb of (% of nouns, adverbs,
MWEs prepositions, verbs)
train 23658 12569 (64.7, 19.2, 14.6, 1.5)
dev 2120 1194 (66.7, 17.7, 14.7, 0.8)
test 4049 2051 (64.5, 19.9, 13.6, 2.0)
</bodyText>
<tableCaption confidence="0.91392">
Table 1: Total number of MWEs and number of
regular MWEs in training, development and test
set (and broken down by POS of MWE).
</tableCaption>
<page confidence="0.995322">
745
</page>
<subsectionHeader confidence="0.647135">
3.2.2 Implementation
</subsectionHeader>
<bodyText confidence="0.999992583333333">
We developed an ad hoc program for structur-
ing the regular MWEs in gold data. MWEs are
first classified as regular or irregular, using reg-
ular expressions over the sequence of parts-of-
speech within the MWE. To define the regular
expressions, we grouped gold MWEs according
to the pair [global POS of the MWE + sequence
of POS of the MWE components], and designed
regular expressions to match the most frequent
patterns that looked regular according to our lin-
guistic knowledge. The internal structure for the
matching MWEs was built deterministically, us-
ing heuristics favoring local attachments.5 Table 1
shows the proportions of MWEs classified as regu-
lar, and thus further structured. About half MWEs
are structured, and about two thirds of structured
MWEs are nouns.
For predicted parses with structured MWEs, we
use an inverse transformation of structured MWEs
into flat MWEs, for evaluation against the gold
data. When a predicted structured MWE is flat-
tened, all the dependents of any token of the MWE
that are not themselves belonging to the MWE are
attached to the head component of the MWE.
</bodyText>
<subsectionHeader confidence="0.999549">
3.3 Integration of MWE features into labels
</subsectionHeader>
<bodyText confidence="0.998104230769231">
In some experiments, we make use of alterna-
tive representations, which we refer later as “la-
beled representation”, in which the MWE features
are incorporated in the dependency labels, so that
MWE composition and/or the POS of the MWE be
totally contained in the tree topology and labels,
and thus predictable via dependency parsing. Fig-
ure shows the labeled representation for the sen-
tence of Figure 1.
For flat MWEs, the only missing information is
the MWE part-of-speech: we concatenate it to the
dep_cpd labels. For instance, the arc from en
to vain is relabeled dep_cpd_ADV. For struc-
tured MWEs, in order to get full MWE account
within the tree structure and labels, we need to in-
corporate both the MWE POS, and to mark it as
5The six regular expressions that we obtained cover nomi-
nal, prepositional, adverbial and verbal compounds. We man-
ually evaluated both the regular versus irregular classification
and the structuring of regular MWEs on the first 200 MWEs
of the development set. 113 of these were classified as regu-
lar, and we judged that all of them were actually regular, and
were correctly structured. Among the 87 classified as irregu-
lar, 7 should have been tagged as regular and structured. For
4 of them, the classification error is due to errors on the (gold)
POS of the MWE components.
</bodyText>
<figure confidence="0.5226125">
c. Labeled representation:
L’ abus de biens sociaux fut denonce en vain
</figure>
<figureCaption confidence="0.786271">
Figure 2: Integration of all MWE information into
labels for the example of Figure 1.
</figureCaption>
<bodyText confidence="0.99958375">
belonging to a MWE. The suffixed label has the
form FCT_r_POS. For instance, in bottom tree
of Figure 1, arcs pointing to the non-head compo-
nents (de, biens, sociaux) are suffixed with _r to
mark them as belonging to a structured MWE, and
with _N since the MWE is a noun.
In both cases, this label suffixing is translated
back into features for evaluation against gold data.
</bodyText>
<sectionHeader confidence="0.7878655" genericHeader="method">
4 Architectures for MWE Analysis and
Parsing
</sectionHeader>
<bodyText confidence="0.997773142857143">
The architectures we investigated vary depending
on whether the MWE status of sequences of to-
kens is predicted via dependency parsing or via an
external tool (described in section 5), and this di-
chotomy applies both to structured MWEs and flat
MWEs. More precisely, we consider the following
alternative for irregular MWEs:
</bodyText>
<listItem confidence="0.947183545454546">
• IRREG-MERGED: gold irregular MWEs are
merged for training; for parsing, irregular
MWEs are predicted externally, merged into
one token at parsing time, and re-expanded
into several tokens for evaluation;
• IRREG-BY-PARSER: the MWE status, flat
topology and POS are all predicted via de-
pendency parsing, using representations for
training and parsing, with all information for
irregular MWEs encoded in topology and la-
bels (as for in vain in Figure 2).
</listItem>
<bodyText confidence="0.99885625">
For regular MWEs, their internal structure is al-
ways predicted by the parser. For instance the un-
labeled dependencies for abus de biens sociaux are
the same, independently of predicting whether it
</bodyText>
<page confidence="0.985288">
746
</page>
<table confidence="0.950121666666667">
Name prediction of prediction of
reg MWEs irreg MWEs
JOINT irreg-by-parser reg-by-parser
JOINT-REG irreg-merged reg-by-parser
JOINT-IRREG irreg-by-parser reg-post-annot
PIPELINE irreg-merged reg-post-annot
</table>
<listItem confidence="0.938963916666667">
forms a MWE or not. But we use two kinds of
predictions for their MWE status and POS:
• REG-POST-ANNOTATION: the regular
MWEs are encoded/predicted as shown for
abus de biens sociaux in bottom tree of
Figure 1, and their MWE status and POS is
predicted after parsing, by an external tool.
• REG-BY-PARSER: all regular MWE infor-
mation (topology, status, POS) is predicted
via dependency parsing, using representa-
tions with all information for regular MWEs
encoded in topology and labels (Figure 2).
</listItem>
<tableCaption confidence="0.529139">
Table 2: The four architectures, depending on how
regular and irregular MWEs are predicted.
</tableCaption>
<bodyText confidence="0.996685">
We obtain four architectures, schematized in ta-
ble 2. We describe more precisely two of them,
the other two being easily inferable:
</bodyText>
<listItem confidence="0.898698842105263">
JOINT-REG architecture:
• training set: irregular MWEs merged into
one token, regular MWEs are structured, and
integration of regular MWE information into
the labels (FCT_r_POS).
• parsing: (i) MWE analysis with classifica-
tion of MWEs into regular or irregular, (ii)
merge of predicted irregular MWEs, (iii) tag-
ging and morphological prediction, (iv) pars-
ing
JOUTIRREG architecture:
• .training set: flat representation of irregu-
lar MWEs, with label suffixing (dep_cpd_
POS), structured representation of regular
MWEs without label suffixing.
• parsing: (i) MWE analysis and classifica-
tion into regular or irregular, used for MWE-
specific features, (ii) tagging and morpholog-
ical prediction, (iii) parsing,
</listItem>
<bodyText confidence="0.9649268">
We compare these four architectures between
them and also with two simpler architectures used
by (Constant et al., 2013) within the SPMRL 13
Shared Task, in which regular and irregular MWEs
are not distinguished:
Uniform joint architecture: The joint systems
perform syntactic parsing and MWE analysis via
a single dependency parser, using representations
as in 3.3.
Uniform pipeline architecture:
</bodyText>
<listItem confidence="0.98793975">
• training set: MWEs merged into one token
• parsing: (i) MWE analysis, (ii) merge ofpre-
dicted MWEs, (iii) tagging and morphologi-
cal prediction, (iv) parsing
</listItem>
<bodyText confidence="0.9986804">
For each architecture, we apply the appropriate
normalization procedures on the predicted parses,
in order to evaluate against (i) the pseudo-gold
data in structured representation, and (ii) the gold
data in flat representation.
</bodyText>
<sectionHeader confidence="0.822071" genericHeader="method">
5 Use of external MWE resources
</sectionHeader>
<bodyText confidence="0.999991444444445">
In order to better deal with MWE prediction, we
use external MWE resources, namely MWE lexi-
cons and an MWE analyzer. Both resources help
to predict MWE-specific features (section 5.3) to
guide the MWE-aware dependency parser. More-
over, in some of the architectures, the external
MWE analyzer is used either to pre-group irreg-
ular MWEs (for the architectures using IRREG-
MERGED), or to post-annotate regular MWEs.
</bodyText>
<subsectionHeader confidence="0.96048">
5.1 MWE lexicons
</subsectionHeader>
<bodyText confidence="0.999984947368421">
MWE lexicons are exploited as sources of fea-
tures for both the dependency parser and the ex-
ternal MWE analyzer. In particular, two large-
coverage general-language lexicons are used: the
Lefff6 lexicon (Sagot, 2010), which contains ap-
proximately half a million inflected word forms,
among which approx. 25,000 are MWEs; and
the DELA7 (Courtois, 2009; Courtois et al., 1997)
lexicon, which contains approx. one million in-
flected forms, among which about 110,000 are
MWEs. These resources are completed with spe-
cific lexicons freely available in the platform Uni-
tex8: the toponym dictionary Prolex (Piton et al.,
1999) and a dictionary of first names. Note that the
lexicons do not include any information on the ir-
regular or the regular status of the MWEs. In order
to compare the MWEs present in the lexicons and
those encoded in the French treebank, we applied
the following procedure (hereafter called lexicon
</bodyText>
<footnote confidence="0.999218666666667">
6We use the version available in the POS tagger MElt (De-
nis and Sagot, 2009).
7We use the version in the platform Unitex
(http://igm.univ-mlv.fr/˜unitex). We had to convert the
DELA POS tagset to that of the French Treebank.
8http://igm.univ-mlv.fr/˜unitex
</footnote>
<page confidence="0.993237">
747
</page>
<bodyText confidence="0.9985168">
lookup): in a given sentence, the maximum num-
ber of non overlapping MWEs according to the
lexicons are systematically marked as such. We
obtain about 70% recall and 50% precision with
respect to MWE spanning.
</bodyText>
<subsectionHeader confidence="0.997401">
5.2 MINE Analyzer
</subsectionHeader>
<bodyText confidence="0.999862769230769">
The MWE analyzer is a CRF-based sequential la-
beler, which, given a tokenized text, jointly per-
forms MWE segmentation and POS tagging (of
simple tokens and of MWEs), both tasks mutu-
ally helping each other9. The MWE analyzer inte-
grates, among others, features computed from the
external lexicons described in section 5.1, which
greatly improve POS tagging (Denis and Sagot,
2009) and MWE segmentation (Constant and Tel-
lier, 2012). The MWE analyzer also jointly classi-
fies its predicted MWEs as regular or irregular (the
distinction being learnt on gold training set, with
structured MWEs cf. section 3.2).
</bodyText>
<subsectionHeader confidence="0.973325">
5.3 MINE-specific features
</subsectionHeader>
<bodyText confidence="0.999980958333333">
We introduce information from the external MWE
resources in different ways:
Flat MINE features: MWE information can
be integrated as features to be used by the de-
pendency parser. We tested to incorporate the
MWE-specific features as defined in the gold flat
representation (section 3.1): the mwehead=POS
feature for the MWE head token, POS being the
part-of-speech of the MWE; the component=y
feature for the non-first MWE component.
Switch: instead or on top of using the mwehead
feature, we use the POS of the MWE instead of the
POS of the first component of a flat MWE. For in-
stance in Figure 1, the token en gets pos=ADV in-
stead of pos=P. The intuition behind this feature
is that for an irregular MWE, the POS of the lin-
early first component, which serves as head, is not
always representative of the external distribution
of the MWE. For regular MWEs, the usefulness of
such a trick is less obvious. The first component
of a regular MWE is not necessarily its head (for
instance for a nominal MWE with internal pattern
adjective+noun), so the switch trick could be detri-
mental in such cases.10
</bodyText>
<footnote confidence="0.907758333333333">
9Note that in our experiments, we use this analyzer for
MWE analysis only, and discard the POS tagging predic-
tion. Tagging is performed along with lemmatization with
the Morfette tool (section 6.1).
10We also experimented to use POS of MWE plus suffixes
to force disjoint tagsets for single words, irregular MWEs and
</footnote>
<sectionHeader confidence="0.997813" genericHeader="evaluation">
6 Experiments
</sectionHeader>
<subsectionHeader confidence="0.999431">
6.1 Settings and evaluation metrics
</subsectionHeader>
<bodyText confidence="0.9999306875">
MINE Analysis and Tagging: For the MWE
analyzer, we used the tool lgtagger11 (version
1.1) with its default set of feature templates, and a
10-fold jackknifing on the training corpus.
Parser: We used the second-order graph-based
parser available in Mate-tools12 (Bohnet, 2010).
We used the Anna3.3 version, in projective
mode, with default feature sets and parameters
proposed in the documentation, augmented or not
with MWE-specific features, depending on the
experiments.
Morphological prediction: Predicted lemmas,
POS and morphology features are computed
with Morfette version 0.3.5 (Chrupała et al.,
2008; Seddah et al., 2010)13, using 10 iterations
for the tagging perceptron, 3 iterations for the
lemmatization perceptron, default beam size for
the decoding of the joint prediction, and the
Lefff (Sagot, 2010) as external lexicon used for
out-of-vocabulary words. We performed a 10-fold
jackknifing on the training corpus.
Evaluation metrics: we evaluate our parsing sys-
tems by using the standard metrics for depen-
dency parsing: Labeled Attachment Score (LAS)
and Unlabeled Attachment Score (UAS), com-
puted using all tokens including punctuation. To
evaluate statistical significance of parsing perfor-
mance differences, we use eval07.pl14 with -b op-
tion, and then Dan Bikel’s comparator.15 For
MWEs, we use the Fmeasure for recognition of
untagged MWEs (hereafter FUM) and for recog-
nition of tagged MWEs (hereafter FTM).
</bodyText>
<subsectionHeader confidence="0.999682">
6.2 MINE-specific feature prediction
</subsectionHeader>
<bodyText confidence="0.999919">
In all our experiments, for the switch trick (section
5.3), the POS of MWE is always predicted using
the MWE analyzer. For the flat MWE features, we
experimented both with features predicted by the
MWE analyzer, and with features predicted using
the external lexicons mentioned in section 5.1 (us-
ing the lexicon lookup procedure). Both kinds of
</bodyText>
<footnote confidence="0.999683571428571">
regular MWEs, but this showed comparable results.
11http://igm.univ-mlv.fr/˜mconstan
12http://code.google.com/p/mate-tools/
13https://sites.google.com/site/morfetteweb/
14http://nextens.uvt.nl/depparse-wiki/SoftwarePage
15The compare.pl script, formerly available at
www.cis.upenn.edu/ dbikel/
</footnote>
<page confidence="0.986465">
748
</page>
<table confidence="0.997700083333333">
LABELED STRUCTURED FLAT
REPRES. REPRESENTATION REPRESENTATION
ARCHI MWE swi. swi. LAS UAS LAS FUM FTM LAS UAS FUM FTM
feats irreg reg irreg irreg
bsline - - - 84.5 89.3 87.0 83.6 80.6 84.2 88.1 73.5 70.7
JOINT best + + + 85.3 89.7 87.5 85.4 82.6 85.2 88.8 77.6 74.5
JOINT- bsline - - - 84.7 89.4 87.0 83.5 80.3 84.5 88.0 78.3 75.9
IRREG best + + + 85.1 89.8 87.4 85.0 81.6 84.9 88.3 79.0 76.5
JOINT- bsline - NA - 84.2 89.1 86.7 84.2 80.8 84.0 88.0 73.3 70.3
REG best + NA + 84.7 89.3 86.9 84.1 80.7 84.6 88.3 76.3 73.2
PIPE bsline - NA - 84.6 89.2 86.9 84.1 80.7 84.5 87.9 78.8 76.3
LINE best - NA + 84.7 89.4 87.0 84.2 80.8 84.6 88.1 78.8 76.3
</table>
<tableCaption confidence="0.990199">
Table 3: Baseline and best results for the four MWE+parsing architectures on the dev set (see text for
</tableCaption>
<bodyText confidence="0.9199442">
statistical significance evaluation). The UAS for the structured representation is the same as the one for
the labeled representation, and is not repeated.
prediction lead to fairly comparable results, so in
all the following, the MWE features, when used,
are predicted using the external lexicons.
</bodyText>
<subsectionHeader confidence="0.998236">
6.3 Tuning features for each architecture
</subsectionHeader>
<bodyText confidence="0.999990275362319">
We ran experiments for all value combinations
of the following parameters: (i) the architecture,
(ii) whether MWE features are used, whether the
switch trick is applied or not (iii) for irregular
MWEs and (iv) for regular MWEs.
We performed evaluation of the predicted parses
using the three representations described in sec-
tion 3, namely flat, structured and labeled repre-
sentations. In the last two cases, the evaluation
is performed against an instance of the gold data
automatically transformed to match the represen-
tation type. Moreover, for the “labeled representa-
tion” evaluation, though the MWE information in
the predicted parses is obtained in various ways,
depending on the architecture, we always map all
this information in the dependency labels, to ob-
tain predicted parses matching the “labeled repre-
sentation”. While the evaluation in flat represen-
tation is the only one comparable to other works
on this dataset, the other two evaluations provide
useful information. In the “labeled representation”
evaluation, the UAS provides a measure of syn-
tactic attachments for sequences of words, inde-
pendently of the (regular) MWE status of subse-
quences. For the sequence abus de biens sociaux,
suppose that the correct internal structure is pre-
dicted, but not the MWE status. The UAS for
labeled representation will be maximal, whereas
for the flat representation, the last two tokens will
count as incorrect for UAS. For LAS, in both cases
the three last tokens will count as incorrect if the
wrong MWE status is predicted. So to sum up on
the “labeled evaluation”, we obtain a LAS eval-
uation for the whole task of parsing plus MWE
recognition, but an UAS evaluation that penalizes
less errors on MWE status, while keeping a rep-
resentation that is richer: predicted parses contain
not only the syntactic dependencies and MWE in-
formation, but also a classification of MWEs into
regular and irregular, and the internal syntactic
structure of regular MWEs.
The evaluation on “structured representation”
can be interpreted as an evaluation of the parsing
task plus the recognition of irregular MWEs only:
both LAS and UAS are measured independently
of errors on regular MWE status (note the UAS is
exactly the same than in the “labeled” case).
For each architecture, Table 3 shows the results
for two systems: first the baseline system without
any MWE features nor switches and immediately
below the best settings for the architecture. The
JOINT baseline corresponds to a “pure” joint sys-
tem without external MWE resources (hence the
minus sign for the first three columns). For each
architecture except the PIPELINE one, differences
between the baseline and the best setting are sta-
tistically significant (p &lt; 0.01). Differences be-
tween best PIPELINE and best JOINT-REG are
not. Best JOINT has statistically significant dif-
ference (p &lt; 0.01) over both best JOINT-REG
and best PIPELINE. The situation for best JOINT-
IRREG with respect to the other three is borderline
(with various p-values depending on the metrics).
Concerning the tuning of parameters, it appears
that the best setting is to use MWE-features, and
switch for both regular and irregular MWEs, ex-
cept for the pipeline architecture for which results
without MWE features are slightly better. So over-
all, informing the parser with independently pre-
</bodyText>
<page confidence="0.995398">
749
</page>
<table confidence="0.999463777777778">
LABELED STRUCTURED FLAT
REPRESENTATION REPRESENTATION REPRESENTATION
SYSTEM LAS UAS LAS UAS FUM FTM LAS UAS FUM FTM
irreg irreg
baseline JOINT 84.13 88.93 86.62 88.93 83.6 79.2 83.97 87.80 73.9 70.5
best JOINT 84.59 89.21 86.92 89.21 85.7 81.4 84.48 88.13 77.0 73.5
best JOINT-IRREG 84.50 89.21 86.97 89.24 86.3 82.1 84.36 87.75 78.6 75.4
best JOINT-REG 84.31 89.0 86.63 89.00 84.5 80.4 84.18 87.95 76.4 73.3
best PIPELINE 84.02 88.83 86.49 88.83 84.4 80.4 83.88 87.33 77.6 74.4
</table>
<tableCaption confidence="0.999094">
Table 4: Final results on test set for baseline and the best system for each architecture.
</tableCaption>
<bodyText confidence="0.99976084">
dicted POS of MWE has positive impact. The
best architectures are JOINT and JOINT-IRREG,
with the former slightly better than the latter for
parsing metrics, though only some of the differ-
ences are significant between the two. It can be
noted though, that JOINT-IRREG performs over-
all better on MWEs (last two columns of table
3), whereas JOINT performs better on irregular
MWEs: the latter seems to be beneficial for pars-
ing, but is less efficient to correctly spot the regular
MWEs.
Concerning the three distinct representations,
evaluating on structured representation (hence
without looking at regular MWE status) leads to
a rough 2 point performance increase for the LAS
and a one point increase for the UAS, with respect
to the evaluation against flat representation. This
quantifies the additional difficulty of deciding for
a regular sequence of tokens whether it forms a
MWE or not. The evaluation on the labeled rep-
resentation provides an evaluation of the full task
(parsing, regular/irregular MWE recognition and
regular MWEs structuring), with a UAS that is less
impacted by errors on regular MWE status, while
LAS reflects the full difficulty of the task.16
</bodyText>
<subsectionHeader confidence="0.982513">
6.4 Results on test set and comparison
</subsectionHeader>
<bodyText confidence="0.9999835">
We provide the final results on the test set in
table 4. We compare the baseline JOINT sys-
tem with the best system for all four reg/irreg
architectures (cf. section 6.3). We observe the
same general trend as in the development corpus,
but with tinier differences. JOINT and JOINT-
IRREG significantly outperform the baseline and
the PIPELINE, on labeled representation and flat
representation. We can see that there is no sig-
nificant difference between JOINT and JOINT-
</bodyText>
<footnote confidence="0.9494478">
16The slight differences in LAS between the labeled and
the flat representations are due to side effects of errors on
MWE status: some wrong reattachments performed to obtain
flat representation decrease the UAS, but also in some cases
the LAS.
</footnote>
<table confidence="0.999720428571428">
DEV TEST
System UAS LAS UAS LAS
reg/irreg joint 88.79 85.15 88.13 84.48
Bjork13 88.30 84.84 87.87 84.37
Const13 pipeline 88.73 85.28 88.35 84.91
Const13 joint 88.21 84.60 87.76 84.14
uniform joint 88.81 85.42 87.96 84.59
</table>
<tableCaption confidence="0.99631">
Table 5: Comparison on dev set of our best archi-
</tableCaption>
<bodyText confidence="0.988878212121212">
tecture with reg/irregular MWE distinction (first
row), with the single-parser architectures of (Con-
stant et al., 2013) (Const13) and (Bj¨orkelund et
al., 2013) (Bjork13). Uniform joint is our reimple-
mentation of Const13 joint, enhanced with mwe-
features and switch.
IRREG and between JOINT-REG and JOINT-
IRREG. JOINT slightly outperforms JOINT-REG
(p &lt; 0.05). On the structured representation, the
two best systems (JOINT and JOINT-IRREG) sig-
nificantly outperform the other systems (p &lt; 0.01
for all; p &lt; 0.05 for JOINT-REG).
Moreover, we provide in table 5 a comparison
of our best architecture with reg/irregular MWE
distinction with other architectures that do not
make this distinction, namely the two best com-
parable systems designed for the SPMRL Shared
Task (Seddah et al., 2013): the pipeline sim-
ple parser based on Mate-tools of Constant et
al. (2013) (Const13) and the Mate-tools system
(without reranker) of Bj¨orkelund et al. (2013)
(Bjork13). We also reimplemented and improved
the uniform joint architecture of Constant et al.
(2013), by adding MWE features and switch. Re-
sults can only be compared on the flat representa-
tion, because the other systems output poorer lin-
guistic information. We computed statistical sig-
nificance of differences between our systems and
Const13. On dev, the best system is the enhanced
uniform joint, but differences are not significant
between that and the best reg/irreg joint (1st row)
and the Const13 pipeline. But on the test corpus
(which is twice bigger), the best system is Const13
</bodyText>
<page confidence="0.989539">
750
</page>
<table confidence="0.993284571428571">
Tasks LAS UAS ALL MWE REG MWE IRREG MWE
System Parsing MWE FUM FTM FUM FTM FUM FTM
Our best system (best JOINT) + all 85.15 88.78 77.6 74.5 70.8 67.8 85.4 82.6
Uniform pipeline/gold MWEs + - 88.73 90.60 - - - - - -
CRF-based MWE analyzer - all - - 78.8 76.3 73.5 71.9 84.2 80.8
JOINT-REG + all 84.58 88.34 76.3 73.2 69.3 66.5 84.1 80.7
JOINT-REG/gold irreg. MWE + reg. 85.86 89.19 82.9 78.8 70.0 67.2 - -
</table>
<tableCaption confidence="0.999953">
Table 6: Comparison with simpler tasks on the flat representation of the development set.
</tableCaption>
<bodyText confidence="0.999693125">
pipeline, with statistically significant differences
over our joint systems. So the first observation
is that our architectures that distinguish between
reg/irreg MWEs do not outperform uniform ar-
chitectures. But we note that the differences are
slight, and the output we obtain is enhanced with
regular MWE internal structure. It can thus be
noted that the increased syntactic uniformity ob-
tained by our MWE representation is mitigated so
far by the additional complexity of the task. The
second observation is that currently the best sys-
tem on this dataset is a pipeline system, as results
on test set show (and somehow contrary to results
on dev set). The joint systems that integrate MWE
information in the labels seem to suffer from in-
creased data sparseness.
</bodyText>
<subsectionHeader confidence="0.6263195">
6.5 Evaluating the double task with respect
to simpler tasks
</subsectionHeader>
<bodyText confidence="0.999984694444444">
In this section, we propose to better evaluate the
difficulty of combining the tasks of MWE analy-
sis and dependency parsing by comparing our sys-
tems with systems performing simpler tasks: i.e.
MWE recognition without parsing, and parsing
with no or limited MWE recognition, simulated by
using gold MWEs. We also provide a finer eval-
uation of the MWE recognition task, in particular
with respect to their regular/irregular status.
We first compare our best system with a parser
where all MWEs have been perfectly pre-grouped,
in order to quantify the difficulty that MWEs add
to the parsing task. We also compare the per-
formance on MWEs of our best system with that
achieved by the CRF-based analyzer described in
section 5.2. Next, we compare the best JOINT-
REG system with the one based on the same ar-
chitecture but where the irregular MWEs are per-
fectly pre-identified, in order to quantify the dif-
ficulty added by the irregular MWEs. Results are
given in table 6. Without any surprise, the task
is much easier without considering MWE recog-
nition. We can see that without considering MWE
analysis the parsing accuracy is about 2.5 points
better in terms of LAS. In the JOINT-REG ar-
chitecture, assuming gold irregular MWE identi-
fication, increases LAS by 1.3 point. In terms
of MWE recognition, as compared with the CRF-
based analyzer, our best system is around 2 points
below. But the situation is quite different when
breaking the evaluation by MWE type. Our sys-
tem is 1 point better than the CRF-based analyzer
for irregular MWEs. This shows that considering
a larger syntactic context helps recognition of ir-
regular MWEs. The ”weak point” of our system is
therefore the identification of regular MWEs.
</bodyText>
<sectionHeader confidence="0.998376" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999991210526316">
We experimented strategies to predict both MWE
analysis and dependency structure, and tested
them on the dependency version of French Tree-
bank (Abeill´e and Barrier, 2004), as instantiated
in the SPMRL Shared Task (Seddah et al., 2013).
Our work focused on using an alternative repre-
sentation of syntactically regular MWEs, which
captures their syntactic internal structure. We ob-
tain a system with comparable performance to that
of previous works on this dataset, but which pre-
dicts both syntactic dependencies and the internal
structure of MWEs. This can be useful for captur-
ing the various degrees of semantic composition-
ality of MWEs. The main weakness of our system
comes from the identification of regular MWEs, a
property which is highly lexical. Our current use
of external lexicons does not seem to suffice, and
the use of data-driven external information to bet-
ter cope with this identification can be envisaged.
</bodyText>
<sectionHeader confidence="0.992443" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.6380312">
Anne Abeill´e and Nicolas Barrier. 2004. Enriching
a french treebank. In Proceedings of LREC 2004,
Lisbon, Portugal.
Abhishek Arun and Frank Keller. 2005. Lexicalization
in crosslinguistic probabilistic parsing: The case of
</reference>
<page confidence="0.99234">
751
</page>
<reference confidence="0.999726441441441">
french. In Proceedings of ACL 2005, Ann Arbor,
USA.
Eduard Bejˇcek and Pavel Stranak. 2010. Annota-
tion of multiword expressions in the prague depen-
dency treebank. Language Resources and Evalua-
tion, 44:7–21.
Anders Bj¨orkelund, ¨Ozlem C¸ etinoˇglu, Thomas Farkas,
Rich´ardand M¨uller, and Wolfgang Seeker. 2013.
(re)ranking meets morphosyntax: State-of-the-art
results from the spmrl 2013 shared task. In Pro-
ceedings of the 4th Workshop on Statistical Parsing
of Morphologically Rich Languages: Shared Task,
Seattle, WA.
Bernd Bohnet. 2010. Top accuracy and fast depen-
dency parsing is not a contradiction. In Proceedings
of COLING 2010, Beijing, China.
Conor Cafferkey, Deirdre Hogan, and Josef van Gen-
abith. 2007. Multi-word units in treebank-based
probabilistic parsing and generation. In Proceed-
ings of the 10th International Conference on Re-
cent Advances in Natural Language Processing
(RANLP’07), Borovets, Bulgaria.
Marie Candito, Benoit Crabb´e, and Pascal Denis.
2010. Statistical french dependency parsing : Tree-
bank conversion and first results. In Proceedings of
LREC 2010, Valletta, Malta.
Grzegorz Chrupała, Georgiana Dinu, and Josef van
Genabith. 2008. Learning morphology with mor-
fette. In Proceedings of LREC 2008, Marrakech,
Morocco. ELDA/ELRA.
Matthieu Constant and Isabelle Tellier. 2012. Eval-
uating the impact of external lexical resources into
a crf-based multiword segmenter and part-of-speech
tagger. In Proceedings of LREC 2012, Istanbul,
Turkey.
Matthieu Constant, Anthony Sigogne, and Patrick Wa-
trin. 2012. Discriminative strategies to integrate
multiword expression recognition and parsing. In
Proceedings ofACL 2012, Stroudsburg, PA, USA.
Matthieu Constant, Marie Candito, and Djam´e Sed-
dah. 2013. The ligm-alpage architecture for the
spmrl 2013 shared task: Multiword expression anal-
ysis and dependency parsing. In Proceedings of the
4th Workshop on Statistical Parsing ofMorphologi-
cally Rich Languages: Shared Task, Seattle, WA.
Blandine Courtois, Myl`ene Garrigues, Gaston Gross,
Maurice Gross, Ren´e Jung, Mathieu-Colas Michel,
Anne Monceaux, Anne Poncet-Montange, Max Sil-
berztein, and Robert Viv´es. 1997. Dictionnaire
´electronique DELAC : les mots compos´es binaires.
Technical Report 56, University Paris 7, LADL.
Blandine Courtois. 2009. Un syst`eme de dictionnaires
´electroniques pour les mots simples du franc¸ais.
Langue Franc¸aise, 87:11–22.
Pascal Denis and Benoit Sagot. 2009. Coupling an
annotated corpus and a morphosyntactic lexicon for
state-of-the-art POS tagging with less human ef-
fort. In Proceedings of the 23rd Pacific Asia Con-
ference on Language, Information and Computation
(PACLIC’09), Hong Kong.
G¨uls¸en Eryiˇgit, Tugay Ilbay, and Ozan Arkan Can.
2011. Multiword expressions in statistical depen-
dency parsing. In Proceedings of the IWPT Work-
shop on Statistical Parsing ofMorphologically-Rich
Languages (SPMRL’11), Dublin, Ireland.
Spence Green, Marie-Catherine de Marneffe, John
Bauer, and Christofer D. Manning. 2011. Multi-
word expression identification with tree substitution
grammars: A parsing tour de force with french. In
Proceedings ofEMNLP 2011, Edinburgh, Scotland.
Spence Green, Marie-Catherine de Marneffe, and
Christopher D Manning. 2013. Parsing models for
identifying multiword expressions. Computational
Linguistics, 39(1):195–227.
Joakim Nivre and Jens Nilsson. 2004. Multiword units
in syntactic parsing. In Proceedings of the LREC
Workshop: Methodologies and Evaluation ofMulti-
word Units in Real-World Applications (MEMURA),
Lisbon, Portugal.
Odile Piton, Denis Maurel, and Claude Belleil. 1999.
The prolex data base : Toponyms and gentiles for
nlp. In Proceedings of the Third International Work-
shop on Applications of Natural Language to Data
Bases (NLDB’99), Klagenfurt, Austria.
Kenji Sagae and Alon Lavie. 2006. Parser combina-
tion by reparsing. In Proceedings of NAACL/HLT
2006, Companion Volume: Short Papers, Strouds-
burg, PA, USA.
Benoit Sagot. 2010. The lefff, a freely available, accu-
rate and large-coverage lexicon for french. In Pro-
ceedings ofLREC 2010, Valletta, Malta.
Djam´e Seddah, Grzegorz Chrupała, Ozlem Cetinoglu,
Josef van Genabith, and Marie Candito. 2010.
Lemmatization and statistical lexicalized parsing of
morphologically-rich languages. In Proceedings of
the NAACL/HLT Workshop on Statistical Parsing of
Morphologically Rich Languages (SPMRL 2010),
Los Angeles, CA.
Djam´e Seddah, Reut Tsarfaty, Sandra K´’ubler, Marie
Candito, Jinho Choi, Rich´ard Farkas, Jennifer Fos-
ter, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg,
Spence Green, Nizar Habash, Marco Kuhlmann,
Wolfgang Maier, Joakim Nivre, Adam Przepi-
orkowski, Ryan Roth, Wolfgang Seeker, Yannick
Versley, Veronika Vincze, Marcin Woli´nski, Alina
Wr´oblewska, and Eric Villemonte de la Cl´ergerie.
2013. Overview of the spmrl 2013 shared task: A
cross-framework evaluation of parsing morpholog-
ically rich languages. In Proceedings of the 4th
Workshop on Statistical Parsing ofMorphologically
Rich Languages: Shared Task, Seattle, WA.
</reference>
<page confidence="0.974507">
752
</page>
<reference confidence="0.998500777777778">
Veronika Vincze, D´ora Szauter, Attila Alm´asi, Gy¨orgy
M´ora, Zolt´an Alexin, and J´anos Csirik. 2010. Hun-
garian dependency treebank. In Proceedings of
LREC 2010, Valletta, Malta.
Veronika Vincze, J´anos Zsibrita, and Istv`an Nagy T.
2013. Dependency parsing for identifying hungar-
ian light verb constructions. In Proceedings of In-
ternational Joint Conference on Natural Language
Processing (IJCNLP 2013), Nagoya, Japan.
</reference>
<page confidence="0.999142">
753
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.463208">
<title confidence="0.839897">Strategies for Contiguous Multiword Expression Analysis Dependency Parsing Marie</title>
<author confidence="0.836342">Paris Diderot</author>
<email confidence="0.976826">linguist.univ-paris-diderot.fr</email>
<abstract confidence="0.998963105263158">In this paper, we investigate various strategies to predict both syntactic dependency parsing and contiguous multiword expression (MWE) recognition, testing them on the dependency version of French Treebank (Abeill´e and Barrier, 2004), as instantiated in the SPMRL Shared Task (Seddah et al., 2013). Our work focuses on using an alternative representation of syntactically regular MWEs, which captures their syntactic internal structure. We obtain a system with comparable performance to that of previous works on this dataset, but which predicts both syntactic dependencies and the internal structure of MWEs. This can be useful for capturing the various degrees of semantic compositionality of MWEs.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Anne Abeill´e</author>
<author>Nicolas Barrier</author>
</authors>
<title>Enriching a french treebank.</title>
<date>2004</date>
<booktitle>In Proceedings of LREC 2004,</booktitle>
<location>Lisbon, Portugal.</location>
<marker>Abeill´e, Barrier, 2004</marker>
<rawString>Anne Abeill´e and Nicolas Barrier. 2004. Enriching a french treebank. In Proceedings of LREC 2004, Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Abhishek Arun</author>
<author>Frank Keller</author>
</authors>
<title>Lexicalization in crosslinguistic probabilistic parsing: The case of french.</title>
<date>2005</date>
<booktitle>In Proceedings of ACL 2005,</booktitle>
<location>Ann Arbor, USA.</location>
<contexts>
<context position="1373" citStr="Arun and Keller, 2005" startWordPosition="193" endWordPosition="196">ies and the internal structure of MWEs. This can be useful for capturing the various degrees of semantic compositionality of MWEs. 1 Introduction A real-life parsing system should comprise the recognition of multi-word expressions (MWEs1), first because downstream semantic-oriented applications need some marking in order to distinguish between regular semantic composition and the typical semantic non-compositionality of MWEs. Second, MWE information, is intuitively supposed to help parsing. That intuition is confirmed in a classical but non-realistic setting in which gold MWEs are pregrouped (Arun and Keller, 2005; Nivre and Nilsson, 2004; Eryiˇgit et al., 2011). But the situation is much less clear when switching to automatic MWE prediction. While Cafferkey et al. (2007) report a small improvement on the pure parsing 1Multiword expressions can be roughly defined as continuous or discontinuous sets of tokens, which either do not exhibit full freedom in lexical selection or whose meaning is not fully compositional. We focus in this paper on contiguous multiword expressions, also known as “words with spaces”. Matthieu Constant Universit´e Paris-Est LIGM CNRS Matthieu.Constant@ u-pem.fr task when using ex</context>
</contexts>
<marker>Arun, Keller, 2005</marker>
<rawString>Abhishek Arun and Frank Keller. 2005. Lexicalization in crosslinguistic probabilistic parsing: The case of french. In Proceedings of ACL 2005, Ann Arbor, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Eduard Bejˇcek</author>
<author>Pavel Stranak</author>
</authors>
<title>Annotation of multiword expressions in the prague dependency treebank. Language Resources and Evaluation,</title>
<date>2010</date>
<pages>44--7</pages>
<marker>Bejˇcek, Stranak, 2010</marker>
<rawString>Eduard Bejˇcek and Pavel Stranak. 2010. Annotation of multiword expressions in the prague dependency treebank. Language Resources and Evaluation, 44:7–21.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Anders Bj¨orkelund</author>
<author>¨Ozlem C¸ etinoˇglu</author>
<author>Thomas Farkas</author>
<author>Rich´ardand M¨uller</author>
<author>Wolfgang Seeker</author>
</authors>
<title>(re)ranking meets morphosyntax: State-of-the-art results from the spmrl 2013 shared task.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Parsing of Morphologically Rich Languages: Shared Task,</booktitle>
<location>Seattle, WA.</location>
<marker>Bj¨orkelund, etinoˇglu, Farkas, M¨uller, Seeker, 2013</marker>
<rawString>Anders Bj¨orkelund, ¨Ozlem C¸ etinoˇglu, Thomas Farkas, Rich´ardand M¨uller, and Wolfgang Seeker. 2013. (re)ranking meets morphosyntax: State-of-the-art results from the spmrl 2013 shared task. In Proceedings of the 4th Workshop on Statistical Parsing of Morphologically Rich Languages: Shared Task, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Bernd Bohnet</author>
</authors>
<title>Top accuracy and fast dependency parsing is not a contradiction.</title>
<date>2010</date>
<booktitle>In Proceedings of COLING 2010,</booktitle>
<location>Beijing, China.</location>
<contexts>
<context position="23047" citStr="Bohnet, 2010" startWordPosition="3692" endWordPosition="3693"> we use this analyzer for MWE analysis only, and discard the POS tagging prediction. Tagging is performed along with lemmatization with the Morfette tool (section 6.1). 10We also experimented to use POS of MWE plus suffixes to force disjoint tagsets for single words, irregular MWEs and 6 Experiments 6.1 Settings and evaluation metrics MINE Analysis and Tagging: For the MWE analyzer, we used the tool lgtagger11 (version 1.1) with its default set of feature templates, and a 10-fold jackknifing on the training corpus. Parser: We used the second-order graph-based parser available in Mate-tools12 (Bohnet, 2010). We used the Anna3.3 version, in projective mode, with default feature sets and parameters proposed in the documentation, augmented or not with MWE-specific features, depending on the experiments. Morphological prediction: Predicted lemmas, POS and morphology features are computed with Morfette version 0.3.5 (Chrupała et al., 2008; Seddah et al., 2010)13, using 10 iterations for the tagging perceptron, 3 iterations for the lemmatization perceptron, default beam size for the decoding of the joint prediction, and the Lefff (Sagot, 2010) as external lexicon used for out-of-vocabulary words. We p</context>
</contexts>
<marker>Bohnet, 2010</marker>
<rawString>Bernd Bohnet. 2010. Top accuracy and fast dependency parsing is not a contradiction. In Proceedings of COLING 2010, Beijing, China.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Conor Cafferkey</author>
<author>Deirdre Hogan</author>
<author>Josef van Genabith</author>
</authors>
<title>Multi-word units in treebank-based probabilistic parsing and generation.</title>
<date>2007</date>
<booktitle>In Proceedings of the 10th International Conference on Recent Advances in Natural Language Processing (RANLP’07), Borovets,</booktitle>
<marker>Cafferkey, Hogan, van Genabith, 2007</marker>
<rawString>Conor Cafferkey, Deirdre Hogan, and Josef van Genabith. 2007. Multi-word units in treebank-based probabilistic parsing and generation. In Proceedings of the 10th International Conference on Recent Advances in Natural Language Processing (RANLP’07), Borovets, Bulgaria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Marie Candito</author>
<author>Benoit Crabb´e</author>
<author>Pascal Denis</author>
</authors>
<title>Statistical french dependency parsing : Treebank conversion and first results.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC 2010,</booktitle>
<location>Valletta,</location>
<marker>Candito, Crabb´e, Denis, 2010</marker>
<rawString>Marie Candito, Benoit Crabb´e, and Pascal Denis. 2010. Statistical french dependency parsing : Treebank conversion and first results. In Proceedings of LREC 2010, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Grzegorz Chrupała</author>
<author>Georgiana Dinu</author>
<author>Josef van Genabith</author>
</authors>
<title>Learning morphology with morfette.</title>
<date>2008</date>
<booktitle>In Proceedings of LREC 2008,</booktitle>
<location>Marrakech, Morocco. ELDA/ELRA.</location>
<marker>Chrupała, Dinu, van Genabith, 2008</marker>
<rawString>Grzegorz Chrupała, Georgiana Dinu, and Josef van Genabith. 2008. Learning morphology with morfette. In Proceedings of LREC 2008, Marrakech, Morocco. ELDA/ELRA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Constant</author>
<author>Isabelle Tellier</author>
</authors>
<title>Evaluating the impact of external lexical resources into a crf-based multiword segmenter and part-of-speech tagger.</title>
<date>2012</date>
<booktitle>In Proceedings of LREC 2012,</booktitle>
<location>Istanbul, Turkey.</location>
<contexts>
<context position="21100" citStr="Constant and Tellier, 2012" startWordPosition="3366" endWordPosition="3370">e, the maximum number of non overlapping MWEs according to the lexicons are systematically marked as such. We obtain about 70% recall and 50% precision with respect to MWE spanning. 5.2 MINE Analyzer The MWE analyzer is a CRF-based sequential labeler, which, given a tokenized text, jointly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually helping each other9. The MWE analyzer integrates, among others, features computed from the external lexicons described in section 5.1, which greatly improve POS tagging (Denis and Sagot, 2009) and MWE segmentation (Constant and Tellier, 2012). The MWE analyzer also jointly classifies its predicted MWEs as regular or irregular (the distinction being learnt on gold training set, with structured MWEs cf. section 3.2). 5.3 MINE-specific features We introduce information from the external MWE resources in different ways: Flat MINE features: MWE information can be integrated as features to be used by the dependency parser. We tested to incorporate the MWE-specific features as defined in the gold flat representation (section 3.1): the mwehead=POS feature for the MWE head token, POS being the part-of-speech of the MWE; the component=y fea</context>
</contexts>
<marker>Constant, Tellier, 2012</marker>
<rawString>Matthieu Constant and Isabelle Tellier. 2012. Evaluating the impact of external lexical resources into a crf-based multiword segmenter and part-of-speech tagger. In Proceedings of LREC 2012, Istanbul, Turkey.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Constant</author>
<author>Anthony Sigogne</author>
<author>Patrick Watrin</author>
</authors>
<title>Discriminative strategies to integrate multiword expression recognition and parsing.</title>
<date>2012</date>
<booktitle>In Proceedings ofACL 2012,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="2040" citStr="Constant et al. (2012)" startWordPosition="296" endWordPosition="299">011). But the situation is much less clear when switching to automatic MWE prediction. While Cafferkey et al. (2007) report a small improvement on the pure parsing 1Multiword expressions can be roughly defined as continuous or discontinuous sets of tokens, which either do not exhibit full freedom in lexical selection or whose meaning is not fully compositional. We focus in this paper on contiguous multiword expressions, also known as “words with spaces”. Matthieu Constant Universit´e Paris-Est LIGM CNRS Matthieu.Constant@ u-pem.fr task when using external MWE lexicons to help English parsing, Constant et al. (2012) report results on the joint MWE recognition and parsing task, in which errors in MWE recognition alleviate their positive effect on parsing performance. While the realistic scenario of syntactic parsing with automatic MWE recognition (either done jointly or in a pipeline) has already been investigated in constituency parsing (Green et al., 2011; Constant et al., 2012; Green et al., 2013), the French dataset of the SPMRL 2013 Shared Task (Seddah et al., 2013) only recently provided the opportunity to evaluate this scenario within the framework of dependency syntax.2 In such a scenario, a syste</context>
</contexts>
<marker>Constant, Sigogne, Watrin, 2012</marker>
<rawString>Matthieu Constant, Anthony Sigogne, and Patrick Watrin. 2012. Discriminative strategies to integrate multiword expression recognition and parsing. In Proceedings ofACL 2012, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Matthieu Constant</author>
<author>Marie Candito</author>
<author>Djam´e Seddah</author>
</authors>
<title>The ligm-alpage architecture for the spmrl 2013 shared task: Multiword expression analysis and dependency parsing.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Parsing ofMorphologically Rich Languages: Shared Task,</booktitle>
<location>Seattle, WA.</location>
<contexts>
<context position="4701" citStr="Constant et al. (2013)" startWordPosition="715" endWordPosition="718">nd, USA, June 23-25 2014. c�2014 Association for Computational Linguistics for predicting both syntax and MWEs. Section 5 presents the external resources targeted to improve MWE recognition. We describe experiments and discuss their results in section 6 and conclude in section 7. 2 Related work We gave in introduction references to previous work on predicting MWEs and constituency parsing. To our knowledge, the first works3 on predicting both MWEs and dependency trees are those presented to the SPMRL 2013 Shared Task that provided scores for French (which is the only dataset containing MWEs). Constant et al. (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. Our contribution with respect to that work is the representation of the internal syntactic structure of MWEs, and use of MWE-specific features for the joint system. The system of Bj¨orkelund et al. (2013) ranked second on French, though with close UAS/LAS scores. It is a less language-specific system that reranks nbest dependency parses from 3 parsers, informed with features from predicted constituency trees. It uses no feature nor treatment specific to MWEs as it focuses</context>
<context position="18149" citStr="Constant et al., 2013" startWordPosition="2898" endWordPosition="2901">is with classification of MWEs into regular or irregular, (ii) merge of predicted irregular MWEs, (iii) tagging and morphological prediction, (iv) parsing JOUTIRREG architecture: • .training set: flat representation of irregular MWEs, with label suffixing (dep_cpd_ POS), structured representation of regular MWEs without label suffixing. • parsing: (i) MWE analysis and classification into regular or irregular, used for MWEspecific features, (ii) tagging and morphological prediction, (iii) parsing, We compare these four architectures between them and also with two simpler architectures used by (Constant et al., 2013) within the SPMRL 13 Shared Task, in which regular and irregular MWEs are not distinguished: Uniform joint architecture: The joint systems perform syntactic parsing and MWE analysis via a single dependency parser, using representations as in 3.3. Uniform pipeline architecture: • training set: MWEs merged into one token • parsing: (i) MWE analysis, (ii) merge ofpredicted MWEs, (iii) tagging and morphological prediction, (iv) parsing For each architecture, we apply the appropriate normalization procedures on the predicted parses, in order to evaluate against (i) the pseudo-gold data in structure</context>
<context position="32170" citStr="Constant et al., 2013" startWordPosition="5150" endWordPosition="5154">t differences in LAS between the labeled and the flat representations are due to side effects of errors on MWE status: some wrong reattachments performed to obtain flat representation decrease the UAS, but also in some cases the LAS. DEV TEST System UAS LAS UAS LAS reg/irreg joint 88.79 85.15 88.13 84.48 Bjork13 88.30 84.84 87.87 84.37 Const13 pipeline 88.73 85.28 88.35 84.91 Const13 joint 88.21 84.60 87.76 84.14 uniform joint 88.81 85.42 87.96 84.59 Table 5: Comparison on dev set of our best architecture with reg/irregular MWE distinction (first row), with the single-parser architectures of (Constant et al., 2013) (Const13) and (Bj¨orkelund et al., 2013) (Bjork13). Uniform joint is our reimplementation of Const13 joint, enhanced with mwefeatures and switch. IRREG and between JOINT-REG and JOINTIRREG. JOINT slightly outperforms JOINT-REG (p &lt; 0.05). On the structured representation, the two best systems (JOINT and JOINT-IRREG) significantly outperform the other systems (p &lt; 0.01 for all; p &lt; 0.05 for JOINT-REG). Moreover, we provide in table 5 a comparison of our best architecture with reg/irregular MWE distinction with other architectures that do not make this distinction, namely the two best comparabl</context>
</contexts>
<marker>Constant, Candito, Seddah, 2013</marker>
<rawString>Matthieu Constant, Marie Candito, and Djam´e Seddah. 2013. The ligm-alpage architecture for the spmrl 2013 shared task: Multiword expression analysis and dependency parsing. In Proceedings of the 4th Workshop on Statistical Parsing ofMorphologically Rich Languages: Shared Task, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blandine Courtois</author>
<author>Myl`ene Garrigues</author>
<author>Gaston Gross</author>
<author>Maurice Gross</author>
<author>Ren´e Jung</author>
<author>Mathieu-Colas Michel</author>
<author>Anne Monceaux</author>
<author>Anne Poncet-Montange</author>
<author>Max Silberztein</author>
<author>Robert Viv´es</author>
</authors>
<title>Dictionnaire ´electronique DELAC : les mots compos´es binaires.</title>
<date>1997</date>
<tech>Technical Report 56,</tech>
<institution>University Paris 7, LADL.</institution>
<marker>Courtois, Garrigues, Gross, Gross, Jung, Michel, Monceaux, Poncet-Montange, Silberztein, Viv´es, 1997</marker>
<rawString>Blandine Courtois, Myl`ene Garrigues, Gaston Gross, Maurice Gross, Ren´e Jung, Mathieu-Colas Michel, Anne Monceaux, Anne Poncet-Montange, Max Silberztein, and Robert Viv´es. 1997. Dictionnaire ´electronique DELAC : les mots compos´es binaires. Technical Report 56, University Paris 7, LADL.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Blandine Courtois</author>
</authors>
<title>Un syst`eme de dictionnaires ´electroniques pour les mots simples du franc¸ais. Langue Franc¸aise,</title>
<date>2009</date>
<pages>87--11</pages>
<contexts>
<context position="19622" citStr="Courtois, 2009" startWordPosition="3129" endWordPosition="3130">eatures (section 5.3) to guide the MWE-aware dependency parser. Moreover, in some of the architectures, the external MWE analyzer is used either to pre-group irregular MWEs (for the architectures using IRREGMERGED), or to post-annotate regular MWEs. 5.1 MWE lexicons MWE lexicons are exploited as sources of features for both the dependency parser and the external MWE analyzer. In particular, two largecoverage general-language lexicons are used: the Lefff6 lexicon (Sagot, 2010), which contains approximately half a million inflected word forms, among which approx. 25,000 are MWEs; and the DELA7 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110,000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex8: the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names. Note that the lexicons do not include any information on the irregular or the regular status of the MWEs. In order to compare the MWEs present in the lexicons and those encoded in the French treebank, we applied the following procedure (hereafter called lexicon 6We use the version available in the </context>
</contexts>
<marker>Courtois, 2009</marker>
<rawString>Blandine Courtois. 2009. Un syst`eme de dictionnaires ´electroniques pour les mots simples du franc¸ais. Langue Franc¸aise, 87:11–22.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Pascal Denis</author>
<author>Benoit Sagot</author>
</authors>
<title>Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less human effort.</title>
<date>2009</date>
<booktitle>In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC’09),</booktitle>
<location>Hong Kong.</location>
<contexts>
<context position="20261" citStr="Denis and Sagot, 2009" startWordPosition="3234" endWordPosition="3238">, 1997) lexicon, which contains approx. one million inflected forms, among which about 110,000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex8: the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names. Note that the lexicons do not include any information on the irregular or the regular status of the MWEs. In order to compare the MWEs present in the lexicons and those encoded in the French treebank, we applied the following procedure (hereafter called lexicon 6We use the version available in the POS tagger MElt (Denis and Sagot, 2009). 7We use the version in the platform Unitex (http://igm.univ-mlv.fr/˜unitex). We had to convert the DELA POS tagset to that of the French Treebank. 8http://igm.univ-mlv.fr/˜unitex 747 lookup): in a given sentence, the maximum number of non overlapping MWEs according to the lexicons are systematically marked as such. We obtain about 70% recall and 50% precision with respect to MWE spanning. 5.2 MINE Analyzer The MWE analyzer is a CRF-based sequential labeler, which, given a tokenized text, jointly performs MWE segmentation and POS tagging (of simple tokens and of MWEs), both tasks mutually hel</context>
</contexts>
<marker>Denis, Sagot, 2009</marker>
<rawString>Pascal Denis and Benoit Sagot. 2009. Coupling an annotated corpus and a morphosyntactic lexicon for state-of-the-art POS tagging with less human effort. In Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation (PACLIC’09), Hong Kong.</rawString>
</citation>
<citation valid="true">
<authors>
<author>G¨uls¸en Eryiˇgit</author>
</authors>
<title>Tugay Ilbay, and Ozan Arkan Can.</title>
<date>2011</date>
<booktitle>In Proceedings of the IWPT Workshop on Statistical Parsing ofMorphologically-Rich Languages (SPMRL’11),</booktitle>
<location>Dublin, Ireland.</location>
<marker>Eryiˇgit, 2011</marker>
<rawString>G¨uls¸en Eryiˇgit, Tugay Ilbay, and Ozan Arkan Can. 2011. Multiword expressions in statistical dependency parsing. In Proceedings of the IWPT Workshop on Statistical Parsing ofMorphologically-Rich Languages (SPMRL’11), Dublin, Ireland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Marie-Catherine de Marneffe</author>
<author>John Bauer</author>
<author>Christofer D Manning</author>
</authors>
<title>Multiword expression identification with tree substitution grammars: A parsing tour de force with french.</title>
<date>2011</date>
<booktitle>In Proceedings ofEMNLP 2011,</booktitle>
<location>Edinburgh, Scotland.</location>
<marker>Green, de Marneffe, Bauer, Manning, 2011</marker>
<rawString>Spence Green, Marie-Catherine de Marneffe, John Bauer, and Christofer D. Manning. 2011. Multiword expression identification with tree substitution grammars: A parsing tour de force with french. In Proceedings ofEMNLP 2011, Edinburgh, Scotland.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Spence Green</author>
<author>Marie-Catherine de Marneffe</author>
<author>Christopher D Manning</author>
</authors>
<title>Parsing models for identifying multiword expressions.</title>
<date>2013</date>
<journal>Computational Linguistics,</journal>
<volume>39</volume>
<issue>1</issue>
<marker>Green, de Marneffe, Manning, 2013</marker>
<rawString>Spence Green, Marie-Catherine de Marneffe, and Christopher D Manning. 2013. Parsing models for identifying multiword expressions. Computational Linguistics, 39(1):195–227.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Joakim Nivre</author>
<author>Jens Nilsson</author>
</authors>
<title>Multiword units in syntactic parsing.</title>
<date>2004</date>
<booktitle>In Proceedings of the LREC Workshop: Methodologies and Evaluation ofMultiword Units in Real-World Applications (MEMURA),</booktitle>
<location>Lisbon, Portugal.</location>
<contexts>
<context position="1398" citStr="Nivre and Nilsson, 2004" startWordPosition="197" endWordPosition="201">ructure of MWEs. This can be useful for capturing the various degrees of semantic compositionality of MWEs. 1 Introduction A real-life parsing system should comprise the recognition of multi-word expressions (MWEs1), first because downstream semantic-oriented applications need some marking in order to distinguish between regular semantic composition and the typical semantic non-compositionality of MWEs. Second, MWE information, is intuitively supposed to help parsing. That intuition is confirmed in a classical but non-realistic setting in which gold MWEs are pregrouped (Arun and Keller, 2005; Nivre and Nilsson, 2004; Eryiˇgit et al., 2011). But the situation is much less clear when switching to automatic MWE prediction. While Cafferkey et al. (2007) report a small improvement on the pure parsing 1Multiword expressions can be roughly defined as continuous or discontinuous sets of tokens, which either do not exhibit full freedom in lexical selection or whose meaning is not fully compositional. We focus in this paper on contiguous multiword expressions, also known as “words with spaces”. Matthieu Constant Universit´e Paris-Est LIGM CNRS Matthieu.Constant@ u-pem.fr task when using external MWE lexicons to he</context>
</contexts>
<marker>Nivre, Nilsson, 2004</marker>
<rawString>Joakim Nivre and Jens Nilsson. 2004. Multiword units in syntactic parsing. In Proceedings of the LREC Workshop: Methodologies and Evaluation ofMultiword Units in Real-World Applications (MEMURA), Lisbon, Portugal.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Odile Piton</author>
<author>Denis Maurel</author>
<author>Claude Belleil</author>
</authors>
<title>The prolex data base : Toponyms and gentiles for nlp.</title>
<date>1999</date>
<booktitle>In Proceedings of the Third International Workshop on Applications of Natural Language to Data Bases (NLDB’99),</booktitle>
<location>Klagenfurt, Austria.</location>
<contexts>
<context position="19889" citStr="Piton et al., 1999" startWordPosition="3169" endWordPosition="3172">cons MWE lexicons are exploited as sources of features for both the dependency parser and the external MWE analyzer. In particular, two largecoverage general-language lexicons are used: the Lefff6 lexicon (Sagot, 2010), which contains approximately half a million inflected word forms, among which approx. 25,000 are MWEs; and the DELA7 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110,000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex8: the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names. Note that the lexicons do not include any information on the irregular or the regular status of the MWEs. In order to compare the MWEs present in the lexicons and those encoded in the French treebank, we applied the following procedure (hereafter called lexicon 6We use the version available in the POS tagger MElt (Denis and Sagot, 2009). 7We use the version in the platform Unitex (http://igm.univ-mlv.fr/˜unitex). We had to convert the DELA POS tagset to that of the French Treebank. 8http://igm.univ-mlv.fr/˜unitex 747 lookup): in a given sentence, the maximum n</context>
</contexts>
<marker>Piton, Maurel, Belleil, 1999</marker>
<rawString>Odile Piton, Denis Maurel, and Claude Belleil. 1999. The prolex data base : Toponyms and gentiles for nlp. In Proceedings of the Third International Workshop on Applications of Natural Language to Data Bases (NLDB’99), Klagenfurt, Austria.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Kenji Sagae</author>
<author>Alon Lavie</author>
</authors>
<title>Parser combination by reparsing.</title>
<date>2006</date>
<booktitle>In Proceedings of NAACL/HLT 2006, Companion Volume: Short Papers,</booktitle>
<location>Stroudsburg, PA, USA.</location>
<contexts>
<context position="4786" citStr="Sagae and Lavie, 2006" startWordPosition="729" endWordPosition="732">ting both syntax and MWEs. Section 5 presents the external resources targeted to improve MWE recognition. We describe experiments and discuss their results in section 6 and conclude in section 7. 2 Related work We gave in introduction references to previous work on predicting MWEs and constituency parsing. To our knowledge, the first works3 on predicting both MWEs and dependency trees are those presented to the SPMRL 2013 Shared Task that provided scores for French (which is the only dataset containing MWEs). Constant et al. (2013) proposed to combine pipeline and joint systems in a reparser (Sagae and Lavie, 2006), and ranked first at the Shared Task. Our contribution with respect to that work is the representation of the internal syntactic structure of MWEs, and use of MWE-specific features for the joint system. The system of Bj¨orkelund et al. (2013) ranked second on French, though with close UAS/LAS scores. It is a less language-specific system that reranks nbest dependency parses from 3 parsers, informed with features from predicted constituency trees. It uses no feature nor treatment specific to MWEs as it focuses on the general aim of the Shared Task, namely coping with prediction of morphologica</context>
</contexts>
<marker>Sagae, Lavie, 2006</marker>
<rawString>Kenji Sagae and Alon Lavie. 2006. Parser combination by reparsing. In Proceedings of NAACL/HLT 2006, Companion Volume: Short Papers, Stroudsburg, PA, USA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Benoit Sagot</author>
</authors>
<title>The lefff, a freely available, accurate and large-coverage lexicon for french.</title>
<date>2010</date>
<booktitle>In Proceedings ofLREC 2010,</booktitle>
<location>Valletta,</location>
<contexts>
<context position="19488" citStr="Sagot, 2010" startWordPosition="3108" endWordPosition="3109">WE prediction, we use external MWE resources, namely MWE lexicons and an MWE analyzer. Both resources help to predict MWE-specific features (section 5.3) to guide the MWE-aware dependency parser. Moreover, in some of the architectures, the external MWE analyzer is used either to pre-group irregular MWEs (for the architectures using IRREGMERGED), or to post-annotate regular MWEs. 5.1 MWE lexicons MWE lexicons are exploited as sources of features for both the dependency parser and the external MWE analyzer. In particular, two largecoverage general-language lexicons are used: the Lefff6 lexicon (Sagot, 2010), which contains approximately half a million inflected word forms, among which approx. 25,000 are MWEs; and the DELA7 (Courtois, 2009; Courtois et al., 1997) lexicon, which contains approx. one million inflected forms, among which about 110,000 are MWEs. These resources are completed with specific lexicons freely available in the platform Unitex8: the toponym dictionary Prolex (Piton et al., 1999) and a dictionary of first names. Note that the lexicons do not include any information on the irregular or the regular status of the MWEs. In order to compare the MWEs present in the lexicons and th</context>
<context position="23588" citStr="Sagot, 2010" startWordPosition="3770" endWordPosition="3771">econd-order graph-based parser available in Mate-tools12 (Bohnet, 2010). We used the Anna3.3 version, in projective mode, with default feature sets and parameters proposed in the documentation, augmented or not with MWE-specific features, depending on the experiments. Morphological prediction: Predicted lemmas, POS and morphology features are computed with Morfette version 0.3.5 (Chrupała et al., 2008; Seddah et al., 2010)13, using 10 iterations for the tagging perceptron, 3 iterations for the lemmatization perceptron, default beam size for the decoding of the joint prediction, and the Lefff (Sagot, 2010) as external lexicon used for out-of-vocabulary words. We performed a 10-fold jackknifing on the training corpus. Evaluation metrics: we evaluate our parsing systems by using the standard metrics for dependency parsing: Labeled Attachment Score (LAS) and Unlabeled Attachment Score (UAS), computed using all tokens including punctuation. To evaluate statistical significance of parsing performance differences, we use eval07.pl14 with -b option, and then Dan Bikel’s comparator.15 For MWEs, we use the Fmeasure for recognition of untagged MWEs (hereafter FUM) and for recognition of tagged MWEs (here</context>
</contexts>
<marker>Sagot, 2010</marker>
<rawString>Benoit Sagot. 2010. The lefff, a freely available, accurate and large-coverage lexicon for french. In Proceedings ofLREC 2010, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Djam´e Seddah</author>
<author>Grzegorz Chrupała</author>
<author>Ozlem Cetinoglu</author>
<author>Josef van Genabith</author>
<author>Marie Candito</author>
</authors>
<title>Lemmatization and statistical lexicalized parsing of morphologically-rich languages.</title>
<date>2010</date>
<booktitle>In Proceedings of the NAACL/HLT Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2010),</booktitle>
<location>Los Angeles, CA.</location>
<marker>Seddah, Chrupała, Cetinoglu, van Genabith, Candito, 2010</marker>
<rawString>Djam´e Seddah, Grzegorz Chrupała, Ozlem Cetinoglu, Josef van Genabith, and Marie Candito. 2010. Lemmatization and statistical lexicalized parsing of morphologically-rich languages. In Proceedings of the NAACL/HLT Workshop on Statistical Parsing of Morphologically Rich Languages (SPMRL 2010), Los Angeles, CA.</rawString>
</citation>
<citation valid="false">
<authors>
<author>Djam´e Seddah</author>
<author>Reut Tsarfaty</author>
<author>Sandra K´’ubler</author>
<author>Marie Candito</author>
<author>Jinho Choi</author>
<author>Rich´ard Farkas</author>
<author>Jennifer Foster</author>
</authors>
<title>Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, Alina Wr´oblewska, and Eric Villemonte de la Cl´ergerie.</title>
<date>2013</date>
<booktitle>In Proceedings of the 4th Workshop on Statistical Parsing ofMorphologically Rich Languages: Shared Task,</booktitle>
<location>Seattle, WA.</location>
<marker>Seddah, Tsarfaty, K´’ubler, Candito, Choi, Farkas, Foster, 2013</marker>
<rawString>Djam´e Seddah, Reut Tsarfaty, Sandra K´’ubler, Marie Candito, Jinho Choi, Rich´ard Farkas, Jennifer Foster, Iakes Goenaga, Koldo Gojenola, Yoav Goldberg, Spence Green, Nizar Habash, Marco Kuhlmann, Wolfgang Maier, Joakim Nivre, Adam Przepiorkowski, Ryan Roth, Wolfgang Seeker, Yannick Versley, Veronika Vincze, Marcin Woli´nski, Alina Wr´oblewska, and Eric Villemonte de la Cl´ergerie. 2013. Overview of the spmrl 2013 shared task: A cross-framework evaluation of parsing morphologically rich languages. In Proceedings of the 4th Workshop on Statistical Parsing ofMorphologically Rich Languages: Shared Task, Seattle, WA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
</authors>
<title>D´ora Szauter, Attila Alm´asi, Gy¨orgy M´ora, Zolt´an Alexin, and J´anos Csirik.</title>
<date>2010</date>
<booktitle>In Proceedings of LREC 2010,</booktitle>
<location>Valletta,</location>
<marker>Vincze, 2010</marker>
<rawString>Veronika Vincze, D´ora Szauter, Attila Alm´asi, Gy¨orgy M´ora, Zolt´an Alexin, and J´anos Csirik. 2010. Hungarian dependency treebank. In Proceedings of LREC 2010, Valletta, Malta.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Veronika Vincze</author>
<author>J´anos Zsibrita</author>
<author>Istv`an Nagy T</author>
</authors>
<title>Dependency parsing for identifying hungarian light verb constructions.</title>
<date>2013</date>
<booktitle>In Proceedings of International Joint Conference on Natural Language Processing (IJCNLP 2013),</booktitle>
<location>Nagoya, Japan.</location>
<contexts>
<context position="5916" citStr="Vincze et al., 2013" startWordPosition="911" endWordPosition="914">cuses on the general aim of the Shared Task, namely coping with prediction of morphological and syntactic analysis. Concerning related work on the representation of MWE internal structure, we can cite the Prague Dependency Bank, which captures both regular syntax of non-compositional MWEs and their MWE status, in two distinct annotation layers (Bejˇcek and Stranak, 2010). Our representation also resembles that of light-verb constructions (LVC) in the hungarian dependency treebank (Vincze et al., 2010): the construction has regular syntax, and a suffix is used on labels to express it is a LVC (Vincze et al., 2013). 3 Data: MWEs in Dependency Trees The data we use is the SPMRL 13 dataset for French, in dependency format. It contains projective dependency trees that were automatically derived from the latest status of the French Treebank (Abeill´e and Barrier, 2004), which consists of constituency trees for sentences from the 3Concerning non contiguous MWEs, we can cite the work of Vincze et al. (2013), who experimented joint dependency parsing and light verb construction identification. newspaper Le Monde, manually annotated with phrase structures, morphological information, and grammatical functional t</context>
</contexts>
<marker>Vincze, Zsibrita, T, 2013</marker>
<rawString>Veronika Vincze, J´anos Zsibrita, and Istv`an Nagy T. 2013. Dependency parsing for identifying hungarian light verb constructions. In Proceedings of International Joint Conference on Natural Language Processing (IJCNLP 2013), Nagoya, Japan.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>