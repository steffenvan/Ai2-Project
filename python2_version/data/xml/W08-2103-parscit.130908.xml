<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000000">
<title confidence="0.875072">
A Fast Boosting-based Learner for Feature-Rich Tagging and Chunking
</title>
<author confidence="0.847535">
Tomoya Iwakura Seishi Okamoto
</author>
<affiliation confidence="0.811438">
Fujitsu Laboratories Ltd.
</affiliation>
<address confidence="0.940709">
1-1, Kamikodanaka 4-chome, Nakahara-ku, Kawasaki 211-8588, Japan
</address>
<email confidence="0.996822">
{iwakura.tomoya,seishi}@jp.fujitsu.com
</email>
<sectionHeader confidence="0.994752" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999708275862069">
Combination of features contributes to a
significant improvement in accuracy on
tasks such as part-of-speech (POS) tag-
ging and text chunking, compared with us-
ing atomic features. However, selecting
combination of features on learning with
large-scale and feature-rich training data
requires long training time. We propose a
fast boosting-based algorithm for learning
rules represented by combination of fea-
tures. Our algorithm constructs a set of
rules by repeating the process to select sev-
eral rules from a small proportion of can-
didate rules. The candidate rules are gen-
erated from a subset of all the features with
a technique similar to beam search. Then
we propose POS tagging and text chunk-
ing based on our learning algorithm. Our
tagger and chunker use candidate POS tags
or chunk tags of each word collected from
automatically tagged data. We evaluate
our methods with English POS tagging and
text chunking. The experimental results
show that the training time of our algo-
rithm are about 50 times faster than Sup-
port Vector Machines with polynomial ker-
nel on the average while maintaining state-
of-the-art accuracy and faster classification
speed.
</bodyText>
<sectionHeader confidence="0.999158" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998660833333333">
Several boosting-based learning algorithms have
been applied to Natural Language Processing
problems successfully. These include text catego-
rization (Schapire and Singer, 2000), Natural Lan-
guage Parsing (Collins and Koo, 2005), English
syntactic chunking (Kudo et al., 2005) and so on.
</bodyText>
<note confidence="0.723388">
© 2008. Licensed under the Creative Commons
</note>
<footnote confidence="0.967553333333333">
Attribution-Noncommercial-Share Alike 3.0 Unported li-
cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.
</footnote>
<bodyText confidence="0.999010702702703">
Furthermore, classifiers based on boosting-
based learners have shown fast classification speed
(Kudo et al., 2005).
However, boosting-based learning algorithms
require long training time. One of the reasons is
that boosting is a method to create a final hypoth-
esis by repeatedly generating a weak hypothesis in
each training iteration with a given weak learner.
These weak hypotheses are combined as the fi-
nal hypothesis. Furthermore, the training speed
of boosting-based algorithms becomes more of a
problem when considering combination of features
that contributes to improvement in accuracy.
This paper proposes a fast boosting-based algo-
rithm for learning rules represented by combina-
tion of features. Our learning algorithm uses the
following methods to learn rules from large-scale
training samples in a short time while maintaining
accuracy; 1) Using a rule learner that learns sev-
eral rules as our weak learner while ensuring a re-
duction in the theoretical upper bound of the train-
ing error of a boosting algorithm, 2) Repeating to
learn rules from a small proportion of candidate
rules that are generated from a subset of all the fea-
tures with a technique similar to beam search, 3)
Changing subsets of features used by weak learner
dynamically for alleviating overfitting.
We also propose feature-rich POS tagging and
text chunking based on our learning algorithm.
Our POS tagger and text chunker use candidate
tags of each word obtained from automatically
tagged data as features.
The experimental results with English POS tag-
ging and text chunking show drastically improve-
ment of training speeds while maintaining compet-
itive accuracy compared with previous best results
and fast classification speeds.
</bodyText>
<sectionHeader confidence="0.997879" genericHeader="method">
2 Boosting-based Learner
</sectionHeader>
<subsectionHeader confidence="0.966797">
2.1 Preliminaries
</subsectionHeader>
<bodyText confidence="0.9835168">
We describe the problem treated by our boosting-
based learner as follows. Let X be the set of ex-
amples and Y be a set of labels {−1, +1}. Let
F = {f1, f2,..., fm} be M types of features rep-
resented by strings. Let S be a set of training sam-
</bodyText>
<page confidence="0.991686">
17
</page>
<note confidence="0.9343915">
CoNLL 2008: Proceedings of the 12th Conference on Computational Natural Language Learning, pages 17–24
Manchester, August 2008
</note>
<figure confidence="0.540836944444444">
## S = {(xi, yi)}mi=1 : xi ⊆ X,yi ∈ {±1}
## a smoothing value a =1
## rule number r: the initial value is 1.
Initialize: For i=1,...,m: w1,i = exp(12 log(W+1
W�1 ));
While (r ≤ R)
## Train weak-learner using (S, {wr,i}mi=1)
## Get v types of rules: {fj}ν j=1
{fj}ν j=1 ← weak-learner(S,{wr,i}m i=1);
## Update weights with confidence value
Foreachf /E {fj}νj=1
c = 2loglWr,±1(f)+ε)
For i=1,...,m: wr+1,i = wr,i exp(−yih(f,c))
fr = f; cr = c; r++;
endForeach
endWhile
Output: F(x) = sign(log(W+1
W�1 ) + ERr=1h(fr,cr)(x))
</figure>
<figureCaption confidence="0.999852">
Figure 1: A generalized version of our learner
</figureCaption>
<bodyText confidence="0.99571">
ples {(x1, y1), ..., (xm, ym)}, where each example
xi ∈ X consists of features in F, which we call a
feature-set, and yi ∈ Y is a class label. The goal is
to induce a mapping
</bodyText>
<equation confidence="0.962076">
F : X → Y
from S.
</equation>
<bodyText confidence="0.942355263157895">
Let |xi |(0 &lt; |xi |≤ M) be the number of fea-
tures included in a feature-set xi, which we call
the size of xi, and xi,j ∈ F (1 ≤ j ≤ |xi |) be a
feature included in xi. 1 We call a feature-set of
size k a k-feature-set. Then we define subsets of
feature-sets as follows.
Definition 1 Subsets offeature-sets
If a feature-set xj contains all the features in a
feature-set xi, then we call xi is a subset of xj and
denote it as
xi ⊆ xj.
Then we define weak hypothesis based on the
idea of the real-valued predictions and abstaining
(RVPA, for short) (Schapire and Singer, 2000). 2
Definition 2 Weak hypothesis for feature-sets
Let f be a feature-set, called a rule, x be a
feature-set, and c be a real number, called a con-
fidence value, then a weak-hypothesis for feature-
sets is defined as
</bodyText>
<footnote confidence="0.9847957">
1Our learner can handle binary vectors as in (Morishita,
2002). When our learner treats binary vectors for M attributes
{X1,...,Xm}, the learner converts each vector to the corre-
sponding feature-set as xi ← {fi|Xi,j ∈ Xi ∧ Xi,j = 1}
(1 ≤ i ≤ m, 1 ≤ j ≤ M).
2We use the RVPA because training with RVPA is faster
than training with Real-valued-predictions (RVP) while main-
taining competitive accuracy (Schapire and Singer, 2000).
The idea of RVP is to output a confidence value for samples
which do not satisfy the given condition too.
</footnote>
<subsectionHeader confidence="0.99816">
2.2 Boosting-based Rule Learning
</subsectionHeader>
<bodyText confidence="0.99988">
Our boosting-based learner selects R types of rules
for creating a final hypothesis F on several training
iterations. The F is defined as
</bodyText>
<equation confidence="0.699890846153846">
F(x) = sign(ERr=1h(fr,cr)(x)).
We use a learning algorithm that generates
several rules from a given training samples
S = {(xi, yi)}mi=1 and weights over samples
{wr,1, ..., wr,m} as input of our weak learner. wr,i
is the weight of sample number i after selecting
r −1 types of rules, where 0&lt;wr,i, 1 ≤ i ≤ m and
1 ≤ r ≤ R.
Given such input, the weak learner selects v
types of rules {fj}ν j=1 (fj ⊆ F) with gain:
� gain(f) def=  |Wr,+1(f) − Wr,�1(f)|,
where f is a feature-set, and Wr,y(f) is
Wr,y(f) = Emi=1 wr,i[[f ⊆ xi ∧ yi = y]],
</equation>
<bodyText confidence="0.9910283">
and [[7r]] is 1 if a proposition 7r holds and 0 other-
wise.
The weak learner selects a feature-set having the
highest gain as the first rule, and the weak learner
finally selects v types of feature-sets having gain
in top v as {fj}ν j=1 at each boosting iteration.
Then the boosting-based learner calculates the
confidence value of each f in {fj}ν j=1 and updates
the weight of each sample. The confidence value
cj for fj is defined as
</bodyText>
<equation confidence="0.997515">
cj = 2log( Wr,+1(fj)
1 Wr,�1(fj)).
</equation>
<bodyText confidence="0.9960635">
After the calculation of cj for fj, the learner up-
dates the weight of each sample with
</bodyText>
<equation confidence="0.997494">
wr+1,i = wr,iexp(−yih(fj,cj)). (1)
</equation>
<bodyText confidence="0.994676217391304">
Then the learner adds (fj, cj) to F as the r-
th rule and its confidence value. 3 When we
calculate the confidence value cj+1 for fj+1, we
use {wr+1,1, ..., wr+1,m}. The learner adds (fj+1,
cj+1) to F as the r+1-th rule and confidence value.
After the updates of weights with {fj}ν j=1, the
learner starts the next boosting iteration. The
learner continues training until obtaining R rules.
Our boosting-based algorithm differs from the
other boosting algorithms in the number of rules
learned at each iteration. The other boosting-based
algorithms usually learn a rule at each iteration
3Eq. (1) is the update of the AdaBoost used in ADTrees
learning algorithm (Freund and Mason, 1999). We use
this AdaBoost by the following two reasons. 1) The pa-
per (Iwakura and Okamoto, 2007) showed that the accuracy
of text chunking with the AdaBoost of ADTrees is slightly
higher than text chunking with the AdaBoost of BoosTexter
for RVPA (Schapire and Singer, 2000), 2) We expect the Ad-
aBoost of ADTrees can realize faster training because this Ad-
aBoost does not normalize weights at each update compared
with the AdaBoost of BoosTexter normalizes weights at each
iteration.
</bodyText>
<equation confidence="0.975794">
�
h(f, c fCx
-)(x) = 0
other
wise.
</equation>
<page confidence="0.998038">
18
</page>
<table confidence="0.981102076923077">
## sortByW(F,fq): Sort features (f ∈ F)
## in ascending order based on weights of features
## (a % b): Return the reminder of (a ÷ b)
## |B|-buckets: B = {B[0], ..., B[|B |− 1]}
procedure distFT(S, |B|)
##Calculate the weight of each feature
Foreach (f∈F) Wr(f) = Pmi=1 wr,i[[{f} ⊆ xi]]
##Sort features based on thier weights and
## store the results in Fs
Fs ← sortByW(F, Wr)
## Distribute features to buckets
For i=0...M : B[(i % |B|)] = (B[(i % |B|)] ∪ Fs[i])
return B
</table>
<figureCaption confidence="0.919688">
Figure 2: Distribute features to buckets based on weights
</figureCaption>
<bodyText confidence="0.997034083333333">
(Schapire and Singer, 2000; Freund and Mason,
1999). Despite the difference, our boosting-based
algorithm ensures a reduction in the theoretical up-
per bound of training error of the AdaBoost. We
list the detailed explanation in Appendix.A.
Figure 1 shows an overview of our boosting-
based rule learner. To avoid to happen that
Wr,+1(f) or Wr,_1(f) is very small or even zero,
we use the smoothed values E (Schapire and
Singer, 1999). Furthermore, to reflect imbalance
class distribution, we use the default rule (Freund
and Mason, 1999), defined as 1 tog( W+1), where
</bodyText>
<equation confidence="0.919376">
2 W−1
Wy = E&apos;`i�1[[yi = y]] for y E {+1}. The initial
weights are defined with the default rule.
</equation>
<sectionHeader confidence="0.990206" genericHeader="method">
3 Fast Rule Learner
</sectionHeader>
<subsectionHeader confidence="0.999916">
3.1 Generating Candidate Rules
</subsectionHeader>
<bodyText confidence="0.999998">
We use a method to generate candidate rules with-
out duplication (Iwakura and Okamoto, 2007).
We denote f&apos; = f + f as the generation of k + 1-
feature-set f&apos; consisting of a feature f and a k-
feature-set f. Let ID(f) be the integer corre-
sponding to f, called id, and 0 be 0-feature-set.
Then we define gen generating a feature-set as
</bodyText>
<equation confidence="0.921796">
_ (f + f if ID(f) &gt; maxID(f&apos;)
gen(f
, f) 0 otherwise f&apos; Ef
</equation>
<bodyText confidence="0.999824571428571">
We assign smaller integer to more infrequent fea-
tures as id. If there are features having the same
frequency, we assign id to each feature with lexi-
cographic order of features. Training based on this
candidate generation showed faster training speed
than generating candidates by an arbitrary order
(Iwakura and Okamoto, 2007).
</bodyText>
<subsectionHeader confidence="0.999939">
3.2 Training with Redistributed Features
</subsectionHeader>
<bodyText confidence="0.999870875">
We propose a method for learning rules by repeat-
ing to select a rule from a small portion of can-
didate rules. We evaluated the effectiveness of
four types of methods to learn a rule from a sub-
set of features on boosting-based learners with a
text chunking task (Iwakura and Okamoto, 2007).
The results showed that Frequency-based distribu-
tion (F-dist) has shown the best accuracy. F-dist
</bodyText>
<construct confidence="0.958021777777778">
## Fk : A set of k-feature-sets
## Ro : v optimal rules (feature-sets)
## Rk,ω : w k-feature-sets for generating candidates
## selectNBest(R, n, S, Wr): n best rules from R
## with gain on {wi,r}mi=1 and training samples S
procedure weak-learner(Fk, S, Wr)
## v best feature-sets as rules
Ro = selectNBest( Ro ∪ Fk, v, S, Wr);
if (S ≤ k) return Ro; ## Size constraint
</construct>
<equation confidence="0.735318888888889">
## w best feature-sets in Fk for generating candidates
Rk,ω = selectNBest(Fk, w, S, Wr);
T = min gain(f); ## The gain of v-th optimal rule
fERo
Foreach ( fk ∈ Rk,ω)
if ( u(fk) &lt; T) continue; ## Upper bound of gain
Foreach (f ∈ F) ## Generate candidates
fk+1 =[gen(fk, f);
/{{
</equation>
<construct confidence="0.964876">
if (S ≤ LST i=1[[fk+1 ⊆ xi]]) Fk+1 = (Fk+1 ∪ fk+1);
end Foreach
end Foreach
return weak-learner(Fk+1, S, W);
</construct>
<figureCaption confidence="0.994846">
Figure 3: Find optimal feature-sets with given weights
</figureCaption>
<bodyText confidence="0.967016333333333">
distributes features to subsets of features, called
buckets, based on frequencies of features.
However, we guess training using a subset of
features depends on how to distribute features to
buckets like online learning algorithms that gener-
ally depend on the order of the training examples
(Kazama and Torisawa, 2007).
To alleviate the dependency on selected buck-
ets, we propose a method that redistributes fea-
tures, called Weight-based distribution (W-dist).
W-dist redistributes features to buckets based on
the weight of f/ [�eature defined as
Wr lf) = L�a&apos; =1 wr,i[[{f} ⊆ xi]]
for each f E F after examining all buckets. Fig-
ure 2 describes an overview of W-dist.
</bodyText>
<subsectionHeader confidence="0.990105">
3.3 Weak Learner for Learning Several Rules
</subsectionHeader>
<bodyText confidence="0.999435666666667">
We propose a weak learner that learns several rules
from a small portion of candidate rules.
Figure 3 describes an overview of the weak
learner. At each iteration, one of the |B|-buckets
is given as an initial 1-feature-sets F1. The weak
learner finds v best feature-sets as rules from can-
didates consisting of F1 and feature-sets generated
from F1. The weak learner generates candidates k-
feature-sets (1 &lt; k) from w best (k-1)-feature-sets
in Fk_1 with gain.
We also use the following pruning techniques
(Morishita, 2002; Kudo et al., 2005).
</bodyText>
<listItem confidence="0.999465666666667">
• Frequency constraint: We examine candidates
seen on at least � different examples.
• Size constraint: We examine candidates whose
size is no greater than a size threshold C.
• Upper bound of gain: We use the upper bound
of gain defined as
</listItem>
<equation confidence="0.744851">
p p
u(f) def = max( Wr,+1(f), Wr,−1(f)).
</equation>
<bodyText confidence="0.982749">
For any feature-set f&apos;CF, which contains f (i.e.
f C f&apos;), the gain(f&apos;) is bounded under u(f), since
05 W,,V(f&apos;) &lt; W,,,(f) for y E {+1}. Thus, if u(f)
</bodyText>
<page confidence="0.923368">
19
</page>
<equation confidence="0.851992">
## S = {(xi, yi)}mi=1 : xiC_X, yi E {+1}
## Wr = {wr,i}mi=1: Weights of samples after learning
## r types of rules. w1,i = 1 (1 &lt; i &lt; m)
## |B |: The size of bucket B = {B[0], ..., B[|B |− 1]}
## b, r : The current bucket and rule number
procedure AdaBoost.SDF()
B = distFT(S, |B|); ## Distributing features into B
## Initialize values and weights:
r=1; b=0; c0= 1log
WW+1 ;
2 g (_1 )
For i = 1,...,m: w1,i = exp(c0);
While (r &lt; R) ## Learning R types of rules
##Select v rules and increment bucket id b
R = weak-learner(B[b], S, Wr); b++;
Foreach (f E R) ##Update weights with each rule
c = 2log( Wr,+1(f)+1
1 Wr,�1(f)+1);
For i=1,..,m wr+1,i = wr,i exp(−yih(f,c));
fr = f; cr = c; r++;
end Foreach
if (b == |B|) ## Redistribution
B = distFT(S, |B|); b=0;
end if
end While
return F(x) = sign(c0 + ERr=1 h(fr,cr)(x))
</equation>
<figureCaption confidence="0.997365">
Figure 4: An overview of AdaBoost.SDF
</figureCaption>
<bodyText confidence="0.793342428571428">
· words, words that are turned into all capitalized,
prefixes and suffixes (up to 4) in a 7-word window.
· labels assigned to three words on the right.
· whether the current word has a hyphen,
a number, a capital letter
· whether the current word is all capital, all small
· candidate POS tags of words in a 7-word window
</bodyText>
<figureCaption confidence="0.995044">
Figure 5: Feature types for POS tagging
</figureCaption>
<bodyText confidence="0.998168">
is less than the gain of the current optimal rule T,
candidates containing f are safely pruned.
Figure 4 describes an overview of our algorithm,
which we call AdaBoost for a weak learner learn-
ing Several rules from Distributed Features (Ad-
aBoost.SDF, for short).
The training of AdaBoost.SDF with (v =
1, W = oc, 1 &lt; |B |) is equivalent to the approach
of AdaBoost.DF (Iwakura and Okamoto, 2007). If
we use (|B |= 1,v = 1), AdaBoost.SDF examines
all features on every iteration like (Freund and Ma-
son, 1999; Schapire and Singer, 2000).
</bodyText>
<sectionHeader confidence="0.949357" genericHeader="method">
4 POS tagging and Text Chunking
</sectionHeader>
<subsectionHeader confidence="0.996818">
4.1 English POS Tagging
</subsectionHeader>
<bodyText confidence="0.998777076923077">
We used the Penn Wall Street Journal treebank
(Marcus et al., 1994). We split the treebank into
training (sections 0-18), development (sections 19-
21) and test (sections 22-24) as in (Collins, 2002).
We used the following candidate POS tags, called
candidate feature, in addition to commonly used
features (Gim´enez and M`arquez, 2003; Toutanova
et al., 2003) shown in Figure 5.
We collect candidate POS tags of each word
from the automatically tagged corpus provided for
the shared task of English Named Entity recog-
nition in CoNLL 2003. 4 The corpus includes
17,003,926 words with POS tags and chunk tags
</bodyText>
<footnote confidence="0.977014">
4http://www.cnts.ua.ac.be/conll2003 /ner/
</footnote>
<bodyText confidence="0.868698388888889">
· words and POS tags in a 5-word window.
· labels assigned to two words on the right.
· candidate chunk tags of words in a 5-word window
Figure 6: Feature types for text chunking
annotated by a POS tagger and a text chunker.
Thus, the corpus includes wrong POS tags and
chunk tags.
We collected candidate POS tags of words that
appear more than 9 times in the corpus. We express
these candidates with one of the following ranges
decided by their frequency fq; 10 &lt; fq &lt; 100,
100 &lt; fq &lt; 1000 and 1000 &lt; fq.
For example, we express ’work’ annotated as
NN 2000 times like “1000&lt;NN”. If ’work’ is cur-
rent word, we add 1000&lt;NN as a candidate POS
tag feature of the current word. If ’work’ appears
the next of the current word, we add 1000&lt;NN as
a candidate POS tag of the next word.
</bodyText>
<subsectionHeader confidence="0.977451">
4.2 Text Chunking
</subsectionHeader>
<bodyText confidence="0.999812318181818">
We used the data prepared for CoNLL-2000 shared
tasks. 5 This task aims to identify 10 types of
chunks, such as, NP, VP and PP, and so on.
The data consists of subsets of Penn Wall Street
Journal treebank; training (sections 15-18) and test
(section 20). We prepared the development set
from section 21 of the treebank as in (Tsuruoka
and Tsujii, 2005). 6
Each base phrase consists of one word or more.
To identify word chunks, we use IOE2 representa-
tion. The chunks are represented by the following
tags: E-X is used for end word of a chunk of class
X. I-X is used for non-end word in an X chunk. O
is used for word outside of any chunk.
For instance, “[He] (NP) [reckons] (VP) [the
current account deficit] (NP)...” is represented by
IOE2 as follows; “He/E-NP reckons/E-VP the/I-
NP current/I-NP account/I-NP deficit/E-NP”.
We used features shown in Figure 6. We col-
lected the followings as candidate chunk tags from
the same automatically tagged corpus used in POS
tagging.
</bodyText>
<listItem confidence="0.9992764">
• Candidate tags expressed with frequency infor-
mation as in POS tagging
• The ranking of each candidate decided by fre-
quencies in the automatically tagged data
• Candidate tags of each word
</listItem>
<bodyText confidence="0.998209">
For example, if we collect “work” anno-
tated as I-NP 2000 times and as E-VP 100
time, we generate the following candidate fea-
tures for “work”; 1000&lt;I-NP, 100&lt;E-VP&lt;1000,
rank:I-NP=1 rank:E-NP=2, candidate=I-NP and
candidate=E-VP.
</bodyText>
<footnote confidence="0.974851333333333">
5http://lcg-www.uia.ac.be/conll2000/chunking/
6We used http://ilk.uvt.nl/˜sabine/chunklink/chunklink 2-2-2000 for conll.pl
for creating development data.
</footnote>
<page confidence="0.988483">
20
</page>
<tableCaption confidence="0.998029">
Table 1: Training data for experiments: 0 of S, M, 0 of
</tableCaption>
<bodyText confidence="0.9495932">
cl and av. 0 of ft indicate the number samples, the distinct
number of feature types, the number of class in each data set,
and the average number of features, respectively. POS and
ETC indicate POS-tagging and text chunking. The “-c” in-
dicates using candidate features collected from parsed unla-
beled data.
We converted the chunk representation of the
automatically tagged corpus to IOE2 and we col-
lected chunk tags of each word appearing more
than nine times.
</bodyText>
<subsectionHeader confidence="0.992756">
4.3 Applying AdaBoost.SDF
</subsectionHeader>
<bodyText confidence="0.999513">
AdaBoost.SDF treats the binary classification
problem. To extend AdaBoost.SDF to multi-class,
we used the one-vs-the-rest method.
To identify proper tag sequences, we use Viterbi
search. We map the confidence value of each clas-
sifier into the range of 0 to 1 with sigmoid function
7, and select a tag sequence which maximizes the
sum of those log values by Viterbi search.
</bodyText>
<sectionHeader confidence="0.999815" genericHeader="method">
5 Experiments
</sectionHeader>
<subsectionHeader confidence="0.994918">
5.1 Experimental Settings
</subsectionHeader>
<bodyText confidence="0.999947260869565">
We compared AdaBoost.SDF with Support Vec-
tor Machines (SVM). SVM has shown good per-
formance on POS tagging (Gim´enez and M`arquez,
2003) and Text Chunking (Kudo and Matsumoto,
2001). Furthermore, SVM with polynomial kernel
implicitly expands all feature combinations with-
out increasing the computational costs. Thus, we
compared AdaBoost.SDF with SVM. 8
To evaluate the effectiveness of candidate fea-
tures, we examined two types of experiments with
candidate features and without them. We list the
statics of training sets in Table 1.
We tested R=100,000, |B|=1,000, v =
{1,10,100}, w={1,10,100,∞}, (={1,2,3}, and
r;={1,5} for AdaBoost.SDF. We tested the soft
margin parameter C={0.1,1,10} and the kernel
degree d={1,2,3} for SVM. 9
We used the followings for comparison; Train-
ing time is time to learn 100,000 rules. Best train-
ing time is time for generating rules to show the
best F-measure (Fβ=1) on development data. Ac-
curacy is Fβ=1 on a test data with the rules at best
training time.
</bodyText>
<equation confidence="0.817949">
7s(X) = 1/(1 + exp(−,QX)), where X = F(x) is a
</equation>
<footnote confidence="0.98110525">
output of a classifier. We used ,Q=5 in this experiment.
8We used TinySVM (http://chasen.org/˜taku/software/TinySVM/).
9We used machines with 2.66 GHz QuadCore Intel Xeon
and 10 GB of memory for all the experiments.
</footnote>
<tableCaption confidence="0.830418">
Table 2: Experimental results of POS tagging and Text
</tableCaption>
<table confidence="0.72902">
Chunking (TC) with candidate features. F and time indicate
the average Fβ=1 of test data and time (hour) to learn 100,000
rules for all classes with F-dist. These results are listed sepa-
rately with respect to each = {1, 51.
</table>
<figureCaption confidence="0.976847">
Figure 7: Accuracy on development data of Text Chunk-
ing (S = 3) obtained with parsers based on F-dist. We mea-
sured accuracy obtained with rules at each training time. The
widest line is AdaBoost.SDF (ν=1,ω=�). The others are Ad-
aBoost.SDF ( ν=10 (e), ν=100(0), ν=1&amp;ω={1,10,100} ).
</figureCaption>
<subsectionHeader confidence="0.99944">
5.2 Effectiveness of Several Rule Learning
</subsectionHeader>
<bodyText confidence="0.998776043478261">
Table 2 shows average accuracy and training time.
We used F-dist as the distribution method. These
average accuracy obtained with rules learned by
AdaBoost.SDF (ν=10) on both tasks are competi-
tive with the average accuracy obtained with rules
learned by AdaBoost.SDF (ν=1). These results
have shown that learning several rules at each iter-
ation contributes significant improvement of train-
ing time. These results have also shown that the
learning several rule at each iteration methods are
more efficient than training by just using the fre-
quency constraint t;.
Figure 7 shows a snapshot for accuracy ob-
tained with chunkers using different number of
rules. This graph shows that chunkers based
on AdaBoost.SDF (ν=10,100) and AdaBoost.SDF
(ν=1,ω=11,10,1001) have shown better accuracy than
chunkers based on AdaBoost.SDF (ν=1,ω=�) at
each training time. These result have shown that
learning several rules at each iteration and learning
combination of features as rules with a technique
similar to beam search are effective in improving
training time while giving a better convergence.
</bodyText>
<figureCaption confidence="0.9480648">
Figure 7 also implies that taggers and chunkers
based on AdaBoost.SDF (ν=100) will show better
or competitive accuracy than accuracy of the oth-
ers by increasing numbers of rules to be learned
while maintaining faster convergence speed.
</figureCaption>
<figure confidence="0.961135869565217">
data 0 of S M 0 of cl av. 0 of ft
POS
POS-c
ETC
ETC-c
912,344 579,052
912,344
211,727
211,727
579,793
92,825
93,333
45
45
22
22
22.09
45.49
35.39
11.37
155.8
93.95
145.3
93.98
195.7
97.23
196.3
97.27
97.23
2.70
93.88
2.69
93.96
22.35
97.17
23.05
0.56
93.14
0.74
93.16
2.91
96.83
2.99
96.82
ν POS(ξ = 1) POS (ξ = 5) TC (ξ = 1) TC (ξ = 5)
F time F time F time F time
</figure>
<equation confidence="0.971554">
ζ=3 ω=∞ ν=1
ζ=3 ω=∞ ν=10
ζ=3 ω=∞ ν=100
ζ=3 ω=1 ν=1
ζ=3 ω=1 ν=10
ζ=3 ω=1 ν=100
ζ=3 ω=10 ν=1
ζ=3 ω=10ν=10
ζ=3 ω=10 ν=100
ζ=3 ω=100 ν=1
ζ=3 ω=100 ν=10
ζ=3 ω=100 ν=100
</equation>
<figure confidence="0.92488025">
0 2 4 6 8 10
Training Time (hour)
1
10
100
94
93
92
91
90
89
88
</figure>
<page confidence="0.998936">
21
</page>
<tableCaption confidence="0.946203571428571">
Table 3: Experimental results on POS tagging and Text
Chunking. Accuracies (Fp=1) on test data and training time
(hour) of AdaBoost.SDF are averages of w={1,10,100,oo} for
each S with F-dist and � = 1. Fp=1 and time (hour) of SVMs
are averages of C={0.1,1,10} for each kernel parameter d.
Table 4: Results obtained with taggers and chunkers based
on F-dist and W-dist. These results obtained with taggers and
</tableCaption>
<bodyText confidence="0.939665666666667">
chunkers trained with w = {1, 10, 100, oo} and S = 2. F
and time indicate average Fp=1 on test data and average best
training time.
</bodyText>
<figure confidence="0.965874142857143">
SVMs
101.63
166.76
96.93
97.15
96.60
625.32
SVMs
170.24
206.39
97.23
97.31
96.76
1346.04
SVMs
8.55
7.38
93.49
93.91
92.14
9.82
</figure>
<table confidence="0.957789736842105">
POS tagging with F-dist
x W=1 W=10 W=100 W=M
F time F time F time F time
1 97.31 30.03 97.31 64.25 97.32 142.9 97.26 89.59
10 97.26 3.21 97.32 9.57 97.30 15.54 97.30 19.64
100 96.86 0.62 96.95 1.32 96.95 2.13 96.96 2.43
POS tagging with W-dist
x W=1 W=10 W=100 W=M
F time F time F time F time
1 97.32 29.96 97.31 57.05 97.31 163.2 97.32 98.71
10 97.24 2.66 97.30 25.70 97.28 16.20 97.29 20.49
100 97.00 0.54 97.02 1.31 97.07 2.22 97.08 2.58
Text Chunking with F-dist
x W=1 W=10 W=100 W=M
F time F time F time F time
1 93.95 7.42 94.30 23.30 94.22 34.74 94.31 21.26
10 93.99 0.98 94.08 2.44 94.19 3.11 94.18 3.18
100 93.32 0.16 93.33 0.32 93.42 0.40 93.42 0.40
Text Chunking with W-dist
x W=1 W=10 W=100 W=M
F time F time F time F time
1 93.99 2.93 94.24 24.77 94.32 35.72 94.32 35.61
10 93.98 0.71 94.30 2.82 94.29 3.60 94.30 4.05
100 93.66 0.17 93.65 0.36 93.50 0.42 93.50 0.42
POS tagging without candidate features
POS tagging with candidate features
Text Chunking without candidate features
Text Chunking with candidate features
Alg. /
C(d)
x=1
x=10
x=100
Fβ=1 time Fβ=1 time Fβ=1 time
96.96 5.09 97.10 27.90 97.10 30.92
96.89 0.79 97.12 4.56 97.07 4.74
96.57 0.10 96.82 0.81 96.73 0.81
1
</table>
<page confidence="0.38387">
2
</page>
<figure confidence="0.7676122">
3
Alg. /
C(d)
x=1
x=10
x=100
Fβ=1 time Fβ=1 time Fβ=1 time
97.06 6.65 97.30 109.20 97.29 330.82
96.98 1.27 97.29 13.26 97.23 38.27
96.61 0.14 96.93 1.64 96.76 5.05
1
2
3
Alg. /
C(d)
x=1
x=10
x=100
Fβ=1 time Fβ=1 time Fβ=1 time
92.50 0.12 93.60 0.26 93.47 0.41
92.34 0.02 93.50 0.05 93.39 0.07
89.70 0.008 92.31 0.02 92.03 0.02
1
2
3
</figure>
<subsectionHeader confidence="0.997631">
5.3 Comparison with SVM
</subsectionHeader>
<bodyText confidence="0.999908714285715">
Table 3 lists average accuracy and training time
on POS tagging and text chunking with respect
to each (v, ) for AdaBoost.SDF and d for SVM.
AdaBoost.SDF with v=10 and v=100 have shown
much faster training speeds than SVM and Ad-
aBoost.SDF ( v=1,w=�) that is equivalent to the
AdaBoost.DF (Iwakura and Okamoto, 2007).
Furthermore, the accuracy of taggers and chun-
kers based on AdaBoost.SDF (v=10) have shown
competitive accuracy with those of SVM-based
and AdaBoost.DF-based taggers and chunkers.
AdaBoost.SDF (v=10) showed about 6 and 54
times faster training speeds than those of Ad-
aBoost.DF on the average in POS tagging and text
chunking. AdaBoost.SDF (v=10) showed about
147 and 9 times faster training speeds than the
training speeds of SVM on the average of POS
tagging and text chunking. On the average of the
both tasks, AdaBoost.SDF (v=10) showed about
25 and 50 times faster training speed than Ad-
aBoost.DF and SVM. These results have shown
that AdaBoost.SDF with a moderate parameter v
can improve training time drastically while main-
taining accuracy.
These results in Table 3 have also shown that
rules represented by combination of features and
the candidate features collected from automati-
cally tagged data contribute to improved accuracy.
</bodyText>
<subsectionHeader confidence="0.999283">
5.4 Effectiveness of Redistribution
</subsectionHeader>
<bodyText confidence="0.999981333333333">
We compared F,3=1 and best training time of F-
dist and W-dist. We used = 2 that has shown
better average accuracy than = 11, 31 in both
tasks. Table 4 lists comparison of F-dist and W-
dist on POS tagging and text chunking. Most of
accuracy obtained with W-dist-based taggers and
parsers better than accuracy obtained with F-dist-
based taggers and parsers. These results have
shown that W-dist improves accuracy without dras-
tically increasing training time. The text chunker
and the tagger trained with AdaBoost.SDF (v = 10,
w = 10 and W-dist) has shown competitive accu-
racy with that of the chunker trained with Ad-
aBoost.SDF (v = 1, w = oo and F-dist) while main-
taining about 7.5 times faster training speed.
</bodyText>
<subsectionHeader confidence="0.995448">
5.5 Tagging and Chunking Speeds
</subsectionHeader>
<bodyText confidence="0.999884466666667">
We measured testing speeds of taggers and chun-
kers based on rules or models listed in Table 5. 10
We examined two types of fast classification al-
gorithms for polynomial kernel: Polynomial Ker-
nel Inverted (PKI) and Polynomial Kernel Ex-
panded (PKE). The PKI leads to about 2 to 12
times improvements, and the PKE leads to 30 to
300 compared with normal classification approach
of SVM (Kudo and Matsumoto, 2003). 11
The POS-taggers based on AdaBoost.SDF,
SVM with PKI, and SVM with PKE processed
4,052 words, 159 words, and 1,676 words per sec-
ond, respectively. The chunkers based on these
three methods processed 2,732 words, 113 words,
and 1,718 words per second, respectively.
</bodyText>
<footnote confidence="0.998283333333333">
10We list average speeds of three times tests measured with
a machine with Xeon 3.8 GHz CPU and 4 GB of memory.
11We use a chunker YamCha for evaluating classification
speeds based on PKI or PKE (http://www.chasen.org/˜taku/software/
yamcha/). We list the average speeds of SVM-based tagger and
chunker with PKE of a threshold parameter v = 0.0005 for
rule selection in both task. The accuracy obtained with mod-
els converted by PKE are slightly lower than the accuracy ob-
tained with their original models in our experiments.
</footnote>
<figure confidence="0.815911153846154">
SVMs 92.77 12.74 94.31 9.63 94.20 49.27
Alg. /
C(d)
x=1
x=10
x=100
Fβ=1 time Fβ=1 time Fβ=1 time
92.89 0.25 94.19 26.10 94.04 300.77
92.85 0.04 94.11 2.97 94.08 3.06
91.99 0.01 93.37 0.32 93.24 0.34
1
2
3
</figure>
<page confidence="0.994003">
22
</page>
<tableCaption confidence="0.999792">
Table 5: Comparison with previous best results: (Top :
</tableCaption>
<table confidence="0.97413980952381">
POS tagging, Bottom: Text Chunking )
POS tagging Fp=1
Perceptron (Collins, 2002) 97.11
Dep. Networks (Toutanova et al., 2003) 97.24
SVM (Gim´enez and M`arquez, 2003) 97.05
ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 97.15
Guided learning for bidirectional sequence classification (Shen et al., 2007) 97.33
AdaBoost.SDF with candidate features (S=2,v=1,w=100, W-dist) 97.32
AdaBoost.SDF with candidate features (S=2,v=10,w=10, F-dist) 97.32
SVM with candidate features (C=0.1, d=2) 97.32
Text Chunking Fp=1
Regularized Winnow + full parser output (Zhang et al., 2001) 94.17
SVM-voting (Kudo and Matsumoto, 2001) 93.91
ASO + unlabeled data (Ando and Zhang, 2005) 94.39
CRF+Reranking(Kudo et al., 2005) 94.12
ME based a bidirectional inference (Tsuruoka and Tsujii, 2005) 93.70
LaSo (Approximate Large Margin Update) (Daum´e III and Marcu, 2005) 94.4
HySOL (Suzuki et al., 2007) 94.36
AdaBoost.SDF with candidate featuers (S=2,v=1,w=00, W-dist) 94.32
AdaBoost.SDF with candidate featuers (S=2,v=10,w=10,W-dist) 94.30
SVM with candidate features (C=1, d=2) 94.31
</table>
<bodyText confidence="0.999848285714286">
One of the reasons that boosting-based classi-
fiers realize faster classification speed is sparseness
of rules. SVM learns a final hypothesis as a linear
combination of the training examples using some
coefficients. In contrast, this boosting-based rule
learner learns a final hypothesis that is a subset of
candidate rules (Kudo and Matsumoto, 2004).
</bodyText>
<sectionHeader confidence="0.999969" genericHeader="method">
6 Related Works
</sectionHeader>
<subsectionHeader confidence="0.999502">
6.1 Comparison with Previous Best Results
</subsectionHeader>
<bodyText confidence="0.999968066666667">
We list previous best results on English POS tag-
ging and Text chunking in Table 5. These results
obtained with the taggers and chunkers based on
AdaBoost.SDF and SVM showed competitive F-
measure with previous best results. These show
that candidate features contribute to create state-
of-the-art taggers and chunkers.
These results have also shown that
AdaBoost.SDF-based taggers and chunkers
show competitive accuracy by learning combi-
nation of features automatically. Most of these
previous works manually selected combination
of features except for SVM with polynomial
kernel and (Kudo and Matsumoto, 2001) a
boosting-based re-ranking (Kudo et al., 2005).
</bodyText>
<subsectionHeader confidence="0.908851">
6.2 Comparison with Boosting-based
Learners
</subsectionHeader>
<bodyText confidence="0.997708142857143">
LazyBoosting randomly selects a small proportion
of features and selects a rule represented by a fea-
ture from the selected features at each iteration
(Escudero et al., 2000).
Collins and Koo proposed a method only up-
dates values of features co-occurring with a rule
feature on examples at each iteration (Collins and
Koo, 2005).
Kudo et al. proposed to perform several pseudo
iterations for converging fast (Kudo et al., 2005)
with features in the cache that maintains the fea-
tures explored in the previous iterations.
AdaBoost.MHKR learns a weak-hypothesis rep-
resented by a set of rules at each boosting iteration
(Sebastiani et al., 2000).
AdaBoost.SDF differs from previous works in
the followings. AdaBoost.SDF learns several rules
at each boosting iteration like AdaBoost.MHKR.
However, the confidence value of each hypothe-
sis in AdaBoost.MHKR does not always minimize
the upper bound of training error for AdaBoost
because the value of each hypothesis consists of
the sum of the confidence value of each rule.
Compared with AdaBoost.MHKR, AdaBoost.SDF
computes the confidence value of each rule to min-
imize the upper bound of training error on given
weights of samples at each update.
Furthermore, AdaBoost.SDF learns several
rules represented by combination of features from
limited search spaces at each boosting itera-
tion. The creation of subsets of features in Ad-
aBoost.SDF enables us to recreate the same classi-
fier with same parameters and training data. Recre-
ation is not ensured in the random selection of sub-
sets in LazyBoosting.
</bodyText>
<sectionHeader confidence="0.998969" genericHeader="conclusions">
7 Conclusion
</sectionHeader>
<bodyText confidence="0.999970666666667">
We have proposed a fast boosting-based learner,
which we call AdaBoost.SDF. AdaBoost.SDF re-
peats to learn several rules represented by combi-
nation of features from a small proportion of can-
didate rules. We have also proposed methods to
use candidate POS tags and chunk tags of each
word obtained from automatically tagged data as
features in POS tagging and text chunking.
The experimental results have shown drastically
improvement of training speed while maintaining
competitive accuracy compared with previous best
results.
Future work should examine our approach on
several tasks. Future work should also compare
our algorithm with other learning algorithms.
</bodyText>
<sectionHeader confidence="0.986275" genericHeader="references">
Appendix A: Convergence
</sectionHeader>
<bodyText confidence="0.999982857142857">
The upper bound of the training error for AdaBoost
of (Freund and Mason, 1999), which is used in Ad-
aBoost.SDF, is induced by adopting THEOREM 1
presented in (Schapire and Singer, 1999). Let ZR
be EZ&apos;1wR+1,i that is a sum of weights updated
with R rules. The bound holds on the training er-
ror after selecting R rules,
</bodyText>
<equation confidence="0.9469304">
E&apos;1[[F(xi) � yi]] &lt; ZR
is induced as follows.
. (1), we obtain
�R
wR+1,i = exp(−yi �=1 h(f,.,.,,,,)(xi)). Thus, we
</equation>
<bodyText confidence="0.3045458">
obtain [[F(xi) 54 yi]] &lt; exp(−yi ER 1 h(f_.,)(xi)),
since if F(xi) # yi, then exp(−yi ER1 hof,.)(xi)) &gt;
1 . Combining these equations gives the stated
bound on training error
By unraveling the Eq
</bodyText>
<page confidence="0.9943">
23
24
</page>
<figure confidence="0.625650727272727">
R
X
t=1
Xm
i=1
Xm
i=1
exp(−yi
h(fr,cr)(xi))
[[F(xi) =� yi]] C
Xm
</figure>
<equation confidence="0.957354714285714">
i=1
=
wR+1,i = ZR- (2)
Xm
i=1
Xm
i=1
ZR =
wR+1,i =
wR,iexp(−yih(fR,cR))
Xm
i=1
=
wR,i − Wr,+1(fR) − Wr,+1(fR) +
</equation>
<bodyText confidence="0.998167375">
Then we show that the upper bound of training er-
ror ZR for R rules shown in Eq. (2) is less than or
equal to the upper bound of the training error ZR_1
for R-1 rules. By unraveling the (2) and plug-
ging the confidence values cR = { 12log( Wr,+1�fR�
Wr,�1�fR�), 0
} given by the weak hypothesis into the unraveled
equation, we obtain ZRGZR_1, since
</bodyText>
<equation confidence="0.86248525">
Wr,+1(fR)exp(−cR) + Wr,−1(fR)exp(cR)
p p
= ZR−1 − ( WR,+1(fR) − WR,−1(fR))2
References
</equation>
<bodyText confidence="0.989521655913978">
Ando, Rie and Tong Zhang. 2005. A high-performance semi-
supervised learning method for text chunking. In Proc. of
43rd Meeting of Association for Computational Linguis-
tics, pages 1–9.
Collins, Michael and Terry Koo. 2005. Discriminative
reranking for natural language parsing. Computational
Linguistics, 31(1):25–70.
Collins, Michael. 2002. Discriminative training methods
for Hidden Markov Models: theory and experiments with
perceptron algorithms. In Proc. of the 2002 Conference
on Empirical Methods in Natural Language Processing,
pages 1–8.
Daum´e III, Hal and Daniel Marcu. 2005. Learning as
search optimization: Approximate large margin methods
for structured prediction. In Proc. of 22th International
Conference on Machine Learning, pages 169–176.
Escudero, Gerard, Llu´ıs M`arquez, and German Rigau. 2000.
Boosting applied to word sense disambiguation. In Proc.
of 11th European Conference on Machine Learning, pages
129–141.
Freund, Yoav and Llew Mason. 1999. The alternating de-
cision tree learning algorithm,. In Proc. of 16th Interna-
tional Conference on Machine Learning, pages 124–133.
Gim´enez, Jes´us and Llu´ıs M`arquez. 2003. Fast and accu-
rate part-of-speech tagging: The SVM approach revisited.
In Proc. of International Conference Recent Advances in
Natural Language Processing 2003, pages 153–163.
Iwakura, Tomoya and Seishi Okamoto. 2007. Fast training
methods of boosting algorithms for text analysis. In Proc.
of International Conference Recent Advances in Natural
Language Processing 2007, pages 274–279.
Kazama, Jun’ichi and Kentaro Torisawa. 2007. A new per-
ceptron algorithm for sequence labeling with non-local
features. In Proc. of the 2007 Joint Conference on Empiri-
cal Methods in Natural Language Processing and Compu-
tational Natural Language Learning, pages 315–324.
Kudo, Taku and Yuji Matsumoto. 2001. Chunking with Sup-
port Vector Machines. In Proc. of The Conference of the
North American Chapter of the Association for Computa-
tional Linguistics, pages 192–199.
Kudo, Taku and Yuji Matsumoto. 2003. Fast methods for
kernel-based text analysis. In Proc. of 41st Meeting of As-
sociation for Computational Linguistics, pages 24–31.
Kudo, Taku and Yuji Matsumoto. 2004. A boosting algo-
rithm for classification of semi-structured text. In Proc.
of the 2004 Conference on Empirical Methods in Natural
Language Processing 2004, pages 301–308, July.
Kudo, Taku, Jun Suzuki, and Hideki Isozaki. 2005.
Boosting-based parse reranking with subtree features. In
Proc. of 43rd Meeting of Association for Computational
Linguistics, pages 189–196.
Marcus, Mitchell P., Beatrice Santorini, and Mary Ann
Marcinkiewicz. 1994. Building a large annotated corpus
of english: The Penn Treebank. pages 313–330.
Morishita, Shinichi. 2002. Computing optimal hypotheses
efficiently for boosting. Proc. of 5th International Confer-
ence Discovery Science, pages 471–481.
Schapire, Robert E. and Yoram Singer. 1999. Improved
boosting using confidence-rated predictions. Machine
Learning, 37(3):297–336.
Schapire, Robert E. and Yoram Singer. 2000. Boostexter:
A boosting-based system for text categorization. Machine
Learning, 39(2/3):135–168.
Sebastiani, Fabrizio, Alessandro Sperduti, and Nicola Val-
dambrini. 2000. An improved boosting algorithm and its
application to text categorization. In Proc. of International
Conference on Information and Knowledge Management,
pages 78–85.
Shen, Libin, Giorgio Satta, and Aravind Joshi. 2007. Guided
learning for bidirectional sequence classification. In Proc.
of 45th Meeting ofAssociation for Computational Linguis-
tics, pages 760–767.
Suzuki, Jun, Akinori Fujino, and Hideki Isozaki. 2007. Semi-
supervised structured output learning based on a hybrid
generative and discriminative approach. In Proc. of the
2007 Joint Conference on Empirical Methods in Natu-
ral Language Processing and Computational Natural Lan-
guage Learning, pages 791–800.
Toutanova, Kristina, Dan Klein, Christopher D. Manning, and
Yoram Singer. 2003. Feature-rich part-of-speech tagging
with a cyclic dependency network. In Proc. of the 2003
Human Language Technology Conference of the North
American Chapter of the Association for Computational
Linguistics, pages 173–180.
Tsuruoka, Yoshimasa and Junichi Tsujii. 2005. Bidirec-
tional inference with the easiest-first strategy for tagging
sequence data. In Proc. of Human Language Technology
Conference and Conference on Empirical Methods in Nat-
ural Language Processing, pages 467–474.
Zhang, Tong, Fred Damerau, and David Johnson. 2001.
Text chunking using regularized winnow. In Proc. of
39th Meeting of Association for Computational Linguis-
tics, pages 539–546.
</bodyText>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.802534">
<title confidence="0.999757">A Fast Boosting-based Learner for Feature-Rich Tagging and Chunking</title>
<author confidence="0.912953">Tomoya Iwakura Seishi</author>
<affiliation confidence="0.94154">Fujitsu Laboratories</affiliation>
<address confidence="0.885821">1-1, Kamikodanaka 4-chome, Nakahara-ku, Kawasaki 211-8588,</address>
<abstract confidence="0.998793366666667">Combination of features contributes to a significant improvement in accuracy on tasks such as part-of-speech (POS) tagging and text chunking, compared with using atomic features. However, selecting combination of features on learning with large-scale and feature-rich training data requires long training time. We propose a fast boosting-based algorithm for learning rules represented by combination of features. Our algorithm constructs a set of rules by repeating the process to select several rules from a small proportion of candidate rules. The candidate rules are generated from a subset of all the features with a technique similar to beam search. Then we propose POS tagging and text chunking based on our learning algorithm. Our tagger and chunker use candidate POS tags or chunk tags of each word collected from automatically tagged data. We evaluate our methods with English POS tagging and text chunking. The experimental results show that the training time of our algorithm are about 50 times faster than Support Vector Machines with polynomial kernel on the average while maintaining stateof-the-art accuracy and faster classification speed.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
</citationList>
</algorithm>
</algorithms>