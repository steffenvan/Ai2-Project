<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.016954">
<title confidence="0.831898">
LIUM SMT Machine Translation System for WMT 2010
</title>
<author confidence="0.85924">
Patrik Lambert, Sadaf Abdul-Rauf and Holger Schwenk
</author>
<affiliation confidence="0.87049">
LIUM, University of Le Mans
</affiliation>
<address confidence="0.699161">
72085 Le Mans cedex 9, FRANCE
</address>
<email confidence="0.966749">
FirstName.LastName@lium.univ-lemans.fr
</email>
<sectionHeader confidence="0.997026" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.99991815">
This paper describes the development of
French–English and English–French ma-
chine translation systems for the 2010
WMT shared task evaluation. These sys-
tems were standard phrase-based statisti-
cal systems based on the Moses decoder,
trained on the provided data only. Most
of our efforts were devoted to the choice
and extraction of bilingual data used for
training. We filtered out some bilingual
corpora and pruned the phrase table. We
also investigated the impact of adding two
types of additional bilingual texts, ex-
tracted automatically from the available
monolingual data. We first collected bilin-
gual data by performing automatic trans-
lations of monolingual texts. The second
type of bilingual text was harvested from
comparable corpora with Information Re-
trieval techniques.
</bodyText>
<sectionHeader confidence="0.999517" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999991944444445">
This paper describes the machine translation sys-
tems developed by the Computer Science labora-
tory at the University of Le Mans (LIUM) for the
2010 WMT shared task evaluation. We only con-
sidered the translation between French and En-
glish (in both directions). The main differences
with respect to previous year’s system (Schwenk
et al., 2009) are as follows: restriction to the data
recommended for the workshop, usage of the (fil-
tered) French–English gigaword bitext, pruning of
the phrase table, and usage of automatic trans-
lations of the monolingual news corpus to im-
prove the translation model. We also used a larger
amount of bilingual data extracted from compara-
ble corpora than was done in 2009. These different
points are described in the rest of the paper, to-
gether with a summary of the experimental results
showing the impact of each component.
</bodyText>
<sectionHeader confidence="0.99541" genericHeader="method">
2 Resources Used
</sectionHeader>
<bodyText confidence="0.99951">
The following sections describe how the resources
provided or allowed in the shared task were used
to train the translation and language models of the
system.
</bodyText>
<subsectionHeader confidence="0.975672">
2.1 Bilingual data
</subsectionHeader>
<bodyText confidence="0.99995403030303">
Our system was developed in two stages. First,
a baseline system was built to generate automatic
translations of some of the monolingual data avail-
able. These automatic translations may be used
directly with the source texts to build additional
bitexts, or as queries of an Information Retrieval
(IR) system to extract new bitexts from compara-
ble corpora. In a second stage, these additional
bilingual data were incorporated to the system (see
Section 4 and Tables 1 and 2).
The latest version of the News-Commentary
(NC) corpus, of the Europarl (Eparl) corpus (ver-
sion 5), and of the United Nations (UN) corpus
were used. We also took as training data a sub-
set of the French–English Gigaword (109) cor-
pus. Since a significant part of the data was
crawled from the web, we thought that many sen-
tence pairs may be only approximate translations
of each other. We applied a lexical filter to dis-
card them. Furthermore, some sentences of this
corpus were extracted from web page menus and
are not grammatical. Although we could have
used a part of the menu items as a dictionary, for
simplicity we applied an n-gram language model
(LM) filter to remove all non-grammatical sen-
tences. Thanks to this filter, sentences out of the
language model domain (in this case, mainly the
news domain), may also have been discarded be-
cause they contain many unknown or unfrequent
n-grams. The lexical filter was based on the IBM
model 1 cost (Brown et al., 1993) of each side of
a sentence pair given the other side, normalised
with respect to both sentence lengths. This filter
</bodyText>
<page confidence="0.973059">
121
</page>
<note confidence="0.453393">
Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 121–126,
Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics
</note>
<bodyText confidence="0.9997208">
was trained on a corpus composed of Eparl, NC,
and UN data. The language model filter was an
n-gram LM cost of the target sentence (see Sec-
tion 3), normalised with respect to its length. This
filter was trained with all monolingual resources
available except the 109 data. We generated a first
subset, 1091, selecting sentence pairs with a lexi-
cal cost inferior to 4 and an LM cost inferior to
2.3. The corpus selected in this way contains 115
million words in the English side (out of 580 mil-
lion in the original corpus). Close to the evaluation
deadline we decided to generate a second corpus
(1092) by raising the LM cost threshold to 2.6. The
1092 corpus contains 232 million words on the En-
glish side (twice as much as in the 1091 corpus).
In the French side of the bilingual corpora, for
the French–English direction only, the contrac-
tions ‘du’ (‘of the’), ‘au’ and ‘aux’ (‘to the’ singu-
lar and plural) were substituted by their expanded
forms (‘de le’, ‘`a le’ and ‘`a les’).
</bodyText>
<subsectionHeader confidence="0.998264">
2.2 Use of Automatic Translations and
Comparable corpora
</subsectionHeader>
<bodyText confidence="0.999989744680851">
Available human translated bitexts such as the UN
corpus seem to be out-of domain for this task.
We used two types of automatically extracted re-
sources to adapt our system to the task domain.
First, we generated automatic translations of the
French News corpus provided (231M words), and
selected the sentences with a normalised transla-
tion cost (returned by the decoder) inferior to a
threshold. The resulting bitext has no new words
in the English side, since all words of the transla-
tion output come from the translation model, but
it contains new combinations (phrases) of known
words, and reinforces the probability of some
phrase pairs (Schwenk, 2008).
Second, as in last year’s evaluation, we auto-
matically extracted and aligned parallel sentences
from comparable in-domain corpora. This year
we used the AFP and APW news texts since there
are available in the French and English LDC Gi-
gaword corpora. The general architecture of our
parallel sentence extraction system is described in
detail by Abdul-Rauf and Schwenk (2009). We
first translated 91M words from French into En-
glish using our first stage SMT system. These En-
glish sentences were then used to search for trans-
lations in the English AFP and APW texts of the
Gigaword corpus using information retrieval tech-
niques. The Lemur toolkit (Ogilvie and Callan,
2001) was used for this purpose. Search was lim-
ited to a window of f5 days of the date of the
French news text. The retrieved candidate sen-
tences were then filtered using the Translation Er-
ror Rate (TER) with respect to the automatic trans-
lations. In this study, sentences with a TER be-
low 65% for the French–English system and 75%
for the English–French system were kept. Sen-
tences with a large length difference (French ver-
sus English) or containing a large fraction of num-
bers were also discarded. By these means, about
15M words of additional bitexts were obtained to
include in the French–English system, and 21M
words to include in the English–French system.
Note that these additional bitexts do not depend
on the translation direction. The most suitable
amount of additional data was just different in
the French–English and English–French transla-
tion directions.
</bodyText>
<subsectionHeader confidence="0.998746">
2.3 Monolingual data
</subsectionHeader>
<bodyText confidence="0.999863">
The French and English target language models
were trained on all provided monolingual data. In
addition, LDC’s Gigaword collection was used for
both languages. Data corresponding to the devel-
opment and test periods were removed from the
Gigaword collections.
</bodyText>
<subsectionHeader confidence="0.998009">
2.4 Development data
</subsectionHeader>
<bodyText confidence="0.999951214285714">
All development was done on news-test2008, and
newstest2009 was used as internal test set. For all
corpora except the French side of the bitexts used
to train the French–English system (see above),
the default Moses tokenization was used. How-
ever, we added abbreviations for the French tok-
enizer. All our models are case sensitive and in-
clude punctuation. The BLEU scores reported in
this paper were calculated with the multi-bleu.perl
tool and are case sensitive. The BLEU score
was one of metrics with the best correlation with
human ratings in last year evaluation (Callison-
Burch et al., 2009) for the French–English and
English–French directions.
</bodyText>
<sectionHeader confidence="0.561375" genericHeader="method">
3 Architecture of the SMT system
</sectionHeader>
<bodyText confidence="0.998668">
The goal of statistical machine translation (SMT)
is to produce a target sentence a from a source
sentence f. It is today common practice to use
phrases as translation units (Koehn et al., 2003;
Och and Ney, 2003) and a log linear framework in
order to introduce several models explaining the
</bodyText>
<page confidence="0.910961">
122
</page>
<equation confidence="0.940873714285714">
translation process:
e* = arg max
e
�
= arg max {exp(
e
i
</equation>
<bodyText confidence="0.99980325">
The feature functions hi are the system mod-
els and the Ai weights are typically optimized to
maximize a scoring function on a development
set (Och and Ney, 2002). In our system fourteen
features functions were used, namely phrase and
lexical translation probabilities in both directions,
seven features for the lexicalized distortion model,
a word and a phrase penalty and a target language
model (LM).
The system is based on the Moses SMT toolkit
(Koehn et al., 2007) and constructed as follows.
First, word alignments in both directions are cal-
culated. We used a multi-threaded version of the
GIZA++ tool (Gao and Vogel, 2008).1 This speeds
up the process and corrects an error of GIZA++
that can appear with rare words.
Phrases and lexical reorderings are extracted
using the default settings of the Moses toolkit.
The parameters of Moses were tuned on news-
test2008, using the ‘new’ MERT tool. We repeated
the training process three times, each with a differ-
ent seed value for the optimisation algorithm. In
this way we have an rough idea of the error intro-
duced by the tuning process.
4-gram back-off LMs were used. The word
list contains all the words of the bitext used to
train the translation model and all words that ap-
pear at least ten times in the monolingual corpora.
Words of the monolingual corpora containing spe-
cial characters or sequences of uppercase charac-
ters were not included in the word list. Separate
LMs were build on each data source with the SRI
LM toolkit (Stolcke, 2002) and then linearly in-
terpolated, optimizing the coefficients with an EM
procedure. The perplexities of these LMs were
103.4 for French and 149.2 for English.
</bodyText>
<sectionHeader confidence="0.999888" genericHeader="evaluation">
4 Results and Discussion
</sectionHeader>
<bodyText confidence="0.9990872">
The results of our SMT system for the French–
English and English–French tasks are summarized
in Tables 1 and 2, respectively. The MT metric
scores are the average of three optimisations per-
formed with different seeds (see Section 3). The
</bodyText>
<footnote confidence="0.9754055">
1The source is available at http://www.cs.cmu.
edu/˜qing/
</footnote>
<bodyText confidence="0.999858818181818">
numbers in parentheses are the standard deviation
of these three values. The standard deviation gives
a lower bound of the significance of the difference
between two systems. If the difference between
two average scores is less than the sum of the stan-
dard deviations, we can say that this difference is
not significant. The reverse is not true. Note that
most of the improvements shown in the tables are
small and not significant. However many of the
gains are cumulative and the sum of several small
gains makes a significant difference.
</bodyText>
<subsectionHeader confidence="0.972353">
Phrase-table Pruning
</subsectionHeader>
<bodyText confidence="0.99994125">
We tried to prune the phrase-table as proposed by
Johnson et. al. (2007), and available in moses
(‘sigtest-filter’). We used the α − c filter2. As
lines 3 and 4 of Table 1, and lines 3 and 4 of Ta-
ble 2 reveal, in addition to the reduction 43% of
the phrase-table, a small gain in BLEU score (0.15
and 0.11 respectively) was obtained with the prun-
ing.
</bodyText>
<subsectionHeader confidence="0.986361">
Baseline French–English System
</subsectionHeader>
<bodyText confidence="0.998321">
The first section of Table 1 (lines 1 to 5) shows re-
sults of the development of the baseline SMT sys-
tem, used to generate automatic translations. Al-
though being out-of-domain data, the introduction
of the UN corpus yields an improvement of one
BLEU point with respect to Eparl+NC. Adding the
109 corpus, we gain 0.7 BLEU point more. Ac-
tually, we obtained the same score with the 109
added directly to Eparl+NC (line 5). However, we
choose to include the UN corpus to generate trans-
lations to have a larger vocabulary. The system
highlighted in bold (line 4) is the one we choose
to generate our English translations.
Although no French translations were gener-
ated, we did similar experiments in the English–
French direction (lines 1 to 4 of Table 2). In this
direction, the 109 corpus is still more valuable than
the UN corpus when added to Eparl+NC, but with
less difference in terms of BLEU score. In this di-
2The p-value of two-by-two contingency tables (describ-
ing the degree of association between a source and a target
phrase) is calculated with Fisher exact test. This probability
is interpreted as the probability of observing by chance an as-
sociation that is at least as strong as the given one, and hence
as its significance. An important special case of a table oc-
curs when a phrase pair occurs exactly once in the corpus,
and each of the component phrases occurs exactly once in its
side of the parallel corpus (1-1-1 phrase pairs). In this case
the negative log of the p-value is α = logN (N is number of
sentence pairs in the corpus). α − e is the largest threshold
that results in all of the 1-1-1 phrase pairs being included.
</bodyText>
<equation confidence="0.961584">
p(e|f)
Aihi(ej))} (1)
</equation>
<page confidence="0.987065">
123
</page>
<bodyText confidence="0.998498">
rection, we obtain a gain by adding the UN corpus
to Eparl+NC+109�.
Filtering the 109 Corpus
Lines 5 to 7 of Table 1 show the impact of filtering
the 109 corpus. The system trained on the full 109
corpus added to Eparl+NC achieves a BLEU score
of 26.83. Substituting the full 109 corpus by 1091 (5
times smaller), i.e. using the first filtering settings,
we gain 0.13 BLEU point. Using 1092 instead of
109�, we gain another 0.16 BLEU point, that is 0.3
in total. With respect to not using the 109 data at
all (as we did last year), we gain 0.8 BLEU point.
</bodyText>
<subsectionHeader confidence="0.779987">
Impact of the Additional Bitexts
</subsectionHeader>
<bodyText confidence="0.9999165">
With the baseline French–English SMT system
(see above), we translated the French News cor-
pus to generated an additional bitext (News). We
also translated some parts of the French LDC Gi-
gaword corpus, to serve as queries to our IR sys-
tem (see section 2.2). The resulting additional bi-
text is referred to as IR. Lines 8 to 13 of Table 1
and lines 6 to 12 of Table 2 summarize the system
development including the additional bitexts.
With the News additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the baseline system used to generate
the automatic translations, but with less than 30%
of the data. This holds in both translation direc-
tions. Adding the News corpus to a larger corpus,
such as Eparl+NC+109�, has less impact but still
yields some improvement: 0.15 BLEU point in
French–English and 0.3 in English–French. Thus,
the News bitext translated from French to English
may have more impact when translating from En-
glish to French than in the opposite direction. Note
that the number of additional phrase-table entries
per additional running word is twice as high for
the News bitext than for the other corpora. For
example, with respect to Eparl+NC+UN+1091 (Ta-
ble 2), Eparl+NC+UN+109�+News has 56M more
words and 116M more entries in the phrase-table,
thus the ratio is more than 2. For all other cor-
pora, the ratio is equal to 1 or less. This is un-
expected, particularly in this case where the News
bitext has no new English vocabulary with respect
to the Eparl+NC+UN+1091 corpus, from which its
English side was generated.
With the IR additional bitext added to
Eparl+NC, we obtain a system of similar perfor-
mance as the system trained on Eparl+NC+UN,
while the IR bitext is 10 times smaller than the
UN corpus. Added to Eparl+NC+109�+News, the
IR bitext allows gains of 0.13 and 0.2 BLEU point
respectively in the French–English and English–
French directions.
Comparing the systems trained on
Eparl+NC+1091 or Eparl+NC+1092 to the sys-
tems trained on the same corpora plus News+IR,
we can estimate the cumulative impact of the
additional bitexts. The gain is around 0.3 BLEU
point for French–English and around 0.5 BLEU
point for English–French.
</bodyText>
<subsectionHeader confidence="0.97865">
Final System
</subsectionHeader>
<bodyText confidence="0.999981727272727">
In both translation directions our best system was
the one trained on Eparl+NC+1092+News+IR. We
further achieved small improvements (0.3 BLEU
point) by pruning the phrase-table (as above) and
by using a language model with no cut-off together
with increasing the beam size and/or the maxi-
mum number of translation table entries per input
phrase. Note that the English LM with cut-off had
a size of 6G, and the one with no cut-off had a
size of 29G. It was too much to fit in our 72G
machines so we pruned it with the SRILM prun-
ing tool down to a size of 19G. The French LM
with cut-off had a size of 2G and the one with
no cut-off had a size of 9G. These sizes corre-
spond to the binary format. Taking as example the
French–English direction, the running time went
from 8600 seconds for the system of line 14 (with
a threshold pruning coefficient of 0.4 and a LM
with cut-off) to 28200 seconds for the system sub-
mitted (with the LM without cut-off pruned by the
SRILM tool and a threshold pruning coefficient of
0.00001).
</bodyText>
<sectionHeader confidence="0.981413" genericHeader="conclusions">
5 Conclusions and Further Work
</sectionHeader>
<bodyText confidence="0.999985571428572">
We presented the development of our machine
translation system for the French–English and
English–French 2010 WMT shared task. Our sys-
tem was actually a standard phrase-based SMT
system based on the Moses decoder. Its original-
ity mostly lied in the choice and extraction of the
training data used.
We decided to use a part of the 109 French–
English corpus. We found this resource useful,
even without filtering. We nevertheless gained 0.3
BLEU point by selecting sentences based on an
IBM Model 1 filter and a language model filter.
We pruned the phrase table with the ‘sigtest-
filter’ distributed in Moses, yielding improve-
</bodyText>
<page confidence="0.996126">
124
</page>
<table confidence="0.99992695">
Bitext #Fr Words P-table Mem news-test2008 newstest2009
(M) size (M) (G) BLEU BLEU
1 Eparl+NC 52 66 19.3 22.80 (0.03) 25.31 (0.2)
2 Eparl+NC+UN 275 250 22.8 23.38 (0.1) 26.30 (0.2)
3 Eparl+NC+UN+109 406 376 25.1 23.81 (0.05) 27.0 (0.2)
1
4 Eparl+NC+UN+1091 pruned 406 215 21.4 23.96 (0.1) 27.15 (0.18)
5 Eparl+NC+109 183 198 22.1 23.83 (0.07) 26.96 (0.04)
1
6 Eparl+NC+109 320 319 24.1 23.95 (0.03) 27.12 (0.1)
2
7 Eparl+NC+109 733 580 29.5 23.65 (0.09) 26.83 (0.2)
8 Eparl+NC+News 111 188 19.5 23.46 (0.1) 26.95 (0.2)
9 Eparl+NC+109�+News 242 317 22.5 23.77 (0.04) 27.11 (0.04)
10 Eparl+NC+IR 68 78 19.5 22.97 (0.03) 26.20 (0.1)
11 Eparl+NC+News+IR 127 198 20.1 23.62 (0.01) 27.04 (0.06)
12 Eparl+NC+109�+News+IR 258 327 22.8 23.75 (0.05) 27.24 (0.05)
13 Eparl+NC+1092+News+IR 395 441 24.4 23.87 (0.03) 27.43 (0.08)
14 Eparl+NC+1092+News+IR pruned 395 285 62.5 24.04 27.72
(+larger beam, +no-cutoff LM)
</table>
<tableCaption confidence="0.8329865">
Table 1: French–English results: number of French words (in million), number of entries in the phrase-
table (in million), memory needed during decoding (in gigabytes) and BLEU scores in the development
(news-test2008) and internal test (newstest2009) sets for the different systems developped. The BLEU
scores and the number in parentheses are the average and standard deviation over 3 values (see Section 3.)
</tableCaption>
<bodyText confidence="0.999956863636364">
ments of 0.1 to 0.2 BLEU point for a 43% reduc-
tion of the phrase-table size.
We used additional bitexts extracted automati-
cally from the available monolingual corpora. The
first type of additional bitext is generated with au-
tomatic translations of the monolingual data with
a baseline SMT system. The second one is ex-
tracted from comparable corpora, with Informa-
tion Retrieval techniques. With the additional bi-
texts we gained 0.3 and 0.5 BLEU point for the
French–English and English–French systems, re-
spectively.
Next year we want to perform an improved se-
lection of parallel training data with re-sampling
techniques. We also want to use a continuous
space language model (Schwenk, 2007) in an n-
best list rescoring step after decoding. Finally, we
plan to train different types of systems (such as
a hierarchical SMT system and a Statistical Post-
Editing system) and combine their outputs with
the MANY open source system combination soft-
ware (Barrault, 2010).
</bodyText>
<sectionHeader confidence="0.998812" genericHeader="acknowledgments">
Acknowledgments
</sectionHeader>
<bodyText confidence="0.999824">
This work has been partially funded by
the European Union under the EuroMatrix
Plus project – Bringing Machine Transla-
tion for European Languages to the User –
(http://www.euromatrixplus.net, IST-2007.2.2-
FP7-231720).
</bodyText>
<sectionHeader confidence="0.998662" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.997387192307692">
Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the
use of comparable corpora to improve SMT perfor-
mance. In Proceedings of the 12th Conference of the
European Chapter of the ACL (EACL 2009), pages
16–23, Athens, Greece.
Loic Barrault. 2010. MANY: Open source machine
translation system combination. Prague Bulletin
of Mathematical Linguistics, Special Issue on Open
Source Tools for Machine Translation, 93:147–155.
Peter F. Brown, Stephen A. Della Pietra, Vincent J.
Della Pietra, and Robert L. Mercer. 1993. The
mathematics of statistical machine translation: Pa-
rameter estimation. Computational Linguistics,
19(2):263–311.
Chris Callison-Burch, Philipp Koehn, Christof Monz,
and Josh Schroeder. 2009. Findings of the 2009
Workshop on Statistical Machine Translation. In
Proceedings of the ACL Fourth Workshop on Sta-
tistical Machine Translation, pages 1–28, Athens,
Greece.
Qin Gao and Stephan Vogel. 2008. Parallel implemen-
tations of word alignment tool. In Software Engi-
neering, Testing, and Quality Assurance for Natu-
ral Language Processing, pages 49–57, Columbus,
Ohio, June. Association for Computational Linguis-
tics.
</reference>
<page confidence="0.997966">
125
</page>
<table confidence="0.996500631578947">
Bitext #En Words Phrase-table news-test2008 newstest2009
(M) size (M) BLEU BLEU
1 Eparl+NC+UN 242 258 24.21 (0.01) 25.29 (0.12)
2 Eparl+NC+10� 163 203 24.24 (0.06) 25.51 (0.13)
3 1 357 385 24.46 (0.08) 25.73 (0.20)
4 Eparl+NC+UN+10� 357 221 24.42 (0.1) 25.84 (0.05)
1
Eparl+NC+UN+109 pruned
5 Eparl+NC+10� 280 330 24.43 (0.04) 25.68 (0.12)
2
6 Eparl+NC+News 103 188 24.27 (0.2) 25.70 (0.15)
7 Eparl+NC+109+News 218 321 24.51 (0.05) 25.83 (0.05)
8 Eparl+NC+UN+109+News 413 501 24.70 (0.1) 25.86 (0.14)
9 Eparl+NC+IR 69 81 24.14 (0.05) 25.17 (0.2)
10 Eparl+NC+News+IR 124 201 24.32 (0.12) 25.84 (0.17)
11 Eparl+NC+109+News+IR 239 333 24.54 (0.1) 26.03 (0.15)
12 Eparl+NC+102+News+IR 356 453 24.68 (0.04) 26.19 (0.05)
13 Eparl+NC+10�2+News+IR pruned 356 293 25.06 26.53
(+larger beam, +no-cutoff LM)
</table>
<tableCaption confidence="0.95764975">
Table 2: English–French results: number of English words (in million), number of entries in the phrase-
table (in million) and BLEU scores in the development (news-test2008) and internal test (newstest2009)
sets for the different systems developped. The BLEU scores and the number in parentheses are the
average and standard deviation over 3 values (see Section 3.)
</tableCaption>
<reference confidence="0.999530707317073">
Howard Johnson, Joel Martin, George Foster, and
Roland Kuhn. 2007. Improving translation qual-
ity by discarding most of the phrasetable. In Pro-
ceedings of the 2007 Joint Conference on Empirical
Methods in Natural Language Processing and Com-
putational Natural Language Learning (EMNLP-
CoNLL), pages 967–975, Prague, Czech Republic.
Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003. Statistical phrased-based machine translation.
In HLT/NACL, pages 127–133.
Philipp Koehn et al. 2007. Moses: Open source toolkit
for statistical machine translation. In ACL, demon-
stration session.
Franz Josef Och and Hermann Ney. 2002. Discrim-
inative training and maximum entropy models for
statistical machine translation. In Proc. of the An-
nual Meeting of the Association for Computational
Linguistics, pages 295–302.
Franz Josef Och and Hermann Ney. 2003. A sys-
tematic comparison of various statistical alignement
models. Computational Linguistics, 29(1):19–51.
Paul Ogilvie and Jamie Callan. 2001. Experiments
using the Lemur toolkit. In In Proceedings of the
Tenth Text Retrieval Conference (TREC-10), pages
103–108.
Holger Schwenk, Sadaf Abdul Rauf, Lo¨ıc Barrault,
and Jean Senellart. 2009. SMT and SPE machine
translation systems for WMT’09. In Proceedings of
the Fourth Workshop on Statistical Machine Trans-
lation, pages 130–134, Athens, Greece. Association
for Computational Linguistics.
Holger Schwenk. 2007. Continuous space language
models. Computer Speech and Language, 21:492–
518.
Holger Schwenk. 2008. Investigations on large-
scale lightly-supervised training for statistical ma-
chine translation. In IWSLT, pages 182–189.
A. Stolcke. 2002. SRILM: an extensible language
modeling toolkit. In Proc. of the Int. Conf. on Spo-
ken Language Processing, pages 901–904, Denver,
CO.
</reference>
<page confidence="0.998519">
126
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.493382">
<title confidence="0.618975">LIUM SMT Machine Translation System for WMT 2010</title>
<author confidence="0.890722">Patrik Lambert</author>
<author confidence="0.890722">Sadaf Abdul-Rauf</author>
<author confidence="0.890722">Holger</author>
<affiliation confidence="0.997619">LIUM, University of Le</affiliation>
<address confidence="0.982905">72085 Le Mans cedex 9,</address>
<email confidence="0.912251">FirstName.LastName@lium.univ-lemans.fr</email>
<abstract confidence="0.996651238095238">This paper describes the development of French–English and English–French machine translation systems for the 2010 WMT shared task evaluation. These systems were standard phrase-based statistical systems based on the Moses decoder, trained on the provided data only. Most of our efforts were devoted to the choice and extraction of bilingual data used for training. We filtered out some bilingual corpora and pruned the phrase table. We also investigated the impact of adding two types of additional bilingual texts, extracted automatically from the available monolingual data. We first collected bilingual data by performing automatic translations of monolingual texts. The second type of bilingual text was harvested from comparable corpora with Information Retrieval techniques.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Sadaf Abdul-Rauf</author>
<author>Holger Schwenk</author>
</authors>
<title>On the use of comparable corpora to improve SMT performance.</title>
<date>2009</date>
<booktitle>In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL</booktitle>
<pages>16--23</pages>
<location>Athens, Greece.</location>
<contexts>
<context position="5880" citStr="Abdul-Rauf and Schwenk (2009)" startWordPosition="964" endWordPosition="967">sulting bitext has no new words in the English side, since all words of the translation output come from the translation model, but it contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). Second, as in last year’s evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. This year we used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using information retrieval techniques. The Lemur toolkit (Ogilvie and Callan, 2001) was used for this purpose. Search was limited to a window of f5 days of the date of the French news text. The retrieved candidate sentences were then filtered using the Translation Error Rate (TER) with respect to the automatic translations. In this study, sentences with a TER below 65% for the French</context>
</contexts>
<marker>Abdul-Rauf, Schwenk, 2009</marker>
<rawString>Sadaf Abdul-Rauf and Holger Schwenk. 2009. On the use of comparable corpora to improve SMT performance. In Proceedings of the 12th Conference of the European Chapter of the ACL (EACL 2009), pages 16–23, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Loic Barrault</author>
</authors>
<title>MANY: Open source machine translation system combination.</title>
<date>2010</date>
<booktitle>Prague Bulletin of Mathematical Linguistics, Special Issue on Open Source Tools for Machine Translation,</booktitle>
<pages>93--147</pages>
<marker>Barrault, 2010</marker>
<rawString>Loic Barrault. 2010. MANY: Open source machine translation system combination. Prague Bulletin of Mathematical Linguistics, Special Issue on Open Source Tools for Machine Translation, 93:147–155.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Peter F Brown</author>
<author>Stephen A Della Pietra</author>
<author>Vincent J Della Pietra</author>
<author>Robert L Mercer</author>
</authors>
<title>The mathematics of statistical machine translation: Parameter estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>19</volume>
<issue>2</issue>
<contexts>
<context position="3499" citStr="Brown et al., 1993" startWordPosition="567" endWordPosition="570">ate translations of each other. We applied a lexical filter to discard them. Furthermore, some sentences of this corpus were extracted from web page menus and are not grammatical. Although we could have used a part of the menu items as a dictionary, for simplicity we applied an n-gram language model (LM) filter to remove all non-grammatical sentences. Thanks to this filter, sentences out of the language model domain (in this case, mainly the news domain), may also have been discarded because they contain many unknown or unfrequent n-grams. The lexical filter was based on the IBM model 1 cost (Brown et al., 1993) of each side of a sentence pair given the other side, normalised with respect to both sentence lengths. This filter 121 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 121–126, Uppsala, Sweden, 15-16 July 2010. c�2010 Association for Computational Linguistics was trained on a corpus composed of Eparl, NC, and UN data. The language model filter was an n-gram LM cost of the target sentence (see Section 3), normalised with respect to its length. This filter was trained with all monolingual resources available except the 109 data. We generated a fir</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>Peter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra, and Robert L. Mercer. 1993. The mathematics of statistical machine translation: Parameter estimation. Computational Linguistics, 19(2):263–311.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Chris Callison-Burch</author>
<author>Philipp Koehn</author>
<author>Christof Monz</author>
<author>Josh Schroeder</author>
</authors>
<date>2009</date>
<booktitle>Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the ACL Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>1--28</pages>
<location>Athens, Greece.</location>
<marker>Callison-Burch, Koehn, Monz, Schroeder, 2009</marker>
<rawString>Chris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the ACL Fourth Workshop on Statistical Machine Translation, pages 1–28, Athens, Greece.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Qin Gao</author>
<author>Stephan Vogel</author>
</authors>
<title>Parallel implementations of word alignment tool.</title>
<date>2008</date>
<booktitle>In Software Engineering, Testing, and Quality Assurance for Natural Language Processing,</booktitle>
<pages>49--57</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Columbus, Ohio,</location>
<contexts>
<context position="9013" citStr="Gao and Vogel, 2008" startWordPosition="1485" endWordPosition="1488">ure functions hi are the system models and the Ai weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are extracted using the default settings of the Moses toolkit. The parameters of Moses were tuned on newstest2008, using the ‘new’ MERT tool. We repeated the training process three times, each with a different seed value for the optimisation algorithm. In this way we have an rough idea of the error introduced by the tuning process. 4-gram back-off LMs were used. The word list contains all the words of the bitext used to train the translation model and all words that ap</context>
</contexts>
<marker>Gao, Vogel, 2008</marker>
<rawString>Qin Gao and Stephan Vogel. 2008. Parallel implementations of word alignment tool. In Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 49–57, Columbus, Ohio, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Howard Johnson</author>
<author>Joel Martin</author>
<author>George Foster</author>
<author>Roland Kuhn</author>
</authors>
<title>Improving translation quality by discarding most of the phrasetable.</title>
<date>2007</date>
<booktitle>In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL),</booktitle>
<pages>967--975</pages>
<location>Prague, Czech Republic.</location>
<marker>Johnson, Martin, Foster, Kuhn, 2007</marker>
<rawString>Howard Johnson, Joel Martin, George Foster, and Roland Kuhn. 2007. Improving translation quality by discarding most of the phrasetable. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLPCoNLL), pages 967–975, Prague, Czech Republic.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
<author>Franz Josef Och</author>
<author>Daniel Marcu</author>
</authors>
<title>Statistical phrased-based machine translation.</title>
<date>2003</date>
<booktitle>In HLT/NACL,</booktitle>
<pages>127--133</pages>
<contexts>
<context position="8223" citStr="Koehn et al., 2003" startWordPosition="1349" endWordPosition="1352">breviations for the French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the multi-bleu.perl tool and are case sensitive. The BLEU score was one of metrics with the best correlation with human ratings in last year evaluation (CallisonBurch et al., 2009) for the French–English and English–French directions. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence a from a source sentence f. It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the 122 translation process: e* = arg max e � = arg max {exp( e i The feature functions hi are the system models and the Ai weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SM</context>
</contexts>
<marker>Koehn, Och, Marcu, 2003</marker>
<rawString>Philipp Koehn, Franz Josef Och, and Daniel Marcu. 2003. Statistical phrased-based machine translation. In HLT/NACL, pages 127–133.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Philipp Koehn</author>
</authors>
<title>Moses: Open source toolkit for statistical machine translation.</title>
<date>2007</date>
<booktitle>In ACL, demonstration session.</booktitle>
<marker>Koehn, 2007</marker>
<rawString>Philipp Koehn et al. 2007. Moses: Open source toolkit for statistical machine translation. In ACL, demonstration session.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>Discriminative training and maximum entropy models for statistical machine translation.</title>
<date>2002</date>
<booktitle>In Proc. of the Annual Meeting of the Association for Computational Linguistics,</booktitle>
<pages>295--302</pages>
<contexts>
<context position="8546" citStr="Och and Ney, 2002" startWordPosition="1410" endWordPosition="1413"> et al., 2009) for the French–English and English–French directions. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence a from a source sentence f. It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the 122 translation process: e* = arg max e � = arg max {exp( e i The feature functions hi are the system models and the Ai weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit (Koehn et al., 2007) and constructed as follows. First, word alignments in both directions are calculated. We used a multi-threaded version of the GIZA++ tool (Gao and Vogel, 2008).1 This speeds up the process and corrects an error of GIZA++ that can appear with rare words. Phrases and lexical reorderings are ex</context>
</contexts>
<marker>Och, Ney, 2002</marker>
<rawString>Franz Josef Och and Hermann Ney. 2002. Discriminative training and maximum entropy models for statistical machine translation. In Proc. of the Annual Meeting of the Association for Computational Linguistics, pages 295–302.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Josef Och</author>
<author>Hermann Ney</author>
</authors>
<title>A systematic comparison of various statistical alignement models.</title>
<date>2003</date>
<journal>Computational Linguistics,</journal>
<volume>29</volume>
<issue>1</issue>
<contexts>
<context position="8243" citStr="Och and Ney, 2003" startWordPosition="1353" endWordPosition="1356">French tokenizer. All our models are case sensitive and include punctuation. The BLEU scores reported in this paper were calculated with the multi-bleu.perl tool and are case sensitive. The BLEU score was one of metrics with the best correlation with human ratings in last year evaluation (CallisonBurch et al., 2009) for the French–English and English–French directions. 3 Architecture of the SMT system The goal of statistical machine translation (SMT) is to produce a target sentence a from a source sentence f. It is today common practice to use phrases as translation units (Koehn et al., 2003; Och and Ney, 2003) and a log linear framework in order to introduce several models explaining the 122 translation process: e* = arg max e � = arg max {exp( e i The feature functions hi are the system models and the Ai weights are typically optimized to maximize a scoring function on a development set (Och and Ney, 2002). In our system fourteen features functions were used, namely phrase and lexical translation probabilities in both directions, seven features for the lexicalized distortion model, a word and a phrase penalty and a target language model (LM). The system is based on the Moses SMT toolkit (Koehn et </context>
</contexts>
<marker>Och, Ney, 2003</marker>
<rawString>Franz Josef Och and Hermann Ney. 2003. A systematic comparison of various statistical alignement models. Computational Linguistics, 29(1):19–51.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Paul Ogilvie</author>
<author>Jamie Callan</author>
</authors>
<title>Experiments using the Lemur toolkit. In</title>
<date>2001</date>
<booktitle>In Proceedings of the Tenth Text Retrieval Conference (TREC-10),</booktitle>
<pages>103--108</pages>
<contexts>
<context position="6177" citStr="Ogilvie and Callan, 2001" startWordPosition="1015" endWordPosition="1018">matically extracted and aligned parallel sentences from comparable in-domain corpora. This year we used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using information retrieval techniques. The Lemur toolkit (Ogilvie and Callan, 2001) was used for this purpose. Search was limited to a window of f5 days of the date of the French news text. The retrieved candidate sentences were then filtered using the Translation Error Rate (TER) with respect to the automatic translations. In this study, sentences with a TER below 65% for the French–English system and 75% for the English–French system were kept. Sentences with a large length difference (French versus English) or containing a large fraction of numbers were also discarded. By these means, about 15M words of additional bitexts were obtained to include in the French–English sys</context>
</contexts>
<marker>Ogilvie, Callan, 2001</marker>
<rawString>Paul Ogilvie and Jamie Callan. 2001. Experiments using the Lemur toolkit. In In Proceedings of the Tenth Text Retrieval Conference (TREC-10), pages 103–108.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
<author>Sadaf Abdul Rauf</author>
<author>Lo¨ıc Barrault</author>
<author>Jean Senellart</author>
</authors>
<title>SMT and SPE machine translation systems for WMT’09.</title>
<date>2009</date>
<booktitle>In Proceedings of the Fourth Workshop on Statistical Machine Translation,</booktitle>
<pages>130--134</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Athens, Greece.</location>
<contexts>
<context position="1345" citStr="Schwenk et al., 2009" startWordPosition="201" endWordPosition="204">extracted automatically from the available monolingual data. We first collected bilingual data by performing automatic translations of monolingual texts. The second type of bilingual text was harvested from comparable corpora with Information Retrieval techniques. 1 Introduction This paper describes the machine translation systems developed by the Computer Science laboratory at the University of Le Mans (LIUM) for the 2010 WMT shared task evaluation. We only considered the translation between French and English (in both directions). The main differences with respect to previous year’s system (Schwenk et al., 2009) are as follows: restriction to the data recommended for the workshop, usage of the (filtered) French–English gigaword bitext, pruning of the phrase table, and usage of automatic translations of the monolingual news corpus to improve the translation model. We also used a larger amount of bilingual data extracted from comparable corpora than was done in 2009. These different points are described in the rest of the paper, together with a summary of the experimental results showing the impact of each component. 2 Resources Used The following sections describe how the resources provided or allowed</context>
</contexts>
<marker>Schwenk, Rauf, Barrault, Senellart, 2009</marker>
<rawString>Holger Schwenk, Sadaf Abdul Rauf, Lo¨ıc Barrault, and Jean Senellart. 2009. SMT and SPE machine translation systems for WMT’09. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 130–134, Athens, Greece. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Continuous space language models.</title>
<date>2007</date>
<journal>Computer Speech and Language,</journal>
<volume>21</volume>
<pages>518</pages>
<marker>Schwenk, 2007</marker>
<rawString>Holger Schwenk. 2007. Continuous space language models. Computer Speech and Language, 21:492– 518.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Holger Schwenk</author>
</authors>
<title>Investigations on largescale lightly-supervised training for statistical machine translation.</title>
<date>2008</date>
<booktitle>In IWSLT,</booktitle>
<pages>182--189</pages>
<contexts>
<context position="5505" citStr="Schwenk, 2008" startWordPosition="907" endWordPosition="908">the UN corpus seem to be out-of domain for this task. We used two types of automatically extracted resources to adapt our system to the task domain. First, we generated automatic translations of the French News corpus provided (231M words), and selected the sentences with a normalised translation cost (returned by the decoder) inferior to a threshold. The resulting bitext has no new words in the English side, since all words of the translation output come from the translation model, but it contains new combinations (phrases) of known words, and reinforces the probability of some phrase pairs (Schwenk, 2008). Second, as in last year’s evaluation, we automatically extracted and aligned parallel sentences from comparable in-domain corpora. This year we used the AFP and APW news texts since there are available in the French and English LDC Gigaword corpora. The general architecture of our parallel sentence extraction system is described in detail by Abdul-Rauf and Schwenk (2009). We first translated 91M words from French into English using our first stage SMT system. These English sentences were then used to search for translations in the English AFP and APW texts of the Gigaword corpus using inform</context>
</contexts>
<marker>Schwenk, 2008</marker>
<rawString>Holger Schwenk. 2008. Investigations on largescale lightly-supervised training for statistical machine translation. In IWSLT, pages 182–189.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Stolcke</author>
</authors>
<title>SRILM: an extensible language modeling toolkit.</title>
<date>2002</date>
<booktitle>In Proc. of the Int. Conf. on Spoken Language Processing,</booktitle>
<pages>901--904</pages>
<location>Denver, CO.</location>
<contexts>
<context position="9884" citStr="Stolcke, 2002" startWordPosition="1639" endWordPosition="1640"> MERT tool. We repeated the training process three times, each with a different seed value for the optimisation algorithm. In this way we have an rough idea of the error introduced by the tuning process. 4-gram back-off LMs were used. The word list contains all the words of the bitext used to train the translation model and all words that appear at least ten times in the monolingual corpora. Words of the monolingual corpora containing special characters or sequences of uppercase characters were not included in the word list. Separate LMs were build on each data source with the SRI LM toolkit (Stolcke, 2002) and then linearly interpolated, optimizing the coefficients with an EM procedure. The perplexities of these LMs were 103.4 for French and 149.2 for English. 4 Results and Discussion The results of our SMT system for the French– English and English–French tasks are summarized in Tables 1 and 2, respectively. The MT metric scores are the average of three optimisations performed with different seeds (see Section 3). The 1The source is available at http://www.cs.cmu. edu/˜qing/ numbers in parentheses are the standard deviation of these three values. The standard deviation gives a lower bound of t</context>
</contexts>
<marker>Stolcke, 2002</marker>
<rawString>A. Stolcke. 2002. SRILM: an extensible language modeling toolkit. In Proc. of the Int. Conf. on Spoken Language Processing, pages 901–904, Denver, CO.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>