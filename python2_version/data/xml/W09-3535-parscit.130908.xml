<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000001">
<title confidence="0.998074">
Tag Confidence Measure for Semi-Automatically Updating
Named Entity Recognition
</title>
<author confidence="0.569929">
Kuniko Saito and Kenji Imamura
</author>
<affiliation confidence="0.443911">
NTT Cyber Space Laboratories, NTT Corporation
</affiliation>
<address confidence="0.907416">
1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan
</address>
<email confidence="0.999532">
{saito.kuniko, imamura.kenji}@lab.ntt.co.jp
</email>
<sectionHeader confidence="0.993915" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999603666666667">
We present two techniques to reduce ma-
chine learning cost, i.e., cost of manually
annotating unlabeled data, for adapting
existing CRF-based named entity recog-
nition (NER) systems to new texts or
domains. We introduce the tag posterior
probability as the tag confidence measure
of an individual NE tag determined by
the base model. Dubious tags are auto-
matically detected as recognition errors,
and regarded as targets of manual correc-
tion. Compared to entire sentence poste-
rior probability, tag posterior probability
has the advantage of minimizing system
cost by focusing on those parts of the
sentence that require manual correction.
Using the tag confidence measure, the
first technique, known as active learning,
asks the editor to assign correct NE tags
only to those parts that the base model
could not assign tags confidently. Active
learning reduces the learning cost by
66%, compared to the conventional
method. As the second technique, we
propose bootstrapping NER, which semi-
automatically corrects dubious tags and
updates its model.
</bodyText>
<sectionHeader confidence="0.999129" genericHeader="keywords">
1 Introduction
</sectionHeader>
<bodyText confidence="0.998105444444444">
Machine learning, especially supervised learning,
has achieved great success in many natural lan-
guage tasks, such as part-of-speech (POS) tag-
ging, named entity recognition (NER), and pars-
ing. This approach automatically encodes lin-
guistic knowledge as statistical parameters
(models) from large annotated corpora. In the
NER task, which is the focus of this paper, se-
quential tagging1 based on statistical models is
</bodyText>
<footnote confidence="0.814658">
1Tags are assigned to each input unit (e.g., word) one by one.
</footnote>
<bodyText confidence="0.99984195">
similarly used; studies include Conditional Ran-
dom Fields (CRFs; Lafferty et al., 2001, Suzuki
et al., 2006). However, the manual costs incurred
in creating annotated corpora are extremely high.
On the other hand, Consumer Generated Me-
dia (CGM) such as blog texts has attracted a lot
of attention recently as an informative resource
for information retrieval and information extrac-
tion tasks. CGM has two distinctive features;
enormous quantities of new texts are generated
day after day, and new vocabularies and topics
come and go rapidly. The most effective ap-
proach to keep up with new linguistic phenom-
ena is creating new annotated corpora for model
re-training at short intervals. However, it is diffi-
cult to build new corpora expeditiously because
of the high manual costs imposed by traditional
schemes.
To reduce the manual labor and costs, vari-
ous learning methods, such as active learning
(Shen et al., 2004, Laws and Schütze, 2008),
semi-supervised learning (Suzuki and Isozaki,
2008) and bootstrapping (Etzioni, 2005) have
been proposed. Active learning automatically
selects effective texts to be annotated from huge
raw-text corpora. The correct answers are then
manually annotated, and the model is re-trained.
In active learning, one major issue is data selec-
tion, namely, determining which sample data is
most effective. The data units used in conven-
tional methods are sentences.
Automatically creating annotated corpora
would dramatically decrease the manual costs. In
fact, there always are some recognition errors in
any automatically annotated corpus and the edi-
tor has to correct errors one by one. Since sen-
tences are used as data units, the editor has to pay
attention to all tags in the selected sentence be-
cause it is not obvious where the recognition er-
ror is. However, it is a waste of manual effort to
</bodyText>
<page confidence="0.966955">
168
</page>
<note confidence="0.9990115">
Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 168–176,
Suntec, Singapore, 7 August 2009. c�2009 ACL and AFNLP
</note>
<bodyText confidence="0.985976782608696">
ti,
annotate all tags because most tags must be la-
beled correctly by the base model2.
In this paper, we propose a confidence meas-
ure based on tag posterior probability for the
NER task. Our method does not use the confi-
dence of a sentence, but instead computes the
confidence of the tag assigned to each word. The
tag confidence measure allows the sentence to
which the base model might assign an incorrect
tag to be selected automatically. Active learning
becomes more efficient because we correct only
those tags that have low confidence (cf. Sec. 4).
We can realize the same effect as active
learning if we can automatically correct the se-
lected data based upon our tag confidence meas-
ure. Our proposal &amp;quot;Semi-Automatically Updating
NER&amp;quot; automatically corrects erroneous data by
using a seed NE list generated from other infor-
mation sources. Semi-Automatically Updating
NER easily keeps up with new words because it
enables us to update the model simply by provid-
ing a new NE list (cf. Sec. 5).
</bodyText>
<sectionHeader confidence="0.97417" genericHeader="introduction">
2 Named Entity Recognition Task
</sectionHeader>
<bodyText confidence="0.962670272727273">
The NER task is to recognize entity names such
as organizations and people. In this paper, we use
17 NE tags based on the IOB2 scheme (Sang and
De Meulder, 1999) combined with eight
Japanese NE types defined in the IREX
workshop (IREX 1999) as shown in Table 1.
For example, “ 東 京 (Tokyo)/ 都 (City)/ に
(in)” is labeled like this:
“東京/B-&lt;LOC&gt; 都/I-&lt;LOC&gt; に/O”.
This task is regarded as the sequential tagging
problem, i.e., assigning NE tag sequences
T = t1 &amp;quot;&apos;tn to word sequences W = w1 &amp;quot;&apos; wn .
Recently, discriminative models such as
Conditional Random Fields (CRFs) have been
successfully applied to this task (Lafferty et al.,
2001). In this paper, we use linear-chain CRFs
based on the Minimum Classification Error
framework (Suzuki et al., 2006). The posterior
probability of a tag sequence is calculated as
follows:
where wi and ti are the i-th word and its
corresponding NE tag, respectively. fa (ti , wi )
</bodyText>
<footnote confidence="0.6680705">
2 A base model is the initial model trained with the initial
annotated corpora.
</footnote>
<bodyText confidence="0.99150575">
and fb (ti−1, ti) is a feature function 3 . λa and
λb is a parameter to be estimated from the
training data. Z(W) is a normalization factor
over all candidate paths expressed as follows:
</bodyText>
<equation confidence="0.9726056">
n
Z(W) exp{ (
= Σ Σ Σ λa ⋅ fa (ti, wi
a
=
</equation>
<listItem confidence="0.54139">
The best tag sequence that maximizes Formula
(1) is located using the Viterbi algorithm.
</listItem>
<tableCaption confidence="0.98503">
Table 1. NE Types and Tags.
</tableCaption>
<sectionHeader confidence="0.904830916666667" genericHeader="method">
NE Types NE Tags
PERSON B-&lt;PSN&gt; I-&lt;PSN&gt;
LOCATION B-&lt;LOC&gt; I-&lt;LOC&gt;
ORGANIZATION B-&lt;ORG&gt; I-&lt;ORG&gt;
ARTIFACT B-&lt;ART&gt; I-&lt;ART&gt;
DATE B-&lt;DAT&gt; I-&lt;DAT&gt;
TIME B-&lt;TIM&gt; I-&lt;TIM&gt;
MONEY B-&lt;MNY&gt; I-&lt;MNY&gt;
PERCENT B-&lt;PCT&gt; I-&lt;PCT&gt;
outside an NE O
3 Error Detection with Tag Confidence
Measure
</sectionHeader>
<subsectionHeader confidence="0.999897">
3.1 Tag Posterior Probability
</subsectionHeader>
<bodyText confidence="0.999878142857143">
It is quite natural to consider sentence posterior
probability as a confidence measure of the esti-
mated tag sequences. We focus on tag posterior
probability, and regard it as the confidence
measure of the decoded tag itself. Our method
tries to detect the recognition error of each tag by
referring to the tag confidence measure.
</bodyText>
<figureCaption confidence="0.709319">
Figure 1 overviews the calculation of tag
confidence measure. The confidence score of tag
</figureCaption>
<bodyText confidence="0.997233">
j , which is a candidate tag for word wi , is
calculated as follows:
</bodyText>
<equation confidence="0.9534724">
P(t  |W) = P(t , T  |W),
i,j ∑ i,j (3)
T
where ∑P(ti, j,T  |W)is the summation of all NE
T
</equation>
<bodyText confidence="0.999536166666667">
tag sequences that pass through ti,j . This prob-
ability is generally called the marginal probabil-
ity. j= 1 ,&amp;quot;&apos;,k represents the number of NE
tags shown in Table 1( i.e., k=17 in this paper).
The tag confidence score of ti,j can be cal-
culated efficiently using forward and backward
</bodyText>
<footnote confidence="0.834293666666667">
3 We used n-grams (n=1, 2, 3) of surface forms and parts-
of-speech within a five word window and 2-gram combina-
tions of NE tags as the feature set.
</footnote>
<equation confidence="0.95007434375">
+Σ
(, ))},
t t
i − 1 i
⋅ fb
b
λb
n
1
= exp{ (
Σ Σ ⋅
λa fa (ti , wi
a
=
)
Z(W)
i
1
P T W
( |
)
(1)
)
T i
1
(2)
+Σ
(ti−1 , ti
b
))}.
⋅ fb
λb
</equation>
<page confidence="0.991344">
169
</page>
<subsectionHeader confidence="0.8548395">
The Word Sequence
The Tag Candidates
</subsectionHeader>
<equation confidence="0.788017727272727">
w1 w2 K wi −1 wi wi+1 K wn
&lt;S&gt; t1 ,1 t2, 1 K ti −1,1 ti , 1 ti+1,1 K tn ,1 &lt;/S&gt;
M M M M M M
t1,
M
t1,k t2,k K ti−1,k ti,k ti+1,k K tn,k
j
t 2,jK ti− ti , ti+1,j K t
1 , j j n,j
M
αi, j βi,j
</equation>
<figureCaption confidence="0.997942">
Figure 1. Overview of the tag confidence measure calculation.
</figureCaption>
<bodyText confidence="0.978465">
algorithms as follows (Manning and Schütze,
1999):
</bodyText>
<equation confidence="0.997817">
1
P(ti,j  |W) = Z(W)αi,j⋅flo, (4)
</equation>
<bodyText confidence="0.504999">
where
</bodyText>
<equation confidence="0.9871615">
α0,j = 1, (7)
fln+1, j =1. (8)
</equation>
<bodyText confidence="0.9997662">
In this manner, the confidence scores of all
tags of each word in a given sentence are calcu-
lated. The rejecter then refers to the highest tag
confidence score in judging whether the decoded
NE tag is correct or incorrect.
</bodyText>
<subsectionHeader confidence="0.991582">
3.2 Rejecter
</subsectionHeader>
<bodyText confidence="0.999855285714286">
The rejecter tries to detect dubious tags in the
NER result derived by the method described in
Section 2. For each word, the rejecter refers to
the decoded tag td, which maximizes Formula (1),
and the most confident tag t1, in terms of the pos-
terior probability as defined in Formula (4). The
judgment procedure is as follows:
</bodyText>
<listItem confidence="0.861712375">
[1] If td is NOT identical to t1, then td is deter-
mined to be dubious, and so is rejected as an
incorrect tag.4
[2] Else, if the confidence score of t1, called cs1,
is below the predefined threshold, td is de-
termined to be dubious, and so is rejected as
an incorrect tag.
[3] Otherwise, td is accepted as a correct tag.
</listItem>
<bodyText confidence="0.964744333333333">
4 The decoded tag td rarely disagrees with the most confi-
dent tag t1 due to a characteristic of the CRFs.
Increasing the threshold also increases the
number of rejected tags and manual annotation
cost. In practice, the threshold should be empiri-
cally set to achieve the lowest judgment error
rate using development data. There are two types
of judgment errors: false acceptance and false
rejection. False rejection is to reject a correct tag,
and false acceptance is to accept an incorrect tag
in error. The judgment error rate is taken as the
ratio of these two types of errors in all instances.
</bodyText>
<sectionHeader confidence="0.987748" genericHeader="method">
4 Active Learning
</sectionHeader>
<bodyText confidence="0.998289545454545">
Tag-wise recognition error detection is also help-
ful for data selection in active learning. If a sen-
tence contains several rejected tags, it contains
some new information which the base model
does not have. In other words, this sentence is
worth learning. Our approach, then, is to base
data selection (sentence selection) on the pres-
ence of rejected tags. However, it is not neces-
sary to check and correct all tags in each selected
sentence. We only have to check and correct the
rejected tags to acquire the annotated sentences.
</bodyText>
<figureCaption confidence="0.844363">
Figure 2 shows our active learning scheme.
Figure 2. Active Learning Scheme
</figureCaption>
<figure confidence="0.995195926470588">
()II,
ti � till
( )II,
ti-1 � ti
+Σ
b
⋅ fb
λb
+Σ
b
fb
λb
αi,j = ∑ {αi−1,k⋅exp{Σ λa
a
k
βi,j = ∑ {βi+1,k⋅exp {Σ λa ⋅ fa (
a
k
,
)
(
ti
⋅ fa
wi
)
, w
1 i + 1
ti
Recognition Error Detector
Calculation of the
Tag Confidence Measure
Morphological
Analyzer
Additional Data
(Unlabeled)
NER Decoder
Selected Data
Data Selection
Rejecter
Correct the Rejected
Tags by Hand
Model Learning
Base
Model
Base Data
(Labeled)
Manually
Corrected Data
Model
Re-training
Updated
Model
170
0 0.2 0.4 0.6 0.8 1
F-measure
0.78
0.76
0.74
0.72
0.68
0.66
0.64
0.62
0.8
0.7
0.6
Tag Base(proposed)
Sentence Base
</figure>
<bodyText confidence="0.999075538461538">
First, the NER decoder assigns an NE tag to each
word5 of the additional data using the base model
trained with the base data. The recognition error
detector then determines whether each tag can be
confidently accepted as described in Section 3. In
this step, the confidence score is calculated using
the same base model used for NER decoding.
Next, the sentences with at least one rejected tag
are selected. Only the rejected tags are manually
checked and corrected. Finally, the model is re-
trained and updated with the merged data con-
sisting of the manually corrected data and the
base data.
</bodyText>
<subsectionHeader confidence="0.850979">
4.1 Experiments
</subsectionHeader>
<bodyText confidence="0.999001384615385">
We evaluated the efficiency of our active learn-
ing method from the perspective of learning cost.
A blog corpus consisting of 45,694 sentences in
blog articles on the WWW was prepared for the
experiments. This corpus was divided into four
segments as shown in Table 2. All sentences
were manually annotated including additional
data. For additional data, these tags were initially
hidden and used only for simulating manual cor-
rection as shown below. Development data was
used for optimizing the threshold by measuring
the rejecter’s judgment error rate as described in
Subsection 3.2.
</bodyText>
<tableCaption confidence="0.998177">
Table 2. Data Used for Active Learning.
</tableCaption>
<table confidence="0.986309">
Base Data 11,553 sentences, 162,227 words
Development Data 1,000 sentences, 19,710 words
Additional Data 32,163 sentences, 584,077 words
Test Data 978 sentences, 17,762 words
</table>
<bodyText confidence="0.905056">
We estimated the learning cost from the rate
of hand-labeled tags. The Word Check Rate
(WCR) represents the ratio of the number of the
words in the additional data that need to be
manually checked and annotated, to the total
number of words in the additional data, and is
expressed as follows:
WCR= Checked Tags / Total Words.
The system obtained various sizes of selected
data as the rejecter changed its threshold from
0.1 to 1.0 for data selection. Only the rejected
tags in the selected data were replaced with the
tags originally assigned by hand (i.e., correct
tags). This procedure simulates manual correc-
tion. The manually corrected data was merged
with the base data to update the base model.
5 The morphological analyzer segments an input sen-
tence into a word sequence and assigns parts-of-
speech to each word.
</bodyText>
<figure confidence="0.235854">
Word Check Rate
</figure>
<figureCaption confidence="0.999241">
Figure 3. Learning Curves.
</figureCaption>
<bodyText confidence="0.999914545454546">
We compared our method with data selection
based on the sentence confidence measure.
Posterior probabilities of sentences were used as
the confidence measure, and low-confidence
scoring sentences were selected. In contrast to
our active learning method, all tags in the se-
lected sentences were replaced with the correct
tags in this case.
We evaluated the effectiveness of the up-
dated models against the test data by F-measure
as follows:
</bodyText>
<sectionHeader confidence="0.76951" genericHeader="evaluation">
4.2 Results and Discussions
</sectionHeader>
<subsectionHeader confidence="0.968411">
4.2.1 Learning Curves and Accuracies
</subsectionHeader>
<bodyText confidence="0.982376846153846">
Figure 3 shows learning curves of two active
learning methods; one is based on our tag confi-
dence measure (Tag Based selection), and the
other is based on the sentence confidence meas-
ure (Sentence Based selection). In order to reach
the F-measure of approximately 0.76, Sentence
Based selection requires approximately 60% of
the entire data set to be checked by hand. In con-
trast, Tag Based selection requires only 20% or
thereabouts. In other words, our Tag Based selec-
tion technique basically matches the performance
of Sentence Based selection with only 1/3 of the
learning cost.
</bodyText>
<subsectionHeader confidence="0.822508">
4.2.2 Types of Tag Replacement
</subsectionHeader>
<bodyText confidence="0.998588">
We further investigated the effects of tag-based
judgment from the results of an experiment on
our Tag Based selection. We categorized tag re-
placements of the rejected tags into the following
four types:
</bodyText>
<listItem confidence="0.99595375">
• No Change: the rejected tag is replaced
with the same tag.
• O-to-BI: the rejected tag is an O-tag. It is
replaced with a B-tag or an I-tag.
</listItem>
<figure confidence="0.9445105">
x precision
+ precision
recall
.
(9)
=
F
2
x
recall
</figure>
<page confidence="0.94106">
171
</page>
<listItem confidence="0.91306875">
• BI-to-O: the rejected tag is a B-tag or an I-
tag. It is replaced with an O-tag.
• BI-to-BI: the rejected tag is a B-tag or an I-
tag. It is replaced with another B-tag or I-tag.
</listItem>
<bodyText confidence="0.999909190476191">
Table 3 shows the distribution of these four
categories in the selected data for the threshold
of 0.5. This threshold achieves the lowest judg-
ment error rate given the development set.
The rate of No Change replacement type is
the highest. This means that the rejecter rejected
too many tags, which actually did not need to be
checked by hand. Although this result does not
have a negative influence on the accuracy of the
updated model, it is not preferable from the
learning cost perspective. Further consideration
should be given in order to improve the rejecter&apos;s
judgment.
O-to-BI type accounts for the 2nd highest per-
centage of all replacements: it is almost one third
of all changes. Excluding No Change type (i.e.,
among O-to-BI, BI-to-O and BI-to-BI types), O-
to-BI type makes up nearly 60% of these three
replacement types. This result shows that there
were many new NEs not recognized by the base
model in the selected data.
</bodyText>
<tableCaption confidence="0.995916">
Table 3. The Distribution of Replacement Types.
</tableCaption>
<table confidence="0.998107333333333">
Replacement Type Frequency %
No Change 13,253 43.6
O-to-BI 10,042 33.0
BI-to-O 2,419 8.0
BI-to-BI 4,688 15.4
Total 30,402 100.0
</table>
<sectionHeader confidence="0.991743" genericHeader="evaluation">
5 Bootstrapping for NER
</sectionHeader>
<bodyText confidence="0.999975029411765">
As mentioned in Section 4, we have to correct an
O-tag to a B-tag or an I-tag in many cases, al-
most 60% of all actual corrections. This situation
arises from a characteristic of the NER task. In
the NER task, most NE tags in the entire corpus
are O-tags. In fact, we found that 91 % of all tags
were O-tags in the additional data discussed in
Section 4. Thus, when a new NE appears in a
sentence, this new NE is often mistakenly given
an O-tag by the base model.
The fact that only O-tags are dominant im-
plies that we have a chance to find a correct B-
tag or I-tag when we look up the 2nd candidate.
This is because one of these top two candidates
is inevitably a B-tag or an I-tag. Thus, it is valu-
able to consider what the NEXT preferable tag is
when the most preferable tag is rejected.
We examined in detail the accuracy of the tag
candidates when the threshold is 0.5 as summa-
rized in Table 4. When the top tag (i.e., the tag
with the highest tag confidence score) is accepted,
its accuracy is 94 %, obviously high. On the
other hand, the top tag’s accuracy is only 43 %
when it is rejected. However, focusing both on
the top tag and on the 2nd tag provides an oppor-
tunity to correct the rejected tag in this case. If
we consider these top two tags together when the
1st tag is rejected, the possibility of finding the
correct tag is 72 %, relatively high. This suggests
that the system is capable of correcting the re-
jected tag automatically by using the top two tag
candidates. On this background, automatic cor-
rection is attempted for re-training the model
through the use of a bootstrapping scheme.
</bodyText>
<tableCaption confidence="0.997056">
Table 4. Accuracy of the Tags.
</tableCaption>
<table confidence="0.59026025">
Rejecter’s Judgment of the Top Tag
ACCEPT REJECT
Top Tag Top Tag 2nd Tag
94 % 43 % 29 %
</table>
<bodyText confidence="0.632258533333333">
Figure 4 shows an example of the top two tag
candidates and their tag confidence scores when
the top tag’s confidence score is lower than the
threshold (=0.5). We call this lattice the “tag
graph” in this paper. The system failed to recog-
nize the movie title “3丁目の夕日” (“Sancho-
me no Yuuhi”, which means “Sunset on Third
Street”) as ARTIFACT only with the top tag
candidates. However, it may find a correct tag
sequence using the top two tag candidates
(shaded cells in Figure 4). Once the system iden-
tifies the correct tag sequence automatically in
the tag graph, the sequence is used as a manually
annotated sequence. We introduce this new tech-
nique, Semi-Automatically Updating NER.
</bodyText>
<figureCaption confidence="0.850247">
Figure 4. The Top Two Tag Candidates with
Tag Confidence Measures.
</figureCaption>
<table confidence="0.995948416666667">
Top Tag 2nd Tag
Tag score Tag scor
e
今日(Today) B-&lt;DAT&gt; 0.95
「(“) O 0.98
3(Third) O 0.47 B-&lt;ART&gt; 0.36
丁目(Street) O 0.38 I-&lt;ART&gt; 0.36
の(on) O 0.49 I-&lt;ART&gt; 0.38
夕日(Sunset) I-&lt;ART&gt; 0.39 O 0.34
」(”) O 0.99
が(is) O 0.99
放映(broadcast) O 0.99
</table>
<page confidence="0.98978">
172
</page>
<subsectionHeader confidence="0.9892">
5.1 Semi-Automatically Updating NER
</subsectionHeader>
<bodyText confidence="0.987647173913043">
By extracting the correct tag sequence in each
tag graph as shown in Figure 4, it is possible to
obtain automatically corrected data, which also
serve as new training data. Based on this idea,
we propose Semi-Automatically Updating NER,
which is hereafter simply referred to as Updating
NER.
Figure 5 overviews Updating NER. The re-
jecter produces the sentences with tag graphs
based on the tag confidence measure. In this new
procedure, however, the rejecter’s role differs
from that described in Section 4 as follows:
[1] When the highest confidence score cs1 equals
or exceeds the threshold, the rejecter accepts
only the top candidate tag t1, otherwise it
goes to Step 2.
[2] When cs1 is less than the threshold, the re-
jecter accepts not only the top tag t1 but also
the 2nd tag t2.
Sentences that contain the 2nd candidates are
selected in data selection for subsequent process-
ing. The correct tag sequence in each tag graph is
identified in automatic correction as follows:
</bodyText>
<listItem confidence="0.743421666666667">
[1] Select the tag sequence that has the longest6
and consistent NE from the tag graph.
[2] If the longest NE also exists in a seed NE list,
which will be described below, the system
extracts the entire sentence with its tag se-
quence as corrected data.
</listItem>
<bodyText confidence="0.999612294117647">
In Step 1, the system selects one preferable
tag sequence based on the longest NE match. In
the tag graph shown in Figure 4, there are 16
possible sequences because four words “3”, “丁
目(Street)”, “の (on)” and “夕日(Sunset)” each
have two tag candidates; O or B for “3”, O or I
for “丁目 (Street)” and “ の (on)”, and I or O for “
夕日(Sunset)”. For example, “B I I I”, “B I I O”,
“B I O O”, “O O O I”, “O O O O” and the rest.
Because the sequence “B I I I” constructs the
longest NE, the system selects the tag sequence
that contains the ARTIFACT “3丁目の夕日.”
Other sequences that contain partial NEs such as
“3”, “3 丁目 ”, “3 丁目の”, which are all ARTI-
FACTs, are ignored.
In Step 2, the system judges whether the tag
sequence selected in Step 1 is indeed correct.
</bodyText>
<footnote confidence="0.969261">
6 By longest, we mean the longest tag sequence that does
not include any O-tags.
</footnote>
<figureCaption confidence="0.9964965">
Figure 5. Semi-Automatically Updating
NER Scheme.
</figureCaption>
<bodyText confidence="0.999989227272728">
However, the system requires some hints to
judge the correctness, so we need to prepare a
seed NE list, which contains surface forms and
NE types. This list can be created by manually
annotation of possible NEs or automatic genera-
tion from other sources such as dictionaries.
When the same NE exists both in the selected tag
sequence and the seed NE list, the system re-
gards the selected tag sequence as reliable and
extracts it as automatically corrected data. Fi-
nally, the model is updated by merging the
automatically corrected data with the base data.
Bootstrapping means that data selection and
correction of the selected data are completely
automatic; we still have to prepare the seed NE
list somehow. Thus the learning cost is quite low
because we only need to provide an NE list as a
seed. Updating NER is capable of modifying the
model to keep up with the emergence of new
named entities. Therefore, it is effective to ana-
lyze the large amount of texts that emerge every-
day, such as blogs on the WWW.
</bodyText>
<subsectionHeader confidence="0.990155">
5.2 Experiments
</subsectionHeader>
<bodyText confidence="0.9999595">
We tested our Updating NER with a large
amount of blog texts from the WWW. One
week’s worth of blog texts was crawled on the
WWW to generate the additional data. Table 5
shows the statistics of the data used in our ex-
periments. The test data contained only the blog
texts generated in December 2006, and the base
data is about a half year older than the test data.
Therefore, it is difficult for the base model to
recognize new NEs in the test data. One week’s
</bodyText>
<figure confidence="0.99304012">
Recognition Error Detector
Calculation of the
Tag Confidence Measure
Selected Data
with Tag Graphs
Morphological
Analyzer
NER Decoder
Additional Data
(Unlabeled)
Data Selection
Rejecter
Automatic Correction
Model Learning
Base
Model
Base Data
(Labeled)
Seed NE List
Automatically
Corrected Data
Model
Re-training
Updated
Model
</figure>
<page confidence="0.997637">
173
</page>
<bodyText confidence="0.999671407407407">
worth of December 2006 blog texts were pre-
pared for bootstrapping. The overlap between the
test data and the additional data was removed in
advance. We set the rejecter’s threshold at 0.5
and selected the data with tag graphs from the
additional data.
Japanese Wikipedia entries were used as the
seed NE list. The titles of Wikipedia articles
were regarded as surface forms. NE types were
estimated from the category sections of each arti-
cle, based on heuristic rules prepared in advance.
We collected 104,296 entries as a seed NE list.
Using this seed list, Updating NER extracted
the seed NE and its context from the selected
data automatically. If the system found a match,
it extracted the sentence with its tag sequence
from the selected data. The automatically cor-
rected data was then merged with the base data in
order to re-train the base model.
For comparison, we evaluated the effect of the
seed NE list itself. If there is a sequence of words
that can be found in the seed list, then that se-
quence is always recognized as a NE. Note that
the other words are simply decoded using the
base model. We call this method ‘user diction-
ary’. Here, we use recall and precision to evalu-
ate the accuracy of the model.
</bodyText>
<tableCaption confidence="0.985348">
Table 5. Data Description for Updating NER.
</tableCaption>
<table confidence="0.99985575">
Base Data 43,716 sentences
(blog in Sep. 04-Jun. 06) 746,304 words
Additional Data 240,474 sentences
(one week’s blog in Dec. 06) 3,677,077 words
Selected Data from the 113,761 sentences
Additional Data 2,466,464 words
Test Data 1,609 sentences
(blog in Dec.06) 21,813 words
</table>
<subsectionHeader confidence="0.814254">
5.3 Results
</subsectionHeader>
<bodyText confidence="0.999972795454546">
Table 6 shows the details of accuracy results re-
garding the following four NE types: PERSON,
LOCATION, ORGANIZATION, and ARTI-
FACT, which are referred to hereafter as PSN,
LOC, ORG and ART, respectively. Although we
added Wikipedia as a user dictionary to the base
model, it only slightly improved the recall. In
fact, it has no positive and sometimes a negative
effect on precision (e.g., ART decreased from
0.666 to 0.619). This indicates that adding an NE
list as a dictionary is not enough to improve the
accuracy of a NER system. This is because the
NER system cannot discriminate an NE from
surrounding unrelated words. It simply extracts
matched sequences of words, so it overestimates
the number of NEs.
On the contrary, our Updating NER improved
both recall and precision (e.g., the recall and the
precision in ART improved from 0.320 to 0.364
and from 0.666 to 0.694, respectively.). This
means that not only the NE list but also the con-
texts are actually needed to retrain the model.
Our Updating NER scheme has the advantage of
finding the reliable context of a seed NE list
automatically. Although some manual effort is
needed to provide a seed NE list, its associated
cost is lower than the cost of annotating the en-
tire training data. Thus, we regard Updating NER
as a promising solution for reducing learning
cost in practical NER systems.
As shown in Table 6, neither user dictionary
method nor Updating NER improves the accu-
racy in ORG. We assume that this is caused by
the distribution of NE types in the seed NE list.
In the seed list selected from the Wikipedia en-
tries, PSN-type is dominant (74%). ORG-type is
scant at only 11%, so the system did not have
enough chances to retrain the ORG-type. Rather,
it might be the case that the system had a ten-
dency to recognize ORG-type as PSN-type be-
cause peoples&apos; names are often used as organiza-
tion names. Further investigation is needed to
clarify the impact of the distribution and the
quality of the seed NE list.
</bodyText>
<tableCaption confidence="0.586025">
Table 6. Details of Accuracy.
</tableCaption>
<table confidence="0.879072666666667">
PSN LOC ORG ART
Base Model rec. 0.640 0.737 0.688 0.320
prec. 0.699 0.811 0.652 0.666
+Wikipedia rec. 0.686 0.729 0.688 0.354
(user dic.)
prec. 0.716 0.815 0.654 0.619
+Wikipedia rec. 0.649 0.747 0.678 0.364
(UpdatingNER)
prec. 0.728 0.822 0.632 0.694
</table>
<subsectionHeader confidence="0.887015">
5.4 Discussions
</subsectionHeader>
<bodyText confidence="0.9999559375">
Compared to conventional machine learning
techniques, the most distinctive feature of Updat-
ing NER is that the system can focus on the top
two candidates when the confidence score of the
top candidate is low. This feature actually has a
great advantage in the NER task, because the
system is capable of determining what the next
preferable tag is when a new NE appears which
is assigned an O-tag by the base model.
Updating NER, however, has one weak point.
That is, the following two strict conditions are
required to correct the selected data automati-
cally. First, the correct tag sequence must appear
in tag graphs (i.e., as one of the top two tag can-
didates). Second, the NE must also appear in the
seed NE list. These conditions decrease the
</bodyText>
<page confidence="0.994424">
174
</page>
<bodyText confidence="0.999806375">
chance of extracting sentences with correct tag
sequences from the selected data.
To overcome this weakness, one practical
approach is to use Updating NER in combination
with active learning. In the case of active learn-
ing, we do not need the correct tags in the top
two candidates. The editor can assign correct
tags without considering the order of candidates.
In short, active learning has broad coverage in
terms of learning, while Updating NER does not.
Therefore, active learning is suitable for improv-
ing the performance level of the entire base
model. Updating NER has the advantage of stay-
ing current with new named entities which
emerge every day on the WWW. In practical use,
for example, it will be better to update the model
every week with Updating NER to keep up with
new named entities, and occasionally perform
active learning (every six months or so) to en-
hance the entire model. In the future, we plan to
evaluate the efficiency of our two learning meth-
ods in practical applications, such as domain ad-
aptation and acquisition of hot trend NE words
from blog texts on the WWW.
</bodyText>
<sectionHeader confidence="0.999918" genericHeader="related work">
6 Related Works
</sectionHeader>
<bodyText confidence="0.999990566666667">
To date, there have been many related works on
active learning not only for the NER task (Shen
et al., 2004, Laws and Schütze, 2008) but also
for other tasks, such as POS tagging (Engelson
and Dagan, 1996), text classification (Lewis and
Catlett, 1994), parsing (Hwa, 2000), and confu-
sion set disambiguation (Banko and Brill, 2001).
Active learning aims at effective data selection
based on criterion measures, such as the confi-
dence measure. Most previous works focus on
the Sentence-Based criterion evaluation and data
selection. Our proposal differs from those previ-
ous works in that we focus on the Tag-Based
strategy, which judges whether each tag should
be accepted or rejected. This approach maxi-
mizes the effectiveness of manual annotation by
leaving the accepted tags in without any manual
correction. As a result, our Tag-based approach
reduces the manual annotation cost by 66 %,
compared to the Sentence-Base method.
Semi-supervised learning has become an ac-
tive area in machine learning; it utilizes not only
annotated corpora but also huge amounts of plain
text for model training. Several studies adapted
semi-supervised learning to suit NLP tasks, such
as word sense disambiguation (Yarowsky, 1995),
text classification (Fujino et al., 2008), and
chunking and NER (Suzuki and Isozaki, 2008).
Suzuki and Isozaki (2008) suggest that a GIGA-
word size plain text corpus may further improve
the performance of the state-of-the-art NLP sys-
tem. In this paper, however, we aim at model
adaptation to the CGM domain to keep up with
the new linguistic phenomena that are emerging
every day. Because it is difficult to obtain GIGA-
word size plain text sets that reflect such new
linguistic phenomena, it is not practical to di-
rectly apply this approach to our task.
Bootstrapping is similar to semi-supervised
learning in that it also allows the use of plain text
(Etzioni 2005, Pantel and Pennacchioti 2006). In
this learning method, it is possible to extract new
instances automatically from plain text with
small seed data prepared manually. Our Updating
NER is similar to bootstrapping in that it extracts
new annotated corpora automatically from plain
text data starting with a seed NE list. However,
the goal of conventional bootstrapping is to de-
velop a new dictionary or thesaurus by extracting
new instances. On the contrary, our goal is to
acquire a new NE and its surrounding context in
a sentence, not to build a NE dictionary (i.e., cor-
rect tag sequence). It is the tag sequence and not
a single NE that is needed for model training.
Updating NER is a novel approach in the point
of applying bootstrapping to the framework of
supervised learning. This approach is quite effec-
tive in that it has the advantage of reducing learn-
ing cost compared with active learning because
only a seed NE list is needed.
</bodyText>
<sectionHeader confidence="0.999613" genericHeader="conclusions">
7 Conclusions
</sectionHeader>
<bodyText confidence="0.99997995">
To reduce machine learning cost, we introduced
two techniques that are based on a tag confidence
measure determined from tag posterior probabil-
ity. Dubious tags are automatically detected as
recognition errors using the tag confidence
measure. This approach maximizes the effective-
ness of manual annotation by leaving the confi-
dent tags in without any manual correction.
We first applied this technique to active
learning by correcting error tags manually. We
found that it matches the performance of the
learning method based on the sentence confi-
dence measure with only 1/3 of the learning cost.
Next, we proposed Semi-Automatic Updat-
ing NER which has a bootstrap learning scheme,
by expanding the scope from the top tag candi-
date to include the 2nd candidate. With this new
scheme, it is possible to collect auto-labeled data
from a large data source, such as blog texts on
the WWW, by simply providing a seed NE list.
</bodyText>
<page confidence="0.998267">
175
</page>
<sectionHeader confidence="0.995891" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999940733333333">
M. Banko and E. Brill. 2001. Scaling to Very Very
Large Corpora for Natural Language Disambigua-
tion. In Proc. of ACL-2001, pages 26-33.
S. A. Engelson and I. Dagan. 1999. Committee-Based
Sample Selection for Probabilistic Classifiers.
Journal of Artificial Intelligence Research,
vol.11(1999), pages 335-360.
O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T.
Shaked, S. Soderland, D. S. Weld, and A. Yates.
2005. Unsupervised Named-Entity Extraction from
the Web: An Experimental Study. Artificial Intelli-
gence, 165(1), pages 91-134.
A. Fujino, N. Ueda, and K. Saito. 2008. Semisuper-
vised Learning for a Hybrid Generative
/Discriminative Classifier Based on the Maximum
Entropy Principle. IEEE Transactions on Pattern
Analysis and Machine Intelligence (TPAMI), 30(3),
pages 424-437.
R. Hwa. 2000. Sample Selection for Statistical
Grammer Induction. In Proc. of EMNLP/VLC-2000,
pages 45-52.
IREX Committee (ed.), 1999. In Proc. of the IREX
workshop. http://nlp.cs.nyu.edu/irex/
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional Random Fields: Probabilistic Models for
Segmenting and Labeling Sequence Data. In Proc.
of ICML-2001. pages 282-289.
F. Laws and H. Schütze. 2008. Stopping Criteria for
Active Learning of Named Entity Recognition. In
Proc. of COLING-2008, pages 465-472.
D. Lewis and J. Gatlett. 1994. Heterogeneous uncer-
tainty sampling for supervised learning. In Proc. of
ICML-1994, pages 148-156.
C. D. Manning and H. Schütze. 1999. Foundations of
Statistical Natural Language Processing. The MIT
Press.
P. Pantel and M. Pennacchiotti. 2006. Espresso: Lev-
eraging Generic Patterns for Automatically Har-
vesting Semantic Relations. In Proc. of COLING-
ACL-2006, pages 113-120.
E. F. T. K. Sang and F. De Meulder. 1999. Represent-
ing text chunks. In Proc. of EACL-1999, pages
173-179.
D. Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan.
2004. Multi-Criteria-based Active Learning for
Named Entity Recognition. In Proc. of ACL-2004,
pages 589-596.
J. Suzuki and H. Izozaki. 2008. Semi-Supervised Se-
quential Labeling and Segmentation using Giga-
word Scale Unlabeled Data. In Proc. of ACL-2008,
pages 665-673.
J. Suzuki, E. McDermott, and H. Isozaki. 2006. Train-
ing Conditional Random Fields with Multivariate
Evaluation Measures. In Proc. of COLING-ACL-
2006. pages 617-624.
D. Yarowsky. 1995. Unsupervised Word Sense Dis-
ambiguation Rivaling Supervised Methods. In Proc.
of ACL-1995, pages 189-196.
X. Zhu. 2007. Semi-Supervised Learning, ICML-
2007 Tutorial.
</reference>
<page confidence="0.998741">
176
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.951427">
<title confidence="0.998406">Tag Confidence Measure for Semi-Automatically Updating Named Entity Recognition</title>
<author confidence="0.998379">Saito Imamura</author>
<affiliation confidence="0.999824">NTT Cyber Space Laboratories, NTT Corporation</affiliation>
<address confidence="0.985872">1-1 Hikarinooka, Yokosuka-shi, Kanagawa, 239-0847, Japan</address>
<email confidence="0.980209">saito.kuniko@lab.ntt.co.jp</email>
<email confidence="0.980209">imamura.kenji@lab.ntt.co.jp</email>
<abstract confidence="0.99947975">We present two techniques to reduce malearning cost, cost of manually annotating unlabeled data, for adapting existing CRF-based named entity recognition (NER) systems to new texts or We introduce the probability as the tag confidence measure of an individual NE tag determined by the base model. Dubious tags are automatically detected as recognition errors, and regarded as targets of manual correc- Compared to entire posterior probability, tag posterior probability has the advantage of minimizing system cost by focusing on those parts of the sentence that require manual correction. Using the tag confidence measure, the first technique, known as active learning, asks the editor to assign correct NE tags only to those parts that the base model could not assign tags confidently. Active learning reduces the learning cost by 66%, compared to the conventional method. As the second technique, we propose bootstrapping NER, which semiautomatically corrects dubious tags and updates its model.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>M Banko</author>
<author>E Brill</author>
</authors>
<title>Scaling to Very Very Large Corpora for Natural Language Disambiguation.</title>
<date>2001</date>
<booktitle>In Proc. of ACL-2001,</booktitle>
<pages>26--33</pages>
<contexts>
<context position="28507" citStr="Banko and Brill, 2001" startWordPosition="4969" endWordPosition="4972">perform active learning (every six months or so) to enhance the entire model. In the future, we plan to evaluate the efficiency of our two learning methods in practical applications, such as domain adaptation and acquisition of hot trend NE words from blog texts on the WWW. 6 Related Works To date, there have been many related works on active learning not only for the NER task (Shen et al., 2004, Laws and Schütze, 2008) but also for other tasks, such as POS tagging (Engelson and Dagan, 1996), text classification (Lewis and Catlett, 1994), parsing (Hwa, 2000), and confusion set disambiguation (Banko and Brill, 2001). Active learning aims at effective data selection based on criterion measures, such as the confidence measure. Most previous works focus on the Sentence-Based criterion evaluation and data selection. Our proposal differs from those previous works in that we focus on the Tag-Based strategy, which judges whether each tag should be accepted or rejected. This approach maximizes the effectiveness of manual annotation by leaving the accepted tags in without any manual correction. As a result, our Tag-based approach reduces the manual annotation cost by 66 %, compared to the Sentence-Base method. Se</context>
</contexts>
<marker>Banko, Brill, 2001</marker>
<rawString>M. Banko and E. Brill. 2001. Scaling to Very Very Large Corpora for Natural Language Disambiguation. In Proc. of ACL-2001, pages 26-33.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S A Engelson</author>
<author>I Dagan</author>
</authors>
<title>Committee-Based Sample Selection for Probabilistic Classifiers.</title>
<date>1999</date>
<journal>Journal of Artificial Intelligence Research,</journal>
<volume>11</volume>
<issue>1999</issue>
<pages>335--360</pages>
<marker>Engelson, Dagan, 1999</marker>
<rawString>S. A. Engelson and I. Dagan. 1999. Committee-Based Sample Selection for Probabilistic Classifiers. Journal of Artificial Intelligence Research, vol.11(1999), pages 335-360.</rawString>
</citation>
<citation valid="true">
<authors>
<author>O Etzioni</author>
<author>M Cafarella</author>
<author>D Downey</author>
<author>A Popescu</author>
<author>T Shaked</author>
<author>S Soderland</author>
<author>D S Weld</author>
<author>A Yates</author>
</authors>
<title>Unsupervised Named-Entity Extraction from the Web: An Experimental Study.</title>
<date>2005</date>
<journal>Artificial Intelligence,</journal>
<volume>165</volume>
<issue>1</issue>
<pages>91--134</pages>
<marker>Etzioni, Cafarella, Downey, Popescu, Shaked, Soderland, Weld, Yates, 2005</marker>
<rawString>O. Etzioni, M. Cafarella, D. Downey, A. Popescu, T. Shaked, S. Soderland, D. S. Weld, and A. Yates. 2005. Unsupervised Named-Entity Extraction from the Web: An Experimental Study. Artificial Intelligence, 165(1), pages 91-134.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A Fujino</author>
<author>N Ueda</author>
<author>K Saito</author>
</authors>
<title>Semisupervised Learning for a Hybrid Generative /Discriminative Classifier Based on the Maximum Entropy Principle.</title>
<date>2008</date>
<journal>IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI),</journal>
<volume>30</volume>
<issue>3</issue>
<pages>424--437</pages>
<contexts>
<context position="29433" citStr="Fujino et al., 2008" startWordPosition="5111" endWordPosition="5114">hether each tag should be accepted or rejected. This approach maximizes the effectiveness of manual annotation by leaving the accepted tags in without any manual correction. As a result, our Tag-based approach reduces the manual annotation cost by 66 %, compared to the Sentence-Base method. Semi-supervised learning has become an active area in machine learning; it utilizes not only annotated corpora but also huge amounts of plain text for model training. Several studies adapted semi-supervised learning to suit NLP tasks, such as word sense disambiguation (Yarowsky, 1995), text classification (Fujino et al., 2008), and chunking and NER (Suzuki and Isozaki, 2008). Suzuki and Isozaki (2008) suggest that a GIGAword size plain text corpus may further improve the performance of the state-of-the-art NLP system. In this paper, however, we aim at model adaptation to the CGM domain to keep up with the new linguistic phenomena that are emerging every day. Because it is difficult to obtain GIGAword size plain text sets that reflect such new linguistic phenomena, it is not practical to directly apply this approach to our task. Bootstrapping is similar to semi-supervised learning in that it also allows the use of p</context>
</contexts>
<marker>Fujino, Ueda, Saito, 2008</marker>
<rawString>A. Fujino, N. Ueda, and K. Saito. 2008. Semisupervised Learning for a Hybrid Generative /Discriminative Classifier Based on the Maximum Entropy Principle. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 30(3), pages 424-437.</rawString>
</citation>
<citation valid="true">
<authors>
<author>R Hwa</author>
</authors>
<title>Sample Selection for Statistical Grammer Induction.</title>
<date>2000</date>
<booktitle>In Proc. of EMNLP/VLC-2000,</booktitle>
<pages>45--52</pages>
<contexts>
<context position="28449" citStr="Hwa, 2000" startWordPosition="4962" endWordPosition="4963"> up with new named entities, and occasionally perform active learning (every six months or so) to enhance the entire model. In the future, we plan to evaluate the efficiency of our two learning methods in practical applications, such as domain adaptation and acquisition of hot trend NE words from blog texts on the WWW. 6 Related Works To date, there have been many related works on active learning not only for the NER task (Shen et al., 2004, Laws and Schütze, 2008) but also for other tasks, such as POS tagging (Engelson and Dagan, 1996), text classification (Lewis and Catlett, 1994), parsing (Hwa, 2000), and confusion set disambiguation (Banko and Brill, 2001). Active learning aims at effective data selection based on criterion measures, such as the confidence measure. Most previous works focus on the Sentence-Based criterion evaluation and data selection. Our proposal differs from those previous works in that we focus on the Tag-Based strategy, which judges whether each tag should be accepted or rejected. This approach maximizes the effectiveness of manual annotation by leaving the accepted tags in without any manual correction. As a result, our Tag-based approach reduces the manual annotat</context>
</contexts>
<marker>Hwa, 2000</marker>
<rawString>R. Hwa. 2000. Sample Selection for Statistical Grammer Induction. In Proc. of EMNLP/VLC-2000, pages 45-52.</rawString>
</citation>
<citation valid="true">
<date>1999</date>
<booktitle>In Proc. of the IREX workshop. http://nlp.cs.nyu.edu/irex/</booktitle>
<editor>IREX Committee (ed.),</editor>
<marker>1999</marker>
<rawString>IREX Committee (ed.), 1999. In Proc. of the IREX workshop. http://nlp.cs.nyu.edu/irex/</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data.</title>
<date>2001</date>
<booktitle>In Proc. of ICML-2001.</booktitle>
<pages>282--289</pages>
<contexts>
<context position="1893" citStr="Lafferty et al., 2001" startWordPosition="277" endWordPosition="280">ects dubious tags and updates its model. 1 Introduction Machine learning, especially supervised learning, has achieved great success in many natural language tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and parsing. This approach automatically encodes linguistic knowledge as statistical parameters (models) from large annotated corpora. In the NER task, which is the focus of this paper, sequential tagging1 based on statistical models is 1Tags are assigned to each input unit (e.g., word) one by one. similarly used; studies include Conditional Random Fields (CRFs; Lafferty et al., 2001, Suzuki et al., 2006). However, the manual costs incurred in creating annotated corpora are extremely high. On the other hand, Consumer Generated Media (CGM) such as blog texts has attracted a lot of attention recently as an informative resource for information retrieval and information extraction tasks. CGM has two distinctive features; enormous quantities of new texts are generated day after day, and new vocabularies and topics come and go rapidly. The most effective approach to keep up with new linguistic phenomena is creating new annotated corpora for model re-training at short intervals.</context>
<context position="5441" citStr="Lafferty et al., 2001" startWordPosition="867" endWordPosition="870">k is to recognize entity names such as organizations and people. In this paper, we use 17 NE tags based on the IOB2 scheme (Sang and De Meulder, 1999) combined with eight Japanese NE types defined in the IREX workshop (IREX 1999) as shown in Table 1. For example, “ 東 京 (Tokyo)/ 都 (City)/ に (in)” is labeled like this: “東京/B-&lt;LOC&gt; 都/I-&lt;LOC&gt; に/O”. This task is regarded as the sequential tagging problem, i.e., assigning NE tag sequences T = t1 &amp;quot;&apos;tn to word sequences W = w1 &amp;quot;&apos; wn . Recently, discriminative models such as Conditional Random Fields (CRFs) have been successfully applied to this task (Lafferty et al., 2001). In this paper, we use linear-chain CRFs based on the Minimum Classification Error framework (Suzuki et al., 2006). The posterior probability of a tag sequence is calculated as follows: where wi and ti are the i-th word and its corresponding NE tag, respectively. fa (ti , wi ) 2 A base model is the initial model trained with the initial annotated corpora. and fb (ti−1, ti) is a feature function 3 . λa and λb is a parameter to be estimated from the training data. Z(W) is a normalization factor over all candidate paths expressed as follows: n Z(W) exp{ ( = Σ Σ Σ λa ⋅ fa (ti, wi a = The best tag</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data. In Proc. of ICML-2001. pages 282-289.</rawString>
</citation>
<citation valid="true">
<authors>
<author>F Laws</author>
<author>H Schütze</author>
</authors>
<title>Stopping Criteria for Active Learning of Named Entity Recognition.</title>
<date>2008</date>
<booktitle>In Proc. of COLING-2008,</booktitle>
<pages>465--472</pages>
<contexts>
<context position="2750" citStr="Laws and Schütze, 2008" startWordPosition="416" endWordPosition="419">ive resource for information retrieval and information extraction tasks. CGM has two distinctive features; enormous quantities of new texts are generated day after day, and new vocabularies and topics come and go rapidly. The most effective approach to keep up with new linguistic phenomena is creating new annotated corpora for model re-training at short intervals. However, it is difficult to build new corpora expeditiously because of the high manual costs imposed by traditional schemes. To reduce the manual labor and costs, various learning methods, such as active learning (Shen et al., 2004, Laws and Schütze, 2008), semi-supervised learning (Suzuki and Isozaki, 2008) and bootstrapping (Etzioni, 2005) have been proposed. Active learning automatically selects effective texts to be annotated from huge raw-text corpora. The correct answers are then manually annotated, and the model is re-trained. In active learning, one major issue is data selection, namely, determining which sample data is most effective. The data units used in conventional methods are sentences. Automatically creating annotated corpora would dramatically decrease the manual costs. In fact, there always are some recognition errors in any a</context>
<context position="28308" citStr="Laws and Schütze, 2008" startWordPosition="4938" endWordPosition="4941">amed entities which emerge every day on the WWW. In practical use, for example, it will be better to update the model every week with Updating NER to keep up with new named entities, and occasionally perform active learning (every six months or so) to enhance the entire model. In the future, we plan to evaluate the efficiency of our two learning methods in practical applications, such as domain adaptation and acquisition of hot trend NE words from blog texts on the WWW. 6 Related Works To date, there have been many related works on active learning not only for the NER task (Shen et al., 2004, Laws and Schütze, 2008) but also for other tasks, such as POS tagging (Engelson and Dagan, 1996), text classification (Lewis and Catlett, 1994), parsing (Hwa, 2000), and confusion set disambiguation (Banko and Brill, 2001). Active learning aims at effective data selection based on criterion measures, such as the confidence measure. Most previous works focus on the Sentence-Based criterion evaluation and data selection. Our proposal differs from those previous works in that we focus on the Tag-Based strategy, which judges whether each tag should be accepted or rejected. This approach maximizes the effectiveness of ma</context>
</contexts>
<marker>Laws, Schütze, 2008</marker>
<rawString>F. Laws and H. Schütze. 2008. Stopping Criteria for Active Learning of Named Entity Recognition. In Proc. of COLING-2008, pages 465-472.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Lewis</author>
<author>J Gatlett</author>
</authors>
<title>Heterogeneous uncertainty sampling for supervised learning.</title>
<date>1994</date>
<booktitle>In Proc. of ICML-1994,</booktitle>
<pages>148--156</pages>
<marker>Lewis, Gatlett, 1994</marker>
<rawString>D. Lewis and J. Gatlett. 1994. Heterogeneous uncertainty sampling for supervised learning. In Proc. of ICML-1994, pages 148-156.</rawString>
</citation>
<citation valid="true">
<authors>
<author>C D Manning</author>
<author>H Schütze</author>
</authors>
<title>Foundations of Statistical Natural Language Processing.</title>
<date>1999</date>
<publisher>The MIT Press.</publisher>
<contexts>
<context position="7931" citStr="Manning and Schütze, 1999" startWordPosition="1362" endWordPosition="1365">We used n-grams (n=1, 2, 3) of surface forms and partsof-speech within a five word window and 2-gram combinations of NE tags as the feature set. +Σ (, ))}, t t i − 1 i ⋅ fb b λb n 1 = exp{ ( Σ Σ ⋅ λa fa (ti , wi a = ) Z(W) i 1 P T W ( | ) (1) ) T i 1 (2) +Σ (ti−1 , ti b ))}. ⋅ fb λb 169 The Word Sequence The Tag Candidates w1 w2 K wi −1 wi wi+1 K wn &lt;S&gt; t1 ,1 t2, 1 K ti −1,1 ti , 1 ti+1,1 K tn ,1 &lt;/S&gt; M M M M M M t1, M t1,k t2,k K ti−1,k ti,k ti+1,k K tn,k j t 2,jK ti− ti , ti+1,j K t 1 , j j n,j M αi, j βi,j Figure 1. Overview of the tag confidence measure calculation. algorithms as follows (Manning and Schütze, 1999): 1 P(ti,j |W) = Z(W)αi,j⋅flo, (4) where α0,j = 1, (7) fln+1, j =1. (8) In this manner, the confidence scores of all tags of each word in a given sentence are calculated. The rejecter then refers to the highest tag confidence score in judging whether the decoded NE tag is correct or incorrect. 3.2 Rejecter The rejecter tries to detect dubious tags in the NER result derived by the method described in Section 2. For each word, the rejecter refers to the decoded tag td, which maximizes Formula (1), and the most confident tag t1, in terms of the posterior probability as defined in Formula (4). The</context>
</contexts>
<marker>Manning, Schütze, 1999</marker>
<rawString>C. D. Manning and H. Schütze. 1999. Foundations of Statistical Natural Language Processing. The MIT Press.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Pantel</author>
<author>M Pennacchiotti</author>
</authors>
<title>Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations.</title>
<date>2006</date>
<booktitle>In Proc. of COLINGACL-2006,</booktitle>
<pages>113--120</pages>
<marker>Pantel, Pennacchiotti, 2006</marker>
<rawString>P. Pantel and M. Pennacchiotti. 2006. Espresso: Leveraging Generic Patterns for Automatically Harvesting Semantic Relations. In Proc. of COLINGACL-2006, pages 113-120.</rawString>
</citation>
<citation valid="true">
<authors>
<author>E F T K Sang</author>
<author>F De Meulder</author>
</authors>
<title>Representing text chunks.</title>
<date>1999</date>
<booktitle>In Proc. of EACL-1999,</booktitle>
<pages>173--179</pages>
<marker>Sang, De Meulder, 1999</marker>
<rawString>E. F. T. K. Sang and F. De Meulder. 1999. Representing text chunks. In Proc. of EACL-1999, pages 173-179.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Shen</author>
<author>J Zhang</author>
<author>J Su</author>
<author>G Zhou</author>
<author>C L Tan</author>
</authors>
<title>Multi-Criteria-based Active Learning for Named Entity Recognition.</title>
<date>2004</date>
<booktitle>In Proc. of ACL-2004,</booktitle>
<pages>589--596</pages>
<contexts>
<context position="2725" citStr="Shen et al., 2004" startWordPosition="412" endWordPosition="415">ntly as an informative resource for information retrieval and information extraction tasks. CGM has two distinctive features; enormous quantities of new texts are generated day after day, and new vocabularies and topics come and go rapidly. The most effective approach to keep up with new linguistic phenomena is creating new annotated corpora for model re-training at short intervals. However, it is difficult to build new corpora expeditiously because of the high manual costs imposed by traditional schemes. To reduce the manual labor and costs, various learning methods, such as active learning (Shen et al., 2004, Laws and Schütze, 2008), semi-supervised learning (Suzuki and Isozaki, 2008) and bootstrapping (Etzioni, 2005) have been proposed. Active learning automatically selects effective texts to be annotated from huge raw-text corpora. The correct answers are then manually annotated, and the model is re-trained. In active learning, one major issue is data selection, namely, determining which sample data is most effective. The data units used in conventional methods are sentences. Automatically creating annotated corpora would dramatically decrease the manual costs. In fact, there always are some re</context>
<context position="28283" citStr="Shen et al., 2004" startWordPosition="4934" endWordPosition="4937"> current with new named entities which emerge every day on the WWW. In practical use, for example, it will be better to update the model every week with Updating NER to keep up with new named entities, and occasionally perform active learning (every six months or so) to enhance the entire model. In the future, we plan to evaluate the efficiency of our two learning methods in practical applications, such as domain adaptation and acquisition of hot trend NE words from blog texts on the WWW. 6 Related Works To date, there have been many related works on active learning not only for the NER task (Shen et al., 2004, Laws and Schütze, 2008) but also for other tasks, such as POS tagging (Engelson and Dagan, 1996), text classification (Lewis and Catlett, 1994), parsing (Hwa, 2000), and confusion set disambiguation (Banko and Brill, 2001). Active learning aims at effective data selection based on criterion measures, such as the confidence measure. Most previous works focus on the Sentence-Based criterion evaluation and data selection. Our proposal differs from those previous works in that we focus on the Tag-Based strategy, which judges whether each tag should be accepted or rejected. This approach maximize</context>
</contexts>
<marker>Shen, Zhang, Su, Zhou, Tan, 2004</marker>
<rawString>D. Shen, J. Zhang, J. Su, G. Zhou, and C. L. Tan. 2004. Multi-Criteria-based Active Learning for Named Entity Recognition. In Proc. of ACL-2004, pages 589-596.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>H Izozaki</author>
</authors>
<title>Semi-Supervised Sequential Labeling and Segmentation using Gigaword Scale Unlabeled Data.</title>
<date>2008</date>
<booktitle>In Proc. of ACL-2008,</booktitle>
<pages>665--673</pages>
<marker>Suzuki, Izozaki, 2008</marker>
<rawString>J. Suzuki and H. Izozaki. 2008. Semi-Supervised Sequential Labeling and Segmentation using Gigaword Scale Unlabeled Data. In Proc. of ACL-2008, pages 665-673.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Suzuki</author>
<author>E McDermott</author>
<author>H Isozaki</author>
</authors>
<title>Training Conditional Random Fields with Multivariate Evaluation Measures.</title>
<date>2006</date>
<booktitle>In Proc. of COLING-ACL2006.</booktitle>
<pages>617--624</pages>
<contexts>
<context position="1915" citStr="Suzuki et al., 2006" startWordPosition="281" endWordPosition="284">pdates its model. 1 Introduction Machine learning, especially supervised learning, has achieved great success in many natural language tasks, such as part-of-speech (POS) tagging, named entity recognition (NER), and parsing. This approach automatically encodes linguistic knowledge as statistical parameters (models) from large annotated corpora. In the NER task, which is the focus of this paper, sequential tagging1 based on statistical models is 1Tags are assigned to each input unit (e.g., word) one by one. similarly used; studies include Conditional Random Fields (CRFs; Lafferty et al., 2001, Suzuki et al., 2006). However, the manual costs incurred in creating annotated corpora are extremely high. On the other hand, Consumer Generated Media (CGM) such as blog texts has attracted a lot of attention recently as an informative resource for information retrieval and information extraction tasks. CGM has two distinctive features; enormous quantities of new texts are generated day after day, and new vocabularies and topics come and go rapidly. The most effective approach to keep up with new linguistic phenomena is creating new annotated corpora for model re-training at short intervals. However, it is diffic</context>
<context position="5556" citStr="Suzuki et al., 2006" startWordPosition="885" endWordPosition="888">cheme (Sang and De Meulder, 1999) combined with eight Japanese NE types defined in the IREX workshop (IREX 1999) as shown in Table 1. For example, “ 東 京 (Tokyo)/ 都 (City)/ に (in)” is labeled like this: “東京/B-&lt;LOC&gt; 都/I-&lt;LOC&gt; に/O”. This task is regarded as the sequential tagging problem, i.e., assigning NE tag sequences T = t1 &amp;quot;&apos;tn to word sequences W = w1 &amp;quot;&apos; wn . Recently, discriminative models such as Conditional Random Fields (CRFs) have been successfully applied to this task (Lafferty et al., 2001). In this paper, we use linear-chain CRFs based on the Minimum Classification Error framework (Suzuki et al., 2006). The posterior probability of a tag sequence is calculated as follows: where wi and ti are the i-th word and its corresponding NE tag, respectively. fa (ti , wi ) 2 A base model is the initial model trained with the initial annotated corpora. and fb (ti−1, ti) is a feature function 3 . λa and λb is a parameter to be estimated from the training data. Z(W) is a normalization factor over all candidate paths expressed as follows: n Z(W) exp{ ( = Σ Σ Σ λa ⋅ fa (ti, wi a = The best tag sequence that maximizes Formula (1) is located using the Viterbi algorithm. Table 1. NE Types and Tags. NE Types N</context>
</contexts>
<marker>Suzuki, McDermott, Isozaki, 2006</marker>
<rawString>J. Suzuki, E. McDermott, and H. Isozaki. 2006. Training Conditional Random Fields with Multivariate Evaluation Measures. In Proc. of COLING-ACL2006. pages 617-624.</rawString>
</citation>
<citation valid="true">
<authors>
<author>D Yarowsky</author>
</authors>
<title>Unsupervised Word Sense Disambiguation Rivaling Supervised Methods.</title>
<date>1995</date>
<booktitle>In Proc. of ACL-1995,</booktitle>
<pages>189--196</pages>
<contexts>
<context position="29390" citStr="Yarowsky, 1995" startWordPosition="5107" endWordPosition="5108">the Tag-Based strategy, which judges whether each tag should be accepted or rejected. This approach maximizes the effectiveness of manual annotation by leaving the accepted tags in without any manual correction. As a result, our Tag-based approach reduces the manual annotation cost by 66 %, compared to the Sentence-Base method. Semi-supervised learning has become an active area in machine learning; it utilizes not only annotated corpora but also huge amounts of plain text for model training. Several studies adapted semi-supervised learning to suit NLP tasks, such as word sense disambiguation (Yarowsky, 1995), text classification (Fujino et al., 2008), and chunking and NER (Suzuki and Isozaki, 2008). Suzuki and Isozaki (2008) suggest that a GIGAword size plain text corpus may further improve the performance of the state-of-the-art NLP system. In this paper, however, we aim at model adaptation to the CGM domain to keep up with the new linguistic phenomena that are emerging every day. Because it is difficult to obtain GIGAword size plain text sets that reflect such new linguistic phenomena, it is not practical to directly apply this approach to our task. Bootstrapping is similar to semi-supervised l</context>
</contexts>
<marker>Yarowsky, 1995</marker>
<rawString>D. Yarowsky. 1995. Unsupervised Word Sense Disambiguation Rivaling Supervised Methods. In Proc. of ACL-1995, pages 189-196.</rawString>
</citation>
<citation valid="true">
<authors>
<author>X Zhu</author>
</authors>
<date>2007</date>
<booktitle>Semi-Supervised Learning, ICML2007 Tutorial.</booktitle>
<marker>Zhu, 2007</marker>
<rawString>X. Zhu. 2007. Semi-Supervised Learning, ICML2007 Tutorial.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>