<?xml version="1.0" encoding="UTF-8"?>
<algorithms version="110505">
<algorithm name="SectLabel" version="110505">
<variant no="0" confidence="0.000090">
<title confidence="0.99897">
Three models for discriminative machine translation using
Global Lexical Selection and Sentence Reconstruction
</title>
<author confidence="0.918657">
Sriram Venkatapathy
</author>
<affiliation confidence="0.888316">
Language Technologies Research
</affiliation>
<address confidence="0.4506515">
Centre, IIIT-Hyderabad
Hyderabad - 500019, India.
</address>
<email confidence="0.947247">
sriram@research.iiit.ac.in
</email>
<title confidence="0.258522">
Srinivas Bangalore
</title>
<author confidence="0.478695">
AT&amp;T Labs - Research
Florham Park, NJ 07932
</author>
<affiliation confidence="0.490534">
USA
</affiliation>
<email confidence="0.998247">
srini@research.att.com
</email>
<sectionHeader confidence="0.993877" genericHeader="abstract">
Abstract
</sectionHeader>
<bodyText confidence="0.999948761904762">
Machine translation of a source language
sentence involves selecting appropriate
target language words and ordering the se-
lected words to form a well-formed tar-
get language sentence. Most of the pre-
vious work on statistical machine transla-
tion relies on (local) associations of target
words/phrases with source words/phrases
for lexical selection. In contrast, in this
paper, we present a novel approach to lex-
ical selection where the target words are
associated with the entire source sentence
(global) without the need for local asso-
ciations. This technique is used by three
models (Bag–of–words model, sequential
model and hierarchical model) which pre-
dict the target language words given a
source sentence and then order the words
appropriately. We show that a hierarchi-
cal model performs best when compared
to the other two models.
</bodyText>
<sectionHeader confidence="0.999133" genericHeader="introduction">
1 Introduction
</sectionHeader>
<bodyText confidence="0.999926230769231">
The problem of machine translation can be viewed
as consisting of two subproblems: (a) lexical se-
lection, where appropriate target language lexi-
cal items are chosen for each source language
lexical item and (b) lexical reordering, where
the chosen target language lexical items are rear-
ranged to produce a meaningful target language
string. Most of the previous work on statisti-
cal machine translation, as exemplified in (Brown
et al., 1993), employs word–alignment algorithm
(such as GIZA++ (Och et al., 1999)) that provides
local associations between source words and target
words. The source–to–target word–alignments are
</bodyText>
<page confidence="0.901674">
96
</page>
<bodyText confidence="0.998720954545455">
sometimes augmented with target–to–source word
alignments in order to improve the precision of
these local associations. Further, the word–level
alignments are extended to phrase–level align-
ments in order to increase the extent of local asso-
ciations. The phrasal associations compile some
amount of (local) lexical reordering of the target
words—those permitted by the size of the phrase.
Most of the state–of–the–art machine translation
systems use these phrase–level associations in
conjunction with a target language model to pro-
duce the target sentence. There is relatively little
emphasis on (global) lexical reordering other than
the local re-orderings permitted within the phrasal
alignments. A few exceptions are the hierarchical
(possibly syntax–based) transduction models (Wu,
1997; Alshawi et al., 1998; Yamada and Knight,
2001; Chiang, 2005) and the string transduction
models (Kanthak et al., 2005).
In this paper, we present three models for doing
discriminative machine translation using global
lexical selection and lexical reordering.
</bodyText>
<listItem confidence="0.982962733333334">
1. Bag–of–Words model: Given a source sen-
tence, each of the target words are chosen by
looking at the entire source sentence. The
target language words are then permuted in
various ways and then, the best permutation
is chosen using the language model on the
target side. The size of the search space of
these permutations can be set by a parameter
called the permutation window. This model
does not allow long distance re-orderings of
target words unless a very large permutation
window chosen which is very expensive.
2. Sequential Lexical Choice model : Given
a source sentence, the target words are pre-
dicted in an order which is faithful to the or-
</listItem>
<affiliation confidence="0.4609145">
Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 96–102,
Rochester, New York, April 2007. c�2007 Association for Computational Linguistics
</affiliation>
<bodyText confidence="0.999791">
der of words in the source sentence. Now,
the number of permutations that need to be
examined to obtain the best target language
strings are much less when compared to the
Bag–of–Words model. This model is ex-
pected to give good results for language pairs
such as English–French for which only lo-
cal word order variations exist between sen-
tences.
</bodyText>
<sectionHeader confidence="0.494075" genericHeader="method">
3. Hierarchical lexical association and re-
</sectionHeader>
<bodyText confidence="0.988407076923077">
ordering model : For language pairs such
as English–Hindi or English–Japanese where
there is a high degree of global reordering
(Figure 1), it is necessary to be able to handle
long distance movement of words/phrases.
In this approach, the target words predicted
through global lexical selection are associ-
ated with various nodes of the source depen-
dency tree and then, hierarchical reordering is
done to obtain the order of words in the tar-
get sentence. Hierarchical reordering allows
phrases to distort to longer distances than the
previous two models.
</bodyText>
<figureCaption confidence="0.8848265">
Figure 1: Sample distortion between En-
glish–Hindi
</figureCaption>
<bodyText confidence="0.999886">
The outline of the paper is as follows. In Section
2, we talk about the global lexical selection. Sec-
tion 3 describes three models for global lexical se-
lection and reordering. In Section 4, we report the
results of the translation models on English–Hindi
language pair and contrast the strengths and limi-
tations of the models.
</bodyText>
<sectionHeader confidence="0.908357" genericHeader="method">
2 Global lexical selection
</sectionHeader>
<bodyText confidence="0.991197266666667">
For global lexical selection, in contrast to the
local approaches of associating target words to
the source words, the target words are associated
to the entire source sentence. The intuition is
that there may be lexico–syntactic features of the
source sentence (not necessarily a single source
word) that might trigger the presence of a target
word in the target sentence. Furthermore, it might
be difficult to exactly associate a target word to
a source sentence in many situations - (a) when
translations are not exact but paraphrases (b) the
target language does not have one lexical item
to express the same concept that is expressed in
the source word. The extensions of word align-
ments to phrasal alignments attempt to address
some of these situations in additional to alleviat-
ing the noise in word–level alignments.
As a consequence of the global lexical selection
approach, we no longer have a tight association
between source language words/phrases and tar-
get language words/phrases. The result of lexical
selection is simply a bag of words(phrases) in the
target language and the target sentence has to be
reconstructed using this bag of words.
The target words in the bag, however, might
be enhanced with rich syntactic information that
could aid in the reconstruction of the target sen-
tence. This approach to lexical selection and
sentence reconstruction has the potential to cir-
cumvent the limitations of word–alignment based
methods for translation between significantly dif-
ferent word order languages. However, in this pa-
per, to handle large word order variations, we asso-
ciate the target words with source language depen-
dency structures to enable long distance reorder-
ing.
3 Training the discriminative models for
lexical selection and reordering
In this section, we present our approach for a
global lexical selection model which is based on
discriminatively trained classification techniques.
Discriminant modeling techniques have become
the dominant method for resolving ambiguity in
speech and natural language processing tasks, out-
performing generative models for the same task.
We expect the discriminatively trained global lex-
ical selection models to outperform generatively
trained local lexical selection models as well as
provide a framework for incorporating rich mor-
pho–syntactic information.
Statistical machine translation can be formu-
lated as a search for the best target sequence that
maximizes P(T I S), where S is the source sen-
tence and T is the target sentence. Ideally, P(T
S) should be estimated directly to maximize the
conditional likelihood on the training data (dis-
criminant model). However, T corresponds to
a sequence with a exponentially large combina-
tion of possible labels, and traditional classifica-
tion approaches cannot be used directly. Although
</bodyText>
<page confidence="0.997454">
97
</page>
<bodyText confidence="0.959813">
Conditional Random Fields (CRF) (Lafferty et al.,
2001) train an exponential model at the sequence
level, in translation tasks such as ours the compu-
tational requirements of training such models are
prohibitively expensive.
</bodyText>
<subsectionHeader confidence="0.999631">
3.1 Bag-of-Words Lexical Choice Model
</subsectionHeader>
<bodyText confidence="0.999680241379311">
This model doesn’t require the sentences to be
word aligned in order to learn the local associa-
tions. Instead, we take the sentence aligned cor-
pus as before but we treat the target sentence as a
bag–of–words or BOW assigned to the source sen-
tence. The goal is, given a source sentence S, to
estimate the probability that we find a given word
(tj) in its translation ie.., we need to estimate the
probabilities P(true|tj, S) and P(false|tj, S).
To train such a model, we need to build binary
classifiers for all the words in the target lan-
guage vocabulary. The probability distributions
of these binary classifiers are learnt using maxi-
mum entropy model (Berger et al., 1996; Haffner,
2006). For the word tj, the training sentence
pairs are considered as positive examples where
the word appears in the target, and negative other-
wise. Thus, the number of training examples for
each binary classifier equals the number of train-
ing examples. In this model, classifiers are train-
ing using n–gram features (BOgrams(S)).
During decoding, instead of producing the tar-
get sentence directly, what we initially obtain is
the target bag of words. Each word in the target
vocabulary is detected independently, so we have
here a very simple use of binary static classifiers.
Given a sentence S, the bag of words (BOW(T)
contains those words whose distributions have the
positive probability greater than a threshold (r).
</bodyText>
<equation confidence="0.9985005">
BOW(T) _ {t  |P(true  |t,BOgrams(S)) &gt; -r}
(1)
</equation>
<bodyText confidence="0.9998966">
In order to reconstruct the proper order of words
in the target sentence, we consider various permu-
tations of words in BOW (T) and weight them by
a target language model. Considering all possible
permutations of the words in the target sentence
is computationally not feasible. But, the number
of permutations examined can be reduced by us-
ing heuristic forward pruning or by constraining
the permutations to be within a local window of
adjustable size (also see (Kanthak et al., 2005)).
We have chosen to constrain permutations here.
Constraining the permutation using a local win-
dow can provide us some very useful local re-
orderings.
The bag–of–words approach can also be modi-
fied to allow for length adjustments of target sen-
tences, if we add optional deletions in the final
step of permutation decoding. The parameter T
and an additional word deletion penalty 6 can then
be used to adjust the length of translated outputs.
</bodyText>
<subsectionHeader confidence="0.999647">
3.2 Sequential Lexical Choice Model
</subsectionHeader>
<bodyText confidence="0.999658739130435">
The previous approach gives us a predetermined
order of words initially which are then permuted to
obtain the best target string. Given that we would
not be able to search the entire space, it would be a
helpful if we could start searching various permu-
tations using a more definite string. One such def-
inite order in which the target words can be placed
is the order of source words itself. In this model,
during the lexical selection, we try to place the
target words in an order which is faithful to the
source sentence.
This model associates sets of target words with
every position in the source sentence and yet re-
tains the power of global lexical selection. For
every position (i) of the source sentence, a prefix
string is formed which consists of the sequence of
words from positions 1 to i. Each of these prefix
strings are used to predict bags of target words us-
ing the global lexical selection. Now, these bags
generated using the prefix strings are processed in
the order of source positions. Let Ti be the bag of
target words generated by prefix string i (Figure
2).
</bodyText>
<figureCaption confidence="0.9554865">
Figure 2: The generation of target bags associated
with source sentence position
</figureCaption>
<bodyText confidence="0.999462444444445">
The goal is to associate a set of target words
with every source position. A target word t
is attached to the ith source position if it is
present in Ti but not in Ti−1 and the probability
P (true|t, Ti) &gt; -r. The intuition behind this ap-
proach is that a word t is associated with a position
i if there was some information present at the ith
source position that triggered the probability of the
t to exceed the threshold T.
</bodyText>
<equation confidence="0.999837">
T (i)
T (i+1)
i i+1
</equation>
<page confidence="0.991738">
98
</page>
<bodyText confidence="0.999011833333333">
Hence, the initial target string is the sequence
of target language words associated with the se-
quence of source language positions. This string
is now permuted in all possible ways (section 3.1)
and the best target string is chosen using the lan-
guage model.
</bodyText>
<subsectionHeader confidence="0.9687625">
3.3 Hierarchical lexical association and
reordering model
</subsectionHeader>
<bodyText confidence="0.999932571428572">
The Sequential Lexical Choice Model presented in
the last section is expected to work best for lan-
guage pairs for which there are mostly local word
order variations. For language pairs with signifi-
cant word order variation, the search for the target
string may still fail examine the best target lan-
guage string given the source sentence. The model
proposed in this section should be able to handle
such long distance movement of words/phrases.
In this model, the goal is to search for the best
target string T which maximizes the probability
P(T|S,D(S)), where S is the source sentence
and D(S) is the dependency structure associated
with the source sentence S. The probabilities of
the target words given the source sentence are
estimated in the same way as the bag–of–words
model. The only main difference during the esti-
mation stage is that we consider the dependency
tree based features apart from the n-gram features.
The decoding of the source sentence S takes
place in three steps,
</bodyText>
<listItem confidence="0.978583846153846">
1. Predict the bag–of–words : Given a source
sentence S, predict the bag of words BOW(T)
whose distributions have a positive probabil-
ities greater than a threshold (r).
2. Attachment to Source nodes : These target
words are now attached to the nodes of source
dependency trees. For making the attach-
ments, the probability distributions of target
words conditioned on features local to the
source nodes are used.
3. Ordering the target language words : Tra-
verse the source dependency tree in a bottom-
up fashion to obtain the best target string.
</listItem>
<subsectionHeader confidence="0.795362">
3.3.1 Predict the bag–of–words
</subsectionHeader>
<bodyText confidence="0.985211666666667">
Given a source sentence S, all the target words
whose positive probability distributions are above
r are included in the bag.
</bodyText>
<equation confidence="0.997175">
BOW(T) = {t  |P(true|t, f(S))} (2)
</equation>
<bodyText confidence="0.999800666666667">
In addition to the n–gram features, this model uses
cues provided by the dependency structure to pre-
dict the target bag–of–words.
</bodyText>
<equation confidence="0.463039">
S3 S4
</equation>
<figureCaption confidence="0.957883">
Figure 3: Dependency tree of a source sentence
with words s1, s2, s3, s4 and s5
</figureCaption>
<bodyText confidence="0.807635">
Hence, the features that we have considered in
the model are (Figure 3),
</bodyText>
<listItem confidence="0.998300125">
1. N-grams. For example, in Figure 2, ‘s1’, ‘s2
s3 s4’, ‘s4 s5’ etc.
2. Dependency pair (The pair of nodes and its
parents). Example in Figure 2., ‘s2 s1’, ‘s4
s2’ etc.
3. Dependency treelet (The triplet of a node, it’s
parent and sibling). For example, ‘s3 s2 s4’,
‘s2 s1 s5’ etc.
</listItem>
<subsectionHeader confidence="0.88403">
3.3.2 Attachment to Source nodes
</subsectionHeader>
<bodyText confidence="0.916603142857143">
For every target word tj in the bag, the most
likely source nodes are determined by measuring
the positive distribution of the word tj given the
features of the particular node (Figure 4). Let
S(tj) denote the set of source nodes to which the
word tj can be attached to, then S(tj) is deter-
mined as,
</bodyText>
<figureCaption confidence="0.772779">
Figure 4: Dependency tree of a source sentence
with words S1, S2, S3, S4 and S5
</figureCaption>
<equation confidence="0.998121">
S(tj) = argmaxs(P (true|tj, f(s)) (3)
</equation>
<bodyText confidence="0.9990445">
where f(s) denotes the features of S in which
only those features are active which contain the
</bodyText>
<equation confidence="0.910266625">
S2
S5
S1
S2
S5
S3 S4
T1 T2 T3 T4
S1
</equation>
<page confidence="0.991284">
99
</page>
<bodyText confidence="0.999938416666666">
lexical item representing the node s. The target
words are in the global bag are processed in the
order of their global probabilities p(tjS). While
attaching the target words, it is ensured that no
source node had more than p target words attached
to it. Also, a target word should not be attached
to more to more than or number of times. There
is another constraint that can be applied to ensure
that the ratio of the total target words (which are
attached to source nodes) to the total number of
words in the source sentence does exceed a value
(�).
</bodyText>
<subsectionHeader confidence="0.999191">
3.4 Ordering the target language words
</subsectionHeader>
<bodyText confidence="0.999466166666667">
In this step, the source sentence dependency tree is
traversed in a bottom–up fashion. At every node,
the best possible order of target words associated
with the sub-tree rooted at the node is determined.
This string is then used as a cohesive unit by the
superior nodes.
</bodyText>
<figureCaption confidence="0.723384">
Figure 5: The target string associated with node
</figureCaption>
<bodyText confidence="0.935657583333333">
S1 is determined by permuting strings attached to
the children (in rectangular boxes, to signify that
they are frozen) and the lexical items attached to
S1
For example, in Figure 5, let ‘t1 t2 t3’, ‘t4 t5’
be the best strings associated with the children of
nodes s2 and s3 respectively. Let t6 and t7 be the
words that are attached to node s1. The best string
for the node s1 is determined by permuting the
strings ‘t1 t2 t3’, ‘t4 t5’, ‘t6’ ‘t7’ in all possible
ways and then choosing the best string using the
language model.
</bodyText>
<sectionHeader confidence="0.99813" genericHeader="method">
4 Dataset
</sectionHeader>
<bodyText confidence="0.999991208333333">
The language pair that we considered for our ex-
periments are English–Hindi. The training set
consists of 37967 sentence pairs, the development
set contains 819 sentence pairs and the test set
has 699 sentence pairs. The dataset is from the
newspaper domain with topics ranging from pol-
itics to tourism. The sentence pairs have a maxi-
mum source sentence length of 30 words. The av-
erage length of English sentences is 18 while that
of Hindi sentences is 20.
The source language vocabulary is 41017 and
target sentence vocabulary is 48576. The to-
ken/type ratio of English in the dataset is 16.70
and that of Hindi is 15.64. This dataset is rela-
tively sparse. So, the translation accuracies on this
dataset would be relatively less when compared to
those on much larger datasets. In the target side
of the development corpus, the percentage of un-
seen tokens is 13.48%(3.87% types) while in the
source side, the percentage of unseen tokens is
10.77%(3.20% types). On furthur inspection of
a small portion of the dataset, we found that the
maximum percentage of the unseen words on the
target side are the named entities.
</bodyText>
<sectionHeader confidence="0.999968" genericHeader="evaluation">
5 Results
</sectionHeader>
<subsectionHeader confidence="0.989131">
5.1 Bag-of-Words model
</subsectionHeader>
<bodyText confidence="0.961992411764706">
The quality of the bag–of–words obtained is gov-
erned by the parameter T (probability threshold).
To determine the best T value, we experiment with
various values of T and measure the lexical accu-
racies (F-score) of the bags generated on the de-
velopment set (See Figure 6). The total number
of features used for training this model are 53166
(with count-cutoff of 2).
Figure 6: Lexical Accuracies of the Bags-of-
words
Now, we order the bags of words obtained
through global selection to get the target lan-
guage strings. While reordering using the lan-
guage model, some of the noisy words from the
bag can be deleted by setting a deletion cost (b).
We experimented with various deletion costs, and
tuned it according to the best BLEU score that we
</bodyText>
<equation confidence="0.928472">
S3 S4
t1 t2 t3
t1 t2 t3 t7 t4 t5 t6
S2
S1
t6 t7
S5
t4 t5
</equation>
<page confidence="0.943651">
100
</page>
<bodyText confidence="0.950983333333333">
obtained on the development set. Figure 7 shows
the best BLEU scores obtained by reordering the
bags associated with various threshold values.
</bodyText>
<figureCaption confidence="0.825411">
Figure 7: Lexical Accuracies of the Bags-of-
words
</figureCaption>
<bodyText confidence="0.98773275">
We can see that we obtained the best BLEU
when we choose a threshold of 0.17 to obtain the
bag–of–words, when the deletion cost is set to 19.
The reference target strings of the development
set has 15986 tokens. So, while tuning the param-
eters, we should ensure that the bags (obtained us-
ing the global lexical selection) that we consider
have more tokens than 15986 to allow some dele-
tions during reordering, and in effect obtain the
target strings whose total token count is approx-
imately equal to 15986. Figure 8 shows the varia-
tion in BLEU scores for various deletion costs by
fixing the threshold at 0.17.
Figure 8: BLEU scores for various deletion costs
when the threshold for global lexical selection is
set to 0.17
On the test set, we now fix the threshold at 0.17
(T) and the deletion cost (b) at 19 to obtain the
target language strings. The BLEU score that we
obtained for this set is 0.0428.
</bodyText>
<subsectionHeader confidence="0.998901">
5.2 Sequential Lexical Choice Model
</subsectionHeader>
<bodyText confidence="0.99998275">
The lexical accuracy values of the sequence of
words obtained by the sequential lexical choice
model are comparable to those obtained using the
bag–of–words model. The real difference comes
for the BLEU score. The best BLEU score ob-
tained on the development set was 0.0586 when T
was set to 0.14 and deletion cost was 15. On the
test set, the BLEU score obtained was 0.0473.
</bodyText>
<subsectionHeader confidence="0.987674">
5.3 Tree based model
</subsectionHeader>
<bodyText confidence="0.998303888888889">
The lexical accuracy values of the words obtained
in this model are comparable to the lexical accu-
racy values of the bag of words model. The total
number of features used for training this model are
118839 (with count-cutoff of 2). On the develop-
ment set, we obtained a BLEU score of 0.0650 for
T set at 0.17 and the deletion cost set at 20. On
the test set, we obtained a BLEU score of 0.0498.
We can see that the BLEU scores are now bet-
ter than the ones obtained using any of the other
models discussed before. This is because the Tree
based model has both the strengths of the global
lexical selection that ensures high quality lexical
items in the target sentences and that of an efficient
reconstruction model which takes care of long dis-
tance reordering. The table summarizes the BLEU
scores obtained by the three models on the devel-
opment and test sets.
</bodyText>
<table confidence="0.962747">
Devel. Set Test. Set
Bag-of-Words 0.0545 0.0428
Sequential 0.0586 0.0473
Hierarchical 0.0650 0.0498
</table>
<tableCaption confidence="0.999629">
Table 1: Summary of the results
</tableCaption>
<sectionHeader confidence="0.99661" genericHeader="conclusions">
6 Conclusion
</sectionHeader>
<bodyText confidence="0.999896230769231">
In this paper, we present a novel approach to lex-
ical selection where the target words are associ-
ated with the entire source sentence (global) with-
out the need for local associations. This technique
is used by three models (Bag–of–words model, se-
quential model and hierarchical model) which pre-
dict the target language words given a source sen-
tence and then order the words appropriately. We
show that a hierarchical model performs best when
compared to the other two models. The hierar-
chical model presented in this paper has both the
strengths of the global lexical selection and effi-
cient reconstruction model.
</bodyText>
<page confidence="0.996172">
101
</page>
<bodyText confidence="0.9998715">
In the future, we are planning to improve the hi-
erarchical model by making two primary additions
</bodyText>
<listItem confidence="0.96238825">
• Handling cases of structural non-
isomorphism between source and target
sentences.
• Obtaining K-best target string per node of the
</listItem>
<bodyText confidence="0.99033625">
source dependency tree instead of just one
per node. This would allow us to explore
more possibilities without having to compro-
mise much on computational complexity.
</bodyText>
<sectionHeader confidence="0.99914" genericHeader="references">
References
</sectionHeader>
<reference confidence="0.999867071428572">
Hiyan Alshawi, Srinivas Bangalore, and Shona Dou-
glas. 1998. Automatic acquisition of hierarchical
transduction models for machine translation. In Pro-
ceedings ofthe 36th Annual Meeting Association for
Computational Linguistics, Montreal, Canada.
A.L. Berger, Stephen A. D. Pietra, D. Pietra, and J. Vin-
cent. 1996. A Maximum Entropy Approach to Nat-
ural Language Processing. Computational Linguis-
tics, 22(1):39–71.
P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer.
1993. The Mathematics of Machine Translation:
Parameter Estimation. Computational Linguistics,
16(2):263–312.
David Chiang. 2005. A hierarchical phrase-based
model for statistical machine translation. In Pro-
ceedings of the 43rd Annual Meeting of the Associa-
tion for Computational Linguistics (ACL’05), pages
263–270, Ann Arbor, Michigan, June. Association
for Computational Linguistics.
P. Haffner. 2006. Scaling large margin classifiers for
spoken language understanding. Speech Communi-
cation, 48(iv):239–261.
S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney.
2005. Novel reordering approaches in phrase-based
statistical machine translation. In Proceedings of
the ACL Workshop on Building and Using Parallel
Texts, pages 167–174, Ann Arbor, Michigan.
J. Lafferty, A. McCallum, and F. Pereira. 2001. Con-
ditional random fields: Probabilistic models for seg-
menting and labeling sequence data. In Proceedings
ofICML, San Francisco, CA.
Franz Och, Christoph Tillmann, and Herman Ney.
1999. Improved alignment models for statistical
machine translation. In In Proc. of the Joint Conf. of
Empirical Methods in Natural Language Processing
and Very Large Corpora, pages 20–28.
Dekai Wu. 1997. Stochastic Inversion Transduction
Grammars and Bilingual Parsing of Parallel Cor-
pora. Computational Linguistics, 23(3):377–404.
K. Yamada and K. Knight. 2001. A syntax-based sta-
tistical translation model. In Proceedings of 391h
ACL.
</reference>
<page confidence="0.998613">
102
</page>
</variant>
</algorithm>
<algorithm name="ParsHed" version="110505">
<variant no="0" confidence="0.238473">
<title confidence="0.8806315">Three models for discriminative machine translation using Global Lexical Selection and Sentence Reconstruction Sriram Language Technologies</title>
<affiliation confidence="0.776527">Centre,</affiliation>
<address confidence="0.983488">Hyderabad - 500019,</address>
<email confidence="0.935601">sriram@research.iiit.ac.in</email>
<author confidence="0.514769">Srinivas</author>
<affiliation confidence="0.691935">AT&amp;T Labs -</affiliation>
<address confidence="0.744225">Florham Park, NJ</address>
<email confidence="0.999428">srini@research.att.com</email>
<abstract confidence="0.999605863636364">Machine translation of a source language sentence involves selecting appropriate target language words and ordering the selected words to form a well-formed target language sentence. Most of the previous work on statistical machine translation relies on (local) associations of target words/phrases with source words/phrases for lexical selection. In contrast, in this paper, we present a novel approach to lexical selection where the target words are associated with the entire source sentence (global) without the need for local associations. This technique is used by three models (Bag–of–words model, sequential model and hierarchical model) which predict the target language words given a source sentence and then order the words appropriately. We show that a hierarchical model performs best when compared to the other two models.</abstract>
</variant>
</algorithm>
<algorithm name="ParsCit" version="110505">
<citationList>
<citation valid="true">
<authors>
<author>Hiyan Alshawi</author>
<author>Srinivas Bangalore</author>
<author>Shona Douglas</author>
</authors>
<title>Automatic acquisition of hierarchical transduction models for machine translation.</title>
<date>1998</date>
<booktitle>In Proceedings ofthe 36th Annual Meeting Association for Computational Linguistics,</booktitle>
<location>Montreal, Canada.</location>
<contexts>
<context position="2631" citStr="Alshawi et al., 1998" startWordPosition="378" endWordPosition="381">ts in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. 1. Bag–of–Words model: Given a source sentence, each of the target words are chosen by looking at the entire source sentence. The target language words are then permuted in various ways and then, the best permutation is chosen using the language model on the target side. The size of the search space of these permutations can be set by a parameter called the p</context>
</contexts>
<marker>Alshawi, Bangalore, Douglas, 1998</marker>
<rawString>Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas. 1998. Automatic acquisition of hierarchical transduction models for machine translation. In Proceedings ofthe 36th Annual Meeting Association for Computational Linguistics, Montreal, Canada.</rawString>
</citation>
<citation valid="true">
<authors>
<author>A L Berger</author>
<author>Stephen A D Pietra</author>
<author>D Pietra</author>
<author>J Vincent</author>
</authors>
<title>A Maximum Entropy Approach to Natural Language Processing.</title>
<date>1996</date>
<journal>Computational Linguistics,</journal>
<volume>22</volume>
<issue>1</issue>
<contexts>
<context position="8805" citStr="Berger et al., 1996" startWordPosition="1366" endWordPosition="1369">igned in order to learn the local associations. Instead, we take the sentence aligned corpus as before but we treat the target sentence as a bag–of–words or BOW assigned to the source sentence. The goal is, given a source sentence S, to estimate the probability that we find a given word (tj) in its translation ie.., we need to estimate the probabilities P(true|tj, S) and P(false|tj, S). To train such a model, we need to build binary classifiers for all the words in the target language vocabulary. The probability distributions of these binary classifiers are learnt using maximum entropy model (Berger et al., 1996; Haffner, 2006). For the word tj, the training sentence pairs are considered as positive examples where the word appears in the target, and negative otherwise. Thus, the number of training examples for each binary classifier equals the number of training examples. In this model, classifiers are training using n–gram features (BOgrams(S)). During decoding, instead of producing the target sentence directly, what we initially obtain is the target bag of words. Each word in the target vocabulary is detected independently, so we have here a very simple use of binary static classifiers. Given a sen</context>
</contexts>
<marker>Berger, Pietra, Pietra, Vincent, 1996</marker>
<rawString>A.L. Berger, Stephen A. D. Pietra, D. Pietra, and J. Vincent. 1996. A Maximum Entropy Approach to Natural Language Processing. Computational Linguistics, 22(1):39–71.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Brown</author>
<author>S D Pietra</author>
<author>V D Pietra</author>
<author>R Mercer</author>
</authors>
<title>The Mathematics of Machine Translation: Parameter Estimation.</title>
<date>1993</date>
<journal>Computational Linguistics,</journal>
<volume>16</volume>
<issue>2</issue>
<contexts>
<context position="1631" citStr="Brown et al., 1993" startWordPosition="237" endWordPosition="240">guage words given a source sentence and then order the words appropriately. We show that a hierarchical model performs best when compared to the other two models. 1 Introduction The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. Most of the previous work on statistical machine translation, as exemplified in (Brown et al., 1993), employs word–alignment algorithm (such as GIZA++ (Och et al., 1999)) that provides local associations between source words and target words. The source–to–target word–alignments are 96 sometimes augmented with target–to–source word alignments in order to improve the precision of these local associations. Further, the word–level alignments are extended to phrase–level alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the</context>
</contexts>
<marker>Brown, Pietra, Pietra, Mercer, 1993</marker>
<rawString>P. Brown, S.D. Pietra, V.D. Pietra, and R. Mercer. 1993. The Mathematics of Machine Translation: Parameter Estimation. Computational Linguistics, 16(2):263–312.</rawString>
</citation>
<citation valid="true">
<authors>
<author>David Chiang</author>
</authors>
<title>A hierarchical phrase-based model for statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05),</booktitle>
<pages>263--270</pages>
<publisher>Association</publisher>
<institution>for Computational Linguistics.</institution>
<location>Ann Arbor, Michigan,</location>
<contexts>
<context position="2671" citStr="Chiang, 2005" startWordPosition="386" endWordPosition="387">ociations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. 1. Bag–of–Words model: Given a source sentence, each of the target words are chosen by looking at the entire source sentence. The target language words are then permuted in various ways and then, the best permutation is chosen using the language model on the target side. The size of the search space of these permutations can be set by a parameter called the permutation window. This model does not a</context>
</contexts>
<marker>Chiang, 2005</marker>
<rawString>David Chiang. 2005. A hierarchical phrase-based model for statistical machine translation. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL’05), pages 263–270, Ann Arbor, Michigan, June. Association for Computational Linguistics.</rawString>
</citation>
<citation valid="true">
<authors>
<author>P Haffner</author>
</authors>
<title>Scaling large margin classifiers for spoken language understanding. Speech Communication,</title>
<date>2006</date>
<contexts>
<context position="8821" citStr="Haffner, 2006" startWordPosition="1370" endWordPosition="1371">rn the local associations. Instead, we take the sentence aligned corpus as before but we treat the target sentence as a bag–of–words or BOW assigned to the source sentence. The goal is, given a source sentence S, to estimate the probability that we find a given word (tj) in its translation ie.., we need to estimate the probabilities P(true|tj, S) and P(false|tj, S). To train such a model, we need to build binary classifiers for all the words in the target language vocabulary. The probability distributions of these binary classifiers are learnt using maximum entropy model (Berger et al., 1996; Haffner, 2006). For the word tj, the training sentence pairs are considered as positive examples where the word appears in the target, and negative otherwise. Thus, the number of training examples for each binary classifier equals the number of training examples. In this model, classifiers are training using n–gram features (BOgrams(S)). During decoding, instead of producing the target sentence directly, what we initially obtain is the target bag of words. Each word in the target vocabulary is detected independently, so we have here a very simple use of binary static classifiers. Given a sentence S, the bag</context>
</contexts>
<marker>Haffner, 2006</marker>
<rawString>P. Haffner. 2006. Scaling large margin classifiers for spoken language understanding. Speech Communication, 48(iv):239–261.</rawString>
</citation>
<citation valid="true">
<authors>
<author>S Kanthak</author>
<author>D Vilar</author>
<author>E Matusov</author>
<author>R Zens</author>
<author>H Ney</author>
</authors>
<title>Novel reordering approaches in phrase-based statistical machine translation.</title>
<date>2005</date>
<booktitle>In Proceedings of the ACL Workshop on Building and Using Parallel Texts,</booktitle>
<pages>167--174</pages>
<location>Ann Arbor, Michigan.</location>
<contexts>
<context position="2729" citStr="Kanthak et al., 2005" startWordPosition="393" endWordPosition="396">mount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. 1. Bag–of–Words model: Given a source sentence, each of the target words are chosen by looking at the entire source sentence. The target language words are then permuted in various ways and then, the best permutation is chosen using the language model on the target side. The size of the search space of these permutations can be set by a parameter called the permutation window. This model does not allow long distance re-orderings of target words unless a v</context>
<context position="10069" citStr="Kanthak et al., 2005" startWordPosition="1573" endWordPosition="1576">those words whose distributions have the positive probability greater than a threshold (r). BOW(T) _ {t |P(true |t,BOgrams(S)) &gt; -r} (1) In order to reconstruct the proper order of words in the target sentence, we consider various permutations of words in BOW (T) and weight them by a target language model. Considering all possible permutations of the words in the target sentence is computationally not feasible. But, the number of permutations examined can be reduced by using heuristic forward pruning or by constraining the permutations to be within a local window of adjustable size (also see (Kanthak et al., 2005)). We have chosen to constrain permutations here. Constraining the permutation using a local window can provide us some very useful local reorderings. The bag–of–words approach can also be modified to allow for length adjustments of target sentences, if we add optional deletions in the final step of permutation decoding. The parameter T and an additional word deletion penalty 6 can then be used to adjust the length of translated outputs. 3.2 Sequential Lexical Choice Model The previous approach gives us a predetermined order of words initially which are then permuted to obtain the best target </context>
</contexts>
<marker>Kanthak, Vilar, Matusov, Zens, Ney, 2005</marker>
<rawString>S. Kanthak, D. Vilar, E. Matusov, R. Zens, and H. Ney. 2005. Novel reordering approaches in phrase-based statistical machine translation. In Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 167–174, Ann Arbor, Michigan.</rawString>
</citation>
<citation valid="true">
<authors>
<author>J Lafferty</author>
<author>A McCallum</author>
<author>F Pereira</author>
</authors>
<title>Conditional random fields: Probabilistic models for segmenting and labeling sequence data.</title>
<date>2001</date>
<booktitle>In Proceedings ofICML,</booktitle>
<location>San Francisco, CA.</location>
<contexts>
<context position="7925" citStr="Lafferty et al., 2001" startWordPosition="1218" endWordPosition="1221">els as well as provide a framework for incorporating rich morpho–syntactic information. Statistical machine translation can be formulated as a search for the best target sequence that maximizes P(T I S), where S is the source sentence and T is the target sentence. Ideally, P(T S) should be estimated directly to maximize the conditional likelihood on the training data (discriminant model). However, T corresponds to a sequence with a exponentially large combination of possible labels, and traditional classification approaches cannot be used directly. Although 97 Conditional Random Fields (CRF) (Lafferty et al., 2001) train an exponential model at the sequence level, in translation tasks such as ours the computational requirements of training such models are prohibitively expensive. 3.1 Bag-of-Words Lexical Choice Model This model doesn’t require the sentences to be word aligned in order to learn the local associations. Instead, we take the sentence aligned corpus as before but we treat the target sentence as a bag–of–words or BOW assigned to the source sentence. The goal is, given a source sentence S, to estimate the probability that we find a given word (tj) in its translation ie.., we need to estimate t</context>
</contexts>
<marker>Lafferty, McCallum, Pereira, 2001</marker>
<rawString>J. Lafferty, A. McCallum, and F. Pereira. 2001. Conditional random fields: Probabilistic models for segmenting and labeling sequence data. In Proceedings ofICML, San Francisco, CA.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Franz Och</author>
<author>Christoph Tillmann</author>
<author>Herman Ney</author>
</authors>
<title>Improved alignment models for statistical machine translation. In</title>
<date>1999</date>
<booktitle>In Proc. of the Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora,</booktitle>
<pages>20--28</pages>
<contexts>
<context position="1700" citStr="Och et al., 1999" startWordPosition="247" endWordPosition="250">ely. We show that a hierarchical model performs best when compared to the other two models. 1 Introduction The problem of machine translation can be viewed as consisting of two subproblems: (a) lexical selection, where appropriate target language lexical items are chosen for each source language lexical item and (b) lexical reordering, where the chosen target language lexical items are rearranged to produce a meaningful target language string. Most of the previous work on statistical machine translation, as exemplified in (Brown et al., 1993), employs word–alignment algorithm (such as GIZA++ (Och et al., 1999)) that provides local associations between source words and target words. The source–to–target word–alignments are 96 sometimes augmented with target–to–source word alignments in order to improve the precision of these local associations. Further, the word–level alignments are extended to phrase–level alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations </context>
</contexts>
<marker>Och, Tillmann, Ney, 1999</marker>
<rawString>Franz Och, Christoph Tillmann, and Herman Ney. 1999. Improved alignment models for statistical machine translation. In In Proc. of the Joint Conf. of Empirical Methods in Natural Language Processing and Very Large Corpora, pages 20–28.</rawString>
</citation>
<citation valid="true">
<authors>
<author>Dekai Wu</author>
</authors>
<title>Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora.</title>
<date>1997</date>
<journal>Computational Linguistics,</journal>
<volume>23</volume>
<issue>3</issue>
<contexts>
<context position="2609" citStr="Wu, 1997" startWordPosition="376" endWordPosition="377">l alignments in order to increase the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. 1. Bag–of–Words model: Given a source sentence, each of the target words are chosen by looking at the entire source sentence. The target language words are then permuted in various ways and then, the best permutation is chosen using the language model on the target side. The size of the search space of these permutations can be set by a </context>
</contexts>
<marker>Wu, 1997</marker>
<rawString>Dekai Wu. 1997. Stochastic Inversion Transduction Grammars and Bilingual Parsing of Parallel Corpora. Computational Linguistics, 23(3):377–404.</rawString>
</citation>
<citation valid="true">
<authors>
<author>K Yamada</author>
<author>K Knight</author>
</authors>
<title>A syntax-based statistical translation model.</title>
<date>2001</date>
<booktitle>In Proceedings of 391h ACL.</booktitle>
<contexts>
<context position="2656" citStr="Yamada and Knight, 2001" startWordPosition="382" endWordPosition="385">e the extent of local associations. The phrasal associations compile some amount of (local) lexical reordering of the target words—those permitted by the size of the phrase. Most of the state–of–the–art machine translation systems use these phrase–level associations in conjunction with a target language model to produce the target sentence. There is relatively little emphasis on (global) lexical reordering other than the local re-orderings permitted within the phrasal alignments. A few exceptions are the hierarchical (possibly syntax–based) transduction models (Wu, 1997; Alshawi et al., 1998; Yamada and Knight, 2001; Chiang, 2005) and the string transduction models (Kanthak et al., 2005). In this paper, we present three models for doing discriminative machine translation using global lexical selection and lexical reordering. 1. Bag–of–Words model: Given a source sentence, each of the target words are chosen by looking at the entire source sentence. The target language words are then permuted in various ways and then, the best permutation is chosen using the language model on the target side. The size of the search space of these permutations can be set by a parameter called the permutation window. This m</context>
</contexts>
<marker>Yamada, Knight, 2001</marker>
<rawString>K. Yamada and K. Knight. 2001. A syntax-based statistical translation model. In Proceedings of 391h ACL.</rawString>
</citation>
</citationList>
</algorithm>
</algorithms>